<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-36"><a id="_idTextAnchor035"/>2</h1>
<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>Designing Some Common Low Latency Applications in C++</h1>
<p>In this chapter, we will look at some applications in different fields from video streaming, online gaming, real-time data analysis, and electronic trading. We will understand their behavior, and what features need to be executed in real time under extremely low-latency considerations. We will introduce the electronic trading ecosystem, since we will use that as a case study in the rest of the book, and build a system from scratch in C++, with a focus on understanding and using low latency ideas.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Understanding low latency performance in live video streaming applications</li>
<li>Understanding which low latency constraints matter in gaming applications</li>
<li>Discussing the design of Internet-of-Things (IoT) and retail analytics systems</li>
<li>Exploring low latency electronic trading</li>
</ul>
<p>This chapter’s goal is to dig into some of the technical aspects of low latency applications in different business areas. By the end of this chapter, you should be able to understand and appreciate the technical challenges that applications such as real-time video streaming, offline and online gaming applications, IoT machines and applications, and electronic trading face. You will be able to understand the different solutions that advancements in technology provide to solve these problems and make these businesses viable and profitable.</p>
<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Understanding low latency performance in live video streaming applications</h1>
<p>In this section, we will <a id="_idIndexMarker128"/>first discuss<a id="_idIndexMarker129"/> the details behind low latency performance in the context of video streaming applications. We will define the important concepts and terms relevant to live video streaming to build an understanding of the field and business use cases. We will understand what causes latencies in these applications and the business impact of those. Finally, we <a id="_idIndexMarker130"/>will discuss<a id="_idIndexMarker131"/> technologies, platforms, and solutions to build and support low latency video streaming applications.</p>
<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>Defining important concepts in low latency streaming</h2>
<p>Here, we will first<a id="_idIndexMarker132"/> define a few important concepts and terms when it comes to low latency streaming applications. Let us get started with a few basics and build up from there into more complex concepts.</p>
<h3>Latency in video streaming</h3>
<p><strong class="bold">Video streaming</strong> is <a id="_idIndexMarker133"/>defined as audio-video content delivered in real time or near real time. Latency in general refers to the time delay between an input event and the output event. In the context of live video streaming applications, latency refers specifically to the time from when a live video stream hits the camera on the recording device and then gets transported to the target audience’s screens and gets rendered and displayed there. It should be easy to intuitively understand why this is also referred to as <strong class="bold">glass-to-glass latency</strong> in <a id="_idIndexMarker134"/>the context of live video streaming applications. Glass-to-glass latency in video streaming applications is quite important regardless of the actual application, whether it be a video call, live video streams for other applications, or online video game rendering. In live streaming, video latency<a id="_idIndexMarker135"/> is basically the delay between when the video frame<a id="_idIndexMarker136"/> captures at the recorder’s side to when the video frame is displayed at the viewer’s side. Another commonly encountered term is <strong class="bold">lag</strong>, which <a id="_idIndexMarker137"/>often just refers to a higher-than-expected glass-to-glass latency, which the user may perceive as reduced or jittery performance.</p>
<h3>Video distribution services and content delivery networks</h3>
<p><strong class="bold">Video distribution service</strong> (<strong class="bold">VDS</strong>) is a<a id="_idIndexMarker138"/> fancy term for a relatively easy-to-understand concept. A VDS basically means the system responsible for taking multiple incoming streams of video and audio from the sources and presenting them to the viewers. One of the most well-known examples of a VDS would be a <strong class="bold">content delivery network</strong> (<strong class="bold">CDN</strong>). A CDN is a means of efficiently distributing<a id="_idIndexMarker139"/> context across the globe.</p>
<h3>Transcoding, transmuxing, and transrating</h3>
<p>Let us discuss three concepts that relate to encoding the audio-video stream:</p>
<ul>
<li><strong class="bold">Transcoding</strong> refers to the <a id="_idIndexMarker140"/>process of decoding a media stream from one format (so lower-level details such as codec, video size, sampling rates, encoder formats, etc.) and possibly recoding it in a different format or with different parameters.</li>
<li><strong class="bold">Transmuxing</strong> is similar to<a id="_idIndexMarker141"/> transcoding but here, the delivery format changes without any changes to the encoding, as in the case of transcoding.</li>
<li><strong class="bold">Transrating</strong> is also <a id="_idIndexMarker142"/>similar to transcoding, but we change the video bitrate; usually, it is compressed to a lower value. The video bitrate is the number of bits (or kilobits) being transferred per second and captures the information and quality in the video stream.</li>
<li>In the next section, we will understand the sources of latencies in low latency video streaming applications.</li>
</ul>
<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Understanding sources of latency in video streaming applications</h2>
<p>Let us look at the<a id="_idIndexMarker143"/> details of what happens in the glass-to-glass journey. Our ultimate motivation in this section is to understand the sources of latencies in video streaming applications. This figure describes at a high level what happens in the glass-to-glass journey from the camera to the display:</p>
<div><div><img alt="Figure 2.1 – Glass-to-glass journey in live video streaming applications" src="img/Figure_2.1_B19434.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Glass-to-glass journey in live video streaming applications</p>
<h3>Discussing the steps in the glass-to-glass journey</h3>
<p>We will begin by understanding all the steps and components involved in the glass-to-glass journey of low latency video streaming applications. There are two forms of latency – the initial startup latency and then the lag between video frames once the live stream starts. Typically for the<a id="_idIndexMarker144"/> user experience, a slightly longer startup latency is much preferred over lag between video frames, but there is usually a trade-off in trying to reduce one latency over the other. So, we need to understand which metric is more important for a specific use case and adjust the design and technical details appropriately. The following are the steps in the glass-to-glass journey from the broadcaster to the receivers:</p>
<ol>
<li>Camera capturing and processing the audio and video at the broadcaster</li>
<li>Video consumption and packaging at the broadcaster</li>
<li>Encoders transcoding, transmuxing, and transrating the content</li>
<li>Sending the data over the network over the appropriate protocol(s)</li>
<li>Distribution over a VDS such as a CDN</li>
<li>Reception at the receivers and buffering</li>
<li>Decoding the content on the viewer’s device</li>
<li>Dealing with packet drops, network changes, and so on at the receiver end</li>
<li>Rendering of the audio-video content on the viewer’s device of choice</li>
<li>Possibly collecting interactive inputs (selections, audio, video, etc.) from the viewer for interactive applications and sending them back to the broadcaster where<a id="_idIndexMarker145"/> needed</li>
</ol>
<p>Now that we have described the details behind content delivery from the sender to the receiver and possibly back to the sender, in the next section, we will describe where we have possibilities of latencies on that path. Typically, each step does not take a long time, but higher latencies in multiple components can accumulate and cause significant degradation in user performance.</p>
<h3>Describing possibilities of high latencies on the path</h3>
<p>We will look at the reasons for<a id="_idIndexMarker146"/> high latencies in low latency video streaming applications. There are numerous reasons for this on each of the components of the glass-to-glass path we discussed in the previous subsection.</p>
<h4>Physical distance, server load, and internet quality</h4>
<ul>
<li>This is an<a id="_idIndexMarker147"/> obvious one: the physical distance between the source and destination will affect the glass-to-glass latency. This is sometimes very obvious when streaming videos from a different country.</li>
<li>In addition to the distance, the quality of the internet connection itself can affect the streaming latency. Slow or limited bandwidth connections lead to instability, buffering, and lags.</li>
</ul>
<p>Depending on how many <a id="_idIndexMarker148"/>users are simultaneously streaming the videos and how much load that puts on the servers involved in the streaming path, the latency and user experience can vary. Overloaded servers <a id="_idIndexMarker149"/>lead to slower response times, higher latencies, buffering, and lag and can even make the streaming come to a grinding halt.</p>
<h4>Capture equipment and hardware</h4>
<p>The video and <a id="_idIndexMarker150"/>audio capture devices have a big impact on the glass-to-glass latencies. Taking an audio and video frame and turning it into digital signals takes time. Advanced systems such as recorders, encoders, processors, re-encoders, decoders, and re-transmitters have a significant impact on the final user experience. The capture equipment and hardware will determine the latency values.</p>
<h4>Streaming protocol, transmission, and jitter buffer</h4>
<p>Given the availability of different streaming protocols (as we will discuss shortly), the final choice can determine the latency of video streaming applications. If the protocol is not optimized for dynamic adaptive streaming, it can increase delays. Overall, there are two<a id="_idIndexMarker151"/> categories of protocols for live video streaming – HTTP-based and non-HTTP-based – and there are differences in latencies and scalability between the two broad options, which will change the performance of the final system.</p>
<p>The internet routes<a id="_idIndexMarker152"/> chosen on the way through the VDS can change the glass-to-glass latency. These routes can also change over time, and packets can be queued on some hops and can even arrive out of order at the receiver. The software that handles these issues is known as a <strong class="bold">jitter buffer</strong>. If the<a id="_idIndexMarker153"/> CDN has<a id="_idIndexMarker154"/> issues, that can also cause additional delays. Then, there are constraints such as the encoded bitrate (lower bitrates mean less data being transferred per unit time and lead to lower latencies), which can change the latencies encountered.</p>
<h4>Encoding – transcoding and transrating</h4>
<p>The encoding <a id="_idIndexMarker155"/>process determines the compression, format, and so on of the final video output, and the choice and quality of encoding protocols will have a huge impact on the performance. Also, there are many options for viewer devices (TVs, phones, PCs, Macs, etc.) and networks (3G, 4G, 5G, LAN, Wi-Fi, etc.) and a streaming provider needs to implement <strong class="bold">adaptive bitrate</strong> (<strong class="bold">ABR</strong>) to <a id="_idIndexMarker156"/>handle these efficiently. The computer or server running the encoder needs to have adequate CPU and memory resources for the encoding process to keep up with the incoming audio-video data. Whether we use encoding software on a computer or encoding hardware such<a id="_idIndexMarker157"/> as <em class="italic">BoxCaster</em> or <em class="italic">Teradek</em>, we<a id="_idIndexMarker158"/> incur processing latencies ranging from a few milliseconds to seconds. The tasks that the encoder needs to perform are to ingest the raw<a id="_idIndexMarker159"/> video data, buffer the content, and decode, process, and re-encode it before forwarding it.</p>
<h4>Decoded and played on the viewer’s device</h4>
<p>Assuming the<a id="_idIndexMarker160"/> content makes it to the viewer’s device without incurring noticeable latencies, the client still must decode, playback, and render the content. Video players do not render video segments one at a time as they receive them, but instead, have a buffer of received segments, usually in memory. This means several segments are buffered before the video begins to play and, depending on the actual size of the segment chosen, can cause latency on the end user side. For instance, if we choose a segment length that contains 10 seconds of video, the player at the end user must at least receive a complete segment before it can play it and will introduce an extra 10-second delay between the sender and receiver. Typically, these segments are between 2 and 10 seconds, trying to balance between optimizing network efficiency and glass-to-glass latency. Obviously, factors such as the viewer’s device, platform, hardware, CPU, memory, and player efficiency can add to the glass-to-glass latencies.</p>
<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>Measuring latencies in low latency video streaming</h2>
<p>Measuring <a id="_idIndexMarker161"/>latencies in low latency video<a id="_idIndexMarker162"/> streaming applications is not extremely complicated, since the latency range we care about should at least be a few seconds to be perceptible to the end user as a delay or lag. The easiest ways to measure end-to-end video latency are the following:</p>
<ul>
<li>The first place to start would be to use <a id="_idIndexMarker163"/>a <strong class="bold">clapperboard</strong> application. A clapperboard is a tool used to synchronize video and audio during filmmaking, and apps are available to detect synchronization issues between two streams due to latency.</li>
<li>Another option is to publish the video stream back to yourself to measure whether there are any latencies in the capturing, encoding, decoding, and rendering steps by taking the network out of it.</li>
<li>An obvious solution is to take a screenshot of two screens running the same live stream to spot differences.</li>
<li>The best<a id="_idIndexMarker164"/> solution to measure live<a id="_idIndexMarker165"/> video streaming latencies is to add a timestamp to the video stream itself at the source and then the receiver can use that to determine the glass-to-glass latencies. Obviously, the clocks used by the sender and the receiver need to be synchronized with each other reasonably well.</li>
</ul>
<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Understanding the impact of high latencies</h2>
<p>Before we understand the impact of high latencies on low latency video streaming applications, first, we need to define what the acceptable latency is for different applications. For video streaming applications that do not require a lot of real-time interactions, anything up to 5 seconds is acceptable. For streaming applications that need to support live and interactive use cases, anything up to 1 second is enough for the users. Obviously, for video-on-demand, latency is not an issue since it is pre-recorded and there is no live-streaming component. Overall, high latency in real-time live-streaming applications negatively impacts an end user’s experience. The key motivation for real time is that viewers want to feel connected and get the feeling of being in person. Large delays in receiving and rendering the content destroy the feeling of watching something in real time. One of the most annoying experiences occurs when a real-time video regularly pauses and buffers due to latencies.</p>
<p>Let us briefly discuss the main negative impacts of real-time video streaming applications due to latency.</p>
<h3>Low audio-video quality</h3>
<p>If the<a id="_idIndexMarker166"/> components of the streaming system cannot achieve real-time latencies, that usually leads to higher levels of compression. Due to the high levels of compression on the audio-video data, the audio quality can sound scrambled and scratchy at times and the video quality can be blurry and pixelated, so it's just a worse user experience overall.</p>
<h3>Buffering pauses and delays</h3>
<p>Buffering is one <a id="_idIndexMarker167"/>of the worst things that can ruin the user experience since the viewer experiences a jittery performance with constant pauses instead of having a smooth experience. This is very frustrating for viewers if a video keeps pausing to buffer and catch up, and will likely lead to the viewer quitting the video, the platform, or the business itself and never returning.</p>
<h3>Audio-video synchronization issues</h3>
<p>In many<a id="_idIndexMarker168"/> implementations of real-time audio-video streaming applications, the audio data is sent separately from the video data and thus the audio data can reach the receiver faster than the video data. This is because by its nature, audio data is smaller in size than video data, and due to high latencies, video data might lag behind audio data at the receiver’s end. This leads to problems with synchronization and hurts the viewer’s experience with real-time video streaming.</p>
<h3>Playback – rewinding and fast-forwarding</h3>
<p>High<a id="_idIndexMarker169"/> latencies can cause issues with rewinding and fast-forwarding, even when the applications aren’t necessarily in 100% real time. This is because the audio-video data will have to be resent so that the end user’s player can re-sync with the newly selected location.</p>
<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Exploring technologies for low latency video streaming</h2>
<p>In this section, we will look at <a id="_idIndexMarker170"/>different technology protocols that apply to the encoding, decoding, streaming, and distribution of audio-video data. These protocols are specially designed for low latency video streaming applications and platforms. These protocols fall into one of two broad categories – HTTP-based protocols <a id="_idIndexMarker171"/>and non-HTTP-based protocols – but for low latency video streaming, typically, HTTP-based protocols are the way to go, as we will see in this section.</p>
<div><div><img alt="Figure 2.2 – Live video streaming latencies and technologies" src="img/Figure_2.2_B19434.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Live video streaming latencies and technologies</p>
<h3>Non-HTTP-based protocols</h3>
<p>Non-HTTP-based protocols use<a id="_idIndexMarker172"/> a combination <a id="_idIndexMarker173"/>of <strong class="bold">User Datagram Protocol</strong> (<strong class="bold">UDP</strong>) and <strong class="bold">Transmission Control Protocol</strong> (<strong class="bold">TCP</strong>) to <a id="_idIndexMarker174"/>transfer data from the sender to the receiver. These protocols can be<a id="_idIndexMarker175"/> used for low latency applications, but many do not have advanced support for adaptive streaming technology and<a id="_idIndexMarker176"/> suffer from limited<a id="_idIndexMarker177"/> scalability. Two <a id="_idIndexMarker178"/>examples of these protocols are <strong class="bold">Real-Time Streaming Protocol</strong> (<strong class="bold">RTSP</strong>) and <strong class="bold">Real-Time Messaging Protocol</strong> (<strong class="bold">RTMP</strong>), which we <a id="_idIndexMarker179"/>will discuss next.</p>
<h4>RTSP</h4>
<p>RTSP is an <a id="_idIndexMarker180"/>application layer protocol that was used for the low latency<a id="_idIndexMarker181"/> streaming of videos. It also has playback capabilities to allow playing and pausing the video content and can handle multiple data streams. This, however, is no longer popular today and has been replaced by other more modern protocols, which we will see in later sections. RTSP was replaced by modern protocols such as HLS and DASH because a lot of receivers did not support RTSP; it was incompatible with HTTP and lost popularity with the advent of web-based streaming applications.</p>
<h4>Flash and RTMP</h4>
<p>Flash-based applications were very popular once upon a time. They use RTMP and work well for low latency <a id="_idIndexMarker182"/>streaming use cases. However, Flash<a id="_idIndexMarker183"/> as a technology has declined a lot in popularity for many <a id="_idIndexMarker184"/>reasons, mostly security-related. Web browsers as well as CDNs have removed support<a id="_idIndexMarker185"/> for RTMP because it did not scale too well as demand grew. RTMP is a streaming protocol that accomplishes low latencies in streaming but, as mentioned before, is being replaced by other technologies now.</p>
<h3>HTTP-based protocols</h3>
<p>HTTP-based <a id="_idIndexMarker186"/>protocols typically <a id="_idIndexMarker187"/>break down the continuous stream of audio-video data into small segments of 2 to 10 seconds in length. These segments are then transported through a CDN or a web service. These are the preferred protocols for low latency live streaming applications since they are still acceptably low latency but also feature-rich and scale better. These protocols, however, do have a disadvantage that we have mentioned before: the latency incurred depends on the length of the segments. The minimum latency is at least the length of the segment because the receiver needs to receive at least one full segment before it can play it. In some cases, the latency can be in the order of multiples of segment length depending on the video player devices’ implementation. For example, iOS buffers at least three to five segments before playing the first segment to ensure smooth rendering.</p>
<p>Some examples<a id="_idIndexMarker188"/> of HTTP-based protocols are as follows:</p>
<ul>
<li><strong class="bold">HTTP Live </strong><strong class="bold">Streaming</strong> (<strong class="bold">HLS</strong>)</li>
<li><strong class="bold">HTTP Dynamic </strong><strong class="bold">Streaming</strong> (<strong class="bold">HDS</strong>)</li>
<li><strong class="bold">Microsoft Smooth </strong><strong class="bold">Streaming</strong> (<strong class="bold">MSS</strong>)</li>
<li><strong class="bold">Dynamic Adaptive Streaming over </strong><strong class="bold">HTTP</strong> (<strong class="bold">DASH</strong>)</li>
<li><strong class="bold">Common Media Application </strong><strong class="bold">Format</strong> (<strong class="bold">CMAF</strong>)</li>
<li><strong class="bold">High-Efficiency Stream </strong><strong class="bold">Protocol</strong> (<strong class="bold">HESP</strong>)</li>
</ul>
<p>We will discuss some of these protocols in this section to understand how they work and how they achieve low latency performance in real-time video streaming applications. Overall, these protocols are designed to scale to millions of simultaneous receivers and support adaptive streaming and playback. HTTP-based streaming protocols use <a id="_idIndexMarker189"/>communication over standard HTTP protocol and require a server for distribution. In <a id="_idIndexMarker190"/>contrast to that, <strong class="bold">Web Real-Time Communication</strong> (<strong class="bold">WebRTC</strong>), which we will explore <a id="_idIndexMarker191"/>later, is a <strong class="bold">Peer-to-Peer</strong> (<strong class="bold">P2P</strong>) protocol that can technically establish direct communication between two machines and skip the need for an intermediate machine or server.</p>
<h4>HLS</h4>
<p>HLS is<a id="_idIndexMarker192"/> used both for real-time and on-demand<a id="_idIndexMarker193"/> audio-video content delivery and can scale tremendously well. HLS is generally converted from RTMP by the video delivery platform. Using both RTMP and HLS is the best way to achieve low latency and stream to all devices. There is a variant of <strong class="bold">Low Latency HLS</strong> (<strong class="bold">LL-HLS</strong>) that<a id="_idIndexMarker194"/> can get latencies down to under 2 seconds, but it is still experimental. LL-HLS enables low latency audio-video real-time streaming by exploiting the ability to stream and render partial segments instead of requiring a full segment. The success of HLS and LL-HLS as the most widely used ABR streaming protocols comes from scalability to many users and compatibility with most kinds of devices, browsers, and players.</p>
<h4>CMAF</h4>
<p>CMAF is <a id="_idIndexMarker195"/>relatively new; strictly speaking, it is not really a new <a id="_idIndexMarker196"/>format but instead packages and delivers various forms of protocols for video streaming. It works with HTTP-based protocols such as HLS and DASH to encode, package, and decode video segments. This typically helps businesses by reducing storage costs and audio-video streaming latencies.</p>
<h4>DASH</h4>
<p>DASH was created from the work of<a id="_idIndexMarker197"/> the <strong class="bold">Moving Picture Experts Group</strong> (<strong class="bold">MPEG</strong>) and is an alternative to the HLS protocol we discussed<a id="_idIndexMarker198"/> before. It<a id="_idIndexMarker199"/> is quite similar to HLS because it prepares different quality levels of the audio-video content and divides them into small segments to enable ABR streaming. Under the hood, DASH still relies on CMAF and, to be specific, one of the features it relies on is <strong class="bold">chunked encoding</strong>, which<a id="_idIndexMarker200"/> facilitates breaking a segment into even smaller subsegments of a few milliseconds. The other feature it relies on is <strong class="bold">chunked transfer encoding</strong>, which <a id="_idIndexMarker201"/>takes these subsegments sent to the distribution layer and distributes them in real time.</p>
<h4>HESP</h4>
<p>HESP is <a id="_idIndexMarker202"/>another ABR <a id="_idIndexMarker203"/>HTTP-based streaming protocol. This protocol has the ambitious goals of ultra-low latencies, increasing scalability, supporting currently popular CDNs, reducing bandwidth requirements, and reducing times to switch between streams (i.e., the latency to start a new audio video stream). Since it is extremely low latency (&lt;500 milliseconds) it is a competitor to the WebRTC protocol, but HESP can be expensive since it is not an open source protocol.</p>
<p>Fundamentally, the major difference in HESP relative to other protocols is that HESP relies on two streams rather than one. One of the streams (which only contains keyframes or snapshot frames) is known as <a id="_idIndexMarker204"/>the <strong class="bold">initialization stream</strong>. The other stream contains data that applies incremental changes to the frames in the initialization stream, and this stream is known as the <strong class="bold">continuation stream</strong>. So, while<a id="_idIndexMarker205"/> the keyframes from the initialization stream contain snapshot data and require higher bandwidth, they support the ability to quickly seek various locations in the video during playback. But the continuation stream is lower-bandwidth since it only contains changes and can be used to quickly play back once the receiver video player synchronizes with the initialization stream.</p>
<p>While, on <a id="_idIndexMarker206"/>paper, HESP might <a id="_idIndexMarker207"/>sound perfect, it has a few drawbacks such as higher costs for encoding and storing two streams instead of one, the need to encode and distribute two streams instead of one, and the need to update the players on the receivers’ platforms to decode and render the two streams.</p>
<h3>WebRTC</h3>
<p>WebRTC is <a id="_idIndexMarker208"/>regarded as the new standard in the<a id="_idIndexMarker209"/> real-time video streaming industry and allows subsecond latencies so can be played back on most platforms and almost every browser (such as Safari, Chrome, Opera, Firefox, etc.). It is a P2P protocol (i.e., it creates a direct communication channel between devices or streaming applications). A big advantage of WebRTC is that it does not need additional plugins to support audio-video streaming and playback. It also supports ABR and adaptive video quality changes for bi-directional and real-time audio-video streaming. Even though WebRTC uses a P2P protocol and so can establish a direct connection for conferencing, the performance is still dependent on the hardware and network quality because that is still a consideration for all protocols, regardless of whether they're P2P or not.</p>
<p>WebRTC does have some challenges, such as needing its own multimedia server infrastructure, the need to encrypt data that is exchanged, security protocols to handle the gaps in UDP, trying to scale worldwide cost-effectively, and the engineering complexity that comes with dealing with the several protocols that WebRTC is a combination of.</p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Exploring solutions and platforms for low latency streaming</h2>
<p>In this section, we will <a id="_idIndexMarker210"/>explore some of the most popular solutions and commercially available platforms for low latency video streaming. These platforms build on all the technologies we discussed in the previous section to solve a lot of the business problems associated with high latencies in real-time audio-video streaming applications. Note that a lot of these platforms support and use multiple underlying streaming protocols, but we will mention the ones that are primarily used for these platforms.</p>
<h3>Twitch</h3>
<p>Twitch is a very<a id="_idIndexMarker211"/> popular online <a id="_idIndexMarker212"/>platform, mostly used by video gamers who want to live-stream their gameplay in real time as well as interact with their target audience via chats, comments, donations, and so on. It goes without saying, but this requires low latency streaming as well as the ability to scale to a large community, which Twitch provides. Twitch uses RTMP for its broadcasting needs.</p>
<h3>Zoom</h3>
<p>Zoom is one of the <a id="_idIndexMarker213"/>real-time<a id="_idIndexMarker214"/> video conferencing platforms that exploded in popularity during the COVID pandemic and working-from-home era. Zoom provides real-time low latency audio and video conferencing with little delays and supports many simultaneous users. It also provides features such as screen sharing and group chats while on video conferencing. Zoom primarily uses the WebRTC streaming protocol technology.</p>
<h3>Dacast</h3>
<p>Dacast is a <a id="_idIndexMarker215"/>platform for<a id="_idIndexMarker216"/> broadcasting events, and even though it is not as low latency as some other real-time streaming applications, it still has acceptable performance when it comes to broadcasting purposes. It is affordable and works well but does not allow for a lot of interactive workflows. Dacast uses the RTMP streaming protocol as well.</p>
<h3>Ant Media Server</h3>
<p>Ant Media Server uses <a id="_idIndexMarker217"/>WebRTC technology to provide an extremely low latency video streaming platform and is intended to be used at an enterprise level <a id="_idIndexMarker218"/>on-premises or on the cloud. It is also used for live video monitoring and surveillance-based applications that require real-time video streaming at their core.</p>
<h3>Vimeo</h3>
<p>Vimeo is <a id="_idIndexMarker219"/>another <a id="_idIndexMarker220"/>very popular video streaming platform that, while not the fastest in the business, is still used quite extensively. It is mostly used to house real-time live event broadcasts and on-demand video distribution applications. Vimeo uses RTMP streaming by default but also supports others, including HLS.</p>
<h3>Wowza</h3>
<p>Wowza has<a id="_idIndexMarker221"/> been<a id="_idIndexMarker222"/> around for a long time in the field of online real-time video streaming and is quite reliable and widely used. It is used by many large corporations such as Sony, Vimeo, and Facebook and focuses on providing video streaming services at a commercial and enterprise level on a very large scale. Wowza is another platform that uses RTMP streaming protocol technology.</p>
<h3>Evercast</h3>
<p>Evercast is an <a id="_idIndexMarker223"/>ultra-low <a id="_idIndexMarker224"/>latency streaming platform that has found a lot of uses for collaborative content creation and editing applications, as well as live streaming applications. Since it can support ultra-low latency performance, multiple collaborators are able to stream their workspaces and create an environment of real-time and collaborative editing. The demand for such use cases has exploded in recent years due to the COVID pandemic, remote work and collaboration, and online collaboration education systems. Evercast primarily uses WebRTC on its streaming servers.</p>
<h3>CacheFly</h3>
<p>CacheFly is <a id="_idIndexMarker225"/>another<a id="_idIndexMarker226"/> platform that provides live video streaming for live event broadcast purposes. It provides an acceptably low latency of single digit seconds and scales very well for real-time audio-video broadcasting applications. CacheFly uses a custom Websocket-based end-to-end streaming solution.</p>
<h3>Vonage Video API</h3>
<p>Vonage Video<a id="_idIndexMarker227"/> API (previously known <a id="_idIndexMarker228"/>as <strong class="bold">TokBox</strong>) is another platform that provides live video streaming capabilities and targets large corporations to support <a id="_idIndexMarker229"/>enterprise-level applications. It supports data encryption, which is what makes it a preferred choice for enterprises, corporations, and healthcare companies looking for audio-video conferencing, meetings, and training online. Vonage uses RTMP as well as HLS as its broadcasting technologies.</p>
<h3>Open Broadcast Software (OBS)</h3>
<p>OBS is another low <a id="_idIndexMarker230"/>latency video<a id="_idIndexMarker231"/> streaming platform that is also open source, which makes it popular in a lot of circles where enterprise-level solutions might be a deterrent. Many live streamers who stream several types of content use OBS, and even some platforms such as Facebook Live and Twitch use some parts of OBS. OBS supports <a id="_idIndexMarker232"/>multiple protocols such as RTMP and <strong class="bold">Secure Reliable </strong><strong class="bold">Transport</strong> (<strong class="bold">SRT</strong>).</p>
<p>Here, we conclude our discussion of low latency considerations for live video streaming applications. Next, we will transition into video gaming applications, which share some common traits when compared to live video streaming applications, especially when it comes to online video games.</p>
<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Understanding what low latency constraints matter in gaming applications</h1>
<p>Video gaming has <a id="_idIndexMarker233"/>evolved greatly since it was first born in the 1960s and, these days, video games are not about playing alone or even playing along with or against the person physically next to you. These days, gaming involves many players all over the globe, and even the quality and complexity of these games have increased tremendously. It is no surprise that ultra-low latency and high scalability are non-negotiable requirements when it comes to modern gaming applications. With new technologies such as AR and VR, this only further increases the need for ultra-low latency performance. Additionally, with the advent of mobile gaming combined with online gaming, complex gaming applications have been ported to smartphones and need ultra-low latency content delivery systems, multiplayer systems, and super-fast processing speeds.</p>
<p>In the previous section, we discussed low latency real-time video streaming applications in detail, including streaming applications that are interactive. In this section, we will look at low latency considerations, high-latency impact, and techniques to facilitate low latency performance in video gaming applications. Since a lot of modern video games are either online or in the cloud, or have a strong online presence due to multiplayer features, a lot of what we learned in the previous section is still important here. Streaming and rendering video games in real time, preventing lag, and responding to player interactions <a id="_idIndexMarker234"/>quickly and efficiently are necessities when it comes to gaming applications. Additionally, there are some extra concepts, considerations, and techniques to maximize low latency gaming performance.</p>
<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>Concepts in low latency gaming applications</h2>
<p>Before we understand the<a id="_idIndexMarker235"/> impact of high latency in gaming applications and how to improve the latencies in these applications, we will define and explain a few concepts related to gaming applications and their performance. When it comes to low latency gaming applications, the most important concepts <a id="_idIndexMarker236"/>are the <strong class="bold">refresh rate</strong>, <strong class="bold">response time</strong>, and <strong class="bold">input lag</strong>. The <a id="_idIndexMarker237"/>main goal of these applications is to minimize the delay<a id="_idIndexMarker238"/> between the player and the character on screen that the player controls. Really, what this means is that any user input reflects the impact on the screen right away, and any changes to the character due to the gameplay environment are rendered on the screen right away. The optimal user experience is achieved when the gameplay feels very smooth, and the player feels like they really are inside the game world being rendered on screen. Now, let us jump into a discussion of the important concepts with regard to low latency gaming applications.</p>
<h3>Ping</h3>
<p>In computer science and <a id="_idIndexMarker239"/>online video gaming applications, <strong class="bold">ping</strong> is the <a id="_idIndexMarker240"/>latency from when data is sent from the user’s computer to a server (or possibly a different player’s computer) until when the data is received back at the original user’s computer. Typically, the magnitude of the ping latency depends on the application; for low latency electronic trading, this will be in hundreds of microseconds, and for gaming applications, it is usually tens to hundreds of milliseconds. Ping latency basically measures how fast a server and client communicate with each other in the absence of any processing delays at the server or client machines.</p>
<p>The closer to real-time the requirements for the gaming application are, the lower the ping times need to be. This is typically required for games such as <strong class="bold">first-person shooters</strong> (<strong class="bold">FPS</strong>) and<a id="_idIndexMarker241"/> sports and racing games, while other games such as <strong class="bold">massive multiplayer online</strong> (<strong class="bold">MMO</strong>) games<a id="_idIndexMarker242"/> and some <strong class="bold">real-time strategy</strong> (<strong class="bold">RTS</strong>) games<a id="_idIndexMarker243"/> can tolerate higher ping latencies. It is common for the game interface itself to have a ping functionality or display ping statistics in real time. In general, 50 to 100 milliseconds is an<a id="_idIndexMarker244"/> acceptable ping time, above 100 milliseconds can cause noticeable<a id="_idIndexMarker245"/> delays during gameplay, and any higher than that degrades the player’s experience too much to be viable. Typically, less than 25 milliseconds is the ideal ping latency for good responsiveness, crisp rendering of visuals, and no gameplay lags.</p>
<h3>Frames per second (FPS)</h3>
<p><strong class="bold">FPS</strong> (not to be confused with <em class="italic">first-person shooters</em>) is another important concept when it comes to <a id="_idIndexMarker246"/>online gaming applications. FPS measures how many frames or images can be rendered each second by the<a id="_idIndexMarker247"/> graphics card. FPS can also be measured for the monitor hardware itself instead of the graphics card (i.e., how many frames can be displayed or updated by the monitor hardware itself). Higher FPS typically leads to the smoother rendering of game worlds and the user experience feels more responsive to inputs and gameplay events. Lower FPS leads to the gameplay and rendering feeling like it is rigid, stuttering, and flickering, and overall, just leads to significantly reduced enjoyment and adoption.</p>
<p>For a game to be functional or even playable, 30 FPS is the bare minimum necessity, and this can support console games and some PC games. As long as the FPS stays above 20 FPS, these games can continue to be playable without any noticeable lag and degradation. For most games, 60 FPS or more is the ideal performance range that is easily supported by most graphics cards, PCs, monitors, and TVs. Beyond 60 FPS, the next milestone is 120 FPS, which is only needed and available for high-end gaming hardware connected to monitors that support at least 144-Hz refresh rates. Beyond this, 240 FPS is the maximum frame rate achievable and needs to be paired with 240-Hz refresh rate monitors. This <a id="_idIndexMarker248"/>high-end configuration is typically only needed for the biggest gaming enthusiasts out there.</p>
<h3>Refresh rate</h3>
<p><strong class="bold">The refresh rate</strong> is <a id="_idIndexMarker249"/>a concept that is very closely related to FPS, and even though technically they are slightly different, they do impact each<a id="_idIndexMarker250"/> other. The refresh rate also measures how quickly the screen refreshes and impacts the maximum possible FPS that hardware can support. Like FPS, the higher the refresh rate, the smoother the rendering transition when animating motion during gameplay on the screen. The maximum refresh rate controls the maximum FPS achievable because even though the graphics card can render faster than a monitor screen can refresh, the bottleneck then becomes the monitor screen refresh rate. When there are cases where the FPS exceeds the refresh rate, one of the display artifacts that we encounter is <a id="_idIndexMarker251"/>called <strong class="bold">screen tearing</strong>. Screen tearing is when the graphics card (GPU) is not synchronized with the monitor, so there are cases where the monitor paints an incomplete frame on top of the current frame, resulting in horizontal or vertical splits where the partial and full frames overlap on the screen. This does not completely break down the gameplay but, at the<a id="_idIndexMarker252"/> very least, can be distracting if it happens rarely, <a id="_idIndexMarker253"/>all the way to completely ruining the visual quality of the gameplay if quite <a id="_idIndexMarker254"/>frequent. There <a id="_idIndexMarker255"/>are various techniques to deal with screen tearing, which<a id="_idIndexMarker256"/> we will look at<a id="_idIndexMarker257"/> shortly, such as <strong class="bold">vertical synchronization</strong> (<strong class="bold">V-Sync</strong>), <strong class="bold">adaptive sync</strong>, <strong class="bold">FreeSync</strong>, <strong class="bold">Fast Sync</strong>, <strong class="bold">G-Sync</strong>, and <strong class="bold">variable refresh </strong><strong class="bold">rate</strong> (<strong class="bold">VRR</strong>).</p>
<h3>Input lag</h3>
<p><strong class="bold">Input lag</strong> measures<a id="_idIndexMarker258"/> the latency between when a user generates an input (such as a keystroke, mouse movement, or a mouse click) to when the response to that input is<a id="_idIndexMarker259"/> rendered on the screen. This is basically the responsiveness of the hardware and the game to user inputs and interactions. Obviously, for all games, this is a non-zero value and is the sum of the hardware itself (the controller, mouse, keyboard, internet connection, processor, display monitor, etc.) or the game software itself (processing input, updating the game and character states, dispatching the graphics updates, rendering them via the graphics card, and refreshing the monitor). When there is high input lag, the game feels unresponsive and lagging, which can affect the player’s performance during multiplayer or online gameplay and even ruin the game completely for the user.</p>
<h3>Response time</h3>
<p><strong class="bold">Response time</strong> is <a id="_idIndexMarker260"/>often mistaken for input lag, but <a id="_idIndexMarker261"/>they are different terms. Response time refers to the pixel response time, which basically means the time it takes for pixels to change colors. While input lag impacts the gameplay’s responsiveness, response time impacts the blurriness of rendered animations on the screen. Intuitively, if the pixel response time is high, the pixels take longer to change colors when rendering motion or animation on screen, thus causing blurriness. Lower pixel response times (1 millisecond or lower) lead to crisp and sharp image and animation quality, even for games that have fast camera movements. Good examples of such games would be first-person shooters and racing games. In cases where the response time is high, we encounter an artifact known<a id="_idIndexMarker262"/> as <strong class="bold">ghosting</strong>, which refers to trails and artifacts slowly fading off the screen when there is motion. Usually, ghosting and high pixel response times are not a problem, and modern hardware can easily provide response times of less than 5 milliseconds and render sharp animations.</p>
<h3>Network bandwidth</h3>
<p>Network bandwidth <a id="_idIndexMarker263"/>affects online gaming applications in the same way as it would affect <a id="_idIndexMarker264"/>real-time video streaming applications. Bandwidth measures how many megabits per second can be uploaded to or downloaded from the gaming application server. Bandwidth is also affected by packet losses, which we will look at next, and varies depending on the location of the players and the gaming server they connect to. <strong class="bold">Contention</strong> is<a id="_idIndexMarker265"/> another term to think about when it comes to network bandwidth. Contention is a measure of how many simultaneous users are trying to access the same server or shared resource and whether that causes the server to be overloaded or not.</p>
<h3>Network packet loss and jitter</h3>
<p>Network packet <a id="_idIndexMarker266"/>loss is<a id="_idIndexMarker267"/> an inescapable fact of transmitting packets over the network. Network packet loss reduces the effective bandwidth and causes retransmission and recovery protocols, which introduce additional delays. Some packet losses are tolerable, but when the network packet losses are very high, they can degrade the user experience of online gaming applications and can even bring it to a grinding halt. <strong class="bold">Jitter</strong> is like packet losses except, in this case, packets arrive<a id="_idIndexMarker268"/> out of order. This <a id="_idIndexMarker269"/>introduces additional delays as the game software is on the user’s end because the receiver must save the out-of-order packets and wait for packets that have not arrived yet and then process the packets in order.</p>
<h3>Networking protocols</h3>
<p>When it comes to<a id="_idIndexMarker270"/> networking protocols, there are<a id="_idIndexMarker271"/> broadly two protocols to transfer data across the internet: <strong class="bold">TCP</strong> and <strong class="bold">UDP</strong>. TCP provides a reliable transport protocol by tracking packets <a id="_idIndexMarker272"/>successfully delivered to the receiver and having <a id="_idIndexMarker273"/>mechanisms to retransmit lost packets. The advantage here is obvious where applications cannot operate with packet and information losses. The disadvantage here is that these additional mechanisms to detect and handle packet drops cause additional latencies (additional milliseconds) and use the available bandwidth less effectively. Examples of applications that must rely on TCP are online shopping and online banking, where it is critical to make sure the data is delivered correctly, even if it is delivered late. UDP instead focuses on making sure that the data is delivered as quickly as possible and with greater bandwidth effectiveness. However, it does so at the cost of not guaranteeing delivery or even guaranteeing in-order delivery of packets since it does not have mechanisms to retransmit dropped packets. UDP works well for applications that can tolerate some packet losses without completely breaking down and some dropped information is preferred over delayed information. Some examples of such applications are real-time video streaming and some components of online gaming applications. For instance, some video components or rendering components in online video games can be transported over UDP, but some components such as user input and game and player state updates need to be sent over TCP.</p>
<div><div><img alt="Figure 2.3 – Components in an end-to-end video gaming system" src="img/Figure_2.3_B19434.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Components in an end-to-end video gaming system</p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Improving gaming application performance</h2>
<p>In the previous <a id="_idIndexMarker274"/>section, we discussed some concepts that apply to low latency gaming applications and their impact on the application and the user experience. In this section, we will explore additional details about the sources of high latency in gaming applications and discuss the steps we can take to improve gaming application latencies and performance and thus improve the user experience.</p>
<h3>Approaching gaming application optimization from the developer’s perspective</h3>
<p>First, we look at<a id="_idIndexMarker275"/> the approach and techniques that game developers use to optimize gaming application performance. Let us quickly describe a few optimization techniques employed by developers – some that apply to all applications and some that only apply to gaming applications.</p>
<h4>Managing memory, optimizing cache access, and optimizing the hot path</h4>
<p>Gaming <a id="_idIndexMarker276"/>applications, like other low latency applications, must use the available resources efficiently and maximize runtime performance. This includes managing the memory correctly to avoid memory leaks and pre-allocating and pre-initializing as many things as possible. Avoiding mechanisms such as garbage collection and dynamic memory allocations and deallocation on the critical path are also important to meet a certain runtime performance expectation. This is especially relevant to gaming applications because there are many objects in video games, especially ones that create and deal with large worlds.</p>
<p>Another important aspect of most low latency applications is using the data and instruction cache as efficiently as possible. Gaming applications are no different, especially given the large amount of data that they must deal with.</p>
<p>A lot of applications, including<a id="_idIndexMarker277"/> gaming applications, spend a lot of their time in a critical loop. For gaming applications, this can be a loop that checks for inputs, updates the game state, character states, and so on based on the physics engine ,and renders on screen, as well as <a id="_idIndexMarker278"/>generates audio output. Gaming developers typically spend a lot of time focusing on the operations performed in this critical path, like what we would do with any low latency application running in a tight loop.</p>
<h4>Frustum culling</h4>
<p>In <a id="_idIndexMarker279"/>computer graphics, the term view <strong class="bold">frustum</strong> refers to the part of the game world<a id="_idIndexMarker280"/> that is<a id="_idIndexMarker281"/> currently visible on the screen. <strong class="bold">Frustum culling</strong> is a term that refers to the technique of determining which objects are visible on the screen and only rendering those objects on the screen. Another way to think about this is that most game engines minimize the amount of processing power being directed toward off-screen objects. This is typically achieved by separating the display or rendering functionality of an object from its data and logic management, such as the location, state, next move, and so on. Eliminating the overhead of rendering objects not currently on the screen reduces the processing cost to a fraction of what it would be. Another way to introduce this separation would be to have an update method that is used when the object is on screen and another update method to be used when the object is off-screen.</p>
<h4>Caching calculations and using mathematical approximations</h4>
<p>This<a id="_idIndexMarker282"/> is an easy-to-understand optimization technique that applies to applications that need to perform a lot of expensive mathematical computations. Gaming<a id="_idIndexMarker283"/> applications especially are heavy on mathematical computations in their physics engines, especially for 3D games with large worlds and lots of objects in the world. Optimization techniques such as caching values instead of recomputing them each time, using lookup tables to trade memory usage for CPU usage to lookup values, and using mathematical approximations instead of extremely accurate but expensive expressions are used in such cases. These optimization techniques have been used for a very long time in the world of video games because, for a long time, hardware resources were extremely limited, and developing such systems needed to rely on these techniques.</p>
<p>The raycasting engine<a id="_idIndexMarker284"/> from <strong class="bold">id Software</strong> (which pioneered and powered games such as Wolfenstein, Doom, Quake, etc.) is an impressive masterpiece of low latency<a id="_idIndexMarker285"/> software development from back in the old days. Another example would be cases where we have side scrollers or top-down shooters where there are a lot of enemies on screen<a id="_idIndexMarker286"/> but a lot of them have similar movement patterns and can be reused instead of being recalculated.</p>
<h4>Prioritizing critical tasks and leveraging CPU idle time</h4>
<p>A <a id="_idIndexMarker287"/>game engine that deals with many objects in a huge game world typically has many objects that update frequently. Instead of updating every object on every frame update, a game engine needs to prioritize tasks that need to be performed in <a id="_idIndexMarker288"/>the critical section (for example, objects whose visual properties have changed since the last frame). A simple implementation would be to have a member method for each object that the game engine can use to check whether it has changed since the last frame and prioritize the updates for those objects. For instance, some game components such as the scenery (stationary environment objects, weather, lighting, etc.) and <strong class="bold">heads-up display</strong> (<strong class="bold">HUD</strong>) do not change very frequently and typically have extremely <a id="_idIndexMarker289"/>limited animation sequences. The tasks related to updating these components are slightly lower-priority than some other game components.</p>
<p>Classifying tasks into high-priority and low-priority tasks also means that the game engine has the option of guaranteeing a good gameplay experience by making sure high-priority tasks are performed in all hardware and game settings. If the game engine detects a lot of CPU idle time, it can add additional low-priority features (such as particle engines, lighting, shading, atmospheric effects, etc.).</p>
<h4>Ordering draw calls depending on layer, depth, and texture</h4>
<p>A <a id="_idIndexMarker290"/>game engine needs to determine which rendering or draw calls to issue to the graphics card. To optimize performance, the goal here is to not only minimize the number of draw calls issued but also to order and group these draw calls to perform them optimally. When rendering objects to the screen, we must think about the following layers or considerations:</p>
<ul>
<li><strong class="bold">The fullscreen layer</strong>: This comprises the HUD, the game layer, the translucent effects layer, and so on</li>
<li><strong class="bold">The viewport layer</strong>: These exist if there are mirrors, portals, split screens, and so on</li>
<li><strong class="bold">Depth considerations</strong>: We need to draw objects in back-to-front order or farthest-to-closest order</li>
<li><strong class="bold">Texturing considerations</strong>: These comprise textures, shading, lighting, and so on</li>
</ul>
<p>There are various decisions to be made about the ordering of these different layers and components and in which order the draw calls are sent to the graphics card. An example would be in cases where translucent objects might be ordered back-to-front (i.e., sorted by depth first and texture second). For opaque objects, it might sort by texture first and eliminate draw calls for objects that are behind opaque objects.</p>
<h3>Approaching gaming application optimization from the gamer’s perspective</h3>
<p>For <a id="_idIndexMarker291"/>gaming applications, a lot of the performance depends on the end user’s hardware, OS, and game settings. This section describes a few things that end users can do to maximize gaming performance under different settings and different resource availability.</p>
<h4>Upgrading hardware</h4>
<p>The <a id="_idIndexMarker292"/>first obvious method to improve gaming application performance is to improve the hardware the end user’s game runs on. Some important candidates would be the gaming monitor, mouse, keyboard, and controllers used. Gaming monitors with higher refresh rates (such as 360-Hz monitors that support 1920 x 1080p (pixels) resolution and 240-Hz monitors that support 2560 x 1440p resolution) can provide high-quality rendering and fluid animations and enhance gameplay. We can also use a mouse with an extremely high polling rate, which allows for clicks and movements to be registered faster than before and reduce latency and lag. Similarly, for keyboards, gaming keyboards have much higher polling rates and can improve response times, especially for games where there are a lot of constant keystrokes, which can often be the case for RTS games. Another important point to mention here would be that using official and reputable controllers for specific consoles and platforms usually results in the best performance.</p>
<h4>Gaming monitor refresh rates</h4>
<p>We <a id="_idIndexMarker293"/>have discussed this aspect a few times before but with the rise of very high-quality images and animations, the quality and capacity of gaming monitors themselves have become quite important. Here, the key is to have a high refresh rate monitor that also has a low pixel response time so that animations can be rendered and updated quickly as well as smoothly. The configuration must also avoid the screen-tearing, ghosting, and blurring artifacts that we discussed before.</p>
<h4>Upgrading your graphics card</h4>
<p>Upgrading the<a id="_idIndexMarker294"/> graphics card is another option that can result in significant improvements by increasing the frame rate and thus improving the gaming performance. NVIDIA found that upgrading the graphics card and the GPU drivers can help improve gaming performance by more than 20% in some cases. NVIDIA GeForce, ATI Radeon, Intel HD graphics, and so on are popular vendors that provide updated and optimized drivers that can be used to boost your gaming performance depending on which graphics cards are installed on the user’s platform.</p>
<h4>Overclocking the graphics card</h4>
<p>Another <a id="_idIndexMarker295"/>possible area of improvement instead of, or in addition to, upgrading the GPU is attempting to overclock the GPU with the aim of adding FPS. <strong class="bold">Overclocking</strong> the <a id="_idIndexMarker296"/>GPU works by increasing the frequency of the GPU and ultimately increasing the FPS output of the GPU. The one drawback of overclocking the GPU is increased internal temperatures, which can lead to overheating in extreme cases. So, when overclocking, you should monitor the increase in temperatures, increase the overclock levels gradually, monitor along the way, and make sure that the PC, laptop, or console has sufficient cooling in place. GPU overclocking can yield a performance boost of around 10%.</p>
<h4>Upgrading your RAM</h4>
<p>This is <a id="_idIndexMarker297"/>another obvious general-purpose improvement technique that applies to low latency gaming applications as well. Adding additional RAM to the PC, smartphone, tablet, or console gives the game applications and graphics rendering tasks to perform their best. Thankfully, RAM costs have dropped tremendously over the last decade, so this is an easy way to boost the performance of gaming applications and is highly recommended.</p>
<h4>Tweaking the hardware, OS, and game settings</h4>
<p>In the <a id="_idIndexMarker298"/>previous subsection, we discussed some options for upgrading the<a id="_idIndexMarker299"/> hardware resources that lead to improved gaming applications <a id="_idIndexMarker300"/>performance. In this subsection, we will discuss settings that can be optimized for the hardware, the platform, the OS, and the game settings themselves to further push gaming application performance.</p>
<h4>Enabling game mode</h4>
<p>Game<a id="_idIndexMarker301"/> mode is a setting available for displays such as high-end TVs and similar high-end display monitors. Enabling game mode disables extra functionality in the display, which improves image and animation quality but comes at the price of higher latency. Enabling game mode will cause a slight deterioration in image quality but can help improve the low latency gaming application end user experience by reducing rendering latencies. An example of game mode is the Windows game mode on Windows 10, which optimizes gaming performance when enabled.</p>
<h4>Using high-performance mode</h4>
<p>The <a id="_idIndexMarker302"/>high-performance mode we discuss here refers to power settings. Different power settings try to optimize between battery usage and performance; high-performance mode drains battery power faster and possibly raises the internal temperature more than low-performance mode but it boosts the performance of the applications running.</p>
<h4>Delaying automatic updates</h4>
<p>Automatic<a id="_idIndexMarker303"/> updates are a feature most notably available in Windows, which downloads and installs security fixes automatically. While this is typically not a major problem, if a particularly large automatic update download and install starts while we are in the middle of an online gaming session, then it can affect the gaming performance and experience. Automatic updates can spike processor usage and bandwidth consumption if this coincides with a gaming session utilizing high processor usage and bandwidth, and can degrade the gaming performance. So, turning off or delaying automatic Windows updates is usually a good idea when running latency-sensitive and resource-intensive gaming applications.</p>
<h4>Turning off background services</h4>
<p>This is <a id="_idIndexMarker304"/>another option similar to delaying automatic updates we just discussed. Here, we find and turn off apps and services that might be running in the background but are not necessarily needed for a low latency gaming session to function properly. In fact, turning these off prevents these applications from consuming hardware resources unexpectedly and non-deterministically during gaming sessions. This maximizes the low latency gaming application performance by making the maximum amount of resources available to that application.</p>
<h4>Meeting or exceeding refresh rates</h4>
<p>We have<a id="_idIndexMarker305"/> discussed the concept of screen tearing before, so we at least need a system where the FPS meets or exceeds the refresh rate to prevent it. Technologies<a id="_idIndexMarker306"/> such as FreeSync and G-Sync facilitate smooth rendering without any screen tearing while still providing low latency performance. When frame rates exceed refresh rates, the latency continues to remain low, but if frame rates start exceeding refresh rates by a large magnitude, screen tearing can show up again. This can be addressed by using V-Sync technology or limiting FPS intentionally. FreeSync and G-Sync need hardware support, so you need a compatible GPU to use these technologies. But the upside with FreeSync and G-Sync is that you can completely disable V-Sync, which introduces latencies, and instead have a low latency and tear-free rendering experience as long as you have the hardware support for it.</p>
<h4>Disabling triple buffering and V-Sync and running exclusively in full-screen mode</h4>
<p>We have<a id="_idIndexMarker307"/> explained before that V-Sync can introduce additional latency due to the need to synchronize the frames rendered by the GPU with the display device. <strong class="bold">Triple buffering</strong> is just<a id="_idIndexMarker308"/> another form of V-Sync and has the same goal of reducing screen tearing. Triple buffering especially comes into play when running a game in windowed mode, in which the game runs inside a window instead of full screen. The key takeaway here is that to disable V-Sync and triple buffering to improve latency and performance, we have to run exclusively in full-screen mode.</p>
<h4>Optimizing game settings for low latency and high frame rate</h4>
<p>Modern <a id="_idIndexMarker309"/>games come with a plethora of options and settings designed to maximize performance (sometimes at the cost of rendering quality), and the end user can optimize these parameters for their target hardware, platform, network resources, and performance requirements. Reducing settings such <a id="_idIndexMarker310"/>as <strong class="bold">anti-aliasing</strong> would be an example, and reducing resolution is another option. Finally, adjusting settings related to the viewing distance, texture rendering, shadows, and lighting can also maximize performance at the cost of lower rendering quality. Anti-aliasing seeks to render smooth edges instead of jagged edges when we try to render high-resolution images in low-resolution environments, so turning it down deteriorates image smoothness but accelerates low latency performance. Advanced rendering effects such as fire, water, motion blur, and lens flares can also be turned down if additional performance is required.</p>
<h3>Further optimizing your hardware</h3>
<p>In the last two subsections, we discussed options for optimizing low latency gaming applications by upgrading hardware resources and tweaking the hardware, OS, and game settings. In this final subsection, we will discuss how to squeeze the performance even further and what our options are to further optimize online low latency gaming application performance.</p>
<h4>Installing DirectX 12 Ultimate</h4>
<p>DirectX<a id="_idIndexMarker311"/> is a Windows graphics and gaming API developed by Microsoft. Upgrading DirectX to the latest version means the gaming platform gets access to the latest fixes and improvements and<a id="_idIndexMarker312"/> better performance. At this time, DirectX 12 Ultimate is the latest version, with DirectX 13 expected to be released at the end of 2022 or early 2023.</p>
<h4>Defragmenting and optimizing disks</h4>
<p>Defragmentation<a id="_idIndexMarker313"/> of disks occurs as files are created and deleted on the hard disk and the free and used disk space blocks are spread out or fragmented causing lower driver performance. <strong class="bold">Hard Disk Drives</strong> (<strong class="bold">HDD</strong>) and <strong class="bold">Solid State Drives</strong> (<strong class="bold">SSD</strong>) are <a id="_idIndexMarker314"/>typically the two commonly used storage options<a id="_idIndexMarker315"/> for most gaming platforms. SSDs are significantly faster than HDDs and do not typically suffer from a lot of fragmentation-related issues but can still become suboptimal over time. Windows, for instance, has a defragmentation and optimization application to optimize the performance of the drives, which can improve gaming application performance as well.</p>
<h4>Ensuring the laptop cools optimally</h4>
<p>When under heavy loads<a id="_idIndexMarker316"/> due to high processor, network, memory, and disk usage, the internal temperatures of the laptop or PC are raised. Other than being dangerous, it also forces the laptop to try and cool itself down by limiting resource consumption and thus, ultimately, performance. We mention laptops specifically to explain this issue because PCs typically have better airflow and cooling abilities than laptops. Ensuring that laptops cool effectively by clearing the vents and fans, removing dirt and dust, placing them on a hard, smooth, and flat surface, using an external power supply to not drain the battery, and possibly even using additional cooling stands can boost gaming performance on laptops.</p>
<h4>Using NVIDIA Reflex low latency technology</h4>
<p><strong class="bold">NVIDIA Reflex</strong> low latency technology<a id="_idIndexMarker317"/> seeks to minimize the input lag measured from the moment the user clicks the mouse or hits a key on the keyboard or controller to the time at which the impact of that action is rendered on screen. We have already discussed the sources of latency here, and NVIDIA breaks this down into nine chunks from the input device to the processors and the display. The NVIDIA Reflex software speeds up this critical path performance by improving communication paths between CPUs and GPUs, optimizing frame delivery and rendering by skipping unnecessary tasks and pauses, and accelerating the GPU rendering time. NVIDIA also provides an NVIDIA Reflex Latency Analyzer to measure the speed-up achieved by using these low latency enhancements.</p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Discussing the design of IoT and retail analytics systems</h1>
<p>In the previous chapter, we discussed <strong class="bold">IoT</strong> and <a id="_idIndexMarker318"/>retail analytics and many of the different use cases that they create. Our focus in this section will be to have a brief discussion about the technologies being used to achieve low latency performance for these applications and use cases. Note that IoT is a technology space that is still actively growing and evolving, so there are going to be a lot of breakthroughs and advancements in the coming years. Let us quickly recap some important use cases of IoT and retail data analytics. A lot of these new applications and future possibilities are<a id="_idIndexMarker319"/> fueled by the research and advancements in 5G wireless technology, <strong class="bold">edge computing</strong>, and <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>). We will<a id="_idIndexMarker320"/> look at those aspects in the next section, along with other technologies that facilitate applications using low latency IoT and retail data analytics.</p>
<p>A lot of applications fall under the remote inspection/analysis category, where drones can replace humans when it comes to being the first line of defense in fields such as remote technicians, monitoring infrastructure such as bridges, tunnels, railways, highways, and waterways, and even things such as transformers, utility wires, gas pipelines, and electricity and telephone lines. Incorporating AI into such applications enhances the complexity of the data analysis possible and thus creates new opportunities and use cases. Incorporating AR technology also increases the possibilities. Modern automobiles collect large amounts of data and, with the possibility of having autonomous driving vehicles at some point, the use cases for IoT only expand further. Automation in agriculture, shipping and logistics, supply chain management, inventory, and warehouse management, and managing fleets of vehicles creates numerous additional use cases for IoT technology and analyzes data generated from and collected by these devices.</p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>Ensuring low latency in IoT devices</h2>
<p>In this section, we will look at some <a id="_idIndexMarker321"/>considerations to facilitate low latency performance in IoT applications and retail analytics. Note that a lot of the considerations we discussed for real-time video streaming and online video gaming use cases apply here as well, such as hardware resources, encoding and decoding data streams, content delivery mechanisms, and hardware and system-level optimizations. We will not repeat those techniques here in the interest of brevity, but we will present additional low latency considerations that apply specifically to IoT and retail data analysis.</p>
<h3>P2P connectivity</h3>
<p>P2P connectivity for IoT<a id="_idIndexMarker322"/> devices establishes direct connectivity between different IoT devices or between an IoT device and the end user application. The user’s input from their device is directly sent to the IoT device that it is meant for without any third-party service or server in between to minimize latencies. Similarly, data from the IoT devices is streamed back to the other devices directly from the device. The P2P approach is an alternative to connecting to the IoT device through a cloud, which has extra latencies due to additional server databases, cloud worker instances, and<a id="_idIndexMarker323"/> so on. P2P is also referred to as a decentralized <strong class="bold">Application Enablement Platform</strong> (<strong class="bold">AEP</strong>) for IoT, which is an alternative to the cloud-based AEPs.</p>
<h3>Using fifth-generation wireless (5G)</h3>
<p>5G wireless <a id="_idIndexMarker324"/>technology delivers higher bandwidth, ultra-low latency, reliability, and scalability. Not only do the end users benefit from 5G but it also helps each step of the IoT devices and applications that require low latency and real-time data streaming and processing. 5G’s low latency facilitates faster and more reliable inventory tracking, transportation services and monitoring, real-time visibility into distribution logistics, and more. The 5G network was designed keeping all the different IoT use cases in mind, so it is an excellent fit for all kinds of IoT applications and more.</p>
<h3>Understanding edge computing</h3>
<p><strong class="bold">Edge computing</strong> is a distributed <a id="_idIndexMarker325"/>processing technology<a id="_idIndexMarker326"/> where the key point is to bring the processing application and the data storage components as close to the source of data as possible, which, in this case, is the IoT devices that capture the data. Edge computing breaks the older paradigm where the data is recorded by remote devices and then transferred to a central storage and processing location and then results are transported back to the devices and client applications. This exciting new technology is revolutionizing how massive amounts of data generated by a lot of IoT devices are transported, stored, and processed. The main goals for edge computing are to reduce bandwidth costs to transfer massive amounts of data across wide distances and to support ultra-low latency to facilitate real-time applications that need to process massive amounts of data as quickly and efficiently as possible. Additionally, it also reduces costs for businesses because they do not necessarily need a centralized and cloud-based storage and processing solution. This point is especially important when it comes to IoT applications because the sheer scale of how many devices generate data means the bandwidth consumption will increase exponentially.</p>
<p>Understanding <a id="_idIndexMarker327"/>all the details of the physical architecture of an edge computing system is difficult and outside the scope of this book. However, at a very high level, the client devices and IoT devices connect to edge modules that are available nearby. Typically, there are many gateways and servers that are deployed by service providers or enterprises looking to build their own edge network to support such edge computing operations. The devices that can use these edge modules range from IoT sensors, laptops and computers, smartphones and tablets, cameras, microphones, and anything else you can imagine.</p>
<h4>Understanding the relationship between 5G and edge computing</h4>
<p>We mentioned before that<a id="_idIndexMarker328"/> 5G was designed and developed keeping IoT and edge computing in mind. So IoT, 5G, and edge computing are all related to each other and work with each other to maximize the use cases and performance of these IoT applications. Theoretically, edge computing could be deployed to non-5G networks, but obviously, 5G is the preferred network. However, the reverse is not true; to leverage 5G’s true power, you need an edge computing infrastructure to really maximize the use of everything 5G offers. This is intuitive because, without an edge computing infrastructure, the data from the devices must travel long distances to get processed, and then results must travel a long distance to reach the end user’s applications or other devices. In those cases, even if you have a 5G network, the latency due to the data’s travel distance far outweighs the latency improvements gained by using 5G. So, edge computing is necessary when it comes to IoT applications and applications that need to analyze retail data in real time.</p>
<h4>Understanding the relationship between edge computing and AI</h4>
<p>Data analytics <a id="_idIndexMarker329"/>techniques, machine learning, and AI have revolutionized how the retail and non-retail data collected from IoT devices is analyzed to derive meaningful insights. NVIDIA is a pioneer when it comes to developing new hardware solutions to push not only edge computing but also AI processing to the maximum. Jetson AGX Orin is a particularly good example of how NVIDIA packages AI and robotics functionality together into a single product.</p>
<p>We will not go into too many details about the Jetson AGX Orin since that is neither the focus nor within the scope of this book. The Jetson AGX Orin has a few qualities that make it excellent for AI, robotics, and autonomous vehicles – it is compact, enormously powerful, and energy-efficient. The power and energy efficiency allows it to be used for AI applications and <a id="_idIndexMarker330"/>enable edge computing. This latest model in particular lets developers combine AI, robotics, <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), computer<a id="_idIndexMarker331"/> vision, and so on into a compact package, making it excellent for robotics. This device also has multiple I/O connectors and is compatible with many different sensors (MIPI, USB, cameras, etc.). There are also additional hardware expansion slots to support storage, wireless, and so on. This powerful GPU-powered device makes it perfect for deep learning (in addition to classic machine learning) and computer vision applications such as robotics.</p>
<h4>Buying and deploying edge computing systems</h4>
<p>When it <a id="_idIndexMarker332"/>comes to purchasing and setting up an edge computing infrastructure, businesses generally go down one of two routes: customize the components and build and manage the infrastructure in-house or use a vendor that provides and manages edge services for the enterprise.</p>
<p>Building and managing the edge<a id="_idIndexMarker333"/> computing infrastructure in-house needs expertise from the IT, network, and business departments. Then, they can select the edge devices from hardware vendors (such as IBM, Dell, etc.) and architect and manage the 5G network infrastructure for the specific use case. This option only makes sense for a large enterprise that sees value in customizing its edge computing infrastructure for a specific use case. When it comes to the option of having a third-party vendor facilitate and manage the edge computing infrastructure, the vendor sets up the hardware, software, and networking architecture for a fee. This leaves the management of a complicated system such as edge computing infrastructure to firms such as GE and Siemens with expertise in this field and allows the client business to focus on building on top of this infrastructure.</p>
<h3>Leveraging proximity</h3>
<p>We have<a id="_idIndexMarker334"/> implicitly discussed this point in the previous sections, but now we will explicitly discuss it here. A key requirement of IoT applications is achieving ultra-low latency performance, and the key to achieving that is leveraging proximity between the different devices and applications involved in the IoT use case. It is no surprise that edge computing is the key to leveraging proximity for IoT applications to minimize latencies from capturing data to processing it and sharing results with other devices or client applications. As we have seen before, the biggest bottleneck with a non-edge computing infrastructure is the distance of data centers and processing resources from the source of the data and the destination of the results. This gets worse with additional distributed data centers spread out miles away from each other, and ultimately leads to critically high latencies and lags. Clearly, placing edge computing resources closer to the data sources is the key to driving IoT adoption, IoT use cases, and scaling IoT businesses to a huge number of devices and users.</p>
<h3>Reducing cloud costs</h3>
<p>This is another<a id="_idIndexMarker335"/> point we have discussed before, but we will have a formal conversation about it in this section. There are billions of IoT devices out there and they generate a continuous stream of data. Any effective IoT-driven business will need to scale extremely well to large increases in the number of devices and clients involved, which leads to exponential increases in the amount of data recorded and data processed by edge computing, and the results being transferred to other devices and clients. Data-heavy infrastructures that rely on centralized cloud infrastructure cannot support IoT applications in a cost-effective manner and the data and cloud infrastructure itself becomes a significant fraction of an enterprise’s expenses. The clear solution here is to find a low-cost edge solution (third-party or in-house) and use it to facilitate the IoT data capture, storage, and processing needs. This removes the costs associated with transferring data in and out of cloud solutions and can improve edge computing reliability and cut costs significantly.</p>
<p>We will conclude our discussion of low latency IoT applications by summarizing the current and future state of IoT applications in the following figure:</p>
<div><div><img alt="Figure 2.4 – Current and future states of IoT applications" src="img/Figure_2.4_B19434.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Current and future states of IoT applications</p>
<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>Exploring low latency electronic trading</h1>
<p>The final example of low latency<a id="_idIndexMarker336"/> applications is the applications used in low latency electronic trading and ultra-low latency electronic trading, also known as HFT. We will build a full end-to-end low latency electronic trading system from scratch in C++ in the rest of this book. So, in this section, we will briefly discuss the important considerations for electronic trading applications to achieve low latency performance and then build out the low-level details in the remaining chapters. <em class="italic">Developing High-Frequency Trading Systems</em> by Sebastian Donadio, Sourav Ghosh, and Romain Rossier would be an excellent book for understanding low latency electronic trading systems in greater detail for interested readers. Our focus in this book will be to design and build each component from scratch in C++ to learn about low latency application development, but that book can be used as a good reference for the additional theory behind the HFT business.</p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Understanding the need for low latency in modern electronic trading</h2>
<p>With the <a id="_idIndexMarker337"/>modernization of electronic trading and the rise of<a id="_idIndexMarker338"/> HFT, low latencies are more important than ever before for these applications. In many cases, achieving lower latency leads to a direct increase in trading revenue. In some cases, there is a constant race to try and reduce latencies more and more to maintain a competitive edge in the markets. And in extreme cases, if a participant falls behind in the arms race to the lowest possible laten<a id="_idTextAnchor051"/>cies, they may go out of business.</p>
<p>Trading opportunities in modern electronic markets are extremely short-lived, so only the market participants that can process market data and find such an opportunity and quickly send orders in reaction to this can be profitable. Failure to react quickly enough means you get a smaller piece of the opportunity, and it is common for only the fastest participant to get all the profit, and for all other slower participants to get nothing. Another nuance here is that if a participant is not quick enough to react to a market event, they can also be caught on the wrong side of the trade and lose money to people who were able to react to the event quickly enough. In such a case, trading profits are not just lower, but trading revenue can be negative (i.e., losses). To understand this better, let us present an example of something we will build in this book: market-making and liquidity-taking algorithms.</p>
<p>Without going into too many details, a market-making algorithm has orders in the market that other participants can trade against when needed. A market-making algorithm thus needs to constantly re-evaluate its active orders and change the prices and quantities <a id="_idIndexMarker339"/>for them depending on the market conditions. A liquidity-taking algorithm, however, does not always have active orders in the market. This algorithm instead waits for an opportunity to present itself and then trades against a market-making algorithm’s active order in the book. A simple view of the HFT <a id="_idIndexMarker340"/>market would be a constant battle between market-making and liquidity-taking algorithms because they naturally take opposite sides.</p>
<p>In this setup, a market-making algorithm loses money when it is slow at modifying its active orders in the market. For instance, say depending on market conditions, it is quite clear that the market prices are going to go up in the short term; a market-making algorithm will try to move or cancel its sell orders if they are at risk of being executed since it no longer wants to sell at those prices. A liquidity-taking algorithm, at the same time, will try to see whether it can send a buy order to trade against a market maker’s sell order at that price. In this race, if the market-making algorithm is slower than the liquidity-taking algorithms, it will not be able to modify or cancel its sell order. If the liquidity-taking algorithm is slow, it will not be able to execute against the orders it wanted to either because a different (and faster) algorithm was able to execute before it or because the market maker was able to move out of the way. This example should make it clear to you that latency directly affects trading revenue in electronic trading.</p>
<p>For HFT, trading applications on the client’s side can receive and process market data, analyze the information, look for opportunities, and send an order out to the exchange, all within sub-10-microsecond latency, and using <strong class="bold">Field-Programmable Gate Arrays</strong> (<strong class="bold">FPGAs</strong>), can reduce <a id="_idIndexMarker341"/>that to sub-1-microsecond latency. FPGAs are special hardware chips that are re-programmable and can be used to build extremely specialized and low latency functionality directly onto the chip itself. Understanding the details and developing and using FPGAs is an advanced topic beyond this book’s scope.</p>
<p>While we <a id="_idIndexMarker342"/>have referred to trading performance and<a id="_idIndexMarker343"/> revenue in the previous example, low latencies are also important in other aspects of electronic trading businesses that might not be immediately obvious. Obviously, trading revenues and performance are still the primary focus for trading applications; another important requirement for long-term business continuity is real-time risk management. Since each electronic market has many trading instruments and each of those continuously changes prices throughout the day, there is a tremendous amount of data that the risk management system needs to keep up with, across all the exchanges and all the products available throughout the day.</p>
<p>Additionally, since a firm employs HFT strategies across all these products and exchanges, the firm’s position on each of these products changes rapidly all day long. A real-time risk management system needs to evaluate the firm’s constantly evolving exposure across all these products against market prices to track profits and losses and risk throughout the day. The risk evaluation metrics and systems can themselves be quite complicated; for instance, in options trading, it is common to run Monte Carlo simulations to try and find worst-case risk evaluations in real time or very close to real time. Some risk management systems are also in charge of shutting down automated trading strategies if they exceed any of their risk limits. These risk systems are often added to multiple <a id="_idIndexMarker344"/>components – a central risk system, the<a id="_idIndexMarker345"/> order gateways, and the trading strategies themselves – but we will understand these details later in this book.</p>
<h2 id="_idParaDest-52"><a id="_idTextAnchor052"/>Achieving the lowest latencies in electronic trading</h2>
<p>In this section, we will <a id="_idIndexMarker346"/>briefly discuss some of the higher-level ideas and concepts when it comes to implementing low latency electronic trading systems. We will, of course, revisit these with examples in greater detail as we work on building our electronic trading ecosystem in the coming chapters.</p>
<h3>Optimizing trading server hardware</h3>
<p>Getting<a id="_idIndexMarker347"/> powerful trading servers to support the low latency trading operations is a first step. Typically, the processing power of these servers depends on the architecture of the trading system processes, such as how many processes we expect to run, how many network res<a id="_idTextAnchor053"/>ources we expect to consume, and how much memory we expect these applications to consume. Typically, low latency trading applications have high CPU usage, low kernel usage (system calls), low memory consumption, and relatively high network resource usage during busy trading periods. CPU registers, cache architecture, and capacity matter as well, and typically, we try to get larger sizes, if possible, but these can be quite expensive. Advanced considerations such as <strong class="bold">Non-Uniform Memory Access</strong> (<strong class="bold">NUMA</strong>), processor<a id="_idIndexMarker348"/> instruction sets, instruction pipelines and instruction parallelism, cache hierarchy architecture details, hyperthreading, and overclocked CPUs are often considered, but those are extremely advanced optimization techniques and outside the scope of this book.</p>
<h3>Network Interface Cards, switches, and kernel bypass</h3>
<p>Trading <a id="_idIndexMarker349"/>servers that need to support ultra-low latency trading applications (especially ones that must read massive amounts of market data, update packets from the network, and process them) need <a id="_idIndexMarker350"/>specialized <strong class="bold">Network Interface Cards</strong> (<strong class="bold">NICs</strong>) and switches. The NICs preferred for such applications need to have very low latency performance, low jitter, and large buffer capacities to handle market data bursts without dropping packets. Also, optimal NICs for modern electronic trading applications support an especially low-latency path that avoids system calls <a id="_idIndexMarker351"/>and buffer copies, known<a id="_idIndexMarker352"/> as <strong class="bold">kernel bypass</strong>. One <a id="_idIndexMarker353"/>example is <strong class="bold">Solarflare</strong>, which<a id="_idIndexMarker354"/> provides <strong class="bold">OpenOnload</strong> and APIs such as <strong class="bold">ef_vi</strong> and <strong class="bold">TCPDirect, which</strong> <a id="_idIndexMarker355"/>bypass the kernel when <a id="_idIndexMarker356"/>using their NICs; <strong class="bold">Exablaze</strong> is <a id="_idIndexMarker357"/>another example of a specialized NIC that <a id="_idIndexMarker358"/>supports kernel bypass. Network switches show up in various places in the network topology, which support interconnectivity between trading servers and trading servers that are located far away from each other and between trading servers and electronic exchange servers. For network switches, one of the important considerations is the size of the buffer that the switch can support to buffer packets that need to be forwarded. Another important requirement is the latency between a switch receiving a packet and forwarding it to the correct interface known <a id="_idIndexMarker359"/>as <strong class="bold">switching latency</strong>. Switching latencies are generally very low, in the order of tens of hundreds of nanoseconds, but this applies to all inbound or outbound traffic going through the switch, so needs to be consistently low to not have a negative impact on trading performance.</p>
<h3>Understanding multithreading, locks, context switches, and CPU scheduling</h3>
<p>We have discussed the closely related but technically different concepts of bandwidth and low latency in the previous chapter. It is sometimes incorrectly assumed that having an architecture with a larger number of threads is always lower-latency, but this is not always true. Multithreading<a id="_idIndexMarker360"/> adds value in certain areas of low <a id="_idTextAnchor054"/>latency electronic trading systems, and we will make use of it in the system we build in this book. But the point here is that we need to be<a id="_idIndexMarker361"/> careful when using additional threads in HFT systems because, while adding threads generally boosts throughput for applications that need it, it can sometimes end up increasing latencies in applications as well. As we increase the number of threads, we must think about concurrency and thread safety, and if we need to use locks for synchronization and concurrency between threads, that adds additional latencies and context switches. Context switches <a id="_idIndexMarker362"/>are not free because the scheduler and OS must save the state of the thread or process being switched out and load the state of the thread or process that will be run next. Many lock implementations are built on top of kernel system calls, which are more expensive than user space routines, thus increasing the latencies in a heavily multithreaded application even further. For optimal performance, we try to get the<a id="_idIndexMarker363"/> CPU scheduler to do little to no work (i.e., processes and threads that are scheduled to run are never context switched out and keep running in user space). Additionally, it is quite common to pin specific threads and processes to specific CPU cores, which eliminates context switching and the OS needing to find free cores to schedule tasks, and additionally, improves memory access efficiency.</p>
<h3>Dynamically allocating memory and managing memory</h3>
<p>Dynamic<a id="_idIndexMarker364"/> memory allocation is a request for memory blocks of arbitrary sizes made at runtime. At a very high level, dynamic memory allocations and deallocations are handled by the OS by looking through a list of free memory blocks and trying to allocate a contiguous block as large as the program requested. Dynamic memory deallocations are handled by appending the freed blocks to the list of free blocks managed by the OS. Searching through this list can incur higher and higher latencies as the program runs through the day and memory gets increasingly fragmented. Additionally, if dynamic memory<a id="_idIndexMarker365"/> allocations and deallocations are on the same critical path, then they incur an additional overhead every single time. This is one of the major reasons we discussed before that led us to choose C++ as our preferred language for building low latency and resource-constrained applications. We will explore the performance impact of dynamic memory allocation and techniques to avoid it during later chapters in this book as we build our own trading system.</p>
<h3>Static versus dynamic linking and compile time versus runtime</h3>
<p><strong class="bold">Linking</strong> is the <a id="_idIndexMarker366"/>compilation or translation step in the process of converting high-level programming language source code into machine code for the target architecture. Linking ties together pieces of code that might be in different libraries – these can be libraries internal to the code base or external standalone libraries. During the<a id="_idIndexMarker367"/> linking step, we have two choices: <strong class="bold">static linking</strong> or <strong class="bold">dynamic linking</strong>.</p>
<p>Dynamic linking<a id="_idIndexMarker368"/> is when the linker does not incorporate the code from libraries into the final binary at linking time. Instead, when the main application requires code from the shared libraries for the first time, then the resolution is performed at runtime. Obviously, there is a particularly large extra cost incurred at runtime the first time the shared library code is called. The bigger downside is that since the compiler and linker do not incorporate the code at compilation and linking time, they are unable to perform possible optimizations, resulting in an application that can be inefficient overall.</p>
<p>Static linking is <a id="_idIndexMarker369"/>when the linker arranges the application code and the code for the library dependencies into a single binary executable file. The upside here is that the libraries are already linked at compile time so there is no need for the OS to find and resolve the dependencies at runtime startup by loading the dependent libraries before the application starts executing. The even bigger upside is that this creates an opportunity for the program to be super-optimized at compile and linking time to yield lower latencies at runtime. The downside to static linking over dynamic linking is that the application binary is much larger and each application binary that relies on the same set of external libraries has a copy of all the external library code compiled and linked into the binary. It is common for ultra-low latency electronic trading systems to link all dependent libraries statically to minimize runtime performance latencies.</p>
<p>We have discussed compile time versus runtime processing in the previous chapter, and that approach tries to move the maximum amount of processing to the compilation step instead of at runtime. This increases compile times, but the runtime performance latencies are much lower because a lot of the work is already done at compile time. We will look at this aspect in detail specifically for C++ in the next few chapters and throughout the course of this book as we build our electronic trading system in C++.</p>
<h1 id="_idParaDest-53"><a id="_idTextAnchor055"/>Summary</h1>
<p>In this chapter, we looked at different low latency applications in different business areas. The goals were to understand how low latency applications impact businesses in different areas and the similarities that some of these applications share, such as the hardware requirements and optimization, software design, performance optimization, and different revolutionary technologies being used to achieve these performance requirements.</p>
<p>The first applications we looked at in detail were real-time, low latency, online video streaming applications. We discussed different concepts and investigated where high latencies come from, and how that affects performance and businesses. Finally, we discussed different technologies and solutions, and platforms that facilitate low latency video streaming applications to be a success.</p>
<p>The next applications we looked at had a lot of overlap with video streaming applications – offline and online video gaming applications. We introduced some additional concepts and considerations that apply to offline and online gaming applications and explained their impact on the user experience and thus, ultimately, on business performance. We discussed a myriad of things to consider when trying to maximize the performance of these applications, ranging from a lot of factors that apply to live video streaming applications to additional hardware and software considerations for gaming applications.</p>
<p>We then briefly discussed the requirement of low latency performance when it comes to IoT devices and retail data collection and analysis applications. This is a relatively new and fast-improving technology and is likely to continue growing aggressively over the next decade. Lots of research and advancements are being made for IoT devices and we find new business ideas and use cases as we make progress here. We discussed how 5G wireless and edge computing technologies are breaking the old paradigm of central data storage and processing and why that is critical for IoT devices and applications.</p>
<p>The last applications we also discussed briefly in this chapter were low latency electronic trading and HFT applications. We kept the discussion short and focused on the higher-level ideas when it comes to maximizing the performance of low latency and ultra-low latency electronic trading applications. We did so because we will build a full end-to-end C++ low latency electronic trading ecosystem from scratch in the remaining chapters of this book. When we do that, we will discuss, understand, and implement all the different low latency C++ concepts and ideas with examples and performance data, so there is a lot more to come on this application.</p>
<p>We will move on from this discussion of different low latency applications to a more in-depth discussion of the C++ programming language. We will discuss the correct approach to using C++ for low latency performance, the different modern C++ features, and how to unleash the power of modern C++ compiler optimizations.</p>
</div>
</body></html>