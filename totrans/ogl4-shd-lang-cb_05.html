<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Using Textures</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li><span>Applying a 2D texture</span></li>
<li><span>Applying multiple textures</span></li>
<li><span>Using alpha maps to discard pixels</span></li>
<li><span>Using normal maps</span></li>
<li><span>Parallax mapping</span></li>
<li>Steep parallax mapping with self shadowing</li>
<li><span>Simulating reflection with cube maps</span></li>
<li><span>Simulating refraction with cube maps</span></li>
<li><span>Applying a projected texture</span></li>
<li><span>Rendering to a texture</span></li>
<li><span>Using sampler objects</span></li>
<li>Diffuse image-based lighting</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p class="mce-root">Textures are an important and fundamental aspect of real-time rendering in general, and OpenGL in particular. The use of textures within a shader opens up a huge range of possibilities. Beyond just using textures as sources of color information, they can be used for things like depth information, shading parameters, displacement maps, normal vectors, and other vertex data. The list is virtually endless. Textures are among the most widely used tools for advanced effects in OpenGL programs, and that isn't likely to change any time soon.</p>
<div class="packt_infobox"><span>In OpenGL 4, we now have the ability to read and write to memory via buffer textures, shader storage buffer objects, and image textures (image load/store). This further muddies the waters of what exactly defines a texture. In general, we might just think of it as a buffer of data that may or may not contain an image.</span></div>
<p class="mce-root">OpenGL 4.2 introduced <strong>immutable storage textures</strong>. Despite what the term may imply, immutable storage textures are not textures that can't change. Instead, the term <em>immutable</em> refers to the fact that, once the texture is allocated, the <em>storage</em> cannot be changed. That is, the size, format, and number of layers are fixed, but the texture content itself can be modified. The word immutable refers to the allocation of the memory, not the content of the memory. Immutable storage textures are preferable in the vast majority of cases because of the fact that many runtime (draw-time) consistency checks can be avoided, and you include a certain degree of "type safety" since we can't accidentally change the allocation of a texture. Throughout this book, we'll use immutable storage textures exclusively.</p>
<div class="packt_infobox"><span>Immutable storage textures are allocated using the </span><kbd>glTexStorage*</kbd> <span>functions. If you're experienced with textures, you might be accustomed to using </span><kbd>glTexImage*</kbd> <span>functions, which are still supported but create mutable storage textures.</span></div>
<p class="mce-root">In this chapter, we'll look at some basic and advanced texturing techniques. We'll start with the basics by just applying color textures and move on to using textures as normal maps and environment maps. With environment maps, we can simulate things such as reflection and refraction. We'll see an example of projecting a texture onto objects in a scene similar to the way that a slide projector projects an image. Finally, we'll wrap up with an example of rendering directly to a texture using <strong>framebuffer objects</strong> (<strong>FBOs</strong>) and then applying that texture to an object.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying a 2D texture</h1>
                </header>
            
            <article>
                
<p class="mce-root">In GLSL, applying a texture to a surface involves accessing texture memory to retrieve a color associated with a texture coordinate, and then applying that color to the output fragment. The application of the color to the output fragment could involve mixing the color with the color produced by a shading model, simply applying the color directly, using the color in the reflection model, or some other mixing process. In GLSL, textures are accessed via <strong>sampler</strong> variables. A sampler variable is a <em>handle</em> to a texture unit. It is typically declared as a uniform variable within the shader and initialized within the main OpenGL application to point to the appropriate texture unit. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">In this recipe, we'll look at a simple example involving the application of a 2D texture to a surface, as shown in the following image. We'll use the texture color as the diffuse (and ambient) reflectivity term in the Blinn-Phong reflection model. The following image shows the results of a brick texture applied to a cube. The texture is shown on the right and the rendered result is on the left:</p>
<div class="packt_figure">
<div class="CDPAlignCenter CDPAlign"><img src="assets/761984c0-af73-483a-b181-ca6d611f8e98.png" style="width:30.83em;height:15.42em;"/></div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">Set up your OpenGL application to provide the vertex position in attribute location 0, the vertex normal in attribute location 1, and the texture coordinate in attribute location 2. The parameters for the Blinn-Phong reflection model are declared again as uniform variables within the shader, and must be initialized from the OpenGL program. Make the handle to the shader available in a variable named <kbd>programHandle</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">To render a simple shape with a 2D texture, use the following steps: </p>
<ol>
<li>We'll define a simple (static) function for loading and initializing textures: </li>
</ol>
<pre style="padding-left: 60px">GLuint Texture::loadTexture( const std::string &amp; fName ) {<br/>  int width, height;<br/>  unsigned char * data = Texture::loadPixels(fName, width, height);<br/>  GLuint tex = 0;<br/>  if( data != nullptr ) {<br/>    glGenTextures(1, &amp;tex);<br/>    glBindTexture(GL_TEXTURE_2D, tex);<br/>    glTexStorage2D(GL_TEXTURE_2D, 1, GL_RGBA8, width, height);<br/>    glTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, <br/>              width, height, GL_RGBA, GL_UNSIGNED_BYTE, data);<br/><br/>    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, <br/>    GL_LINEAR);<br/>    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, <br/>    GL_NEAREST);<br/><br/>    Texture::deletePixels(data);<br/>  }<br/>  return tex;<br/>}</pre>
<ol start="2">
<li>In the initialization of the OpenGL application, use the following code to load the texture, bind it to texture unit <kbd>0</kbd>, and set the uniform variable <kbd>Tex1</kbd> to that texture unit:</li>
</ol>
<pre style="padding-left: 60px">GLuint tid = Texture::loadTexture("brick1.png");<br/><br/>glActiveTexture(GL_TEXTURE0); <br/>glBindTexture(GL_TEXTURE_2D, tid);
 
// Set the Tex1 sampler uniform to refer to texture unit 0 
int loc = glGetUniformLocation(programHandle, "Tex1"); 
glUniform1i(loc, 0); </pre>
<ol start="3">
<li>The vertex shader passes the texture coordinate to the fragment shader:</li>
</ol>
<pre style="padding-left: 60px">layout (location = 0) in vec3 VertexPosition; 
layout (location = 1) in vec3 VertexNormal; 
layout (location = 2) in vec2 VertexTexCoord; 
 
out vec3 Position; 
out vec3 Normal; 
out vec2 TexCoord; 
 
// Other uniforms...<br/> 
void main() 
{ 
  TexCoord = VertexTexCoord; 
  // Assign other output variables here... 
}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>The fragment shader looks up the texture value and applies it to the diffuse reflectivity in the Blinn-Phong model:</li>
</ol>
<pre style="padding-left: 60px">in vec3 Position; 
in vec3 Normal; 
in vec2 TexCoord; 
 <br/>// The texture sampler object
uniform sampler2D Tex1; 
 
// Light/material uniforms...<br/> 
void blinnPhong( vec3 pos, vec3 n ) { <br/>  vec3 texColor = texture(Tex1, TexCoord).rgb;<br/>  vec3 ambient = Light.La * texColor;<br/>  // ...<br/>  vec3 diffuse = texColor * sDotN;<br/>  // Compute spec...
  return ambient + Light.L * (diffuse + spec);   
} 
void main() { 
  FragColor = vec4( blinnPhong(Position, normalize(Normal) ), 1 ); 
} </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The first code segment defines a simple function that loads the texture from a file, copies the texture data to OpenGL memory, and sets up the <kbd>mag</kbd> and <kbd>min</kbd> filters. It returns the texture ID. The first step, loading the texture image file, is accomplished by calling another method (<kbd>Texture::loadPixels</kbd>), which uses an image loader that is provided along with the example code. The loader comes from a header file <kbd>stb_image.h</kbd>, available on GitHub (<a href="https://github.com/nothings/stb">https://github.com/nothings/stb</a>). It reads the image and stores the pixel data into an array of unsigned bytes in RGBA order. The width and height of the image are returned via the last two parameters. We keep a pointer to the image data, simply named <kbd>data</kbd>.</p>
<p>The next two lines involve creating a new texture object by calling <kbd>glGenTextures</kbd>. The handle for the new texture object is stored in the <kbd>tex</kbd> variable.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To load and configure the texture object, we do the following.</p>
<ol>
<li><span>W</span><span>e call</span> <kbd>glBindTexture</kbd> <span>to bind the new texture object to the</span> <kbd>GL_TEXTURE_2D</kbd> <span>target.</span></li>
<li><span>Once the texture is bound to that target, we allocate immutable storage for the texture with</span> <kbd>glTexStorage2D</kbd><span>.</span></li>
<li><span>After that, we copy the data for that texture into the texture object using</span> <kbd>glTexSubImage2D</kbd><span>. The last argument to this function is a pointer to the raw data for the image.</span></li>
<li><span>The next steps involve setting the magnification and minification filters for the texture object using</span> <kbd>glTexParameteri</kbd><span>.  For this example, we'll use</span> <kbd>GL_LINEAR</kbd> <span>for the former and</span> <kbd>GL_NEAREST</kbd> <span>for the latter.</span></li>
</ol>
<div class="packt_infobox"><span>The texture filter setting determines whether any interpolation will be done prior to returning the color from the texture. This setting can have a strong effect on the quality of the results. In this example, </span><kbd>GL_LINEAR</kbd><span> indicates that it will return a weighted average of the four texels that are nearest to the texture coordinates. For details on the other filtering options, see the OpenGL documentation for </span><kbd>glTexParameteri</kbd><span>: </span><a href="http://www.opengl.org/wiki/GLAPI/glTexParameter"><span class="URLPACKT">http://www.opengl.org/wiki/GLAPI/glTexParameter</span></a><span>.</span></div>
<p class="mce-root">Next, we delete the texture data pointed to by <kbd>data</kbd>. There's no need to hang on to this because it was copied into texture memory via <kbd>glTexSubImage2D</kbd>. To do so, we call the <kbd>Texture::deletePixels</kbd> function.  (Internally, that calls the function provided by the <kbd>stb_image</kbd> library <kbd>stbi_image_free</kbd>.) Then, we return the ID of the new texture object.</p>
<p class="mce-root"><span>In the next code segment, we call our <kbd>Texture::loadTexture</kbd> function to load the texture, then we bind the texture to texture unit <kbd>0</kbd>. To do so, first we call </span><kbd>glActiveTexture</kbd><span> to set the current active texture unit to </span><kbd>GL_TEXTURE0</kbd><span> (the first texture unit, also called a <strong>texture </strong></span><strong>channel</strong><span>). The subsequent texture state calls will be effective on texture unit zero. Then, we bind the new texture to that unit using <kbd>glBindTexture</kbd>. </span>Finally, we set the uniform variable <kbd>Tex1</kbd> in the GLSL program to zero. This is our sampler variable. Note that it is declared within the fragment shader with type <kbd>sampler2D</kbd>. Setting its value to zero indicates to the OpenGL system that the variable should refer to texture unit zero (the same one selected previously with <kbd>glActiveTexture</kbd>).</p>
<p class="mce-root">The vertex shader is very similar to the one used in the previous examples except for the addition of the texture coordinate input variable <kbd>VertexTexCoord</kbd>, which is bound to attribute location <kbd>2</kbd>. Its value is simply passed along to the fragment shader by assigning it to the shader output variable <kbd>TexCoord</kbd>.</p>
<p class="mce-root">The fragment shader is also very similar to those used in the recipes of previous chapters.  The primary changes are the <kbd>Tex1</kbd> uniform variable and the <kbd>blinnPhong</kbd> function.  <kbd>Tex1</kbd> is a <kbd>sampler2D</kbd> variable that was assigned by the OpenGL program to refer to texture unit zero. In the <kbd>blinnPhong</kbd> function, we use that variable along with the texture coordinate (<kbd>TexCoord</kbd>) to access the texture. We do so by calling the built-in function <kbd>texture</kbd>. This is a general purpose function, which is used to access a variety of different textures. The first parameter is a sampler variable indicating which texture unit is to be accessed, and the second parameter is the texture coordinate used to access the texture. The return value is a <kbd>vec4</kbd> containing the color obtained by the texture access. We select only the first three components (<kbd>.rgb</kbd>) and store them in <kbd>texColor</kbd>. Then, we use <kbd>texColor</kbd> as the ambient and diffuse reflectivity terms in the Blinn-Phong model.</p>
<div class="packt_tip">When using a texture for both ambient and diffuse reflectivity, it is important to set the ambient light intensity to a small value, in order to avoid <em>wash-out</em>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">There are several choices that could be made when deciding how to combine the texture color with other colors associated with the fragment. In this example, we used the texture color as the ambient and diffuse reflectivity, but one could have chosen to use the texture color directly or to mix it with the reflection model in some way. There are endless options—the choice is up to you!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Specifying the sampler binding within GLSL</h1>
                </header>
            
            <article>
                
<p class="mce-root">As of OpenGL 4.2, we now have the ability to specify the default value of the sampler's binding (the value of the sampler uniform) within GLSL. In the previous example, we set the value of the uniform variable from the OpenGL side using the following code:</p>
<pre>int loc = glGetUniformLocation(programHandle, "Tex1"); 
glUniform1i(loc, 0);</pre>
<p class="mce-root">Instead, if we're using OpenGL 4.2, we can specify the default value within the shader using the layout qualifier, as shown in the following statement:</p>
<pre>layout (binding=0) uniform sampler2D Tex1;</pre>
<p class="mce-root">This simplifies the code on the OpenGL side and makes this one less thing we need to worry about. The example code that accompanies this book uses this technique to specify the value of <kbd>Tex1</kbd>, so take a look there for a more complete example. We'll also use this layout qualifier in the following recipes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter05/scenetexture.cpp</kbd> file in the example code</li>
<li>For more information about sending data to a shader via vertex attributes refer to the <em>Sending data to a shader using vertex attributes and vertex buffer objects</em> recipe in <a href="3b817a9a-28a1-4be7-936c-b982b4dfacdf.xhtml"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Working with GLSL Programs</em></li>
<li><span class="ChapterrefPACKT">The <em>Using per-fragment shading for improved realism</em></span> recipe in <span class="ChapterrefPACKT"><a href="343fbd70-0012-4449-afe6-a724b330b441.xhtml">Chapter 4</a>, <em>Lighting and Shading</em></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying multiple textures</h1>
                </header>
            
            <article>
                
<p class="mce-root">The application of multiple textures to a surface can be used to create a wide variety of effects. The base layer texture might represent the <em>clean</em> surface and the second layer could provide additional detail such as shadow, blemishes, roughness, or damage. In many games, so-called light maps are applied as an additional texture layer to provide information about light exposure, effectively producing shadows, and shading without the need to explicitly calculate the reflection model. These kinds of textures are sometimes referred to as <em>pre-baked</em> lighting. In this recipe, we'll demonstrate this multiple texture technique by applying two layers of texture. The base layer will be a fully opaque brick image, and the second layer will be one that is partially transparent. The non-transparent parts look like moss that has grown on the bricks beneath.</p>
<p class="mce-root">The following image shows an example of multiple textures. The textures on the left are applied to the cube on the right. The base layer is the brick texture, and the moss texture is applied on top. The transparent parts of the moss texture reveal the brick texture underneath:</p>
<div class="packt_figure">
<div class="CDPAlignCenter CDPAlign"><img src="assets/a518e8fc-d6e5-4a3a-ade8-be9b8988e259.png" style="width:21.92em;height:14.58em;"/></div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">We'll start with the shaders developed in the previous recipe, <em>Applying a 2D texture</em>, as well as the <kbd>Texture::loadTexture</kbd> function described there.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>In the initialization section of your OpenGL program, load the two images into texture memory in the same way as indicated in the previous recipe, <em>Applying a 2D texture</em>. Make sure that the brick texture is loaded into texture unit 0 and the moss texture in texture unit 1: </li>
</ol>
<pre style="padding-left: 60px"> GLuint brick = Texture::loadTexture("brick1.jpg");<br/> GLuint moss = Texture::loadTexture("moss.png");<br/><br/>// Load brick texture file into channel 0<br/>glActiveTexture(GL_TEXTURE0);<br/>glBindTexture(GL_TEXTURE_2D, brick);<br/>// Load moss texture file into channel 1<br/>glActiveTexture(GL_TEXTURE1);<br/>glBindTexture(GL_TEXTURE_2D, moss); </pre>
<ol start="2">
<li>Starting with the fragment shader from the recipe <em>Applying a 2D texture</em>, replace the declaration of the sampler variable <kbd>Tex1</kbd> with the following code:</li>
</ol>
<pre style="padding-left: 60px">layout(binding=0) uniform sampler2D BrickTex; 
layout(binding=1) uniform sampler2D MossTex;</pre>
<ol start="3">
<li>In the  <kbd>blinnPhong</kbd> function, get samples from both textures and mix them together. Then, apply the mixed color to both the ambient and diffuse reflectivity:</li>
</ol>
<pre style="padding-left: 60px">vec4 brickTexColor = texture( BrickTex, TexCoord );<br/>vec4 mossTexColor = texture( MossTex, TexCoord );<br/>vec3 col = mix(brickTexColor.rgb, mossTexColor.rgb, mossTexColor.a);<br/>vec3 ambient = Light.La * col;<br/>// ...<br/>vec3 diffuse = col * sDotN;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The preceding code that loads the two textures into the OpenGL program is very similar to the code from the previous recipe, <em>Applying a 2D texture</em>. The main difference is that we load each texture into a different texture unit. When loading the brick texture, we set the OpenGL state such that the active texture unit is unit zero:</p>
<pre>glActiveTexture(GL_TEXTURE0); </pre>
<p class="mce-root">And when loading the second texture, we set the OpenGL state to texture unit one:</p>
<pre>glActiveTexture(GL_TEXTURE1); </pre>
<p class="mce-root">In the fragment shader, we specify the texture binding for each sampler variable using the layout qualifier corresponding to the appropriate texture unit. We access the two textures using the corresponding uniform variables, and store the results in <kbd>brickTexColor</kbd> and <kbd>mossTexColor</kbd>. The two colors are blended together using the built-in function <kbd>mix</kbd>. The third parameter to the <kbd>mix</kbd> function is the percentage used when mixing the two colors. We use the alpha value of the moss texture for that parameter. This causes the result to be a linear interpolation of the two colors based on the value of the alpha in the moss texture. For those familiar with OpenGL blending functions, this is the same as the following blending function:</p>
<pre>glBlendFunc( GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA ); </pre>
<p class="mce-root">In this case, the color of the moss would be the source color, and the color of the brick would be the destination color. Finally, we use the result of the <kbd>mix</kbd> function as the ambient and diffuse reflectivities in the Blinn-Phong reflection model.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this example, we mixed the two texture colors together using the alpha value of the second texture. This is just one of many options for mixing the texture colors. There are a number of different choices here, and your choice will be dependent on the kind of texture data available and the desired effect. A popular technique is to use an additional vertex attribute to augment the amount of blending between the textures. This additional vertex attribute would allow us to vary the blending factor throughout a model. For example, we could vary the amount of moss that grows on a surface by defining another vertex attribute that would control the amount of blending between the moss texture and the base texture. A value of zero might correspond to zero moss, up to a value of one, which would enable blending based on the texture's alpha value alone.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter05/scenemultitex.cpp</kbd> file in the example code</li>
<li>The <em>Applying a 2D texture</em> recipe</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using alpha maps to discard pixels</h1>
                </header>
            
            <article>
                
<p class="mce-root">To create the effect of an object that has holes, we could use a texture with an appropriate alpha channel that contains information about the transparent parts of the object. However, that requires us to make the depth buffer read-only and render all of our polygons from back to front in order to avoid blending problems. We would need to sort our polygons based on the camera position and then render them in the correct order. What a pain! With GLSL shaders, we can avoid all of this by using the <kbd>discard</kbd> keyword to completely discard fragments when the alpha value of the texture map is below a certain value. By completely discarding the fragments, there's no need to modify the depth buffer because when discarded, they aren't evaluated against the depth buffer at all. We don't need to depth-sort our polygons because there is no blending.</p>
<p class="mce-root">The following image on the right shows a teapot with fragments discarded based upon the texture on the left. The fragment shader discards fragments that correspond to texels that have an alpha value below a certain threshold:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/ca5ff33e-71b9-4248-a964-0ecb834d24eb.png" style="width:35.42em;height:14.00em;"/></div>
<p>If we create a texture map that has an alpha channel, we can use the value of the alpha channel to determine whether or not the fragment should be discarded. If the alpha value is below a certain value, then the pixel is discarded.</p>
<p>As this will allow the viewer to see within the object, possibly making some back faces visible, we'll need to use two-sided lighting when rendering the object.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Start with the same shader pair and setup from the previous recipe, <em>Applying a 2D texture</em>. Load the base texture for the object into texture unit 0, and your alpha map into texture unit 1.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To discard fragments based on alpha data from a texture, use the following steps:</p>
<ol>
<li>Use the same vertex and fragment shaders from the <em>Applying a 2D texture </em><span>recipe.</span> However, make the following modifications to the fragment shader.</li>
<li>Replace the <kbd>sampler2D</kbd> uniform variable with the following:</li>
</ol>
<pre style="padding-left: 60px">layout(binding=0) uniform sampler2D BaseTex; 
layout(binding=1) uniform sampler2D AlphaTex;</pre>
<ol start="3">
<li>In the <kbd>blinnPhong</kbd> function, use <kbd>BaseTex</kbd> to look up the value of the ambient and diffuse reflectivity.</li>
<li>Replace the contents of the <kbd>main</kbd> function with the following code:</li>
</ol>
<pre style="padding-left: 60px">void main() {<br/>    vec4 alphaMap = texture( AlphaTex, TexCoord );<br/><br/>    if(alphaMap.a &lt; 0.15 )<br/>        discard;<br/>    else {<br/>        if( gl_FrontFacing ) {<br/>            FragColor = vec4( <br/>            blinnPhong(Position,normalize(Normal)), 1.0 );<br/>        } else {<br/>            FragColor = vec4( blinnPhong(Position,normalize(-<br/>            Normal)), 1.0 );<br/>        }<br/>    }<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Within the <kbd>main</kbd> function of the fragment shader, we access the alpha map texture and store the result in <kbd>alphaMap</kbd>. If the alpha component of <kbd>alphaMap</kbd> is less than a certain value (<kbd>0.15</kbd>, in this example), then we discard the fragment using the <kbd>discard</kbd> keyword.</p>
<p>Otherwise, we compute the Blinn-Phong lighting model using the normal vector oriented appropriately, depending on whether or not the fragment is a front facing fragment.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>This technique is fairly simple and straightforward, and is a nice alternative to traditional blending techniques. It is a great way to make holes in objects or to present the appearance of decay. If your alpha map has a gradual change in the alpha throughout the map (for example, an alpha map where the alpha values make a smoothly varying height field), then it can be used to animate the decay of an object. We could vary the alpha threshold (<kbd>0.15</kbd>, in the preceding example) from 0.0 to 1.0 to create an animated effect of the object gradually decaying away to nothing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter05/sceneaphatest.cpp</kbd> file in the example code</li>
<li>The <em>Applying multiple textures</em> recipe</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using normal maps</h1>
                </header>
            
            <article>
                
<p><strong>Normal </strong><strong>mapping</strong> is a technique for "faking" variations in a surface that doesn't really exist in the geometry of the surface. It is useful for producing surfaces that have bumps, dents, roughness, or wrinkles without actually providing enough position information (vertices) to fully define those deformations. The underlying surface is actually smooth, but is made to appear rough by varying the normal vectors using a texture (the normal map). The technique is closely related to bump mapping or displacement mapping. With normal maps, we modify the normal vectors based on information that is stored in a texture. This creates the appearance of a bumpy surface without actually providing the geometry of the bumps.</p>
<p>A normal map is a texture in which the data stored within the texture is interpreted as normal vectors instead of colors. The normal vectors are typically encoded into the RGB information of the normal map so that the red channel contains the <em>x</em> coordinate, the green channel contains the <em>y</em> coordinate, and the blue channel contains the <em>z</em> coordinate. The normal map can then be used as a <em>texture</em> in the sense that the texture values affect the normal vector used in the reflection model rather than the color of the surface. This can be used to make a surface look like it contains variations (bumps or wrinkles) that do not actually exist in the geometry of the mesh.</p>
<p>The following images show an ogre mesh (courtesy of Keenan Crane) with and without a normal map. The upper-left corner shows the base color texture for the ogre. In this example, we use this texture as the diffuse reflectivity in the Phong reflection model. The upper right shows the ogre with the color texture and default normal vectors. The bottom left is the normal map texture. The bottom right shows the ogre with the color texture and normal map. Note the additional detail in the wrinkles provided by the normal map:</p>
<div class="packt_figure">
<div class="CDPAlignCenter CDPAlign"><img src="assets/3b4c094a-b157-40f8-899d-3a1f4e8a0d6f.png" style="width:21.50em;height:21.50em;"/></div>
</div>
<p>A normal map can be produced in a number of ways. Many 3D modeling programs such as Maya, Blender, or 3D Studio Max can generate normal maps. Normal maps can also be generated directly from grayscale hightmap textures. There is a NVIDIA plugin for Adobe Photoshop that provides this functionality (see <a href="http://developer.nvidia.com/object/photoshop_dds_plugins.html"><span class="URLPACKT">http://developer.nvidia.com/object/photoshop_dds_plugins.html</span></a>).</p>
<p>Normal maps are interpreted as vectors in a <strong>tangent </strong><strong>space</strong> (also called the <strong>object </strong><strong>local </strong><strong>coordinate </strong><strong>system</strong>). In the tangent coordinate system, the origin is located at the surface point and the normal to the surface is aligned with the <em>z</em> axis (0, 0, 1). Therefore, the <em>x</em> and <em>y</em> axes are at a tangent to the surface. The following image shows an example of the tangent frames at two different positions on a surface:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/3d332f28-53f1-4b13-9abe-9dde26f44581.png" style="width:21.58em;height:16.42em;"/></div>
<p>The advantage of using such a coordinate system lies in the fact that the normal vectors stored within the normal map can be treated as perturbations to the true normal, and are independent of the object coordinate system. This avoids the need to transform the normals, add the perturbed normal, and renormalize. Instead, we can use the value in the normal map directly in the reflection model without any modification.</p>
<p>To make all of this work, we need to evaluate the reflection model in tangent space. In order to do so, we transform the vectors used in our reflection model into tangent space in the vertex shader, and then pass them along to the fragment shader where the reflection model will be evaluated. To define a transformation from the camera (eye) coordinate system to the tangent space coordinate system, we need three normalized, co-orthogonal vectors (defined in eye coordinates) that define the tangent space system. The <em>z</em> axis is defined by the normal vector (<em>n</em>), the <em>x</em> axis is defined by a vector called the <em>tangent vector</em> (<em>t</em>), and the <em>y</em> axis is often called the <em>binormal vector</em> (<em>b</em>). A point, <em>P</em>, defined in camera coordinates, could then be transformed into tangent space in the following way:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/05d564ec-48a3-4773-aaa1-8145d8b9cd20.png" style="width:17.67em;height:5.42em;"/></div>
<p>In the preceding equation, <em>S</em> is the point in tangent space and <em>P</em> is the point in camera coordinates. In order to apply this transformation within the vertex shader, the OpenGL program must provide at least two of the three vectors that define the object local system along with the vertex position. The usual situation is to provide the normal vector (<em>n</em>) and the tangent vector (<em>t</em>). If the tangent vector is provided, the binormal vector can be computed as the cross product of the tangent and normal vectors.</p>
<p>Tangent vectors are sometimes included as additional data in mesh data structures. If the tangent data is not available, we can approximate the tangent vectors by deriving them from the variation of the texture coordinates across the surface (see <em>Computing Tangent Space Basis Vectors for an Arbitrary Mesh</em>, Eric Lengyel, Terathon Software 3D Graphics Library, 2001, at <a href="http://www.terathon.com/code/tangent.html"><span class="URLPACKT">http://www.terathon.com/code/tangent.html</span></a>).</p>
<div class="packt_infobox"><span>One must take care that the tangent vectors are consistently defined across the surface. In other words, the direction of the tangent vectors should not vary greatly from one vertex to its neighboring vertex. Otherwise, it can lead to ugly shading artifacts.</span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the following example, we'll read the vertex position, normal vector, tangent vector, and texture coordinate in the vertex shader. We'll transform the position, normal, and tangent to camera space, and then compute the binormal vector (in camera space). Next, we'll compute the viewing direction (<em>v</em>) and the direction toward the light source (<em>s</em>) and then transform them to tangent space. We'll pass the tangent space <em>v</em> and <em>s</em> vectors and the (unchanged) texture coordinate to the fragment shader, where we'll evaluate the Blinn-Phong reflection model using the tangent space vectors and the normal vector retrieved from the normal map.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Set up your OpenGL program to provide the position in attribute location <kbd>0</kbd>, the normal in attribute location <kbd>1</kbd>, the texture coordinate in location <kbd>2</kbd>, and the tangent vector in location <kbd>3</kbd>. For this example, the fourth coordinate of the tangent vector should contain the <em>handedness</em> of the tangent coordinate system (either <kbd>-1</kbd> or <kbd>+1</kbd>). This value will be multiplied by the result of the cross product.</p>
<p>Load the normal map into texture unit one and the color texture into texture unit zero.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To render an image using normal mapping, use the following shaders:</p>
<ol>
<li>In the vertex shader, find the object local coordinate system (tangent space) and transform everything into that space. Pass the tangent space light direction and view direction to the fragment shader:</li>
</ol>
<pre style="padding-left: 60px">layout (location = 0) in vec3 VertexPosition; 
layout (location = 1) in vec3 VertexNormal; 
layout (location = 2) in vec2 VertexTexCoord; 
layout (location = 3) in vec4 VertexTangent; 
 
out vec3 LightDir; 
out vec2 TexCoord; 
out vec3 ViewDir; 
 
// Other uniform variables...
 
void main() { 
  // Transform normal and tangent to eye space 
  vec3 norm = normalize(NormalMatrix * VertexNormal); 
  vec3 tang = normalize(NormalMatrix * VertexTangent.xyz); 
  // Compute the binormal 
  vec3 binormal = normalize( cross( norm, tang ) ) * <br/>  VertexTangent.w; 
  // Matrix for transformation to tangent space 
  mat3 toObjectLocal = mat3( 
      tang.x, binormal.x, norm.x, 
      tang.y, binormal.y, norm.y, 
      tang.z, binormal.z, norm.z ) ; 
  // Get the position in eye coordinates 
  vec3 pos = vec3( ModelViewMatrix *  
                     vec4(VertexPosition,1.0) ); 
 
  // Transform light dir. and view dir. to tangent space 
  LightDir = toObjectLocal * (Light.Position.xyz - pos); 
  ViewDir = toObjectLocal * normalize(-pos); 
 
  // Pass along the texture coordinate 
  TexCoord = VertexTexCoord; 
 
  gl_Position = MVP * vec4(VertexPosition,1.0); 
} </pre>
<ol start="2">
<li>In the fragment shader, update the <kbd>blinnPhong</kbd> function to use the normal from the texture and to use the input variables for the light and view directions:</li>
</ol>
<pre style="padding-left: 60px">in vec3 LightDir; 
in vec2 TexCoord; 
in vec3 ViewDir; 
 
layout(binding=0) uniform sampler2D ColorTex; 
layout(binding=1) uniform sampler2D NormalMapTex; 
<br/>// Other uniform variables...<br/> 
layout( location = 0 ) out vec4 FragColor; 
 
vec3 blinnPhong( vec3 n ) {<br/>  // Similar to previous examples, except <br/>  // using normalize(LightDir) and normalize(ViewDir)...
}
 
void main() { 
    // Lookup the normal from the normal map<br/>    vec3 norm = texture(NormalMapTex, TexCoord).xyz;<br/>    norm = 2.0 * norm - 1.0;<br/>    FragColor = vec4( blinnPhong(norm), 1.0 );
}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The vertex shader starts by transforming the vertex normal and the tangent vectors into eye coordinates by multiplying by the normal matrix (and renormalizing). The binormal vector is then computed as the cross product of the normal and tangent vectors. The result is multiplied by the <kbd>w</kbd> coordinate of the vertex tangent vector, which determines the handedness of the tangent space coordinate system. Its value will be either <kbd>-1</kbd> or <kbd>+1</kbd>.</p>
<p>Next, we create the transformation matrix used to convert from eye coordinates to tangent space and store the matrix in <kbd>toObjectLocal</kbd>. The position is converted to eye space and stored in <kbd>pos</kbd>, and we compute the light direction by subtracting <kbd>pos</kbd> from the light position. The result is multiplied by <kbd>toObjectLocal</kbd> to convert it into tangent space, and the final result is stored in the output variable <kbd>LightDir</kbd>. This value is the direction to the light source in tangent space and will be used by the fragment shader in the reflection model.</p>
<p>Similarly, the view direction is computed and converted to tangent space by normalizing <kbd>pos</kbd> and multiplying by <kbd>toObjectLocal</kbd>. The result is stored in the output variable <kbd>ViewDir</kbd>.</p>
<p>The texture coordinate is passed to the fragment shader unchanged by just assigning it to the output variable <kbd>TexCoord</kbd>.</p>
<p>In the fragment shader, the tangent space values for the light direction and view direction are received in the variables <kbd>LightDir</kbd> and <kbd>ViewDir</kbd>. The <kbd>blinnPhong</kbd> function is slightly modified from what has been used in previous recipes. The only parameter is the normal vector. The function computes the Blinn-Phong reflection model, taking the value for the diffuse reflectivity from the texture <kbd>ColorTex</kbd>, and uses <kbd>LightDir</kbd> and <kbd>ViewDir</kbd> for the light and view directions rather than computing them.</p>
<p>In the main function, the normal vector is retrieved from the normal map texture and stored in the variable <kbd>normal</kbd>. Since textures store values that range from zero to one and normal vectors have components that range from -1 to +1, we need to re-scale the value to that range. We do so by multiplying by <kbd>2.0</kbd> and then subtracting <kbd>1.0</kbd>. </p>
<div class="packt_infobox">For some normal maps, the <em>z</em> coordinate is never negative because in tangent space that would correspond to a normal that points into the surface. In which case, we could assume that <em>z</em> ranges from 0 to 1, and use the full resolution of the channel for that range. However, there is no standard convention for the <em>z</em> coordinate.</div>
<p>Finally, the <kbd>blinnPhong</kbd> function is called and is passed the <kbd>normal</kbd>. The <kbd>blinnPhong</kbd> function evaluates the reflection model using <kbd>LightDir</kbd>, <kbd>ViewDir</kbd>, and <kbd>n</kbd>, all of which are defined in tangent space. The result is applied to the output fragment by assigning it to <kbd>FragColor</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter05/scenenormalmap.cpp</kbd> file in the example code</li>
<li>The <em>Applying multiple textures</em> recipe</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parallax mapping</h1>
                </header>
            
            <article>
                
<p>Normal maps are a great way to introduce surface detail without adding additional geometry.  However, they have some limitations. For example, normal maps do not provide parallax effects as the viewer's position changes and they don't support self-occlusion.  <strong>Parallax mapping</strong> is a technique, originally introduced in 2001, that uses modification of texture coordinates based on a height map to simulate parallax and self-occlusion effects.  It requires both a <strong>normal map</strong> and a <strong>height map</strong>.  A height map (also called a <strong>bump map</strong>) is a grayscale image where each texel has a single scalar value representing the height of the surface at the texel. We can consider any height between 0 and 1 as the <em>true surface</em>, and then use the value in the height map as an offset from there. In this recipe, we'll use a value of <kbd>1.0</kbd> as the true surface, so a height map value of <kbd>0.0</kbd> is a distance of <kbd>1.0</kbd> <em>below</em> the true surface (see the following images).</p>
<p>To simulate parallax, we want to offset the texture coordinates by an amount that depends on the direction toward the viewer (camera). A parallax effect is stronger at steeper angles, so we want the offset amount to be stronger when the angle between the normal and the view vector (vector pointing toward the camera) is larger. In addition, we want to offset the texture coordinates in the same direction as the view vector. Similar to the normal mapping recipe, we'll work with tangent space.</p>
<p>As we discussed earlier, in tangent space, the normal vector is the same as the <em>z</em> axis. If <em>e</em> is the vector toward the camera in tangent space, we'll use the vector pointing in the opposite direction (<em>v = -e</em>). First, let's consider the case of standard normal mapping. The viewer perceives the color and normal at point <strong>P</strong>, but they should see the color and normal at point <strong>Q</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6fe28cbc-166c-46c2-ae0b-c0a8c38d9196.png" style="width:30.67em;height:13.00em;"/></div>
<p>Therefore, we want to offset the texture coordinates by an amount that is proportional to <strong>Δx</strong> in the preceding diagram so that the viewer sees the shading for point <strong>Q</strong>, not point <strong>P</strong>. You could draw a similar picture for the y-z cross-section as well, the results would be nearly the same.</p>
<p>So, what we need to do is approximate <strong><span>Δx</span></strong> somehow. <span>Consider the right triangles, as shown here:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5320f83e-6bb0-42d5-8dee-15cc684af6ad.png" style="width:11.50em;height:8.50em;"/></div>
<p><span>The value of <strong>d</strong> is the depth of point <strong>Q</strong> (below the true surface), or in other words: <em>d = 1 - h<sub>q</sub></em>, where <em>h<sub>q</sub></em> is the height of the bump map at point <em>Q</em>. By the rule of similar triangles, we can write the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span><img class="fm-editor-equation" src="assets/b906fae2-2c75-4929-878b-36f7eb329ca4.png" style="width:4.83em;height:2.67em;"/></span></div>
<p class="mce-root CDPAlignLeft CDPAlign"><span>Applying the same analysis for <em>y</em>, we get the following pair of offsets:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span><span><img class="fm-editor-equation" src="assets/95722ae0-94ef-47b7-b300-f346ec34a4af.png" style="width:6.92em;height:5.17em;"/></span></span></div>
<p class="mce-root CDPAlignLeft CDPAlign"><span>Unfortunately, we don't have a value for <em>d</em> in the preceding equations, because we don't know the value for <em>Q</em>. There's no way of quickly finding it either; we'd need to trace a ray through the height map (which is what we'll do in the next recipe). So for now, we'll just approximate <em>d</em> by using the height (depth) at <em>P (1 - h<sub>p</sub>)</em>. It is a rough estimate, but if we assume the height map doesn't have a lot of really high frequency variation, it works fairly well in practice.</span></p>
<p class="mce-root CDPAlignLeft CDPAlign">Therefore, we have the following equation for offsetting a texture coordinate (<em>P</em>) at a given surface point:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2bbfd56e-7731-49c6-b0b3-2eab96d7928f.png" style="width:13.58em;height:2.58em;"/></div>
<p>In the preceding equation, <em>S</em> is a scale factor that can be used to restrict the magnitude of the effect and to scale it to texture space. It is usually a very small value (between 0 and 0.05), and may need to be tuned to a particular surface. </p>
<p> The following images show the effect compared to basic normal mapping. On the left, a single quad rendered with simple normal mapping, and on the right is the same geometry using normal mapping, along with parallax mapping:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/41225a36-26c2-4e0a-b1ac-2696d450d366.png" style="width:19.75em;height:15.42em;"/></div>
<p>The effect is admittedly quite subtle, and there are some undesirable artifacts in this example, but the overall effect is clear. Note that both images use the same geometry, camera position, and texture maps. If you focus on the bricks in the distance (furthest from the viewer), you can see some simulation of occlusion, and overall the effect is more realistic on the right. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>For parallax mapping, we need three textures: a height map texture, a normal map texture, and a color texture. We could combine the height map and normal map into a single texture, storing the height values in the alpha channel and the normal in the R, G, and B. This is a common technique and saves a significant amount of disk and memory space. In this recipe, we'll treat them as separate textures.</p>
<p>We also need a mesh with tangent vectors as well so that we can transform into tangent space. For more information on tangent space, see the previous recipe.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>We can use the same vertex shader as was used in the previous recipe, <em>Using normal maps</em>.  The vertex shader transforms the view direction and the light direction and passes them to the fragment shader. It also passes along the texture coordinate.</p>
<p>The fragment shader uses the tangent space view direction and the height map value at the current texture coordinate to offset the texture coordinates. It then uses the new texture coordinate value to do shading as usual:</p>
<pre>in vec3 LightDir;  // Tangent space<br/>in vec2 TexCoord;<br/>in vec3 ViewDir;  // Tangent space<br/><br/>layout(binding=0) uniform sampler2D ColorTex;<br/>layout(binding=1) uniform sampler2D NormalMapTex;<br/>layout(binding=2) uniform sampler2D HeightMapTex;<br/><br/>// Light and material uniforms <br/><br/>layout( location = 0 ) out vec4 FragColor;<br/><br/>vec3 blinnPhong( ) {<br/>  vec3 v = normalize(ViewDir);<br/>  vec3 s = normalize(LightDir);<br/><br/>  const float bumpFactor = 0.015; <br/>  float height = 1 - texture(HeightMapTex, TexCoord).r;<br/>  vec2 delta = v.xy * height * bumpFactor / v.z;<br/>  vec2 tc = TexCoord.xy - delta;<br/><br/>  vec3 n = texture(NormalMapTex, tc).xyz;<br/>  n.xy = 2.0 * n.xy - 1.0;<br/>  n = normalize(n);<br/><br/>  float sDotN = max( dot(s,n), 0.0 );<br/>  <br/>  vec3 texColor = texture(ColorTex, tc).rgb;<br/>  vec3 ambient = Light.La * texColor;<br/>  vec3 diffuse = texColor * sDotN;<br/>  vec3 spec = vec3(0.0);<br/>  if( sDotN &gt; 0.0 ) { <br/>    vec3 h = normalize( v + s );<br/>    spec = Material.Ks *<br/>            pow( max( dot(h,n), 0.0 ), Material.Shininess );<br/>  }<br/>  return ambient + Light.L * (diffuse + spec);<br/>}<br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In the <kbd>blinnPhong</kbd> method within the fragment shader, we start by computing the offset for the texture coordinate (the <span><kbd>delta</kbd> </span>variable ). The <kbd>bumpFactor</kbd> constant is generally somewhere between 0 and 0.05.  In this case, we use <kbd>0.015</kbd>, but you'll need to tune this for your particular normal/height map. We offset the texture coordinate by the value of delta. We subtract rather than add here because <kbd>ViewDir</kbd> is actually pointing toward the viewer, so we need to offset in the opposite direction. Note that we also invert the height value, as discussed in the preceding analysis. Using the offset texture coordinate (<kbd>tc</kbd>), we compute the shading using the Blinn-Phong model with data from the normal map and color texture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Parallax mapping produces subtle but pleasing effects. However, it does suffer from some undesirable artifacts such as so-called <strong>texture swim</strong> and performs poorly with bump maps that have steep bumps or high frequency bumps. An improvement to parallax mapping that performs better is called steep parallax mapping, which is discussed in the next recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter05/sceneparallax.cpp</kbd> file in the example code</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Steep parallax mapping with self shadowing</h1>
                </header>
            
            <article>
                
<p>This recipe builds on the previous one, parallax mapping, so if you haven't already done so, you may want to review that recipe prior to reading this one.  </p>
<p><strong>Steep parallax mapping</strong> is a technique, first published by Morgan McGuire and Max McGuire in 2005. It improves upon parallax mapping, producing much better results at the cost of more fragment shader work. Despite the additional cost, the algorithm is still well suited to real-time rendering on modern GPUs.  </p>
<p>The technique involves tracing the eye ray through the height map in discrete steps until a collision is found in order to more precisely determine the appropriate offset for the texture coordinate. Let's revisit the diagram from the previous recipe, but this time, we'll break up the height map into <strong>n</strong> discrete levels (indicated by the dashed lines):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3a5892c0-3ea7-43e8-95d3-64e5f001efa2.png" style="width:37.67em;height:10.67em;"/></div>
<p>As before, our goal is to offset the texture coordinates so that the surface is shaded based on the bump surface, not the true surface. The point <strong>P</strong> is the surface point on the polygon being rendered. We trace the view vector from point <strong>P</strong> to each level consecutively until we find a point that lies on or below the bump surface. In the following image, we'll find the point <strong>Q</strong> after three iterations.  </p>
<p>As in the previous recipe, we can derive the change in <strong>x</strong> and <strong>y</strong> for a single iteration using similar triangles (see the <em>Parallax mapping</em> recipe):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ea7dcad2-e8b6-405f-942d-1d2b69297d36.png" style="width:14.17em;height:3.58em;"/></div>
<p>As before, the scale factor (<em>S</em>) is used to vary the influence of the effect and to scale it to texture space. <em>n</em> is the number of height levels.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Using this equation, we can step through the height levels, starting at <em>P</em> and following the view vector away from the camera. We continue until we find a point that lies on or below the surface of the height map. We then use the texture coordinate at that point for shading.  Essentially, we're implementing a very simple ray marcher in the fragment shader.</p>
<p>The results are impressive. The following image shows three versions of the same surface for comparison. On the left is the surface with normal map applied. The middle image is the same surface rendered with parallax mapping. The right-hand image is generated using steep parallax mapping. All three images use the same normal map, height map, geometry, and camera position. They are all rendered as a single quad (two triangles). Note how the steep parallax shows the varying height of each brick. The height of each brick was always included in the height map, but the parallax mapping technique didn't make it noticeable:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/73ed96f0-c548-4311-a705-340a1f23d363.png" style="width:25.92em;height:13.67em;"/></div>
<p>You may have noticed that the image on the right also includes shadows. Some bricks cast shadows onto other bricks. This is accomplished with a simple addition to the preceding technique. Once we find point <strong>Q</strong>, we march another ray in the direction of the light source.  If that ray collides with the surface, the point is in shadow and we shade with ambient lighting only. Otherwise, we shade the point normally. The following diagram illustrates this idea:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1daaac37-edbd-4811-8a3d-e826c40b9779.png" style="width:47.67em;height:11.50em;"/></div>
<p>In the preceding diagram, point <strong>Q</strong> is in shadow and point <strong>T</strong> is not. In each case, we march the ray along the direction toward the light source (<strong>s</strong>). We evaluate the height map at each discrete height level. In the case of point <strong>Q</strong>, we find a point that lies below the bump surface, but for point <strong>T</strong>, all points lie above it. </p>
<p>The ray marchine process is nearly identical to that described before for the view vector. We start at point <strong>Q</strong> and move along the ray toward the light. If we find a point that is below the surface, then the point is occluded from the light source. Otherwise, the point is shaded normally. We can use the same equation we used for marching the view vector, replacing the view vector with the vector toward the light source.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>For this algorithm, we need a height map, a normal map, and a color map. We also need tangent vectors in our mesh so that we can transform into tangent space.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The vertex shader is the same as the one used in the <em>Parallax mapping</em> <span>recipe</span>.</p>
<p>In the fragment shader, we break the process into two functions: <kbd>findOffset</kbd> and <kbd>isOccluded</kbd>.  The first traces the view vector to determine the texture coordinate offset.  The second traces the light vector to determine whether or not the point is in shadow:</p>
<pre>in vec3 LightDir;  // Tangent space<br/>in vec2 TexCoord;<br/>in vec3 ViewDir;   // Tangent space<br/><br/>layout(binding=0) uniform sampler2D ColorTex;<br/>layout(binding=1) uniform sampler2D NormalMapTex;<br/>layout(binding=2) uniform sampler2D HeightMapTex;<br/><br/>// Material and light uniforms...<br/><br/>layout( location = 0 ) out vec4 FragColor;<br/><br/>const float bumpScale = 0.03;<br/><br/>vec2 findOffset(vec3 v, out float height) {<br/>  const int nSteps = int(mix(60, 10, abs(v.z)));<br/>  float htStep = 1.0 / nSteps;<br/>  vec2 deltaT = (v.xy * bumpScale) / (nSteps * v.z);<br/>  float ht = 1.0;<br/>  vec2 tc = TexCoord.xy;<br/>  height = texture(HeightMapTex, tc).r;<br/>  while( height &lt; ht ) {<br/>    ht -= htStep;<br/>    tc -= deltaT;<br/>    height = texture(HeightMapTex, tc).r;<br/>  }<br/>  return tc;<br/>}<br/><br/>bool isOccluded(float height, vec2 tc, vec3 s) {<br/>  // Shadow ray cast<br/>  const int nShadowSteps = int(mix(60,10,s.z));<br/>  float htStep = 1.0 / nShadowSteps;<br/>  vec2 deltaT = (s.xy * bumpScale) / ( nShadowSteps * s.z );<br/>  float ht = height + htStep * 0.5;<br/>  while( height &lt; ht &amp;&amp; ht &lt; 1.0 ) {<br/>    ht += htStep;<br/>    tc += deltaT;<br/>    height = texture(HeightMapTex, tc).r;<br/>  }<br/><br/>  return ht &lt; 1.0;<br/>}<br/><br/>vec3 blinnPhong( ) { <br/>  vec3 v = normalize(ViewDir);<br/>  vec3 s = normalize( LightDir );<br/><br/>  float height = 1.0;<br/>  vec2 tc = findOffset(v, height);<br/><br/>  vec3 texColor = texture(ColorTex, tc).rgb;<br/>  vec3 n = texture(NormalMapTex, tc).xyz;<br/>  n.xy = 2.0 * n.xy - 1.0;<br/>  n = normalize(n);<br/><br/>  float sDotN = max( dot(s,n), 0.0 );<br/>  vec3 diffuse = vec3(0.0), <br/>      ambient = Light.La * texColor,<br/>      spec = vec3(0.0);<br/><br/>  if( sDotN &gt; 0.0 &amp;&amp; !isOccluded(height, tc, s) ) {<br/>    diffuse = texColor * sDotN;<br/>    vec3 h = normalize( v + s );<br/>    spec = Material.Ks *</pre>
<pre>            pow( max( dot(h,n), 0.0 ), Material.Shininess );<br/>  }<br/><br/>  return ambient + Light.L * (diffuse + spec);<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The <kbd>findOffset</kbd> <span>function </span>determines the texture coordinate to use when shading. We pass in the vector <em>toward</em> the viewer (we negate the direction to move away from the eye), and the function returns the texture coordinate. It also returns the value of the height at that location via the output parameter <kbd>height</kbd>. The first line determines the number of discrete height levels (<kbd>nSteps</kbd>). We pick a number between 10 and 60 by interpolating using the value of the <em>z</em> coordinate of the view vector. If the <em>z</em> coordinate is small, then the view vector is close to vertical with respect to the height levels. When the view vector is close to vertical, we can use fewer steps because the ray travels a shorter relative horizontal distance between levels. However, when the vector is closer to horizontal, we need more steps as the ray travels a larger horizontal distance when moving from one level to the next.  The <kbd>deltaT</kbd> <span>variable </span>is the amount that we move through texture space when moving from one height level to the next. This is the second term in the equation listed previously.  </p>
<p>The ray marching proceeds with the following loop. The <kbd>ht</kbd> <span>variable </span>tracks the height level. We start it at <kbd>1.0</kbd>. The <kbd>height</kbd> <span>variable </span>will be the value of the height map at the current position. The <kbd>tc</kbd> <span>variable </span>will track our movement through texture space, initially at the texture coordinate of the fragment (<kbd>TexCoord</kbd>). We look up the value in the height map at <kbd>tc</kbd>, and then enter the loop.</p>
<p>The loop continues until the value in the height map (<kbd>height</kbd>) is less than the value of the discrete height level (<kbd>ht</kbd>). Within the loop, we change <kbd>ht</kbd> to move down one level and update the texture coordinate by <kbd>deltaT</kbd>. Note that we subtract <kbd>deltaT</kbd> because we are moving away from the viewer. Then, we look up the value of the height map (<kbd>height</kbd>) at the new texture coordinate and repeat.</p>
<p>When the loop terminates, <kbd>tc</kbd> should have the value of the offset texture coordinate, and <kbd>height</kbd> is the value in the height map at that location. We return <kbd>tc</kbd>, and the value of <kbd>height</kbd> at the end of the loop is also returned to the caller via the output parameter.</p>
<p>Note that this loop isn't correct when we are viewing the back side of the face. However, the loop will still terminate at some point because we always decrease <kbd>ht</kbd> and the height map texture is assumed to be between 0 and 1. If back faces are visible, we need to modify this to properly follow the ray or invert the normal.</p>
<p>The <kbd>isOccluded</kbd> <span>function </span>returns whether or not the light source is occluded by the height map at that point. It is quite similar to the <kbd>findOffset</kbd> function. We pass in <kbd>height</kbd> previously determined by <kbd>findOffset</kbd>, the corresponding texture coordinate (<kbd>tc</kbd>), and the direction toward the light source (<kbd>s</kbd>). Similar to <kbd>findOffset</kbd>, we march the ray in the direction of <kbd>s</kbd>, beginning at the height and texture coordinate provided. Note that we begin the loop with a value for <kbd>ht</kbd> that is slightly offset from the value of the bump map there (<kbd>ht= height + htStep * 0.1</kbd>).  This is to avoid the so-called <strong>shadow acne</strong> effect. If we don't offset it, we can sometimes get <em>false positives</em> when the ray collides with the surface that it starts on, producing speckled shadows.  </p>
<p>The rest of the function contains a loop that is quite similar to the loop in <kbd>findOffset</kbd>.  However, we move upward through the height levels, and we are careful to stop when the value of <kbd>ht</kbd> reaches or exceeds 1.0.  At the end of the loop, we don't need the values of <kbd>height</kbd> or <kbd>tc</kbd>; we only need to know whether or not the loop stopped due to the first of the two conditions.  If <kbd>ht &lt; 1.0</kbd>, then we exit the loop before exceeding the range of the height map, indicating that we found a point along the ray that had a larger height.  Therefore, the point must be in shadow, so we return true. Otherwise, the light source is unoccluded, so we return false.</p>
<p>The <kbd>blinnPhong</kbd> function calls <kbd>findOffset</kbd> to determine the appropriate texture coordinates to use.  It then looks up the values in the normal map and color map at that location.  Next, it evaluates the Blinn-Phong reflection model using those values. However, it uses the <kbd>isOccluded</kbd> function to determine whether or not we should include diffuse and specular components. We also won't evaluate those components if the value of <kbd>sDotN</kbd> is less than or equal to zero, meaning that the light is behind (or tangent to) the face, as determined by the shading normal. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter05/sceneparallax.cpp</kbd> file in the example code</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simulating reflection with cube maps</h1>
                </header>
            
            <article>
                
<p>Textures can be used to simulate a surface that has a component that is purely reflective (a mirror-like surface such as chrome). In order to do so, we need a texture that is representative of the environment surrounding the reflective object. This texture could then be mapped onto the surface of the object in a way that represents how it would look when reflected off the surface. This general technique is known as <strong>environment </strong><strong>mapping</strong>.</p>
<p>In general, environment mapping involves creating a texture that is representative of the environment and mapping it onto the surface of an object. It is typically used to simulate the effects of reflection or refraction.</p>
<p>A <strong>cube </strong><strong>map</strong> is one of the more common varieties of textures used in environment mapping. A cube map is a set of six separate images that represent the environment projected onto each of the six faces of a cube. The six images represent a view of the environment from the point of view of a viewer located at the center of the cube. An example of a cube map is shown in the following image. The images are laid out as if the cube was <em>unfolded</em> and laid flat. The four images across the middle would make up the sides of the cube, and the top and bottom images correspond to the top and bottom of the cube:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/f5ed0318-06f2-4960-a0ac-df9ed3bf79bb.png" style="width:30.50em;height:22.83em;"/></div>
<p>OpenGL provides built-in support for cube map textures (using the <kbd>GL_TEXTURE_CUBE_MAP</kbd> target). The texture is accessed using a three-dimensional texture coordinate (s, t, r). The texture coordinate is interpreted as a direction vector from the center of the cube. The line defined by the vector and the center of the cube is extended to intersect one of the faces of the cube. The image that corresponds to that face is then accessed at the location of the intersection.</p>
<div class="packt_infobox"><span>Truth be told, the conversion between the three-dimensional texture coordinate used to access the cube map and the two-dimensional texture coordinate used to access the individual face image is somewhat complicated. It can be non-intuitive and confusing. A very accessible explanation can be found in the OpenGL specification document ( <a href="https://www.khronos.org/registry/OpenGL/index_gl.php">https://www.khronos.org/registry/OpenGL/index_gl.php</a> )</span><span>.  However, the good news is that if you are careful to orient your textures correctly within the cube map, the details of the conversion can be ignored and the texture coordinate can be visualized as a three-dimensional vector as described previously.</span></div>
<p>In this example, we'll demonstrate using a cube map to simulate a reflective surface. We'll also use the cube map to draw the environment around the reflective object (sometimes called a <strong>skybox</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>First, prepare the six images of the cube map. In this example, the images will have the following naming convention. There is a base name (stored in the <kbd>baseFileName</kbd> variable) followed by an underscore, followed by one of the six possible suffixes (<kbd>posx</kbd>, <kbd>negx</kbd>, <kbd>posy</kbd>, <kbd>negy</kbd>, <kbd>posz</kbd>, or <kbd>negz</kbd>), followed by the file extension. The suffixes <kbd>posx</kbd>, <kbd>posy</kbd>, and so on, indicate the axis that goes through the center of the face (positive <em>x</em>, positive <em>y</em>, and so on).</p>
<p>Make sure that they are all square images (preferably with dimensions that are a power of two), and that they are all the same size.</p>
<p>Set up your OpenGL program to provide the vertex position in attribute location 0, and the vertex normal in attribute location 1.</p>
<p>This vertex shader requires the model matrix (the matrix that converts from object coordinates to world coordinates) to be separated from the model-view matrix and provided to the shader as a separate uniform. Your OpenGL program should provide the model matrix in the uniform variable <kbd>ModelMatrix</kbd>.</p>
<p>The vertex shader also requires the location of the camera in world coordinates. Make sure that your OpenGL program sets the uniform <kbd>WorldCameraPosition</kbd> to the appropriate value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To render an image with reflection based on a cube map, and also render the cube map itself, carry out the following steps:</p>
<ol>
<li>We'll start by defining a function that will load the six images of the cube map into a single texture target:</li>
</ol>
<pre style="padding-left: 60px">GLuint Texture::loadCubeMap(const std::string &amp;baseName, <br/>const std::string &amp;extension) {<br/>    GLuint texID;<br/>    glGenTextures(1, &amp;texID);<br/>    glBindTexture(GL_TEXTURE_CUBE_MAP, texID);<br/><br/>    const char * suffixes[] = { "posx", "negx", "posy", <br/>    "negy", "posz", "negz" };<br/>    GLint w, h;<br/><br/>    // Load the first one to get width/height<br/>    std::string texName = baseName + "_" + suffixes[0] + extension;<br/>    GLubyte * data = Texture::loadPixels(texName, w, h, false);<br/><br/>    // Allocate immutable storage for the whole cube map texture<br/>    glTexStorage2D(GL_TEXTURE_CUBE_MAP, 1, GL_RGBA8, w, h);<br/>    glTexSubImage2D(GL_TEXTURE_CUBE_MAP_POSITIVE_X, <br/>        0, 0, 0, w, h, GL_RGBA, GL_UNSIGNED_BYTE, data);<br/>    stbi_image_free(data);<br/><br/>    // Load the other 5 cube-map faces<br/>    for( int i = 1; i &lt; 6; i++ ) {<br/>        std::string texName = baseName + "_" + suffixes[i] + <br/>        extension;<br/>        data = Texture::loadPixels(texName, w, h, false);<br/>        glTexSubImage2D(GL_TEXTURE_CUBE_MAP_POSITIVE_X + i, <br/>            0, 0, 0, w, h, GL_RGBA, GL_UNSIGNED_BYTE, data);<br/>        stbi_image_free(data);<br/>    }<br/><br/>    glTexParameteri(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_MAG_FILTER, <br/>    GL_LINEAR);<br/>    glTexParameteri(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_MIN_FILTER, <br/>    GL_NEAREST);<br/>    glTexParameteri(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_WRAP_S, <br/>    GL_CLAMP_TO_EDGE);<br/>    glTexParameteri(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_WRAP_T, <br/>    GL_CLAMP_TO_EDGE);<br/>    glTexParameteri(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_WRAP_R, <br/>    GL_CLAMP_TO_EDGE);<br/><br/>    return texID;<br/>}</pre>
<ol start="2">
<li>Use the following code for the vertex shader:</li>
</ol>
<pre style="padding-left: 60px"> 
layout (location = 0) in vec3 VertexPosition; 
layout (location = 1) in vec3 VertexNormal; 
layout (location = 2) in vec2 VertexTexCoord; 
 
out vec3 ReflectDir;      // The direction of the reflected ray <br/> 
uniform vec3 WorldCameraPosition; 
uniform mat4 ModelViewMatrix; 
uniform mat4 ModelMatrix; 
uniform mat3 NormalMatrix; 
uniform mat4 ProjectionMatrix; 
uniform mat4 MVP; 
 
void main() {
  // Compute the reflected direction in world coords. 
  vec3 worldPos = vec3(ModelMatrix *  
                             vec4(VertexPosition,1.0) ); 
  vec3 worldNorm = vec3(ModelMatrix *  
                              vec4(VertexNormal, 0.0)); 
  vec3 worldView = normalize( WorldCameraPosition - worldPos );
  ReflectDir = reflect(-worldView, worldNorm ); 
  <br/>  gl_Position = MVP * vec4(VertexPosition,1.0); 
} </pre>
<ol start="3">
<li>Use the following code for the fragment shader:</li>
</ol>
<pre style="padding-left: 60px">in vec3 ReflectDir;   // The direction of the reflected ray 
 
// The cube map 
layout(binding=0) uniform samplerCube CubeMapTex; 
 
uniform float ReflectFactor; // Amount of reflection 
uniform vec4 MaterialColor;  // Color of the object's "Tint"  
 
layout( location = 0 ) out vec4 FragColor; 
 
void main() { 
  // Access the cube map texture 
  vec4 cubeMapColor = texture(CubeMapTex, ReflectDir); 
  FragColor = mix(MaterialColor, CubeMapColor, ReflectFactor);
} </pre>
<ol start="4">
<li>In the render portion of the OpenGL program, draw a cube centered at the origin and apply the cube map to the cube. You can use the normalized position as the texture coordinate. Use a separate shader for this sky box. See the example code for details.</li>
<li>Switch to the preceding shaders and draw the object(s) within the scene.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In OpenGL, a cube map texture consists of six separate images. To fully initialize a cube map texture, we need to bind to the cube map texture and then load each image individually into the six "slots" within that texture. In the preceding code (within the <kbd>Texture::loadCubeMap</kbd> function), we start by binding to texture unit zero with <kbd>glActiveTexture</kbd>. Then, we create a new texture object by calling <kbd>glGenTextures</kbd>, store its handle within the variable <kbd>texID</kbd>, and then bind that texture object to the <kbd>GL_TEXTURE_CUBE_MAP</kbd> target using <kbd>glBindTexture</kbd>. We load the first image to determine the dimensions of the image, and then load the others in a loop. The following loop loads each texture file and copies the texture data into OpenGL memory using <kbd>glTexSubImage2D</kbd>. Note that the first argument to this function is the texture target, which corresponds to <kbd>GL_TEXTURE_CUBE_MAP_POSITIVE_X + i</kbd>. OpenGL defines consecutive constants that correspond to the six faces of the cube, so we can just add an integer to the value of the constant for the first face. After the loop is finished, the cube map texture should be fully initialized with the six images.</p>
<p>Following this, we set up the cube map texture environment. We use linear filtering, and we also set the texture wrap mode to <kbd>GL_CLAMP_TO_EDGE</kbd> for all three of the texture coordinate's components. This tends to work well, avoiding the possibility of a border color appearing between the cube edges.</p>
<div class="packt_tip">Even better would be to use seamless cube map textures (available since OpenGL 3.2).  It is a simple matter to enable them, just call:<kbd> glEnable(GL_TEXTURE_CUBE_MAP_SEAMLESS)</kbd> .</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Within the vertex shader, the main goal is to compute the direction of reflection and pass that to the fragment shader to be used to access the cube map. The output variable <kbd>ReflectDir</kbd> will store this result. We can compute the reflected direction (in world coordinates) by reflecting the vector toward the viewer about the normal vector.</p>
<div class="packt_infobox"><span>We choose to compute the reflection direction in world coordinates because, if we were to use eye coordinates, the reflection would not change as the camera moved within the scene.</span></div>
<p>In the <kbd>else</kbd> branch within the main function, we start by converting the position to world coordinates and storing them in <kbd>worldPos</kbd>. We then do the same for the normal, storing the result in <kbd>worldNorm</kbd>. Note that the <kbd>ModelMatrix</kbd> is used to transform the vertex normal. It is important when doing this to use a value of <kbd>0.0</kbd> for the fourth coordinate of the normal to avoid the translation component of the model matrix affecting the normal. Also, the model matrix must not contain any non-uniform scaling component; otherwise the normal vector will be transformed incorrectly.</p>
<p>The direction toward the viewer is computed in world coordinates and stored in <kbd>worldView</kbd>.</p>
<p>Finally, we reflect <kbd>worldView</kbd> about the normal and store the result in the output variable <kbd>ReflectDir</kbd>. The fragment shader will use this direction to access the cube map texture and apply the corresponding color to the fragment. One can think of this as a light ray that begins at the viewer's eye, strikes the surface, reflects off the surface, and hits the cube map. The color that the ray <em>sees</em> when it strikes the cube map is the color that we need for the object.</p>
<p>When drawing the sky box, we use the vertex position as the reflection direction. Why? Well, when the sky box is rendered, we want the location on the sky box to correspond to the equivalent location in the cube map (the sky box is really just a rendering of the cube map).  Therefore, if we want to access a position on the cube map corresponding to a location on a cube centered at the origin, we need a vector that points at that location. The vector we need is the position of that point minus the origin (which is (0,0,0)). Hence, we just need the position of the vertex.</p>
<div class="packt_infobox"><span>Sky boxes can be rendered with the viewer at the center of the sky box and the sky box moving along with the viewer (so the viewer is always at the center of the sky box). We have not done so in this example; however, we could do so by transforming the sky box using the rotational component of the view matrix (not the translational).</span></div>
<p>Within the fragment shader, we simply use the value of <kbd>ReflectDir</kbd> to access the cube map texture:</p>
<pre>vec4 cubeMapColor = texture(CubeMapTex, ReflectDir) </pre>
<p>We'll mix the sky box color with some material color. This allows us to provide some slight <em>tint</em> to the object. The amount of tint is adjusted by the variable <kbd>ReflectFactor</kbd>. A value of 1.0 would correspond to zero tint (all reflection), and a value of 0.0 corresponds to no reflection. The following images show a teapot rendered with different values of <kbd>ReflectFactor</kbd>. The teapot on the left uses a reflection factor of 0.5, and the one on the right uses a value of 0.85. The base material color is grey (the cube map used is an image of St. Peter's Basilica, Rome. ©Paul Debevec):</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/9d226a80-2d22-4b93-bbd7-71654f499ff8.png" style="width:31.67em;height:12.00em;"/></div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>There are two important points to keep in mind about this technique. First, the objects will only reflect the environment map. They will not reflect the image of any other objects within the scene. In order to do so, we would need to generate an environment map from the point of view of each object by rendering the scene six times with the view point located at the center of the object and the view direction in each of the six coordinate directions. Then, we could use the appropriate environment map for the appropriate object's reflections. Of course, if any of the objects were to move relative to one another, we'd need to regenerate the environment maps. All of this effort may be prohibitive in an interactive application.</p>
<p>The second point involves the reflections that appear on moving objects. In these shaders, we compute the reflection direction and treat it as a vector emanating from the center of the environment map. This means that regardless of where the object is located, the reflections will appear as if the object is in the center of the environment. In other words, the environment is treated as if it were <em>infinitely</em> far away. <em>Chapter 19</em> of the book <em>GPU Gems</em>, by Randima Fernando, Addison-Wesley Professional, 2009, has an excellent discussion of this issue and provides some possible solutions for localizing the reflections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter05/scenereflectcube.cpp</kbd> file in the example code</li>
<li>The <em>Applying a 2D texture</em> recipe</li>
<li>Chapter 19 of the book <em>GPU Gems</em>, by Randima Fernando, Addison-Wesley Professional, 2009</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simulating refraction with cube maps</h1>
                </header>
            
            <article>
                
<p>Objects that are transparent cause the light rays that pass through them to bend slightly at the interface between the object and the surrounding environment. This effect is called <strong>refraction</strong>. When rendering transparent objects, we simulate that effect by using an environment map and mapping the environment onto the object is such a way as to mimic the way that light would pass through the object. In other words, we can trace the rays from the viewer, through the object (bending in the process), and along to the environment. Then, we can use that ray intersection as the color for the object.</p>
<p>As in the previous recipe, we'll do this using a cube map for the environment. We'll trace rays from the viewer position, through the object, and finally intersect with the cube map.</p>
<p>The process of refraction is described by <strong>Snell's </strong><strong>law</strong>, which defines the relationship between the angle of incidence and the angle of refraction:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/303ff9cd-8343-4d4a-805d-97796da2acfe.png" style="width:14.33em;height:13.75em;"/></div>
<p>Snell's law describes the angle of incidence (<strong>a<sub>i</sub></strong>) as the angle between the incoming light ray and the normal to the surface, and the angle of refraction (<strong>a<sub>t</sub></strong>) as the angle between the transmitted ray and the extended normal. The material through which the incident light ray travels and the material containing the transmitted light ray are each described by an index of refraction (<strong>n<sub>1</sub></strong> and <strong>n<sub>2</sub></strong> in the diagram). The ratio between the two indices of refraction defines the amount that the light ray will be bent at the interface.</p>
<p>Starting with Snell's law, and with a bit of mathematical effort, we can derive a formula for the transmitted vector, given the ratio of the indices of refraction, the normal vector, and the incoming vector:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f1421506-660d-4086-af88-fb58a5ee3177.png" style="width:7.00em;height:3.08em;"/></div>
<p>However, there's no real need to do so because GLSL provides a built-in function for computing this transmitted vector called <kbd>refract</kbd>. We'll make use of that function within this example.</p>
<p>It is usually the case that for transparent objects, not all of the light is transmitted through the surface. Some of the light is reflected. In this example, we'll model that in a very simple way, and at the end of this recipe, we'll discuss a more accurate representation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Set up your OpenGL program to provide the vertex position in attribute location 0 and the vertex normal in attribute location 1. As with the previous recipe, we'll need to provide the model matrix in the uniform variable <kbd>ModelMatrix</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Load the cube map using the technique shown in the previous recipe. Place it in texture unit zero.</p>
<p>Set the uniform variable <kbd>WorldCameraPosition</kbd> to the location of your viewer in world coordinates. Set the value of the uniform variable <kbd>Material.Eta</kbd> to the ratio between the index of refraction of the environment <em>n<sub>1</sub></em> and the index of refraction of the material <em>n<sub>2</sub></em> (<em>n<sub>1</sub>/n<sub>2</sub></em>). Set the value of the uniform <kbd>Material.ReflectionFactor</kbd> to the fraction of light that is reflected at the interface (a small value is probably what you want).</p>
<p>As with the preceding example, if you want to draw the environment, draw a large cube surrounding the scene and use a separate shader to apply the texture to the cube. See the example code for details.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To render an object with reflection and refraction, carry out the following steps:</p>
<ol>
<li>Use the following code within the vertex shader:</li>
</ol>
<pre style="padding-left: 60px"> 
layout (location = 0) in vec3 VertexPosition; 
layout (location = 1) in vec3 VertexNormal; 
 
out vec3 ReflectDir;  // Reflected direction 
out vec3 RefractDir;  // Transmitted direction 
 
struct MaterialInfo { 
   float Eta;       // Ratio of indices of refraction 
   float ReflectionFactor; // Percentage of reflected light 
}; 
uniform MaterialInfo Material; 
 
uniform vec3 WorldCameraPosition; 
uniform mat4 ModelViewMatrix; 
uniform mat4 ModelMatrix; 
uniform mat3 NormalMatrix; 
uniform mat4 ProjectionMatrix; 
uniform mat4 MVP; 
 
void main() { 
   vec3 worldPos = vec3( ModelMatrix *  
                              vec4(VertexPosition,1.0) ); 
   vec3 worldNorm = vec3(ModelMatrix *  
                              vec4(VertexNormal, 0.0)); 
   vec3 worldView = normalize( WorldCameraPosition -  
                                    worldPos ); 
 
   ReflectDir = reflect(-worldView, worldNorm ); 
   RefractDir = refract(-worldView, worldNorm,  
                             Material.Eta );
    gl_Position = MVP * vec4(VertexPosition,1.0); 
} </pre>
<ol start="2">
<li>Use the following code within the fragment shader:</li>
</ol>
<pre style="padding-left: 60px">in vec3 ReflectDir; 
in vec3 RefractDir; 
 
layout(binding=0) uniform samplerCube CubeMapTex; <br/>struct MaterialInfo { 
  float Eta;  // Ratio of indices of refraction 
  float ReflectionFactor; // Percentage of reflected light 
}; 
uniform MaterialInfo Material; 
 
layout( location = 0 ) out vec4 FragColor; 
 
void main() { 
  // Access the cube map texture 
  vec4 reflectColor = texture(CubeMapTex, ReflectDir); 
  vec4 refractColor = texture(CubeMapTex, RefractDir); 
 
  FragColor = mix(refractColor, reflectColor,  
                     Material.ReflectionFactor); 
}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Both shaders are quite similar to the shaders in the previous recipe.</p>
<p>The vertex shader computes the position, normal, and view direction in world coordinates (<kbd>worldPos</kbd>, <kbd>worldNorm</kbd>, and <kbd>worldView</kbd>). They are then used to compute the reflected direction using the <kbd>reflect</kbd> function, and the result is stored in the output variable <kbd>ReflectDir</kbd>. The transmitted direction is computed using the built-in function <kbd>refract</kbd> (which requires the ratio of the indices of refraction <kbd>Material.Eta</kbd>). This function makes use of Snell's law to compute the direction of the transmitted vector, which is then stored in the output variable <kbd>RefractDir</kbd>.</p>
<p>In the fragment shader, we use the two vectors <kbd>ReflectDir</kbd> and <kbd>RefractDir</kbd> to access the cube map texture. The color retrieved by the reflected ray is stored in <kbd>reflectColor</kbd> and the color retrieved by the transmitted ray is stored in <kbd>refractColor</kbd>. We then mix those two colors together based on the value of <kbd>Material.ReflectionFactor</kbd>. The result is a mixture between the color of the reflected ray and the color of the transmitted ray.</p>
<p>The following image shows the teapot rendered with 10 percent reflection and 90 percent refraction (Cubemap © Paul Debevec):</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/3e815c65-8d5a-42b2-b537-c053df53a79e.png" style="width:18.58em;height:13.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>This technique has the same drawbacks that were discussed in the <em>There's more...</em> section of the preceding recipe, <em>Simulating reflection with cube maps</em>.</p>
<p>Like most real-time techniques, this is a simplification of the real physics of the situation. There are a number of things about the technique that could be improved to provide more realistic-looking results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Fresnel equations</h1>
                </header>
            
            <article>
                
<p>The amount of reflected light actually depends on the angle of incidence of the incoming light. For example, when looking at the surface of a lake from the shore, much of the light is reflected and it is easy to see reflections of the surrounding environment on the surface. However, when floating on a boat on the surface of the lake and looking straight down, there is less reflection and it is easier to see what lies below the surface. This effect is described by the Fresnel equations (after Augustin-Jean Fresnel).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The Fresnel equations describe the amount of light that is reflected as a function of the angle of incidence, the polarization of the light, and the ratio of the indices of refraction. If we ignore the polarization, it is easy to incorporate the Fresnel equations into the preceding shaders. A very good explanation of this can be found in Chapter 14 of the book <em>The OpenGL Shading Language</em>, <em>3rd Edition</em>, Randi J Rost, Addison-Wesley Professional, 2009.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chromatic aberration</h1>
                </header>
            
            <article>
                
<p>White light is of course composed of many different individual wavelengths (or colors). The amount that a light ray is refracted is actually wavelength dependent. This causes an effect where a spectrum of colors can be observed at the interface between materials. The most well-known example of this is the rainbow that is produced by a prism.</p>
<p>We can model this effect by using slightly different values of <kbd>Eta</kbd> for the red, green, and blue components of the light ray. We would store three different values for <kbd>Eta</kbd>, compute three different reflection directions (red, green, and blue), and use those three directions to look up colors in the cube map. We take the red component from the first color, the green component from the second, and the blue component for the third, and combine the three components together to create the final color for the fragment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Refracting through both sides of the object</h1>
                </header>
            
            <article>
                
<p>It is important to note that we have simplified things by only modeling the interaction of the light with one of the boundaries of the object. In reality, the light would be bent once when entering the transparent object, and again when leaving the other side. However, this simplification generally does not result in unrealistic-looking results. As is often the case in real-time graphics, we are more interested in a result that looks good than one that models the physics accurately.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter05/scenerefractcube.cpp</kbd> file in the example code</li>
<li>The <em>Simulating reflection with cube maps</em> recipe</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying a projected texture</h1>
                </header>
            
            <article>
                
<p>We can apply a texture to the objects in a scene as if the texture was a projection from an imaginary "slide projector" located somewhere within the scene. This technique is often called <strong>projective </strong><strong>texture </strong><strong>mapping</strong> and produces a very nice effect.</p>
<p>The following images show an example of projective texture mapping. The flower texture on the left (Stan Shebs via Wikimedia Commons) is projected onto the teapot and plane beneath:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/c4015828-2ebb-47d4-8a77-c9596a594fe3.png" style="width:28.00em;height:11.50em;"/></div>
<p>To project a texture onto a surface, all we need do is determine the texture coordinates based on the relative position of the surface location and the source of the projection (the slide projector). An easy way to do this is to think of the projector as a camera located somewhere within the scene. In the same way that we would define an OpenGL camera, we define a coordinate system centered at the projector's location, and a <strong>view matrix</strong> (<strong>V</strong>) that converts coordinates to the projector's coordinate system. Next, we'll define a perspective <strong>projection matrix</strong> (<strong>P</strong>) that converts the view frustum (in the projector's coordinate system) into a cubic volume of size two, centered at the origin. Putting these two things together, and adding an additional matrix for re-scaling and translating the volume to a volume of size one shifted so that the volume is centered at (0.5, 0.5, 0.5), we have the following transformation matrix:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dcff488d-fe98-4a2d-b25e-24b6d01fa66f.png" style="width:13.42em;height:5.08em;"/></div>
<p>The goal here is basically to convert the view frustum to a range between 0 and 1 in <em>x</em> and <em>y</em>. The preceding matrix can be used to do just that! It will convert world coordinates that lie within the view frustum of the projector to a range between 0 and 1 (homogeneous), which can then be used to access the texture. Note that the coordinates are homogeneous and need to be divided by the w coordinate before they can be used as a real position.</p>
<p class="mce-root"/>
<div class="packt_infobox"><span>For more details on the mathematics of this technique, take a look at the following white paper, written by Cass Everitt from NVIDIA: <a href="https://www.nvidia.com/object/Projective_Texture_Mapping.html">https://www.nvidia.com/object/Projective_Texture_Mapping.html</a></span></div>
<p>In this example, we'll apply a single texture to a scene using projective texture mapping.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Set up your OpenGL application to provide the vertex position in attribute location 0 and the normal in attribute location 1. The OpenGL application must also provide the material and lighting properties for the Phong reflection model (see the fragment shader given in the following section). Make sure to provide the model matrix (for converting to world coordinates) in the uniform variable <kbd>ModelMatrix</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To apply a projected texture to a scene, use the following steps:</p>
<ol>
<li>In the OpenGL application, load the texture into texture unit zero. While the texture object is bound to the <kbd>GL_TEXTURE_2D</kbd> target, use the following code to set the texture's settings:</li>
</ol>
<pre style="padding-left: 60px">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, <br/>                 GL_LINEAR); 
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER,  
                GL_LINEAR); 
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S,  
                GL_CLAMP_TO_BORDER); 
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T,   
                GL_CLAMP_TO_BORDER); </pre>
<ol start="2">
<li>Also within the OpenGL application, set up your transformation matrix for the slide projector and assign it to the uniform <kbd>ProjectorMatrix</kbd>. Use the following code to do this. Note that this code makes use of the GLM libraries discussed in <a href="3b817a9a-28a1-4be7-936c-b982b4dfacdf.xhtml"><span class="ChapterrefPACKT">Chapter </span><span class="ChapterrefPACKT">1</span></a>, <em>Getting Started with GLSL</em>:</li>
</ol>
<pre style="padding-left: 60px">vec3 projPos, projAt, projUp;<br/>// Set the above 3 to appropriate values...<br/>mat4 projView = glm::lookAt(projPos, projAt, projUp);</pre>
<pre style="padding-left: 60px">mat4 projProj = glm::perspective(glm::radians(30.0f), 1.0f, 0.2f, 1000.0f);<br/>mat4 bias = glm::translate(mat4(1.0f), vec3(0.5f));<br/>bias = glm::scale(bias, vec3(0.5f));<br/>prog.setUniform("ProjectorMatrix", bias * projProj * projView);</pre>
<ol start="3">
<li>Use the following code for the vertex shader:</li>
</ol>
<pre style="padding-left: 60px">layout (location = 0) in vec3 VertexPosition;<br/>layout (location = 1) in vec3 VertexNormal;<br/><br/>out vec3 EyeNormal;   // Normal in eye coordinates<br/>out vec4 EyePosition; // Position in eye coordinates<br/>out vec4 ProjTexCoord;<br/><br/>uniform mat4 ProjectorMatrix;<br/><br/>uniform mat4 ModelViewMatrix;<br/>uniform mat4 ModelMatrix;<br/>uniform mat3 NormalMatrix;<br/>uniform mat4 MVP;<br/><br/>void main() {<br/>  vec4 pos4 = vec4(VertexPosition,1.0);<br/><br/>  EyeNormal = normalize(NormalMatrix * VertexNormal);<br/>  EyePosition = ModelViewMatrix * pos4;<br/>  ProjTexCoord = ProjectorMatrix * (ModelMatrix * pos4);<br/>  gl_Position = MVP * pos4;<br/>}</pre>
<ol start="4">
<li>Use the following code for the fragment shader:</li>
</ol>
<pre style="padding-left: 60px">in vec3 EyeNormal;       // Normal in eye coordinates 
in vec4 EyePosition;     // Position in eye coordinates 
in vec4 ProjTexCoord; 
 
layout(binding=0) uniform sampler2D ProjectorTex; 
<br/>// Light and material uniforms...
 
layout( location = 0 ) out vec4 FragColor; 
 
vec3 blinnPhong( vec3 pos, vec3 norm ) { 
  // Blinn-Phong model...
} 
 
void main() { 
  vec3 color = blinnPhong(EyePosition.xyz, normalize(EyeNormal));<br/><br/>  vec3 projTexColor = vec3(0.0);<br/>  if( ProjTexCoord.z &gt; 0.0 )<br/>    projTexColor = textureProj( ProjectorTex, ProjTexCoord ).rgb;<br/><br/>  FragColor = vec4(color + projTexColor * 0.5, 1);
} </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>When loading the texture into the OpenGL application, we make sure to set the wrap mode for the <em>s</em> and <em>t</em> directions to <kbd>GL_CLAMP_TO_BORDER</kbd>. We do this because if the texture coordinates are outside of the range of zero to one, we do not want any contribution from the projected texture. With this mode, using the default border color, the texture will return (0,0,0,0) when the texture coordinates are outside of the range between 0 and 1 inclusive.</p>
<p>The transformation matrix for the slide projector is set up in the OpenGL application. We start by using the GLM function <kbd>glm::lookAt</kbd> to produce a view matrix for the projector. In this example, we locate the projector at (5, 5, 5), looking toward the point (-2, -4,0), with an <em>up vector</em> of (0, 1, 0). This function works in a similar way to the <kbd>gluLookAt</kbd> function. It returns a matrix for converting to the coordinate system located at (5, 5, 5), and oriented based on the second and third arguments.</p>
<p>Next, we create the projection matrix using <kbd>glm::perspective</kbd>, and the scale/translate matrix <em>M</em> (shown in the introduction to this recipe). These two matrices are stored in <kbd>projProj</kbd> and <kbd>projScaleTrans</kbd>, respectively. The final matrix is the product of <kbd>projScaleTrans</kbd>, <kbd>projProj</kbd>, and <kbd>projView</kbd>, which is stored in <kbd>m</kbd> and assigned to the uniform variable <kbd>ProjectorTex</kbd>.</p>
<p>In the vertex shader, we have three output variables: <kbd>EyeNormal</kbd>, <kbd>EyePosition</kbd>, and <kbd>ProjTexCoord</kbd>. The first two are the vertex normal and vertex position in eye coordinates. We transform the input variables appropriately, and assign the results to the output variables within the <kbd>main</kbd> function.</p>
<p>We compute <kbd>ProjTexCoord</kbd> by first transforming the position to world coordinates (by multiplying by <kbd>ModelMatrix</kbd>), and then applying the projector's transformation.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the fragment shader, within the <kbd>main</kbd> function, we start by computing the reflection model and storing the result in the variable <kbd>color</kbd>. The next step is to look up the color from the texture. First, however, we check the <em>z</em> coordinate of <kbd>ProjTexCoord</kbd>. If this is negative, then the location is behind the projector, so we avoid doing the texture lookup. Otherwise, we use <kbd>textureProj</kbd> to look up the texture value and store it in <kbd>projTexColor</kbd>.</p>
<p>The function <kbd>textureProj</kbd> is designed for accessing textures with coordinates that have been projected. It will divide the coordinates of the second argument by its last coordinate before accessing the texture. In our case, that is exactly what we want. We mentioned earlier that after transforming by the projector's matrix, we will be left with homogeneous coordinates, so we need to divide by the w coordinate before accessing the texture. The <kbd>textureProj</kbd> function will do exactly that for us.</p>
<p>Finally, we add the projected texture's color to the base color from the Phong model. We scale the projected texture color slightly so that it is not overwhelming.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>There's one big drawback to the technique presented here. There is no support for shadows yet, so the projected texture will shine right through any objects in the scene and appear on objects that are behind them (with respect to the projector). In later recipes, we will look at some examples of techniques for handling shadows that could help solve this problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter05/sceneprojtex.cpp</kbd> file in the example code</li>
<li><em>The Blinn-Phong reflection model</em> recipe in <span class="ChapterrefPACKT"><a href="74703f9d-f69a-4b08-bb38-6e1066371207.xhtml">Chapter 3</a>, <em>The Basics of GLSL Shaders</em></span></li>
<li>The <em>Applying a 2D texture</em> recipe</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rendering to a texture</h1>
                </header>
            
            <article>
                
<p>Sometimes, it makes sense to generate textures <em>on the fly</em> during the execution of the program. The texture could be a pattern that is generated from some internal algorithm (a so-called <strong>procedural texture</strong>), or it could be that the texture is meant to represent another portion of the scene.</p>
<p>An example of the latter case might be a video screen where one can see another part of the <em>world</em>, perhaps via a security camera in another room. The video screen could be constantly updated as objects move around in the other room, by re-rendering the view from the security camera to the texture that is applied to the video screen!</p>
<p>In the following image, the texture appearing on the cube was generated by rendering the cow to an internal texture and then applying that texture to the faces of the cube:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4fa3ad96-38bb-4c39-b3e4-465820aa4f00.png" style="width:12.00em;height:11.92em;"/></div>
<p>In OpenGL, rendering directly to textures has been greatly simplified by the introduction of <strong>framebuffer objects</strong> (<strong>FBOs</strong>). We can create a separate rendering target buffer (the FBO), attach our texture to that FBO, and render to the FBO in exactly the same way that we would render to the default framebuffer. All that is required is to swap in the FBO, and swap it out when we are done.</p>
<p>Basically, the process involves the following steps when rendering:</p>
<ol>
<li>Bind to the FBO</li>
<li>Render the texture</li>
<li>Unbind from the FBO (back to the default framebuffer)</li>
<li>Render the scene using the texture</li>
</ol>
<p>There's actually not much that we need to do on the GLSL side in order to use this kind of texture. In fact, the shaders will see it as any other texture. However, there are some important points that we'll talk about regarding fragment output variables.</p>
<p>In this example, we'll cover the steps needed to create the FBO and its backing texture, and how to set up a shader to work with the texture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>For this example, we'll use the shaders from the previous recipe, <em>Applying a 2D texture</em>, with some minor changes. Set up your OpenGL program as described in that recipe. The only change that we'll make to the shaders is changing the name of the <kbd>sampler2D</kbd> variable from <kbd>Tex1</kbd> to <kbd>Texture</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To render to a texture and then apply that texture to a scene in a second pass, use the following steps:</p>
<ol>
<li>Within the main OpenGL program, use the following code to set up the framebuffer object:</li>
</ol>
<pre style="padding-left: 60px">GLuint fboHandle;  // The handle to the FBO 
 
// Generate and bind the framebuffer 
glGenFramebuffers(1, &amp;fboHandle); 
glBindFramebuffer(GL_FRAMEBUFFER, fboHandle); 
 
// Create the texture object 
GLuint renderTex; 
glGenTextures(1, &amp;renderTex); 
glActiveTexture(GL_TEXTURE0);  // Use texture unit 0 
glBindTexture(GL_TEXTURE_2D, renderTex); 
glTexStorage2D(GL_TEXTURE_2D, 1, GL_RGBA8, 512, 512); 
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER,  
                GL_LINEAR); 
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER,  
                GL_LINEAR); 
 
// Bind the texture to the FBO 
glFramebufferTexture2D(GL_FRAMEBUFFER,GL_COLOR_ATTACHMENT0,  
                       GL_TEXTURE_2D, renderTex, 0); 
 
// Create the depth buffer 
GLuint depthBuf; 
glGenRenderbuffers(1, &amp;depthBuf); 
glBindRenderbuffer(GL_RENDERBUFFER, depthBuf); 
glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH_COMPONENT,  
                      512, 512); 
 
// Bind the depth buffer to the FBO 
glFramebufferRenderbuffer(GL_FRAMEBUFFER,  
                          GL_DEPTH_ATTACHMENT, 
                          GL_RENDERBUFFER, depthBuf); 
 
// Set the target for the fragment shader outputs 
GLenum drawBufs[] = {GL_COLOR_ATTACHMENT0}; 
glDrawBuffers(1, drawBufs); 
 
// Unbind the framebuffer, and revert to default 
glBindFramebuffer(GL_FRAMEBUFFER, 0); </pre>
<ol start="2">
<li>In your render function within the OpenGL program, bind to the framebuffer, draw the scene that is to be rendered to the texture, then unbind from that framebuffer and draw the cube:</li>
</ol>
<pre style="padding-left: 60px">// Bind to texture's FBO 
glBindFramebuffer(GL_FRAMEBUFFER, fboHandle); 
glViewport(0,0,512,512);  // Viewport for the texture<br/> 
// Use the texture for the cow here 
int loc = glGetUniformLocation(programHandle, "Texture"); 
glUniform1i(loc, 1);
 
// Setup the projection matrix and view matrix 
// for the scene to be rendered to the texture here. 
// (Don't forget to match aspect ratio of the viewport.) 
 
renderTextureScene(); 
 
// Unbind texture's FBO (back to default FB) 
glBindFramebuffer(GL_FRAMEBUFFER, 0); 
glViewport(0,0,width,height);  // Viewport for main window 
 
// Use the texture that is linked to the FBO 
int loc = glGetUniformLocation(programHandle, "Texture"); 
glUniform1i(loc, 0); 
 
// Reset projection and view matrices here...<br/>renderScene();</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Let's start by looking at the code for creating the framebuffer object (step 1). Our FBO will be 512 pixels square because we intend to use it as a texture. We begin by generating the FBO using <kbd>glGenFramebuffers</kbd> and binding the framebuffer to the <kbd>GL_FRAMEBUFFER</kbd> target with <kbd>glBindFramebuffer</kbd>. Next, we create the texture object to which we will be rendering, and use <kbd>glActiveTexture</kbd> to select texture unit zero. The rest is very similar to creating any other texture. We allocate space for the texture using <kbd>glTexStorage2D</kbd>. We don't need to copy any data into that space (using <kbd>glTexSubImage2D</kbd>) because we'll be writing to that memory later when rendering to the FBO.</p>
<p>Next, we link the texture to the FBO by calling the function <kbd>glFramebufferTexture2D</kbd>. This function attaches a texture object to an attachment point in the currently bound framebuffer object. The first argument (<kbd>GL_FRAMEBUFFER</kbd>) indicates that the texture is to be attached to the FBO currently bound to the <kbd>GL_FRAMEBUFFER</kbd> target. The second argument is the attachment point. Framebuffer objects have several attachment points for color buffers, one for the depth buffer, and a few others. This allows us to have several color buffers to target from our fragment shaders. We'll see more about this later. We use <kbd>GL_COLOR_ATTACHMENT0</kbd> to indicate that this texture is linked to color attachment 0 of the FBO. The third argument (<kbd>GL_TEXTURE_2D</kbd>) is the texture target, and the fourth (<kbd>renderTex</kbd>) is the handle to our texture. The last argument (<kbd>0</kbd>) is the mip-map level of the texture that is being attached to the FBO. In this case, we only have a single level, so we use a value of zero.</p>
<p>As we want to render to the FBO with depth testing, we need to also attach a depth buffer. The next few lines of code create the depth buffer. The <kbd>glGenRenderbuffer</kbd> <span>function </span>creates a <kbd>renderbuffer</kbd> object, and <kbd>glRenderbufferStorage</kbd> allocates space for the <kbd>renderbuffer</kbd> object. The second argument to <kbd>glRenderbufferStorage</kbd> indicates the internal format for the buffer, and as we are using this as a depth buffer, we use the special format <kbd>GL_DEPTH_COMPONENT</kbd>.</p>
<p>Next, the depth buffer is attached to the <kbd>GL_DEPTH_ATTACHMENT</kbd> attachment point of the FBO using <kbd>glFramebufferRenderbuffer</kbd>.</p>
<p>The shader's output variables are assigned to the attachments of the FBO using <kbd>glDrawBuffers</kbd>. The second argument to <kbd>glDrawBuffers</kbd> is an array indicating the FBO buffers to be associated with the output variables. The <em>i</em><sup>th</sup> element of the array corresponds to the fragment shader output variable at location <kbd>i</kbd>. In our case, we only have one shader output variable (<kbd>FragColor</kbd>) at location zero. This statement associates that output variable with <kbd>GL_COLOR_ATTACHMENT0</kbd>.</p>
<p>The last statement in step 1 unbinds the FBO to revert back to the default framebuffer.</p>
<p>In the last step (within the render function), we bind to the FBO, use the texture in unit one, and render the texture. Note that we need to be careful to set up the viewport (<kbd>glViewport</kbd>) and the view and projection matrices appropriately for our FBO. As our FBO is 512 x 512, we use <kbd>glViewport(0,0,512,512)</kbd>. Similar changes should be made to the view and projection matrices to match the aspect ratio of the viewport and set up the scene to be rendered to the FBO.</p>
<p>Once we've rendered to the texture, we unbind from the FBO, reset the viewport, and the view and projection matrices, use the FBO's texture (texture unit 0), and draw the cube!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>As FBOs have multiple color attachment points, we can have several output targets from our fragment shaders. Note that so far, all of our fragment shaders have only had a single output variable assigned to location zero. Hence, we set up our FBO so that its texture corresponds to color attachment zero. In later chapters, we'll look at examples where we use more than one of these attachments for things like deferred shading.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter05/scenerendertotex.cpp</kbd> file in the example code</li>
<li>The <em>Applying a 2D texture</em> recipe</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using sampler objects</h1>
                </header>
            
            <article>
                
<p>Sampler objects were introduced in OpenGL 3.3 and provide a convenient way to specify the sampling parameters for a GLSL sampler variable. The traditional way to specify the parameters for a texture is to specify them using <kbd>glTexParameter</kbd>, typically at the time that the texture is defined. The parameters define the sampling state (sampling mode, wrapping and clamping rules, and so on) for the associated texture. This essentially combines the texture and its sampling state into a single object. If we wanted to sample from a single texture in more than one way (with and without linear filtering for example), we'd have two choices. We would either need to modify the texture's sampling state, or use two copies of the same texture.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In addition, we might want to use the same set of texture sampling parameters for multiple textures. With what we've seen up until now, there's no easy way to do that. With sampler objects, we can specify the parameters once and share them among several texture objects.</p>
<p>Sampler objects separate the sampling state from the texture object. We can create sampler objects that define a particular sampling state and apply that to multiple textures or bind different sampler objects to the same texture. A single sampler object can be bound to multiple textures, which allows us to define a particular sampling state once and share it among several texture objects.</p>
<p>Sampler objects are defined on the OpenGL side (not in GLSL), which makes it effectively transparent to the GLSL.</p>
<p>In this recipe, we'll define two sampler objects and apply them to a single texture. The following image shows the result. The same texture is applied to the two planes. On the left, we use a sampler object set up for nearest-neighbor filtering, and on the right we use the same texture with a sampler object set up for linear filtering:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/c598a1c5-fc5c-4d19-a043-291744270ee5.png" style="width:25.42em;height:12.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will start with the same shaders used in the recipe <em>Applying a 2D texture</em>. The shader code will not change at all, but we'll use sampler objects to change the state of the sampler variable <kbd>Tex1</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To set up the texture object and the sampler objects, perform the following steps:</p>
<ol>
<li>Create and fill the texture object in the usual way, but this time, we won't set any sampling state using <kbd>glTexParameter</kbd>:</li>
</ol>
<pre style="padding-left: 60px">GLuint texID; 
glGenTextures(1, &amp;texID); 
glBindTexture(GL_TEXTURE_2D, texID); 
glTexStorage2D(GL_TEXTURE_2D, 1, GL_RGBA8, w, h); 
glTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, w, h, GL_RGBA, <br/>   GL_UNSIGNED_BYTE, data);</pre>
<ol start="2">
<li>Bind the texture to texture unit 0, which is the unit that is used by the shader:</li>
</ol>
<pre style="padding-left: 60px">glActiveTexture(GL_TEXTURE0); 
glBindTexture(GL_TEXTURE_2D, texID); </pre>
<ol start="3">
<li>Next, we create two sampler objects and assign their IDs to separate variables<br/>
for clarity:</li>
</ol>
<pre style="padding-left: 60px">GLuint samplers[2]; 
glGenSamplers(2, samplers); 
linearSampler = samplers[0]; 
nearestSampler = samplers[1]; </pre>
<ol start="4">
<li>Set up <kbd>linearSampler</kbd> for linear interpolation:</li>
</ol>
<pre style="padding-left: 60px">glSamplerParameteri(linearSampler, GL_TEXTURE_MAG_FILTER,  
                    GL_LINEAR); 
glSamplerParameteri(linearSampler, GL_TEXTURE_MIN_FILTER,  
                    GL_LINEAR); </pre>
<ol start="5">
<li>Set up <kbd>nearestSampler</kbd> for nearest-neighbor sampling:</li>
</ol>
<pre style="padding-left: 60px">glSamplerParameteri(nearestSampler, GL_TEXTURE_MAG_FILTER,  
                    GL_NEAREST); 
glSamplerParameteri(nearestSampler, GL_TEXTURE_MIN_FILTER,  
                    GL_NEAREST); </pre>
<ol start="6">
<li>When rendering, we bind to each sampler object when needed:</li>
</ol>
<pre style="padding-left: 60px">glBindSampler(0, nearestSampler); 
// Render objects that use nearest-neighbor sampling 
glBindSampler(0, linearSampler); 
// Render objects that use linear sampling</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Sampler objects are simple to use and make it easy to switch between different sampling parameters for the same texture or use the same sampling parameters for different textures. In steps 1 and 2, we create a texture and bind it to texture unit 0. Normally, we would set the sampling parameters here using <kbd>glTexParameteri</kbd>, but in this case, we'll set them in the sampler objects using <kbd>glSamplerParameter</kbd>. In step 3, we create the sampler objects and assign their IDs to some variables. In steps 4 and 5, we set up the appropriate sampling parameters using <kbd>glSamplerParameter</kbd>. This function is almost exactly the same as <kbd>glTexParameter</kbd> except the first argument is the ID of the sampler object instead of the texture target. This defines the sampling state for each of the two sampler objects (linear for <kbd>linearSampler</kbd> and nearest for <kbd>nearestSampler</kbd>).</p>
<p>Finally, we use the sampler objects by binding them to the appropriate texture unit using <kbd>glBindSampler</kbd> just prior to rendering. In step 6 we bind <kbd>nearestSampler</kbd> to texture unit 0 first, render some objects, bind <kbd>linearSampler</kbd> to texture unit 0, and render some more objects. The result here is that the same texture uses different sampling parameters by binding different sampler objects to the texture unit during rendering.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter05/scenesamplerobj.cpp</kbd> file in the example code</li>
<li>The <em>Applying a 2D texture</em> recipe</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Diffuse image-based lighting</h1>
                </header>
            
            <article>
                
<p><strong> Image-based lighting</strong> is a technique that involves using an image as a light source.  The image represents an omni-directional view of the environment of the scene. With image-based lighting, the image itself is treated as a highly detailed light source that surrounds the scene completely. Objects in the scene are illuminated by the content of the image, making it possible to have a very complex lighting environment and/or to simulate a real world setting. Often, these images are produced using a special camera or special photographic techniques, and are recorded in high dynamic range. An example of one such image is shown here (image courtesy of USC Institute for Creative Technologies and Paul Debevec): </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/87eaa865-5317-4ceb-aabf-734add52613f.png" style="width:27.75em;height:13.83em;"/></div>
<p>These images may be provided as a cube map (set of six images), or some other type of environment map such as a <strong>equirectangular</strong> panoramic map (the type shown previously). Conversion between the two is straightforward.</p>
<div class="packt_infobox">Since each texel in the equirectangular map represents a direction, the same is true for a cube map. To convert from one to the other, we just need to convert directions in one map to directions in the other.</div>
<p>In this recipe, we'll go through the process of using an image as a light source for diffuse reflection. Most of the effort here is involved in the creation of the <strong>diffuse convolution</strong> map. The diffuse convolution is a transformation of the environment map into a form that can be used directly when computing the diffuse reflection. In the following images, the original environment map is shown on the left, and the right side is the diffuse convolution <span>(image courtesy of USC Institute for Creative Technologies and Paul Debevec):</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4e67920f-d80d-4a5d-b8ea-40159bb107a6.png" style="width:41.42em;height:10.25em;"/></div>
<p>To understand the diffuse convolution map, lets review the reflectance equation (presented in <em>A Physically-based reflection model</em> in <a href="74703f9d-f69a-4b08-bb38-6e1066371207.xhtml">Chapter 3</a>, <em>The Basics of GLSL Shaders</em>):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1fc383d0-4ec6-4eef-aaa1-1aee84b6cbde.png" style="width:16.25em;height:2.83em;"/></div>
<p>This equation represents the integral over all directions of incoming light (<em>l</em>) on the hemisphere above the surface. The <em>f</em> term in the preceding equation is the <strong>Bidirectional Reflectance Distribution Functio</strong>n (<strong><span>BRDF</span></strong>). It represents the fraction of light that is reflected from a surface point given the incoming (<em>l</em>) and outgoing (<em>v</em>) directions. If we only consider diffuse reflection (Lambertian), we can use the following constant term for the BRDF:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f04266cb-5ecd-4019-a504-e3ded97109eb.png" style="width:8.00em;height:2.67em;"/></div>
<p>Which gives us the following for the reflectance equation:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f13a30d4-420a-4f27-9dec-711844532685.png" style="width:17.00em;height:3.08em;"/></div>
<p>Since the BRDF is just a constant, it can be factored outside of the integral. Note that there is nothing in this equation that depends upon the outgoing direction (<em>v</em>). That leads us to the following insight. The environment map that we discussed previously represents the amount of incoming radiance for a given direction, which is the <em>L<sub>i</sub>(l)</em> term in the preceding equation. We could estimate the value of this integral for a given value of <em>n</em> using the <strong>Monte Carlo estimator</strong>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c6ea15d8-ff10-4fd0-b733-52bc8567b47f.png" style="width:15.50em;height:3.92em;"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">In the preceding equation, <em>l<sub>j</sub></em> represents a pseudo-random direction sampled <em>uniformly</em> from the hemisphere above the surface (around <em>n</em>), and <em>N</em> is the number of samples. The constant factor of <em>2π</em> comes from the probability density function for uniform samples as a function of solid angle.  </p>
<p>Sampling the directions uniformly over the hemisphere is not quite as straightforward as you might think. Common practice is to sample directions in a system where the <em>z</em> axis is aligned with the vector <em>n</em>, and then transform the sample into world coordinates. However, we have to be careful to pick directions uniformly. For example, suppose we just pick random values between -1 and 1 for <em>x</em> and <em>y</em>, and 0 and 1 for <em>z</em> and then normalize. That would give us directions that are biased or "clumped" around the <em>z</em> axis and are not uniform over the hemisphere. To get uniform directions, we can use the following formulae:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0f695731-89b1-447a-a6e5-5852ccd0cd67.png" style="width:12.67em;height:7.25em;"/></div>
<p>The values <em>ξ<sub>1</sub></em> and <em>ξ<sub>2</sub></em> are uniform pseudo-random values in the range [0, 1]. For a derivation of this, see <em>Physically Based Rendering</em>, third edition, Chapter 13, <em>Monte Carlo Integration</em>.</p>
<p>Now that we have a way to estimate the integral for a given value of <em>n</em>, we can convolve the original environment map in the following way. We'll create a new environment map (the diffuse convolution map), where each texel represents a direction of <em>n</em> in world coordinates.  The value of the texel will be the estimated value of the preceding integral (except for the <em>c<sub>diff</sub></em> term) by taking a number of random samples (<em>l<sub>j</sub></em>) from the original environment map. We can do this <em>offline</em> and pre-compute this diffuse convolution. It is a somewhat slow process, but we don't need a lot of detail. The diffuse convolution is usually fairly smoothly varying, so we can use a small resolution without sacrificing much quality.</p>
<div class="packt_infobox">I've admittedly glossed over some of the math here. For a very good introduction to Monte Carlo integration for graphics, please see <em>Physically Based Rendering</em>, third Edition by Pharr, Jakob, and Humphreys, Chapter 13, <em>Monte Carlo Integration</em>.</div>
<p>Once we have the pre-computed diffuse convolution, we can use that as a <em>lookup table</em> to give us the value of our diffuse integral (again, without <em>c<sub>diff</sub></em>) using the normal vector. We can multiply the value retrieved by our material's diffuse color <em>c</em><sub><em>dif</em>f</sub> to get the outgoing radiance. In other words, the diffuse convolution represents the <em>outgoing</em> radiance for a given value of <em>n</em>, rather than the incoming radiance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Most of the preparation here is involved in convolving the environment map. The following pseudocode outlines this process:</p>
<pre>nSamples = 1000<br/><strong>foreach</strong> texel t <strong>in</strong> output map<br/>  n = direction towards t<br/>  rad = 0<br/>  <strong>for</strong> i = 1 <strong>to</strong> nSamples<br/>     li = uniform random direction in the <br/>          hemisphere around n in world coords<br/>     L = read from environment map at li<br/>     nDotL = dot( n, li )<br/>     rad += L * nDotL<br/>  set texel t to (2 / nSamples) * rad</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To render a scene with diffuse image-based lighting, the process is fairly simple. We simply need to read from our diffuse map using the normal vector.</p>
<p>The vertex shader just converts the position and normal to world coordinates and passes them along:</p>
<pre>// Input attributes...<br/><br/>out vec3 Position; // world coords<br/>out vec3 Normal;   // world coords.<br/>out vec2 TexCoord;<br/><br/>// Matrix uniforms ...<br/><br/>void main() {<br/>    TexCoord = VertexTexCoord;<br/>    Position = (ModelMatrix * vec4(VertexPosition,1)).xyz;<br/>    Normal = normalize( ModelMatrix * vec4(VertexNormal,0) ).xyz;<br/>    gl_Position = MVP * vec4(VertexPosition,1.0);<br/>}</pre>
<p>The fragment shader then uses the diffuse convolution map to determine the value of the integral, multiplies it by a color taken from a texture map, and applies gamma correction:</p>
<pre>const float PI = 3.14159265358979323846;<br/><br/>in vec3 Position;<br/>in vec3 Normal;<br/>in vec2 TexCoord;<br/><br/>layout(binding=0) uniform samplerCube DiffConvTex;<br/>layout(binding=1) uniform sampler2D ColorTex;<br/><br/>layout( location = 0 ) out vec4 FragColor;<br/><br/>const float gamma = 2.2;<br/><br/>void main() {<br/>    vec3 n = normalize(Normal);<br/><br/>    // Look up reflected light from diffuse cube map<br/>    vec3 light = texture(DiffConvTex, n).rgb;<br/>    vec3 color = texture(ColorTex, TexCoord).rgb;<br/>    color = pow(color, vec3(gamma));   // decode<br/>    color *= light;<br/><br/>    color = pow( color, vec3(1.0/gamma));  // gamma encode<br/><br/>    FragColor = vec4( color, 1 );<br/>}</pre>
<p>Results for the environment shown in the introduction to this recipe are shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/964e0e63-4936-4320-924c-3be121850676.png" style="width:27.67em;height:17.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Once we've created the diffuse convolution, there's not much to this technique. We just simply look up the value in the convolved map <kbd>DiffConvTex</kbd> and multiply it with the base color of the surface. In this example, the base color for the surface is taken from a second texture map (<kbd>ColorTex</kbd>). We apply gamma decoding to the base color texture to move it into linear color space before multiplying with the environment map. This assumes that the texture is stored in sRGB or has already been gamma encoded. The final value is then gamma encoded before display. The values in the environment map are in linear color space, so we need to move everything to linear space before combining. For some details about gamma encoding/decoding, please see the <em>Using gamma correction</em><span> <em>to improve image quality </em>recipe </span>in <a href="a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml">Chapter 6</a><span>, </span><em>Image Processing and Screen Space Techniques</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>It would also be nice to include a specular/glossy component to this model. We didn't cover that here. In addition, it would be nice to include proper Fresnel reflection. Including the specular component is a bit harder because it also depends on the direction of the viewer.  There are techniques for creating specular convolutions and I'll refer you to the following sources. These usually involve several simplifying assumptions to make things achievable in real time.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter05/scenediffibl.cpp</kbd> file in the example code</li>
<li><em>Physically Based Rendering</em>, Third Edition, by Pharr, Jakob, and Humphreys</li>
<li>The <em>Using gamma correction to improve image quality</em> recipe in <a href="a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml">Chapter 6</a>, <em>Image Processing and Screen Space Techniques</em></li>
<li>For lots of details about the specular contribution with image-based lighting, see <a href="http://blog.selfshadow.com/publications/s2013-shading-course/">http://blog.selfshadow.com/publications/s2013-shading-course/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>