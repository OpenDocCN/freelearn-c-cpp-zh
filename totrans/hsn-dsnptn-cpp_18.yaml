- en: '18'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Patterns for Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last chapter is dedicated to a set of patterns for use in concurrent programs.
    Concurrency and C++ have a somewhat complex relationship. On the one hand, C++
    is a performance-oriented language, and concurrency is almost always employed
    to improve performance, so the two are a natural fit. Certainly, C++ was used
    to develop concurrent programs since the earliest days of the language. On the
    other hand, for a language so often used for writing concurrent programs, C++
    has a surprising dearth of constructs and features that directly address the needs
    of concurrent programming. These needs are mostly addressed by a wide range of
    community-developed libraries and, often, application-specific solutions. In this
    chapter, we will review common problems encountered in the development of concurrent
    programs and solutions that emerged from years of experience; together, these
    are the two sides of a design pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the state of concurrency support in C++?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the main challenges of concurrency?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The challenges of data synchronization and the C++ tools to meet them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the design for concurrency?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are common patterns for managing concurrent workloads in C++?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The example code for this chapter can be found on GitHub at the following link:
    [https://github.com/PacktPublishing/Hands-On-Design-Patterns-with-CPP-Second-Edition/tree/master/Chapter18](https://github.com/PacktPublishing/Hands-On-Design-Patterns-with-CPP-Second-Edition/tree/master/Chapter18).
    Also, basic knowledge of concurrency in general as well as concurrency support
    in C++ are a pre-requisite.'
  prefs: []
  type: TYPE_NORMAL
- en: C++ and concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of concurrency was introduced into the language in C++11, but concurrent
    programs were written in C++ long before that. This chapter is not meant to be
    an introduction to concurrency or even an introduction to concurrency in C++.
    This subject is well-covered in the literature (at the time of publication of
    this book, one of the works that are both general and up-to-date is the book *C++
    Concurrency in Action* by Anthony Williams). Also, while concurrency is almost
    always used to improve performance, we will not directly address performance and
    optimization issues here; for that, you can refer to my book *The Art of Writing
    Efficient Programs*. We are going to focus on the problems that arise in the design
    of concurrent software.
  prefs: []
  type: TYPE_NORMAL
- en: There are, broadly speaking, three types of challenges we encounter when developing
    concurrent programs. First, how to make sure the program is correct even when
    multiple threads operate on the same data concurrency? Second, how to execute
    the work of a program on multiple threads to improve the overall performance?
    Finally, how to design software in a way that allows us to reason about it, understand
    its functions, and maintain it, all with the added complexity of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first set of challenges broadly relates to data sharing and synchronization.
    We will examine the related patterns first: the program must be first and foremost
    correct, and great performance in a program that crashes or produces results that
    cannot be trusted is useless.'
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Synchronization patterns have one overarching purpose: to ensure correct operations
    on data shared by multiple threads. These patterns are critically important for
    the absolute majority of concurrent programs. The only programs that do not have
    any need for synchronization are the ones that execute several entirely independent
    tasks that do not involve any common data (except for, possibly, reading shared
    and immutable inputs) and produce separate results. For every other program, there
    is a need to manage some shared state, which exposes us to the danger of the dreaded
    data races. Formally, the C++ standard says that concurrent access to the same
    object (same memory location) without the appropriate synchronization that guarantees
    exclusive access for each thread results in undefined behavior. To be precise,
    the behavior is undefined if at least one thread can modify the shared data: if
    the data is never changed by any thread, then there is no possibility of a data
    race. There are design patterns that take advantage of that loophole, but let
    us start with the most widely known synchronization pattern. What comes to mind
    first when you hear about avoiding data races?'
  prefs: []
  type: TYPE_NORMAL
- en: Mutex and locking patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If there is one tool for writing concurrent programs, it is a mutex. A mutex
    is used to guarantee exclusive access to the shared data accessed by multiple
    threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The data-modifying operation `transmogrify()` must be guaranteed exclusive
    access to the shared data: only one thread may do this operation at any given
    time. The programmer uses a `lock()` and `unlock()`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of a mutex is sufficient to ensure correct access to the shared data,
    but this is hardly a good design. The first issue is that it is error-prone: if
    `transmogrify()` throws an exception, or if the programmer adds a check for the
    return value and exits the critical section early, the final `unlock()` is never
    executed and the mutex remains locked forever, thus blocking every other thread
    from ever accessing the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This challenge is easily addressed by a particular application of the very
    general C++ pattern we have already seen in [*Chapter 5*](B19262_05.xhtml#_idTextAnchor199),
    *A Comprehensive Look at RAII*. All we need is an object to lock and unlock the
    mutex, and the C++ standard library already provides one, `std::lock_guard`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The function `add()` modifies the shared variable `i` and, therefore, needs
    exclusive access; this is provided by the use of the mutex `m`. Note that if you
    run this example without the mutex, chances are you will get the correct result
    nonetheless because one of the threads will execute before the other. Sometimes
    the program will fail, and more often it won’t. This doesn’t make it correct,
    it just makes it hard to debug. You can see the race condition with the help of
    the `--sanitize=address` to enable it. Remove the mutex from `add()` (*Example
    02*), compile with TSAN, run the program, and you will see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: There is a lot more information shown to help you figure out which threads have
    a data race and for which variable. This is a far more reliable way to test for
    data races than waiting for your program to fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In C++17, the use of `std::lock_guard` is slightly simpler since the compiler
    figures out the template argument from the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In C++20, we can use `std::jthread` instead of calling `join()` explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that care must be taken to destroy the threads before using the result
    of the computation since the destructor now joins the thread and waits for the
    calculation to complete. Otherwise, there is another data race: the main thread
    is reading the value of `i` while it is being incremented (TSAN finds this race
    as well).'
  prefs: []
  type: TYPE_NORMAL
- en: The use of RAII ensures that every time a mutex is locked it is also unlocked,
    but this does not avoid other errors that can happen when using mutexes. The most
    common one is forgetting to use the mutex in the first place. The synchronization
    guarantees apply only if every thread uses the same mechanism to ensure exclusive
    access to the data. If even one thread does not use the mutex, even if it’s only
    to read the data, then the entire program is incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pattern was developed to prevent unsynchronized access to the shared. This
    pattern usually goes by the name “mutex-guarded” or “mutex-protected” and it has
    two key elements: first, the data that needs to be protected and the mutex that
    is used to do so are combined in the same object. Second, the design ensures that
    every access to the data is protected by the mutex. Here is the basic mutex-guarded
    class template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, this template combines the mutex and the data guarded by it,
    and offers only one way to access the data: by invoking the `MutexGuarded` object
    with an arbitrary callable. This ensures that all data accesses are synchronized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the most basic versions of the patterns for the correct and reliable
    use of mutexes. In practice, the needs are often more complex, and so are the
    solutions: there are much more efficient locks than `std::mutex` (for example,
    spinlocks for guarding short computations, which you can find in my book *The
    Art of Writing Efficient Programs*), and there are also more complex locks such
    as shared and exclusive locks for efficient read-write access. Also, often we
    have to operate on several shared objects at the same time, which leads to the
    problem of safely locking multiple mutexes. Many of these problems are solved
    by more complex variations of the patterns we have just seen. Some call for an
    entirely different approach to the synchronization of data accesses, and we will
    see several of those later in this section. Finally, some data access challenges
    are better solved at a much higher level of the overall system design; this will
    also be illustrated in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Let us next review different approaches to data sharing that go beyond the commonly
    used mutex.
  prefs: []
  type: TYPE_NORMAL
- en: No sharing is the best sharing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While protecting shared data with a mutex does not seem that complicated, in
    reality, data races are the most common bugs in any concurrent program. While
    it may seem a useless truism to state that you cannot have data races accessing
    data you do not share, not sharing is a frequently overlooked alternative to sharing.
    To put it another way, it is often possible to redesign a program to avoid sharing
    some of the variables or to restrict access to shared data to a smaller part of
    the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'This idea is the basis of a design pattern that is simple to explain but often
    hard to apply because it requires thinking outside of the box – the thread-specific
    data pattern. It is also known as “thread-local data,” but the name invites confusion
    with the C++ `thread_local` keyword. To illustrate the idea, we consider this
    example: we need to count certain events that may be happening in multiple threads
    simultaneously (for this demonstration, it does not matter what is counted). We
    need the total count of these events in the entire program, so the straightforward
    approach is to have a shared count and increment it when a thread detects an event
    (in the demonstration, we count random numbers divisible by 10):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a straightforward design; it is not the best one. Notice that while
    each thread is counting events, it does not need to know how many events were
    counted by other threads. This is not to be confused with the fact that, in our
    implementation, each thread needs to know what the current value of the count
    is so it can correctly increment it. The distinction is subtle but important and
    suggests an alternative: each thread can count its own events using a thread-specific
    count, one for each thread. None of these counts are correct, but it doesn’t matter
    as long as we can add all counts together when we need the correct total event
    count. There are several possible designs here. We can use a local count for the
    events and update the shared count once before the thread exits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Any local (stack-allocated) variable declared in a function that is being executed
    by one or more threads is specific to each thread: there is a unique copy of this
    variable on the stack of each thread, and each thread accesses its own variable
    when they all refer to the same name `n`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We could also give each thread a unique count variable to increment and add
    them together in the main thread after all the counting threads are finished:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When calling this counting function on multiple threads, we have to take some
    precautions. Obviously, we should give each thread its own variable for the count
    `n`. This is not enough: due to the hardware-related effect known as “false sharing,”
    we must also ensure that the thread-specific counts are not adjacent in memory
    (a detailed description of false sharing can be found in my book *The Art of Writing*
    *Efficient Programs*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `alignas` attribute ensures 64-byte alignment for each count variable, thus
    ensuring that there is at least 64 bytes difference between the addresses of `n1`
    and `n2` (64 is the size of the cache line on most modern CPUs, including X86
    and ARM). Note the `std::ref` wrapper that is needed for `std::thread` to invoke
    functions that use reference arguments.
  prefs: []
  type: TYPE_NORMAL
- en: The previous example reduces the need for shared data access to once per thread,
    while the last one does not have any shared data at all; the preferred solution
    depends on exactly when is the value of the total count needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last example can be examined from a slightly different point of view; it
    would help to slightly rewrite it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This does not change anything of substance, but we can view the thread-specific
    counts as parts of the same data structure rather than independent variables created
    for each thread. This way of thinking leads us to another variant of the thread-specific
    data pattern: sometimes, multiple threads must operate on the same data, but it
    may be possible to partition the data and give each thread its own subset to work
    on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next example, we need to clamp each element in the vector (if an element
    exceeds the maximum value, it is replaced by this value, so the result is always
    in the range between zero and the maximum). The computation is implemented by
    this templated algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: A production-quality implementation would ensure that the iterator arguments
    satisfy the iterator requirements and the maximum value is comparable with the
    iterator value type, but we omit all that for brevity (we had an entire chapter
    on concepts and other ways to restrict templates).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `clamp()` function can be called on any sequence, and sometimes we will
    be lucky to have separate unrelated data structures we can process independently
    on multiple threads. But to continue this example, let us say that we have only
    one vector we need to clamp. All is not lost, however, as we can process non-overlapping
    parts of it on multiple threads with no risk of data races:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though the data structure in our program is shared between two threads
    and both threads modify it, this program is correct: for each vector element,
    there is only one thread that can modify it. But what about the vector object
    itself? Isn’t it shared between all threads?'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already highlighted that there is one case when data sharing is allowed
    without any synchronization: any number of threads can read the same variable
    as long as no other thread is modifying it. Our example takes advantage of this:
    all threads read the size of the vector and other data members of the vector object,
    but no threads change them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The application of the thread-specific data pattern must be carefully thought
    through and often requires a good understanding of the data structures. We must
    be absolutely certain that none of the threads attempt to modify the variables
    they do share, such as the size and the pointer to the data that are members of
    the vector object itself. For example, if one of the threads could resize the
    vector, that would be a data race even if no two threads access the same element:
    the size of the vector is a variable that is modified by one or more threads without
    a lock.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last pattern we want to describe in this subsection applies when several
    threads need to modify the entire data set (so it cannot be partitioned) but the
    threads do not need to see the modifications done by other threads. Usually, this
    happens when the modifications are done as a part of the computation of some result,
    but the modified data itself is not the final result. In this case, sometimes
    the best approach is to create a thread-specific copy of the data for each thread.
    This pattern works best when such copy is a “throw-away” object: each thread needs
    to modify its copy but the result of the modifications does not need to be committed
    back to the original data structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we use an algorithm to count unique elements in a
    vector that sorts the vector in place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, when we need to count only elements that satisfy a predicate, we erase
    all other elements first (`std::erase_if` is a C++20 addition, but is easy to
    implement in a prior version of C++):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Both are destructive operations on a vector, but they are only means to an
    end: the altered vector can be discarded once we have our count. The simplest,
    and often the most efficient, way to compute our counts on several threads simultaneously
    is to make thread-specific copies of the vector. Actually, we did this already:
    both counting functions take the vector argument by value and, therefore, make
    a copy. Usually, this would be a mistake, but in our case, it is intentional and
    allows both functions to operate on the same vector concurrently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, there is still concurrent access to the original data, and it is
    done without a lock: both threads need to make their thread-specific copies. However,
    this falls under the exception of read-only concurrent access and is safe.'
  prefs: []
  type: TYPE_NORMAL
- en: In principle, avoiding data sharing when possible and using mutexes otherwise
    is sufficient to arrange race-free access to data in any program. However, this
    may not be an efficient way to accomplish this goal, and good performance is almost
    always the goal of concurrency. We will now consider several other patterns for
    concurrent access to shared data that, when applicable, can offer superior performance.
    We are going to start with synchronization primitives that go beyond a mutex and
    are specifically designed to allow threads to efficiently wait for some event.
  prefs: []
  type: TYPE_NORMAL
- en: Waiting patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Waiting is a problem that is frequently encountered in concurrent programs
    and takes many forms. We have already seen one: the mutex. Indeed, if two threads
    are trying to enter the critical section simultaneously, one of them will have
    to wait. But waiting is not the goal here, just an unfortunate side effect of
    exclusive access to the critical section. There are other situations where waiting
    is the primary objective. For example, we may have threads that are waiting for
    some event to happen. This could be a user interface thread waiting for input
    (little to no performance requirements) or a thread waiting on a network socket
    (moderate performance requirements) or even a high-performance thread such as
    a computing thread in a thread pool waiting for a task to execute (extremely high
    performance requirements). Not surprisingly, there are different implementations
    for these scenarios, but fundamentally there are two approaches: polling and notifications.
    We are going to look at notifications first.'
  prefs: []
  type: TYPE_NORMAL
- en: The basic pattern for waiting for a notification is the **condition pattern**.
    It usually consists of a condition variable and a mutex. One or more threads are
    blocked waiting on the condition variable. During this time, there is one more
    thread that locks the mutex (thus guaranteeing exclusive access) and does the
    work whose completion the other threads are waiting for. Once the work is done,
    the thread that completed it must release the mutex (so other threads can access
    the shared data containing the results of this work) and notify the waiting threads
    that they can proceed.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a thread pool, the waiting threads are the pool worker threads
    that are waiting for tasks to be added to the pool. Since the pool task queue
    is a shared resource, a thread needs exclusive access to push or pop tasks. A
    thread that adds one or more tasks to the queue must hold the mutex while doing
    it and then notify the worker threads that there are tasks for them to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now see a very basic example of the notification pattern with just two
    threads. First, we have the main thread that starts a worker thread and then waits
    for it to produce some results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The locking in this case is provided by `std::unique_lock`, an object that wraps
    around a mutex and has a mutex-like interface with `lock()` and `unlock()` member
    functions. The mutex is locked in the constructor and almost immediately unlocked
    by the `wait()` function when we start waiting on the condition. When the notification
    is received, `wait()` locks the mutex again before returning control to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many implementations of waiting and conditions suffer from what is known as
    spurious wake-up: wait can be interrupted even without notification. This is why
    we also check whether the results are ready, in our case, by checking the result
    count `n`: if it is still zero, there are no results, the main thread has been
    awakened in error and we can go back to waiting (note that the waiting thread
    must still acquire the mutex before `wait()` returns, so it must wait for the
    worker thread to release this mutex).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The worker thread must lock the same mutex before it can access the shared
    data, then unlock it before notifying the main thread that the work is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'It is not necessary to hold the mutex the entire time the worker thread is
    active: its only purpose is to protect the shared data such as the result count
    `n` in our example.'
  prefs: []
  type: TYPE_NORMAL
- en: The two synchronization primitives `std::conditional_variable` and `std::unique_lock`
    are standard C++ tools for implementing the waiting pattern with a condition.
    Just as with a mutex, there are many variations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The alternative to notifications is polling. In this pattern, the waiting thread
    repeatedly checks whether some condition is met. In C++20, we can implement a
    simple example of polling wait using `std::atomic_flag` which is essentially an
    atomic boolean variable (prior to C++20 we could do the same with `std::atomic<bool>`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Atomic operations such as `test_and_set()` make use of **memory barriers**:
    a kind of global synchronization flag that ensures that all changes made to the
    memory before the flag is set (release) are visible to any operation on any other
    thread that is executed after the flag is tested (acquire). There is a lot more
    to these barriers, but it is outside of the scope of this book and can be found
    in many books dedicated to concurrency and efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important difference between this and the previous example is the
    explicit polling loop for the waiting thread in *Example 12*. If the wait is long,
    this is highly inefficient since the waiting thread is busy computing (reading
    from memory) the entire time it waits. Any practical implementation would introduce
    some sleep into the wait loop, but doing so also comes at a cost: the waiting
    thread will not wake up immediately after the worker thread sets the flag but
    must finish the sleep first. These efficiency concerns are outside of the scope
    of this book; here we want to show the overall structure and the components of
    these patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The boundary between polling and waiting is not always clear. For example,
    for all we know, `wait()` could be implemented by polling some internal state
    of the condition variable periodically. In fact, the same atomic flag we just
    saw can be used to wait for a notification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The call to `wait()` requires a corresponding call to `notify_one()` (or `notify_all()`
    if we have more than one thread waiting on the flag). Its implementation is almost
    certainly more efficient than our simple polling loop. After the notification
    is received and the wait is over, we check the flag to make sure it was really
    set. The standard says that this is not necessary and `std::atomic_flag::wait()`
    does not suffer from spurious wakeups, but TSAN in both GCC and Clang disagree
    (this could be a false positive in TSAN or a bug in the standard library implementation).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many other situations where waiting is needed, and the conditions
    we need to wait for vary widely. Another common need is to wait for a certain
    number of events to occur. For example, we may have several threads producing
    results and we may need all of them to complete their share of the work before
    the main thread can proceed. This is accomplished by waiting on a barrier or a
    latch. Prior to C++20, we would need to implement these synchronization primitives
    ourselves or use a third-party library, but in C++20 they became standard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The latch is initialized with the count of events to wait for. It will unlock
    when that many `count_down()` calls have been done.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other applications of waiting, but almost all waiting patterns
    fall broadly into one of the categories we have seen in this section (a specific
    implementation can have a dramatic effect on performance in a particular case,
    which is you are far more likely to see custom application-specific versions of
    these synchronization constructs than you are to find non-standard containers
    or other basic data structures).
  prefs: []
  type: TYPE_NORMAL
- en: We are now going to see several examples of very specialized and very efficient
    synchronization patterns. They are not for all situations, but when they fit the
    need, they often offer the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: Lock-free synchronization patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most of the time, safely accessing shared data relies on mutexes. C++ also
    supports another type of synchronizing concurrent threads: atomic operations.
    Again, a detailed explanation is outside of the scope of this book, and this section
    requires some prior knowledge of the atomics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea is this: some data types (usually integers) have special hardware
    instructions that allow a few simple operations such as reading or writing or
    incrementing the values to be done atomically, in a single event. During this
    atomic operation, other threads cannot access the atomic variable at all, so if
    one thread performs an atomic operation, all other threads can see the same variable
    as it was before the operation or after the operation but not in the middle of
    the operation. For example, an increment is a read-modify-write operation, but
    an atomic increment is a special hardware transaction such that once the read
    began, no other thread can access the variable until the write completes. These
    atomic operations are often accompanied by memory barriers; we have used them
    already to ensure that not just atomic but all other operations on all variables
    in the program are synchronized and free from data races.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest but useful application of atomic operations is for counting. We
    often need to count something in programs, and in concurrent programs, we may
    need to count some events that can occur on multiple threads. If we are only interested
    in the total count after all threads are done, this is best handled by the “non-sharing”
    or thread-specific counter we saw earlier. But what if all threads need to know
    the current count as well? We can always use a mutex, but using a mutex to protect
    a simple increment of an integer is highly inefficient. C++ gives us a better
    way, the atomic counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: There is only one shared variable in this example, `count` itself. Since we
    do not have any other shared data, we have no need for memory barriers (“relaxed”
    memory order means there are no requirements on the order of accesses to other
    data). The `fetch_add()` operation is an atomic increment, it increments `count`
    by one and returns the old value of `count`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The atomic count can also be used to let multiple threads work on the same
    data structure without any need for locking: to do this, we need to make sure
    there is only one thread working on each element of the data structure. When used
    in this manner, the pattern is often referred to as the atomic index. In the next
    example, we have an array of data that is shared between all threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We also have an atomic index that is used by all threads that need to store
    the results of their work in the array. To do so safely, each thread increments
    an atomic index and used the pre-increment value as the index into the array.
    No two atomic increment operations can produce the same value, therefore, each
    thread gets its own array elements to work on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Each thread gets to initialize however many array elements it can and stops
    when it (and all other threads) fill the entire array. The main thread has to
    wait until all the work is done before accessing any of the results. This cannot
    be done with just the atomic index since it is incremented when a thread starts
    working on a particular array element, not when that thread is done with the work.
    We have to use some other synchronization mechanism to make the main thread wait
    until all the work is done, such as a latch or, in a simple case, joining the
    producer threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The atomic count is good when we don’t rely on the value of the count to access
    the results that are already produced. In the last example, the producer threads
    did not need access to the array elements computed by other threads, and the main
    thread waits for all threads to complete before accessing the results. Often,
    this is not the case and we need to access data as it is being produced. This
    is where memory barriers come in.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest but surprisingly powerful lock-free pattern that relies on memory
    barriers is known as the publishing protocol. The pattern is applicable when one
    thread is producing some data that is going to be made accessible to one or more
    other threads when it is ready. The pattern looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The shared variable is an atomic pointer to the data. It is often called the
    “root” pointer because the data itself may be a complex data structure with multiple
    pointers connecting its parts. The key requirement of this pattern is that there
    is only one way to access the entire data structure and this is through the root
    pointer.
  prefs: []
  type: TYPE_NORMAL
- en: The producer thread builds all the data it needs to produce. It uses a thread-specific
    pointer, usually a local variable, to access the data. No other thread can see
    the data yet because the root pointer does not point to it and the local pointer
    of the producer thread is not shared with other threads.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when the data is complete, the producer atomically stores the pointer
    to the data in the shared root pointer. It is often said that the producer atomically
    publishes the data, hence the name of the pattern, the publishing protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'The consumers must wait for the data to be published: as long as the root pointer
    is null, there is nothing for them to do. They wait for the root pointer to become
    non-null (the wait does not have to use polling, a notification mechanism is also
    possible). Once the data is published, the consumer threads can access it through
    the root pointer. Because there is no other synchronization, no thread can modify
    the data once it’s published (the data may contain a mutex or some other mechanism
    to allow parts of it to be modified safely).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The atomic variable itself is insufficient for this pattern to guarantee no
    data races: all threads access not just the atomic pointer but the memory it points
    to. This is why we needed the specific memory barriers: when publishing the data,
    the producer uses the release barrier to not only initialize the pointer atomically
    but also ensure that all memory modifications that were done before the atomic
    write operations on the pointer become visible to anyone who reads the new value
    of the pointer. The consumer uses the acquire barrier to ensure that any operation
    on the shared data that is done after the new value of the pointer is read observes
    the latest state of the shared data as it existed at the moment the data was published.
    In other words, if you read the value of the pointer and then dereference it,
    you generally do not know if you will get the latest value of the data the pointer
    points to. But if you read the pointer with the acquire barrier (and the pointer
    was written with the release barrier), then you can be sure that you will read
    (acquire) the data as it was last written (released). Together, the release and
    acquire barriers guarantee that the consumer sees the shared data exactly as it
    was seen by the producer at the moment it published the address of the data in
    the root pointer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The same pattern can be used to publish completed elements of a larger data
    structure shared between threads. For example, we can have a producer thread publish
    how many array elements it initialized with the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The idea is exactly the same as in the previous example, only instead of the
    pointer we use the index into an array. In both cases, we have one producer thread
    that computes and publishes data, and one or more consumer threads that wait for
    the data to be published. If we need multiple producers, we must use some other
    synchronization mechanism to ensure that they don’t work on the same data, such
    as the atomic index we just saw.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a program with multiple producer and consumer threads, we often have to
    combine several synchronization patterns. In the next example, we have a large
    shared data structure organized as an array of pointers to the individual elements.
    Several producer threads fill this data structure with results; we are going to
    use the atomic index to ensure that each element is handled by only one producer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Our producer computes the result, then fetches the current index value and,
    at the same time, increments the index so the next producer cannot get the same
    index value. The array slot `data[s]` is, therefore, uniquely reserved for this
    producer thread. This is enough to avoid sharing conflicts between producers,
    but the consumers cannot use the same index to know how many elements are already
    in the array: the index is incremented before the corresponding array element
    is initialized. For the consumers, we use the publishing protocol: each array
    element is an atomic pointer that remains null until the data is published. The
    consumers must wait for a pointer to become non-null before they can access the
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the consumer stops as soon as it finds a data element that
    is not ready. We could continue scanning the array: some of the subsequent elements
    may be ready because they were filled by another producer thread. If we do, we
    have to somehow remember to come back to handle the elements we missed. The right
    approach depends on the problem we need to solve, of course.'
  prefs: []
  type: TYPE_NORMAL
- en: The literature on lock-free programming is extensive and full of (usually) very
    complex examples. The concurrency patterns we have demonstrated are only the basic
    building blocks for more complex data structures and data synchronization protocols.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see some of the much higher-level patterns that
    are applicable to the design of such data structures or even entire programs and
    their major components.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent design patterns and guidelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing and implementing concurrent software is hard. Even the basic patterns
    for controlling access to shared data, such as the ones we saw in the last section,
    are complex and full of subtle details. Failing to notice one of these details
    usually results in hard-to-debug data races. To simplify the task of writing concurrent
    programs, the programming community came up with several guidelines. All of them
    arise out of earlier disastrous experiences, so take these guidelines seriously.
    Central to these guidelines is the concept of thread safety guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: Thread safety guarantees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While this is not a pattern, it is a concept that is much broader in scope and
    one of the key design principles for any concurrent software. Every class, function,
    module, or component of a concurrent program should specify the thread safety
    guarantees it provides, as well as the guarantees it requires from the components
    it uses.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, a software component can offer three levels of thread-safety guarantees:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Strong thread safety guarantee**: Any number of threads can access this component
    without restrictions and without encountering undefined behavior. For a function,
    it means that any number of threads can call this function at the same time (possibly,
    with some restrictions on parameters). For a class, it means that any number of
    threads can call member functions of this class concurrently. For a larger component,
    any number of threads can operate its interfaces (again, possibly with some restrictions).
    Such components, classes, and data structures are sometimes called thread-safe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`const` member functions). Only one thread can modify the state of the component
    at any time, and the locking or another way of ensuring such exclusive access
    is the responsibility of the caller. Such components, classes, and data structures
    are sometimes called thread-compatible because you can build a concurrent program
    from them using the appropriate synchronization mechanisms. All STL containers
    offer this level of guarantee.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No thread-safety guarantee**: Such components cannot be used in a concurrent
    program at all and are sometimes called thread-hostile. These classes and functions
    often have hidden global states that cannot be accessed in a thread-safe manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By designing each component to provide certain thread safety guarantees, we
    can divide the intractable problem of making the entire program thread-safe into
    a hierarchy of design challenges where the more complex components take advantage
    of the guarantees provided by the simpler ones. Central to this process is the
    notion of the transactional interface.
  prefs: []
  type: TYPE_NORMAL
- en: Transactional interface design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea of the transactional interface design is very simple: every component
    should have an interface such that every operation is an atomic transaction. From
    the point of view of the rest of the program, the operation either has not happened
    yet or is done. No other thread can observe the state of the component during
    the operation. This can be accomplished using mutexes or any other synchronization
    scheme that fits the need – the particular implementation can influence performance
    but is not essential for correctness as long as the interface guarantees transaction
    processing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This guideline is most useful for designing data structures for concurrent
    programs. Here, it is so important that it is generally accepted that one cannot
    design a thread-safe data structure that does not offer a transactional interface
    (at least not a useful data structure). For example, we can consider a queue.
    The C++ standard library offers a `std::queue` template. As with any other STL
    container, it offers the weak guarantee: any number of threads can call `const`
    methods of the queue as long as no thread calls any non-`const` methods. Alternatively,
    any one thread can call a non-`const` method. To ensure the latter, we have to
    lock all accesses to the queue with an external mutex. If we want to pursue this
    approach, we should combine the queue and the mutex in a new class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To push another element onto a queue, we need to lock the mutex, since the
    `push()` member function modifies the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This works exactly as we want it to: any number of threads can call `push()`
    and every element will be added to the queue exactly once (the order is going
    to be arbitrary if multiple calls happen simultaneously, but this is the nature
    of concurrency). We have successfully provided the strong thread safety guarantee!'
  prefs: []
  type: TYPE_NORMAL
- en: 'The triumph is going to be short-lived, unfortunately. Let us see what it takes
    to pop an element from the queue. There is a member function `pop()` that removes
    the element from the queue, so we can protect it with the same mutex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that this function does not return anything: it removes the oldest element
    in the queue and destroys it, but that’s not what we need to find out what that
    element is (or was). For that, we need to use the function `front()` which returns
    a reference to the oldest element but does not modify the queue. It is a `const`
    member function, so we need to lock it only if we call any non-`const` functions
    at the same time; we are going to ignore this optimization possibility for now
    and always lock this call as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: If we call `front()` from multiple threads and don’t call any other functions,
    this implementation is sub-optimal, but it is not wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one special case we have neglected to mention: if the queue is empty,
    you should not call `pop()` or `front()` – doing so leads to undefined behavior,
    according to the standard. How do you know if it is safe to pop an element from
    the queue? You can check if the queue is empty. This is another `const` member
    function, and again we are going to over-protect it and lock every call to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now every member function of the underlying `std::queue` is protected by a mutex.
    We can call any of them from any number of threads and be guaranteed that only
    one thread can access the queue at any time. Technically, we have achieved the
    strong guarantee. Unfortunately, it is not very useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see why, let us consider the process of removing an element from the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This works fine on one thread, but we didn’t need a mutex for that. It still
    (mostly) works when we have two threads, one of which is pushing new elements
    onto the queue and the other one is taking them from the queue. Let us consider
    what happens when two threads try to pop one element each. First, they both call
    `empty()`. Let us assume that the queue is not empty and both calls return `true`.
    Then, they both call `front()`. Since neither thread did a `pop()` yet, both threads
    get the same front element. This is not what was supposed to happen if we want
    each thread to pop an element from the queue. Finally, both threads call `pop()`,
    and two elements are removed from the queue. One of these elements we have never
    seen and will never see again, so we lost some of the data that was enqueued.
  prefs: []
  type: TYPE_NORMAL
- en: But this isn’t the only way it can go wrong. What happens if there is only one
    element on the queue? Both calls to `empty()` still return true – a queue with
    one element is not empty. Both calls to `front()` still return the (same) front
    element. The first call to `pop()` succeeds but the second one is undefined behavior
    because the queue is now empty. It is also possible that one thread calls `pop()`
    before the other thread calls `front()` but after it calls `empty()`. In this
    case, the second call to `front()` is also undefined.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a perfectly safe and a perfectly useless data structure. Clearly, a
    thread safety guarantee is not enough. We also need an interface that does not
    expose us to undefined behavior, and the only way to do this is to perform all
    three steps of the pop operation (`empty()`, `front()`, and `pop()`) in a single
    critical section, i.e., without releasing the mutex between the calls. Unless
    we want the caller to supply their own mutex, the only way to do this is to change
    the interface of our `ts_queue` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `push()` function is the same as it was before (we made the argument type
    more flexible, but this is not related to thread safety). The reason we did not
    need to change the push operation is that it is already transactional: at the
    end, the queue has one more element than it had at the beginning of the operation,
    and the state of the queue is otherwise identical. We just made it atomic by protecting
    it with a mutex (no other thread that also uses the same mutex correctly can observe
    our queue in its transitional non-invariant state).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pop()` operation is where the transactional interface looks very different.
    In order to provide a meaningful thread safety guarantee, we have to provide an
    operation that returns the front element to the caller and removes it from the
    queue atomically: no other thread should be able to see the same front element,
    therefore, we have to lock both `front()` and `pop()` on the original queue with
    the same mutex. We also have to consider the possibility that the queue is empty
    and we have no front element to return to the caller. What do we return in this
    case? If we decided to return the front element by value, we would have to default-construct
    this value (or return some other agreed-upon value that means “no element”). In
    C++17, a better way is to return a `std::optional` that holds the front element
    if there is one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now both `pop()` and `push()` are atomic and transactional: we can call both
    methods from as many threads as we want, and the results are always well-defined.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may wonder why didn’t `std::queue` offer this transactional interface,
    to begin with. First, STL was designed long before threads made it into the standard.
    But the other, very important, reason is that the queue interface was influenced
    by the need to provide exception safety. Exception safety is the guarantee that
    the object remains in a well-defined state if an exception is thrown. Here, the
    original queue interface does very well: `empty()` just returns the size and cannot
    throw an exception, `front()` returns the reference to the front element and also
    cannot throw, and finally `pop()` calls the destructor of the front element, which
    normally does not throw either. Of course, when accessing the front element, the
    caller’s code may throw (for example, if the caller needs to copy the front element
    to another object) but the caller is expected to handle that. In any case, the
    queue itself remains in a well-defined state.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our thread-safe queue, however, has an exception safety problem: the code that
    copies the front element of the queue to return it to the caller is now inside
    `pop()`. If the copy constructor throws during the construction of the local `std::optional`
    variable `res`, we are probably OK. However, if an exception is thrown when the
    result is returned to the caller (which can happen by move or copy), then `pop()`
    was already done, so we are going to lose the element we just popped from the
    queue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This tension between thread safety and exception safety is often unavoidable
    and has to be considered when designing thread-safe data structures for concurrent
    programs. Regardless, it must be reiterated that the only way to design thread-safe
    data structures or larger modules is to ensure that every interface call is a
    complete transaction: any steps that are conditionally defined must be packaged
    into a single transactional call together with the operations that are needed
    to ensure that such conditions are met. Then, the entire call should be guarded
    by a mutex or some other way to ensure race-free exclusive access.'
  prefs: []
  type: TYPE_NORMAL
- en: Designing thread-safe data structures is generally very hard, especially if
    we want good performance (and what is the point of concurrency if we don’t?).
    That is why it is very important to take advantage of any use restrictions or
    special requirements that allow us to impose restrictions on how these data structures
    are used. In the next section, we will see one common case of such restrictions.
  prefs: []
  type: TYPE_NORMAL
- en: Data structures with access limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Designing thread-safe data structures is so hard that one should look for any
    opportunity to simplify the requirements and the implementation. If there is any
    scenario you don’t need right now, think if you can make your code simpler if
    you do not support that scenario. One obvious case is a data structure that is
    built by a single thread (no thread safety guarantees needed) then becomes immutable
    and is accessed by many threads that act as readers (a weak guarantee is sufficient).
    Any STL container, for example, can operate in this mode as-is. We still need
    to ensure that no reader can access the container while it’s still being filled
    with data, but that can be easily done with a barrier or a condition. This is
    a very useful but rather trivial case. Are there any other restrictions we can
    make use of?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we consider a particular use case that occurs quite frequently
    and allows for much simpler data structures. Specifically, we examine the situation
    when a particular data structure is accessed by only two threads. One thread is
    the producer, it adds data to the data structure. The other thread is the consumer,
    it removes the data. Both threads do modify the data structure but in different
    ways. This situation occurs rather frequently and often allows for very specialized
    and very efficient data structure implementations. It probably deserves recognition
    as a design pattern for concurrent designs, and it already has a commonly recognized
    name: “single-producer single-consumer data structure.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we are going to see an example of a single-producer single-consumer
    queue. It is a data structure that is frequently used with one producer and one
    consumer thread, but the ideas we explore here can be used to design other data
    structures as well. The main distinguishing feature of this queue is going to
    be that it is lock-free: there are no mutexes in it at all, so we can expect much
    higher performance from it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The queue is built on an array of a fixed size, so, unlike a regular queue,
    it cannot grow indefinitely (this is another common restriction used to simplify
    lock-free data structures):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example, we default-construct elements in the array. If this is undesirable,
    we can also use a properly aligned uninitialized buffer. All accesses to the queue
    are determined by two atomic variables, `back_` and `front_`. The former is the
    index of the array element that we will write into when we push a new element
    onto the queue. The latter is the index of the array element we will read from
    when we need to pop an element from the queue. All array elements in the range
    [`front_`, `back_`) are filled with elements currently on the queue. Note that
    this range can wrap over the end of the buffer: after using the element `buffer_[N-1]`
    the queue does not run out of space but starts again from `buffer_[0]`. This is
    known as a **circular buffer**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we use these indices to manage the queue? Let us start with the push
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to read the current value of `back_`, of course: this is the index
    of the array element we are about to write. We support only one producer, and
    only the producer thread can increment `back_`, so we do not need any particular
    precautions here. We do, however, need to be careful to avoid overwriting any
    elements already in the queue. To do this we must check the current value of `front_`
    (we can read it before or after reading `back_`, it makes no difference). If the
    element `buffer_[back]` that we are about to overwrite is also the front element,
    then the queue is full and the `push()` operation fails (note that there is another
    solution to this problem that is often used in real-time systems: if the queue
    is full, the oldest element is silently overwritten and lost). After the new element
    is stored, we atomically increment the `back_` value to signal to the consumer
    that this slot is now available for reading. Because we are publishing this memory
    location, we must use the release barrier. Also note the modular arithmetic: after
    reaching the array element `N-1`, we’re looping back to element 0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let us see the `pop()` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we need to read both `front_` and `back_`: `front_` is the index of
    the element we are about to read, and only the consumer can advance this index.
    On the other hand, `back_` is needed to make sure we actually have an element
    to read: if the front and back are the same, the queue is empty; again, we use
    `std::optional` to return a value that might not exist. We must use acquire barrier
    when reading `back_` to make sure we see the element values that were written
    into the array by the producer thread. Finally, we advance `front_` to ensure
    that we don’t read the same element again and to make this array slot available
    to the producer thread.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several subtle details here that must be pointed out. Reading `back_`
    and `front_` is not done in a single transaction (it is not atomic). In particular,
    if the producer reads `front_` first, it is possible that, by the time it reads
    `back_` and compares the two, the consumer has already advanced `front_`. That
    does not make our data structure incorrect, though. At worst, the producer can
    report that the queue is full when in fact, it is no longer full. We could read
    both values atomically, but this will only degrade the performance, and the caller
    still has to handle the case when the queue is full. Similarly, when `pop()` reports
    that the queue is empty, it may no longer be so by the time the call completes.
    Again, these are the inevitable complexities of concurrency: each operation reflects
    the state of the data at some point in time. By the time the caller gets the return
    value and can analyze it, the data may have changed already.'
  prefs: []
  type: TYPE_NORMAL
- en: Another note-worthy detail is the careful management of the queue elements’
    lifetime. We default-construct all elements in the array, so the proper way to
    transfer the data from the caller into the queue during `push()` is by copy or
    move assignment (`std::forward` does both). On the other hand, once a value is
    returned to the caller by `pop()`, we never need that value again, so the right
    operation here is move, first into the optional and then into the caller’s return
    value object. Note that moving an object is not the same as destroying it; indeed,
    the moved-from array elements are not destroyed until the queue itself is. If
    an array element is reused, it is copy- or move-assigned a new value, and assignments
    are two of the three operations that are safe to do on a moved-from object (the
    third one is the destructor, which we will also call eventually).
  prefs: []
  type: TYPE_NORMAL
- en: The single-producer single-consumer pattern is a common pattern that allows
    a programmer to greatly simplify their concurrent data structures. There are others,
    you can find them in books and papers dedicated to concurrent data structures.
    All these patterns are ultimately designed to help you write data structures that
    perform correctly and efficiently when accessed by multiple threads. We, however,
    must move on and finally tackle the problem of using these threads to get some
    useful work done.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent execution patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next group of patterns for concurrency we must learn are execution patterns.
    These patterns are used to organize the computations done on multiple threads.
    You will find out that, just as with the synchronization patterns we saw earlier,
    all of these are low-level patterns: most solutions for practical problems must
    combine these patterns into larger, more complex, designs. This is not because
    C++ is ill-suited for such larger designs; if anything, it is the opposite: there
    are so many ways to implement, for example, a thread pool, in C++, that for every
    concrete application, there is a version that is ideally suited in terms of performance
    and features. This is why it is hard to describe these more complete solutions
    as patterns: while the problems they address are common, the solutions vary a
    great deal. But all of these designs have a number of challenges to resolve, and
    the solutions to those challenges usually use the same tools over and over, so
    we can at least describe these more basic challenges and their common solutions
    as design patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: Active object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first concurrent execution pattern we are going to see is the Active Object.
    An active object usually encapsulates the code to be executed, the data needed
    for the execution, and the flow of control needed to execute the code asynchronously.
    This flow of control could be as simple as a separate thread that the object starts
    and joins. In most cases, we do not start a new thread for every task, so an active
    object would have some way to run its code on a multi-threaded executor such as
    a thread pool. From the caller’s point of view, an active object is an object
    that the caller constructs, initializes with the data, then tells the object to
    execute itself, and the execution happens asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic active object looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In the simplest case shown here, the active object contains a thread that is
    used to execute the code asynchronously. In most practical cases you would use
    an executor that schedules the work on one of the threads it manages, but this
    gets us into implementation-specific details. The execution starts when `operator()`
    is called; we can also make the object execute as soon as it is constructed by
    calling `operator()` from the constructor. At some point, we have to wait for
    the results. If we use a separate thread, we can join the thread at that time
    (and take care to not attempt to join it twice if the caller calls `wait()` again).
    If the object represents not a thread but a task in a thread pool or some other
    executor, we would do the cleanup necessary for that case.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, once we settle on a particular way to execute code asynchronously,
    writing active objects with different data and code is a rather repetitive task.
    Nobody writes active objects the way we just did, we always use some generic reusable
    framework. There are two general approaches to implementing such a framework.
    The first one uses inheritance: the base class does the boilerplate work, and
    the derived class contains the unique task-specific data and code. Staying with
    our simple approach to active objects, we could write the base class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The base object `Job` contains everything needed to implement the asynchronous
    control flow: the thread and the state flag needed to join the thread only once.
    It also defines the way to execute the code by calling the non-virtual function
    `run()`. The code that is executed on the thread must be provided by the derived
    object by overriding `operator()`. Note that only `run()` is public, and `operator()`
    is not: this is the non-virtual idiom in action (we saw it in [*Chapter 14*](B19262_14.xhtml#_idTextAnchor640),
    *The Template Method Pattern and the* *Non-Virtual Idiom*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The derived object is problem-specific, of course, but generally looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The only subtlety here is the call to `run()` done at the end of the constructor
    of the derived object. It is not necessary (we can execute the active object later
    ourselves) but if we do want the constructor to run it, it has to be done in the
    derived class. If we start the thread and the asynchronous execution in the base
    class constructor, then we will have a race between the execution on the thread
    – `operator()` – and the rest of the initialization which continues in the derived
    class constructor. For the same reason, an active object that starts executing
    from the constructor should not be derived from again; we ensure that by making
    the object final.
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of our active object is very simple: we create it, the object starts
    executing the code in the background (on a separate thread), and when we need
    the result we ask for it (this may involve waiting):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’ve written any concurrent code in C++ at all, you will have definitely
    used an active object already: `std::thread` is an active object, it lets us execute
    arbitrary code on a separate thread. There are concurrency libraries for C++ where
    a thread is a base object and all concrete threads are derived from it. But this
    is not the approach chosen for the C++ standard thread. It follows the second
    way to implement a reusable active object: type erasure. If you need to familiarize
    yourself with it, reread [*Chapter 6*](B19262_06.xhtml#_idTextAnchor266), *Understanding
    Type Erasure*. Even though `std::thread` itself is a type-erased active object,
    we’re going to implement our own just to demonstrate the design (the standard
    library code is rather hard to read). This time, there is no base class. The framework
    is provided by a single class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'To implement the type-erased callable, we use `std::function` (we could also
    use one of the more efficient implementations from [*Chapter 6*](B19262_06.xhtml#_idTextAnchor266),
    *Understanding Type Erasure*, or implement type erasure ourselves following the
    same approach). The code supplied by the caller to be executed on a thread comes
    from the callable `f` in the constructor argument. Note that the order of the
    class members is very important: the asynchronous execution starts as soon as
    the thread `t_` is initialized, so other data members, in particular, the callable
    `f_`, must be initialized before that happens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the active object of this style, we need to supply a callable. It could
    be a lambda expression or a named object, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that, in this design, there is no easy way to access the data members
    of the callable `TheJob`, unless it was created as a named object. For this reason,
    the results are usually returned through arguments passed to the constructor by
    reference (the same as we do with `std::thread`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Active objects can be found in every concurrent C++ program, but some uses of
    them are common and specialized, and so are recognized as concurrent design patterns
    in their own right. We will now see several of these patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Reactor Object pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Reactor pattern often uses for event handling or responding to service
    requests. It solves a specific problem where we have multiple requests for certain
    actions that are issued by multiple threads; however, the nature of these actions
    is such that at least part of them must be executed on one thread or otherwise
    synchronized. The reactor object is the object that services these requests: it
    accepts requests from multiple threads and executes them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a reactor that can accept requests to perform a specific
    computation with caller-supplied inputs and store the results. The requests can
    come from any number of threads. Each request is allocated a slot in the array
    of results – that is the part that must be synchronized across all threads. After
    the slot is allocated, we can do the computations concurrently. To implement this
    reactor, we are going to use the atomic index to allocate unique array slots to
    each request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The calls to `operator()` are thread-safe: any number of threads can call this
    operator simultaneously, and each call will add the result of the computation
    to the next array slot without overwriting any data produced by other calls. To
    retrieve the results from the object, we can either wait until all requests are
    done or implement another synchronization mechanism such as the publishing protocol
    to make calls to `operator()` and `print_results()` thread-safe with respect to
    each other.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that usually, a reactor object processes requests asynchronously: it has
    a separate thread to execute the computations and a queue to channel all the requests
    to a single thread. We can build such a reactor by combining several patterns
    we saw earlier, for example, we can add a thread-safe queue to our basic reactor
    to get an asynchronous reactor (we are about to see an example of such a design).'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we focused on starting and executing jobs, and then we wait for the
    work to complete. The next pattern focuses on handling the completion of asynchronous
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Proactor Object pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Proactor pattern is used to execute asynchronous tasks, usually long-running,
    by requests from one or more threads. This sounds a lot like the Reactor, but
    the difference is what happens when a task is done: in the case of the Reactor,
    we just have to wait for the work to get done (the wait can be blocking or non-blocking,
    but in all cases, the caller initiates the check for completion). The Proactor
    object associates each task with a callback, and the callback is executed asynchronously
    when the task is done. The Reactor and the Proactor are the synchronous and asynchronous
    solutions to the same problem: handling the completion of concurrent tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: A proactor object typically has a queue of tasks to be executed asynchronously
    or uses another executor to schedule these tasks. Each task is submitted with
    a callback, usually a callable. The callback is executed when the task is done;
    often, the same thread that executed the task will also invoke the callback. Since
    the callback is always asynchronous, care must be taken if it needs to modify
    any shared data (for example, any data that is accessed by the thread that submitted
    the task to the proactor). If there is any data shared between the callback and
    other threads, it must be accessed in a thread-safe way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a proactor object that uses the thread-safe queue from
    the previous section. In this example, each task takes one integer as input and
    computes a `double` result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The queue stores the work requests which consist of the input and the callable;
    any number of threads can call `operator()` to add requests to the queue. A more
    generic proactor might take a callable for the work request instead of having
    the computation coded into the concrete proactor object. The proactor executes
    all the requests in order on a single thread. When the requested computation is
    done, the thread invokes the callback and passes the result to it. This is how
    we may use such a proactor object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Note that our proactor executes all callbacks on one thread, and the main thread
    does not do any output. Otherwise, we would have to protect `std::cout` with a
    mutex.
  prefs: []
  type: TYPE_NORMAL
- en: The Proactor pattern is used to both execute asynchronous events and perform
    additional actions (callbacks) when these events happen. The last pattern we explore
    in this section does not execute anything but is used to react to external events.
  prefs: []
  type: TYPE_NORMAL
- en: Monitor pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Monitor pattern is used when we need to observe, or monitor, some conditions
    and respond to certain events. Usually, a monitor runs on its own thread that
    is sleeping or waiting most of the time. The thread is awakened either by a notification
    or simply by the passage of time. Once awakened, the monitor object examines the
    state of the system it is tasked to observe. It may take certain actions if the
    specified conditions are met, then the thread goes back to waiting.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to see a monitor implementation that uses a timeout; a monitor
    with a condition variable can be implemented using the same approach but with
    the waiting on notification pattern as seen earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need something to monitor. Let us say that we have several producer
    threads that do some computations and store results in an array using an atomic
    index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Our producer also stores the count of results computed by the thread in the
    `count` variable passed to it. Here is how we launch the producer threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We have one result count per thread, so each producer has its own count to
    increment. Why, then, did we make the counts atomic? Because the counts are also
    what we are going to monitor: our monitor thread will periodically report on how
    much work is done. Thus, each work count is accessed by two threads, the producer
    and the monitor, and we need to either use atomic operations or a mutex to avoid
    data races.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The monitor is going to be a separate thread that wakes up every now and then,
    reads the values of the result counts, and reports the progress of the work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The monitor can be started before the producer threads, or at any time we need
    to monitor the progress of the work, and it will report how many results are computed
    by each producer thread, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we used five threads to compute the total of 64K results, and the monitor
    reports the counts for each thread and the total result count. To shut down the
    monitor, we need to set the `done` flag and join the monitor thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The other common variant of the Monitor pattern is the one where, instead of
    waiting on a timer, we wait on a condition. This monitor is a combination of the
    basic monitor and the pattern for waiting on a notification that we saw earlier
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The concurrent programming community has come up with many other patterns for
    solving common problems related to concurrency; most of these can be used in C++
    programs but they are not specific to C++. There are C++-specific features such
    as atomic variables that influence the way we implement and use these patterns.
    The examples from this chapter should give you enough guidance to be able to adapt
    any other concurrent pattern to C++.
  prefs: []
  type: TYPE_NORMAL
- en: The description of execution patterns mostly concludes the brief study of C++
    patterns for concurrency. Before you turn the last page, I want to show you a
    totally different type of concurrent pattern that is just coming to C++.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutine patterns in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Coroutines are a very recent addition to C++: they were introduced in C++20,
    and their present state is a foundation for building libraries and frameworks
    as opposed to features you should use in the application code directly. It is
    a complex feature with many subtle details, and it would take an entire chapter
    to explain what it does (there is a chapter like that in my book *The Art of Writing
    Efficient Programs*). Briefly, coroutines are functions that can suspend and resume
    themselves. They cannot be forced to suspend, a coroutine continues to execute
    until it suspends itself. They are used to implement what is known as cooperative
    multitasking, where multiple streams of execution voluntarily yield control to
    each other rather than being forcibly preempted by the OS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Every execution pattern we saw in this chapter, and many more, can be implemented
    using coroutines. It is, however, too early to say whether this is going to become
    a common use of coroutines in C++, so we cannot say whether a “proactor coroutine”
    will ever become a pattern. One application of coroutines, however, is well on
    the way to becoming a new pattern in C++: the coroutine generator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This pattern comes into play when we want to take some computation that is
    normally done with a complex loop and rewrite it as an iterator. For example,
    let us say that we have a 3D array and we want to iterate over all its elements
    and do some computation on them. This is easy enough to do with a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'But it’s hard to write reusable code this way: if we need to customize the
    work we do on each array element, we have to modify the inner loop. It would be
    much easier if we had an iterator that runs over the entire 3D array. Unfortunately,
    to implement this iterator we have to turn the loops inside out: first, we increment
    `k` until it reaches `N3`; then, we increment `j` by one and go back to incrementing
    `k`, and so on. The result is a very convoluted code that reduced many a programmer
    to counting on their fingers to avoid one-off errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We even took a shortcut and gave our iterator a non-standard interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The implementation is even more convoluted if we want to conform to the STL
    iterator interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Problems such as this, where a complex function such as our nested loop must
    be suspended in the middle of the execution so the caller can execute some arbitrary
    code and resume the suspended function, are an ideal fit for coroutines. Indeed,
    a coroutine that produces the same sequence as our iterator looks very simple
    and natural:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s it; we have a function that takes the parameters necessary to loop over
    a 3D array, a regular nested loop, and we do something to each element. The secret
    is in that innermost line where “something” happens: the C++20 keyword `co_yield`
    suspends the coroutine and returns the value `a[i][j][k]` to the caller. It is
    very similar to the `return` operator, except `co_yield` does not exit the coroutine
    permanently: the caller can resume the coroutine, and the execution continues
    from the next line after `co_yield`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of this coroutine is also straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The coroutine magic happens inside the generator object that is returned by
    the coroutine. Its implementation is anything but simple, and, if you want to
    write one yourself, you have to become an expert on C++ coroutines (and do so
    by reading another book or article). You can find a very minimal implementation
    in *Example 28*, and, with the help of a good reference for coroutines, you can
    understand its inner working line by line. Fortunately, if all you want is to
    write code like that shown previously, you don’t really have to learn the details
    of the coroutines: there are several open-source libraries that provide utility
    types such as the generator (with slightly different interfaces), and in C++23
    `std::generator` will be added to the standard library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While it is certainly easier to write the coroutine with a loop and `co_yield`
    than the convoluted inverted loop of the iterator, what is the price of this convenience?
    Obviously, you have to either write a generator or find one in a library, but
    once that is done, are there any more disadvantages to the coroutines? In general,
    a coroutine involves more work than a regular function, but the performance of
    the resulting code depends greatly on the compiler and can vary with seemingly
    insignificant changes to the code (as is the case for any compiler optimizations).
    The coroutines are still quite new and the compilers do not have comprehensive
    optimizations for them. That being said, the performance of coroutines can be
    comparable to that of the hand-crafted iterator. For our *Example 28*, the current
    (at the moment of this writing) release of Clang 17 gives the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, GCC 13 gives an advantage to the iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We can expect the compilers to get better at optimizing coroutines in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Another variant of the coroutine generator is useful when the sequence of values
    that we want to produce is not limited in advance and we want to generate new
    elements only when they are needed (lazy generator). Again, the advantage of coroutines
    is the simplicity of returning results to the caller from inside of the loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple random number generator implemented as a coroutine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'This coroutine never ends: it suspends itself to return the next pseudo-random
    number `i`, and every time it is resumed, the execution jumps back into the infinite
    loop. Again, the generator is a rather complex object with a lot of boilerplate
    code that you would be better off getting from a library (or waiting until C++23).
    But once that’s done, the use of the generator is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Every time you call `gen()`, you get a new random number (of rather poor quality
    since we have implemented one of the oldest and simplest pseudo-random number
    generators, so consider this example useful for illustration only). The generator
    can be called as many times as you need; when it is finally destroyed, so is the
    coroutine.
  prefs: []
  type: TYPE_NORMAL
- en: We will likely see more design patterns that take advantage of coroutines develop
    in the coming years. For now, the generator is the only established one, and just
    recently at that, so it is fitting to conclude the last chapter of the book on
    C++ design patterns with the newest addition to our pattern toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored common C++ solutions to the problems of developing
    concurrent software. This is a very different type of problem compared to everything
    we studied before. Our main concerns here are correctness, specifically, by avoiding
    data races, and performance. Synchronization patterns are standard ways to control
    access to shared data to avoid undefined behavior. Execution patterns are the
    basic building blocks of thread schedulers and asynchronous executors. Finally,
    the high-level patterns and guidelines for the concurrent design are the ways
    we, the programmers, keep our sanity while trying to think about all the things
    that could happen before, after, or at the same time as one another.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is concurrency?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does C++ support concurrency?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are synchronization design patterns?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are execution design patterns?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What overall guidelines for the design and the architecture of concurrent programs
    should be followed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a transactional interface?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chapter 1, An Introduction to Inheritance and Polymorphism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Objects and classes are the building blocks of a C++ program. By combining data
    and algorithms (*code*) into a single unit, the C++ program represents the components
    of the system that it models, as well as their interactions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Public inheritance represents an *is-a* relationship between objects—an object
    of the derived class can be used as if it was an object of the base class. This
    relation implies that the interface of the base class, with its invariants and
    restrictions, is also a valid interface for the derived class.Unlike public inheritance,
    private inheritance says nothing about the interfaces. It expresses a *has-a*
    or *is implemented in terms of* relationship. The derived class reuses the implementation
    provided by the base class. For the most part, the same can be accomplished by
    composition. Composition should be preferred when possible; however, empty base
    optimization and (less often) virtual method overrides are valid reasons to use
    private inheritance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A polymorphic object in C++ is an object whose behavior depends on its type,
    and the type is not known at compile time (at least at the point where the behavior
    in question is requested). An object that is referred to as a base class object
    can demonstrate the behavior of the derived class if that is its true type. In
    C++, polymorphic behavior is implemented using virtual functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dynamic cast verifies at run time that the destination type of the cast is
    valid: it must be either the actual type of the object (the type the object was
    created with) or one if its base types. It is the latter part, checking all possible
    base types of an object, that makes dynamic casts expensive.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 2, Class and Function Templates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A template is not a type; it is a *Factory* for many different types with similar
    structures. A template is written in terms of generic types; substituting concrete
    types for these generic types results in a type generated from the template.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are class, function, and variable templates. Each kind of template generates
    the corresponding entities—functions in the case of function templates, classes
    (types) from class templates, and variables from variable templates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Templates can have type and non-type parameters. Type parameters are types.
    Non-type parameters can be integral or enumerated values or templates (in the
    case of variadic templates, the placeholders are also non-type parameters).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A template instantiation is the code generated by a template. Usually, the instantiations
    are implicit; the use of a template forces its instantiation. An explicit instantiation,
    without use, is also possible; it generates a type or a function that can be used
    later. An explicit specialization of a template is a specialization where all
    generic types are specified; it is not an instantiation, and no code is generated
    until the template is used. It is only an alternative recipe for generating code
    for these specific types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Usually, the parameter pack is iterated over using recursion. The compiler will
    typically inline the code generated by this recursion, so the recursion exists
    only during compilation (as well as in the head of the programmer reading the
    code). In C++17 (and, rarely, in C++14), it is possible to operate on the entire
    pack without recursion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lambda expressions are essentially a compact way to declare local classes that
    can be called like functions. They are used to effectively store a fragment of
    code in a variable (or, rather, associate the code with a variable) so that this
    code can be called later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Concepts impose restrictions on the template parameters. This can be used to
    avoid substituting types and instantiating a template that would lead to an error
    in the body of the template. In the more complex cases, concepts can be used to
    disambiguate the choice between multiple template overloads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 3, Memory and Ownership
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clear memory ownership, and by extension, resource ownership, is one of the
    key attributes of a good design. With clear ownership, resources are certain to
    be created and made available in time for when they are needed, maintained while
    they are in use, and released/cleaned up when no longer needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resource leaks, including memory leaks; dangling handles (resource handles,
    such as pointers, references, or iterators, pointing to resources that do not
    exist); multiple attempts to release the same resource; multiple attempts to construct
    the same resource.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-ownership, exclusive ownership, shared ownership, as well as conversion
    between different types of ownership and transfer of ownership.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ownership-agnostic functions and classes should refer to objects by raw pointers
    and references if the corresponding ownership is handled through owning pointers.
    If the objects are owned by rich pointers or containers, the problem becomes more
    difficult. If the additional data contained in the rich pointer is not needed
    or a single element of a container is accessed, raw pointers and references are
    perfectly adequate. Otherwise, ideally, we would use a corresponding non-owning
    reference object like `std::string_view` or one of the views from the ranges library.
    If none is available, we may have to pass the owning object itself by reference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exclusive memory ownership is easier to understand and follow the control flow
    of the program. It is also more efficient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preferably, by allocating the object on the stack or as a data member of the
    owning class (including container classes). If reference semantics or certain
    move semantics are needed, a unique pointer should be used. For conditionally
    constructed objects, `std::optional` is a great solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shared ownership should be expressed through a shared pointer such as `std::shared_ptr`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shared ownership in a large system is difficult to manage and may delay the
    deallocation of resources unnecessarily. It also has a nontrivial performance
    overhead, compared to exclusive ownership. Maintaining shared ownership in a thread-safe
    concurrent program requires very careful implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Views such as `std::string_view`, `std::span`, and views from `std::ranges`
    are essentially non-owning rich pointers. A string view to a string is what a
    raw pointer is to a unique pointer: a non-owning object containing the same information
    as the corresponding owning object.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 4, Swap - From Simple to Subtle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The swap function exchanges the state of the two objects. After the swap call,
    the objects should remain unchanged, except for the names they are accessed by.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Swap is usually employed in programs that provide commit-or-rollback semantics;
    a temporary copy of the result is created first, then swapped into its final destination
    only if no errors were detected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The use of swap to provide commit-or-rollback semantics assumes that the swap
    operation itself cannot throw an exception or otherwise fail and leave the swapped
    objects in an undefined state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A non-member swap function should always be provided, to ensure that the calls
    to non-member swap are executed correctly. A member swap function can also be
    provided, for two reasons—first, it is the only way to swap an object with a temporary,
    and second, the swap implementation usually needs access to the private data members
    of the class. If both are provided, the non-member function should call the member
    swap function on one of the two parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All STL containers and some other standard library classes provide a member
    function `swap()`. In addition, the non-member `std::swap()` function template
    has standard overloads for all STL types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `std:: qualifier` disables the argument-dependent lookup and forces the
    default `std::swap` template instantiation to be called, even if a custom swap
    function was implemented with the class. To avoid this problem, it is recommended
    to also provide an explicit instantiation of the `std::swap` template.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 5, Comprehensive Look at RAII
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Memory is the most common resource, but any object can be a resource. Any virtual
    or physical quantity that the program operates on is a resource.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resources should not be lost (leaked). If a resource is accessed through a handle,
    such as a pointer or an ID, that handle should not be dangling (referring to a
    resource that does not exist). Resources should be released when they are no longer
    needed, in the manner that corresponds to the way they were acquired.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resource Acquisition Is Initialization is an idiom; it is the dominant C++ approach
    to resource management, where each resource is owned by an object, acquired in
    the constructor, and released in the destructor of that object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An RAII object should always be created on the stack or as a data member of
    another object. When the flow of the program leaves the scope containing the RAII
    object or the larger object containing the RAII object is deleted, the destructor
    of the RAII object is executed. This happens regardless of how the control flow
    leaves the scope.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If each resource is owned by an RAII object and the RAII object does not give
    out raw handles (or the user is careful to not clone the raw handle), the handle
    can only be obtained from the RAII object and the resource is not released as
    long as that object remains.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The most frequently used is `std::unique_ptr` for memory management; `std::lock_guard`
    is used to manage mutexes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a rule, RAII objects must be non-copyable. Moving an RAII object transfers
    the ownership of the resource; the classic RAII pattern does not support this,
    so most RAII objects should be non-movable (differentiate between `std::unique_ptr`
    and `const std::unique_ptr`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RAII has difficulty handing release failures, because exceptions cannot propagate
    from the destructors, and hence there is no good way to report the failure to
    the caller. For that reason, failing to release a resource often results in undefined
    behavior (this approach is sometimes taken by the C++ standard as well).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 6, Understanding Type Erasure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Type erasure is a programming technique where the program, as written, does
    not show an explicit dependence on some of the types it uses. It is a powerful
    design tool when used for separating abstract behavior from a particular implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The implementation involves either a polymorphic object and a virtual function
    call, or a function that is implemented specifically for the erased type and is
    invoked through a function pointer. Usually, this is combined with generic programming
    to construct such polymorphic objects or generate functions from a template automatically
    and ensure that the reified type is always the same as the one provided during
    construction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A program may be written in a way that avoids explicit mention of most types.
    The types are deduced by template functions and declared as `auto` or as template-deduced
    typedef types. However, the actual types of objects that are hidden by `auto`
    still depend on all types the object operates on (such as the deleter type for
    a pointer). The erased type is not captured by the object type at all. In other
    words, if you could get the compiler to tell you what this particular auto stands
    for, all types would be explicitly there. But if the type was erased, even the
    most detailed declaration of the containing object will not reveal it (such as
    `std::shared_ptr<int>` —this is the entire type, the deleter type is not there).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The type is reified by the function that is generated for that type: while
    its signature (arguments) does not depend on the erased type, the body of the
    function does. Usually, the first step is casting one of the arguments from a
    generic pointer such as `void*` to the pointer to the erased type.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The performance of type erasure always incurs some overhead compared to invoking
    the same callable directly: there is always an extra indirection and the pointer
    associated with it. Almost all implementations use runtime polymorphism (virtual
    functions or dynamic casts) or the equivalent virtual table of function pointers,
    which increases both the time (indirect function calls) and memory (virtual pointers).
    The greatest overhead usually comes from additional memory allocations, necessary
    for storing objects whose size is not known at compile time. If such allocations
    can be minimized and the additional memory made local to the object, the total
    overhead at runtime may be quite small (the overhead in memory remains and is
    often increased by such optimizations).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 7, SFINAE, Concepts, and Overload Resolution Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For each function call, it is the set of all functions with the specified name
    that are accessible from the call location (the accessibility may be affected
    by the namespaces, nested scopes, and so on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is the process of selecting which function in the overload set is going to
    be called, given the arguments and their types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For template functions and member functions (and class constructors in C++17),
    type deduction determines the types of template parameters from the types of the
    function arguments. For each parameter, it may be possible to deduce the type
    from several arguments. In this case, the results of this deduction must be the
    same, otherwise, the type deduction fails. Once the template parameter types are
    deduced, the concrete types are substituted for the template parameters in all
    arguments, the return type, and the default arguments. This is a type substitution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type substitution, described previously, can result in invalid types, such as
    a member function pointer for a type that has no member functions. Such substitution
    failures do not generate compilation errors; instead, the failing overload is
    removed from the overload set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is only in the function declaration (return type, parameter types, and
    default values). Substitution failures in the body of the function chosen by the
    overload resolution are hard errors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If each overload returns a different type, these types can be examined at compile
    time. The types must have some way to distinguish them, for example, different
    sizes or different values of embedded constants. For `constexpr` functions, we
    can also examine the return values (the function needs the body in this case).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is used with great care and caution. By deliberately causing substitution
    failures, we can direct the overload resolution toward a particular overload.
    Generally, the desired overload is preferred unless it fails; otherwise, the variadic
    overload remains and is chosen, indicating that the expression we wanted to test
    was invalid. By differentiating between the overloads using their return types,
    we can generate a compile-time (`constexpr`) constant that can be used in conditional
    compilation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C++20 constraints offer a more natural, easier-to-understand syntax. They also
    result in much clearer error messages when a called function does not meet the
    requirements. Also, unlike SFINAE, constraints are not limited to the substituted
    parameters of function templates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The standard did not just define concepts and constraints in the language. It
    also offers a way of thinking about template restrictions. While a wide range
    of SFINAE-based techniques exists, the code is easier to read and maintain if
    the use of SFINAE is restricted to a few powerful approaches that resemble the
    use of concepts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 8, The Curiously Recurring Template Pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While not very expensive in absolute numbers (a few nanoseconds at most), a
    virtual function call is several times more expensive than a non-virtual one,
    and could easily be an order of magnitude or more slower than an inlined function
    call. The overhead comes from the indirection: a virtual function is always invoked
    by a function pointer, and the actual function is unknown at compile time and
    cannot be inlined.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the compiler knows the exact function that is going to be called, it can
    optimize away the indirection and may be able to inline the function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Just like the runtime polymorphic calls are made through the pointer to the
    base class, the static polymorphic calls must be also made through a pointer or
    reference to the base class. In the case of CRTP and static polymorphism, the
    base type is actually a whole collection of types generated by the base class
    template, one for each derived class. To make a polymorphic call we have to use
    a function template that can be instantiated on any of these base types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the derived class is called directly, the use of CRTP is quite different
    from the compile-time equivalent of the virtual functions. It becomes an implementation
    technique, where common functionality is provided to multiple derived classes,
    and each one expands and customizes the interface of the base class template.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Strictly speaking, nothing new is needed to use multiple CRTP bases: the derived
    class can inherit from several such base types, each an instantiation of a CRTP
    base class template. However, listing these based together with the correct template
    parameter (derived class itself) for each one becomes cumbersome. It is easier
    and less error-prone to declare the derived class as a variadic template with
    a template template parameter and inherit from the entire parameter pack.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 9, Named Arguments, Method Chaining, and the Builder Pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is easy to miscount arguments, change the wrong argument, or use an argument
    of the wrong type that happens to convert to the parameter type. Also, adding
    a new parameter requires changing all function signatures that must pass these
    parameters along.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The argument values within the aggregate have explicit names. Adding a new value
    does not require changing the function signatures. Classes made for different
    groups of arguments have different types and cannot be accidentally mixed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The named argument idiom permits the use of temporary aggregate objects. Instead
    of changing each data member by name, we write a method to set the value of each
    argument. All such methods return a reference to the object itself and can be
    chained together in one statement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Method cascading applies multiple methods to the same object. In a method chain,
    in general, each method returns a new object and the next method applies to it.
    Often, method chaining is used to cascade methods. In this case, all chained methods
    return the reference to the original object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Builder pattern is a design pattern that uses a separate builder object
    to construct complex objects. It is used when a constructor is not sufficient
    or not easy to use to construct an object in its desired fully built state. The
    need for a builder can arise when the constructor of the object being built cannot
    be modified (a general object used for a particular purpose), when a desired constructor
    would have many similar arguments and be hard to use when the construction process
    is complex, or when the construction process is computationally expensive but
    some of the results can be reused.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fluent interface is an interface that uses method chaining to present multiple
    instructions, commands, or operations that can be executed on an object. In particular,
    fluent builders are used in C++ to split complex object construction into multiple
    smaller steps. Some of these steps can be conditional or depend on other data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 10, Local Buffer Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Micro-benchmarks can measure the performance of small fragments of code in isolation.
    To measure the performance of the same fragment in the context of a program, we
    have to use a profiler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Processing small amounts of data usually involve a correspondingly small amount
    of computing and are therefore very fast. Memory allocation adds a constant overhead,
    not proportional to the data size. The relative impact is larger when the processing
    time is short. In addition, memory allocation may use a global lock or otherwise
    serialize multiple threads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Local buffer optimization replaces external memory allocation with a buffer
    that is a part of the object itself. This avoids the cost, and the overhead, of
    an additional memory allocation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The object has to be constructed and the memory for it must be allocated, regardless
    of whether any secondary allocations happen. This allocation has some cost – more
    if the object is allocated on the heap and less if it’s a stack variable – but
    that cost must be paid before the object can be used. Local buffer optimization
    increases the size of the object and therefore of the original allocation, but
    that usually does not significantly affect the cost of that allocation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Short string optimization involves storing string characters in a local buffer
    contained inside the string object, up to a certain length of the string.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Small vector optimization involves storing a few elements of the vector’s content
    in a local buffer contained in the vector object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 11, ScopeGuard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An error-safe program maintains a well-defined state (a set of invariants) even
    if it encounters an error. Exception safety is a particular kind of error safety;
    it assumes that errors are signaled by throwing expressions. The program must
    not enter an undefined state when an (allowed) expression is thrown. An exception-safe
    program may require that certain operations do not throw exceptions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a consistent state must be maintained across several actions, each of which
    may fail, then the prior actions must be undone if a subsequent action fails.
    This often requires that the actions do not commit fully until the end of the
    transaction is reached successfully. The final commit operation must not fail
    (for example, throw an exception), otherwise error safety cannot be guaranteed.
    The rollback operation also must not fail.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RAII classes ensure that a certain action is always taken when the program leaves
    a scope, such as a function. With RAII, the closing action cannot be skipped or
    bypassed, even if the function exits the scope prematurely with an early return
    or by throwing an exception.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The classic RAII needs a special class for every action. ScopeGuard automatically
    generates an RAII class from an arbitrary code fragment (at least, if lambda expressions
    are supported).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the status is returned through error codes, it cannot. If all errors in the
    program are signaled by exceptions and any return from a function is a success,
    we can detect at runtime whether an exception was thrown. The complication is
    that the guarded operation may itself take place during stack unwinding caused
    by another exception. That exception is propagating when the guard class has to
    decide whether the operation succeeded or failed, but its presence does not indicate
    the failure of the guarded operation (it may indicate that something else failed
    elsewhere). Robust exception detection must keep track of how many exceptions
    are propagating at the beginning and at the end of the guarded scope, which is
    possible only in C++17 (or using compiler extensions).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ScopeGuard classes are usually template instantiations. This means that
    the concrete type of the ScopeGuard is unknown to the programmer, or at least
    difficult to specify explicitly. The ScopeGuard relies on lifetime extension and
    template argument deduction to manage this complexity. A type-erased ScopeGuard
    is a concrete type; it does not depend on the code it holds. The downside is that
    type erasure requires runtime polymorphism and, most of the time, a memory allocation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 12, Friend Factory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A non-member friend function has the same access to the members of the class
    as a member function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Granting friendship to a template makes every instantiation of this template
    a friend; this includes instantiations of the same template but with different,
    unrelated, types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Binary operators implemented as member functions are always called on the left-hand-side
    operand of the operator, with no conversions allowed for that object. Conversions
    are allowed for the right-hand-side operand, according to the type of the argument
    of the member operator. This creates an asymmetry between expressions such as
    `x + 2` and `2 + x`, where the latter cannot be handled by a member function since
    the type of `2` (`int`) does not have any.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first operand of the inserter is always the stream, not the object that
    is printed. Therefore, a member function would have to be on that stream, which
    is a part of the standard library; it cannot be extended by the user to include
    user-defined types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While the details are complex, the main difference is that user-defined conversions
    (implicit constructors and conversion operators) are considered when calling non-template
    functions but, for template functions, the argument types must match the parameter
    types (almost) exactly, and no user-defined conversions are permitted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining an in situ friend function (with the definition immediately following
    the declaration) in a class template causes every instantiation of that template
    to generate one non-template, non-member function with the given name and parameter
    types in the containing scope.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 13, Virtual Constructors and Factories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several reasons, but the simplest is that the memory must be allocated
    in the amount `sizeof(T)`, where `T` is the actual object type, and the `sizeof()`
    operator is `constexpr` (a compile-time constant).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Factory pattern is a creational pattern that solves the problem of creating
    objects without having to explicitly specify the type of the object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While in C++ the actual type has to be specified at the construction point,
    the Factory pattern allows us to separate the point of construction from the place
    where the program has to decide what object to construct and identify the type
    using some alternative identifier, a number, a value, or another type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The virtual copy constructor is a particular kind of factory where the object
    to construct is identified by the type of another object we already have. A typical
    implementation involves a virtual `clone()` method that is overridden in every
    derived class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Template pattern describes the design where the overall control flow is
    dictated by the base class, with derived classes providing customizations at certain
    predefined points. In our case, the overall control flow is that of factory construction,
    and the customization point is the act of construction of an object (memory allocation
    and constructor invocation).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Builder pattern is used when it is necessary (or just more convenient) to
    delegate the work of constructing an object to another class instead of doing
    the complete initialization in the constructor. An object that constructs other
    objects of different types depending on some run-time information using the factory
    method is also a builder. In addition to the factory itself, such builders, or
    factory classes, usually have other run-time data that is used for constructing
    objects and must be stored in another object – in our case, the factory object,
    which is also the builder object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "Chapter 14, The Template Method Pattern and the \LNon-Virtual Idiom"
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A behavioral pattern describes a way to solve a common problem by using a specific
    method to communicate between different objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The template method pattern is a standard way to implement an algorithm that
    has a rigid *skeleton,* or the overall flow of control, but allows for one or
    more customization points for specific kinds of problems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Template Method lets the sub-classes (derived types) implement specific
    behaviors of the otherwise generic algorithm. The key to this pattern is the way
    the base and the derived types interact.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The more common hierarchical approach to design sees the low-level code provide
    *building blocks* from which the high-level code builds the specific algorithm,
    by combining them in a particular flow of control. In the template pattern, the
    high-level code does not determine the overall algorithm and is not in control
    of the overall flow. The lower-level code controls the algorithm and determines
    when the high-level code is called to adjust specific aspects of the execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is a pattern where the public interface of a class hierarchy is implemented
    by non-virtual public methods of the base class and the derived classes contain
    only virtual private methods (as well as any necessary data and non-virtual methods
    needed to implement them).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A public virtual function performs two separate tasks – it provides the interface
    (since it is public) and also modifies the implementation. A better separation
    of concerns is to use virtual functions only to customize the implementation and
    to specify the common interface using the non-virtual functions of the base class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the NVI is employed, virtual functions can usually be made private. One
    exception is when the derived class needs to invoke a virtual function of the
    base class to delegate part of the implementation. In this case, the function
    should be made protected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Destructors are called in *nested* order, starting from the most derived class.
    When the destructor for the derived class is done, it calls the destructor of
    the base class. By that time, the *extra* information that the derived class contained
    is already destroyed, and only the base portion is left. If the base class destructor
    were to call a virtual function, it would have to be dispatched to the base class
    (since the derived class is gone by then). There is no way for the base class
    destructor to call the virtual functions of the derived class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fragile base class problem manifests itself when a change to the base class
    unintentionally breaks the derived class. While not specific to the template method,
    it affects, potentially, all object-oriented designs, including ones based on
    the template pattern. In the simplest example, changing the non-virtual public
    function in the base class, in a way that changes the names of the virtual functions
    called to customize the behavior of the algorithm, will break all existing derived
    classes because their current customizations, implemented by the virtual functions
    with the old names, would suddenly stop working. To avoid this problem, the existing
    customization points should not be changed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 15, Policy-Based Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Strategy pattern is a behavioral pattern that allows the user to customize
    a certain aspect of the behavior of the class by selecting an algorithm that implements
    this behavior from a set of provided alternatives, or by providing a new implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While the traditional OOP Strategy applies at runtime, C++ combines generic
    programming with the Strategy pattern in a technique known as policy-based design.
    In this approach, the primary class template delegates certain aspects of its
    behavior to the user-specified policy types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In general, there are almost no restrictions on the policy type, although the
    particular way in which the type is declared and used imposes certain restrictions
    by convention. For example, if a policy is invoked as a function, then any callable
    type can be used. On the other hand, if a specific member function of the policy
    is called, the policy must necessarily be a class and provide the required member
    function. Template policies can be used as well but must match the specified number
    of template parameters exactly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The two primary ways are composition and inheritance. The composition should
    generally be preferred; however, many policies in practice are empty classes with
    no data members and can benefit from empty base class optimization. Private inheritance
    should be preferred unless the policy must also modify the public interface of
    the primary class. Policies that need to operate on the primary policy-based class
    itself often have to employ CRTP. In other cases, when the policy object itself
    does not depend on the types used in the construction of the primary template,
    the policy behavior can be exposed through a static member function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As a general rule, policies that contain only constants and are used to constrain
    the public interface are easier to write and maintain. However, there are several
    cases when injecting public member functions through the base class policies is
    preferred: when we also need to add member variables to the class or when the
    complete set of public functions would be difficult to maintain or lead to conflicts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The primary drawback is complexity, in various manifestations. Policy-based
    types with different policies are, generally, different types (the only alternative,
    type erasure, usually carries a prohibitive runtime overhead). This may force
    large parts of the code to be templated as well. Long lists of policies are difficult
    to maintain and use correctly. For this reason, care should be taken to avoid
    creating unnecessary or hard-to-justify policies. Sometimes a type with two sufficiently
    unrelated sets of policies is better to be split into two separate types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 16, Adapters and Decorators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Adapter is a very general pattern that modifies an interface of a class
    or a function (or a template, in C++) so it can be used in a context that requires
    a different interface but similar underlying behavior.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Decorator pattern is a more narrow pattern; it modifies the existing interface
    by adding or removing behavior but does not convert an interface into a completely
    different one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the classic OOP implementation, both the decorated class and the Decorator
    class inherit from a common base class. This has two limitations; the most important
    one is that the decorated object preserves the polymorphic behavior of the decorated
    class but cannot preserve the interface that is added in a concrete (derived)
    decorated class and was not present in the base class. The second limitation is
    that the Decorator is specific to a particular hierarchy. We can remove both limitations
    using the generic programming tools of C++.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In general, a Decorator preserves as much of the interface of the decorated
    class as possible. Any functions the behavior of which is not modified are left
    unchanged. For that reason, public inheritance is commonly used. If a Decorator
    has to forward most calls to the decorated class explicitly, then the inheritance
    aspect is less important, and composition or private inheritance can be used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unlike Decorators, adapters usually present a very different interface from
    that of the original class. Composition is often preferred in this case. The exception
    is compile-time adapters that modify the template parameters but otherwise are
    essentially the same class template (similar to template aliases). These adapters
    must use public inheritance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The main limitation is that it cannot be applied to template functions. It also
    cannot be used to replace function arguments with expressions containing those
    arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Template aliases are never considered by the argument type deduction when function
    templates are instantiated. Both adapter and policy patterns can be used to add
    or modify the public interface of a class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adapters are easy to stack (compose) to build a complex interface one function
    at a time. The features that are not enabled do not need any special treatment
    at all; if the corresponding adapter is not used, then that feature is not enabled.
    The traditional policy pattern requires predetermined slots for every pattern.
    With the exception of the default arguments after the last explicitly specified
    one, all policies, even the default ones, must be explicitly specified. On the
    other hand, the adapters in the middle of the stack do not have access to the
    final type of the object, which complicates the implementation. The policy-based
    class is always the final type, and using CRTP, this type can be propagated into
    the policies that need it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 17, The Visitor Pattern and Multiple Dispatch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Visitor pattern provides a way to separate the implementation of algorithms
    from the objects they operate on; in other words, it is a way to add operations
    to classes without modifying them by writing new member functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Visitor pattern allows us to extend the functionality of class hierarchies.
    It can be used when the source code of the class is not available for modification
    or when such modifications would be difficult to maintain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Double dispatch is the process of dispatching a function call (selecting the
    algorithm to run) based on two factors. Double dispatch can be implemented at
    runtime using the Visitor pattern (virtual functions provide the single dispatch)
    or at compile time using templates or compile-time visitors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The classic visitor has a circular dependency between the visitor class hierarchy
    and the visitable class hierarchy. While the visitable classes do not need to
    be edited when a new visitor is added, they do need to be recompiled when the
    visitor hierarchy changes. The latter must happen every time a new visitable class
    is added, hence a dependency circle. The Acyclic Visitor breaks this circle by
    using cross-casting and multiple inheritance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A natural way to accept a visitor into an object composed of smaller objects
    is to visit each of these objects one by one. This pattern, implemented recursively,
    ends up visiting every built-in data member contained in an object and does so
    in a fixed, predetermined order. Hence, the pattern maps naturally onto the requirement
    for serialization and deserialization, where we must deconstruct an object into
    a collection of built-in types, then restore it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 18, Patterns for Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency is the property of a program that allows multiple tasks to execute
    at the same time or partially overlapping in time. Usually, concurrency is achieved
    through the use of multiple threads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'C++11 has the basic support for writing concurrent programs: threads, mutexes,
    and condition variables. C++14 and C++17 added several convenience classes and
    utilities, but the next major addition to concurrency features of C++ is C++20:
    here, we have several new synchronization primitives as well as the introduction
    of the coroutines.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Synchronization patterns are the common solutions to the basic problems of accessing
    shared data. Usually, they provide ways to arrange exclusive access to the data
    that is modified by multiple threads or is accessed by some threads while being
    modified by others.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execution patterns are the standard ways to arrange asynchronous execution of
    some computations using one or more threads. These patterns offer ways to initiate
    the execution of some code and receive the results of this execution without the
    caller being responsible for the execution itself (some other entity in the program
    has that duty).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The most important guideline for design for concurrency is modularity; when
    applies specifically to concurrency, it means building concurrent software from
    components that satisfy certain restrictions on their behavior in concurrent programs.
    The most important of these restrictions is the thread safety guarantee: generally,
    it is much easier to build concurrent software from components that allow a wide
    range of thread-safe operations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In order to be useful in concurrent programs, any data structure or component
    must provide a transactional interface. An operation is a transaction if it performs
    a well-defined complete computation and leaves the system in a well-defined state.
    A simplified way to identify which operations are and are not transactions is
    this: if a concurrent program executes each operation under lock but there are
    no order guarantees between the operations, is the state of the system guaranteed
    to be well-defined? If it isn’t, then some of the operations should be executed
    as a sequence without releasing the lock. That sequence is a transaction; the
    operations that are the steps of the sequence are not transactions by themselves.
    It should not be the responsibility of the caller to arrange these operations
    in a sequence. Instead, the data structure or component should offer the interface
    for performing the entire transaction as a single operation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
