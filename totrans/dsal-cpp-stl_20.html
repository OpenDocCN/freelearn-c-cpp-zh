<html><head></head><body>
		<div><h1 id="_idParaDest-696" class="chapter-number"><a id="_idTextAnchor696"/>20</h1>
			<h1 id="_idParaDest-697"><a id="_idTextAnchor697"/>Thread Safety and Concurrency with the STL</h1>
			<p>This chapter explores concurrency within the C++ <strong class="bold">Standard Template Library</strong> (<strong class="bold">STL</strong>). The chapter begins by building a solid foundational understanding of thread safety, race conditions, and their inherent risks. We then shift to the STL, decoding its thread safety guarantees and spotlighting its potential pitfalls. As we proceed, readers will gain insights into the array of synchronization tools available in C++, mastering their application to safeguard STL containers in multi-threaded environments. Upon concluding this chapter, readers can ensure data consistency and stability in concurrent C++ applications.</p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>Concurrency versus thread safety</li>
				<li>Understanding thread safety</li>
				<li>Race conditions</li>
				<li>Mutexes and locks</li>
				<li>STL containers and thread safety</li>
				<li>Specific container concerns</li>
				<li>Concurrency support within the STL</li>
				<li>Using <code>std::thread</code>, <code>std::async</code>, <code>std::future</code>, and thread-local storage</li>
				<li>Concurrent data structures in the STL</li>
			</ul>
			<h1 id="_idParaDest-698"><a id="_idTextAnchor698"/>Technical requirements</h1>
			<p>The code in this chapter can be found on GitHub:</p>
			<p><a href="https://github.com/PacktPublishing/Data-Structures-and-Algorithms-with-the-CPP-STL">https://github.com/PacktPublishing/Data-Structures-and-Algorithms-with-the-CPP-STL</a></p>
			<h1 id="_idParaDest-699"><a id="_idTextAnchor699"/>Concurrency versus thread safety</h1>
			<p><strong class="bold">Concurrency</strong> is the<a id="_idIndexMarker1067"/> concept of multiple tasks executing in overlapping periods. These tasks can either run at the same time on different processing units or might interleave on a single processing unit. The main goal of concurrency is to increase the system’s responsiveness and throughput. Concurrency is beneficial in various scenarios, such as when designing servers that handle multiple simultaneous client requests or in user interfaces that must remain responsive while processing tasks in the background.</p>
			<p>In C++, concurrency <a id="_idIndexMarker1068"/>can manifest in multiple forms: multi-threading, where separate threads of execution run (potentially) in parallel, or asynchronous programming, in which specific tasks are offloaded to be executed later.</p>
			<p>In C++, it’s crucial to understand that <a id="_idIndexMarker1069"/>concurrency and <strong class="bold">thread safety</strong> are related but distinct concepts. Concurrency refers to the program’s ability to execute multiple sequences of operations simultaneously, which can be achieved through multi-threading or other parallel execution techniques. However, being concurrent does not inherently guarantee thread safety. Thread safety is the property that ensures code functions correctly when accessed by multiple threads concurrently. This involves carefully managing shared resources, synchronizing data access, and avoiding race conditions. Achieving thread safety in a concurrent environment is a challenging aspect of C++ programming. It requires deliberate design choices and the use of specific mechanisms, such as mutexes, locks, and atomic operations, to prevent data corruption and ensure consistent behavior across all threads.</p>
			<h2 id="_idParaDest-700"><a id="_idTextAnchor700"/>Thread safety – a pillar for stable concurrency</h2>
			<p><strong class="bold">Thread safety</strong> refers <a id="_idIndexMarker1070"/>to the capability of a piece of code to function correctly when accessed by multiple threads concurrently. It ensures that shared data maintain their integrity and that the results remain consistent. Thread safety doesn’t inherently mean a function or method is lock-free or lacks performance bottlenecks; instead, it signifies that concurrent access won’t lead to unpredictable results or compromise data.</p>
			<p>Consider an analogy: If <strong class="bold">concurrency</strong> were akin to a busy city intersection, then thread safety would be the traffic signals ensuring that cars (threads) don’t crash into each other.</p>
			<h2 id="_idParaDest-701"><a id="_idTextAnchor701"/>The interplay of concurrency and thread safety</h2>
			<p>While <a id="_idIndexMarker1071"/>both concepts are intertwined, they serve different purposes. Concurrency focuses on designing systems to perform multiple tasks in overlapping time frames, aiming for improved performance and responsiveness. Thread safety, on the other hand, is all about correctness. It’s about ensuring they don’t step on each other’s toes when these concurrent tasks interact with shared resources.</p>
			<p>Let’s consider a simple example: a counter class in C++. Concurrency might involve incrementing the counter’s value from multiple threads. However, if the counter’s increment operation isn’t thread-safe, two threads might read the same value simultaneously, increment it, and then write back the same incremented value. In such a case, despite trying to be faster using concurrency, the counter would end up missing counts, leading to incorrect results.</p>
			<h2 id="_idParaDest-702"><a id="_idTextAnchor702"/>Challenges and rewards</h2>
			<p>Introducing concurrency can undoubtedly make applications faster and more responsive. However, it also introduces complexities. Managing multiple threads with issues such as deadlocks and race conditions can be challenging.</p>
			<p>But, when done right, the rewards are substantial. Programs become more efficient, potentially utilizing all available processing units fully. Applications can be more responsive, leading to improved user experiences. Concurrent programming is no longer a choice but is necessary for many modern high-performance applications.</p>
			<h2 id="_idParaDest-703"><a id="_idTextAnchor703"/>Concurrency without thread safety – a recipe for chaos</h2>
			<p>Imagine a <a id="_idIndexMarker1072"/>world where every task tries to execute itself as fast as possible without coordination. In such a world, tasks might collide, disrupt each other, and produce nonsensical outcomes. That’s what concurrent programming without thread safety looks like. It’s a realm where speed is prioritized over correctness, often leading to chaos.</p>
			<p>As a C++ developer, the key is to find the right balance. While striving for high concurrency to make applications fast, investing in thread safety mechanisms is equally crucial to ensure <a id="_idIndexMarker1073"/>correctness.</p>
			<p>Understanding the difference between concurrency and thread safety sets the stage for the following sections. We’ll be looking at the tools and constructs provided by the STL to achieve high concurrency and ensure thread safety.</p>
			<h1 id="_idParaDest-704"><a id="_idTextAnchor704"/>Understanding thread safety</h1>
			<p>Executing multiple<a id="_idIndexMarker1074"/> simultaneous tasks can lead to boosted performance and responsiveness. Ensuring thread safety, especially when using the STL, becomes paramount. If overlooked, the dream of seamless concurrency can quickly morph into the nightmare of data inconsistency and unpredictable behavior.</p>
			<h2 id="_idParaDest-705"><a id="_idTextAnchor705"/>Thread safety in STL containers – laying the groundwork</h2>
			<p>The allure <a id="_idIndexMarker1075"/>of the STL lies in its rich ensemble of containers, which offer a smooth experience for storing and managing data. But the moment we introduce multiple threads, potential dangers loom.</p>
			<p>Thread safety is primarily about ensuring that your code behaves predictably and correctly when accessed by multiple threads, even when those threads overlap. For STL containers, the basic guarantee is simple: simultaneous read-only access to containers is safe. However, once you introduce writes (modifications), things get intricate.</p>
			<p>It’s critical to understand that while STL containers have thread-safe read operations, write operations don’t. If one thread is updating a container, no other thread should be reading or writing to it. Otherwise, we’re courting disaster or, in technical jargon, <strong class="bold">undefined behavior</strong>.</p>
			<h2 id="_idParaDest-706"><a id="_idTextAnchor706"/>Grasping the thread-safe nature of STL algorithms</h2>
			<p>If STL containers <a id="_idIndexMarker1076"/>are the soul of the library, algorithms are undoubtedly its beating heart. They’re responsible for the STL’s rich functionality, from searching and sorting to transforming data.</p>
			<p>Here’s the catch: STL algorithms are functions, and their thread safety isn’t determined by the algorithm itself but by the data they operate on. If an algorithm operates on shared data across threads without adequate synchronization, you’re setting the stage for race conditions, even if that algorithm only reads data.</p>
			<p>Consider the <a id="_idIndexMarker1077"/>scenario where you’re using <code>std::find</code> across multiple threads. While the algorithm is inherently safe for concurrent read operations, the results could be skewed if another thread modifies the data during the search.</p>
			<h2 id="_idParaDest-707"><a id="_idTextAnchor707"/>Race conditions – the ghosts in the machine</h2>
			<p>Race conditions<a id="_idIndexMarker1078"/> keep concurrent programmers up at night. A <strong class="bold">race condition</strong> occurs <a id="_idIndexMarker1079"/>when the behavior of your software depends on the relative timing of events, such as the order in which threads are scheduled. The consequences range from benign (slightly incorrect data) to catastrophic (complete data corruption or application crashes).</p>
			<p>Using the STL in a multi-threaded environment without the proper precautions can introduce race conditions. For instance, imagine two threads simultaneously pushing elements onto a <code>std::vector</code>. Without synchronization, the internal memory of the vector could become corrupted, leading to a host of problems.</p>
			<p>Let’s look at a simple race condition. In this example, we will use two threads to increment a counter:</p>
			<pre class="source-code">
#include &lt;iostream&gt;
#include &lt;thread&gt;
// Shared variable
int counter = 0;
// Function that increments the counter
void incrementCounter() {
  for (int i = 0; i &lt; 100000; ++i) {
    ++counter; // Race condition occurs here
  }
}
int main() {
  // Creating two threads that run incrementCounter()
  std::thread thread1(incrementCounter);
  std::thread thread2(incrementCounter);
  // Wait for both threads to finish
  thread1.join();
  thread2.join();
  // Print the final value of counter
  std::cout &lt;&lt; "Final value of counter is: " &lt;&lt; counter
            &lt;&lt; std::endl;
  return 0;
}</pre>			<p>Here is a possible example output:</p>
			<pre class="console">
Final value of counter is: 130750</pre>			<p>A race condition <a id="_idIndexMarker1080"/>occurs because both threads access and modify the shared variable counter simultaneously without any synchronization mechanism (such as <strong class="bold">mutexes</strong> or <strong class="bold">locks</strong>). Due to the lack of synchronization, the two threads may read, increment, and write back the value of the counter in an unpredictable order. This leads to the final value of the counter being unpredictable and usually less than the expected 200,000, as some increments are lost. Running this program multiple times will likely yield different results for the final value of the counter due to the race condition. To resolve this issue, proper synchronization mechanisms, such as mutexes, should be used to ensure that only one thread modifies the shared variable at a time.</p>
			<h2 id="_idParaDest-708"><a id="_idTextAnchor708"/>Safeguarding concurrency – the way forward</h2>
			<p>It’s evident <a id="_idIndexMarker1081"/>that merely understanding thread safety is half the battle. As we progress through this chapter, we’ll arm you with the tools and techniques to tackle race conditions head-on, master the synchronization mechanisms at your disposal, and ensure that your STL-powered multi-threaded applications stand as bastions of stability and consistency.</p>
			<h1 id="_idParaDest-709"><a id="_idTextAnchor709"/>Race conditions</h1>
			<p>A race condition<a id="_idIndexMarker1082"/> in programming occurs when the behavior of a system depends on the relative timing of multiple threads or processes. In such scenarios, the system’s outcome becomes unpredictable because different threads may access and modify shared data concurrently without proper synchronization. This can lead to inconsistent or erroneous results, as the final state of the data depends on the order in which the threads execute, which cannot be determined in advance. Race conditions are a common issue in concurrent programming. They can be particularly challenging to detect and resolve, requiring careful design and synchronization mechanisms to ensure correct and predictable program behavior.</p>
			<h2 id="_idParaDest-710"><a id="_idTextAnchor710"/>Steering clear of a silent peril – race conditions in the STL</h2>
			<p>As you <a id="_idIndexMarker1083"/>journey into concurrent programming, race conditions represent one of the most subtle yet treacherous pitfalls. Though silent in their manifestation, they can cause unexpected and, at times, bewildering results. Recognizing and sidestepping these race conditions, especially within the realm of the STL, is crucial to crafting robust multi-threaded applications.</p>
			<h2 id="_idParaDest-711"><a id="_idTextAnchor711"/>The anatomy of a race condition in the STL</h2>
			<p>At its core, a<a id="_idIndexMarker1084"/> race condition materializes when the behavior of your application hinges on the sequence or timing of uncontrollable events. In the STL context, this typically arises when multiple threads access shared data in an uncoordinated fashion.</p>
			<p>Imagine a scenario where two threads, in an unfortunate coincidence, try to insert elements into the same position of <code>std::vector</code> concurrently or consider another instance where<a id="_idIndexMarker1085"/> one thread reads from <code>std::unordered_map</code> while another erases an element. What is the outcome? Undefined behavior, which in the world of C++, is the equivalent of opening Pandora’s box.</p>
			<h2 id="_idParaDest-712"><a id="_idTextAnchor712"/>More than meets the eye</h2>
			<p>Race conditions are especially treacherous due to their unpredictable nature. While a concurrent application may seem to work flawlessly in one run, slight changes in thread execution timings can lead to entirely different results in the next.</p>
			<p>Beyond erratic behavior, race conditions with STL containers and algorithms can lead to more sinister problems. Data corruption, memory leaks, and crashes are just the tip of the iceberg. Given their elusive and intermittent appearance, these issues can be challenging to debug.</p>
			<h2 id="_idParaDest-713"><a id="_idTextAnchor713"/>Anticipating race conditions</h2>
			<p>Forewarned is<a id="_idIndexMarker1086"/> forearmed. By familiarizing yourself with common scenarios where race conditions manifest in the STL, you position yourself to tackle them preemptively:</p>
			<ul>
				<li><code>std::vector</code> and <code>std::string</code>, automatically resize when their capacity is exceeded. If two threads simultaneously trigger a resize, the internal state could be left in turmoil.</li>
				<li><strong class="bold">Iterator invalidation</strong>: Modifying containers often invalidates existing iterators. If one thread traverses using an iterator while another modifies the container, the first thread’s iterator can end up in no-man’s-land.</li>
				<li><strong class="bold">Algorithm assumptions</strong>: STL algorithms make certain assumptions about the data they operate upon. Concurrent modifications can violate these assumptions, leading to incorrect results or infinite loops.</li>
			</ul>
			<h2 id="_idParaDest-714"><a id="_idTextAnchor714"/>Safeguarding your code – a proactive stance</h2>
			<p>Having <a id="_idIndexMarker1087"/>acquainted ourselves with the potential hotspots, the natural progression is to fortify our code against these hazards. The essence lies in synchronization. We can effectively thwart race conditions by ensuring that only one thread can access shared data or perform certain operations simultaneously.</p>
			<p>However, indiscriminate synchronization can lead to performance bottlenecks, rendering the benefits of concurrency moot. The key is to strike a balance, applying synchronization judiciously.</p>
			<p>We’ll introduce a robust arsenal of tools and techniques as we move further into this chapter. From mutexes to locks, you’ll acquire the means to detect and effectively neutralize race conditions, ensuring your STL-driven applications are swift and steadfast.</p>
			<p>Are you ready to conquer the challenges of concurrent programming with the STL? Let’s navigate this landscape together, ensuring your software remains consistent, reliable, and race condition-free.</p>
			<h1 id="_idParaDest-715"><a id="_idTextAnchor715"/>Mutexes and locks</h1>
			<p>A <strong class="bold">mutex</strong>, short<a id="_idIndexMarker1088"/> for <strong class="bold">mutual exclusion</strong>, is akin to a digital gatekeeper. It regulates access, ensuring that at any given moment, only a single thread can enter its protected domain, eliminating the chaos of concurrent access. Imagine a high-stakes auction room where only one person can place a bid at any instant, thereby preventing overlap and conflict. That’s the function of a mutex in the world of multi-threaded applications.</p>
			<p>Within the C++ Standard Library, the header <code>&lt;mutex&gt;</code> bestows several types of mutexes upon us. The most commonly used among them is <code>std::mutex</code>. This basic mutex is a versatile tool suitable for many synchronization needs. A pair of operations—<code>lock()</code> and <code>unlock()</code>—provides a straightforward means to guard shared resources.</p>
			<h2 id="_idParaDest-716"><a id="_idTextAnchor716"/>From manual to automatic – lock guards and unique locks</h2>
			<p>Manually locking and unlocking mutexes can be error-prone. There’s always the lurking danger of<a id="_idIndexMarker1089"/> forgetting to unlock a mutex, leading to a deadlock. Enter lock guards and <a id="_idIndexMarker1090"/>unique locks; these simplify mutex management by embracing the <strong class="bold">resource acquisition is initialization</strong> (<strong class="bold">RAII</strong>) principle.</p>
			<p><code>std::lock_guard</code> is a lightweight wrapper that automatically manages the mutex’s state. Once a lock guard acquires a mutex, it guarantees its release when the lock guard’s scope ends. This eliminates the risk of forgetting to release the mutex.</p>
			<p>On the other hand, <code>std::unique_lock</code> is a bit more flexible. Besides the automatic lock management that <code>lock_guard</code> offers, <code>unique_lock</code> provides manual control, deferred locking, and even the ability to transfer ownership of a mutex. This makes it suitable for more complex synchronization scenarios.</p>
			<h2 id="_idParaDest-717"><a id="_idTextAnchor717"/>Avoiding the stalemate – deadlock prevention</h2>
			<p>Imagine a <a id="_idIndexMarker1091"/>scenario where two threads are in a standoff, each expecting the other to relinquish a resource. As a result, both are stuck in a perpetual waiting, leading to a classic deadlock. This situation isn’t merely hypothetical, especially when mutexes are involved, as they can inadvertently create such a deadlock if not managed carefully. When multiple mutexes are involved, it is essential to adopt strategies to avoid deadlocks. One common approach is always to acquire the mutexes in the same order, regardless of which thread you are in. But when this isn’t feasible, <code>std::lock</code> comes to the rescue. It’s designed to lock multiple mutexes simultaneously without the risk of causing a deadlock.</p>
			<h2 id="_idParaDest-718"><a id="_idTextAnchor718"/>Incorporating mutexes with STL containers</h2>
			<p>With the<a id="_idIndexMarker1092"/> knowledge of mutexes, lock guards, unique locks, and deadlock prevention techniques, integrating these synchronization tools with STL containers becomes an intuitive exercise.</p>
			<p>For instance, protecting <code>std::vector</code> from concurrent access might involve placing <code>std::lock_guard</code> at every function that modifies or accesses the vector. Similarly, if multiple operations on <code>std::unordered_map</code> must be executed atomically, <code>std::unique_lock</code> can offer protection and the flexibility to manually control the lock’s state when needed.</p>
			<p>With the tools of mutexes and locks in hand, threading in the STL no longer feels like treading on thin ice. By ensuring the reasonable and consistent application of these synchronization primitives, you can harness the full power of concurrency while keeping the pitfalls of <a id="_idIndexMarker1093"/>race conditions and deadlocks at bay.</p>
			<p>In the following sections, we’ll continue our exploration, specifically focusing on the unique challenges and considerations when threading with specific STL containers.</p>
			<h1 id="_idParaDest-719"><a id="_idTextAnchor719"/>STL containers and thread safety</h1>
			<p>When<a id="_idIndexMarker1094"/> discussing STL containers, assuming a blanket level of thread safety across all of them is tempting. However, such assumptions can be misleading. By default, STL containers are not thread-safe for modifications, meaning if one thread modifies a container, other threads simultaneously accessing it might lead to undefined behavior.</p>
			<p>However, some inherent guarantees exist. For instance, it is safe for multiple threads to simultaneously read from an STL container, as long as no thread is modifying it. This is often referred <a id="_idIndexMarker1095"/>to as <strong class="bold">read concurrency</strong>. Yet, the moment even a single thread tries to change the container while others read, we’re back in the dangerous territory of race conditions.</p>
			<h2 id="_idParaDest-720"><a id="_idTextAnchor720"/>When safety needs reinforcements – concurrent modifications</h2>
			<p>While<a id="_idIndexMarker1096"/> reading concurrently is safe, modifications bring a different set of challenges. Suppose two or more threads attempt to modify an STL container simultaneously. In that case, the behavior becomes undefined unless synchronization mechanisms (such as those we explored with mutexes and locks) are used.</p>
			<p>Take the case of <code>std::vector</code>. A race condition emerges if one thread appends an element using <code>push_back</code> while another tries to remove one with <code>pop_back</code> without a mutex guarding these operations. The vector’s size could change mid-operation, or memory could be reallocated, leading to crashes or data inconsistencies.</p>
			<h2 id="_idParaDest-721"><a id="_idTextAnchor721"/>Container iterators – the fragile bridge</h2>
			<p>Iterators<a id="_idIndexMarker1097"/> are fundamental to STL containers, providing a means to traverse and manipulate container elements. However, iterators are fragile when it comes to concurrency. If a thread modifies the container in a way that causes reallocation or restructuring, other threads’ iterators might become invalidated. Using invalidated iterators is, yet again, undefined behavior.</p>
			<p>For example, in <a id="_idIndexMarker1098"/>containers such as <code>std::list</code> or <code>std::map</code>, adding an element won’t invalidate the existing iterators. However, with <code>std::vector</code>, a reallocation triggered when the vector exceeds its current capacity can invalidate all existing iterators. Being aware of these nuances is crucial when orchestrating multi-threaded operations.</p>
			<h2 id="_idParaDest-722"><a id="_idTextAnchor722"/>Containers with a built-in shield – concurrent containers</h2>
			<p>In <a id="_idIndexMarker1099"/>recognizing the challenges developers face when synchronizing standard STL containers, the library introduced concurrent containers. These containers, such as <code>std::atomic</code> and those in the <code>concurrency</code> namespace (for some compilers), come with built-in synchronization, offering thread-safe operations at the potential cost of performance.</p>
			<p>It’s important to note that these containers might not provide the same interface or performance characteristics as their standard STL counterparts. They are specialized tools that are ideal for scenarios where the overhead of manual synchronization might be too significant.</p>
			<p>While STL containers bring a world of convenience and efficiency to C++ programming, they come with the responsibility of understanding their threading characteristics. By discerning when and where explicit synchronization is required and leveraging the tools and techniques at our disposal, we can ensure that our multi-threaded applications remain robust, efficient, and free of concurrency-induced bugs.</p>
			<h1 id="_idParaDest-723"><a id="_idTextAnchor723"/>Specific container concerns</h1>
			<p>Different <a id="_idIndexMarker1100"/>STL container types present unique challenges and considerations in a multi-threaded environment. The thread safety of operations on these containers is not inherently guaranteed, making their use in concurrent scenarios a matter of careful planning. For instance, containers such as <code>std::vector</code> or <code>std::map</code> might behave unpredictably when simultaneously accessed or modified from multiple threads, leading to data corruption or race conditions. In contrast, containers such as <code>std::atomic</code> are designed for safe concurrent operations on individual elements, but they don’t safeguard the container’s structure as a whole. Therefore, understanding the specific threading implications of each STL container type is essential. Developers must implement appropriate locking mechanisms or <a id="_idIndexMarker1101"/>use thread-safe variants where necessary to ensure data integrity and correct program behavior in a multi-threaded environment.</p>
			<h2 id="_idParaDest-724"><a id="_idTextAnchor724"/>Behaviors of std::vector in multi-threading</h2>
			<p><code>std::vector</code> is a <a id="_idIndexMarker1102"/>widely-used STL container that acts as a dynamic array, adjusting its size as needed. Its contiguous memory allocation provides advantages such as cache locality. However, in multi-threaded scenarios, challenges arise.</p>
			<p>For example, when a vector’s capacity is surpassed and reallocates memory, all associated iterators, pointers, and references can be invalidated. If one thread iterates the vector while another prompts a reallocation (adding elements beyond its limit), this can lead to issues. To prevent such scenarios, synchronization mechanisms should be implemented during operations that trigger reallocations when multiple threads access the vector.</p>
			<h2 id="_idParaDest-725"><a id="_idTextAnchor725"/>Characteristics of std::list in concurrency</h2>
			<p><code>std::list</code>, which <a id="_idIndexMarker1103"/>is a doubly-linked list, has behaviors that are beneficial in multi-threaded situations but also require caution. A key advantage is that insertions or deletions do not invalidate iterators unless they target the specific removed element, making some operations naturally thread-safe.</p>
			<p>However, there’s a need for caution. While iterators may remain intact, concurrent modifications can alter the sequence of elements, resulting in inconsistent outcomes.</p>
			<h2 id="_idParaDest-726"><a id="_idTextAnchor726"/>Considerations with associative containers</h2>
			<p>Containers<a id="_idIndexMarker1104"/> such as <code>std::set</code>, <code>std::map</code>, <code>std::multiset</code>, and <code>std::multimap</code> order elements based on their keys. This ensures organized data retrieval.</p>
			<p>In multi-threaded situations, this trait presents challenges. Concurrent element insertions might result in an unpredictable final sequence. Additionally, concurrent removals can give rise to race conditions.</p>
			<h2 id="_idParaDest-727"><a id="_idTextAnchor727"/>Concurrency aspects of unordered containers</h2>
			<p>The<a id="_idIndexMarker1105"/> unordered versions of associative containers, such as <code>std::unordered_set</code> and <code>std::unordered_map</code>, do not keep elements in a defined order. However, they are not exempt from multi-threading issues. These containers leverage hashing, and element additions might trigger rehashing to optimize performance.</p>
			<p>Rehashing can lead to iterator invalidation. Hence, despite their unordered nature, careful handling is necessary during concurrent operations.</p>
			<h2 id="_idParaDest-728"><a id="_idTextAnchor728"/>Insights into container adaptors</h2>
			<p>The <a id="_idIndexMarker1106"/>STL provides container adaptors such as <code>std::stack</code>, <code>std::queue</code>, and <code>std::priority_queue</code>. These don’t possess their storage and instead encapsulate other containers. Their thread safety properties depend on the containers they are based on. For example, an instance of <code>std::stack</code> that utilizes <code>std::vector</code> would have the same reallocation and iterator invalidation issues.</p>
			<p>Being informed about the specific behaviors of each STL container is vital for developing thread-safe C++ programs. While the STL delivers numerous tools with distinct advantages, they also have challenges in multi-threaded contexts.</p>
			<h1 id="_idParaDest-729"><a id="_idTextAnchor729"/>Concurrency support within the STL</h1>
			<p>The <a id="_idIndexMarker1107"/>STL has evolved significantly, transforming from a collection of data structures and algorithms into a comprehensive library incorporating advanced constructs for concurrent programming. This expansion responds to the increasing demand for efficient and robust multi-threaded applications, especially in the era of multi-core processors. Modern software development frequently requires leveraging the power of concurrency to enhance performance and responsiveness. As such, a deep understanding of the STL’s concurrency support is beneficial and essential for developers looking to optimize their applications in this multi-threaded landscape.</p>
			<p>This section <a id="_idIndexMarker1108"/>will examine the concurrency features integrated within the STL. This includes a detailed examination of thread management, asynchronous tasks, atomic operations, and challenges with utilizing concurrency.</p>
			<p>The STL’s offerings in the area of concurrency are not just about facilitating multi-threading but are also about doing it in an effective and manageable way. This section is designed to provide a comprehensive understanding of these tools, enabling you to write high-performance, scalable, and reliable C++ applications in today’s computationally demanding world.</p>
			<h2 id="_idParaDest-730"><a id="_idTextAnchor730"/>Introduction to threads</h2>
			<p>At the <a id="_idIndexMarker1109"/>heart of concurrent programming lies the concept of threads. Within the STL, this is represented by <code>std::thread</code>. This class offers a straightforward interface for creating and overseeing threads. Initiating a new thread is essentially about defining a function or a callable entity and passing it to the thread constructor. After executing your task, you can join (await its conclusion) or detach (permit its independent execution) the thread. However, here’s a word of caution: manually handling threads requires careful attention. It’s imperative to ensure all threads are correctly joined or detached to avoid potential issues, including lingering threads.</p>
			<h2 id="_idParaDest-731"><a id="_idTextAnchor731"/>The advent of asynchronous tasks</h2>
			<p>Direct thread management provides considerable control, but the STL introduces <code>std::async</code> and <code>std::future</code> for tasks that don’t require such meticulous oversight. These constructs enable developers to delegate tasks for potential parallel execution without the intricacies of direct thread oversight. The function <code>std::async</code> initiates a task, and its resultant <code>std::future</code> offers a method to fetch the result when it’s ready. This fosters more organized code, mainly when the focus is on task-centric parallelism.</p>
			<h2 id="_idParaDest-732"><a id="_idTextAnchor732"/>Atomic operations</h2>
			<p>The STL <a id="_idIndexMarker1110"/>provides a robust solution through atomic operations for inefficient, low-overhead operations, where the locking mechanisms may appear disproportionate. The atomic operations, encapsulated within the <code>std::atomic</code> class template, play a pivotal role in concurrent programming by guaranteeing the atomicity of operations in fundamental data types.</p>
			<p><code>std::atomic</code> is designed to ensure that operations on basic types, such as integers and pointers, are executed as indivisible units. This atomicity is crucial in multi-threaded environments, as it prevents the potential hazards of interrupted operations, which can lead to inconsistent or corrupt data states. By ensuring that these operations are completed without interruption, <code>std::atomic</code> obviates the need for traditional locking mechanisms, such as mutexes, thereby enhancing performance by reducing the overhead associated with lock contention and context switching.</p>
			<p>However, it is essential to note that using atomic operations requires careful consideration and an understanding of their characteristics and limitations. While they provide a mechanism for lock-free programming, atomic operations are not a panacea for all concurrency problems. Developers must know the memory order constraints and the potential performance implications on different hardware architectures. In particular, the choice between memory orderings (such as <code>memory_order_relaxed</code>, <code>memory_order_acquire</code>, <code>memory_order_release</code>, etc.) demands a thorough understanding of the synchronization requirements and the trade-offs involved.</p>
			<p>Memory orderings, such as <code>memory_order_relaxed</code>, <code>memory_order_acquire</code>, and <code>memory_order_release</code>, dictate how operations on atomic variables are ordered with respect to other memory operations.</p>
			<p>Choosing the correct memory ordering is crucial for ensuring the desired level of synchronization while balancing performance. For instance, <code>memory_order_relaxed</code> offers minimal synchronization and imposes no ordering constraints on memory operations, leading to higher performance but at the risk of allowing other threads to see operations in a different order. On the other hand, <code>memory_order_acquire</code> and <code>memory_order_release</code> provide stronger guarantees about the ordering of reads and writes, which is essential for correctly implementing lock-free data structures and algorithms but can come with a performance cost, especially in systems with weak memory models.</p>
			<p>The trade-offs involved in these decisions are significant. A more relaxed memory ordering can lead to performance gains but also introduce subtle bugs if the program’s correctness relies on certain memory ordering guarantees. Conversely, opting for stronger memory orderings can simplify the reasoning about the correctness of concurrent code but may lead to decreased performance due to additional memory synchronization barriers.</p>
			<p>Therefore, developers <a id="_idIndexMarker1111"/>must be aware of the synchronization requirements of their specific application and understand how their choice of memory ordering will interact with the underlying hardware architecture. This knowledge is critical for writing efficient and correct concurrent programs in C++.</p>
			<h2 id="_idParaDest-733"><a id="_idTextAnchor733"/>Potential concurrent challenges</h2>
			<p>Concurrency, though <a id="_idIndexMarker1112"/>powerful, isn’t devoid of challenges. Developers might confront deadlocks, race conditions, and resource contention. Deadlocks transpire when multiple threads indefinitely wait for each other to release resources. Race conditions can give rise to erratic bugs stemming from unforeseen overlaps in thread operations.</p>
			<p><strong class="bold">False sharing</strong> is another notable challenge. It happens when different threads modify data situated in the same cache line. This can hamper performance because even if threads modify distinct data, their memory closeness can trigger redundant cache invalidations. Awareness and prudence can aid in sidestepping these challenges.</p>
			<h2 id="_idParaDest-734"><a id="_idTextAnchor734"/>Using the STL’s concurrency features</h2>
			<p>The <a id="_idIndexMarker1113"/>STL provides a range of tools for concurrent programming, spanning from the initiation of threads to the assurance of atomic tasks. These tools cater to a variety of requirements. Nevertheless, it’s vital to employ them judiciously.</p>
			<p>Concurrency promises enhanced performance and nimble applications but comes with complexities and potential bugs. In concurrency, knowing what tools are available is a necessary starting point, but effectively using them requires ongoing trial and learning.</p>
			<p>The following C++ code example illustrates the STL’s various concurrency features. This example encompasses thread creation, asynchronous task execution, and atomic operations while highlighting the importance of proper thread management and the potential <a id="_idIndexMarker1114"/>pitfalls of concurrency:</p>
			<pre class="source-code">
#include &lt;atomic&gt;
#include &lt;future&gt;
#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;
// A simple function that we will run in a separate thread.
void threadTask(int n) {
  std::this_thread::sleep_for(std::chrono::seconds(n));
  std::cout &lt;&lt; "Thread " &lt;&lt; std::this_thread::get_id()
            &lt;&lt; " completed after " &lt;&lt; n &lt;&lt; " seconds.\n";
}
// A function that performs a task and returns a result.
int performComputation(int value) {
  std::this_thread::sleep_for(std::chrono::seconds(1));
  return (value * value);
}
int main() {
  // Start a thread that runs threadTask with n=2
  std::thread t(threadTask, 2);
  // task management with std::async and std::future
  std::future&lt;int&gt; futureResult = std::async(
      std::launch::async, performComputation, 5);
  // Atomic operation with std::atomic
  std::atomic&lt;int&gt; atomicCounter(0);
  // Demonstrate atomicity in concurrent operations
  std::vector&lt;std::thread&gt; threads;
  for (int i = 0; i &lt; 10; ++i) {
    threads.emplace_back([&amp;atomicCounter]() {
      for (int j = 0; j &lt; 100; ++j) {
        atomicCounter += 1; // Atomic increment
      }
    });
  }
  // Joining the initial thread to ensure it has finished
  // before main exits
  if (t.joinable()) { t.join(); }
  // Retrieving the result from the future
  int computationResult = futureResult.get();
  std::cout &lt;&lt; "The result of the computation is "
            &lt;&lt; computationResult &lt;&lt; ".\n";
  // Joining all threads to ensure complete execution
  for (auto &amp;th : threads) {
    if (th.joinable()) { th.join(); }
  }
  std::cout &lt;&lt; "The final value of the atomic counter is "
            &lt;&lt; atomicCounter &lt;&lt; ".\n";
  return 0;
}</pre>			<p>Here is the example output:</p>
			<pre class="console">
Thread 32280 completed after 2 seconds.
The result of the computation is 25.
The final value of the atomic counter is 1000.</pre>			<p>In this <a id="_idIndexMarker1115"/>example, we did the following:</p>
			<ul>
				<li>Created a thread using <code>std::thread</code> that sleeps for a given number of seconds and then prints a message.</li>
				<li>Used <code>std::async</code> to perform a computation in a potentially parallel manner,  and we used <code>std::future</code> to obtain the result once it was ready.</li>
				<li>Demonstrated using <code>std::atomic</code> to perform an atomic increment operation within multiple threads.</li>
				<li> Ensured that all threads are correctly joined to avoid dangling threads.</li>
			</ul>
			<p>This code is a simple demonstration and serves as a starting point for understanding concurrency in C++. Developers must further explore and handle more complex scenarios, including synchronization, preventing deadlocks, and avoiding race conditions and false sharing for robust concurrent applications.</p>
			<h1 id="_idParaDest-735"><a id="_idTextAnchor735"/>Using std::thread, std::async, std::future, and thread
-local storage</h1>
			<p>Let’s look at four <a id="_idIndexMarker1116"/>core components of C++’s concurrency toolkit: <code>std::thread</code>, <code>std::async</code>, <code>std::future</code>, and thread-local storage. Each of these elements is vital for facilitating multi-threaded programming in C++. <code>std::thread</code> is the foundation, allowing for the creation and management of threads. <code>std::async</code> and <code>std::future</code> work <a id="_idIndexMarker1117"/>in tandem to asynchronously execute tasks and retrieve<a id="_idIndexMarker1118"/> their results in a controlled manner, offering a higher level of abstraction over raw threads. Thread-local storage, on the other hand, provides a unique data instance for each thread. This is crucial for avoiding data conflicts in a concurrent environment. This section aims to comprehensively understand these tools, demonstrating how they can be used effectively to write robust, efficient, and thread-safe C++ applications.</p>
			<h2 id="_idParaDest-736"><a id="_idTextAnchor736"/>Initiating threads using std::thread</h2>
			<p>A primary <a id="_idIndexMarker1119"/>tool in the realm of concurrency within C++ is <code>std::thread</code>. This class allows developers to concurrently run procedures by starting distinct threads for execution. To launch a new thread, pass a callable entity (such as a function or a lambda) to the <code>std::thread</code> constructor. For instance, to print “Hello, Concurrent World!” from an independent thread, see the following:</p>
			<pre class="source-code">
std::thread my_thread([]{
    std::cout &lt;&lt; "Hello, Concurrent World!" &lt;&lt; "\n";
});
my_thread.join();</pre>			<p>Utilizing the <code>join()</code> function ensures that the main thread waits until <code>my_thread </code>completes. There’s also <code>detach()</code>, which lets the primary thread progress without delay. However, the careful management of detached threads is crucial to avoid unexpected behavior.</p>
			<h2 id="_idParaDest-737"><a id="_idTextAnchor737"/>Managing asynchronous operations with std::async and std::future</h2>
			<p>Though <code>std::thread</code> offers significant capabilities, direct thread management can be intricate. The<a id="_idIndexMarker1120"/> STL presents an elevated abstraction for administering potential parallel operations through <code>std::async</code> and <code>std::future</code>.</p>
			<p>The approach is <a id="_idIndexMarker1121"/>clear-cut: assign a task to <code>std::async</code> and retrieve a <code>std::future</code> object that will eventually contain that task’s result. This division allows the primary thread to either continue or optionally await the outcome using the <code>get()</code> method of <code>std::future</code>, as shown in the following code example:</p>
			<pre class="source-code">
auto future_result = std::async([]{
    return "Response from async!";
});
std::cout &lt;&lt; future_result.get() &lt;&lt; "\n";</pre>			<p>As you can see, <code>std::async</code> and <code>std::future</code> are designed to work well together to help manage asynchronous operations.</p>
			<h2 id="_idParaDest-738"><a id="_idTextAnchor738"/>Preserving data consistency using thread-local storage</h2>
			<p>Ensuring <a id="_idIndexMarker1122"/>distinct data storage for each thread to avoid overlap and maintain data consistency in concurrent programming can be challenging. This is addressed by <strong class="bold">thread-local </strong><strong class="bold">storage</strong> (<strong class="bold">TLS</strong>).</p>
			<p>Using the <code>thread_local</code> keyword when declaring a variable ensures a unique instance of that variable for each thread. This is instrumental in sustaining data consistency and circumventing the issues associated with shared data access:</p>
			<pre class="source-code">
thread_local int thread_counter = 0;</pre>			<p>Here, <code>thread_counter</code> is instantiated for each thread, shielding it from inter-thread interference.</p>
			<h2 id="_idParaDest-739"><a id="_idTextAnchor739"/>Integrating tools for proficient concurrency</h2>
			<p>With <code>std::thread</code>, <code>std::async</code>, <code>std::future</code>, and TLS, you are prepared to navigate <a id="_idIndexMarker1123"/>various concurrent programming situations in C++. The STL offers the requisite tools for delegating tasks for parallel execution or adeptly managing thread-specific data.</p>
			<p>It’s pivotal to note that while initiating threads or tasks is straightforward, ensuring synchronized operations devoid of contention, deadlocks, or data races demands attentiveness and continual refinement.</p>
			<p>Retaining the foundational insights from this segment is paramount as we transition to the subsequent sections that review the STL’s concurrent data structures. Concurrent programming is an evolving landscape, and mastering each tool and concept augments your capacity to develop efficient and stable concurrent applications.</p>
			<p>Let’s walk through a code example that illustrates the use of <code>std::thread</code>, <code>std::async</code>, <code>std::future</code>, and TLS to concurrently execute tasks and manage per-thread data:</p>
			<pre class="source-code">
#include &lt;future&gt;
#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;
// Function to demonstrate the use of Thread Local Storage
void incrementThreadCounter() {
  // Unique to each thread
  thread_local int thread_counter = 0;
  thread_counter++;
  std::cout &lt;&lt; "Thread " &lt;&lt; std::this_thread::get_id()
            &lt;&lt; " counter: " &lt;&lt; thread_counter &lt;&lt; "\n";
}
int main() {
  // Initiating a new thread using std::thread
  std::thread my_thread([] {
    std::cout &lt;&lt; "Hello, Concurrent World!"
              &lt;&lt; "\n";
  });
  // Ensure the main thread waits for my_thread to complete
  if (my_thread.joinable()) { my_thread.join(); }
  // Asynchronous operations w/std::async and std::future
  auto future_result =
      std::async([] { return "Response from async!"; });
  // Retrieve the result with std::future::get when ready
  std::cout &lt;&lt; future_result.get() &lt;&lt; "\n";
  // Demonstrating the use of Thread Local Storage (TLS)
  std::vector&lt;std::thread&gt; threads;
  for (int i = 0; i &lt; 5; ++i) {
    threads.emplace_back(incrementThreadCounter);
  }
  // Join all threads to the main thread
  for (auto &amp;thread : threads) {
    if (thread.joinable()) { thread.join(); }
  }
  return 0;
}</pre>			<p>Here is the example output:</p>
			<pre class="console">
Hello, Concurrent World!
Response from async!
Thread 11672 counter: Thread 1
32816 counter: 1
Thread 7124 counter: 1
Thread 43792 counter: 1
Thread 23932 counter: 1</pre>			<p>In this code, we <a id="_idIndexMarker1124"/>did the following:</p>
			<ul>
				<li>Created a thread to print a message to the console using <code>std::thread</code>.</li>
				<li>Used <code>std::async</code> to perform an asynchronous operation that returns a string. The result is accessed via a <code>std::future</code> object.</li>
				<li>Demonstrated the use of TLS with the <code>thread_local</code> keyword to maintain a separate counter for each thread.</li>
				<li>Started multiple threads, each incrementing its local counter, to show how TLS variables are instantiated for each thread.</li>
			</ul>
			<p>This example encapsulates the essentials of concurrent programming with the STL, from thread creation and synchronization to data isolation with TLS. While these mechanisms simplify parallel execution, we must exercise careful judgment to prevent concurrency-related issues, such as deadlocks and race conditions. The upcoming sections will explore STL’s concurrent data structures, which build upon these foundational concepts to enable the creation of robust concurrent programs.</p>
			<h1 id="_idParaDest-740"><a id="_idTextAnchor740"/>Concurrent data structures in the STL</h1>
			<p>The STL provides a <a id="_idIndexMarker1125"/>variety of data structures, but not all are inherently suited for concurrent access. Understanding how to effectively utilize and adapt these data structures for safe and efficient use in a multi-threaded context is crucial. We will examine the thread safety aspects of common STL data structures, discuss the appropriate use cases for each in a concurrent environment, and explore the strategies to ensure safe and effective concurrent access. This section is designed to equip developers with the knowledge to leverage STL data structures to maximize performance while maintaining data integrity in a multi-threaded landscape.</p>
			<h2 id="_idParaDest-741"><a id="_idTextAnchor741"/>The STL’s concurrency-optimized containers</h2>
			<p>While the<a id="_idIndexMarker1126"/> STL provides many containers, not all are optimized for concurrent access. However, with the increasing demand for concurrent programming, specific concurrency-friendly containers have made their way into the repertoire of many C++ programmers.</p>
			<p>One notable example is <code>std::shared_timed_mutex</code> and its sibling <code>std::shared_mutex</code> (from C++17 onwards). These synchronization primitives allow multiple threads to read shared data simultaneously while ensuring exclusive access for writing. This is particularly handy when read operations are more frequent than writes, such as in caching scenarios.</p>
			<p>Consider a situation where you have <code>std::map</code> storing configuration data:</p>
			<pre class="source-code">
std::map&lt;std::string, std::string&gt; config_data;
std::shared_timed_mutex config_mutex;</pre>			<p>To read from this map, multiple threads can acquire a shared lock:</p>
			<pre class="source-code">
std::shared_lock lock(config_mutex);
auto val = config_data["some_key"];</pre>			<p>However, for writing, a unique lock ensures exclusive access:</p>
			<pre class="source-code">
std::unique_lock lock(config_mutex);
config_data["some_key"] = "new_value";</pre>			<p>While not a container, <code>std::shared_timed_mutex</code> can protect any STL container, ensuring concurrent read access while serializing writes.</p>
			<h2 id="_idParaDest-742"><a id="_idTextAnchor742"/>Striving for maximum efficiency in concurrent environments</h2>
			<p>Concurrency isn’t just about making operations thread-safe but is also about achieving better<a id="_idIndexMarker1127"/> performance. As you’ve seen, atomic types and concurrency-optimized containers help ensure safety, but there’s more to it than that. Fine-tuning performance may involve considering lock contention, avoiding false sharing, and minimizing synchronization overhead.</p>
			<p>A few tips for maximizing efficiency include the following:</p>
			<ul>
				<li><strong class="bold">Limit the scope of locks</strong>: While locks are essential for ensuring data consistency, holding them for extended durations can impede performance. Ensure you’re only holding locks for the necessary duration.</li>
				<li><strong class="bold">Choose the right data structure</strong>: Containers optimized for concurrency might offer better performance for multi-threaded applications, even if they might be slower in single-threaded scenarios.</li>
				<li><strong class="bold">Consider granularity</strong>: Think about the granularity of your locks. Sometimes, a finer-grained lock (protecting just a part of your data) can perform better than a coarser-grained one (protecting the entire data structure).</li>
			</ul>
			<h2 id="_idParaDest-743"><a id="_idTextAnchor743"/>Best practices in action</h2>
			<p>Let’s look at a <a id="_idIndexMarker1128"/>code example demonstrating best practices in using STL containers in a concurrent environment, focusing on performance optimization techniques such as minimizing lock scope, selecting appropriate data structures, and considering lock granularity.</p>
			<p>First, we will write a concurrency-optimized container, specifically <code>ConcurrentVector</code>, designed to handle multi-threaded environments effectively. This custom container class, which is templated to hold elements of any type (<code>T</code>), encapsulates a standard <code>std::vector</code> for data storage while employing <code>std::shared_mutex</code> to manage concurrent access (we will break this example up into a few sections. For<a id="_idIndexMarker1129"/> the complete code, please refer to the book's GitHub repository):</p>
			<pre class="source-code">
// A hypothetical concurrency-optimized container that uses
// fine-grained locking
template &lt;typename T&gt; class ConcurrentVector {
private:
  std::vector&lt;T&gt; data;
  mutable std::shared_mutex mutex;
public:
  // Inserts an element into the container with minimal
  // lock duration
  void insert(const T &amp;value) {
    std::unique_lock&lt;std::shared_mutex&gt; lock(mutex);
    data.push_back(value);
  }
  // Finds an element with read access, demonstrating
  // shared locking
  bool find(const T &amp;value) const {
    std::shared_lock&lt;std::shared_mutex&gt; lock(mutex);
    return std::find(data.begin(), data.end(), value) !=
           data.end();
  }
  // Size accessor that uses shared locking
  size_t size() const {
    std::shared_lock&lt;std::shared_mutex&gt; lock(mutex);
    return data.size();
  }
};</pre>			<p>Next, we <a id="_idIndexMarker1130"/>will write the function <code>performConcurrentOperations</code>, which will demonstrate the practical application of our <code>ConcurrentVector</code> class in a multi-threaded context. This function accepts a reference to <code>ConcurrentVector&lt;int&gt;</code> and initiates two parallel operations using C++ standard threads:</p>
			<pre class="source-code">
void performConcurrentOperations(
    ConcurrentVector&lt;int&gt; &amp;concurrentContainer) {
  // Multiple threads perform operations on the container
  std::thread writer([&amp;concurrentContainer]() {
    for (int i = 0; i &lt; 100; ++i) {
      concurrentContainer.insert(i);
    }
  });
  std::thread reader([&amp;concurrentContainer]() {
    for (int i = 0; i &lt; 100; ++i) {
      if (concurrentContainer.find(i)) {
        std::cerr &lt;&lt; "Value " &lt;&lt; i
                  &lt;&lt; " found in the container\n";
      }
    }
  });
  // Join threads to ensure complete execution
  writer.join();
  reader.join();
  // Output the final size of the container
  std::cout &lt;&lt; "Final size of the container:"
            &lt;&lt; concurrentContainer.size() &lt;&lt; "\n";
}</pre>			<p>Finally, we <a id="_idIndexMarker1131"/>write <code>main()</code> to drive the program:</p>
			<pre class="source-code">
int main() {
  ConcurrentVector&lt;int&gt; concurrentContainer;
  performConcurrentOperations(concurrentContainer);
  return 0;
}</pre>			<p>Here is the example output:</p>
			<pre class="console">
...
Value 98 found in the container.
Value 99 found in the container.
Final size of the container: 100</pre>			<p>In total, in the preceding code example, we did the following:</p>
			<ul>
				<li>We have defined a <code>ConcurrentVector</code> template class that mimics a concurrency-optimized container, which internally uses <code>std::shared_mutex</code> to enable fine-grained control over read and write operations.</li>
				<li>The <code>insert</code> method uses a unique lock to ensure exclusive access during write operations, but the lock is held only for the insert duration, minimizing the lock scope.</li>
				<li>The <code>find</code> and <code>size</code> methods use shared locks, allowing for concurrent reads, demonstrating the use of shared locking to enable higher read throughput.</li>
				<li>A writer thread and a reader thread were created to perform concurrent insertions and searches on the <code>ConcurrentVector</code> instance, showcasing the container’s ability to handle concurrent operations.</li>
			</ul>
			<p>This<a id="_idIndexMarker1132"/> example illustrates critical considerations for optimizing concurrent performance, such as limiting the duration of locks, choosing appropriate concurrency-friendly data structures, and using fine-grained locking to protect smaller sections of the data. These practices are crucial for intermediate-level C++ developers looking to enhance the performance of multi-threaded applications.</p>
			<h1 id="_idParaDest-744"><a id="_idTextAnchor744"/>Summary</h1>
			<p>This chapter discussed the intricacies of thread safety and concurrency within the STL. We started by distinguishing between concurrency and thread safety, underscoring that while related, each serves a distinct purpose. Our journey began with a foundational understanding of thread safety as a pillar for stable concurrency and how the lack thereof can lead to unpredictable software behavior. We examined the interplay between these concepts, addressing the challenges and highlighting the rewards of concurrent programming when thread safety is maintained.</p>
			<p>We looked into the thread-safe nature of STL containers and algorithms, dissecting race conditions and the techniques to anticipate and guard against them. The chapter provided detailed insights into the behaviors of various STL containers under multi-threaded scenarios, from <code>std::vector</code> to <code>std::list</code> and associative to unordered containers. We also uncovered the concurrency aspects of container adaptors, asserting that knowledge is power when writing concurrent applications.</p>
			<p>We’ve been equipped with the core tools: <code>std::thread</code>, <code>std::async</code>, <code>std::future</code>, and TLS. With these, we initiated threads, managed asynchronous operations, and preserved data consistency across threads. These capabilities have prepared us for proficient concurrency about safety and performance.</p>
			<p>The chapter examined the STL’s atomic types and concurrency-optimized containers, providing tips for maximizing efficiency in concurrent environments. These insights are pivotal for developing high-performance, thread-safe applications using the STL.</p>
			<p>The knowledge imparted in this chapter is essential because thread safety and efficient concurrency are critical for modern C++ developers. As multi-core and multi-threaded applications become the norm, it is crucial to understand these principles to be able to leverage the full power of the STL.</p>
			<p>In the next chapter, we will dig further into advanced STL usage. We will introduce concepts and robust template features, allowing for more precise type checks at compile-time. We will learn how to refine the constraints in STL algorithms and effectively use these constraints to enhance data structures with explicit requirements. Moreover, we will explore the integration of the STL with coroutines, assessing the potential synergies with ranges and views and preparing for the paradigm shift that awaits in contemporary C++ programming.</p>
		</div>
	</body></html>