- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring C++ Concepts from A Low-Latency Application’s Perspective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we assume that the reader has an intermediate level of understanding
    of C++ programming concepts, features, and so on. We will discuss how to approach
    low-latency application development in C++. We will move on to discussing what
    C++ features to avoid specifically when it comes to low-latency applications.
    We will then discuss the key C++ features that make it perfect for low-latency
    applications and how we will use them in the rest of the book. We will conclude
    by discussing how to maximize compiler optimizations and which C++ compiler flags
    are important for low-latency applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Approaching low-latency application development in C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding pitfalls and leveraging C++ features to minimize application latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximizing C++ compiler optimization parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us start by discussing the higher-level ideas when it comes to approaching
    low-latency application development in C++ in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code for this book can be found in the GitHub repository for this book
    at [https://github.com/PacktPublishing/Building-Low-Latency-Applications-with-CPP](https://github.com/PacktPublishing/Building-Low-Latency-Applications-with-CPP).
    The source code for this chapter is in the Chapter3 directory in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: Approaching low-latency application development in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss the higher-level ideas to keep in mind when
    trying to build low-latency applications in C++. Overall, the ideas are to understand
    the architecture that your application runs on, your application use cases that
    are latency-sensitive, the programming language of your choice (C++ in this case),
    how to work with the development tools (the compiler, linker, etc.) and how to
    measure application performance in practice to understand which parts of the application
    to optimize first.
  prefs: []
  type: TYPE_NORMAL
- en: Coding for correctness first, optimizing second
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For low-latency applications, correct behavior of the application under different
    use cases and scenarios and robust handling of edge conditions is still the primary
    focus. A fast application that does not do what we need is useless, so the best
    approach when it comes to developing a low-latency application is to first code
    for correctness, not speed. Once the application works correctly, only then the
    focus should be shifted to optimizing the critical parts of the application while
    maintaining correctness. This ensures that developers spend time focusing on the
    correct parts to optimize because it is common to find that our intuition on which
    pieces are critical to performance does not match what happens in practice. Optimizing
    the code can also take significantly longer than coding for correctness, so it
    is important to optimize the most important things first.
  prefs: []
  type: TYPE_NORMAL
- en: Designing optimal data structures and algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Designing custom data structures that are optimal for the application’s use
    cases is an important part of building low-latency applications. A good amount
    of thought needs to be put into each data structure used in the critical parts
    of the application in terms of scalability, robustness, and performance under
    the use cases and data encountered *in practice*. It is important to understand
    why we mention the term *in practice* here because different data structure choices
    will perform better under different use cases and input data even if the different
    data structures themselves have the same output or behavior. Before we discuss
    an example of different possible data structures and algorithms to solve the same
    problem, let us quickly review Big-O notation. Big-O notation is used to describe
    the asymptotic worst-case time complexity of performing a certain task. The term
    asymptotic here is used to describe the fact that we discuss cases where we measure
    the performance over a theoretically infinite (in practice an exceptionally large)
    number of data points. The asymptotic performance eliminates all the constant
    terms and describes the performance only as a function of the number of input
    data elements.
  prefs: []
  type: TYPE_NORMAL
- en: A simple example of using different data structures to solve the same problem
    would be searching for an entry in a container by a key value. We can solve this
    either by using a hash map implementation that has an expected *amortized* complexity
    of `O(1)` or using an array that has a complexity of `O(n)`, where `n` is the
    number of elements in the container. While on paper it might appear that the hash
    map is clearly the way to go, other factors such as the number of elements, the
    complexity of applying the hash function to the keys, and so on might change which
    data structure is the way to go. In this case, for a handful of elements, the
    array solution is faster due to better cache performance, while for many elements,
    the hash map solution is better. Here, we chose a suboptimal algorithm because
    the underlying data structure for the suboptimal algorithm performed better in
    practice due to cache performance.
  prefs: []
  type: TYPE_NORMAL
- en: Another slightly different example would be using lookup tables over recomputing
    values for some mathematical functions, say, trigonometric functions. While it
    makes complete sense that looking up the result in a precomputed lookup table
    *should* always be faster compared to performing some calculations, this might
    not always be true. For instance, if the lookup table is very large, then the
    cost of evaluating a floating-point expression might be less than the cost of
    getting a cache miss and reading the lookup table value from the main memory.
    The overall application performance might also be better if accessing the lookup
    table from the main memory leads to a lot of cache pollution, leading to performance
    degradation in other parts of the application code.
  prefs: []
  type: TYPE_NORMAL
- en: Being mindful of the processor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern processors have a lot of architectural and functional details that a
    low-latency application developer should understand, especially a C++ developer
    since it allows very low-level control. Modern processors have multiple cores,
    larger and specialized register banks, pipelined instruction processing where
    instructions needed next are prefetched while executing the current one, instruction
    level parallelism, branch predictions, extended instruction sets to facilitate
    faster and specialized processing, and so on. The better the application developer
    understands these aspects of the processor on which their applications will run,
    the better they can avoid sub-optimal code and/or compilation choices and make
    sure that the compiled machine code is optimal for their target architecture.
    At the very least, the developer should instruct the compiler to output code for
    their specific target architecture using compiler optimization flags, but we will
    discuss that topic later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the cache and memory access costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Typically, a lot of effort is put into the design and development of data structures
    and algorithms when it comes to low-latency application development from the perspective
    of reducing the amount of work done or the number of instructions executed. While
    this is the correct approach, in this section, we would like to point out that
    thinking about cache and memory accesses is equally important.
  prefs: []
  type: TYPE_NORMAL
- en: We saw in the previous sub-section, *Designing optimal data structures and algorithms*,
    that it is common for data structures and algorithms that are sub-optimal on paper
    to outperform ones that are optimal on paper. A large reason behind that can be
    the higher cache and memory access costs for the optimal solution outweighing
    the time saved because of the reduced number of instructions the processor needs
    to execute. Another way to think about this is that even though the amount of
    work from the perspective of the number of algorithmic steps is less, in practice,
    it takes longer to finish with the modern processor, cache, and memory access
    architectures today.
  prefs: []
  type: TYPE_NORMAL
- en: Let us quickly review the memory hierarchy in a modern computer architecture.
    Note that details of what we will recap here can be found in our other book, *Developing
    High-Frequency Trading Systems*. The key points here are that the memory hierarchy
    works in such a way that if the CPU cannot find the data or instruction it needs
    next in the register, it goes to the L0 cache, and if it cannot find it there,
    goes to the L1 cache, L2, other caches, and so on, then goes to the main memory
    in that order. Note that the storage is accessed from fastest to slowest, which
    also happens to be least amount of space to most amount of space. The art of effective
    low-latency and cache-friendly application development relies on writing code
    that is cognizant of code and data access patterns to maximize the likelihood
    of finding data in the fastest form of storage possible. This relies on maximizing
    the concepts of **temporal locality** and **spatial locality**. These terms mean
    that data accessed recently is likely to be in the cache and data next to what
    we just accessed is likely to be in the cache, respectively. The following diagram
    visually lays out the register, cache, and memory banks and provides some data
    on access times from the CPU. Note that there is a good amount of variability
    in the access times depending on the hardware and the constant improvements being
    made to technologies. The key takeaway here should be that there is a significant
    increase in access times as we go from CPU registers to cache banks to the main
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – The hierarchy of memory in modern computer architectures. ](img/Figure_3.1_B19434.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – The hierarchy of memory in modern computer architectures.
  prefs: []
  type: TYPE_NORMAL
- en: I would advise you to think carefully about the cache and memory access patterns
    for the algorithm locally, as well as the entire application globally, to make
    sure that your source code optimizes cache and memory access patterns, which will
    boost overall application performance. If you have a function that executes very
    quickly when it is called but causes a lot of cache pollution, that will degrade
    the complete application’s performance because other components will incur additional
    cache miss penalties. In such a case, we have failed in our objective of having
    an application that performs optimally even though we might have managed to make
    this function perform optimally locally.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how C++ features work under the hood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When developing low-latency applications, it is very important that the developers
    have an extremely good understanding of how the high-level language abstractions
    work at a lower level or “under the hood.” For applications that are not latency-sensitive,
    this is perhaps not as important since if the application behaves the way the
    developer intends it to, the extremely low-level details of how their source code
    achieves that is not relevant.
  prefs: []
  type: TYPE_NORMAL
- en: For low-latency applications in C++, the more knowledge the developer has of
    how their program gets compiled into machine code, the better they can use the
    programming language to achieve low-latency performance. A lot of high-level abstractions
    available in C++ improve the ease and speed of development, robustness and safety,
    maintainability, software design elegance, and so on, but not all of them might
    be optimal when it comes to low-latency applications.
  prefs: []
  type: TYPE_NORMAL
- en: Many C++ features, such as dynamic polymorphism, dynamic memory allocation,
    and exception handling, are great additions to the language for most applications.
    However, these are best avoided or used sparingly or used in a specific manner
    when it comes to low-latency applications since they have larger overheads.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, traditional programming practices suggest the developer break everything
    down into numerous very small functions for reusability; use recursive functions
    when applicable; use **Object-Oriented Programming** (**OOP**) principles, such
    as inheritance and virtual functions; always use smart pointers instead of raw
    pointers; and so on. These principles are sensible for most applications, but
    for low-latency applications, these need to be evaluated and used carefully because
    they might add non-trivial amounts of overhead and latency.
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway here is that it is important for low-latency application developers
    to understand each one of these C++ features very well to understand how they
    are implemented in machine code and what impact they have on the hardware resources
    and how they perform in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging the C++ compiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The modern C++ compiler is truly a fascinating piece of software. There is an
    immense amount of effort invested into building these compilers to be robust and
    correct. A lot of effort is also made to make them very intelligent in terms of
    the transformations and optimizations they apply to the developer’s high-level
    source code. Understanding how the compiler translates the developer’s code into
    machine instructions, how it tries to optimize the code, and when it fails is
    important for low-latency application developers looking to squeeze as much performance
    out of their applications as possible. We will discuss the workings of the compiler
    and optimization opportunities extensively in this chapter so that we can learn
    to work with the compiler instead of against it when it comes to optimizing our
    final application’s representation (machine code executable).
  prefs: []
  type: TYPE_NORMAL
- en: Measuring and improving performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We mentioned that the ideal application development journey involves first building
    the application for correctness and then worrying about optimizing it after that.
    We also mentioned that it is not uncommon for a developer’s intuition to be incorrect
    when it comes to identifying performance bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we also mentioned that the task of optimizing an application can take
    significantly longer than the task of developing it to perform correctly. For
    that reason, it is advisable that before embarking on an optimization journey,
    the developer try to run the application under practical constraints and inputs
    to check performance. It is important to add instrumentation to the application
    in different forms to measure the performance and find bottlenecks to understand
    and prioritize the optimization opportunities. This is also an important step
    since as the application evolves, measuring and improving performance continues
    to be part of the workflow, that is, measuring and improving performance is a
    part of the application’s evolution. In the last section of this book, *Analyzing
    and improving performance*, we will discuss this idea with a real case study to
    understand this better.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding pitfalls and leveraging C++ features to minimize application latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at different C++ features that, if used correctly,
    can minimize application latency. We will also discuss the details of using these
    features in a manner that optimizes application performance throughout this sub-section.
    Now, let us start learning about how to use these features correctly to maximize
    application performance and avoid the pitfalls to minimize latency. Note that
    all the code snippets for this chapter are in the `Chapter3` directory in the
    GitHub repository for this book.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Local variables created within a function are stored on the stack by default
    and the stack memory is also used to store function return values. Assuming no
    large objects are created, the same range of stack storage space is reused a lot,
    resulting in great cache performance due to locality of reference.
  prefs: []
  type: TYPE_NORMAL
- en: Register variables are closest to the processor and are the fastest possible
    form of storage available. They are extremely limited, and the compiler will try
    to use them for the local variables that are used the most, another reason to
    prefer *local variables*.
  prefs: []
  type: TYPE_NORMAL
- en: Static variables are inefficient from the perspective of cache performance since
    that memory cannot be re-used for other variables and accessing static variables
    is likely a small fraction of all memory accesses. So, it is best to avoid static
    variables as well as global variables, which have similarly inefficient cache
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: The `volatile` keyword instructs the compiler to disable a lot of optimizations
    that rely on the assumption that the variable value does not change without the
    compiler’s knowledge. This should only ever be used carefully in multi-threaded
    use cases since it prevents optimizations such as storing the variables in registers
    and force-flushing them to the main memory from the cache every time the value
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamically allocated memory is inefficient to allocate and deallocate and,
    depending on how it is used, can suffer from poor cache performance. More on dynamically
    allocated memory inefficiencies will be discussed later in this section in the
    *Dynamically allocating* *memory* sub-section.
  prefs: []
  type: TYPE_NORMAL
- en: An example of C++ optimization technique that leverages storage choice optimization
    is **Small String Optimization** (**SSO**). SSO attempts to use local storage
    for short strings if they are smaller than a certain size (typically 32 characters)
    instead of the default of dynamically allocated memory for string content storage.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, you should think carefully about where the data gets stored during
    the execution of your program, especially in the critical sections. We should
    try to use registers and local variables as much as possible and optimize cache
    performance. Use volatile, static, global, and dynamic memory only when necessary
    or when it does not affect performance on the critical path.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing data types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C++ integer operations are typically super-fast as long as the size of the largest
    register is larger than the integer size. Integers smaller or larger than the
    register size are sometimes slightly slower than regular integers. This is because
    the processor must use multiple registers for a single variable and apply some
    carry-over logic for large integers. Conversely, handling integers smaller than
    a register size is usually handled by using a regular register, zeroing out the
    upper bits, using only the lower bits, and possibly invoking a type conversion
    operation. Note that the extra overhead is very small and generally not something
    to worry about. Signed and unsigned integers are equally fast, but in some cases
    unsigned integers are faster than signed integers. The only cases where signed
    integer operations are a tiny bit slower is where the processor needs to check
    and adjust for the sign bit. Again, the extra overhead is extremely small when
    present and not necessarily something we need to worry about in most cases. We
    will look at the cost of different operations – addition, subtraction, comparison,
    bit operations, and so on typically take a single clock cycle. Multiplication
    operations take longer, and division operations take longest.
  prefs: []
  type: TYPE_NORMAL
- en: Using casting and conversion operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Converting between signed and unsigned integers is free. Converting integers
    from a smaller size into a larger one can take a single clock cycle but sometimes
    can be optimized to be free. Converting integer sizes down from a larger size
    into a smaller one has no additional cost.
  prefs: []
  type: TYPE_NORMAL
- en: Conversion between floats, doubles, and long doubles is typically free except
    under very few conditions. Conversion of signed and unsigned integers into floats
    or doubles takes a few clock cycles. Conversion from unsigned integers can take
    longer than signed integers.
  prefs: []
  type: TYPE_NORMAL
- en: Conversion from floating-point values into integers can be extremely expensive
    – 50 to 100 clock cycles or more. If these conversions are on the critical path,
    it is common for low-latency application developers to try and make these more
    efficient by enabling special instruction sets, avoiding or refactoring these
    conversions, if possible, using special assembly language rounding implementations,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Converting pointers from one type into another type is completely free; whether
    the conversions are safe or not is the developer’s responsibility. Type-casting
    a pointer to an object to a pointer to a different object violates the strict
    aliasing rule stating that *two pointers of different types cannot point to the
    same memory location*, which really means that it is possible the compiler might
    not use the same register to store the two different pointers, even though they
    point to the same address. Remember that the CPU registers are the fastest form
    of storage available to the processor but are extremely limited in storage capacity.
    So, when an extra register gets used to store the same variable, it is an inefficient
    use of the registers and negatively impacts performance overall.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of type-casting a pointer to be a different object is presented
    here. This example uses a conversion from `double *` into `uint64_t *` and modifies
    the sign bit using the `uint64_t` pointer. This is nothing more than a convoluted
    and more efficient method of achieving `x = -std::abs(x)` but demonstrates how
    this violates the strict aliasing rule (`strict_alias.cpp` in `Chapter3` on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It yields something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using modern C++ casting operations, `const_cast`, `static_cast`, and `reinterpret_cast`
    do not incur any additional overhead when used. However, when it comes to `dynamic_cast`,
    which converts an object of a certain class into an object of a different class,
    this can be expensive at runtime. `dynamic_cast` checks whether the conversion
    is valid using **Run-Time Type Information** (**RTTI**), which is slow and possibly
    throws an exception if the conversion is invalid – this makes it safer but increases
    latency.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing numerical operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Typically, double-precision calculations take about the same time as single-precision
    operations. In general, for integers and floating values, additions are fast,
    multiplications are slightly more expensive than additions, and division is quite
    a bit more expensive than multiplication. Integer multiplications take around
    5 clock cycles and floating-point multiplications take around 8 clock cycles.
    Integer additions take a single clock cycle on most processors and floating-point
    additions take around 2-5 clock cycles. Floating-point divisions and integer divisions
    both take about the same amount of time around 20-80 clock cycles, depending on
    the processor and depending on whether it has special floating-point operations
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: Compilers will try to rewrite and reduce expressions wherever possible to prefer
    faster operations such as rewriting divisions to be multiplications by reciprocals.
    Multiplication and division by values that are powers of 2 are significantly faster
    because the compiler rewrites them to be bit-shift operations, which are much
    faster. There is additional overhead when the compiler uses this optimization
    since it must handle signs and rounding errors. Obviously, this only applies when
    the expressions involve values that can be determined to be powers of 2 at compile
    time. When dealing with multi-dimensional arrays, for instance, the compiler converts
    multiplications into bitwise shift operations wherever possible.
  prefs: []
  type: TYPE_NORMAL
- en: Mixing single- and double-precision operations in the same expression and expressions
    involving floating and integer values should be avoided because they implicitly
    force type conversions. We saw before that type conversions are not always free,
    so these expressions can take longer to compute than we would guess. For instance,
    when mixing single- and double-precision values in an expression, the single-precision
    values must first be converted into double-precision values, which can consume
    a few clock cycles before the expression is computed. Similarly, when mixing integers
    and floating-point values in an expression, either the floating-point value has
    to be converted into an integer or the integer must be converted into a floating-point
    value, which adds a few clock cycles to the final calculation time.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing boolean and bitwise operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boolean operations such as `&&`) and `||`) are evaluated such that for `&&`,
    if the first operand is false, then the second one is not evaluated, and, for
    `||`, if the first operand is true, then the second one is not evaluated. A simple
    optimization technique is to order the operands of `&&` in order from lowest to
    highest probability of being evaluated to true.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, for `||`, ordering the operands from highest to lowest probability
    of being true is best. This technique is referred to as `&&` boolean operation,
    the second operand should not be evaluated if the first one is false. Or for an
    `||` boolean operation, the second operand should not be evaluated if the first
    one is true, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another aspect of using boolean variables is understanding the way they are
    stored. Boolean variables are stored as 8 bits and not a single bit, as might
    match our intuition from the way they are used. What this means is that operations
    involving boolean values have to be implemented such that any 8-bit values other
    than 0 are treated as 1, which leads to implementations with branches in them
    with comparisons against 0\. For example, the `c = a && b;` expression is implemented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If there was a guarantee that `a` and `b` could not have values other than 0
    or 1, then `c = a && b;` would simply be `c = a & b;`, which is super-fast and
    avoids branching and branching-related overheads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bitwise operations can also help speed up other cases of boolean expressions
    by treating each bit of an integer as a single boolean variable and then rewriting
    expressions involving comparisons of multiple booleans with bit-masking operations.
    For instance, take an expression such as this, where `market_state` is `uint64_t`
    and `PreOpen`, `Opening`, and `Trading` are enum values that reflect different
    market states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be rewritten as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If the enum values are chosen such that each bit in the `market_state` variable
    represents a state of true or false, one choice would be for the `PreOpen`, `Opening`,
    and `Trading` enums to be set to `0x001`, `0x010`, and `0x100`.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing, destroying, copying, and moving objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Constructors and destructors for developer-defined classes should be kept as
    light and efficient as possible since they can be called without the developer
    expecting it. Keeping these methods super-simple and small also allows the compiler
    to *inline* these methods to improve performance. The same applies to copy and
    move constructors, which should be kept simple, with using move constructors preferred
    over using copy constructors wherever possible. In many cases where high levels
    of optimization are required, the developer can delete the default constructor
    and the copy constructor to make sure unnecessary or unexpected copies of their
    objects are not being made.
  prefs: []
  type: TYPE_NORMAL
- en: Using references and pointers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A lot of C++ features are built around implicitly accessing class members through
    the `this` pointer, so access through references and pointers occurs very frequently
    regardless of whether the developer explicitly does so or not. Accessing objects
    through pointers and references is mostly as efficient as directly accessing the
    objects. This is because most modern processors have support to efficiently fetch
    the pointer values and dereference them. The big disadvantage of using references
    and pointers is that they take up an extra register for the pointer themselves
    and the other one consists of the extra dereference instructions to access the
    variable pointed to by the pointer value.
  prefs: []
  type: TYPE_NORMAL
- en: Pointer arithmetic is just as fast as integer arithmetic except computing the
    differences between pointers requires a division by the size of the object, which
    can potentially be very slow. This is not necessarily a problem if the size of
    the type of object is a multiple of 2, which is quite often the case with primitive
    types and optimized structures.
  prefs: []
  type: TYPE_NORMAL
- en: Smart pointers are an important feature of modern C++ that offers safety, life
    cycle management, automatic memory management, and clear ownership control for
    dynamically allocated objects. Smart pointers such as `std::unique_ptr`, `std::shared_ptr`,
    and `std::weak_ptr` use the `std::shared_ptr` due to the reference counting overhead
    but generally, smart pointers are expected to add very little overhead to the
    entire program unless there are a lot of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important aspect of using pointers is that it can prevent compiler
    optimizations due to `a[0]` to `a[n-1]` and `b`. That means that this optimization
    is valid because `*b` is a constant for the entire loop and can be computed once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'There are really two options for instructing the compiler to assume no pointer
    aliasing in cases where the developer is confident that there is no behavior that
    is dependent on the side effects of pointer aliasing. Use `__restrict__`, or `__restrict`,
    a similar specifier keyword, for your compiler on the function arguments or functions
    to specify no aliasing on the pointers. However, this is a hint, and the compiler
    does not guarantee that this will make a difference. The other option is to specify
    the `-fstrict-aliasing` compiler option to assume no pointer aliasing globally.
    The following code block demonstrates the use of the `restrict` specifier for
    the preceding `func()` function (`pointer_alias.cpp` in `Chapter3` on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Optimizing jumping and branching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In modern processor pipelines, instructions and data are fetched and decoded
    in stages. When there is a branch instruction, the processor tries to predict
    which of the branches will be taken and fetches and decodes instructions from
    that branch. However, when the processor has mispredicted the branch taken, it
    takes 10 or more clock cycles before it detects the misprediction. After that,
    it must spend a bunch of clock cycles fetching the instructions and data from
    the correct branch and evaluate it. The key takeaway here is that a branch misprediction
    wastes many clock cycles every time it happens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us discuss some of the most used forms of jumps and branches in C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '`if-else` branching is the most common thing that comes to mind when discussing
    branching. Long chains of `if-else` conditionals are best avoided, if possible,
    because it is difficult to predict these correctly as they grow. Keeping the number
    of conditions small and trying to structure them so they are more predictable
    is the way to optimize them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`for` and `while` loops are also types of branching that are typically predicted
    well if the loop count is relatively small. This, of course, gets complicated
    with nested loops and loops containing hard-to-predict exit conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`switch` statements are branches with multiple jump targets, so they can be
    very difficult to predict. When label values are widely spread out, the compiler
    must use `switch` statements as a long sequence of `if-else` branching trees.
    An optimization technique that works well with `switch` statements is to assign
    case label values that increment by one and are arranged in ascending order because
    there is a very good chance they will get implemented as jump tables, which is
    significantly more efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing branching with table lookups containing different output values in
    the source code is a good optimization wherever possible. We can also create a
    table of function pointers indexed by jump conditions but beware that function
    pointers are not necessarily much more efficient than the branching itself.
  prefs: []
  type: TYPE_NORMAL
- en: '`loop_unroll.cpp` in `Chapter3` on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The compiler can unroll the loop into the following code shown here. Note that
    it is more than likely that for such a simple example, the compiler will use additional
    optimizations and reduce this loop even further. But for now, we limit ourselves
    to only present the impact of loop unrolling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Compile-time branching using an `if constexpr (condition-expression) {}` format
    can obviously help a lot by moving the overhead of branching to compile time,
    but this requires that `condition-expression` be something that can be evaluated
    at compile time. This is technically part of the **Compile time Polymorphism**
    or **Template Metaprogramming** paradigm, which we will discuss more in the *Using
    compile-time polymorphism* sub-section in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to provide the compiler with branch prediction hints in the
    source code since the developer has a better idea of the expected use cases. These
    do not make a significant difference overall since modern processors are good
    at learning which branches are most likely to be taken after a few iterations
    through the branches. For GNU C++, these are traditionally implemented as follows
    using `__builtin_expect`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For C++ 20, these are standardized as the `[[likely]]` and `[[``unlikely]]`
    attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Calling functions efficiently
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are numerous overheads associated with calling functions – the overhead
    of fetching the function address and jumping to it, passing the parameters to
    it and returning the results, setting up the stack frame, saving and restoring
    registers, exception handling, possible latency in the code cache misses, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: When breaking up the code base into functions, some general things to consider
    to maximize the performance would be the following.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking before creating an excessive number of functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Functions should only be created if there is enough re-usability to justify
    them. The criteria for creating functions should be logical program flow and re-usability
    and not the length of code because, as we saw, calling functions is not free,
    and creating excessive functions is not a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping related functions together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Class member and non-class member functions typically get assigned memory addresses
    in the order in which they are created, so it is generally a good idea to group
    together performance-critical functions that call each other frequently or operate
    on the same datasets. This facilitates better code and data cache performance.
  prefs: []
  type: TYPE_NORMAL
- en: Link Time Optimization (LTO) or Whole Program Optimization (WPO)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When writing performance-critical functions, it is important to place them in
    the same module where they are used if possible. Doing so unlocks a lot of compiler
    optimizations, the most important of which is the ability to inline the function
    call.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `static` keyword to declare a function does the equivalent of putting
    it in an `inline` keyword achieves this as well, but we will explore that in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying WPO and LTO parameters for the compiler instructs it to treat the
    entire code base as a single module and enable compiler optimizations across modules.
    Without enabling these compiler options, optimizations occur across functions
    in the same module but not between modules which can be quite sub-optimal for
    large code bases which typically have a lot of source files and modules.
  prefs: []
  type: TYPE_NORMAL
- en: Macros, inline functions, and template metaprogramming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Macro expressions** are a pre-processor directive and are expanded even before
    compilation begins. This eliminates the overhead associated with calling and returning
    from functions at runtime. Macros have several disadvantages though, such as namespace
    collision, cryptic compilation errors, unnecessary evaluation of conditions and
    expressions, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Inlined functions, whether they are part of a class or not, are similar to macros
    but solve a lot of the problems associated with macros. Inlined functions are
    expanded at their usage during compilation and link times and eliminate the overhead
    associated with function calls.
  prefs: []
  type: TYPE_NORMAL
- en: Using template metaprogramming, it is possible to move a lot of the computation
    load from runtime to compile time. This involves using partial and full template
    specialization and recursive loop templates. However, template metaprogramming
    can be clumsy and difficult to use, compile, and debug and should only really
    be used where the performance improvements justify the increased development discomfort.
    We will explore templates and template metaprogramming shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding function pointers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Calling a function through a function pointer has a larger overhead than directly
    calling the function. For one, if the pointer changes, then the compiler cannot
    predict which function will be called and cannot pre-fetch the instructions and
    data. Additionally, this also prevents a lot of compiler optimizations since these
    cannot be inlined at compile time.
  prefs: []
  type: TYPE_NORMAL
- en: The `std::function` is a much more powerful construct available in modern C++
    but should be used only if necessary since there is potential for misuse and extra
    overhead of a few clock cycles compared to direct inlined functions. `std::bind`
    is another construct to be very careful about when using and should also only
    be used if absolutely necessary. If `std::function` must be used, try to see whether
    you can use a lambda expression instead of `std::bind` since that is typically
    a few clock cycles faster to invoke. Overall, be careful when using `std::function`
    and/or `std::bind` since a lot of developers are surprised that these constructs
    can perform virtual function calls and invoke dynamic memory allocations under
    the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Passing function parameters by reference or pointers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For primitive types, passing parameters by value is super-efficient. For composite
    types that are function parameters, the preferred way of passing them would be
    const references. The **constness** means that the object cannot be modified and
    allows the compiler to apply optimizations based on that and the reference allows
    the compiler to possibly inline the object itself. If the function needs to modify
    the object passed to it, then obviously a non-const reference or pointer is the
    way to go.
  prefs: []
  type: TYPE_NORMAL
- en: Returning simple types from functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Functions that return primitive types are very efficient. Returning composite
    types is much more inefficient and can lead to a couple of copies being created
    in some cases, which is quite sub-optimal especially if these are large and/or
    have slow copy constructors and assignment operators. When the compiler can apply
    **Return Value Optimization** (**RVO**), it can eliminate the temporary copy created
    and just write the result to the caller’s object directly. The optimal way to
    return a composite type is to have the caller create an object of that type and
    pass it to the function using a reference or a pointer for the function to modify.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at an example to explain what happens with RVO; let us say we have
    the following function definition and call to the function (`rvo.cpp` in `Chapter3`
    on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: With RVO, instead of creating a temporary `LargeClass` object inside `rvoExample()`
    and then copying it into the `LargeClass lc_obj` object in `main()`, the `rvoExample()`
    function can directly update `lc_obj` and avoid the temporary object and copy.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding recursive functions or replacing them with a loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recursive functions are inefficient because of the overhead of calling themselves
    repeatedly. Additionally, recursive functions can go very deep in the stack and
    take up a lot of stack space, and, in worst-case scenarios, even cause a stack
    overflow. This causes a lot of cache misses due to the new memory areas and makes
    predicting the return address difficult and inefficient. In such cases, replacing
    recursive functions with a loop is significantly more efficient since it avoids
    a lot of the cache performance issues that recursive functions encounter.
  prefs: []
  type: TYPE_NORMAL
- en: Using bitfields
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bitfields** are just structs where the developer controls the number of bits
    assigned to each member. This makes the data as compact as possible and greatly
    improves cache performance for many objects. Bitfield members are also usually
    modified using bitmask operations, which are very efficient, as we have seen before.
    Accessing the members of bitfields is less efficient than accessing the members
    of a regular structure, so it is important to carefully assess whether using bitfields
    and improving the cache **performance** is worthwhile.'
  prefs: []
  type: TYPE_NORMAL
- en: Using runtime polymorphism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Virtual` functions are the key to implementing runtime polymorphism, but they
    have an additional overhead compared to non-virtual function calls.'
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the compiler cannot determine at compile time which implementation
    of a virtual function will be called. At runtime, this causes many branch mispredictions
    unless the same version of the virtual function gets called most of the time.
    It is possible for the compiler to determine the virtual function implementation
    called at compile time using `virtual` functions is that the compiler cannot apply
    many of the compile-time optimizations in the presence of `virtual` functions,
    the most important one being inlining.
  prefs: []
  type: TYPE_NORMAL
- en: Inheritance in C++ is another important OOP concept but be careful when the
    inheritance structure gets too complicated since there are many subtle inefficiencies
    that can be introduced. Child classes inherit every single data member from their
    parent class, so the size of the child classes can become quite large and lead
    to poor cache performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, instead of inheriting from multiple parent classes, we can consider
    using the `composition.cpp` in `Chapter3` on GitHub) builds `OrderBook`, which
    basically holds a vector of `Order` objects, in two different ways. The benefit
    (if used properly) of the inheritance model is that it now inherits all the methods
    that `std::vector` provides while the composition model would need to implement
    them. In this example, we demonstrate this by implementing a `size()` method in
    `CompositionOrderBook`, which calls the `size()` method on the `std::vector` object,
    while `InheritanceOrderBook` inherits it directly from `std::vector`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: C++ `dynamic_cast`, as we discussed before, usually uses the RTTI information
    to perform the cast and should also be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: Using compile-time polymorphism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us discuss an alternative to using runtime polymorphism, which is to use
    templates to achieve compile-time polymorphism. Templates are similar to macros,
    meaning they are expanded before compilation, and because of this, not only is
    the runtime overhead eliminated but it also unlocks additional compiler optimization
    opportunities. Templates make the compiler machine code super-efficient but they
    come at the cost of additional source code complexity, as well as larger executable
    sizes.
  prefs: []
  type: TYPE_NORMAL
- en: The `virtual` functions and the base class and derived class relationships are
    similar but slightly different using the *CRTP*. A simple example of converting
    runtime polymorphism into compile-time polymorphism is shown here. In both cases,
    the derived classes, `SpecificRuntimeExample` and `SpecificCRTPExample`, override
    the `placeOrder()` method. The code discussed in this sub-section is in the `crtp.cpp`
    file in the GitHub repo for this book under the `Chapter3` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Runtime polymorphism using virtual functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, we have an example of implementing runtime polymorphism where `SpecificRuntimeExample`
    derives `RuntimeExample` and overrides the `placeOrder()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Compile-time polymorphism using the CRTP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we implement similar functionality as discussed in the previous section,
    but instead of using runtime polymorphism, we use compile-time polymorphism. Here,
    we use the CRTP pattern and `SpecificCRTPExample` specializes/implements the `CRTPExample`
    interface and has a different implementation of `placeOrder()` via `actualPlaceOrder()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Invoking polymorphic methods in the two cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, in the following snippet presented, we show how we would create `SpecificRuntimeExample`
    and `SpecificCRTPExample` objects. We then invoke runtime and compile-time polymorphism
    respectively using the `placeOrder()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this yields the following output, the first line using runtime polymorphism
    and the second line using compile time polymorphism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Using additional compile-time processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Template metaprogramming** is a more general term that means writing code
    that itself yields more code. The benefit here is also to move computations from
    runtime to compile time and maximize compiler optimization opportunities and runtime
    performance. It is possible to write almost anything with template metaprogramming,
    but it can get extremely complicated and difficult to understand, maintain, and
    debug, lead to very long compilation times, and increase the binary size to a
    very large size.'
  prefs: []
  type: TYPE_NORMAL
- en: Handling exceptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The C++ exception handling system is designed to detect unexpected error conditions
    at runtime and either gracefully recover or shut down from that point. When it
    comes to low-latency applications, it is important to evaluate the use of exception
    handling since while it is true that exception handling incurs the largest latencies
    during these rare error cases, there can still be some overhead even when exceptions
    are not raised. There is some bookkeeping overhead related to the logic used to
    recover gracefully when exceptions are raised under various scenarios. With nested
    functions, exceptions need to be propagated all the way up to the top-most caller
    function and each stack frame needs to be cleaned up. This is known as **stack
    unwinding** and requires the exception handler to track all the information it
    needs to walk backward during an exception.
  prefs: []
  type: TYPE_NORMAL
- en: For low-latency applications, exceptions are either disabled per function using
    the `throw()` or `noexcept` specifications or disabled across the entire program
    using compiler flags. This allows the compiler to assume that some or all methods
    will not throw an exception and hence the processor does not have to worry about
    saving and tracking recovery information. Note that using `noexcept` or disabling
    the C++ exception handling system is not without some disadvantages. For one,
    usually, the C++ exception handling system does not typically add a lot of extra
    overhead unless an exception is thrown, so this decision must be made with careful
    consideration. Another point is that if a method marked as `noexcept` throws an
    exception for some reason, the exception can no longer be propagated up the stack
    and instead the program is terminated right there. What this means is that disabling
    the C++ exception handling system either partially or fully makes handling failures
    and exceptions harder and completely the developer’s responsibility. Usually,
    what this means is that the developer will still need to make sure that exceptional
    error conditions are not encountered or handled elsewhere, but the point is that
    now the developer has explicit control over this and can move it out of the critical
    hot path. For this reason, it is common that during the development and testing
    phases, the C++ exception handling system is not disabled, but only during the
    very last optimization steps do we consider removing exception handling.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing cache and memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have frequently referred to cache performance while discussing different
    uses of C++ features since accessing the main memory is significantly slower than
    the clock cycles used to execute CPU instructions or access registers or cache
    storage. Here are some general points to keep in mind when trying to optimize
    cache and memory access.
  prefs: []
  type: TYPE_NORMAL
- en: Aligning data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Variables that are aligned, in that they are placed at memory locations that
    are multiples of the size of the variable, are accessed most efficiently. The
    term **word size** for processors describes the number of bits read by and processed
    by processors, which for modern processors is either 32-bits or 64-bits. This
    is because the processor can read a variable from memory up to the word size in
    a single read operation. If the variable is aligned in memory, then the processor
    does not have to do any extra work to get it into the required register to be
    processed.
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, aligned variables are more efficient to handle, and the compiler
    will take care of automatically aligning variables. This includes adding padding
    in between member variables in a class or a struct to keep those variables aligned.
    When adding member variables to structures where we expect to have a lot of objects,
    it is important to consider the extra padding added carefully because the size
    of the struct will be larger than expected. The extra space in each instance of
    this struct’s or class’s objects means that they can have worse cache performance
    if there are a lot of them. The recommended approach here would be to reorder
    the members of the struct so that minimal extra padding is added to keep the members
    aligned.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see an example that orders the same members inside a structure in three
    different ways – one where there is a lot of additional padding added to keep
    each variable aligned, another where the developer reorders the member variables
    to minimize space waste due to compiler-added padding, and, finally, where we
    use the `pack()` pragma to eliminate all padding. This code is available in the
    `Chapter3/alignment.cpp` file in the GitHub repository for this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This code outputs the following on my system, displaying the offsets of the
    different data members in each of the three designs of the same structure. Note
    that the first version has an extra 11 bytes of padding, the second one only has
    an extra 3 bytes of padding due to the reordering, and the last version has no
    extra padding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Accessing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cache-friendly data access (read and/or write) is when the data is accessed
    sequentially or somewhat sequentially. If the data is accessed backward, it is
    less efficient than this, and cache performance is worse if the data is accessed
    randomly. This is something to consider, especially when accessing multi-dimensional
    arrays of objects and/or objects residing in a container with a non-trivial underlying
    storage of the objects.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, accessing elements in an array is significantly more efficient
    than accessing elements in a linked list, tree, or hash-map container because
    of the contiguous memory storage versus random memory storage locations. From
    the perspective of algorithmic complexity, searching linearly in an array is less
    efficient than using a hash map since the array search has `O(n)` and the hash
    map has `O(1)` theoretical algorithmic complexity. However, if the number of elements
    is small enough, then using the array still yields better performance, a large
    reason being due to cache performance and algorithm overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Using large data structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When dealing with large multi-dimensional matrix datasets, for instance, with
    linear algebra operations, cache access performance dominates the performance
    of the operation. Often, the actual algorithm implementation for matrix operations
    is different from that used in classic texts to reorder the matrix access operations
    for cache performance. The best approach here is to measure the performance of
    different algorithms and access patterns and find the one that performs best under
    different matrix dimensions, cache contention conditions, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping variables together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When designing classes and method or non-method functions, grouping variables
    that are accessed together greatly improves cache performance by reducing the
    number of cache misses. We discussed that preferring local variables over global,
    static, and dynamically allocated memory leads to better cache performance.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping functions together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Grouping class member functions and non-member functions together so that functions
    that are used together are close together in memory also leads to better cache
    performance. This is because functions are placed in memory addresses depending
    on where they are in the developer’s source code and functions next to each other
    get assigned addresses close to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamically allocating memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dynamically allocated memory has several good use cases, specifically when the
    size of containers is not known at compile time and when they can grow or shrink
    in size during the application instance’s life cycle. Dynamically allocated memory
    is also important for objects that are very large and take up a lot of stack space.
    Dynamically allocated memory can have a place in low-latency applications if allocation
    and deallocation are not done on the critical path and an allocated block of memory
    is used so that the cache performance is not hurt.
  prefs: []
  type: TYPE_NORMAL
- en: A disadvantage of dynamically allocated memory is that the process of allocating
    and deallocating memory blocks is awfully slow. The repeated allocation and deallocation
    of memory blocks of varied sizes fragments the heap, that is, it creates free
    memory blocks of different sizes interspersed with allocated memory blocks.
  prefs: []
  type: TYPE_NORMAL
- en: A fragmented heap makes the allocation and deallocation process even slower.
    Allocated memory blocks might not be optimally aligned unless the developer is
    careful about it. Dynamically allocated memory accessed through pointers causes
    pointer aliasing and prevents compiler optimizations, as we have seen before.
    There are other disadvantages of dynamically allocated memory, but these are the
    biggest ones for low-latency applications. Hence, it is best to avoid dynamically
    allocated memory completely when it comes to low-latency applications, or at the
    very least use it carefully and sparingly.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-threading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If low-latency applications use multi-threading, the threads and the interactions
    between these threads should be designed carefully. Starting and stopping threads
    takes time, so it is best to avoid launching new threads when they are needed
    and instead use a thread pool of worker threads. Task switching or context switching
    is when one thread is paused or blocked, and another thread starts executing in
    its place. Context switching is very expensive since it requires the OS to save
    the state of the current thread, load the state of the next thread, start the
    processing, and so on, and is usually accompanied by memory reads and writes,
    cache misses, instruction pipeline stalls, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization using locks and mutexes between threads is also expensive and
    involves additional checks around concurrent access and context-switching overhead.
    When multiple threads access shared resources, they need to use the `volatile`
    keyword and that also prevents several compiler optimizations. Additionally, different
    threads can compete for the same cache lines and invalidate each other’s caches
    and this contention leads to terrible cache performance. Each thread gets its
    own stack, so it’s best to keep the shared data to a minimum and allocate variables
    locally on the thread’s stack.
  prefs: []
  type: TYPE_NORMAL
- en: Maximizing C++ compiler optimization parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last section, we will understand how advanced and amazing modern C++
    compilers are at optimizing the C++ code that the developers write. We will understand
    how compilers optimize the C++ code during the compilation, linking, and optimization
    stages to generate the most efficient machine code possible. We will understand
    how compilers optimize high-level C++ code and when they fail to do the best job.
    We will follow that up with a discussion on what the application developer can
    do to aid the compilers in their optimization task. Finally, we will look at different
    options available in modern C++ compilers by looking specifically at the **GNU
    compiler** (**GCC**). Let us start by understanding how compilers optimize our
    C++ program.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how compilers optimize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this sub-section, we will understand the different compiler optimization
    techniques that the compiler employs during its many passes over the high-level
    C++ code. The compiler typically first performs local optimizations and then tries
    to globally optimize these smaller code sections. It does so over several passes
    through the translated machine code during the pre-processing, compilation, linking,
    and optimization stages. Broadly, most compiler optimization techniques have some
    common themes, some of which overlap and some of which conflict with each other,
    which we will look at next.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the common cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This concept applies to software development too and helps the compiler optimize
    the code better. If the compiler can understand which code paths the program execution
    will spend most of its time in, it can optimize the common path to be faster even
    if it slows down the paths that are rarely taken. This results in better performance
    overall, but typically this is harder for the compiler to achieve at compilation
    time since it is not obvious which code paths are expected to be more likely unless
    the developer adds directives to specify this. We will discuss the hints that
    a developer can provide to the compiler to help specify which code paths are expected
    to be more likely during runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing branching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern processors typically prefetch data and instructions before they are required
    so that the processors can execute instructions as quickly as possible. However,
    when there are jumps and branches (conditional and unconditional), the processor
    cannot know which instructions and data will be needed ahead of time with 100%
    certainty. What this means is that sometimes the processor incorrectly predicts
    the branch taken and thus the instructions and data prefetched are incorrect.
    When this happens, there is an extra penalty incurred since now the processor
    must remove the instructions and data that were fetched incorrectly and replace
    them with the correct instructions and data and then execute them after that.
    Techniques such as loop unrolling, inlining, and branch prediction hints help
    reduce branching and the misprediction of branching and improve performance. We
    will explore these concepts in more detail later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several cases in which a developer can refactor code in such a way
    that they avoid branching and achieve the same behavior. Sometimes, these optimization
    opportunities are only available to the developer, who understands the code and
    behavior at a deeper level than the compiler. A very simple example of how to
    convert a code block that uses branching and transform it to avoid branching is
    presented next. Here we have an enumeration to track side for an execution and
    we track the last bought/sold quantity, as well as updating the position in two
    different ways. The first way uses a branch on the `fill_side` variable and the
    second method avoids that branching by assuming that the `fill_side` variable
    can only have `BUY`/`SELL` values and can be cast to integers to be indexed into
    an array. This code can be found in the `Chapter3/branch.cpp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And both the branching and branchless implementations compute the same values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Reordering and scheduling instructions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The compiler can take advantage of advanced processors by re-ordering instructions
    in such a way that parallel processing can happen at the instruction, memory,
    and thread levels. The compiler can detect dependencies between code blocks and
    re-order them so that the program still works correctly but executes faster by
    executing instructions and processing data in parallel at the processor level.
    Modern processors can reorder instructions even without the compiler doing so,
    but it helps if the compiler can make it easier for the processors to do so as
    well. The main objective here is to prevent stalls and bubbles in modern processors,
    which have multiple pipelined processors, by choosing and ordering instructions
    in such a way as to preserve the original logical flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple example of how an expression can be reordered to take advantage of
    parallelism is shown here. Note that this is somewhat hypothetical since the actual
    implementation of this will vary greatly depending on the processor and the compiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As it is written, this expression has a data dependency and would be executed
    sequentially, roughly as follows, and cost 5 clock cycles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be re-ordered into the following instructions, and assuming the advanced
    processor can perform two additions at a time, can be reduced to three clock cycles.
    This is because two operations such as `x = a + b;` and `p = c +d;` can be performed
    in parallel since they are independent of each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Using special instructions depending on the architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During the compilation process, the compiler can choose which CPU instructions
    to use to implement the high-level program logic. When the compiler generates
    an executable for a specific architecture, it can use special instructions that
    the architecture supports. This means there is an opportunity to generate even
    more efficient instruction sequences, which leverage the special instructions
    that the architecture provides. We will look at how to specify this in the *Learning
    about compiler optimization* *flags* section.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern processors can use vector registers to perform multiple calculations
    on multiple pieces of data in parallel. For instance, the SSE2 instruction set
    has 128-bit vector registers, which can be used to perform multiple operations
    on multiple integers or floating values depending on the size of these types.
    Extending this further, the AVX2 instruction set, for example, has 256-bit vector
    registers and can support a higher degree of vectorized operations. This optimization
    can be technically considered as part of the discussion in the *Using special
    instructions depending on the architecture* section from before.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand vectorization even better, let us present the following very
    simple example of a loop that operates on two arrays and stores the result in
    another array (`vector.cpp` in `Chapter3` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For architectures that support special vector registers such as the SSE2 instruction
    set we discussed before, it can hold 4 4-byte float values simultaneously and
    perform 4 additions at a time. In this case, the compiler can leverage the vectorization
    optimization technique and re-write this as the following with loop unrolling
    to use the SSE2 instruction set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Strength reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Strength reduction** is a term used to describe compiler optimizations where
    complex operations that are quite expensive are replaced by instructions that
    are simpler and cheaper to improve performance. A classic example is one in which
    the compiler replaces operations involving division by some value with multiplication
    by the reciprocal of that value. Another example would be replacing multiplication
    by a loop index with an addition operation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest example we could think of is presented here, where we try to convert
    a price from its double notation into its integer notation by dividing the floating
    value by its minimum valid price increment. The variant that demonstrates the
    strength reduction that a compiler would perform is a simple multiplication instead
    of a division. Note that `inv_min_price_increment = 1 / min_price_increment;`
    is a `constexpr` expression, so it is not evaluated at runtime. This code is available
    in the `Chapter3/strength.cpp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Inlining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Calling functions is expensive, as we have already seen before. There are several
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Saving the current state of variables and execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the variables and instructions from the function being called
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing them and possibly returning back values and resuming execution after
    the function call
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The compiler tries to replace a call to a function with the body of the function
    where possible to remove this overhead associated with calling functions and optimize
    performance. Not only that but now that it has replaced a call to a function with
    the actual body of the function, that opens room for more optimizations since
    the compiler can inspect this new larger code block.
  prefs: []
  type: TYPE_NORMAL
- en: Constant folding and constant propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Constant folding** is a no-brainer optimization technique and applies when
    there are expressions whose output can be computed entirely at compile time that
    do not depend on runtime branches or variables. Then, the compiler computes these
    expressions at compile time and replaces the evaluation of these expressions with
    the compile-time constant output value.'
  prefs: []
  type: TYPE_NORMAL
- en: A similar and closely related compiler optimization tracks values in the code
    that are known to be compile-time constants and tries to propagate those constant
    values and unlock additional optimization opportunities. This optimization technique
    is known as **constant propagation**. An example would be loop unrolling if the
    compiler can determine the starting value, incremental value, or stopping value
    of the loop iterator.
  prefs: []
  type: TYPE_NORMAL
- en: Dead Code Elimination (DCE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**DCE** applies when the compiler can detect code blocks that have no impact
    on the program behavior. This can be due to code blocks that are never needed
    or code blocks where the calculations do not end up being used or affect the outcome.
    Once the compiler detects such *dead* code blocks, it can remove them and boost
    program performance. Modern compilers emit warnings when the outcome of running
    some code ends up not being used to help developers find such cases, but the compiler
    cannot detect all of these cases at compile time and there are still opportunities
    for DOE once it is translated into machine code instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: Common Subexpression Elimination (CSE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**CSE** is a specific optimization technique where the compiler finds duplicated
    sets of instructions or calculations. Here, the compiler restructures the code
    to remove this redundancy by computing the result only once and then using the
    value where it is required.'
  prefs: []
  type: TYPE_NORMAL
- en: Peephole optimizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Peephole optimization** is a relatively generic compiler optimization term
    that refers to a compiler optimization technique where the compiler tries to search
    for local optimizations in short sequences of instructions. We use the term local
    because the compiler does not necessarily try to understand the entire program
    and optimize it globally. Of course, however, by repeatedly and iteratively performing
    peephole optimizations, the compiler can achieve a decent degree of optimization
    at a global scale.'
  prefs: []
  type: TYPE_NORMAL
- en: Tail call optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know that function calls are not cheap because they have overhead associated
    with passing parameters and results and affect the cache performance and processor
    pipeline. `__attribute__ ((noinline))` attribute, which is there to explicitly
    prevent the compiler from inlining the `factorial()` function directly into `main()`.
    You can find this example in the `Chapter3/tail_call.cpp` source file on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'For this implementation, we would expect that in the machine code for the `factorial()`
    function, we would find a call to itself, but when compiled with optimization
    turned on, the compiler performs tail call optimization and implements the `factorial()`
    function as a loop and not a recursion. To observe that machine code, you can
    compile this code with something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: And in that `tail_call.s` file, you will see the call to `factorial()` in `main()`
    to be something like the following example. If this is your first time looking
    at assembly code, then let us quickly describe the instructions you will encounter.
  prefs: []
  type: TYPE_NORMAL
- en: The `movl` instruction moves a value into a register (100 in the following block)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `call` instruction calls a function (`factorial()` with name mangling (step
    where the C++ compiler changes the function names in intermediate code) and the
    parameter is passed in the `edi` register)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `testl` instruction compares two registers and sets the zero flag if they’re
    equal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`je` and `jne` check whether the zero flag is set and jump to the specified
    memory address if it is (`je`) or jump to the specified memory address if it is
    not (`jne`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `ret` instruction returns from the function and the return value is in
    the `eax` register:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When you look at the `factorial()` function itself, you will find a loop (the
    `je` and `jne` instructions) instead of an additional `call` instruction to itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Loop unrolling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Loop unrolling** duplicates the body of the loop multiple times. Sometimes,
    it is not possible for the compiler to know at compile time how many times the
    loop will be executed – in which case, it will partially unroll the loop. For
    loops where the loop body is small and/or where it can be determined that the
    number of times that the loop will execute is low, the compiler can completely
    unroll the loop. This avoids the need for checking the loop counters and the overhead
    associated with conditional branching or looping. This is like function inlining
    where the call to the function is replaced by the body of the function. For loop
    unrolling, the entire loop is rolled out and replaces the conditional loop body.'
  prefs: []
  type: TYPE_NORMAL
- en: Additional loop optimizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Loop unrolling** is the primary loop-related optimization technique employed
    by compilers but there are additional loop optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loop fission** breaks a loop down into multiple loops operating on smaller
    sets of data to improve cache reference locality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loop fusion** does the opposite, where if two adjacent loops are executed
    the same number of times, they can be merged into one to reduce the loop overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`while` loop is transformed into a `do-while` loop inside a conditional `if`
    statement. This reduces the total number of jumps by two when the loop is executed
    and is typically applied to loops that are expected to execute at least once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loop interchange** exchanges inner loops and outer loops especially when
    doing so leads to better cache reference locality – for example, in the cases
    of iterating over an array where accessing memory contiguously makes a huge performance
    difference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Register variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Registers** are internal processor memory and are the fastest form of storage
    available for the processor on account of being the closest to them. Because of
    this, the compiler tries to store variables that have the highest number of accesses
    in the registers. Registers, however, are limited, so the compiler needs to choose
    the variables to store effectively, and the effectiveness of this choice can make
    a significant difference to performance. The compiler typically picks variables
    such as local variables, loop counter and iterator variables, function parameters,
    commonly used expressions, or **induction variables** (variables that change by
    fixed amounts on each loop iteration). There are some limitations to what the
    compiler can place in registers such as variables whose address needs to be taken
    via pointers or references that need to reside in the main memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we present a very simple example of how a compiler will transform a loop
    expression using induction variables. See the following code (`Chapter3/induction.cpp`
    on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Live range analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The term **live range** describes the code block within which a variable is
    active or used. If there are multiple variables in the same code block with overlapping
    live ranges, then each variable needs a different storage location. However, if
    there are variables with live ranges that do not overlap, then the compiler can
    use the same register for multiple variables in each live range.
  prefs: []
  type: TYPE_NORMAL
- en: Rematerialization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Rematerialization** is a compiler technique where the compiler chooses to
    re-calculate a value (assuming the calculation is trivial) instead of accessing
    the memory location that contains the value of this calculation already. The output
    value of this recalculation must be stored in registers, so this technique works
    in tandem with *register allocation techniques*. The main objective here is to
    avoid accessing the caches and main memory, which are slower to access than accessing
    the register storage. This, of course, depends on making sure that the recalculation
    takes less time than a cache or memory access.'
  prefs: []
  type: TYPE_NORMAL
- en: Algebraic reductions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The compiler can find expressions that can be further reduced and simplified
    using algebraic laws. While software developers do not unnecessarily complicate
    expressions, there are cases where simpler forms of expressions exist compared
    to what the developer originally wrote in C++. Opportunities for algebraic reductions
    also show up as the compiler optimizes code iteratively due to inlining, macro
    expansions, constant folding, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Something to note here is that compilers do not typically apply algebraic reductions
    to floating-point operations because, in C++, floating-point operations are not
    safe to reduce due to precision issues. Flags need to be turned on to force the
    compiler to perform unsafe floating-point algebraic reductions, but it would be
    preferable for developers to reduce them explicitly and correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest example we can think of here is where a compiler might rewrite
    this expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, it uses two operations instead of three previously like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Induction variable analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea behind **induction variable**-related compiler optimization techniques
    is that an expression that is a linear function of the loop counter variable can
    be reduced into an expression that is a simple addition to a previous value. The
    simplest possible example would be calculating the address of elements in an array
    where the next element is at a memory location equal to the current element’s
    location plus the size of the object type. This is just a simple example since
    in modern compilers and processors, there are special instructions to calculate
    addresses of array elements and induction is not really used there, but induction
    variable-based optimizations are still performed for other loop expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Loop invariant code movement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When the compiler can ascertain that some code and instructions within a loop
    are constant for the entire duration of the loop, that expression can be moved
    out of the loop. If there are expressions within the loop that conditionally yield
    one value or the other depending on branching conditions, those can also be moved
    out of the loop. Also, if there are expressions executed on each branch within
    a loop, these can be moved out of the branches and possibly the loop. There are
    many such optimization possibilities, but the fundamental idea is that code that
    does not need to be executed on each loop iteration or can be evaluated once before
    the loop falls under the umbrella of loop invariant code refactoring. Here is
    a hypothetical example of how loop invariant code movement implemented by the
    compiler would work. The first block is what the developer originally wrote, but
    the compiler can understand that the call to `doSomething()` and the expression
    involving the `b` variable are loop invariants and only need to be computed once.
    You will find this code in the `Chapter3/loop_invariant.cpp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Static Single Assignment (SSA)-based optimizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SSA is a transformed form of the original program where instructions are re-ordered
    such that every variable is assigned in a single place. After this transformation,
    the compiler can apply many additional optimizations, leveraging the property
    that every variable is assigned in only a single place.
  prefs: []
  type: TYPE_NORMAL
- en: Devirtualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Devirtualization** is a compiler optimization technique, especially for C++,
    that tries to avoid **Virtual Table** (**vtable**) lookups when calling virtual
    functions. This optimization technique boils down to the compiler figuring out
    the correct method to call at compile time. This can happen even when using virtual
    functions because in some cases, the object type is known at compile time, such
    as when there is only a single implementation of pure virtual functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Another case is where the compiler can determine that only a single derived
    class is created and used in some contexts or code branches, and it can replace
    the indirect functional call using vtable to be a direct call to the correct derived
    type’s method.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding when compilers fail to optimize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss the different scenarios under which a compiler
    cannot apply some of the optimization techniques we discussed in the previous
    section. Understanding when compilers fail to optimize will help us develop C++
    code that avoids these failures so that the code can be highly optimized by the
    compiler to yield highly efficient machine code.
  prefs: []
  type: TYPE_NORMAL
- en: Failure to optimize across modules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the compiler compiles the entire program, it compiles modules independently
    of each other on a file-by-file basis. So, the compiler does not have information
    about functions in a module other than the one it is currently compiling. This
    prevents it from being able to optimize functions across modules and a lot of
    the techniques we saw cannot be applied since the compiler does not understand
    the whole program. Modern compilers solve such issues by using **LTO**, where,
    after the individual modules are compiled, the linker treats the different modules
    as if they were part of the same translation unit at compile time. This activates
    all the optimizations we have discussed so far, so it is important to enable LTO
    when trying to optimize the entire application.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic memory allocation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We already know that dynamic memory allocation is slow at runtime and introduces
    non-deterministic latency into your applications. They also have another side
    effect and that is **pointer aliasing** in the pointers that point to these dynamically
    allocated memory blocks. We will look at pointer aliasing in more detail next,
    but with dynamically allocated memory blocks, the compiler cannot ascertain that
    the pointers will necessarily point to different and non-overlapping memory areas,
    even though for the programmer it might seem obvious. This prevents various compiler
    optimizations that depend on aligning data or assuming alignment, as well as pointer
    aliasing-related inefficiencies, which we will see next. Local storage and declarations
    are also more cache-efficient because the memory space gets reused frequently
    as new functions are called and local objects are created. Dynamically allocated
    memory blocks can be randomly scattered in memory and yield poor cache performance.
  prefs: []
  type: TYPE_NORMAL
- en: Pointer aliasing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When accessing variables through pointers or references, while it might be obvious
    to the developer which pointers point to different and non-overlapping memory
    locations, the compiler cannot be 100% sure. To put it another way, the compiler
    cannot guarantee that a pointer is not pointing to another variable in the code
    block or different pointers are not pointing to overlapping memory locations.
    Since the compiler must assume this possibility, this prevents a lot of the compiler
    optimizations we discussed before since they can no longer be applied safely.
    There are ways to specify which pointers the compiler can safely assume are not
    aliases in C++ code. Another way would be to instruct the compiler to assume no
    pointer aliasing across the entire code, but that would require the developer
    to analyze all pointers and references and make sure there is never any aliasing,
    which is not trivial to do. Finally, the last option is to optimize the code explicitly
    keeping these hindrances to compiler optimizations in mind, which is not trivial
    either.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our advice on dealing with pointer aliasing would be to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `__restrict` keyword in the function declarations when passing pointers
    to functions to instruct the compiler to assume no pointer aliasing for the pointers
    marked with that specifier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If additional optimization is required, we recommend explicitly optimizing code
    paths, being aware of pointer aliasing considerations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, if additional optimizations are still required, we can instruct the
    compiler to assume no pointer aliasing across the entire code base, but this is
    a dangerous option and should only be used as a last resort
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Floating-point induction variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compilers typically do not use induction variable optimizations for floating-point
    expressions and variables. This is because of the rounding errors and issues with
    precision that we have discussed before. This prevents compiler optimizations
    when dealing with floating-point expressions and values. There are compiler options
    that can enable unsafe floating-point optimizations, but the developer must make
    sure to check each expression and formulate them in such a way that these precision
    issues due to compiler optimizations do not have unintended side effects. This
    is not a trivial task; hence, developers should be careful to either optimize
    floating-point expressions explicitly or analyze side effects from unsafe compiler
    optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual functions and function pointers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have already discussed that when it comes to virtual functions and function
    pointers, the compiler cannot perform optimizations at compile time since in many
    cases it is not possible for the compiler to determine which method will be called
    at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about compiler optimization flags
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have discussed the different optimization techniques that the compiler
    uses, as well as the different cases where the compiler fails to optimize our
    C++ code. There are two fundamental keys to generating optimized low-latency code.
    The first is to write efficient C++ code and optimize manually in cases where
    the compiler might not be able to do so. Secondly, you can provide the compiler
    with as much visibility and information as possible so it can make the correct
    and best optimization decisions. We can convey our intent to the compiler through
    the compiler flags we use to configure it.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn about the compiler flags for the GCC since that
    is the compiler we will use in this book. However, most modern compilers have
    flags to configure optimizations like the ones we will discuss in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Approaching compiler optimization flags
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At a high level, the general approach toward GCC compiler optimization flags
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The highest optimization level is typically preferred so `–O3` is a good starting
    point and enables a lot of optimizations, which we will see shortly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the performance of the application in practice is the best way to
    measure and optimize the most critical code paths. GCC itself can perform `-fprofile-generate`
    option is enabled. The compiler determines the flow of the program and counts
    how many times each function and code branch is executed to find optimizations
    for the critical code paths.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling `–flto` parameter enables LTO for our applications. The `-fwhole-program`
    option enables **WPO** to enable inter-procedural optimizations, treating the
    entire code base as a whole program.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allowing the compiler to generate a build for a specific architecture where
    the application will run is a good idea. This lets the compiler use special instruction
    sets specific to that architecture and maximize optimization opportunities. For
    GCC, this is enabled using the `–``march` parameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is recommended to disable `-``no-rtti` parameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to instruct the GCC compiler to enable fast floating-point value
    optimizations and even enable unsafe floating-point optimizations. GCC has the
    `-ffp-model=fast`, `-funsafe-math-optimizations` and `-ffinite-math-only` options
    to enable these unsafe floating-point optimizations. When using these flags, it
    is important that the developer carefully thinks about the order of operations
    and the precision resulting from these operations. When using a parameter such
    as `-ffinite-math-only`, make sure that all floating-point variables and expressions
    are finite because this optimization depends on that property. `-fno-trapping-math`
    and `-fno-math-errno` allow the compiler to vectorize loops containing floating-point
    operations by assuming that there will be no reliance on exception handling or
    the `errno` global variable for error signaling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the details of GCC optimization flags
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will provide additional details on the GCC optimization
    flags available. The complete list of optimization flags available is exceptionally
    large and out of the scope of this book. First, we will describe what turning
    on the higher-level optimization directives, `–O1`, `–O2`, and `–O3`, enables
    in GCC, and we encourage interested readers to learn about each one of these in
    greater detail from the GCC manual.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization level -O1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`–O1` is the first level of optimization and enables the following flags presented
    in the following table. At this level, the compiler tries to reduce the code size
    and execution time without incurring a very large increase in compilation, linking,
    and optimization times. These are the most important levels of optimization and
    provide tremendous optimization opportunities based on the ones we discussed in
    this chapter. We will discuss a few of the flags next.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-fdce` and `–fdse` perform DCE and **Dead Store** **Elimination** (**DSE**).'
  prefs: []
  type: TYPE_NORMAL
- en: '`-fdelayed-branch` is supported on many architectures and tries to reorder
    instructions to try and maximize the throughput of the pipeline after delayed
    branch instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-fguess-branch-probability` tries to guess branch probabilities based on heuristics
    for branches that the developer has not provided any hints.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-fif-conversion` and `-fif-conversion2` try to eliminate branching by changing
    them into branchless equivalents using tricks similar to what we discussed in
    this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-fmove-loop-invariants` enables loop invariant code movement optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested, you should investigate the details of these flags since
    discussing every parameter is outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '| `-``fauto-inc-dec` | `-``fshrink-wrap` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fbranch-count-reg` | `-``fshrink-wrap-separate` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fcombine-stack-adjustments` | `-``fsplit-wide-types` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fcompare-elim` | `-``fssa-backprop` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fcprop-registers` | `-``fssa-phiopt` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fdce` | `-``ftree-bit-ccp` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fdefer-pop` | `-``ftree-ccp` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fdelayed-branch` | `-``ftree-ch` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fdse` | `-``ftree-coalesce-vars` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fforward-propagate` | `-``ftree-copy-prop` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fguess-branch-probability` | `-``ftree-dce` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fif-conversion` | `-``ftree-dominator-opts` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fif-conversion2` | `-``ftree-dse` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``finline-functions-called-once` | `-``ftree-forwprop` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fipa-modref` | `-``ftree-fre` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fipa-profile` | `-``ftree-phiprop` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fipa-pure-const` | `-``ftree-pta` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fipa-reference` | `-``ftree-scev-cprop` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fipa-reference-addressable` | `-``ftree-sink` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fmerge-constants162` | `-``ftree-slsr` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fmove-loop-invariants` | `-``ftree-sra` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fmove-loop-stores` | `-``ftree-ter` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fomit-frame-pointer` | `-``funit-at-a-time` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``freorder-blocks` |  |'
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – GCC optimization flags enabled when -O1 is enabled
  prefs: []
  type: TYPE_NORMAL
- en: Optimization level -O2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`-O2` is the next optimization level and at this level, GCC will perform a
    lot more optimizations and will lead to longer compilation and linking times.
    `-O2` adds the flags in the following table in addition to the flags enabled by
    `–O1`. We will quickly discuss a few of these flags and leave a detailed discussion
    of each flag up to interested readers to pursue.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-falign-functions`, `-falign-labels`, and `-falign-loops` align the starting
    address of functions, jump targets, and loop locations so that the processor can
    access them as efficiently as possible. The principles we discussed on optimal
    data alignment in this chapter apply to the instruction addresses as well.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-fdelete-null-pointer-checks` lets the program assume that dereferencing null
    pointers is not safe and leverages that assumption to perform constant folding,
    eliminate null pointer checks, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-fdevirtualize` and `-fdevirtualize-speculatively` attempt to convert virtual
    function calls into direct function calls wherever possible. This, in turn, can
    lead to even more optimization due to inlining.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-fgcse` enables **Global Common Subexpression Elimination** (**GCSE**) and
    constant propagation.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-finline-functions`, `-finline-functions-called-once`, and `-findirect-inlining`
    increase the aggressiveness of the compiler in its attempts to inline functions
    and look for indirect inline opportunities due to previous optimization passes.'
  prefs: []
  type: TYPE_NORMAL
- en: '| `-``falign-functions -falign-jumps` | `-``foptimize-sibling-calls` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``falign-labels -falign-loops` | `-``foptimize-strlen` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fcaller-saves` | `-``fpartial-inlining` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fcode-hoisting` | `-``fpeephole2` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fcrossjumping` | `-``freorder-blocks-algorithm=stc` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fcse-follow-jumps -fcse-skip-blocks` | `-``freorder-blocks-and-partition
    -freorder-functions` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fdelete-null-pointer-checks` | `-``frerun-cse-after-loop` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fdevirtualize -fdevirtualize-speculatively` | `-``fschedule-insns -fschedule-insns2`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fexpensive-optimizations` | `-``fsched-interblock -fsched-spec` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``ffinite-loops` | `-``fstore-merging` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fgcse -fgcse-lm` | `-``fstrict-aliasing` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fhoist-adjacent-loads` | `-``fthread-jumps` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``finline-functions` | `-``ftree-builtin-call-dce` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``finline-small-functions` | `-``ftree-loop-vectorize` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``findirect-inlining` | `-``ftree-pre` |'
  prefs: []
  type: TYPE_TB
- en: '| `-fipa-bit-cp -``fipa-cp -fipa-icf` | `-``ftree-slp-vectorize` |'
  prefs: []
  type: TYPE_TB
- en: '| `-fipa-ra -``fipa-sra -fipa-vrp` | `-``ftree-switch-conversion -ftree-tail-merge`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fisolate-erroneous-paths-dereference` | `-``ftree-vrp` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``flra-remat` | `-``fvect-cost-model=very-cheap` |'
  prefs: []
  type: TYPE_TB
- en: Table 3.2 – GCC optimization flags enabled in addition to the ones from -O1
    when -O2 is enabled
  prefs: []
  type: TYPE_NORMAL
- en: Optimization level –O3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`–O3` is the most aggressive optimization option in GCC and it will optimize
    even when it leads to larger executable sizes as long as the program performs
    better. `-O3` enables the following flags presented in the next table beyond `–O2`.
    We quickly discuss a few important ones first and then provide the complete list.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-fipa-cp-clone` creates function clones to make interprocedural constant propagation
    and other forms of optimization stronger by trading execution speed at the cost
    of higher executable sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-fsplit-loops` attempts to split a loop if it can avoid branching within the
    loop by having the loop for one side and then the other side – for instance, in
    a case where we check the side of execution in a trading algorithm within a loop
    and execute two different code blocks within the loop.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-funswitch-loops` moves loop invariant branches out of the loop to minimize
    branching.'
  prefs: []
  type: TYPE_NORMAL
- en: '| `-``fgcse-after-reload` | `-``fsplit-paths` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fipa-cp-clone -floop-interchange` | `-``ftree-loop-distribution` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``floop-unroll-and-jam` | `-``ftree-partial-pre` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fpeel-loops` | `-``funswitch-loops` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fpredictive-commoning` | `-``fvect-cost-model=dynamic` |'
  prefs: []
  type: TYPE_TB
- en: '| `-``fsplit-loops` | `-``fversion-loops-for-strides` |'
  prefs: []
  type: TYPE_TB
- en: Table 3.3 – GCC optimization flags enabled in addition to the ones from -O2
    when -O3 is enabled
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss some additional compiler optimization flags we have found useful
    when it comes to optimizing low-latency applications.
  prefs: []
  type: TYPE_NORMAL
- en: Static linkage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `–l library` option is passed to the linker to specify which library to
    link the executables with. However, if the linker finds a static library that
    has a name such as `liblibrary.a` and a shared library that has a name such as
    `liblibrary.so`, then we must specify the `–static` parameter to prevent linking
    with shared libraries and opt for the static library instead. We have discussed
    before why static linkage is preferred over shared library linkage for low-latency
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Target architecture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `–march` parameter is used to specify the target architecture for which
    the compiler should build the final executable binary. For example, `–march=native`
    specifies that the compiler should build the executable for the architecture that
    it is being built on. We reiterate here that when the compiler knows the target
    architecture that the application is being built to run on, it can leverage information
    about that architecture, such as extended instruction sets and so on, to improve
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Warnings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The`–Wall`, `–Wextra`, and `–Wpendantic` parameters control the number of warnings
    that are generated by the compiler when it detects a variety of different cases
    that are not technically errors but could be unsafe. It is advisable to turn these
    on for most applications because they detect potential bugs and typos in developers’
    code. While these do not directly affect the compiler’s ability to optimize the
    application, sometimes, the warnings force developers to inspect cases of ambiguity
    or sub-optimal code, such as unexpected or implicit type conversions, which can
    be inefficient. The `–Werror` parameter turns these warnings into errors and will
    force the developer to inspect and fix each case that generates a compiler warning
    before compilation can succeed.
  prefs: []
  type: TYPE_NORMAL
- en: Unsafe fast math
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This category of compiler optimization flags should not be enabled without
    a lot of consideration and due diligence. In C++, the compiler cannot apply a
    lot of floating-point optimizations that depend on properties such as floating-point
    operations yielding valid values, floating-point expressions being associative,
    and so on. To recap, this is because of the way floating-point values are represented
    in hardware, and a lot of these optimizations can lead to precision loss and different
    (and possibly incorrect) results. Enabling the `–ffast-math` parameter in turn
    enables the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`–``fno-math-errno`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`–``funsafe-math-optimizations`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`–``ffinite-math-only`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`–``fno-rounding-math`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`–``fno-signaling-nans`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`–``fcx-limited-range`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`–``fexcess-precision=fast`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These parameters will allow the compiler to apply optimizations to floating-point
    expressions even if they are unsafe. These are not automatically enabled in any
    of the three optimization levels because they are unsafe and should only be enabled
    if the developer is confident that there are no errors or side effects that show
    up because of these.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, first, we discussed general advice that applies to developing
    low-latency applications in any programming language. We discussed the ideal software
    engineering approach when it comes to these applications and how to think about,
    design, develop, and evaluate building blocks such as the data structures and
    algorithms to use.
  prefs: []
  type: TYPE_NORMAL
- en: We emphasized that when it comes to low-latency application development specifically,
    the depth of knowledge on topics such as processor architecture, cache and memory
    layout and access, how the C++ programming language works under the hood, and
    how the compiler works to optimize your code will dictate your success. Measuring
    and improving performance is also a critical component for low-latency applications
    but we will dive into those details at the end of this book.
  prefs: []
  type: TYPE_NORMAL
- en: We spent a lot of time discussing different C++ principles, constructs, and
    features with the objective of understanding how they are implemented at a lower
    level. The goal here was to unlearn sub-optimal practices and emphasize some of
    the ideal aspects of using C++ for low-latency application development.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of this book, as we build our low-latency electronic trading
    exchange ecosystem (collection of applications that interact with each other),
    we will reinforce and build on these ideas we discussed here as we avoid certain
    C++ features and use others instead.
  prefs: []
  type: TYPE_NORMAL
- en: In the last section of this chapter, we discussed many aspects of the C++ compiler
    in detail. We tried to build an understanding of how compilers optimize developers’
    high-level code, as in, what techniques they have at their disposal. We also investigated
    scenarios in which the compiler fails to optimize a developer’s code. The goal
    there was for you to understand how to use a compiler to your advantage when trying
    to output the most optimal machine code possible and help the compiler help you
    avoid conditions where the compiler fails to optimize. Finally, we looked at the
    different compiler optimization flags available for the GNU GCC compiler, which
    is what we will use in the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: We will put our theoretical knowledge into practice in the next chapter where
    we jump into implementing some common building blocks of low-latency applications
    in C++. We will keep our goal of building these components to be low-latency and
    highly performant. We will carefully use the principles and techniques we discussed
    in this chapter to build these high-performance components. In later chapters,
    we will use these components to build an electronic trading ecosystem.
  prefs: []
  type: TYPE_NORMAL
