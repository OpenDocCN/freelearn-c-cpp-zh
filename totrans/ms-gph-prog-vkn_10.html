<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-149"><a id="_idTextAnchor152"/>10</h1>
<h1 id="_idParaDest-150"><a id="_idTextAnchor153"/>Adding Volumetric Fog</h1>
<p>After adding variable rate shading in the previous chapter, we will implement another modern technique that will enhance the visuals of the Raptor Engine: <strong class="bold">Volumetric Fog</strong>. Volumetric rendering and fog are very old topics in rendering literature, but until a few years ago, they were considered impossible for real-time usage.</p>
<p>The possibility of making this technique feasible in real-time stems from the observation that fog is a low-frequency effect; thus the rendering can be at a much lower resolution than the screen, increasing the performance in real-time usage.</p>
<p>Also, the introduction of compute shaders, and thus generic GPU programming, paired with clever observations about approximations and optimizations of the volumetric aspect of the technique, paved the way to unlocking real-time Volumetric Fog.</p>
<p>The main idea comes from the seminal paper by Bart Wronski (<a href="https://bartwronski.files.wordpress.com/2014/08/bwronski_volumetric_fog_siggraph2014.pdf">https://bartwronski.files.wordpress.com/2014/08/bwronski_volumetric_fog_siggraph2014.pdf</a>) at Siggraph 2014, where he described what is still the core idea behind this technique even after almost 10 years.</p>
<p>Implementing this technique will also be important for learning more about the synergies between different rendering parts of a frame: developing a single technique can be challenging, but the interaction with the rest of the technology is a very important part as well and can add to the challenge of the technique</p>
<p>In this chapter, we’ll cover the following main topics:</p>
<ul>
<li>Introducing Volumetric Fog rendering</li>
<li>Implementing the Volumetric Fog base technique</li>
<li>Adding spatial and temporal filtering to improve visuals</li>
</ul>
<p>By the end of this chapter, we will have Volumetric Fog integrated into the Raptor Engine, interacting with the scenery and all the dynamic lights, as shown in the following figure:</p>
<div><div><img alt="Figure 10.1 – Volumetric Fog with a density volume and three shadow casting lights" height="630" src="img/B18395_10_01.jpg" width="1099"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Volumetric Fog with a density volume and three shadow casting lights</p>
<h1 id="_idParaDest-151"><a id="_idTextAnchor154"/>Technical requirements</h1>
<p>The code for this chapter can be found at the following URL: <a href="https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter10">https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter10</a>.</p>
<h1 id="_idParaDest-152"><a id="_idTextAnchor155"/>Introducing Volumetric Fog Rendering</h1>
<p>What exactly is <strong class="bold">Volumetric Fog Rendering</strong>? As the name suggests, it is the combination <a id="_idIndexMarker507"/>of Volumetric Rendering and the fog phenomena. We will now give some background on those components and see how they are combined in the final technique.</p>
<p>Let’s begin with Volumetric Rendering.</p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor156"/>Volumetric Rendering</h2>
<p>This rendering technique describes the visuals associated with what happens to light when it <a id="_idIndexMarker508"/>travels through a participating medium. A participating medium is a volume that contains local changes to density or albedo.</p>
<p>The following diagram summarizes what happens to photons in a participating medium:</p>
<div><div><img alt="Figure 10.2 – Light behavior in a participating medium" height="1180" src="img/B18395_10_02.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Light behavior in a participating medium</p>
<p>What we are trying to describe is how light changes when going through a participating medium, namely a fog volume (or clouds or atmospheric scattering).</p>
<p>There are three main phenomena that happen, as follows:</p>
<ul>
<li><strong class="bold">Absorption</strong>: This <a id="_idIndexMarker509"/>happens when light is simply trapped inside the medium and does not go outside. It is a net loss of energy.</li>
<li><strong class="bold">Out-scattering</strong>: This is <a id="_idIndexMarker510"/>depicted using green arrows in <em class="italic">Figure 10</em><em class="italic">.2</em> and is again a loss of energy coming out (and thus visible) from the medium.</li>
<li><strong class="bold">In-scattering</strong>: This <a id="_idIndexMarker511"/>is the energy coming from the lights that are interacting with the medium.</li>
</ul>
<p>While these three phenomena are enough to describe what happens to light, there are three other components that need to be understood before having a complete picture of volumetric rendering.</p>
<h3>Phase function</h3>
<p>The first <a id="_idIndexMarker512"/>component is the <strong class="bold">phase function</strong>. This function <a id="_idIndexMarker513"/>describes the scattering of light in different directions. It is dependent on the angle between the light vector and the outgoing directions.</p>
<p>This function ca<a id="_idTextAnchor157"/>n be complex and tries to describe scattering in a realistic way, but<a id="_idTextAnchor158"/> the most commonly used is the Henyey-Greenstein function, a function that also takes into consideration anisotropy.</p>
<p>The formula for the Henyey-Greenstein function is as follows:</p>
<div><div><img alt="Figure 10.3 – The Henyey-Greenstein function" height="135" src="img/B18395_10_03.jpg" width="851"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – The Henyey-Greenstein function</p>
<p>In the preceding equation, the angle theta is the angle between the view vector and the light vector. We will see in the shader code how to translate this to something usable.</p>
<h3>Extinction</h3>
<p>The second <a id="_idIndexMarker514"/>component is <strong class="bold">extinction</strong>. Extinction is <a id="_idIndexMarker515"/>a quantity that describes how much light is scattered. We will use this in the intermediate steps of the algorithm, but to apply the calculated fog, we will need transmittance.</p>
<h3>Transmittance</h3>
<p>The third <a id="_idIndexMarker516"/>and final component is <strong class="bold">transmittance</strong>. Transmittance is the <a id="_idIndexMarker517"/>extinction of light through a segment of the medium, and it is calculated using the Beer-Lambert law:</p>
<div><div><img alt="Figure 10.4 – The Beer-Lambert law" height="83" src="img/B18395_10_04.jpg" width="638"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – The Beer-Lambert law</p>
<p>In the final integration step, we will calculate the transmittance and use it to choose how to apply fog to the scene. The important thing here is to get a basic grasp of the concepts; there will be links provided to deepen your understanding of the mathematical background at the end of the chapter.</p>
<p>We now have all the concepts needed to see the implementation details of Volumetric Fog.</p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor159"/>Volumetric Fog</h2>
<p>Now that <a id="_idIndexMarker518"/>we have an idea of the different components that contribute to Volumetric Rendering, we can take a bird’s-eye view of the algorithm. One of the first and most clever ideas that Bart Wronski had while developing this technique is the usage of a Frustum Aligned Volume Texture, like so:</p>
<div><div><img alt="Figure 10.5 – Frustum Aligned Volume Texture" height="583" src="img/B18395_10_05.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Frustum Aligned Volume Texture</p>
<p>Using a volume texture and math associated with standard rasterization rendering, we can create a mapping between the camera frustum and the texture. This mapping is already happening in the different stages of rendering, for example, when multiplying a vertex position for the view-projection matrix, so it is not something new.</p>
<p>What is new is storing information in a volume texture to calculate the volumetric rendering. Each element <a id="_idIndexMarker519"/>of this texture is commonly called the <strong class="bold">froxel</strong>, that st<a id="_idTextAnchor160"/>ands for <strong class="bold">frustum voxel</strong>.</p>
<p>We chose to have a texture with a width, height, and depth of 128 units, but other solutions use a width and height dependent on the screen resolution, similar to clustered shading.</p>
<p>We will <a id="_idIndexMarker520"/>use different textures with this resolution as an intermediate step, and for additional filtering, we will discuss this later. One additional decision is to increase the resolution of the camera by using a non-linear depth distribution to map a linear range to an exponential one.</p>
<p>We will use a distribution function, such as the one used by Id in their iD Tech engine, like so:</p>
<div><div><img alt="Figure 10.6 – Volume texture depth slice on the Z coordinate function" height="72" src="img/B18395_10_06.jpg" width="1153"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Volume texture depth slice on the Z coordinate function</p>
<p>Now that we have decided on the mapping between the volumetric texture and world units, we can describe the steps needed to have a fully working Volumetric Fog solution.</p>
<p>The algorithm is outlined in the following diagram, where rectangles represent shader executions while ellipses represent textures:</p>
<div><div><img alt="Figure 10.7 – Algorithm overview" height="1021" src="img/B18395_10_07.jpg" width="1644"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Algorithm overview</p>
<p>We will <a id="_idIndexMarker521"/>now see each step of the algorithm to create a mind model of what is happening, and we will review the shader later in the chapter.</p>
<h3>Data injection</h3>
<p>The first <a id="_idIndexMarker522"/>step is the data injection. This shader will add some colored fog in the form of color and density into the first Frustum Aligned Texture containing only data. We decided to add a constant fog, a heig<a id="_idTextAnchor161"/>ht-based fog, and a fog volume to mimic a more realistic game development setup.</p>
<h3>Light scattering</h3>
<p>When <a id="_idIndexMarker523"/>performing the light scattering, we calculate the in-scattering coming from the lights in the scene.</p>
<p>Having a working Clustered Lighting algorithm, we will reuse the same data structures to calculate the light contribution for each froxel, paying attention to treating the light in a different way than the standard Clustered Lighting – we don’t have diffuse or specular here, but just a global term given by attenuation, shadow, and phase.</p>
<p>We also sample shadow maps associated with the lights for even more realistic behavior.</p>
<h3>Spatial filtering</h3>
<p>To remove <a id="_idIndexMarker524"/>some of the noise, we apply a Gaussian filter only on the <em class="italic">X</em> and <em class="italic">Y</em> axis of the Frustum Aligned Texture, and then we pass to the most important filter, the temporal one.</p>
<h3>Temporal filtering</h3>
<p>This filter is what really improves the visuals by giving the possibility of adding some noise at <a id="_idIndexMarker525"/>different steps of the algorithm to remove some banding. It will read the previous frame’s final texture (the one before the integration) and blend the current light scattering result with the previous one based on some constant factor.</p>
<p>This is a very difficult topic, as temporal filtering and reprojection can cause a few issues. We will <a id="_idIndexMarker526"/>have a much bigger discussion in the next chapter when talking about <strong class="bold">Temporal </strong><strong class="bold">Anti-Aliasing</strong> (<strong class="bold">TAA</strong>)</p>
<p>With the scattering and extinction finalized, we can perform the light integration and thus prepare the texture that will be sampled by the scene.</p>
<h3>Light integration</h3>
<p>This step prepares another Frustum Aligned Volumetric Texture to contain an integration <a id="_idIndexMarker527"/>of the fog. Basically, this shader simulates a low-resolution ray marching so that this result can be sampled by the scene.</p>
<p>Ray marching normally starts from the camera toward the far plane of the scene. The combination of the Frustum Aligned Texture and this integration gives, for each froxel, a cached ray marching of the light scattering to be easily sampled by the scene. In this step, from all the extinction saved in previous textures, we finally calculate the transmittance with the Beer-Lambert law and use that to merge the fog into the scene.</p>
<p>This and temporal filtering are some of the big innovations that unlocked the real-time possibility of this algorithm. In more advanced solutions, such as in the game Red Dead Redemption 2, an additional ray marching can be added to simulate fog at much further distances.</p>
<p>It also allows for blending fog and Volumetric Clouds, which use a pure ray marching approach, to have <a id="_idIndexMarker528"/>an almost seamless transition. This is explained in detail in the Siggraph presentation about Red Dead Redemption 2 rendering.</p>
<h3>Scene application in Clustered Lighting</h3>
<p>The final step is to read the Volumetric Texture in the lighting shader using the world position. We can read the depth buffer, calculate the world position, calculate the froxel coordinates and sample the texture.</p>
<p>An <a id="_idIndexMarker529"/>additional step to further smooth the volumetric look is to render to a half-resolution texture the scene application and then apply it to the scene with a geometry-aware upsampling, but this will be left as an exercise for you to complete.</p>
<h1 id="_idParaDest-155"><a id="_idTextAnchor162"/>Implementing Volumetric Fog Rendering</h1>
<p>We now have all the knowledge necessary to read the code needed to get this algorithm fully working. From a CPU perspective, it is just a series of compute shaders dispa<a id="_idTextAnchor163"/>tches, so it is straightforward.</p>
<p>The core <a id="_idIndexMarker530"/>of this technique is implemented throughout various shaders, and thus on the GPU, working for almost all steps on the frustum aligned Volumetric Texture we talked about in the previous section.</p>
<p><em class="italic">Figure 10</em><em class="italic">.7</em> shows the different algorithm steps, and we will see each one individually in the following sections.</p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor164"/>Data injection</h2>
<p>In the <a id="_idIndexMarker531"/>first shader, we will write scattering <a id="_idIndexMarker532"/>and extinction, starting from the color and density of different fog phenomena.</p>
<p>We decided to add three different fog effects, as follows:</p>
<ul>
<li>A constant fog</li>
<li>Height fog</li>
<li>Fog in a volume</li>
</ul>
<p>For each fog, we need to calculate scattering and extinction and accumulate them.</p>
<p>The following <a id="_idIndexMarker533"/>code converts color and density to scattering and extinction:</p>
<pre class="source-code">
vec4 scattering_extinction_from_color_density( vec3 color,
    float density ) {
    const float extinction = scattering_factor * density;
    return vec4( color * extinction, extinction );
}</pre>
<p>We can <a id="_idIndexMarker534"/>now have a look at the main shader. This shader, as most of the others in this chapter, will be scheduled to have one thread for one froxel cell.</p>
<p>In the first section, we will see the dispatch and code to calculate world position:</p>
<pre class="source-code">
layout (local_size_x = 8, local_size_y = 8, local_size_z =
        1) in;
void main() {
    ivec3 froxel_coord = ivec3(gl_GlobalInvocationID.xyz);
    vec3 world_position = world_from_froxel(froxel_coord);
    vec4 scattering_extinction = vec4(0);</pre>
<p>We add an optional noise to animate the fog and break the constant density:</p>
<pre class="source-code">
    vec3 sampling_coord = world_position *
       volumetric_noise_position_multiplier +
       vec3(1,0.1,2) * current_frame *
       volumetric_noise_speed_multiplier; 
    vec4 sampled_noise = texture(
       global_textures_3d[volumetric_noise_texture_index],
       sampling_coord);
    float fog_noise = sampled_noise.x;</pre>
<p>Here, we add <a id="_idIndexMarker535"/>and accumulate constant fog:</p>
<pre class="source-code">
    // Add constant fog
    float fog_density = density_modifier * fog_noise;
    scattering_extinction += 
       scattering_extinction_from_color_density( 
       vec3(0.5), fog_density ); </pre>
<p>Then, add <a id="_idIndexMarker536"/>and accumulate height fog:</p>
<pre class="source-code">
    // Add height fog
    float height_fog = height_fog_density *
       exp(-height_fog_falloff * max(world_position.y, 0)) *
       fog_noise;
    scattering_extinction += 
       scattering_extinction_from_color_density( 
       vec3(0.5), height_fog ); </pre>
<p>And finally, add density from a box:</p>
<pre class="source-code">
    // Add density from box
    vec3 box = abs(world_position - box_position);
    if (all(lessThanEqual(box, box_size))) {
        vec4 box_fog_color = unpack_color_rgba( box_color
                                              );
        scattering_extinction +=
            scattering_extinction_from_color_density(
                box_fog_color.rgb, box_fog_density *
                    fog_noise);
    }</pre>
<p>We finally <a id="_idIndexMarker537"/>store the scattering and extinction, ready to be lit in the next <a id="_idIndexMarker538"/>shader:</p>
<pre class="source-code">
    imageStore(global_images_3d[froxel_data_texture_index],
               froxel_coord.xyz, scattering_extinction );
}</pre>
<h2 id="_idParaDest-157"><a id="_idTextAnchor165"/>Calculating the lighting contribution</h2>
<p>Lighting will be performed using the Clustered Lighting data structures already used in general <a id="_idIndexMarker539"/>lighting functions. In this shader, we calculate the in-scattering of light.</p>
<p>Shader dispatching is the same as for the previous shader, one thread for one froxel:</p>
<pre class="source-code">
layout (local_size_x = 8, local_size_y = 8, local_size_z =
        1) in;
void main() {
    ivec3 froxel_coord = ivec3(gl_GlobalInvocationID.xyz);
    vec3 world_position = world_from_froxel(froxel_coord);
    vec3 rcp_froxel_dim = 1.0f / froxel_dimensions.xyz;</pre>
<p>We read scattering and extinction from the result of the injection shader:</p>
<pre class="source-code">
vec4 scattering_extinction = texture(global_textures_3d 
   [nonuniformEXT(froxel_data_texture_index)], 
   froxel_coord * rcp_froxel_dim);
   float extinction = scattering_extinction.a;</pre>
<p>We then start accumulating light and using c<a id="_idTextAnchor166"/>lustered bins.</p>
<p>Notice <a id="_idIndexMarker540"/>the cooperation between different rendering algorithms: having the clustered bin already developed, we can use that to query lights in a defined volume starting from the world space position:</p>
<pre class="source-code">
vec3 lighting = vec3(0);
vec3 V = normalize(camera_position.xyz - world_position);
// Read clustered lighting data
// Calculate linear depth
float linear_d = froxel_coord.z * 1.0f /
   froxel_dimension_z;
linear_d = raw_depth_to_linear_depth(linear_d,
   froxel_near, froxel_far) / froxel_far;
// Select bin
int bin_index = int( linear_d / BIN_WIDTH );
uint bin_value = bins[ bin_index ];
// As in calculate_lighting method, cycle through
// lights to calculate contribution
for ( uint light_id = min_light_id;
    light_id &lt;= max_light_id;
    ++light_id ) {
    // Same as calculate_lighting method
    // Calculate point light contribution
    // Read shadow map for current light
    float shadow = current_depth –
       bias &lt; closest_depth ? 1 : 0;
    const vec3 L = normalize(light_position –
       world_position);
    float attenuation = attenuation_square_falloff(
       L, 1.0f / light_radius) * shadow;</pre>
<p>Up <a id="_idIndexMarker541"/>until now, the code is almost identical to the one used in lighting, but we add <code>phase_function</code> to finalize the lighting factor:</p>
<pre class="source-code">
    lighting += point_light.color *
       point_light.intensity *
       phase_function(V, -L,
         phase_anisotropy_01) *
       attenuation;
                    }</pre>
<p>Final <a id="_idIndexMarker542"/>scattering is calculated and stored, as follows:</p>
<pre class="source-code">
vec3 scattering = scattering_extinction.rgb * lighting;
imageStore( global_images_3d
            [light_scattering_texture_index],
            ivec3(froxel_coord.xyz), vec4(scattering,
            extinction) );
}</pre>
<p>We will now have a look at the integration/ray marching shader to conclude the main shaders needed to have the algorithm work for the volumetric part.</p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor167"/>Integrating scattering and extinction</h2>
<p>This shader is responsible for performing the ray marching in the froxel texture and performing <a id="_idIndexMarker543"/>the intermediate calculations in each cell. It will still write in a frustum-aligned texture, but each cell will contain the accumulated scattering and transmittance starting from that cell.</p>
<p>Notice that we now use transmittance instead of extinction, transmittance being a quantity that integrates extinction to a certain space. The dispatch is just on the <em class="italic">X</em> and <em class="italic">Y</em> axis of the frustum texture, reading the light scattering texture, as we will perform the integration steps and write to each froxel in the main loop.</p>
<p>The final stored result is scattering and transmittance, so it can be easier to apply it to the scene:</p>
<pre class="source-code">
// Dispatch with Z = 1 as we perform the integration.
layout (local_size_x = 8, local_size_y = 8, local_size_z =
        1) in;
void main() {
    ivec3 froxel_coord = ivec3(gl_GlobalInvocationID.xyz);
    vec3 integrated_scattering = vec3(0,0,0);
    float integrated_transmittance = 1.0f;
    float current_z = 0;
    vec3 rcp_froxel_dim = 1.0f / froxel_dimensions.xyz;</pre>
<p>We <a id="_idIndexMarker544"/>integrate on the <em class="italic">Z</em> axis as this texture is frustum aligned.</p>
<p>First, we calculate the depth difference to have the thickness needed for the extinction integral:</p>
<pre class="source-code">
    for ( int z = 0; z &lt; froxel_dimension_z; ++z ) {
        froxel_coord.z = z;
         float next_z = slice_to_exponential_depth(
                        froxel_near, froxel_far, z + 1,
                        int(froxel_dimension_z) );
        const float z_step = abs(next_z - current_z);
        current_z = next_z;</pre>
<p>We will calculate scattering and transmittance and accumulate them for the following cell on the <em class="italic">Z</em> axis:</p>
<pre class="source-code">
        // Following equations from Physically Based Sky,
           Atmosphere and Cloud Rendering by Hillaire
        const vec4 sampled_scattering_extinction =
        texture(global_textures_3d[
        nonuniformEXT(light_scattering_texture_index)],
        froxel_coord * rcp_froxel_dim);
        const vec3 sampled_scattering =
            sampled_scattering_extinction.xyz;
        const float sampled_extinction =
            sampled_scattering_extinction.w;
        const float clamped_extinction =
            max(sampled_extinction, 0.00001f);
        const float transmittance = exp(-sampled_extinction
                                        * z_step);
        const vec3 scattering = (sampled_scattering –
                                (sampled_scattering *
                                transmittance)) /
                                clamped_extinction;
        integrated_scattering += scattering *
                                 integrated_transmittance;
        integrated_transmittance *= transmittance;
        imageStore( global_images_3d[
           integrated_light_scattering_texture_index],
           froxel_coord.xyz,
           vec4(integrated_scattering,
              integrated_transmittance) );
    }
}</pre>
<p>We now have a volume texture containing ray marched scattering and transmittance values <a id="_idIndexMarker545"/>that can be queried from anywhere in the frame to know how much fog there is and what color it is at that point.</p>
<p>This concludes the main volumetric rendering aspect of the algorithm. We will now have a look at how easy it is to apply the fog to a scene.</p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor168"/>Applying Volumetric Fog to the scene</h2>
<p>We can finally apply the Volumetric Fog. To do that, we use the screen space coordinates to calculate <a id="_idIndexMarker546"/>the sampling coordinates for the texture. This function will be used at the end of the lighting calculations for both deferred and forward rendering paths.</p>
<p>We first calculate the sampling coordinates:</p>
<pre class="source-code">
vec3 apply_volumetric_fog( vec2 screen_uv, float raw_depth,
                           vec3 color ) {
    const float near = volumetric_fog_near;
    const float far = volumetric_fog_far;
    // Fog linear depth distribution
    float linear_depth = raw_depth_to_linear_depth(
                         raw_depth, near, far );
    // Exponential
    float depth_uv = linear_depth_to_uv( near, far,
        linear_depth, volumetric_fog_num_slices );
vec4 scattering_transmittance =
   texture(global_textures_3d
   [nonuniformEXT(volumetric_fog_texture_index)], 
   froxel_uvw);</pre>
<p>After we read the scattering and transmittance at the specified position, we use the transmittance <a id="_idIndexMarker547"/>to modulate the current scene color and add the fog scattered color, like so:</p>
<pre class="source-code">
    color.rgb = color.rgb * scattering_transmittance.a +
                scattering_transmittance.rgb;
    return color;
}</pre>
<p>And this concludes the necessary steps to fully implement Volumetric Fog rendering. But still, there is <a id="_idIndexMarker548"/>a big problem: <strong class="bold">banding</strong>.</p>
<p>This is a large topic covered in several papers, but for the sake of simplicity, we can say that having a low-resolution volume texture adds banding problems, but it is necessary for achieving real-time performance.</p>
<h2 id="_idParaDest-160"><a id="_idTextAnchor169"/>Adding filters</h2>
<p>To further <a id="_idIndexMarker549"/>improve the visuals, we add two different filters: a temporal and a spatial one.</p>
<p>The temporal filter is what really makes the difference because it gives us the possibility of adding noise in different parts of the algorithm and thus removing banding. The spatial filter smooths out the fog even further.</p>
<h3>Spatial filtering</h3>
<p>This shader <a id="_idIndexMarker550"/>will smooth out the volumetric texture in the <em class="italic">X</em> and <em class="italic">Y</em> axis by <a id="_idIndexMarker551"/>applying a Gaussian filter. It will read the result of the light scattering and write into the froxel data texture, unused at this point of the frame, removing the need to create a temporary texture.</p>
<p>We first define the Gaussian function and its representing code:</p>
<pre class="source-code">
#define SIGMA_FILTER 4.0
#define RADIUS 2
float gaussian(float radius, float sigma) {
    const float v = radius / sigma;
    return exp(-(v*v));
}</pre>
<p>We then <a id="_idIndexMarker552"/>read the light scattering texture and accumulate values and weight <a id="_idIndexMarker553"/>only if the calculated coordinates are valid:</p>
<pre class="source-code">
    vec4 scattering_extinction =
       texture( global_textures_3d[
       nonuniformEXT(light_scattering_texture_index)],
       froxel_coord * rcp_froxel_dim );
    if ( use_spatial_filtering == 1 ) {
        float accumulated_weight = 0;
        vec4 accumulated_scattering_extinction = vec4(0);
        for (int i = -RADIUS; i &lt;= RADIUS; ++i ) {
            for (int j = -RADIUS; j &lt;= RADIUS; ++j ) {
                ivec3 coord = froxel_coord + ivec3(i, j,
                                                   0);
                // if inside
                if (all(greaterThanEqual(coord, ivec3(0)))
                    &amp;&amp; all(lessThanEqual(coord,
                    ivec3(froxel_dimension_x,
                    froxel_dimension_y,
                    froxel_dimension_z)))) {
                    const float weight =
                        gaussian(length(ivec2(i, j)),
                            SIGMA_FILTER);
                    const vec4 sampled_value =
                      texture(global_textures_3d[
                        nonuniformEXT(
                          light_scattering_texture_index)],
                            coord * rcp_froxel_dim);
                  accumulated_scattering_extinction.rgba +=
                      sampled_value.rgba * weight;
                    accumulated_weight += weight;
                }
            }
        }
        scattering_extinction =
           accumulated_scattering_extinction /
           accumulated_weight;
    }</pre>
<p>We store <a id="_idIndexMarker554"/>the result <a id="_idIndexMarker555"/>in the froxel data texture:</p>
<pre class="source-code">
    imageStore(global_images_3d[froxel_data_texture_index],
               froxel_coord.xyz, scattering_extinction );
}</pre>
<p>The next step is temporal filtering.</p>
<h3>Temporal filtering</h3>
<p>This shader <a id="_idIndexMarker556"/>will take the currently calculated 3D light scattering texture and apply a temporal filter. In order to do that it will need two textures, one for <a id="_idIndexMarker557"/>the current and one for the previous frame, and thanks to bindless, we just need to change the indices to use them.</p>
<p>Dispatch is like most of the shaders in this chapter, with one thread for each froxel element of the volume texture. Let’s begin with reading the current light scattering texture.</p>
<p>This currently resides in <code>froxel_data_texture</code>, coming from the spatial filtering:</p>
<pre class="source-code">
    vec4 scattering_extinction =
       texture( global_textures_3d[
       nonuniformEXT(froxel_data_texture_index)],
       froxel_coord * rcp_froxel_dim );</pre>
<p>We need to calculate the previous screen space position to read the previous frame texture.</p>
<p>We will calculate the world position and then use the previous view projection to get the UVW coordinates to read the texture:</p>
<pre class="source-code">
    // Temporal reprojection
    if (use_temporal_reprojection == 1) {
        vec3 world_position_no_jitter =
            world_from_froxel_no_jitter(froxel_coord);
        vec4 sceen_space_center_last =
            previous_view_projection *
                vec4(world_position_no_jitter, 1.0);
        vec3 ndc = sceen_space_center_last.xyz /
                   sceen_space_center_last.w;
        float linear_depth = raw_depth_to_linear_depth(
                             ndc.z, froxel_near, froxel_far
                             );
        float depth_uv = linear_depth_to_uv( froxel_near,
                         froxel_far, linear_depth,
                         int(froxel_dimension_z) );
        vec3 history_uv = vec3( ndc.x * .5 + .5, ndc.y * -
                                .5 + .5, depth_uv );</pre>
<p>We then <a id="_idIndexMarker558"/>check whether the calculated UVWs are valid and if so, we will <a id="_idIndexMarker559"/>read the previous texture:</p>
<pre class="source-code">
        // If history UV is outside the frustum, skip
        if (all(greaterThanEqual(history_uv, vec3(0.0f)))
             &amp;&amp; all(lessThanEqual(history_uv, vec3(1.0f)))) {
            // Fetch history sample
            vec4 history = textureLod(global_textures_3d[
               previous_light_scattering_texture_index],
               history_uv, 0.0f);</pre>
<p>Once we <a id="_idIndexMarker560"/>read the sample, we can merge the current result with the <a id="_idIndexMarker561"/>previous one based on a user-defined percentage:</p>
<pre class="source-code">
            scattering_extinction.rgb = mix(history.rgb,
                scattering_extinction.rgb,
                    temporal_reprojection_percentage);
            scattering_extinction.a = mix(history.a,
                scattering_extinction.a,
                    temporal_reprojection_percentage);
        }
    }</pre>
<p>We store the result back into the light scattering tex<a id="_idTextAnchor170"/>ture so that the integration can use it for the last step of the volumetric side of the algorithm.</p>
<pre class="source-code">
    imageStore(global_images_3d[light_scattering_texture_in
               dex],
               froxel_coord.xyz, scattering_extinction );
}</pre>
<p>At this point, we have seen all of the steps for the complete algorithm for the Volumetric Fog.</p>
<p>The last <a id="_idIndexMarker562"/>thing to see is the volumetric noise generation used to <a id="_idIndexMarker563"/>animate the fog and briefly talk about noise and jittering used to remove banding.</p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor171"/>Volumetric noise generation</h2>
<p>To break the fog density up a bit so that it is more interesting, we can sample a volumetric <a id="_idIndexMarker564"/>noise texture to modify the density a little. We can add a single execution compute shader that creates and stores Perlin noise in a 3D texture and then reads it when sampling the fog density.</p>
<p>Additionally, we can animate th<a id="_idTextAnchor172"/>is noise to simulate wind animation. The shader is straightforward and uses Perlin noise functions as follows:</p>
<pre class="source-code">
layout (local_size_x = 8, local_size_y = 8, local_size_z =
        1) in;
void main() {
    ivec3 pos = ivec3(gl_GlobalInvocationID.xyz);
    vec3 xyz = pos / volumetric_noise_texture_size;
    float perlin_data = get_perlin_7_octaves(xyz, 4.0);
    imageStore( global_images_3d[output_texture_index],
                pos, vec4(perlin_data, 0, 0, 0) );
}</pre>
<p>The result is a volume texture with a single channel and Perlin noise to be sampled. We also use a special Sampler that has a repeat filter on the <em class="italic">U</em>, <em class="italic">V,</em> and <em class="italic">W</em> axes.</p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor173"/>Blue noise</h2>
<p>As an additional noise used to offset sampling in different areas of the algorithm, we use blue noise, reading it from a texture and adding a temporal component to it.</p>
<p>There are <a id="_idIndexMarker565"/>many interesting properties of blue noise and much literature on why it is a great noise for visual perception, and we will post links at the end of this chapter, but for now, we just read the noise from a texture with two channels and map it to the <code>–1</code> to <code>1</code> range.</p>
<p>The mapping function is as follows:</p>
<pre class="source-code">
float triangular_mapping( float noise0, float noise1 ) {
    return noise0 + noise1 - 1.0f;
}</pre>
<p>And the following is performed to read the blue noise:</p>
<pre class="source-code">
float generate_noise(vec2 pixel, int frame, float scale) {
    vec2 uv = vec2(pixel.xy / blue_noise_dimensions.xy);
    // Read blue noise from texture
    vec2 blue_noise = texture(global_textures[
        nonuniformEXT(blue_noise_128_rg_texture_index)],
                      uv ).rg;
    const float k_golden_ratio_conjugate = 0.61803398875;
    float blue_noise0 = fract(ToLinear1(blue_noise.r) +
        float(frame % 256) * k_golden_ratio_conjugate);
    float blue_noise1 = fract(ToLinear1(blue_noise.g) +
        float(frame % 256) * k_golden_ratio_conjugate);
    return triangular_noise(blue_noise0, blue_noise1) *
        scale;
}</pre>
<p>The final value will be between <code>–1</code> and <code>1</code> and can be scaled to any need and used everywhere.</p>
<p>There is <a id="_idIndexMarker566"/>an animated blue noise paper that promises even better quality, but due to licensing problems, we opted to use this free version.</p>
<h1 id="_idParaDest-163"><a id="_idTextAnchor174"/>Summary</h1>
<p>In this chapter, we introduced the Volumetric Fog rendering technique. We provided a brief mathematical background and algorithmic overview before showing the code. We also showed the different techniques available to improve banding – a vast topic that requires a careful balance of noise and temporal reprojection.</p>
<p>The algorithm presented is also an almost complete implementation that can be found behind many commercial games. We also talked about filtering, especially the temporal filter, which is linked to the next chapter, where we will talk about an anti-aliasing technique that uses temporal reprojection.</p>
<p>In the next chapter, we will see how the synergy between Temporal Anti-Aliasing and noises used to jitter the sampling in Volumetric Fog <a id="_idTextAnchor175"/>will ease out the visual bandings. We will also show a feasible way to generate custom textures with a single-use compute shader used to generate a volumetric noise.</p>
<p>This technique is also used for other volumetric algorithms, such as Volumetric Clouds, to store more custom noises used for generating the cloud shapes.</p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor176"/>Further reading</h1>
<p>There are many different papers that are referenced in this chapter, but the most important is the <em class="italic">Real-Time Volumetric Rendering</em> paper for general GPU-based volumetric rendering: <a href="https://patapom.com/topics/Revision2013/Revision%202013%20-%20Real-time%20Volumetric%20Rendering%20Course%20Notes.pdf">https://patapom.com/topics/Revision2013/Revision%202013%20-%20Real-time%20Volumetric%20Rendering%20Course%20Notes.pdf</a>.</p>
<p>The algorithm is still a derivation of the seminal paper from Bart Wronski: <a href="https://bartwronski.files.wordpress.com/2014/08/bwronski_volumetric_fog_siggraph2014.pdf">https://bartwronski.files.wordpress.com/2014/08/bwronski_volumetric_fog_siggraph2014.pdf</a>.</p>
<p>With some evolutions and mathematical improvements in the following link: <a href="https://www.ea.com/frostbite/news/physically-based-unified-volumetric-rendering-in-frostbite">https://www.ea.com/frostbite/news/physically-based-unified-volumetric-rendering-in-frostbite</a>.</p>
<p>For the depth distribution, we referenced the formula used in iD Tech 6: <a href="https://advances.realtimerendering.com/s2016/Siggraph2016_idTech6.pdf">https://advances.realtimerendering.com/s2016/Siggraph2016_idTech6.pdf</a>.</p>
<p>For banding and noise, the most comprehensive papers come from Playdead:</p>
<ul>
<li><a href="https://loopit.dk/rendering_inside.pdf">https://loopit.dk/rendering_inside.pdf</a></li>
<li><a href="https://loopit.dk/banding_in_games.pdf">https://loopit.dk/banding_in_games.pdf</a></li>
</ul>
<p>For information on animated blue noise: <a href="https://blog.demofox.org/2017/10/31/animating-noise-for-integration-over-time/">https://blog.demofox.org/2017/10/31/animating-noise-for-integration-over-time/</a></p>
<p>For information on dithering, blue noise, and the golden ratio sequence:<a href="https://bartwronski.com/2016/10/30/dithering-part-two-golden-ratio-sequence-blue-noise-and-highpass-and-remap/"> https://bartwronski.com/2016/10/30/dithering-part-two-golden-ratio-sequence-blue-noise-and-highpass-and-remap/</a></p>
<p>A free blue noise texture can be found here: <a href="http://momentsingraphics.de/BlueNoise.xhtml">http://momentsingraphics.de/BlueNoise.xhtml</a>.</p>
</div>
</div>

<div><div><h1 id="_idParaDest-165"><a id="_idTextAnchor177"/>Part 3: Advanced Rendering Techniques</h1>
<p>In this part, we will continue to add advanced techniques to our renderer and we will also explore how to replace or improve some of the techniques developed in earlier chapters using ray tracing.</p>
<p>We will cover the following chapters in this section:</p>
<ul>
<li><a href="B18395_11.xhtml#_idTextAnchor178"><em class="italic">Chapter 11</em></a><em class="italic">, Temporal Anti-Aliasing</em></li>
<li><a href="B18395_12.xhtml#_idTextAnchor205"><em class="italic">Chapter 12</em></a><em class="italic">, Getting Started with Ray Tracing</em></li>
<li><a href="B18395_13.xhtml#_idTextAnchor213"><em class="italic">Chapter 13</em></a><em class="italic">, Revisiting Shadows with Ray Tracing</em></li>
<li><a href="B18395_14.xhtml#_idTextAnchor241"><em class="italic">Chapter 14</em></a><em class="italic">, Adding Dynamic Diffuse Global Illumination with Ray Tracing</em></li>
<li><a href="B18395_15.xhtml#_idTextAnchor280"><em class="italic">Chapter 15</em></a><em class="italic">, Adding Reflections with Ray Tracing</em></li>
</ul>
</div>
<div><div></div>
</div>
</div></body></html>