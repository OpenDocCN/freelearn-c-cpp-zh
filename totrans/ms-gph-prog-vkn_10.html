<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer059">
<h1 class="chapter-number" id="_idParaDest-149"><a id="_idTextAnchor152"/>10</h1>
<h1 id="_idParaDest-150"><a id="_idTextAnchor153"/>Adding Volumetric Fog</h1>
<p>After adding variable rate shading in the previous chapter, we will implement another modern technique that will enhance the visuals of the Raptor Engine: <strong class="bold">Volumetric Fog</strong>. Volumetric rendering and fog are very old topics in rendering literature, but until a few years ago, they were considered impossible for <span class="No-Break">real-time usage.</span></p>
<p>The possibility of making this technique feasible in real-time stems from the observation that fog is a low-frequency effect; thus the rendering can be at a much lower resolution than the screen, increasing the performance in <span class="No-Break">real-time usage.</span></p>
<p>Also, the introduction of compute shaders, and thus generic GPU programming, paired with clever observations about approximations and optimizations of the volumetric aspect of the technique, paved the way to unlocking real-time <span class="No-Break">Volumetric Fog.</span></p>
<p>The main idea comes from the seminal paper by Bart Wronski (<a href="https://bartwronski.files.wordpress.com/2014/08/bwronski_volumetric_fog_siggraph2014.pdf">https://bartwronski.files.wordpress.com/2014/08/bwronski_volumetric_fog_siggraph2014.pdf</a>) at Siggraph 2014, where he described what is still the core idea behind this technique even after almost <span class="No-Break">10 years.</span></p>
<p>Implementing this technique will also be important for learning more about the synergies between different rendering parts of a frame: developing a single technique can be challenging, but the interaction with the rest of the technology is a very important part as well and can add to the challenge of <span class="No-Break">the technique</span></p>
<p>In this chapter, we’ll cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Introducing Volumetric <span class="No-Break">Fog rendering</span></li>
<li>Implementing the Volumetric Fog <span class="No-Break">base technique</span></li>
<li>Adding spatial and temporal filtering to <span class="No-Break">improve visuals</span></li>
</ul>
<p>By the end of this chapter, we will have Volumetric Fog integrated into the Raptor Engine, interacting with the scenery and all the dynamic lights, as shown in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer052">
<img alt="Figure 10.1 – Volumetric Fog with a density volume and three shadow casting lights" height="630" src="image/B18395_10_01.jpg" width="1099"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Volumetric Fog with a density volume and three shadow casting lights</p>
<h1 id="_idParaDest-151"><a id="_idTextAnchor154"/>Technical requirements</h1>
<p>The code for this chapter can be found at the following <span class="No-Break">URL: </span><a href="https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter10"><span class="No-Break">https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter10</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-152"><a id="_idTextAnchor155"/>Introducing Volumetric Fog Rendering</h1>
<p>What exactly is <strong class="bold">Volumetric Fog Rendering</strong>? As the name suggests, it is the combination <a id="_idIndexMarker507"/>of Volumetric Rendering and the fog phenomena. We will now give some background on those components and see how they are combined in the <span class="No-Break">final technique.</span></p>
<p>Let’s begin with <span class="No-Break">Volumetric Rendering.</span></p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor156"/>Volumetric Rendering</h2>
<p>This rendering technique describes the visuals associated with what happens to light when it <a id="_idIndexMarker508"/>travels through a participating medium. A participating medium is a volume that contains local changes to density <span class="No-Break">or albedo.</span></p>
<p>The following diagram summarizes what happens to photons in a <span class="No-Break">participating medium:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<img alt="Figure 10.2 – Light behavior in a participating medium" height="1180" src="image/B18395_10_02.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Light behavior in a participating medium</p>
<p>What we are trying to describe is how light changes when going through a participating medium, namely a fog volume (or clouds or <span class="No-Break">atmospheric scattering).</span></p>
<p>There are three main phenomena that happen, <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Absorption</strong>: This <a id="_idIndexMarker509"/>happens when light is simply trapped inside the medium and does not go outside. It is a net loss <span class="No-Break">of energy.</span></li>
<li><strong class="bold">Out-scattering</strong>: This is <a id="_idIndexMarker510"/>depicted using green arrows in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.2</em> and is again a loss of energy coming out (and thus visible) from <span class="No-Break">the medium.</span></li>
<li><strong class="bold">In-scattering</strong>: This <a id="_idIndexMarker511"/>is the energy coming from the lights that are interacting with <span class="No-Break">the medium.</span></li>
</ul>
<p>While these three phenomena are enough to describe what happens to light, there are three other components that need to be understood before having a complete picture of <span class="No-Break">volumetric rendering.</span></p>
<h3>Phase function</h3>
<p>The first <a id="_idIndexMarker512"/>component is the <strong class="bold">phase function</strong>. This function <a id="_idIndexMarker513"/>describes the scattering of light in different directions. It is dependent on the angle between the light vector and the <span class="No-Break">outgoing directions.</span></p>
<p>This function ca<a id="_idTextAnchor157"/>n be complex and tries to describe scattering in a realistic way, but<a id="_idTextAnchor158"/> the most commonly used is the Henyey-Greenstein function, a function that also takes into <span class="No-Break">consideration anisotropy.</span></p>
<p>The formula for the Henyey-Greenstein function is <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<img alt="Figure 10.3 – The Henyey-Greenstein function" height="135" src="image/B18395_10_03.jpg" width="851"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – The Henyey-Greenstein function</p>
<p>In the preceding equation, the angle theta is the angle between the view vector and the light vector. We will see in the shader code how to translate this to <span class="No-Break">something usable.</span></p>
<h3>Extinction</h3>
<p>The second <a id="_idIndexMarker514"/>component is <strong class="bold">extinction</strong>. Extinction is <a id="_idIndexMarker515"/>a quantity that describes how much light is scattered. We will use this in the intermediate steps of the algorithm, but to apply the calculated fog, we will <span class="No-Break">need transmittance.</span></p>
<h3>Transmittance</h3>
<p>The third <a id="_idIndexMarker516"/>and final component is <strong class="bold">transmittance</strong>. Transmittance is the <a id="_idIndexMarker517"/>extinction of light through a segment of the medium, and it is calculated using the <span class="No-Break">Beer-Lambert law:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<img alt="Figure 10.4 – The Beer-Lambert law" height="83" src="image/B18395_10_04.jpg" width="638"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – The Beer-Lambert law</p>
<p>In the final integration step, we will calculate the transmittance and use it to choose how to apply fog to the scene. The important thing here is to get a basic grasp of the concepts; there will be links provided to deepen your understanding of the mathematical background at the end of <span class="No-Break">the chapter.</span></p>
<p>We now have all the concepts needed to see the implementation details of <span class="No-Break">Volumetric Fog.</span></p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor159"/>Volumetric Fog</h2>
<p>Now that <a id="_idIndexMarker518"/>we have an idea of the different components that contribute to Volumetric Rendering, we can take a bird’s-eye view of the algorithm. One of the first and most clever ideas that Bart Wronski had while developing this technique is the usage of a Frustum Aligned Volume Texture, <span class="No-Break">like so:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<img alt="Figure 10.5 – Frustum Aligned Volume Texture" height="583" src="image/B18395_10_05.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Frustum Aligned Volume Texture</p>
<p>Using a volume texture and math associated with standard rasterization rendering, we can create a mapping between the camera frustum and the texture. This mapping is already happening in the different stages of rendering, for example, when multiplying a vertex position for the view-projection matrix, so it is not <span class="No-Break">something new.</span></p>
<p>What is new is storing information in a volume texture to calculate the volumetric rendering. Each element <a id="_idIndexMarker519"/>of this texture is commonly called the <strong class="bold">froxel</strong>, that st<a id="_idTextAnchor160"/>ands for <span class="No-Break"><strong class="bold">frustum voxel</strong></span><span class="No-Break">.</span></p>
<p>We chose to have a texture with a width, height, and depth of 128 units, but other solutions use a width and height dependent on the screen resolution, similar to <span class="No-Break">clustered shading.</span></p>
<p>We will <a id="_idIndexMarker520"/>use different textures with this resolution as an intermediate step, and for additional filtering, we will discuss this later. One additional decision is to increase the resolution of the camera by using a non-linear depth distribution to map a linear range to an <span class="No-Break">exponential one.</span></p>
<p>We will use a distribution function, such as the one used by Id in their iD Tech engine, <span class="No-Break">like so:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<img alt="Figure 10.6 – Volume texture depth slice on the Z coordinate function" height="72" src="image/B18395_10_06.jpg" width="1153"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Volume texture depth slice on the Z coordinate function</p>
<p>Now that we have decided on the mapping between the volumetric texture and world units, we can describe the steps needed to have a fully working Volumetric <span class="No-Break">Fog solution.</span></p>
<p>The algorithm is outlined in the following diagram, where rectangles represent shader executions while ellipses <span class="No-Break">represent textures:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<img alt="Figure 10.7 – Algorithm overview" height="1021" src="image/B18395_10_07.jpg" width="1644"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Algorithm overview</p>
<p>We will <a id="_idIndexMarker521"/>now see each step of the algorithm to create a mind model of what is happening, and we will review the shader later in <span class="No-Break">the chapter.</span></p>
<h3>Data injection</h3>
<p>The first <a id="_idIndexMarker522"/>step is the data injection. This shader will add some colored fog in the form of color and density into the first Frustum Aligned Texture containing only data. We decided to add a constant fog, a heig<a id="_idTextAnchor161"/>ht-based fog, and a fog volume to mimic a more realistic game <span class="No-Break">development setup.</span></p>
<h3>Light scattering</h3>
<p>When <a id="_idIndexMarker523"/>performing the light scattering, we calculate the in-scattering coming from the lights in <span class="No-Break">the scene.</span></p>
<p>Having a working Clustered Lighting algorithm, we will reuse the same data structures to calculate the light contribution for each froxel, paying attention to treating the light in a different way than the standard Clustered Lighting – we don’t have diffuse or specular here, but just a global term given by attenuation, shadow, <span class="No-Break">and phase.</span></p>
<p>We also sample shadow maps associated with the lights for even more <span class="No-Break">realistic behavior.</span></p>
<h3>Spatial filtering</h3>
<p>To remove <a id="_idIndexMarker524"/>some of the noise, we apply a Gaussian filter only on the <em class="italic">X</em> and <em class="italic">Y</em> axis of the Frustum Aligned Texture, and then we pass to the most important filter, the <span class="No-Break">temporal one.</span></p>
<h3>Temporal filtering</h3>
<p>This filter is what really improves the visuals by giving the possibility of adding some noise at <a id="_idIndexMarker525"/>different steps of the algorithm to remove some banding. It will read the previous frame’s final texture (the one before the integration) and blend the current light scattering result with the previous one based on some <span class="No-Break">constant factor.</span></p>
<p>This is a very difficult topic, as temporal filtering and reprojection can cause a few issues. We will <a id="_idIndexMarker526"/>have a much bigger discussion in the next chapter when talking about <strong class="bold">Temporal </strong><span class="No-Break"><strong class="bold">Anti-Aliasing</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">TAA</strong></span><span class="No-Break">)</span></p>
<p>With the scattering and extinction finalized, we can perform the light integration and thus prepare the texture that will be sampled by <span class="No-Break">the scene.</span></p>
<h3>Light integration</h3>
<p>This step prepares another Frustum Aligned Volumetric Texture to contain an integration <a id="_idIndexMarker527"/>of the fog. Basically, this shader simulates a low-resolution ray marching so that this result can be sampled by <span class="No-Break">the scene.</span></p>
<p>Ray marching normally starts from the camera toward the far plane of the scene. The combination of the Frustum Aligned Texture and this integration gives, for each froxel, a cached ray marching of the light scattering to be easily sampled by the scene. In this step, from all the extinction saved in previous textures, we finally calculate the transmittance with the Beer-Lambert law and use that to merge the fog into <span class="No-Break">the scene.</span></p>
<p>This and temporal filtering are some of the big innovations that unlocked the real-time possibility of this algorithm. In more advanced solutions, such as in the game Red Dead Redemption 2, an additional ray marching can be added to simulate fog at much <span class="No-Break">further distances.</span></p>
<p>It also allows for blending fog and Volumetric Clouds, which use a pure ray marching approach, to have <a id="_idIndexMarker528"/>an almost seamless transition. This is explained in detail in the Siggraph presentation about Red Dead Redemption <span class="No-Break">2 rendering.</span></p>
<h3>Scene application in Clustered Lighting</h3>
<p>The final step is to read the Volumetric Texture in the lighting shader using the world position. We can read the depth buffer, calculate the world position, calculate the froxel coordinates and sample <span class="No-Break">the texture.</span></p>
<p>An <a id="_idIndexMarker529"/>additional step to further smooth the volumetric look is to render to a half-resolution texture the scene application and then apply it to the scene with a geometry-aware upsampling, but this will be left as an exercise for you <span class="No-Break">to complete.</span></p>
<h1 id="_idParaDest-155"><a id="_idTextAnchor162"/>Implementing Volumetric Fog Rendering</h1>
<p>We now have all the knowledge necessary to read the code needed to get this algorithm fully working. From a CPU perspective, it is just a series of compute shaders dispa<a id="_idTextAnchor163"/>tches, so it <span class="No-Break">is straightforward.</span></p>
<p>The core <a id="_idIndexMarker530"/>of this technique is implemented throughout various shaders, and thus on the GPU, working for almost all steps on the frustum aligned Volumetric Texture we talked about in the <span class="No-Break">previous section.</span></p>
<p><span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.7</em> shows the different algorithm steps, and we will see each one individually in the <span class="No-Break">following sections.</span></p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor164"/>Data injection</h2>
<p>In the <a id="_idIndexMarker531"/>first shader, we will write scattering <a id="_idIndexMarker532"/>and extinction, starting from the color and density of different <span class="No-Break">fog phenomena.</span></p>
<p>We decided to add three different fog effects, <span class="No-Break">as follows:</span></p>
<ul>
<li>A <span class="No-Break">constant fog</span></li>
<li><span class="No-Break">Height fog</span></li>
<li>Fog in <span class="No-Break">a volume</span></li>
</ul>
<p>For each fog, we need to calculate scattering and extinction and <span class="No-Break">accumulate them.</span></p>
<p>The following <a id="_idIndexMarker533"/>code converts color and density to scattering <span class="No-Break">and extinction:</span></p>
<pre class="source-code">
vec4 scattering_extinction_from_color_density( vec3 color,
    float density ) {
    const float extinction = scattering_factor * density;
    return vec4( color * extinction, extinction );
}</pre>
<p>We can <a id="_idIndexMarker534"/>now have a look at the main shader. This shader, as most of the others in this chapter, will be scheduled to have one thread for one <span class="No-Break">froxel cell.</span></p>
<p>In the first section, we will see the dispatch and code to calculate <span class="No-Break">world position:</span></p>
<pre class="source-code">
layout (local_size_x = 8, local_size_y = 8, local_size_z =
        1) in;
void main() {
    ivec3 froxel_coord = ivec3(gl_GlobalInvocationID.xyz);
    vec3 world_position = world_from_froxel(froxel_coord);
    vec4 scattering_extinction = vec4(0);</pre>
<p>We add an optional noise to animate the fog and break the <span class="No-Break">constant density:</span></p>
<pre class="source-code">
    vec3 sampling_coord = world_position *
       volumetric_noise_position_multiplier +
       vec3(1,0.1,2) * current_frame *
       volumetric_noise_speed_multiplier; 
    vec4 sampled_noise = texture(
       global_textures_3d[volumetric_noise_texture_index],
       sampling_coord);
    float fog_noise = sampled_noise.x;</pre>
<p>Here, we add <a id="_idIndexMarker535"/>and accumulate <span class="No-Break">constant fog:</span></p>
<pre class="source-code">
    // Add constant fog
    float fog_density = density_modifier * fog_noise;
    scattering_extinction += 
       scattering_extinction_from_color_density( 
       vec3(0.5), fog_density ); </pre>
<p>Then, add <a id="_idIndexMarker536"/>and accumulate <span class="No-Break">height fog:</span></p>
<pre class="source-code">
    // Add height fog
    float height_fog = height_fog_density *
       exp(-height_fog_falloff * max(world_position.y, 0)) *
       fog_noise;
    scattering_extinction += 
       scattering_extinction_from_color_density( 
       vec3(0.5), height_fog ); </pre>
<p>And finally, add density from <span class="No-Break">a box:</span></p>
<pre class="source-code">
    // Add density from box
    vec3 box = abs(world_position - box_position);
    if (all(lessThanEqual(box, box_size))) {
        vec4 box_fog_color = unpack_color_rgba( box_color
                                              );
        scattering_extinction +=
            scattering_extinction_from_color_density(
                box_fog_color.rgb, box_fog_density *
                    fog_noise);
    }</pre>
<p>We finally <a id="_idIndexMarker537"/>store the scattering and extinction, ready to be lit in the <span class="No-Break">next </span><span class="No-Break"><a id="_idIndexMarker538"/></span><span class="No-Break">shader:</span></p>
<pre class="source-code">
    imageStore(global_images_3d[froxel_data_texture_index],
               froxel_coord.xyz, scattering_extinction );
}</pre>
<h2 id="_idParaDest-157"><a id="_idTextAnchor165"/>Calculating the lighting contribution</h2>
<p>Lighting will be performed using the Clustered Lighting data structures already used in general <a id="_idIndexMarker539"/>lighting functions. In this shader, we calculate the in-scattering <span class="No-Break">of light.</span></p>
<p>Shader dispatching is the same as for the previous shader, one thread for <span class="No-Break">one froxel:</span></p>
<pre class="source-code">
layout (local_size_x = 8, local_size_y = 8, local_size_z =
        1) in;
void main() {
    ivec3 froxel_coord = ivec3(gl_GlobalInvocationID.xyz);
    vec3 world_position = world_from_froxel(froxel_coord);
    vec3 rcp_froxel_dim = 1.0f / froxel_dimensions.xyz;</pre>
<p>We read scattering and extinction from the result of the <span class="No-Break">injection shader:</span></p>
<pre class="source-code">
vec4 scattering_extinction = texture(global_textures_3d 
   [nonuniformEXT(froxel_data_texture_index)], 
   froxel_coord * rcp_froxel_dim);
   float extinction = scattering_extinction.a;</pre>
<p>We then start accumulating light and using <span class="No-Break">c<a id="_idTextAnchor166"/>lustered bins.</span></p>
<p>Notice <a id="_idIndexMarker540"/>the cooperation between different rendering algorithms: having the clustered bin already developed, we can use that to query lights in a defined volume starting from the world <span class="No-Break">space position:</span></p>
<pre class="source-code">
vec3 lighting = vec3(0);
vec3 V = normalize(camera_position.xyz - world_position);
// Read clustered lighting data
// Calculate linear depth
float linear_d = froxel_coord.z * 1.0f /
   froxel_dimension_z;
linear_d = raw_depth_to_linear_depth(linear_d,
   froxel_near, froxel_far) / froxel_far;
// Select bin
int bin_index = int( linear_d / BIN_WIDTH );
uint bin_value = bins[ bin_index ];
// As in calculate_lighting method, cycle through
// lights to calculate contribution
for ( uint light_id = min_light_id;
    light_id &lt;= max_light_id;
    ++light_id ) {
    // Same as calculate_lighting method
    // Calculate point light contribution
    // Read shadow map for current light
    float shadow = current_depth –
       bias &lt; closest_depth ? 1 : 0;
    const vec3 L = normalize(light_position –
       world_position);
    float attenuation = attenuation_square_falloff(
       L, 1.0f / light_radius) * shadow;</pre>
<p>Up <a id="_idIndexMarker541"/>until now, the code is almost identical to the one used in lighting, but we add <strong class="source-inline">phase_function</strong> to finalize the <span class="No-Break">lighting factor:</span></p>
<pre class="source-code">
    lighting += point_light.color *
       point_light.intensity *
       phase_function(V, -L,
         phase_anisotropy_01) *
       attenuation;
                    }</pre>
<p>Final <a id="_idIndexMarker542"/>scattering is calculated and stored, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
vec3 scattering = scattering_extinction.rgb * lighting;
imageStore( global_images_3d
            [light_scattering_texture_index],
            ivec3(froxel_coord.xyz), vec4(scattering,
            extinction) );
}</pre>
<p>We will now have a look at the integration/ray marching shader to conclude the main shaders needed to have the algorithm work for the <span class="No-Break">volumetric part.</span></p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor167"/>Integrating scattering and extinction</h2>
<p>This shader is responsible for performing the ray marching in the froxel texture and performing <a id="_idIndexMarker543"/>the intermediate calculations in each cell. It will still write in a frustum-aligned texture, but each cell will contain the accumulated scattering and transmittance starting from <span class="No-Break">that cell.</span></p>
<p>Notice that we now use transmittance instead of extinction, transmittance being a quantity that integrates extinction to a certain space. The dispatch is just on the <em class="italic">X</em> and <em class="italic">Y</em> axis of the frustum texture, reading the light scattering texture, as we will perform the integration steps and write to each froxel in the <span class="No-Break">main loop.</span></p>
<p>The final stored result is scattering and transmittance, so it can be easier to apply it to <span class="No-Break">the scene:</span></p>
<pre class="source-code">
// Dispatch with Z = 1 as we perform the integration.
layout (local_size_x = 8, local_size_y = 8, local_size_z =
        1) in;
void main() {
    ivec3 froxel_coord = ivec3(gl_GlobalInvocationID.xyz);
    vec3 integrated_scattering = vec3(0,0,0);
    float integrated_transmittance = 1.0f;
    float current_z = 0;
    vec3 rcp_froxel_dim = 1.0f / froxel_dimensions.xyz;</pre>
<p>We <a id="_idIndexMarker544"/>integrate on the <em class="italic">Z</em> axis as this texture is <span class="No-Break">frustum aligned.</span></p>
<p>First, we calculate the depth difference to have the thickness needed for the <span class="No-Break">extinction integral:</span></p>
<pre class="source-code">
    for ( int z = 0; z &lt; froxel_dimension_z; ++z ) {
        froxel_coord.z = z;
         float next_z = slice_to_exponential_depth(
                        froxel_near, froxel_far, z + 1,
                        int(froxel_dimension_z) );
        const float z_step = abs(next_z - current_z);
        current_z = next_z;</pre>
<p>We will calculate scattering and transmittance and accumulate them for the following cell on the <span class="No-Break"><em class="italic">Z</em></span><span class="No-Break"> axis:</span></p>
<pre class="source-code">
        // Following equations from Physically Based Sky,
           Atmosphere and Cloud Rendering by Hillaire
        const vec4 sampled_scattering_extinction =
        texture(global_textures_3d[
        nonuniformEXT(light_scattering_texture_index)],
        froxel_coord * rcp_froxel_dim);
        const vec3 sampled_scattering =
            sampled_scattering_extinction.xyz;
        const float sampled_extinction =
            sampled_scattering_extinction.w;
        const float clamped_extinction =
            max(sampled_extinction, 0.00001f);
        const float transmittance = exp(-sampled_extinction
                                        * z_step);
        const vec3 scattering = (sampled_scattering –
                                (sampled_scattering *
                                transmittance)) /
                                clamped_extinction;
        integrated_scattering += scattering *
                                 integrated_transmittance;
        integrated_transmittance *= transmittance;
        imageStore( global_images_3d[
           integrated_light_scattering_texture_index],
           froxel_coord.xyz,
           vec4(integrated_scattering,
              integrated_transmittance) );
    }
}</pre>
<p>We now have a volume texture containing ray marched scattering and transmittance values <a id="_idIndexMarker545"/>that can be queried from anywhere in the frame to know how much fog there is and what color it is at <span class="No-Break">that point.</span></p>
<p>This concludes the main volumetric rendering aspect of the algorithm. We will now have a look at how easy it is to apply the fog to <span class="No-Break">a scene.</span></p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor168"/>Applying Volumetric Fog to the scene</h2>
<p>We can finally apply the Volumetric Fog. To do that, we use the screen space coordinates to calculate <a id="_idIndexMarker546"/>the sampling coordinates for the texture. This function will be used at the end of the lighting calculations for both deferred and forward <span class="No-Break">rendering paths.</span></p>
<p>We first calculate the <span class="No-Break">sampling coordinates:</span></p>
<pre class="source-code">
vec3 apply_volumetric_fog( vec2 screen_uv, float raw_depth,
                           vec3 color ) {
    const float near = volumetric_fog_near;
    const float far = volumetric_fog_far;
    // Fog linear depth distribution
    float linear_depth = raw_depth_to_linear_depth(
                         raw_depth, near, far );
    // Exponential
    float depth_uv = linear_depth_to_uv( near, far,
        linear_depth, volumetric_fog_num_slices );
vec4 scattering_transmittance =
   texture(global_textures_3d
   [nonuniformEXT(volumetric_fog_texture_index)], 
   froxel_uvw);</pre>
<p>After we read the scattering and transmittance at the specified position, we use the transmittance <a id="_idIndexMarker547"/>to modulate the current scene color and add the fog scattered color, <span class="No-Break">like so:</span></p>
<pre class="source-code">
    color.rgb = color.rgb * scattering_transmittance.a +
                scattering_transmittance.rgb;
    return color;
}</pre>
<p>And this concludes the necessary steps to fully implement Volumetric Fog rendering. But still, there is <a id="_idIndexMarker548"/>a big <span class="No-Break">problem: </span><span class="No-Break"><strong class="bold">banding</strong></span><span class="No-Break">.</span></p>
<p>This is a large topic covered in several papers, but for the sake of simplicity, we can say that having a low-resolution volume texture adds banding problems, but it is necessary for achieving <span class="No-Break">real-time performance.</span></p>
<h2 id="_idParaDest-160"><a id="_idTextAnchor169"/>Adding filters</h2>
<p>To further <a id="_idIndexMarker549"/>improve the visuals, we add two different filters: a temporal and a <span class="No-Break">spatial one.</span></p>
<p>The temporal filter is what really makes the difference because it gives us the possibility of adding noise in different parts of the algorithm and thus removing banding. The spatial filter smooths out the fog <span class="No-Break">even further.</span></p>
<h3>Spatial filtering</h3>
<p>This shader <a id="_idIndexMarker550"/>will smooth out the volumetric texture in the <em class="italic">X</em> and <em class="italic">Y</em> axis by <a id="_idIndexMarker551"/>applying a Gaussian filter. It will read the result of the light scattering and write into the froxel data texture, unused at this point of the frame, removing the need to create a <span class="No-Break">temporary texture.</span></p>
<p>We first define the Gaussian function and its <span class="No-Break">representing code:</span></p>
<pre class="source-code">
#define SIGMA_FILTER 4.0
#define RADIUS 2
float gaussian(float radius, float sigma) {
    const float v = radius / sigma;
    return exp(-(v*v));
}</pre>
<p>We then <a id="_idIndexMarker552"/>read the light scattering texture and accumulate values and weight <a id="_idIndexMarker553"/>only if the calculated coordinates <span class="No-Break">are valid:</span></p>
<pre class="source-code">
    vec4 scattering_extinction =
       texture( global_textures_3d[
       nonuniformEXT(light_scattering_texture_index)],
       froxel_coord * rcp_froxel_dim );
    if ( use_spatial_filtering == 1 ) {
        float accumulated_weight = 0;
        vec4 accumulated_scattering_extinction = vec4(0);
        for (int i = -RADIUS; i &lt;= RADIUS; ++i ) {
            for (int j = -RADIUS; j &lt;= RADIUS; ++j ) {
                ivec3 coord = froxel_coord + ivec3(i, j,
                                                   0);
                // if inside
                if (all(greaterThanEqual(coord, ivec3(0)))
                    &amp;&amp; all(lessThanEqual(coord,
                    ivec3(froxel_dimension_x,
                    froxel_dimension_y,
                    froxel_dimension_z)))) {
                    const float weight =
                        gaussian(length(ivec2(i, j)),
                            SIGMA_FILTER);
                    const vec4 sampled_value =
                      texture(global_textures_3d[
                        nonuniformEXT(
                          light_scattering_texture_index)],
                            coord * rcp_froxel_dim);
                  accumulated_scattering_extinction.rgba +=
                      sampled_value.rgba * weight;
                    accumulated_weight += weight;
                }
            }
        }
        scattering_extinction =
           accumulated_scattering_extinction /
           accumulated_weight;
    }</pre>
<p>We store <a id="_idIndexMarker554"/>the result <a id="_idIndexMarker555"/>in the froxel <span class="No-Break">data texture:</span></p>
<pre class="source-code">
    imageStore(global_images_3d[froxel_data_texture_index],
               froxel_coord.xyz, scattering_extinction );
}</pre>
<p>The next step is <span class="No-Break">temporal filtering.</span></p>
<h3>Temporal filtering</h3>
<p>This shader <a id="_idIndexMarker556"/>will take the currently calculated 3D light scattering texture and apply a temporal filter. In order to do that it will need two textures, one for <a id="_idIndexMarker557"/>the current and one for the previous frame, and thanks to bindless, we just need to change the indices to <span class="No-Break">use them.</span></p>
<p>Dispatch is like most of the shaders in this chapter, with one thread for each froxel element of the volume texture. Let’s begin with reading the current light <span class="No-Break">scattering texture.</span></p>
<p>This currently resides in <strong class="source-inline">froxel_data_texture</strong>, coming from the <span class="No-Break">spatial filtering:</span></p>
<pre class="source-code">
    vec4 scattering_extinction =
       texture( global_textures_3d[
       nonuniformEXT(froxel_data_texture_index)],
       froxel_coord * rcp_froxel_dim );</pre>
<p>We need to calculate the previous screen space position to read the previous <span class="No-Break">frame texture.</span></p>
<p>We will calculate the world position and then use the previous view projection to get the UVW coordinates to read <span class="No-Break">the texture:</span></p>
<pre class="source-code">
    // Temporal reprojection
    if (use_temporal_reprojection == 1) {
        vec3 world_position_no_jitter =
            world_from_froxel_no_jitter(froxel_coord);
        vec4 sceen_space_center_last =
            previous_view_projection *
                vec4(world_position_no_jitter, 1.0);
        vec3 ndc = sceen_space_center_last.xyz /
                   sceen_space_center_last.w;
        float linear_depth = raw_depth_to_linear_depth(
                             ndc.z, froxel_near, froxel_far
                             );
        float depth_uv = linear_depth_to_uv( froxel_near,
                         froxel_far, linear_depth,
                         int(froxel_dimension_z) );
        vec3 history_uv = vec3( ndc.x * .5 + .5, ndc.y * -
                                .5 + .5, depth_uv );</pre>
<p>We then <a id="_idIndexMarker558"/>check whether the calculated UVWs are valid and if so, we will <a id="_idIndexMarker559"/>read the <span class="No-Break">previous texture:</span></p>
<pre class="source-code">
        // If history UV is outside the frustum, skip
        if (all(greaterThanEqual(history_uv, vec3(0.0f)))
             &amp;&amp; all(lessThanEqual(history_uv, vec3(1.0f)))) {
            // Fetch history sample
            vec4 history = textureLod(global_textures_3d[
               previous_light_scattering_texture_index],
               history_uv, 0.0f);</pre>
<p>Once we <a id="_idIndexMarker560"/>read the sample, we can merge the current result with the <a id="_idIndexMarker561"/>previous one based on a <span class="No-Break">user-defined percentage:</span></p>
<pre class="source-code">
            scattering_extinction.rgb = mix(history.rgb,
                scattering_extinction.rgb,
                    temporal_reprojection_percentage);
            scattering_extinction.a = mix(history.a,
                scattering_extinction.a,
                    temporal_reprojection_percentage);
        }
    }</pre>
<p>We store the result back into the light scattering tex<a id="_idTextAnchor170"/>ture so that the integration can use it for the last step of the volumetric side of <span class="No-Break">the algorithm.</span></p>
<pre class="source-code">
    imageStore(global_images_3d[light_scattering_texture_in
               dex],
               froxel_coord.xyz, scattering_extinction );
}</pre>
<p>At this point, we have seen all of the steps for the complete algorithm for the <span class="No-Break">Volumetric Fog.</span></p>
<p>The last <a id="_idIndexMarker562"/>thing to see is the volumetric noise generation used to <a id="_idIndexMarker563"/>animate the fog and briefly talk about noise and jittering used to <span class="No-Break">remove banding.</span></p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor171"/>Volumetric noise generation</h2>
<p>To break the fog density up a bit so that it is more interesting, we can sample a volumetric <a id="_idIndexMarker564"/>noise texture to modify the density a little. We can add a single execution compute shader that creates and stores Perlin noise in a 3D texture and then reads it when sampling the <span class="No-Break">fog density.</span></p>
<p>Additionally, we can animate th<a id="_idTextAnchor172"/>is noise to simulate wind animation. The shader is straightforward and uses Perlin noise functions <span class="No-Break">as follows:</span></p>
<pre class="source-code">
layout (local_size_x = 8, local_size_y = 8, local_size_z =
        1) in;
void main() {
    ivec3 pos = ivec3(gl_GlobalInvocationID.xyz);
    vec3 xyz = pos / volumetric_noise_texture_size;
    float perlin_data = get_perlin_7_octaves(xyz, 4.0);
    imageStore( global_images_3d[output_texture_index],
                pos, vec4(perlin_data, 0, 0, 0) );
}</pre>
<p>The result is a volume texture with a single channel and Perlin noise to be sampled. We also use a special Sampler that has a repeat filter on the <em class="italic">U</em>, <em class="italic">V,</em> and <span class="No-Break"><em class="italic">W</em></span><span class="No-Break"> axes.</span></p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor173"/>Blue noise</h2>
<p>As an additional noise used to offset sampling in different areas of the algorithm, we use blue noise, reading it from a texture and adding a temporal component <span class="No-Break">to it.</span></p>
<p>There are <a id="_idIndexMarker565"/>many interesting properties of blue noise and much literature on why it is a great noise for visual perception, and we will post links at the end of this chapter, but for now, we just read the noise from a texture with two channels and map it to the <strong class="source-inline">–1</strong> to <span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break"> range.</span></p>
<p>The mapping function is <span class="No-Break">as follows:</span></p>
<pre class="source-code">
float triangular_mapping( float noise0, float noise1 ) {
    return noise0 + noise1 - 1.0f;
}</pre>
<p>And the following is performed to read the <span class="No-Break">blue noise:</span></p>
<pre class="source-code">
float generate_noise(vec2 pixel, int frame, float scale) {
    vec2 uv = vec2(pixel.xy / blue_noise_dimensions.xy);
    // Read blue noise from texture
    vec2 blue_noise = texture(global_textures[
        nonuniformEXT(blue_noise_128_rg_texture_index)],
                      uv ).rg;
    const float k_golden_ratio_conjugate = 0.61803398875;
    float blue_noise0 = fract(ToLinear1(blue_noise.r) +
        float(frame % 256) * k_golden_ratio_conjugate);
    float blue_noise1 = fract(ToLinear1(blue_noise.g) +
        float(frame % 256) * k_golden_ratio_conjugate);
    return triangular_noise(blue_noise0, blue_noise1) *
        scale;
}</pre>
<p>The final value will be between <strong class="source-inline">–1</strong> and <strong class="source-inline">1</strong> and can be scaled to any need and <span class="No-Break">used everywhere.</span></p>
<p>There is <a id="_idIndexMarker566"/>an animated blue noise paper that promises even better quality, but due to licensing problems, we opted to use this <span class="No-Break">free version.</span></p>
<h1 id="_idParaDest-163"><a id="_idTextAnchor174"/>Summary</h1>
<p>In this chapter, we introduced the Volumetric Fog rendering technique. We provided a brief mathematical background and algorithmic overview before showing the code. We also showed the different techniques available to improve banding – a vast topic that requires a careful balance of noise and <span class="No-Break">temporal reprojection.</span></p>
<p>The algorithm presented is also an almost complete implementation that can be found behind many commercial games. We also talked about filtering, especially the temporal filter, which is linked to the next chapter, where we will talk about an anti-aliasing technique that uses <span class="No-Break">temporal reprojection.</span></p>
<p>In the next chapter, we will see how the synergy between Temporal Anti-Aliasing and noises used to jitter the sampling in Volumetric Fog <a id="_idTextAnchor175"/>will ease out the visual bandings. We will also show a feasible way to generate custom textures with a single-use compute shader used to generate a <span class="No-Break">volumetric noise.</span></p>
<p>This technique is also used for other volumetric algorithms, such as Volumetric Clouds, to store more custom noises used for generating the <span class="No-Break">cloud shapes.</span></p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor176"/>Further reading</h1>
<p>There are many different papers that are referenced in this chapter, but the most important is the <em class="italic">Real-Time Volumetric Rendering</em> paper for general GPU-based volumetric <span class="No-Break">rendering: </span><a href="https://patapom.com/topics/Revision2013/Revision%202013%20-%20Real-time%20Volumetric%20Rendering%20Course%20Notes.pdf"><span class="No-Break">https://patapom.com/topics/Revision2013/Revision%202013%20-%20Real-time%20Volumetric%20Rendering%20Course%20Notes.pdf</span></a><span class="No-Break">.</span></p>
<p>The algorithm is still a derivation of the seminal paper from Bart <span class="No-Break">Wronski: </span><a href="https://bartwronski.files.wordpress.com/2014/08/bwronski_volumetric_fog_siggraph2014.pdf"><span class="No-Break">https://bartwronski.files.wordpress.com/2014/08/bwronski_volumetric_fog_siggraph2014.pdf</span></a><span class="No-Break">.</span></p>
<p>With some evolutions and mathematical improvements in the following <span class="No-Break">link: </span><a href="https://www.ea.com/frostbite/news/physically-based-unified-volumetric-rendering-in-frostbite"><span class="No-Break">https://www.ea.com/frostbite/news/physically-based-unified-volumetric-rendering-in-frostbite</span></a><span class="No-Break">.</span></p>
<p>For the depth distribution, we referenced the formula used in iD Tech <span class="No-Break">6: </span><a href="https://advances.realtimerendering.com/s2016/Siggraph2016_idTech6.pdf"><span class="No-Break">https://advances.realtimerendering.com/s2016/Siggraph2016_idTech6.pdf</span></a><span class="No-Break">.</span></p>
<p>For banding and noise, the most comprehensive papers come <span class="No-Break">from Playdead:</span></p>
<ul>
<li><a href="https://loopit.dk/rendering_inside.pdf"><span class="No-Break">https://loopit.dk/rendering_inside.pdf</span></a></li>
<li><a href="https://loopit.dk/banding_in_games.pdf"><span class="No-Break">https://loopit.dk/banding_in_games.pdf</span></a></li>
</ul>
<p>For information on animated blue <span class="No-Break">noise: </span><a href="https://blog.demofox.org/2017/10/31/animating-noise-for-integration-over-time/"><span class="No-Break">https://blog.demofox.org/2017/10/31/animating-noise-for-integration-over-time/</span></a></p>
<p>For information on dithering, blue noise, and the golden ratio <span class="No-Break">sequence:</span><a href="https://bartwronski.com/2016/10/30/dithering-part-two-golden-ratio-sequence-blue-noise-and-highpass-and-remap/"><span class="No-Break"> </span><span class="No-Break">https://bartwronski.com/2016/10/30/dithering-part-two-golden-ratio-sequence-blue-noise-and-highpass-and-remap/</span></a></p>
<p>A free blue noise texture can be found <span class="No-Break">here: </span><a href="http://momentsingraphics.de/BlueNoise.xhtml"><span class="No-Break">http://momentsingraphics.de/BlueNoise.xhtml</span></a><span class="No-Break">.</span></p>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer060">
<h1 id="_idParaDest-165"><a id="_idTextAnchor177"/>Part 3: Advanced Rendering Techniques</h1>
<p>In this part, we will continue to add advanced techniques to our renderer and we will also explore how to replace or improve some of the techniques developed in earlier chapters using <span class="No-Break">ray tracing.</span></p>
<p>We will cover the following chapters in <span class="No-Break">this section:</span></p>
<ul>
<li><a href="B18395_11.xhtml#_idTextAnchor178"><em class="italic">Chapter 11</em></a><em class="italic">, Temporal Anti-Aliasing</em></li>
<li><a href="B18395_12.xhtml#_idTextAnchor205"><em class="italic">Chapter 12</em></a><em class="italic">, Getting Started with Ray Tracing</em></li>
<li><a href="B18395_13.xhtml#_idTextAnchor213"><em class="italic">Chapter 13</em></a><em class="italic">, Revisiting Shadows with Ray Tracing</em></li>
<li><a href="B18395_14.xhtml#_idTextAnchor241"><em class="italic">Chapter 14</em></a><em class="italic">, Adding Dynamic Diffuse Global Illumination with Ray Tracing</em></li>
<li><a href="B18395_15.xhtml#_idTextAnchor280"><em class="italic">Chapter 15</em></a><em class="italic">, Adding Reflections with Ray Tracing</em></li>
</ul>
</div>
<div>
<div id="_idContainer061">
</div>
</div>
</div></body></html>