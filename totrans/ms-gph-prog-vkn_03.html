<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-46"><a id="_idTextAnchor045"/>3</h1>
<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Unlocking Multi-Threading</h1>
<p>In this chapter, we will talk about adding multi-threading to the Raptor Engine.</p>
<p>This requires both a big change in the underlying architecture and some Vulkan-specific changes and synchronization work so that the different cores of the CPU and the GPU can cooperate in the most correct and the fastest way.</p>
<p><strong class="bold">Multi-threading</strong> rendering is a topic covered many times over the years and a feature that most game engines have<a id="_idIndexMarker121"/> needed since the era of multi-core architectures exploded. Consoles such as the PlayStation 2 and the Sega Saturn already offered multi-threading support, and later generations continued the trend by providing an increasing number of cores that developers could take advantage of.</p>
<p>The first trace of multi-threading rendering in a game engine is as far back as 2008 when Christer Ericson wrote a blog post (<a href="https://realtimecollisiondetection.net/blog/?p=86">https://realtimecollisiondetection.net/blog/?p=86</a>) and showed that it was possible to parallelize and optimize the generation of commands used to render objects on the screen.</p>
<p>Older APIs such as OpenGL and DirectX (up until version 11) did not have proper multi-threading support, especially because they were big state machines with a global context tracking down each change after each command. Still, the command generation across different objects could take a few milliseconds, so multi-threading was already a big save in performance.</p>
<p>Luckily for us, Vulkan fully supports multi-threading command buffers natively, especially with the creation of the <code>VkCommandBuffer</code> class, from an architectural perspective of the Vulkan API.</p>
<p>The Raptor Engine, up until now, was a single-threaded application and thus required some architectural changes to fully support multi-threading. In this chapter, we will see those changes, learn how to use a task-based multi-threading library called enkiTS, and then unlock both asynchronous resource loading and multi-threading command recording.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>How to use a task-based multi-threading library</li>
<li>How to asynchronously load resources</li>
<li>How to draw in parallel threads</li>
</ul>
<p>By the end of the chapter, we will know how to run concurrent tasks both for loading resources and drawing objects on the screen. By learning how to reason with a task-based multi-threading system, we will be able to perform other parallel tasks in future chapters as well.</p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Technical requirements</h1>
<p>The code for this chapter can be found at the following URL: <a href="https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter3">https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter3</a>.</p>
<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>Task-based multi-threading using enkiTS</h1>
<p>To achieve parallelism, we need to<a id="_idIndexMarker122"/> understand some basic concepts and choices that led to the architecture developed in this chapter. First, we should note that when we talk about parallelism in software engineering, we mean the act of executing chunks of code at the same time.</p>
<p>This is possible because<a id="_idIndexMarker123"/> modern hardware has different units that can be operated independently, and operating systems have dedicated execution <a id="_idIndexMarker124"/>units called <strong class="bold">threads</strong>.</p>
<p>A common way to achieve parallelism is to reason with tasks – small independent execution units that can run on any thread.</p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/>Why task-based parallelism?</h2>
<p>Multi-threading is not a new subject, and since the early years of it being added to various game engines, there have <a id="_idIndexMarker125"/>been different ways of implementing it. Game engines are pieces of software that use all of the hardware available in the most efficient way, thus paving the way for more optimized software architectures.</p>
<p>Therefore, we’ll take some ideas from game engines and gaming-related presentations. The initial implementations started by adding a thread with a single job to do – something specific, such as rendering a single thread, an asynchronous <strong class="bold">input/output</strong> (<strong class="bold">I/O</strong>) thread, and so on.</p>
<p>This helped add more granularity to what could be done in parallel, and it was perfect for the older CPUs (having two cores only), but it soon became limiting.</p>
<p>There was the need to use cores in a more agnostic way so that any type of job could be done by almost any core and to improve performance. This gave way to the emergence of two new architectures: <strong class="bold">task-based</strong> and <strong class="bold">fiber-based</strong> architectures.</p>
<p>Task-based parallelism is<a id="_idIndexMarker126"/> achieved by feeding multiple threads with different tasks and orchestrating them through<a id="_idIndexMarker127"/> dependencies. Tasks are inherently platform agnostic and cannot be interrupted, leading to a more straightforward capability to schedule and organize code to be executed with them.</p>
<p>On the other hand, fibers are software constructs similar to tasks, but they rely heavily on the scheduler to interrupt their flow and resume when needed. This main difference makes it hard to write a proper fiber system and normally leads to a lot of subtle errors.</p>
<p>For the simplicity of using tasks over fibers and the bigger availability of libraries implementing task-based parallelism, the enkiTS library was chosen to handle everything. For those curious about more in-depth explanations, there are a couple of great presentations about these architectures.</p>
<p>A great example of a task-based engine is the one behind the Destiny franchise (with an in-depth architecture you can view at <a href="https://www.gdcvault.com/play/1021926/Destiny-s-Multithreaded-Rendering">https://www.gdcvault.com/play/1021926/Destiny-s-Multithreaded-Rendering</a>), while <a id="_idIndexMarker128"/>a fiber-based one is used by the game studio Naughty Dog for their games (there is a presentation about it at <a href="https://www.gdcvault.com/play/1022186/Parallelizing-the-Naughty-Dog-Engine">https://www.gdcvault.com/play/1022186/Parallelizing-the-Naughty-Dog-Engine</a>).</p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Using the enkiTS (Task-Scheduler) library</h2>
<p>Task-based multi-threading is<a id="_idIndexMarker129"/> based on the concept of a task, defined as a <em class="italic">unit of independent work that can be executed on any core of </em><em class="italic">a CPU</em>.</p>
<p>To do that, there is a need for a scheduler to coordinate different tasks and take care of the possible dependencies between them. Another interesting aspect of a task is that it could have one or more dependencies so that it could be scheduled to run only after certain tasks finish their execution.</p>
<p>This means that tasks can be submitted to the scheduler at any time, and with proper dependencies, we create a graph-based execution of the engine. If done properly, each core can be utilized fully and results in optimal performance to the engine.</p>
<p>The scheduler is the brain behind all the tasks: it checks dependencies and priorities, and schedules or removes tasks based on need, and it is a new system added to the Raptor Engine.</p>
<p>When initializing the<a id="_idIndexMarker130"/> scheduler, the library spawns a number of threads, each waiting to execute a task. When adding tasks to the scheduler, they are inserted into a queue. When the scheduler is told to execute pending tasks, each thread gets the next available task from the queue – according to dependency and priority – and executes it.</p>
<p>It’s important to note that running tasks can spawn other tasks. These tasks will be added to the thread’s local queue, but they are up for grabs if another thread is idle. This implementation is <a id="_idIndexMarker131"/>called a <strong class="bold">work-stealing queue</strong>.</p>
<p>Initializing the scheduler is as simple as creating a configuration and calling the <code>Initialize</code> method:</p>
<pre class="source-code">
enki::TaskSchedulerConfig config;
config.numTaskThreadsToCreate = 4;
enki::TaskScheduler task_scheduler;
task_scheduler.Initialize( config );</pre>
<p>With this code, we are telling the task scheduler to spawn four threads that it will use to perform its duties. enkiTS uses the <code>TaskSet</code> class as a unit of work, and it uses both inheritance and lambda functions to drive the execution of tasks in the scheduler:</p>
<pre class="source-code">
Struct ParallelTaskSet : enki::ItaskSet {
    void ExecuteRange(  enki::TaskSetPartition range_,
                        uint32_t threadnum_ ) override {
        // do something here, can issue tasks with
           task_scheduler
    }
};
int main(int argc, const char * argv[]) {
    enki::TaskScheduler task_scheduler;
    task_scheduler.Initialize( config );
    ParallelTaskSet task; // default constructor has a set
                             size of 1
    task_scheduler.AddTaskSetToPipe( &amp;task );
    // wait for task set (running tasks if they exist)
    // since we've just added it and it has no range we'll
       likely run it.
    Task_scheduler.WaitforTask( &amp;task );
    return 0;
}</pre>
<p>In this simple snippet, we see how to create an empty <code>TaskSet</code> (as the name implies, a set of tasks) that defines how a task will execute the code, leaving the scheduler with the job of deciding how<a id="_idIndexMarker132"/> many of the tasks will be needed and which thread will be used.</p>
<p>A more streamlined version of the previous code uses lambda functions:</p>
<pre class="source-code">
enki::TaskSet task( 1, []( enki::TaskSetPartition range_,
  uint32_t threadnum_  ) {
         // do something here
  }  );
task_scheduler.AddTaskSetToPipe( &amp;task );</pre>
<p>This version can be easier when reading the code as it does break the flow less, but it is functionally equivalent to the previous one.</p>
<p>Another feature of the enkiTS scheduler is the possibility to add pinned tasks – special tasks that will be bound to a thread and will always be executed there. We will see the use of pinned tasks in the next section to perform asynchronous I/O operations.</p>
<p>In this section, we<a id="_idIndexMarker133"/> talked briefly about the different types of multi-threading so that we could express the reason for choosing to use task-based multi-threading. We then showed some simple examples of the enkiTS library and its usage, adding multi-threading capabilities to the Raptor Engine.</p>
<p>In the next section, we will finally see a real use case in the engine, which is the asynchronous loading of resources.</p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>Asynchronous loading</h1>
<p>The loading of resources is one of the (if not <em class="italic">the</em>) slowest operations that can be done in any framework. This is because the<a id="_idIndexMarker134"/> files to be loaded are big, and they can come from different sources, such as optical units (DVD and Blu-ray), hard drives, and even the network.</p>
<p>It is another great topic, but the most important concept to understand is the inherent speed necessary to read the memory:</p>
<div><div><img alt="Figure 3.1 – A memory hierarchy" height="541" src="img/B18395_03_01.jpg" width="607"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – A memory hierarchy</p>
<p>As shown in the<a id="_idIndexMarker135"/> preceding diagram, the fastest memory is the registers memory. After registers follows the cache, with different levels and access speeds: both registers and caches are directly in the processing unit (both the CPU and GPU have registers and caches, even with different underlying architectures).</p>
<p>Main memory refers to the RAM, which is the area that is normally populated with the data used by the application. It is slower than the cache, but it is the target of the loading operations as the only one directly accessible from the code. Then there are magnetic disks (hard drives) and optical drives – much slower but with greater capacity. They normally contain the asset data that will be loaded into the main memory.</p>
<p>The final memory is in remote storage, such as from some servers, and it is the slowest. We will not deal with that here, but it can be used when working on applications that have some form of online service, such as multiplayer games.</p>
<p>With the objective of optimizing the read access in an application, we want to transfer all the needed data into the main memory, as we can’t interact with caches and registers. To hide the slow speed of magnetic and optical disks, one of the most important things that can be done is to parallelize the loading of any resource coming from any medium so that the fluidity of the application is not slowed down.</p>
<p>The most common way of doing it, and one example of the thread-specialization architecture we talked briefly about before, is to have a separate thread that handles just the loading of resources and interacts with other systems to update the used resources in the engine.</p>
<p>In the following<a id="_idIndexMarker136"/> sections, we will talk about how to set up enkiTS and create tasks for parallelizing the Raptor Engine, as well as talk about Vulkan queues, which are necessary for parallel command submission. Finally, we will dwell on the actual code used for asynchronous loading.</p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor052"/>Creating the I/O thread and tasks</h2>
<p>In the enkiTS library, there is a <a id="_idIndexMarker137"/>feature called <strong class="bold">pinned-task</strong> that associates a task to a specific thread so that it is <a id="_idIndexMarker138"/>continuously running there unless stopped by the user or a higher priority task is scheduled on that thread.</p>
<p>To simplify things, we will add a new thread and avoid it being used by the application. This thread will be mostly idle, so the context switch should be low:</p>
<pre class="source-code">
config.numTaskThreadsToCreate = 4;</pre>
<p>We then create a pinned task and associate it with a thread ID:</p>
<pre class="source-code">
// Create IO threads at the end
RunPinnedTaskLoopTask run_pinned_task;
run_pinned_task.threadNum = task_scheduler.
                            GetNumTaskThreads() - 1;
task_scheduler.AddPinnedTask( &amp;run_pinned_task );</pre>
<p>At this point, we can create the actual task responsible for asynchronous loading, associating it with the same thread as the pinned task:</p>
<pre class="source-code">
// Send async load task to external thread
AsynchronousLoadTask async_load_task;
async_load_task.threadNum = run_pinned_task.threadNum;
task_scheduler.AddPinnedTask( &amp;async_load_task );</pre>
<p>The final piece of the<a id="_idIndexMarker139"/> puzzle is the actual code for these two tasks. First, let us have a look at the first pinned task:</p>
<pre class="source-code">
struct RunPinnedTaskLoopTask : enki::IPinnedTask {
    void Execute() override {
        while ( task_scheduler-&gt;GetIsRunning() &amp;&amp; execute )
         {
            task_scheduler-&gt;WaitForNewPinnedTasks();
            // this thread will 'sleep' until there are new
               pinned tasks
            task_scheduler-&gt;RunPinnedTasks();
        }
    }
    enki::TaskScheduler*task_scheduler;
    bool execute = true;
}; // struct RunPinnedTaskLoopTask</pre>
<p>This task will wait for any other pinned task and run them when possible. We have added an <code>execute</code> flag to stop the execution when needed, for example, when exiting the application, but it could be used in general to suspend it in other situations (such as when the application is minimized).</p>
<p>The other task is the one executing the asynchronous loading using the <code>AsynchronousLoader</code> class:</p>
<pre class="source-code">
struct AsynchronousLoadTask : enki::IPinnedTask {
    void Execute() override {
        while ( execute ) {
            async_loader-&gt;update();
        }
    }
    AsynchronousLoader*async_loader;
    enki::TaskScheduler*task_scheduler;
    bool execute = true;
}; // struct AsynchronousLoadTask</pre>
<p>The idea behind this task is to always be active and wait for requests for resource loading. The <code>while</code> loop ensures that the root pinned task never schedules other tasks on this thread, locking it to I/O as intended.</p>
<p>Before moving on to<a id="_idIndexMarker140"/> look at the <code>AsynchronousLoader</code> class, we need to look at an important concept in Vulkan, namely queues, and why they are a great addition for asynchronous loading.</p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor053"/>Vulkan queues and the first parallel command generation</h2>
<p>The concept of a <em class="italic">queue</em> – which can be defined<a id="_idIndexMarker141"/> as the entry point to <a id="_idIndexMarker142"/>submit commands recorded in <code>VkCommandBuffers</code> to the GPU – is an addition to Vulkan compared to OpenGL and needs to be taken care of.</p>
<p>Submission using a queue is a single-threaded operation, and a costly operation that becomes a synchronization point between CPU and GPU to be aware of. Normally, there is the main queue to which the engine submits command buffers before presenting the frame. This will send the work to the GPU and create the rendered image intended.</p>
<p>But where there is one queue, there can be more. To enhance parallel execution, we can instead create different <em class="italic">queues</em> – and use them in different threads instead of the main one.</p>
<p>A more in-depth look at queues can be found at <a href="https://github.com/KhronosGroup/Vulkan-Guide/blob/master/chapters/queues.adoc">https://github.com/KhronosGroup/Vulkan-Guide/blob/master/chapters/queues.adoc</a>, but what we need to know is that each queue can submit certain types of commands, visible through a queue’s flag:</p>
<ul>
<li><code>VK_QUEUE_GRAPHICS_BIT</code> can submit all <code>vkCmdDraw</code> commands</li>
<li><code>VK_QUEUE_COMPUTE</code> can submit all <code>vkCmdDispatch</code> and <code>vkCmdTraceRays</code> (used for ray tracing)</li>
<li><code>VK_QUEUE_TRANSFER</code> can submit copy commands, such as <code>vkCmdCopyBuffer</code>, <code>vkCmdCopyBufferToImage</code>, and <code>vkCmdCopyImageToBuffer</code></li>
</ul>
<p>Each available queue is exposed<a id="_idIndexMarker143"/> through a queue family. Each queue family can have multiple capabilities and can expose multiple queues. Here is an example to clarify:</p>
<pre class="source-code">
{
    "VkQueueFamilyProperties": {
        "queueFlags": [
            "VK_QUEUE_GRAPHICS_BIT",
            "VK_QUEUE_COMPUTE_BIT",
            "VK_QUEUE_TRANSFER_BIT",
            "VK_QUEUE_SPARSE_BINDING_BIT"
        ],
        "queueCount": 1,
    }
},
{
    "VkQueueFamilyProperties": {
        "queueFlags": [
            "VK_QUEUE_COMPUTE_BIT",
            "VK_QUEUE_TRANSFER_BIT",
            "VK_QUEUE_SPARSE_BINDING_BIT"
        ],
        "queueCount": 2,
    }
},
{
    "VkQueueFamilyProperties": {
        "queueFlags": [
            "VK_QUEUE_TRANSFER_BIT",
            "VK_QUEUE_SPARSE_BINDING_BIT"
        ],
        "queueCount": 2,
    }
}</pre>
<p>The first queue exposes all capabilities, and we only have one of them. The next queue can be used for compute and transfer, and the third one for transfer (we’ll ignore the sparse feature for now). We have two queues for each of these families.</p>
<p>It is guaranteed that on a GPU there <a id="_idIndexMarker144"/>will always be at least one queue that can submit all types of commands, and that will be our main queue.</p>
<p>In some GPUs, though, there can be specialized queues that have only the <code>VK_QUEUE_TRANSFE</code>R flag activated, which <a id="_idIndexMarker145"/>means that they can use <strong class="bold">direct memory access</strong> (<strong class="bold">DMA</strong>) to speed up the transfer of data between the CPU and the GPU.</p>
<p>One last thing: the Vulkan logical device is responsible for creating and destroying queues – an operation normally done at the startup/shutdown of the application. Let us briefly see the code to query the support for different queues:</p>
<pre class="source-code">
u32 queue_family_count = 0;
    vkGetPhysicalDeviceQueueFamilyProperties(
    vulkan_physical_device, &amp;queue_family_count, nullptr );
    VkQueueFamilyProperties*queue_families = (
        VkQueueFamilyProperties* )ralloca( sizeof(
            VkQueueFamilyProperties ) * queue_family_count,
                temp_allocator );
        vkGetPhysicalDeviceQueueFamilyProperties(
            vulkan_physical_device, &amp;queue_family_count,
                queue_families );
    u32 main_queue_index = u32_max, transfer_queue_index =
    u32_max;
    for ( u32 fi = 0; fi &lt; queue_family_count; ++fi) {
        VkQueueFamilyProperties queue_family =
            queue_families[ fi ];
        if ( queue_family.queueCount == 0 ) {
            continue;
        }
        // Search for main queue that should be able to do
           all work (graphics, compute and transfer)
        if ( (queue_family.queueFlags &amp; (
              VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT |
              VK_QUEUE_TRANSFER_BIT )) == (
              VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT |
              VK_QUEUE_TRANSFER_BIT ) ) {
                 main_queue_index = fi;
        }
        // Search for transfer queue
        if ( ( queue_family.queueFlags &amp;
               VK_QUEUE_COMPUTE_BIT ) == 0 &amp;&amp;
               (queue_family.queueFlags &amp;
               VK_QUEUE_TRANSFER_BIT) ) {
            transfer_queue_index = fi;
        }
    }</pre>
<p>As can be seen in the preceding code, we get the list of all queues for the selected GPU, and we check the different bits that identify the types of commands that can be executed there.</p>
<p>In our case, we will save the <em class="italic">main queue</em> and the <em class="italic">transfer queue</em>, if it is present on the GPU, and we will save the indices of the <em class="italic">queues</em> to retrieve the <code>VkQueue</code> after the device creation. Some devices don’t expose a separate transfer queue. In this case, we will use the main queue to <a id="_idIndexMarker146"/>perform transfer operations, and we need to make sure that access to the queue is correctly synchronized for upload and graphics submissions.</p>
<p>Let’s see how to create the <em class="italic">queues</em>:</p>
<pre class="source-code">
// Queue creation
VkDeviceQueueCreateInfo queue_info[ 2 ] = {};
VkDeviceQueueCreateInfo&amp; main_queue = queue_info[ 0 ];
main_queue.sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE
                   _CREATE_INFO;
main_queue.queueFamilyIndex = main_queue_index;
main_queue.queueCount = 1;
main_queue.pQueuePriorities = queue_priority;
if ( vulkan_transfer_queue_family &lt; queue_family_count ) {
    VkDeviceQueueCreateInfo&amp; transfer_queue_info =
        queue_info[ 1 ];
    transfer_queue_info.sType = VK_STRUCTURE_TYPE
                                _DEVICE_QUEUE_CREATE_INFO;
    transfer_queue_info.queueFamilyIndex = transfer_queue
                                           _index;
transfer_queue_info.queueCount = 1;
transfer_queue_info.pQueuePriorities = queue_priority;
}
VkDeviceCreateInfo device_create_info {
    VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO };
device_create_info.queueCreateInfoCount = vulkan_transfer
    _queue_family &lt; queue_family_count ? 2 : 1;
device_create_info.pQueueCreateInfos = queue_info;
...
result = vkCreateDevice( vulkan_physical_device,
                         &amp;device_create_info,
                         vulkan_allocation_callbacks,
                         &amp;vulkan_device );</pre>
<p>As already<a id="_idIndexMarker147"/> mentioned, <code>vkCreateDevice</code> is the command that creates <em class="italic">queues</em> by adding <code>pQueueCreateInfos</code> in the <code>VkDeviceCreateInfo</code> struct.</p>
<p>Once the device is created, we can query for all the queues as follows:</p>
<pre class="source-code">
// Queue retrieval
// Get main queue
vkGetDeviceQueue( vulkan_device, main_queue_index, 0,
                  &amp;vulkan_main_queue );
// Get transfer queue if present
if ( vulkan_transfer_queue_family &lt; queue_family_count ) {
    vkGetDeviceQueue( vulkan_device, transfer_queue_index,
                      0, &amp;vulkan_transfer_queue );
}</pre>
<p>At this point, we have both the main and the transfer queues ready to be used to submit work in parallel.</p>
<p>We had a look at how <a id="_idIndexMarker148"/>to submit parallel work to copy memory over the GPU without blocking either the GPU or the CPU, and we created a specific class to do that, <code>AsynchronousLoader</code>, which we will cover in the next section.</p>
<h2 id="_idParaDest-55"><a id="_idTextAnchor054"/>The AsynchronousLoader class</h2>
<p>Here, we’ll finally see the code<a id="_idIndexMarker149"/> for the class that implements <a id="_idIndexMarker150"/>asynchronous loading.</p>
<p>The <code>AsynchronousLoader</code> class has the following responsibilities:</p>
<ul>
<li>Process load from file requests</li>
<li>Process GPU upload transfers</li>
<li>Manage a staging buffer to handle a copy of the data</li>
<li>Enqueue the command buffers with copy commands</li>
<li>Signal to the renderer that a texture has finished a transfer</li>
</ul>
<p>Before focusing on the code that uploads data to the GPU, there is some Vulkan-specific code that is important to understand, relative to command pools, transfer queues, and using a staging buffer.</p>
<h3>Creating command pools for the transfer queue</h3>
<p>In order to submit commands to the transfer queue, we need to create command pools that are linked to that<a id="_idIndexMarker151"/> queue:</p>
<pre class="source-code">
for ( u32 i = 0; i &lt; GpuDevice::k_max_frames; ++i) {
VkCommandPoolCreateInfo cmd_pool_info = {
    VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO, nullptr };
cmd_pool_info.queueFamilyIndex = gpu-&gt;vulkan
                                 _transfer_queue_family;
cmd_pool_info.flags = VK_COMMAND_POOL_CREATE_RESET
                      _COMMAND_BUFFER_BIT;
vkCreateCommandPool( gpu-&gt;vulkan_device, &amp;cmd_pool_info,
                     gpu-&gt;vulkan_allocation_callbacks,
                     &amp;command_pools[i]);
}</pre>
<p>The important part is <code>queueFamilyIndex</code>, to link <code>CommandPool</code> to the transfer queue so that every command buffer allocated from this pool can be properly submitted to the transfer queue.</p>
<p>Next, we will simply allocate the command buffers linked to the newly created pools:</p>
<pre class="source-code">
for ( u32 i = 0; i &lt; GpuDevice::k_max_frames; ++i) {
    VkCommandBufferAllocateInfo cmd = {
        VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
            nullptr };
       cmd.commandPool = command_pools[i];
cmd.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
cmd.commandBufferCount = 1;
vkAllocateCommandBuffers( renderer-&gt;gpu-&gt;vulkan_device,
                          &amp;cmd, &amp;command_buffers[i].
                          vk_command_buffer );</pre>
<p>With this setup, we are now ready to submit commands to the transfer queue using the command buffers.</p>
<p>Next, we will have a<a id="_idIndexMarker152"/> look at the staging buffer – an addition to ensure that the transfer to the GPU is the fastest possible from the CPU.</p>
<h3>Creating the staging buffer</h3>
<p>To optimally transfer <a id="_idIndexMarker153"/>data between the CPU and the GPU, there is the need to create an area of memory that can be used as a source to issue commands related to copying data to the GPU.</p>
<p>To achieve this, we will create a staging buffer, a persistent buffer that will serve this purpose. We will see both the Raptor wrapper and the Vulkan-specific code to create a persistent staging buffer.</p>
<p>In the following code, we will allocate a persistently mapped buffer of 64 MB:</p>
<pre class="source-code">
BufferCreation bc;
bc.reset().set( VK_BUFFER_USAGE_TRANSFER_SRC_BIT,
                ResourceUsageType::Stream, rmega( 64 )
                ).set_name( "staging_buffer" ).
                set_persistent( true );
BufferHandle staging_buffer_handle = gpu-&gt;create_buffer
                                     ( bc );</pre>
<p>This translates to the following code:</p>
<pre class="source-code">
VkBufferCreateInfo buffer_info{
    VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO };
buffer_info.usage = <strong class="bold">VK_BUFFER_USAGE_TRANSFER_SRC_BIT;</strong>
buffer_info.size = 64 * 1024 * 1024; // 64 MB
VmaAllocationCreateInfo allocation_create_info{};
allocation_create_info.flags = VMA_ALLOCATION_CREATE
_STRATEGY_BEST_FIT_BIT | <strong class="bold">VMA_ALLOCATION_CREATE_MAPPED_BIT;</strong>
VmaAllocationInfo allocation_info{};
check( vmaCreateBuffer( vma_allocator, &amp;buffer_info,
       &amp;allocation_create_info, &amp;buffer-&gt;vk_buffer,
       &amp;buffer-&gt;vma_allocation, &amp;allocation_info ) );</pre>
<p>This buffer will be the source of the memory transfers, and the <code>VMA_ALLOCATION_CREATE_MAPPED_BIT</code> flag ensures that it will always be mapped.</p>
<p>We can retrieve and use the pointer to the allocated data from the <code>allocation_info</code> structure, filled by <code>vmaCreateBuffer</code>:</p>
<pre class="source-code">
buffer-&gt;mapped_data = static_cast&lt;u8*&gt;(allocation_info.
                                       pMappedData);</pre>
<p>We can now use<a id="_idIndexMarker154"/> the staging buffer for any operation to send data to the GPU, and if ever there is the need for a bigger allocation, we could recreate a new staging buffer with a bigger size.</p>
<p>Next, we need to see the code to create a semaphore and a fence used to submit and synchronize the CPU and GPU execution of commands.</p>
<h3>Creating semaphores and fences for GPU synchronization</h3>
<p>The code here is<a id="_idIndexMarker155"/> straightforward; the only important part is the creation of a signaled fence because it will let the code start to process uploads:</p>
<pre class="source-code">
VkSemaphoreCreateInfo semaphore_info{
    VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO };
vkCreateSemaphore( gpu-&gt;vulkan_device, &amp;semaphore_info,
                   gpu-&gt;vulkan_allocation_callbacks,
                   &amp;transfer_complete_semaphore );
VkFenceCreateInfo fence_info{
    VK_STRUCTURE_TYPE_FENCE_CREATE_INFO };
fence_info.flags = VK_FENCE_CREATE_SIGNALED_BIT;
vkCreateFence( gpu-&gt;vulkan_device, &amp;fence_info,
               gpu-&gt;vulkan_allocation_callbacks,
               &amp;transfer_fence );</pre>
<p>Finally, we have<a id="_idIndexMarker156"/> now arrived at processing the requests.</p>
<h3>Processing a file request</h3>
<p>File requests are not specifically Vulkan-related, but it is useful to see how they are done.</p>
<p>We use the STB image library (<a href="https://github.com/nothings/stb">https://github.com/nothings/stb</a>) to load the texture into memory and then<a id="_idIndexMarker157"/> simply add the loaded memory and the associated texture to create an upload request. This will be responsible for copying the data from the memory to the GPU using the transfer queue:</p>
<pre class="source-code">
FileLoadRequest load_request = file_load_requests.back();
// Process request
int x, y, comp;
u8* texture_data = stbi_load( load_request.path, &amp;x, &amp;y,
                              &amp;comp, 4 );
// Signal the loader that an upload data is ready to be
   transferred to the GPU
UploadRequest&amp; upload_request = upload_requests.push_use();
upload_request.data = texture_data;
upload_request.texture = load_request.texture;</pre>
<p>Next, we will see how to process an upload request.</p>
<h3>Processing an upload request</h3>
<p>This is the part that finally <a id="_idIndexMarker158"/>uploads the data to the GPU. First, we need to ensure that the fence is signaled to proceed, which is why we created it already signaled.</p>
<p>If it is signaled, we can reset it so we can let the API signal it when the submission is done:</p>
<pre class="source-code">
// Wait for transfer fence to be finished
if ( vkGetFenceStatus( gpu-&gt;vulkan_device, transfer_fence )
     != VK_SUCCESS ) {
return;
}
// Reset if file requests are present.
vkResetFences( gpu-&gt;vulkan_device, 1, &amp;transfer_fence );</pre>
<p>We then proceed to take a request, allocate memory from the staging buffer, and use a command buffer to upload the GPU:</p>
<pre class="source-code">
// Get last request
UploadRequest request = upload_requests.back();
const sizet aligned_image_size = memory_align(
                                 texture-&gt;width *
                                 texture-&gt;height *
                                 k_texture_channels,
                                 k_texture_alignment );
// Request place in buffer
const sizet current_offset = staging_buffer_offset +
                             aligned_image_size;
CommandBuffer* cb = &amp;command_buffers[ gpu-&gt;current_frame ;
cb-&gt;begin();
cb-&gt;upload_texture_data( texture-&gt;handle, request.data,
                         staging_buffer-&gt;handle,
                         current_offset );
free( request.data );
cb-&gt;end();</pre>
<p>The <code>upload_texture_data</code> method is the one that takes care of uploading data and adding the needed<a id="_idIndexMarker159"/> barriers. This can be tricky, so we’ve included the code to show how it can be done.</p>
<p>First, we need to copy the data to the staging buffer:</p>
<pre class="source-code">
// Copy buffer_data to staging buffer
memcpy( staging_buffer-&gt;mapped_data +
        staging_buffer_offset, texture_data,
        static_cast&lt; size_t &gt;( image_size ) );</pre>
<p>Then we can prepare a copy, in this case, from the staging buffer to an image. Here, it is important to specify the offset into the staging buffer:</p>
<pre class="source-code">
VkBufferImageCopy region = {};
<strong class="bold">region.bufferOffset = staging_buffer_offset</strong>;
region.bufferRowLength = 0;
region.bufferImageHeight = 0;</pre>
<p>We then proceed with adding a precopy memory barrier to perform a layout transition and specify that the data is using the transfer queue.</p>
<p>This uses the code suggested in the synchronization examples provided by the Khronos Group (<a href="https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples">https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples</a>).</p>
<p>Once again, we show the raw Vulkan code that is simplified with some utility functions, highlighting the important lines:</p>
<pre class="source-code">
// Pre copy memory barrier to perform layout transition
VkImageMemoryBarrier preCopyMemoryBarrier;
...
.srcAccessMask = 0,
.dstAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT,
.oldLayout = VK_IMAGE_LAYOUT_UNDEFINED,
.newLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
.srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED,
.dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED,
.image = image,
.subresourceRange = ... };
...</pre>
<p>The texture is now <a id="_idIndexMarker160"/>ready to be copied to the GPU:</p>
<pre class="source-code">
// Copy from the staging buffer to the image
vkCmdCopyBufferToImage( vk_command_buffer,
                        staging_buffer-&gt;vk_buffer,
                        texture-&gt;vk_image,
                        VK_IMAGE_LAYOUT_TRANSFER_DST
                        _OPTIMAL, 1, &amp;region );</pre>
<p>The texture is now on the GPU, but it is still not usable from the main queue.</p>
<p>That is why we need another memory barrier that will also transfer ownership:</p>
<pre class="source-code">
// Post copy memory barrier
VkImageMemoryBarrier postCopyTransferMemoryBarrier = {
...
.srcAccessMask = VK_ACCESS_TRANFER_WRITE_BIT,
.dstAccessMask = 0,
.oldLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
.newLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL,
.srcQueueFamilyIndex = transferQueueFamilyIndex,
.dstQueueFamilyIndex = graphicsQueueFamilyIndex,
.image = image,
.subresourceRange = ... };</pre>
<p>Once the ownership is transferred, a final barrier is needed to ensure that the transfer is complete and the texture<a id="_idIndexMarker161"/> can be read from the shaders, but this will be done by the renderer because it needs to use the main queue.</p>
<h3>Signaling the renderer of the finished transfer</h3>
<p>The signaling is implemented <a id="_idIndexMarker162"/>by simply adding the texture to a mutexed list of textures to update so that it is thread safe.</p>
<p>At this point, we need to perform a final barrier for each transferred texture. We opted to add these barriers after all the rendering is done and before the present step, but it could also be done at the beginning of the frame.</p>
<p>As stated before, one last barrier is needed to signal that the newly updated image is ready to be read by shaders and that all the writing operations are done:</p>
<pre class="source-code">
VkImageMemoryBarrier postCopyGraphicsMemoryBarrier = {
...
.srcAccessMask = 0,
<strong class="bold">.dstAccessMask = VK_ACCESS_SHADER_READ_BIT,</strong>
.oldLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
<strong class="bold">.newLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL,</strong>
.srcQueueFamilyIndex = transferQueueFamilyIndex,
.dstQueueFamilyIndex = graphicsQueueFamilyIndex,
.image = image,
.subresourceRange = ... };</pre>
<p>We are now ready to use the texture on the GPU in our shaders, and the asynchronous loading is working. A very similar path is created for uploading buffers and thus will be omitted from the book but present in the code.</p>
<p>In this section, we <a id="_idIndexMarker163"/>saw how to unlock the asynchronous loading of resources to the GPU by using a transfer queue and different command buffers. We also showed how to manage ownership transfer between queues. Then, we finally saw the first steps in setting up tasks with the task scheduler, which is used to add multi-threading capabilities to the Raptor Engine.</p>
<p>In the next section, we will use the acquired knowledge to add the parallel recording of commands to draw objects on the screen.</p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/>Recording commands on multiple threads</h1>
<p>To record commands using<a id="_idIndexMarker164"/> multiple threads, it is necessary to use different command buffers, at least one on each thread, to record the commands and then submit them to the main queue. To be more precise, in Vulkan, any kind of pool <a id="_idIndexMarker165"/>needs to be externally synchronized by the user; thus, the best option is to have an association between a thread and a pool.</p>
<p>In the case of command buffers, they are allocated from the associated pool and commands registered in it. Pools can be <code>CommandPools</code>, <code>DescriptorSetPools</code>, and <code>QueryPools</code> (for time and occlusion queries), and once associated with a thread, they can be used freely inside that thread of execution.</p>
<p>The execution order of the command buffers is based on the order of the array submitted to the main queue – thus, from a Vulkan perspective, sorting can be performed on a command buffer level.</p>
<p>We will see how important the allocation strategy for command buffers is and how easy it is to draw in parallel once the<a id="_idIndexMarker166"/> allocation is in place. We <a id="_idIndexMarker167"/>will also talk about the different types of command buffers, a unique feature of Vulkan.</p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>The allocation strategy</h2>
<p>The success in recording commands in parallel is achieved by taking into consideration both thread access and frame access. When<a id="_idIndexMarker168"/> creating command pools, not only does each thread need a unique pool to allocate command buffers and commands from, but it also needs to not be in flight in the GPU.</p>
<p>A simple allocation strategy is to decide the maximum number of threads (we will call them <code>T</code>) that will record commands and the max number of frames (we will call them <code>F</code>) that can be in flight, then allocate command pools that are <code>F * </code><code>T</code>.</p>
<p>For each task that wants to render, using the pair frame-thread ID, we will guarantee that no pool will be either in flight or used by another thread.</p>
<p>This is a very conservative approach and can lead to unbalanced command generations, but it can be a great starting point and, in our case, enough to provide support for parallel rendering to the Raptor Engine.</p>
<p>In addition, we will allocate a maximum of five empty command buffers, two primary and three secondary, so that more tasks can execute chunks of rendering in parallel.</p>
<p>The class responsible for this is the <code>CommandBufferManager</code> class, accessible from the device, and it gives the user the possibility to request a command buffer through the <code>get_command_buffer</code> method.</p>
<p>In the next section, we will see the difference between primary and secondary command buffers, which are necessary to decide the granularity of the tasks to draw the frame in parallel.</p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor057"/>Command buffer recycling</h2>
<p>Linked to the allocation strategy is the recycling of the buffers. When a buffer has been executed, it can be reused to<a id="_idIndexMarker169"/> record new commands instead of always allocating new ones.</p>
<p>Thanks to the allocation strategy we chose, we associate a fixed amount of <code>CommandPools</code> to each frame, and thus to reuse the command buffers, we will reset its corresponding <code>CommandPool</code> instead of manually freeing buffers: this has been proven to be much more efficient on CPU time.</p>
<p>Note that we are not freeing the memory associated with the buffer, but we give <code>CommandPool</code> the freedom to reuse the total memory allocated between the command buffers that will be recorded, and it will reset all the states of all its command buffers to their initial state.</p>
<p>At the beginning of each frame, we call a simple method to reset pools:</p>
<pre class="source-code">
void CommandBufferManager::reset_pools( u32 frame_index ) {
    for ( u32 i = 0; i &lt; num_pools_per_frame; i++ ) {
        const u32 pool_index = pool_from_indices(
                               frame_index, i );
        vkResetCommandPool( gpu-&gt;vulkan_device,
                            vulkan_command_pools[
                            pool_index ], 0 );
    }
}</pre>
<p>There is a utility method to calculate the pool index, based on the thread and frame. </p>
<p>After the reset of the pools, we can reuse the command buffers to record commands without needing to explicitly do so for each command.</p>
<p>We can finally have a look at the different types of command buffers.</p>
<h2 id="_idParaDest-59"><a id="_idTextAnchor058"/>Primary versus secondary command buffers</h2>
<p>The Vulkan API has a unique difference in what command buffers can do: a command buffer can either be primary or secondary.</p>
<p>Primary command buffers are the <a id="_idIndexMarker170"/>most used ones and can perform any of the commands – drawing, compute, or copy commands, but their granularity is pretty coarse – at least one render pass must be used, and no pass can be further parallelized.</p>
<p>Secondary command buffers are much more limited – they can actually only execute draw commands within a render pass – but they can be used to parallelize the rendering of render passes that contain many draw calls (such as a G-Buffer render pass).</p>
<p>It is paramount then to make an informed decision about the granularity of the tasks, and especially important is to understand when to record using a primary or secondary buffer.</p>
<p>In <a href="B18395_04.xhtml#_idTextAnchor064"><em class="italic">Chapter 4</em></a>, <em class="italic">Implementing a Frame Graph</em>, we will see how a graph of the frame can give enough information to decide which command buffer type to use and how many objects and render passes should be used in a task.</p>
<p>In the next section, we will see how to use both primary and secondary command buffers.</p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor059"/>Drawing using primary command buffers</h2>
<p>Drawing using primary<a id="_idIndexMarker171"/> command buffers is the most common way of using Vulkan and also the simplest. A primary command buffer, as already stated before, can execute any kind of command with no limitation, and it is the only one that can be submitted to a queue to be executed on the GPU.</p>
<p>Creating a primary command buffer is simply a matter of using <code>VK_COMMAND_BUFFER_LEVEL_PRIMARY</code> in the <code>VkCommandBufferAllocateInfo</code> structure passed to the <code>vkAllocateCommandBuffers</code> function.</p>
<p>Once created, at any time, we can begin the commands recording (with the <code>vkBeginCommandBuffer</code> function), bind passes and pipelines, and issue draw commands, copy commands, and compute ones.</p>
<p>Once the recording is finished, the <code>vkEndCommandBuffer</code> function must be used to signal the end of recording and prepare the buffer to be ready to be submitted to a queue:</p>
<pre class="source-code">
VkSubmitInfo submit_info = {
    VK_STRUCTURE_TYPE_SUBMIT_INFO };
submit_info.commandBufferCount = num_queued_command
                                 _buffers;
submit_info.pCommandBuffers = enqueued_command_buffers;
...
vkQueueSubmit( vulkan_main_queue, 1, &amp;submit_info,
               *render_complete_fence );</pre>
<p>To record commands in parallel, there are only two conditions that must be respected by the recording threads:</p>
<ul>
<li>Simultaneous recording on the same <code>CommandPool</code> is forbidden</li>
<li>Commands relative to <code>RenderPass</code> can only be executed in one thread</li>
</ul>
<p>What happens if a pass (such as a Forward or G-Buffer typical pass) contains a lot of draw-calls, thus requiring<a id="_idIndexMarker172"/> parallel rendering? This is where secondary command buffers can be useful.</p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor060"/>Drawing using secondary command buffers</h2>
<p>Secondary<a id="_idIndexMarker173"/> command buffers have a very specific set of conditions to be used – they can record commands relative to only one render pass.</p>
<p>That is why it is important to allow the user to record more than one secondary command buffer: it could be possible that more than one pass needs per-pass parallelism, and thus more than one secondary command buffer is needed.</p>
<p>Secondary buffers always need a primary buffer and can’t be submitted directly to any queue: they must be copied into the primary buffer and inherit only <code>RenderPass</code> and <code>FrameBuffers</code> set when beginning to record commands.</p>
<p>Let’s have a look at the different steps involving the usage of secondary command buffers. First, we need to have a primary command buffer that needs to set up a render pass and frame buffer to be rendered into, as this is absolutely necessary because no secondary command buffer can be submitted to a queue or set <code>RenderPass</code> or <code>FrameBuffer</code>.</p>
<p>Those will be the only<a id="_idIndexMarker174"/> states inherited from the primary command buffer, thus, even when beginning to record commands, viewport and stencil states must be set again.</p>
<p>Let’s start by showing a primary command buffer setup:</p>
<pre class="source-code">
VkClearValue clearValues[2];
VkRenderPassBeginInfo renderPassBeginInfo {};
<strong class="bold">renderPassBeginInfo.renderPass = renderPass;</strong>
<strong class="bold">renderPassBeginInfo.framebuffer = frameBuffer;</strong>
vkBeginCommandBuffer(primaryCommandBuffer, &amp;cmdBufInfo);</pre>
<p>When beginning a render pass that will be split among one or more secondary command buffers, we need to add the <code>VK_SUBPASS_CONTENTS_SECONDARY_COMMAND_BUFFERS</code> flag:</p>
<pre class="source-code">
vkCmdBeginRenderPass(primaryCommandBuffer, &amp;renderPassBeginInfo, <strong class="bold">VK_SUBPASS_CONTENTS_SECONDARY_COMMAND_BUFFERS);</strong></pre>
<p>We can then pass the <code>inheritanceInfo</code> struct to the secondary buffer:</p>
<pre class="source-code">
VkCommandBufferInheritanceInfo inheritanceInfo {};
inheritanceInfo.renderPass = renderPass;
inheritanceInfo.framebuffer = frameBuffer;</pre>
<p>And then we can begin the secondary command buffer:</p>
<pre class="source-code">
VkCommandBufferBeginInfo commandBufferBeginInfo {};
commandBufferBeginInfo.flags =
<strong class="bold">VK_COMMAND_BUFFER_USAGE_RENDER_PASS_CONTINUE_BIT;</strong>
commandBufferBeginInfo.pInheritanceInfo = &amp;inheritanceInfo;
VkBeginCommandBuffer(secondaryCommandBuffer,
                     &amp;commandBufferBeginInfo);</pre>
<p>The secondary command buffer is now ready to start issuing drawing commands:</p>
<pre class="source-code">
vkCmdSetViewport(secondaryCommandBuffers.background, 0, 1,
                 &amp;viewport);
vkCmdSetScissor(secondaryCommandBuffers.background, 0, 1,
                &amp;scissor);
vkCmdBindPipeline(secondaryCommandBuffers.background,
                  VK_PIPELINE_BIND_POINT_GRAPHICS,
                  pipelines.starsphere);
VkDrawIndexed(…)</pre>
<p>Note that the scissor <a id="_idIndexMarker175"/>and viewport must always be set at the beginning, as no state is inherited outside of the bound render pass and frame buffer.</p>
<p>Once we have finished recording the commands, we can call the <code>VkEndCommandBuffer</code> function and put the buffer into a copiable state in the primary command buffer. To copy the secondary command buffers into the primary one, there is a specific function, <code>vkCmdExecuteCommands</code>, that needs to be called:</p>
<pre class="source-code">
vkCmdExecuteCommands(primaryCommandBuffer,
                     commandBuffers.size(),
                     commandBuffers.data());</pre>
<p>This function accepts an array of secondary command buffers that will be sequentially copied into the primary one.</p>
<p>To ensure a correct ordering of the commands recorded, not guaranteed by multi-threading (as threads can finish in any order), we can give each command buffer an execution index, put them all into an array, sort them, and then use this sorted array in the <code>vkCmdExecuteCommands</code> function.</p>
<p>At this point, the primary <a id="_idIndexMarker176"/>command buffer can record other commands or be submitted to the queue, as it contains all the commands copied from the secondary command buffers.</p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor061"/>Spawning multiple tasks to record command buffers</h2>
<p>The last step is to create multiple tasks to record command buffers in parallel. We have decided to group multiple <a id="_idIndexMarker177"/>meshes per command buffer as an example, but usually, you would record separate command buffers per render pass.</p>
<p>Let’s take a look at the code:</p>
<pre class="source-code">
SecondaryDrawTask secondary_tasks[ parallel_recordings ]{ };
u32 start = 0;
for ( u32 secondary_index = 0;
      secondary_index &lt; parallel_recordings;
      ++secondary_index ) {
    SecondaryDrawTask&amp; task = secondary_tasks[
                              secondary_index ];
    task.init( scene, renderer, gpu_commands, start,
               start + draws_per_secondary );
    start += draws_per_secondary;
    task_scheduler-&gt;AddTaskSetToPipe( &amp;task );
}</pre>
<p>We add a task to the scheduler for each mesh group. Each task will record a command buffer for a range of meshes.</p>
<p>Once we have added all the tasks, we have to wait until they complete before adding the secondary command<a id="_idIndexMarker178"/> buffers for execution on the main command buffer:</p>
<pre class="source-code">
for ( u32 secondary_index = 0;
      secondary_index &lt; parallel_recordings;
      ++secondary_index ) {
    SecondaryDrawTask&amp; task = secondary_tasks[
                              secondary_index ];
    task_scheduler-&gt;WaitforTask( &amp;task );
    vkCmdExecuteCommands( gpu_commands-&gt;vk_command_buffer,
                          1, &amp;task.cb-&gt;vk_command_buffer );
}</pre>
<p>We suggest reading the code for this chapter for more details on the implementation.</p>
<p>In this section, we have described how to record multiple command buffers in parallel to optimize this operation on the CPU. We have detailed our allocation strategy for command buffers and how they can be reused across frames.</p>
<p>We have highlighted the differences between primary and secondary buffers and how they are used in our renderer. Finally, we have demonstrated how to record multiple command buffers in parallel.</p>
<p>In the next chapter, we are going to introduce the frame graph, a system that allows us to define multiple render passes and that can take advantage of the task system we have described to record the command buffer for each render pass in parallel.</p>
<h1 id="_idParaDest-63"><a id="_idTextAnchor062"/>Summary</h1>
<p>In this chapter, we learned about the concept of task-based parallelism and saw how using a library such as enkiTS can quickly add multi-threading capabilities to the Raptor Engine.</p>
<p>We then learned how to add support for loading data from files to the GPU using an asynchronous loader. We also focused on Vulkan-related code to have a second queue of execution that can run in parallel to the one responsible for drawing. We saw the difference between primary and secondary command buffers.</p>
<p>We talked about the importance of the buffer’s allocation strategy to ensure safety when recording commands in parallel, especially taking into consideration command reuse between frames.</p>
<p>Finally, we showed step by step how to use both types of command buffers, and this should be enough to add the desired level of parallelism to any application that decides to use Vulkan as its graphics API.</p>
<p>In the next chapter, we will work on<a id="_idIndexMarker179"/> a data structure called <strong class="bold">Frame Graph</strong>, which will give us enough information to automate some of the recording processes, including barriers, and will ease the decision making about the granularity of the tasks that will perform parallel rendering.</p>
<h1 id="_idParaDest-64"><a id="_idTextAnchor063"/>Further reading</h1>
<p>Task-based systems have been in use for many years. <a href="https://www.gdcvault.com/play/1012321/Task-based-Multithreading-How-to">https://www.gdcvault.com/play/1012321/Task-based-Multithreading-How-to</a> provides a good overview.</p>
<p>Many articles can be found that cover work-stealing queues at <a href="https://blog.molecular-matters.com/2015/09/08/job-system-2-0-lock-free-work-stealing-part-2-a-specialized-allocator/">https://blog.molecular-matters.com/2015/09/08/job-system-2-0-lock-free-work-stealing-part-2-a-specialized-allocator/</a> and are a good starting point on the subject.</p>
<p>The PlayStation 3 and Xbox 360 use the Cell processor from IBM to provide more performance to developers through multiple cores. In particular, the PlayStation 3 has several <strong class="bold">synergistic processor units</strong> (<strong class="bold">SPUs</strong>) that developers can use to offload work from the main processor.</p>
<p>There are many presentations and articles that detail many clever ways developers have used these processors, for example, <a href="https://www.gdcvault.com/play/1331/The-PlayStation-3-s-SPU">https://www.gdcvault.com/play/1331/The-PlayStation-3-s-SPU</a> and <a href="https://gdcvault.com/play/1014356/Practical-Occlusion-Culling-on">https://gdcvault.com/play/1014356/Practical-Occlusion-Culling-on</a>.</p>
</div>
</div></body></html>