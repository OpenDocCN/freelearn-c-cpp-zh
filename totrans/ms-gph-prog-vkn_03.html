<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer015">
<h1 class="chapter-number" id="_idParaDest-46"><a id="_idTextAnchor045"/>3</h1>
<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Unlocking Multi-Threading</h1>
<p>In this chapter, we will talk about adding multi-threading to the <span class="No-Break">Raptor Engine.</span></p>
<p>This requires both a big change in the underlying architecture and some Vulkan-specific changes and synchronization work so that the different cores of the CPU and the GPU can cooperate in the most correct and the <span class="No-Break">fastest way.</span></p>
<p><strong class="bold">Multi-threading</strong> rendering is a topic covered many times over the years and a feature that most game engines have<a id="_idIndexMarker121"/> needed since the era of multi-core architectures exploded. Consoles such as the PlayStation 2 and the Sega Saturn already offered multi-threading support, and later generations continued the trend by providing an increasing number of cores that developers could take <span class="No-Break">advantage of.</span></p>
<p>The first trace of multi-threading rendering in a game engine is as far back as 2008 when Christer Ericson wrote a blog post (<a href="https://realtimecollisiondetection.net/blog/?p=86">https://realtimecollisiondetection.net/blog/?p=86</a>) and showed that it was possible to parallelize and optimize the generation of commands used to render objects on <span class="No-Break">the screen.</span></p>
<p>Older APIs such as OpenGL and DirectX (up until version 11) did not have proper multi-threading support, especially because they were big state machines with a global context tracking down each change after each command. Still, the command generation across different objects could take a few milliseconds, so multi-threading was already a big save <span class="No-Break">in performance.</span></p>
<p>Luckily for us, Vulkan fully supports multi-threading command buffers natively, especially with the creation of the <strong class="source-inline">VkCommandBuffer</strong> class, from an architectural perspective of the <span class="No-Break">Vulkan API.</span></p>
<p>The Raptor Engine, up until now, was a single-threaded application and thus required some architectural changes to fully support multi-threading. In this chapter, we will see those changes, learn how to use a task-based multi-threading library called enkiTS, and then unlock both asynchronous resource loading and multi-threading <span class="No-Break">command recording.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>How to use a task-based <span class="No-Break">multi-threading library</span></li>
<li>How to asynchronously <span class="No-Break">load resources</span></li>
<li>How to draw in <span class="No-Break">parallel threads</span></li>
</ul>
<p>By the end of the chapter, we will know how to run concurrent tasks both for loading resources and drawing objects on the screen. By learning how to reason with a task-based multi-threading system, we will be able to perform other parallel tasks in future chapters <span class="No-Break">as well.</span></p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Technical requirements</h1>
<p>The code for this chapter can be found at the following <span class="No-Break">URL: </span><a href="https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter3"><span class="No-Break">https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter3</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>Task-based multi-threading using enkiTS</h1>
<p>To achieve parallelism, we need to<a id="_idIndexMarker122"/> understand some basic concepts and choices that led to the architecture developed in this chapter. First, we should note that when we talk about parallelism in software engineering, we mean the act of executing chunks of code at the <span class="No-Break">same time.</span></p>
<p>This is possible because<a id="_idIndexMarker123"/> modern hardware has different units that can be operated independently, and operating systems have dedicated execution <a id="_idIndexMarker124"/>units <span class="No-Break">called </span><span class="No-Break"><strong class="bold">threads</strong></span><span class="No-Break">.</span></p>
<p>A common way to achieve parallelism is to reason with tasks – small independent execution units that can run on <span class="No-Break">any thread.</span></p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/>Why task-based parallelism?</h2>
<p>Multi-threading is not a new subject, and since the early years of it being added to various game engines, there have <a id="_idIndexMarker125"/>been different ways of implementing it. Game engines are pieces of software that use all of the hardware available in the most efficient way, thus paving the way for more optimized <span class="No-Break">software architectures.</span></p>
<p>Therefore, we’ll take some ideas from game engines and gaming-related presentations. The initial implementations started by adding a thread with a single job to do – something specific, such as rendering a single thread, an asynchronous <strong class="bold">input/output</strong> (<strong class="bold">I/O</strong>) thread, and <span class="No-Break">so on.</span></p>
<p>This helped add more granularity to what could be done in parallel, and it was perfect for the older CPUs (having two cores only), but it soon <span class="No-Break">became limiting.</span></p>
<p>There was the need to use cores in a more agnostic way so that any type of job could be done by almost any core and to improve performance. This gave way to the emergence of two new architectures: <strong class="bold">task-based</strong> and <span class="No-Break"><strong class="bold">fiber-based</strong></span><span class="No-Break"> architectures.</span></p>
<p>Task-based parallelism is<a id="_idIndexMarker126"/> achieved by feeding multiple threads with different tasks and orchestrating them through<a id="_idIndexMarker127"/> dependencies. Tasks are inherently platform agnostic and cannot be interrupted, leading to a more straightforward capability to schedule and organize code to be executed <span class="No-Break">with them.</span></p>
<p>On the other hand, fibers are software constructs similar to tasks, but they rely heavily on the scheduler to interrupt their flow and resume when needed. This main difference makes it hard to write a proper fiber system and normally leads to a lot of <span class="No-Break">subtle errors.</span></p>
<p>For the simplicity of using tasks over fibers and the bigger availability of libraries implementing task-based parallelism, the enkiTS library was chosen to handle everything. For those curious about more in-depth explanations, there are a couple of great presentations about <span class="No-Break">these architectures.</span></p>
<p>A great example of a task-based engine is the one behind the Destiny franchise (with an in-depth architecture you can view at <a href="https://www.gdcvault.com/play/1021926/Destiny-s-Multithreaded-Rendering">https://www.gdcvault.com/play/1021926/Destiny-s-Multithreaded-Rendering</a>), while <a id="_idIndexMarker128"/>a fiber-based one is used by the game studio Naughty Dog for their games (there is a presentation about it <span class="No-Break">at </span><a href="https://www.gdcvault.com/play/1022186/Parallelizing-the-Naughty-Dog-Engine"><span class="No-Break">https://www.gdcvault.com/play/1022186/Parallelizing-the-Naughty-Dog-Engine</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Using the enkiTS (Task-Scheduler) library</h2>
<p>Task-based multi-threading is<a id="_idIndexMarker129"/> based on the concept of a task, defined as a <em class="italic">unit of independent work that can be executed on any core of </em><span class="No-Break"><em class="italic">a CPU</em></span><span class="No-Break">.</span></p>
<p>To do that, there is a need for a scheduler to coordinate different tasks and take care of the possible dependencies between them. Another interesting aspect of a task is that it could have one or more dependencies so that it could be scheduled to run only after certain tasks finish <span class="No-Break">their execution.</span></p>
<p>This means that tasks can be submitted to the scheduler at any time, and with proper dependencies, we create a graph-based execution of the engine. If done properly, each core can be utilized fully and results in optimal performance to <span class="No-Break">the engine.</span></p>
<p>The scheduler is the brain behind all the tasks: it checks dependencies and priorities, and schedules or removes tasks based on need, and it is a new system added to the <span class="No-Break">Raptor Engine.</span></p>
<p>When initializing the<a id="_idIndexMarker130"/> scheduler, the library spawns a number of threads, each waiting to execute a task. When adding tasks to the scheduler, they are inserted into a queue. When the scheduler is told to execute pending tasks, each thread gets the next available task from the queue – according to dependency and priority – and <span class="No-Break">executes it.</span></p>
<p>It’s important to note that running tasks can spawn other tasks. These tasks will be added to the thread’s local queue, but they are up for grabs if another thread is idle. This implementation is <a id="_idIndexMarker131"/>called a <span class="No-Break"><strong class="bold">work-stealing queue</strong></span><span class="No-Break">.</span></p>
<p>Initializing the scheduler is as simple as creating a configuration and calling the <span class="No-Break"><strong class="source-inline">Initialize</strong></span><span class="No-Break"> method:</span></p>
<pre class="source-code">
enki::TaskSchedulerConfig config;
config.numTaskThreadsToCreate = 4;
enki::TaskScheduler task_scheduler;
task_scheduler.Initialize( config );</pre>
<p>With this code, we are telling the task scheduler to spawn four threads that it will use to perform its duties. enkiTS uses the <strong class="source-inline">TaskSet</strong> class as a unit of work, and it uses both inheritance and lambda functions to drive the execution of tasks in <span class="No-Break">the scheduler:</span></p>
<pre class="source-code">
Struct ParallelTaskSet : enki::ItaskSet {
    void ExecuteRange(  enki::TaskSetPartition range_,
                        uint32_t threadnum_ ) override {
        // do something here, can issue tasks with
           task_scheduler
    }
};
int main(int argc, const char * argv[]) {
    enki::TaskScheduler task_scheduler;
    task_scheduler.Initialize( config );
    ParallelTaskSet task; // default constructor has a set
                             size of 1
    task_scheduler.AddTaskSetToPipe( &amp;task );
    // wait for task set (running tasks if they exist)
    // since we've just added it and it has no range we'll
       likely run it.
    Task_scheduler.WaitforTask( &amp;task );
    return 0;
}</pre>
<p>In this simple snippet, we see how to create an empty <strong class="source-inline">TaskSet</strong> (as the name implies, a set of tasks) that defines how a task will execute the code, leaving the scheduler with the job of deciding how<a id="_idIndexMarker132"/> many of the tasks will be needed and which thread will <span class="No-Break">be used.</span></p>
<p>A more streamlined version of the previous code uses <span class="No-Break">lambda functions:</span></p>
<pre class="source-code">
enki::TaskSet task( 1, []( enki::TaskSetPartition range_,
  uint32_t threadnum_  ) {
         // do something here
  }  );
task_scheduler.AddTaskSetToPipe( &amp;task );</pre>
<p>This version can be easier when reading the code as it does break the flow less, but it is functionally equivalent to the <span class="No-Break">previous one.</span></p>
<p>Another feature of the enkiTS scheduler is the possibility to add pinned tasks – special tasks that will be bound to a thread and will always be executed there. We will see the use of pinned tasks in the next section to perform asynchronous <span class="No-Break">I/O operations.</span></p>
<p>In this section, we<a id="_idIndexMarker133"/> talked briefly about the different types of multi-threading so that we could express the reason for choosing to use task-based multi-threading. We then showed some simple examples of the enkiTS library and its usage, adding multi-threading capabilities to the <span class="No-Break">Raptor Engine.</span></p>
<p>In the next section, we will finally see a real use case in the engine, which is the asynchronous loading <span class="No-Break">of resources.</span></p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>Asynchronous loading</h1>
<p>The loading of resources is one of the (if not <em class="italic">the</em>) slowest operations that can be done in any framework. This is because the<a id="_idIndexMarker134"/> files to be loaded are big, and they can come from different sources, such as optical units (DVD and Blu-ray), hard drives, and even <span class="No-Break">the network.</span></p>
<p>It is another great topic, but the most important concept to understand is the inherent speed necessary to read <span class="No-Break">the memory:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer014">
<img alt="Figure 3.1 – A memory hierarchy" height="541" src="image/B18395_03_01.jpg" width="607"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – A memory hierarchy</p>
<p>As shown in the<a id="_idIndexMarker135"/> preceding diagram, the fastest memory is the registers memory. After registers follows the cache, with different levels and access speeds: both registers and caches are directly in the processing unit (both the CPU and GPU have registers and caches, even with different <span class="No-Break">underlying architectures).</span></p>
<p>Main memory refers to the RAM, which is the area that is normally populated with the data used by the application. It is slower than the cache, but it is the target of the loading operations as the only one directly accessible from the code. Then there are magnetic disks (hard drives) and optical drives – much slower but with greater capacity. They normally contain the asset data that will be loaded into the <span class="No-Break">main memory.</span></p>
<p>The final memory is in remote storage, such as from some servers, and it is the slowest. We will not deal with that here, but it can be used when working on applications that have some form of online service, such as <span class="No-Break">multiplayer games.</span></p>
<p>With the objective of optimizing the read access in an application, we want to transfer all the needed data into the main memory, as we can’t interact with caches and registers. To hide the slow speed of magnetic and optical disks, one of the most important things that can be done is to parallelize the loading of any resource coming from any medium so that the fluidity of the application is not <span class="No-Break">slowed down.</span></p>
<p>The most common way of doing it, and one example of the thread-specialization architecture we talked briefly about before, is to have a separate thread that handles just the loading of resources and interacts with other systems to update the used resources in <span class="No-Break">the engine.</span></p>
<p>In the following<a id="_idIndexMarker136"/> sections, we will talk about how to set up enkiTS and create tasks for parallelizing the Raptor Engine, as well as talk about Vulkan queues, which are necessary for parallel command submission. Finally, we will dwell on the actual code used for <span class="No-Break">asynchronous loading.</span></p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor052"/>Creating the I/O thread and tasks</h2>
<p>In the enkiTS library, there is a <a id="_idIndexMarker137"/>feature called <strong class="bold">pinned-task</strong> that associates a task to a specific thread so that it is <a id="_idIndexMarker138"/>continuously running there unless stopped by the user or a higher priority task is scheduled on <span class="No-Break">that thread.</span></p>
<p>To simplify things, we will add a new thread and avoid it being used by the application. This thread will be mostly idle, so the context switch should <span class="No-Break">be low:</span></p>
<pre class="source-code">
config.numTaskThreadsToCreate = 4;</pre>
<p>We then create a pinned task and associate it with a <span class="No-Break">thread ID:</span></p>
<pre class="source-code">
// Create IO threads at the end
RunPinnedTaskLoopTask run_pinned_task;
run_pinned_task.threadNum = task_scheduler.
                            GetNumTaskThreads() - 1;
task_scheduler.AddPinnedTask( &amp;run_pinned_task );</pre>
<p>At this point, we can create the actual task responsible for asynchronous loading, associating it with the same thread as the <span class="No-Break">pinned task:</span></p>
<pre class="source-code">
// Send async load task to external thread
AsynchronousLoadTask async_load_task;
async_load_task.threadNum = run_pinned_task.threadNum;
task_scheduler.AddPinnedTask( &amp;async_load_task );</pre>
<p>The final piece of the<a id="_idIndexMarker139"/> puzzle is the actual code for these two tasks. First, let us have a look at the first <span class="No-Break">pinned task:</span></p>
<pre class="source-code">
struct RunPinnedTaskLoopTask : enki::IPinnedTask {
    void Execute() override {
        while ( task_scheduler-&gt;GetIsRunning() &amp;&amp; execute )
         {
            task_scheduler-&gt;WaitForNewPinnedTasks();
            // this thread will 'sleep' until there are new
               pinned tasks
            task_scheduler-&gt;RunPinnedTasks();
        }
    }
    enki::TaskScheduler*task_scheduler;
    bool execute = true;
}; // struct RunPinnedTaskLoopTask</pre>
<p>This task will wait for any other pinned task and run them when possible. We have added an <strong class="source-inline">execute</strong> flag to stop the execution when needed, for example, when exiting the application, but it could be used in general to suspend it in other situations (such as when the application <span class="No-Break">is minimized).</span></p>
<p>The other task is the one executing the asynchronous loading using the <span class="No-Break"><strong class="source-inline">AsynchronousLoader</strong></span><span class="No-Break"> class:</span></p>
<pre class="source-code">
struct AsynchronousLoadTask : enki::IPinnedTask {
    void Execute() override {
        while ( execute ) {
            async_loader-&gt;update();
        }
    }
    AsynchronousLoader*async_loader;
    enki::TaskScheduler*task_scheduler;
    bool execute = true;
}; // struct AsynchronousLoadTask</pre>
<p>The idea behind this task is to always be active and wait for requests for resource loading. The <strong class="source-inline">while</strong> loop ensures that the root pinned task never schedules other tasks on this thread, locking it to I/O <span class="No-Break">as intended.</span></p>
<p>Before moving on to<a id="_idIndexMarker140"/> look at the <strong class="source-inline">AsynchronousLoader</strong> class, we need to look at an important concept in Vulkan, namely queues, and why they are a great addition for <span class="No-Break">asynchronous loading.</span></p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor053"/>Vulkan queues and the first parallel command generation</h2>
<p>The concept of a <em class="italic">queue</em> – which can be defined<a id="_idIndexMarker141"/> as the entry point to <a id="_idIndexMarker142"/>submit commands recorded in <strong class="source-inline">VkCommandBuffers</strong> to the GPU – is an addition to Vulkan compared to OpenGL and needs to be taken <span class="No-Break">care of.</span></p>
<p>Submission using a queue is a single-threaded operation, and a costly operation that becomes a synchronization point between CPU and GPU to be aware of. Normally, there is the main queue to which the engine submits command buffers before presenting the frame. This will send the work to the GPU and create the rendered <span class="No-Break">image intended.</span></p>
<p>But where there is one queue, there can be more. To enhance parallel execution, we can instead create different <em class="italic">queues</em> – and use them in different threads instead of the <span class="No-Break">main one.</span></p>
<p>A more in-depth look at queues can be found at <a href="https://github.com/KhronosGroup/Vulkan-Guide/blob/master/chapters/queues.adoc">https://github.com/KhronosGroup/Vulkan-Guide/blob/master/chapters/queues.adoc</a>, but what we need to know is that each queue can submit certain types of commands, visible through a <span class="No-Break">queue’s flag:</span></p>
<ul>
<li><strong class="source-inline">VK_QUEUE_GRAPHICS_BIT</strong> can submit all <span class="No-Break"><strong class="source-inline">vkCmdDraw</strong></span><span class="No-Break"> commands</span></li>
<li><strong class="source-inline">VK_QUEUE_COMPUTE</strong> can submit all <strong class="source-inline">vkCmdDispatch</strong> and <strong class="source-inline">vkCmdTraceRays</strong> (used for <span class="No-Break">ray tracing)</span></li>
<li><strong class="source-inline">VK_QUEUE_TRANSFER</strong> can submit copy commands, such as <strong class="source-inline">vkCmdCopyBuffer</strong>, <strong class="source-inline">vkCmdCopyBufferToImage</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">vkCmdCopyImageToBuffer</strong></span></li>
</ul>
<p>Each available queue is exposed<a id="_idIndexMarker143"/> through a queue family. Each queue family can have multiple capabilities and can expose multiple queues. Here is an example <span class="No-Break">to clarify:</span></p>
<pre class="source-code">
{
    "VkQueueFamilyProperties": {
        "queueFlags": [
            "VK_QUEUE_GRAPHICS_BIT",
            "VK_QUEUE_COMPUTE_BIT",
            "VK_QUEUE_TRANSFER_BIT",
            "VK_QUEUE_SPARSE_BINDING_BIT"
        ],
        "queueCount": 1,
    }
},
{
    "VkQueueFamilyProperties": {
        "queueFlags": [
            "VK_QUEUE_COMPUTE_BIT",
            "VK_QUEUE_TRANSFER_BIT",
            "VK_QUEUE_SPARSE_BINDING_BIT"
        ],
        "queueCount": 2,
    }
},
{
    "VkQueueFamilyProperties": {
        "queueFlags": [
            "VK_QUEUE_TRANSFER_BIT",
            "VK_QUEUE_SPARSE_BINDING_BIT"
        ],
        "queueCount": 2,
    }
}</pre>
<p>The first queue exposes all capabilities, and we only have one of them. The next queue can be used for compute and transfer, and the third one for transfer (we’ll ignore the sparse feature for now). We have two queues for each of <span class="No-Break">these families.</span></p>
<p>It is guaranteed that on a GPU there <a id="_idIndexMarker144"/>will always be at least one queue that can submit all types of commands, and that will be our <span class="No-Break">main queue.</span></p>
<p>In some GPUs, though, there can be specialized queues that have only the <strong class="source-inline">VK_QUEUE_TRANSFE</strong>R flag activated, which <a id="_idIndexMarker145"/>means that they can use <strong class="bold">direct memory access</strong> (<strong class="bold">DMA</strong>) to speed up the transfer of data between the CPU and <span class="No-Break">the GPU.</span></p>
<p>One last thing: the Vulkan logical device is responsible for creating and destroying queues – an operation normally done at the startup/shutdown of the application. Let us briefly see the code to query the support for <span class="No-Break">different queues:</span></p>
<pre class="source-code">
u32 queue_family_count = 0;
    vkGetPhysicalDeviceQueueFamilyProperties(
    vulkan_physical_device, &amp;queue_family_count, nullptr );
    VkQueueFamilyProperties*queue_families = (
        VkQueueFamilyProperties* )ralloca( sizeof(
            VkQueueFamilyProperties ) * queue_family_count,
                temp_allocator );
        vkGetPhysicalDeviceQueueFamilyProperties(
            vulkan_physical_device, &amp;queue_family_count,
                queue_families );
    u32 main_queue_index = u32_max, transfer_queue_index =
    u32_max;
    for ( u32 fi = 0; fi &lt; queue_family_count; ++fi) {
        VkQueueFamilyProperties queue_family =
            queue_families[ fi ];
        if ( queue_family.queueCount == 0 ) {
            continue;
        }
        // Search for main queue that should be able to do
           all work (graphics, compute and transfer)
        if ( (queue_family.queueFlags &amp; (
              VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT |
              VK_QUEUE_TRANSFER_BIT )) == (
              VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT |
              VK_QUEUE_TRANSFER_BIT ) ) {
                 main_queue_index = fi;
        }
        // Search for transfer queue
        if ( ( queue_family.queueFlags &amp;
               VK_QUEUE_COMPUTE_BIT ) == 0 &amp;&amp;
               (queue_family.queueFlags &amp;
               VK_QUEUE_TRANSFER_BIT) ) {
            transfer_queue_index = fi;
        }
    }</pre>
<p>As can be seen in the preceding code, we get the list of all queues for the selected GPU, and we check the different bits that identify the types of commands that can be <span class="No-Break">executed there.</span></p>
<p>In our case, we will save the <em class="italic">main queue</em> and the <em class="italic">transfer queue</em>, if it is present on the GPU, and we will save the indices of the <em class="italic">queues</em> to retrieve the <strong class="source-inline">VkQueue</strong> after the device creation. Some devices don’t expose a separate transfer queue. In this case, we will use the main queue to <a id="_idIndexMarker146"/>perform transfer operations, and we need to make sure that access to the queue is correctly synchronized for upload and <span class="No-Break">graphics submissions.</span></p>
<p>Let’s see how to create <span class="No-Break">the </span><span class="No-Break"><em class="italic">queues</em></span><span class="No-Break">:</span></p>
<pre class="source-code">
// Queue creation
VkDeviceQueueCreateInfo queue_info[ 2 ] = {};
VkDeviceQueueCreateInfo&amp; main_queue = queue_info[ 0 ];
main_queue.sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE
                   _CREATE_INFO;
main_queue.queueFamilyIndex = main_queue_index;
main_queue.queueCount = 1;
main_queue.pQueuePriorities = queue_priority;
if ( vulkan_transfer_queue_family &lt; queue_family_count ) {
    VkDeviceQueueCreateInfo&amp; transfer_queue_info =
        queue_info[ 1 ];
    transfer_queue_info.sType = VK_STRUCTURE_TYPE
                                _DEVICE_QUEUE_CREATE_INFO;
    transfer_queue_info.queueFamilyIndex = transfer_queue
                                           _index;
transfer_queue_info.queueCount = 1;
transfer_queue_info.pQueuePriorities = queue_priority;
}
VkDeviceCreateInfo device_create_info {
    VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO };
device_create_info.queueCreateInfoCount = vulkan_transfer
    _queue_family &lt; queue_family_count ? 2 : 1;
device_create_info.pQueueCreateInfos = queue_info;
...
result = vkCreateDevice( vulkan_physical_device,
                         &amp;device_create_info,
                         vulkan_allocation_callbacks,
                         &amp;vulkan_device );</pre>
<p>As already<a id="_idIndexMarker147"/> mentioned, <strong class="source-inline">vkCreateDevice</strong> is the command that creates <em class="italic">queues</em> by adding <strong class="source-inline">pQueueCreateInfos</strong> in the <span class="No-Break"><strong class="source-inline">VkDeviceCreateInfo</strong></span><span class="No-Break"> struct.</span></p>
<p>Once the device is created, we can query for all the queues <span class="No-Break">as follows:</span></p>
<pre class="source-code">
// Queue retrieval
// Get main queue
vkGetDeviceQueue( vulkan_device, main_queue_index, 0,
                  &amp;vulkan_main_queue );
// Get transfer queue if present
if ( vulkan_transfer_queue_family &lt; queue_family_count ) {
    vkGetDeviceQueue( vulkan_device, transfer_queue_index,
                      0, &amp;vulkan_transfer_queue );
}</pre>
<p>At this point, we have both the main and the transfer queues ready to be used to submit work <span class="No-Break">in parallel.</span></p>
<p>We had a look at how <a id="_idIndexMarker148"/>to submit parallel work to copy memory over the GPU without blocking either the GPU or the CPU, and we created a specific class to do that, <strong class="source-inline">AsynchronousLoader</strong>, which we will cover in the <span class="No-Break">next section.</span></p>
<h2 id="_idParaDest-55"><a id="_idTextAnchor054"/>The AsynchronousLoader class</h2>
<p>Here, we’ll finally see the code<a id="_idIndexMarker149"/> for the class that implements <a id="_idIndexMarker150"/><span class="No-Break">asynchronous loading.</span></p>
<p>The <strong class="source-inline">AsynchronousLoader</strong> class has the <span class="No-Break">following responsibilities:</span></p>
<ul>
<li>Process load from <span class="No-Break">file requests</span></li>
<li>Process GPU <span class="No-Break">upload transfers</span></li>
<li>Manage a staging buffer to handle a copy of <span class="No-Break">the data</span></li>
<li>Enqueue the command buffers with <span class="No-Break">copy commands</span></li>
<li>Signal to the renderer that a texture has finished <span class="No-Break">a transfer</span></li>
</ul>
<p>Before focusing on the code that uploads data to the GPU, there is some Vulkan-specific code that is important to understand, relative to command pools, transfer queues, and using a <span class="No-Break">staging buffer.</span></p>
<h3>Creating command pools for the transfer queue</h3>
<p>In order to submit commands to the transfer queue, we need to create command pools that are linked to <span class="No-Break">that</span><span class="No-Break"><a id="_idIndexMarker151"/></span><span class="No-Break"> queue:</span></p>
<pre class="source-code">
for ( u32 i = 0; i &lt; GpuDevice::k_max_frames; ++i) {
VkCommandPoolCreateInfo cmd_pool_info = {
    VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO, nullptr };
cmd_pool_info.queueFamilyIndex = gpu-&gt;vulkan
                                 _transfer_queue_family;
cmd_pool_info.flags = VK_COMMAND_POOL_CREATE_RESET
                      _COMMAND_BUFFER_BIT;
vkCreateCommandPool( gpu-&gt;vulkan_device, &amp;cmd_pool_info,
                     gpu-&gt;vulkan_allocation_callbacks,
                     &amp;command_pools[i]);
}</pre>
<p>The important part is <strong class="source-inline">queueFamilyIndex</strong>, to link <strong class="source-inline">CommandPool</strong> to the transfer queue so that every command buffer allocated from this pool can be properly submitted to the <span class="No-Break">transfer queue.</span></p>
<p>Next, we will simply allocate the command buffers linked to the newly <span class="No-Break">created pools:</span></p>
<pre class="source-code">
for ( u32 i = 0; i &lt; GpuDevice::k_max_frames; ++i) {
    VkCommandBufferAllocateInfo cmd = {
        VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
            nullptr };
       cmd.commandPool = command_pools[i];
cmd.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
cmd.commandBufferCount = 1;
vkAllocateCommandBuffers( renderer-&gt;gpu-&gt;vulkan_device,
                          &amp;cmd, &amp;command_buffers[i].
                          vk_command_buffer );</pre>
<p>With this setup, we are now ready to submit commands to the transfer queue using the <span class="No-Break">command buffers.</span></p>
<p>Next, we will have a<a id="_idIndexMarker152"/> look at the staging buffer – an addition to ensure that the transfer to the GPU is the fastest possible from <span class="No-Break">the CPU.</span></p>
<h3>Creating the staging buffer</h3>
<p>To optimally transfer <a id="_idIndexMarker153"/>data between the CPU and the GPU, there is the need to create an area of memory that can be used as a source to issue commands related to copying data to <span class="No-Break">the GPU.</span></p>
<p>To achieve this, we will create a staging buffer, a persistent buffer that will serve this purpose. We will see both the Raptor wrapper and the Vulkan-specific code to create a persistent <span class="No-Break">staging buffer.</span></p>
<p>In the following code, we will allocate a persistently mapped buffer of <span class="No-Break">64 MB:</span></p>
<pre class="source-code">
BufferCreation bc;
bc.reset().set( VK_BUFFER_USAGE_TRANSFER_SRC_BIT,
                ResourceUsageType::Stream, rmega( 64 )
                ).set_name( "staging_buffer" ).
                set_persistent( true );
BufferHandle staging_buffer_handle = gpu-&gt;create_buffer
                                     ( bc );</pre>
<p>This translates to the <span class="No-Break">following code:</span></p>
<pre class="source-code">
VkBufferCreateInfo buffer_info{
    VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO };
buffer_info.usage = <strong class="bold">VK_BUFFER_USAGE_TRANSFER_SRC_BIT;</strong>
buffer_info.size = 64 * 1024 * 1024; // 64 MB
VmaAllocationCreateInfo allocation_create_info{};
allocation_create_info.flags = VMA_ALLOCATION_CREATE
_STRATEGY_BEST_FIT_BIT | <strong class="bold">VMA_ALLOCATION_CREATE_MAPPED_BIT;</strong>
VmaAllocationInfo allocation_info{};
check( vmaCreateBuffer( vma_allocator, &amp;buffer_info,
       &amp;allocation_create_info, &amp;buffer-&gt;vk_buffer,
       &amp;buffer-&gt;vma_allocation, &amp;allocation_info ) );</pre>
<p>This buffer will be the source of the memory transfers, and the <strong class="source-inline">VMA_ALLOCATION_CREATE_MAPPED_BIT</strong> flag ensures that it will always <span class="No-Break">be mapped.</span></p>
<p>We can retrieve and use the pointer to the allocated data from the <strong class="source-inline">allocation_info</strong> structure, filled <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">vmaCreateBuffer</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
buffer-&gt;mapped_data = static_cast&lt;u8*&gt;(allocation_info.
                                       pMappedData);</pre>
<p>We can now use<a id="_idIndexMarker154"/> the staging buffer for any operation to send data to the GPU, and if ever there is the need for a bigger allocation, we could recreate a new staging buffer with a <span class="No-Break">bigger size.</span></p>
<p>Next, we need to see the code to create a semaphore and a fence used to submit and synchronize the CPU and GPU execution <span class="No-Break">of commands.</span></p>
<h3>Creating semaphores and fences for GPU synchronization</h3>
<p>The code here is<a id="_idIndexMarker155"/> straightforward; the only important part is the creation of a signaled fence because it will let the code start to <span class="No-Break">process uploads:</span></p>
<pre class="source-code">
VkSemaphoreCreateInfo semaphore_info{
    VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO };
vkCreateSemaphore( gpu-&gt;vulkan_device, &amp;semaphore_info,
                   gpu-&gt;vulkan_allocation_callbacks,
                   &amp;transfer_complete_semaphore );
VkFenceCreateInfo fence_info{
    VK_STRUCTURE_TYPE_FENCE_CREATE_INFO };
fence_info.flags = VK_FENCE_CREATE_SIGNALED_BIT;
vkCreateFence( gpu-&gt;vulkan_device, &amp;fence_info,
               gpu-&gt;vulkan_allocation_callbacks,
               &amp;transfer_fence );</pre>
<p>Finally, we have<a id="_idIndexMarker156"/> now arrived at processing <span class="No-Break">the requests.</span></p>
<h3>Processing a file request</h3>
<p>File requests are not specifically Vulkan-related, but it is useful to see how they <span class="No-Break">are done.</span></p>
<p>We use the STB image library (<a href="https://github.com/nothings/stb">https://github.com/nothings/stb</a>) to load the texture into memory and then<a id="_idIndexMarker157"/> simply add the loaded memory and the associated texture to create an upload request. This will be responsible for copying the data from the memory to the GPU using the <span class="No-Break">transfer queue:</span></p>
<pre class="source-code">
FileLoadRequest load_request = file_load_requests.back();
// Process request
int x, y, comp;
u8* texture_data = stbi_load( load_request.path, &amp;x, &amp;y,
                              &amp;comp, 4 );
// Signal the loader that an upload data is ready to be
   transferred to the GPU
UploadRequest&amp; upload_request = upload_requests.push_use();
upload_request.data = texture_data;
upload_request.texture = load_request.texture;</pre>
<p>Next, we will see how to process an <span class="No-Break">upload request.</span></p>
<h3>Processing an upload request</h3>
<p>This is the part that finally <a id="_idIndexMarker158"/>uploads the data to the GPU. First, we need to ensure that the fence is signaled to proceed, which is why we created it <span class="No-Break">already signaled.</span></p>
<p>If it is signaled, we can reset it so we can let the API signal it when the submission <span class="No-Break">is done:</span></p>
<pre class="source-code">
// Wait for transfer fence to be finished
if ( vkGetFenceStatus( gpu-&gt;vulkan_device, transfer_fence )
     != VK_SUCCESS ) {
return;
}
// Reset if file requests are present.
vkResetFences( gpu-&gt;vulkan_device, 1, &amp;transfer_fence );</pre>
<p>We then proceed to take a request, allocate memory from the staging buffer, and use a command buffer to upload <span class="No-Break">the GPU:</span></p>
<pre class="source-code">
// Get last request
UploadRequest request = upload_requests.back();
const sizet aligned_image_size = memory_align(
                                 texture-&gt;width *
                                 texture-&gt;height *
                                 k_texture_channels,
                                 k_texture_alignment );
// Request place in buffer
const sizet current_offset = staging_buffer_offset +
                             aligned_image_size;
CommandBuffer* cb = &amp;command_buffers[ gpu-&gt;current_frame ;
cb-&gt;begin();
cb-&gt;upload_texture_data( texture-&gt;handle, request.data,
                         staging_buffer-&gt;handle,
                         current_offset );
free( request.data );
cb-&gt;end();</pre>
<p>The <strong class="source-inline">upload_texture_data</strong> method is the one that takes care of uploading data and adding the needed<a id="_idIndexMarker159"/> barriers. This can be tricky, so we’ve included the code to show how it can <span class="No-Break">be done.</span></p>
<p>First, we need to copy the data to the <span class="No-Break">staging buffer:</span></p>
<pre class="source-code">
// Copy buffer_data to staging buffer
memcpy( staging_buffer-&gt;mapped_data +
        staging_buffer_offset, texture_data,
        static_cast&lt; size_t &gt;( image_size ) );</pre>
<p>Then we can prepare a copy, in this case, from the staging buffer to an image. Here, it is important to specify the offset into the <span class="No-Break">staging buffer:</span></p>
<pre class="source-code">
VkBufferImageCopy region = {};
<strong class="bold">region.bufferOffset = staging_buffer_offset</strong>;
region.bufferRowLength = 0;
region.bufferImageHeight = 0;</pre>
<p>We then proceed with adding a precopy memory barrier to perform a layout transition and specify that the data is using the <span class="No-Break">transfer queue.</span></p>
<p>This uses the code suggested in the synchronization examples provided by the Khronos <span class="No-Break">Group (</span><a href="https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples"><span class="No-Break">https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples</span></a><span class="No-Break">).</span></p>
<p>Once again, we show the raw Vulkan code that is simplified with some utility functions, highlighting the <span class="No-Break">important lines:</span></p>
<pre class="source-code">
// Pre copy memory barrier to perform layout transition
VkImageMemoryBarrier preCopyMemoryBarrier;
...
.srcAccessMask = 0,
.dstAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT,
.oldLayout = VK_IMAGE_LAYOUT_UNDEFINED,
.newLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
.srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED,
.dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED,
.image = image,
.subresourceRange = ... };
...</pre>
<p>The texture is now <a id="_idIndexMarker160"/>ready to be copied to <span class="No-Break">the GPU:</span></p>
<pre class="source-code">
// Copy from the staging buffer to the image
vkCmdCopyBufferToImage( vk_command_buffer,
                        staging_buffer-&gt;vk_buffer,
                        texture-&gt;vk_image,
                        VK_IMAGE_LAYOUT_TRANSFER_DST
                        _OPTIMAL, 1, &amp;region );</pre>
<p>The texture is now on the GPU, but it is still not usable from the <span class="No-Break">main queue.</span></p>
<p>That is why we need another memory barrier that will also <span class="No-Break">transfer ownership:</span></p>
<pre class="source-code">
// Post copy memory barrier
VkImageMemoryBarrier postCopyTransferMemoryBarrier = {
...
.srcAccessMask = VK_ACCESS_TRANFER_WRITE_BIT,
.dstAccessMask = 0,
.oldLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
.newLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL,
.srcQueueFamilyIndex = transferQueueFamilyIndex,
.dstQueueFamilyIndex = graphicsQueueFamilyIndex,
.image = image,
.subresourceRange = ... };</pre>
<p>Once the ownership is transferred, a final barrier is needed to ensure that the transfer is complete and the texture<a id="_idIndexMarker161"/> can be read from the shaders, but this will be done by the renderer because it needs to use the <span class="No-Break">main queue.</span></p>
<h3>Signaling the renderer of the finished transfer</h3>
<p>The signaling is implemented <a id="_idIndexMarker162"/>by simply adding the texture to a mutexed list of textures to update so that it is <span class="No-Break">thread safe.</span></p>
<p>At this point, we need to perform a final barrier for each transferred texture. We opted to add these barriers after all the rendering is done and before the present step, but it could also be done at the beginning of <span class="No-Break">the frame.</span></p>
<p>As stated before, one last barrier is needed to signal that the newly updated image is ready to be read by shaders and that all the writing operations <span class="No-Break">are done:</span></p>
<pre class="source-code">
VkImageMemoryBarrier postCopyGraphicsMemoryBarrier = {
...
.srcAccessMask = 0,
<strong class="bold">.dstAccessMask = VK_ACCESS_SHADER_READ_BIT,</strong>
.oldLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
<strong class="bold">.newLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL,</strong>
.srcQueueFamilyIndex = transferQueueFamilyIndex,
.dstQueueFamilyIndex = graphicsQueueFamilyIndex,
.image = image,
.subresourceRange = ... };</pre>
<p>We are now ready to use the texture on the GPU in our shaders, and the asynchronous loading is working. A very similar path is created for uploading buffers and thus will be omitted from the book but present in <span class="No-Break">the code.</span></p>
<p>In this section, we <a id="_idIndexMarker163"/>saw how to unlock the asynchronous loading of resources to the GPU by using a transfer queue and different command buffers. We also showed how to manage ownership transfer between queues. Then, we finally saw the first steps in setting up tasks with the task scheduler, which is used to add multi-threading capabilities to the <span class="No-Break">Raptor Engine.</span></p>
<p>In the next section, we will use the acquired knowledge to add the parallel recording of commands to draw objects on <span class="No-Break">the screen.</span></p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/>Recording commands on multiple threads</h1>
<p>To record commands using<a id="_idIndexMarker164"/> multiple threads, it is necessary to use different command buffers, at least one on each thread, to record the commands and then submit them to the main queue. To be more precise, in Vulkan, any kind of pool <a id="_idIndexMarker165"/>needs to be externally synchronized by the user; thus, the best option is to have an association between a thread and <span class="No-Break">a pool.</span></p>
<p>In the case of command buffers, they are allocated from the associated pool and commands registered in it. Pools can be <strong class="source-inline">CommandPools</strong>, <strong class="source-inline">DescriptorSetPools</strong>, and <strong class="source-inline">QueryPools</strong> (for time and occlusion queries), and once associated with a thread, they can be used freely inside that thread <span class="No-Break">of execution.</span></p>
<p>The execution order of the command buffers is based on the order of the array submitted to the main queue – thus, from a Vulkan perspective, sorting can be performed on a command <span class="No-Break">buffer level.</span></p>
<p>We will see how important the allocation strategy for command buffers is and how easy it is to draw in parallel once the<a id="_idIndexMarker166"/> allocation is in place. We <a id="_idIndexMarker167"/>will also talk about the different types of command buffers, a unique feature <span class="No-Break">of Vulkan.</span></p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>The allocation strategy</h2>
<p>The success in recording commands in parallel is achieved by taking into consideration both thread access and frame access. When<a id="_idIndexMarker168"/> creating command pools, not only does each thread need a unique pool to allocate command buffers and commands from, but it also needs to not be in flight in <span class="No-Break">the GPU.</span></p>
<p>A simple allocation strategy is to decide the maximum number of threads (we will call them <strong class="source-inline">T</strong>) that will record commands and the max number of frames (we will call them <strong class="source-inline">F</strong>) that can be in flight, then allocate command pools that are <strong class="source-inline">F * </strong><span class="No-Break"><strong class="source-inline">T</strong></span><span class="No-Break">.</span></p>
<p>For each task that wants to render, using the pair frame-thread ID, we will guarantee that no pool will be either in flight or used by <span class="No-Break">another thread.</span></p>
<p>This is a very conservative approach and can lead to unbalanced command generations, but it can be a great starting point and, in our case, enough to provide support for parallel rendering to the <span class="No-Break">Raptor Engine.</span></p>
<p>In addition, we will allocate a maximum of five empty command buffers, two primary and three secondary, so that more tasks can execute chunks of rendering <span class="No-Break">in parallel.</span></p>
<p>The class responsible for this is the <strong class="source-inline">CommandBufferManager</strong> class, accessible from the device, and it gives the user the possibility to request a command buffer through the <span class="No-Break"><strong class="source-inline">get_command_buffer</strong></span><span class="No-Break"> method.</span></p>
<p>In the next section, we will see the difference between primary and secondary command buffers, which are necessary to decide the granularity of the tasks to draw the frame <span class="No-Break">in parallel.</span></p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor057"/>Command buffer recycling</h2>
<p>Linked to the allocation strategy is the recycling of the buffers. When a buffer has been executed, it can be reused to<a id="_idIndexMarker169"/> record new commands instead of always allocating <span class="No-Break">new ones.</span></p>
<p>Thanks to the allocation strategy we chose, we associate a fixed amount of <strong class="source-inline">CommandPools</strong> to each frame, and thus to reuse the command buffers, we will reset its corresponding <strong class="source-inline">CommandPool</strong> instead of manually freeing buffers: this has been proven to be much more efficient on <span class="No-Break">CPU time.</span></p>
<p>Note that we are not freeing the memory associated with the buffer, but we give <strong class="source-inline">CommandPool</strong> the freedom to reuse the total memory allocated between the command buffers that will be recorded, and it will reset all the states of all its command buffers to their <span class="No-Break">initial state.</span></p>
<p>At the beginning of each frame, we call a simple method to <span class="No-Break">reset pools:</span></p>
<pre class="source-code">
void CommandBufferManager::reset_pools( u32 frame_index ) {
    for ( u32 i = 0; i &lt; num_pools_per_frame; i++ ) {
        const u32 pool_index = pool_from_indices(
                               frame_index, i );
        vkResetCommandPool( gpu-&gt;vulkan_device,
                            vulkan_command_pools[
                            pool_index ], 0 );
    }
}</pre>
<p>There is a utility method to calculate the pool index, based on the thread and frame. </p>
<p>After the reset of the pools, we can reuse the command buffers to record commands without needing to explicitly do so for <span class="No-Break">each command.</span></p>
<p>We can finally have a look at the different types of <span class="No-Break">command buffers.</span></p>
<h2 id="_idParaDest-59"><a id="_idTextAnchor058"/>Primary versus secondary command buffers</h2>
<p>The Vulkan API has a unique difference in what command buffers can do: a command buffer can either be primary <span class="No-Break">or secondary.</span></p>
<p>Primary command buffers are the <a id="_idIndexMarker170"/>most used ones and can perform any of the commands – drawing, compute, or copy commands, but their granularity is pretty coarse – at least one render pass must be used, and no pass can be <span class="No-Break">further parallelized.</span></p>
<p>Secondary command buffers are much more limited – they can actually only execute draw commands within a render pass – but they can be used to parallelize the rendering of render passes that contain many draw calls (such as a G-Buffer <span class="No-Break">render pass).</span></p>
<p>It is paramount then to make an informed decision about the granularity of the tasks, and especially important is to understand when to record using a primary or <span class="No-Break">secondary buffer.</span></p>
<p>In <a href="B18395_04.xhtml#_idTextAnchor064"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Implementing a Frame Graph</em>, we will see how a graph of the frame can give enough information to decide which command buffer type to use and how many objects and render passes should be used in <span class="No-Break">a task.</span></p>
<p>In the next section, we will see how to use both primary and secondary <span class="No-Break">command buffers.</span></p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor059"/>Drawing using primary command buffers</h2>
<p>Drawing using primary<a id="_idIndexMarker171"/> command buffers is the most common way of using Vulkan and also the simplest. A primary command buffer, as already stated before, can execute any kind of command with no limitation, and it is the only one that can be submitted to a queue to be executed on <span class="No-Break">the GPU.</span></p>
<p>Creating a primary command buffer is simply a matter of using <strong class="source-inline">VK_COMMAND_BUFFER_LEVEL_PRIMARY</strong> in the <strong class="source-inline">VkCommandBufferAllocateInfo</strong> structure passed to the <span class="No-Break"><strong class="source-inline">vkAllocateCommandBuffers</strong></span><span class="No-Break"> function.</span></p>
<p>Once created, at any time, we can begin the commands recording (with the <strong class="source-inline">vkBeginCommandBuffer</strong> function), bind passes and pipelines, and issue draw commands, copy commands, and <span class="No-Break">compute ones.</span></p>
<p>Once the recording is finished, the <strong class="source-inline">vkEndCommandBuffer</strong> function must be used to signal the end of recording and prepare the buffer to be ready to be submitted to <span class="No-Break">a queue:</span></p>
<pre class="source-code">
VkSubmitInfo submit_info = {
    VK_STRUCTURE_TYPE_SUBMIT_INFO };
submit_info.commandBufferCount = num_queued_command
                                 _buffers;
submit_info.pCommandBuffers = enqueued_command_buffers;
...
vkQueueSubmit( vulkan_main_queue, 1, &amp;submit_info,
               *render_complete_fence );</pre>
<p>To record commands in parallel, there are only two conditions that must be respected by the <span class="No-Break">recording threads:</span></p>
<ul>
<li>Simultaneous recording on the same <strong class="source-inline">CommandPool</strong> <span class="No-Break">is forbidden</span></li>
<li>Commands relative to <strong class="source-inline">RenderPass</strong> can only be executed in <span class="No-Break">one thread</span></li>
</ul>
<p>What happens if a pass (such as a Forward or G-Buffer typical pass) contains a lot of draw-calls, thus requiring<a id="_idIndexMarker172"/> parallel rendering? This is where secondary command buffers can <span class="No-Break">be useful.</span></p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor060"/>Drawing using secondary command buffers</h2>
<p>Secondary<a id="_idIndexMarker173"/> command buffers have a very specific set of conditions to be used – they can record commands relative to only one <span class="No-Break">render pass.</span></p>
<p>That is why it is important to allow the user to record more than one secondary command buffer: it could be possible that more than one pass needs per-pass parallelism, and thus more than one secondary command buffer <span class="No-Break">is needed.</span></p>
<p>Secondary buffers always need a primary buffer and can’t be submitted directly to any queue: they must be copied into the primary buffer and inherit only <strong class="source-inline">RenderPass</strong> and <strong class="source-inline">FrameBuffers</strong> set when beginning to <span class="No-Break">record commands.</span></p>
<p>Let’s have a look at the different steps involving the usage of secondary command buffers. First, we need to have a primary command buffer that needs to set up a render pass and frame buffer to be rendered into, as this is absolutely necessary because no secondary command buffer can be submitted to a queue or set <strong class="source-inline">RenderPass</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">FrameBuffer</strong></span><span class="No-Break">.</span></p>
<p>Those will be the only<a id="_idIndexMarker174"/> states inherited from the primary command buffer, thus, even when beginning to record commands, viewport and stencil states must be <span class="No-Break">set again.</span></p>
<p>Let’s start by showing a primary command <span class="No-Break">buffer setup:</span></p>
<pre class="source-code">
VkClearValue clearValues[2];
VkRenderPassBeginInfo renderPassBeginInfo {};
<strong class="bold">renderPassBeginInfo.renderPass = renderPass;</strong>
<strong class="bold">renderPassBeginInfo.framebuffer = frameBuffer;</strong>
vkBeginCommandBuffer(primaryCommandBuffer, &amp;cmdBufInfo);</pre>
<p>When beginning a render pass that will be split among one or more secondary command buffers, we need to add the <span class="No-Break"><strong class="source-inline">VK_SUBPASS_CONTENTS_SECONDARY_COMMAND_BUFFERS</strong></span><span class="No-Break"> flag:</span></p>
<pre class="source-code">
vkCmdBeginRenderPass(primaryCommandBuffer, &amp;renderPassBeginInfo, <strong class="bold">VK_SUBPASS_CONTENTS_SECONDARY_COMMAND_BUFFERS);</strong></pre>
<p>We can then pass the <strong class="source-inline">inheritanceInfo</strong> struct to the <span class="No-Break">secondary buffer:</span></p>
<pre class="source-code">
VkCommandBufferInheritanceInfo inheritanceInfo {};
inheritanceInfo.renderPass = renderPass;
inheritanceInfo.framebuffer = frameBuffer;</pre>
<p>And then we can begin the secondary <span class="No-Break">command buffer:</span></p>
<pre class="source-code">
VkCommandBufferBeginInfo commandBufferBeginInfo {};
commandBufferBeginInfo.flags =
<strong class="bold">VK_COMMAND_BUFFER_USAGE_RENDER_PASS_CONTINUE_BIT;</strong>
commandBufferBeginInfo.pInheritanceInfo = &amp;inheritanceInfo;
VkBeginCommandBuffer(secondaryCommandBuffer,
                     &amp;commandBufferBeginInfo);</pre>
<p>The secondary command buffer is now ready to start issuing <span class="No-Break">drawing commands:</span></p>
<pre class="source-code">
vkCmdSetViewport(secondaryCommandBuffers.background, 0, 1,
                 &amp;viewport);
vkCmdSetScissor(secondaryCommandBuffers.background, 0, 1,
                &amp;scissor);
vkCmdBindPipeline(secondaryCommandBuffers.background,
                  VK_PIPELINE_BIND_POINT_GRAPHICS,
                  pipelines.starsphere);
VkDrawIndexed(…)</pre>
<p>Note that the scissor <a id="_idIndexMarker175"/>and viewport must always be set at the beginning, as no state is inherited outside of the bound render pass and <span class="No-Break">frame buffer.</span></p>
<p>Once we have finished recording the commands, we can call the <strong class="source-inline">VkEndCommandBuffer</strong> function and put the buffer into a copiable state in the primary command buffer. To copy the secondary command buffers into the primary one, there is a specific function, <strong class="source-inline">vkCmdExecuteCommands</strong>, that needs to <span class="No-Break">be called:</span></p>
<pre class="source-code">
vkCmdExecuteCommands(primaryCommandBuffer,
                     commandBuffers.size(),
                     commandBuffers.data());</pre>
<p>This function accepts an array of secondary command buffers that will be sequentially copied into the <span class="No-Break">primary one.</span></p>
<p>To ensure a correct ordering of the commands recorded, not guaranteed by multi-threading (as threads can finish in any order), we can give each command buffer an execution index, put them all into an array, sort them, and then use this sorted array in the <span class="No-Break"><strong class="source-inline">vkCmdExecuteCommands</strong></span><span class="No-Break"> function.</span></p>
<p>At this point, the primary <a id="_idIndexMarker176"/>command buffer can record other commands or be submitted to the queue, as it contains all the commands copied from the secondary <span class="No-Break">command buffers.</span></p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor061"/>Spawning multiple tasks to record command buffers</h2>
<p>The last step is to create multiple tasks to record command buffers in parallel. We have decided to group multiple <a id="_idIndexMarker177"/>meshes per command buffer as an example, but usually, you would record separate command buffers per <span class="No-Break">render pass.</span></p>
<p>Let’s take a look at <span class="No-Break">the code:</span></p>
<pre class="source-code">
SecondaryDrawTask secondary_tasks[ parallel_recordings ]{ };
u32 start = 0;
for ( u32 secondary_index = 0;
      secondary_index &lt; parallel_recordings;
      ++secondary_index ) {
    SecondaryDrawTask&amp; task = secondary_tasks[
                              secondary_index ];
    task.init( scene, renderer, gpu_commands, start,
               start + draws_per_secondary );
    start += draws_per_secondary;
    task_scheduler-&gt;AddTaskSetToPipe( &amp;task );
}</pre>
<p>We add a task to the scheduler for each mesh group. Each task will record a command buffer for a range <span class="No-Break">of meshes.</span></p>
<p>Once we have added all the tasks, we have to wait until they complete before adding the secondary command<a id="_idIndexMarker178"/> buffers for execution on the main <span class="No-Break">command buffer:</span></p>
<pre class="source-code">
for ( u32 secondary_index = 0;
      secondary_index &lt; parallel_recordings;
      ++secondary_index ) {
    SecondaryDrawTask&amp; task = secondary_tasks[
                              secondary_index ];
    task_scheduler-&gt;WaitforTask( &amp;task );
    vkCmdExecuteCommands( gpu_commands-&gt;vk_command_buffer,
                          1, &amp;task.cb-&gt;vk_command_buffer );
}</pre>
<p>We suggest reading the code for this chapter for more details on <span class="No-Break">the implementation.</span></p>
<p>In this section, we have described how to record multiple command buffers in parallel to optimize this operation on the CPU. We have detailed our allocation strategy for command buffers and how they can be reused <span class="No-Break">across frames.</span></p>
<p>We have highlighted the differences between primary and secondary buffers and how they are used in our renderer. Finally, we have demonstrated how to record multiple command buffers <span class="No-Break">in parallel.</span></p>
<p>In the next chapter, we are going to introduce the frame graph, a system that allows us to define multiple render passes and that can take advantage of the task system we have described to record the command buffer for each render pass <span class="No-Break">in parallel.</span></p>
<h1 id="_idParaDest-63"><a id="_idTextAnchor062"/>Summary</h1>
<p>In this chapter, we learned about the concept of task-based parallelism and saw how using a library such as enkiTS can quickly add multi-threading capabilities to the <span class="No-Break">Raptor Engine.</span></p>
<p>We then learned how to add support for loading data from files to the GPU using an asynchronous loader. We also focused on Vulkan-related code to have a second queue of execution that can run in parallel to the one responsible for drawing. We saw the difference between primary and secondary <span class="No-Break">command buffers.</span></p>
<p>We talked about the importance of the buffer’s allocation strategy to ensure safety when recording commands in parallel, especially taking into consideration command reuse <span class="No-Break">between frames.</span></p>
<p>Finally, we showed step by step how to use both types of command buffers, and this should be enough to add the desired level of parallelism to any application that decides to use Vulkan as its <span class="No-Break">graphics API.</span></p>
<p>In the next chapter, we will work on<a id="_idIndexMarker179"/> a data structure called <strong class="bold">Frame Graph</strong>, which will give us enough information to automate some of the recording processes, including barriers, and will ease the decision making about the granularity of the tasks that will perform <span class="No-Break">parallel rendering.</span></p>
<h1 id="_idParaDest-64"><a id="_idTextAnchor063"/>Further reading</h1>
<p>Task-based systems have been in use for many years. <a href="https://www.gdcvault.com/play/1012321/Task-based-Multithreading-How-to">https://www.gdcvault.com/play/1012321/Task-based-Multithreading-How-to</a> provides a <span class="No-Break">good overview.</span></p>
<p>Many articles can be found that cover work-stealing queues at <a href="https://blog.molecular-matters.com/2015/09/08/job-system-2-0-lock-free-work-stealing-part-2-a-specialized-allocator/">https://blog.molecular-matters.com/2015/09/08/job-system-2-0-lock-free-work-stealing-part-2-a-specialized-allocator/</a> and are a good starting point on <span class="No-Break">the subject.</span></p>
<p>The PlayStation 3 and Xbox 360 use the Cell processor from IBM to provide more performance to developers through multiple cores. In particular, the PlayStation 3 has several <strong class="bold">synergistic processor units</strong> (<strong class="bold">SPUs</strong>) that developers can use to offload work from the <span class="No-Break">main processor.</span></p>
<p>There are many presentations and articles that detail many clever ways developers have used these processors, for example, <a href="https://www.gdcvault.com/play/1331/The-PlayStation-3-s-SPU">https://www.gdcvault.com/play/1331/The-PlayStation-3-s-SPU</a> <span class="No-Break">and </span><a href="https://gdcvault.com/play/1014356/Practical-Occlusion-Culling-on"><span class="No-Break">https://gdcvault.com/play/1014356/Practical-Occlusion-Culling-on</span></a><span class="No-Break">.</span></p>
</div>
</div></body></html>