- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimization through Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this last chapter, we are going to discuss the last thing we should think
    about before releasing our games: optimization. Optimization patterns are designed
    to leave our code functioning as it was before but in a faster, more elegant way
    that impacts our hardware less. This chapter is quite wordy, but the underlying
    principles that guide these patterns require a certain understanding of how the
    hardware resources at our disposal work. By the end, we will have covered everything
    from how to help the CPU do its job better to making a system you can plug into
    any game to make it potentially faster at runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The patterns making this possible are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dirty Flag**, which focuses on reducing the number of times we need to update
    calculated values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Locality**, which concerns optimizing the code layout to work with the
    way the CPU’s memory works. As a description, this sounds much more complicated
    than the reality of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object Pooling**, where we offset as much of the heavy memory allocation
    processing to the start of the game, where it can be excused under a loading screen,
    so as not to impact runtime efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using dirty flags to reduce unnecessary processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How data locality affects code efficiency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object pooling our resources to save time later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The starting point for this chapter can really be from any project, but we
    have a branch of the GitHub repository that carries on from where [*Chapter 9*](B18297_09.xhtml#_idTextAnchor130)
    left off. This provides a set of systems we can integrate with the Object Pooler
    we will be building. You can find this starting point in the [*Chapter 10*](B18297_10.xhtml#_idTextAnchor148)
    branch here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Game-Development-Patterns-with-Unreal-Engine-5/tree/main/Chapter10](https://github.com/PacktPublishing/Game-Development-Patterns-with-Unreal-Engine-5/tree/main/Chapter10)'
  prefs: []
  type: TYPE_NORMAL
- en: Using dirty flags to reduce unnecessary processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dirty flag involves updating values only when they are needed. The best explanation
    of how it works comes in the context of base-level engine development and the
    transform hierarchy. When you set a local location on a transform, you are indicating
    that you want to be *x*, *y*, and *z* units away from the parent’s location. This
    is easy enough to update, but in doing this, we are also changing the transform’s
    world space location. It is easy to calculate the matrix that will deal with this
    local-to-world space conversion, then multiply our vector by it; that process
    doesn’t even cost many resources. Then, we must remember that this is a hierarchy.
    What if we were moving the root of a tree that is hundreds of transforms deep?
    Not a great position to be in for multiple reasons, but if the parent of a transform
    moves, then the child moves with it, and so on, recursively. This presents a lot
    of world space value transforms that need to be updated every time any parent
    in the tree is changed.
  prefs: []
  type: TYPE_NORMAL
- en: To make the utility of dirty flag easier to quantify, we can consider a hypothetical
    situation where we have a hierarchy of transforms that is 100 parent-child connections
    deep. We want to move each of them like they are a chain with a torque ripple.
    Starting at the top of the hierarchy and moving down, at every step, we update
    the position of the transform at that level to a new local location defined by
    some periodic function. With each local update, we also update the local-to-world
    matrix for every transform lower in the hierarchy as they will have moved in world
    space, as shown in *Figure 10**.1*. This would require (101-n) matrix updates
    at each step, which means to move the entire hierarchy of 100 transforms, we will
    end up with the 100th triangular number, which is 5,050\. I think we can safely
    say that’s ridiculous and there has to be a better way. Consider the utility of
    the work done. Why are we updating these transforms? So that something else can
    read its world space location and get an accurate, up-to-date value. Have we read
    the world space location at any point in this algorithm? No. The function for
    setting the local location doesn’t need the world space location. So, do we need
    to update the local to world matrices of these objects? Not until something else
    needs us to or the end of the frame is reached. For the best case, that means
    we could get away with only 100 matrix updates at the very end. That is the purpose
    of the dirty flag pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Diagram showing a local location change affecting the world
    space location of its child](img/Figure_10.01_B18297.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Diagram showing a local location change affecting the world space
    location of its child
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the application of dirty flags.
  prefs: []
  type: TYPE_NORMAL
- en: Application of dirty flags
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In practice, this whole pattern is just a Boolean value; when the object is
    considered *dirty*, the Boolean is one value, and when it is *clean*, it is the
    other. Which way around that is doesn’t matter, as long as it’s consistent with
    the naming. An object is dirty if it has pending changes that still need to be
    made to the values it represents. In our example, we postpone the local-to-world
    matrix update until something requests the matrix or the end of the frame is reached.
    During the time between the change to the local location and the matrix update,
    the transform is considered dirty.
  prefs: []
  type: TYPE_NORMAL
- en: The mechanics of how this is dealt with then extend beyond the Boolean value.
    On raising a dirty flag, that object is added to a relevant static dirty list
    of objects. For our transform, that means it, and all the transforms below it
    in the hierarchy, will be added recursively to a static list of other dirty transforms.
    We then have some cleaning function that describes how to go from a dirty state
    to a clean state. With the transform, that would be a function that calculates
    the updated local-to-world matrix.
  prefs: []
  type: TYPE_NORMAL
- en: That example is only something you’d have to be worried about if you were designing
    your own engine, but the dirty flag pattern can be applied to great effect anywhere
    you have a value that may be updated several times before it is needed. You can
    find this pattern in `destroy` command, which marks actors for destruction at
    the end of the frame due to the trickle-down effects of removing them. There will
    be cases where you are storing data that needs to be displayed to the UI and the
    dirty flag pattern can reduce the number of times you tell the UI to update per
    frame; it should only happen once at most per frame.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll go deeper, from a one-variable pattern to a guiding principle of
    optimization that we can apply anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: How data locality affects code efficiency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a simple concept that requires very little to implement. We all actively
    think about how variables take up memory. We tend to forget that instructions
    also take up memory. They must be loaded from storage into faster memory, then
    into the CPU to be executed. CPUs try to make things run quicker by leveraging
    the fact that they have very fast, very small storage within them called the cache.
    The cache allows the CPU to pre-load instructions ready for execution and store
    its state in case of temporary context switching. This pre-loading behavior is
    necessary for the CPU to run at its most efficient as while there has been a technology
    race in CPU speeds, that hasn’t been mirrored in the world of RAM. You might be
    able to store massive programs entirely in your RAM but the bus speed of the motherboard
    limits how many instructions can be sent to the CPU per second. When we reach
    the bottleneck, it doesn’t matter how fast the CPU cores are at calculating results,
    as they would spend most of their time idle, waiting for instruction. Pre-loading
    provides a mechanism that has the potential to fix this by sending large chunks
    of instructions to the CPU cache for processing. We say *potential to fix* as
    when pulling instructions from RAM, there is no way of knowing which instructions
    will be next. That information is in the instruction daisy chain and can only
    be accessed once the work is done. This means that the contents of the CPU cache
    are entirely dictated by the geography of our system architecture.
  prefs: []
  type: TYPE_NORMAL
- en: That’s lots of technical terms, so let’s explain it with an analogy. Imagine
    working in a factory where your job is to build flatpack furniture. You’re really
    fast at your job when you have the materials and can put together each piece lighting
    fast. The catch is, you can only see one instruction at a time, and when you need
    pieces, you must request them from a porter. These resources, such as panels and
    screws, are stored in a warehouse miles away. When you request something, the
    porter spends a day traveling to and from the warehouse, which means you can only
    get one instruction completed per day regardless of how fast you work. Most of
    your time is spent staring at the wall. In this example, you are the CPU, executing
    instructions, and the porter is the data bus, ferrying instructions from RAM to
    be executed.
  prefs: []
  type: TYPE_NORMAL
- en: One day, a new manager is brought in who decides to revamp the process. They
    change the material request process so that now, when the porter gets the required
    item, they also get everything else within arm’s reach. This bundle of screws
    and panels is then dumped on the ground in your workstation. In real terms, we
    call this the CPU cache, a tiny amount of extremely fast memory within the CPU.
    The benefit of this is that when you get your next instruction, there is a chance
    you already have the required materials next to you. If not, then all the materials
    need to be taken back and a new bunch collected. It then stands to reason that
    if the warehouse is organized so that materials that are often requested together
    are placed near each other, the porter is more likely to take the correct materials
    for the next instruction as well. Nothing needs to change about the porter’s knowledge
    of the situation or skills, simply proper planning at the start to achieve an
    efficient outcome.
  prefs: []
  type: TYPE_NORMAL
- en: As programmers, we can be the warehouse manager in that example, making sure
    that the data a function requires is physically close to that function so that
    when the CPU request for resources comes in, the cache is more likely to fill
    with useful data. When the CPU can execute from the cache, that is called a cache
    hit. Likewise, when the data needs to be requested, that is a cache miss. We want
    to achieve as many cache hits as possible to reduce the number of times the cache
    needs to be refilled. The gains from achieving high levels of cache hits are surprising;
    sometimes, they can be up to 50 times faster due to organizing data effectively.
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to have a look at two methods for implementing data locality as
    a principle but there are doubtless others that, now that you understand the problem,
    will make more sense as implementations. Let’s look at the two methods.
  prefs: []
  type: TYPE_NORMAL
- en: Hot/cold splitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first technique is very similar to how the type object pattern from [*Chapter
    9*](B18297_09.xhtml#_idTextAnchor130) needed to consider implicit and explicit
    data, but instead of thinking about what the values are defining, we look at how
    frequently they are accessed. The go-to example of this would be an NPC in a game
    with loot drops. The NPC’s health is accessed regularly as they heal and take
    damage over their life cycle, whereas the loot table, which describes what items
    they drop on, is accessed once at the end of the object’s life. We can classify
    the frequently accessed data as hot; this can stay in the object as member variables.
    The more single-use data, such as loot tables, is then marked as cold and separated
    off into a struct, held inside the object as a pointer.
  prefs: []
  type: TYPE_NORMAL
- en: Why do all this? It has to do with the size of the object when being pulled
    into the cache. When the object is pulled in, all data it directly contains makes
    up the amount of space it takes up in the cache. That means that all pointers
    effectively only take up the space of `uint64_t`. The data they point to is not
    necessarily loaded until it is directly accessed, as it is declared physically
    elsewhere, hence the pointer. Without separating our hot and cold data, as we
    described previously, our class takes up more cache memory than is necessary with
    data that is unlikely to be needed, increasing the chance of a cache miss.
  prefs: []
  type: TYPE_NORMAL
- en: Contiguous arrays
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second technique is using contiguous arrays of data. We know that there
    are two types of memory: stack and heap. Data locality is the main reason why
    stack memory is considered faster. Everything in the stack has been defined before
    the program runs and so it is neatly organized. Arrays of data are held in the
    stack and are defined together in one continuous line. This makes the CPU cache
    more efficient when looping over these elements as they have been stored physically
    closer to one another. This is part of the reason why data-oriented ECS is faster,
    as discussed back in [*Chapter 4*](B18297_04.xhtml#_idTextAnchor057). However,
    dynamic collections and pointers are declared in heap memory at runtime. We sacrifice
    that benefit of efficiency for the flexibility of defining data at a later point.
    Data on the heap uses whatever free space is available and because of this may
    end up defining multiple objects large distances from each other. *Figure 10**.2*
    shows visually how storing an array of values instead of an array of pointers
    can make a difference to what is loaded into the cache. This is a concept to keep
    in mind when we implement an object pool later in this chapter. When we spawn
    objects, they are held as `TObjectPtrs` in a `TArray`. Could this array be made
    into a standard C++ array? What dynamic property would we have to sacrifice to
    do this? It would likely be dynamic sizing, but do you need that in your context?'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Possible layout of an array of pointers versus an array of
    values in memory](img/Figure_10.02_B18297.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Possible layout of an array of pointers versus an array of values
    in memory
  prefs: []
  type: TYPE_NORMAL
- en: 'So, is the solution to just use arrays of data all the time? Well, no. There
    are many situations where pointers and dynamic collections are still necessary
    within object-oriented programming. This is more a point to consider their usage,
    and if you can replace a dynamic collection with a static array, then do so. This
    point is especially important to remember in our last pattern of the book coming
    up next: Object Pooling.'
  prefs: []
  type: TYPE_NORMAL
- en: Object pooling our resources to save time later
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last pattern we are discussing in this book is object pooling. This pattern
    aims to tackle one of the core problems with the CPU: allocating and deallocating
    memory is a slow process. Every time you spawn a new actor, the space it needs
    in memory must be reserved in the right subdivisions for each variable and handed
    back to the game process ready to receive data. Every time you delete an actor,
    that memory must be freed from all references and returned to the heap. For something
    such as a minigun spawning 3,000 projectiles per minute, that means a lot of allocation
    of the chunks for memory, which are all the same size. Object pooling is the practice
    of predicting this massive cost and offsetting it to a place where the lag it
    causes is not so noticeable. This for us means spawning all the projectiles we
    could possibly need at the start of the game and hiding them. When one is required,
    it is taken from the shelf of deactivated projectiles, teleported to the right
    position, and activated. Then, to preserve the pool’s integrity, when it would
    have been destroyed, it is simply deactivated and returned to the shelf with the
    other pooled items. Although this pattern does make the one frame when the level
    has just loaded much worse, as we are spawning all the projectiles at once, we
    can disguise this under a loading screen. This offset can dramatically increase
    the processing speed during hectic gameplay sequences when there would have been
    a lot of spawning and destroying occurring. Our pool then stays active until the
    end of the level when all objects are destroyed together, again under a loading
    screen.'
  prefs: []
  type: TYPE_NORMAL
- en: With the theory covered, let’s make some object pools.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing object pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a few ways we could look at implementing this and, realistically,
    if it works to spawn the objects you need in a place where you can access them,
    then it is the best method for you. The implementation options are as a world
    subsystem, level actor component, game mode component, or floating actor in the
    level.
  prefs: []
  type: TYPE_NORMAL
- en: World subsystem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Subsystems are Unreal’s effort at implementing a standardized form of the singleton
    pattern we covered in [*Chapter 8*](B18297_08.xhtml#_idTextAnchor113) with limited
    scope. This form it takes means that we can make an almost static class that we
    know will exist for the lifetime of whatever it is attached to. Subsystems are,
    however, not well protected with regard to access as anything with reference to
    their attached object can call functions on them. This is why they tend to be
    used for hidden logic behavior systems that run regardless of interaction. This
    results in most of the public functions on them being getters to get the state
    of something they are processing. There are five levels of subsystem that exist.
    Let’s describe them in order of decreasing lifetime:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Engine**: Exists in both the editor and the game for the length of the executable
    running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Editor**: Runs as an editor tool and will not build with the game.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UGameInstance` and so exists for the play session while the executable is
    running. Only one instance can exist at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ULocalPlayer` it is attached to and moves between levels in the same way.
    There is one instance per local player.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UWorld` it is attached to. There is one instance per `UWorld` that is currently
    loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to consider the scope of the system being created and match
    it to the parent that best describes its lifetime. For an object pooler, this
    would be a world subsystem as any objects spawned in a pool will exist within
    a world so when that world is unloaded, they will be too. If the system was made
    as a local player subsystem, this would break references when changing maps and
    possibly spawn items in menu worlds where they are unnecessary.
  prefs: []
  type: TYPE_NORMAL
- en: LevelScriptActor child
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ALevelScriptActor` is what people know on the blueprint side as the level
    blueprint. It provides a place for level-specific code to execute. This can be
    useful for tutorials where the mechanics are built badly in order to introduce
    them slowly, or for map-based mechanics, such as the “Levolutions” in Battlefield
    4, where each map has the ability to completely change if different conditions
    are met. What isn’t advertised very well in the Unreal documentation is that we
    can change the level blueprint in the C++ layer. Simply create a new C++ child
    of `ALevelScriptActor` and add your code here. This new child can be where we
    set up systems for object pooling as the `ALevelScriptActor` exists in a hidden
    state for as long as our world exists and has easy access to anything else within
    the world outliner for that particular map. The downside of this is that every
    new map created in the editor comes with a level blueprint that already inherits
    from the base `ALevelScriptActor` class. This means every new map would have to
    have its level blueprint manually reparented to your custom C++ type, which could
    lead to a lot of admin with easily missable steps.
  prefs: []
  type: TYPE_NORMAL
- en: Game mode component
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The game mode is a class that is guaranteed to be in every level, so there is
    the option to make the object pooler an actor component that is attached to it,
    or consolidate the behavior into the game mode inheritance hierarchy with a custom
    pooling game mode, which innately has the logic for running an object pool built
    in. This approach would require some diligence on the part of the designers as
    making a new level or prototyping a new game mode would require the component
    to be added or the correct parent to be selected; but it would make the implementation
    easy seeing as it is collected in one place and self-contained within an easily
    accessible system.
  prefs: []
  type: TYPE_NORMAL
- en: Floating actor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last method of getting an object pooler to work is the simplest but least
    elegant solution: making it into an actor that you spawn into the level. The benefit
    of this is you can easily have multiple object pools for different things or segregate
    your object pools by area if you are dealing using world partition, the system
    we discussed back in [*Chapter 3*](B18297_03.xhtml#_idTextAnchor046). The setup
    is also simple as all the GUI for setting it up is collected into the details
    panel for that object pool. The reason we call this method inelegant is down to
    how it must be managed. With there being no central method for referencing it
    or making sure the required functions have been called, it leaves a lot up to
    the end user and therefore is error prone.'
  prefs: []
  type: TYPE_NORMAL
- en: Making an object pool
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we start, the object pooling pattern is probably the most useful pattern
    to have in a plugin that we can take between projects. So, anything we make here
    should probably be done as part of a new plugin, which we can make within rider
    using the right-click menu on the game project. Simply select `ObjectPooler`.
    Then, just be sure to add new classes for the object pooler under the new folder
    this creates in the **Source** directory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Screenshot of the plugin creation process within Rider](img/Figure_10.03_B18297.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Screenshot of the plugin creation process within Rider
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first step is to make the struct that will define the attributes of a single
    type of pool. The code for this is presented next, but let’s explain some of the
    key points. First, the `BlueprintType` property in the `USTRUCT` block, in combination
    with the `EditAnywhere` property specifiers, will allow the end user to change
    the pool behavior in the editor. There is also the constructor, which must give
    every property a value as a struct cannot be a `nullptr` in memory. Saving the
    `_ActorName` variable as an `FString` is done to make debugging easier, but if
    you prefer to save it as an `FName`, that works and will save some processing
    when the pool is warming up:'
  prefs: []
  type: TYPE_NORMAL
- en: PooledObjectData.h struct
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll turn our attention to the component that will be on every object
    that came from the pool. We use an actor component instead of making a new child
    of `AActor` that must then be inherited from as it provides a clean separation
    between the object existing and doing what it needs to and the hook that attaches
    it back to the pool it came from. With this setup, we can dynamically spawn the
    component at runtime and attach it to the object when it joins the pool, keeping
    reference only to the component. This should make the pool totally class agnostic,
    improving its versatility.
  prefs: []
  type: TYPE_NORMAL
- en: 'Elements to note in the following class definition would be the custom initializer
    function allowing us to set the component up properly (more on that when we get
    to the object pool side) and the `BlueprintCallable` function used to recycle
    the actor. The recycle function is to be used instead of the standard `Destroy`
    on the actor as it will return its owning actor to the pool it came from. A useful
    extension you might want to add here would be to save the index of the pool it
    is supposed to return to. This will save some string comparisons later:'
  prefs: []
  type: TYPE_NORMAL
- en: PooledObject.h
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation of these functions is then very simple as most of the logic
    will be run in the pool itself. The only interesting point of note here is the
    `OnComponentDestroyed` override. This function removes the `RecycleSelf` function
    as a listener to a delegate on the pooler as a safety in case the pooler functionality
    is ignored, and the object is deleted in error:'
  prefs: []
  type: TYPE_NORMAL
- en: PooledObject.cpp
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now for the main event, the object pool itself. Breaking down the definition,
    we start with a new delegate type with no arguments. This exists as a tether for
    each of the objects taken from the pool. If we need to recall them due to a level
    change, we can broadcast this delegate to recycle all active objects. We then
    have the definition of a new struct type. This only exists as a workaround for
    the fact that the template collections inside Unreal do not cater to multi-dimensional
    arrays. We would like to store an array of pools that in themselves are arrays.
    So, to get around this limitation, we define a new struct type that will hold
    all the objects we consider to be a part of one pool:'
  prefs: []
  type: TYPE_NORMAL
- en: ObjectPool.h excerpt part 1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next is the object pooler class. An exception to the rule, we don’t mark this
    as abstract. The reason for this is this actor just needs to exist. There is no
    need for any visual elements and so it can exist entirely on the C++ side, calling
    back to our separation rules for establishing the fuzzy layer in [*Chapter 1*](B18297_01.xhtml#_idTextAnchor016).
    The API includes functions for broadcasting the cleanup delegate, getting an object
    from the pool, and two methods for returning an object to the pool with either
    a `UPooledObject` component reference or a straight `AActor` reference. We’ll
    go over why there are two later in the definitions. In the protected section,
    we need a `BeginPlay` override, an array of the data about the pools marked as
    `EditAnywhere` for designers to use the tool, and an array of the struct we made
    earlier to store a reference to every object this pool spawns. You could make
    this simpler by having a different object pool per object type, but that creates
    more actors than is necessary in the scene. Lastly, there is a private function
    for regenerating objects that have been deleted, leaving holes in the pool:'
  prefs: []
  type: TYPE_NORMAL
- en: ObjectPool.h excerpt part 2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With everything declared, we can move on to the definitions of our functions.
    To start, we have the `broadcast` function, which works as its name suggests,
    and the `BeginPlay` override for *warming up* the pool by spawning all the requested
    objects. Each pool iterates over the predefined number of times spawning new actors
    in the world. The code here names them and crucially adds an instance of the `UPooledObject`
    component to them. Having the pooler add this component dynamically means that
    the person who developed the actor being pooled didn’t need to know this was going
    to be added as a pooled class. This implementation uses `NewObject<>`, `RegisterComponent`,
    and `AddInstanceComponent` to create and add the component to the new actor as
    we are in runtime, and we would like to see the component in the actor details
    panel for debugging purposes. The new component needs its initialization function
    running before we hide it from view, disable its collision, and stop it from executing:'
  prefs: []
  type: TYPE_NORMAL
- en: ObjectPool.cpp excerpt part 1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The method for getting an object from the pool has been made with an FString
    argument to make it as foolproof as possible, but it is advised that you establish
    an enum type that can be used to reference the pools as indexes. In its current
    form, it goes through a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Finds the index of the pool that matches the input string, returning `out` if
    one isn’t found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Loops through the objects in the found pool to find the next object, which
    is marked as inactive:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a `nullptr` is found, then regenerate an object at that position and return
    it as it will be available.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the end of the list is reached, then check whether the pool is allowed to
    grow. If it can, then make and return the new item; otherwise, it would be sensible
    to output a warning so that the designers know the pool probably needs expanding.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code, the section for returning the object, if it is new or
    existing, is repeated due to the slightly different situations where a new object
    must have the component added and initialized but then does not need to be deactivated:'
  prefs: []
  type: TYPE_NORMAL
- en: ObjectPool.cpp excerpt part 2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The two recycling functions act as a way to do overloading with `UFUNCTION`s.
    Unreal does not support this standard C++ practice out of the box, and so we must
    define new functions for each as a workaround. In this case, the `RecycleActor`
    function tries to get a `UPooledObject` component reference from the input actor.
    It may be worth adding a summary comment above this function, with triple forward
    slashes, letting the user know that it may fail and a better method would be to
    use the `UPooledObject` version. If it succeeds, it then calls the `RecyclePooledObject`
    function with this new information. Otherwise, it currently does nothing, but
    this may be a good place to log out the situation as a warning and maybe have
    the function return a Boolean value on successful recycling as feedback on the
    action. The main recycling function simply returns the object to its initial disabled
    and hidden state in the pool, resetting the `_IsActive` flag in the pooled component:'
  prefs: []
  type: TYPE_NORMAL
- en: ObjectPool.cpp excerpt part 3
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The last function rounding out our object pooler is a function for regenerating
    items. This could maybe be separated better to make it more useful, in the `GetPooledActor`
    function, but as it stands, this follows the standard object generation as in
    the `BeginPlay` method, just with a twist. It uses indexes to add an object to
    a specific place in the pooled array. There is a lot of room for improvement with
    this function to make it more versatile, but that is left to your implementation’s
    needs:'
  prefs: []
  type: TYPE_NORMAL
- en: ObjectPool.cpp excerpt part 4
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As stated a few times previously, this object pooler will do the job, but it
    is very basic in its utility. There are many extensions that you could, and probably
    should, consider, such as having pool groups so that objects are pooled based
    on the requested groups from the level or making it into a world subsystem that
    is universal to that world, allowing easy setup via the P**roject Settings** panel.
    However, the principle use of it stays the same: to offset the cost of spawning
    to the start of a level, where it can be hidden under a loading screen.'
  prefs: []
  type: TYPE_NORMAL
- en: Using what we have created in its current form is quite simple. Simply drag
    an instance of the object pooler into your world from the Project panel and set
    up its data variable in the details panel. Once the game starts, it will spawn
    all the required objects in and hide them. To get an object, all you need to do
    is obtain a reference to the pooler somehow and call the `GetPooledObject` function,
    as shown in *Figure 10**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Screenshot of the blueprint usage of the object pooler](img/Figure_10.04_B18297.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Screenshot of the blueprint usage of the object pooler
  prefs: []
  type: TYPE_NORMAL
- en: With that, we are at the end of our journey through object pooling. If you have
    been following along, you will not only have an object pooler that you can migrate
    between projects but also an idea of how you can remake and improve it to suit
    specific needs as and when required. Not only this, but the end of this section
    also brings us to the end of the chapter and the book. Even though this wasn’t
    specifically designed as a book to be read in order, from cover to cover, if you
    have been following this journey from the beginning, then you have a good set
    of practical skills and templates for how to improve your code in numerous ways.
    There will be some more parting words of wisdom after this, but let’s round this
    chapter content out by saying that it is more important to get something working
    than to build it exactly correct from the start, which is why the term refactoring
    exists.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last chapter, we have covered three patterns that will boost the efficiency
    of your game, if implemented correctly. In game development, optimization should
    not be something you consider until it becomes a problem. It is far more important
    that you get something working first. Data locality should probably be considered
    as a first measure as it requires the least refactoring of code. Likewise, an
    object pool is something we would always recommend you have in your project, via
    a plugin, on standby for when you start to spawn a lot of the same object. The
    dirty flag pattern is much more situational, though, and is only applicable when
    an object has lots of edits versus few read actions per second. Armed with these
    tools, you should be able to make a dent in the frame rate, destroying the spaghetti
    mess that all projects become before release. There are always more ways to optimize
    code beyond this too – some not quite so obvious – but the key is to remember
    that all data and all instructions are stored somewhere and actions using them
    require them to be moved, which takes time.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the finished project with all the elements from this book completed
    on GitHub in the same place as the other chapters in the `Complete` branch. Feel
    free to create a fork from here and make your own improvements to each of these
    patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Game-Development-Patterns-with-Unreal-Engine-5/tree/main/Complete](https://github.com/PacktPublishing/Game-Development-Patterns-with-Unreal-Engine-5/tree/main/Complete)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A final rule: Good code doesn’t make a game good, but it does make your team
    better.'
  prefs: []
  type: TYPE_NORMAL
