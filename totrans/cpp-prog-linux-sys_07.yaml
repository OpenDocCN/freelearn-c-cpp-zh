- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Proceeding with Inter-Process Communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapter presented many features of C++20 that allow you to execute
    tasks in parallel. Outside of the global variables, it didn’t cover ways to communicate
    between processes or threads. On a system level, most of the asynchronous calls
    are born in the continuous communication between processes and different computer
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the **inter-process communication** (**IPC**)
    interfaces that Linux provides. Through them, you will get a full picture of possibilities
    to cover your system and software requirements. You’ll start by learning about
    **message queues** (**MQs**) as a continuation of the discussion about pipes in
    [*Chapter 3*](B20833_03.xhtml#_idTextAnchor047). In addition, we will analyze
    in detail the work of the **semaphore** and **mutex** synchronization techniques.
    We will introduce you to some new C++20 features in this area that are easy to
    use, and you will no longer have to implement such yourself.
  prefs: []
  type: TYPE_NORMAL
- en: This allows us to proceed with the **shared memory** technique, which will give
    you the option to transfer large amounts of data fast. Finally, if you’re interested
    in communication between computer systems on the network, you’ll learn about sockets
    and network communication protocols. With this, we give you some practical and
    commands to administer your own system on the network.
  prefs: []
  type: TYPE_NORMAL
- en: We will build on the discussions started in this chapter in [*Chapter 9*](B20833_09.xhtml#_idTextAnchor129).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing MQs and the pub/sub mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guaranteeing atomic operations through semaphores and mutual exclusions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using shared memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communicating through the network with sockets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the code examples, you must prepare the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A Linux-based system capable of compiling and executing C++20 (for example,
    **Linux** **Mint 21**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GCC 12.2 compiler ([https://gcc.gnu.org/git/gcc.git gcc-source](https://gcc.gnu.org/git/gcc.gitgcc-source))
    with the `-std=c++2a`, `-lpthread`, and `-``lrt` flags
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all the examples, you can alternatively use [https://godbolt.org/](https://godbolt.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All code examples in this chapter are available for download from [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%207](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%207)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing MQs and the pub/sub mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re glad to be back on the IPC topic. The last time we discussed it was in
    [*Chapter 3*](B20833_03.xhtml#_idTextAnchor047), where we explained pipes and
    used some code examples. You learned about the basic mechanism of exchanging data
    between processes, but as you remember, there are some blocking points. As with
    any programming instrument, pipes have particular usage – they are fast, and they
    can help you send and receive data from both related (forked) processes (through
    **anonymous pipes**) and unrelated processes (through **named pipes**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In a similar fashion, we could use MQs to transfer data, which are available
    to related and unrelated processes, too. They provide the ability to send a single
    message to multiple receiving processes. But as you saw, pipes are primitive in
    the sense of sending and receiving binary data as is, while MQs bring the notion
    of a *message* to the table. The policy of the transfer is still configured in
    the calling process – queue name, size, signal handling, priority, and so on –
    but its policy and ability to serialize data are now in the hands of the MQ’s
    implementation. This gives the programmer a relatively simple and flexible way
    to prepare and handle messages of data. Based on our software design, we could
    easily implement an asynchronous send-receive data transfer or a **publish/subscribe**
    (**pub/sub**) mechanism. Linux provides two different interfaces for MQs – one
    designed for local server applications (coming from System V) and one designed
    for real-time applications (coming from POSIX). For the purposes of the book,
    we prefer to use the POSIX interface as it is richer and cleaner in configuration.
    It is also a file-based mechanism, as discussed in [*Chapter 1*](B20833_01.xhtml#_idTextAnchor014),
    and you can find a mounted queue through the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This interface is available through the OS real-time functions library, `librt`,
    so you need to link it during compilation. The MQ itself can be visualized as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Representation of IPC through the MQ](img/Figure_7.1_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Representation of IPC through the MQ
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example where we send data from one process to another. The
    exemplary data is already stored in a file and loaded to be sent through the MQ.
    The full example can be found at [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%207](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%207):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We set our initial configuration together with the queue name as the pathname:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Additional configuration is applied to the MQ and the receiving end is prepared.
    The `mq_open()`function is called in order to create the MQ on the filesystem
    and open its reading end. Through an endless loop, the data is received as it
    is read from a binary file and printed out (markers `{2}` and `{3}` in the preceding
    code) until the file is fully consumed. Then, the receiving ends and the reading
    end are closed (marker `{4}` in the following code). If there’s nothing else to
    be done, the MQ is deleted from the filesystem through `mq_unlink()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This example is implemented with two threads but could be done in the same fashion
    with two processes. The MQ functionality will remain the same. We call `mq_open()`
    again and open the MQ for writing (marker `{5}` in the following code). The created
    queue can fit up to 10 messages and each message can be 1,024 bytes in size –
    this is defined through the MQ attributes in the earlier code snippet. If you
    don’t want the MQ operations to be blocking, you could use the `O_NONBLOCK` flag
    in the attributes, or use `mq_notify()` prior to the `mq_receive()` call. That
    way, if the MQ is empty, the reader will be blocked, but `mq_notify()` will trigger
    a signal on message arrival and the process will be resumed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the locally stored file is opened with the test data and we read from
    it (markers `{6}` and `{7}` in the following code). While we read (you could use
    `std::ofstream` as well), we send its contents through the MQ (marker `{8}` in
    the following code). The message has the lowest priority possible, which means
    `0`. In a system with more messages in a queue, we could set a higher priority
    and they will be handled in a decreasing order. The maximum value is visible from
    `sysconf(_SC_MQ_PRIO_MAX)`, where, for Linux, this is `32768`, but POSIX enforces
    a range from 0 to 31 in order to be compliant with other OSs as well. Let’s check
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we send a zero-sized message to indicate the end of the communication
    (marker `{9}`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the following (the printed data from the file is reduced for
    readability):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is a very simple example considering we have only two workers – `readFromQueue()`
    and `writeToQueue()`. The MQs allow us to scale up and execute a many-to-many
    communication. This approach could be found on many embedded systems, as it’s
    also real-time compliant and doesn’t expect any synchronization primitives to
    be used. Many microservice architectures and serverless applications rely on it.
    In the next section, we are going to discuss one of the most popular patterns,
    based on MQs.
  prefs: []
  type: TYPE_NORMAL
- en: The pub/sub mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve probably figured out that one MQ could become a bottleneck while scaling
    up. As you observed in the previous example, there’s the message count and size
    limitation. Another issue is the fact that after a message is consumed, it is
    removed from the queue – there can be only one consumer of a given message at
    a time. The data provider (the producer) has to manage the correct message address
    as well, meaning adding extra data to help the consumers identify to whom the
    message is sent, and each consumer has to follow that policy.
  prefs: []
  type: TYPE_NORMAL
- en: A preferred approach is to create a separate MQ for each consumer. The producer
    will be aware of those MQs a priori, either at compile time (all MQs are listed
    in the data segment by the system programmer) or runtime (each consumer will send
    its MQ pathname at startup and the producer will handle this information). That
    way, the consumers are *subscribing* to receive data from a given producer, and
    the producer *publishes* its data to all MQs it’s aware of. Therefore, we call
    this a **publish-subscribe** mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the exact implementations might vary, depending on the software
    design, but the idea will remain the same. In addition, there could be multiple
    producers sending data to multiple consumers, and we say this is a **many-to-many**
    realization. Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Representation of the MQ realization of the pub/sub mechanism](img/Figure_7.2_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Representation of the MQ realization of the pub/sub mechanism
  prefs: []
  type: TYPE_NORMAL
- en: As we proceed toward the decoupling of processes, we make our system more flexible.
    It becomes easier to scale as the subscribers don’t lose computational time identifying
    whether the messages are directed to them or not. It is also easy to add a new
    producer or consumer without disturbing others. The MQ is implemented on an OS
    level, thus we could take it as a robust IPC mechanism. One possible disadvantage,
    though, is the fact that producers usually don’t receive any health information
    from the subscribers. This leads to MQs being full of unconsumed data and the
    producers being blocked. Thus, additional implementation frameworks are implemented
    on a more abstract level, which takes care of such use cases. We encourage you
    to additionally research the **Observer** and **Message Broker** design patterns.
    In-house-developed pub/sub mechanisms are usually built on top of them and not
    always through MQs. Nonetheless, as you have probably guessed, sending large amounts
    of data is going to be a slow operation through such mechanisms. So, we need an
    instrument to get a big portion of data fast. Unfortunately, this requires additional
    synchronization management to avoid data races, similar to [*Chapter 6*](B20833_06.xhtml#_idTextAnchor086).
    The next section is about the synchronization primitives.
  prefs: []
  type: TYPE_NORMAL
- en: Guaranteeing atomic operations through semaphores and mutual exclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s try to *zoom in* on a shared resource and see what happens in the CPU.
    We will provide a simple and effective way to explain where exactly the data races
    start from. They were already thoroughly discussed in [*Chapter 6*](B20833_06.xhtml#_idTextAnchor086).
    Everything we learn here should be considered as an addition, in a sense, but
    the analysis methodology of concurrent and parallel processing remains the same
    as earlier. But now, we focus on concrete low-level problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look closely at the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'It is a very simple piece of code in which a variable is incremented and printed
    out. According to C++ standards, such a modification is an undefined behavior
    in multithreaded environments. Let’s see how – instead of going through the process’s
    memory layout here, we will analyze its pseudo-assembly code side by side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Suppose this increment procedure is in a thread function and there’s more than
    one thread executing it. The `add 1` instruction is done on the loaded value,
    and not on the actual memory location of `shrd_res`. The preceding code snippet
    will be executed multiple times, and most probably in parallel. If we note that
    the thread is a set of instructions, the intuition would be that the instructions
    are executed in a monolithic manner. In other words, each thread routine should
    be run without interruption, which is usually the case. However, there is a small
    particularity that we should keep in mind – the CPU is engineered to keep a small
    latency. It is not built for data parallelism. Therefore, figuratively speaking,
    its main goal is to load itself with a large number of small tasks. Each of our
    threads is executed in a separate processor; this could be a separate CPU, a CPU
    thread, or a CPU core – it really depends on the system. If the number of processors
    (CPUs, cores, or threads) is smaller than *N*, then the remaining threads are
    expected to queue themselves and wait until a processor is freed up.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the initial threads’ instructions are already loaded there and executed
    as they are. Even when the CPU cores are architecturally the same, their goal
    is to be executed as fast as possible. This means that it is not expected for
    them to be equal in speed because of multiple hardware fluctuations. But `shared_resource`
    is a variable that is, well... a shared resource. This means that whoever gets
    to increment it first will do it and others will follow. Even if we don’t care
    about the `std::cout` result (for example, the printing order stops being sequential),
    we still have something to worry about. And you’ve probably guessed it! We don’t
    know which value we are actually going to increment – is it going to be the last
    stored value of `shared_resource` or the newly incremented one? How could this
    happen?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Did you follow what just happened? `Thread 1`’s sequence of instructions was
    disrupted, because of the execution of `Thread 2`. Now, can we predict what’s
    going to be printed? This is known as an `Thread 2` was never executed, as the
    last value to be stored in `shared_resource` will be the one incremented in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In other words, we lost one increment. There was nothing instructing the CPU
    that both procedures have to be called separately and continuously executed. It
    should be clear that a finite number of instruction combinations are possible,
    all of them leading to unexpected behavior, because it depends on the hardware’s
    state. Such an operation is called **non-atomic**. In order to handle parallelism
    correctly, we need to rely on **atomic** operations! It is the job of the software
    developer to consider this and inform the CPU about such sets of instructions.
    Mechanisms such as mutexes and semaphores are used to manage *atomic* scopes.
    We are going to analyze their roles thoroughly in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Semaphore
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you make a questionnaire asking people in multiple professions what a **semaphore**
    is, you will get different answers. A person from the airport will tell you that
    this is a system for signaling someone through the use of flags. A police officer
    might tell you that this is just a traffic light. Asking a train driver will probably
    give you a similar response. Interestingly, this is where *our* semaphores come
    from. Overall, these answers should hint to you that this is a *signaling* mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Programming semaphores were invented by Edsger Dijkstra and are mainly used
    to prevent race conditions. They help us signal when a resource is available or
    not and count how many shared resource units of a given kind are available.
  prefs: []
  type: TYPE_NORMAL
- en: Like the previously mentioned signaling mechanisms, semaphores don’t guarantee
    error-free code, as they do not prevent processes or threads from acquiring a
    resource unit – they just inform. In the same way that a train might ignore the
    signal and proceed to an occupied train track or a car could proceed at a busy
    crossroad, this might be catastrophic! Again, it is the software engineer’s task
    to figure out how to use semaphores for the system’s good health. Therefore, let’s
    get to using them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dijkstra provided us with two main functions surrounding a critical section:
    `P(S)` and `V(S)`. As you probably know, he was Dutch, so these functions’ names
    come from the Dutch words for *try* and *increase* (*probeer* and *vrhoog*, respectively),
    where `S` is the semaphore variable. Just by their names, you already get an idea
    about what they are going to do. Let’s look at them in pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'So, `P(S)` will endlessly check whether the semaphore has signaled that the
    resource is available – the semaphore is incremented. As soon as `S` is incremented,
    the loop is stopped, and the semaphore value is decreased for some other code
    to be executed. Based on the increment’s value, we recognize two types of semaphores:
    **binary** and **counting**. The binary semaphore is often mistaken for a **mutual
    exclusion** (**mutex**) mechanism. The logic is the same – for example, whether
    the resource is free to be accessed and modified or not – but the nature of the
    technique is different, and as we explained earlier, nothing is stopping some
    bad concurrent design from ignoring a semaphore. We will get to that in a minute,
    but for now, let’s pay attention to what the semaphore does. Before we begin with
    the code, let’s put a disclaimer that there are a few semaphore interfaces on
    Unix-like OSs. The choice of usage depends on the level of abstraction and the
    standards. For example, not every system has POSIX, or it is not exposed fully.
    As we are going to focus on the C++20 usage, we will use the next examples just
    for reference. The full source code of the next examples can be found at [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%207](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%207).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at two common semaphore interfaces on Linux. The first one
    is the **unnamed semaphore** – we can present it through the following interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `sem` variable is the semaphore, which is initialized and de-initialized
    by `sem_init()` and `sem_destroy()`, respectively. The `P(S)` function is represented
    by `sem_wait()` and the `V(S)` function by `sem_post()`. There are also `sem_trywait()`,
    if you want to report an error when the decrement doesn’t happen immediately,
    and `sem_timedwait()`, which is a blocking call for a time window in which the
    decrement could happen. This seems pretty clear, except for the initialization
    part. You’ve probably noticed the `value` and `pshared` arguments. The first one
    shows the initial value of the semaphore. For example, a binary semaphore could
    be `0` or `1`. The second is more interesting.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you might recall, in [*Chapter 2*](B20833_02.xhtml#_idTextAnchor029) we
    discussed memory segments. Imagine that we create the semaphore on the `pshared`
    is used exactly for this purpose. If it’s set to `0`, then the semaphore is local
    for the process, but if it is set to a non-zero value, then it is shared between
    processes. The catch is to create the semaphore on a globally visible region of
    memory, such as shmem, including the filesystem as a shared resource pool. Here
    is an overview of **named semaphores**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `/dev/shm`. We treat it as a file. For example, the following code will
    create a semaphore with the name `/sem` and `0644` permissions – it will be readable
    and writable only by its owner, but only readable by others, and it will be visible
    on the filesystem until it is later removed through code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `P(S)` and `V(S)` calls remain the same. After we finish, we must close
    the file, and remove it, if we don’t need it anymore:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As mentioned in [*Chapter 1*](B20833_01.xhtml#_idTextAnchor014), you see that
    the POSIX calls follow the same pattern through the `<object>_open`, `<object>_close`,
    `<object>_unlink`, and `<object>_<specific function>` suffixes. This makes their
    usage common for every POSIX object, as you probably already observed earlier
    in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A quick remark is that there are **lower-level semaphores** where the system
    calls are strongly related to the OS types or are based on direct OS signal manipulations.
    Such approaches are complex to implement and maintain because they are specific
    and considered fine-tuning. Feel free to research more about your own system.
  prefs: []
  type: TYPE_NORMAL
- en: A C++ semaphores primer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With this in mind, we’d like to continue leveling up the abstraction, and so
    we’ll discuss the C++ semaphore objects. This is a new feature in C++20 and it’s
    useful when you want to make the code more system-generic. Let’s check it out
    through the `atomic<uint16_t> shared_resource`. As mentioned at the beginning
    of this section, the semaphores help in task synchronization, but we need a data
    race guard. The `atomic` type is making sure we follow the C++ memory model and
    the compiler will keep the sequence of CPU instructions as per `std::memory_oder`.
    You can revisit [*Chapter 6*](B20833_06.xhtml#_idTextAnchor086) for a data race
    explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will continue by creating two global `binary_semaphore` objects in order
    to synchronize the access appropriately (like a ping-pong). The `binary_semaphore`
    object is an alias of the `counting_semaphore` object with a maximum value of
    `1`. We will need a program-ending rule so we will define a limit of iterations.
    We will ask the compiler to make it a constant, if possible, through the `constexpr`
    keyword. Last, but not least, we will create two threads that will act as a producer
    (incrementing the shared resource) and a consumer (decrementing it). Let’s look
    at the code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The semaphores are constructed and initialized. We proceed with the threads.
    The `release()` function increments an internal counter, which signals the others
    (marker `{2}` in the following code, similar to `sem_post()`). We use `osyncstream(cout)`
    to build a non-interleaved output. Here’s the producer thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'And here’s the consumer thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As we do this iteratively, we see this output multiple times, depending on
    `limit`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Going back to the code’s logic, we must emphasize that the C++ semaphores are
    considered lightweight and allow multiple concurrent accesses to the shared resource.
    But be careful: the provided code uses `acquire()` (marker `{1}`, similar to `sem_wait()`),
    which is a blocking call – for example, your task will be blocked until the semaphore
    is released. You could use `try_acquire()` for non-blocking purposes. We rely
    on both semaphores to create a predictable sequence of operations. We start the
    process (for example, the main thread) by releasing the producer semaphore, so
    the producer would be signaled to start first.'
  prefs: []
  type: TYPE_NORMAL
- en: The code could be changed to use POSIX semaphores, just by removing the C++
    primitives and adding the aforementioned system calls to the same places in the
    code. In addition, we encourage you to achieve the same effect with one semaphore.
    Think about using a helper variable or a condition variable. Keep in mind that
    such an action makes the synchronization heterogenous and on a large scale, which
    is hard to manage.
  prefs: []
  type: TYPE_NORMAL
- en: The current code is obviously not able to synchronize multiple processes, unlike
    the **named semaphore**, so it’s not really an alternative there. We also could
    want to be stricter on the shared resource access – for example, to have a single
    moment of access in a concurrent environment. Then, we’d need the help of the
    mutex, as described in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Mutual exclusion (mutex)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mutex is a mechanism that comes from the operations of the OS. A shared
    resource is also known as a **critical section** and it needs to be accessed without
    the risk of race conditions. A mechanism that allows only a single task to modify
    the critical section at a given moment, excluding every other task’s request to
    do the same, is called a **mutual exclusion** or a **mutex**. The mutexes are
    implemented internally by the OS and remain hidden from the user space. They provide
    a *lock-unlock* access functionality and are considered stricter than the semaphores,
    although they are controlled as binary semaphores.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The calling thread locks the resource and is obliged to unlock it. There’s no
    guarantee that a higher entity in the system’s hierarchy would be able to override
    the lock and unblock the parallel functionality. It is advisable for each lock
    to be released as fast as possible to allow the system threads to scale up and
    save idle time.
  prefs: []
  type: TYPE_NORMAL
- en: 'A POSIX mutex is created and used in much the same way as the unnamed semaphore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The pattern of the function names is followed again, so let’s focus on `pthread_mutex_lock()`
    and `pthread_mutex_unlock()`. We use them to lock and unlock a critical section
    for manipulation, but they cannot help us in the sequence of events. Locking the
    resource only guarantees there are no race conditions. The correct sequencing
    of events, if required, is designed by the system programmer. Bad sequencing might
    lead to **deadlocks** and **livelocks**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deadlock**: One or more threads are blocked and cannot change their state
    because they are waiting for an event that never occurs. A common bug is two (or
    more) threads being looped together – for example, one is waiting for a shared
    resource A while holding a lock on shared resource B, and a second thread holds
    a lock on A but will unlock it when B is unlocked. Both will remain blocked because
    neither will be the first to *give up on the resource*. Such a behavior could
    be caused even without mutexes. Another bug is to lock a mutex twice, which, in
    the case of Linux, is detectable by the OS. There are deadlock resolution algorithms,
    where locking a number of mutexes will not succeed at first because of the deadlock,
    but will be successful with a guarantee after a finite number of attempts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Livelock**: The threads are not blocked, but then again, they cannot change
    their state because they require the shared resource to continue forward. A good
    real-world example is two people meeting face to face at an entrance. Both will
    move aside out of politeness, but they will most probably move in the same direction
    as their counterpart. If that happens and they continue to do that all the time,
    then nobody will be blocked, but at the same time, they cannot proceed forward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both classes of bugs are common and could be reproduced with semaphores, as
    they are blocking too, and rarely happen on small-scale systems, where they are
    easy to debug. It is trivial to follow the code’s logic with just a few threads,
    and the processes are manageable. Large-scale systems with thousands of threads
    execute an enormous number of locks at the same time. The bug reproductions are
    usually a matter of bad timing and ambiguous task sequences. Therefore, they are
    hard to catch and debug, and we advise you to be careful when you lock a critical
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'C++ provides a flexible lock interface. It is constantly upgraded and we now
    have several behaviors to choose from. Let’s do a parallel increment of a variable.
    We use the `increment()` thread procedure for the sake of clarity, similar to
    the previous code, but we replace the semaphores with one mutex. And you’ve probably
    guessed that the code will be guarded against race conditions, but the sequence
    of the thread executions is undefined. We could arrange this sequence through
    an additional flag, condition variable, or just a simple sleep, but let’s keep
    it this way for the experiment. The updated code snippet is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We defined our shared resource and the mutex. Let’s see how the increment happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The observed output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: It’s obvious that incrementing the variable without multithreading will be much
    faster than this result. You could even try running it until `UINT_MAX`.
  prefs: []
  type: TYPE_NORMAL
- en: So, the preceding code creates a globally visible mutex and uses a `unique_lock`
    object (marker `{1}`) to wrap it. It is similar to `pthread_mutex_init()` – it
    allows us to defer locking, do a recursive lock, transfer lock ownership, and
    carry out attempts to unlock it within certain time constraints. The lock is in
    effect for the scope block it is in – in the current example, it is the thread
    procedures’ scope. The lock takes ownership of the mutex. When it reaches the
    end of the scope, the lock is destroyed and the mutex is released. You should
    already know this approach as `scoped_lock` object to lock multiple mutexes while
    avoiding a deadlock by its design.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is something else you should consider when using a mutex. The mutex reaches
    the kernel level. The task states are affected by it directly and multiple locks
    will cause multiple **context switches**. As you recall from earlier, we will
    probably lose time in rescheduling. This means that the OS needs to jump from
    one memory region in RAM to another just to load another task’s instructions.
    You must consider what’s beneficial for you: many locks with small scopes leading
    to many switches, or a few locks with bigger scope blocks holding resources for
    longer timespans.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the day, our goal was just to instruct the CPU about an atomic
    region. If you remember, we used an `atomic` template in the semaphore example.
    We could update our code with an `atomic` variable and remove the mutex with the
    lock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there is a significant time improvement just by the removal
    of the mutex. For the sake of argument, you could add the semaphores back and
    you will still observe a faster execution than the mutex. We advise you to look
    at the code’s disassembly for the three cases – just with the `atomic` variable,
    with the mutex, and with the semaphore. You will observe that an `atomic` object
    is very simple instruction-wise and is executed at a user level. As it is truly
    atomic, the CPU (or its core) will be kept busy during the increment. Bear in
    mind that any technique for resolving data races will inherently carry a performance
    cost. The best performance can be achieved by minimizing the places and their
    scope where synchronization primitives are needed.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: C++20 provides exciting features for concurrent execution, such as **jthread**,
    **coroutines**, **updated atomic types**, and **cooperative cancelation**. Except
    for the first one, we will look at the others later in the book. In addition to
    these, Linux has system calls for using the IPC entities, which are built for
    the purposes of multiprocessing data exchange. That said, we advise you to think
    about using an already existing mechanism for asynchronous work before you attempt
    combinations of mutexes, semaphores, flags, and conditional variables. All those
    C++ and Linux features are designed to scale up in a stable manner and save you
    time for solution design.
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything we did until now is just to make sure we have atomic access to a
    critical section. Atomics, mutexes, and semaphores will give you this – a way
    to instruct the CPU about the scope of instructions. But two questions remain:
    Could we do it faster and lighter? Does being atomic mean we keep the order of
    the instructions? The answer to the first question is *Probably*. To the second
    one, the answer is *No*! Now we have the incentive to move and dive into the C++
    **memory model** and **memory order**. If this interests you, we invite you to
    jump to [*Chapter 9*](B20833_09.xhtml#_idTextAnchor129), where we discuss more
    interesting concurrent tasks. Now, we will continue the topic of shared resources
    through the **shmem** **IPC** mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: Using shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with pipes, the MQ data is lost once consumed. Duplex message data copying
    increases user space-kernel space calls, therefore an overhead is to be expected.
    The **shmem** mechanism is fast. As you learned in the previous chapter and the
    previous section, the synchronization of the data access is an issue that must
    be resolved by the system programmer, especially when it comes to race conditions.
  prefs: []
  type: TYPE_NORMAL
- en: An important remark is that the term *shared memory* is vague in itself. Is
    it a global variable that two threads could access simultaneously? Or is it a
    shared region of RAM, which multiple CPU cores use as a common ground to transfer
    data between each other? Is it a file in the filesystem that many processes modify?
    Great questions – thanks for asking! In general, all of those are kinds of shared
    resources, but when we speak about the term *memory*, we should really think about
    a region in the **main memory** that is visible to many processes and where multiple
    tasks could use it to exchange and modify data. Not only tasks but also different
    processor cores and core complexes (such as ARM) if they have access to the same
    predefined memory region. Such techniques require a specific configuration file
    – a memory map, which strictly depends on the processor and is implementation-specific.
    It provides the opportunity to use, for example, **tightly coupled memory** (**TCM**)
    to speed up, even more, the frequently used portions of code and data, or to use
    a portion of the RAM as shmem for data exchange between the cores. As this is
    too dependent on the processor, we are not going to continue discussing it. Instead,
    we will move on to discuss Linux’s **shmem** **IPC** mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The processes allocate a portion of their **virtual memory** as a shared segment.
    Traditionally, the OS forbids processes to access each other’s memory regions,
    but the shmem is a mechanism for the processes to ask for the removal of this
    restriction in the boundaries of the shmem. We use it to ingest and modify large
    portions of data quickly through simple read and write operations, or the already
    provided functions in POSIX. Such functionality is not possible through MQs or
    pipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to MQs, there’s no serialization or synchronization here. The system
    programmer is responsible for managing the IPC’s data transfer policy (again).
    But with the shared region being in the RAM, we have fewer context switches, thus
    we reduce the overhead. We can visualize it through the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Shmem presentation through the process’s memory segments](img/Figure_7.3_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Shmem presentation through the process’s memory segments
  prefs: []
  type: TYPE_NORMAL
- en: The shmem region is usually depicted between the two processes’ address spaces.
    The idea is to emphasize how that space is truly shared between the processes.
    In reality, this is implementation-specific and we leave it to the kernel – what
    we care about is the map to the shmem segments itself. It allows both processes
    to observe the same contents simultaneously. Let’s get to it then.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about mmap() and shm_open()
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initial system call for the creation of a shmem mapping is `shmget()`. This
    is applicable to any Unix-based OS, but for POSIX-compliant systems, there are
    more comfortable approaches. If we imagine that we do a mapping between a process’s
    address space and a file, then the `mmap()` function will pretty much get the
    job done. It is POSIX-compliant and executes the read operation on demand. You
    can simply use `mmap()` to point to a regular file, but the data will remain there
    after the processes have finished their work. Do you remember the pipes from [*Chapter
    3*](B20833_03.xhtml#_idTextAnchor047)? It’s a similar case here. There are `mmap()`
    system call with `fork()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have independent processes, then the only way for them to know how to
    address the shared region is through its pathname. The `shm_open()` function will
    provide you a file with a name, in the same way that `mq_open()` did – you could
    observe it in `/dev/shm`. It would require `librt` as well. Knowing this, you
    intuitively get that we limit the I/O overhead and the context switches because
    of the filesystem operations, as this file is in the RAM. Last but not least,
    this kind of shared memory is flexible in size and could be enlarged to gigabytes
    in size when needed. Its limitations are dependent on the system. The full version
    of the following example can be found at https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%207:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This example is very specific as we intentionally used processes instead of
    threads. This allows us to demonstrate the usage of `shm_open()` (marker `{1})`
    as the different processes use the shmem’s pathname (which is known at compile
    time) to access it. Let’s continue with reading the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We could use mutexes, but currently, we only need one process to signal to
    the other that its work is done, so we apply semaphores (markers `{3}` and `{7}`
    in the previous code block) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'To make the memory region shared, we use the `mmap()` function with the `MAP_SHARED`
    option, and we mark the reader and the writer credentials accordingly through
    the following page settings: `PROT_READ` and `PROT_WRITE` (markers `{2}` and `{6}`).
    We also use the `ftruncate()` function to set the region’s size (marker `{5}`).
    In the given example, the information is written in the shmem, and someone has
    to read it. It’s a kind of a single-shot producer-consumer because after the writing
    is done, the writer gives the reader time (marker `{8}`), and then the shmem is
    set to zero (marker `{9}`) and deleted (marker `{10}`). Now, let’s proceed with
    the parent’s code - the producer of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, the shmem region is mapped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As done previously, we use the `sem_open()` named semaphore (marker `{11}`)
    to allow both processes to synchronize. We wouldn’t be able to do so through the
    semaphores we discussed earlier in the chapter, as they don’t have a name and
    are known only in the context of a single process. At the end, we remove the semaphore
    from the filesystem as well (marker `{12}`), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The program’s result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Shmem is an interesting topic, which we will return to in [*Chapter 9*](B20833_09.xhtml#_idTextAnchor129).
    One reason for being so is that C++ allows us to wrap the POSIX code appropriately
    and make the code safer. Similar to [*Chapter 3*](B20833_03.xhtml#_idTextAnchor047),
    mixing system calls with C++ code should be well thought out. But it’s worthwhile
    to visit the `memory_order` use cases as well. If **jthreads** or **coroutines**
    are not applicable to your use cases, then the currently discussed synchronization
    mechanisms, together with the **smart pointers**, give you the flexibility to
    design the best possible solution for your system. But before we get there, we
    need to talk about something else first. Let’s proceed to the communication between
    computer systems.
  prefs: []
  type: TYPE_NORMAL
- en: Communicating through the network with sockets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the pipes, MQs, and the shmem could together overcome their problems, then
    why do we need sockets? This is a great question with a simple answer – we need
    them to communicate between different systems on the network. With this, we have
    our full set of instruments to exchange data. Before we understand sockets, we
    need to get a quick overview of network communication. No matter the network type
    or its medium, we must follow the design established by the **Open Systems Interconnection**
    (**OSI**) **basic reference model**. Nowadays, almost all OSs support the **Internet
    Protocol** (**IP**) family. The easiest way to set up communications with other
    computer systems is by using these protocols. They follow layering, as described
    in the **ISO-OSI** model, and now we are going to take a quick look at that.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the OSI model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The OSI model is typically represented as shown in the next table. System programmers
    usually require it to analyze where their communication is disturbed. Although
    sockets are intended to execute the network data transfer, they are also applicable
    for a local IPC. One reason is that the communication layers, especially on large
    systems, are separate utilities or abstraction layers over the applications. As
    we want to make them environmentally agnostic, meaning we don’t care whether the
    data is transferred locally or over the internet, then the sockets fit perfectly.
    That said, we must be aware of the channel we use and where our data is transported.
    Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – The OSI model represented as a table](img/Figure_7.4_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – The OSI model represented as a table
  prefs: []
  type: TYPE_NORMAL
- en: Global network communication, especially the internet, is a broad and complex
    topic, which we cannot grasp in a single section of the book. But it’s worthwhile
    to think about your system – what kind of hardware for network communication it
    has; maybe you should consider checking out the *Physical* and *Data Link* layers.
    A simple exercise is to configure your home network – connected devices, routers,
    and so on – yourself. Could the system be safely and securely addressed by the
    outside (if needed)? Then check the *Network*, *Presentation*, and *Application*
    layers. Try out some **port forwarding** and create an application with data exchange
    encryption. Could the software scale fast enough, with the current bandwidth and
    speed? Let’s see what the *Session* and *Transport* layers have to offer – we
    will look into them in the next paragraph. Is it robust and does it remain available
    if attacked? Then revisit all the layers. Of course, these are simple and one-sided
    observations, but they allow you to double-check your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: So, if we ignore the role of the hardware and just focus on establishing a connection,
    we could get back to the sockets and the respective *Session* layer. You’ve probably
    noticed that some websites log you out automatically after some time. Ever wondered
    why? Well, the **session** is an established two-way link for information exchange
    between devices or ends. It’s highly recommended to apply time limits and requirements
    for a session to be destroyed. The opened connection means not only an opened
    channel for sniffing by attackers but also a used resource on the server side.
    This requires computational power, which could be redirected elsewhere. The server
    usually holds the current state and the session history, so we note this kind
    of communication as *stateful* – at least one of the devices keeps the state.
    But if we manage to handle requests without the need to know and keep previous
    data, we could proceed with *stateless* communication. Still, we require the session
    to build a connection-oriented data exchange. A known protocol for the job is
    found in the *Transport* layer – the **Transmission Control Protocol** (**TCP**).
    If we don’t want to establish a two-way information transfer channel but just
    want to implement a broadcast application, then we could proceed with the connectionless
    communication, provided through the **User Datagram Protocol** (**UDP**). Let’s
    check them out in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Getting familiar with networking through UDP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we said, this protocol could realize connectionless communication, although
    this doesn’t mean there’s no connection between the endpoints. It means that they
    don’t need to be constantly in connection to maintain the data transfer and interpret
    it on their ends. In other words, losing some packets (leading to not hearing
    someone well on the call while in an online meeting, for example) is probably
    not going to be crucial for the system’s behavior itself. It might be crucial
    to you, but let’s be honest, we bet you require the high speed more, and it comes
    with a cost. Network applications such as the **Domain Name System** (**DNS**),
    the **Dynamic Host Configuration Protocol** (**DHCP**), audio-video streaming
    platforms, and others use UDP. Discrepancies and loss of packets are usually handled
    by data retransmission, but this is realized on the *Application* layer and depends
    on the programmer’s implementation. Schematically, the system calls for establishing
    such a connection are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – UDP system call realization](img/Figure_7.5_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – UDP system call realization
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, it is truly simple – applications on both (or more) sides of
    the communication must only follow that sequence. The protocol doesn’t oblige
    you with the message order or the transfer quality, it’s just fast. Let’s see
    the following example, requesting a die roll from a socket *N* number of times.
    The full version of the code is found at [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%207](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%207):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the communication configuration is fairly easy – one side has
    to bind to an address in order to be aware of where to receive data from (marker
    `{3}`), whereas the other only writes data directly to the socket. The socket
    configuration is described at marker `{1}`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The address family is defined as `AF_INET`, meaning we will rely on IPv4-compliant
    addresses. We could use `AF_INET6` for IPv6, or `AF_BLUETOOTH` for Bluetooth.
    We are using the UDP through the `SOCK_DGRAM` setting of the socket (markers `{2}`
    and `{10}`). Through this, we are transferring a number from one process to another.
    You could imagine them as a server and a client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'A request for a new die roll is received (marker `{4}`) and the request data
    is printed out. Then, the request string is compared to an immutable one, so we
    know that this request is just for a die roll (marker `{5}`). As you can see,
    we use the `MSG_WAITALL` setting, which means that the socket operation will block
    the calling process – usually when there is no incoming data. In addition, this
    is a UDP communication, therefore the packet order might not be followed, and
    receiving `0` bytes through `recvfrom()` is a valid use case. That said, we use
    additional messages to mark the ending of the communication (markers `{6}` and
    `{14}`). For simplicity, if the `request.compare()` result is not `0`, the communication
    is ended. Additional checks for multiple options could be added, though. We could
    use a similar handshake to start the communication in the first place – this is
    depending on the system programmer’s decision and the application requirements.
    Proceeding with the client’s functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `die_roll()` function is called for `dice_rolls` a number of times (markers
    `{10}` and `{11}`) and the result is sent through the socket (marker `{12}`).
    After the results are received back (marker `{13}`), an ending message is sent
    (marker `{14}`). We have mostly used `MSG_CONFIRM` for this example, but you must
    be careful with this flag. It should be used when you expect a response from the
    same peer you send to. It is telling the Data Link layer of the OSI model that
    there’s a successful reply. We could change the `recvfrom()` setting to `MSG_DONTWAIT`,
    as in marker `{12}`, but it would be a good idea to implement our own retry mechanism,
    or switch to TCP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the communication after the closing statement (markers `{8}` and `{15}`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The shortened version of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We have to set the address and port where our server could be accessed from.
    Usually, server computers have many applications constantly running, some of which
    execute services for customers. These services bind with the ports of the server
    and users can call them to do some work – get an online store’s contents, check
    the weather, get some banking details, visualize a graphical website, and so on.
    Only one application (service) can work with a given port at a time. If you try
    to use it with another while the first one is active, you will get an `Address
    already in use` error (or similar). Currently, we’re using port `8080`, which
    is commonly opened for TCP/UDP (and HTTP). You could also try `80`, but on Linux,
    non-root users don’t have this capability – you will need higher user permissions
    to use ports less than `1000`. Last but not least, the IP address is set as `INADDR_ANY`.
    This is often used when we do the communication on a single system and we don’t
    care about its address. Still, we could use it, if we want, after we take it from
    the result of the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, this is `192.168.136.128`. We could update the code at marker
    `{1}` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Another option is that the localhost address – `127.0.0.1` – could be used
    with the loopback device address: `INADDR_LOOPBACK`. We use it to run local servers,
    usually for testing purposes. But if we use an exact IP address, then this is
    done when we need to be very specific about the application’s endpoint, and if
    the IP address is a static one, we expect others on the local network to be able
    to call it. If we want to expose it to the outside world so we make our service
    available to others (let’s say we own an online shop and we want to provide our
    shopping service to the world), then we must think about **port forwarding**.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, just exposing the port is considered unsafe because the device can
    be accessed by anybody. Instead, services are not only guarded by firewalls, encryption
    mechanisms, and so on but are also deployed on virtual machines. This creates
    an extra layer of security as the attacker will never have access to the real
    device, just to a very limited version of it. Such a decision also provides higher
    availability as the attacked surface could be immediately removed and the system
    administrator could bring up a new virtual machine from a healthy snapshot, making
    the service available again. Depending on the implementation, this could be automated
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: One last thing – the file’s contents might be misplaced if we are transferring
    larger amounts of data. This is again expected from UDP, as expressed earlier,
    because of the packets’ ordering. If it does not suit your purpose and you require
    a more robust implementation, then you should check the TCP description in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about robustness through TCP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The alternative to UDP is TCP. It is considered reliable – the messages are
    ordered, it is connection-oriented, and it has a lengthened latency. Applications
    such as the **World Wide Web** (**WWW**), email, remote administration applications,
    and so on are based on this protocol. What you’ve probably noticed already (and
    you’re going to observe in *Figure 7**.6*) is that the respective system calls
    are in the same sequence and have similar names as in other programming languages.
    This helps people with different areas of expertise to have a common ground for
    designing network applications and easily understand the sequence of events. This
    is a very simple way to help them follow the protocols in the OSI model, using
    those names as hints for where the communication is currently at. As we already
    mentioned in the previous section, sockets are used for environment-agnostic solutions,
    where systems have different OSs and the communicating applications are in different
    programming languages. For example, they are implemented in C, C++, Java, or Python,
    and their clients could be in PHP, JavaScript, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The system calls for TCP communication are represented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – TCP system call realization](img/Figure_7.6_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – TCP system call realization
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it is more complicated than UDP, as was expected. How so? Well,
    we need to keep an established connection and the kernel acknowledges the packet
    transfer. If you remember, in [*Chapter 1*](B20833_01.xhtml#_idTextAnchor014)
    and [*Chapter 2*](B20833_02.xhtml#_idTextAnchor029), we discussed that sockets
    are files as well, and we could treat them as such. Instead of doing the `send()`
    and `recv()` calls, you could simply do `write()` and `read()` calls. The first
    ones are specialized in the role of network communication, while the latter are
    generally for all files. Using the `read()` and `write()` calls will be like communicating
    through a pipe but between computer systems, therefore it again depends on your
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the following example – a simple request-response exchange, which
    we will execute on different machines on the local network, as the IP address
    from earlier is valid only for our internal network. First, let’s see whether
    we can ping the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we have access to the machine. Now, let’s run the server as a separate
    application (the full code can be found at https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%207).
    The configuration is almost the same, so we skip those parts from the snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We open the socket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `SOCK_STREAM` to indicate this ias a TCP connection. We also use the
    hardcoded IP. After we bind to the address, we need to listen for a `BACKLOG`
    number of active connections. Each new connection could be accepted in general
    if the number of connections is smaller than the `BACKLOG` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Until this point, we just have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s prepare to accept a client and handle its requests. We use the `MSG_PEEK`
    flag to check for incoming messages, and we send messages with `MSG_DONTWAIT`.
    We leave `sendto()` without a result check for simplicity and readability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'And the socket is closed at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s connect a client from another system. Its implementation is similar
    to the UDP one, except `connect()` must be called and must be successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The server’s output changes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s continue the communication, sending information back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We are closing the communication on the client side, including the socket.
    The client’s output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'As the client’s job is done, the process terminates and its socket is closed,
    but the server remains active for other clients, so if we call the client multiple
    times from different shells, we will have the following output for the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The server will handle up to five client sessions in its backlog. If the clients
    don’t close their sockets or the server doesn’t forcefully terminate their connections
    after some timeout, it will not be able to accept new clients, and the `Client
    connection failed` message will be observed. In the next chapter, we will discuss
    different time-based techniques, so think about combining them with your implementation
    to provide a meaningful session timeout.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to gracefully handle the server termination, we could simply implement
    a signal handler, as we did in [*Chapter 3*](B20833_03.xhtml#_idTextAnchor047).
    This time, we will handle the *Ctrl* + *C* key combination, leading to the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned earlier, ungraceful termination of servers and clients could lead
    to hanging sockets and opened ports. This will become problematic for a system,
    as simple application restarts will fail with `Address already in use`. If this
    happens, double-check for remaining processes through the `ps` command. You can
    terminate the running process through the `kill` command, as you learned in [*Chapter
    1*](B20833_01.xhtml#_idTextAnchor014) and [*Chapter 2*](B20833_02.xhtml#_idTextAnchor029).
    Sometimes, this is not enough either, and servers should not be terminated that
    easily. Therefore, you could just change a port after checking which ports are
    opened. You could do that through the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the server is up and running on the respective address and port:
    `192.168.136.128:8080`. We can also check the connections to a certain port by
    using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: With multiple online services nowadays, we cannot escape network programming.
    We encourage you to use these examples as simple applications to start from. It’s
    also important to spend some time learning more about the multiple socket settings
    as they will help you cover your specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned about various ways to execute IPC. You got familiar
    with MQs as simple, real-time, and reliable instruments for sending small chunks
    of data. We also got into the details of fundamental synchronization mechanisms
    such as semaphores and mutexes, along with their C++20 interfaces. In combination
    with shmem, you observed how we could exchange large amounts of data fast. At
    the end, the network communication through sockets was introduced to you through
    the main protocols, UDP and TCP.
  prefs: []
  type: TYPE_NORMAL
- en: Complex applications usually rely on multiple IPC techniques to achieve their
    goals. It’s important to be aware of them – both their strengths and their disadvantages.
    This will help you decide on your particular implementation. Most of the time,
    we build layers on top of IPC solutions in order to guarantee the robustness of
    an application – for example, through retry mechanisms, polling, event-driven
    designs, and so on. We will revisit these topics in [*Chapter 9*](B20833_09.xhtml#_idTextAnchor129).
    The next chapter will give you the instruments to self-monitor your availability
    and performance through different timers.
  prefs: []
  type: TYPE_NORMAL
