<html><head></head><body>
  <div><h1 class="chapter-number" id="_idParaDest-15">
    <a id="_idTextAnchor014">
    </a>
    
     1
    
   </h1>
   <h1 id="_idParaDest-16">
    <a id="_idTextAnchor015">
    </a>
    
     Parallel Programming Paradigms
    
   </h1>
   <p>
    
     Before we dive into
    
    <strong class="bold">
     
      parallel programming
     
    </strong>
    
     using C++, throughout the first two chapters, we will
    
    <a id="_idIndexMarker000">
    </a>
    
     focus on acquiring some foundational knowledge about the different approaches to building parallel software and how the software interacts with the
    
    
     
      machine hardware.
     
    
   </p>
   <p>
    
     In this chapter, we will introduce parallel programming and the different paradigms and models that we can use when developing efficient, responsive, and scalable concurrent and
    
    
     
      asynchronous software.
     
    
   </p>
   <p>
    
     There are many ways to group concepts and methods when classifying the different approaches we can take to develop parallel software.
    
    
     As we are focusing on software built with C++ in this book, we can divide the different parallel programming paradigms as follows: concurrency, asynchronous programming, parallel programming, reactive programming, dataflows, multithreading programming, and
    
    
     
      event-driven programming.
     
    
   </p>
   <p>
    
     Depending on the problem at hand, a specific paradigm could be more suitable than others to solve a given scenario.
    
    
     Understanding the different paradigms will help us to analyze the problem and narrow down the best
    
    
     
      solution possible.
     
    
   </p>
   <p>
    
     In this chapter, we’re going to cover the following
    
    
     
      main topics:
     
    
   </p>
   <ul>
    <li>
     
      What is parallel programming and why does
     
     
      
       it matter?
      
     
    </li>
    <li>
     
      What are the different parallel programming paradigms and why do we need to
     
     
      
       understand them?
      
     
    </li>
    <li>
     
      What will you learn in
     
     
      
       this book?
      
     
    </li>
   </ul>
   <h1 id="_idParaDest-17">
    <a id="_idTextAnchor016">
    </a>
    
     Technical requirements
    
   </h1>
   <p>
    
     No technical requirements apply for
    
    
     
      this chapter.
     
    
   </p>
   <p>
    
     Throughout the book, we will develop different solutions using C++20 and, in some examples, C++23.
    
    
     Therefore, we will need to install GCC 14 and
    
    
     
      Clang 8.
     
    
   </p>
   <p>
    
     All the code blocks shown in this book can be found in the following GitHub
    
    
     
      repository:
     
    
    <a href="https://github.com/PacktPublishing/Asynchronous-Programming-with-CPP">
     
      
       https://github.com/PacktPublishing/Asynchronous-Programming-with-CPP
      
     
    </a>
    
     
      .
     
    
   </p>
   <h1 id="_idParaDest-18">
    <a id="_idTextAnchor017">
    </a>
    
     Getting to know classifications, techniques, and models
    
   </h1>
   <p>
    
     Parallel computing occurs when tasks or computations are done simultaneously, with a task being a unit of execution or unit of work in a software application.
    
    
     As there are many ways to achieve parallelism, understanding the different approaches will be helpful to write efficient parallel algorithms.
    
    
     These approaches are described via paradigms
    
    
     
      and models.
     
    
   </p>
   <p>
    
     But first, let us start by classifying the different parallel
    
    
     
      computing systems.
     
    
   </p>
   <h2 id="_idParaDest-19">
    <a id="_idTextAnchor018">
    </a>
    
     Systems classification and techniques
    
   </h2>
   <p>
    
     One of
    
    <a id="_idIndexMarker001">
    </a>
    
     the earliest classifications of parallel computing systems was made by Michael J.
    
    
     Flynn in 1966.
    
    
     Flynn’s taxonomy defines the following classification
    
    <a id="_idIndexMarker002">
    </a>
    
     based on the
    
    <strong class="bold">
     
      data streams
     
    </strong>
    
     and number of instructions a parallel computing architecture
    
    
     
      can handle:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Single-instruction-single-data (SISD) systems
      
     </strong>
     
      : Define
     
     <a id="_idIndexMarker003">
     </a>
     
      a
     
     
      
       sequential program
      
     
    </li>
    <li>
     <strong class="bold">
      
       Single-instruction-multiple-data (SIMD) systems
      
     </strong>
     
      : Where operations are done
     
     <a id="_idIndexMarker004">
     </a>
     
      over a large dataset, for example in signal processing of
     
     
      
       GPU computing
      
     
    </li>
    <li>
     <strong class="bold">
      
       Multiple-instructions-single-data (MISD) systems
      
     </strong>
     
      :
     
     
      
       Rarely
      
     
     
      <a id="_idIndexMarker005">
      </a>
     
     
      
       used
      
     
    </li>
    <li>
     <strong class="bold">
      
       Multiple-instructions-multiple-data (MIMD) systems
      
     </strong>
     
      : The most common parallel
     
     <a id="_idIndexMarker006">
     </a>
     
      architectures based in multicore and
     
     
      
       multi-processor computers
      
     
    </li>
   </ul>
   <div><div><img alt="Figure 1.1: Flynn’s taxonomy" src="img/B22219_01_1.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 1.1: Flynn’s taxonomy
    
   </p>
   <p>
    
     This book
    
    <a id="_idIndexMarker007">
    </a>
    
     is not only about building software with C++ but also about keeping an eye on how it interacts with the underlying hardware.
    
    
     A more interesting division or taxonomy can probably be done at the software level, where we can define the techniques.
    
    
     We will learn about these in the
    
    
     
      subsequent sections.
     
    
   </p>
   <h3>
    
     Data parallelism
    
   </h3>
   <p>
    
     Many different data units are processed in parallel by the same program or sequence of instructions
    
    <a id="_idIndexMarker008">
    </a>
    
     running in different processing units such as CPU or
    
    
     
      GPU cores.
     
    
   </p>
   <p>
    <strong class="bold">
     
      Data parallelism
     
    </strong>
    
     is achieved by how many disjoint datasets can be processed at the same time
    
    <a id="_idIndexMarker009">
    </a>
    
     by the same operations.
    
    
     Large datasets can be divided into smaller and independent data chunks
    
    
     
      exploiting parallelism.
     
    
   </p>
   <p>
    
     This technique is also highly scalable, as adding more processing units allows for the processing of a higher volume
    
    
     
      of data.
     
    
   </p>
   <p>
    
     In this subset, we can include SIMD instruction sets such as SSE, AVX, VMX, or NEON, which are accessible via intrinsic functions in C++.
    
    
     Also, libraries such as OpenMP and CUDA for NVIDIA GPUs.
    
    
     Some examples of its usage can be found in machine learning training and image processing.
    
    
     This technique is related to the SIMD taxonomy defined
    
    
     
      by Flynn.
     
    
   </p>
   <p>
    
     As usual, there are
    
    <a id="_idIndexMarker010">
    </a>
    
     also some drawbacks.
    
    
     Data must be easily divisible into
    
    <a id="_idIndexMarker011">
    </a>
    
     independent chunks.
    
    
     This data division and posterior merging also introduces some overhead that can reduce the benefits
    
    
     
      of parallelization.
     
    
   </p>
   <h3>
    
     Task parallelism
    
   </h3>
   <p>
    
     In computers where each CPU core runs different tasks using processes or threads,
    
    <strong class="bold">
     
      task parallelism
     
    </strong>
    
     can be achieved when these tasks simultaneously receive data, process it, and send back the results that they generate via
    
    
     
      message passing.
     
    
   </p>
   <p>
    
     The advantage
    
    <a id="_idIndexMarker012">
    </a>
    
     of task parallelism
    
    <a id="_idIndexMarker013">
    </a>
    
     resides in the ability to design heterogeneous and granular tasks that can make better usage of processing resources, being more flexible when designing a solution with potentially
    
    
     
      higher speed-up.
     
    
   </p>
   <p>
    
     Due to the possible dependencies between tasks that can be created by the data, as well as the different nature of each task, scheduling and coordination are more complex than with data parallelism.
    
    
     Also, task creation adds some
    
    
     
      processing overhead.
     
    
   </p>
   <p>
    
     Here we can include Flynn’s MISD and MIMD taxonomies.
    
    
     Some examples can be found in a web server request processing system or a user interface
    
    
     
      events handler.
     
    
   </p>
   <h3>
    
     Stream parallelism
    
   </h3>
   <p>
    
     A continuous sequence of data elements, also known as a
    
    <strong class="bold">
     
      data stream
     
    </strong>
    
     , can be processed
    
    <a id="_idIndexMarker014">
    </a>
    
     concurrently by dividing the computation into various stages processing a subset of
    
    
     
      the data.
     
    
   </p>
   <p>
    
     Stages
    
    <a id="_idIndexMarker015">
    </a>
    
     can run concurrently.
    
    
     Some
    
    <a id="_idIndexMarker016">
    </a>
    
     generate the input of other stages, building a
    
    <strong class="bold">
     
      pipeline
     
    </strong>
    
     from stage dependencies.
    
    
     A processing stage can send results to the next stage without waiting to receive the entire
    
    
     
      stream data.
     
    
   </p>
   <p>
    
     Stream parallel techniques are effective when handling continuous data.
    
    
     They are also highly scalable, as they can be scaled by adding more processing units to handle the extra input data.
    
    
     Since the stream data is processed as it arrives, this means not needing to wait for the entire data stream to be sent, which means that memory usage is
    
    
     
      also reduced.
     
    
   </p>
   <p>
    
     However, as usual, there are some drawbacks.
    
    
     These systems are more complex to implement due to their processing logic, error handling, and recovery.
    
    
     As we might also need
    
    <a id="_idIndexMarker017">
    </a>
    
     to process the data stream
    
    <a id="_idIndexMarker018">
    </a>
    
     in real time, the hardware could be a limitation
    
    
     
      as well.
     
    
   </p>
   <p>
    
     Some examples of these systems include monitoring systems, sensor data processing, and audio and
    
    
     
      video streaming.
     
    
   </p>
   <h3>
    
     Implicit parallelism
    
   </h3>
   <p>
    
     In this case, the compiler, the runtime, or the hardware takes care of parallelizing the execution
    
    <a id="_idIndexMarker019">
    </a>
    
     of the instructions transparently for
    
    
     
      the programmer.
     
    
   </p>
   <p>
    
     This makes
    
    <a id="_idIndexMarker020">
    </a>
    
     it easier to write parallel programs but limits the programmer’s control over the strategies used, or even makes it more difficult to analyze performance
    
    
     
      or debugging.
     
    
   </p>
   <p>
    
     Now that we have a better understanding of the different parallel systems and techniques, it’s time to learn about the different models we can use when designing a
    
    
     
      parallel program.
     
    
   </p>
   <h2 id="_idParaDest-20">
    <a id="_idTextAnchor019">
    </a>
    
     Parallel programming models
    
   </h2>
   <p>
    
     A
    
    <strong class="bold">
     
      parallel programming model
     
    </strong>
    
     is a parallel computer’s architecture used to express algorithms
    
    <a id="_idIndexMarker021">
    </a>
    
     and build programs.
    
    
     The more generic the model the more valuable it becomes, as it can be used in a broader range of scenarios.
    
    
     In that
    
    <a id="_idIndexMarker022">
    </a>
    
     sense, C++ implements a parallel model through a library within the
    
    <strong class="bold">
     
      Standard Template Library
     
    </strong>
    
     (
    
    <strong class="bold">
     
      STL
     
    </strong>
    
     ), which can be used to achieve parallel execution of programs from
    
    
     
      sequential applications.
     
    
   </p>
   <p>
    
     These models describe how the different tasks interact during the program’s lifetime to achieve a result from input data.
    
    
     Their main differences are related to how the tasks interact with each other and how they process the
    
    
     
      incoming data.
     
    
   </p>
   <h3>
    
     Phase parallel
    
   </h3>
   <p>
    
     In
    
    <strong class="bold">
     
      phase parallel
     
    </strong>
    
     , also known as the agenda or loosely synchronous paradigm, multiple jobs
    
    <a id="_idIndexMarker023">
    </a>
    
     or tasks perform independent computations in parallel.
    
    
     At some point, the program needs to perform a synchronous
    
    <a id="_idIndexMarker024">
    </a>
    
     interaction operation using a barrier to synchronize the different processes.
    
    
     A barrier is a synchronization mechanism that ensures that a group of tasks reach a particular point in their execution before any of them can proceed further.
    
    
     The next steps execute other asynchronous operations, and
    
    
     
      so on.
     
    
   </p>
   <div><div><img alt="Figure 1.2: Phase parallel model" src="img/B22219_01_2.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 1.2: Phase parallel model
    
   </p>
   <p>
    
     The advantage of this model is that the interaction between tasks does not overlap with computation.
    
    
     On the other hand, it is difficult to reach a balanced workload and throughput among all
    
    
     
      processing units.
     
    
   </p>
   <h3>
    
     Divide and conquer
    
   </h3>
   <p>
    
     The application using this model uses a main task or job that divides the workload among
    
    <a id="_idIndexMarker025">
    </a>
    
     its children, assigning them to
    
    
     
      smaller tasks.
     
    
   </p>
   <p>
    
     Child tasks compute the results in parallel and return them to the parent task, where the partial results are merged into the final one.
    
    
     Child tasks can also subdivide the assigned task into even smaller ones and create their own
    
    
     
      child tasks.
     
    
   </p>
   <p>
    
     This model has the same disadvantage as the phase parallel model; it is difficult to achieve a good
    
    
     
      load balance.
     
    
   </p>
   <div><div><img alt="Figure 1.3: Divide and conquer model" src="img/B22219_01_3.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 1.3: Divide and conquer model
    
   </p>
   <p>
    
     In
    
    
     <em class="italic">
      
       Figure 1
      
     </em>
    
    <em class="italic">
     
      .3
     
    </em>
    
     , we can see
    
    <a id="_idIndexMarker026">
    </a>
    
     how the main job divides the work among several child tasks, and how
    
    <strong class="bold">
     
      Child Task 2
     
    </strong>
    
     , in turn, subdivides its assigned work into two
    
    
     
      additional tasks.
     
    
   </p>
   <h3>
    
     Pipeline
    
   </h3>
   <p>
    
     Several
    
    <a id="_idIndexMarker027">
    </a>
    
     tasks are interconnected, building a virtual pipeline.
    
    
     In this pipeline, the various stages can run simultaneously, overlapping their execution when fed
    
    
     
      with data.
     
    
   </p>
   <div><div><img alt="Figure 1.4: Pipeline model" src="img/B22219_01_4.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 1.4: Pipeline model
    
   </p>
   <p>
    
     In the preceding figure, three tasks interact in a pipeline composed of five stages.
    
    
     In each stage, some tasks are running, generating output results that are used by other tasks in the
    
    
     
      next stages.
     
    
   </p>
   <h3>
    
     Master-slave
    
   </h3>
   <p>
    
     Using the
    
    <strong class="bold">
     
      master-slave model
     
    </strong>
    
     , also
    
    <a id="_idIndexMarker028">
    </a>
    
     known as
    
    <strong class="bold">
     
      process farm
     
    </strong>
    
     , a master
    
    <a id="_idIndexMarker029">
    </a>
    
     job executes the sequential part of the algorithm and spawns
    
    <a id="_idIndexMarker030">
    </a>
    
     and coordinates slave tasks that execute parallel operations in the workload.
    
    
     When a slave task finishes its computation, it informs the master job of the result, which might then send more data to the slave task to
    
    
     
      be processed.
     
    
   </p>
   <div><div><img alt="Figure 1.5: The master-slave model" src="img/B22219_01_5.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 1.5: The master-slave model
    
   </p>
   <p>
    
     The main disadvantage is that the master can become a bottleneck if it needs to deal with too many slaves or when tasks are too small.
    
    
     There is a tradeoff when selecting the amount of work to
    
    <a id="_idIndexMarker031">
    </a>
    
     be performed by each task, also known as
    
    <strong class="bold">
     
      granularity
     
    </strong>
    
     .
    
    
     When tasks are small, they are named fine-grained, and when they are large, they
    
    
     
      are coarse-grained.
     
    
   </p>
   <h3>
    
     Work pool
    
   </h3>
   <p>
    
     In the work
    
    <a id="_idIndexMarker032">
    </a>
    
     pool model, a global structure holds
    
    <a id="_idIndexMarker033">
    </a>
    
     a pool of work items to do.
    
    
     Then, the main program creates jobs that fetch pieces of work from the pool to
    
    
     
      execute them.
     
    
   </p>
   <p>
    
     These jobs can generate more work units that are inserted into the work pool.
    
    
     The parallel program finishes its execution when all work units are completed and the pool is
    
    
     
      thus empty.
     
    
   </p>
   <div><div><img alt="Figure 1.6: The work pool model" src="img/B22219_01_6.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 1.6: The work pool model
    
   </p>
   <p>
    
     This
    
    <a id="_idIndexMarker034">
    </a>
    
     mechanism facilitates load balancing
    
    <a id="_idIndexMarker035">
    </a>
    
     among free
    
    
     
      processing units.
     
    
   </p>
   <p>
    
     In C++, this pool is usually implemented by using an unordered set, a queue, or a priority queue.
    
    
     We will implement some examples in
    
    
     
      this book.
     
    
   </p>
   <p>
    
     Now that we have learned about a variety of models that we can use to build a parallel system, let’s explore the different parallel programming paradigms available to develop software that efficiently runs tasks
    
    
     
      in parallel.
     
    
   </p>
   <h1 id="_idParaDest-21">
    <a id="_idTextAnchor020">
    </a>
    
     Understanding various parallel programming paradigms
    
   </h1>
   <p>
    
     Now that
    
    <a id="_idIndexMarker036">
    </a>
    
     we have explored some of the different models used for building parallel programs, it is time to move to a more abstract classification and learn about the fundamental styles or principles of how to code parallel programs by exploring the different parallel programming
    
    
     
      language paradigms.
     
    
   </p>
   <h2 id="_idParaDest-22">
    <a id="_idTextAnchor021">
    </a>
    
     Synchronous programming
    
   </h2>
   <p>
    
     A
    
    <strong class="bold">
     
      synchronous programming
     
    </strong>
    
     language is used to build programs where code is executed
    
    <a id="_idIndexMarker037">
    </a>
    
     in a strict sequential order.
    
    
     While one instruction is being executed, the program remains blocked until the
    
    <a id="_idIndexMarker038">
    </a>
    
     instruction finishes.
    
    
     In other words, there is no multitasking.
    
    
     This makes the code easier to understand
    
    
     
      and debug.
     
    
   </p>
   <p>
    
     However, this behavior makes the program unresponsive to external events while it is blocked while running an instruction and difficult
    
    
     
      to scale.
     
    
   </p>
   <p>
    
     This is the traditional paradigm used by most programming languages such as C, Python,
    
    
     
      or Java.
     
    
   </p>
   <p>
    
     This paradigm is especially useful for reactive or embedded systems that need to respond in real time and in an ordered way to input events.
    
    
     The processing speed must match the one imposed by the environment with strict
    
    
     
      time bounds.
     
    
   </p>
   <div><div><img alt="Figure 1.7: Asynchronous versus synchronous execution time" src="img/B22219_01_7.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 1.7: Asynchronous versus synchronous execution time
    
   </p>
   <p>
    
     <em class="italic">
      
       Figure 1
      
     </em>
    
    <em class="italic">
     
      .7
     
    </em>
    
     shows two tasks running in a system.
    
    
     In the synchronous system, task A is interrupted by task B and only resumes its execution once task B has finished its work.
    
    
     In the asynchronous system, tasks A and B can run simultaneously, thus completing both of their work in
    
    
     
      less time.
     
    
   </p>
   <h2 id="_idParaDest-23">
    <a id="_idTextAnchor022">
    </a>
    
     Concurrency programming
    
   </h2>
   <p>
    
     With
    
    <strong class="bold">
     
      concurrency programming
     
    </strong>
    
     , more than one task can run at the
    
    
     
      same time.
     
    
   </p>
   <p>
    
     Tasks can
    
    <a id="_idIndexMarker039">
    </a>
    
     run independently without waiting for other tasks’ instructions to finish.
    
    
     They can also share resources and communicate with each other.
    
    
     Their instructions can run asynchronously, meaning that they can be executed
    
    <a id="_idIndexMarker040">
    </a>
    
     in any order without affecting the outcome, adding the potential for parallel processing.
    
    
     On the other hand, that makes this kind of program more difficult to understand
    
    
     
      and debug.
     
    
   </p>
   <p>
    
     Concurrency increases the program throughput, as the number of tasks completed in a time interval increases with concurrency (see the formula for Gustafson’s law in the section
    
    <em class="italic">
     
      Exploring the metrics to assess parallelism
     
    </em>
    
     at the end of this chapter).
    
    
     Also, it achieves better input and output responsiveness, as the program can perform other tasks during
    
    
     
      waiting periods.
     
    
   </p>
   <p>
    
     The main problem in concurrent software is achieving correct concurrency control.
    
    
     Exceptional care must be taken when coordinating access to shared resources and ensuring that the correct sequence of interactions is taking place between the different computational executions.
    
    
     Incorrect decisions can lead to race conditions, deadlocks, or resource starvation, which are explained in depth in
    
    <a href="B22219_03.xhtml#_idTextAnchor051">
     
      <em class="italic">
       
        Chapter 3
       
      </em>
     
    </a>
    
     and
    
    <a href="B22219_04.xhtml#_idTextAnchor074">
     
      <em class="italic">
       
        Chapter 4
       
      </em>
     
    </a>
    
     .
    
    
     Most of these issues are solved by following a consistency or memory model, which defines rules on how and in which order operations should be performed when accessing the
    
    
     
      shared memory.
     
    
   </p>
   <p>
    
     Designing efficient concurrent algorithms is done by finding techniques to coordinate tasks’ execution, data exchange, memory allocations, and scheduling to minimize the response time and
    
    
     
      maximize throughput.
     
    
   </p>
   <p>
    
     The first academic paper introducing concurrency,
    
    <em class="italic">
     
      Solution of a Problem in Concurrent Programming Control
     
    </em>
    
     , was published by Dijkstra in 1965.
    
    
     Mutual exclusion was also identified and
    
    
     
      solved there.
     
    
   </p>
   <p>
    
     Concurrency can happen at the operating system level in a preemptive way, whereby the scheduler switches contexts (switching from one task to another) without interacting with the tasks.
    
    
     It can also happen in a non-preemptive or cooperative way, whereby the task yields control to the scheduler, which chooses another task to
    
    
     
      continue work.
     
    
   </p>
   <p>
    
     The scheduler interrupts the running program by saving its state (memory and register contents), then loading the saved state of a resumed program and transferring control to it.
    
    
     This is
    
    <a id="_idIndexMarker041">
    </a>
    
     called
    
    <strong class="bold">
     
      context switching
     
    </strong>
    
     .
    
    
     Depending on the priority of a task, the scheduler might allow a high-priority task to use more CPU time than a
    
    
     
      low-priority one.
     
    
   </p>
   <p>
    
     Also, some
    
    <a id="_idIndexMarker042">
    </a>
    
     special operating software such as memory protection might use special hardware to keep supervisory software undamaged by user-mode
    
    
     
      program errors.
     
    
   </p>
   <p>
    
     This
    
    <a id="_idIndexMarker043">
    </a>
    
     mechanism is not only used in single-core computers but also in multicore ones, allowing many more tasks to be executed than the number of
    
    
     
      available cores.
     
    
   </p>
   <p>
    <strong class="bold">
     
      Preemptive multitasking
     
    </strong>
    
     also allows important tasks to be scheduled earlier to deal with
    
    <a id="_idIndexMarker044">
    </a>
    
     important external events quickly.
    
    
     These tasks wake up and deal with the important work when the operating system sends them a signal that triggers
    
    
     
      an interruption.
     
    
   </p>
   <p>
    
     Older
    
    <a id="_idIndexMarker045">
    </a>
    
     versions of Mac and Windows operating systems used
    
    <strong class="bold">
     
      non-preemptive multitasking
     
    </strong>
    
     .
    
    
     This is still used today on the RISC operating system.
    
    
     Unix systems started to use preemptive multitasking in 1969, being a core feature of all Unix-like systems and modern Windows versions from Windows NT
    
    <strong class="source-inline">
     
      3.1
     
    </strong>
    
     and Windows
    
    
     
      95 onward.
     
    
   </p>
   <p>
    
     Early-days CPUs could only run one path of instructions at a given time.
    
    
     Parallelism was achieved by switching between instruction streams, giving the illusion of parallelism at the software level by seemingly overlapping
    
    
     
      in execution.
     
    
   </p>
   <p>
    
     However, in 2005, Intel® introduced multicore processors, which allowed several instruction streams to execute at once at the hardware level.
    
    
     This imposed some challenges at the time of writing software, as hardware-level concurrency now needed to be addressed
    
    
     
      and exploited.
     
    
   </p>
   <p>
    
     C++ has supported concurrent programming since C++11 with the
    
    <strong class="source-inline">
     
      std::thread
     
    </strong>
    
     library.
    
    
     Earlier versions did not include any specific functionality, so programmers relied on platform-specific libraries based on the POSIX threading model in Unix systems or on proprietary Microsoft libraries in
    
    
     
      Windows systems.
     
    
   </p>
   <p>
    
     Now that we better understand what concurrency is, we need to distinguish between concurrency and parallelism.
    
    
     Concurrency happens when many execution paths can run in overlapping time periods with interleaved execution, while parallelism happens when these tasks are executed at the same time by different CPU units, exploiting available
    
    
     
      multicore resources.
     
    
   </p>
   <div><div><img alt="Figure 1.8: Concurrency versus parallelism" src="img/B22219_01_8.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 1.8: Concurrency versus parallelism
    
   </p>
   <p>
    
     Concurrent
    
    <a id="_idIndexMarker046">
    </a>
    
     programming is considered more general than parallel programming as the latter has a predefined communication pattern while the former
    
    <a id="_idIndexMarker047">
    </a>
    
     can involve arbitrary and dynamic patterns of communication and interaction
    
    
     
      between tasks.
     
    
   </p>
   <p>
    
     Parallelism can exist without concurrency (without interleaved time periods) and concurrency without parallelism (by multitasking by time-sharing on a
    
    
     
      single-core CPU).
     
    
   </p>
   <h2 id="_idParaDest-24">
    <a id="_idTextAnchor023">
    </a>
    
     Asynchronous programming
    
   </h2>
   <p>
    
     Asynchronous programming allows some tasks to be scheduled and run in the background
    
    <a id="_idIndexMarker048">
    </a>
    
     while continuing
    
    <a id="_idIndexMarker049">
    </a>
    
     to work on the current job without waiting for the scheduled tasks to finish.
    
    
     When these tasks are finished, they will report their results back to the main job
    
    
     
      or scheduler.
     
    
   </p>
   <p>
    
     One of the key issues of synchronous applications is that a long operation can leave the program unresponsive to further input or processing.
    
    
     Asynchronous programs solve this issue by accepting new input while some operations are being executed with non-blocking tasks and the system can do more than one task at a time.
    
    
     This also allows for better
    
    
     
      resource utilization.
     
    
   </p>
   <p>
    
     As the tasks are executed asynchronously and they report results back when they finish, this paradigm is especially suitable for event-driven programs.
    
    
     Also, it is a paradigm usually used for user interfaces, web servers, network communications, or long-running
    
    
     
      background processing.
     
    
   </p>
   <p>
    
     As hardware has evolved toward multiple processing cores on a single processor chip, it has become
    
    <a id="_idIndexMarker050">
    </a>
    
     mandatory to use asynchronous programming
    
    <a id="_idIndexMarker051">
    </a>
    
     to take advantage of all the available compute power by running tasks in parallel across the
    
    
     
      different cores.
     
    
   </p>
   <p>
    
     However, asynchronous programming has its challenges, as we will explore in this book.
    
    
     For example, it adds complexity, as the code is not interpreted in sequence.
    
    
     This can lead to race conditions.
    
    
     Also, error handling and testing are essential to ensure program stability and
    
    
     
      prevent issues.
     
    
   </p>
   <p>
    
     As we will learn in this book, modern C++ also provides asynchronous mechanisms such as coroutines, which are programs that can be suspended and resumed later, or futures and promises as a proxy for unknown results in asynchronous programs for synchronizing the
    
    
     
      program execution.
     
    
   </p>
   <h2 id="_idParaDest-25">
    <a id="_idTextAnchor024">
    </a>
    
     Parallel programming
    
   </h2>
   <p>
    
     With parallel
    
    <a id="_idIndexMarker052">
    </a>
    
     programming, multiple computation tasks can be done simultaneously on multiple processing units, either with all of them in the same computer (multicore) or on multiple
    
    
     
      computers (cluster).
     
    
   </p>
   <p>
    
     There are two
    
    
     
      main approaches:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Shared-memory parallelism
      
     </strong>
     
      : Tasks
     
     <a id="_idIndexMarker053">
     </a>
     
      can communicate
     
     <a id="_idIndexMarker054">
     </a>
     
      via shared memory, a memory space accessible by
     
     
      
       all processors
      
     
    </li>
    <li>
     <strong class="bold">
      
       Message-passing parallelism
      
     </strong>
     
      : Each
     
     <a id="_idIndexMarker055">
     </a>
     
      task has its own memory space and uses message passing techniques
     
     <a id="_idIndexMarker056">
     </a>
     
      to communicate
     
     
      
       with others
      
     
    </li>
   </ul>
   <p>
    
     As with the previous paradigms, to achieve full potential and avoid bugs or issues, parallel computing needs synchronization mechanisms to avoid tasks interfering with each other.
    
    
     It also calls for load balancing the workload to reach its full potential, as well as reducing overhead when creating and managing tasks.
    
    
     These needs increase design, implementation, and
    
    
     
      debugging complexity.
     
    
   </p>
   <h2 id="_idParaDest-26">
    <a id="_idTextAnchor025">
    </a>
    
     Multithreading programming
    
   </h2>
   <p>
    
     Multithreading programming is a subset of parallel programming wherein a program is divided
    
    <a id="_idIndexMarker057">
    </a>
    
     into multiple threads executing independent units within the same process.
    
    
     The process, memory space, and resources are shared
    
    
     
      between threads.
     
    
   </p>
   <p>
    
     As we
    
    <a id="_idIndexMarker058">
    </a>
    
     already mentioned, sharing memory needs synchronization mechanisms.
    
    
     On the other hand, as there is no need for inter-process communication, resource sharing
    
    
     
      is simplified.
     
    
   </p>
   <p>
    
     For example, multithreading programming is usually used to achieve
    
    <strong class="bold">
     
      graphical user interface
     
    </strong>
    
     (
    
    <strong class="bold">
     
      GUI
     
    </strong>
    
     ) responsiveness with fluid animations, in web servers to handle multiple clients’ requests, or in
    
    
     
      data processing.
     
    
   </p>
   <h2 id="_idParaDest-27">
    <a id="_idTextAnchor026">
    </a>
    
     Event-driven programming
    
   </h2>
   <p>
    
     In event-driven programming, the control flow is driven by external events.
    
    
     The application
    
    <a id="_idIndexMarker059">
    </a>
    
     detects events in real time and responds to these by invoking the appropriate event-handling method
    
    
     
      or callback.
     
    
   </p>
   <p>
    
     An event
    
    <a id="_idIndexMarker060">
    </a>
    
     signals an action that needs to be taken.
    
    
     This event is listened to by the event loop that continuously listens for incoming events and dispatches them to the appropriate callback, which will execute the desired action.
    
    
     As the code is only executed when an action occurs, this paradigm improves efficiency with resource usage
    
    
     
      and scalability.
     
    
   </p>
   <p>
    
     Event-driven programming is useful to act on actions happening in user interfaces, real-time applications, and network
    
    
     
      connection listeners.
     
    
   </p>
   <p>
    
     As with many of the other paradigms, the increased complexity, synchronization, and debugging make this paradigm complex to implement
    
    
     
      and apply.
     
    
   </p>
   <p>
    
     As C++ is a low-level language, techniques such as callbacks or functors are used to write the
    
    
     
      event handlers.
     
    
   </p>
   <h2 id="_idParaDest-28">
    <a id="_idTextAnchor027">
    </a>
    
     Reactive programming
    
   </h2>
   <p>
    
     Reactive programming deals with data streams, which are continuous flows of data or values
    
    <a id="_idIndexMarker061">
    </a>
    
     over time.
    
    
     A program is usually built using declarative or functional programming, defining a pipeline of operators
    
    <a id="_idIndexMarker062">
    </a>
    
     and transformations applied to the stream.
    
    
     These
    
    <a id="_idIndexMarker063">
    </a>
    
     operations happen asynchronously using schedulers and
    
    <strong class="bold">
     
      backpressure
     
    </strong>
    
     
      handling mechanisms.
     
    
   </p>
   <p>
    
     Backpressure happens when the quantity of data overwhelms the consumers and they are not able to process all of it.
    
    
     To avoid a system collapse, a reactive system needs to use backpressure
    
    <a id="_idIndexMarker064">
    </a>
    
     strategies to prevent system failures.
    
   </p>
   <p>
    
     Some of these strategies include
    
    
     
      the following:
     
    
   </p>
   <ul>
    <li>
     
      Controlling input throughput by requesting the publisher to reduce the rate of published events.
     
     
      This can be achieved by following a pull strategy, where the publisher sends events only when the consumer requests them, or by limiting the number of events sent, creating a limited and controlled
     
     
      
       push strategy.
      
     
    </li>
    <li>
     
      Buffering the extra data, which is especially useful when there are data bursts or a high-bandwidth transmission over a
     
     
      
       short period.
      
     
    </li>
    <li>
     
      Dropping some events or delaying their publication until the consumers recover from the
     
     
      
       backpressure state.
      
     
    </li>
   </ul>
   <p>
    
     Thus, reactive
    
    <a id="_idIndexMarker065">
    </a>
    
     programs can be
    
    <strong class="bold">
     
      pull-based
     
    </strong>
    
     or
    
    <strong class="bold">
     
      push-based
     
    </strong>
    
     .
    
    
     Pull-based
    
    <a id="_idIndexMarker066">
    </a>
    
     programs implement the classic case where the events are actively pulled from the data source.
    
    
     On the other hand, push-based programs push events through a signal network to reach the subscriber.
    
    
     Subscribers react to changes without blocking the program, making these systems ideal for rich user interface environments where responsiveness
    
    
     
      is crucial.
     
    
   </p>
   <p>
    
     Reactive programming is like an event-driven model where event streams from various sources can be transformed, filtered, processed, and so on.
    
    
     Both increase code modularity and are suitable for real-time applications.
    
    
     However, there are some differences,
    
    
     
      as follows:
     
    
   </p>
   <ul>
    <li>
     
      Reactive programming reacts to event streams, while event-driven programming deals with
     
     
      
       discrete events.
      
     
    </li>
    <li>
     
      In event-driven programming, an event triggers a callback or event handlers.
     
     
      With reactive programming, a pipeline with different transformation operators can be created whereby the data stream will flow and modify
     
     
      
       the events.
      
     
    </li>
   </ul>
   <p>
    
     Examples of systems and software using reactive programming include the X Windows system
    
    <a id="_idIndexMarker067">
    </a>
    
     and libraries such as Qt, WxWidgets, and Gtk+.
    
    
     Reactive programming is also used in real-time sensors data processing
    
    <a id="_idIndexMarker068">
    </a>
    
     and dashboards.
    
    
     Additionally, it is applied to handling network or file I/O traffic and
    
    
     
      data processing.
     
    
   </p>
   <p>
    
     To reach full potential, there are some challenges to address when using reactive programming.
    
    
     For example, it’s important to debug distributed dataflows and asynchronous processes or to optimize performance by fine-tuning the schedulers.
    
    
     Also, the use of declarative or functional programming makes developing software by using reactive programming techniques a bit more challenging to understand
    
    
     
      and learn.
     
    
   </p>
   <h2 id="_idParaDest-29">
    <a id="_idTextAnchor028">
    </a>
    
     Dataflow programming
    
   </h2>
   <p>
    
     With
    
    <strong class="bold">
     
      dataflow programming
     
    </strong>
    
     , a program is designed as a directed graph of nodes representing
    
    <a id="_idIndexMarker069">
    </a>
    
     computation units and edges representing the flow
    
    <a id="_idIndexMarker070">
    </a>
    
     of data.
    
    
     A node only executes when there is some available data.
    
    
     This paradigm was invented by Jack Dennis at MIT in
    
    
     
      the 1960s.
     
    
   </p>
   <p>
    
     Dataflow programming makes the code and design more readable and clearer, as it provides a visual representation of the different computation units and how they interact.
    
    
     Also, independent nodes can run in parallel with dataflow programming, increasing parallelism and throughput.
    
    
     So, it is like reactive programming but offers a graph-based approach and visual aid to
    
    
     
      modeling systems.
     
    
   </p>
   <p>
    
     To implement a dataflow program, we can use a hash table.
    
    
     The key identifies a set of inputs and the value describes the task to run.
    
    
     When all inputs for a given key are available, the task associated with that key is executed, generating additional input values that may trigger tasks for other keys in the hash table.
    
    
     In these systems, the scheduler can find opportunities for parallelism by using a topological sort on the graph data structure, sorting the different tasks by
    
    
     
      their interdependencies.
     
    
   </p>
   <p>
    
     This paradigm is usually used for large-scale data processing pipelines for machine learning, real-time analysis from sensors or financial markets data, and audio, video, and image processing systems.
    
    
     Examples of software libraries using the dataflow paradigm are Apache
    
    <a id="_idIndexMarker071">
    </a>
    
     Spark and TensorFlow.
    
    
     In hardware, we can find examples for digital signal processing, network routing, GPU architecture, telemetry, and artificial intelligence,
    
    
     
      among others.
     
    
   </p>
   <p>
    
     A variant
    
    <a id="_idIndexMarker072">
    </a>
    
     of dataflow programming is
    
    <strong class="bold">
     
      incremental computing
     
    </strong>
    
     , whereby
    
    <a id="_idIndexMarker073">
    </a>
    
     only the outputs that depend on changed input data are recomputed.
    
    
     This is like recomputing affected cells in an Excel spreadsheet when a cell
    
    
     
      value changes.
     
    
   </p>
   <p>
    
     Now that we have learned about the different parallel programming systems, models, and paradigms, it’s time to
    
    <a id="_idIndexMarker074">
    </a>
    
     introduce some
    
    <strong class="bold">
     
      metrics
     
    </strong>
    
     that help measure parallel
    
    
     
      systems’ performance.
     
    
   </p>
   <h1 id="_idParaDest-30">
    <a id="_idTextAnchor029">
    </a>
    
     Exploring the metrics to assess parallelism
    
   </h1>
   <p>
    
     Metrics
    
    <a id="_idIndexMarker075">
    </a>
    
     are measurements that can help us understand how a system is performing and to compare different
    
    
     
      improvement approaches.
     
    
   </p>
   <p>
    
     Here are some metrics and formulas commonly used to evaluate parallelism in
    
    
     
      a system.
     
    
   </p>
   <h2 id="_idParaDest-31">
    <a id="_idTextAnchor030">
    </a>
    
     Degree of parallelism
    
   </h2>
   <p>
    <strong class="bold">
     
      Degree of parallelism
     
    </strong>
    
     (
    
    <strong class="bold">
     
      DOP
     
    </strong>
    
     ) is a metric that indicates the number of operations being
    
    <a id="_idIndexMarker076">
    </a>
    
     simultaneously
    
    <a id="_idIndexMarker077">
    </a>
    
     executed by a computer.
    
    
     It is useful to describe the performance
    
    <strong class="bold">
    </strong>
    
     of parallel programs and
    
    
     
      multi-processor systems.
     
    
   </p>
   <p>
    
     When computing the DOP, we can use the maximum number of operations that could be done simultaneously, measuring the ideal case scenario without bottlenecks or dependencies.
    
    
     Alternatively, we can use either the average number of operations or the number of simultaneous operations at a given point in time, reflecting the actual DOP achieved by a system.
    
    
     An approximation can be done by using profilers and performance analysis tools to measure the number of threads during a particular
    
    
     
      time period.
     
    
   </p>
   <p>
    
     That means that the DOP is not a constant; it is a dynamic metric that changes during
    
    
     
      application execution.
     
    
   </p>
   <p>
    
     For example, consider a script tool that processes multiple files.
    
    
     These files can be processed sequentially or simultaneously, increasing efficiency.
    
    
     If we have a machine with
    
    <strong class="source-inline">
     
      N
     
    </strong>
    
     cores and we want to process
    
    <strong class="source-inline">
     
      N
     
    </strong>
    
     files, we can assign a file to
    
    
     
      each core.
     
    
   </p>
   <p>
    
     The time
    
    <a id="_idIndexMarker078">
    </a>
    
     to process all files sequentially would be
    
    
     
      as follows:
     
    
   </p>
   <p>
    
     <img alt="&lt;mml:math   display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mo&gt;⋯&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;≅&lt;/mml:mo&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/1.png" style="vertical-align:-0.533em;height:1.181em;width:18.808em"/>
    
   </p>
   <p>
    
     And, the time
    
    <a id="_idIndexMarker079">
    </a>
    
     to process them in parallel
    
    
     
      would be:
     
    
   </p>
   <p>
    
     <img alt="&lt;mml:math   display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;⋯&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/2.png" style="vertical-align:-0.533em;height:1.172em;width:12.905em"/>
    
   </p>
   <p>
    
     Therefore, the DOP is
    
    <strong class="source-inline">
     
      N
     
    </strong>
    
     , the number of cores actively processing
    
    
     
      separate files.
     
    
   </p>
   <p>
    
     There is a theoretical upper bound on the speed-up that parallelization can achieve, which is given by
    
    
     <strong class="bold">
      
       Amdahl’s law
      
     </strong>
    
    
     
      .
     
    
   </p>
   <h2 id="_idParaDest-32">
    <a id="_idTextAnchor031">
    </a>
    
     Amdahl’s law
    
   </h2>
   <p>
    
     In a parallel system, we could believe that doubling the number of CPU cores could make the
    
    <a id="_idIndexMarker080">
    </a>
    
     program run twice as fast, thereby halving the runtime.
    
    
     However, the speed-up from parallelization is not linear.
    
    
     After a
    
    <a id="_idIndexMarker081">
    </a>
    
     certain number of cores, the runtime is not reduced anymore due to different circumstances such as context switching, memory paging, and
    
    
     
      so on.
     
    
   </p>
   <p>
    
     The Amdahl’s law formula computes the theoretical maximum speed-up a task can perform after parallelization
    
    
     
      as follows:
     
    
   </p>
   <p>
    
     <img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/3.png" style="vertical-align:-0.772em;height:1.737em;width:12.435em"/>
    
   </p>
   <p>
    
     Here,
    
    <strong class="source-inline">
     
      s
     
    </strong>
    
     is the speed-up factor of the improved part and
    
    <strong class="source-inline">
     
      p
     
    </strong>
    
     is the fraction of the parallelizable part compared to the entire process.
    
    
     Therefore,
    
    <strong class="source-inline">
     
      1-p
     
    </strong>
    
     represents the ratio of the task not parallelizable (the bottleneck or sequential part), while
    
    <strong class="source-inline">
     
      p/s
     
    </strong>
    
     represents the speed-up achieved by the
    
    
     
      parallelizable part.
     
    
   </p>
   <p>
    
     That means that the maximum speed-up is limited by the sequential portion of the task.
    
    
     The greater the fraction of the parallelizable task (
    
    <strong class="source-inline">
     
      p
     
    </strong>
    
     approaches
    
    <strong class="source-inline">
     
      1
     
    </strong>
    
     ), the more the maximum
    
    <a id="_idIndexMarker082">
    </a>
    
     speed-up increases up to the
    
    <a id="_idIndexMarker083">
    </a>
    
     speed-up factor (
    
    <strong class="source-inline">
     
      s
     
    </strong>
    
     ).
    
    
     On the other hand, when the sequential portion becomes larger (
    
    <strong class="source-inline">
     
      p
     
    </strong>
    
     approaches
    
    <strong class="source-inline">
     
      0
     
    </strong>
    
     ),
    
    <strong class="source-inline">
     
      Smax
     
    </strong>
    
     tends to
    
    <strong class="source-inline">
     
      1
     
    </strong>
    
     , meaning that no improvement
    
    
     
      is possible.
     
    
   </p>
   <div><div><img alt="Figure 1.9: The speed-up limit by the number of processors and percentage of parallelizable parts" src="img/B22219_01_9.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 1.9: The speed-up limit by the number of processors and percentage of parallelizable parts
    
   </p>
   <p>
    
     The
    
    <strong class="bold">
     
      critical path
     
    </strong>
    
     in parallel
    
    <a id="_idIndexMarker084">
    </a>
    
     systems is defined by the longest chain of dependent calculations.
    
    
     As the critical path is hardly parallelizable, it defines the sequential portion and thus the quicker runtime that a program
    
    
     
      can achieve.
     
    
   </p>
   <p>
    
     For example, if the sequential part of a process represents 10% of the runtime, then the fraction of the parallelizable part is
    
    <strong class="source-inline">
     
      p=0.9.
     
    </strong>
    
     In this case, the potential speed-up will not exceed 10 times the speed-up, regardless of the number of
    
    
     
      processors available.
     
    
   </p>
   <h2 id="_idParaDest-33">
    <a id="_idTextAnchor032">
    </a>
    
     Gustafson’s law
    
   </h2>
   <p>
    
     The Amdahl’s law formula can only be used with fixed-sized problems and increasing resources.
    
    
     When using larger datasets, time spent in the parallelizable part grows much faster
    
    <a id="_idIndexMarker085">
    </a>
    
     than that in the sequential part.
    
    
     In these cases, the Gustafson’s law
    
    <a id="_idIndexMarker086">
    </a>
    
     formula is less pessimistic and more accurate, as it accounts for fixed execution time and increasing problem size with
    
    
     
      additional resources.
     
    
   </p>
   <p>
    
     The Gustafson’s law formula computes the speed-up gained by using
    
    <strong class="source-inline">
     
      p
     
    </strong>
    
     processors
    
    
     
      as follows:
     
    
   </p>
   <p>
    
     <img alt="&lt;mml:math   display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/4.png" style="vertical-align:-0.482em;height:1.243em;width:7.604em"/>
    
   </p>
   <p>
    
     Here,
    
    <strong class="source-inline">
     
      p
     
    </strong>
    
     is the number of processors and
    
    <strong class="source-inline">
     
      f
     
    </strong>
    
     is the fraction of the task that remains sequential.
    
    
     Therefore,
    
    <strong class="source-inline">
     
      (1-f)*p
     
    </strong>
    
     represents the speed-up achieved with the parallelization of the
    
    <strong class="source-inline">
     
      (1-f)
     
    </strong>
    
     task distributed across
    
    <strong class="source-inline">
     
      p
     
    </strong>
    
     processors, and
    
    <strong class="source-inline">
     
      p
     
    </strong>
    
     represents the extra work done when
    
    
     
      increasing resources.
     
    
   </p>
   <p>
    
     Gustafson’s law formula shows that the speed-up is affected by parallelization when lowering
    
    <strong class="source-inline">
     
      f
     
    </strong>
    
     and by scalability by
    
    
     
      increasing
     
    
    
     <strong class="source-inline">
      
       p
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     As with Amdahl’s law, Gustafson’s law formula is an approximation that provides valuable perspective when measuring improvements in parallel systems.
    
    
     Other factors can reduce efficiency such as overhead communication between processors or memory and
    
    
     
      storage limitations.
     
    
   </p>
   <h1 id="_idParaDest-34">
    <a id="_idTextAnchor033">
    </a>
    
     Summary
    
   </h1>
   <p>
    
     In this chapter, we learned about the different architectures and models we can use to build parallel systems.
    
    
     Then we explored the details of the variety of parallel programming paradigms available to develop parallel software and learned about their behavior and nuances.
    
    
     Finally, we defined some useful metrics to measure the performance of
    
    
     
      parallel programs.
     
    
   </p>
   <p>
    
     In the next chapter, we will explore the relationship between hardware and software, as well as how software maps and interacts with the underlying hardware.
    
    
     We will also learn what threads, processes, and services are, how threads are scheduled, and how they communicate with each other.
    
    
     Furthermore, we will cover inter-process communication and
    
    
     
      much more.
     
    
   </p>
   <h1 id="_idParaDest-35">
    <a id="_idTextAnchor034">
    </a>
    
     Further reading
    
   </h1>
   <ul>
    <li>
     
      Topological
     
     
      
       sorting:
      
     
     <a href="https://en.wikipedia.org/wiki/Topological_sorting">
      
       
        https://en.wikipedia.org/wiki/Topological_sorting
       
      
     </a>
    </li>
    <li>
     
      C++ compiler
     
     
      
       support:
      
     
     <a href="https://en.cppreference.com/w/cpp/compiler_support">
      
       
        https://en.cppreference.com/w/cpp/compiler_support
       
      
     </a>
    </li>
    <li>
     
      C++20 compiler
     
     
      
       support:
      
     
     <a href="https://en.cppreference.com/w/cpp/compiler_support/20">
      
       
        https://en.cppreference.com/w/cpp/compiler_support/20
       
      
     </a>
    </li>
    <li>
     
      C++23 compiler
     
     
      
       support:
      
     
     <a href="https://en.cppreference.com/w/cpp/compiler_support/23">
      
       
        https://en.cppreference.com/w/cpp/compiler_support/23
       
      
     </a>
    </li>
   </ul>
  </div>
 </body></html>