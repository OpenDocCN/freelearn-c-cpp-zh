<html><head></head><body>
  <div id="_idContainer023">
   <h1 class="chapter-number" id="_idParaDest-15">
    <a id="_idTextAnchor014">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     1
    </span>
   </h1>
   <h1 id="_idParaDest-16">
    <a id="_idTextAnchor015">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     Parallel Programming Paradigms
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.3.1">
     Before we dive into
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.4.1">
      parallel programming
     </span>
    </strong>
    <span class="koboSpan" id="kobo.5.1">
     using C++, throughout the first two chapters, we will
    </span>
    <a id="_idIndexMarker000">
    </a>
    <span class="koboSpan" id="kobo.6.1">
     focus on acquiring some foundational knowledge about the different approaches to building parallel software and how the software interacts with the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.7.1">
      machine hardware.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.8.1">
     In this chapter, we will introduce parallel programming and the different paradigms and models that we can use when developing efficient, responsive, and scalable concurrent and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.9.1">
      asynchronous software.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.10.1">
     There are many ways to group concepts and methods when classifying the different approaches we can take to develop parallel software.
    </span>
    <span class="koboSpan" id="kobo.10.2">
     As we are focusing on software built with C++ in this book, we can divide the different parallel programming paradigms as follows: concurrency, asynchronous programming, parallel programming, reactive programming, dataflows, multithreading programming, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.11.1">
      event-driven programming.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.12.1">
     Depending on the problem at hand, a specific paradigm could be more suitable than others to solve a given scenario.
    </span>
    <span class="koboSpan" id="kobo.12.2">
     Understanding the different paradigms will help us to analyze the problem and narrow down the best
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.13.1">
      solution possible.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.14.1">
     In this chapter, we’re going to cover the following
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.15.1">
      main topics:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.16.1">
      What is parallel programming and why does
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.17.1">
       it matter?
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.18.1">
      What are the different parallel programming paradigms and why do we need to
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.19.1">
       understand them?
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.20.1">
      What will you learn in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.21.1">
       this book?
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-17">
    <a id="_idTextAnchor016">
    </a>
    <span class="koboSpan" id="kobo.22.1">
     Technical requirements
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.23.1">
     No technical requirements apply for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.24.1">
      this chapter.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.25.1">
     Throughout the book, we will develop different solutions using C++20 and, in some examples, C++23.
    </span>
    <span class="koboSpan" id="kobo.25.2">
     Therefore, we will need to install GCC 14 and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.26.1">
      Clang 8.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.27.1">
     All the code blocks shown in this book can be found in the following GitHub
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.28.1">
      repository:
     </span>
    </span>
    <a href="https://github.com/PacktPublishing/Asynchronous-Programming-with-CPP">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.29.1">
       https://github.com/PacktPublishing/Asynchronous-Programming-with-CPP
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.30.1">
      .
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-18">
    <a id="_idTextAnchor017">
    </a>
    <span class="koboSpan" id="kobo.31.1">
     Getting to know classifications, techniques, and models
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.32.1">
     Parallel computing occurs when tasks or computations are done simultaneously, with a task being a unit of execution or unit of work in a software application.
    </span>
    <span class="koboSpan" id="kobo.32.2">
     As there are many ways to achieve parallelism, understanding the different approaches will be helpful to write efficient parallel algorithms.
    </span>
    <span class="koboSpan" id="kobo.32.3">
     These approaches are described via paradigms
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.33.1">
      and models.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.34.1">
     But first, let us start by classifying the different parallel
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.35.1">
      computing systems.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-19">
    <a id="_idTextAnchor018">
    </a>
    <span class="koboSpan" id="kobo.36.1">
     Systems classification and techniques
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.37.1">
     One of
    </span>
    <a id="_idIndexMarker001">
    </a>
    <span class="koboSpan" id="kobo.38.1">
     the earliest classifications of parallel computing systems was made by Michael J.
    </span>
    <span class="koboSpan" id="kobo.38.2">
     Flynn in 1966.
    </span>
    <span class="koboSpan" id="kobo.38.3">
     Flynn’s taxonomy defines the following classification
    </span>
    <a id="_idIndexMarker002">
    </a>
    <span class="koboSpan" id="kobo.39.1">
     based on the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.40.1">
      data streams
     </span>
    </strong>
    <span class="koboSpan" id="kobo.41.1">
     and number of instructions a parallel computing architecture
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.42.1">
      can handle:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.43.1">
       Single-instruction-single-data (SISD) systems
      </span>
     </strong>
     <span class="koboSpan" id="kobo.44.1">
      : Define
     </span>
     <a id="_idIndexMarker003">
     </a>
     <span class="koboSpan" id="kobo.45.1">
      a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.46.1">
       sequential program
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.47.1">
       Single-instruction-multiple-data (SIMD) systems
      </span>
     </strong>
     <span class="koboSpan" id="kobo.48.1">
      : Where operations are done
     </span>
     <a id="_idIndexMarker004">
     </a>
     <span class="koboSpan" id="kobo.49.1">
      over a large dataset, for example in signal processing of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.50.1">
       GPU computing
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.51.1">
       Multiple-instructions-single-data (MISD) systems
      </span>
     </strong>
     <span class="koboSpan" id="kobo.52.1">
      :
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.53.1">
       Rarely
      </span>
     </span>
     <span class="No-Break">
      <a id="_idIndexMarker005">
      </a>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.54.1">
       used
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.55.1">
       Multiple-instructions-multiple-data (MIMD) systems
      </span>
     </strong>
     <span class="koboSpan" id="kobo.56.1">
      : The most common parallel
     </span>
     <a id="_idIndexMarker006">
     </a>
     <span class="koboSpan" id="kobo.57.1">
      architectures based in multicore and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.58.1">
       multi-processor computers
      </span>
     </span>
    </li>
   </ul>
   <div>
    <div class="IMG---Figure" id="_idContainer010">
     <span class="koboSpan" id="kobo.59.1">
      <img alt="Figure 1.1: Flynn’s taxonomy" src="image/B22219_01_1.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.60.1">
     Figure 1.1: Flynn’s taxonomy
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.61.1">
     This book
    </span>
    <a id="_idIndexMarker007">
    </a>
    <span class="koboSpan" id="kobo.62.1">
     is not only about building software with C++ but also about keeping an eye on how it interacts with the underlying hardware.
    </span>
    <span class="koboSpan" id="kobo.62.2">
     A more interesting division or taxonomy can probably be done at the software level, where we can define the techniques.
    </span>
    <span class="koboSpan" id="kobo.62.3">
     We will learn about these in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.63.1">
      subsequent sections.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.64.1">
     Data parallelism
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.65.1">
     Many different data units are processed in parallel by the same program or sequence of instructions
    </span>
    <a id="_idIndexMarker008">
    </a>
    <span class="koboSpan" id="kobo.66.1">
     running in different processing units such as CPU or
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.67.1">
      GPU cores.
     </span>
    </span>
   </p>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.68.1">
      Data parallelism
     </span>
    </strong>
    <span class="koboSpan" id="kobo.69.1">
     is achieved by how many disjoint datasets can be processed at the same time
    </span>
    <a id="_idIndexMarker009">
    </a>
    <span class="koboSpan" id="kobo.70.1">
     by the same operations.
    </span>
    <span class="koboSpan" id="kobo.70.2">
     Large datasets can be divided into smaller and independent data chunks
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.71.1">
      exploiting parallelism.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.72.1">
     This technique is also highly scalable, as adding more processing units allows for the processing of a higher volume
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.73.1">
      of data.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.74.1">
     In this subset, we can include SIMD instruction sets such as SSE, AVX, VMX, or NEON, which are accessible via intrinsic functions in C++.
    </span>
    <span class="koboSpan" id="kobo.74.2">
     Also, libraries such as OpenMP and CUDA for NVIDIA GPUs.
    </span>
    <span class="koboSpan" id="kobo.74.3">
     Some examples of its usage can be found in machine learning training and image processing.
    </span>
    <span class="koboSpan" id="kobo.74.4">
     This technique is related to the SIMD taxonomy defined
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.75.1">
      by Flynn.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.76.1">
     As usual, there are
    </span>
    <a id="_idIndexMarker010">
    </a>
    <span class="koboSpan" id="kobo.77.1">
     also some drawbacks.
    </span>
    <span class="koboSpan" id="kobo.77.2">
     Data must be easily divisible into
    </span>
    <a id="_idIndexMarker011">
    </a>
    <span class="koboSpan" id="kobo.78.1">
     independent chunks.
    </span>
    <span class="koboSpan" id="kobo.78.2">
     This data division and posterior merging also introduces some overhead that can reduce the benefits
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.79.1">
      of parallelization.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.80.1">
     Task parallelism
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.81.1">
     In computers where each CPU core runs different tasks using processes or threads,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.82.1">
      task parallelism
     </span>
    </strong>
    <span class="koboSpan" id="kobo.83.1">
     can be achieved when these tasks simultaneously receive data, process it, and send back the results that they generate via
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.84.1">
      message passing.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.85.1">
     The advantage
    </span>
    <a id="_idIndexMarker012">
    </a>
    <span class="koboSpan" id="kobo.86.1">
     of task parallelism
    </span>
    <a id="_idIndexMarker013">
    </a>
    <span class="koboSpan" id="kobo.87.1">
     resides in the ability to design heterogeneous and granular tasks that can make better usage of processing resources, being more flexible when designing a solution with potentially
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.88.1">
      higher speed-up.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.89.1">
     Due to the possible dependencies between tasks that can be created by the data, as well as the different nature of each task, scheduling and coordination are more complex than with data parallelism.
    </span>
    <span class="koboSpan" id="kobo.89.2">
     Also, task creation adds some
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.90.1">
      processing overhead.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.91.1">
     Here we can include Flynn’s MISD and MIMD taxonomies.
    </span>
    <span class="koboSpan" id="kobo.91.2">
     Some examples can be found in a web server request processing system or a user interface
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.92.1">
      events handler.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.93.1">
     Stream parallelism
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.94.1">
     A continuous sequence of data elements, also known as a
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.95.1">
      data stream
     </span>
    </strong>
    <span class="koboSpan" id="kobo.96.1">
     , can be processed
    </span>
    <a id="_idIndexMarker014">
    </a>
    <span class="koboSpan" id="kobo.97.1">
     concurrently by dividing the computation into various stages processing a subset of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.98.1">
      the data.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.99.1">
     Stages
    </span>
    <a id="_idIndexMarker015">
    </a>
    <span class="koboSpan" id="kobo.100.1">
     can run concurrently.
    </span>
    <span class="koboSpan" id="kobo.100.2">
     Some
    </span>
    <a id="_idIndexMarker016">
    </a>
    <span class="koboSpan" id="kobo.101.1">
     generate the input of other stages, building a
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.102.1">
      pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.103.1">
     from stage dependencies.
    </span>
    <span class="koboSpan" id="kobo.103.2">
     A processing stage can send results to the next stage without waiting to receive the entire
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.104.1">
      stream data.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.105.1">
     Stream parallel techniques are effective when handling continuous data.
    </span>
    <span class="koboSpan" id="kobo.105.2">
     They are also highly scalable, as they can be scaled by adding more processing units to handle the extra input data.
    </span>
    <span class="koboSpan" id="kobo.105.3">
     Since the stream data is processed as it arrives, this means not needing to wait for the entire data stream to be sent, which means that memory usage is
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.106.1">
      also reduced.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.107.1">
     However, as usual, there are some drawbacks.
    </span>
    <span class="koboSpan" id="kobo.107.2">
     These systems are more complex to implement due to their processing logic, error handling, and recovery.
    </span>
    <span class="koboSpan" id="kobo.107.3">
     As we might also need
    </span>
    <a id="_idIndexMarker017">
    </a>
    <span class="koboSpan" id="kobo.108.1">
     to process the data stream
    </span>
    <a id="_idIndexMarker018">
    </a>
    <span class="koboSpan" id="kobo.109.1">
     in real time, the hardware could be a limitation
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.110.1">
      as well.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.111.1">
     Some examples of these systems include monitoring systems, sensor data processing, and audio and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.112.1">
      video streaming.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.113.1">
     Implicit parallelism
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.114.1">
     In this case, the compiler, the runtime, or the hardware takes care of parallelizing the execution
    </span>
    <a id="_idIndexMarker019">
    </a>
    <span class="koboSpan" id="kobo.115.1">
     of the instructions transparently for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.116.1">
      the programmer.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.117.1">
     This makes
    </span>
    <a id="_idIndexMarker020">
    </a>
    <span class="koboSpan" id="kobo.118.1">
     it easier to write parallel programs but limits the programmer’s control over the strategies used, or even makes it more difficult to analyze performance
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.119.1">
      or debugging.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.120.1">
     Now that we have a better understanding of the different parallel systems and techniques, it’s time to learn about the different models we can use when designing a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.121.1">
      parallel program.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-20">
    <a id="_idTextAnchor019">
    </a>
    <span class="koboSpan" id="kobo.122.1">
     Parallel programming models
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.123.1">
     A
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.124.1">
      parallel programming model
     </span>
    </strong>
    <span class="koboSpan" id="kobo.125.1">
     is a parallel computer’s architecture used to express algorithms
    </span>
    <a id="_idIndexMarker021">
    </a>
    <span class="koboSpan" id="kobo.126.1">
     and build programs.
    </span>
    <span class="koboSpan" id="kobo.126.2">
     The more generic the model the more valuable it becomes, as it can be used in a broader range of scenarios.
    </span>
    <span class="koboSpan" id="kobo.126.3">
     In that
    </span>
    <a id="_idIndexMarker022">
    </a>
    <span class="koboSpan" id="kobo.127.1">
     sense, C++ implements a parallel model through a library within the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.128.1">
      Standard Template Library
     </span>
    </strong>
    <span class="koboSpan" id="kobo.129.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.130.1">
      STL
     </span>
    </strong>
    <span class="koboSpan" id="kobo.131.1">
     ), which can be used to achieve parallel execution of programs from
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.132.1">
      sequential applications.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.133.1">
     These models describe how the different tasks interact during the program’s lifetime to achieve a result from input data.
    </span>
    <span class="koboSpan" id="kobo.133.2">
     Their main differences are related to how the tasks interact with each other and how they process the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.134.1">
      incoming data.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.135.1">
     Phase parallel
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.136.1">
     In
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.137.1">
      phase parallel
     </span>
    </strong>
    <span class="koboSpan" id="kobo.138.1">
     , also known as the agenda or loosely synchronous paradigm, multiple jobs
    </span>
    <a id="_idIndexMarker023">
    </a>
    <span class="koboSpan" id="kobo.139.1">
     or tasks perform independent computations in parallel.
    </span>
    <span class="koboSpan" id="kobo.139.2">
     At some point, the program needs to perform a synchronous
    </span>
    <a id="_idIndexMarker024">
    </a>
    <span class="koboSpan" id="kobo.140.1">
     interaction operation using a barrier to synchronize the different processes.
    </span>
    <span class="koboSpan" id="kobo.140.2">
     A barrier is a synchronization mechanism that ensures that a group of tasks reach a particular point in their execution before any of them can proceed further.
    </span>
    <span class="koboSpan" id="kobo.140.3">
     The next steps execute other asynchronous operations, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.141.1">
      so on.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer011">
     <span class="koboSpan" id="kobo.142.1">
      <img alt="Figure 1.2: Phase parallel model" src="image/B22219_01_2.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.143.1">
     Figure 1.2: Phase parallel model
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.144.1">
     The advantage of this model is that the interaction between tasks does not overlap with computation.
    </span>
    <span class="koboSpan" id="kobo.144.2">
     On the other hand, it is difficult to reach a balanced workload and throughput among all
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.145.1">
      processing units.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.146.1">
     Divide and conquer
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.147.1">
     The application using this model uses a main task or job that divides the workload among
    </span>
    <a id="_idIndexMarker025">
    </a>
    <span class="koboSpan" id="kobo.148.1">
     its children, assigning them to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.149.1">
      smaller tasks.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.150.1">
     Child tasks compute the results in parallel and return them to the parent task, where the partial results are merged into the final one.
    </span>
    <span class="koboSpan" id="kobo.150.2">
     Child tasks can also subdivide the assigned task into even smaller ones and create their own
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.151.1">
      child tasks.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.152.1">
     This model has the same disadvantage as the phase parallel model; it is difficult to achieve a good
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.153.1">
      load balance.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer012">
     <span class="koboSpan" id="kobo.154.1">
      <img alt="Figure 1.3: Divide and conquer model" src="image/B22219_01_3.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.155.1">
     Figure 1.3: Divide and conquer model
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.156.1">
     In
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.157.1">
       Figure 1
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.158.1">
      .3
     </span>
    </em>
    <span class="koboSpan" id="kobo.159.1">
     , we can see
    </span>
    <a id="_idIndexMarker026">
    </a>
    <span class="koboSpan" id="kobo.160.1">
     how the main job divides the work among several child tasks, and how
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.161.1">
      Child Task 2
     </span>
    </strong>
    <span class="koboSpan" id="kobo.162.1">
     , in turn, subdivides its assigned work into two
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.163.1">
      additional tasks.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.164.1">
     Pipeline
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.165.1">
     Several
    </span>
    <a id="_idIndexMarker027">
    </a>
    <span class="koboSpan" id="kobo.166.1">
     tasks are interconnected, building a virtual pipeline.
    </span>
    <span class="koboSpan" id="kobo.166.2">
     In this pipeline, the various stages can run simultaneously, overlapping their execution when fed
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.167.1">
      with data.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer013">
     <span class="koboSpan" id="kobo.168.1">
      <img alt="Figure 1.4: Pipeline model" src="image/B22219_01_4.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.169.1">
     Figure 1.4: Pipeline model
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.170.1">
     In the preceding figure, three tasks interact in a pipeline composed of five stages.
    </span>
    <span class="koboSpan" id="kobo.170.2">
     In each stage, some tasks are running, generating output results that are used by other tasks in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.171.1">
      next stages.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.172.1">
     Master-slave
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.173.1">
     Using the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.174.1">
      master-slave model
     </span>
    </strong>
    <span class="koboSpan" id="kobo.175.1">
     , also
    </span>
    <a id="_idIndexMarker028">
    </a>
    <span class="koboSpan" id="kobo.176.1">
     known as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.177.1">
      process farm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.178.1">
     , a master
    </span>
    <a id="_idIndexMarker029">
    </a>
    <span class="koboSpan" id="kobo.179.1">
     job executes the sequential part of the algorithm and spawns
    </span>
    <a id="_idIndexMarker030">
    </a>
    <span class="koboSpan" id="kobo.180.1">
     and coordinates slave tasks that execute parallel operations in the workload.
    </span>
    <span class="koboSpan" id="kobo.180.2">
     When a slave task finishes its computation, it informs the master job of the result, which might then send more data to the slave task to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.181.1">
      be processed.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer014">
     <span class="koboSpan" id="kobo.182.1">
      <img alt="Figure 1.5: The master-slave model" src="image/B22219_01_5.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.183.1">
     Figure 1.5: The master-slave model
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.184.1">
     The main disadvantage is that the master can become a bottleneck if it needs to deal with too many slaves or when tasks are too small.
    </span>
    <span class="koboSpan" id="kobo.184.2">
     There is a tradeoff when selecting the amount of work to
    </span>
    <a id="_idIndexMarker031">
    </a>
    <span class="koboSpan" id="kobo.185.1">
     be performed by each task, also known as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.186.1">
      granularity
     </span>
    </strong>
    <span class="koboSpan" id="kobo.187.1">
     .
    </span>
    <span class="koboSpan" id="kobo.187.2">
     When tasks are small, they are named fine-grained, and when they are large, they
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.188.1">
      are coarse-grained.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.189.1">
     Work pool
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.190.1">
     In the work
    </span>
    <a id="_idIndexMarker032">
    </a>
    <span class="koboSpan" id="kobo.191.1">
     pool model, a global structure holds
    </span>
    <a id="_idIndexMarker033">
    </a>
    <span class="koboSpan" id="kobo.192.1">
     a pool of work items to do.
    </span>
    <span class="koboSpan" id="kobo.192.2">
     Then, the main program creates jobs that fetch pieces of work from the pool to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.193.1">
      execute them.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.194.1">
     These jobs can generate more work units that are inserted into the work pool.
    </span>
    <span class="koboSpan" id="kobo.194.2">
     The parallel program finishes its execution when all work units are completed and the pool is
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.195.1">
      thus empty.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer015">
     <span class="koboSpan" id="kobo.196.1">
      <img alt="Figure 1.6: The work pool model" src="image/B22219_01_6.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.197.1">
     Figure 1.6: The work pool model
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.198.1">
     This
    </span>
    <a id="_idIndexMarker034">
    </a>
    <span class="koboSpan" id="kobo.199.1">
     mechanism facilitates load balancing
    </span>
    <a id="_idIndexMarker035">
    </a>
    <span class="koboSpan" id="kobo.200.1">
     among free
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.201.1">
      processing units.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.202.1">
     In C++, this pool is usually implemented by using an unordered set, a queue, or a priority queue.
    </span>
    <span class="koboSpan" id="kobo.202.2">
     We will implement some examples in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.203.1">
      this book.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.204.1">
     Now that we have learned about a variety of models that we can use to build a parallel system, let’s explore the different parallel programming paradigms available to develop software that efficiently runs tasks
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.205.1">
      in parallel.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-21">
    <a id="_idTextAnchor020">
    </a>
    <span class="koboSpan" id="kobo.206.1">
     Understanding various parallel programming paradigms
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.207.1">
     Now that
    </span>
    <a id="_idIndexMarker036">
    </a>
    <span class="koboSpan" id="kobo.208.1">
     we have explored some of the different models used for building parallel programs, it is time to move to a more abstract classification and learn about the fundamental styles or principles of how to code parallel programs by exploring the different parallel programming
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.209.1">
      language paradigms.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-22">
    <a id="_idTextAnchor021">
    </a>
    <span class="koboSpan" id="kobo.210.1">
     Synchronous programming
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.211.1">
     A
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.212.1">
      synchronous programming
     </span>
    </strong>
    <span class="koboSpan" id="kobo.213.1">
     language is used to build programs where code is executed
    </span>
    <a id="_idIndexMarker037">
    </a>
    <span class="koboSpan" id="kobo.214.1">
     in a strict sequential order.
    </span>
    <span class="koboSpan" id="kobo.214.2">
     While one instruction is being executed, the program remains blocked until the
    </span>
    <a id="_idIndexMarker038">
    </a>
    <span class="koboSpan" id="kobo.215.1">
     instruction finishes.
    </span>
    <span class="koboSpan" id="kobo.215.2">
     In other words, there is no multitasking.
    </span>
    <span class="koboSpan" id="kobo.215.3">
     This makes the code easier to understand
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.216.1">
      and debug.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.217.1">
     However, this behavior makes the program unresponsive to external events while it is blocked while running an instruction and difficult
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.218.1">
      to scale.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.219.1">
     This is the traditional paradigm used by most programming languages such as C, Python,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.220.1">
      or Java.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.221.1">
     This paradigm is especially useful for reactive or embedded systems that need to respond in real time and in an ordered way to input events.
    </span>
    <span class="koboSpan" id="kobo.221.2">
     The processing speed must match the one imposed by the environment with strict
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.222.1">
      time bounds.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer016">
     <span class="koboSpan" id="kobo.223.1">
      <img alt="Figure 1.7: Asynchronous versus synchronous execution time" src="image/B22219_01_7.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.224.1">
     Figure 1.7: Asynchronous versus synchronous execution time
    </span>
   </p>
   <p>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.225.1">
       Figure 1
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.226.1">
      .7
     </span>
    </em>
    <span class="koboSpan" id="kobo.227.1">
     shows two tasks running in a system.
    </span>
    <span class="koboSpan" id="kobo.227.2">
     In the synchronous system, task A is interrupted by task B and only resumes its execution once task B has finished its work.
    </span>
    <span class="koboSpan" id="kobo.227.3">
     In the asynchronous system, tasks A and B can run simultaneously, thus completing both of their work in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.228.1">
      less time.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-23">
    <a id="_idTextAnchor022">
    </a>
    <span class="koboSpan" id="kobo.229.1">
     Concurrency programming
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.230.1">
     With
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.231.1">
      concurrency programming
     </span>
    </strong>
    <span class="koboSpan" id="kobo.232.1">
     , more than one task can run at the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.233.1">
      same time.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.234.1">
     Tasks can
    </span>
    <a id="_idIndexMarker039">
    </a>
    <span class="koboSpan" id="kobo.235.1">
     run independently without waiting for other tasks’ instructions to finish.
    </span>
    <span class="koboSpan" id="kobo.235.2">
     They can also share resources and communicate with each other.
    </span>
    <span class="koboSpan" id="kobo.235.3">
     Their instructions can run asynchronously, meaning that they can be executed
    </span>
    <a id="_idIndexMarker040">
    </a>
    <span class="koboSpan" id="kobo.236.1">
     in any order without affecting the outcome, adding the potential for parallel processing.
    </span>
    <span class="koboSpan" id="kobo.236.2">
     On the other hand, that makes this kind of program more difficult to understand
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.237.1">
      and debug.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.238.1">
     Concurrency increases the program throughput, as the number of tasks completed in a time interval increases with concurrency (see the formula for Gustafson’s law in the section
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.239.1">
      Exploring the metrics to assess parallelism
     </span>
    </em>
    <span class="koboSpan" id="kobo.240.1">
     at the end of this chapter).
    </span>
    <span class="koboSpan" id="kobo.240.2">
     Also, it achieves better input and output responsiveness, as the program can perform other tasks during
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.241.1">
      waiting periods.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.242.1">
     The main problem in concurrent software is achieving correct concurrency control.
    </span>
    <span class="koboSpan" id="kobo.242.2">
     Exceptional care must be taken when coordinating access to shared resources and ensuring that the correct sequence of interactions is taking place between the different computational executions.
    </span>
    <span class="koboSpan" id="kobo.242.3">
     Incorrect decisions can lead to race conditions, deadlocks, or resource starvation, which are explained in depth in
    </span>
    <a href="B22219_03.xhtml#_idTextAnchor051">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.243.1">
        Chapter 3
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.244.1">
     and
    </span>
    <a href="B22219_04.xhtml#_idTextAnchor074">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.245.1">
        Chapter 4
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.246.1">
     .
    </span>
    <span class="koboSpan" id="kobo.246.2">
     Most of these issues are solved by following a consistency or memory model, which defines rules on how and in which order operations should be performed when accessing the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.247.1">
      shared memory.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.248.1">
     Designing efficient concurrent algorithms is done by finding techniques to coordinate tasks’ execution, data exchange, memory allocations, and scheduling to minimize the response time and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.249.1">
      maximize throughput.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.250.1">
     The first academic paper introducing concurrency,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.251.1">
      Solution of a Problem in Concurrent Programming Control
     </span>
    </em>
    <span class="koboSpan" id="kobo.252.1">
     , was published by Dijkstra in 1965.
    </span>
    <span class="koboSpan" id="kobo.252.2">
     Mutual exclusion was also identified and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.253.1">
      solved there.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.254.1">
     Concurrency can happen at the operating system level in a preemptive way, whereby the scheduler switches contexts (switching from one task to another) without interacting with the tasks.
    </span>
    <span class="koboSpan" id="kobo.254.2">
     It can also happen in a non-preemptive or cooperative way, whereby the task yields control to the scheduler, which chooses another task to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.255.1">
      continue work.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.256.1">
     The scheduler interrupts the running program by saving its state (memory and register contents), then loading the saved state of a resumed program and transferring control to it.
    </span>
    <span class="koboSpan" id="kobo.256.2">
     This is
    </span>
    <a id="_idIndexMarker041">
    </a>
    <span class="koboSpan" id="kobo.257.1">
     called
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.258.1">
      context switching
     </span>
    </strong>
    <span class="koboSpan" id="kobo.259.1">
     .
    </span>
    <span class="koboSpan" id="kobo.259.2">
     Depending on the priority of a task, the scheduler might allow a high-priority task to use more CPU time than a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.260.1">
      low-priority one.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.261.1">
     Also, some
    </span>
    <a id="_idIndexMarker042">
    </a>
    <span class="koboSpan" id="kobo.262.1">
     special operating software such as memory protection might use special hardware to keep supervisory software undamaged by user-mode
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.263.1">
      program errors.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.264.1">
     This
    </span>
    <a id="_idIndexMarker043">
    </a>
    <span class="koboSpan" id="kobo.265.1">
     mechanism is not only used in single-core computers but also in multicore ones, allowing many more tasks to be executed than the number of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.266.1">
      available cores.
     </span>
    </span>
   </p>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.267.1">
      Preemptive multitasking
     </span>
    </strong>
    <span class="koboSpan" id="kobo.268.1">
     also allows important tasks to be scheduled earlier to deal with
    </span>
    <a id="_idIndexMarker044">
    </a>
    <span class="koboSpan" id="kobo.269.1">
     important external events quickly.
    </span>
    <span class="koboSpan" id="kobo.269.2">
     These tasks wake up and deal with the important work when the operating system sends them a signal that triggers
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.270.1">
      an interruption.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.271.1">
     Older
    </span>
    <a id="_idIndexMarker045">
    </a>
    <span class="koboSpan" id="kobo.272.1">
     versions of Mac and Windows operating systems used
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.273.1">
      non-preemptive multitasking
     </span>
    </strong>
    <span class="koboSpan" id="kobo.274.1">
     .
    </span>
    <span class="koboSpan" id="kobo.274.2">
     This is still used today on the RISC operating system.
    </span>
    <span class="koboSpan" id="kobo.274.3">
     Unix systems started to use preemptive multitasking in 1969, being a core feature of all Unix-like systems and modern Windows versions from Windows NT
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.275.1">
      3.1
     </span>
    </strong>
    <span class="koboSpan" id="kobo.276.1">
     and Windows
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.277.1">
      95 onward.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.278.1">
     Early-days CPUs could only run one path of instructions at a given time.
    </span>
    <span class="koboSpan" id="kobo.278.2">
     Parallelism was achieved by switching between instruction streams, giving the illusion of parallelism at the software level by seemingly overlapping
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.279.1">
      in execution.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.280.1">
     However, in 2005, Intel® introduced multicore processors, which allowed several instruction streams to execute at once at the hardware level.
    </span>
    <span class="koboSpan" id="kobo.280.2">
     This imposed some challenges at the time of writing software, as hardware-level concurrency now needed to be addressed
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.281.1">
      and exploited.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.282.1">
     C++ has supported concurrent programming since C++11 with the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.283.1">
      std::thread
     </span>
    </strong>
    <span class="koboSpan" id="kobo.284.1">
     library.
    </span>
    <span class="koboSpan" id="kobo.284.2">
     Earlier versions did not include any specific functionality, so programmers relied on platform-specific libraries based on the POSIX threading model in Unix systems or on proprietary Microsoft libraries in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.285.1">
      Windows systems.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.286.1">
     Now that we better understand what concurrency is, we need to distinguish between concurrency and parallelism.
    </span>
    <span class="koboSpan" id="kobo.286.2">
     Concurrency happens when many execution paths can run in overlapping time periods with interleaved execution, while parallelism happens when these tasks are executed at the same time by different CPU units, exploiting available
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.287.1">
      multicore resources.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer017">
     <span class="koboSpan" id="kobo.288.1">
      <img alt="Figure 1.8: Concurrency versus parallelism" src="image/B22219_01_8.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.289.1">
     Figure 1.8: Concurrency versus parallelism
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.290.1">
     Concurrent
    </span>
    <a id="_idIndexMarker046">
    </a>
    <span class="koboSpan" id="kobo.291.1">
     programming is considered more general than parallel programming as the latter has a predefined communication pattern while the former
    </span>
    <a id="_idIndexMarker047">
    </a>
    <span class="koboSpan" id="kobo.292.1">
     can involve arbitrary and dynamic patterns of communication and interaction
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.293.1">
      between tasks.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.294.1">
     Parallelism can exist without concurrency (without interleaved time periods) and concurrency without parallelism (by multitasking by time-sharing on a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.295.1">
      single-core CPU).
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-24">
    <a id="_idTextAnchor023">
    </a>
    <span class="koboSpan" id="kobo.296.1">
     Asynchronous programming
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.297.1">
     Asynchronous programming allows some tasks to be scheduled and run in the background
    </span>
    <a id="_idIndexMarker048">
    </a>
    <span class="koboSpan" id="kobo.298.1">
     while continuing
    </span>
    <a id="_idIndexMarker049">
    </a>
    <span class="koboSpan" id="kobo.299.1">
     to work on the current job without waiting for the scheduled tasks to finish.
    </span>
    <span class="koboSpan" id="kobo.299.2">
     When these tasks are finished, they will report their results back to the main job
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.300.1">
      or scheduler.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.301.1">
     One of the key issues of synchronous applications is that a long operation can leave the program unresponsive to further input or processing.
    </span>
    <span class="koboSpan" id="kobo.301.2">
     Asynchronous programs solve this issue by accepting new input while some operations are being executed with non-blocking tasks and the system can do more than one task at a time.
    </span>
    <span class="koboSpan" id="kobo.301.3">
     This also allows for better
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.302.1">
      resource utilization.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.303.1">
     As the tasks are executed asynchronously and they report results back when they finish, this paradigm is especially suitable for event-driven programs.
    </span>
    <span class="koboSpan" id="kobo.303.2">
     Also, it is a paradigm usually used for user interfaces, web servers, network communications, or long-running
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.304.1">
      background processing.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.305.1">
     As hardware has evolved toward multiple processing cores on a single processor chip, it has become
    </span>
    <a id="_idIndexMarker050">
    </a>
    <span class="koboSpan" id="kobo.306.1">
     mandatory to use asynchronous programming
    </span>
    <a id="_idIndexMarker051">
    </a>
    <span class="koboSpan" id="kobo.307.1">
     to take advantage of all the available compute power by running tasks in parallel across the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.308.1">
      different cores.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.309.1">
     However, asynchronous programming has its challenges, as we will explore in this book.
    </span>
    <span class="koboSpan" id="kobo.309.2">
     For example, it adds complexity, as the code is not interpreted in sequence.
    </span>
    <span class="koboSpan" id="kobo.309.3">
     This can lead to race conditions.
    </span>
    <span class="koboSpan" id="kobo.309.4">
     Also, error handling and testing are essential to ensure program stability and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.310.1">
      prevent issues.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.311.1">
     As we will learn in this book, modern C++ also provides asynchronous mechanisms such as coroutines, which are programs that can be suspended and resumed later, or futures and promises as a proxy for unknown results in asynchronous programs for synchronizing the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.312.1">
      program execution.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-25">
    <a id="_idTextAnchor024">
    </a>
    <span class="koboSpan" id="kobo.313.1">
     Parallel programming
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.314.1">
     With parallel
    </span>
    <a id="_idIndexMarker052">
    </a>
    <span class="koboSpan" id="kobo.315.1">
     programming, multiple computation tasks can be done simultaneously on multiple processing units, either with all of them in the same computer (multicore) or on multiple
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.316.1">
      computers (cluster).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.317.1">
     There are two
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.318.1">
      main approaches:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.319.1">
       Shared-memory parallelism
      </span>
     </strong>
     <span class="koboSpan" id="kobo.320.1">
      : Tasks
     </span>
     <a id="_idIndexMarker053">
     </a>
     <span class="koboSpan" id="kobo.321.1">
      can communicate
     </span>
     <a id="_idIndexMarker054">
     </a>
     <span class="koboSpan" id="kobo.322.1">
      via shared memory, a memory space accessible by
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.323.1">
       all processors
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.324.1">
       Message-passing parallelism
      </span>
     </strong>
     <span class="koboSpan" id="kobo.325.1">
      : Each
     </span>
     <a id="_idIndexMarker055">
     </a>
     <span class="koboSpan" id="kobo.326.1">
      task has its own memory space and uses message passing techniques
     </span>
     <a id="_idIndexMarker056">
     </a>
     <span class="koboSpan" id="kobo.327.1">
      to communicate
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.328.1">
       with others
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.329.1">
     As with the previous paradigms, to achieve full potential and avoid bugs or issues, parallel computing needs synchronization mechanisms to avoid tasks interfering with each other.
    </span>
    <span class="koboSpan" id="kobo.329.2">
     It also calls for load balancing the workload to reach its full potential, as well as reducing overhead when creating and managing tasks.
    </span>
    <span class="koboSpan" id="kobo.329.3">
     These needs increase design, implementation, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.330.1">
      debugging complexity.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-26">
    <a id="_idTextAnchor025">
    </a>
    <span class="koboSpan" id="kobo.331.1">
     Multithreading programming
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.332.1">
     Multithreading programming is a subset of parallel programming wherein a program is divided
    </span>
    <a id="_idIndexMarker057">
    </a>
    <span class="koboSpan" id="kobo.333.1">
     into multiple threads executing independent units within the same process.
    </span>
    <span class="koboSpan" id="kobo.333.2">
     The process, memory space, and resources are shared
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.334.1">
      between threads.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.335.1">
     As we
    </span>
    <a id="_idIndexMarker058">
    </a>
    <span class="koboSpan" id="kobo.336.1">
     already mentioned, sharing memory needs synchronization mechanisms.
    </span>
    <span class="koboSpan" id="kobo.336.2">
     On the other hand, as there is no need for inter-process communication, resource sharing
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.337.1">
      is simplified.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.338.1">
     For example, multithreading programming is usually used to achieve
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.339.1">
      graphical user interface
     </span>
    </strong>
    <span class="koboSpan" id="kobo.340.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.341.1">
      GUI
     </span>
    </strong>
    <span class="koboSpan" id="kobo.342.1">
     ) responsiveness with fluid animations, in web servers to handle multiple clients’ requests, or in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.343.1">
      data processing.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-27">
    <a id="_idTextAnchor026">
    </a>
    <span class="koboSpan" id="kobo.344.1">
     Event-driven programming
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.345.1">
     In event-driven programming, the control flow is driven by external events.
    </span>
    <span class="koboSpan" id="kobo.345.2">
     The application
    </span>
    <a id="_idIndexMarker059">
    </a>
    <span class="koboSpan" id="kobo.346.1">
     detects events in real time and responds to these by invoking the appropriate event-handling method
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.347.1">
      or callback.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.348.1">
     An event
    </span>
    <a id="_idIndexMarker060">
    </a>
    <span class="koboSpan" id="kobo.349.1">
     signals an action that needs to be taken.
    </span>
    <span class="koboSpan" id="kobo.349.2">
     This event is listened to by the event loop that continuously listens for incoming events and dispatches them to the appropriate callback, which will execute the desired action.
    </span>
    <span class="koboSpan" id="kobo.349.3">
     As the code is only executed when an action occurs, this paradigm improves efficiency with resource usage
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.350.1">
      and scalability.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.351.1">
     Event-driven programming is useful to act on actions happening in user interfaces, real-time applications, and network
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.352.1">
      connection listeners.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.353.1">
     As with many of the other paradigms, the increased complexity, synchronization, and debugging make this paradigm complex to implement
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.354.1">
      and apply.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.355.1">
     As C++ is a low-level language, techniques such as callbacks or functors are used to write the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.356.1">
      event handlers.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-28">
    <a id="_idTextAnchor027">
    </a>
    <span class="koboSpan" id="kobo.357.1">
     Reactive programming
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.358.1">
     Reactive programming deals with data streams, which are continuous flows of data or values
    </span>
    <a id="_idIndexMarker061">
    </a>
    <span class="koboSpan" id="kobo.359.1">
     over time.
    </span>
    <span class="koboSpan" id="kobo.359.2">
     A program is usually built using declarative or functional programming, defining a pipeline of operators
    </span>
    <a id="_idIndexMarker062">
    </a>
    <span class="koboSpan" id="kobo.360.1">
     and transformations applied to the stream.
    </span>
    <span class="koboSpan" id="kobo.360.2">
     These
    </span>
    <a id="_idIndexMarker063">
    </a>
    <span class="koboSpan" id="kobo.361.1">
     operations happen asynchronously using schedulers and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.362.1">
      backpressure
     </span>
    </strong>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.363.1">
      handling mechanisms.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.364.1">
     Backpressure happens when the quantity of data overwhelms the consumers and they are not able to process all of it.
    </span>
    <span class="koboSpan" id="kobo.364.2">
     To avoid a system collapse, a reactive system needs to use backpressure
    </span>
    <a id="_idIndexMarker064">
    </a>
    <span class="koboSpan" id="kobo.365.1">
     strategies to prevent system failures.
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.366.1">
     Some of these strategies include
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.367.1">
      the following:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.368.1">
      Controlling input throughput by requesting the publisher to reduce the rate of published events.
     </span>
     <span class="koboSpan" id="kobo.368.2">
      This can be achieved by following a pull strategy, where the publisher sends events only when the consumer requests them, or by limiting the number of events sent, creating a limited and controlled
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.369.1">
       push strategy.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.370.1">
      Buffering the extra data, which is especially useful when there are data bursts or a high-bandwidth transmission over a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.371.1">
       short period.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.372.1">
      Dropping some events or delaying their publication until the consumers recover from the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.373.1">
       backpressure state.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.374.1">
     Thus, reactive
    </span>
    <a id="_idIndexMarker065">
    </a>
    <span class="koboSpan" id="kobo.375.1">
     programs can be
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.376.1">
      pull-based
     </span>
    </strong>
    <span class="koboSpan" id="kobo.377.1">
     or
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.378.1">
      push-based
     </span>
    </strong>
    <span class="koboSpan" id="kobo.379.1">
     .
    </span>
    <span class="koboSpan" id="kobo.379.2">
     Pull-based
    </span>
    <a id="_idIndexMarker066">
    </a>
    <span class="koboSpan" id="kobo.380.1">
     programs implement the classic case where the events are actively pulled from the data source.
    </span>
    <span class="koboSpan" id="kobo.380.2">
     On the other hand, push-based programs push events through a signal network to reach the subscriber.
    </span>
    <span class="koboSpan" id="kobo.380.3">
     Subscribers react to changes without blocking the program, making these systems ideal for rich user interface environments where responsiveness
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.381.1">
      is crucial.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.382.1">
     Reactive programming is like an event-driven model where event streams from various sources can be transformed, filtered, processed, and so on.
    </span>
    <span class="koboSpan" id="kobo.382.2">
     Both increase code modularity and are suitable for real-time applications.
    </span>
    <span class="koboSpan" id="kobo.382.3">
     However, there are some differences,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.383.1">
      as follows:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.384.1">
      Reactive programming reacts to event streams, while event-driven programming deals with
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.385.1">
       discrete events.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.386.1">
      In event-driven programming, an event triggers a callback or event handlers.
     </span>
     <span class="koboSpan" id="kobo.386.2">
      With reactive programming, a pipeline with different transformation operators can be created whereby the data stream will flow and modify
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.387.1">
       the events.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.388.1">
     Examples of systems and software using reactive programming include the X Windows system
    </span>
    <a id="_idIndexMarker067">
    </a>
    <span class="koboSpan" id="kobo.389.1">
     and libraries such as Qt, WxWidgets, and Gtk+.
    </span>
    <span class="koboSpan" id="kobo.389.2">
     Reactive programming is also used in real-time sensors data processing
    </span>
    <a id="_idIndexMarker068">
    </a>
    <span class="koboSpan" id="kobo.390.1">
     and dashboards.
    </span>
    <span class="koboSpan" id="kobo.390.2">
     Additionally, it is applied to handling network or file I/O traffic and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.391.1">
      data processing.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.392.1">
     To reach full potential, there are some challenges to address when using reactive programming.
    </span>
    <span class="koboSpan" id="kobo.392.2">
     For example, it’s important to debug distributed dataflows and asynchronous processes or to optimize performance by fine-tuning the schedulers.
    </span>
    <span class="koboSpan" id="kobo.392.3">
     Also, the use of declarative or functional programming makes developing software by using reactive programming techniques a bit more challenging to understand
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.393.1">
      and learn.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-29">
    <a id="_idTextAnchor028">
    </a>
    <span class="koboSpan" id="kobo.394.1">
     Dataflow programming
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.395.1">
     With
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.396.1">
      dataflow programming
     </span>
    </strong>
    <span class="koboSpan" id="kobo.397.1">
     , a program is designed as a directed graph of nodes representing
    </span>
    <a id="_idIndexMarker069">
    </a>
    <span class="koboSpan" id="kobo.398.1">
     computation units and edges representing the flow
    </span>
    <a id="_idIndexMarker070">
    </a>
    <span class="koboSpan" id="kobo.399.1">
     of data.
    </span>
    <span class="koboSpan" id="kobo.399.2">
     A node only executes when there is some available data.
    </span>
    <span class="koboSpan" id="kobo.399.3">
     This paradigm was invented by Jack Dennis at MIT in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.400.1">
      the 1960s.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.401.1">
     Dataflow programming makes the code and design more readable and clearer, as it provides a visual representation of the different computation units and how they interact.
    </span>
    <span class="koboSpan" id="kobo.401.2">
     Also, independent nodes can run in parallel with dataflow programming, increasing parallelism and throughput.
    </span>
    <span class="koboSpan" id="kobo.401.3">
     So, it is like reactive programming but offers a graph-based approach and visual aid to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.402.1">
      modeling systems.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.403.1">
     To implement a dataflow program, we can use a hash table.
    </span>
    <span class="koboSpan" id="kobo.403.2">
     The key identifies a set of inputs and the value describes the task to run.
    </span>
    <span class="koboSpan" id="kobo.403.3">
     When all inputs for a given key are available, the task associated with that key is executed, generating additional input values that may trigger tasks for other keys in the hash table.
    </span>
    <span class="koboSpan" id="kobo.403.4">
     In these systems, the scheduler can find opportunities for parallelism by using a topological sort on the graph data structure, sorting the different tasks by
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.404.1">
      their interdependencies.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.405.1">
     This paradigm is usually used for large-scale data processing pipelines for machine learning, real-time analysis from sensors or financial markets data, and audio, video, and image processing systems.
    </span>
    <span class="koboSpan" id="kobo.405.2">
     Examples of software libraries using the dataflow paradigm are Apache
    </span>
    <a id="_idIndexMarker071">
    </a>
    <span class="koboSpan" id="kobo.406.1">
     Spark and TensorFlow.
    </span>
    <span class="koboSpan" id="kobo.406.2">
     In hardware, we can find examples for digital signal processing, network routing, GPU architecture, telemetry, and artificial intelligence,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.407.1">
      among others.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.408.1">
     A variant
    </span>
    <a id="_idIndexMarker072">
    </a>
    <span class="koboSpan" id="kobo.409.1">
     of dataflow programming is
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.410.1">
      incremental computing
     </span>
    </strong>
    <span class="koboSpan" id="kobo.411.1">
     , whereby
    </span>
    <a id="_idIndexMarker073">
    </a>
    <span class="koboSpan" id="kobo.412.1">
     only the outputs that depend on changed input data are recomputed.
    </span>
    <span class="koboSpan" id="kobo.412.2">
     This is like recomputing affected cells in an Excel spreadsheet when a cell
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.413.1">
      value changes.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.414.1">
     Now that we have learned about the different parallel programming systems, models, and paradigms, it’s time to
    </span>
    <a id="_idIndexMarker074">
    </a>
    <span class="koboSpan" id="kobo.415.1">
     introduce some
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.416.1">
      metrics
     </span>
    </strong>
    <span class="koboSpan" id="kobo.417.1">
     that help measure parallel
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.418.1">
      systems’ performance.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-30">
    <a id="_idTextAnchor029">
    </a>
    <span class="koboSpan" id="kobo.419.1">
     Exploring the metrics to assess parallelism
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.420.1">
     Metrics
    </span>
    <a id="_idIndexMarker075">
    </a>
    <span class="koboSpan" id="kobo.421.1">
     are measurements that can help us understand how a system is performing and to compare different
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.422.1">
      improvement approaches.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.423.1">
     Here are some metrics and formulas commonly used to evaluate parallelism in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.424.1">
      a system.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-31">
    <a id="_idTextAnchor030">
    </a>
    <span class="koboSpan" id="kobo.425.1">
     Degree of parallelism
    </span>
   </h2>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.426.1">
      Degree of parallelism
     </span>
    </strong>
    <span class="koboSpan" id="kobo.427.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.428.1">
      DOP
     </span>
    </strong>
    <span class="koboSpan" id="kobo.429.1">
     ) is a metric that indicates the number of operations being
    </span>
    <a id="_idIndexMarker076">
    </a>
    <span class="koboSpan" id="kobo.430.1">
     simultaneously
    </span>
    <a id="_idIndexMarker077">
    </a>
    <span class="koboSpan" id="kobo.431.1">
     executed by a computer.
    </span>
    <span class="koboSpan" id="kobo.431.2">
     It is useful to describe the performance
    </span>
    <strong class="bold">
    </strong>
    <span class="koboSpan" id="kobo.432.1">
     of parallel programs and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.433.1">
      multi-processor systems.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.434.1">
     When computing the DOP, we can use the maximum number of operations that could be done simultaneously, measuring the ideal case scenario without bottlenecks or dependencies.
    </span>
    <span class="koboSpan" id="kobo.434.2">
     Alternatively, we can use either the average number of operations or the number of simultaneous operations at a given point in time, reflecting the actual DOP achieved by a system.
    </span>
    <span class="koboSpan" id="kobo.434.3">
     An approximation can be done by using profilers and performance analysis tools to measure the number of threads during a particular
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.435.1">
      time period.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.436.1">
     That means that the DOP is not a constant; it is a dynamic metric that changes during
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.437.1">
      application execution.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.438.1">
     For example, consider a script tool that processes multiple files.
    </span>
    <span class="koboSpan" id="kobo.438.2">
     These files can be processed sequentially or simultaneously, increasing efficiency.
    </span>
    <span class="koboSpan" id="kobo.438.3">
     If we have a machine with
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.439.1">
      N
     </span>
    </strong>
    <span class="koboSpan" id="kobo.440.1">
     cores and we want to process
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.441.1">
      N
     </span>
    </strong>
    <span class="koboSpan" id="kobo.442.1">
     files, we can assign a file to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.443.1">
      each core.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.444.1">
     The time
    </span>
    <a id="_idIndexMarker078">
    </a>
    <span class="koboSpan" id="kobo.445.1">
     to process all files sequentially would be
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.446.1">
      as follows:
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.447.1">
     <img alt="&lt;mml:math   display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mo&gt;⋯&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;≅&lt;/mml:mo&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/1.png" style="vertical-align:-0.533em;height:1.181em;width:18.808em"/>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.448.1">
     And, the time
    </span>
    <a id="_idIndexMarker079">
    </a>
    <span class="koboSpan" id="kobo.449.1">
     to process them in parallel
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.450.1">
      would be:
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.451.1">
     <img alt="&lt;mml:math   display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;⋯&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/2.png" style="vertical-align:-0.533em;height:1.172em;width:12.905em"/>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.452.1">
     Therefore, the DOP is
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.453.1">
      N
     </span>
    </strong>
    <span class="koboSpan" id="kobo.454.1">
     , the number of cores actively processing
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.455.1">
      separate files.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.456.1">
     There is a theoretical upper bound on the speed-up that parallelization can achieve, which is given by
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.457.1">
       Amdahl’s law
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.458.1">
      .
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-32">
    <a id="_idTextAnchor031">
    </a>
    <span class="koboSpan" id="kobo.459.1">
     Amdahl’s law
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.460.1">
     In a parallel system, we could believe that doubling the number of CPU cores could make the
    </span>
    <a id="_idIndexMarker080">
    </a>
    <span class="koboSpan" id="kobo.461.1">
     program run twice as fast, thereby halving the runtime.
    </span>
    <span class="koboSpan" id="kobo.461.2">
     However, the speed-up from parallelization is not linear.
    </span>
    <span class="koboSpan" id="kobo.461.3">
     After a
    </span>
    <a id="_idIndexMarker081">
    </a>
    <span class="koboSpan" id="kobo.462.1">
     certain number of cores, the runtime is not reduced anymore due to different circumstances such as context switching, memory paging, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.463.1">
      so on.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.464.1">
     The Amdahl’s law formula computes the theoretical maximum speed-up a task can perform after parallelization
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.465.1">
      as follows:
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.466.1">
     <img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/3.png" style="vertical-align:-0.772em;height:1.737em;width:12.435em"/>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.467.1">
     Here,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.468.1">
      s
     </span>
    </strong>
    <span class="koboSpan" id="kobo.469.1">
     is the speed-up factor of the improved part and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.470.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.471.1">
     is the fraction of the parallelizable part compared to the entire process.
    </span>
    <span class="koboSpan" id="kobo.471.2">
     Therefore,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.472.1">
      1-p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.473.1">
     represents the ratio of the task not parallelizable (the bottleneck or sequential part), while
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.474.1">
      p/s
     </span>
    </strong>
    <span class="koboSpan" id="kobo.475.1">
     represents the speed-up achieved by the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.476.1">
      parallelizable part.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.477.1">
     That means that the maximum speed-up is limited by the sequential portion of the task.
    </span>
    <span class="koboSpan" id="kobo.477.2">
     The greater the fraction of the parallelizable task (
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.478.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.479.1">
     approaches
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.480.1">
      1
     </span>
    </strong>
    <span class="koboSpan" id="kobo.481.1">
     ), the more the maximum
    </span>
    <a id="_idIndexMarker082">
    </a>
    <span class="koboSpan" id="kobo.482.1">
     speed-up increases up to the
    </span>
    <a id="_idIndexMarker083">
    </a>
    <span class="koboSpan" id="kobo.483.1">
     speed-up factor (
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.484.1">
      s
     </span>
    </strong>
    <span class="koboSpan" id="kobo.485.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.485.2">
     On the other hand, when the sequential portion becomes larger (
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.486.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.487.1">
     approaches
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.488.1">
      0
     </span>
    </strong>
    <span class="koboSpan" id="kobo.489.1">
     ),
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.490.1">
      Smax
     </span>
    </strong>
    <span class="koboSpan" id="kobo.491.1">
     tends to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.492.1">
      1
     </span>
    </strong>
    <span class="koboSpan" id="kobo.493.1">
     , meaning that no improvement
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.494.1">
      is possible.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer021">
     <span class="koboSpan" id="kobo.495.1">
      <img alt="Figure 1.9: The speed-up limit by the number of processors and percentage of parallelizable parts" src="image/B22219_01_9.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.496.1">
     Figure 1.9: The speed-up limit by the number of processors and percentage of parallelizable parts
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.497.1">
     The
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.498.1">
      critical path
     </span>
    </strong>
    <span class="koboSpan" id="kobo.499.1">
     in parallel
    </span>
    <a id="_idIndexMarker084">
    </a>
    <span class="koboSpan" id="kobo.500.1">
     systems is defined by the longest chain of dependent calculations.
    </span>
    <span class="koboSpan" id="kobo.500.2">
     As the critical path is hardly parallelizable, it defines the sequential portion and thus the quicker runtime that a program
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.501.1">
      can achieve.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.502.1">
     For example, if the sequential part of a process represents 10% of the runtime, then the fraction of the parallelizable part is
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.503.1">
      p=0.9.
     </span>
    </strong>
    <span class="koboSpan" id="kobo.504.1">
     In this case, the potential speed-up will not exceed 10 times the speed-up, regardless of the number of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.505.1">
      processors available.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-33">
    <a id="_idTextAnchor032">
    </a>
    <span class="koboSpan" id="kobo.506.1">
     Gustafson’s law
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.507.1">
     The Amdahl’s law formula can only be used with fixed-sized problems and increasing resources.
    </span>
    <span class="koboSpan" id="kobo.507.2">
     When using larger datasets, time spent in the parallelizable part grows much faster
    </span>
    <a id="_idIndexMarker085">
    </a>
    <span class="koboSpan" id="kobo.508.1">
     than that in the sequential part.
    </span>
    <span class="koboSpan" id="kobo.508.2">
     In these cases, the Gustafson’s law
    </span>
    <a id="_idIndexMarker086">
    </a>
    <span class="koboSpan" id="kobo.509.1">
     formula is less pessimistic and more accurate, as it accounts for fixed execution time and increasing problem size with
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.510.1">
      additional resources.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.511.1">
     The Gustafson’s law formula computes the speed-up gained by using
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.512.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.513.1">
     processors
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.514.1">
      as follows:
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.515.1">
     <img alt="&lt;mml:math   display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/4.png" style="vertical-align:-0.482em;height:1.243em;width:7.604em"/>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.516.1">
     Here,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.517.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.518.1">
     is the number of processors and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.519.1">
      f
     </span>
    </strong>
    <span class="koboSpan" id="kobo.520.1">
     is the fraction of the task that remains sequential.
    </span>
    <span class="koboSpan" id="kobo.520.2">
     Therefore,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.521.1">
      (1-f)*p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.522.1">
     represents the speed-up achieved with the parallelization of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.523.1">
      (1-f)
     </span>
    </strong>
    <span class="koboSpan" id="kobo.524.1">
     task distributed across
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.525.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.526.1">
     processors, and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.527.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.528.1">
     represents the extra work done when
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.529.1">
      increasing resources.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.530.1">
     Gustafson’s law formula shows that the speed-up is affected by parallelization when lowering
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.531.1">
      f
     </span>
    </strong>
    <span class="koboSpan" id="kobo.532.1">
     and by scalability by
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.533.1">
      increasing
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.534.1">
       p
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.535.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.536.1">
     As with Amdahl’s law, Gustafson’s law formula is an approximation that provides valuable perspective when measuring improvements in parallel systems.
    </span>
    <span class="koboSpan" id="kobo.536.2">
     Other factors can reduce efficiency such as overhead communication between processors or memory and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.537.1">
      storage limitations.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-34">
    <a id="_idTextAnchor033">
    </a>
    <span class="koboSpan" id="kobo.538.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.539.1">
     In this chapter, we learned about the different architectures and models we can use to build parallel systems.
    </span>
    <span class="koboSpan" id="kobo.539.2">
     Then we explored the details of the variety of parallel programming paradigms available to develop parallel software and learned about their behavior and nuances.
    </span>
    <span class="koboSpan" id="kobo.539.3">
     Finally, we defined some useful metrics to measure the performance of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.540.1">
      parallel programs.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.541.1">
     In the next chapter, we will explore the relationship between hardware and software, as well as how software maps and interacts with the underlying hardware.
    </span>
    <span class="koboSpan" id="kobo.541.2">
     We will also learn what threads, processes, and services are, how threads are scheduled, and how they communicate with each other.
    </span>
    <span class="koboSpan" id="kobo.541.3">
     Furthermore, we will cover inter-process communication and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.542.1">
      much more.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-35">
    <a id="_idTextAnchor034">
    </a>
    <span class="koboSpan" id="kobo.543.1">
     Further reading
    </span>
   </h1>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.544.1">
      Topological
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.545.1">
       sorting:
      </span>
     </span>
     <a href="https://en.wikipedia.org/wiki/Topological_sorting">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.546.1">
        https://en.wikipedia.org/wiki/Topological_sorting
       </span>
      </span>
     </a>
    </li>
    <li>
     <span class="koboSpan" id="kobo.547.1">
      C++ compiler
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.548.1">
       support:
      </span>
     </span>
     <a href="https://en.cppreference.com/w/cpp/compiler_support">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.549.1">
        https://en.cppreference.com/w/cpp/compiler_support
       </span>
      </span>
     </a>
    </li>
    <li>
     <span class="koboSpan" id="kobo.550.1">
      C++20 compiler
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.551.1">
       support:
      </span>
     </span>
     <a href="https://en.cppreference.com/w/cpp/compiler_support/20">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.552.1">
        https://en.cppreference.com/w/cpp/compiler_support/20
       </span>
      </span>
     </a>
    </li>
    <li>
     <span class="koboSpan" id="kobo.553.1">
      C++23 compiler
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.554.1">
       support:
      </span>
     </span>
     <a href="https://en.cppreference.com/w/cpp/compiler_support/23">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.555.1">
        https://en.cppreference.com/w/cpp/compiler_support/23
       </span>
      </span>
     </a>
    </li>
   </ul>
  </div>
 </body></html>