<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-205"><a id="_idTextAnchor241"/>4</h1>
<h1 id="_idParaDest-206"><a id="_idTextAnchor242"/>Exploring Techniques for Lighting, Shading, and Shadows</h1>
<p>Welcome to an exploration of lighting and shading techniques designed to infuse realism into your scenes. In the world of graphics, both lighting and shading play an integral role in enhancing the aesthetic appeal and realism of 3D visuals. This chapter delves into these topics, presenting a spectrum of algorithms ranging from the fundamental to the complex which can add realism to your scenes. In this chapter, we will cover the following recipes:</p>
<ul>
<li>Implementing G-buffer for deferred rendering</li>
<li>Implementing screen space reflections</li>
<li>Implementing shadow maps for real-time shadows</li>
<li>Implementing screen space ambient occlusion</li>
<li>Implementing a lighting pass for illuminating the scene</li>
</ul>
<p>By the end of this chapter, you will have a comprehensive understanding of these techniques, enabling you to adeptly implement them in your rendering projects.</p>
<h1 id="_idParaDest-207"><a id="_idTextAnchor243"/>Technical requirements</h1>
<p>For this chapter, you will need to make sure you have VS 2022 installed along with the Vulkan SDK. Basic familiarity with the C++ programming language and an understanding of OpenGL or any other graphics API will be useful. Please revisit <a href="B18491_01.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a><em class="italic">, Vulkan Core Concepts</em>, under the T<em class="italic">echnical requirements</em> section for details on setting up and building executables for this chapter. We also assume that by now you are familiar with how to use the Vulkan API and various concepts that were introduced in previous chapters. All recipes for this chapter are encapsulated in a single executable and can be launched using <code>Chapter04_Deferred_Renderer.exe</code> executable.</p>
<h1 id="_idParaDest-208"><a id="_idTextAnchor244"/>Implementing G-buffer for deferred rendering</h1>
<p><strong class="bold">Deferred rendering</strong> is a<a id="_idIndexMarker337"/> technique <a id="_idIndexMarker338"/>that adds an additional render pass at the beginning of the scene rendering that accumulates various information about the scene in screen space, such as position, surface normal, surface color, and others. This extra information is stored in a buffer called the <strong class="bold">geometry buffer</strong> (<strong class="bold">G-buffer</strong>), where each one of the values computed during this step is stored for each pixel. Once this initial pass has finished, the final scene rendering can take place, and the extra information to improve the rendering quality by computing things such as reflections, ambient occlusion, atmospheric effects, and others can be used. The benefit of using deferred rendering is that it provides more efficient handling of complex scenes with many lights, as each light only needs to be calculated once per pixel, rather than once per object. We have essentially decoupled geometry and shading, which allows for more flexibility in the rendering pipeline. The technique also has some disadvantages, such as increased memory usage (for the G-buffer itself), and difficulty handling transparency and anti-aliasing.</p>
<p>In this tutorial, you will gain an understanding of the implementation of G-buffer for deferred rendering, its advantages in managing complex scenes with multiple lights, and the challenges it may present, such as increased memory usage.</p>
<h2 id="_idParaDest-209"><a id="_idTextAnchor245"/>Getting ready</h2>
<p>Creating a G-buffer in Vulkan is somewhat straightforward. The bulk of the technique relies on creating a framebuffer that contains references to all render targets (textures) that will store the scene’s information, such as position, normal, and material data. The render pass also needs to dictate how those render targets should be loaded and stored at the end of the pass. Finally, in the fragment shader, each render target is specified as an output variable and the value of each render target is written to the output that refers to the correct texture or storage buffer.</p>
<div><div><img alt="Figure 4.1 – G-buffer textures" src="img/B18491_04_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – G-buffer textures</p>
<p>In the<a id="_idIndexMarker339"/> repository, the<a id="_idIndexMarker340"/> G-buffer generation is encapsulated in the <code>GBufferPass</code> class.</p>
<h2 id="_idParaDest-210"><a id="_idTextAnchor246"/>How to do it...</h2>
<p>To generate a G-buffer and its artifacts, we need to first create a framebuffer and a corresponding <code>RenderPass</code>. In the following steps, we will show you how to create targets for the base color of the material, the normal, and the depth components:</p>
<ol>
<li>Before creating theFrambuffer object, it is necessary to create the textures (render targets) that will store the output of the G-buffer pass:<pre class="source-code">
gBufferBaseColorTexture_ = context-&gt;createTexture(
    VK_IMAGE_TYPE_2D, VK_FORMAT_R8G8B8A8_UNORM, 0,
    VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT |
        VK_IMAGE_USAGE_SAMPLED_BIT |
        VK_IMAGE_USAGE_STORAGE_BIT,…
gBufferNormalTexture_ = context-&gt;createTexture(
    VK_IMAGE_TYPE_2D,
    VK_FORMAT_R16G16B16A16_SFLOAT, 0,
    VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT |
        VK_IMAGE_USAGE_SAMPLED_BIT |
        VK_IMAGE_USAGE_STORAGE_BIT,…
gBufferPositionTexture_ = context-&gt;createTexture(
      VK_IMAGE_TYPE_2D, VK_FORMAT_R16G16B16A16_SFLOAT, 0,
      VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT | VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_STORAGE_BIT,…
depthTexture_ = context-&gt;createTexture(
    VK_IMAGE_TYPE_2D, VK_FORMAT_D24_UNORM_S8_UINT,
    0,
    VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT |
        VK_IMAGE_USAGE_TRANSFER_DST_BIT |
        VK_IMAGE_USAGE_SAMPLED_BIT,…</pre></li> <li> The <code>Framebuffer</code> object <a id="_idIndexMarker341"/>references <a id="_idIndexMarker342"/>the preceding targets. The order is important here and should be mirrored in the shader where the outputs are specified:<pre class="source-code">
frameBuffer_ = context-&gt;createFramebuffer(
      renderPass_-&gt;vkRenderPass(),
      {gBufferBaseColorTexture_, gBufferNormalTexture_, gBufferEmissiveTexture_,
       gBufferSpecularTexture_, gBufferPositionTexture_, depthTexture_},
      nullptr, nullptr, "GBuffer framebuffer ");</pre></li> <li>The <code>RenderPass</code> object describes how each render target should be loaded and stored. The<a id="_idIndexMarker343"/> operations<a id="_idIndexMarker344"/> should match the order of the targets used by the framebuffer:<pre class="source-code">
renderPass_ = context-&gt;createRenderPass(
      {gBufferBaseColorTexture_, gBufferNormalTexture_, gBufferEmissiveTexture_,
       gBufferSpecularTexture_, gBufferPositionTexture_, depthTexture_},
      {VK_ATTACHMENT_LOAD_OP_CLEAR, VK_ATTACHMENT_LOAD_OP_CLEAR,
       VK_ATTACHMENT_LOAD_OP_CLEAR, VK_ATTACHMENT_LOAD_OP_CLEAR,
       VK_ATTACHMENT_LOAD_OP_CLEAR, VK_ATTACHMENT_LOAD_OP_CLEAR},
      {VK_ATTACHMENT_STORE_OP_STORE, VK_ATTACHMENT_STORE_OP_STORE,
       VK_ATTACHMENT_STORE_OP_STORE, VK_ATTACHMENT_STORE_OP_STORE,
       VK_ATTACHMENT_STORE_OP_STORE, VK_ATTACHMENT_STORE_OP_STORE},
      // final layout for all attachments
      {VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL, VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL,
       VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL, VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL,
       VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL,
       VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL},
      VK_PIPELINE_BIND_POINT_GRAPHICS, "GBuffer RenderPass");</pre></li> <li>In the fragment shader, besides the input data originating from the previous stages in the pipeline, the output data is directed to each one of the targets using the layout keyword and the location qualifier. The location index must match the render <a id="_idIndexMarker345"/>target <a id="_idIndexMarker346"/>index on the framebuffer:<pre class="source-code">
layout(location=0) in vec2 inTexCoord;
layout(location=1) in flat uint inflatMeshId;
layout(location=2) in flat int inflatMaterialId;
layout(location=3) in vec3 inNormal;
layout(location=4) in vec4 inTangent;
layout(location = 0) out vec4 outgBufferBaseColor;
layout(location = 1) out vec4 outgBufferWorldNormal;
layout(location = 2) out vec4 outgBufferEmissive;
layout(location = 3) out vec4 outgBufferSpecular;
layout(location = 4) out vec4 outgBufferPosition;
const vec3 n = normalize(inNormal);
const vec3 t = normalize(inTangent.xyz);
const vec3 b = normalize(cross(n,t) * inTangent.w);
const mat3 tbn =  mat3(t, b, n);
outgBufferWorldNormal.rgb = normalize(tbn * normalize(normalTan));</pre></li> </ol>
<p>In the preceding code snippet, the world normal is calculated based on the normal and tangent values and stored in the <code>outgBufferWorldNormal</code> location, which corresponds to<a id="_idIndexMarker347"/> the <a id="_idIndexMarker348"/>attachment with <code>index 1</code> (see code fragment in <em class="italic">step 2</em>).</p>
<h1 id="_idParaDest-211"><a id="_idTextAnchor247"/>Implementing screen space reflections</h1>
<p>Physically correct <a id="_idIndexMarker349"/>reflections involve tracing the path of light rays as they bounce off surfaces. This process accounts for the geometry, material properties, and light sources in the scene, as well as the view angle. However, it is a very computationally intensive process, often too demanding for real-time rendering, especially in complex scenes or on less powerful hardware. To achieve a balance between visual quality and performance, an approximation technique known as <strong class="bold">screen space reflection</strong> (<strong class="bold">SSR</strong>) can be used. SSR is a method that approximates reflections by reusing data that has already been rendered to the screen. By utilizing a screen-space variant, the heavy computational cost associated with physically correct reflections can be significantly reduced, making it a viable technique for real-time rendering. In this recipe, we will explain how to compute reflections using buffers derived from the previous section, such as the normal and depth buffers.</p>
<h2 id="_idParaDest-212"><a id="_idTextAnchor248"/>Getting ready</h2>
<p>SSR uses the depth buffer to find intersections between a reflected ray and the geometry’s depth. The reflection ray is computed in world space based on the surface normal and the view direction and is marched in small increments until it leaves the screen bounds. For every step, the ray’s location is projected onto the screen and its coordinates are compared against the depth buffer. If the difference between the ray’s location and the depth buffer’s depth is less than a small threshold, then the ray has collided with some geometry, and the ray’s originating point on the surface is obscured. This reflection vector is then used to look up the color of the pixel in the already-rendered image at the reflected position. This color is then used as the reflected color, creating the illusion of a reflection. SSR can produce visually pleasing reflections that come close to those produced by much more computationally expensive physically correct reflection models; however, it can only reflect what’s already visible on the screen, and it may produce inaccurate results for complex surfaces or at screen edges.</p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor249"/>How to do it...</h2>
<p>Once the depth and normal buffers have been calculated, the SSR can be easily computed in a render or compute pass:</p>
<ol>
<li>The following SSR code is used by a compute pass and specifies the buffers used as input, generated by the deferred rendering step, as well as the transformation <a id="_idIndexMarker350"/>data it needs to perform the intersection in screen space:<pre class="source-code">
layout(set = 0, binding = 0, rgba16f) uniform image2D SSRIntersect;
layout(set = 1, binding = 0)uniform sampler2D gBufferWorldNormal;
layout(set = 1, binding = 1)uniform sampler2D gBufferSpecular;
layout(set = 1, binding = 2)uniform sampler2D gBufferBaseColor;
layout(set = 1, binding = 3)uniform sampler2D hierarchicalDepth;
layout(set = 2, binding = 0)uniform Transforms
{
  mat4 model;
  mat4 view;
  mat4 projection;
  mat4 projectionInv;
  mat4 viewInv;
} cameraData;</pre></li> <li>Two auxiliary functions are defined in the shader. They perform the projection from a point in world space to screen space and calculate the projection from a screen space along with depth coordinate to world space:<pre class="source-code">
vec3 generatePositionFromDepth(vec2 texturePos, float depth);
vec2 generateProjectedPosition(vec3 worldPos);</pre></li> <li>In this step, the data needed for the reflection calculations from the G-buffer is fetched. This includes the world normal, specular data, and base color for the current pixel. The UV coordinates are calculated, which are used to sample the base color from the G-buffer. The roughness, which controls how blurry or sharp the reflection is, is also extracted from the specular data. We also check the metalness value from the G-buffer specular data. If the material is not metallic (<code>metalness &lt; 0.01</code>), it assumes it doesn’t reflect and simply writes the base color to the <a id="_idIndexMarker351"/>result and exits:<pre class="source-code">
layout(local_size_x = 16, local_size_y = 16,
       local_size_z = 1) in;
void main() {
  // Return if the coordinate is outside the screen
  ...
  imageStore(SSRIntersect,
             ivec2(gl_GlobalInvocationID.xy),
             vec4(0));
  vec2 uv = (vec2(gl_GlobalInvocationID.xy) +
             vec2(0.5f)) /
            vec2(pushConstant.textureResolution);
  ivec2 pixelPos = ivec2(gl_GlobalInvocationID.xy);
  vec4 gbufferNormalData =
      texelFetch(gBufferWorldNormal, pixelPos, 0);
  vec4 gbufferSpecularData =
      texelFetch(gBufferSpecular, pixelPos, 0);
  vec3 basecolor =
      texture(gBufferBaseColor, uv).xyz;
  float roughness = gbufferSpecularData.g;
  if (gbufferSpecularData.r &lt;
      .01) { // Metal-ness check
    imageStore(SSRIntersect,
               ivec2(gl_GlobalInvocationID.xy),
               vec4(basecolor, 1.0));
    return;
  }</pre></li> <li>The following snippet fetches the depth of the current pixel from the depth buffer and generates the world position of the pixel using the UV and depth. The view direction<a id="_idIndexMarker352"/> is calculated from the camera position to the pixel’s world position. The reflection direction is then calculated using the view direction and the normal. The shader then performs ray marching along the reflection direction in screen space. It steps along the reflection ray and at each step, it checks whether the ray has intersected with any geometry, based on the depth difference between the current position of the ray and the depth at the corresponding screen position:<pre class="source-code">
  float z =
      texelFetch(hierarchicalDepth, pixelPos, 0).r;
  vec3 position = generatePositionFromDepth(uv, z);
  vec3 normal = normalize(gbufferNormalData.xyz);
  vec3 camPos = cameraData.viewInv[3].xyz;
  vec3 viewDirection = normalize(position - camPos);
  vec3 reflectionDirection =
      reflect(viewDirection, normal);
;
  float stepSize = 0.05; // Initial step size
  vec3 currentPos = position;
  for (int i = 0; i &lt; 50; i++) {
    currentPos += reflectionDirection * stepSize;
    vec2 screenPos =
        generateProjectedPosition(currentPos);
    if (screenPos.x &lt; 0.0 || screenPos.x &gt; 1.0 ||
        screenPos.y &lt; 0.0 || screenPos.y &gt; 1.0) {
      break; // Ray went out of screen bounds
    }
    float depthAtCurrent =
        texture(hierarchicalDepth, screenPos).r;
    vec3 positionFromDepth =
        generatePositionFromDepth(screenPos,
                                  depthAtCurrent);
    float depthDifference =
        length(currentPos - positionFromDepth);</pre></li> <li>If an intersection is found, the code fetches the color at the intersection point and blends it with the base color. The blending is based on the roughness value, which represents <a id="_idIndexMarker353"/>a characteristic of the surface at the intersection point:<pre class="source-code">
    if (depthDifference &lt; 0.05) {
      vec3 hitColor =
          texture(gBufferBaseColor, screenPos).xyz;
      if (hitColor.x &lt;= .1 &amp;&amp; hitColor.y &lt;= .1 &amp;&amp;
          hitColor.z &lt;= .1 &amp;&amp; hitColor.x &gt;= .08 &amp;&amp;
          hitColor.y &gt;= .08 &amp;&amp;
          hitColor.z &gt;=
              .08) { // .1 is considered sky color,
                     // ignore if we hit sky
        hitColor = basecolor;
      }
      vec3 blendColor =
          hitColor * (1.0 - roughness) +
          roughness * basecolor;
      imageStore(SSRIntersect,
                 ivec2(gl_GlobalInvocationID.xy),
                 vec4(blendColor, 1.0));
      return;
    }
  }
  // Fallback
  imageStore(SSRIntersect,
             ivec2(gl_GlobalInvocationID.xy),
             vec4(basecolor, 1.0));
}</pre></li> </ol>
<p>In the preceding code, we <a id="_idIndexMarker354"/>learned how SSR is computed using the depth and normal buffers.</p>
<h2 id="_idParaDest-214"><a id="_idTextAnchor250"/>See also</h2>
<p>The following are some references that go into more detail on how to implement SSR; we suggest you go <a id="_idIndexMarker355"/>through these references to gain a more thorough understanding:</p>
<ul>
<li><a href="https://interplayoflight.wordpress.com/2022/09/28/notes-on-screenspace-reflections-with-fidelityfx-sssr/">https://interplayoflight.wordpress.com/2022/09/28/notes-on-screenspace-reflections-with-fidelityfx-sssr/</a></li>
<li><a href="http://roar11.com/2015/07/screen-space-glossy-reflections/">http://roar11.com/2015/07/screen-space-glossy-reflections/</a></li>
<li><a href="https://interplayoflight.wordpress.com/2019/09/07/hybrid-screen-space-reflections/">https://interplayoflight.wordpress.com/2019/09/07/hybrid-screen-space-reflections/</a></li>
</ul>
<h1 id="_idParaDest-215"><a id="_idTextAnchor251"/>Implementing shadow maps for real-time shadows</h1>
<p>As the name <a id="_idIndexMarker356"/>implies, <strong class="bold">shadow maps</strong> are used <a id="_idIndexMarker357"/>to simulate shadows. The goal of shadow mapping is to determine which parts of a scene are in shadow and which parts are illuminated by a light source by first rendering the scene from the light’s perspective, generating a depth map.</p>
<p>This depth map (also known as a shadow map) serves as a spatial record, storing the shortest distance from the light source to any point in the scene. By encapsulating the scene from the vantage point of the light source, the depth map effectively captures the areas of the scene that are directly visible to the light source and those that are occluded.</p>
<p>This depth map is then used during the main render pass to determine if the fragment can’t be reached from the light by comparing its depth value with the one in the depth map. For each fragment in the scene, we perform a test to evaluate whether it lies in shadow. This is achieved by comparing the depth value of the fragment from the light source, derived from the main camera’s perspective, with the corresponding depth value stored in the depth map.</p>
<p>If the fragment’s depth value exceeds the value recorded in the depth map, it implies that the fragment is occluded by another object in the scene and is, therefore, in shadow. Conversely, if the fragment’s depth value is less than or equal to the depth map value, it signifies that the fragment is directly visible to the light source and is thus illuminated.</p>
<p>In this recipe, you will learn how to implement shadow maps to create real-time shadows in your 3D scene. This involves understanding the theory of shadow mapping, generating a depth <a id="_idIndexMarker358"/>map<a id="_idIndexMarker359"/> from the light’s perspective, and finally using this depth map in the main render pass to accurately determine which fragments of the scene are in shadow and which are illuminated.</p>
<h2 id="_idParaDest-216"><a id="_idTextAnchor252"/>Getting ready</h2>
<p>To obtain the shadow map, we first need to render the scene from the light’s perspective and retain the depth map. This render pass needs a depth texture that will store the depth information and simple vertex and fragment shaders. The main render pass, in which the scene is rendered, is where the depth map is used as a reference to determine if a pixel is lit or not and needs to refer to the shadow map generated in the previous step, along with a special sampler to access the depth map in the shader code. It also has code to perform the comparison between the fragment and the value stored in the depth map.</p>
<p>In the repository, a shadow map generation is encapsulated in the <code>ShadowPass</code> class, and usage of shadow depth texture is encapsulated in the <code>LightingPass</code> class.</p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor253"/>How to do it...</h2>
<p>We’ll start with a walk-through of the shadow map pass first:</p>
<ol>
<li>The shadow map is a regular texture with a format that supports depth values. Our depth<a id="_idIndexMarker360"/> texture is<a id="_idIndexMarker361"/> 4x the resolution of the normal texture and uses the  <code>VK_FORMAT_D24_UNORM_S8_UINT</code> format:<pre class="source-code">
void ShadowPass::initTextures(
    VulkanCore::Context *context) {
  depthTexture_ = context-&gt;createTexture(
      VK_IMAGE_TYPE_2D, VK_FORMAT_D24_UNORM_S8_UINT,
      0,
      VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT |
          VK_IMAGE_USAGE_TRANSFER_DST_BIT |
          VK_IMAGE_USAGE_SAMPLED_BIT,
      {
          .width =
              context-&gt;swapchain()-&gt;extent().width *
              4, // 4x resolution for shadow maps
          .height = context-&gt;swapchain()
                        -&gt;extent()
                        .height *
                    4,
          .depth = 1,
      },
      1, 1, VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT,
      false, "ShadowMap Depth buffer");
}</pre></li> <li>The render pass needs to clear the depth attachment at the beginning and then store it at the end. There are no color attachments in the shadow map render pass or in the framebuffer:<pre class="source-code">
renderPass_ = context-&gt;createRenderPass(
    {depthTexture_}, {VK_ATTACHMENT_LOAD_OP_CLEAR},
    {VK_ATTACHMENT_STORE_OP_STORE},
    // final layout for all attachments
    {VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL},
    VK_PIPELINE_BIND_POINT_GRAPHICS,
    "ShadowMap RenderPass");
frameBuffer_ = context-&gt;createFramebuffer(
    renderPass_-&gt;vkRenderPass(), {depthTexture_},
    nullptr, nullptr, "ShadowMap framebuffer ");</pre></li> <li>This render <a id="_idIndexMarker362"/>pass’s pipeline<a id="_idIndexMarker363"/> definition needs to match the size of the viewport to the size of the shadow map and use the special vertex and fragment shaders for this pass. The fragment and vertex shader are conceptually identical to the G-buffer pass but they just need to output the depth buffer instead of multiple geometry buffers; it also needs a light view projection matrix instead of the camera’s one. As a future optimization, you could use the specialization constant with the G-buffer pass instead of using a separate shader:<pre class="source-code">
auto vertexShader = context-&gt;createShaderModule(
    (resourcesFolder / "shadowpass.vert").string(),
    VK_SHADER_STAGE_VERTEX_BIT, "shadowmap vertex");
auto fragmentShader = context-&gt;createShaderModule(
    (resourcesFolder / "empty.frag").string(),
    VK_SHADER_STAGE_FRAGMENT_BIT,
    "shadowmap fragment");
const VulkanCore::Pipeline::
    GraphicsPipelineDescriptor gpDesc = {
        .sets_ = setLayout,
        .vertexShader_ = vertexShader,
        .fragmentShader_ = fragmentShader,
        .dynamicStates_ =
            {VK_DYNAMIC_STATE_VIEWPORT,
             VK_DYNAMIC_STATE_SCISSOR},
        .colorTextureFormats = {},
        .depthTextureFormat =
            VK_FORMAT_D24_UNORM_S8_UINT,
        .sampleCount = VK_SAMPLE_COUNT_1_BIT,
        .cullMode = VK_CULL_MODE_BACK_BIT,
        .viewport = VkExtent2D(
            depthTexture_-&gt;vkExtents().width,
            depthTexture_-&gt;vkExtents().height),
        .depthTestEnable = true,
        .depthWriteEnable = true,
        .depthCompareOperation = VK_COMPARE_OP_LESS,
};
pipeline_ = context-&gt;createGraphicsPipeline(
    gpDesc, renderPass_-&gt;vkRenderPass(),</pre></li> <li>The vertex shader needs the light’s transformation matrix, which is set into the <code>depthTestEnable</code>, <code>depthWriteEnable</code>, and <code>depthCompareOperation</code> – will govern how we evaluate and store depth information<a id="_idIndexMarker365"/> during<a id="_idIndexMarker366"/> this process:<pre class="source-code">
#version 460
#extension GL_EXT_nonuniform_qualifier : require
#extension GL_EXT_debug_printf : enable
#extension GL_GOOGLE_include_directive : require
#include "CommonStructs.glsl"
#include "IndirectCommon.glsl"
void main() {
  Vertex vertex = vertexAlias[VERTEX_INDEX]
                      .vertices[gl_VertexIndex];
  vec3 position =
      vec3(vertex.posX, vertex.posY, vertex.posZ);
  gl_Position = MVP.projection * MVP.view *
                MVP.model * vec4(position, 1.0);
}</pre></li> <li>The fragment shader is empty, as it doesn’t need to output any color information:<pre class="source-code">
#version 460
void main() {
}</pre><p class="list-inset">The main render (lighting) pass uses the shadow map calculated before as a reference to determine if a fragment is lit or not. There is no special setup for the scene, except for the sampler used with the shadow map, which needs to enable a comparison function. The vertex and fragment shaders also need some special treatment to perform the depth comparison against the shadow map.</p></li> <li>The sampler used to access the shadow map in the shader needs to enable the comparison <a id="_idIndexMarker367"/>function. We use <a id="_idIndexMarker368"/>the <code>VK_COMPARE_OP_LESS_OR_EQUAL </code>function:<pre class="source-code">
samplerShadowMap_ = context.createSampler(
    VK_FILTER_NEAREST, VK_FILTER_NEAREST,
    VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE,
    VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE,
    VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, 1.0f,
    <strong class="bold">true, VK_COMPARE_OP_LESS_OR_EQUAL</strong>,
    "lighting pass shadow");</pre></li> <li>The fragment shader needs a shadow map as well as a light’s view projection matrix. The following code includes the uniform <code>sampler2Dshadow</code>, which holds the depth map or shadow map. The uniform <code>Lights</code> structure contains information about the light source, including its position, direction, color, and the view projection matrix from the light’s perspective:<pre class="source-code">
#version 460
layout(set = 0, binding = 6)uniform sampler2DShadow shadowMap;
layout(set = 1, binding = 1)uniform Lights
{
    vec4 lightPos;
    vec4 lightDir;
    vec4 lightColor;
    vec4 ambientColor; // environment light color
    mat4 lightVP;
    float innerConeAngle;
    float outerConeAngle;
} lightData;</pre></li> <li>We introduce a <code>computeShadow</code> auxiliary function that takes as input a position in light-projective space. It first converts this position into <strong class="bold">normalized device coordinates</strong> (<strong class="bold">NDCs</strong>), then<a id="_idIndexMarker369"/> it looks up the shadow map at the corresponding position <a id="_idIndexMarker370"/>and<a id="_idIndexMarker371"/> returns the shadow intensity at that point:<pre class="source-code">
#version 460
layout(set = 0, binding = 6)uniform sampler2DShadow shadowMap;
layout(set = 1, binding = 1)uniform Lights
{
    vec4 lightPos;
    vec4 lightDir;
    vec4 lightColor;
    vec4 ambientColor; // environment light color
    mat4 lightVP;
    float innerConeAngle;
    float outerConeAngle;
} lightData;</pre></li> <li>Next, we introduce another auxiliary function, the <code>PCF</code> function. <code>16</code> different offsets around the pixel. Then, it would calculate the average of these samples to determine the final shadow value for the pixel. This averaging process results in pixels on the edge of shadows having intermediate shadow values between fully lit and fully shadowed, creating <a id="_idIndexMarker373"/>a <a id="_idIndexMarker374"/>soft transition that makes the shadow look more natural:<pre class="source-code">
float PCF(vec4 shadowCoord) {
  vec2 texCoord = shadowCoord.xy / shadowCoord.w;
  texCoord = texCoord * .5 + .5;
  texCoord.y = 1.0 - texCoord.y;
  if (texCoord.x &gt; 1.0 || texCoord.y &gt; 1.0 ||
      texCoord.x &lt; 0.0 || texCoord.y &lt; 0.0) {
    return 1.0;
  }
  vec2 texSize = textureSize(shadowMap, 0);
  float result = 0.0;
  vec2 offset = (1.0 / texSize) * shadowCoord.w;
  for(float x = -1.5; x &lt;= 1.5; x += 1.0) {
    for(float y = -1.5; y &lt;= 1.5; y += 1.0) {
      result += computeShadow(shadowCoord + vec4(vec2(x, y) * offset, 0.0, 0.0));
    }
  }
  return result / 16.0;</pre></li> <li>In the main function, we first retrieve the world position and base color of the fragment from the G-buffer. If the world position is zero (indicating no meaningful information), we simply set the output color to the base color and return early. As a next step, the world position is transformed into light-projective space using the light’s view projection matrix and passes this position to the <code>PCF</code> function <a id="_idIndexMarker375"/>to <a id="_idIndexMarker376"/>compute the visibility factor, which represents how much the fragment is in shadow. If the visibility factor is below a threshold (meaning the fragment is in deep shadow), it sets the visibility to a fixed value for a minimum amount of ambient light. Finally, we multiply the computed <code>outColor</code> by the visibility factor to create the final color, which will be darker if the fragment is in shadow and lighter if it’s lit:<pre class="source-code">
void main() {
  vec4 worldPos =
      texture(gBufferPosition, fragTexCoord);
  vec3 basecolor =
      texture(gBufferBaseColor, fragTexCoord).rgb;
  if (worldPos.x == 0.0 &amp;&amp; worldPos.y == 0.0 &amp;&amp;
      worldPos.z == 0.0 &amp;&amp; worldPos.w == 0.0) {
    outColor = vec4(basecolor, 1.0);
    return;
  }
  // compute outColor …
  vec4 shadowProjPos =
      lightData.lightVP * vec4(worldPos.xyz, 1.0f);
  float vis = PCF(shadowProjPos);
  if (vis &lt;= .001) {
    vis = .3;
  }
  outColor.xyz *= vis;
}</pre></li> </ol>
<p>In the preceding<a id="_idIndexMarker377"/> section, we<a id="_idIndexMarker378"/> illustrated how to implement shadow pass, but there are limitations to this technique that will be discussed in the next section.</p>
<h2 id="_idParaDest-218"><a id="_idTextAnchor254"/>There’s more …</h2>
<p>We have demonstrated the use of the basic shadow mapping technique; however, this technique has some limitations, as follows:</p>
<ul>
<li><strong class="bold">Aliasing and pixelation</strong>: One <a id="_idIndexMarker379"/>of the primary issues with simple shadow mapping is the problem of <strong class="bold">aliasing</strong>, where shadows can appear pixelated or blocky. This is because the resolution of the shadow map directly affects the shadow’s quality. If the shadow map’s resolution is too low, the resulting shadows will be pixelated. While increasing the shadow map’s resolution can mitigate this, it comes at the cost of increasing memory usage and computational load.</li>
<li><strong class="bold">Hard shadow edges</strong>: Basic shadow mapping produces shadows with hard edges since it uses a binary lit or unlit test for shadow determination. Shadows often have soft edges due to light scattering (penumbra).</li>
<li><strong class="bold">Shadow acne or self-shadowing artifacts</strong>: This problem arises when a surface incorrectly shadows itself due to precision errors in the depth test. Techniques such as biasing are used to handle this issue but choosing the right bias can be challenging.<p class="list-inset">Some of these challenges can be overcome with more advanced techniques such as the following:</p><ul><li><strong class="bold">Cascade shadow maps</strong>: This technique addresses the issue of resolution by dividing the camera’s view frustum into multiple <strong class="bold">cascades</strong> or sections, each with its own shadow map. This allows for higher-resolution shadow maps to be used close to the camera, where detail is more important, and lower-resolution shadow maps to be used farther away.</li><li><strong class="bold">Moment shadow maps</strong>: This technique uses statistical moments to store more information about the depth distribution within a pixel, which can handle transparency and provide anti-aliased, soft shadows. Moment shadow maps require more memory and computation than basic shadow maps but can<a id="_idIndexMarker380"/> provide higher-quality shadows.</li></ul></li>
</ul>
<h2 id="_idParaDest-219"><a id="_idTextAnchor255"/>See also</h2>
<p>The following are references that discuss and provide implementation details about advanced techniques such as cascade shadow maps and moment shadows:</p>
<ul>
<li><a href="https://learn.microsoft.com/en-us/windows/win32/dxtecharts/cascaded-shadow-maps">https://learn.microsoft.com/en-us/windows/win32/dxtecharts/cascaded-shadow-maps</a></li>
<li><a href="https://momentsingraphics.de/I3D2015.html">https://momentsingraphics.de/I3D2015.html</a></li>
</ul>
<h1 id="_idParaDest-220"><a id="_idTextAnchor256"/>Implementing screen space ambient occlusion</h1>
<p><strong class="bold">Screen space ambient occlusion</strong> (<strong class="bold">SSAO</strong>) can be used to approximate the effect of ambient occlusion<a id="_idIndexMarker381"/> in real time. Ambient occlusion is a shading and rendering method used to calculate how exposed each point in a scene is to ambient lighting. This technique adds more realistic shadows where two surfaces or objects meet, or where an object blocks light from reaching another object.</p>
<p>In this recipe, you will learn how to implement SSAO to realistically estimate ambient occlusion in real time. You will grasp how to use this shading and rendering technique to calculate the exposure of each point in a scene to ambient lighting.</p>
<h2 id="_idParaDest-221"><a id="_idTextAnchor257"/>Getting ready</h2>
<p>The algorithm described in this recipe calculates the difference between the depth of a pixel and its neighbors (samples) in a circular fashion. If a sample is closer to the camera than the central pixel, it contributes to the occlusion factor, making the pixel darker.</p>
<p>A depiction of the algorithm is shown in <em class="italic">Figure 4</em><em class="italic">.2</em>. The code loops over several <em class="italic">rings</em> around a central point, the pixel being processed. Within each ring, it ta<a id="_idTextAnchor258"/>kes several samples, as seen in item (a).</p>
<div><div><img alt="Figure 4.2 – SSAO sampling pattern" src="img/B18491_04_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – SSAO sampling pattern</p>
<p>A small amount of noise is applied to each sample’s location to avoid banding effects, as shown in item (b). Additionally, a weight is applied to samples on the same ring, with rings farther from <a id="_idIndexMarker382"/>the center with the smallest weights, as depicted in item (c).</p>
<h2 id="_idParaDest-222"><a id="_idTextAnchor259"/>How to do it...</h2>
<p>The entire SSAO algorithm is implemented as a compute shader that writes its output to an image:</p>
<ol>
<li>We start by declaring the inputs and outputs. The input is the depth buffer. The output is an image in which we’ll store the result of the algorithm. We also need a function to generate noise in 2D. A small amount of noise is applied to each sample’s location to avoid banding effects:<pre class="source-code">
#version 460
layout(local_size_x = 16, local_size_y = 16,
       local_size_z = 1) in;
layout(set = 0, binding = 0,
       rgba8) uniform image2D OutputSSAO;
layout(set = 1, binding = 0) uniform sampler2D
    gBufferDepth;
const float nearDistance = .1f;
const float farDistance = 100.0f;
vec2 generateRandomNoise(
    in vec2 coord) // generating random noise
{
  float noiseX = (fract(
      sin(dot(coord, vec2(12.9898, 78.233))) *
      43758.5453));
  float noiseY = (fract(
      sin(dot(coord,
              vec2(12.9898, 78.233) * 2.0)) *
      43758.5453));
  return vec2(noiseX, noiseY) * 0.004;
}</pre></li> <li>We also need a <a id="_idIndexMarker383"/>function to convert the depth value from the depth buffer to a linear scale, as the values are not stored in a linear fashion:<pre class="source-code">
float calculateLinearDepth(float depth) {
  return (2.0 * nearDistance) /
         (farDistance + nearDistance -
          depth * (farDistance - nearDistance));
}</pre></li> <li>The comparison between the depth value of the pixel being processed and the surrounding samples is done by the <code>compareDepths</code> function, which returns the difference between the samples:<pre class="source-code">
float compareDepths(float depth1, float depth2) {
  const float aoCap = 0.5;
  const float aoMultiplier = 50.0;
  const float depthTolerance = 0.001;
  const float aoRange = 60.0;
  float depthDifference = sqrt(
      clamp(1.0 - (depth1 - depth2) /
                      (aoRange / (farDistance -
                                  nearDistance)),
            0.0, 1.0));
  float ao =
      min(aoCap, max(0.0, depth1 - depth2 -
                              depthTolerance) *
                     aoMultiplier) *
      depthDifference;
  return ao;
}</pre></li> <li>The main part of<a id="_idIndexMarker384"/> the algorithm starts by collecting the pixel’s position and its depth value, which is converted to a linear scale. It also calculates the size of the buffers and calculates the noise:<pre class="source-code">
void main() {
  if (gl_GlobalInvocationID.x &gt;=
          pushConstant.textureResolution.x ||
      gl_GlobalInvocationID.y &gt;=
          pushConstant.textureResolution.y) {
    return;
  }
  imageStore(OutputSSAO,
             ivec2(gl_GlobalInvocationID.xy),
             vec4(0));
  vec2 uv = (vec2(gl_GlobalInvocationID.xy) +
             vec2(0.5f)) /
            vec2(pushConstant.textureResolution);
  ivec2 pixelPos =
      ivec2(gl_GlobalInvocationID.xy);
  float depthBufferValue =
      texelFetch(gBufferDepth, pixelPos, 0).r;
  float depth =
      calculateLinearDepth(depthBufferValue);
  float textureWidth =
      float(pushConstant.textureResolution.x);
  float textureHeight =
      float(pushConstant.textureResolution.y);
  float aspectRatio =
      textureWidth / textureHeight;
  vec2 noise =
      generateRandomNoise(vec2(pixelPos));</pre></li> <li>The size of the area inspected for samples is proportional to the depth value of the pixel: the farther<a id="_idIndexMarker385"/> away the pixel is from the camera, the smaller the area is:<pre class="source-code">
  float w = (1.0 / textureWidth) /
                clamp(depth, 0.05, 1.0) +
            (noise.x * (1.0 - noise.x));
  float h = (1.0 / textureHeight) /
                clamp(depth, 0.05, 1.0) +
            (noise.y * (1.0 - noise.y));
  w *= textureWidth / 2.0;
  h *= textureHeight / 2.0;
  float sampleWidth;
  float sampleHeight;
  float ao = 0.0;
  float totalSamples = 0.0;
  float fade = 1.0;
  const int NUM_RINGS = 3;
  const int NUM_SAMPLES = 6;</pre></li> <li>The bulk of the algorithm is where the ring radiuses and the number of samples are calculated. The number of samples is proportional to the ring’s diameter. For each sample, we compare their depths, apply the ring weight, and accumulate the <a id="_idIndexMarker386"/>output, which is averaged out at the end of the function:<pre class="source-code">
  for (int i = 0; i &lt; NUM_RINGS; i++) {
    fade *= 0.5;
    for (int j = 0; j &lt; NUM_SAMPLES * i; j++) {
      float step = 3.14159265 * 2.0 /
                   float(NUM_SAMPLES * i);
      sampleWidth =
          (cos(float(j) * step) * float(i));
      sampleHeight =
          (sin(float(j) * step) * float(i));
      float newDepthValue =
          texelFetch(
              gBufferDepth,
              pixelPos +
                  ivec2(int(sampleWidth * w),
                        int(sampleHeight * h)),
              0)
              .r;
      ao += compareDepths(depth,
                          calculateLinearDepth(
                              newDepthValue)) *
            fade;
      totalSamples += 1.0 * fade;
    }
  }
  ao /= totalSamples;
  ao = 1.0 - ao;
  imageStore(OutputSSAO, ivec2(gl_GlobalInvocationID.xy), vec4(ao,ao,ao, 1.0));</pre></li> </ol>
<p>This concludes our recipe for SSAO. For a deeper understanding and further exploration, we highly <a id="_idIndexMarker387"/>recommend visiting the various resources provided in the following section.</p>
<h2 id="_idParaDest-223"><a id="_idTextAnchor260"/>See also</h2>
<p>For further understanding and an exploration of the topic of SSAO, you may find the following resources <a id="_idIndexMarker388"/>helpful:</p>
<ul>
<li><a href="https://github.com/NVIDIAGameWorks/HBAOPlus">https://github.com/NVIDIAGameWorks/HBAOPlus</a></li>
<li><a href="https://www.gamedevs.org/uploads/comparative-study-of-ssao-methods.pdf">https://www.gamedevs.org/uploads/comparative-study-of-ssao-methods.pdf</a></li>
<li><a href="https://research.nvidia.com/sites/default/files/pubs/2012-06_Scalable-Ambient-Obscurance/McGuire12SAO.pdf">https://research.nvidia.com/sites/default/files/pubs/2012-06_Scalable-Ambient-Obscurance/McGuire12SAO.pdf</a></li>
<li><a href="https://www.ppsloan.org/publications/vo.pdf">https://www.ppsloan.org/publications/vo.pdf</a></li>
<li><a href="https://github.com/GameTechDev/XeGTAO">https://github.com/GameTechDev/XeGTAO</a></li>
</ul>
<h1 id="_idParaDest-224"><a id="_idTextAnchor261"/>Implementing a lighting pass for illuminating the scene</h1>
<p>The last recipe in <a id="_idIndexMarker389"/>the book you how to<a id="_idIndexMarker390"/> implement a lighting pass; this is where we calculate the lighting for the scene. For each light in the scene, we draw a volume (for point lights, this would be a sphere; for directional lights, a full-screen quad; for spotlights, we would draw a cone) and for each pixel in that volume, we fetch the data from the G-buffer and calculate the lighting contribution of that light to the pixel. The results are then usually added together (blended) to a final render target to get the final image. In the demo, we only have one spotlight that is used as a demonstration, but we can easily add multiple lights. For each light in the scene, we will need to consider the area affected by the light (i.e., we use a shader that fetches the relevant data for each pixel from the G-buffer, which then uses this data to calculate how much this light source contributes to the final color of each pixel). For example, if we’re dealing with a spotlight, this volume is a cone centered at the light’s position, oriented in the light’s direction, and with an angle that matches the spread of the spotlight. The length or height of the cone should be equal to the range of the spotlight. Lastly, we use a physically based lighting <a id="_idIndexMarker391"/>model (the <strong class="bold">Cook-Torrance lighting model</strong>), which is applied in the fragment shader. The inputs to the lighting model include the light’s properties (color, intensity, position, etc.) and the surface properties (material color, shininess, normal, etc.), which are fetched from the G-buffer.</p>
<h2 id="_idParaDest-225"><a id="_idTextAnchor262"/>Getting ready</h2>
<p>The recipe is implemented in the <code>LightingPass</code> class and the <code>Lighting.frag</code> shader. It simply uses a full-screen vertex shader to draw a full-screen quad.</p>
<p>As mentioned in the introduction of this recipe, we use the Cook-Torrance lighting model, which is a physically based rendering model that simulates how light interacts with a surface. It considers various factors such as the angle of incidence, surface roughness, and microfacet distribution to render realistic lighting effects. The algorithm uses the <code>Fresnel-Schlick</code> function, which is used to determine the proportion of light reflected versus refracted depending on the view angle. The GGX distribution function calculates the distribution of microfacets on the surface, which influences how rough or smooth a surface appears.</p>
<h2 id="_idParaDest-226"><a id="_idTextAnchor263"/>How to do it…</h2>
<p>The entire algorithm is implemented as a full-screen space quad shader that writes its output to a final color texture:</p>
<ol>
<li>We start by declaring the inputs and outputs. The inputs include the G-buffer data (normal, specular, base color, depth, and position), ambient occlusion map, shadow map, camera, and light data. The output is simply <code>fragColor</code>, which is written to the <a id="_idIndexMarker392"/>color <a id="_idIndexMarker393"/>attachment as specified by the render pass:<pre class="source-code">
#version 460
#extension GL_EXT_nonuniform_qualifier : require
#extension GL_GOOGLE_include_directive : require
layout(set = 0, binding = 0)uniform sampler2D gBufferWorldNormal;
layout(set = 0, binding = 1)uniform sampler2D gBufferSpecular;
layout(set = 0, binding = 2)uniform sampler2D gBufferBaseColor;
layout(set = 0, binding = 3)uniform sampler2D gBufferDepth;
layout(set = 0, binding = 4)uniform sampler2D gBufferPosition;
layout(set = 0, binding = 5)uniform sampler2D ambientOcclusion;
layout(set = 0, binding = 6)uniform sampler2DShadow shadowMap;
layout(set = 1, binding = 0)uniform Transforms
{
    mat4 viewProj;
    mat4 viewProjInv;
    mat4 viewInv;
} cameraData;
layout(set = 1, binding = 1)uniform Lights
{
    vec4 lightPos;
    vec4 lightDir;
    vec4 lightColor;
    vec4 ambientColor; // environment light color
    mat4 lightVP;
    float innerConeAngle;
    float outerConeAngle;
} lightData;
layout(location=0) in vec2 fragTexCoord;
layout(location = 0) out vec4 outColor;</pre></li> <li>Afterward, we<a id="_idIndexMarker394"/> define<a id="_idIndexMarker395"/> a few auxiliary functions; these will be used in the main function. These are defined in the <code>brdf.glsl</code> file.<p class="list-inset">The <code>fresnelSchlick</code> function calculates the <code>Fresnel-Schlick</code> approximation, which models the amount of reflection and refraction based on the angle at which light hits the surface. The result is used to determine the specular color.</p><p class="list-inset">The <code>distributionGGX</code> function calculates the distribution of microfacets on a surface. The result models how rough or smooth a surface appears, influencing the spread of the specular highlight.</p><p class="list-inset">The <code>geometrySchlickGGX</code> function calculates the geometric attenuation term using <code>geometrySmith</code> function calculates the total geometric attenuation considering both the view direction and the light direction. It calculates geometric attenuation for both view and light direction and multiplies both to get the final geometric attenuation, this function assumes that microfacet distribution is the same in all directions. These functions are <a id="_idIndexMarker397"/>combined <a id="_idIndexMarker398"/>in the <code>Cook-Torrance BRDF</code> model to account for microfacet occlusion and shadowing effects:</p><pre class="source-code">
vec3 fresnelSchlick(float cosTheta, vec3 F0) {
  return F0 + (1.0 - F0) * pow(1.0 - cosTheta, 5.0);
}
float distributionGGX(vec3 N, vec3 H,
                      float roughness) {
  float a = roughness * roughness;
  float a2 = a * a;
  float NdotH = max(dot(N, H), 0.0);
  float NdotH2 = NdotH * NdotH;
  float nom = a2;
  float denom = (NdotH2 * (a2 - 1.0) + 1.0);
  denom = 3.14159265359 * denom * denom;
  return nom / denom;
}
float geometrySchlickGGX(float NdotV,
                         float roughness) {
  float r = (roughness + 1.0) * 0.5;
  float r2 = r * r;
  float nom = NdotV;
  float denom = NdotV * (1.0 - r2) + r2;
  return nom / denom;
}
float geometrySmith(vec3 N, vec3 V, vec3 L,
                    float roughness) {
  float NdotV = max(dot(N, V), 0.0);
  float NdotL = max(dot(N, L), 0.0);
  float ggx2 = geometrySchlickGGX(NdotV, roughness);
  float ggx1 = geometrySchlickGGX(NdotL, roughness);
  return ggx1 * ggx2;
}</pre></li> <li>The shader first retrieves the base color, world position, camera position, specular data, and normal data from the G-buffer:<pre class="source-code">
void main() {
  vec4 worldPos =
      texture(gBufferPosition, fragTexCoord);
  vec3 basecolor =
      texture(gBufferBaseColor, fragTexCoord).rgb;
  if (worldPos.x == 0.0 &amp;&amp; worldPos.y == 0.0 &amp;&amp;
      worldPos.z == 0.0 &amp;&amp; worldPos.w == 0.0) {
    outColor = vec4(basecolor, 1.0);
    return;
  }
  vec2 gbufferSpecularData =
      texture(gBufferSpecular, fragTexCoord).rg;
  float metallic = gbufferSpecularData.r;
  float roughness = gbufferSpecularData.g;
  vec4 gbufferNormalData =
      texture(gBufferWorldNormal, fragTexCoord);
  vec3 N = normalize(gbufferNormalData.xyz);</pre></li> <li>In the next steps, it <a id="_idIndexMarker399"/>calculates <a id="_idIndexMarker400"/>the view vector (<code>V</code>), light vector (<code>L</code>), and half vector (<code>H</code>). These vectors are used in lighting calculations:<pre class="source-code">
vec3 camPos = cameraData.viewInv[3].xyz;
vec3 V = normalize(camPos - worldPos.xyz);
vec3 F0 = vec3(0.04);
F0 = mix(F0, basecolor, metallic);
vec3 L = normalize(
    lightData.lightDir.xyz -
    worldPos.xyz); // Using spotlight direction
vec3 H = normalize(V + L);</pre></li> <li>During this part, the <code>Fresnel-Schlick</code>, <code>distributionGGX</code>, and <code>geometrySmith</code> functions <a id="_idIndexMarker401"/>are called to <a id="_idIndexMarker402"/>calculate the specular reflection:<pre class="source-code">
vec3 F = fresnelSchlick(max(dot(H, V), 0.0), F0);
float D = distributionGGX(N, H, roughness);
float G = geometrySmith(N, V, L, roughness);
vec3 nominator = D * G * F;
float denominator = 4.0 * max(dot(N, V), 0.0) *
                        max(dot(N, L), 0.0) +
                    0.001;
vec3 specular = nominator / denominator;</pre></li> <li>In this step, the shader calculates the diffuse reflection. It’s a simple model based on Lambert’s cosine law, but it’s modified by applying an energy conservation principle:<pre class="source-code">
vec3 kS = F;
vec3 kD = vec3(1.0) - kS;
kD *= 1.0 - metallic;
float NdotL = max(dot(N, L), 0.0);
vec3 diffuse = kD * basecolor / 3.14159265359;</pre></li> <li>In these last few steps, ambient light is calculated by simply multiplying the ambient light color by the base color. Here, we also calculate the attenuation based on the distance to the light, and the spot attenuation based on the angle between the light direction and the direction to the fragment. These are used to calculate the light intensity. The shader calculates the final color by adding the ambient, diffuse, and <a id="_idIndexMarker403"/>specular<a id="_idIndexMarker404"/> components together:<pre class="source-code">
vec3 ambient =
    lightData.ambientColor.rgb * basecolor;
// Spotlight calculations
vec3 lightToFragment =
    lightData.lightPos.xyz - worldPos.xyz;
vec3 lightDirection =
    normalize(-lightData.lightDir.xyz);
float distanceToLight = length(lightToFragment);
float attenuation =
    1.0 /
    (1.0 + 0.1 * distanceToLight +
     0.01 * distanceToLight * distanceToLight);
vec3 lightDir = normalize(lightToFragment);
float cosTheta = dot(-lightDir, lightDirection);
float spotAttenuation =
    smoothstep(lightData.outerConeAngle,
               lightData.innerConeAngle, cosTheta);
vec3 lightIntensity = spotAttenuation *
                      attenuation *
                      lightData.lightColor.rgb;
// Final light contribution
vec3 finalColor = (NdotL * (lightIntensity) *
                   (diffuse + specular)) +
                  ambient;</pre></li> <li>In the last step, the final color is then multiplied by the ambient occlusion factor (sampled from ambient occlusion texture calculated in the <em class="italic">Implementing screen space ambient occlusion</em> recipe) and the shadow visibility factor to account for<a id="_idIndexMarker405"/> shadows <a id="_idIndexMarker406"/>and ambient occlusion:<pre class="source-code">
float ao =
    texture(ambientOcclusion, fragTexCoord).r;
finalColor *= ao;
outColor = vec4(finalColor, 1.0);
vec4 shadowProjPos =
    lightData.lightVP * vec4(worldPos.xyz, 1.0f);
float vis = PCF(shadowProjPos);
if (vis &lt;= .001) {
  vis = .3;
}
outColor.xyz *= vis;</pre><p class="list-inset">In the following screenshot, we present an image that shows a shadow created using the <a id="_idIndexMarker407"/>shadow <a id="_idIndexMarker408"/>map technique:</p></li> </ol>
<div><div><img alt="Figure 4.3 – A shadow using a shadow map" src="img/B18491_04_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – A shadow using a shadow map</p>
<p class="list-inset">The following screenshot demonstrates the result of the SSR technique:</p>
<div><div><img alt="Figure 4.4 – SSR" src="img/B18491_04_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – SSR</p>
<p>In this chapter, we embarked on a journey to comprehend and implement some of the most influential techniques in 3D graphics for achieving real-time physically based effects using the Vulkan API. We began our exploration with the principles of G-buffer generation, a foundational concept<a id="_idIndexMarker409"/> of <a id="_idIndexMarker410"/>deferred rendering. This technique allows us to manage the complexity of modern lighting and shading, paving the way for the implementation of more advanced rendering effects. We then described techniques such as SSR and shadows, which are needed for simulating realism in rendered scenes.  We also explored the complexities of lighting and shading with a deep dive into SSAO. This technique provided us with the tools to simulate the intricate ways light radiates in real life, adding depth and detail to corners in our 3D world. Finally, our exploration ended with the implementation of a lighting pass. By calculating the contributions of various light sources on each object in our scene, we successfully illuminated our 3D environment. We hope that you have gained a comprehensive understanding of several core techniques in modern lighting, shading, and shadows, which will empower you<a id="_idIndexMarker411"/> to<a id="_idIndexMarker412"/> create stunning and realistic 3D graphics with Vulkan.</p>
</div>
</body></html>