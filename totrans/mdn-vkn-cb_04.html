<html><head></head><body>
<div id="_idContainer047">
<h1 class="chapter-number" id="_idParaDest-205"><a id="_idTextAnchor241"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-206"><a id="_idTextAnchor242"/><span class="koboSpan" id="kobo.2.1">Exploring Techniques for Lighting, Shading, and Shadows</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Welcome to an exploration of lighting and shading techniques designed to infuse realism into your scenes. </span><span class="koboSpan" id="kobo.3.2">In the world of graphics, both lighting and shading play an integral role in enhancing the aesthetic appeal and realism of 3D visuals. </span><span class="koboSpan" id="kobo.3.3">This chapter delves into these topics, presenting a spectrum of algorithms ranging from the fundamental to the complex which can add realism to your scenes. </span><span class="koboSpan" id="kobo.3.4">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">following recipes:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.5.1">Implementing G-buffer for </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">deferred rendering</span></span></li>
<li><span class="koboSpan" id="kobo.7.1">Implementing screen </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">space reflections</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Implementing shadow maps for </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">real-time shadows</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Implementing screen space </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">ambient occlusion</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Implementing a lighting pass for illuminating </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">the scene</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.15.1">By the end of this chapter, you will have a comprehensive understanding of these techniques, enabling you to adeptly implement them in your </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">rendering projects.</span></span></p>
<h1 id="_idParaDest-207"><a id="_idTextAnchor243"/><span class="koboSpan" id="kobo.17.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.18.1">For this chapter, you will need to make sure you have VS 2022 installed along with the Vulkan SDK. </span><span class="koboSpan" id="kobo.18.2">Basic familiarity with the C++ programming language and an understanding of OpenGL or any other graphics API will be useful. </span><span class="koboSpan" id="kobo.18.3">Please revisit </span><a href="B18491_01.xhtml#_idTextAnchor019"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.19.1">Chapter 1</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.20.1">, Vulkan Core Concepts</span></em><span class="koboSpan" id="kobo.21.1">, under the T</span><em class="italic"><span class="koboSpan" id="kobo.22.1">echnical requirements</span></em><span class="koboSpan" id="kobo.23.1"> section for details on setting up and building executables for this chapter. </span><span class="koboSpan" id="kobo.23.2">We also assume that by now you are familiar with how to use the Vulkan API and various concepts that were introduced in previous chapters. </span><span class="koboSpan" id="kobo.23.3">All recipes for this chapter are encapsulated in a single executable and can be launched using </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.24.1">Chapter04_Deferred_Renderer.exe</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.25.1"> executable.</span></span></p>
<h1 id="_idParaDest-208"><a id="_idTextAnchor244"/><span class="koboSpan" id="kobo.26.1">Implementing G-buffer for deferred rendering</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.27.1">Deferred rendering</span></strong><span class="koboSpan" id="kobo.28.1"> is a</span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.29.1"> technique </span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.30.1">that adds an additional render pass at the beginning of the scene rendering that accumulates various information about the scene in screen space, such as position, surface normal, surface color, and others. </span><span class="koboSpan" id="kobo.30.2">This extra information is stored in a buffer called the </span><strong class="bold"><span class="koboSpan" id="kobo.31.1">geometry buffer</span></strong><span class="koboSpan" id="kobo.32.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.33.1">G-buffer</span></strong><span class="koboSpan" id="kobo.34.1">), where each one of the values computed during this step is stored for each pixel. </span><span class="koboSpan" id="kobo.34.2">Once this initial pass has finished, the final scene rendering can take place, and the extra information to improve the rendering quality by computing things such as reflections, ambient occlusion, atmospheric effects, and others can be used. </span><span class="koboSpan" id="kobo.34.3">The benefit of using deferred rendering is that it provides more efficient handling of complex scenes with many lights, as each light only needs to be calculated once per pixel, rather than once per object. </span><span class="koboSpan" id="kobo.34.4">We have essentially decoupled geometry and shading, which allows for more flexibility in the rendering pipeline. </span><span class="koboSpan" id="kobo.34.5">The technique also has some disadvantages, such as increased memory usage (for the G-buffer itself), and difficulty handling transparency </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">and anti-aliasing.</span></span></p>
<p><span class="koboSpan" id="kobo.36.1">In this tutorial, you will gain an understanding of the implementation of G-buffer for deferred rendering, its advantages in managing complex scenes with multiple lights, and the challenges it may present, such as increased </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">memory usage.</span></span></p>
<h2 id="_idParaDest-209"><a id="_idTextAnchor245"/><span class="koboSpan" id="kobo.38.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.39.1">Creating a G-buffer in Vulkan is somewhat straightforward. </span><span class="koboSpan" id="kobo.39.2">The bulk of the technique relies on creating a framebuffer that contains references to all render targets (textures) that will store the scene’s information, such as position, normal, and material data. </span><span class="koboSpan" id="kobo.39.3">The render pass also needs to dictate how those render targets should be loaded and stored at the end of the pass. </span><span class="koboSpan" id="kobo.39.4">Finally, in the fragment shader, each render target is specified as an output variable and the value of each render target is written to the output that refers to the correct texture or </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">storage buffer.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer043">
<span class="koboSpan" id="kobo.41.1"><img alt="Figure 4.1 – G-buffer textures" src="image/B18491_04_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.42.1">Figure 4.1 – G-buffer textures</span></p>
<p><span class="koboSpan" id="kobo.43.1">In the</span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.44.1"> repository, the</span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.45.1"> G-buffer generation is encapsulated in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.46.1">GBufferPass</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.47.1"> class.</span></span></p>
<h2 id="_idParaDest-210"><a id="_idTextAnchor246"/><span class="koboSpan" id="kobo.48.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.49.1">To generate a G-buffer and its artifacts, we need to first create a framebuffer and a corresponding </span><strong class="source-inline"><span class="koboSpan" id="kobo.50.1">RenderPass</span></strong><span class="koboSpan" id="kobo.51.1">. </span><span class="koboSpan" id="kobo.51.2">In the following steps, we will show you how to create targets for the base color of the material, the normal, and the </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">depth components:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.53.1">Before creating theFrambuffer object, it is necessary to create the textures (render targets) that will store the output of the </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">G-buffer pass:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.55.1">
gBufferBaseColorTexture_ = context-&gt;createTexture(
    VK_IMAGE_TYPE_2D, VK_FORMAT_R8G8B8A8_UNORM, 0,
    VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT |
        VK_IMAGE_USAGE_SAMPLED_BIT |
        VK_IMAGE_USAGE_STORAGE_BIT,…
gBufferNormalTexture_ = context-&gt;createTexture(
    VK_IMAGE_TYPE_2D,
    VK_FORMAT_R16G16B16A16_SFLOAT, 0,
    VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT |
        VK_IMAGE_USAGE_SAMPLED_BIT |
        VK_IMAGE_USAGE_STORAGE_BIT,…
gBufferPositionTexture_ = context-&gt;createTexture(
      VK_IMAGE_TYPE_2D, VK_FORMAT_R16G16B16A16_SFLOAT, 0,
      VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT | VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_STORAGE_BIT,…
depthTexture_ = context-&gt;createTexture(
    VK_IMAGE_TYPE_2D, VK_FORMAT_D24_UNORM_S8_UINT,
    0,
    VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT |
        VK_IMAGE_USAGE_TRANSFER_DST_BIT |
        VK_IMAGE_USAGE_SAMPLED_BIT,…</span></pre></li> <li><span class="koboSpan" id="kobo.56.1"> The </span><strong class="source-inline"><span class="koboSpan" id="kobo.57.1">Framebuffer</span></strong><span class="koboSpan" id="kobo.58.1"> object </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.59.1">references </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.60.1">the preceding targets. </span><span class="koboSpan" id="kobo.60.2">The order is important here and should be mirrored in the shader where the outputs </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">are specified:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.62.1">
frameBuffer_ = context-&gt;createFramebuffer(
      renderPass_-&gt;vkRenderPass(),
      {gBufferBaseColorTexture_, gBufferNormalTexture_, gBufferEmissiveTexture_,
       gBufferSpecularTexture_, gBufferPositionTexture_, depthTexture_},
      nullptr, nullptr, "GBuffer framebuffer ");</span></pre></li> <li><span class="koboSpan" id="kobo.63.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.64.1">RenderPass</span></strong><span class="koboSpan" id="kobo.65.1"> object describes how each render target should be loaded and stored. </span><span class="koboSpan" id="kobo.65.2">The</span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.66.1"> operations</span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.67.1"> should match the order of the targets used by </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">the framebuffer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.69.1">
renderPass_ = context-&gt;createRenderPass(
      {gBufferBaseColorTexture_, gBufferNormalTexture_, gBufferEmissiveTexture_,
       gBufferSpecularTexture_, gBufferPositionTexture_, depthTexture_},
      {VK_ATTACHMENT_LOAD_OP_CLEAR, VK_ATTACHMENT_LOAD_OP_CLEAR,
       VK_ATTACHMENT_LOAD_OP_CLEAR, VK_ATTACHMENT_LOAD_OP_CLEAR,
       VK_ATTACHMENT_LOAD_OP_CLEAR, VK_ATTACHMENT_LOAD_OP_CLEAR},
      {VK_ATTACHMENT_STORE_OP_STORE, VK_ATTACHMENT_STORE_OP_STORE,
       VK_ATTACHMENT_STORE_OP_STORE, VK_ATTACHMENT_STORE_OP_STORE,
       VK_ATTACHMENT_STORE_OP_STORE, VK_ATTACHMENT_STORE_OP_STORE},
      // final layout for all attachments
      {VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL, VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL,
       VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL, VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL,
       VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL,
       VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL},
      VK_PIPELINE_BIND_POINT_GRAPHICS, "GBuffer RenderPass");</span></pre></li> <li><span class="koboSpan" id="kobo.70.1">In the fragment shader, besides the input data originating from the previous stages in the pipeline, the output data is directed to each one of the targets using the layout keyword and the location qualifier. </span><span class="koboSpan" id="kobo.70.2">The location index must match the render </span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.71.1">target </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.72.1">index on </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">the framebuffer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.74.1">
layout(location=0) in vec2 inTexCoord;
layout(location=1) in flat uint inflatMeshId;
layout(location=2) in flat int inflatMaterialId;
layout(location=3) in vec3 inNormal;
layout(location=4) in vec4 inTangent;
layout(location = 0) out vec4 outgBufferBaseColor;
layout(location = 1) out vec4 outgBufferWorldNormal;
layout(location = 2) out vec4 outgBufferEmissive;
layout(location = 3) out vec4 outgBufferSpecular;
layout(location = 4) out vec4 outgBufferPosition;
const vec3 n = normalize(inNormal);
const vec3 t = normalize(inTangent.xyz);
const vec3 b = normalize(cross(n,t) * inTangent.w);
const mat3 tbn =  mat3(t, b, n);
outgBufferWorldNormal.rgb = normalize(tbn * normalize(normalTan));</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.75.1">In the preceding code snippet, the world normal is calculated based on the normal and tangent values and stored in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.76.1">outgBufferWorldNormal</span></strong><span class="koboSpan" id="kobo.77.1"> location, which corresponds to</span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.78.1"> the </span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.79.1">attachment with </span><strong class="source-inline"><span class="koboSpan" id="kobo.80.1">index 1</span></strong><span class="koboSpan" id="kobo.81.1"> (see code fragment in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.82.1">step 2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">).</span></span></p>
<h1 id="_idParaDest-211"><a id="_idTextAnchor247"/><span class="koboSpan" id="kobo.84.1">Implementing screen space reflections</span></h1>
<p><span class="koboSpan" id="kobo.85.1">Physically correct </span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.86.1">reflections involve tracing the path of light rays as they bounce off surfaces. </span><span class="koboSpan" id="kobo.86.2">This process accounts for the geometry, material properties, and light sources in the scene, as well as the view angle. </span><span class="koboSpan" id="kobo.86.3">However, it is a very computationally intensive process, often too demanding for real-time rendering, especially in complex scenes or on less powerful hardware. </span><span class="koboSpan" id="kobo.86.4">To achieve a balance between visual quality and performance, an approximation technique known as </span><strong class="bold"><span class="koboSpan" id="kobo.87.1">screen space reflection</span></strong><span class="koboSpan" id="kobo.88.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.89.1">SSR</span></strong><span class="koboSpan" id="kobo.90.1">) can be used. </span><span class="koboSpan" id="kobo.90.2">SSR is a method that approximates reflections by reusing data that has already been rendered to the screen. </span><span class="koboSpan" id="kobo.90.3">By utilizing a screen-space variant, the heavy computational cost associated with physically correct reflections can be significantly reduced, making it a viable technique for real-time rendering. </span><span class="koboSpan" id="kobo.90.4">In this recipe, we will explain how to compute reflections using buffers derived from the previous section, such as the normal and </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">depth buffers.</span></span></p>
<h2 id="_idParaDest-212"><a id="_idTextAnchor248"/><span class="koboSpan" id="kobo.92.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.93.1">SSR uses the depth buffer to find intersections between a reflected ray and the geometry’s depth. </span><span class="koboSpan" id="kobo.93.2">The reflection ray is computed in world space based on the surface normal and the view direction and is marched in small increments until it leaves the screen bounds. </span><span class="koboSpan" id="kobo.93.3">For every step, the ray’s location is projected onto the screen and its coordinates are compared against the depth buffer. </span><span class="koboSpan" id="kobo.93.4">If the difference between the ray’s location and the depth buffer’s depth is less than a small threshold, then the ray has collided with some geometry, and the ray’s originating point on the surface is obscured. </span><span class="koboSpan" id="kobo.93.5">This reflection vector is then used to look up the color of the pixel in the already-rendered image at the reflected position. </span><span class="koboSpan" id="kobo.93.6">This color is then used as the reflected color, creating the illusion of a reflection. </span><span class="koboSpan" id="kobo.93.7">SSR can produce visually pleasing reflections that come close to those produced by much more computationally expensive physically correct reflection models; however, it can only reflect what’s already visible on the screen, and it may produce inaccurate results for complex surfaces or at </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">screen edges.</span></span></p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor249"/><span class="koboSpan" id="kobo.95.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.96.1">Once the depth and normal buffers have been calculated, the SSR can be easily computed in a render or </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">compute pass:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.98.1">The following SSR code is used by a compute pass and specifies the buffers used as input, generated by the deferred rendering step, as well as the transformation </span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.99.1">data it needs to perform the intersection in </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">screen space:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.101.1">
layout(set = 0, binding = 0, rgba16f) uniform image2D SSRIntersect;
layout(set = 1, binding = 0)uniform sampler2D gBufferWorldNormal;
layout(set = 1, binding = 1)uniform sampler2D gBufferSpecular;
layout(set = 1, binding = 2)uniform sampler2D gBufferBaseColor;
layout(set = 1, binding = 3)uniform sampler2D hierarchicalDepth;
layout(set = 2, binding = 0)uniform Transforms
{
  mat4 model;
  mat4 view;
  mat4 projection;
  mat4 projectionInv;
  mat4 viewInv;
} cameraData;</span></pre></li> <li><span class="koboSpan" id="kobo.102.1">Two auxiliary functions are defined in the shader. </span><span class="koboSpan" id="kobo.102.2">They perform the projection from a point in world space to screen space and calculate the projection from a screen space along with depth coordinate to </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">world space:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.104.1">
vec3 generatePositionFromDepth(vec2 texturePos, float depth);
vec2 generateProjectedPosition(vec3 worldPos);</span></pre></li> <li><span class="koboSpan" id="kobo.105.1">In this step, the data needed for the reflection calculations from the G-buffer is fetched. </span><span class="koboSpan" id="kobo.105.2">This includes the world normal, specular data, and base color for the current pixel. </span><span class="koboSpan" id="kobo.105.3">The UV coordinates are calculated, which are used to sample the base color from the G-buffer. </span><span class="koboSpan" id="kobo.105.4">The roughness, which controls how blurry or sharp the reflection is, is also extracted from the specular data. </span><span class="koboSpan" id="kobo.105.5">We also check the metalness value from the G-buffer specular data. </span><span class="koboSpan" id="kobo.105.6">If the material is not metallic (</span><strong class="source-inline"><span class="koboSpan" id="kobo.106.1">metalness &lt; 0.01</span></strong><span class="koboSpan" id="kobo.107.1">), it assumes it doesn’t reflect and simply writes the base color to the </span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.108.1">result </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">and exits:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.110.1">
layout(local_size_x = 16, local_size_y = 16,
       local_size_z = 1) in;
void main() {
  // Return if the coordinate is outside the screen
  ...
</span><span class="koboSpan" id="kobo.110.2">  imageStore(SSRIntersect,
             ivec2(gl_GlobalInvocationID.xy),
             vec4(0));
  vec2 uv = (vec2(gl_GlobalInvocationID.xy) +
             vec2(0.5f)) /
            vec2(pushConstant.textureResolution);
  ivec2 pixelPos = ivec2(gl_GlobalInvocationID.xy);
  vec4 gbufferNormalData =
      texelFetch(gBufferWorldNormal, pixelPos, 0);
  vec4 gbufferSpecularData =
      texelFetch(gBufferSpecular, pixelPos, 0);
  vec3 basecolor =
      texture(gBufferBaseColor, uv).xyz;
  float roughness = gbufferSpecularData.g;
  if (gbufferSpecularData.r &lt;
      .01) { // Metal-ness check
    imageStore(SSRIntersect,
               ivec2(gl_GlobalInvocationID.xy),
               vec4(basecolor, 1.0));
    return;
  }</span></pre></li> <li><span class="koboSpan" id="kobo.111.1">The following snippet fetches the depth of the current pixel from the depth buffer and generates the world position of the pixel using the UV and depth. </span><span class="koboSpan" id="kobo.111.2">The view direction</span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.112.1"> is calculated from the camera position to the pixel’s world position. </span><span class="koboSpan" id="kobo.112.2">The reflection direction is then calculated using the view direction and the normal. </span><span class="koboSpan" id="kobo.112.3">The shader then performs ray marching along the reflection direction in screen space. </span><span class="koboSpan" id="kobo.112.4">It steps along the reflection ray and at each step, it checks whether the ray has intersected with any geometry, based on the depth difference between the current position of the ray and the depth at the corresponding </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">screen position:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.114.1">
  float z =
      texelFetch(hierarchicalDepth, pixelPos, 0).r;
  vec3 position = generatePositionFromDepth(uv, z);
  vec3 normal = normalize(gbufferNormalData.xyz);
  vec3 camPos = cameraData.viewInv[3].xyz;
  vec3 viewDirection = normalize(position - camPos);
  vec3 reflectionDirection =
      reflect(viewDirection, normal);
;
  float stepSize = 0.05; // Initial step size
  vec3 currentPos = position;
  for (int i = 0; i &lt; 50; i++) {
    currentPos += reflectionDirection * stepSize;
    vec2 screenPos =
        generateProjectedPosition(currentPos);
    if (screenPos.x &lt; 0.0 || screenPos.x &gt; 1.0 ||
        screenPos.y &lt; 0.0 || screenPos.y &gt; 1.0) {
      break; // Ray went out of screen bounds
    }
    float depthAtCurrent =
        texture(hierarchicalDepth, screenPos).r;
    vec3 positionFromDepth =
        generatePositionFromDepth(screenPos,
                                  depthAtCurrent);
    float depthDifference =
        length(currentPos - positionFromDepth);</span></pre></li> <li><span class="koboSpan" id="kobo.115.1">If an intersection is found, the code fetches the color at the intersection point and blends it with the base color. </span><span class="koboSpan" id="kobo.115.2">The blending is based on the roughness value, which represents </span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.116.1">a characteristic of the surface at the </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">intersection point:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.118.1">
    if (depthDifference &lt; 0.05) {
      vec3 hitColor =
          texture(gBufferBaseColor, screenPos).xyz;
      if (hitColor.x &lt;= .1 &amp;&amp; hitColor.y &lt;= .1 &amp;&amp;
          hitColor.z &lt;= .1 &amp;&amp; hitColor.x &gt;= .08 &amp;&amp;
          hitColor.y &gt;= .08 &amp;&amp;
          hitColor.z &gt;=
              .08) { // .1 is considered sky color,
                     // ignore if we hit sky
        hitColor = basecolor;
      }
      vec3 blendColor =
          hitColor * (1.0 - roughness) +
          roughness * basecolor;
      imageStore(SSRIntersect,
                 ivec2(gl_GlobalInvocationID.xy),
                 vec4(blendColor, 1.0));
      return;
    }
  }
  // Fallback
  imageStore(SSRIntersect,
             ivec2(gl_GlobalInvocationID.xy),
             vec4(basecolor, 1.0));
}</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.119.1">In the preceding code, we </span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.120.1">learned how SSR is computed using the depth and </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">normal buffers.</span></span></p>
<h2 id="_idParaDest-214"><a id="_idTextAnchor250"/><span class="koboSpan" id="kobo.122.1">See also</span></h2>
<p><span class="koboSpan" id="kobo.123.1">The following are some references that go into more detail on how to implement SSR; we suggest you go </span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.124.1">through these references to gain a more </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">thorough understanding:</span></span></p>
<ul>
<li><a href="https://interplayoflight.wordpress.com/2022/09/28/notes-on-screenspace-reflections-with-fidelityfx-sssr/"><span class="No-Break"><span class="koboSpan" id="kobo.126.1">https://interplayoflight.wordpress.com/2022/09/28/notes-on-screenspace-reflections-with-fidelityfx-sssr/</span></span></a></li>
<li><a href="http://roar11.com/2015/07/screen-space-glossy-reflections/"><span class="No-Break"><span class="koboSpan" id="kobo.127.1">http://roar11.com/2015/07/screen-space-glossy-reflections/</span></span></a></li>
<li><a href="https://interplayoflight.wordpress.com/2019/09/07/hybrid-screen-space-reflections/"><span class="No-Break"><span class="koboSpan" id="kobo.128.1">https://interplayoflight.wordpress.com/2019/09/07/hybrid-screen-space-reflections/</span></span></a></li>
</ul>
<h1 id="_idParaDest-215"><a id="_idTextAnchor251"/><span class="koboSpan" id="kobo.129.1">Implementing shadow maps for real-time shadows</span></h1>
<p><span class="koboSpan" id="kobo.130.1">As the name </span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.131.1">implies, </span><strong class="bold"><span class="koboSpan" id="kobo.132.1">shadow maps</span></strong><span class="koboSpan" id="kobo.133.1"> are used </span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.134.1">to simulate shadows. </span><span class="koboSpan" id="kobo.134.2">The goal of shadow mapping is to determine which parts of a scene are in shadow and which parts are illuminated by a light source by first rendering the scene from the light’s perspective, generating a </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">depth map.</span></span></p>
<p><span class="koboSpan" id="kobo.136.1">This depth map (also known as a shadow map) serves as a spatial record, storing the shortest distance from the light source to any point in the scene. </span><span class="koboSpan" id="kobo.136.2">By encapsulating the scene from the vantage point of the light source, the depth map effectively captures the areas of the scene that are directly visible to the light source and those that </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">are occluded.</span></span></p>
<p><span class="koboSpan" id="kobo.138.1">This depth map is then used during the main render pass to determine if the fragment can’t be reached from the light by comparing its depth value with the one in the depth map. </span><span class="koboSpan" id="kobo.138.2">For each fragment in the scene, we perform a test to evaluate whether it lies in shadow. </span><span class="koboSpan" id="kobo.138.3">This is achieved by comparing the depth value of the fragment from the light source, derived from the main camera’s perspective, with the corresponding depth value stored in the </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">depth map.</span></span></p>
<p><span class="koboSpan" id="kobo.140.1">If the fragment’s depth value exceeds the value recorded in the depth map, it implies that the fragment is occluded by another object in the scene and is, therefore, in shadow. </span><span class="koboSpan" id="kobo.140.2">Conversely, if the fragment’s depth value is less than or equal to the depth map value, it signifies that the fragment is directly visible to the light source and is </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">thus illuminated.</span></span></p>
<p><span class="koboSpan" id="kobo.142.1">In this recipe, you will learn how to implement shadow maps to create real-time shadows in your 3D scene. </span><span class="koboSpan" id="kobo.142.2">This involves understanding the theory of shadow mapping, generating a depth </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.143.1">map</span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.144.1"> from the light’s perspective, and finally using this depth map in the main render pass to accurately determine which fragments of the scene are in shadow and which </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">are illuminated.</span></span></p>
<h2 id="_idParaDest-216"><a id="_idTextAnchor252"/><span class="koboSpan" id="kobo.146.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.147.1">To obtain the shadow map, we first need to render the scene from the light’s perspective and retain the depth map. </span><span class="koboSpan" id="kobo.147.2">This render pass needs a depth texture that will store the depth information and simple vertex and fragment shaders. </span><span class="koboSpan" id="kobo.147.3">The main render pass, in which the scene is rendered, is where the depth map is used as a reference to determine if a pixel is lit or not and needs to refer to the shadow map generated in the previous step, along with a special sampler to access the depth map in the shader code. </span><span class="koboSpan" id="kobo.147.4">It also has code to perform the comparison between the fragment and the value stored in the </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">depth map.</span></span></p>
<p><span class="koboSpan" id="kobo.149.1">In the repository, a shadow map generation is encapsulated in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.150.1">ShadowPass</span></strong><span class="koboSpan" id="kobo.151.1"> class, and usage of shadow depth texture is encapsulated in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.152.1">LightingPass</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.153.1"> class.</span></span></p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor253"/><span class="koboSpan" id="kobo.154.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.155.1">We’ll start with a walk-through of the shadow map </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">pass first:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.157.1">The shadow map is a regular texture with a format that supports depth values. </span><span class="koboSpan" id="kobo.157.2">Our depth</span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.158.1"> texture is</span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.159.1"> 4x the resolution of the normal texture and uses the  </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.160.1">VK_FORMAT_D24_UNORM_S8_UINT</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.161.1"> format:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.162.1">
void ShadowPass::initTextures(
    VulkanCore::Context *context) {
  depthTexture_ = context-&gt;createTexture(
      VK_IMAGE_TYPE_2D, VK_FORMAT_D24_UNORM_S8_UINT,
      0,
      VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT |
          VK_IMAGE_USAGE_TRANSFER_DST_BIT |
          VK_IMAGE_USAGE_SAMPLED_BIT,
      {
          .width =
              context-&gt;swapchain()-&gt;extent().width *
              4, // 4x resolution for shadow maps
          .height = context-&gt;swapchain()
                        -&gt;extent()
                        .height *
                    4,
          .depth = 1,
      },
      1, 1, VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT,
      false, "ShadowMap Depth buffer");
}</span></pre></li> <li><span class="koboSpan" id="kobo.163.1">The render pass needs to clear the depth attachment at the beginning and then store it at the end. </span><span class="koboSpan" id="kobo.163.2">There are no color attachments in the shadow map render pass or in </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">the framebuffer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.165.1">
renderPass_ = context-&gt;createRenderPass(
    {depthTexture_}, {VK_ATTACHMENT_LOAD_OP_CLEAR},
    {VK_ATTACHMENT_STORE_OP_STORE},
    // final layout for all attachments
    {VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL},
    VK_PIPELINE_BIND_POINT_GRAPHICS,
    "ShadowMap RenderPass");
frameBuffer_ = context-&gt;createFramebuffer(
    renderPass_-&gt;vkRenderPass(), {depthTexture_},
    nullptr, nullptr, "ShadowMap framebuffer ");</span></pre></li> <li><span class="koboSpan" id="kobo.166.1">This render </span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.167.1">pass’s pipeline</span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.168.1"> definition needs to match the size of the viewport to the size of the shadow map and use the special vertex and fragment shaders for this pass. </span><span class="koboSpan" id="kobo.168.2">The fragment and vertex shader are conceptually identical to the G-buffer pass but they just need to output the depth buffer instead of multiple geometry buffers; it also needs a light view projection matrix instead of the camera’s one. </span><span class="koboSpan" id="kobo.168.3">As a future optimization, you could use the specialization constant with the G-buffer pass instead of using a </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">separate shader:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.170.1">
auto vertexShader = context-&gt;createShaderModule(
    (resourcesFolder / "shadowpass.vert").string(),
    VK_SHADER_STAGE_VERTEX_BIT, "shadowmap vertex");
auto fragmentShader = context-&gt;createShaderModule(
    (resourcesFolder / "empty.frag").string(),
    VK_SHADER_STAGE_FRAGMENT_BIT,
    "shadowmap fragment");
const VulkanCore::Pipeline::
    GraphicsPipelineDescriptor gpDesc = {
        .sets_ = setLayout,
        .vertexShader_ = vertexShader,
        .fragmentShader_ = fragmentShader,
        .dynamicStates_ =
            {VK_DYNAMIC_STATE_VIEWPORT,
             VK_DYNAMIC_STATE_SCISSOR},
        .colorTextureFormats = {},
        .depthTextureFormat =
            VK_FORMAT_D24_UNORM_S8_UINT,
        .sampleCount = VK_SAMPLE_COUNT_1_BIT,
        .cullMode = VK_CULL_MODE_BACK_BIT,
        .viewport = VkExtent2D(
            depthTexture_-&gt;vkExtents().width,
            depthTexture_-&gt;vkExtents().height),
        .depthTestEnable = true,
        .depthWriteEnable = true,
        .depthCompareOperation = VK_COMPARE_OP_LESS,
};
pipeline_ = context-&gt;createGraphicsPipeline(
    gpDesc, renderPass_-&gt;vkRenderPass(),</span></pre></li> <li><span class="koboSpan" id="kobo.171.1">The vertex shader needs the light’s transformation matrix, which is set into the </span><strong class="bold"><span class="koboSpan" id="kobo.172.1">model view projection</span></strong><span class="koboSpan" id="kobo.173.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.174.1">MVP</span></strong><span class="koboSpan" id="kobo.175.1">) matrix; the vertex shader is almost an identical copy </span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.176.1">of the G-buffer vertex shader. </span><span class="koboSpan" id="kobo.176.2">The shader applies the light transformation to the scene’s vertices and sends the data to the fragment shader. </span><span class="koboSpan" id="kobo.176.3">The depth value of each mesh will be recorded in the depth map, guided by the depth-related parameters we’ve defined in the preceding step. </span><span class="koboSpan" id="kobo.176.4">These parameters – including </span><strong class="source-inline"><span class="koboSpan" id="kobo.177.1">depthTestEnable</span></strong><span class="koboSpan" id="kobo.178.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.179.1">depthWriteEnable</span></strong><span class="koboSpan" id="kobo.180.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.181.1">depthCompareOperation</span></strong><span class="koboSpan" id="kobo.182.1"> – will govern how we evaluate and store depth information</span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.183.1"> during</span><a id="_idIndexMarker366"/> <span class="No-Break"><span class="koboSpan" id="kobo.184.1">this process:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.185.1">
#version 460
#extension GL_EXT_nonuniform_qualifier : require
#extension GL_EXT_debug_printf : enable
#extension GL_GOOGLE_include_directive : require
#include "CommonStructs.glsl"
#include "IndirectCommon.glsl"
void main() {
  Vertex vertex = vertexAlias[VERTEX_INDEX]
                      .vertices[gl_VertexIndex];
  vec3 position =
      vec3(vertex.posX, vertex.posY, vertex.posZ);
  gl_Position = MVP.projection * MVP.view *
                MVP.model * vec4(position, 1.0);
}</span></pre></li> <li><span class="koboSpan" id="kobo.186.1">The fragment shader is empty, as it doesn’t need to output any </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">color information:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.188.1">
#version 460
void main() {
}</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.189.1">The main render (lighting) pass uses the shadow map calculated before as a reference to determine if a fragment is lit or not. </span><span class="koboSpan" id="kobo.189.2">There is no special setup for the scene, except for the sampler used with the shadow map, which needs to enable a comparison function. </span><span class="koboSpan" id="kobo.189.3">The vertex and fragment shaders also need some special treatment to perform the depth comparison against the </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">shadow map.</span></span></p></li> <li><span class="koboSpan" id="kobo.191.1">The sampler used to access the shadow map in the shader needs to enable the comparison </span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.192.1">function. </span><span class="koboSpan" id="kobo.192.2">We use </span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.193.1">the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">VK_COMPARE_OP_LESS_OR_EQUAL </span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">function:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.196.1">
samplerShadowMap_ = context.createSampler(
    VK_FILTER_NEAREST, VK_FILTER_NEAREST,
    VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE,
    VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE,
    VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, 1.0f,
    </span><strong class="bold"><span class="koboSpan" id="kobo.197.1">true, VK_COMPARE_OP_LESS_OR_EQUAL</span></strong><span class="koboSpan" id="kobo.198.1">,
    "lighting pass shadow");</span></pre></li> <li><span class="koboSpan" id="kobo.199.1">The fragment shader needs a shadow map as well as a light’s view projection matrix. </span><span class="koboSpan" id="kobo.199.2">The following code includes the uniform </span><strong class="source-inline"><span class="koboSpan" id="kobo.200.1">sampler2Dshadow</span></strong><span class="koboSpan" id="kobo.201.1">, which holds the depth map or shadow map. </span><span class="koboSpan" id="kobo.201.2">The uniform </span><strong class="source-inline"><span class="koboSpan" id="kobo.202.1">Lights</span></strong><span class="koboSpan" id="kobo.203.1"> structure contains information about the light source, including its position, direction, color, and the view projection matrix from the </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">light’s perspective:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.205.1">
#version 460
layout(set = 0, binding = 6)uniform sampler2DShadow shadowMap;
layout(set = 1, binding = 1)uniform Lights
{
    vec4 lightPos;
    vec4 lightDir;
    vec4 lightColor;
    vec4 ambientColor; // environment light color
    mat4 lightVP;
    float innerConeAngle;
    float outerConeAngle;
} lightData;</span></pre></li> <li><span class="koboSpan" id="kobo.206.1">We introduce a </span><strong class="source-inline"><span class="koboSpan" id="kobo.207.1">computeShadow</span></strong><span class="koboSpan" id="kobo.208.1"> auxiliary function that takes as input a position in light-projective space. </span><span class="koboSpan" id="kobo.208.2">It first converts this position into </span><strong class="bold"><span class="koboSpan" id="kobo.209.1">normalized device coordinates</span></strong><span class="koboSpan" id="kobo.210.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.211.1">NDCs</span></strong><span class="koboSpan" id="kobo.212.1">), then</span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.213.1"> it looks up the shadow map at the corresponding position </span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.214.1">and</span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.215.1"> returns the shadow intensity at </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">that point:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.217.1">
#version 460
layout(set = 0, binding = 6)uniform sampler2DShadow shadowMap;
layout(set = 1, binding = 1)uniform Lights
{
    vec4 lightPos;
    vec4 lightDir;
    vec4 lightColor;
    vec4 ambientColor; // environment light color
    mat4 lightVP;
    float innerConeAngle;
    float outerConeAngle;
} lightData;</span></pre></li> <li><span class="koboSpan" id="kobo.218.1">Next, we introduce another auxiliary function, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.219.1">PCF</span></strong><span class="koboSpan" id="kobo.220.1"> function. </span><strong class="bold"><span class="koboSpan" id="kobo.221.1">Percentage closer filtering</span></strong><span class="koboSpan" id="kobo.222.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.223.1">PCF</span></strong><span class="koboSpan" id="kobo.224.1">) is a</span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.225.1"> technique used in shadow mapping to create softer, more natural-looking shadows. </span><span class="koboSpan" id="kobo.225.2">Shadow mapping, by its nature, creates shadows with hard edges because each pixel is either fully in shadow or fully lit. </span><span class="koboSpan" id="kobo.225.3">This is because the shadow mapping algorithm simply checks if a pixel is occluded from the light source or not. </span><span class="koboSpan" id="kobo.225.4">However, in real life, shadows often have softer edges due to the scattering of light. </span><span class="koboSpan" id="kobo.225.5">In the case of PCF, instead of sampling the shadow map only once for each pixel, we sample the shadow map multiple times in a small area around the pixel and average the result; for example, in our implementation PCF sample, the shadow map at </span><strong class="source-inline"><span class="koboSpan" id="kobo.226.1">16</span></strong><span class="koboSpan" id="kobo.227.1"> different offsets around the pixel. </span><span class="koboSpan" id="kobo.227.2">Then, it would calculate the average of these samples to determine the final shadow value for the pixel. </span><span class="koboSpan" id="kobo.227.3">This averaging process results in pixels on the edge of shadows having intermediate shadow values between fully lit and fully shadowed, creating </span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.228.1">a </span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.229.1">soft transition that makes the shadow look </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">more natural:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.231.1">
float PCF(vec4 shadowCoord) {
  vec2 texCoord = shadowCoord.xy / shadowCoord.w;
  texCoord = texCoord * .5 + .5;
  texCoord.y = 1.0 - texCoord.y;
  if (texCoord.x &gt; 1.0 || texCoord.y &gt; 1.0 ||
      texCoord.x &lt; 0.0 || texCoord.y &lt; 0.0) {
    return 1.0;
  }
  vec2 texSize = textureSize(shadowMap, 0);
  float result = 0.0;
  vec2 offset = (1.0 / texSize) * shadowCoord.w;
  for(float x = -1.5; x &lt;= 1.5; x += 1.0) {
    for(float y = -1.5; y &lt;= 1.5; y += 1.0) {
      result += computeShadow(shadowCoord + vec4(vec2(x, y) * offset, 0.0, 0.0));
    }
  }
  return result / 16.0;</span></pre></li> <li><span class="koboSpan" id="kobo.232.1">In the main function, we first retrieve the world position and base color of the fragment from the G-buffer. </span><span class="koboSpan" id="kobo.232.2">If the world position is zero (indicating no meaningful information), we simply set the output color to the base color and return early. </span><span class="koboSpan" id="kobo.232.3">As a next step, the world position is transformed into light-projective space using the light’s view projection matrix and passes this position to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">PCF</span></strong><span class="koboSpan" id="kobo.234.1"> function </span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.235.1">to </span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.236.1">compute the visibility factor, which represents how much the fragment is in shadow. </span><span class="koboSpan" id="kobo.236.2">If the visibility factor is below a threshold (meaning the fragment is in deep shadow), it sets the visibility to a fixed value for a minimum amount of ambient light. </span><span class="koboSpan" id="kobo.236.3">Finally, we multiply the computed </span><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">outColor</span></strong><span class="koboSpan" id="kobo.238.1"> by the visibility factor to create the final color, which will be darker if the fragment is in shadow and lighter if </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">it’s lit:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.240.1">
void main() {
  vec4 worldPos =
      texture(gBufferPosition, fragTexCoord);
  vec3 basecolor =
      texture(gBufferBaseColor, fragTexCoord).rgb;
  if (worldPos.x == 0.0 &amp;&amp; worldPos.y == 0.0 &amp;&amp;
      worldPos.z == 0.0 &amp;&amp; worldPos.w == 0.0) {
    outColor = vec4(basecolor, 1.0);
    return;
  }
  // compute outColor …
  vec4 shadowProjPos =
      lightData.lightVP * vec4(worldPos.xyz, 1.0f);
  float vis = PCF(shadowProjPos);
  if (vis &lt;= .001) {
    vis = .3;
  }
  outColor.xyz *= vis;
}</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.241.1">In the preceding</span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.242.1"> section, we</span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.243.1"> illustrated how to implement shadow pass, but there are limitations to this technique that will be discussed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">next section.</span></span></p>
<h2 id="_idParaDest-218"><a id="_idTextAnchor254"/><span class="koboSpan" id="kobo.245.1">There’s more …</span></h2>
<p><span class="koboSpan" id="kobo.246.1">We have demonstrated the use of the basic shadow mapping technique; however, this technique has some limitations, </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.248.1">Aliasing and pixelation</span></strong><span class="koboSpan" id="kobo.249.1">: One </span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.250.1">of the primary issues with simple shadow mapping is the problem of </span><strong class="bold"><span class="koboSpan" id="kobo.251.1">aliasing</span></strong><span class="koboSpan" id="kobo.252.1">, where shadows can appear pixelated or blocky. </span><span class="koboSpan" id="kobo.252.2">This is because the resolution of the shadow map directly affects the shadow’s quality. </span><span class="koboSpan" id="kobo.252.3">If the shadow map’s resolution is too low, the resulting shadows will be pixelated. </span><span class="koboSpan" id="kobo.252.4">While increasing the shadow map’s resolution can mitigate this, it comes at the cost of increasing memory usage and </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">computational load.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.254.1">Hard shadow edges</span></strong><span class="koboSpan" id="kobo.255.1">: Basic shadow mapping produces shadows with hard edges since it uses a binary lit or unlit test for shadow determination. </span><span class="koboSpan" id="kobo.255.2">Shadows often have soft edges due to light </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">scattering (penumbra).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.257.1">Shadow acne or self-shadowing artifacts</span></strong><span class="koboSpan" id="kobo.258.1">: This problem arises when a surface incorrectly shadows itself due to precision errors in the depth test. </span><span class="koboSpan" id="kobo.258.2">Techniques such as biasing are used to handle this issue but choosing the right bias can </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">be challenging.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.260.1">Some of these challenges can be overcome with more advanced techniques such as </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">the following:</span></span></p><ul><li><strong class="bold"><span class="koboSpan" id="kobo.262.1">Cascade shadow maps</span></strong><span class="koboSpan" id="kobo.263.1">: This technique addresses the issue of resolution by dividing the camera’s view frustum into multiple </span><strong class="bold"><span class="koboSpan" id="kobo.264.1">cascades</span></strong><span class="koboSpan" id="kobo.265.1"> or sections, each with its own shadow map. </span><span class="koboSpan" id="kobo.265.2">This allows for higher-resolution shadow maps to be used close to the camera, where detail is more important, and lower-resolution shadow maps to be used </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">farther away.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.267.1">Moment shadow maps</span></strong><span class="koboSpan" id="kobo.268.1">: This technique uses statistical moments to store more information about the depth distribution within a pixel, which can handle transparency and provide anti-aliased, soft shadows. </span><span class="koboSpan" id="kobo.268.2">Moment shadow maps require more memory and computation than basic shadow maps but can</span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.269.1"> provide </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">higher-quality shadows.</span></span></li></ul></li>
</ul>
<h2 id="_idParaDest-219"><a id="_idTextAnchor255"/><span class="koboSpan" id="kobo.271.1">See also</span></h2>
<p><span class="koboSpan" id="kobo.272.1">The following are references that discuss and provide implementation details about advanced techniques such as cascade shadow maps and </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">moment shadows:</span></span></p>
<ul>
<li><a href="https://learn.microsoft.com/en-us/windows/win32/dxtecharts/cascaded-shadow-maps"><span class="No-Break"><span class="koboSpan" id="kobo.274.1">https://learn.microsoft.com/en-us/windows/win32/dxtecharts/cascaded-shadow-maps</span></span></a></li>
<li><a href="https://momentsingraphics.de/I3D2015.html"><span class="No-Break"><span class="koboSpan" id="kobo.275.1">https://momentsingraphics.de/I3D2015.html</span></span></a></li>
</ul>
<h1 id="_idParaDest-220"><a id="_idTextAnchor256"/><span class="koboSpan" id="kobo.276.1">Implementing screen space ambient occlusion</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.277.1">Screen space ambient occlusion</span></strong><span class="koboSpan" id="kobo.278.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.279.1">SSAO</span></strong><span class="koboSpan" id="kobo.280.1">) can be used to approximate the effect of ambient occlusion</span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.281.1"> in real time. </span><span class="koboSpan" id="kobo.281.2">Ambient occlusion is a shading and rendering method used to calculate how exposed each point in a scene is to ambient lighting. </span><span class="koboSpan" id="kobo.281.3">This technique adds more realistic shadows where two surfaces or objects meet, or where an object blocks light from reaching </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">another object.</span></span></p>
<p><span class="koboSpan" id="kobo.283.1">In this recipe, you will learn how to implement SSAO to realistically estimate ambient occlusion in real time. </span><span class="koboSpan" id="kobo.283.2">You will grasp how to use this shading and rendering technique to calculate the exposure of each point in a scene to </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">ambient lighting.</span></span></p>
<h2 id="_idParaDest-221"><a id="_idTextAnchor257"/><span class="koboSpan" id="kobo.285.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.286.1">The algorithm described in this recipe calculates the difference between the depth of a pixel and its neighbors (samples) in a circular fashion. </span><span class="koboSpan" id="kobo.286.2">If a sample is closer to the camera than the central pixel, it contributes to the occlusion factor, making the </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">pixel darker.</span></span></p>
<p><span class="koboSpan" id="kobo.288.1">A depiction of the algorithm is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.289.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.290.1">.2</span></em><span class="koboSpan" id="kobo.291.1">. </span><span class="koboSpan" id="kobo.291.2">The code loops over several </span><em class="italic"><span class="koboSpan" id="kobo.292.1">rings</span></em><span class="koboSpan" id="kobo.293.1"> around a central point, the pixel being processed. </span><span class="koboSpan" id="kobo.293.2">Within each ring, it ta</span><a id="_idTextAnchor258"/><span class="koboSpan" id="kobo.294.1">kes several samples, as seen in </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">item (a).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer044">
<span class="koboSpan" id="kobo.296.1"><img alt="Figure 4.2 – SSAO sampling pattern" src="image/B18491_04_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.297.1">Figure 4.2 – SSAO sampling pattern</span></p>
<p><span class="koboSpan" id="kobo.298.1">A small amount of noise is applied to each sample’s location to avoid banding effects, as shown in item (b). </span><span class="koboSpan" id="kobo.298.2">Additionally, a weight is applied to samples on the same ring, with rings farther from </span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.299.1">the center with the smallest weights, as depicted in </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">item (c).</span></span></p>
<h2 id="_idParaDest-222"><a id="_idTextAnchor259"/><span class="koboSpan" id="kobo.301.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.302.1">The entire SSAO algorithm is implemented as a compute shader that writes its output to </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">an image:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.304.1">We start by declaring the inputs and outputs. </span><span class="koboSpan" id="kobo.304.2">The input is the depth buffer. </span><span class="koboSpan" id="kobo.304.3">The output is an image in which we’ll store the result of the algorithm. </span><span class="koboSpan" id="kobo.304.4">We also need a function to generate noise in 2D. </span><span class="koboSpan" id="kobo.304.5">A small amount of noise is applied to each sample’s location to avoid </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">banding effects:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.306.1">
#version 460
layout(local_size_x = 16, local_size_y = 16,
       local_size_z = 1) in;
layout(set = 0, binding = 0,
       rgba8) uniform image2D OutputSSAO;
layout(set = 1, binding = 0) uniform sampler2D
    gBufferDepth;
const float nearDistance = .1f;
const float farDistance = 100.0f;
vec2 generateRandomNoise(
    in vec2 coord) // generating random noise
{
  float noiseX = (fract(
      sin(dot(coord, vec2(12.9898, 78.233))) *
      43758.5453));
  float noiseY = (fract(
      sin(dot(coord,
              vec2(12.9898, 78.233) * 2.0)) *
      43758.5453));
  return vec2(noiseX, noiseY) * 0.004;
}</span></pre></li> <li><span class="koboSpan" id="kobo.307.1">We also need a </span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.308.1">function to convert the depth value from the depth buffer to a linear scale, as the values are not stored in a </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">linear fashion:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.310.1">
float calculateLinearDepth(float depth) {
  return (2.0 * nearDistance) /
         (farDistance + nearDistance -
          depth * (farDistance - nearDistance));
}</span></pre></li> <li><span class="koboSpan" id="kobo.311.1">The comparison between the depth value of the pixel being processed and the surrounding samples is done by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.312.1">compareDepths</span></strong><span class="koboSpan" id="kobo.313.1"> function, which returns the difference between </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">the samples:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.315.1">
float compareDepths(float depth1, float depth2) {
  const float aoCap = 0.5;
  const float aoMultiplier = 50.0;
  const float depthTolerance = 0.001;
  const float aoRange = 60.0;
  float depthDifference = sqrt(
      clamp(1.0 - (depth1 - depth2) /
                      (aoRange / (farDistance -
                                  nearDistance)),
            0.0, 1.0));
  float ao =
      min(aoCap, max(0.0, depth1 - depth2 -
                              depthTolerance) *
                     aoMultiplier) *
      depthDifference;
  return ao;
}</span></pre></li> <li><span class="koboSpan" id="kobo.316.1">The main part of</span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.317.1"> the algorithm starts by collecting the pixel’s position and its depth value, which is converted to a linear scale. </span><span class="koboSpan" id="kobo.317.2">It also calculates the size of the buffers and calculates </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">the noise:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.319.1">
void main() {
  if (gl_GlobalInvocationID.x &gt;=
          pushConstant.textureResolution.x ||
      gl_GlobalInvocationID.y &gt;=
          pushConstant.textureResolution.y) {
    return;
  }
  imageStore(OutputSSAO,
             ivec2(gl_GlobalInvocationID.xy),
             vec4(0));
  vec2 uv = (vec2(gl_GlobalInvocationID.xy) +
             vec2(0.5f)) /
            vec2(pushConstant.textureResolution);
  ivec2 pixelPos =
      ivec2(gl_GlobalInvocationID.xy);
  float depthBufferValue =
      texelFetch(gBufferDepth, pixelPos, 0).r;
  float depth =
      calculateLinearDepth(depthBufferValue);
  float textureWidth =
      float(pushConstant.textureResolution.x);
  float textureHeight =
      float(pushConstant.textureResolution.y);
  float aspectRatio =
      textureWidth / textureHeight;
  vec2 noise =
      generateRandomNoise(vec2(pixelPos));</span></pre></li> <li><span class="koboSpan" id="kobo.320.1">The size of the area inspected for samples is proportional to the depth value of the pixel: the farther</span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.321.1"> away the pixel is from the camera, the smaller the </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">area is:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.323.1">
  float w = (1.0 / textureWidth) /
                clamp(depth, 0.05, 1.0) +
            (noise.x * (1.0 - noise.x));
  float h = (1.0 / textureHeight) /
                clamp(depth, 0.05, 1.0) +
            (noise.y * (1.0 - noise.y));
  w *= textureWidth / 2.0;
  h *= textureHeight / 2.0;
  float sampleWidth;
  float sampleHeight;
  float ao = 0.0;
  float totalSamples = 0.0;
  float fade = 1.0;
  const int NUM_RINGS = 3;
  const int NUM_SAMPLES = 6;</span></pre></li> <li><span class="koboSpan" id="kobo.324.1">The bulk of the algorithm is where the ring radiuses and the number of samples are calculated. </span><span class="koboSpan" id="kobo.324.2">The number of samples is proportional to the ring’s diameter. </span><span class="koboSpan" id="kobo.324.3">For each sample, we compare their depths, apply the ring weight, and accumulate the </span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.325.1">output, which is averaged out at the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">the function:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.327.1">
  for (int i = 0; i &lt; NUM_RINGS; i++) {
    fade *= 0.5;
    for (int j = 0; j &lt; NUM_SAMPLES * i; j++) {
      float step = 3.14159265 * 2.0 /
                   float(NUM_SAMPLES * i);
      sampleWidth =
          (cos(float(j) * step) * float(i));
      sampleHeight =
          (sin(float(j) * step) * float(i));
      float newDepthValue =
          texelFetch(
              gBufferDepth,
              pixelPos +
                  ivec2(int(sampleWidth * w),
                        int(sampleHeight * h)),
              0)
              .r;
      ao += compareDepths(depth,
                          calculateLinearDepth(
                              newDepthValue)) *
            fade;
      totalSamples += 1.0 * fade;
    }
  }
  ao /= totalSamples;
  ao = 1.0 - ao;
  imageStore(OutputSSAO, ivec2(gl_GlobalInvocationID.xy), vec4(ao,ao,ao, 1.0));</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.328.1">This concludes our recipe for SSAO. </span><span class="koboSpan" id="kobo.328.2">For a deeper understanding and further exploration, we highly </span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.329.1">recommend visiting the various resources provided in the </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">following section.</span></span></p>
<h2 id="_idParaDest-223"><a id="_idTextAnchor260"/><span class="koboSpan" id="kobo.331.1">See also</span></h2>
<p><span class="koboSpan" id="kobo.332.1">For further understanding and an exploration of the topic of SSAO, you may find the following </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">resources </span></span><span class="No-Break"><a id="_idIndexMarker388"/></span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">helpful:</span></span></p>
<ul>
<li><a href="https://github.com/NVIDIAGameWorks/HBAOPlus"><span class="No-Break"><span class="koboSpan" id="kobo.335.1">https://github.com/NVIDIAGameWorks/HBAOPlus</span></span></a></li>
<li><a href="https://www.gamedevs.org/uploads/comparative-study-of-ssao-methods.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.336.1">https://www.gamedevs.org/uploads/comparative-study-of-ssao-methods.pdf</span></span></a></li>
<li><a href="https://research.nvidia.com/sites/default/files/pubs/2012-06_Scalable-Ambient-Obscurance/McGuire12SAO.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.337.1">https://research.nvidia.com/sites/default/files/pubs/2012-06_Scalable-Ambient-Obscurance/McGuire12SAO.pdf</span></span></a></li>
<li><a href="https://www.ppsloan.org/publications/vo.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.338.1">https://www.ppsloan.org/publications/vo.pdf</span></span></a></li>
<li><a href="https://github.com/GameTechDev/XeGTAO"><span class="No-Break"><span class="koboSpan" id="kobo.339.1">https://github.com/GameTechDev/XeGTAO</span></span></a></li>
</ul>
<h1 id="_idParaDest-224"><a id="_idTextAnchor261"/><span class="koboSpan" id="kobo.340.1">Implementing a lighting pass for illuminating the scene</span></h1>
<p><span class="koboSpan" id="kobo.341.1">The last recipe in </span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.342.1">the book you how to</span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.343.1"> implement a lighting pass; this is where we calculate the lighting for the scene. </span><span class="koboSpan" id="kobo.343.2">For each light in the scene, we draw a volume (for point lights, this would be a sphere; for directional lights, a full-screen quad; for spotlights, we would draw a cone) and for each pixel in that volume, we fetch the data from the G-buffer and calculate the lighting contribution of that light to the pixel. </span><span class="koboSpan" id="kobo.343.3">The results are then usually added together (blended) to a final render target to get the final image. </span><span class="koboSpan" id="kobo.343.4">In the demo, we only have one spotlight that is used as a demonstration, but we can easily add multiple lights. </span><span class="koboSpan" id="kobo.343.5">For each light in the scene, we will need to consider the area affected by the light (i.e., we use a shader that fetches the relevant data for each pixel from the G-buffer, which then uses this data to calculate how much this light source contributes to the final color of each pixel). </span><span class="koboSpan" id="kobo.343.6">For example, if we’re dealing with a spotlight, this volume is a cone centered at the light’s position, oriented in the light’s direction, and with an angle that matches the spread of the spotlight. </span><span class="koboSpan" id="kobo.343.7">The length or height of the cone should be equal to the range of the spotlight. </span><span class="koboSpan" id="kobo.343.8">Lastly, we use a physically based lighting </span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.344.1">model (the </span><strong class="bold"><span class="koboSpan" id="kobo.345.1">Cook-Torrance lighting model</span></strong><span class="koboSpan" id="kobo.346.1">), which is applied in the fragment shader. </span><span class="koboSpan" id="kobo.346.2">The inputs to the lighting model include the light’s properties (color, intensity, position, etc.) and the surface properties (material color, shininess, normal, etc.), which are fetched from </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">the G-buffer.</span></span></p>
<h2 id="_idParaDest-225"><a id="_idTextAnchor262"/><span class="koboSpan" id="kobo.348.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.349.1">The recipe is implemented in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.350.1">LightingPass</span></strong><span class="koboSpan" id="kobo.351.1"> class and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.352.1">Lighting.frag</span></strong><span class="koboSpan" id="kobo.353.1"> shader. </span><span class="koboSpan" id="kobo.353.2">It simply uses a full-screen vertex shader to draw a </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">full-screen quad.</span></span></p>
<p><span class="koboSpan" id="kobo.355.1">As mentioned in the introduction of this recipe, we use the Cook-Torrance lighting model, which is a physically based rendering model that simulates how light interacts with a surface. </span><span class="koboSpan" id="kobo.355.2">It considers various factors such as the angle of incidence, surface roughness, and microfacet distribution to render realistic lighting effects. </span><span class="koboSpan" id="kobo.355.3">The algorithm uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.356.1">Fresnel-Schlick</span></strong><span class="koboSpan" id="kobo.357.1"> function, which is used to determine the proportion of light reflected versus refracted depending on the view angle. </span><span class="koboSpan" id="kobo.357.2">The GGX distribution function calculates the distribution of microfacets on the surface, which influences how rough or smooth a </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">surface appears.</span></span></p>
<h2 id="_idParaDest-226"><a id="_idTextAnchor263"/><span class="koboSpan" id="kobo.359.1">How to do it…</span></h2>
<p><span class="koboSpan" id="kobo.360.1">The entire algorithm is implemented as a full-screen space quad shader that writes its output to a final </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">color texture:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.362.1">We start by declaring the inputs and outputs. </span><span class="koboSpan" id="kobo.362.2">The inputs include the G-buffer data (normal, specular, base color, depth, and position), ambient occlusion map, shadow map, camera, and light data. </span><span class="koboSpan" id="kobo.362.3">The output is simply </span><strong class="source-inline"><span class="koboSpan" id="kobo.363.1">fragColor</span></strong><span class="koboSpan" id="kobo.364.1">, which is written to the </span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.365.1">color </span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.366.1">attachment as specified by the </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">render pass:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.368.1">
#version 460
#extension GL_EXT_nonuniform_qualifier : require
#extension GL_GOOGLE_include_directive : require
layout(set = 0, binding = 0)uniform sampler2D gBufferWorldNormal;
layout(set = 0, binding = 1)uniform sampler2D gBufferSpecular;
layout(set = 0, binding = 2)uniform sampler2D gBufferBaseColor;
layout(set = 0, binding = 3)uniform sampler2D gBufferDepth;
layout(set = 0, binding = 4)uniform sampler2D gBufferPosition;
layout(set = 0, binding = 5)uniform sampler2D ambientOcclusion;
layout(set = 0, binding = 6)uniform sampler2DShadow shadowMap;
layout(set = 1, binding = 0)uniform Transforms
{
    mat4 viewProj;
    mat4 viewProjInv;
    mat4 viewInv;
} cameraData;
layout(set = 1, binding = 1)uniform Lights
{
    vec4 lightPos;
    vec4 lightDir;
    vec4 lightColor;
    vec4 ambientColor; // environment light color
    mat4 lightVP;
    float innerConeAngle;
    float outerConeAngle;
} lightData;
layout(location=0) in vec2 fragTexCoord;
layout(location = 0) out vec4 outColor;</span></pre></li> <li><span class="koboSpan" id="kobo.369.1">Afterward, we</span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.370.1"> define</span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.371.1"> a few auxiliary functions; these will be used in the main function. </span><span class="koboSpan" id="kobo.371.2">These are defined in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">brdf.glsl</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.373.1"> file.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.374.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.375.1">fresnelSchlick</span></strong><span class="koboSpan" id="kobo.376.1"> function calculates the </span><strong class="source-inline"><span class="koboSpan" id="kobo.377.1">Fresnel-Schlick</span></strong><span class="koboSpan" id="kobo.378.1"> approximation, which models the amount of reflection and refraction based on the angle at which light hits the surface. </span><span class="koboSpan" id="kobo.378.2">The result is used to determine the </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">specular color.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.380.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.381.1">distributionGGX</span></strong><span class="koboSpan" id="kobo.382.1"> function calculates the distribution of microfacets on a surface. </span><span class="koboSpan" id="kobo.382.2">The result models how rough or smooth a surface appears, influencing the spread of the </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">specular highlight.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.384.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.385.1">geometrySchlickGGX</span></strong><span class="koboSpan" id="kobo.386.1"> function calculates the geometric attenuation term using </span><strong class="bold"><span class="koboSpan" id="kobo.387.1">Schlick’s approximation</span></strong><span class="koboSpan" id="kobo.388.1">. </span><span class="koboSpan" id="kobo.388.2">This term represents the probability that light isn’t </span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.389.1">blocked by microfacets. </span><span class="koboSpan" id="kobo.389.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.390.1">geometrySmith</span></strong><span class="koboSpan" id="kobo.391.1"> function calculates the total geometric attenuation considering both the view direction and the light direction. </span><span class="koboSpan" id="kobo.391.2">It calculates geometric attenuation for both view and light direction and multiplies both to get the final geometric attenuation, this function assumes that microfacet distribution is the same in all directions. </span><span class="koboSpan" id="kobo.391.3">These functions are </span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.392.1">combined </span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.393.1">in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.394.1">Cook-Torrance BRDF</span></strong><span class="koboSpan" id="kobo.395.1"> model to account for microfacet occlusion and </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">shadowing effects:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.397.1">
vec3 fresnelSchlick(float cosTheta, vec3 F0) {
  return F0 + (1.0 - F0) * pow(1.0 - cosTheta, 5.0);
}
float distributionGGX(vec3 N, vec3 H,
                      float roughness) {
  float a = roughness * roughness;
  float a2 = a * a;
  float NdotH = max(dot(N, H), 0.0);
  float NdotH2 = NdotH * NdotH;
  float nom = a2;
  float denom = (NdotH2 * (a2 - 1.0) + 1.0);
  denom = 3.14159265359 * denom * denom;
  return nom / denom;
}
float geometrySchlickGGX(float NdotV,
                         float roughness) {
  float r = (roughness + 1.0) * 0.5;
  float r2 = r * r;
  float nom = NdotV;
  float denom = NdotV * (1.0 - r2) + r2;
  return nom / denom;
}
float geometrySmith(vec3 N, vec3 V, vec3 L,
                    float roughness) {
  float NdotV = max(dot(N, V), 0.0);
  float NdotL = max(dot(N, L), 0.0);
  float ggx2 = geometrySchlickGGX(NdotV, roughness);
  float ggx1 = geometrySchlickGGX(NdotL, roughness);
  return ggx1 * ggx2;
}</span></pre></li> <li><span class="koboSpan" id="kobo.398.1">The shader first retrieves the base color, world position, camera position, specular data, and normal data from </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">the G-buffer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.400.1">
void main() {
  vec4 worldPos =
      texture(gBufferPosition, fragTexCoord);
  vec3 basecolor =
      texture(gBufferBaseColor, fragTexCoord).rgb;
  if (worldPos.x == 0.0 &amp;&amp; worldPos.y == 0.0 &amp;&amp;
      worldPos.z == 0.0 &amp;&amp; worldPos.w == 0.0) {
    outColor = vec4(basecolor, 1.0);
    return;
  }
  vec2 gbufferSpecularData =
      texture(gBufferSpecular, fragTexCoord).rg;
  float metallic = gbufferSpecularData.r;
  float roughness = gbufferSpecularData.g;
  vec4 gbufferNormalData =
      texture(gBufferWorldNormal, fragTexCoord);
  vec3 N = normalize(gbufferNormalData.xyz);</span></pre></li> <li><span class="koboSpan" id="kobo.401.1">In the next steps, it </span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.402.1">calculates </span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.403.1">the view vector (</span><strong class="source-inline"><span class="koboSpan" id="kobo.404.1">V</span></strong><span class="koboSpan" id="kobo.405.1">), light vector (</span><strong class="source-inline"><span class="koboSpan" id="kobo.406.1">L</span></strong><span class="koboSpan" id="kobo.407.1">), and half vector (</span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">H</span></strong><span class="koboSpan" id="kobo.409.1">). </span><span class="koboSpan" id="kobo.409.2">These vectors are used in </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">lighting calculations:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.411.1">
vec3 camPos = cameraData.viewInv[3].xyz;
vec3 V = normalize(camPos - worldPos.xyz);
vec3 F0 = vec3(0.04);
F0 = mix(F0, basecolor, metallic);
vec3 L = normalize(
    lightData.lightDir.xyz -
    worldPos.xyz); // Using spotlight direction
vec3 H = normalize(V + L);</span></pre></li> <li><span class="koboSpan" id="kobo.412.1">During this part, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.413.1">Fresnel-Schlick</span></strong><span class="koboSpan" id="kobo.414.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.415.1">distributionGGX</span></strong><span class="koboSpan" id="kobo.416.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">geometrySmith</span></strong><span class="koboSpan" id="kobo.418.1"> functions </span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.419.1">are called to </span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.420.1">calculate the </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">specular reflection:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.422.1">
vec3 F = fresnelSchlick(max(dot(H, V), 0.0), F0);
float D = distributionGGX(N, H, roughness);
float G = geometrySmith(N, V, L, roughness);
vec3 nominator = D * G * F;
float denominator = 4.0 * max(dot(N, V), 0.0) *
                        max(dot(N, L), 0.0) +
                    0.001;
vec3 specular = nominator / denominator;</span></pre></li> <li><span class="koboSpan" id="kobo.423.1">In this step, the shader calculates the diffuse reflection. </span><span class="koboSpan" id="kobo.423.2">It’s a simple model based on Lambert’s cosine law, but it’s modified by applying an energy </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">conservation principle:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.425.1">
vec3 kS = F;
vec3 kD = vec3(1.0) - kS;
kD *= 1.0 - metallic;
float NdotL = max(dot(N, L), 0.0);
vec3 diffuse = kD * basecolor / 3.14159265359;</span></pre></li> <li><span class="koboSpan" id="kobo.426.1">In these last few steps, ambient light is calculated by simply multiplying the ambient light color by the base color. </span><span class="koboSpan" id="kobo.426.2">Here, we also calculate the attenuation based on the distance to the light, and the spot attenuation based on the angle between the light direction and the direction to the fragment. </span><span class="koboSpan" id="kobo.426.3">These are used to calculate the light intensity. </span><span class="koboSpan" id="kobo.426.4">The shader calculates the final color by adding the ambient, diffuse, and </span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.427.1">specular</span><a id="_idIndexMarker404"/> <span class="No-Break"><span class="koboSpan" id="kobo.428.1">components together:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.429.1">
vec3 ambient =
    lightData.ambientColor.rgb * basecolor;
// Spotlight calculations
vec3 lightToFragment =
    lightData.lightPos.xyz - worldPos.xyz;
vec3 lightDirection =
    normalize(-lightData.lightDir.xyz);
float distanceToLight = length(lightToFragment);
float attenuation =
    1.0 /
    (1.0 + 0.1 * distanceToLight +
     0.01 * distanceToLight * distanceToLight);
vec3 lightDir = normalize(lightToFragment);
float cosTheta = dot(-lightDir, lightDirection);
float spotAttenuation =
    smoothstep(lightData.outerConeAngle,
               lightData.innerConeAngle, cosTheta);
vec3 lightIntensity = spotAttenuation *
                      attenuation *
                      lightData.lightColor.rgb;
// Final light contribution
vec3 finalColor = (NdotL * (lightIntensity) *
                   (diffuse + specular)) +
                  ambient;</span></pre></li> <li><span class="koboSpan" id="kobo.430.1">In the last step, the final color is then multiplied by the ambient occlusion factor (sampled from ambient occlusion texture calculated in the </span><em class="italic"><span class="koboSpan" id="kobo.431.1">Implementing screen space ambient occlusion</span></em><span class="koboSpan" id="kobo.432.1"> recipe) and the shadow visibility factor to account for</span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.433.1"> shadows </span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.434.1">and </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">ambient occlusion:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.436.1">
float ao =
    texture(ambientOcclusion, fragTexCoord).r;
finalColor *= ao;
outColor = vec4(finalColor, 1.0);
vec4 shadowProjPos =
    lightData.lightVP * vec4(worldPos.xyz, 1.0f);
float vis = PCF(shadowProjPos);
if (vis &lt;= .001) {
  vis = .3;
}
outColor.xyz *= vis;</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.437.1">In the following screenshot, we present an image that shows a shadow created using the </span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.438.1">shadow </span><a id="_idIndexMarker408"/><span class="No-Break"><span class="koboSpan" id="kobo.439.1">map technique:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer045">
<span class="koboSpan" id="kobo.440.1"><img alt="Figure 4.3 – A shadow using a shadow map" src="image/B18491_04_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.441.1">Figure 4.3 – A shadow using a shadow map</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.442.1">The following screenshot demonstrates the result of the </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">SSR technique:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer046">
<span class="koboSpan" id="kobo.444.1"><img alt="Figure 4.4 – SSR" src="image/B18491_04_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.445.1">Figure 4.4 – SSR</span></p>
<p><span class="koboSpan" id="kobo.446.1">In this chapter, we embarked on a journey to comprehend and implement some of the most influential techniques in 3D graphics for achieving real-time physically based effects using the Vulkan API. </span><span class="koboSpan" id="kobo.446.2">We began our exploration with the principles of G-buffer generation, a foundational concept</span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.447.1"> of </span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.448.1">deferred rendering. </span><span class="koboSpan" id="kobo.448.2">This technique allows us to manage the complexity of modern lighting and shading, paving the way for the implementation of more advanced rendering effects. </span><span class="koboSpan" id="kobo.448.3">We then described techniques such as SSR and shadows, which are needed for simulating realism in rendered scenes.  </span><span class="koboSpan" id="kobo.448.4">We also explored the complexities of lighting and shading with a deep dive into SSAO. </span><span class="koboSpan" id="kobo.448.5">This technique provided us with the tools to simulate the intricate ways light radiates in real life, adding depth and detail to corners in our 3D world. </span><span class="koboSpan" id="kobo.448.6">Finally, our exploration ended with the implementation of a lighting pass. </span><span class="koboSpan" id="kobo.448.7">By calculating the contributions of various light sources on each object in our scene, we successfully illuminated our 3D environment. </span><span class="koboSpan" id="kobo.448.8">We hope that you have gained a comprehensive understanding of several core techniques in modern lighting, shading, and shadows, which will empower you</span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.449.1"> to</span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.450.1"> create stunning and realistic 3D graphics </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">with Vulkan.</span></span></p>
</div>
</body></html>