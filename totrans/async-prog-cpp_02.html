<html><head></head><body>
  <div><h1 class="chapter-number" id="_idParaDest-36">
    <a id="_idTextAnchor035">
    </a>
    
     2
    
   </h1>
   <h1 id="_idParaDest-37">
    <a id="_idTextAnchor036">
    </a>
    
     Processes, Threads, and Services
    
   </h1>
   <p>
    
     Asynchronous programming involves initiating operations without waiting for them to complete before moving on to the next task.
    
    
     This non-blocking behavior allows for developing highly responsive and efficient applications, capable of handling numerous operations simultaneously without unnecessary delays or wasting computational resources waiting for tasks to
    
    
     
      be finished.
     
    
   </p>
   <p>
    
     Asynchronous programming is very important, especially in the development of networked applications, user interfaces, and systems programming.
    
    
     It enables developers to
    
    <a id="_idIndexMarker087">
    </a>
    
     create applications that can manage high volumes of requests, perform
    
    <strong class="bold">
     
      Input/Output
     
    </strong>
    
     (
    
    <strong class="bold">
     
      I/O
     
    </strong>
    
     ) operations, or execute concurrent tasks efficiently, thereby significantly enhancing user experience and
    
    
     
      application performance.
     
    
   </p>
   <p>
    
     The Linux operating system (in this book, we will focus on development on the Linux operating system when the code cannot be platform-independent), with its robust process management, native support for threading, and advanced I/O capabilities, is an ideal environment for developing high-performance asynchronous applications.
    
    
     These systems offer
    
    <a id="_idIndexMarker088">
    </a>
    
     a rich set of features such as powerful APIs for process and thread management, non-blocking I/O, and
    
    <strong class="bold">
     
      Inter-Process Communication
     
    </strong>
    
     (
    
    
     <strong class="bold">
      
       IPC
      
     </strong>
    
    
     
      ) mechanisms.
     
    
   </p>
   <p>
    
     This chapter is an introduction to the fundamental concepts and components essential for asynchronous programming within the
    
    
     
      Linux environment.
     
    
   </p>
   <p>
    
     We will explore the
    
    
     
      following topics:
     
    
   </p>
   <ul>
    <li>
     
      Processes
     
     
      
       in Linux
      
     
    </li>
    <li>
     
      Services
     
     
      
       and daemons
      
     
    </li>
    <li>
     
      Threads
     
     
      
       and concurrency
      
     
    </li>
   </ul>
   <p>
    
     By the end of this chapter, you will possess a foundational understanding of the asynchronous programming landscape in Linux, setting the stage for deeper exploration and practical application in
    
    
     
      subsequent chapters.
     
    
   </p>
   <h1 id="_idParaDest-38">
    <a id="_idTextAnchor037">
    </a>
    
     Processes in Linux
    
   </h1>
   <p>
    
     A process can be defined as an instance of a running program.
    
    
     It includes the program’s code, all the
    
    <a id="_idIndexMarker089">
    </a>
    
     threads belonging to this process (which are represented by the program counter), the stack (which is an area of memory containing temporary data such as function parameters, return addresses, and local variables), the heap, for memory allocated dynamically, and its data section containing global variables and initialized variables.
    
    
     Each process operates within its own virtual address space and is isolated from other processes, ensuring that its operations do not interfere directly with those
    
    
     
      of others.
     
    
   </p>
   <h2 id="_idParaDest-39">
    <a id="_idTextAnchor038">
    </a>
    
     Process life cycle – creation, execution, and termination
    
   </h2>
   <p>
    
     The life cycle
    
    <a id="_idIndexMarker090">
    </a>
    
     of a process can be broken down into three primary stages: creation, execution,
    
    
     
      and termination:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Creation
      
     </strong>
     
      : A new
     
     <a id="_idIndexMarker091">
     </a>
     
      process is created using the
     
     <strong class="source-inline">
      
       fork()
      
     </strong>
     
      system call, which creates a new process by duplicating an existing one.
     
     
      The parent process is the one that calls
     
     <strong class="source-inline">
      
       fork()
      
     </strong>
     
      , and the newly created process is the child.
     
     
      This mechanism is essential for the execution of new programs within the system and is a precursor to executing different
     
     
      
       tasks concurrently.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Execution
      
     </strong>
     
      : After
     
     <a id="_idIndexMarker092">
     </a>
     
      creation, the child process may execute the same code as the parent or use the
     
     <strong class="source-inline">
      
       exec()
      
     </strong>
     
      family of system calls to load and run a
     
     
      
       different program.
      
     
     <p class="list-inset">
      
       If the parent process has more than one thread of execution, only the thread calling
      
      <strong class="source-inline">
       
        fork()
       
      </strong>
      
       is duplicated in the child process.
      
      
       Consequently, the child process contains a single thread: the one that executed the
      
      <strong class="source-inline">
       
        fork()
       
      </strong>
      
       
        system call.
       
      
     </p>
     <p class="list-inset">
      
       Since only the thread that called
      
      <strong class="source-inline">
       
        fork()
       
      </strong>
      
       is copied to the child, any
      
      <strong class="bold">
       
        Mutual Exclusions
       
      </strong>
      
       (
      
      <strong class="bold">
       
        mutexes
       
      </strong>
      
       ), condition variables, or other synchronization primitives
      
      <a id="_idIndexMarker093">
      </a>
      
       that were held by other threads at the time of the fork remain in their then-current state in the parent but do not carry over to the child.
      
      
       This can lead to complex synchronization issues, as mutexes
      
      <a id="_idIndexMarker094">
      </a>
      
       that were locked by other threads (which do not exist in the child) might remain in a locked state, potentially causing deadlocks if the child tries to unlock or wait on
      
      
       
        these primitives.
       
      
     </p>
     <p class="list-inset">
      
       At this stage, the process performs its designated operations such as reading from or writing to files and communicating with
      
      
       
        other processes.
       
      
     </p>
    </li>
    <li>
     <strong class="bold">
      
       Termination
      
     </strong>
     
      : A
     
     <a id="_idIndexMarker095">
     </a>
     
      process terminates either voluntarily, by calling the
     
     <strong class="source-inline">
      
       exit()
      
     </strong>
     
      system call, or involuntarily, due to receiving a signal from another process that causes it to stop.
     
     
      Upon termination, the process returns an exit status to its parent process and releases its resources back to
     
     
      
       the system.
      
     
    </li>
   </ul>
   <p>
    
     The process life cycle is integral to asynchronous operations as it enables the concurrent execution of
    
    
     
      multiple tasks.
     
    
   </p>
   <p>
    
     Each process is
    
    <a id="_idIndexMarker096">
    </a>
    
     uniquely identified by a
    
    <strong class="bold">
     
      Process ID
     
    </strong>
    
     (
    
    <strong class="bold">
     
      PID
     
    </strong>
    
     ), an integer that the kernel uses to manage processes.
    
    
     PIDs are used to control and monitor processes.
    
    
     Parent processes also use PIDs to communicate with or control the execution of child processes, such as waiting for them to terminate or
    
    
     
      sending signals.
     
    
   </p>
   <p>
    
     Linux provides mechanisms for process control and signaling, allowing processes to be managed and communicated with asynchronously.
    
    
     Signals are one of the primary means of IPC, enabling processes to interrupt or to be notified of events.
    
    
     For example, the
    
    <strong class="source-inline">
     
      kill
     
    </strong>
    
     command can send signals to stop a process or to prompt it to reload its
    
    
     
      configuration files.
     
    
   </p>
   <p>
    
     Process scheduling is how the Linux kernel allocates CPU time to processes.
    
    
     The scheduler determines which process runs at any given time, based on scheduling algorithms and policies that aim to optimize for factors such as responsiveness and efficiency.
    
    
     Processes can be in various states, such as running, waiting, or stopped, and the scheduler transitions them between these states to manage
    
    
     
      execution efficiently.
     
    
   </p>
   <h2 id="_idParaDest-40">
    <a id="_idTextAnchor039">
    </a>
    
     Exploring IPC
    
   </h2>
   <p>
    
     In the Linux operating system, processes operate in isolation, meaning that they cannot directly
    
    <a id="_idIndexMarker097">
    </a>
    
     access the memory space of other processes.
    
    
     This isolated nature of processes presents challenges when multiple processes
    
    <a id="_idIndexMarker098">
    </a>
    
     need to communicate and synchronize their actions.
    
    
     To address these challenges, the Linux kernel provides a versatile set of IPC mechanisms.
    
    
     Each IPC mechanism is tailored to suit different scenarios and requirements, enabling developers to build complex, high-performance applications that leverage asynchronous
    
    
     
      processing effectively.
     
    
   </p>
   <p>
    
     Understanding these IPC techniques is crucial for developers aiming to create scalable and efficient applications.
    
    
     IPC allows processes to exchange data, share resources, and coordinate their activities, facilitating smooth and reliable communication between different components of a software system.
    
    
     By utilizing the appropriate IPC mechanism, developers can achieve improved throughput, reduced latency, and enhanced concurrency in their applications, leading to better performance and
    
    
     
      user experiences.
     
    
   </p>
   <p>
    
     In a multitasking environment, where multiple processes run concurrently, IPC plays a vital role in enabling the efficient and coordinated execution of tasks.
    
    
     For example, consider a web server application that handles multiple concurrent requests from clients.
    
    
     The web server process might use IPC to communicate with the child processes responsible for processing each request.
    
    
     This approach allows the web server to handle multiple requests simultaneously, improving the overall performance and scalability of
    
    
     
      the application.
     
    
   </p>
   <p>
    
     Another common scenario where IPC is essential is in distributed systems or microservice architectures.
    
    
     In such environments, multiple independent processes or services need to communicate and collaborate to achieve a common goal.
    
    
     IPC mechanisms
    
    <a id="_idIndexMarker099">
    </a>
    
     such as message queues and sockets or
    
    <strong class="bold">
     
      Remote Procedure Calls
     
    </strong>
    
     (
    
    <strong class="bold">
     
      RPCs
     
    </strong>
    
     ) enable these processes to exchange messages, invoke methods on remote objects, and synchronize their actions, ensuring seamless and
    
    
     
      reliable IPC.
     
    
   </p>
   <p>
    
     By leveraging the IPC mechanisms provided by the Linux kernel, developers can design systems where multiple processes can work together harmoniously.
    
    
     This enables the creation of complex, high-performance applications that utilize system resources efficiently, handle concurrent tasks effectively, and scale to meet increasing
    
    
     
      demands effortlessly.
     
    
   </p>
   <h3>
    
     IPC mechanisms in Linux
    
   </h3>
   <p>
    
     Linux supports several IPC mechanisms, each with its unique characteristics and
    
    
     
      use cases.
     
    
   </p>
   <p>
    
     The
    
    <a id="_idIndexMarker100">
    </a>
    
     fundamental IPC mechanisms supported by the Linux operating system include shared memory, which is commonly employed for process communication on a single server, and sockets, which facilitate inter-server communication.
    
    
     There are other mechanisms (which are briefly described here), but shared memory and sockets are the most
    
    
     
      commonly used:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Pipes and named pipes
      
     </strong>
     
      : Pipes
     
     <a id="_idIndexMarker101">
     </a>
     
      are one of the simplest forms of IPC, allowing for
     
     <a id="_idIndexMarker102">
     </a>
     
      unidirectional communication between
     
     <a id="_idIndexMarker103">
     </a>
     
      processes.
     
     
      A named pipe, or
     
     <strong class="bold">
      
       First-in-First-out
      
     </strong>
     
      (
     
     <strong class="bold">
      
       FIFO
      
     </strong>
     
      ), extends this concept by providing a pipe that is accessible via a name in the filesystem, allowing unrelated processes
     
     
      
       to communicate.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Signals
      
     </strong>
     
      : Signals
     
     <a id="_idIndexMarker104">
     </a>
     
      are a form of software interrupt that can be sent to a process to notify it of events.
     
     
      While they are not a method for transferring data, signals are useful for controlling process behavior and triggering actions
     
     
      
       within processes.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Message queues
      
     </strong>
     
      : Message
     
     <a id="_idIndexMarker105">
     </a>
     
      queues allow processes to exchange messages in a FIFO manner.
     
     
      Unlike pipes, message queues support asynchronous communication, whereby messages are stored in a queue and can be retrieved by the receiving process at
     
     
      
       its convenience.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Semaphores
      
     </strong>
     
      : Semaphores are used for synchronization, helping processes manage
     
     <a id="_idIndexMarker106">
     </a>
     
      access to shared resources.
     
     
      They prevent race conditions by ensuring that only a specified number of processes can access a resource at any
     
     
      
       given time.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Shared memory
      
     </strong>
     
      : Shared
     
     <a id="_idIndexMarker107">
     </a>
     
      memory is a fundamental concept in IPC that enables multiple processes to access and manipulate the same segment of physical memory.
     
     
      It offers a blazing-fast method for exchanging data between different processes, reducing the need for time-consuming data copying operations.
     
     
      This technique is particularly advantageous when dealing with large datasets or requiring high-speed communication.
     
     
      The mechanism of shared memory involves creating a shared memory segment, which is a dedicated portion of physical memory accessible by multiple processes.
     
     
      This shared memory segment is treated as a common workspace, allowing processes to read, write, and collaboratively modify data.
     
     
      To ensure data integrity and prevent conflicts, shared memory requires synchronization mechanisms such as semaphores or mutexes.
     
     
      These mechanisms regulate access to the shared memory segment, preventing multiple processes from simultaneously modifying the same data.
     
     
      This coordination is crucial to maintain data consistency and avoid overwriting
     
     
      
       or corruption.
      
     
     <p class="list-inset">
      
       Shared
      
      <a id="_idIndexMarker108">
      </a>
      
       memory is often the preferred IPC mechanism in single-server environments where performance is paramount.
      
      
       Its primary advantage lies in its speed.
      
      
       Since data is directly shared in physical memory without the need for intermediate copying or context switching, it significantly reduces communication overhead and
      
      
       
        minimizes latency.
       
      
     </p>
     <p class="list-inset">
      
       However, shared memory also comes with certain considerations.
      
      
       It requires careful management to prevent race conditions and memory leaks.
      
      
       Processes accessing shared memory must adhere to well-defined protocols to ensure data integrity and avoid deadlocks.
      
      
       Additionally, shared memory is typically implemented as a system-level feature, requiring specific operating system support and potentially introducing
      
      
       
        platform-specific dependencies.
       
      
     </p>
     <p class="list-inset">
      
       Despite these considerations, shared memory remains a powerful and widely used IPC technique, particularly in applications where speed and performance are
      
      
       
        critical factors.
       
      
     </p>
    </li>
    <li>
     <strong class="bold">
      
       Sockets
      
     </strong>
     
      : Sockets are
     
     <a id="_idIndexMarker109">
     </a>
     
      a fundamental mechanism for IPC in operating systems.
     
     
      They provide a way for processes to communicate with each other, either within the same machine or across networks.
     
     
      Sockets are used to establish and maintain connections between processes, and they support both
     
     <strong class="bold">
      
       connection-oriented
      
     </strong>
     
      and
     
     
      <strong class="bold">
       
        connectionless communication
       
      </strong>
     
     
      
       .
      
     
     <p class="list-inset">
      
       Connection-oriented communication is a type of communication in which a reliable
      
      <a id="_idIndexMarker110">
      </a>
      
       connection is established between two processes before any data is transferred.
      
      
       This type of communication
      
      <a id="_idIndexMarker111">
      </a>
      
       is often used for applications such as file transfer and remote login, where it is important to ensure that all data is delivered reliably and in the correct order.
      
      
       Connectionless communication is a type of communication in which no reliable connection is established between two processes before data is transferred.
      
      
       This type of communication is often used for applications such as streaming media and real-time gaming, where it is more important to have low latency than to guarantee reliable delivery of
      
      
       
        all data.
       
      
     </p>
     <p class="list-inset">
      
       Sockets are the backbone of networked applications.
      
      
       They are used by a wide variety
      
      <a id="_idIndexMarker112">
      </a>
      
       of applications, including web browsers, email clients, and file-sharing applications.
      
      
       Sockets are also used by many
      
      <a id="_idIndexMarker113">
      </a>
      
       operating system services, such as the
      
      <strong class="bold">
       
        Network File System
       
      </strong>
      
       (
      
      <strong class="bold">
       
        NFS
       
      </strong>
      
       ) and the
      
      <strong class="bold">
       
        Domain Name
       
      </strong>
      
       <strong class="bold">
        
         System
        
       </strong>
      
      
       
        (
       
      
      
       <strong class="bold">
        
         DNS
        
       </strong>
      
      
       
        ).
       
      
     </p>
     <p class="list-inset">
      
       Here are some of the key benefits of
      
      
       
        using sockets:
       
      
     </p>
     <ul>
      <li>
       <strong class="bold">
        
         Reliability
        
       </strong>
       
        : Sockets
       
       <a id="_idIndexMarker114">
       </a>
       
        provide a reliable way to communicate between processes, even when those processes are located on
       
       
        
         different machines.
        
       
      </li>
      <li>
       <strong class="bold">
        
         Scalability
        
       </strong>
       
        : Sockets
       
       <a id="_idIndexMarker115">
       </a>
       
        can be used to support a large number of concurrent connections, making them ideal for applications that need to handle a lot
       
       
        
         of traffic.
        
       
      </li>
      <li>
       <strong class="bold">
        
         Flexibility
        
       </strong>
       
        : Sockets
       
       <a id="_idIndexMarker116">
       </a>
       
        can be used to implement a wide variety of communication protocols, making them suitable for a wide range
       
       
        
         of applications.
        
       
      </li>
      <li>
       <strong class="bold">
        
         Use in IPC
        
       </strong>
       
        : Sockets
       
       <a id="_idIndexMarker117">
       </a>
       
        are a powerful tool for IPC.
       
       
        They are used by a wide variety of applications and are essential for building scalable, reliable, and flexible
       
       
        
         networked applications.
        
       
      </li>
     </ul>
    </li>
   </ul>
   <p>
    
     Microservices-based applications are an example of asynchronous programming using different processes communicating between them in an asynchronous way.
    
    
     A simple example would be a log processor.
    
    
     Different processes generate log entries and send them to
    
    <a id="_idIndexMarker118">
    </a>
    
     another process for further processing such as special formatting, deduplication, and statistics.
    
    
     The producers just send the lines of the log without waiting for any reply from the process they are sending to
    
    
     
      the log.
     
    
   </p>
   <p>
    
     In this section, we saw processes in Linux, their life cycles, and how IPC is implemented by the operating system.
    
    
     In the next section, we will introduce a special kind of Linux process
    
    
     
      called
     
    
    
     <strong class="bold">
      
       daemons
      
     </strong>
    
    
     
      .
     
    
   </p>
   <h1 id="_idParaDest-41">
    <a id="_idTextAnchor040">
    </a>
    
     Services and daemons in Linux
    
   </h1>
   <p>
    
     In the realm of Linux operating systems, daemons are a fundamental component that runs quietly
    
    <a id="_idIndexMarker119">
    </a>
    
     in the background, silently executing essential tasks without the direct involvement of an interactive user.
    
    
     These processes are traditionally
    
    <a id="_idIndexMarker120">
    </a>
    
     identified by their names ending with the letter
    
    <em class="italic">
     
      d
     
    </em>
    
     , such as
    
    <strong class="bold">
     
      sshd
     
    </strong>
    
     for the
    
    <strong class="bold">
     
      Secure Shell
     
    </strong>
    
     (
    
    <strong class="bold">
     
      SSH
     
    </strong>
    
     ) daemon and
    
    <strong class="source-inline">
     
      httpd
     
    </strong>
    
     for the
    
    <strong class="bold">
     
      web server daemon
     
    </strong>
    
     .
    
    
     They
    
    <a id="_idIndexMarker121">
    </a>
    
     play a vital role in handling system-level tasks crucial for both the operating system and the applications running
    
    
     
      on it.
     
    
   </p>
   <p>
    
     Daemons serve
    
    <a id="_idIndexMarker122">
    </a>
    
     an array of purposes, ranging from file serving, web serving, and network communications to logging and monitoring services.
    
    
     They are designed to be autonomous and resilient, starting at system boot and running continuously until the system is shut down.
    
    
     Unlike regular processes initiated and controlled
    
    <a id="_idIndexMarker123">
    </a>
    
     by users, daemons possess
    
    
     
      distinct characteristics:
     
    
   </p>
   <ul>
    <li>
     
      <strong class="bold">
       
        Background operation
       
      </strong>
     
     
      
       :
      
     
     <ul>
      <li>
       
        Daemons operate in
       
       
        
         the background
        
       
      </li>
      <li>
       
        They lack a controlling terminal for direct
       
       
        
         user interaction
        
       
      </li>
      <li>
       
        They do not require a user interface or manual intervention to perform
       
       
        
         their tasks
        
       
      </li>
     </ul>
    </li>
    <li>
     
      <strong class="bold">
       
        User independence
       
      </strong>
     
     
      
       :
      
     
     <ul>
      <li>
       
        Daemons operate independently of
       
       
        
         user sessions
        
       
      </li>
      <li>
       
        They
       
       <a id="_idIndexMarker124">
       </a>
       
        function autonomously without direct
       
       
        
         user involvement
        
       
      </li>
      <li>
       
        They wait for system events or specific requests to trigger
       
       
        
         their actions
        
       
      </li>
     </ul>
    </li>
    <li>
     
      <strong class="bold">
       
        Task-oriented focus
       
      </strong>
     
     
      
       :
      
     
     <ul>
      <li>
       
        Each daemon is tailored to execute a specific task or a set
       
       
        
         of tasks
        
       
      </li>
      <li>
       
        They are designed to handle specific functions or listen for particular events
       
       
        
         or requests
        
       
      </li>
      <li>
       
        This ensures efficient
       
       
        
         task execution
        
       
      </li>
     </ul>
    </li>
   </ul>
   <p>
    
     Creating a
    
    <a id="_idIndexMarker125">
    </a>
    
     daemon process involves more than merely running a process in the background.
    
    
     To ensure effective operation as a daemon, developers
    
    <a id="_idIndexMarker126">
    </a>
    
     must consider several
    
    
     
      key steps:
     
    
   </p>
   <ol>
    <li>
     <strong class="bold">
      
       Detaching from the terminal
      
     </strong>
     
      : The
     
     <strong class="source-inline">
      
       fork()
      
     </strong>
     
      system call is employed to detach the daemon from the terminal.
     
     
      The parent process exits after the fork, leaving the child process running in
     
     
      
       the background.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Session creation
      
     </strong>
     
      : The
     
     <strong class="source-inline">
      
       setsid()
      
     </strong>
     
      system call creates a new session and designates the calling process as the leader of both the session and the process group.
     
     
      This step is crucial for complete detachment from
     
     
      
       the terminal.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Working directory change
      
     </strong>
     
      : To prevent blocking the unmounting of the filesystem, daemons typically change their working directory to the
     
     
      
       root directory.
      
     
    </li>
    <li>
     <strong class="bold">
      
       File descriptor handling
      
     </strong>
     
      : Inherited file descriptors are closed by daemons, and
     
     <strong class="source-inline">
      
       stdin
      
     </strong>
     
      ,
     
     <strong class="source-inline">
      
       stdout
      
     </strong>
     
      , and
     
     <strong class="source-inline">
      
       stderr
      
     </strong>
     
      are often redirected
     
     
      
       to
      
     
     
      <strong class="source-inline">
       
        /dev/null
       
      </strong>
     
     
      
       .
      
     
    </li>
    <li>
     <strong class="bold">
      
       Signal handling
      
     </strong>
     
      : Proper handling of signals, such as
     
     <strong class="source-inline">
      
       SIGHUP
      
     </strong>
     
      for configuration
     
     <a id="_idIndexMarker127">
     </a>
     
      reloading or
     
     <strong class="source-inline">
      
       SIGTERM
      
     </strong>
     
      for graceful shutdown, is essential for effective
     
     
      
       daemon management.
      
     
    </li>
   </ol>
   <p>
    
     Daemons communicate with other processes or daemons through various
    
    
     
      IPC mechanisms.
     
    
   </p>
   <p>
    
     Daemons are integral to the architecture of many asynchronous systems, providing essential services without direct user interaction.
    
    
     Some prominent use cases of daemons
    
    <a id="_idIndexMarker128">
    </a>
    
     include
    
    
     
      the following:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Web servers
      
     </strong>
     
      : Daemons such as
     
     <strong class="source-inline">
      
       httpd
      
     </strong>
     
      and nginx serve web pages in response to client requests, handling multiple requests concurrently and ensuring seamless
     
     
      
       web browsing.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Database servers
      
     </strong>
     
      : Daemons such as mysqld and postgresql manage database services, allowing for asynchronous access and manipulation of databases by
     
     
      
       various applications.
      
     
    </li>
    <li>
     <strong class="bold">
      
       File servers
      
     </strong>
     
      : Daemons such as
     
     <strong class="bold">
      
       smbd
      
     </strong>
     
      and
     
     <strong class="bold">
      
       nfsd
      
     </strong>
     
      provide networked file services, enabling asynchronous file sharing and access across
     
     
      
       different systems.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Logging and monitoring
      
     </strong>
     
      : Daemons such as
     
     <strong class="bold">
      
       syslogd
      
     </strong>
     
      and
     
     <strong class="bold">
      
       snmpd
      
     </strong>
     
      collect and log system events, providing asynchronous monitoring of system health
     
     
      
       and performance.
      
     
    </li>
   </ul>
   <p>
    
     In summary, daemons are essential components of Linux systems, silently performing critical
    
    <a id="_idIndexMarker129">
    </a>
    
     tasks in the background to ensure smooth system operation and efficient application execution.
    
    
     Their autonomous nature and resilience make them indispensable for maintaining system stability and providing essential services to users
    
    
     
      and applications.
     
    
   </p>
   <p>
    
     We have seen processes and demons, a special type of process.
    
    
     A process can have one or more threads of execution.
    
    
     In the next section, we will be
    
    
     
      introducing threads.
     
    
   </p>
   <h1 id="_idParaDest-42">
    <a id="_idTextAnchor041">
    </a>
    
     Threads
    
   </h1>
   <p>
    
     Processes and threads represent two fundamental ways of executing code concurrently, but they
    
    <a id="_idIndexMarker130">
    </a>
    
     differ significantly in their operation and resource management.
    
    
     A process is an instance of a running program that owns its private set of resources, including memory, file descriptors, and execution context.
    
    
     Processes are isolated from each other, providing robust stability across the system since the failure of one process generally does not
    
    
     
      affect others.
     
    
   </p>
   <p>
    
     Threads are a fundamental concept in computer science, representing a lightweight and efficient way to execute multiple tasks within a single process.
    
    
     In contrast to processes, which are independent entities with their own private memory space and resources, threads are closely intertwined with the process they belong to.
    
    
     This intimate relationship allows threads to share the same memory space and resources, including file descriptors, heap memory, and any other global data structures allocated by
    
    
     
      the process.
     
    
   </p>
   <p>
    
     One of the key advantages of threads is their ability to communicate and share data efficiently.
    
    
     Since all threads within a process share the same memory space, they can directly access and modify common variables without the need for complex IPC mechanisms.
    
    
     This shared environment enables rapid data exchange and facilitates the implementation of concurrent algorithms and
    
    
     
      data structures.
     
    
   </p>
   <p>
    
     However, sharing the same memory space also introduces the challenge of managing access to shared resources.
    
    
     To prevent data corruption and ensure the integrity of shared data, threads must employ synchronization mechanisms such as locks, semaphores, or mutexes.
    
    
     These mechanisms enforce rules and protocols for accessing shared resources, ensuring that only one thread can access a particular resource at any
    
    
     
      given time.
     
    
   </p>
   <p>
    
     Effective synchronization is crucial in multithreaded programming to avoid race conditions, deadlocks, and other
    
    
     
      concurrency-related issues.
     
    
   </p>
   <p>
    
     To address these challenges, various synchronization primitives and techniques have been developed.
    
    
     These include mutexes, which provide exclusive access to a shared resource, semaphores, which allow for controlled access to a limited number of resources, and condition variables, which enable threads to wait for specific conditions to be met
    
    
     
      before proceeding.
     
    
   </p>
   <p>
    
     By carefully managing synchronization and employing appropriate concurrency patterns, developers can harness the power of threads to achieve high performance and scalability in their applications.
    
    
     Threads are particularly well-suited for tasks that can be parallelized, such as image processing, scientific simulations, and web servers, where multiple independent computations can be
    
    
     
      executed concurrently.
     
    
   </p>
   <p>
    
     Threads, as described previously, are system threads.
    
    
     This means that they are created and managed
    
    <a id="_idIndexMarker131">
    </a>
    
     by the kernel.
    
    
     However, there are scenarios, which we will explore in depth in
    
    <a href="B22219_08.xhtml#_idTextAnchor164">
     
      <em class="italic">
       
        Chapter 8
       
      </em>
     
    </a>
    
     , where we will require a multitude of threads.
    
    
     In such cases, the system might not have sufficient resources to create numerous system threads.
    
    
     The solution to this
    
    <a id="_idIndexMarker132">
    </a>
    
     problem is the use of
    
    <strong class="bold">
     
      user threads
     
    </strong>
    
     .
    
    
     One approach to implementing user threads is through
    
    <strong class="bold">
     
      coroutines
     
    </strong>
    
     , which have been included in the C++ standard
    
    
     
      since C++20.
     
    
   </p>
   <p>
    
     Coroutines are a relatively
    
    <a id="_idIndexMarker133">
    </a>
    
     new feature in C++.
    
    
     Coroutines can be defined as functions that can be paused and resumed at specific points, allowing for cooperative multitasking within a single thread.
    
    
     Unlike standard functions that run from start to finish without interruption, coroutines can suspend their execution and yield control back to the caller, which can later resume the coroutine from the point it
    
    
     
      was paused.
     
    
   </p>
   <p>
    
     Coroutines are much more lightweight than system threads.
    
    
     This means that they can be created and destroyed much more quickly, and that they require
    
    
     
      less overhead.
     
    
   </p>
   <p>
    
     Coroutines are cooperative, which means that they must explicitly yield control to the caller in order to switch execution context.
    
    
     This can be a disadvantage in some cases, but it can also be an advantage, as it gives the user program more control over the execution
    
    
     
      of coroutines.
     
    
   </p>
   <p>
    
     Coroutines can be used to create a variety of different concurrency patterns.
    
    
     For example, coroutines can be used to implement tasks, which are lightweight work units that can be scheduled and run concurrently.
    
    
     Coroutines can also be used to implement channels, which are communication channels that can pass data
    
    
     
      between them.
     
    
   </p>
   <p>
    
     Coroutines can be classified into stackful and stackless categories.
    
    
     C++20 coroutines are stackless.
    
    
     We will see these concepts in depth in
    
    <a href="B22219_08.xhtml#_idTextAnchor164">
     
      <em class="italic">
       
        Chapter 8
       
      </em>
     
    </a>
    
     
      .
     
    
   </p>
   <p>
    
     Overall, coroutines are a powerful tool for creating concurrent programs in C++.
    
    
     They are lightweight, cooperative, and can be used to implement a variety of different concurrency patterns.
    
    
     They cannot be used to implement parallelism entirely because coroutines still need CPU execution context, which can be only provided by
    
    
     
      a thread.
     
    
   </p>
   <h2 id="_idParaDest-43">
    <a id="_idTextAnchor042">
    </a>
    
     Thread life cycle
    
   </h2>
   <p>
    
     The life cycle
    
    <a id="_idIndexMarker134">
    </a>
    
     of a system thread, often referred to as a lightweight process, encompasses the stages from its creation until its termination.
    
    
     Each stage plays a crucial role in managing and utilizing threads in a concurrent
    
    
     
      programming environment:
     
    
   </p>
   <ol>
    <li>
     <strong class="bold">
      
       Creation
      
     </strong>
     
      : This phase begins when a new thread is created in the system.
     
     
      The creation process involves using the function, which takes several parameters.
     
     
      One critical parameter is the thread’s attributes, such as its scheduling policy, stack size, and priority.
     
     
      Another essential parameter is the function that the thread will execute, known as the start routine.
     
     
      Upon its successful creation, the thread is allocated its own stack and
     
     
      
       other resources.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Execution
      
     </strong>
     
      : After creation, the thread starts executing its assigned start routine.
     
     
      During execution, the thread can perform various tasks independently or interact with other threads if necessary.
     
     
      Threads can also create and manage their own local variables and data structures, making them self-contained and capable of performing specific
     
     
      
       tasks concurrently.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Synchronization
      
     </strong>
     
      : To ensure orderly access to shared resources and prevent data corruption, threads employ synchronization mechanisms.
     
     
      Common synchronization primitives include locks, semaphores, and barriers.
     
     
      Proper synchronization allows threads to coordinate their activities, avoiding race conditions, deadlocks, and other issues that can arise in
     
     
      
       concurrent programming.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Termination
      
     </strong>
     
      : A thread can terminate in several ways.
     
     
      It can explicitly call the function to terminate itself.
     
     
      It can also terminate by returning from its start routine.
     
     
      In some cases, a thread can be canceled by another thread using the function.
     
     
      Upon termination, the system reclaims the resources allocated to the thread, and any pending operations or locks held by the thread
     
     
      
       are released.
      
     
    </li>
   </ol>
   <p>
    
     Understanding the life cycle of a system thread is essential for designing and implementing concurrent programs.
    
    
     By carefully managing thread creation, execution, synchronization, and termination, developers can create efficient and scalable applications that leverage the benefits
    
    
     
      of concurrency.
     
    
   </p>
   <h2 id="_idParaDest-44">
    <a id="_idTextAnchor043">
    </a>
    
     Thread scheduling
    
   </h2>
   <p>
    
     System threads, managed by the operating system kernel’s scheduler, are scheduled preemptively.
    
    
     The scheduler decides when to switch execution between threads based on factors
    
    <a id="_idIndexMarker135">
    </a>
    
     such as thread priority, allocated time, or mutex blocking.
    
    
     This context switch, controlled by the kernel, can incur significant overhead.
    
    
     The high cost of context switches, coupled with the resource usage of each thread (such as its own stack), makes coroutines a more efficient alternative for some applications because we can run more than one coroutine in a
    
    
     
      single thread.
     
    
   </p>
   <p>
    
     Coroutines offer several advantages.
    
    
     First, they reduce the overhead associated with context switches.
    
    
     Since context switching on coroutine yield or await is handled by the user space code rather than the kernel, the process is more lightweight and efficient.
    
    
     This results in significant performance gains, especially in scenarios where frequent context
    
    
     
      switching occurs.
     
    
   </p>
   <p>
    
     Coroutines also provide greater control over thread scheduling.
    
    
     Developers can define custom scheduling policies based on the specific requirements of their application.
    
    
     This flexibility allows for fine-tuned thread management, resource utilization optimization, and desired performance
    
    
     
      characteristics achievement.
     
    
   </p>
   <p>
    
     Another important feature of coroutines is that they are generally more lightweight compared to system threads.
    
    
     Coroutines don’t maintain their own stack, which is a great resource consumption advantage, making them suitable for
    
    
     
      resource-constrained environments.
     
    
   </p>
   <p>
    
     Overall, coroutines offer a more efficient and flexible approach to thread management, particularly in situations where frequent context switching is required or where fine-grained control over thread scheduling is essential.
    
    
     Threads can access the memory process and this memory is shared among all the threads, so we need to be careful and control memory access.
    
    
     This control is achieved by different mechanisms called
    
    
     
      synchronization primitives.
     
    
   </p>
   <h1 id="_idParaDest-45">
    <a id="_idTextAnchor044">
    </a>
    
     Synchronization primitives
    
   </h1>
   <p>
    
     Synchronization primitives are essential tools for managing concurrent access to shared resources
    
    <a id="_idIndexMarker136">
    </a>
    
     in multithreaded programming.
    
    
     There are several synchronization primitives, each with its own specific purpose
    
    
     
      and characteristics:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Mutexes
      
     </strong>
     
      : Mutexes
     
     <a id="_idIndexMarker137">
     </a>
     
      are used to enforce
     
     <a id="_idIndexMarker138">
     </a>
     
      exclusive access to critical sections of code.
     
     
      A mutex can be locked by a thread, preventing other threads from entering the protected section until the mutex is unlocked.
     
     
      Mutexes guarantee that only one thread can execute the critical section at any given time, ensuring data integrity and preventing
     
     
      
       race conditions.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Semaphores
      
     </strong>
     
      : Semaphores are more versatile than mutexes and can be used for a
     
     <a id="_idIndexMarker139">
     </a>
     
      wider range of synchronization tasks, including signaling between threads.
     
     
      A semaphore maintains
     
     <a id="_idIndexMarker140">
     </a>
     
      an integer counter that can be incremented (signaling) or decremented (waiting) by threads.
     
     
      Semaphores allow for more complex coordination patterns, such as counting semaphores (for resource allocation) and binary semaphores (similar
     
     
      
       to mutexes).
      
     
    </li>
    <li>
     <strong class="bold">
      
       Condition variables
      
     </strong>
     
      : Condition variables are used for thread synchronization based
     
     <a id="_idIndexMarker141">
     </a>
     
      on specific conditions.
     
     
      Threads can block (wait on) a condition variable until a particular condition
     
     <a id="_idIndexMarker142">
     </a>
     
      becomes true.
     
     
      Other threads can signal the condition variable, causing waiting threads to wake up and continue execution.
     
     
      Condition variables are often used in conjunction with mutexes to achieve more fine-grained synchronization and avoid
     
     
      
       busy waiting.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Additional synchronization primitives
      
     </strong>
     
      : In addition to the core synchronization primitives discussed previously, there are several other
     
     
      
       synchronization mechanisms:
      
     
     <ul>
      <li>
       <strong class="bold">
        
         Barriers
        
       </strong>
       
        : Barriers
       
       <a id="_idIndexMarker143">
       </a>
       
        allow a group of threads to synchronize their execution, ensuring that all threads reach a certain point before
       
       
        
         proceeding further
        
       
      </li>
      <li>
       <strong class="bold">
        
         Read-write locks
        
       </strong>
       
        : Read-write
       
       <a id="_idIndexMarker144">
       </a>
       
        locks provide a way to control concurrent access to shared data, allowing multiple readers but only a single writer at
       
       
        
         a time
        
       
      </li>
      <li>
       <strong class="bold">
        
         Spinlocks
        
       </strong>
       
        : Spinlocks
       
       <a id="_idIndexMarker145">
       </a>
       
        are a type of mutex that involves busy waiting, continuously checking a memory location until it
       
       
        
         becomes available
        
       
      </li>
     </ul>
    </li>
   </ul>
   <p>
    
     In
    
    <em class="italic">
     
      Chapters 4
     
    </em>
    
     and
    
    <em class="italic">
     
      5
     
    </em>
    
     , we
    
    <a id="_idIndexMarker146">
    </a>
    
     will see the synchronization primitives implemented in the C++
    
    <strong class="bold">
     
      Standard Template Library
     
    </strong>
    
     (
    
    <strong class="bold">
     
      STL
     
    </strong>
    
     ) in depth and examples of how to
    
    
     
      use them.
     
    
   </p>
   <h2 id="_idParaDest-46">
    <a id="_idTextAnchor045">
    </a>
    
     Choosing the right synchronization primitive
    
   </h2>
   <p>
    
     The choice
    
    <a id="_idIndexMarker147">
    </a>
    
     of the appropriate synchronization primitive depends on the specific requirements of the application and the nature of the shared resources being accessed.
    
    
     Here are some
    
    
     
      general guidelines:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Mutexes
      
     </strong>
     
      : Use
     
     <a id="_idIndexMarker148">
     </a>
     
      mutexes when exclusive access to a critical section is required to ensure data integrity and prevent
     
     
      
       race conditions
      
     
    </li>
    <li>
     <strong class="bold">
      
       Semaphores
      
     </strong>
     
      : Use
     
     <a id="_idIndexMarker149">
     </a>
     
      semaphores when more complex coordination patterns are needed, such as resource allocation or signaling
     
     
      
       between threads
      
     
    </li>
    <li>
     <strong class="bold">
      
       Condition variables
      
     </strong>
     
      : Use
     
     <a id="_idIndexMarker150">
     </a>
     
      condition variables when threads need to wait for a specific condition to become true
     
     
      
       before proceeding
      
     
    </li>
   </ul>
   <p>
    
     Effective use of synchronization primitives is crucial for developing safe and efficient multithreaded programs.
    
    
     By understanding the purpose and characteristics of different synchronization mechanisms, developers can choose the most suitable primitives for their specific needs and achieve reliable and predictable
    
    
     
      concurrent execution.
     
    
   </p>
   <h1 id="_idParaDest-47">
    <a id="_idTextAnchor046">
    </a>
    
     Common problems when using multiple threads
    
   </h1>
   <p>
    
     Threading
    
    <a id="_idIndexMarker151">
    </a>
    
     introduces several challenges that must be managed to ensure application correctness and performance.
    
    
     These challenges arise from the inherent concurrency and non-deterministic nature of
    
    
     
      multithreaded programming.
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Race conditions
      
     </strong>
     
      occur when multiple threads access and modify shared data concurrently.
     
     
      The outcome of a race condition depends on the non-deterministic sequencing of threads’ operations, which can lead to unpredictable and inconsistent results.
     
     
      For example, consider two threads that are updating a shared counter.
     
     
      If the threads increment the counter concurrently, the final value may be incorrect due to a
     
     
      
       race condition.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Deadlocks
      
     </strong>
     
      occur when two or more threads wait indefinitely for resources held by each other.
     
     
      This creates a cycle of dependencies that cannot be resolved, causing the threads to become permanently blocked.
     
     
      For instance, consider two threads that are waiting for each other to release locks on shared resources.
     
     
      If neither thread releases the lock it holds, a
     
     
      
       deadlock occurs.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Starvation
      
     </strong>
     
      occurs when a thread is perpetually denied access to resources it needs to make progress.
     
     
      This can happen when other threads continuously acquire and hold resources, leaving the starved thread unable
     
     
      
       to execute.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Livelocks
      
     </strong>
     
      are like deadlocks, but instead of being permanently blocked, the threads remain active and repeatedly try to acquire resources, only without making
     
     
      
       any progress.
      
     
    </li>
   </ul>
   <p>
    
     Several
    
    <a id="_idIndexMarker152">
    </a>
    
     techniques can be used to manage the challenges of threading, including
    
    
     
      the following:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Synchronization mechanisms
      
     </strong>
     
      : As described previously, synchronization primitives such as locks and mutexes can be used to control access to shared data and ensure that only one thread can access the data at
     
     
      
       a time.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Deadlock prevention and detection
      
     </strong>
     
      : Deadlock prevention algorithms can be used to avoid deadlocks, while deadlock detection algorithms can be used to identify and resolve deadlocks when
     
     
      
       they occur.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Thread scheduling
      
     </strong>
     
      : Thread scheduling algorithms can be used to determine which
     
     <a id="_idIndexMarker153">
     </a>
     
      thread should run at any given time, as well as which can help to prevent starvation and improve application performance.
     
     
      We will see the different solutions to multithreading issues in much
     
     
      
       more detail.
      
     
    </li>
   </ul>
   <h1 id="_idParaDest-48">
    <a id="_idTextAnchor047">
    </a>
    
     Strategies for effective thread management
    
   </h1>
   <p>
    
     There
    
    <a id="_idIndexMarker154">
    </a>
    
     are different ways to handle threads to avoid multithreading issues.
    
    
     The following are some of the most common ways to
    
    
     
      handle threads:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Minimize shared state
      
     </strong>
     
      : Designing threads to operate on private data as much as possible significantly reduces the need for synchronization.
     
     
      By allocating memory for thread-specific data using thread-local storage, the need for global variables is eliminated, further reducing the potential for data contention.
     
     
      Careful management of shared data access through synchronization primitives is essential to ensure data integrity.
     
     
      This approach enhances the efficiency and correctness of multithreaded applications by minimizing the need for synchronization and ensuring that shared data is accessed in a controlled and
     
     
      
       consistent manner.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Lock hierarchy
      
     </strong>
     
      : Establishing a well-defined lock hierarchy is crucial for preventing deadlocks in multithreaded programming.
     
     
      A lock hierarchy dictates the order in which locks are acquired and released, ensuring a consistent locking pattern across threads.
     
     
      By acquiring locks in a hierarchical manner, from the coarsest to the finest granularity, the possibility of deadlocks is
     
     
      
       significantly reduced.
      
     
     <p class="list-inset">
      
       The coarsest level of granularity refers to locks that control access to a large portion of the shared resource, while the finest granularity locks are used for specific, fine-grained parts of the resource.
      
      
       By acquiring the coarse-grained lock first, threads can gain exclusive access to a larger section of the resource, reducing the likelihood of conflicts with other threads attempting to access the same resource.
      
      
       Once the coarse-grained lock is acquired, finer-grained locks can be obtained to control access to specific parts of the resource, providing more granular control and reducing the waiting time for
      
      
       
        other threads.
       
      
     </p>
     <p class="list-inset">
      
       In some cases, lock-free data structures can be employed to eliminate the need for locks
      
      <a id="_idIndexMarker155">
      </a>
      
       altogether.
      
      
       Lock-free data structures are designed to provide concurrent access to shared resources without explicit locks.
      
      
       Instead, they rely on atomic operations and non-blocking algorithms to ensure data integrity and consistency.
      
      
       By utilizing lock-free data structures, the overhead associated with lock acquisition and release is eliminated, resulting in improved performance and scalability in
      
      
       
        multithreaded applications:
       
      
     </p>
    </li>
    <li>
     <strong class="bold">
      
       Timeouts
      
     </strong>
     
      : To prevent threads from waiting indefinitely when trying to acquire a lock, it is important to set timeouts for lock acquisition.
     
     
      This ensures that if a thread cannot acquire the lock within the specified timeout period, it will automatically give up and try again later.
     
     
      This helps prevent deadlocks and ensures that no thread is left
     
     
      
       waiting indefinitely.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Thread pools
      
     </strong>
     
      : Managing a pool of reusable threads is a key technique for optimizing the performance of multithreaded applications.
     
     
      By creating and destroying threads dynamically, the overhead of thread creation and termination can be reduced significantly.
     
     
      The size of the thread pool should be tuned based on the application’s workload and resource constraints.
     
     
      A too-small pool may result in tasks waiting for available threads, while a too-large pool may waste resources.
     
     
      Work queues are used to manage tasks and assign them to available threads in the pool.
     
     
      Tasks are added to the queue and processed by the threads in a FIFO order.
     
     
      This ensures fairness and prevents the starvation of tasks.
     
     
      The use of work queues also allows for load balancing, as tasks can be distributed evenly across the
     
     
      
       available threads.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Synchronization primitives
      
     </strong>
     
      : Understand the different types of synchronization primitives, such as mutexes, semaphores, and condition variables.
     
     
      Choose the appropriate primitive based on the synchronization requirements of the specific scenario.
     
     
      Use synchronization primitives correctly to avoid race conditions
     
     
      
       and deadlocks.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Testing and debugging
      
     </strong>
     
      : Test multi-threaded applications thoroughly to identify and fix threading issues.
     
     
      Use tools such as thread sanitizers and profilers to detect
     
     <a id="_idIndexMarker156">
     </a>
     
      data races and performance bottlenecks.
     
     
      Employ debugging techniques such as step-by-step execution and thread dumps to analyze and resolve threading problems.
     
     
      We will see testing and debugging in
     
     <em class="italic">
      
       Chapters 11
      
     </em>
     
      
       and
      
     
     
      <em class="italic">
       
        12
       
      </em>
     
     
      
       .
      
     
    </li>
    <li>
     <strong class="bold">
      
       Scalability and performance considerations
      
     </strong>
     
      : Design thread-safe data structures and algorithms to ensure scalability and performance.
     
     
      Balance the number of threads with the available resources to avoid over-subscription.
     
     
      Monitor system metrics such as CPU utilization and thread contention to identify potential
     
     
      
       performance bottlenecks.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Communication and collaboration
      
     </strong>
     
      : Foster collaboration among developers working on multi-threaded code to ensure consistency and correctness.
     
     
      Establish coding guidelines and best practices for thread management to maintain code quality and readability.
     
     
      Regularly review and update the threading strategy as the
     
     
      
       application evolves.
      
     
    </li>
   </ul>
   <p>
    
     Threading is a powerful tool that can be used to improve the performance and scalability of applications.
    
    
     However, it is important to understand the challenges of threading and to use appropriate techniques to manage these challenges.
    
    
     By doing so, developers can create multithreaded applications that are correct, efficient,
    
    
     
      and reliable.
     
    
   </p>
   <h1 id="_idParaDest-49">
    <a id="_idTextAnchor048">
    </a>
    
     Summary
    
   </h1>
   <p>
    
     In this chapter, we explored the concept of processes in operating systems.
    
    
     Processes are fundamental entities that execute programs and manage resources on the computer.
    
    
     We delved into the process life cycle, examining the various stages a process goes through from creation to termination.
    
    
     Additionally, we discussed IPC, which is crucial for processes to interact and exchange information with
    
    
     
      each other.
     
    
   </p>
   <p>
    
     Furthermore, we introduced daemons in the context of Linux operating systems.
    
    
     Daemons are special types of processes that run in the background as services and perform specific tasks such as managing system resources, handling network connections, or providing other essential services to the system.
    
    
     We also explored the concepts of system and user threads, which are lightweight processes that share the same address space as the parent process.
    
    
     We discussed the advantages of multithreaded applications, including improved performance and responsiveness, as well as the challenges associated with managing and synchronizing multiple threads within a
    
    
     
      single process.
     
    
   </p>
   <p>
    
     Knowing the different issues created by multithreading is fundamental to understanding how to fix them.
    
    
     In the next chapter, we will see how to create threads, and then in
    
    <a href="B22219_04.xhtml#_idTextAnchor074">
     
      <em class="italic">
       
        Chapter 4
       
      </em>
     
    </a>
    
     and
    
    <a href="B22219_05.xhtml#_idTextAnchor097">
     
      <em class="italic">
       
        Chapter 5
       
      </em>
     
    </a>
    
     , we will study the different synchronization primitives the standard C++ offers and their different applications
    
    
     
      in depth.
     
    
   </p>
   <h1 id="_idParaDest-50">
    <a id="_idTextAnchor049">
    </a>
    
     Further reading
    
   </h1>
   <ul>
    <li>
     
      [Butenhof, 1997] David R.
     
     
      Butenhof,
     
     <em class="italic">
      
       Programming with POSIX Threads
      
     </em>
     
      , Addison
     
     
      
       Wesley, 1997.
      
     
    </li>
    <li>
     
      [Kerrisk, 2010] Michael Kerrisk,
     
     <em class="italic">
      
       The Linux Programming Interface
      
     </em>
     
      , No Starch
     
     
      
       Press, 2010.
      
     
    </li>
    <li>
     
      [Stallings, 2018] William Stallings,
     
     <em class="italic">
      
       Operating Systems Internals and Design Principles
      
     </em>
     
      , Ninth Edition, Pearson
     
     
      
       Education 2018.
      
     
    </li>
    <li>
     
      [Williams, 2019] Anthony Williams,
     
     <em class="italic">
      
       C++ Concurrency in Action
      
     </em>
     
      , Second Edition,
     
     
      
       Manning 2019.
      
     
    </li>
   </ul>
  </div>
 

  <div><h1 id="_idParaDest-51" lang="en-US" xml:lang="en-US">
    <a id="_idTextAnchor050">
    </a>
    
     Part 2: Advanced Thread  Management and Synchronization Techniques
    
   </h1>
  </div>
  <div><p>
    
     In this part, we build upon the foundational knowledge of parallel programming and dive deeper into advanced techniques for managing threads and synchronizing concurrent operations.
    
    
     We will explore essential concepts such as thread creation and management, exception handling across threads, and efficient thread coordination, acquiring a solid understanding of key synchronization primitives, including mutexes, semaphores, condition variables, and atomic operations.
    
    
     All this knowledge will equip us with the tools needed to implement both lock-based and lock-free multithreaded solutions, offering a glimpse into high-performance concurrent systems, and providing the skills necessary to avoid common pitfalls such as race conditions, deadlocks, and livelocks when managing
    
    
     
      multithreaded systems.
     
    
   </p>
   <p>
    
     This part has the
    
    
     
      following chapters:
     
    
   </p>
   <ul>
    <li>
     <a href="B22219_03.xhtml#_idTextAnchor051">
      <em class="italic">
       
        Chapter 3
       
      </em>
     </a>
     
      ,
     
     <em class="italic">
      
       How to Create and Manage Threads in C++
      
     </em>
    </li>
    <li>
     <a href="B22219_04.xhtml#_idTextAnchor074">
      <em class="italic">
       
        Chapter 4
       
      </em>
     </a>
     
      ,
     
     <em class="italic">
      
       Thread Synchronization with Locks
      
     </em>
    </li>
    <li>
     <a href="B22219_05.xhtml#_idTextAnchor097">
      <em class="italic">
       
        Chapter 5
       
      </em>
     </a>
     
      ,
     
     <em class="italic">
      
       Atomic Operations
      
     </em>
    </li>
   </ul>
  </div>
  <div><div></div>
  </div>
  <div><div></div>
  </div>
 </body></html>