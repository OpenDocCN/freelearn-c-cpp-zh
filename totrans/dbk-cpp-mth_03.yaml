- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There’s a Single C++, and It Is Object-Oriented
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Only if you ignore all* *the others*'
  prefs: []
  type: TYPE_NORMAL
- en: C++ was born as C with objects, which makes many developers still consider it
    an OOP language. We will see in this chapter that C++ allows multiple paradigms,
    and you could safely describe it as multiple programming languages in one. We
    will look at a few paradigms supported in C++, including structured programming,
    OOP, functional programming, and metaprogramming, in combination with the choice
    of strong versus quasi-optional types.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The multiple facets of C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional programming in C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metaprogramming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strong types to the limit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What about ignoring types?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter is available from the GitHub repository [https://github.com/PacktPublishing/Debunking-CPP-Myths](https://github.com/PacktPublishing/Debunking-CPP-Myths)
    , in the **ch3** folder. It uses Makefile, g++, and the doctest library ( [https://github.com/doctest/doctest](https://github.com/doctest/doctest)
    ) for unit testing. The code is compiled for C++20.
  prefs: []
  type: TYPE_NORMAL
- en: The multiple facets of C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you, like me, frequently move between different organizations, teams, and
    technical conferences, you will quickly notice two things: C++ programmers have
    distinct interests compared to other developers, and the C++ community is more
    aptly described as small, specialized pockets of C++ developers. That’s different
    from other communities; if you discuss Java, you’ll likely end up talking about
    Spring Framework and REST APIs or the Android toolkit. C# is mostly fairly standardized
    around the Microsoft libraries, and JavaScript is mostly about React. But get
    100 C++ programmers in a room from different organizations and you’ll soon notice
    the differences. Embedded C++ is all about keeping all the resources in check
    because adding an extra 1 MB of memory to a device sold in millions of units quickly
    pumps up the cost. Game developers are on the other side of the spectrum, looking
    at how to squeeze extra frame rate out of next-generation GPUs and CPUs. The high-frequency
    trading people know all about avoiding CPU cache misses and how to brush off a
    picosecond of the automated transaction algorithm because the smallest time fraction
    can mean millions of euros. Engineering software developers are more relaxed,
    but still worried about the validity of the changes in a complex rendering model.
    And then you find the programmers dealing with automated systems for rails, cars,
    or factories, whose main concern is resilience and robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This picture, while far from complete, is enough to show us the immense variability
    of C++ programmers, unlike their peers using any other language. We could almost
    say that from a certain point of view, C++ is the last remaining de facto general-purpose
    language, since the other mainstream ones are used in practice mostly for specific
    types of programs: Java for enterprise backend services and Android development,
    C# for web and Windows applications and services, JavaScript for rich web frontends
    and serverless backends, and Python for scripts, data science, and DevOps. But
    C++ is in embedded software, factory systems, trading, simulations, engineering
    tools, operating systems, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: The old saying “form follows function” is about design applying to everything
    built by people, including programming languages, and applies equally well to
    C++. The large variability in projects and types of programmers fed into the language,
    along with Stroustrup’s desire to make it as capable as possible. C++ is not a
    single language; every programmer uses a subset of C++ that is often different
    from their colleagues working in the same organization.
  prefs: []
  type: TYPE_NORMAL
- en: Yes, C++ started as C with objects, at a time when OOP was on the rise. But,
    at the same time, C++ is backward compatible with C, which means you can still
    write structured programming in C++. Then, templates were needed. Then, lambdas
    were useful. While C++ has always been a collection of different languages, today
    it’s even more so. To prove this point, let’s look at a few paradigms you can
    use in C++, starting with functional programming.
  prefs: []
  type: TYPE_NORMAL
- en: Functional programming in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I remember being in university, fascinated about programming, and already quite
    adept at writing BASIC, Pascal, Logo, and simple C++. I think it was in my second
    year when I took a course on functional programming. The teacher was very passionate
    and eager to show us the wonders of this paradigm, explaining a lot of concepts
    that I couldn’t quite grasp. The course turned into a complete miss for me, since
    the only thing I learned was how to write imperative code in Lisp and how to translate
    the idioms I knew into something that would work in this weird language that wears
    its parentheses on the outside of expressions.
  prefs: []
  type: TYPE_NORMAL
- en: I tried to go back to functional programming after starting my career as a software
    engineer. There were plenty of resources online, only the way they explained the
    paradigm didn’t help. “It’s basically category theory,” they said. Everything
    is a function, even numbers (check out Church encoding). You can easily understand
    monads since they are a monoid in the category of endofunctors. This style of
    explanation uses a more complicated concept to explain a practical one and doesn’t
    facilitate understanding.
  prefs: []
  type: TYPE_NORMAL
- en: This is why it took me years to understand what functional programming is and
    how it helps with software development. I became a fan, but not a fanatic, of
    this paradigm. Like any engineer, I like to solve problems, and in my case, most
    often I solve them with code. Having code that is simpler is always great, although
    often simpler does not mean more familiar.
  prefs: []
  type: TYPE_NORMAL
- en: 'If I were to explain functional programming today, I would focus on three important
    things: *immutability* , *pure functions* , and *operations with functions* .
    Perhaps unexpectedly, C++ is a good fit for all these traits. Immutability is
    where C++ shines compared to the other mainstream programming languages (although
    less than Rust, but we’ll talk about that in the final chapter).'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there’s one catch: functional programming is a different paradigm,
    with its own trade-offs. I’ve noticed that C++ programmers find it difficult to
    think about lambdas since they see lambdas not as a fundamental concept but as
    something built on top of the existing language. That’s fair enough since lambdas
    are objects and not first-class design elements in C++. However, thinking in a
    functional paradigm requires the programmers to temporarily forget this knowledge
    and embrace the functional design elements. You can go back to this knowledge
    when you have implemented something that works and are looking for improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explain the three characteristics in more detail and then discuss the
    impact of using functional programming on our software architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Immutability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Immutability fundamentally means that each variable is initialized with a value,
    but a new value cannot be assigned to the variable. In C++, this can be done with
    **const** or **constexpr** , depending on whether we want the value to be immutable
    at runtime or at compile time.
  prefs: []
  type: TYPE_NORMAL
- en: 'While immutability is easy to understand for simple types, collections and
    objects introduce challenges. An immutable collection is one that returns a new
    collection upon every change. So, for example, the following code shows a mutable
    collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Contrast this example with a hypothetic immutable collection, shown in the
    next code sample, that returns a new collection upon adding to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This trait guarantees that you’re using the correct version of the data structure
    that you need. But the memory optimization bells might ring in your C++ brain.
    There seems to be a lot of memory allocation happening for immutable collections!
    Isn’t that a waste?
  prefs: []
  type: TYPE_NORMAL
- en: It is indeed possible to be temporarily using more memory than you’d expect
    upon performing a change in an immutable collection. However, functional languages
    have found smart ways to avoid this, and C++ is perfectly capable of using the
    same mechanisms. It depends on the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The way to optimize memory for immutable collections is to use *smart pointers*
    . Remember that values are immutable once assigned to a variable. Therefore, when
    the collection is first initialized, memory is allocated for each element of the
    collection and each memory area is assigned to a specific value. When a new element
    is added, the pointers to each element are copied and a new memory area is allocated
    for the new value. If an element is removed from the collection, all the pointers
    to existing elements are copied except the one pointing to the removed element.
    Once a memory area is no longer referenced by any pointer, it gets deleted.
  prefs: []
  type: TYPE_NORMAL
- en: While immutable collections are not implemented as such in STL, libraries such
    as immer ( [https://github.com/arximboldi/immer](https://github.com/arximboldi/immer)
    ) allow you to use this pattern without worrying too much about the internal details.
  prefs: []
  type: TYPE_NORMAL
- en: OK, but what about immutable objects? Isn’t the whole purpose of OOP to mix
    behavior with data?
  prefs: []
  type: TYPE_NORMAL
- en: To this, I have three things to say.
  prefs: []
  type: TYPE_NORMAL
- en: First, good question!
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, OOP was misunderstood to be about encapsulation, inheritance, and polymorphism
    when in fact it’s about message passing. C++ was unfortunately the trendsetter
    for what I like to call “class-oriented programming”: a style of programming focused
    on classes and their relationships instead of objects and their relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: 'And third, functional programming has in fact no qualms with objects. Implementing
    immutable objects is very simple: either we implement an immutable data structure
    using **const** , or every method that changes data returns instead a new object
    with the modified data.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth mentioning at this point that you don’t have to use immutability
    to its fullest in your programs to benefit from functional programming. I write
    enough code that maximizes constness but still uses the standard STL collections
    and objects that change their internal data. However, you need to be aware that
    the level of immutability described previously allows you to introduce parallelism
    much more easily into your programs. If values cannot change, you have no problems
    with critical sections. Each thread works with its own value, and changing the
    value will change it only for the specific thread. Indeed, this is one of the
    side benefits of immutability. I say side benefits because immutability combined
    with pure functions and good naming gives you programs that are easier to understand
    once you get used to the building blocks. So, let’s look at pure functions next.
  prefs: []
  type: TYPE_NORMAL
- en: Pure functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A pure function is a function that returns the same output for the same input
    and doesn’t change any value in the context. By definition, a pure function cannot
    do **Input/Output** ( **I/O** ) operations. However, any non-trivial program can
    be written as a combination of pure functions and I/O functions.
  prefs: []
  type: TYPE_NORMAL
- en: Pure functions are the simplest types of functions you can think of. They are
    easy to understand, very predictable, and cacheable because they lack side effects.
    This leads to easy testing with data-driven unit tests and potential optimizations
    such as caching the result of the function upon first call with specific inputs
    and reusing it later.
  prefs: []
  type: TYPE_NORMAL
- en: Pure functions are at the core of functional programming. In C++, they are very
    easy to implement using the support for immutability.
  prefs: []
  type: TYPE_NORMAL
- en: The original way of writing functions in pure functional languages is lambdas.
    Lambdas have made their way into the standard since C++11. However, C++ lambdas
    can be mutable because they can change the variables they capture in their context.
    So, writing pure functions in C++, even with lambdas, requires you to ensure the
    constness of all variables involved.
  prefs: []
  type: TYPE_NORMAL
- en: In a functional paradigm, everything is either a function or a data structure,
    and in pure functional languages, the two are interchangeable. So, how do we create
    complex behaviors from simple functions? We compose functions using various operations,
    of course.
  prefs: []
  type: TYPE_NORMAL
- en: Operations on functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since functions are the main design element of functional programming, it’s
    par to the course to think about how functions can change through operations.
    The most common functional operations are partial application and composition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Partial application refers to creating a new function by binding the value
    of one parameter of a function to a specific value. For example, if we have a
    function, **add(const int first, const int second)** , we can obtain the **increment(const
    int)** function by binding the **second** parameter to the value **1** . Let’s
    take a moment to consider the consequence: every function, no matter how many
    arguments it receives, can be reduced through subsequent partial applications
    to functions that take no parameters. This gives us a universal language for expressing
    anything in code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement a partial application in C++, we can use the **std::bind** function
    from the **<functional>** header. Let’s see how we can obtain the **increment**
    function from the **add** function by binding the second parameter of **add**
    to the value **1** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is a neat solution from a functional programming perspective. However,
    the return value is complicated and approximates a function instead of being a
    function. This is one of the mental hurdles for C++ programmers when trying functional
    programming. I’ve been away from the language long enough to allow myself to think
    in higher-level concepts instead of always analyzing the implementation. So, when
    I use **std::bind** to do the partial application, I treat the result as a function
    and hope the implementors have done their job of optimizing and providing the
    necessary behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: The other fundamental operation with functions is functional composition. You’ve
    probably encountered this construct in mathematics. Functional composition refers
    to creating a function, *f* , from two functions, *g* and *h* , such that *f(x)
    = g(h(x))* for any value *x* . This is commonly denoted in math as *f =* *g* *∘*
    *h* .
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there’s no function or operation in the C++ standard that implements
    functional composition, but it’s easy to implement this operation with templates.
    Once again, the result of this operation in C++ is complicated, but I encourage
    you to think about it as a function rather than the actual data structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see a possible implementation for functional composition in C++. The
    **compose** function takes two type parameters, **F** and **G** , that each denote
    the type of functions **f** and **g** to compose. The **compose** function returns
    a lambda that takes one parameter, **value** , and returns **f(g(value)** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The preceding example is borrowed from Alex’s other book on the topic with Packt
    Publishing, *Hands-On Functional Programming* *in C++* .
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we could use this function with a simple example. Let’s implement
    a price calculator that takes as parameters the price, a discount, a service fee,
    and tax and returns the final price. Let’s look first at an imperative implementation,
    using a single function that computes everything inline. The **computePriceImperative**
    function takes the price, subtracts the discount, adds the service fee, and then
    adds the tax percentage on top:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is a simple implementation, and good enough to give a result. Challenges
    usually appear for this type of code when we need to add more types of discounts,
    modify taxes depending on items, or change the order of discounts. Of course,
    we can apply an imperative or object-oriented style when the time comes, and extract
    multiple functions, one for each operation, that we then combine however we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let’s look at the functional style now. The first thing we can do is to
    use lambdas for every operation, and another lambda for the final computation.
    We implement a few lambdas: one that subtracts the discount from the price, a
    second that applies the service fee, a third that applies the tax, and a final
    one that computes the price by chaining calls to all the previously defined lambdas.
    We end up with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Is this code better? Well, it depends. One factor is familiarity with this
    paradigm, but don’t let that stop you; as I said before, familiarity is often
    confused with simplicity, but the two are not the same. Another factor is to see
    the lambdas as functions and not as data structures. Once you pass these two challenges,
    we notice a few things: the lambdas are very small, they are easy to understand,
    and they are pure functions, which are objectively the simplest types of functions
    out there. We can chain the calls in multiple ways, for example, applying the
    discount at the price with tax, so we have more options with this implementation.
    Still, there’s nothing we couldn’t do with imperative programming until now.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s then take the next step and make this fully functional. We will use the
    lambdas we created, but instead of returning a value, our implementation will
    use partial application and functional composition to return a function that gives
    us the answer we are looking for. Since the preceding lambdas have two parameters,
    we need to bind one of the arguments to the corresponding input before applying
    the functional composition. So, for the **discountPrice** lambda, we bind the
    discount argument to the value passed to the **computePriceFunctional** function
    and we obtain a lambda that takes a single parameter, the initial price, and returns
    the price with a discount. For the **addServiceFee** lambda, we bind the **serviceFee**
    argument to the value passed to the **computePriceFunctional** function and obtain
    a function that takes a single parameter, the price before service, and returns
    the price with the service fee. For the **applyTax** lambda, we bind the **taxPercentage**
    argument to the value passed to the **computePriceFunctional** function and we
    obtain a function that takes a single parameter, the price without tax, and returns
    the price with tax. Once we obtain these functions that take a single parameter,
    we compose them using the **compose** function shown previously, and we obtain
    a function that takes a single argument price and, when called, computes the correct
    final price. Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This style of programming is at first glance very different from what OOP or
    structured programming does. But if you think for a little bit, you will realize
    that an object is just a set of cohesive, partially applied functions. If you
    extract the functions from objects, you need to pass in the data members used
    in the object, a style familiar to those who have ever programmed in C. Including
    a method in an object is therefore equivalent to binding a few of the arguments
    to the object data members that are initialized by the constructor. Therefore,
    OOP and functional programming are not really enemies, just different and equivalent
    ways of expressing the same behavior, with different trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a prelude to the *Metaprogramming* section coming later, let’s look at making
    all these functions available at compile time. We need to do a little bit of magic
    with templates and pass in the value parameters as template arguments, and we
    need to add a lot of **constexpr** , but the following code works equally well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With this, we have seen the fundamental blocks of functional programming in
    C++. Let’s now look at where and why they are useful.
  prefs: []
  type: TYPE_NORMAL
- en: Architectural patterns in functional style
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s first look at how we would implement an application going all the way
    to the functional style. We can’t discuss all the possible design patterns of
    such an application, but we can show a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: We notice first that functional programming places a few constraints upon our
    design. We favor immutability and pure functions. We use data structures, but
    they are immutable, meaning that every change to the data structure gives us a
    new version. Finally, the I/O part needs to be separate and as thin as possible
    since it needs mutations.
  prefs: []
  type: TYPE_NORMAL
- en: A simple design pattern using these constraints is the pipe pattern. Let’s imagine
    we receive a file in XML format, and we call web services with data from it. We
    have an input layer that reads the XML file, an output layer that writes to web
    services, and a layer in the center that uses a functional style. We can now consider
    the input and output data and implement consequent transformations on the input
    that lead to the desired output. Each of these transformations is a pure function
    working on immutable data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Such a process is highly parallelizable because of the lack of mutation. In
    fact, C++17 introduced the **<execution>** header, which allows running the common
    STL algorithms in parallel. Similar patterns are used in data transformation architectures
    such as **Extract, Transform, Load** ( **ETL** ) and in the MapReduce architecture
    made popular by Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: The pattern can be extended beyond data transformation, to the more loosely
    defined **functional core, imperative shell** architecture, aptly named by Gary
    Bernhardt. If you want more specific details, look into the hexagonal architecture
    with a functional core.
  prefs: []
  type: TYPE_NORMAL
- en: This shows not only that we can design programs using a functional paradigm
    in C++ but also that there are situations when this architecture fits. It also
    shows that we can take some parts of this style of programming and use it on pieces
    of our implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Metaprogramming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One thing seems to unite programmers, no matter how different they are otherwise:
    the enjoyment of jokes on recursion. There’s something in the programmer’s mind
    that appreciates a certain type of symmetry. When it comes to programming languages
    and programming paradigms, you’d be hard-pressed to find a more symmetrical type
    of language than one that can understand itself.'
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding programming paradigm is called metaprogramming, and programming
    languages that take this idea to the limit are known as homoiconic, meaning that
    a program can manipulate another program’s representation or its own as data.
    Programming languages that have this property include Lisp and its derived dialects,
    the latest being Clojure.
  prefs: []
  type: TYPE_NORMAL
- en: Metaprogramming is very powerful, but also very difficult to master, and can
    introduce a lot of issues in large projects. Some features connected to metaprogramming
    are available in modern languages, such as instrumentation, reflection, or dynamic
    execution of instructions. But other than using annotations, very little of all
    this is used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: C++ is different, however. One feature of metaprogramming is the ability to
    move computations from runtime to compile time, and C++ has fully embraced it
    with template metaprogramming. In more recent versions of the language, the implementation
    of compile-time computations has been simplified with the introduction of generalized
    constant expressions with **constexpr** and **consteval** .
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical example of this technique is the factorial implementation. A recursive
    factorial implementation computed at runtime looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The same implementation can be done using template metaprogramming. It is perhaps
    a lesser-known characteristic of C++ templates that they can take a value as a
    parameter, not just a type. Moreover, both a generic template, for example, one
    that takes any integer value as a parameter, and a specialization, which takes
    only a specific value, can be provided. In our case, we can implement a factorial
    template that takes an integer and a specialization for the value **0** , resulting
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation achieves the same goal as the previous one, with the exception
    that a call to **Factorial<25>** , for example, will be computed at compile time
    rather than runtime. Starting with C++11 and generalized constant expressions,
    we can avoid templates altogether and instead use **constexpr** and **consteval**
    to tell the compiler which values are to be computed at compile time. Here’s a
    simplified implementation of the same code with a constant expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: These metaprogramming techniques available to C++ programmers allow more flexibility
    in decisions related to what happens at compile time versus runtime. They offer
    a trade-off of CPU cycles versus the executable size. If you have a lot of memory
    available but the computations need to happen extremely fast, caching results
    in the executable can be the way to go, and **constexpr** and **consteval** become
    your friends.
  prefs: []
  type: TYPE_NORMAL
- en: But the possibilities don’t stop here. We can create in C++ programs that are
    demonstrably valid from compilation. We just need to take strong types to their
    limit.
  prefs: []
  type: TYPE_NORMAL
- en: Strong types to the limit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest challenges in software development is avoiding bugs. This
    is such a pervasive problem that we have taken to naming it something that suggests
    something bad has happened to our code. In fact, however, we should be calling
    bugs *mistakes* , because that is what they are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have compilers, why can’t we place enough restrictions on the code
    so that they tell us when there’s a bug? We might be able to do just that, only
    not for free. We discussed template metaprogramming in the previous section, but
    we have left out one important characteristic: template metaprogramming is Turing
    complete. This means that for any program that we can write in the normal way,
    we can also write it using template metaprogramming.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This idea is very powerful, and it has been discussed in various contexts over
    time. If you want to try a programming language built entirely around this notion,
    try Idris ( [https://www.idris-lang.org/](https://www.idris-lang.org/) ). Many
    programmers might be familiar with the support available in Haskell for validation
    at compilation time. But my first encounter with this idea was Andrei Alexandrescu’s
    seminal book *Modern C++ Design: Generic Programming and Design Patterns Applied*
    , published in 2001.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a simple problem. One of the common sources for bugs and code
    smell is the so-called **primitive obsession** , that is, the obsession to use
    primitive types to represent complex data. A typical example of primitive obsession
    is to represent length, money, temperature, or weight as a number, by completely
    ignoring their units of measure. Rather than doing this, a specific type for money
    would use a value that allows for a specific precision depending on the context,
    such as seven decimals for accounting and banks, and the currency. This is often
    useful in software development even when the program deals with a single currency
    because one thing you can bet on when it comes to features is that eventually,
    one thing will become more – there will be a time when your client will ask you
    to add a second currency.
  prefs: []
  type: TYPE_NORMAL
- en: A typical challenge relating to primitive obsession is with constraining primitive
    types. For example, consider a type that can store the hour of the day. Not only
    is this value an unsigned int, but it can only be from 0 to 23, assuming a 24-hour
    format for simplicity. It would be great to be able to tell the compiler that
    no value outside of the 0-23 range is ever accepted as an hour and to give a relevant
    error when passing, for example, a value of 27.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, an enum can be a solution, since the number of values is small.
    But we’ll ignore this option and consider first how we would implement this at
    runtime. We can imagine a class called **Hour** that throws an exception if the
    value passed in the constructor is not between 0 and 23:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'What if we want to move the check at compile time? Well, time to use the power
    of **constexpr** to tell the compiler what values are defined at compile time,
    and **static_assert** to verify the range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding implementation, the following code works perfectly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'But if we try to pass a value outside the range, we get a compilation error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This error tells us that we can’t have an hour with the value 30, which is precisely
    what we needed!
  prefs: []
  type: TYPE_NORMAL
- en: This is just one technique in the toolbox of C++ programmers who want to create
    programs that are provably valid at compile time. As we mentioned, template metaprogramming
    is Turing complete, which means we can theoretically implement any program at
    compile time that we can implement at runtime. As always, there are trade-offs.
    Notice how the **Hour** value must be **constexpr** , which means the value will
    be stored in the executable. This is by design, since the only way to constrain
    the types to the maximum is to compile them into the unit.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, I noticed that this technique can easily lead to code that is extremely
    difficult to understand and modify. Making changes to this code requires a strong
    discipline, since modifying existing code can still introduce bugs that we otherwise
    have weeded out through our strong types. The fundamental technique is always
    to add, never to modify, unless to fix issues. We have kept this code clean until
    now, but types can get very abstract very quickly, which makes reconstructing
    the reasoning that led to them very difficult after, say, six months. On the upside,
    this technique works best when creating libraries focused on a very specific domain.
  prefs: []
  type: TYPE_NORMAL
- en: While I find this technique intriguing, I tend to prefer more freedom when I
    program. I use my own disciplines – test-driven development, merciless refactoring,
    extreme attention to names, and simple design – when I code. I’d rather have a
    way to write the code that I want and let the compiler figure out the details,
    which is why the last paradigm I’m going to discuss ignores types as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: What about ignoring types?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A few years ago, I led a team that built a few web applications in a language
    called Groovy with a framework named Grails. Groovy is an optionally typed and
    dynamic language, meaning that it assigns types at runtime, but you can provide
    type hints for the compiler. It can also be compiled statically, and since it’s
    built on JVM, the code ends up in a Java unit.
  prefs: []
  type: TYPE_NORMAL
- en: I had noticed in previous web projects that types were useful at the edges of
    the system, for checking request parameters, interacting with databases, and other
    I/O operations. But types in the core of a web application tended to make things
    more difficult. We often had to change code or write extra code to accommodate
    new ways of using the already-implemented behaviors, since users of web apps often
    notice a scenario that is useful and want it to work in other contexts or for
    other types of data. So, I decided from the very beginning that we would use types
    for request validation, to ensure security and correctness, and for the interaction
    with external systems, to ensure simplicity. But we did not use types in the core.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plan was always to use a sound strategy for automated testing so that all
    the code was proven valid through tests. I expected that the lack of types would
    make us write more tests, but I was in for a big surprise: the number of tests
    was relatively the same as before, but we had less code. Also, the code we wrote,
    because there were no types involved, pushed us to name things very carefully
    since names were the only hints we had as programmers as to what a function or
    a variable was doing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is, to this day, my favorite style of programming. I want to write the
    code as I want, and as expressive as I can, and let the compiler work out the
    types. You can think about this approach as extreme polymorphism: if you pass
    a variable of a type that has the required methods, the code should work irrespective
    of the type you pass in. It is not a style I would recommend for everyone, because
    it’s not obvious if it works solely in combination with a specific design experience,
    but it is a style that you can experiment with. However, the first hurdle is to
    let go of controlling what the compiler does, a feat more difficult to achieve
    for C++ programmers who are very detailed-oriented.'
  prefs: []
  type: TYPE_NORMAL
- en: How would this work in C++? Well, fortunately for me, the **auto** keyword was
    introduced in C++ since C++11, and its features were improved little by little
    in consequent standards. On the downside, C++ is not as permissive as Groovy on
    dynamic typing, so occasionally I need templates.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let me amaze you with the most polymorphic function you can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This function works no matter what we pass into it. Isn’t that neat? Imagine
    that you have a bunch of functions like this that you can use in the core of your
    system, without needing to change them. That sounds like an ideal programming
    environment to me. Alas, life is more complicated than this, and programs need
    more than identity functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a slightly more complicated example. We’ll start by checking
    whether a string is a palindrome, that is, whether it reads the same both forward
    and reversed. A simple implementation in C++ is to take the string, reverse it
    by using **std::reverse_copy** , and then compare the initial string with its
    reverse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'What if we were to make this code less interested in types? First, we would
    change the parameter type to **auto** . Then, we need a way to reverse it without
    constraining ourselves to a string input. Fortunately, the **ranges** library
    has a **reverse_view** that we can use. Finally, we need to compare the initial
    value with the reversed one, again without restraining the type too much. C++
    provides us with **std::equal** . So, we end up with the following code, which
    we can use not only for strings but also for a **vector<string>** that represents
    a phrase, or with tokens defined in an enum. Let’s see the extreme polymorphism
    in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Perhaps I have now shown you why I find this style of programming very appealing.
    If we ignore the types, or make our functions extremely polymorphic, we can write
    code that applies to future situations without needing to change. The trade-off
    is that the code has its constraints in the deducted types and that the names
    of the parameters and functions matter a lot. For example, if I pass in an integer
    value to **isPalindrome** , I will get a complicated error instead of the simple
    one telling me that the parameter is of the incorrect type. This is the beginning
    of the g++ compiler output on my computer when trying to pass in an integer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s now up to you: do you prefer strong types or extreme polymorphic behavior?
    Both have their trade-offs and their own application domains.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen in this chapter that we can use multiple paradigms to program
    in C++. We looked briefly at a few: functional programming, metaprogramming, types
    that ensure compile-time validation, and extreme polymorphism. Each of these approaches,
    as well as the standard object-oriented and structured programming, are useful
    for various contexts when building libraries or specific programs. Each of them
    has something to offer to the curious programmer who wants to learn as much as
    possible about their craft. Each of them has its trade-offs and its own implementations
    in the world of software development.'
  prefs: []
  type: TYPE_NORMAL
- en: We have shown that C++ programmers perhaps only use a subset of the language,
    and it doesn’t have to be an object-oriented one. Instead, it’s best to experiment
    with all of them, making the most of the fact that C++ is powerful enough to offer
    so many options, and to pick and choose depending on the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see that the **main()** function might not actually
    be the entry point of our applications.
  prefs: []
  type: TYPE_NORMAL
