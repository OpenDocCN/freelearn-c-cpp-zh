<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-129"><a id="_idTextAnchor129"/>9</h1>
<h1 id="_idParaDest-130"><a id="_idTextAnchor130"/>Understanding the C++ Memory Model</h1>
<p>This chapter is a continuation of the discussion from <a href="B20833_07.xhtml#_idTextAnchor101"><em class="italic">Chapter 7</em></a>, where we discussed a few multiprocess and multi-threaded techniques; this chapter will enhance their usage. We will guide you through various techniques while narrowing down to the main focus of the chapter – the C++ memory model. But in order to discuss this, you will start first with a brief examination of memory robustness through the smart pointer and the optional objects. We will use them later to implement <em class="italic">lazy initialization</em> and handle <em class="italic">shared memory</em> regions safely. An improved memory access analysis of <em class="italic">cache-friendly</em> code follows. You will learn when and why using multi-threaded execution could be a trap, even though you did everything right in the software design.</p>
<p>This chapter gives you the opportunity to broaden your understanding of the synchronization primitives. While learning about the <em class="italic">condition variables</em>, you will also understand the benefits of the <em class="italic">read-write locks</em>. We will use the <em class="italic">ranges</em> from C++20 to visualize the same shared data differently. Combining these mechanisms one by one, we will finalize our analysis with the biggest topic – instruction ordering. Through the C++ <em class="italic">memory order</em>, you will learn more about the significance of the correct atomic routine setup. The <em class="italic">spinlock</em> implementation will be used to summarize all techniques at the end.</p>
<p>In this chapter, we are going to cover the following main topics:</p>
<ul>
<li>Getting to know smart pointers and optionals in C++</li>
<li>Learning about condition variables, read-write locks, and ranges in C++</li>
<li>Discussing multiprocessor systems – cache locality and cache friendliness in C++</li>
<li>Revisiting shared resources through the C++ memory model via the spinlock implementation</li>
</ul>
<h1 id="_idParaDest-131"><a id="_idTextAnchor131"/>Technical requirements</h1>
<p>In order to run the code examples, the reader must prepare the following:</p>
<ul>
<li>A Linux-based system capable of compiling and executing C++20 (for example, <strong class="bold">Linux </strong><strong class="bold">Mint 21</strong>)</li>
<li>The GCC12.2 compiler: <a href="https://gcc.gnu.org/git/gcc.gitgcc-source">https://gcc.gnu.org/git/gcc.git gcc-source</a><ul><li>With the <code>-std=c++2a</code>, <code>-lpthread</code>, and <code>-</code><code>lrt</code> flags</li></ul></li>
<li>For all the examples, you can alternatively use <a href="https://godbolt.org/">https://godbolt.org/</a>.</li>
<li>All code examples in this chapter are available for download from <a href="https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209">https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209</a>.</li>
</ul>
<h1 id="_idParaDest-132"><a id="_idTextAnchor132"/>Getting to know smart pointers and optionals in C++</h1>
<p>In <a href="B20833_04.xhtml#_idTextAnchor060"><em class="italic">Chapter 4</em></a>, we revisited the C++ fundamentals in order to be on the same page when it comes to the language. One instrument that is also considered a <em class="italic">must </em>is smart pointers. Through<a id="_idIndexMarker791"/> these, we are able to improve the safety of the program and also manage our resources more effectively. And as discussed in the earlier chapters, this is one of our main goals as system programmers. Remember the <strong class="bold">RAII</strong> principle? Smart pointers are based on this, helping the C++ developer reduce and even eliminate <em class="italic">memory leaks</em>. They could also help with shared memory management as you will see later in the chapter.</p>
<p><em class="italic">Memory leaks</em> appear<a id="_idIndexMarker792"/> when we allocate memory but fail to free it. This could happen not only because we forgot to call the object’s destructor, but also when we lose the pointer to that memory address. In addition to these, there are also the <em class="italic">wild</em> and <em class="italic">dangling pointers</em> to consider as well. The first one happens when the pointer is there on the <em class="italic">stack</em>, but it’s never associated with the real object (or address). The second one happens when we free the memory, used by the object, but the value of the pointer remains <em class="italic">dangling</em> around, and we reference an already-deleted object. Altogether, these errors can lead not only<a id="_idIndexMarker793"/> to <strong class="bold">memory fragmentation</strong>, but also<a id="_idIndexMarker794"/> to <strong class="bold">buffer </strong><strong class="bold">overflow</strong> vulnerabilities.</p>
<p>These issues are hard to catch and reproduce, especially on large systems. System programmers and software integration engineers use tools such as address sanitizers, static and dynamic <a id="_idIndexMarker795"/>code analyzers, and profilers, among others, relying on them to predict future defects. But such tools are expensive and consume a lot of computational power, so we cannot rely on them constantly for higher code quality. That said, what can we do, then? The answer is simple – use smart pointers.</p>
<p class="callout-heading">Note</p>
<p class="callout">You can read more on the subject of smart pointers in the standard, or refer to <a href="https://en.cppreference.com/w/cpp/memory">https://en.cppreference.com/w/cpp/memory</a>.</p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor133"/>Retracing RAII via smart pointers</h2>
<p>Even <a id="_idIndexMarker796"/>experienced C++ developers make mistakes when it comes to the right time for memory deallocation. Other languages use garbage collection techniques to handle memory management, but it’s important to mention that memory leaks happen there as well. Multiple algorithms are implemented for detecting such cases in the code but are not always successful. For example, the cycle <a id="_idIndexMarker797"/>dependency between objects is sometimes difficult to resolve – should two objects pointing to each other be deleted, or should they remain allocated? If they remain allocated, does this constitute a leak or not? So, it’s our job to be cautious about memory usage. In addition, garbage collectors work to free up memory, but do not manage opened files, network connections, locks, and so on. To this end, C++ implements its own instrument for control – wrapper classes over the pointers, helping us free the memory at the right time, usually when the object goes out of scope (the object life cycle was discussed already in <a href="B20833_04.xhtml#_idTextAnchor060"><em class="italic">Chapter 4</em></a>). Smart pointers are efficient in terms of memory and performance, meaning they don’t cost (much) more than raw pointers. At the same time, they give us robustness in memory management. There are three types of smart pointers in the C++ standard:</p>
<ul>
<li><code>unique_ptr</code>: This<a id="_idIndexMarker798"/> is a <a id="_idIndexMarker799"/>pointer that is allowed one owner only. It cannot be copied or shared, but the ownership can be transferred. It has the size of a single raw pointer. It is destroyed and the object deallocated when it goes out of the scope.</li>
<li><code>shared_ptr</code>: This <a id="_idIndexMarker800"/>can have multiple owners and is <a id="_idIndexMarker801"/>destroyed when all owners have given up ownership on it or all go out of scope. It uses a reference counter to the pointer of an object. Its size is two raw pointers – one for the allocated object, and one for the shared control block containing the reference count.</li>
<li><code>weak_ptr</code>: This <a id="_idIndexMarker802"/>provides access to an object owned by one or<a id="_idIndexMarker803"/> more shared pointers, but doesn’t count references. It is used for observing an object, but not for managing its life cycle. It consists of two pointers – one for the control block, and one for pointing to the shared pointer it was constructed from. Through <code>weak_ptr</code> you can learn whether the underlying <code>shared_ptr</code> is still valid – just call the <code>expired()</code> method.</li>
</ul>
<p>Let’s<a id="_idIndexMarker804"/> demonstrate their initial roles through the following <a id="_idIndexMarker805"/>example:</p>
<pre class="source-code">
struct Book {
   string_view title;
   Book(string_view p_title) : title(p_title) {
        cout &lt;&lt; "Constructor for: " &lt;&lt; title &lt;&lt; endl; }
   ~Book() {cout &lt;&lt; "Destructor for: " &lt;&lt; title &lt;&lt; endl;}};
int main() {
    unique_ptr&lt;Book&gt; book1 =
        make_unique&lt;Book&gt;("Jaws");
    unique_ptr&lt;Book&gt; book1_new;
    book1_new = move(book1); // {1}
    cout &lt;&lt; book1_new-&gt;title &lt;&lt; endl;
    shared_ptr&lt;Book&gt; book2 =
        make_unique&lt;Book&gt;("Dune");
    shared_ptr&lt;Book&gt; book2_new;
    book2_new = book2; // {2}
    cout &lt;&lt; book2-&gt;title &lt;&lt;" "&lt;&lt; book2_new-&gt;title &lt;&lt; endl;
    cout &lt;&lt; book2.use_count() &lt;&lt; endl;</pre> <p>As you can <a id="_idIndexMarker806"/>see, we use the heap as we call <code>new</code> for the <a id="_idIndexMarker807"/>creation of the <code>Book</code> objects. But as the smart pointer handles memory management, we don’t need to call the destructor explicitly:</p>
<pre class="console">
Constructor for: Jaws
Jaws
Constructor for: Dune
Dune Dune
2
Destructor for: Dune
Destructor for: Jaws</pre> <p>First, we move the ownership of <code>book1</code>’s object to another <code>unique_ptr</code> – <code>book1_new</code> (marker <code>{1}</code>). We print out its <code>title</code> through the second <code>unique_ptr</code> as the first one is already invalid. We do the same operation for another <code>Book</code> object, but through a <code>shared_ptr</code> object (marker <code>{2}</code>). This time the <code>title</code> variable can be accessed from both pointers. We also print the reference count, and we see there are two references to that object.</p>
<p><code>weak_ptr</code> has useful strengths in system programming, too. You can use <code>weak_ptr</code> to check for pointer validity. <code>weak_ptr</code> could also resolve the issue of cyclic dependency between objects. Let’s consider an example of a list node of a doubly linked list. The next example illustrates the benefits of <code>weak_ptr</code>. This is a good time to advise you not to implement such data structures yourself, especially when they are already a part of the C++ standard. </p>
<p>Now, let’s use the <code>Book</code> object as content of the <code>ListNode</code> <code>struct</code>:</p>
<pre class="source-code">
struct ListNode {
    Book data;
    ListNode(string_view p_title) {
        data.title = p_title;
        cout &lt;&lt; "Node created: " &lt;&lt; data.title &lt;&lt; endl;
    }</pre> <p>We also <a id="_idIndexMarker808"/>add two member variables for the previous and following nodes, but one of them will be <code>weak_ptr</code>. One remark is that the <code>weak_ptr</code> reference<a id="_idIndexMarker809"/> is not counted as such in the <code>shared_ptr</code> control block. Now, we have both access to the objects and the opportunity to count the references to zero with each deallocation:</p>
<pre class="source-code">
    ~ListNode() {
        cout &lt;&lt; "Node destroyed: " &lt;&lt; data.title
             &lt;&lt; endl;
    }
    shared_ptr&lt;ListNode&gt; next;
    weak_ptr&lt;ListNode&gt; prev;
};
int main() {
    shared_ptr&lt;ListNode&gt; head =
        make_shared&lt;ListNode&gt;("Dune");
    head-&gt;next = make_shared&lt;ListNode&gt;("Jaws");
    if (!head-&gt;next-&gt;prev.expired())
        head-&gt;next-&gt;prev = head;</pre> <p>From the output, it’s clear that all objects were removed successfully:</p>
<pre class="console">
Node created: Dune
Node created: Jaws
Node destroyed: Dune
Node destroyed: Jaws</pre> <p><code>weak_ptr</code> is also useful for cache implementation. Think about it – if you lose all references to an object, you will lose the object itsel; but with smart pointers, it will certainly be destroyed. So, imagine that recently accessed objects or objects with higher importance are kept through <code>shared_ptr</code> in the current code scope. But <code>weak_ptr</code> allows us to keep a reference to an object in the same scope if we need to reference the object later <a id="_idIndexMarker810"/>in that same scope. We would create a <code>weak_ptr</code> object to it in this case. But imagine that meanwhile, some other code scope holds a reference to the object through <code>shared_ptr</code>, thus keeping it allocated. In other words, we <a id="_idIndexMarker811"/>know about the object, but we don’t need to be concerned about its management. Thus, that object is accessible if it’s still required later, but removed when nothing else needs it. The following diagram shows how <code>shared_ptr</code> could be incorrectly used on the left-hand side, along with the implementation just described on the right-hand side:</p>
<div><div><img alt="Figure 9.1 – Cyclic dependency through shared_ptr and resolving through weak_ptr" height="502" src="img/Figure_9.1_B20833.jpg" width="1285"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Cyclic dependency through shared_ptr and resolving through weak_ptr</p>
<p>We are not <a id="_idIndexMarker812"/>going to dive further into other design solutions where smart pointers could come in handy in this section, but we will return to them in the realm of system programming later in the chapter. In the next section, we discuss a technique that’s the opposite to <code>weak_ptr</code>, where we retain the awareness of an object that hasn’t been created in memory yet.</p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor134"/>Doing a lazy initialization in C++</h2>
<p>Do<a id="_idIndexMarker813"/> you play video games? Have you ever seen a missing texture somewhere in the graphics while playing? Has a graphical resource appeared suddenly when you moved close to it with your character? Have you observed such behavior in other UIs as well? If your answers are mostly in the positive, then you have probably encountered <strong class="bold">lazy initialization</strong> already. It’s easy to figure out that its purpose is to postpone the construction of an object until it’s really needed. By doing so, we allow the system to allocate the required resources only. We also use it to speed up our code, especially if it’s run during high CPU loads, such as at system startup. Instead of wasting CPU cycles to create large objects that won’t be needed until (much) later, we free up the CPU to handle other requests. On the negative side, we might end up failing to load the object on time, as you have likely observed in video games. As we discussed in <a href="B20833_02.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a>, this is also used when a program is loaded, and the kernel allocates virtual memory in a lazy fashion – a page of executable code is not loaded until referenced.</p>
<p>As with every other pattern, <strong class="bold">lazy initialization</strong> cannot solve all of the problems. So, the system programmer has to choose whether it should be applied for the given application’s functions or not. Usually, it is preferred that parts of the graphical and network storage resources remain lazily initialized as they are loaded on demand either way. In other words, the user doesn’t see the UI in its entirety all the time. Therefore, it’s not required to store it in memory a priori. C++ has features that allow us to easily implement this approach. We present <strong class="bold">lazy initialization</strong> in the following example:</p>
<pre class="source-code">
#include &lt;iostream&gt;
#include &lt;chrono&gt;
#include &lt;optional&gt;
#include &lt;string_view&gt;
#include &lt;thread&gt;
using namespace std;
using namespace std::literals::chrono_literals;
struct Settings {
    Settings(string_view fileName) {
        cout &lt;&lt; "Loading settings: " &lt;&lt; fileName &lt;&lt; endl;
    }
    ~Settings() {
        cout &lt;&lt; "Removing settings" &lt;&lt; endl;
    }</pre> <p>We <a id="_idIndexMarker814"/>propose a <code>Settings</code> class that will help us simulate the loading and updating of a list of settings from the disk. Note that we pass it by value and not by reference:</p>
<pre class="source-code">
    void setSetting(string_view setting,
                    string_view value) {
        cout &lt;&lt; "Set setting: " &lt;&lt; setting
             &lt;&lt; " to: " &lt;&lt; value &lt;&lt; endl;
    }
};</pre> <p>This technique saves time due to reduced loading from memory. In C++, pass-by-value (or pass-by-copy) is the default argument passing technique, except for in the case of arrays. It is cheap and optimal for small types, such as <code>int</code>. Pass-by-reference is an alternative to pass-by-value and the <code>string_view</code> object is handled in the same manner as <code>int</code>, using a cheaper copy constructor than other standard objects such as <code>string</code>. Getting back to our example, we’re creating a configuration object, <code>Config</code>, which will consist of the settings file (which could be more than one file in real-world scenarios) and will allow settings to be changed in that configuration. Our <code>main()</code> method simulates an application’s startup. The <code>Config</code> object will be constructed, but the settings file will be loaded only when the startup is finished, and the process resources are available:</p>
<pre class="source-code">
struct Config {
    optional&lt;Settings&gt; settings{};
    Config() {
        cout &lt;&lt; "Config loaded..." &lt;&lt; endl;
    }
    void changeSetting(string_view setting,
                       string_view value) {
        if (!settings)
            settings.emplace("settings.cfg");
        settings-&gt;setSetting(setting, value);
    }
};
int main() {
    Config cfg;
    cout &lt;&lt; "Application startup..." &lt;&lt; endl;
    this_thread::sleep_for(10s);
    cfg.changeSetting("Drive mode", "Sport");
    cfg.changeSetting("Gear label", "PRNDL");</pre> <p>We <a id="_idIndexMarker815"/>observe that the file is loaded after the startup has finished, as we expected:</p>
<pre class="console">
Config loaded...
Application startup...
Loading settings: settings.cfg
Set setting: Drive mode to: Sport
Set setting: Gear label to: PRNDL
Removing settings</pre> <p>The <code>optional</code> class template is designed so that functions can return <em class="italic">nothing</em> when they fail, or a valid result when they succeed. We could also use it to handle objects whose construction is expensive. It also manages a value that may or may not be present at a given time. It is also readable, and its intent is clear. If an <code>optional</code> object contains a value, the value is guaranteed to be allocated as part of the <code>optional</code> object, and no dynamic memory allocation happens. Thus, an <code>optional</code> object models a <em class="italic">reservation</em> to an object, not a pointer. This is a key difference between <code>optional</code> and the smart pointer. Although using a smart pointer to handle large and complex objects <a id="_idIndexMarker816"/>might be a better idea, <code>optional</code> gives you the opportunity to construct an object at a later point in time when all parameters are known, if they weren’t known earlier in the execution. Both of them will work well in implementing <strong class="bold">lazy initialization</strong> – it’s a matter of your preference.</p>
<p>Later in the chapter, we will return to smart pointers and their usability for managing shared memory. First, though, we will use the next section to present some useful mechanisms for synchronization.</p>
<h1 id="_idParaDest-135"><a id="_idTextAnchor135"/>Learning about condition variables, read-write locks, and ranges in C++</h1>
<p>Let’s now start our discussion of synchronization primitives, a fundamental one of which is the <strong class="bold">condition variable</strong>. Its <a id="_idIndexMarker817"/>purpose is to allow multiple threads to remain blocked until an event occurs (i.e., a condition is satisfied). The implementation of <strong class="bold">condition variables</strong> requires an additional Boolean variable to indicate <a id="_idIndexMarker818"/>whether the condition is met or not, a <em class="italic">mutex</em> to serialize the access to the Boolean variable, and the condition variable itself.</p>
<p>POSIX provides an interface for multiple use cases. Do you remember the producer-consumer example in <a href="B20833_07.xhtml#_idTextAnchor101"><em class="italic">Chapter 7</em></a><em class="italic">, Using Shared Memory</em>? So, <code>pthread_cond_timedwait()</code> is used to block a thread for a given period of time. Or simply wait for a condition through <code>pthread_cond_wait ()</code> and signal with <code>pthread_cond_signal()</code> to one thread, or <code>pthread_cond_broadcast()</code> to all threads. Typically, the condition is checked periodically in the scope of a mutex lock:</p>
<pre class="source-code">
...
pthread_cond_t  condition_variable;
pthread_mutex_t condition_lock;
...
pthread_cond_init(&amp;condition_variable, NULL);
...
void consume() {
    pthread_mutex_lock(&amp;condition_lock);
    while (shared_res == 0)
        pthread_cond_wait(&amp;condition_variable,
                          &amp;condition_lock);
    // Consume from shared_res;
    pthread_mutex_unlock(&amp;condition_lock);
}
void produce() {
    pthread_mutex_lock(&amp;condition_lock);
    if (shared_res == 0)
        pthread_cond_signal(&amp;condition_variable);
    // Produce for shared_res;
    pthread_mutex_unlock(&amp;condition_lock);
}
pthread_mutex_unlock(&amp;condition_lock);
...
pthread_cond_destroy(&amp;condition_variable);
...</pre> <p>If we level <a id="_idIndexMarker819"/>up the abstraction, as we did in <a href="B20833_07.xhtml#_idTextAnchor101"><em class="italic">Chapter 7</em></a>, C++ gives us access to the same technique, but a bit simpler and safer to use – we are guarded by the RAII principle. Let’s check the following snippet in C++:</p>
<pre class="source-code">
...
#include &lt;condition_variable&gt;
mutex cv_mutex;
condition_variable cond_var;
...
void waiting() {
    cout &lt;&lt; "Waiting for work..." &lt;&lt; endl;
    unique_lock&lt;mutex&gt; lock(cv_mutex);
    cond_var.wait(lock);
    processing();
    cout &lt;&lt; "Work done." &lt;&lt; endl;
}
void done() {
    cout &lt;&lt; "Shared resource ready."  &lt;&lt; endl;
    cond_var.notify_one();
}
int main () {
    jthread t1(waiting); jthread t2(done);
    t1.join(); t2.join();
    return 0;
}</pre> <p>The output is as follows:</p>
<pre class="console">
Waiting for work...
Shared resource ready.
Processing shared resource.
Work done.</pre> <p>In this <a id="_idIndexMarker820"/>form, the code is not correct. There is no condition to be checked, and the shared resource itself is missing. We are simply setting the stage for the following examples, which are a continuation of what we covered in <a href="B20833_07.xhtml#_idTextAnchor101"><em class="italic">Chapter 7</em></a>. But observe the use of a <code>{4}</code>), while the first one was waiting (marker <code>{2}</code>). As you see, we rely on a <em class="italic">mutex</em> to lock the shared resource in the scope (marker <code>{1}</code>) and<a id="_idIndexMarker821"/> the condition variable is triggered through it in order to continue to work (markers <code>{2}</code> and <code>{3}</code>). Thus, the CPU is not busy waiting, as there’s no endless loop to wait for a condition, freeing up access to the CPU for other processes and threads. But the thread remains blocked, because the <code>wait()</code> method of the <strong class="bold">condition variable</strong> unlocks <a id="_idIndexMarker822"/>the <strong class="bold">mutex</strong> and the thread is put to sleep atomically. When the thread is signaled, it will be resumed and will re-acquire the <strong class="bold">mutex</strong>. This is not always useful as you will see in the next section.</p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor136"/>Cooperative cancellation through condition variables</h2>
<p>An <a id="_idIndexMarker823"/>important remark is that the condition variable should wait only with a condition and through a predicate. If not, the thread waiting on it will remain blocked. Do you remember the thread cancellation example from <a href="B20833_06.xhtml#_idTextAnchor086"><em class="italic">Chapter 6</em></a>? We used <code>jthread</code> and sent <em class="italic">stop notifications</em> between threads through the <code>stop_token</code> class and the <code>stop_requested</code> method. This mechanism is known <a id="_idIndexMarker824"/>as <code>jthread</code> technique is considered safe and easy to apply, but it might not be an option for your software design, or it might not be enough. Canceling threads could be directly related to waiting for an event. In that case, <strong class="bold">condition variables</strong> could come in handy as no endless loops or polling will be required. Revisiting the thread cancellation example from <a href="B20833_06.xhtml#_idTextAnchor086"><em class="italic">Chapter 6</em></a><em class="italic">, Canceling Threads, Is This Really Possible?</em>, we have the following:</p>
<pre class="source-code">
while (!token.stop_requested())</pre> <p>We are doing polling as the thread worker checks periodically whether the cancellation has been sent while doing something else in the meantime. But if the cancellation is the only thing we care about, then instead of polling, we could simply <em class="italic">subscribe</em> to the cancellation event using the <code>stop_requested</code> function. C++20 allows us to define a <code>stop_callback</code> function, so together with the condition variable and <code>get_stop_token()</code>, we can do the cooperative cancellation without endless loops:</p>
<pre class="source-code">
#include &lt;condition_variable&gt;
#include &lt;iostream&gt;
#include &lt;mutex&gt;
#include &lt;thread&gt;
#include &lt;syncstream&gt;
using namespace std;
int main() {
    osyncstream{cout} &lt;&lt; "Main thread id: "
                      &lt;&lt; this_thread::get_id()
                      &lt;&lt; endl;</pre> <p>So, let’s finish<a id="_idIndexMarker825"/> the work from the example in the previous section and add a predicate to the <strong class="bold">condition variable</strong> in a worker thread:</p>
<pre class="source-code">
    jthread worker{[](stop_token token) {
        mutex mutex;
        unique_lock lock(mutex);
        condition_variable_any().wait(lock, token,
            [&amp;token] { return token.stop_requested(); });
        osyncstream{cout} &lt;&lt; "Thread with id "
                          &lt;&lt; this_thread::get_id()
                          &lt;&lt; " is currently working."
                          &lt;&lt; endl;
    }};
    stop_callback callback(worker.get_stop_token(), [] {
    osyncstream{cout} &lt;&lt;"Stop callback executed by thread:"
                      &lt;&lt; this_thread::get_id()
                      &lt;&lt; endl;
    });
    auto stopper_func = [&amp;worker] {
        if (worker.request_stop())
            osyncstream{cout} &lt;&lt; "Stop request executed by
              thread: "
                              &lt;&lt; this_thread::get_id()
                              &lt;&lt; endl;
    };
    jthread stopper(stopper_func);
    stopper.join(); }</pre> <p>The <a id="_idIndexMarker826"/>output is as follows:</p>
<pre class="console">
Main thread id: 140323902175040
Stop callback executed by thread: 140323893778176
Stop request executed by thread: 140323893778176
Thread with id 140323902170880 is currently working.</pre> <p>So, the worker thread remains in execution, but the <code>stopper</code> thread gets the stop token in the <code>stop_callback</code> function. When the stop is requested through the stopper function, the <strong class="bold">condition variable</strong> is signaled through the token.</p>
<p>Now that we have another mechanism besides the <strong class="bold">semaphore</strong> to signal between threads, we can get the <strong class="bold">shared memory</strong> back in the game. Let’s see how this can work together with the condition variables and smart pointers.</p>
<h2 id="_idParaDest-137"><a id="_idTextAnchor137"/>Combining smart pointers, condition variables, and shared memory</h2>
<p>We<a id="_idIndexMarker827"/> already explored the concept of <strong class="bold">shared memory</strong> in <a href="B20833_07.xhtml#_idTextAnchor101"><em class="italic">Chapter 7</em></a><em class="italic">, Using Shared Memory</em>. Let’s <a id="_idIndexMarker828"/>use the knowledge from the earlier sections in this chapter to enhance the code safety through some C++ techniques. We’re simplifying the scenario a little bit. The full example can be found at <a href="https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209">https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209</a>. </p>
<p>We use the <code>unique_ptr</code> argument to provide a specific deallocator:</p>
<pre class="source-code">
template&lt;typename T&gt;
struct mmap_deallocator {
    size_t m_size;
    mmap_deallocator(size_t size) : m_size{size} {}
    void operator()(T *ptr) const {
       munmap(ptr, m_size);
    }
};</pre> <p>We rely on the following:</p>
<pre class="source-code">
unique_ptr&lt;T, mmap_deallocator&lt;T&gt;&gt;(obj, del);</pre> <p>As you <a id="_idIndexMarker829"/>see, we are also using templates in order to provide the possibility of storing any type of objects in the <strong class="bold">shared memory</strong>. It is easy to keep complex objects with large hierarchies and members in the heap, but storing and accessing their data is not trivial. Multiple <a id="_idIndexMarker830"/>processes will have access to those objects in the <strong class="bold">shared memory</strong>, but are the processes able to reference the memory behind the pointers? If the referenced memory is not in there or the shared virtual address space, then a memory access violation exception will be thrown. So, approach this with caution.</p>
<p>We proceed with the next example. The already-known condition variable technique is used, but this time we add a real predicate to wait for:</p>
<pre class="source-code">
mutex cv_mutex;
condition_variable cond_var;
bool work_done = false;</pre> <p>Our <code>producer()</code> method creates and maps the <code>{1}</code>). This technique is known as <code>new</code> operator does these two operations together. Additionally, the object itself is wrapped by a <code>unique_ptr</code> object with the respective deallocator. As soon as the scope is left, that portion of the memory will be reset through the <code>munmap()</code> method. A <strong class="bold">condition variable</strong> is used to signal to the consumer that the data has been prepared:</p>
<pre class="source-code">
template&lt;typename T, typename N&gt;
auto producer(T buffer, N size) {
    unique_lock&lt;mutex&gt; lock(cv_mutex);
    cond_var.wait(lock, [] { return work_done == false; });
    if (int fd =
            shm_open(SHM_ID, O_CREAT | O_RDWR, 0644);
                     fd != -1) {
        ftruncate(fd, size);</pre> <p>The <code>shm</code> region <a id="_idIndexMarker831"/>is created <a id="_idIndexMarker832"/>and sized. Now, let us use it to store the data:</p>
<pre class="source-code">
        if (auto ptr =
                mmap(0, size,
                     PROT_RW, MAP_SHARED,
                     fd, 0); ptr != MAP_FAILED) {
            auto obj = new (ptr) T(buffer);
            auto del = mmap_deallocator&lt;T&gt;(size);
            work_done = true;
            lock.unlock();
            cond_var.notify_one();
            return unique_ptr&lt;T,
                mmap_deallocator&lt;T&gt;&gt;(obj, del);
        }
        else {
          const auto ecode{ make_error_code(errc{errno}) };
…
        }
    }
    else {
        const auto ecode{ make_error_code(errc{errno}) };
...
        throw exception;
    }
    // Some shm function failed.
    throw bad_alloc();
}</pre> <p>The<a id="_idIndexMarker833"/> consumer is implemented similarly, just waiting for the following:</p>
<pre class="source-code">
cond_var.wait(lock, []{ return work_done == true; });</pre> <p>Finally, two <a id="_idIndexMarker834"/>threads are started and joined as a producer and consumer to provide the following output:</p>
<pre class="console">
Sending: This is a testing message!
Receiving: This is a testing message!</pre> <p>Of course, the example could be much more complex, adding periodic production and consumption. We encourage you to try it out, just using another type of buffer – as you may remember, the <code>string_view</code> object is a constant. Be sure that the deallocator is correctly implemented and called. It is used to make the code safer and discard the possibility of memory leaks.</p>
<p>As you may have observed, throughout our work in this book, we often want to access an object just to read it, without modifying its data. In that case, we don’t need full-scale locking, but something to make a difference between just reading data or modifying it. This technique is the <em class="italic">read-write lock</em> and we present it in the following section.</p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor138"/>Implementing read-write locks and ranges with C++</h2>
<p>POSIX <a id="_idIndexMarker835"/>provides the read-write locks mechanism directly, while C++ hides it under different names – <code>shared_mutex</code> and <code>shared_timed_mutex</code>. Let’s see how it works traditionally in POSIX. We have the <em class="italic">read-write lock</em> object (<code>rwlock</code>) with the expected POSIX interface, where a thread could hold multiple concurrent read locks on it. The goal is to allow multiple readers to access the data until a thread decides to modify it. That thread locks the resource through a write lock. Most implementations favor the write lock over the read lock in order to avoid write starvation. Such behavior is not necessary when it comes to data races, but it definitely causes a minimal application execution bottleneck.</p>
<p>This is especially true when dealing with large-scale systems’ data readers – for example, multiple read-only UIs. The C++ features again give us a simple and robust instrument for this task. Therefore, we will not devote time to studying examples of POSIX. We advise you to take a look yourself if interested, starting with https://linux.die.net/man/3/pthread_rwlock_rdlock.</p>
<p>Proceeding with the C++ example, let’s consider the following scenario – a small number of threads want to modify a shared resource – a vector of numbers – and a larger number of threads wants to visualize the data. What we want to use here is <code>shared_timed_mutex</code>. It allows two levels of access: <em class="italic">exclusive</em>, where only one thread can own the mutex; and <em class="italic">shared</em>, where multiple threads share ownership of the mutex.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Keep in mind <a id="_idIndexMarker836"/>that <a id="_idIndexMarker837"/>both the <code>shared_timed_mutex</code> and <code>shared_mutex</code> types are heavier than a simple <code>mutex</code>, although <code>shared_mutex</code> is considered more efficient on some platforms than <code>shared_timed_mutex</code>. You’re expected to use them when your read operations are really resource-hungry, slow, and frequent. For short operation bursts it would be preferable to stick with just the mutex. You’ll need to measure your resource usage specifically for your system in order to work out which to choose.</p>
<p>The following example illustrates the usage of <code>shared_mutex</code>. We’ll also use the opportunity to<a id="_idIndexMarker838"/> present the <code>ranges</code> library in C++. This feature comes with C++20 and together with <code>string_views</code> provides an agile way to visualize, filter, transform, and slice C++ containers, among other things. Through this example, you’ll learn about some useful techniques with the <code>ranges</code> library, which will be explained along with the code. The full example can be found at https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209.</p>
<p>Let’s have a <code>Book</code> struct with a shared resource – <code>vector</code> of books. We are going to use <code>shared_mutex</code> to handle read-write locking:</p>
<pre class="source-code">
struct Book {
    string_view title;
    string_view author;
    uint32_t    year;
};
shared_mutex shresMutex;
vector&lt;Book&gt; shared_data =  {{"Harry Potter", ...</pre> <p>We<a id="_idIndexMarker839"/> implement the method for adding a book to the shared resource with the <code>wr_</code> prefix in order to distinguish its role from the other methods. We also execute a write lock on the resource (marker <code>{1}</code>):</p>
<pre class="source-code">
void wr_addNewBook(string_view title,
                   string_view author,
                   uint32_t year) {
    lock_guard&lt;shared_mutex&gt; writerLock(shresMutex); // {1}
    osyncstream{cout} &lt;&lt; "Add new book: " &lt;&lt; title &lt;&lt; endl;
    shared_data.emplace_back(Book {title, author, year});
    this_thread::sleep_for(500ms);
}</pre> <p>Now, we start with the implementation of multiple reader routines. They are marked with the <code>rd_</code> prefix, and each of them executes a read lock, meaning that the resource will be available for multiple readers at a time:</p>
<pre class="source-code">
void rd_applyYearFilter(uint32_t yearKey) {
    auto year_filter =
        [yearKey](const auto&amp; book)
       { return book.year &lt; yearKey; };
    shared_lock&lt;shared_mutex&gt; readerLock(shresMutex);
    osyncstream{cout}
   &lt;&lt; "Apply year filter: " &lt;&lt; endl; // {2}
    for (const auto &amp;book : shared_data |
                            views::filter(year_filter))
        osyncstream{cout} &lt;&lt; book.title &lt;&lt; endl;
}</pre> <p>Observe <a id="_idIndexMarker840"/>the <code>for</code> loop after marker <code>{2}</code>. It not only iterates <a id="_idIndexMarker841"/>through the shared resource, but with the pipe (|) character we filter out portions of it, which is similar to piping and <code>grep</code> as covered in <a href="B20833_03.xhtml#_idTextAnchor047"><em class="italic">Chapter 3</em></a>, except here, it’s not a pipe. We are creating a <em class="italic">range view</em> through the pipe operator, thus providing additional logic to the iteration. In other words, we manipulate the view to the container. This approach can be used not only for <code>vectors</code>, but for the other C++ iterable objects as well. Why? <em class="italic">Ranges</em> are used to extend and generalize the algorithms with iterators so the code becomes tighter and less error prone.</p>
<p>It’s easy to see the intention of the <em class="italic">range</em> here, too. Additionally, the <em class="italic">range view</em> is a lightweight object, similar to <code>string_view</code>. It represents an iterable sequence – the <em class="italic">range</em> itself, created on top of the containers’ iterators. It is based on the <em class="italic">Curiously Recurring Template Pattern</em>. Through the <em class="italic">range</em> interface, we can change the presentation of a container, present its values as transformed in a given manner, filter out values, split and combine sequences, present unique elements, shuffle elements, slide a window through the values, and so on. All of this is done via the simple syntax of already-implemented <em class="italic">range adapters</em>. In our example, <code>rd_applyYearFilter</code> has a <code>for</code> loop wherein books older than <code>yearKey</code> are filtered out. We could also print out the shared resource’s elements in reverse order:</p>
<pre class="source-code">
void rd_Reversed() {
    for (const auto &amp;book : views::reverse(shared_data))
        osyncstream{cout} &lt;&lt; book.title &lt;&lt; endl; ...</pre> <p>We could even combine views, as follows:</p>
<pre class="source-code">
for (const auto &amp;book :
         views::reverse(shared_data) |
         views::filter([nameSizeKey](Book book)
              {return book.author.size() &lt; nameSizeKey;}))}</pre> <p>The <a id="_idIndexMarker842"/>previous<a id="_idIndexMarker843"/> snippet iterates through the elements in reverse order, but it also filters out those books where the length of the author’s name is longer than a given value. With the next snippet, we demonstrate how to simply drop a portion of the container during iteration:</p>
<pre class="source-code">
for (const auto &amp;book :
   ranges::drop_view(shared_data, dropKey))
        osyncstream{cout} &lt;&lt; book.title &lt;&lt; endl;</pre> <p>If this is too generic, you could instead use a specific subrange, which will create a <code>range</code> object. The <code>range</code> object can be used like any other, as follows:</p>
<pre class="source-code">
auto const sub_res =
    ranges::subrange(shared_data.begin(),
                     shared_data.begin()+5);
    for (const auto&amp; book: sub_res){
        osyncstream{cout}
        &lt;&lt; book.title &lt;&lt; " " &lt;&lt; book.author
             &lt;&lt;  " " &lt;&lt; book.year &lt;&lt; endl;</pre> <p>With all of this complete, we create threads to execute all of these actions in a concurrent manner <a id="_idIndexMarker844"/>and see how the <em class="italic">read-write lock</em> manages them. Running the example will <a id="_idIndexMarker845"/>produce different output orders depending on the thread’s scheduling:</p>
<pre class="source-code">
    thread yearFilter1(
        []{ rd_applyYearFilter(1990); });
    thread reversed(
        []{ rd_Reversed(); });
    thread reversed_and_filtered(
        []{ rd_ReversedFilteredByAuthorNameSize(8); });
    thread addBook1(
        []{ wr_addNewBook("Dune", "Herbert", 1965); });
    thread dropFirstElements(
        []{ rd_dropFirstN(1); });
    thread addBook2(
        []{ wr_addNewBook("Jaws", "Benchley", 1974); });
    thread yearFilter2(
        []{ rd_applyYearFilter(1970); });</pre> <p>The output is per the <a id="_idIndexMarker846"/>described <em class="italic">range views</em> (the following has been rearranged slightly for easier reading):</p>
<pre class="console">
Apply reversed order:
It
East of Eden
Harry Potter
Drop first N elements:
East of Eden
It
Apply reversed order and filter by author name size:
It
Harry Potter
Apply year filter:
East of Eden
It
Add new book: Dune
Apply year filter:
East of Eden
Dune
Add new book: Jaws
Print subranged books in main thread:
East of Eden Steinbeck 1952
It King 1986</pre> <p>You have now <a id="_idIndexMarker847"/>learned about another combination of techniques with which you can scale up a system with multiple threads that handle presentation tasks. Let’s now take a step back and discuss the possible traps arising from concurrent execution that are not directly related to data races. We continue with cache-friendly code.</p>
<h1 id="_idParaDest-139"><a id="_idTextAnchor139"/>Discussing multiprocessor systems – cache locality and cache friendliness in C++</h1>
<p>You probably<a id="_idIndexMarker848"/> recall <a href="B20833_02.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a> at this point, where we discussed multi-thread and multi-core processors. The respective computational units were presented as processors. We also visualized the transport of instructions from the <strong class="bold">NVM</strong> (the disk) to the processors, through which we explained the creation of processes and <em class="italic">software</em> threads.</p>
<p>We want our code to be as performant as required. The most important aspect of getting the code to perform well is the choice of appropriate algorithms and data structures. With a bit of thought, you can try to squeeze the most out of every last CPU cycle. One of the most common examples of misusing algorithms is sorting a large, unordered array with bubble sort. So, make sure to learn your algorithms and data structures – together with the knowledge from this section and beyond, it will make you a really powerful developer.</p>
<p>As you already know, the<a id="_idIndexMarker849"/> further we get from the RAM and the closer we get to the processor registers, the faster the operations and the smaller the memory capacity becomes. Each time the processor loads data from the RAM to the cache, it will either just sit and wait for that data to show up, or execute other non-related tasks. Thus, from the perspective of the current task, the CPU cycles are wasted. Of course, reaching 100% CPU utilization might be impossible, but we should at least be aware when it’s doing needless work. All of this might sound meaningless to you at this point, but concurrent systems will suffer if we act carelessly.</p>
<p>The C++ language provides access to multiple tools for even better performance improvements, including <strong class="bold">prefetching mechanisms</strong> through<a id="_idIndexMarker850"/> hardware instructions <a id="_idIndexMarker851"/>and <strong class="bold">branch prediction</strong> <strong class="bold">optimization</strong>. Even without doing anything in particular, modern compilers and CPUs do a great job with these techniques. Still, we could improve this performance further by providing the right hints, options, and instructions. It’s also a good idea to be aware of the data in the cache to help reduce the time taken when accessing it. Remember that the cache is just a type of fast, temporary storage for data and instructions. So, we can use the features of C++ to our advantage when we treat the cache in a good manner, known<a id="_idIndexMarker852"/> as <strong class="bold">cache-friendly code</strong>. An important remark to note is the inverse of this statement – misusing C++ features will lead to poor cache performance, or at least not the best performance possible. You’ve probably already guessed that this is related to the system’s scale and the requirement for fast data access. Let’s discuss this further in the next section.</p>
<h2 id="_idParaDest-140"><a id="_idTextAnchor140"/>Considering cache locality through cache-friendly code</h2>
<p>We <a id="_idIndexMarker853"/>mentioned <a id="_idIndexMarker854"/>the concept of cache-friendly code already, but what does it truly mean? First of all, you need to be aware of<a id="_idIndexMarker855"/> the <code>int</code> or even an unsigned <code>char</code>.</p>
<p>As a result, caching <a id="_idIndexMarker856"/>has become a major aspect of almost every system. Earlier in the book we mentioned that slower hardware, such as disks, sometimes has its own cache memory to reduce the time taken to access frequently opened files. OSs can cache frequently used data, for example, files, as chunks of<a id="_idIndexMarker857"/> virtual address space, thus improving performance even more. This is also known as <strong class="bold">temporal locality</strong>.</p>
<p>Consider the following scenario: a piece of data is not found in the cache on the first try – this is known <a id="_idIndexMarker858"/>as a <strong class="bold">cache miss</strong>. Then it is looked up in the RAM, is found, and is loaded into the cache as<a id="_idIndexMarker859"/> one or <a id="_idIndexMarker860"/>multiple <strong class="bold">cache blocks</strong> or <strong class="bold">cache lines</strong>. Afterwards, if this data is requested a number of subsequent times and is still found in the cache, known <a id="_idIndexMarker861"/>as a<strong class="bold"> cache hit</strong>, it will remain in the cache and guarantee faster access, or at least faster than the first <strong class="bold">cache miss</strong>. You can observe this in the following diagram:</p>
<div><div><img alt="Figure 9.2 – Representation of temporal locality on the hardware level" height="470" src="img/Figure_9.2_B20833.jpg" width="1158"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Representation of temporal locality on the hardware level</p>
<p>As we mentioned with<a id="_idIndexMarker862"/> the <strong class="bold">prefetching mechanisms</strong> earlier, it’s a known fact that having an object with <a id="_idIndexMarker863"/>multiple <strong class="bold">cache hits</strong> means that the data around it might also be referenced soon. This causes the processor to <em class="italic">request</em> or <em class="italic">prefetch</em> that additional nearby data from the RAM and load it a priori, so it will be there in the cache when it is eventually needed. This causes <strong class="bold">spatial locality</strong>, meaning<a id="_idIndexMarker864"/> accessing  nearby memory and benefiting from the fact that caching is done in chunks, known as <strong class="bold">cache lines, t</strong>hus <a id="_idIndexMarker865"/>paying for a single transfer and using several bytes of memory. The prefetching technique assumes that the code already has <strong class="bold">spatial locality</strong> in order to improve performance.</p>
<p>Both locality <a id="_idIndexMarker866"/>principles are based on assumptions. But code branching requires good design. The simpler the branch tree, the simpler to predict. Again, you need to consider carefully the data structures and algorithms to be used. You also need to aim at contiguous memory access and reduce the code to simple loops and small functions; for example, switching from using linked lists to arrays or matrices. For small-sized objects, the <code>std::vector</code> container is still the optimal choice. Additionally, we ideally seek a data structure object that can fit into one <strong class="bold">cache line</strong> – but sometimes this is just not possible because of the application’s requirements.</p>
<p>Our process should access the data in contiguous blocks, where each one has the size of a cache line (typically 64 bytes but depends on the system). But if we want to do parallel evaluations, then it would be preferable for each CPU core (processor) to handle data in different <strong class="bold">cache lines</strong> from other cores’ data. If not, the cache hardware will have to move data back and forth between cores and the CPU will waste time on meaningless work again and the performance will worsen, instead of being improved. This term is known as <strong class="bold">false sharing</strong>, which we’ll now have a look at in the following section.</p>
<h2 id="_idParaDest-141"><a id="_idTextAnchor141"/>A glance at false sharing</h2>
<p>As a rule, small pieces <a id="_idIndexMarker867"/>of data will be put together in a single <strong class="bold">cache line</strong> unless the programmer instructs otherwise, as we will see in the following examples. This is the way processors work in order to keep latency low – they handle one cache line for each core at a time. Even if it’s not full, the <strong class="bold">cache line</strong>’s size will be allocated as the smallest possible block for the CPU to handle. As mentioned earlier, if the data in that <strong class="bold">cache line</strong> is requested by two or more threads independently, then this will slow down the multi-threaded execution.</p>
<p>Dealing with the effects of <strong class="bold">false sharing</strong> means getting predictability. Just as code branching can be predicted, so can the system programmer predict if an object is of the size of a cache line, and thus each separate object can reside in its own memory block. In addition, all computations can happen in the local scope and the shared data modifications take place at the end of a given procedure. Of course, such activities will lead to the wasting of resources at some point, but it’s a matter of design and preferences. Nowadays, we <a id="_idIndexMarker868"/>can use compiler optimizations to improve this predictability and performance, too, but we shouldn’t always rely on this. Let’s first check the size of our cache line:</p>
<pre class="source-code">
#include &lt;iostream&gt;
#include &lt;new&gt;
using std::hardware_destructive_interference_size;
int main() {
    std::cout &lt;&lt; "L1 Cache Line size: "
        &lt;&lt; hardware_destructive_interference_size
        &lt;&lt; " bytes";
    return 0;
}</pre> <p>The expected output is as follows:</p>
<pre class="console">
L1 Cache Line size: 64 bytes</pre> <p>Now that we know how to get the <code>std::atomic</code> to guarantee a single modifier to a shared resource, but we also emphasized that this is not the full picture. Let’s enrich the previous example with three atomic variables:</p>
<pre class="source-code">
    cout &lt;&lt; "L1 Cache Line size: "
         &lt;&lt; hardware_constructive_interference_size
         &lt;&lt; " bytes" &lt;&lt; endl;
    atomic&lt;uint32_t&gt; a_var1;
    atomic&lt;uint32_t&gt; a_var2;
    atomic&lt;uint32_t&gt; a_var3;</pre> <p>Printing the addresses out gives the following:</p>
<pre class="source-code">
       cout &lt;&lt; "The atomic var size is: " &lt;&lt; sizeof(a_var1)
            &lt;&lt; " and its address are: \n"
            &lt;&lt; &amp;a_var1 &lt;&lt; endl
            &lt;&lt; &amp;a_var2 &lt;&lt; endl
            &lt;&lt; &amp;a_var3 &lt;&lt; endl;
        ...</pre> <p>The output<a id="_idIndexMarker869"/> is as follows:</p>
<pre class="console">
L1 Cache Line size: 64 bytes
The atomic var size is: 4 and the addresses are:
0x7ffeb0a11c7c
0x7ffeb0a11c78
0x7ffeb0a11c74</pre> <p>This means that even when we have atomic variables, they can be fitted into a single <code>atomic_ref&lt;T&gt;::required_alignment</code>, which allows the programmer to align atomics as per the current cache line size, thus keeping them well apart. Let’s apply it for all atomics as follows:</p>
<pre class="source-code">
    alignas(hardware_destructive_interference_size)
        atomic&lt;uint32_t&gt; a_var1;</pre> <p>The output is as follows:</p>
<pre class="console">
L1 Cache Line size: 64 bytes
The atomic var size is: 4 and the addresses are:
0x7ffc3ac0af40
0x7ffc3ac0af00
0x7ffc3ac0aec0</pre> <p>In the preceding snippet, you <a id="_idIndexMarker873"/>can see that the differences in the addresses are as expected and the variables are well aligned, which was always the system programmer’s responsibility. Now, let’s apply the <code>increment()</code> method that you might remember from <a href="B20833_07.xhtml#_idTextAnchor101"><em class="italic">Chapter 7</em></a>:</p>
<pre class="source-code">
void increment(std::atomic&lt;uint32_t&gt;&amp; shared_res) {
    for(int I = 0; i &lt; 100000; ++i) {shared_res++;}
}</pre> <p>We increment an atomic resource, and as covered in <a href="B20833_08.xhtml#_idTextAnchor116"><em class="italic">Chapter 8</em></a>, we know how to measure the duration of a procedure. So, we can analyze the performance for the next four scenarios. One remark – if you feel so inclined, you could play with the compiler optimization levels to spot the difference in the following values, as we are not using any of the optimization flags. The full code example could be found at <a href="https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209">https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209</a>. Our scenarios are as follows:</p>
<ul>
<li>A single-threaded application, calling <code>increment()</code> 3 times, doing 300,000 increments of an atomic variable, which takes 2,744 microseconds</li>
<li>Direct sharing with one atomic variable, incremented 100,000 times by each of 3 threads in parallel, taking 5,796 microseconds</li>
<li>False sharing with three unaligned atomic variables, incremented 100,000 times by each of the 3 threads in parallel, taking 3,545 microseconds</li>
<li>No sharing with three aligned atomic variables, incremented 100,000 times by each of 3 threads in parallel, taking 1,044 microseconds</li>
</ul>
<p>As we are not using a benchmarking tool, we cannot measure the number of cache misses or hits. We simply do the following:</p>
<pre class="source-code">
    ...
    auto start = chrono::steady_clock::now();
    alignas(hardware_destructive_interference_size)
        atomic&lt;uint32_t&gt; a_var1 = 0;
    alignas(hardware_destructive_interference_size)
        atomic&lt;uint32_t&gt; a_var2 = 0;
    alignas(hardware_destructive_interference_size)
        atomic&lt;uint32_t&gt; a_var3 = 0;
    jthread t1([&amp;]() {increment(a_var1);});
    jthread t2([&amp;]() {increment(a_var2);});
    jthread t3([&amp;]() {increment(a_var3);});
    t1.join();
    t2.join();
    t3.join();
    auto end = chrono::steady_clock::now();
    ...</pre> <p>The <strong class="bold">no-sharing</strong> work <a id="_idIndexMarker874"/>is presented in the following diagram:</p>
<div><div><img alt="Figure 9.3 – Representation of no-sharing (correct sharing) of data on multiple cores/threads" height="548" src="img/Figure_9.3_B20833.jpg" width="1079"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Representation of no-sharing (correct sharing) of data on multiple cores/threads</p>
<p class="callout-heading">Important note</p>
<p class="callout">It’s obvious that we either have to align our atomic resources before we modify them in parallel, or use single-threaded applications for small procedures. The time metric could differ, depending on the system and the compiler optimization flags. Keep in mind that these speed-ups are great when you get the best out of your hardware, but going into so much detail might also lead to complex code, harder debugging, and time wasted on maintenance. It’s a balancing act.</p>
<p>False sharing happens <a id="_idIndexMarker875"/>during multi-threading and can be fixed if the shared object is fitted into one cache line. But what happens if the object is larger than one cache line in size?</p>
<h2 id="_idParaDest-142"><a id="_idTextAnchor142"/>Sharing resources larger than a cache line in C++</h2>
<p>The analysis here is relatively simple, as it is not so dependent on the language. Large objects, representing large data structures, are just... large. They don’t fit into single <strong class="bold">cache lines</strong> and therefore they are not <strong class="bold">cache friendly</strong> by nature. Data-oriented design deals with this issue. For example, you could think about using smaller objects or share only small parts of them for parallel work. Additionally, it is good to think about optimizations in algorithms. Making them linear leads to better <strong class="bold">branch predictions</strong>. This <a id="_idIndexMarker876"/>means making conditional statements depend on predictable, not random, data. Complex conditional statements can be replaced with arithmetic solutions and templates, or chained differently, so it is easier for the CPU to predict which branch has a higher probability of occurring. Such operations, again, could lead to unreadable code and complex debugging, so proceed with them only when the code is not fast enough for your requirements.</p>
<p>As <strong class="bold">branch misprediction</strong> could<a id="_idIndexMarker877"/> be expensive and remain well hidden, another proposal is the<a id="_idIndexMarker878"/> so-called <strong class="bold">conditional move</strong>. It is not based on predictions, but on data. The data dependencies include both <em class="italic">condition true</em> and <em class="italic">condition false</em> cases. After an instruction that conditionally moves data from one register to another, the contents of the second depend on both their previous values and the values from the first register. As mentioned, well-designed branching allows better performance. But data dependencies require one or two CPU cycles to arrive, sometimes making them a safer bet. A probable trap is when the condition is such that the value taken from the memory is not assigned to the register – then it’s just meaningless waiting. Luckily for the system programmer, the <strong class="bold">conditional move</strong> instructions in the instruction sets are typically close register-wise.</p>
<p><code>std::array</code> and <code>std::vector</code>. Yes, the vector could be resized, but it’s still cache friendly, as the elements are next to each other in the memory. Of course, if you have to reallocate the vector due to constant resizing, then probably it’s not the data structure you need. You could consider the <code>std::deque</code> container, which is efficient for modifications in the middle of the collection, or <code>std::list</code> as an alternative, which is a linked list and is not cache friendly at all.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Depending on the system, many reallocations (construction and destruction) of contiguous memory blocks could cause memory fragmentation. This can happen due to software algorithms for memory management, language standards, OSs, drivers, devices, and so on. It is hard to predict it until it happens. It might take a good portion of non-stop execution time for the memory allocations to start failing. There could be enough free space in the sum of the free memory blocks in the RAM, but not a single block big enough to hold the currently reallocated or created contiguous block. Excessive fragmentation could lead to poor performance and even denial of service.</p>
<p>A final remark<a id="_idIndexMarker880"/> on the topic is that there are many articles discussing optimal ways of using C++’s algorithms and containers efficiently. It deserves a book on its own and most of the time is very CPU specific – or at least when you get to the absolute performance. For example, the <strong class="bold">conditional moves</strong> lead directly to assembly code, which we don’t have the opportunity to explore here. That said, the variety of solutions for different practical problems is enormous when it comes to algorithms and data structures.</p>
<h1 id="_idParaDest-143"><a id="_idTextAnchor143"/>Revisiting shared resources through the C++ memory model via spinlock implementation</h1>
<p>We learned about atomic operations back in <a href="B20833_07.xhtml#_idTextAnchor101"><em class="italic">Chapter 7</em></a>. In this chapter, you learned that the placement of atomic variables in the cache is crucial as well. Originally, atomics and locks<a id="_idIndexMarker881"/> were introduced because of correctness when multiple threads want to enter the same critical section. Now, our investigation will continue a bit deeper. There’s one last piece of the puzzle of atomic operations. Examine the following snippet:</p>
<pre class="console">
Thread 1: shrd_res++; T1: load value
                      T1: add 1
Thread 2: shrd_res++; T2: load value
                      T2: add 1
                      T2: store value
                      T1: store value</pre> <p>This was an example of a non-atomic operation. Even when we make it atomic, we still don’t have a <a id="_idIndexMarker882"/>word about the order of the instructions. Until now, we used the synchronization primitives to instruct the CPU about which section of instructions has to be taken as a unitary context. What we need now is to instruct the processor about the order of those instructions. We do this through C++’s <code>memory_order</code>, which is a part of the C++ standard memory model.</p>
<h2 id="_idParaDest-144"><a id="_idTextAnchor144"/>Introducing the memory_order type in C++</h2>
<p>With<a id="_idIndexMarker883"/> the <code>memory_order</code> type, we specify how atomic and non-atomic memory accesses are ordered around an atomic operation. The atomic realization of the snippet from the preceding section and the example using read-write locks earlier in the chapter could both suffer from the same issue: two atomic operations are not atomic as a whole. The order of instructions inside the atomic scope will be kept, but not around it. This is usually done after optimization techniques in the CPU and the compiler. So, if there are many reader threads, the order in which we (and the threads) expect to observe changes could vary. Such an effect could appear even during single-threaded execution as the compiler might re-arrange instructions as allowed by the memory model.</p>
<p class="callout-heading">Note</p>
<p class="callout">We encourage you <a id="_idIndexMarker884"/>to check out the full information on <code>memory_order</code> here: https://en.cppreference.com/w/cpp/atomic/memory_order.</p>
<p>An important remark is that the default behavior of all atomic operations in C++ applies sequentially consistent ordering. The defined memory orders in C++20 are as follows:</p>
<ul>
<li>Relaxed ordering, tagged like so:<pre class="source-code">
memory_order_relaxed = memory_order::relaxed;</pre><p class="list-inset">This ordering is the bare minimum. It is the cheapest option and provides no guarantees, except of the current operation’s atomicity. One example of this in action is the incrementation of the <code>shared_ptr</code> reference counter, as it needs to be atomic, but no ordering is required.</p></li> <li>Release-acquire ordering, tagged as follows:<pre class="source-code">
memory_order_acquire = memory_order::acquire;
memory_order_release = memory_order::release;
memory_order_acq_rel = memory_order::acq_rel;</pre><p class="list-inset">Reads and writes are prevented from reordering right after an atomic region when the release operation is in effect. The <code>acquire</code> operation is similar, but reordering is prohibited before the atomic region. The third model, <code>acq_rel</code>, is a combination of both. This model could really help in the creation of read-write locks, except there’s no locking going on. The decrementing of the <code>shared_ptr</code> reference count is done through this technique as it needs to be synchronized with the destructor.</p></li> <li>Release-consume ordering, tagged as follows:<pre class="source-code">
memory_order_consume = memory_order::consume;</pre><p class="list-inset">The<a id="_idIndexMarker885"/> <code>consume</code> operation’s requirements are still being revised to this day. It is designed to work as the <code>acquire</code> operation does, but only for specific data. That way, the compiler is more flexible in optimizing the code than the <code>acquire</code> operation. Obviously, getting the data dependencies right makes the code more complex, therefore this model is not widely used. You can see it when accessing rarely written concurrent data structures – configurations and settings, security policies, firewall rules, or publish-subscribe applications with pointer-mediated publication; the producer publishes a pointer through which the consumer can access information.</p></li> <li>Sequentially consistent ordering, tagged as follows:<pre class="source-code">
memory_order_seq_cst = memory_order::seq_cst;</pre><p class="list-inset">This is the exact opposite of the relaxed order. All operations in and around the atomic region follow a strict order. Neither instruction can cross the barrier imposed by the atomic operation. It is considered the most expensive model as all optimization opportunities are lost. Sequentially consistent ordering is helpful for multiple producer-multiple consumer applications, where all consumers <a id="_idIndexMarker886"/>must observe the actions of all producers occurring in an exact order.</p></li> </ul>
<p>One famous example directly benefiting from the memory order is the <strong class="bold">spinlock</strong> mechanism. We will proceed to examine this in the next section.</p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor145"/>Designing spinlocks for multiprocessor systems in C++</h2>
<p>Operating systems <a id="_idIndexMarker887"/>often use this technique as it’s very efficient for short-period operations, including the ability to escape <a id="_idIndexMarker888"/>rescheduling and context switching. But locks held for longer periods will be at risk of being interrupted by the OS scheduler. The <strong class="bold">spinlock</strong> means<a id="_idIndexMarker889"/> that a given thread will either acquire a lock or will wait <em class="italic">spinning</em> (in a loop) – checking the lock’s availability. We discussed a similar example of <em class="italic">busy waiting</em> earlier in the chapter when we<a id="_idIndexMarker890"/> presented <strong class="bold">cooperative cancellation</strong>. The risk here is that keeping the lock acquired for longer periods will put the system into <a id="_idIndexMarker891"/>a <strong class="bold">livelock</strong> state, as described in <a href="B20833_02.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a>. The thread holding the lock will not progress further by releasing it, and the other threads will remain <em class="italic">spinning</em> while trying to acquire the lock. C++ is well suited for the implementation of the spinlock as atomic operations can be configured in detail. In low-level programming, this approach is also known as test-and-set. Here’s an example:</p>
<pre class="source-code">
struct SpinLock {
    atomic_bool state = false;
    void lock() {
        while (state.exchange(true,
                              std::memory_order_acquire){
            while (state.load(std::memory_order_relaxed))
           // Consider this_thread::yield()
                // for excessive iterations, which
                // go over a given threshold.
}
    void unlock() noexcept {
        state.store(false, std::memory_order_release); };</pre> <p>You’re probably wondering why we aren’t using the already-known synchronization techniques. Well, keep in mind that all memory order settings here cost only one CPU instruction. They are fast and simple, both software- and hardware-wise. You should limit your use of them to very short periods of time, though, since the CPU is prevented from doing a useful job for another process.</p>
<p>An <a id="_idIndexMarker892"/>atomic Boolean is used to mark<a id="_idIndexMarker893"/> whether the state of <code>SpinLock</code> is locked or unlocked. The <code>unlock()</code> method is easy – when the critical section is released, the <code>false</code> value is set (<code>store()</code> is atomic) to the <code>state</code> member through the release order. All following read/write operations have to be ordered in an atomic manner. The <code>lock()</code> method firstly runs a loop, trying to access the critical section. The <code>exchange()</code> method will set <code>state</code> to <code>true</code> and will return the previous value, <code>false</code>, thus interrupting the loop. Logically, this is very similar to the semaphore <code>P(S)</code> and <code>V(S)</code> functions. The inner loop will execute the busy wait scenario without order limitations and without <a id="_idIndexMarker894"/>producing <strong class="bold">cache misses</strong>.</p>
<p class="callout-heading">Important note</p>
<p class="callout">The <code>store()</code>, <code>load()</code>, and <code>exchange()</code> operations have <code>memory_order</code> requirements and a list of supported orders. Using additional and unexpected orders leads to undefined behavior and keeps the CPU busy without doing useful work.</p>
<p>An advanced version of the <strong class="bold">spinlock</strong> is the<a id="_idIndexMarker895"/> ticket lock algorithm. In the same fashion as with queues, tickets are provided to the threads in a FIFO manner. That way, the order in which they enter a critical section is managed fairly. In contrast with spinlocks, starvation is avoided here. However, this mechanism does not scale well. First of all, there’s a greater number of instructions to read, test, and acquire the lock, as there are more instructions for managing the order. Secondly, as soon as the critical section is free for access, all threads must have their context loaded into the cache to determine whether they are allowed to acquire the lock and enter the critical section.</p>
<p>C++ has an advantage here thanks to its low latency. The full example is available at <a href="https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209">https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209</a>.</p>
<p>First, we implement the <code>TicketLock</code> mechanism, providing the necessary <code>lock()</code> and <code>unlock()</code> methods. We use two helper member variables, <code>serving</code> and <code>next</code>. As you see, they are aligned to be in separate <code>lock()</code> and <code>unlock()</code> methods are implemented as in the <code>SpinLock</code> example. Additionally, an atomic increment is done through <code>fetch_add()</code>, allowing the<a id="_idIndexMarker896"/> lock to generate tickets. No read/write operations happen around it, so it is executed in a relaxed order. Instead of just <a id="_idIndexMarker897"/>setting the variable to <code>false</code> as with <code>SpinLock</code>, the <code>unlock()</code> method loads a ticket number value, again in a relaxed manner, and stores it as the currently served thread:</p>
<pre class="source-code">
struct TicketLock {
    alignas(hardware_destructive_interference_size)
        atomic_size_t serving;
    alignas(hardware_destructive_interference_size)
        atomic_size_t next;</pre> <p>The methods for locking and unlocking of the <code>TicketLock</code> algorithm follow:</p>
<pre class="source-code">
    void lock() {
        const auto ticket = next.fetch_add(1,
                                memory_order_relaxed);
        while (serving.load(memory_order_acquire) !=
               ticket);
    }
    void unlock() {
        serving.fetch_add(1, memory_order_release);
    }
};</pre> <p>Now, a global <code>spinlock</code> object of type <code>TicketLock</code> is created. We also create a <code>vector</code> that plays the role of a shared resource. The <code>producer()</code> and <code>consumer()</code> routines are as expected – the first will create data and the latter will consume it, including clearing the shared resource. As both operations will be carried out in parallel, the order of their execution is random. If you want instead to create a ping-pong-like behavior for this, <strong class="bold">conditional variables</strong> or <strong class="bold">semaphores</strong> could be used as<a id="_idIndexMarker898"/> signaling mechanisms. The current implementation is limited just to the purposes <a id="_idIndexMarker899"/>of<a id="_idIndexMarker900"/> the <strong class="bold">ticket lock</strong>:</p>
<pre class="source-code">
TicketLock spinlock = {0};
vector&lt;string&gt; shared_res {};
void producer() {
    for(int i = 0; i &lt; 100; i ++) {
        osyncstream{cout} &lt;&lt; "Producing: " &lt;&lt; endl;
        spinlock.lock();
        shared_res.emplace_back("test1");
        shared_res.emplace_back("test2");
        for (const auto&amp; el : shared_res)
            osyncstream{cout} &lt;&lt; "p:" &lt;&lt; el &lt;&lt; endl;
        spinlock.unlock();
        this_thread::sleep_for(100ms);
    }
}</pre> <p>And the consumer is similar to what you’ve already learned:</p>
<pre class="source-code">
void consumer() {
    for (int i = 0; i &lt; 100; i ++) {
         this_thread::sleep_for(100ms);
         osyncstream{cout} &lt;&lt; "Consuming: " &lt;&lt; endl;
         spinlock.lock();
         for (const auto&amp; el : shared_res)
             osyncstream{cout} &lt;&lt; "c:" &lt;&lt; el &lt;&lt; endl;</pre> <p>Remove the contents of the vector:</p>
<pre class="source-code">
         shared_res.clear();
         spinlock.unlock();
         if (shared_res.empty())
             osyncstream{cout} &lt;&lt; "Consumed" &lt;&lt; endl;
     }
}</pre> <p>The output is as follows:</p>
<pre class="console">
Producing:
p:test1
p:test2
Consuming:
c:test1
c:test2
...</pre> <p>The<a id="_idIndexMarker901"/> output shows that the production <a id="_idIndexMarker902"/>and the consumption routines are treated as a whole, although they are not called an equal number of times, which is expected. As mentioned previously, instead of pausing the threads for <code>100ms</code>, you could also modify the code by adding a <strong class="bold">condition variable</strong>:</p>
<pre class="source-code">
void producer() {
    for(int i = 0; i &lt; 100; i ++) {
        cout &lt;&lt;"Producing:" &lt;&lt; endl;
        unique_lock&lt;mutex&gt; mtx(cv_mutex);
        cond_var.wait(mtx, []{ return work_done ==
                                      !work_done; });</pre> <p>Proceed with the expected critical section:</p>
<pre class="source-code">
        spinlock.lock();
        shared_res.emplace_back"test1");
        shared_res.emplace_back"test2");
        for (const auto&amp; el : shared_res)
            cout &lt;&lt;"p" &lt;&lt; el &lt;&lt; endl;
        spinlock.unlock();
        work_done = !work_done;
    }
}</pre> <p>With<a id="_idIndexMarker903"/> all of these techniques combined – memory robustness, synchronization primitives, cache friendliness, and instruction <a id="_idIndexMarker904"/>ordering awareness – you have the instruments to really sharpen your code’s performance and tweak it to get the best performance on your specific system. We want to take this opportunity to remind you that such detailed optimizations could lead to unreadable code and hard debugging, so use them only when required.</p>
<h1 id="_idParaDest-146"><a id="_idTextAnchor146"/>Summary</h1>
<p>In this chapter, we’ve gathered together the entire set of instruments required for optimal code performance with C++. You learned techniques on many different system and software levels, so it’s understandable if you want to take a breather now. It is true that it would be good to spend more time on some of what we covered, for example, <strong class="bold">branch predictions</strong> and <strong class="bold">cache friendliness</strong>, or to implement more algorithms through <strong class="bold">condition variables</strong> and memory order. We strongly encourage you to use this chapter as a step in the direction of system improvements and more efficient work.</p>
<p>The next chapter is dedicated to one more significant improvement in C++’s features – <strong class="bold">coroutines</strong>. You will see that they are much lighter and, for some of the mechanisms discussed here, such as event waiting, they are much more preferable.</p>
</div>
</div></body></html>