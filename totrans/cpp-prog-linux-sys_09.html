<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer067">
<h1 class="chapter-number" id="_idParaDest-129"><a id="_idTextAnchor129"/>9</h1>
<h1 id="_idParaDest-130"><a id="_idTextAnchor130"/>Understanding the C++ Memory Model</h1>
<p>This chapter is a continuation of the discussion from <a href="B20833_07.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, where we discussed a few multiprocess and multi-threaded techniques; this chapter will enhance their usage. We will guide you through various techniques while narrowing down to the main focus of the chapter – the C++ memory model. But in order to discuss this, you will start first with a brief examination of memory robustness through the smart pointer and the optional objects. We will use them later to implement <em class="italic">lazy initialization</em> and handle <em class="italic">shared memory</em> regions safely. An improved memory access analysis of <em class="italic">cache-friendly</em> code follows. You will learn when and why using multi-threaded execution could be a trap, even though you did everything right in the <span class="No-Break">software design.</span></p>
<p>This chapter gives you the opportunity to broaden your understanding of the synchronization primitives. While learning about the <em class="italic">condition variables</em>, you will also understand the benefits of the <em class="italic">read-write locks</em>. We will use the <em class="italic">ranges</em> from C++20 to visualize the same shared data differently. Combining these mechanisms one by one, we will finalize our analysis with the biggest topic – instruction ordering. Through the C++ <em class="italic">memory order</em>, you will learn more about the significance of the correct atomic routine setup. The <em class="italic">spinlock</em> implementation will be used to summarize all techniques at <span class="No-Break">the end.</span></p>
<p>In this chapter, we are going to cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Getting to know smart pointers and optionals <span class="No-Break">in C++</span></li>
<li>Learning about condition variables, read-write locks, and ranges <span class="No-Break">in C++</span></li>
<li>Discussing multiprocessor systems – cache locality and cache friendliness <span class="No-Break">in C++</span></li>
<li>Revisiting shared resources through the C++ memory model via the <span class="No-Break">spinlock implementation</span></li>
</ul>
<h1 id="_idParaDest-131"><a id="_idTextAnchor131"/>Technical requirements</h1>
<p>In order to run the code examples, the reader must prepare <span class="No-Break">the following:</span></p>
<ul>
<li>A Linux-based system capable of compiling and executing C++20 (for example, <strong class="bold">Linux </strong><span class="No-Break"><strong class="bold">Mint 21</strong></span><span class="No-Break">)</span></li>
<li>The GCC12.2 compiler: <a href="https://gcc.gnu.org/git/gcc.gitgcc-source"><span class="No-Break">https://gcc.gnu.org/git/gcc.git gcc-source</span></a><ul><li>With the <strong class="source-inline">-std=c++2a</strong>, <strong class="source-inline">-lpthread</strong>, and <strong class="source-inline">-</strong><span class="No-Break"><strong class="source-inline">lrt</strong></span><span class="No-Break"> flags</span></li></ul></li>
<li>For all the examples, you can alternatively <span class="No-Break">use </span><a href="https://godbolt.org/"><span class="No-Break">https://godbolt.org/</span></a><span class="No-Break">.</span></li>
<li>All code examples in this chapter are available for download <span class="No-Break">from </span><a href="https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209"><span class="No-Break">https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209</span></a><span class="No-Break">.</span></li>
</ul>
<h1 id="_idParaDest-132"><a id="_idTextAnchor132"/>Getting to know smart pointers and optionals in C++</h1>
<p>In <a href="B20833_04.xhtml#_idTextAnchor060"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, we revisited the C++ fundamentals in order to be on the same page when it comes to the language. One instrument that is also considered a <em class="italic">must </em>is smart pointers. Through<a id="_idIndexMarker791"/> these, we are able to improve the safety of the program and also manage our resources more effectively. And as discussed in the earlier chapters, this is one of our main goals as system programmers. Remember the <strong class="bold">RAII</strong> principle? Smart pointers are based on this, helping the C++ developer reduce and even eliminate <em class="italic">memory leaks</em>. They could also help with shared memory management as you will see later in <span class="No-Break">the chapter.</span></p>
<p><em class="italic">Memory leaks</em> appear<a id="_idIndexMarker792"/> when we allocate memory but fail to free it. This could happen not only because we forgot to call the object’s destructor, but also when we lose the pointer to that memory address. In addition to these, there are also the <em class="italic">wild</em> and <em class="italic">dangling pointers</em> to consider as well. The first one happens when the pointer is there on the <em class="italic">stack</em>, but it’s never associated with the real object (or address). The second one happens when we free the memory, used by the object, but the value of the pointer remains <em class="italic">dangling</em> around, and we reference an already-deleted object. Altogether, these errors can lead not only<a id="_idIndexMarker793"/> to <strong class="bold">memory fragmentation</strong>, but also<a id="_idIndexMarker794"/> to <strong class="bold">buffer </strong><span class="No-Break"><strong class="bold">overflow</strong></span><span class="No-Break"> vulnerabilities.</span></p>
<p>These issues are hard to catch and reproduce, especially on large systems. System programmers and software integration engineers use tools such as address sanitizers, static and dynamic <a id="_idIndexMarker795"/>code analyzers, and profilers, among others, relying on them to predict future defects. But such tools are expensive and consume a lot of computational power, so we cannot rely on them constantly for higher code quality. That said, what can we do, then? The answer is simple – use <span class="No-Break">smart pointers.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">You can read more on the subject of smart pointers in the standard, or refer <span class="No-Break">to </span><a href="https://en.cppreference.com/w/cpp/memory"><span class="No-Break">https://en.cppreference.com/w/cpp/memory</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor133"/>Retracing RAII via smart pointers</h2>
<p>Even <a id="_idIndexMarker796"/>experienced C++ developers make mistakes when it comes to the right time for memory deallocation. Other languages use garbage collection techniques to handle memory management, but it’s important to mention that memory leaks happen there as well. Multiple algorithms are implemented for detecting such cases in the code but are not always successful. For example, the cycle <a id="_idIndexMarker797"/>dependency between objects is sometimes difficult to resolve – should two objects pointing to each other be deleted, or should they remain allocated? If they remain allocated, does this constitute a leak or not? So, it’s our job to be cautious about memory usage. In addition, garbage collectors work to free up memory, but do not manage opened files, network connections, locks, and so on. To this end, C++ implements its own instrument for control – wrapper classes over the pointers, helping us free the memory at the right time, usually when the object goes out of scope (the object life cycle was discussed already in <a href="B20833_04.xhtml#_idTextAnchor060"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>). Smart pointers are efficient in terms of memory and performance, meaning they don’t cost (much) more than raw pointers. At the same time, they give us robustness in memory management. There are three types of smart pointers in the <span class="No-Break">C++ standard:</span></p>
<ul>
<li><strong class="source-inline">unique_ptr</strong>: This<a id="_idIndexMarker798"/> is a <a id="_idIndexMarker799"/>pointer that is allowed one owner only. It cannot be copied or shared, but the ownership can be transferred. It has the size of a single raw pointer. It is destroyed and the object deallocated when it goes out of <span class="No-Break">the scope.</span></li>
<li><strong class="source-inline">shared_ptr</strong>: This <a id="_idIndexMarker800"/>can have multiple owners and is <a id="_idIndexMarker801"/>destroyed when all owners have given up ownership on it or all go out of scope. It uses a reference counter to the pointer of an object. Its size is two raw pointers – one for the allocated object, and one for the shared control block containing the <span class="No-Break">reference count.</span></li>
<li><strong class="source-inline">weak_ptr</strong>: This <a id="_idIndexMarker802"/>provides access to an object owned by one or<a id="_idIndexMarker803"/> more shared pointers, but doesn’t count references. It is used for observing an object, but not for managing its life cycle. It consists of two pointers – one for the control block, and one for pointing to the shared pointer it was constructed from. Through <strong class="source-inline">weak_ptr</strong> you can learn whether the underlying <strong class="source-inline">shared_ptr</strong> is still valid – just call the <span class="No-Break"><strong class="source-inline">expired()</strong></span><span class="No-Break"> method.</span></li>
</ul>
<p>Let’s<a id="_idIndexMarker804"/> demonstrate their initial roles through the <span class="No-Break">following </span><span class="No-Break"><a id="_idIndexMarker805"/></span><span class="No-Break">example:</span></p>
<pre class="source-code">
struct Book {
   string_view title;
   Book(string_view p_title) : title(p_title) {
        cout &lt;&lt; "Constructor for: " &lt;&lt; title &lt;&lt; endl; }
   ~Book() {cout &lt;&lt; "Destructor for: " &lt;&lt; title &lt;&lt; endl;}};
int main() {
    unique_ptr&lt;Book&gt; book1 =
        make_unique&lt;Book&gt;("Jaws");
    unique_ptr&lt;Book&gt; book1_new;
    book1_new = move(book1); // {1}
    cout &lt;&lt; book1_new-&gt;title &lt;&lt; endl;
    shared_ptr&lt;Book&gt; book2 =
        make_unique&lt;Book&gt;("Dune");
    shared_ptr&lt;Book&gt; book2_new;
    book2_new = book2; // {2}
    cout &lt;&lt; book2-&gt;title &lt;&lt;" "&lt;&lt; book2_new-&gt;title &lt;&lt; endl;
    cout &lt;&lt; book2.use_count() &lt;&lt; endl;</pre> <p>As you can <a id="_idIndexMarker806"/>see, we use the heap as we call <strong class="source-inline">new</strong> for the <a id="_idIndexMarker807"/>creation of the <strong class="source-inline">Book</strong> objects. But as the smart pointer handles memory management, we don’t need to call the <span class="No-Break">destructor explicitly:</span></p>
<pre class="console">
Constructor for: Jaws
Jaws
Constructor for: Dune
Dune Dune
2
Destructor for: Dune
Destructor for: Jaws</pre> <p>First, we move the ownership of <strong class="source-inline">book1</strong>’s object to another <strong class="source-inline">unique_ptr</strong> – <strong class="source-inline">book1_new</strong> (marker <strong class="source-inline">{1}</strong>). We print out its <strong class="source-inline">title</strong> through the second <strong class="source-inline">unique_ptr</strong> as the first one is already invalid. We do the same operation for another <strong class="source-inline">Book</strong> object, but through a <strong class="source-inline">shared_ptr</strong> object (marker <strong class="source-inline">{2}</strong>). This time the <strong class="source-inline">title</strong> variable can be accessed from both pointers. We also print the reference count, and we see there are two references to <span class="No-Break">that object.</span></p>
<p><strong class="source-inline">weak_ptr</strong> has useful strengths in system programming, too. You can use <strong class="source-inline">weak_ptr</strong> to check for pointer validity. <strong class="source-inline">weak_ptr</strong> could also resolve the issue of cyclic dependency between objects. Let’s consider an example of a list node of a doubly linked list. The next example illustrates the benefits of <strong class="source-inline">weak_ptr</strong>. This is a good time to advise you not to implement such data structures yourself, especially when they are already a part of the C++ standard. </p>
<p>Now, let’s use the <strong class="source-inline">Book</strong> object as content of the <span class="No-Break"><strong class="source-inline">ListNode</strong></span><span class="No-Break"> </span><span class="No-Break"><strong class="source-inline">struct</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
struct ListNode {
    Book data;
    ListNode(string_view p_title) {
        data.title = p_title;
        cout &lt;&lt; "Node created: " &lt;&lt; data.title &lt;&lt; endl;
    }</pre> <p>We also <a id="_idIndexMarker808"/>add two member variables for the previous and following nodes, but one of them will be <strong class="source-inline">weak_ptr</strong>. One remark is that the <strong class="source-inline">weak_ptr</strong> reference<a id="_idIndexMarker809"/> is not counted as such in the <strong class="source-inline">shared_ptr</strong> control block. Now, we have both access to the objects and the opportunity to count the references to zero with <span class="No-Break">each deallocation:</span></p>
<pre class="source-code">
    ~ListNode() {
        cout &lt;&lt; "Node destroyed: " &lt;&lt; data.title
             &lt;&lt; endl;
    }
    shared_ptr&lt;ListNode&gt; next;
    weak_ptr&lt;ListNode&gt; prev;
};
int main() {
    shared_ptr&lt;ListNode&gt; head =
        make_shared&lt;ListNode&gt;("Dune");
    head-&gt;next = make_shared&lt;ListNode&gt;("Jaws");
    if (!head-&gt;next-&gt;prev.expired())
        head-&gt;next-&gt;prev = head;</pre> <p>From the output, it’s clear that all objects were <span class="No-Break">removed successfully:</span></p>
<pre class="console">
Node created: Dune
Node created: Jaws
Node destroyed: Dune
Node destroyed: Jaws</pre> <p><strong class="source-inline">weak_ptr</strong> is also useful for cache implementation. Think about it – if you lose all references to an object, you will lose the object itsel; but with smart pointers, it will certainly be destroyed. So, imagine that recently accessed objects or objects with higher importance are kept through <strong class="source-inline">shared_ptr</strong> in the current code scope. But <strong class="source-inline">weak_ptr</strong> allows us to keep a reference to an object in the same scope if we need to reference the object later <a id="_idIndexMarker810"/>in that same scope. We would create a <strong class="source-inline">weak_ptr</strong> object to it in this case. But imagine that meanwhile, some other code scope holds a reference to the object through <strong class="source-inline">shared_ptr</strong>, thus keeping it allocated. In other words, we <a id="_idIndexMarker811"/>know about the object, but we don’t need to be concerned about its management. Thus, that object is accessible if it’s still required later, but removed when nothing else needs it. The following diagram shows how <strong class="source-inline">shared_ptr</strong> could be incorrectly used on the left-hand side, along with the implementation just described on the <span class="No-Break">right-hand side:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer064">
<img alt="Figure 9.1 – Cyclic dependency through shared_ptr and resolving through weak_ptr" height="502" src="image/Figure_9.1_B20833.jpg" width="1285"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Cyclic dependency through shared_ptr and resolving through weak_ptr</p>
<p>We are not <a id="_idIndexMarker812"/>going to dive further into other design solutions where smart pointers could come in handy in this section, but we will return to them in the realm of system programming later in the chapter. In the next section, we discuss a technique that’s the opposite to <strong class="source-inline">weak_ptr</strong>, where we retain the awareness of an object that hasn’t been created in <span class="No-Break">memory yet.</span></p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor134"/>Doing a lazy initialization in C++</h2>
<p>Do<a id="_idIndexMarker813"/> you play video games? Have you ever seen a missing texture somewhere in the graphics while playing? Has a graphical resource appeared suddenly when you moved close to it with your character? Have you observed such behavior in other UIs as well? If your answers are mostly in the positive, then you have probably encountered <strong class="bold">lazy initialization</strong> already. It’s easy to figure out that its purpose is to postpone the construction of an object until it’s really needed. By doing so, we allow the system to allocate the required resources only. We also use it to speed up our code, especially if it’s run during high CPU loads, such as at system startup. Instead of wasting CPU cycles to create large objects that won’t be needed until (much) later, we free up the CPU to handle other requests. On the negative side, we might end up failing to load the object on time, as you have likely observed in video games. As we discussed in <a href="B20833_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, this is also used when a program is loaded, and the kernel allocates virtual memory in a lazy fashion – a page of executable code is not loaded <span class="No-Break">until referenced.</span></p>
<p>As with every other pattern, <strong class="bold">lazy initialization</strong> cannot solve all of the problems. So, the system programmer has to choose whether it should be applied for the given application’s functions or not. Usually, it is preferred that parts of the graphical and network storage resources remain lazily initialized as they are loaded on demand either way. In other words, the user doesn’t see the UI in its entirety all the time. Therefore, it’s not required to store it in memory a priori. C++ has features that allow us to easily implement this approach. We present <strong class="bold">lazy initialization</strong> in the <span class="No-Break">following example:</span></p>
<pre class="source-code">
#include &lt;iostream&gt;
#include &lt;chrono&gt;
#include &lt;optional&gt;
#include &lt;string_view&gt;
#include &lt;thread&gt;
using namespace std;
using namespace std::literals::chrono_literals;
struct Settings {
    Settings(string_view fileName) {
        cout &lt;&lt; "Loading settings: " &lt;&lt; fileName &lt;&lt; endl;
    }
    ~Settings() {
        cout &lt;&lt; "Removing settings" &lt;&lt; endl;
    }</pre> <p>We <a id="_idIndexMarker814"/>propose a <strong class="source-inline">Settings</strong> class that will help us simulate the loading and updating of a list of settings from the disk. Note that we pass it by value and not <span class="No-Break">by reference:</span></p>
<pre class="source-code">
    void setSetting(string_view setting,
                    string_view value) {
        cout &lt;&lt; "Set setting: " &lt;&lt; setting
             &lt;&lt; " to: " &lt;&lt; value &lt;&lt; endl;
    }
};</pre> <p>This technique saves time due to reduced loading from memory. In C++, pass-by-value (or pass-by-copy) is the default argument passing technique, except for in the case of arrays. It is cheap and optimal for small types, such as <strong class="source-inline">int</strong>. Pass-by-reference is an alternative to pass-by-value and the <strong class="source-inline">string_view</strong> object is handled in the same manner as <strong class="source-inline">int</strong>, using a cheaper copy constructor than other standard objects such as <strong class="source-inline">string</strong>. Getting back to our example, we’re creating a configuration object, <strong class="source-inline">Config</strong>, which will consist of the settings file (which could be more than one file in real-world scenarios) and will allow settings to be changed in that configuration. Our <strong class="source-inline">main()</strong> method simulates an application’s startup. The <strong class="source-inline">Config</strong> object will be constructed, but the settings file will be loaded only when the startup is finished, and the process resources <span class="No-Break">are available:</span></p>
<pre class="source-code">
struct Config {
    optional&lt;Settings&gt; settings{};
    Config() {
        cout &lt;&lt; "Config loaded..." &lt;&lt; endl;
    }
    void changeSetting(string_view setting,
                       string_view value) {
        if (!settings)
            settings.emplace("settings.cfg");
        settings-&gt;setSetting(setting, value);
    }
};
int main() {
    Config cfg;
    cout &lt;&lt; "Application startup..." &lt;&lt; endl;
    this_thread::sleep_for(10s);
    cfg.changeSetting("Drive mode", "Sport");
    cfg.changeSetting("Gear label", "PRNDL");</pre> <p>We <a id="_idIndexMarker815"/>observe that the file is loaded after the startup has finished, as <span class="No-Break">we expected:</span></p>
<pre class="console">
Config loaded...
Application startup...
Loading settings: settings.cfg
Set setting: Drive mode to: Sport
Set setting: Gear label to: PRNDL
Removing settings</pre> <p>The <strong class="source-inline">optional</strong> class template is designed so that functions can return <em class="italic">nothing</em> when they fail, or a valid result when they succeed. We could also use it to handle objects whose construction is expensive. It also manages a value that may or may not be present at a given time. It is also readable, and its intent is clear. If an <strong class="source-inline">optional</strong> object contains a value, the value is guaranteed to be allocated as part of the <strong class="source-inline">optional</strong> object, and no dynamic memory allocation happens. Thus, an <strong class="source-inline">optional</strong> object models a <em class="italic">reservation</em> to an object, not a pointer. This is a key difference between <strong class="source-inline">optional</strong> and the smart pointer. Although using a smart pointer to handle large and complex objects <a id="_idIndexMarker816"/>might be a better idea, <strong class="source-inline">optional</strong> gives you the opportunity to construct an object at a later point in time when all parameters are known, if they weren’t known earlier in the execution. Both of them will work well in implementing <strong class="bold">lazy initialization</strong> – it’s a matter of <span class="No-Break">your preference.</span></p>
<p>Later in the chapter, we will return to smart pointers and their usability for managing shared memory. First, though, we will use the next section to present some useful mechanisms <span class="No-Break">for synchronization.</span></p>
<h1 id="_idParaDest-135"><a id="_idTextAnchor135"/>Learning about condition variables, read-write locks, and ranges in C++</h1>
<p>Let’s now start our discussion of synchronization primitives, a fundamental one of which is the <strong class="bold">condition variable</strong>. Its <a id="_idIndexMarker817"/>purpose is to allow multiple threads to remain blocked until an event occurs (i.e., a condition is satisfied). The implementation of <strong class="bold">condition variables</strong> requires an additional Boolean variable to indicate <a id="_idIndexMarker818"/>whether the condition is met or not, a <em class="italic">mutex</em> to serialize the access to the Boolean variable, and the condition <span class="No-Break">variable itself.</span></p>
<p>POSIX provides an interface for multiple use cases. Do you remember the producer-consumer example in <a href="B20833_07.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><em class="italic">, Using Shared Memory</em>? So, <strong class="source-inline">pthread_cond_timedwait()</strong> is used to block a thread for a given period of time. Or simply wait for a condition through <strong class="source-inline">pthread_cond_wait ()</strong> and signal with <strong class="source-inline">pthread_cond_signal()</strong> to one thread, or <strong class="source-inline">pthread_cond_broadcast()</strong> to all threads. Typically, the condition is checked periodically in the scope of a <span class="No-Break">mutex lock:</span></p>
<pre class="source-code">
...
pthread_cond_t  condition_variable;
pthread_mutex_t condition_lock;
...
pthread_cond_init(&amp;condition_variable, NULL);
...
void consume() {
    pthread_mutex_lock(&amp;condition_lock);
    while (shared_res == 0)
        pthread_cond_wait(&amp;condition_variable,
                          &amp;condition_lock);
    // Consume from shared_res;
    pthread_mutex_unlock(&amp;condition_lock);
}
void produce() {
    pthread_mutex_lock(&amp;condition_lock);
    if (shared_res == 0)
        pthread_cond_signal(&amp;condition_variable);
    // Produce for shared_res;
    pthread_mutex_unlock(&amp;condition_lock);
}
pthread_mutex_unlock(&amp;condition_lock);
...
pthread_cond_destroy(&amp;condition_variable);
...</pre> <p>If we level <a id="_idIndexMarker819"/>up the abstraction, as we did in <a href="B20833_07.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, C++ gives us access to the same technique, but a bit simpler and safer to use – we are guarded by the RAII principle. Let’s check the following snippet <span class="No-Break">in C++:</span></p>
<pre class="source-code">
...
#include &lt;condition_variable&gt;
mutex cv_mutex;
condition_variable cond_var;
...
void waiting() {
    cout &lt;&lt; "Waiting for work..." &lt;&lt; endl;
    unique_lock&lt;mutex&gt; lock(cv_mutex);
    cond_var.wait(lock);
    processing();
    cout &lt;&lt; "Work done." &lt;&lt; endl;
}
void done() {
    cout &lt;&lt; "Shared resource ready."  &lt;&lt; endl;
    cond_var.notify_one();
}
int main () {
    jthread t1(waiting); jthread t2(done);
    t1.join(); t2.join();
    return 0;
}</pre> <p>The output is <span class="No-Break">as follows:</span></p>
<pre class="console">
Waiting for work...
Shared resource ready.
Processing shared resource.
Work done.</pre> <p>In this <a id="_idIndexMarker820"/>form, the code is not correct. There is no condition to be checked, and the shared resource itself is missing. We are simply setting the stage for the following examples, which are a continuation of what we covered in <a href="B20833_07.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. But observe the use of a <strong class="bold">conditional variable</strong> by one thread to notify another that a resource is ready to be consumed (marker <strong class="source-inline">{4}</strong>), while the first one was waiting (marker <strong class="source-inline">{2}</strong>). As you see, we rely on a <em class="italic">mutex</em> to lock the shared resource in the scope (marker <strong class="source-inline">{1}</strong>) and<a id="_idIndexMarker821"/> the condition variable is triggered through it in order to continue to work (markers <strong class="source-inline">{2}</strong> and <strong class="source-inline">{3}</strong>). Thus, the CPU is not busy waiting, as there’s no endless loop to wait for a condition, freeing up access to the CPU for other processes and threads. But the thread remains blocked, because the <strong class="source-inline">wait()</strong> method of the <strong class="bold">condition variable</strong> unlocks <a id="_idIndexMarker822"/>the <strong class="bold">mutex</strong> and the thread is put to sleep atomically. When the thread is signaled, it will be resumed and will re-acquire the <strong class="bold">mutex</strong>. This is not always useful as you will see in the <span class="No-Break">next section.</span></p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor136"/>Cooperative cancellation through condition variables</h2>
<p>An <a id="_idIndexMarker823"/>important remark is that the condition variable should wait only with a condition and through a predicate. If not, the thread waiting on it will remain blocked. Do you remember the thread cancellation example from <a href="B20833_06.xhtml#_idTextAnchor086"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>? We used <strong class="source-inline">jthread</strong> and sent <em class="italic">stop notifications</em> between threads through the <strong class="source-inline">stop_token</strong> class and the <strong class="source-inline">stop_requested</strong> method. This mechanism is known <a id="_idIndexMarker824"/>as <strong class="bold">cooperative cancellation</strong>. The <strong class="source-inline">jthread</strong> technique is considered safe and easy to apply, but it might not be an option for your software design, or it might not be enough. Canceling threads could be directly related to waiting for an event. In that case, <strong class="bold">condition variables</strong> could come in handy as no endless loops or polling will be required. Revisiting the thread cancellation example from <a href="B20833_06.xhtml#_idTextAnchor086"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic">, Canceling Threads, Is This Really Possible?</em>, we have <span class="No-Break">the following:</span></p>
<pre class="source-code">
while (!token.stop_requested())</pre> <p>We are doing polling as the thread worker checks periodically whether the cancellation has been sent while doing something else in the meantime. But if the cancellation is the only thing we care about, then instead of polling, we could simply <em class="italic">subscribe</em> to the cancellation event using the <strong class="source-inline">stop_requested</strong> function. C++20 allows us to define a <strong class="source-inline">stop_callback</strong> function, so together with the condition variable and <strong class="source-inline">get_stop_token()</strong>, we can do the cooperative cancellation without <span class="No-Break">endless loops:</span></p>
<pre class="source-code">
#include &lt;condition_variable&gt;
#include &lt;iostream&gt;
#include &lt;mutex&gt;
#include &lt;thread&gt;
#include &lt;syncstream&gt;
using namespace std;
int main() {
    osyncstream{cout} &lt;&lt; "Main thread id: "
                      &lt;&lt; this_thread::get_id()
                      &lt;&lt; endl;</pre> <p>So, let’s finish<a id="_idIndexMarker825"/> the work from the example in the previous section and add a predicate to the <strong class="bold">condition variable</strong> in a <span class="No-Break">worker thread:</span></p>
<pre class="source-code">
    jthread worker{[](stop_token token) {
        mutex mutex;
        unique_lock lock(mutex);
        condition_variable_any().wait(lock, token,
            [&amp;token] { return token.stop_requested(); });
        osyncstream{cout} &lt;&lt; "Thread with id "
                          &lt;&lt; this_thread::get_id()
                          &lt;&lt; " is currently working."
                          &lt;&lt; endl;
    }};
    stop_callback callback(worker.get_stop_token(), [] {
    osyncstream{cout} &lt;&lt;"Stop callback executed by thread:"
                      &lt;&lt; this_thread::get_id()
                      &lt;&lt; endl;
    });
    auto stopper_func = [&amp;worker] {
        if (worker.request_stop())
            osyncstream{cout} &lt;&lt; "Stop request executed by
              thread: "
                              &lt;&lt; this_thread::get_id()
                              &lt;&lt; endl;
    };
    jthread stopper(stopper_func);
    stopper.join(); }</pre> <p>The <a id="_idIndexMarker826"/>output is <span class="No-Break">as follows:</span></p>
<pre class="console">
Main thread id: 140323902175040
Stop callback executed by thread: 140323893778176
Stop request executed by thread: 140323893778176
Thread with id 140323902170880 is currently working.</pre> <p>So, the worker thread remains in execution, but the <strong class="source-inline">stopper</strong> thread gets the stop token in the <strong class="source-inline">stop_callback</strong> function. When the stop is requested through the stopper function, the <strong class="bold">condition variable</strong> is signaled through <span class="No-Break">the token.</span></p>
<p>Now that we have another mechanism besides the <strong class="bold">semaphore</strong> to signal between threads, we can get the <strong class="bold">shared memory</strong> back in the game. Let’s see how this can work together with the condition variables and <span class="No-Break">smart pointers.</span></p>
<h2 id="_idParaDest-137"><a id="_idTextAnchor137"/>Combining smart pointers, condition variables, and shared memory</h2>
<p>We<a id="_idIndexMarker827"/> already explored the concept of <strong class="bold">shared memory</strong> in <a href="B20833_07.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><em class="italic">, Using Shared Memory</em>. Let’s <a id="_idIndexMarker828"/>use the knowledge from the earlier sections in this chapter to enhance the code safety through some C++ techniques. We’re simplifying the scenario a little bit. The full example can be found at <a href="https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209">https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209</a>. </p>
<p>We use the <strong class="source-inline">unique_ptr</strong> argument to provide a <span class="No-Break">specific deallocator:</span></p>
<pre class="source-code">
template&lt;typename T&gt;
struct mmap_deallocator {
    size_t m_size;
    mmap_deallocator(size_t size) : m_size{size} {}
    void operator()(T *ptr) const {
       munmap(ptr, m_size);
    }
};</pre> <p>We rely on <span class="No-Break">the following:</span></p>
<pre class="source-code">
unique_ptr&lt;T, mmap_deallocator&lt;T&gt;&gt;(obj, del);</pre> <p>As you <a id="_idIndexMarker829"/>see, we are also using templates in order to provide the possibility of storing any type of objects in the <strong class="bold">shared memory</strong>. It is easy to keep complex objects with large hierarchies and members in the heap, but storing and accessing their data is not trivial. Multiple <a id="_idIndexMarker830"/>processes will have access to those objects in the <strong class="bold">shared memory</strong>, but are the processes able to reference the memory behind the pointers? If the referenced memory is not in there or the shared virtual address space, then a memory access violation exception will be thrown. So, approach this <span class="No-Break">with caution.</span></p>
<p>We proceed with the next example. The already-known condition variable technique is used, but this time we add a real predicate to <span class="No-Break">wait for:</span></p>
<pre class="source-code">
mutex cv_mutex;
condition_variable cond_var;
bool work_done = false;</pre> <p>Our <strong class="source-inline">producer()</strong> method creates and maps the <strong class="bold">shared memory</strong> in the familiar fashion. But this time, instead of doing system calls to write, the shared resource is created directly in the <em class="italic">shared memory</em> (marker <strong class="source-inline">{1}</strong>). This technique is known as <strong class="bold">placement new</strong>. The memory is allocated a priori, and we construct an object into that memory. The standard <strong class="source-inline">new</strong> operator does these two operations together. Additionally, the object itself is wrapped by a <strong class="source-inline">unique_ptr</strong> object with the respective deallocator. As soon as the scope is left, that portion of the memory will be reset through the <strong class="source-inline">munmap()</strong> method. A <strong class="bold">condition variable</strong> is used to signal to the consumer that the data has <span class="No-Break">been prepared:</span></p>
<pre class="source-code">
template&lt;typename T, typename N&gt;
auto producer(T buffer, N size) {
    unique_lock&lt;mutex&gt; lock(cv_mutex);
    cond_var.wait(lock, [] { return work_done == false; });
    if (int fd =
            shm_open(SHM_ID, O_CREAT | O_RDWR, 0644);
                     fd != -1) {
        ftruncate(fd, size);</pre> <p>The <strong class="source-inline">shm</strong> region <a id="_idIndexMarker831"/>is created <a id="_idIndexMarker832"/>and sized. Now, let us use it to store <span class="No-Break">the data:</span></p>
<pre class="source-code">
        if (auto ptr =
                mmap(0, size,
                     PROT_RW, MAP_SHARED,
                     fd, 0); ptr != MAP_FAILED) {
            auto obj = new (ptr) T(buffer);
            auto del = mmap_deallocator&lt;T&gt;(size);
            work_done = true;
            lock.unlock();
            cond_var.notify_one();
            return unique_ptr&lt;T,
                mmap_deallocator&lt;T&gt;&gt;(obj, del);
        }
        else {
          const auto ecode{ make_error_code(errc{errno}) };
…
        }
    }
    else {
        const auto ecode{ make_error_code(errc{errno}) };
...
        throw exception;
    }
    // Some shm function failed.
    throw bad_alloc();
}</pre> <p>The<a id="_idIndexMarker833"/> consumer is implemented similarly, just waiting for <span class="No-Break">the following:</span></p>
<pre class="source-code">
cond_var.wait(lock, []{ return work_done == true; });</pre> <p>Finally, two <a id="_idIndexMarker834"/>threads are started and joined as a producer and consumer to provide the <span class="No-Break">following output:</span></p>
<pre class="console">
Sending: This is a testing message!
Receiving: This is a testing message!</pre> <p>Of course, the example could be much more complex, adding periodic production and consumption. We encourage you to try it out, just using another type of buffer – as you may remember, the <strong class="source-inline">string_view</strong> object is a constant. Be sure that the deallocator is correctly implemented and called. It is used to make the code safer and discard the possibility of <span class="No-Break">memory leaks.</span></p>
<p>As you may have observed, throughout our work in this book, we often want to access an object just to read it, without modifying its data. In that case, we don’t need full-scale locking, but something to make a difference between just reading data or modifying it. This technique is the <em class="italic">read-write lock</em> and we present it in the <span class="No-Break">following section.</span></p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor138"/>Implementing read-write locks and ranges with C++</h2>
<p>POSIX <a id="_idIndexMarker835"/>provides the read-write locks mechanism directly, while C++ hides it under different names – <strong class="source-inline">shared_mutex</strong> and <strong class="source-inline">shared_timed_mutex</strong>. Let’s see how it works traditionally in POSIX. We have the <em class="italic">read-write lock</em> object (<strong class="source-inline">rwlock</strong>) with the expected POSIX interface, where a thread could hold multiple concurrent read locks on it. The goal is to allow multiple readers to access the data until a thread decides to modify it. That thread locks the resource through a write lock. Most implementations favor the write lock over the read lock in order to avoid write starvation. Such behavior is not necessary when it comes to data races, but it definitely causes a minimal application <span class="No-Break">execution bottleneck.</span></p>
<p>This is especially true when dealing with large-scale systems’ data readers – for example, multiple read-only UIs. The C++ features again give us a simple and robust instrument for this task. Therefore, we will not devote time to studying examples of POSIX. We advise you to take a look yourself if interested, starting <span class="No-Break">with </span><span class="No-Break">https://linux.die.net/man/3/pthread_rwlock_rdlock</span><span class="No-Break">.</span></p>
<p>Proceeding with the C++ example, let’s consider the following scenario – a small number of threads want to modify a shared resource – a vector of numbers – and a larger number of threads wants to visualize the data. What we want to use here is <strong class="source-inline">shared_timed_mutex</strong>. It allows two levels of access: <em class="italic">exclusive</em>, where only one thread can own the mutex; and <em class="italic">shared</em>, where multiple threads share ownership of <span class="No-Break">the mutex.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Keep in mind <a id="_idIndexMarker836"/>that <a id="_idIndexMarker837"/>both the <strong class="source-inline">shared_timed_mutex</strong> and <strong class="source-inline">shared_mutex</strong> types are heavier than a simple <strong class="source-inline">mutex</strong>, although <strong class="source-inline">shared_mutex</strong> is considered more efficient on some platforms than <strong class="source-inline">shared_timed_mutex</strong>. You’re expected to use them when your read operations are really resource-hungry, slow, and frequent. For short operation bursts it would be preferable to stick with just the mutex. You’ll need to measure your resource usage specifically for your system in order to work out which <span class="No-Break">to choose.</span></p>
<p>The following example illustrates the usage of <strong class="source-inline">shared_mutex</strong>. We’ll also use the opportunity to<a id="_idIndexMarker838"/> present the <strong class="source-inline">ranges</strong> library in C++. This feature comes with C++20 and together with <strong class="source-inline">string_views</strong> provides an agile way to visualize, filter, transform, and slice C++ containers, among other things. Through this example, you’ll learn about some useful techniques with the <strong class="source-inline">ranges</strong> library, which will be explained along with the code. The full example can be found <span class="No-Break">at </span><span class="No-Break">https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209</span><span class="No-Break">.</span></p>
<p>Let’s have a <strong class="source-inline">Book</strong> struct with a shared resource – <strong class="source-inline">vector</strong> of books. We are going to use <strong class="source-inline">shared_mutex</strong> to handle <span class="No-Break">read-write locking:</span></p>
<pre class="source-code">
struct Book {
    string_view title;
    string_view author;
    uint32_t    year;
};
shared_mutex shresMutex;
vector&lt;Book&gt; shared_data =  {{"Harry Potter", ...</pre> <p>We<a id="_idIndexMarker839"/> implement the method for adding a book to the shared resource with the <strong class="source-inline">wr_</strong> prefix in order to distinguish its role from the other methods. We also execute a write lock on the resource (<span class="No-Break">marker </span><span class="No-Break"><strong class="source-inline">{1}</strong></span><span class="No-Break">):</span></p>
<pre class="source-code">
void wr_addNewBook(string_view title,
                   string_view author,
                   uint32_t year) {
    lock_guard&lt;shared_mutex&gt; writerLock(shresMutex); // {1}
    osyncstream{cout} &lt;&lt; "Add new book: " &lt;&lt; title &lt;&lt; endl;
    shared_data.emplace_back(Book {title, author, year});
    this_thread::sleep_for(500ms);
}</pre> <p>Now, we start with the implementation of multiple reader routines. They are marked with the <strong class="source-inline">rd_</strong> prefix, and each of them executes a read lock, meaning that the resource will be available for multiple readers at <span class="No-Break">a time:</span></p>
<pre class="source-code">
void rd_applyYearFilter(uint32_t yearKey) {
    auto year_filter =
        [yearKey](const auto&amp; book)
       { return book.year &lt; yearKey; };
    shared_lock&lt;shared_mutex&gt; readerLock(shresMutex);
    osyncstream{cout}
   &lt;&lt; "Apply year filter: " &lt;&lt; endl; // {2}
    for (const auto &amp;book : shared_data |
                            views::filter(year_filter))
        osyncstream{cout} &lt;&lt; book.title &lt;&lt; endl;
}</pre> <p>Observe <a id="_idIndexMarker840"/>the <strong class="source-inline">for</strong> loop after marker <strong class="source-inline">{2}</strong>. It not only iterates <a id="_idIndexMarker841"/>through the shared resource, but with the pipe (|) character we filter out portions of it, which is similar to piping and <strong class="source-inline">grep</strong> as covered in <a href="B20833_03.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, except here, it’s not a pipe. We are creating a <em class="italic">range view</em> through the pipe operator, thus providing additional logic to the iteration. In other words, we manipulate the view to the container. This approach can be used not only for <strong class="source-inline">vectors</strong>, but for the other C++ iterable objects as well. Why? <em class="italic">Ranges</em> are used to extend and generalize the algorithms with iterators so the code becomes tighter and less <span class="No-Break">error prone.</span></p>
<p>It’s easy to see the intention of the <em class="italic">range</em> here, too. Additionally, the <em class="italic">range view</em> is a lightweight object, similar to <strong class="source-inline">string_view</strong>. It represents an iterable sequence – the <em class="italic">range</em> itself, created on top of the containers’ iterators. It is based on the <em class="italic">Curiously Recurring Template Pattern</em>. Through the <em class="italic">range</em> interface, we can change the presentation of a container, present its values as transformed in a given manner, filter out values, split and combine sequences, present unique elements, shuffle elements, slide a window through the values, and so on. All of this is done via the simple syntax of already-implemented <em class="italic">range adapters</em>. In our example, <strong class="source-inline">rd_applyYearFilter</strong> has a <strong class="source-inline">for</strong> loop wherein books older than <strong class="source-inline">yearKey</strong> are filtered out. We could also print out the shared resource’s elements in <span class="No-Break">reverse order:</span></p>
<pre class="source-code">
void rd_Reversed() {
    for (const auto &amp;book : views::reverse(shared_data))
        osyncstream{cout} &lt;&lt; book.title &lt;&lt; endl; ...</pre> <p>We could even combine views, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
for (const auto &amp;book :
         views::reverse(shared_data) |
         views::filter([nameSizeKey](Book book)
              {return book.author.size() &lt; nameSizeKey;}))}</pre> <p>The <a id="_idIndexMarker842"/>previous<a id="_idIndexMarker843"/> snippet iterates through the elements in reverse order, but it also filters out those books where the length of the author’s name is longer than a given value. With the next snippet, we demonstrate how to simply drop a portion of the container <span class="No-Break">during iteration:</span></p>
<pre class="source-code">
for (const auto &amp;book :
   ranges::drop_view(shared_data, dropKey))
        osyncstream{cout} &lt;&lt; book.title &lt;&lt; endl;</pre> <p>If this is too generic, you could instead use a specific subrange, which will create a <strong class="source-inline">range</strong> object. The <strong class="source-inline">range</strong> object can be used like any other, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
auto const sub_res =
    ranges::subrange(shared_data.begin(),
                     shared_data.begin()+5);
    for (const auto&amp; book: sub_res){
        osyncstream{cout}
        &lt;&lt; book.title &lt;&lt; " " &lt;&lt; book.author
             &lt;&lt;  " " &lt;&lt; book.year &lt;&lt; endl;</pre> <p>With all of this complete, we create threads to execute all of these actions in a concurrent manner <a id="_idIndexMarker844"/>and see how the <em class="italic">read-write lock</em> manages them. Running the example will <a id="_idIndexMarker845"/>produce different output orders depending on the <span class="No-Break">thread’s scheduling:</span></p>
<pre class="source-code">
    thread yearFilter1(
        []{ rd_applyYearFilter(1990); });
    thread reversed(
        []{ rd_Reversed(); });
    thread reversed_and_filtered(
        []{ rd_ReversedFilteredByAuthorNameSize(8); });
    thread addBook1(
        []{ wr_addNewBook("Dune", "Herbert", 1965); });
    thread dropFirstElements(
        []{ rd_dropFirstN(1); });
    thread addBook2(
        []{ wr_addNewBook("Jaws", "Benchley", 1974); });
    thread yearFilter2(
        []{ rd_applyYearFilter(1970); });</pre> <p>The output is per the <a id="_idIndexMarker846"/>described <em class="italic">range views</em> (the following has been rearranged slightly for <span class="No-Break">easier reading):</span></p>
<pre class="console">
Apply reversed order:
It
East of Eden
Harry Potter
Drop first N elements:
East of Eden
It
Apply reversed order and filter by author name size:
It
Harry Potter
Apply year filter:
East of Eden
It
Add new book: Dune
Apply year filter:
East of Eden
Dune
Add new book: Jaws
Print subranged books in main thread:
East of Eden Steinbeck 1952
It King 1986</pre> <p>You have now <a id="_idIndexMarker847"/>learned about another combination of techniques with which you can scale up a system with multiple threads that handle presentation tasks. Let’s now take a step back and discuss the possible traps arising from concurrent execution that are not directly related to data races. We continue with <span class="No-Break">cache-friendly code.</span></p>
<h1 id="_idParaDest-139"><a id="_idTextAnchor139"/>Discussing multiprocessor systems – cache locality and cache friendliness in C++</h1>
<p>You probably<a id="_idIndexMarker848"/> recall <a href="B20833_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> at this point, where we discussed multi-thread and multi-core processors. The respective computational units were presented as processors. We also visualized the transport of instructions from the <strong class="bold">NVM</strong> (the disk) to the processors, through which we explained the creation of processes and <span class="No-Break"><em class="italic">software</em></span><span class="No-Break"> threads.</span></p>
<p>We want our code to be as performant as required. The most important aspect of getting the code to perform well is the choice of appropriate algorithms and data structures. With a bit of thought, you can try to squeeze the most out of every last CPU cycle. One of the most common examples of misusing algorithms is sorting a large, unordered array with bubble sort. So, make sure to learn your algorithms and data structures – together with the knowledge from this section and beyond, it will make you a really <span class="No-Break">powerful developer.</span></p>
<p>As you already know, the<a id="_idIndexMarker849"/> further we get from the RAM and the closer we get to the processor registers, the faster the operations and the smaller the memory capacity becomes. Each time the processor loads data from the RAM to the cache, it will either just sit and wait for that data to show up, or execute other non-related tasks. Thus, from the perspective of the current task, the CPU cycles are wasted. Of course, reaching 100% CPU utilization might be impossible, but we should at least be aware when it’s doing needless work. All of this might sound meaningless to you at this point, but concurrent systems will suffer if we <span class="No-Break">act carelessly.</span></p>
<p>The C++ language provides access to multiple tools for even better performance improvements, including <strong class="bold">prefetching mechanisms</strong> through<a id="_idIndexMarker850"/> hardware instructions <a id="_idIndexMarker851"/>and <strong class="bold">branch prediction</strong> <strong class="bold">optimization</strong>. Even without doing anything in particular, modern compilers and CPUs do a great job with these techniques. Still, we could improve this performance further by providing the right hints, options, and instructions. It’s also a good idea to be aware of the data in the cache to help reduce the time taken when accessing it. Remember that the cache is just a type of fast, temporary storage for data and instructions. So, we can use the features of C++ to our advantage when we treat the cache in a good manner, known<a id="_idIndexMarker852"/> as <strong class="bold">cache-friendly code</strong>. An important remark to note is the inverse of this statement – misusing C++ features will lead to poor cache performance, or at least not the best performance possible. You’ve probably already guessed that this is related to the system’s scale and the requirement for fast data access. Let’s discuss this further in the <span class="No-Break">next section.</span></p>
<h2 id="_idParaDest-140"><a id="_idTextAnchor140"/>Considering cache locality through cache-friendly code</h2>
<p>We <a id="_idIndexMarker853"/>mentioned <a id="_idIndexMarker854"/>the concept of cache-friendly code already, but what does it truly mean? First of all, you need to be aware of<a id="_idIndexMarker855"/> the <strong class="bold">cache locality</strong>. This means that our first goal is to make frequently used data easily accessible, thus the process will run faster. The second goal is to store in memory only what we need to store. Let’s keep the allocations small. For example, if you need to store a number of dice values (1-6), you don’t need unsigned long longs. Those values will fit in an unsigned <strong class="source-inline">int</strong> or even an <span class="No-Break">unsigned </span><span class="No-Break"><strong class="source-inline">char</strong></span><span class="No-Break">.</span></p>
<p>As a result, caching <a id="_idIndexMarker856"/>has become a major aspect of almost every system. Earlier in the book we mentioned that slower hardware, such as disks, sometimes has its own cache memory to reduce the time taken to access frequently opened files. OSs can cache frequently used data, for example, files, as chunks of<a id="_idIndexMarker857"/> virtual address space, thus improving performance even more. This is also known as <span class="No-Break"><strong class="bold">temporal locality</strong></span><span class="No-Break">.</span></p>
<p>Consider the following scenario: a piece of data is not found in the cache on the first try – this is known <a id="_idIndexMarker858"/>as a <strong class="bold">cache miss</strong>. Then it is looked up in the RAM, is found, and is loaded into the cache as<a id="_idIndexMarker859"/> one or <a id="_idIndexMarker860"/>multiple <strong class="bold">cache blocks</strong> or <strong class="bold">cache lines</strong>. Afterwards, if this data is requested a number of subsequent times and is still found in the cache, known <a id="_idIndexMarker861"/>as a<strong class="bold"> cache hit</strong>, it will remain in the cache and guarantee faster access, or at least faster than the first <strong class="bold">cache miss</strong>. You can observe this in the <span class="No-Break">following diagram:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<img alt="Figure 9.2 – Representation of temporal locality on the hardware level" height="470" src="image/Figure_9.2_B20833.jpg" width="1158"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Representation of temporal locality on the hardware level</p>
<p>As we mentioned with<a id="_idIndexMarker862"/> the <strong class="bold">prefetching mechanisms</strong> earlier, it’s a known fact that having an object with <a id="_idIndexMarker863"/>multiple <strong class="bold">cache hits</strong> means that the data around it might also be referenced soon. This causes the processor to <em class="italic">request</em> or <em class="italic">prefetch</em> that additional nearby data from the RAM and load it a priori, so it will be there in the cache when it is eventually needed. This causes <strong class="bold">spatial locality</strong>, meaning<a id="_idIndexMarker864"/> accessing  nearby memory and benefiting from the fact that caching is done in chunks, known as <strong class="bold">cache lines, t</strong>hus <a id="_idIndexMarker865"/>paying for a single transfer and using several bytes of memory. The prefetching technique assumes that the code already has <strong class="bold">spatial locality</strong> in order to <span class="No-Break">improve performance.</span></p>
<p>Both locality <a id="_idIndexMarker866"/>principles are based on assumptions. But code branching requires good design. The simpler the branch tree, the simpler to predict. Again, you need to consider carefully the data structures and algorithms to be used. You also need to aim at contiguous memory access and reduce the code to simple loops and small functions; for example, switching from using linked lists to arrays or matrices. For small-sized objects, the <strong class="source-inline">std::vector</strong> container is still the optimal choice. Additionally, we ideally seek a data structure object that can fit into one <strong class="bold">cache line</strong> – but sometimes this is just not possible because of the <span class="No-Break">application’s requirements.</span></p>
<p>Our process should access the data in contiguous blocks, where each one has the size of a cache line (typically 64 bytes but depends on the system). But if we want to do parallel evaluations, then it would be preferable for each CPU core (processor) to handle data in different <strong class="bold">cache lines</strong> from other cores’ data. If not, the cache hardware will have to move data back and forth between cores and the CPU will waste time on meaningless work again and the performance will worsen, instead of being improved. This term is known as <strong class="bold">false sharing</strong>, which we’ll now have a look at in the <span class="No-Break">following section.</span></p>
<h2 id="_idParaDest-141"><a id="_idTextAnchor141"/>A glance at false sharing</h2>
<p>As a rule, small pieces <a id="_idIndexMarker867"/>of data will be put together in a single <strong class="bold">cache line</strong> unless the programmer instructs otherwise, as we will see in the following examples. This is the way processors work in order to keep latency low – they handle one cache line for each core at a time. Even if it’s not full, the <strong class="bold">cache line</strong>’s size will be allocated as the smallest possible block for the CPU to handle. As mentioned earlier, if the data in that <strong class="bold">cache line</strong> is requested by two or more threads independently, then this will slow down the <span class="No-Break">multi-threaded execution.</span></p>
<p>Dealing with the effects of <strong class="bold">false sharing</strong> means getting predictability. Just as code branching can be predicted, so can the system programmer predict if an object is of the size of a cache line, and thus each separate object can reside in its own memory block. In addition, all computations can happen in the local scope and the shared data modifications take place at the end of a given procedure. Of course, such activities will lead to the wasting of resources at some point, but it’s a matter of design and preferences. Nowadays, we <a id="_idIndexMarker868"/>can use compiler optimizations to improve this predictability and performance, too, but we shouldn’t always rely on this. Let’s first check the size of our <span class="No-Break">cache line:</span></p>
<pre class="source-code">
#include &lt;iostream&gt;
#include &lt;new&gt;
using std::hardware_destructive_interference_size;
int main() {
    std::cout &lt;&lt; "L1 Cache Line size: "
        &lt;&lt; hardware_destructive_interference_size
        &lt;&lt; " bytes";
    return 0;
}</pre> <p>The expected output is <span class="No-Break">as follows:</span></p>
<pre class="console">
L1 Cache Line size: 64 bytes</pre> <p>Now that we know how to get the <strong class="bold">cache line</strong>’s size, we are able to align our objects in such a way that no <strong class="bold">false sharing</strong> occurs. In <a href="B20833_07.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, we used <strong class="source-inline">std::atomic</strong> to guarantee a single modifier to a shared resource, but we also emphasized that this is not the full picture. Let’s enrich the previous example with three <span class="No-Break">atomic variables:</span></p>
<pre class="source-code">
    cout &lt;&lt; "L1 Cache Line size: "
         &lt;&lt; hardware_constructive_interference_size
         &lt;&lt; " bytes" &lt;&lt; endl;
    atomic&lt;uint32_t&gt; a_var1;
    atomic&lt;uint32_t&gt; a_var2;
    atomic&lt;uint32_t&gt; a_var3;</pre> <p>Printing the addresses out gives <span class="No-Break">the following:</span></p>
<pre class="source-code">
       cout &lt;&lt; "The atomic var size is: " &lt;&lt; sizeof(a_var1)
            &lt;&lt; " and its address are: \n"
            &lt;&lt; &amp;a_var1 &lt;&lt; endl
            &lt;&lt; &amp;a_var2 &lt;&lt; endl
            &lt;&lt; &amp;a_var3 &lt;&lt; endl;
        ...</pre> <p>The output<a id="_idIndexMarker869"/> is <span class="No-Break">as follows:</span></p>
<pre class="console">
L1 Cache Line size: 64 bytes
The atomic var size is: 4 and the addresses are:
0x7ffeb0a11c7c
0x7ffeb0a11c78
0x7ffeb0a11c74</pre> <p>This means that even when we have atomic variables, they can be fitted into a single <strong class="bold">cache line </strong>with high, albeit system-specific, probability. So, even if they are responsible for handling different shared resources, the hardware threads (or cores) will not be able to write in parallel due to the back-and-forth activity in the cache hardware. To keep the cache in<a id="_idIndexMarker870"/> line, the CPU implements different <strong class="bold">cache coherency protocols</strong>, including <strong class="bold">MESI</strong>, <strong class="bold">MESIF</strong>, and <strong class="bold">MOESI</strong>. None of them allow <a id="_idIndexMarker871"/>multiple cores to<a id="_idIndexMarker872"/> modify one <strong class="bold">cache line</strong> in parallel, though. The <strong class="bold">cache line</strong> can only be occupied by one core. Luckily, C++20 provides <strong class="source-inline">atomic_ref&lt;T&gt;::required_alignment</strong>, which allows the programmer to align atomics as per the current cache line size, thus keeping them well apart. Let’s apply it for all atomics <span class="No-Break">as follows:</span></p>
<pre class="source-code">
    alignas(hardware_destructive_interference_size)
        atomic&lt;uint32_t&gt; a_var1;</pre> <p>The output is <span class="No-Break">as follows:</span></p>
<pre class="console">
L1 Cache Line size: 64 bytes
The atomic var size is: 4 and the addresses are:
0x7ffc3ac0af40
0x7ffc3ac0af00
0x7ffc3ac0aec0</pre> <p>In the preceding snippet, you <a id="_idIndexMarker873"/>can see that the differences in the addresses are as expected and the variables are well aligned, which was always the system programmer’s responsibility. Now, let’s apply the <strong class="source-inline">increment()</strong> method that you might remember from <a href="B20833_07.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">:</span></p>
<pre class="source-code">
void increment(std::atomic&lt;uint32_t&gt;&amp; shared_res) {
    for(int I = 0; i &lt; 100000; ++i) {shared_res++;}
}</pre> <p>We increment an atomic resource, and as covered in <a href="B20833_08.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, we know how to measure the duration of a procedure. So, we can analyze the performance for the next four scenarios. One remark – if you feel so inclined, you could play with the compiler optimization levels to spot the difference in the following values, as we are not using any of the optimization flags. The full code example could be found at <a href="https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209">https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209</a>. Our scenarios are <span class="No-Break">as follows:</span></p>
<ul>
<li>A single-threaded application, calling <strong class="source-inline">increment()</strong> 3 times, doing 300,000 increments of an atomic variable, which takes <span class="No-Break">2,744 microseconds</span></li>
<li>Direct sharing with one atomic variable, incremented 100,000 times by each of 3 threads in parallel, taking <span class="No-Break">5,796 microseconds</span></li>
<li>False sharing with three unaligned atomic variables, incremented 100,000 times by each of the 3 threads in parallel, taking <span class="No-Break">3,545 microseconds</span></li>
<li>No sharing with three aligned atomic variables, incremented 100,000 times by each of 3 threads in parallel, taking <span class="No-Break">1,044 microseconds</span></li>
</ul>
<p>As we are not using a benchmarking tool, we cannot measure the number of cache misses or hits. We simply do <span class="No-Break">the following:</span></p>
<pre class="source-code">
    ...
    auto start = chrono::steady_clock::now();
    alignas(hardware_destructive_interference_size)
        atomic&lt;uint32_t&gt; a_var1 = 0;
    alignas(hardware_destructive_interference_size)
        atomic&lt;uint32_t&gt; a_var2 = 0;
    alignas(hardware_destructive_interference_size)
        atomic&lt;uint32_t&gt; a_var3 = 0;
    jthread t1([&amp;]() {increment(a_var1);});
    jthread t2([&amp;]() {increment(a_var2);});
    jthread t3([&amp;]() {increment(a_var3);});
    t1.join();
    t2.join();
    t3.join();
    auto end = chrono::steady_clock::now();
    ...</pre> <p>The <strong class="bold">no-sharing</strong> work <a id="_idIndexMarker874"/>is presented in the <span class="No-Break">following diagram:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<img alt="Figure 9.3 – Representation of no-sharing (correct sharing) of data on multiple cores/threads" height="548" src="image/Figure_9.3_B20833.jpg" width="1079"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Representation of no-sharing (correct sharing) of data on multiple cores/threads</p>
<p class="callout-heading">Important note</p>
<p class="callout">It’s obvious that we either have to align our atomic resources before we modify them in parallel, or use single-threaded applications for small procedures. The time metric could differ, depending on the system and the compiler optimization flags. Keep in mind that these speed-ups are great when you get the best out of your hardware, but going into so much detail might also lead to complex code, harder debugging, and time wasted on maintenance. It’s a <span class="No-Break">balancing act.</span></p>
<p>False sharing happens <a id="_idIndexMarker875"/>during multi-threading and can be fixed if the shared object is fitted into one cache line. But what happens if the object is larger than one cache line <span class="No-Break">in size?</span></p>
<h2 id="_idParaDest-142"><a id="_idTextAnchor142"/>Sharing resources larger than a cache line in C++</h2>
<p>The analysis here is relatively simple, as it is not so dependent on the language. Large objects, representing large data structures, are just... large. They don’t fit into single <strong class="bold">cache lines</strong> and therefore they are not <strong class="bold">cache friendly</strong> by nature. Data-oriented design deals with this issue. For example, you could think about using smaller objects or share only small parts of them for parallel work. Additionally, it is good to think about optimizations in algorithms. Making them linear leads to better <strong class="bold">branch predictions</strong>. This <a id="_idIndexMarker876"/>means making conditional statements depend on predictable, not random, data. Complex conditional statements can be replaced with arithmetic solutions and templates, or chained differently, so it is easier for the CPU to predict which branch has a higher probability of occurring. Such operations, again, could lead to unreadable code and complex debugging, so proceed with them only when the code is not fast enough for <span class="No-Break">your requirements.</span></p>
<p>As <strong class="bold">branch misprediction</strong> could<a id="_idIndexMarker877"/> be expensive and remain well hidden, another proposal is the<a id="_idIndexMarker878"/> so-called <strong class="bold">conditional move</strong>. It is not based on predictions, but on data. The data dependencies include both <em class="italic">condition true</em> and <em class="italic">condition false</em> cases. After an instruction that conditionally moves data from one register to another, the contents of the second depend on both their previous values and the values from the first register. As mentioned, well-designed branching allows better performance. But data dependencies require one or two CPU cycles to arrive, sometimes making them a safer bet. A probable trap is when the condition is such that the value taken from the memory is not assigned to the register – then it’s just meaningless waiting. Luckily for the system programmer, the <strong class="bold">conditional move</strong> instructions in the instruction sets are typically <span class="No-Break">close register-wise.</span></p>
<p><strong class="bold">Cache unfriendliness</strong> is <a id="_idIndexMarker879"/>something you must consider when using excessively complex object designs or design patterns that spread the data around the memory. That doesn’t mean you shouldn’t think about improvements. If you rely on C++, the simplest and the most useful thing to apply quickly is to use contiguous containers in the code, such as <strong class="source-inline">std::array</strong> and <strong class="source-inline">std::vector</strong>. Yes, the vector could be resized, but it’s still cache friendly, as the elements are next to each other in the memory. Of course, if you have to reallocate the vector due to constant resizing, then probably it’s not the data structure you need. You could consider the <strong class="source-inline">std::deque</strong> container, which is efficient for modifications in the middle of the collection, or <strong class="source-inline">std::list</strong> as an alternative, which is a linked list and is not cache friendly <span class="No-Break">at all.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Depending on the system, many reallocations (construction and destruction) of contiguous memory blocks could cause memory fragmentation. This can happen due to software algorithms for memory management, language standards, OSs, drivers, devices, and so on. It is hard to predict it until it happens. It might take a good portion of non-stop execution time for the memory allocations to start failing. There could be enough free space in the sum of the free memory blocks in the RAM, but not a single block big enough to hold the currently reallocated or created contiguous block. Excessive fragmentation could lead to poor performance and even denial <span class="No-Break">of service.</span></p>
<p>A final remark<a id="_idIndexMarker880"/> on the topic is that there are many articles discussing optimal ways of using C++’s algorithms and containers efficiently. It deserves a book on its own and most of the time is very CPU specific – or at least when you get to the absolute performance. For example, the <strong class="bold">conditional moves</strong> lead directly to assembly code, which we don’t have the opportunity to explore here. That said, the variety of solutions for different practical problems is enormous when it comes to algorithms and <span class="No-Break">data structures.</span></p>
<h1 id="_idParaDest-143"><a id="_idTextAnchor143"/>Revisiting shared resources through the C++ memory model via spinlock implementation</h1>
<p>We learned about atomic operations back in <a href="B20833_07.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. In this chapter, you learned that the placement of atomic variables in the cache is crucial as well. Originally, atomics and locks<a id="_idIndexMarker881"/> were introduced because of correctness when multiple threads want to enter the same critical section. Now, our investigation will continue a bit deeper. There’s one last piece of the puzzle of atomic operations. Examine the <span class="No-Break">following snippet:</span></p>
<pre class="console">
Thread 1: shrd_res++; T1: load value
                      T1: add 1
Thread 2: shrd_res++; T2: load value
                      T2: add 1
                      T2: store value
                      T1: store value</pre> <p>This was an example of a non-atomic operation. Even when we make it atomic, we still don’t have a <a id="_idIndexMarker882"/>word about the order of the instructions. Until now, we used the synchronization primitives to instruct the CPU about which section of instructions has to be taken as a unitary context. What we need now is to instruct the processor about the order of those instructions. We do this through C++’s <strong class="source-inline">memory_order</strong>, which is a part of the C++ standard <span class="No-Break">memory model.</span></p>
<h2 id="_idParaDest-144"><a id="_idTextAnchor144"/>Introducing the memory_order type in C++</h2>
<p>With<a id="_idIndexMarker883"/> the <strong class="source-inline">memory_order</strong> type, we specify how atomic and non-atomic memory accesses are ordered around an atomic operation. The atomic realization of the snippet from the preceding section and the example using read-write locks earlier in the chapter could both suffer from the same issue: two atomic operations are not atomic as a whole. The order of instructions inside the atomic scope will be kept, but not around it. This is usually done after optimization techniques in the CPU and the compiler. So, if there are many reader threads, the order in which we (and the threads) expect to observe changes could vary. Such an effect could appear even during single-threaded execution as the compiler might re-arrange instructions as allowed by the <span class="No-Break">memory model.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">We encourage you <a id="_idIndexMarker884"/>to check out the full information on <strong class="source-inline">memory_order</strong> <span class="No-Break">here: </span><span class="No-Break">https://en.cppreference.com/w/cpp/atomic/memory_order</span><span class="No-Break">.</span></p>
<p>An important remark is that the default behavior of all atomic operations in C++ applies sequentially consistent ordering. The defined memory orders in C++20 are <span class="No-Break">as follows:</span></p>
<ul>
<li>Relaxed ordering, tagged <span class="No-Break">like so:</span><pre class="source-code">
memory_order_relaxed = memory_order::relaxed;</pre><p class="list-inset">This ordering is the bare minimum. It is the cheapest option and provides no guarantees, except of the current operation’s atomicity. One example of this in action is the incrementation of the <strong class="source-inline">shared_ptr</strong> reference counter, as it needs to be atomic, but no ordering <span class="No-Break">is required.</span></p></li> <li>Release-acquire ordering, tagged <span class="No-Break">as follows:</span><pre class="source-code">
memory_order_acquire = memory_order::acquire;
memory_order_release = memory_order::release;
memory_order_acq_rel = memory_order::acq_rel;</pre><p class="list-inset">Reads and writes are prevented from reordering right after an atomic region when the release operation is in effect. The <strong class="source-inline">acquire</strong> operation is similar, but reordering is prohibited before the atomic region. The third model, <strong class="source-inline">acq_rel</strong>, is a combination of both. This model could really help in the creation of read-write locks, except there’s no locking going on. The decrementing of the <strong class="source-inline">shared_ptr</strong> reference count is done through this technique as it needs to be synchronized with <span class="No-Break">the destructor.</span></p></li> <li>Release-consume ordering, tagged <span class="No-Break">as follows:</span><pre class="source-code">
memory_order_consume = memory_order::consume;</pre><p class="list-inset">The<a id="_idIndexMarker885"/> <strong class="source-inline">consume</strong> operation’s requirements are still being revised to this day. It is designed to work as the <strong class="source-inline">acquire</strong> operation does, but only for specific data. That way, the compiler is more flexible in optimizing the code than the <strong class="source-inline">acquire</strong> operation. Obviously, getting the data dependencies right makes the code more complex, therefore this model is not widely used. You can see it when accessing rarely written concurrent data structures – configurations and settings, security policies, firewall rules, or publish-subscribe applications with pointer-mediated publication; the producer publishes a pointer through which the consumer can <span class="No-Break">access information.</span></p></li> <li>Sequentially consistent ordering, tagged <span class="No-Break">as follows:</span><pre class="source-code">
memory_order_seq_cst = memory_order::seq_cst;</pre><p class="list-inset">This is the exact opposite of the relaxed order. All operations in and around the atomic region follow a strict order. Neither instruction can cross the barrier imposed by the atomic operation. It is considered the most expensive model as all optimization opportunities are lost. Sequentially consistent ordering is helpful for multiple producer-multiple consumer applications, where all consumers <a id="_idIndexMarker886"/>must observe the actions of all producers occurring in an <span class="No-Break">exact order.</span></p></li> </ul>
<p>One famous example directly benefiting from the memory order is the <strong class="bold">spinlock</strong> mechanism. We will proceed to examine this in the <span class="No-Break">next section.</span></p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor145"/>Designing spinlocks for multiprocessor systems in C++</h2>
<p>Operating systems <a id="_idIndexMarker887"/>often use this technique as it’s very efficient for short-period operations, including the ability to escape <a id="_idIndexMarker888"/>rescheduling and context switching. But locks held for longer periods will be at risk of being interrupted by the OS scheduler. The <strong class="bold">spinlock</strong> means<a id="_idIndexMarker889"/> that a given thread will either acquire a lock or will wait <em class="italic">spinning</em> (in a loop) – checking the lock’s availability. We discussed a similar example of <em class="italic">busy waiting</em> earlier in the chapter when we<a id="_idIndexMarker890"/> presented <strong class="bold">cooperative cancellation</strong>. The risk here is that keeping the lock acquired for longer periods will put the system into <a id="_idIndexMarker891"/>a <strong class="bold">livelock</strong> state, as described in <a href="B20833_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. The thread holding the lock will not progress further by releasing it, and the other threads will remain <em class="italic">spinning</em> while trying to acquire the lock. C++ is well suited for the implementation of the spinlock as atomic operations can be configured in detail. In low-level programming, this approach is also known as test-and-set. Here’s <span class="No-Break">an example:</span></p>
<pre class="source-code">
struct SpinLock {
    atomic_bool state = false;
    void lock() {
        while (state.exchange(true,
                              std::memory_order_acquire){
            while (state.load(std::memory_order_relaxed))
           // Consider this_thread::yield()
                // for excessive iterations, which
                // go over a given threshold.
}
    void unlock() noexcept {
        state.store(false, std::memory_order_release); };</pre> <p>You’re probably wondering why we aren’t using the already-known synchronization techniques. Well, keep in mind that all memory order settings here cost only one CPU instruction. They are fast and simple, both software- and hardware-wise. You should limit your use of them to very short periods of time, though, since the CPU is prevented from doing a useful job for <span class="No-Break">another process.</span></p>
<p>An <a id="_idIndexMarker892"/>atomic Boolean is used to mark<a id="_idIndexMarker893"/> whether the state of <strong class="source-inline">SpinLock</strong> is locked or unlocked. The <strong class="source-inline">unlock()</strong> method is easy – when the critical section is released, the <strong class="source-inline">false</strong> value is set (<strong class="source-inline">store()</strong> is atomic) to the <strong class="source-inline">state</strong> member through the release order. All following read/write operations have to be ordered in an atomic manner. The <strong class="source-inline">lock()</strong> method firstly runs a loop, trying to access the critical section. The <strong class="source-inline">exchange()</strong> method will set <strong class="source-inline">state</strong> to <strong class="source-inline">true</strong> and will return the previous value, <strong class="source-inline">false</strong>, thus interrupting the loop. Logically, this is very similar to the semaphore <strong class="source-inline">P(S)</strong> and <strong class="source-inline">V(S)</strong> functions. The inner loop will execute the busy wait scenario without order limitations and without <a id="_idIndexMarker894"/>producing <span class="No-Break"><strong class="bold">cache misses</strong></span><span class="No-Break">.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">The <strong class="source-inline">store()</strong>, <strong class="source-inline">load()</strong>, and <strong class="source-inline">exchange()</strong> operations have <strong class="source-inline">memory_order</strong> requirements and a list of supported orders. Using additional and unexpected orders leads to undefined behavior and keeps the CPU busy without doing <span class="No-Break">useful work.</span></p>
<p>An advanced version of the <strong class="bold">spinlock</strong> is the<a id="_idIndexMarker895"/> ticket lock algorithm. In the same fashion as with queues, tickets are provided to the threads in a FIFO manner. That way, the order in which they enter a critical section is managed fairly. In contrast with spinlocks, starvation is avoided here. However, this mechanism does not scale well. First of all, there’s a greater number of instructions to read, test, and acquire the lock, as there are more instructions for managing the order. Secondly, as soon as the critical section is free for access, all threads must have their context loaded into the cache to determine whether they are allowed to acquire the lock and enter the <span class="No-Break">critical section.</span></p>
<p>C++ has an advantage here thanks to its low latency. The full example is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209"><span class="No-Break">https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209</span></a><span class="No-Break">.</span></p>
<p>First, we implement the <strong class="source-inline">TicketLock</strong> mechanism, providing the necessary <strong class="source-inline">lock()</strong> and <strong class="source-inline">unlock()</strong> methods. We use two helper member variables, <strong class="source-inline">serving</strong> and <strong class="source-inline">next</strong>. As you see, they are aligned to be in separate <strong class="bold">cache lines</strong> to avoid <strong class="bold">false sharing</strong>. Both the <strong class="source-inline">lock()</strong> and <strong class="source-inline">unlock()</strong> methods are implemented as in the <strong class="source-inline">SpinLock</strong> example. Additionally, an atomic increment is done through <strong class="source-inline">fetch_add()</strong>, allowing the<a id="_idIndexMarker896"/> lock to generate tickets. No read/write operations happen around it, so it is executed in a relaxed order. Instead of just <a id="_idIndexMarker897"/>setting the variable to <strong class="source-inline">false</strong> as with <strong class="source-inline">SpinLock</strong>, the <strong class="source-inline">unlock()</strong> method loads a ticket number value, again in a relaxed manner, and stores it as the currently <span class="No-Break">served thread:</span></p>
<pre class="source-code">
struct TicketLock {
    alignas(hardware_destructive_interference_size)
        atomic_size_t serving;
    alignas(hardware_destructive_interference_size)
        atomic_size_t next;</pre> <p>The methods for locking and unlocking of the <strong class="source-inline">TicketLock</strong> <span class="No-Break">algorithm follow:</span></p>
<pre class="source-code">
    void lock() {
        const auto ticket = next.fetch_add(1,
                                memory_order_relaxed);
        while (serving.load(memory_order_acquire) !=
               ticket);
    }
    void unlock() {
        serving.fetch_add(1, memory_order_release);
    }
};</pre> <p>Now, a global <strong class="source-inline">spinlock</strong> object of type <strong class="source-inline">TicketLock</strong> is created. We also create a <strong class="source-inline">vector</strong> that plays the role of a shared resource. The <strong class="source-inline">producer()</strong> and <strong class="source-inline">consumer()</strong> routines are as expected – the first will create data and the latter will consume it, including clearing the shared resource. As both operations will be carried out in parallel, the order of their execution is random. If you want instead to create a ping-pong-like behavior for this, <strong class="bold">conditional variables</strong> or <strong class="bold">semaphores</strong> could be used as<a id="_idIndexMarker898"/> signaling mechanisms. The current implementation is limited just to the purposes <a id="_idIndexMarker899"/>of<a id="_idIndexMarker900"/> the <span class="No-Break"><strong class="bold">ticket lock</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
TicketLock spinlock = {0};
vector&lt;string&gt; shared_res {};
void producer() {
    for(int i = 0; i &lt; 100; i ++) {
        osyncstream{cout} &lt;&lt; "Producing: " &lt;&lt; endl;
        spinlock.lock();
        shared_res.emplace_back("test1");
        shared_res.emplace_back("test2");
        for (const auto&amp; el : shared_res)
            osyncstream{cout} &lt;&lt; "p:" &lt;&lt; el &lt;&lt; endl;
        spinlock.unlock();
        this_thread::sleep_for(100ms);
    }
}</pre> <p>And the consumer is similar to what you’ve <span class="No-Break">already learned:</span></p>
<pre class="source-code">
void consumer() {
    for (int i = 0; i &lt; 100; i ++) {
         this_thread::sleep_for(100ms);
         osyncstream{cout} &lt;&lt; "Consuming: " &lt;&lt; endl;
         spinlock.lock();
         for (const auto&amp; el : shared_res)
             osyncstream{cout} &lt;&lt; "c:" &lt;&lt; el &lt;&lt; endl;</pre> <p>Remove the contents of <span class="No-Break">the vector:</span></p>
<pre class="source-code">
         shared_res.clear();
         spinlock.unlock();
         if (shared_res.empty())
             osyncstream{cout} &lt;&lt; "Consumed" &lt;&lt; endl;
     }
}</pre> <p>The output is <span class="No-Break">as follows:</span></p>
<pre class="console">
Producing:
p:test1
p:test2
Consuming:
c:test1
c:test2
...</pre> <p>The<a id="_idIndexMarker901"/> output shows that the production <a id="_idIndexMarker902"/>and the consumption routines are treated as a whole, although they are not called an equal number of times, which is expected. As mentioned previously, instead of pausing the threads for <strong class="source-inline">100ms</strong>, you could also modify the code by adding a <span class="No-Break"><strong class="bold">condition variable</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
void producer() {
    for(int i = 0; i &lt; 100; i ++) {
        cout &lt;&lt;"Producing:" &lt;&lt; endl;
        unique_lock&lt;mutex&gt; mtx(cv_mutex);
        cond_var.wait(mtx, []{ return work_done ==
                                      !work_done; });</pre> <p>Proceed with the expected <span class="No-Break">critical section:</span></p>
<pre class="source-code">
        spinlock.lock();
        shared_res.emplace_back"test1");
        shared_res.emplace_back"test2");
        for (const auto&amp; el : shared_res)
            cout &lt;&lt;"p" &lt;&lt; el &lt;&lt; endl;
        spinlock.unlock();
        work_done = !work_done;
    }
}</pre> <p>With<a id="_idIndexMarker903"/> all of these techniques combined – memory robustness, synchronization primitives, cache friendliness, and instruction <a id="_idIndexMarker904"/>ordering awareness – you have the instruments to really sharpen your code’s performance and tweak it to get the best performance on your specific system. We want to take this opportunity to remind you that such detailed optimizations could lead to unreadable code and hard debugging, so use them only <span class="No-Break">when required.</span></p>
<h1 id="_idParaDest-146"><a id="_idTextAnchor146"/>Summary</h1>
<p>In this chapter, we’ve gathered together the entire set of instruments required for optimal code performance with C++. You learned techniques on many different system and software levels, so it’s understandable if you want to take a breather now. It is true that it would be good to spend more time on some of what we covered, for example, <strong class="bold">branch predictions</strong> and <strong class="bold">cache friendliness</strong>, or to implement more algorithms through <strong class="bold">condition variables</strong> and memory order. We strongly encourage you to use this chapter as a step in the direction of system improvements and more <span class="No-Break">efficient work.</span></p>
<p>The next chapter is dedicated to one more significant improvement in C++’s features – <strong class="bold">coroutines</strong>. You will see that they are much lighter and, for some of the mechanisms discussed here, such as event waiting, they are much <span class="No-Break">more preferable.</span></p>
</div>
</div></body></html>