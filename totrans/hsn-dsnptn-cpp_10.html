<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-180"><a id="_idTextAnchor453"/><a id="_idTextAnchor454"/><a id="_idTextAnchor455"/><a id="_idTextAnchor456"/>10</h1>
<h1 id="_idParaDest-181"><a id="_idTextAnchor457"/>Local Buffer Optimization</h1>
<p>Not all design patterns are concerned with designing class hierarchies. For commonly occurring problems, a software design pattern is the most general and reusable solution, and, for those programming in C++, one of the most commonly occurring problems is inadequate performance. One of the most common causes of such poor performance is inefficient memory management. Patterns were developed to deal with these problems. In this chapter, we will explore one such pattern that addresses, in particular, the overhead of small, frequent memory allocations.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>What is the overhead of small memory allocations, and how can it be measured?</li>
<li>What is local buffer optimization, how does it improve performance, and how can the improvements be measured?</li>
<li>When can the local buffer optimization pattern be used effectively?</li>
<li>What are the possible downsides of, and restrictions on, the use of the local buffer optimization pattern?</li>
</ul>
<h1 id="_idParaDest-182"><a id="_idTextAnchor458"/>Technical requirements</h1>
<p>You will need the Google Benchmark library installed and configured, details for which can be found here: <a href="https://github.com/google/benchmark">https://github.com/google/benchmark</a> (see <a href="B19262_04.xhtml#_idTextAnchor152"><em class="italic">Chapter 4</em></a>, <em class="italic">Swap – From Simple to Subtle</em>, for installation instructions).</p>
<p>Example code can be found at the following link: <a href="https://github.com/PacktPublishing/Hands-On-Design-Patterns-with-CPP-Second-Edition/tree/main/Chapter10">https://github.com/PacktPublishing/Hands-On-Design-Patterns-with-CPP-Second-Edition/tree/main/Chapter10</a>.</p>
<h1 id="_idParaDest-183"><a id="_idTextAnchor459"/><a id="_idTextAnchor460"/><a id="_idTextAnchor461"/>The overhead of small memory allocations</h1>
<p>The local buffer <a id="_idIndexMarker545"/>optimization is just that - an optimization. It is a performance-oriented pattern, and we must, therefore, keep in mind the first rule of performance - never guess anything about performance. Performance, and the effect of any optimization, must be<a id="_idTextAnchor462"/> measured.<a id="_idTextAnchor463"/></p>
<h2 id="_idParaDest-184"><a id="_idTextAnchor464"/>The cost of memory allocations</h2>
<p>Since we are exploring the overhead of memory allocations and the ways to reduce it, the first question we must answer is how expensive a memory allocation is. After all, nobody wants <a id="_idIndexMarker546"/>to optimize something so fast that it needs no optimization. We can use Google Benchmark (or any other microbenchmark, if you prefer) to answer this question. The simplest benchmark to measure the cost of memory allocation might look like this:</p>
<pre class="source-code">
void BM_malloc(benchmark::State&amp; state) {
  for (auto _ : state) {
    void* p = malloc(64);
    benchmark::DoNotOptimize(p);
  }
  state.SetItemsProcessed(state.iterations());
}
BENCHMARK(BM_malloc_free);</pre>
<p>The <code>benchmark::DoNotOptimize</code> wrapper prevents the compiler from optimizing away the unused variable. Alas, this experiment is probably not going to end well; the microbenchmark library needs to run the test many times, often millions of times, to accumulate a sufficiently accurate average runtime. It is highly likely that the machine will run out of memory before the benchmark is complete. The fix is easy enough, we must also free the memory we allocated:</p>
<pre class="source-code">
// Example 01
void BM_malloc_free(benchmark::State&amp; state) {
  const size_t S = state.range(0);
  for (auto _ : state) {
    void* p = malloc(S);
    benchmark::DoNotOptimize(p); free(p);
  }
  state.SetItemsProcessed(state.iterations());
}
BENCHMARK(BM_malloc_free)-&gt;Arg(64);</pre>
<p>We must note that we now measure the cost of both allocation and deallocation, which is reflected in the changed name of the function. This is not an unreasonable change; any allocated <a id="_idIndexMarker547"/>memory will need to be deallocated sometime later, so the cost must be paid at some point. We have also changed the benchmark to be parameterized by the allocation size. If you run this benchmark, you should get something like this:</p>
<pre class="source-code">
Benchmark                 Time   Items per second
BM_malloc_free/64        19.2 ns 52.2041M/s</pre>
<p>This tells us that the allocation and deallocation of <code>64</code> bytes of memory cost about <code>19</code> nanoseconds on this particular machine, which adds up to 52 million allocations/deallocations per second. If you’re curious whether the <em class="italic">64 bytes</em> size is special in some way, you can change the size value in the argument of the benchmark, or run the benchmark for a whole range of sizes:</p>
<pre class="source-code">
void BM_malloc_free(benchmark::State&amp; state) {
  const size_t S = state.range(0);
  for (auto _ : state) {
    void* p = malloc(S);
    benchmark::DoNotOptimize(p); free(p);
  }
  state.SetItemsProcessed(state.iterations());
}
BENCHMARK(BM_malloc_free)-&gt;
  RangeMultiplier(2)-&gt;Range(32,   256);</pre>
<p>You might also note that, so far, we have measured the time it takes to make the very first memory allocation in the program since we have not allocated anything else. The C++ runtime <a id="_idIndexMarker548"/>system probably did some dynamic allocations at the startup of the program, but still, this is not a very realistic benchmark. We can make the measurement more relevant by reallocating some amount of memory:</p>
<pre class="source-code">
// Example 02
#define REPEAT2(x) x x
#define REPEAT4(x) REPEAT2(x) REPEAT2(x)
#define REPEAT8(x) REPEAT4(x) REPEAT4(x)
#define REPEAT16(x) REPEAT8(x) REPEAT8(x)
#define REPEAT32(x) REPEAT16(x) REPEAT16(x)
#define REPEAT(x) REPEAT32(x)
void BM_malloc_free(benchmark::State&amp; state) {
  const size_t S = state.range(0);
  const size_t N = state.range(1);
  std::vector&lt;void*&gt; v(N);
  for (size_t i = 0; i &lt; N; ++i) v[i] = malloc(<a id="_idTextAnchor465"/>S);
  for (auto _ : state) {
    REPEAT({
      void* p = malloc(S);
      benchmark::DoNotOptimize(p);
      free(p);
    });
  }
  state.SetItemsProcessed(32*state.iterations());
  for (size_t i = 0; i &lt; N; ++i) free(v[i]);
}
BENCHMARK(BM_malloc_free)-&gt;
  RangeMultiplier(2)-&gt;Ranges({{32, 256}, {1&lt;&lt;15, 1&lt;&lt;15}});</pre>
<p>Here, we make <code>N</code> calls to <code>malloc</code> before starting the benchmark. Further improvements can be achieved by varying the allocation size during the reallocations. We have also <a id="_idIndexMarker549"/>replicated the body of the benchmark loop <code>32</code> times (using the C preprocessor macro) to reduce the overhead of the loop itself on the measurement. The time reported by the benchmark is now the time it takes to do <code>32</code> allocations and deallocations, which is not very convenient, but the allocation rate remains valid, since we have accounted for the loop unrolling, and multiplied the number of iterations by <code>32</code> when setting the number of processed items (in Google Benchmark, an item is whatever you want it to be, and the number of items per second is reported at the end of the benchmark, so we have declared one allocation/deallocation to be an item).</p>
<p>Even with all these modifications and improvements, the final result is going to be pretty close to our initial measurement of <code>54</code> million allocations per second. This seems very fast, just <code>18</code> nanoseconds. Remember, however, that a modern CPU can do dozens of instructions in this time. As we are dealing with small allocations, it is highly likely that the processing time spent on each allocated memory fragment is also small, and the overhead of allocation is non-trivial. This, of course, represents guessing about performance and is something I warned you against, and so we will confirm this claim via direct experiments.</p>
<p>First, however, I want to show you another reason why small memory allocations are particularly inefficient. So far, we have explored the cost of memory allocations on only one thread. Today, most programs that have any performance requirements at all are concurrent, and C++ supports concurrency and multi-threading. Let’s take a look at how the cost of memory allocations changes when we do it on several threads at once:</p>
<pre class="source-code">
// Example 03
void BM_malloc_free(benchmark::State&amp; state) {
  const size_t S = state.range(0);
  const size_t N = state.range(1);
  std::vector&lt;void*&gt; v(N);
  for (size_t i = 0; i &lt; N; ++i) v[i] = malloc(S);
  for (auto _ : state) {
    REPE<a id="_idTextAnchor466"/>AT({
      void* p = malloc(S);
      benchmark::DoNotOptimize(p);
      free(p);
    });
  }
  state.SetItemsProcessed(32*state.iterations());
  for (size_t i = 0; i &lt; N; ++i) free(v[i]);
}
BENCHMARK(BM_malloc_free)-&gt;
  RangeMultiplier(2)-&gt;Ranges({{32, 256}, {1&lt;&lt;15, 1&lt;&lt;15}})
  -&gt;ThreadRange(1, 2);</pre>
<p>The result greatly <a id="_idIndexMarker550"/>depends on the hardware and the version of <code>malloc</code> used by the system. Also, on large machines with many CPUs, you can have many more than two threads.</p>
<p>Nonetheless, the overall trend should look something like this:</p>
<pre class="source-code">
Benchmark                          Time   Items per second
BM_malloc_free/32/32768/threads:1  778 ns 41.1468M/s
BM_malloc_free/32/32768/threads:2  657 ns 24.3749M/s
BM_malloc_free/32/32768/threads:4  328 ns 24.3854M/s
BM_malloc_free/32/32768/threads:8  242 ns 16.5146M/s</pre>
<p>This is quite dismal; the cost of allocations increased several times when we went from one thread to two (on a larger machine, a similar increase is going to happen, but probably with more than two threads). The system memory allocator appears to be the bane of effective <a id="_idIndexMarker551"/>concurrency. There are better allocators that can be used to replace the default <code>malloc()</code> allocator, but they have their own downsides. Plus, it would be better if our C++ program did not depend on a particular, non-standard, system library replacement f<a id="_idTextAnchor467"/>or its performance. We need a better way to allocate memory. Let’s have a look <a id="_idTextAnchor468"/>at it.</p>
<h1 id="_idParaDest-185"><a id="_idTextAnchor469"/>Introducing local buffer optimization</h1>
<p>The least amount of work a program can do to accomplish a certain task is no work at all. Free stuff <a id="_idIndexMarker552"/>is great. Similarly, the fastest way to allocate and deallocate memory is this - don’t. Local buffer optimization is a way to get something for nothing; in this case, to get some memory for no additional computin<a id="_idTextAnchor470"/><a id="_idTextAnchor471"/><a id="_idTextAnchor472"/>g cost.</p>
<h2 id="_idParaDest-186"><a id="_idTextAnchor473"/>The main idea</h2>
<p>To understand local buffer optimization, you have to remember that memory allocations do <a id="_idIndexMarker553"/>not happen in isolation. Usually, if a small amount of memory is needed, the allocated memory is used as a part of some data structure. For example, let’s consider a very simple character string:</p>
<pre class="source-code">
// Example 04
class simple_string {
  public:
  simple_string() = default;
  explicit simple_string(const char* s) : s_(strdup(s)) {}
  simple_string(const simple_string&amp; s)
    : s_(strdup(s.s_)) {}
  simple_string&amp; operator=(const char* s) {
    free(s_);
    s_ = strdup(s);
    return *this;
  }
  simple_string&amp; operator=(const simple_string&amp; s) {
    if (this == &amp;s) return *this;
    free(s_);
    s_ = strdup(s.s_);
    return *this;
  }
  bool operator==(const simple_string&amp; rhs) const {
    return strcmp(s_, rhs.s_) == 0;
  }
  ~simple_string() { free(s_); }
  private:
  char* s_ = nullptr;
};</pre>
<p>The string allocates <a id="_idIndexMarker554"/>its memory from <code>malloc()</code> via a <code>strdup()</code> call and returns it by calling <code>free()</code>. To be in any way useful, the string would need many more member functions, but these are sufficient for now to explore the overhead of memory allocation. Speaking of allocation, every time a string is constructed, copied, or assigned, an allocation happens. To be more precise, every time a string is constructed, an additional allocation happens; the string object itself has to be allocated somewhere, which may be on the stack for a local variable, or on the heap if the string is a part of some dynamically allocated data structure. In addition to that, an allocation for the string data happens, and the memory is always taken from <code>malloc()</code>.</p>
<p>This, then, is the idea of the local buffer optimization - why don’t we make the string object larger so it can contain its own data? That really would be getting something for nothing; the memory for the string object has to be allocated anyway, but the additional memory for the string data we would get at no extra cost. Of course, a string can be arbitrarily long, so we do not know in advance how much larger we need to make the string object to store any string the program will encounter. Even if we did, it would be a tremendous waste of memory to always allocate an object of that large size, even for very short<a id="_idTextAnchor474"/> strings.</p>
<p>We can, however, make an observation - the longer the string is, the longer it takes to process it (copy, search, convert, or whatever we need to do with it).</p>
<p>For very long strings, the cost of allocations is going to be small compared to the cost of processing. For short strings, on the other hand, the cost of the allocation could be significant. Therefore, the most performance benefit can be obtained by storing short strings in the <a id="_idIndexMarker555"/>object itself, while any string that is too long to fit in the object will be stored in allocated memory as before. This is, in a nutshell, local buffer <a id="_idIndexMarker556"/>optimization, which for strings is also known as <strong class="bold">short string optimization</strong>; the object (string) contains a local buffer of a certain size, and any string that fits into that buffer is stored directly inside the object:</p>
<pre class="source-code">
// Example 04
class small_string {
  public:
  small_string() = default;
  explicit small_string(const char* s) :
    s_((strlen(s) + 1 &lt; sizeof(buf_)) ? strcpy(buf_, s)
                                      : strdup(s)) {}
  small_string(const small_string&amp; s) :
    s_((s.s_ == s.buf_) ? strcpy(buf_, s.buf_)
                        : strdup(s.s_)) {}
  small_string&amp; operator=(const char* s) {
    if (s_ != buf_) free(s_);
    s_ = (strlen(s) + 1 &lt; sizeof(buf_)) ? strcpy(buf_, s)
                                        : strdup(s);
    return *this;
  }
  small_string&amp; operator=(const small_string&amp; s) {
    if (this == &amp;s) return *this;
    if (s_ != buf_) free(s_);
    s_ = (s.s_ == s.buf_) ? strcpy(buf_, s.buf_)
                          : strdup(s.s_);
    return *this;
  }
  bool operator==(const small_string&amp; rhs) const {
    return strcmp(s_, rhs.s_) == 0;
  }
  ~small_string() {
    if (s_ != buf_) free(s_);
  }
  private:
  char* s_ = nullptr;
  char b<a id="_idTextAnchor475"/>uf_[16];
};</pre>
<p>In the preceding <a id="_idIndexMarker557"/>code example, the buffer size is set statically at <code>16</code> characters, including the null character used to terminate the string. Any string that is longer than <code>16</code> is allocated from <code>malloc()</code>. When assigning or destroying a string object, we must check whether the allocation was done or the internal buffer was used, in order to appropriately<a id="_idTextAnchor476"/> release the memory used by<a id="_idTextAnchor477"/> the string.</p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor478"/>Effect of local buffer optimization</h2>
<p>How much faster is <code>small_string</code> compared to <code>simple_string</code>? That depends, of course, on what <a id="_idIndexMarker558"/>you need to do with it. Let’s start with just creating and deleting the strings. To avoid typing the same benchmark code twice, we can use the template benchmark, as follows:</p>
<pre class="source-code">
// Example 04
template &lt;typename T&gt;
void BM_string_create_short(benchmark::State&amp; state) {
  const char* s = "Simple string";
  for (auto _ : state) {
    REPEAT({
      T S(s);
      benchmark::DoNotOptimize(S);
    })
  }
  state.SetItemsProcessed(32*state.iterations());
}
BENCHMARK_TEMPLATE1(BM_string_create_short, simple_string);
BENCHMARK_TEMPLATE1(BM_string_create_short, small_string);</pre>
<p>The result is quite impressive:</p>
<pre class="source-code">
Benchmark                                Time Items per sec
BM_string_create_short&lt;simple_string&gt;     835 ns 38.34M/s
BM_string_create_short&lt;small_string&gt;     18.7 ns 1.71658G/s</pre>
<p>It gets even better when we try the same test on multiple threads:</p>
<pre class="source-code">
Benchmark                                Time Items per sec
BM_create&lt;simple_string&gt;/threads:2        743 ns 21.5644M/s
BM_create&lt;simple_string&gt;/threads:4        435 ns 18.4288M/s
BM_create&lt;small_string&gt;/threads:2        9.34 ns 1.71508G/s
BM_create&lt;small_string&gt;/threads:4        4.77 ns 1.67998G/s</pre>
<p>Regular string creation is slightly faster on two threads, but creating short strings is almost exactly twice as fast (and again twice as fast on four threads). Of course, this is pretty much the best-case scenario for small string optimization - firstly because all we do is create and delete strings, which is the very part we optimized, and secondly because the string is a local variable its memory is allocated as a part of the stack frame, so there is no additional allocation cost.</p>
<p>However, this is not an unreasonable case; after all, local variables are not rare at all, and if the string <a id="_idIndexMarker559"/>is a part of some larger data structure, the allocation cost for that structure has to be paid anyway, so allocating anything else at the same time and without additional cost is effectively free.</p>
<p>Nonetheless, it is unlikely that we only allocate the strings to immediately deallocate them, so we should consider the cost of other operations. We can expect similar improvements for copying or assigning strings, as long as they stay short, of course:</p>
<pre class="source-code">
template &lt;typename T&gt;
void BM_string_copy_short(benchmark::State&amp; state) {
  const T s("Simple string");
  for (auto _ : state) {
    REPEAT({
      T S(s);
      benchmark::DoNotOptimize(S);
    })
  }
  state.SetItemsProcessed(32*state.iterations());
}
template &lt;typename T&gt;
void BM_string_assign_short(benchmark::State&amp; state) {
  const T s("Simple string");
  T S;
  for (auto _ : state) {
    REPEAT({ benchmark::DoNotOptimize(S = s); })
  }
  state.SetItemsProcessed(32*state.iterations());
}
BENCHMARK_TEMPLATE1(BM_string_copy_short, simple_string);
BENCHMARK_TEMPLATE1(BM_string_copy_short, small_string);
BENCHMARK_TEMPLATE1(BM_string_assign_short, simple_string);
BENCHMARK_TEMPLATE1(BM_string_assign_short, small_string);</pre>
<p>Indeed, a similar <a id="_idIndexMarker560"/>dramatic performance gain is observed:</p>
<pre class="source-code">
Benchmark                                Time Items per sec
BM_string_copy_short&lt;simple_string&gt;       786 ns 40.725M/s
BM_string_copy_short&lt;small_string&gt;       53.5 ns 598.847M/s
BM_string_assign_short&lt;simple_string&gt;     770 ns 41.5977M/s
BM_string_assign_short&lt;small_string&gt;     46.9 ns 683.182M/s</pre>
<p>We are also likely to need to read the data in the strings at least once, to compare them or search for a specific string or character, or compute some derived value. We do not expect improvements of a similar scale for these operations, of course, since none of them involves any allocations or deallocations. You might ask why, then, should we expect any improvements at all?</p>
<p>Indeed, a simple test of string comparison, for example, shows no difference between the two versions of the string. In order to see any benefit, we have to create many string objects and compare them:</p>
<pre class="source-code">
template &lt;typename T&gt;
void BM_string_compare_short(benchmark::State&amp; state) {
  const size_t N = state.range(0);
  const T s("Simple string");
  std::vector&lt;T&gt; v1, v2;
  ... populate the vectors with strings ...
  for (auto _ : state) {
    for (size_t i = 0; i &lt; N; ++i) {
      benchmark::DoNotOptimize(v1[i] == v2[i]);
    }
  }
  state.SetItemsProcessed(N*state.iterations());
}
BENCHMARK_TEMPLATE1(BM_string_compare_short,
                    simple_string)-&gt;Arg(1&lt;&lt;22);
BENCHMARK_TEMPLATE1(BM_string_compare_short,
                    small_string)-&gt;Arg(1&lt;&lt;22);</pre>
<p>For small values of <code>N</code> (a small total number of strings), there won’t be any significant benefit from <a id="_idIndexMarker561"/>the optimization. But when we have to process many strings, comparing strings with the small string optimization can be approximately twice as fast:</p>
<pre class="source-code">
Benchmark                                Time Items per sec
BM_compare&lt;simple_string&gt;/4194304    30230749 ns 138.855M/s
BM_compare&lt;small_string&gt;/4194304     15062582 ns 278.684M/s</pre>
<p>Why is that happening, if there are no allocations at all? This experiment shows the second, very important, benefit of local buffer optimization - improved cache locality. The string object itself has to be accessed before the string data can be read; it contains the pointer to the data. For the regular string, accessing the string characters involves two memory accesses at different, generally unrelated addresses. If the total amount of data is large, then the second access, to the string data, is likely to miss the cache and wait for the <a id="_idIndexMarker562"/>data to be brought from the main memory. On the other hand, the optimized string keeps the data close to the string object, so that once the string itself is in the cache, so is the data. The reason that we need a sufficient amount of different strings to see this benefit is that with few strings, all string objects and their data can reside in the cache permanently. Only when the total size of the strings exceeds the size of the cache will the performance benefits manifest themselves. Now, let’s dive deeper into some addition<a id="_idTextAnchor479"/><a id="_idTextAnchor480"/><a id="_idTextAnchor481"/>al optimizations.</p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor482"/>Additional optimizations</h2>
<p>The <code>small_string</code> class we have implemented has an obvious inefficiency - when the string is <a id="_idIndexMarker563"/>stored in the local buffer, we do not really need the pointer to the data. We know exactly where the data is, in the local buffer. We do need to know, somehow, whether the data is in the local buffer or in the externally allocated memory, but we don’t need to use 8 bytes (on a 64-bit machine) just to store that. Of course, we still need the pointer for storing longer strings, but we could reuse that memory for the buffer when the string is short:</p>
<pre class="source-code">
// Example 05
class small_string {
  ...
  private:
  union {
    char* s_;
    struct {
      char buf[15];
      char tag;
    } b_;
  };
};</pre>
<p>Here, we use the last byte as a <code>tag</code> to indicate whether the string is stored locally (<code>tag == 0</code>) or in a separate allocation (<code>tag == 1</code>). Note that the total buffer size is still <code>16</code> characters, <code>15</code> for the string itself and <code>1</code> for the tag, which also doubles at the trailing zero if the string needs all <code>16</code> bytes (this is why we have to use <code>tag == 0</code> to indicate local storage, as it would cost us an extra byte to do otherwise). The pointer is overlaid in memory with the first <code>8</code> bytes of the character buffer. In this example, we have chosen to <a id="_idIndexMarker564"/>optimize the total memory occupied by the string; this string still has a 16-character local buffer, just like the previous version, but the object itself is now only 16 bytes, not 24. If we were willing to keep the object size the same, we could have used a larger buffer and stored longer strings locally. The benefit of the small string optimization does, generally, diminish as the strings become longer. The optimal crossover point from local to remote str<a id="_idTextAnchor483"/>ings depends on the particular application and must of course be determined by benc<a id="_idTextAnchor484"/>hmark measurements.</p>
<h1 id="_idParaDest-189"><a id="_idTextAnchor485"/>Local buffer optimization beyond strings</h1>
<p>The local buffer <a id="_idIndexMarker565"/>optimization can be used effectively for much more than just short strings. In fact, any time a small dynamic allocation of a size that is determined at runtime is needed, this optimization should be considered. In this section, we will consider several s<a id="_idTextAnchor486"/><a id="_idTextAnchor487"/><a id="_idTextAnchor488"/>uch data structures.</p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor489"/>Small vector</h2>
<p>Another very <a id="_idIndexMarker566"/>common data structure that often benefits from local buffer optimization is vectors. Vectors are essentially dynamic contiguous arrays of data elements of the specified type (in this sense, a string is a vector of bytes, although null termination gives strings their own specifics). A basic vector, such as <code>std::vector</code> found in the C++ standard library, needs two data members, a data pointer and the data size:</p>
<pre class="source-code">
// Example 06
class simple_vector {
  public:
  simple_vector() = default;
  simple_vector(std::initializer_list&lt;int&gt; il) :
    n_(il.size()),
    p_(static_cast&lt;int*&gt;(malloc(sizeof(int)*n_)))
  {
    int* p = p_;
    for (auto x : il) *p++ = x;
  }
  ~simple_vector() { free(p_); }
  size_t size() const { return n_; }
  private:
  size_t n_ = 0;
  int* p_ = nullptr;
};</pre>
<p>Vectors are <a id="_idIndexMarker567"/>usually templates, like the standard <code>std::vector</code>, but we have simplified this example to show a vector of integers (converting this vector class to a template is left as an exercise for you, and does not in any way alter the application of the local buffer optimization pattern). We can apply <em class="italic">small vector optimization</em> and store the vector data in the body of the vector object as long as it is small enough:</p>
<pre class="source-code">
// Example 06
class small_vector {
  public:
  small_vector() = default;
  small_vector(std::initializer_list&lt;int&gt; il) :
    n_(il.size()), p_((n_ &lt; sizeof(buf_)/sizeof(buf_[0]))
      ? buf_ : static_cast&lt;int*&gt;(malloc(sizeof(int)*n_)))
  {
    int* p = p_;
    for (auto x : il) *p++ = x;
  }
  ~small_vector() {
    if (p_ != buf_) free(p_);
  }
  private:
  size_t n_ = nullptr;
  int* p_ = nullptr;
  int buf_[16];
};</pre>
<p>We can further optimize the vector in a similar manner to the string and overlay the local buffer <a id="_idIndexMarker568"/>with the pointer. We cannot use the last byte as a <code>tag</code>, as we did before, since any element of the vector can have any value, and the value of zero is, in general, not special. However, we need to store the size of the vector anyway, so we can use it at any time to determine whether the local buffer is used or not. We can take further advantage of the fact that if the local buffer optimization is used, the size of the vector cannot be very large, so we do not need a field of the <code>size_t</code> type to store it:</p>
<pre class="source-code">
// Example 07
class small_vector {
  public:
  small_vector() = default;
  small_vector(std::initializer_list&lt;int&gt; il) {
    int* p;
    if (il.size() &lt; sizeof(short_.buf)/
                    sizeof(short_.buf[0])) {
      short_.n = il.size();
      p = short_.buf;
    } else {
      short_.n = UCHAR_MAX;
      long_.n = il.size();
      p = long_.p = static_cast&lt;int*&gt;(
        malloc(sizeof(int)*long_.n));
    }
    for (auto x : il) *p++ = x;
  }
  ~small_vector() {
    if (short_.n == UCHAR_MAX) free(long_.p);
  }
  private:
  union {
    struct {
      int buf[15];
      unsigned char n;
    } short_ = { {}, '\0' };
    struct {
      size_t n;
      int* <a id="_idTextAnchor490"/>p;
    } long_;
  };
};</pre>
<p>Here, we store the vector size either in <code>size_t</code> <code>long_.n</code> or in <code>unsigned</code> <code>char</code> <code>short_.n</code>, depending on whether or not the local buffer is used. A remote buffer is indicated by storing <code>UCHAR_MAX</code> (that is, 255) in the short size. Since this value is larger than the size of the local buffer, this <code>tag</code> is unambiguous (were the local buffer increased to store more than 255 elements, the type of <code>short_.n</code> would need to be changed to a longer integer).</p>
<p>We can <a id="_idIndexMarker569"/>measure the performance gains from small vector optimization using a benchmark similar to the one we used for the strings. Depending on the actual size of the vector, gains of about 10x can be expected in creating and copying the vectors, and more if the benchmark runs on multiple threads. Of course, other data structur<a id="_idTextAnchor491"/>es can be optimized in a similar manner when they store small amounts of dynamically allocated data. The optimizations of these data structures are fundamentally similar, but there is one noteworthy variant we should highlight.</p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor492"/>Small queue</h2>
<p>The small vector we have just seen uses a local buffer to store a small array of vector elements. This is the standard way of optimizing data structures that store a variable number of <a id="_idIndexMarker570"/>elements when this number is often small. A particular version of this optimization is used for data structures based on a queue, where the buffer grows on one end and is consumed on the other end. If there are only a few elements in the queue at any time, the queue can be optimized with a local buffer. The technique commonly employed here is a <code>buffer[N]</code>, so, as the elements are added to the end of the queue, we are going to reach the end of the array. By then some elements were taken from the queue, so the first few elements of the array are no longer used. When we reach the end of the array, the next enqueued value goes into the first element of the array, <code>buffer[0]</code>. The array is treated like a ring, after the element <code>buffer[N-1]</code> comes the element <code>buffer[0]</code> (hence another name for this <a id="_idIndexMarker572"/>technique, a <em class="italic">ring buffer</em>).</p>
<p>The circular buffer technique is commonly used for queues and other data structures where data is added and removed many times while the total volume of data stored at any given <a id="_idIndexMarker573"/>time is limited. Here is one possible implementation of a circular buffer queue:</p>
<pre class="source-code">
// Example 08
class small_queue {
  public:
  bool push(int i) {
    if (front_ - tail_ &gt; buf_size_) return false;
    buf_[(++front_) &amp; (buf_size_ - 1)] = i;
    return true;
  }
  int front() const {
    return buf_[tail_ &amp; (buf_size_ - 1)];
  }
  void pop() { ++tail_; }
  size_t size() const { return front_ - tail_; }
  bool empty() const { return front_ == tail_; }
  private:
  static constexpr size_t buf_size_ = 16;
  static_assert((buf_size_ &amp; (buf_size_ - 1)) == 0,
                "Buffer size must be a power of 2");
  int buf_[buf_size_];
  size_t front_ = 0;
  size_t tail_ = 0;
};</pre>
<p>In this example, we support only the local buffer; if the number of elements the queue must hold exceeds the size of the buffer, the call to <code>push()</code> returns <code>false</code>. We could have switched to a heap-allocated array instead, just like we did in <code>Example 07</code> for <code>small_vector</code>.</p>
<p>In this implementation, we increment the indices <code>front_</code> and <code>tail_</code> without bounds, but when these values are used as indices into the local buffer, we take the index value modulo buffer size. Of note is the optimization that is very common when dealing with circular buffers: the size of the buffer is a power of two (enforced by the assert). This allows us to <a id="_idIndexMarker574"/>replace the general (and slow) modulo calculation such as <code>front_ % buf_size_</code> by much faster bitwise arithmetic. We do not have to worry about integer overflow either: even if we call <code>push()</code> and <code>pop()</code> more than <code>2^64</code> times, the unsigned integer index values will overflow and go back to zero and the queue continues to work fine.</p>
<p>As expected, the queue with the local buffer optimization far outperforms a general queue such as <code>std::queue&lt;int&gt;</code> (as long as the optimization remains valid and the number of elements in the queue is small, of course):</p>
<pre class="source-code">
Benchmark                         Time   items_per_second
BM_queue&lt;std::queue&lt;int&gt;&gt;       472 ns          67.787M/s
BM_queue&lt;small_queue&gt;           100 ns         319.857M/s</pre>
<p>The circular local buffer can be used very effectively in many situations where we need to process large volumes of data but hold only a few elements at a time. Possible applications include network and I/O buffers, pipelines for exchanging data between threads in concurrent programs, and many more.</p>
<p>Let us now look at applications of local buffer optimizations b<a id="_idTextAnchor493"/>eyond common data structures.</p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor494"/>Type-erased and callable objects</h2>
<p>There is another, very different, type of application where the local buffer optimization can be used <a id="_idIndexMarker575"/>very effectively - storing <a id="_idIndexMarker576"/>callable objects, which are objects that can be invoked as functions. Many template classes provide an option to customize some part of their behavior using a callable object. For example, <code>std::shared_ptr</code>, the standard shared pointer in C++, allows the user to specify a custom deleter. This deleter will be called with the address of the object to be deleted, so it is a callable with one argument of the <code>void*</code> type. It could be a function pointer, a member function pointer, or a functor object (an object with an <code>operator()</code> defined) - any type that can be called on a <code>p</code> pointer; that is, any type that compiles in the <code>callable(p)</code> function call syntax can be used. The deleter, however, is more than a type; it is an object and is specified at runtime, and so it needs to be stored someplace <a id="_idIndexMarker577"/>where the shared pointer can <a id="_idIndexMarker578"/>get to it. Were the deleter a part of the shared pointer type, we could simply declare a data member of that type in the shared pointer object (or, in the case of the C++ shared pointer, in its reference object that is shared between all copies of the shared pointer). You could consider it a trivial application of the local buffer optimization, as in the following smart pointer that automatically deletes the object when the pointer goes out of scope (just like <code>std::unique_ptr</code>):</p>
<pre class="source-code">
// Example 09
template &lt;typename T, typename Deleter&gt; class smartptr {
  public:
  smartptr(T* p, Deleter d) : p_(p), d_(d) {}
  ~smartptr() { d_(p_); }
  T* operator-&gt;() { return p_; }
  const T* operator-&gt;() const { return p_; } private:
  T* p_;
  Deleter d_;
};</pre>
<p>We are after more interesting things, however, and one such thing can be found when we deal with type-erased objects. The details of such objects were considered in the chapter dedicated to type erasure, but in a nutshell, they are objects where the callable is not a part of the type itself (as in, it is <em class="italic">erased</em> from the type of the containing object). The callable is instead stored in a polymorphic object, and a virtual function is used to call the object of the right type at runtime. The polymorphic object, in turn, is manipulated through the base class pointer.</p>
<p>Now, we have a problem that is, in a sense, similar to the preceding small vector - we need to store some data, in our case the callable object, whose type, and therefore size, is not <a id="_idIndexMarker579"/>statically known. The general solution <a id="_idIndexMarker580"/>is to dynamically allocate such objects and access them through the base class pointer. In the case of a smart pointer <code>deleter</code>, we could do it like this:</p>
<pre class="source-code">
// Example 09
template &lt;typename T&gt; class smartptr_te {
  struct deleter_base {
    virtual void apply(void*) = 0;
    virtual ~deleter_base() {}
  };
  template &lt;typename Deleter&gt;
  struct deleter : public deleter_base {
    deleter(Deleter d) : d_(d) {}
    void apply(void* p) override {
      d_(static_cast&lt;T*&gt;(p));
    }
    Deleter d_;
  };
  public:
  template &lt;typename Deleter&gt;
  smartptr_te(T* p, Deleter d) : p_(p),
    d_(new deleter&lt;Deleter&gt;(d)) {}
  ~smartptr_te() {
    d_-&gt;apply(p_);
    delete d_;
  }
  T* operator-&gt;() { return p_; }
  const T* operator-&gt;() const { return p_; }
  private:
  T* p_;
  deleter_base* d_;
};</pre>
<p>Note that the <code>Deleter</code> type is no longer a part of the smart pointer type; it was <em class="italic">erased</em>. All smart pointers for the same <code>T</code> object type have the same type, <code>smartptr_te&lt;T&gt;</code> (here, <code>te</code> stands for <em class="italic">type-erased</em>). However, we have to pay a steep price <a id="_idIndexMarker581"/>for this syntactic convenience - every time <a id="_idIndexMarker582"/>a smart pointer is created, there is an additional memory allocation. How steep? The first rule of performance must again be remembered - <em class="italic">steep</em> is only a guess until confirmed by an experiment, such as the following benchmark:</p>
<pre class="source-code">
// Example 09
struct deleter {    // Very simple deleter for operator new
  template &lt;typename T&gt; void operator()(T* p) { delete p; }
};
void BM_smartptr(benchmark::State&amp; state) {
  deleter d;
  for (auto _ : state) {
    smartptr&lt;int, deleter&gt; p(new int, d);
  }
  state.SetItemsProcessed(state.iterations());
}
void BM_smartptr_te(benchmark::State&amp; state) {
  deleter d;
  for (auto _ : state) {
    smartptr_te&lt;int&gt; p(new int, d);
  }
  state.SetItemsProcessed(state.iterations());
}
BENCHMARK(BM_smartptr);
BENCHMARK(BM_smartptr_te);
BENCHMARK_MAIN();</pre>
<p>For a smart <a id="_idIndexMarker583"/>pointer with a statically defined deleter, we can <a id="_idIndexMarker584"/>expect the cost of each iteration to be very similar to the cost of calling <code>malloc()</code> and <code>free()</code>, which we measured earlier:</p>
<pre class="source-code">
Benchmark                  Time Items per second
BM_smartptr             21.0 ns 47.5732M/s
BM_smartptr_te          44.2 ns 22.6608M/s</pre>
<p>For a type-erased smart pointer, there are two allocations instead of one, and so the time it takes to create the pointer object is doubled. By the way, we can also measure the performance of a raw pointer, and it should be the same as the smart pointer within the accuracy of the measurements (this was, in fact, a stated design <a id="_idTextAnchor495"/>goal for the <code>std::unique_ptr</code> standard).</p>
<p>We can apply the same idea of local buffer optimization here, and it is likely to be even more effective than it was for strings; after all, most callable objects are small. We can’t completely count on that, however, and must handle the case of a callable object that is larger than the local buffer:</p>
<pre class="source-code">
// Example 09
template &lt;typename T&gt; class smartptr_te_lb {
  struct deleter_base {
    virtual void apply(void*) = 0;
    virtual ~deleter_base() {}
  };
  template &lt;typename Deleter&gt;
    struct deleter : public deleter_base {
    deleter(Deleter d) : d_(d) {}
    void apply(void* p) override {
      d_(static_cast&lt;T*&gt;(p));
    }
    Deleter d_;
  };
  public:
  template &lt;typename Deleter&gt;
    smartptr_te_lb(T* p, Deleter d) : p_(p),
      d_((sizeof(Deleter) &gt; sizeof(buf_))
         ? new deleter&lt;Deleter&gt;(d)
         : new (buf_) deleter&lt;Deleter&gt;(d)) {}
  ~smartptr_te_lb() {
    d_-&gt;apply(p_);
    if ((void*)(d_) == (void*)(buf_)) {
      d_-&gt;~deleter_base();
    } else {
      delete d_;
    }
  }
  T* operator-&gt;() { return p_; }
  const T* operator-&gt;() const { return p_; }
  private:
  T* p_;
  deleter_base* d_;
  char buf_[16];
};</pre>
<p>Using the <a id="_idIndexMarker585"/>same benchmark as before, we can <a id="_idIndexMarker586"/>measure the performance of the type-erased smart pointer with local buffer optimization:</p>
<pre class="source-code">
Benchmark                  Time Items per second
BM_smartptr             21.0 ns 47.5732M/s
BM_smartptr_te          44.2 ns 22.6608M/s
BM_smartptr_te_lb       22.3 ns 44.8747M/s</pre>
<p>While the construction and deletion of a smart pointer without type erasure took 21 nanoseconds, and 44 nanoseconds with type erasure, the optimized type-erased shared pointer test takes 22 nanoseconds on the same machine. The slight overhead comes from checking whether t<a id="_idTextAnchor496"/><a id="_idTextAnchor497"/><a id="_idTextAnchor498"/>he <code>deleter</code> is stored locally or remotely.</p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor499"/>Local buffer optimization in the standard library</h2>
<p>We should <a id="_idIndexMarker587"/>note that the last application of local buffer optimization, storing callables for type-erased objects, is widely used in the C++ standard template library. For example, <code>std::shared_ptr</code> has a type-erased deleter, and most implementations use the local buffer optimization; the deleter is stored with the reference object and not with each copy of the shared pointer, of course. The <code>std::unique_pointer</code> standard, on the other hand, is not type-erased at all, to avoid even a small overhead, or potentially a much larger overhead should the deleter not fit into the local buffer.</p>
<p>The “<em class="italic">ultimate</em>” type-erased object of the C++ standard library, <code>std::function</code>, is also typically implemented with a local buffer for storing small callable objects without the expense of an additional allocation. The universal container ob<a id="_idTextAnchor500"/>ject for any type, <code>std::any</code> (since C++17), is also typically implemented without a dynamic allocation when possible.</p>
<h1 id="_idParaDest-194"><a id="_idTextAnchor501"/>Local buffer optimization in detail</h1>
<p>We have seen the applications of local buffer optimization; for simplicity, we stayed with the most <a id="_idIndexMarker588"/>basic implementation of it. This simple implementation misses several important details, which we will now highlight.</p>
<p>First of all, we completely neglected the alignment of the buffer. The type we used to reserve the space inside an object is <code>char</code>; therefore, our buffer is byte-aligned. Most data types have higher alignment requirements: the exact requirements are platform-specific, but most built-in types are aligned on their own size (double is 8-byte-aligned on a 64-bit platform such as x86). Higher alignments are needed for some machine-specific types such as packed integer or floating-point arrays for AVX instructions.</p>
<p>Alignment is important: depending on the processor and the code generated by the compiler, accessing memory not aligned as required by the data type can result in poor performance or memory access violations (crashes). For example, most AVX instructions require 16- or 32-byte alignment, and the unaligned versions of these instructions are significantly slower. Another example is atomic operations such as the ones used in mutexes and other concurrent data structures: they also don’t work if the data type is not properly aligned (for example, an atomic <code>long</code> must be aligned on an 8-byte boundary).</p>
<p>Specifying the minimum alignment for our buffer is not hard, at least if we know the type we want to store in the buffer. For example, if we have a small vector for an arbitrary type <code>T</code>, we can simply write:</p>
<pre class="source-code">
template &lt;typename T&gt;
class small_vector {
  alignas(T) char buffer_[buffer_size_];
  …
};</pre>
<p>If the buffer is used for storing an object of one of several types, we have to use the highest alignment of all possible types. Finally, if the type of the object to be stored is unknown – the typical case for type-erased implementations – we have to select a “<em class="italic">high enough</em>” alignment and add a compile-time check at the point where a specific object is constructed in the buffer.</p>
<p>The second important subtlety to remember is how the buffer is defined. Usually, it is an aligned array of characters (or <code>std::byte_t</code>). In the previous section, we used an array of <code>int</code> for the small vector of integers. Again, there is a subtlety here: declaring the buffer as an object or an array of objects of the right type will cause these objects to be destroyed automatically when the object containing the buffer is destroyed. For trivially destructible types such as integers, it makes no difference at all – their destructors do nothing.</p>
<p>In general, this is not so, and an arbitrary destructor can be invoked only if an object was constructed <a id="_idIndexMarker589"/>at this location. For our small vector, this is not always the case: the vector may be empty or contain fewer objects than the buffer can hold. This is the most common case by far: usually, if we employ local buffer optimization, we cannot be sure that an object was constructed in the buffer. In this case, declaring the buffer as an array of non-trivially-destructible objects would be a mistake. However, if you have a guarantee that, in your particular case, the buffer always contains an object (or several objects, for an array), declaring them with the corresponding type greatly simplifies the implementation of the destructor, as well as the copy/move operations.</p>
<p>You should have noticed by now that a typical implementation of a local buffer needs a lot of boilerplate code. There are <code>reinterpret_cast</code> casts everywhere, you have to remember to add the alignment, there are some compile-time checks you should always add to make sure only suitable types are stored in the buffer, and so on. It is good to combine these details together in a single general reusable implementation. Unfortunately, as is often the case, there is a tension between reusability and complexity, so we will have to settle for several general reusable implementations.</p>
<p>If we put together everything we have learned about local buffers, we can come up with something like this:</p>
<pre class="source-code">
// Example 10
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  constexpr static auto size = S, alignment = A;
  alignas(alignment) char space_[size];
  …
};</pre>
<p>Here we have a buffer of arbitrary size and alignment (both are template parameters). Now that we <a id="_idIndexMarker590"/>have a space to store objects, we have to make sure the type we want to erase fits into this space. To this end, it is convenient to add a <code>constexpr</code> validator function (it’s used only in compile-time syntax checks):</p>
<pre class="source-code">
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  template &lt;typename T&gt; static constexpr bool valid_type()
  {
    return sizeof(T) &lt;= S &amp;&amp; (A % alignof(T)) == 0;
  }
  …
};</pre>
<p>The buffer can be used as if it contained an object of type <code>T</code> by calling the member function <code>as&lt;T&gt;()</code>:</p>
<pre class="source-code">
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  …
  template &lt;typename T&gt; requires(valid_type&lt;T&gt;())
    T* as() noexcept {
    return reinterpret_cast&lt;T*&gt;(&amp;space_);
  }
  template &lt;typename T&gt; requires(valid_type&lt;T&gt;())
    const T* as() const noexcept {
    return const_cast&lt;Buffer*&gt;(this)-&gt;as&lt;T&gt;();
  }
};</pre>
<p>The buffer can be constructed empty (default-constructed) or with an immediately constructed object. In the former case, the object can be emplaced later. Either way, we validate that the <a id="_idIndexMarker591"/>type fits into the buffer and meets the alignment requirements (if C++20 and concepts are not available, SFINAE can be used instead). The default constructor is trivial, but the emplacing constructor and the <code>emplace()</code> method have constraints on the type and the constructor arguments:</p>
<pre class="source-code">
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  …
  Buffer() = default;
  template &lt;typename T, typename... Args&gt;
    requires(valid_type&lt;T&gt;() &amp;&amp;
             std::constructible_from&lt;T, Args...&gt;)
  Buffer(std::in_place_type_t&lt;T&gt;, Args&amp;&amp; ...args)
    noexcept(std::is_nothrow_constructible_v&lt;T, Args...&gt;)
  {
    ::new (static_cast&lt;void*&gt;(as&lt;T&gt;()))
      T(std::forward&lt;Args&gt;(args)...);
  }
  template&lt;typename T, typename... Args&gt;
    requires(valid_type&lt;T&gt;() &amp;&amp;
             std::constructible_from&lt;T, Args...&gt;)
  T* emplace(Args&amp;&amp; ...args)
    noexcept(std::is_nothrow_constructible_v&lt;T, Args...&gt;)
  {
    return ::new (static_cast&lt;void*&gt;(as&lt;T&gt;()))
      T(std::forward&lt;Args&gt;(args)...);
  }
};</pre>
<p>Note that we do check that the requested type can be stored in the buffer but no checking is done at run time to ensure that the buffer does indeed contain such an object. Such checking can be added at the cost of additional space and run-time computations <a id="_idIndexMarker592"/>and might make sense as a debugging instrumentation. We do not do anything special for copying, moving, or deleting the buffer. As-is, this implementation is suitable for trivially copyable and trivially destructible objects. In this case, we will want to assert these restrictions when an object is constructed in the buffer (in both the constructor and the <code>emplace()</code> method):</p>
<pre class="source-code">
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  template &lt;typename T&gt;
  Buffer(std::in_place_type_t&lt;T&gt;, Args&amp;&amp; ...args) … {
    static_assert(std::is_trivially_destructible_v&lt;T&gt;, "");
    static_assert(std::is_trivially_copyable_v&lt;T&gt;, "");
    …
  }
};</pre>
<p>In this case, it may also make sense to add a <code>swap()</code> method to the class:</p>
<pre class="source-code">
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  …
  void swap(Buffer&amp; that) noexcept {
    alignas(alignment) char tmp[size];
    ::memcpy(tmp, this-&gt;space_, size);
    ::memcpy(this-&gt;space_, that.space_, size);
    ::memcpy(that.space_, tmp, size);
  }
};</pre>
<p>On the other hand, if we’re using this buffer for storing objects of a single known type and that type is not trivially destructible, we end up writing something like this all the time:</p>
<pre class="source-code">
buffer_.as&lt;T&gt;()-&gt;~T();</pre>
<p>We can simplify <a id="_idIndexMarker593"/>the client code by adding another generally usable method:</p>
<pre class="source-code">
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  …
  template &lt;typename T&gt; void destroy() {
    this-&gt;as&lt;T&gt;()-&gt;~T();
  }
};</pre>
<p>We can add similar methods to copy and move objects stored in the buffer, or leave that to the client.</p>
<p>Our general local buffer implementation works for all trivially copyable and destructible types, as well as for all cases where the type is known and the client of our code handles copying and destroying the objects stored in the buffer. There is one special case that is left out but is nonetheless worth considering: when a local buffer is used in type-erased classes, the stored (erased) type may require non-trivial copying or deletion but the client cannot do these operations since the whole point of type erasure is that the client code does not know the erased type after it was emplaced into the buffer. In this special case, we need to capture the type at the point when it was stored and generate the corresponding copy, move, and deletion operations. In other words, we have to combine our <a id="_idIndexMarker594"/>local buffer with the techniques we learned earlier, in <a href="B19262_06.xhtml#_idTextAnchor266"><em class="italic">Chapter 6</em></a>, <em class="italic">Understanding Type Erasure</em>, about type erasure. The most suitable variant of type erasure, in this case, is <code>vtable</code> – a table of function pointers we generate using templates. The <code>vtable</code> itself is an aggregate (<code>struct</code>) holding function pointers that will do the deletion, copying, or moving:</p>
<pre class="source-code">
// Example 11
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  …
  struct vtable_t {
    using deleter_t = void(Buffer*);
    using copy_construct_t = void(Buffer*, const Buffer*);
    using move_construct_t = void(Buffer*, Buffer*);
    deleter_t*  deleter_;
    copy_construct_t* copy_construct_;
    move_construct_t* move_construct_;
  };
  const vtable_t* vtable_ = nullptr;
};</pre>
<p>We need one class member, <code>vtable_</code>, to store a pointer to the <code>vtable</code>. The object that we will point to needs to be created by the constructor or the <code>emplace()</code> method, of course – that is the only time we know the real type and how to delete or copy it. But we are not going to do dynamic memory allocation for it. Instead, we create a static template variable and initialize it with pointers to static member functions (also templates). The compiler creates an instance of this static variable for every type we store in the buffer. Of course, we also need static template functions (a pointer to a static member function is the same as a regular function pointer, rather than a member function pointer). These <a id="_idIndexMarker595"/>functions are instantiated by the compiler with the same type <code>T</code> of the object that is stored in the buffer:</p>
<pre class="source-code">
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  …
  template &lt;typename U, typename T&gt;
  constexpr static vtable_t vtable = {
    U::template deleter&lt;T&gt;,
    U::template copy_construct&lt;T&gt;,
    U::template move_construct&lt;T&gt;
  };
  template &lt;typename T&gt;
    requires(valid_type&lt;T&gt;() &amp;&amp;
    std::is_nothrow_destructible_v&lt;T&gt;)
  static void deleter(Buffer* space) {
    space-&gt;as&lt;T&gt;()-&gt;~T();
  }
  template &lt;typename T&gt;
    requires(valid_type&lt;T&gt;())
  static void copy_construct(Buffer* to,
                             const Buffer* from)
    noexcept(std::is_nothrow_copy_constructible_v&lt;T&gt;)
  {
    ::new (static_cast&lt;void*&gt;(to-&gt;as&lt;T&gt;()))
      T(*from-&gt;as&lt;T&gt;());
    to-&gt;vtable_ = from-&gt;vtable_;
  }
  template &lt;typename T&gt;
    requires(valid_type&lt;T&gt;())
    static void move_construct(Buffer* to, Buffer* from)
    noexcept(std::is_nothrow_move_constructible_v&lt;T&gt;)
  {
    ::new (static_cast&lt;void*&gt;(to-&gt;as&lt;T&gt;()))
      T(std::move(*from-&gt;as&lt;T&gt;()));
    to-&gt;vtable_ = from-&gt;vtable_;
  }
};</pre>
<p>As shown in <a href="B19262_06.xhtml#_idTextAnchor266"><em class="italic">Chapter 6</em></a>, <em class="italic">Understanding Type Erasure</em>, we first use template static functions to generate copy, move, and delete operations for any type <code>T</code> we need. We store the <a id="_idIndexMarker596"/>pointers to these functions in an instance of a static template variable <code>vtable</code>, and a pointer to that instance in a (non-static) data member <code>vtable_</code>. The latter is our only cost, size-wise (the rest is static variables and functions that are generated by the compiler once for each type stored in the buffer).</p>
<p>This <code>vtable_</code> has to be initialized at the time the object is emplaced in the buffer since this is the last time we explicitly know the type of the stored object:</p>
<pre class="source-code">
// Example 11
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  template &lt;typename T, typename... Args&gt;
    requires(valid_type&lt;T&gt;() &amp;&amp;
    std::constructible_from&lt;T, Args...&gt;)
  Buffer(std::in_place_type_t&lt;T&gt;, Args&amp;&amp; ...args)
    noexcept(std::is_nothrow_constructible_v&lt;T, Args...&gt;)
    : vtable_(&amp;vtable&lt;Buffer, T&gt;)
  {
    ::new (static_cast&lt;void*&gt;(as&lt;T&gt;()))
      T(std::forward&lt;Args&gt;(args)...);
  }
  template&lt;typename T, typename... Args&gt;
    requires(valid_type&lt;T&gt;() &amp;&amp;
    std::constructible_from&lt;T, Args...&gt;)
  T* emplace(Args&amp;&amp; ...args)
    noexcept(std::is_nothrow_constructible_v&lt;T, Args...&gt;)
  {
    if (this-&gt;vtable_) this-&gt;vtable_-&gt;deleter_(this);
    this-&gt;vtable_ = &amp;vtable&lt;Buffer, T&gt;;
    return ::new (static_cast&lt;void*&gt;(as&lt;T&gt;()))
      T(std::forward&lt;Args&gt;(args)...);
  }
  …
};</pre>
<p>Note the <a id="_idIndexMarker597"/>initialization of the <code>vtable_</code> member in the constructor. In the <code>emplace()</code> method, we also have to delete the object previously constructed in the buffer, if one exists.</p>
<p>With the type erasure machinery in place, we can finally implement the destructors and the copy/move operations. They all use a similar approach – call the corresponding function in the <code>vtable</code>. Here are the copy operations:</p>
<pre class="source-code">
// Example 11
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  …
  Buffer(const Buffer&amp; that) {
    if (that.vtable_)
      that.vtable_-&gt;copy_construct_(this, &amp;that);
  }
  Buffer&amp; operator=(const Buffer&amp; that) {
    if (this == &amp;that) return *this;
    if (this-&gt;vtable_) this-&gt;vtable_-&gt;deleter_(this);
    if (that.vtable_)
      that.vtable_-&gt;copy_construct_(this, &amp;that);
    else this-&gt;vtable_ = nullptr;
    return *this;
  }
};</pre>
<p>The move <a id="_idIndexMarker598"/>operations are similar, only they use the <code>move_construct_</code> function:</p>
<pre class="source-code">
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  …
  Buffer(Buffer&amp;&amp; that) {
    if (that.vtable_)
      that.vtable_-&gt;move_construct_(this, &amp;that);
  }
  Buffer&amp; operator=(Buffer&amp;&amp; that) {
    if (this == &amp;that) return *this;
    if (this-&gt;vtable_) this-&gt;vtable_-&gt;deleter_(this);
    if (that.vtable_)
      that.vtable_-&gt;move_construct_(this, &amp;that);
    else this-&gt;vtable_ = nullptr;
    return *this;
  }
};</pre>
<p>Note that the <a id="_idIndexMarker599"/>move-assignment operator is not required to check for self-assignment, but it’s also not wrong to do so. It is highly desirable for the move operations to be <code>noexcept</code>; unfortunately, we cannot guarantee that because we do not know the erased type at compile time. We can make a design choice and declare them <code>noexcept</code> anyway. If we do, we can also assert, at compile-time, that the object we store in the buffer is <code>noexcept</code> movable.</p>
<p>Finally, we have the destruction operations. Since we allow the caller to destroy the contained object without destroying the buffer itself (by calling <code>destroy()</code>), we have to take care to ensure that the object gets destroyed only once:</p>
<pre class="source-code">
template&lt;size_t S, size_t A = alignof(void*)&gt;
struct Buffer {
  …
  ~Buffer() noexcept {
    if (this-&gt;vtable_) this-&gt;vtable_-&gt;deleter_(this);
  }
  // Destroy the object stored in the aligned space.
  void destroy() noexcept {
    if (this-&gt;vtable_) this-&gt;vtable_-&gt;deleter_(this);
    this-&gt;vtable_ = nullptr;
  }
};</pre>
<p>Having the type-erased <code>vtable</code> allows us to reconstruct, at run time, the type stored in the buffer (it is embedded in the code generated for the static functions such as <code>copy_construct()</code>). There is, of course, a cost to it; we already noted the additional data member <code>vtable_</code>, but there is also some run-time cost arising from the indirect function calls. We can estimate it by using both implementations of the local buffer (with and without type erasure) to store and copy some trivially copyable object, for example, a lambda with a captured reference:</p>
<pre class="source-code">
Benchmark                        Time
BM_lambda_copy_trivial          5.45 ns
BM_lambda_copy_typeerased       4.02 ns</pre>
<p>The overhead of (well-implemented) type erasure is non-negligible but modest. An added advantage <a id="_idIndexMarker600"/>is that we could also verify at run-time whether or not our calls to <code>as&lt;T&gt;()</code> refer to a valid type and that the object is indeed constructed. Relatively to the very cheap implementation of an unchecked method, this would add significant overhead, so probably should be restricted to debug builds.</p>
<p>We have seen significant, sometimes dramatic, improvements to the performance of many different data structures and classes provided by the local buffer optimization. With the easy-to-use general implementations we just learned, why would you not use this optimization all the time? As is the case for any design pattern, our exploration is not comple<a id="_idTextAnchor502"/>te without mentioning the trade-offs and the downsides.</p>
<h1 id="_idParaDest-195"><a id="_idTextAnchor503"/>Downsides of local buffer optimization</h1>
<p>Local buffer <a id="_idIndexMarker601"/>optimization is not without its downsides. The most obvious one is that all objects with a local buffer are larger than they would be without one. If the typical data stored in the buffer is smaller than the chosen buffer size, then every object is wasting some memory, but at least the optimization is paying off. Worse, if our choice of buffer size is badly off and most data is, in fact, larger than the local buffer, the data is stored remotely but the local buffers are still created inside every object, and all that memory is wasted.</p>
<p>There is an obvious trade-off between the amount of memory we are willing to waste and the range of data sizes where the optimization is effective. The size of the local buffer should be carefully chosen with the application in mind.</p>
<p>The more <a id="_idIndexMarker602"/>subtle complication is this - the data that used to be external to the object is now stored inside the object. This has several consequences, in addition to the performance benefits we were so focused on. First of all, every copy of the object contains its own copy of the data as long as it fits into the local buffer. This prevents <a id="_idIndexMarker603"/>designs such as the reference counting of data; for example, a <strong class="bold">Copy-On-Write </strong>(<strong class="bold">COW</strong>) string, where the data is not copied as long as all string copies <a id="_idTextAnchor504"/>remain the same, cannot use the small string optimization.</p>
<p>Secondly, the data must be moved if the object itself is moved. Contrast this with <code>std::vector</code>, which is moved or swapped, essentially like a pointer - the pointer to the data is moved but the data remains in place. A similar consideration exists for the object contained inside <code>std::any</code>. You could dismiss this concern as trivial; after all, local buffer optimization is used primarily for small amounts of data, and the cost of moving them should be comparable to the cost of copying the pointer. However, more than performance is at stake here - moving an instance of <code>std::vector</code> (or <code>std::any</code>, for that matter) is guaranteed not to throw an exception. However, no such guarantees are offered when moving an arbitrary object. Therefore, <code>std::any</code> can be implemented with a local buffer optimization only if the object it contains is <code>std::is_nothrow_move_constructible</code>.</p>
<p>Even such a guarantee does not suffice for the case of <code>std::vector</code>, however; the standard explicitly states that moving, or swapping, a vector does not invalidate iterators pointing to any element of the vector. Obviously, this requirement is incompatible with local buffer optimization, since moving a small vector would relocate all its elements to a different region of memory. For that reason, many high-efficiency libraries offer a custom vector-like container that supports small vector optimization, at the<a id="_idTextAnchor505"/> expense of the standard iterator invalidation guarantees.</p>
<h1 id="_idParaDest-196"><a id="_idTextAnchor506"/>Summary</h1>
<p>We have just introduced a design pattern aimed solely at improved performance. Efficiency is an important consideration for the C++ language; thus, the C++ community developed patterns to address the most common inefficiencies. Repeated or wasteful memory allocation is perhaps the most common of all. The design pattern we have just seen - local buffer optimization - is a powerful tool that can greatly reduce such allocations. We have seen how it can be applied to compact data structures, as well as to store small objects, such as callables. We have also reviewed the possible downsides of using this pattern.</p>
<p>With the next chapter, <a href="B19262_11.xhtml#_idTextAnchor509"><em class="italic">Chapter 11</em></a>, <em class="italic">ScopeGuard</em>, we move on to study more complex patterns that address broader design issues. The idioms we have learned so f<a id="_idTextAnchor507"/>ar are often used in the implementation of these patterns.</p>
<h1 id="_idParaDest-197"><a id="_idTextAnchor508"/>Questions</h1>
<ol>
<li value="1">How can we measure the performance of a small fragment of code?</li>
<li>Why are small and frequent memory allocations particularly bad for performance?</li>
<li>What is local buffer optimization, and how does it work?</li>
<li>Why is an allocation of an additional buffer inside an object effectively <em class="italic">free</em>?</li>
<li>What is short string optimization?</li>
<li>What is small vector optimization?</li>
<li>Why is local buffer optimization particularly effective for callable objects?</li>
<li>What are the trade-offs to consider when using local buffer optimization?</li>
<li>When should an object not be placed in a local buffer?</li>
</ol>
</div>
</body></html>