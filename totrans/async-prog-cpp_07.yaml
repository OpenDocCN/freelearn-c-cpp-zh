- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Async Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about promises, futures, and packaged tasks.
    When we introduced packaged tasks, we mentioned that **std::async** provides a
    simpler way to achieve the same result, with less code and thus being cleaner
    and more concise.
  prefs: []
  type: TYPE_NORMAL
- en: The **async function** ( **std::async** ) is a function template that runs a
    callable object asynchronously where we can also select the method of execution
    by passing some flags defining the launch policy. It is a powerful tool for handling
    asynchronous operations, but its automatic management and lack of control over
    the thread of execution, among other aspects, can also make it unsuitable for
    certain tasks where fine-grained control or cancellation is required.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the async function and how do we use it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the different launch policies?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the differences from previous methods, especially packaged tasks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the advantages and disadvantages of using **std::async** ?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical scenarios and examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The async function has been available since C++11, but some examples use features
    from C++14, such as **chrono_literals** , and C++20, such as **counting_semaphore**
    , so the code shown in this chapter can be compiled by compilers supporting C++20.
  prefs: []
  type: TYPE_NORMAL
- en: Please check the *Technical requirements* section in [*Chapter 3*](B22219_03.xhtml#_idTextAnchor051)
    , for guidance on how to install GCC 13 and Clang 8 compilers.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find all the complete code in the following GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Asynchronous-Programming-with-CPP](https://github.com/PacktPublishing/Asynchronous-Programming-with-CPP)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The examples for this chapter are located under the **Chapter_07** folder.
    All source code files can be compiled using CMake as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Executable binaries will be generated under the **bin** directory.
  prefs: []
  type: TYPE_NORMAL
- en: What is std::async?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**std::async** is a function template in C++ introduced by the C++ standard
    in the **<future>** header as part of the thread support library from C++11. It
    is used to run a function asynchronously, allowing the main thread (or other threads)
    to continue running concurrently.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, **std::async** is a powerful tool for asynchronous programming in
    C++, making it easier to run tasks in parallel and manage their results efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Launching an asynchronous task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To execute a function asynchronously using **std::async** , we can use the same
    approaches we used when starting threads in [*Chapter 3*](B22219_03.xhtml#_idTextAnchor051)
    , with the different callable objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'One approach is using a function pointer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Another approach is using a lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use an embedded lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use a function object where **operator()** is overloaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use a non-static member function by passing the address of the member
    function and the address of an object to call the member function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use a static member function where only the address of the member
    function is needed as the method is static:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When **std::async** is called, it returns a future where the result of the function
    will be stored, as we already learned in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Passing values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Again, similarly to when we passed arguments when creating threads, arguments
    can be passed to the thread by value, by reference, or as pointers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see how to pass arguments by value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Passing by value implies a copy as a temporary object is created and the argument
    value is copied into it. This avoids data races, but it is much more costly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next example shows how to pass values by reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also pass values as a **const reference** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Passing by reference is achieved by using **std::ref()** (non-constant references)
    or **std::cref()** (constant references), both defined in the **<functional>**
    header file, letting the variadic template (a class or function template that
    supports an arbitrary number of arguments) defining the thread constructor to
    treat the argument as a reference. Missing these functions when passing arguments
    means passing the arguments by value, which implies a copy, as mentioned earlier,
    making the function call more costly.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also move an object into the thread created by **std::async** , as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that the vector **v** is in a valid but empty state after its content being
    moved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can also pass values by lambda captures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the **str** variable is accessed by the lambda function executed
    by **std::async** as a reference.
  prefs: []
  type: TYPE_NORMAL
- en: Returning values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When **std:async** is called, it immediately returns a future that will hold
    the value that the function or callable object will compute, as we saw in the
    previous chapter when using promises and futures.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous examples, we didn´t use the returned object from **std::async**
    at all. Let’s rewrite the last example from the *Packaged tasks* section in [*Chapter
    6*](B22219_06.xhtml#_idTextAnchor125) , where we used a **std::packaged_task**
    object to compute the power of two values. But in this case, we will spawn several
    asynchronous tasks using **std::async** to compute these values, wait for the
    tasks to finish, store the results, and finally, show them in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The **compute()** function simply gets two numbers, **x** and **y** , and computes
    ![<mml:math  ><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msup></mml:math>](img/5.png)
    . It also gets a number representing the task identifier and waits for up to two
    seconds before printing a message in the console and computing the result.
  prefs: []
  type: TYPE_NORMAL
- en: In the **main()** function, the main thread launches several tasks computing
    a sequence of power-of-two values. The futures returned by calling **std::async**
    are stored in the **futVec** vector. Then, the main thread waits for one second,
    emulating some work. Finally, we traverse the **futVec** vector and call the **get()**
    function in each future element, thus waiting for that specific task to finish
    and return a value, and we store the returned value in another vector called **results**
    . Then, we print the content of the **results** vector before exiting the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the output when running that program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, each task took a different amount of time to complete, thus the
    output is not ordered by task identifier. But as we traverse the **futVec** vector
    in order when getting the results, these are shown as in order.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how to launch asynchronous tasks and pass arguments and
    return values, let’s learn how to use launch policies to control the methods of
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: Launch policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from specifying the function or callable object as an argument when using
    the **std::async** function, we can also specify the **launch policy** . Launch
    policies control how **std::async** schedules the execution of asynchronous task.
    These are defined in the **<** **future>** library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The launch policy must be specified as the first argument when calling **std::async**
    . This argument is of the type **std::launch** , a bitmask value where its bits
    control the allowed methods of execution, which can be one or more of the following
    enumeration constants:'
  prefs: []
  type: TYPE_NORMAL
- en: '**std::launch::async** : The task is executed in a separate thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std::launch::deferred** : Enables lazy evaluation by executing the task in
    the calling thread the first time its result is requested via the future **get()**
    or **wait()** method. All further accesses to the same **std::future** will return
    the result immediately. That means that the task will only be executed when the
    result is explicitly requested, which can lead to unexpected delays.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If not defined, by default the launch policy will be **std::launch::async |
    std::launch::deferred** . Also, implementations can provide additional launch
    policies.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, by default the C++ standard states that **std::async** can run in
    either asynchronous or deferred mode.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when more than one flag is specified, the behavior is implementation-defined,
    so depending on the compiler we are using. The standard recommends using available
    concurrency and deferring the task if the default launch policy is specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement the following example to test the different launch policy behaviors.
    First, we define the **square()** function, which will serve as the asynchronous
    task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in the **main()** function, the program starts by launching three different
    asynchronous tasks, one using the **std::launch::async** launch policy, another
    task using the **std::launch::deferred** launch policy, and a third task using
    the default launch policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in the previous chapter, **wait_for()** returns a **std::future_status**
    object indicating whether the future is ready, deferred, or has timed out. Therefore,
    we can use that function to check whether any of the returned futures are deferred.
    We do that by using a lambda function, **is_deferred()** , that returns **true**
    in that case. At least one future object, **fut_deferred** , is expected to return
    **true** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the main program waits for one second, emulating some processing, and
    finally retrieves the results from the asynchronous tasks and prints their value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output from running the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note how the tasks with the default and **std::launch::async** launch policies
    are executed while the main thread is sleeping. Therefore, the task is started
    as soon as it can be scheduled. Also note how the deferred task, using the **std::launch::deferred**
    launch policy, starts executing once the value is requested.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s learn how to handle exceptions happening in the asynchronous task.
  prefs: []
  type: TYPE_NORMAL
- en: Handling exceptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exception propagation from the asynchronous task to the main thread is not supported
    when using **std::async** . To enable exception propagation, we might need a promise
    object to store the exception that later can be accessed by the future returned
    when calling **std::async** . But that promise object is not accessible or provided
    by **std::async** .
  prefs: []
  type: TYPE_NORMAL
- en: One feasible way to achieve this is to use a **std::packaged_task** object wrapping
    the asynchronous task. But if that is the case, we should directly use a packaged
    task as described in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We could also use nested exceptions, available since C++11, by using **std::nested_exception**
    , a polymorphic mixin class that can capture and store the current exception,
    allowing nested exceptions of arbitrary types. From a **std::nested_exception**
    object, we can retrieve the stored exception by using the **nested_ptr()** method
    or rethrow it by calling **rethrow_nested()** .
  prefs: []
  type: TYPE_NORMAL
- en: To create a nested exception, we can throw an exception using the **std::throw_with_nested()**
    method. If we want to rethrow an exception only if it’s nested, we can use **std::rethrow_if_nested()**
    . All these functions are defined in the **<** **exception>** header.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using all these functions, we can implement the following example, where an
    asynchronous task throws a **std::runtime_error** exception, which is caught in
    the main body of the asynchronous task and rethrown as a nested exception. This
    nested exception object is then caught again in the main function and the sequence
    of exceptions is printed out, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the example, an asynchronous task is created that executes
    the **func_throwing()** function inside a **try-catch** block. This function simply
    throws a **std::runtime_error** exception, which is caught and then rethrown as
    part of a **std::nested_exception** class by using the **std::throw_with_nested()**
    function. Later, in the main thread, when we try to retrieve the result from the
    **fut** future object by calling its **get()** method, the nested exception is
    thrown and captured again in the main try-catch block, where the **print_exceptions()**
    function is called with the captured nested exception as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: The **print_exceptions()** function prints the reason for the current exception
    ( **e.what()** ) and rethrows the exception if nested, thus catching it again
    and recursively printing exception reasons with indentation by nesting level.
  prefs: []
  type: TYPE_NORMAL
- en: As each asynchronous task has its own future, the program can handle exceptions
    from multiple tasks separately.
  prefs: []
  type: TYPE_NORMAL
- en: Exceptions when calling std::async
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apart from exceptions happening in the asynchronous task, there are also some
    cases when **std::async** might throw an exception. These exceptions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**std::bad_alloc** : If there is not enough memory to store internal data structures
    needed by **std::async** .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std:system_error** : If a new thread cannot be started when using **std::launch::async**
    as the launch policy. In this case, the error condition will be **std::errc::resource_unavailable_try_again**
    . Depending on the implementation, if the policy is the default one, it might
    fall back to deferred invocation or implementation-defined policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the time, these exceptions are thrown out due to resource exhaustion.
    A solution can be retrying later when some asynchronous tasks currently working
    have finished and released their resources. Another, more reliable, solution is
    to limit the number of asynchronous tasks running at a given time. We will implement
    this solution shortly, but first, let’s understand the futures returned by **std::async**
    and how to achieve better performance when dealing with them.
  prefs: []
  type: TYPE_NORMAL
- en: Async futures and performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Futures returned by **std::async** behave differently from the ones obtained
    from promises when their destructors are called. When these futures are destroyed,
    their **~future** destructor is called where the **wait()** function is executed,
    causing the thread that was spawned at creation to join.
  prefs: []
  type: TYPE_NORMAL
- en: That would impact the program performance by adding some overhead if the thread
    used by **std::async** has not already been joined, therefore we need to understand
    when the future object will go out of scope and thus its destructor will be called.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see, with several short examples, how these futures behave and some recommendations
    on how to use them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining a task, **func** , that simply multiplies its input value
    by 2 and also waits for some time, emulating a costly operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To measure the performance of a block of code, we will asynchronously run several
    tasks (in this example, **NUM_TASKS = 32** ) and measure the running time using
    the steady clock from the **<chrono>** library. To do that, we simply record a
    time point representing the current point in time when the task starts by using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can define in the **main()** function the following lambda function to be
    called when the task finishes to obtain the duration in milliseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: With that code in place, we can start measuring different approaches to how
    futures can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by running several asynchronous tasks but discarding the future
    returned by **std::async** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The duration of this test is 334 ms on my PC, a Pentium i7 4790K at 4 GHz with
    four cores and eight threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the next test, let’s store the returned future, but don’t wait for the
    result to be ready. Obviously, this is not the right way of using computer power
    by spawning asynchronous tasks as consuming resources and not processing the results,
    but we are doing this for testing and learning purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the duration is still 334 ms. In both cases, a future is created,
    and when going out of scope at the end of each loop iteration, it must wait for
    the thread spawn by **std::async** to finish and join.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we are launching 32 tasks, each one consuming at least 10 ms.
    That totals 320 ms, a value equivalent to 334 ms obtained in these tests. The
    remaining performance cost comes from starting threads, checking the **for** loop
    variable, storing the time points when using the steady clock, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid creating a new future object each time **std::async** is called, and
    waiting for its destructor to be called, let’s reuse the future object as shown
    in the following code. Again, this is not the proper way as we are discarding
    access to the results of previous tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now the duration is 166 ms. The reductions are due to not waiting for each future,
    as they are not destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: 'But this is not ideal as we might be interested in knowing the result of the
    asynchronous tasks. Therefore, we need to store the results in a vector. Let’s
    modify the previous example by using the **res** vector to store the results from
    each task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the duration goes back to 334 ms. This is because we are again
    waiting for the results after spawning each task by calling **fut.get()** before
    launching another asynchronous task. We are serializing the tasks’ execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'A solution could be to store the futures returned by **std::async** in a vector,
    and later traverse that vector and get the results. The following code illustrates
    how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now the duration is only 22 ms! But why is that possible?
  prefs: []
  type: TYPE_NORMAL
- en: Now all tasks are truly running asynchronously. The first loop launches all
    tasks and stores the futures in the **futsVec** vector. There is no longer any
    waiting period due to future destructors being called.
  prefs: []
  type: TYPE_NORMAL
- en: The second loop traverses **futsVec** , retrieves each result, and stores them
    in the results vector, **res** . The time to execute the second loop will be approximately
    the time needed to traverse the **res** vector plus the time used by the slowest
    task to be scheduled and executed.
  prefs: []
  type: TYPE_NORMAL
- en: If the system where the tests were running had enough threads to run all asynchronous
    tasks at once, the runtime could be halved. There are systems that can automatically
    manage several asynchronous tasks under the hood by letting the scheduler decide
    what tasks to run. In other systems, when trying to launch many threads at once,
    they might complain by raising an exception. In the next section, we implement
    a thread limiter by using semaphores.
  prefs: []
  type: TYPE_NORMAL
- en: Limiting the number of threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw earlier, if there are not enough threads to run several **std::async**
    calls, a **std::runtime_system** exception can be thrown and indicate resource
    exhaustion.
  prefs: []
  type: TYPE_NORMAL
- en: We can implement a simple solution by creating a thread limiter using counting
    semaphores ( **std::counting_semaphore** ), a multithreading synchronization mechanism
    explained in [*Chapter 4*](B22219_04.xhtml#_idTextAnchor074) .
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to use a **std::counting_semaphore** object, setting its initial
    value to the maximum concurrent tasks that the system allows, which can be retrieved
    by calling the **std::thread::hardware_concurrency()** function, as learned in
    [*Chapter 2*](B22219_02.xhtml#_idTextAnchor035) , and then use that semaphore
    in the task function to block if the total number of asynchronous tasks exceed
    the maximum concurrent tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet implements this idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The program starts by setting the total number of tasks that will be launched.
    Then, it creates a counting semaphore, **sem** , setting its initial value to
    the hardware concurrency value, as explained earlier. Finally, it just launches
    all tasks and waits for their futures to be ready, as usual.
  prefs: []
  type: TYPE_NORMAL
- en: The key point in this example is that each task, before performing its job,
    acquires the semaphore, thus decrementing the internal counter or blocking until
    the counter can be decremented. When the job is done, the semaphore is released,
    which increments the internal counter and unblocks other tasks that try to acquire
    the semaphore at that time. That means that a task will launch only if there is
    a free hardware thread to be used for that task. Otherwise, it will be blocked
    until another task releases the semaphore.
  prefs: []
  type: TYPE_NORMAL
- en: Before exploring some real-life scenarios, let’s first understand some drawbacks
    of using **std::async** .
  prefs: []
  type: TYPE_NORMAL
- en: When not to use std::async
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen during this chapter, **std::async** does not provide direct
    control over the number of threads used or access to the thread objects themselves.
    We know now how to limit the number of asynchronous tasks by using counting semaphores,
    but there might be some applications where this is not the optimal solution and
    fine-grained control is required.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the automatic management of threads can reduce performance by introducing
    overhead, especially when many small tasks are launched, leading to excessive
    context switching and resource contention.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation imposes some limit on the number of concurrent threads that
    can be used, which can degrade performance or even throw exceptions. As **std::async**
    and the available **std::launch** policies are implementation-dependent, the performance
    is not uniform across different compilers and platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in this chapter, we didn’t mention how to cancel an asynchronous task
    started by **std::async** as there is no standard way of doing so before completion.
  prefs: []
  type: TYPE_NORMAL
- en: Practical examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now it’s time to implement some examples to tackle real-life scenarios using
    **std::async** . We will learn how to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform parallel computation and aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asynchronously search across different containers or a large dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asynchronously multiply two matrices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chain asynchronous operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve the pipeline example from the last chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel computation and aggregation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data aggregation** is the process of collecting raw data from multiple sources
    and organizing, processing, and providing a summary of the data for easy consumption.
    This process is useful in many fields, such as business reporting, financial services,
    healthcare, social media monitoring, research, and academia.'
  prefs: []
  type: TYPE_NORMAL
- en: As a naive example, let’s compute the result of squaring all numbers between
    *1* and *n* and obtaining their average value. We know that using the following
    formula to compute the sum of square values would be much quicker and require
    less computer power. Also, the task could be more meaningful, but the purpose
    of this example is to understand the relationship between the tasks, not the task
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: '![<math  display="block"><mrow><mrow><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>i</mi><mn>2</mn></msup></mrow><mo>=</mo><mfrac><mrow><mi>n</mi><mfenced
    open="(" close=")"><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mfenced><mstyle
    scriptlevel="+1"><mfrac><mrow><mi>n</mi><mfenced open="(" close=")"><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mfenced><mfenced
    open="(" close=")"><mrow><mn>2</mn><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mfenced></mrow><mn>6</mn></mfrac></mstyle><mo>(</mo><mn>2</mn><mi>n</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow><mn>6</mn></mfrac></mrow></mrow></math>](img/6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **average_squares()** function in the following example launches an asynchronous
    task per value between *1* and *n* to compute the square value. The resulting
    future objects are stored in the **futsVec** vector, which is later used by the
    **sum_results()** function to compute the sum of the squared values. The result
    is then divided by **n** to obtain the average value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: For example, for ![<mml:math  ><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:math>](img/7.png)
    , we can check that the value will be the same as the one returned by   the function
    divided by * n * , * 3,383.50 * .
  prefs: []
  type: TYPE_NORMAL
- en: This example can easily be modified to implement a solution using the **MapReduce**
    programming model to handle large datasets efficiently. MapReduce works by dividing
    the data processing into two phases; the Map phase, where independent chunks of
    data are filtered, sorted, and processed in parallel across multiple computers,
    and the Reduce phase where results from the Map phase are aggregated, summarizing
    the data. This is like what we just implemented, using the **square()** function
    in the Map phase, and the **average_squares()** and **sum_results()** functions
    in the Reduce phase.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous searches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way to speed up searching a target value into large containers is to parallelize
    the search. Next, we will present two examples. The first one involves searching
    across different containers by using one task per container, while the second
    one involves searching across a large container, dividing it into smaller segments,
    and using a task per segment.
  prefs: []
  type: TYPE_NORMAL
- en: Searching across different containers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we need to search for a **target** value in different containers
    of diverse types ( **vector** , **list** , **forward_list** , and **set** ) containing
    names of animals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To search for the target value, we launch an asynchronous task for each container,
    using the **search()** template function, which simply calls the **std::find**
    function in a container and returns **true** if the target value is found, or
    **false** otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'These asynchronous tasks are launched using the **std::async** function with
    the **std::launch::async** launch policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we simply retrieve all return values from the futures created when
    using **std::async** and bitwise OR them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This example also shows the power of the **Standard Template Library** ( **STL**
    ) as it provides generic and reusable algorithms that can be applied to different
    containers and data types.
  prefs: []
  type: TYPE_NORMAL
- en: Searching in a large container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the next example, we will implement a solution to find a target value in
    a large vector containing 5 million integer values.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate the vector, we use a random number generator with a uniform integer
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'To search for a target value in a segment of a vector, we can use the **std::find**
    function with **begin** and **end** iterators pointing to the segment limits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the **main()** function, we start by generating the large vector using the
    **generate_vector()** function, then defining the **target** value to find and
    the number of segments ( **num_segments** ) which the vector will be split for
    parallel searches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for each segment, we define its **begin** and **end** iterators and launch
    an asynchronous task to search for the target value in that segment. Thus, we
    execute **search_segment** asynchronously in a separate thread by using **std::async**
    with the **std::launch::async** launch policy. To avoid copying the large vector
    when passing it as an input argument of **search_segment** , we use a constant
    reference, **std::cref** . The futures returned by **std::async** are stored in
    the **futs** vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note that the vector size is not always a multiple of the segment size, thus
    the last segment might be shorter than the others. To deal with this situation
    and avoid issues when accessing out-of-bounds memory when checking the last segment,
    we need to properly set the **begin** and **end** indexes for each segment. For
    that, we use **std::min** to get the minimum value between the size of the vector
    and the hypothetical index of the last element in the current segment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we check all results by calling **get()** on each future and print
    a message to the console if the target value was found in any of the segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This solution can be used as the base for more advanced solutions dealing with
    huge datasets in distributed systems where each asynchronous task tries to find
    a target value in a specific machine or cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous matrix multiplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Matrix multiplication** is one of the most relevant operations in computer
    science, used in many domains, such as computer graphics, computer vision, machine
    learning, and scientific computing.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will implement a parallel computing solution by
    distributing the computation across multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by defining a matrix type, **matrix_t** , as a vector of vectors
    holding integer values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Then, we implement the **matrix_multiply** function, which accepts two matrices,
    **A** and **B** , passing them as constant references, and returns their multiplication.
    We know that if **A** is a matrix ![<mml:math  ><mml:mi>m</mml:mi><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:math>](img/8.png)
    ( * m * stands for rows and * n * for columns) and ** B ** is a matrix ![<mml:math  ><mml:mi>p</mml:mi><mml:mi>x</mml:mi><mml:mi>q</mml:mi></mml:math>](img/9.png)
    , we can multiply ** A ** and ** B ** if ![<mml:math  ><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi></mml:math>](img/10.png)
    , and the resulting matrix will be of dimensions ![<mml:math  ><mml:mi>m</mml:mi><mml:mi>x</mml:mi><mml:mi>q</mml:mi></mml:math>](img/11.png)
    ( * m * rows and * q * columns).
  prefs: []
  type: TYPE_NORMAL
- en: 'The **matrix_multiply** function just starts by reserving some space to the
    result matrix, **res** . Then, it loops over the matrix by extracting column **j**
    from **B** and multiplying it by row **i** from **A** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The multiplication is done asynchronously by using **std::async** with the
    **std::launch::async** launch policy, running the **dot_product** function. Each
    returned future from **std::async** is stored in the **futs** vector. The **dot_product**
    function computes the dot product of vectors **a** and **b** , representing a
    row from **A** and a column from **B** , by multiplying element by element and
    returning the sum of these products:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As the **dot_product** function expects two vectors, we need to extract each
    column from **B** before launching each asynchronous task. This also enhances
    the overall performance as the vectors might be stored in contiguous blocks of
    memory, thus being more cache-friendly during computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **main()** function, we just define two matrices, **A** and **B** ,
    and use the **matrix_multipy** function to compute their product. All matrices
    are printed into the console using the **show_matrix** lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output from running this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Using contiguous memory blocks improves performance when traversing vectors
    as many of their elements can be read at once into the cache. Using contiguous
    memory allocation is not guaranteed when using **std::vector** , therefore it
    might be better to use **new** or **malloc** .
  prefs: []
  type: TYPE_NORMAL
- en: Chain asynchronous operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we will implement a simple pipeline composed of three stages
    where each stage takes the result from the previous stage and computes a value.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Simple pipeline example](img/B22219_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Simple pipeline example
  prefs: []
  type: TYPE_NORMAL
- en: 'The first stage only accepts positive integers as input, otherwise it raises
    an exception, and adds 10 to that value before returning the result. The second
    stage multiplies its input by 2, and the third subtracts 5 from its input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In the **main()** function, for the intermediate and final stages, we define
    the pipeline by using as input the futures generated by the previous stages. These
    futures are passed by reference to the lambda expression running the asynchronous
    code, where their **get()** function is used to get their result.
  prefs: []
  type: TYPE_NORMAL
- en: 'To retrieve the result from the pipeline, we just need to call the **get()**
    function from the future returned by the last stage. If an exception happens,
    for example, when **input_value** is negative, it is caught by the try-catch block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The pipeline defined in this example is a simple one where each stage uses the
    future from the previous stage to get the input value and produce its result.
    In the next example, we will rewrite the pipeline implemented in the previous
    chapter using **std:async** with deferred launch policies to only execute the
    stages that are needed.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As promised, in the last chapter, when we were implementing a pipeline, we mentioned
    that the different tasks could be kept switched off until needed by using futures
    with deferred execution. As also mentioned, this is useful in scenarios where
    the computation cost is high, but the result may not always be needed. As futures
    with deferred status can only be created by using **std::async** , now it’s time
    to see how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will implement the same pipeline described in the previous chapter, which
    follows the next task graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Pipeline example](img/B22219_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Pipeline example
  prefs: []
  type: TYPE_NORMAL
- en: We start by defining the **Task** class. This class is like the one implemented
    in an example in the previous chapter but using the **std::async** function and
    storing the returned future instead of the promise used previously. Here, we will
    only comment on the relevant code changes from that example, so please look at
    that example for a full explanation of the **Task** class or check it out in the
    GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: '**Task** constructors store the task identifier ( **id_** ), the function to
    launch ( **func_** ), and whether the task has dependencies ( **has_dependency_**
    ). It also starts the asynchronous task in deferred launch mode by using **std::async**
    with the **std::launch::deferred** launch policy, meaning that the task is created
    but not started until needed. The returned future is stored in the **fut_** variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The asynchronous tasks started by **std::async** call the **operator()** of
    their own instance (the **this** object). When that happens, **wait_completion()**
    is called, checking whether all futures in the shared future vector, **deps_**
    , storing dependent tasks are valid by calling their **valid()** function, and
    if so, waiting for them to finish by calling the **get()** function. When all
    dependent tasks are complete, the **func_** function is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'There is also a new member function, **start()** , that waits for the **fut_**
    future created during the task construction when calling **std::async** . This
    will be used to trigger the pipeline by requesting the result of the last task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the example in the previous chapter, we also define a member function
    called **get_dependency()** that returns a shared future constructed from **fut_**
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'In the **main()** function, we define the pipeline by chaining task objects
    and setting their dependencies and the lambda function to run, **sleep1s** or
    **sleep2s** , following the diagram shown in *Figure 7* *.2* :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Starting the pipeline is as simple as getting the result from the last task’s
    future. We can do that by calling the **start()** method of **task5** . This will
    recursively call their dependency tasks by using the dependency vector and start
    the deferred asynchronous tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the output of executing the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We can see how the pipeline is created by calling each task’s constructor and
    getting the futures from previous dependent tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Then, when the pipeline is triggered, **task5** is started, starting **task3**
    , **task2** , and **task1** recursively. As **task1** has no dependencies, it
    doesn’t need to wait for any other task to run its work, so it completes, allowing
    **task2** to complete, and later **task3** .
  prefs: []
  type: TYPE_NORMAL
- en: Next, **task5** continues checking its dependent tasks, so it’s now **task4**
    ‘s turn to run. Since all **task4** ‘s dependent tasks are complete, **task4**
    just executes, allowing **task5** to run afterward, thus completing the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: This example can be improved by performing real computations and transferring
    results between tasks. Also, instead of deferred tasks, we could also think of
    stages with several parallel steps that can be computed in separate threads. Feel
    free to implement these improvements as an additional exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about **std::async** , how to use this function
    to execute asynchronous tasks, how to define its behavior by using launch policies,
    and how to handle exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: We also now understand how the futures returned by the async function can impact
    performance and how to use them wisely. Also, we saw how to limit the number of
    asynchronous tasks by the number of available threads in the system by using counting
    semaphores.
  prefs: []
  type: TYPE_NORMAL
- en: We also mentioned some scenarios where **std::async** might not be the best
    tool for the job.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we implemented several examples covering real-life scenarios, which
    is useful for parallelizing many common tasks.
  prefs: []
  type: TYPE_NORMAL
- en: With all the knowledge acquired in this chapter, now we know when (and when
    not) to use the **std::async** function to run asynchronous tasks in parallel,
    improving the overall performance of applications, achieving better computer resource
    usage, and reducing resource exhaustion.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to achieve asynchronous execution by
    using coroutines, which have been available since C++20.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Effective Modern C++* : *42 Specific Ways to Improve Your Use of C++11 and
    C++14* , Scott Meyers, O’Reilly Media, Inc., 1st Edition – Chapter 7 , Item 35
    and Item 36'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std::async** : [https://en.cppreference.com/w/cpp/thread/async](https://en.cppreference.com/w/cpp/thread/async)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std::launch** : [https://en.cppreference.com/w/cpp/thread/launch](https://en.cppreference.com/w/cpp/thread/launch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strassen algorithm: [https://en.wikipedia.org/wiki/Strassen_algorithm](https://en.wikipedia.org/wiki/Strassen_algorithm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karatsuba algorithm: [https://en.wikipedia.org/wiki/Karatsuba_algorithm](https://en.wikipedia.org/wiki/Karatsuba_algorithm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenBLAS: [https://www.openblas.net](https://www.openblas.net)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BLIS library: [https://github.com/flame/blis](https://github.com/flame/blis)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MapReduce: [https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
