- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processes, Threads, and Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asynchronous programming involves initiating operations without waiting for
    them to complete before moving on to the next task. This non-blocking behavior
    allows for developing highly responsive and efficient applications, capable of
    handling numerous operations simultaneously without unnecessary delays or wasting
    computational resources waiting for tasks to be finished.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming is very important, especially in the development of
    networked applications, user interfaces, and systems programming. It enables developers
    to create applications that can manage high volumes of requests, perform **Input/Output**
    ( **I/O** ) operations, or execute concurrent tasks efficiently, thereby significantly
    enhancing user experience and application performance.
  prefs: []
  type: TYPE_NORMAL
- en: The Linux operating system (in this book, we will focus on development on the
    Linux operating system when the code cannot be platform-independent), with its
    robust process management, native support for threading, and advanced I/O capabilities,
    is an ideal environment for developing high-performance asynchronous applications.
    These systems offer a rich set of features such as powerful APIs for process and
    thread management, non-blocking I/O, and **Inter-Process Communication** ( **IPC**
    ) mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is an introduction to the fundamental concepts and components essential
    for asynchronous programming within the Linux environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will explore the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Processes in Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services and daemons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threads and concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will possess a foundational understanding of
    the asynchronous programming landscape in Linux, setting the stage for deeper
    exploration and practical application in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Processes in Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A process can be defined as an instance of a running program. It includes the
    program’s code, all the threads belonging to this process (which are represented
    by the program counter), the stack (which is an area of memory containing temporary
    data such as function parameters, return addresses, and local variables), the
    heap, for memory allocated dynamically, and its data section containing global
    variables and initialized variables. Each process operates within its own virtual
    address space and is isolated from other processes, ensuring that its operations
    do not interfere directly with those of others.
  prefs: []
  type: TYPE_NORMAL
- en: Process life cycle – creation, execution, and termination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The life cycle of a process can be broken down into three primary stages: creation,
    execution, and termination:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creation** : A new process is created using the **fork()** system call, which
    creates a new process by duplicating an existing one. The parent process is the
    one that calls **fork()** , and the newly created process is the child. This mechanism
    is essential for the execution of new programs within the system and is a precursor
    to executing different tasks concurrently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execution** : After creation, the child process may execute the same code
    as the parent or use the **exec()** family of system calls to load and run a different
    program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the parent process has more than one thread of execution, only the thread
    calling **fork()** is duplicated in the child process. Consequently, the child
    process contains a single thread: the one that executed the **fork()** system
    call.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since only the thread that called **fork()** is copied to the child, any **Mutual
    Exclusions** ( **mutexes** ), condition variables, or other synchronization primitives
    that were held by other threads at the time of the fork remain in their then-current
    state in the parent but do not carry over to the child. This can lead to complex
    synchronization issues, as mutexes that were locked by other threads (which do
    not exist in the child) might remain in a locked state, potentially causing deadlocks
    if the child tries to unlock or wait on these primitives.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this stage, the process performs its designated operations such as reading
    from or writing to files and communicating with other processes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Termination** : A process terminates either voluntarily, by calling the **exit()**
    system call, or involuntarily, due to receiving a signal from another process
    that causes it to stop. Upon termination, the process returns an exit status to
    its parent process and releases its resources back to the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process life cycle is integral to asynchronous operations as it enables
    the concurrent execution of multiple tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Each process is uniquely identified by a **Process ID** ( **PID** ), an integer
    that the kernel uses to manage processes. PIDs are used to control and monitor
    processes. Parent processes also use PIDs to communicate with or control the execution
    of child processes, such as waiting for them to terminate or sending signals.
  prefs: []
  type: TYPE_NORMAL
- en: Linux provides mechanisms for process control and signaling, allowing processes
    to be managed and communicated with asynchronously. Signals are one of the primary
    means of IPC, enabling processes to interrupt or to be notified of events. For
    example, the **kill** command can send signals to stop a process or to prompt
    it to reload its configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: Process scheduling is how the Linux kernel allocates CPU time to processes.
    The scheduler determines which process runs at any given time, based on scheduling
    algorithms and policies that aim to optimize for factors such as responsiveness
    and efficiency. Processes can be in various states, such as running, waiting,
    or stopped, and the scheduler transitions them between these states to manage
    execution efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring IPC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the Linux operating system, processes operate in isolation, meaning that
    they cannot directly access the memory space of other processes. This isolated
    nature of processes presents challenges when multiple processes need to communicate
    and synchronize their actions. To address these challenges, the Linux kernel provides
    a versatile set of IPC mechanisms. Each IPC mechanism is tailored to suit different
    scenarios and requirements, enabling developers to build complex, high-performance
    applications that leverage asynchronous processing effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding these IPC techniques is crucial for developers aiming to create
    scalable and efficient applications. IPC allows processes to exchange data, share
    resources, and coordinate their activities, facilitating smooth and reliable communication
    between different components of a software system. By utilizing the appropriate
    IPC mechanism, developers can achieve improved throughput, reduced latency, and
    enhanced concurrency in their applications, leading to better performance and
    user experiences.
  prefs: []
  type: TYPE_NORMAL
- en: In a multitasking environment, where multiple processes run concurrently, IPC
    plays a vital role in enabling the efficient and coordinated execution of tasks.
    For example, consider a web server application that handles multiple concurrent
    requests from clients. The web server process might use IPC to communicate with
    the child processes responsible for processing each request. This approach allows
    the web server to handle multiple requests simultaneously, improving the overall
    performance and scalability of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Another common scenario where IPC is essential is in distributed systems or
    microservice architectures. In such environments, multiple independent processes
    or services need to communicate and collaborate to achieve a common goal. IPC
    mechanisms such as message queues and sockets or **Remote Procedure Calls** (
    **RPCs** ) enable these processes to exchange messages, invoke methods on remote
    objects, and synchronize their actions, ensuring seamless and reliable IPC.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the IPC mechanisms provided by the Linux kernel, developers can
    design systems where multiple processes can work together harmoniously. This enables
    the creation of complex, high-performance applications that utilize system resources
    efficiently, handle concurrent tasks effectively, and scale to meet increasing
    demands effortlessly.
  prefs: []
  type: TYPE_NORMAL
- en: IPC mechanisms in Linux
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linux supports several IPC mechanisms, each with its unique characteristics
    and use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental IPC mechanisms supported by the Linux operating system include
    shared memory, which is commonly employed for process communication on a single
    server, and sockets, which facilitate inter-server communication. There are other
    mechanisms (which are briefly described here), but shared memory and sockets are
    the most commonly used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipes and named pipes** : Pipes are one of the simplest forms of IPC, allowing
    for unidirectional communication between processes. A named pipe, or **First-in-First-out**
    ( **FIFO** ), extends this concept by providing a pipe that is accessible via
    a name in the filesystem, allowing unrelated processes to communicate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Signals** : Signals are a form of software interrupt that can be sent to
    a process to notify it of events. While they are not a method for transferring
    data, signals are useful for controlling process behavior and triggering actions
    within processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Message queues** : Message queues allow processes to exchange messages in
    a FIFO manner. Unlike pipes, message queues support asynchronous communication,
    whereby messages are stored in a queue and can be retrieved by the receiving process
    at its convenience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semaphores** : Semaphores are used for synchronization, helping processes
    manage access to shared resources. They prevent race conditions by ensuring that
    only a specified number of processes can access a resource at any given time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shared memory** : Shared memory is a fundamental concept in IPC that enables
    multiple processes to access and manipulate the same segment of physical memory.
    It offers a blazing-fast method for exchanging data between different processes,
    reducing the need for time-consuming data copying operations. This technique is
    particularly advantageous when dealing with large datasets or requiring high-speed
    communication. The mechanism of shared memory involves creating a shared memory
    segment, which is a dedicated portion of physical memory accessible by multiple
    processes. This shared memory segment is treated as a common workspace, allowing
    processes to read, write, and collaboratively modify data. To ensure data integrity
    and prevent conflicts, shared memory requires synchronization mechanisms such
    as semaphores or mutexes. These mechanisms regulate access to the shared memory
    segment, preventing multiple processes from simultaneously modifying the same
    data. This coordination is crucial to maintain data consistency and avoid overwriting
    or corruption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared memory is often the preferred IPC mechanism in single-server environments
    where performance is paramount. Its primary advantage lies in its speed. Since
    data is directly shared in physical memory without the need for intermediate copying
    or context switching, it significantly reduces communication overhead and minimizes
    latency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: However, shared memory also comes with certain considerations. It requires careful
    management to prevent race conditions and memory leaks. Processes accessing shared
    memory must adhere to well-defined protocols to ensure data integrity and avoid
    deadlocks. Additionally, shared memory is typically implemented as a system-level
    feature, requiring specific operating system support and potentially introducing
    platform-specific dependencies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Despite these considerations, shared memory remains a powerful and widely used
    IPC technique, particularly in applications where speed and performance are critical
    factors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sockets** : Sockets are a fundamental mechanism for IPC in operating systems.
    They provide a way for processes to communicate with each other, either within
    the same machine or across networks. Sockets are used to establish and maintain
    connections between processes, and they support both **connection-oriented** and
    **connectionless communication** .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connection-oriented communication is a type of communication in which a reliable
    connection is established between two processes before any data is transferred.
    This type of communication is often used for applications such as file transfer
    and remote login, where it is important to ensure that all data is delivered reliably
    and in the correct order. Connectionless communication is a type of communication
    in which no reliable connection is established between two processes before data
    is transferred. This type of communication is often used for applications such
    as streaming media and real-time gaming, where it is more important to have low
    latency than to guarantee reliable delivery of all data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sockets are the backbone of networked applications. They are used by a wide
    variety of applications, including web browsers, email clients, and file-sharing
    applications. Sockets are also used by many operating system services, such as
    the **Network File System** ( **NFS** ) and the **Domain Name** **System** ( **DNS**
    ).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here are some of the key benefits of using sockets:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Reliability** : Sockets provide a reliable way to communicate between processes,
    even when those processes are located on different machines.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability** : Sockets can be used to support a large number of concurrent
    connections, making them ideal for applications that need to handle a lot of traffic.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility** : Sockets can be used to implement a wide variety of communication
    protocols, making them suitable for a wide range of applications.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use in IPC** : Sockets are a powerful tool for IPC. They are used by a wide
    variety of applications and are essential for building scalable, reliable, and
    flexible networked applications.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices-based applications are an example of asynchronous programming
    using different processes communicating between them in an asynchronous way. A
    simple example would be a log processor. Different processes generate log entries
    and send them to another process for further processing such as special formatting,
    deduplication, and statistics. The producers just send the lines of the log without
    waiting for any reply from the process they are sending to the log.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw processes in Linux, their life cycles, and how IPC is
    implemented by the operating system. In the next section, we will introduce a
    special kind of Linux process called **daemons** .
  prefs: []
  type: TYPE_NORMAL
- en: Services and daemons in Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the realm of Linux operating systems, daemons are a fundamental component
    that runs quietly in the background, silently executing essential tasks without
    the direct involvement of an interactive user. These processes are traditionally
    identified by their names ending with the letter *d* , such as **sshd** for the
    **Secure Shell** ( **SSH** ) daemon and **httpd** for the **web server daemon**
    . They play a vital role in handling system-level tasks crucial for both the operating
    system and the applications running on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Daemons serve an array of purposes, ranging from file serving, web serving,
    and network communications to logging and monitoring services. They are designed
    to be autonomous and resilient, starting at system boot and running continuously
    until the system is shut down. Unlike regular processes initiated and controlled
    by users, daemons possess distinct characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Background operation** :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daemons operate in the background
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They lack a controlling terminal for direct user interaction
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They do not require a user interface or manual intervention to perform their
    tasks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User independence** :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daemons operate independently of user sessions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They function autonomously without direct user involvement
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They wait for system events or specific requests to trigger their actions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task-oriented focus** :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each daemon is tailored to execute a specific task or a set of tasks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They are designed to handle specific functions or listen for particular events
    or requests
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This ensures efficient task execution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creating a daemon process involves more than merely running a process in the
    background. To ensure effective operation as a daemon, developers must consider
    several key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Detaching from the terminal** : The **fork()** system call is employed to
    detach the daemon from the terminal. The parent process exits after the fork,
    leaving the child process running in the background.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Session creation** : The **setsid()** system call creates a new session and
    designates the calling process as the leader of both the session and the process
    group. This step is crucial for complete detachment from the terminal.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Working directory change** : To prevent blocking the unmounting of the filesystem,
    daemons typically change their working directory to the root directory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**File descriptor handling** : Inherited file descriptors are closed by daemons,
    and **stdin** , **stdout** , and **stderr** are often redirected to **/dev/null**
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Signal handling** : Proper handling of signals, such as **SIGHUP** for configuration
    reloading or **SIGTERM** for graceful shutdown, is essential for effective daemon
    management.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Daemons communicate with other processes or daemons through various IPC mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Daemons are integral to the architecture of many asynchronous systems, providing
    essential services without direct user interaction. Some prominent use cases of
    daemons include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Web servers** : Daemons such as **httpd** and nginx serve web pages in response
    to client requests, handling multiple requests concurrently and ensuring seamless
    web browsing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database servers** : Daemons such as mysqld and postgresql manage database
    services, allowing for asynchronous access and manipulation of databases by various
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**File servers** : Daemons such as **smbd** and **nfsd** provide networked
    file services, enabling asynchronous file sharing and access across different
    systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logging and monitoring** : Daemons such as **syslogd** and **snmpd** collect
    and log system events, providing asynchronous monitoring of system health and
    performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, daemons are essential components of Linux systems, silently performing
    critical tasks in the background to ensure smooth system operation and efficient
    application execution. Their autonomous nature and resilience make them indispensable
    for maintaining system stability and providing essential services to users and
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen processes and demons, a special type of process. A process can
    have one or more threads of execution. In the next section, we will be introducing
    threads.
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processes and threads represent two fundamental ways of executing code concurrently,
    but they differ significantly in their operation and resource management. A process
    is an instance of a running program that owns its private set of resources, including
    memory, file descriptors, and execution context. Processes are isolated from each
    other, providing robust stability across the system since the failure of one process
    generally does not affect others.
  prefs: []
  type: TYPE_NORMAL
- en: Threads are a fundamental concept in computer science, representing a lightweight
    and efficient way to execute multiple tasks within a single process. In contrast
    to processes, which are independent entities with their own private memory space
    and resources, threads are closely intertwined with the process they belong to.
    This intimate relationship allows threads to share the same memory space and resources,
    including file descriptors, heap memory, and any other global data structures
    allocated by the process.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of threads is their ability to communicate and share
    data efficiently. Since all threads within a process share the same memory space,
    they can directly access and modify common variables without the need for complex
    IPC mechanisms. This shared environment enables rapid data exchange and facilitates
    the implementation of concurrent algorithms and data structures.
  prefs: []
  type: TYPE_NORMAL
- en: However, sharing the same memory space also introduces the challenge of managing
    access to shared resources. To prevent data corruption and ensure the integrity
    of shared data, threads must employ synchronization mechanisms such as locks,
    semaphores, or mutexes. These mechanisms enforce rules and protocols for accessing
    shared resources, ensuring that only one thread can access a particular resource
    at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: Effective synchronization is crucial in multithreaded programming to avoid race
    conditions, deadlocks, and other concurrency-related issues.
  prefs: []
  type: TYPE_NORMAL
- en: To address these challenges, various synchronization primitives and techniques
    have been developed. These include mutexes, which provide exclusive access to
    a shared resource, semaphores, which allow for controlled access to a limited
    number of resources, and condition variables, which enable threads to wait for
    specific conditions to be met before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: By carefully managing synchronization and employing appropriate concurrency
    patterns, developers can harness the power of threads to achieve high performance
    and scalability in their applications. Threads are particularly well-suited for
    tasks that can be parallelized, such as image processing, scientific simulations,
    and web servers, where multiple independent computations can be executed concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Threads, as described previously, are system threads. This means that they are
    created and managed by the kernel. However, there are scenarios, which we will
    explore in depth in [*Chapter 8*](B22219_08.xhtml#_idTextAnchor164) , where we
    will require a multitude of threads. In such cases, the system might not have
    sufficient resources to create numerous system threads. The solution to this problem
    is the use of **user threads** . One approach to implementing user threads is
    through **coroutines** , which have been included in the C++ standard since C++20.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines are a relatively new feature in C++. Coroutines can be defined as
    functions that can be paused and resumed at specific points, allowing for cooperative
    multitasking within a single thread. Unlike standard functions that run from start
    to finish without interruption, coroutines can suspend their execution and yield
    control back to the caller, which can later resume the coroutine from the point
    it was paused.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines are much more lightweight than system threads. This means that they
    can be created and destroyed much more quickly, and that they require less overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines are cooperative, which means that they must explicitly yield control
    to the caller in order to switch execution context. This can be a disadvantage
    in some cases, but it can also be an advantage, as it gives the user program more
    control over the execution of coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines can be used to create a variety of different concurrency patterns.
    For example, coroutines can be used to implement tasks, which are lightweight
    work units that can be scheduled and run concurrently. Coroutines can also be
    used to implement channels, which are communication channels that can pass data
    between them.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines can be classified into stackful and stackless categories. C++20 coroutines
    are stackless. We will see these concepts in depth in [*Chapter 8*](B22219_08.xhtml#_idTextAnchor164)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Overall, coroutines are a powerful tool for creating concurrent programs in
    C++. They are lightweight, cooperative, and can be used to implement a variety
    of different concurrency patterns. They cannot be used to implement parallelism
    entirely because coroutines still need CPU execution context, which can be only
    provided by a thread.
  prefs: []
  type: TYPE_NORMAL
- en: Thread life cycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The life cycle of a system thread, often referred to as a lightweight process,
    encompasses the stages from its creation until its termination. Each stage plays
    a crucial role in managing and utilizing threads in a concurrent programming environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creation** : This phase begins when a new thread is created in the system.
    The creation process involves using the function, which takes several parameters.
    One critical parameter is the thread’s attributes, such as its scheduling policy,
    stack size, and priority. Another essential parameter is the function that the
    thread will execute, known as the start routine. Upon its successful creation,
    the thread is allocated its own stack and other resources.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Execution** : After creation, the thread starts executing its assigned start
    routine. During execution, the thread can perform various tasks independently
    or interact with other threads if necessary. Threads can also create and manage
    their own local variables and data structures, making them self-contained and
    capable of performing specific tasks concurrently.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Synchronization** : To ensure orderly access to shared resources and prevent
    data corruption, threads employ synchronization mechanisms. Common synchronization
    primitives include locks, semaphores, and barriers. Proper synchronization allows
    threads to coordinate their activities, avoiding race conditions, deadlocks, and
    other issues that can arise in concurrent programming.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Termination** : A thread can terminate in several ways. It can explicitly
    call the function to terminate itself. It can also terminate by returning from
    its start routine. In some cases, a thread can be canceled by another thread using
    the function. Upon termination, the system reclaims the resources allocated to
    the thread, and any pending operations or locks held by the thread are released.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding the life cycle of a system thread is essential for designing and
    implementing concurrent programs. By carefully managing thread creation, execution,
    synchronization, and termination, developers can create efficient and scalable
    applications that leverage the benefits of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Thread scheduling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: System threads, managed by the operating system kernel’s scheduler, are scheduled
    preemptively. The scheduler decides when to switch execution between threads based
    on factors such as thread priority, allocated time, or mutex blocking. This context
    switch, controlled by the kernel, can incur significant overhead. The high cost
    of context switches, coupled with the resource usage of each thread (such as its
    own stack), makes coroutines a more efficient alternative for some applications
    because we can run more than one coroutine in a single thread.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines offer several advantages. First, they reduce the overhead associated
    with context switches. Since context switching on coroutine yield or await is
    handled by the user space code rather than the kernel, the process is more lightweight
    and efficient. This results in significant performance gains, especially in scenarios
    where frequent context switching occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines also provide greater control over thread scheduling. Developers can
    define custom scheduling policies based on the specific requirements of their
    application. This flexibility allows for fine-tuned thread management, resource
    utilization optimization, and desired performance characteristics achievement.
  prefs: []
  type: TYPE_NORMAL
- en: Another important feature of coroutines is that they are generally more lightweight
    compared to system threads. Coroutines don’t maintain their own stack, which is
    a great resource consumption advantage, making them suitable for resource-constrained
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, coroutines offer a more efficient and flexible approach to thread management,
    particularly in situations where frequent context switching is required or where
    fine-grained control over thread scheduling is essential. Threads can access the
    memory process and this memory is shared among all the threads, so we need to
    be careful and control memory access. This control is achieved by different mechanisms
    called synchronization primitives.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization primitives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Synchronization primitives are essential tools for managing concurrent access
    to shared resources in multithreaded programming. There are several synchronization
    primitives, each with its own specific purpose and characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutexes** : Mutexes are used to enforce exclusive access to critical sections
    of code. A mutex can be locked by a thread, preventing other threads from entering
    the protected section until the mutex is unlocked. Mutexes guarantee that only
    one thread can execute the critical section at any given time, ensuring data integrity
    and preventing race conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semaphores** : Semaphores are more versatile than mutexes and can be used
    for a wider range of synchronization tasks, including signaling between threads.
    A semaphore maintains an integer counter that can be incremented (signaling) or
    decremented (waiting) by threads. Semaphores allow for more complex coordination
    patterns, such as counting semaphores (for resource allocation) and binary semaphores
    (similar to mutexes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Condition variables** : Condition variables are used for thread synchronization
    based on specific conditions. Threads can block (wait on) a condition variable
    until a particular condition becomes true. Other threads can signal the condition
    variable, causing waiting threads to wake up and continue execution. Condition
    variables are often used in conjunction with mutexes to achieve more fine-grained
    synchronization and avoid busy waiting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Additional synchronization primitives** : In addition to the core synchronization
    primitives discussed previously, there are several other synchronization mechanisms:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Barriers** : Barriers allow a group of threads to synchronize their execution,
    ensuring that all threads reach a certain point before proceeding further'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Read-write locks** : Read-write locks provide a way to control concurrent
    access to shared data, allowing multiple readers but only a single writer at a
    time'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spinlocks** : Spinlocks are a type of mutex that involves busy waiting, continuously
    checking a memory location until it becomes available'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Chapters 4* and *5* , we will see the synchronization primitives implemented
    in the C++ **Standard Template Library** ( **STL** ) in depth and examples of
    how to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right synchronization primitive
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The choice of the appropriate synchronization primitive depends on the specific
    requirements of the application and the nature of the shared resources being accessed.
    Here are some general guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutexes** : Use mutexes when exclusive access to a critical section is required
    to ensure data integrity and prevent race conditions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semaphores** : Use semaphores when more complex coordination patterns are
    needed, such as resource allocation or signaling between threads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Condition variables** : Use condition variables when threads need to wait
    for a specific condition to become true before proceeding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective use of synchronization primitives is crucial for developing safe and
    efficient multithreaded programs. By understanding the purpose and characteristics
    of different synchronization mechanisms, developers can choose the most suitable
    primitives for their specific needs and achieve reliable and predictable concurrent
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: Common problems when using multiple threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threading introduces several challenges that must be managed to ensure application
    correctness and performance. These challenges arise from the inherent concurrency
    and non-deterministic nature of multithreaded programming.
  prefs: []
  type: TYPE_NORMAL
- en: '**Race conditions** occur when multiple threads access and modify shared data
    concurrently. The outcome of a race condition depends on the non-deterministic
    sequencing of threads’ operations, which can lead to unpredictable and inconsistent
    results. For example, consider two threads that are updating a shared counter.
    If the threads increment the counter concurrently, the final value may be incorrect
    due to a race condition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deadlocks** occur when two or more threads wait indefinitely for resources
    held by each other. This creates a cycle of dependencies that cannot be resolved,
    causing the threads to become permanently blocked. For instance, consider two
    threads that are waiting for each other to release locks on shared resources.
    If neither thread releases the lock it holds, a deadlock occurs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Starvation** occurs when a thread is perpetually denied access to resources
    it needs to make progress. This can happen when other threads continuously acquire
    and hold resources, leaving the starved thread unable to execute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Livelocks** are like deadlocks, but instead of being permanently blocked,
    the threads remain active and repeatedly try to acquire resources, only without
    making any progress.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Several techniques can be used to manage the challenges of threading, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synchronization mechanisms** : As described previously, synchronization primitives
    such as locks and mutexes can be used to control access to shared data and ensure
    that only one thread can access the data at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deadlock prevention and detection** : Deadlock prevention algorithms can
    be used to avoid deadlocks, while deadlock detection algorithms can be used to
    identify and resolve deadlocks when they occur.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Thread scheduling** : Thread scheduling algorithms can be used to determine
    which thread should run at any given time, as well as which can help to prevent
    starvation and improve application performance. We will see the different solutions
    to multithreading issues in much more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategies for effective thread management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are different ways to handle threads to avoid multithreading issues.
    The following are some of the most common ways to handle threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Minimize shared state** : Designing threads to operate on private data as
    much as possible significantly reduces the need for synchronization. By allocating
    memory for thread-specific data using thread-local storage, the need for global
    variables is eliminated, further reducing the potential for data contention. Careful
    management of shared data access through synchronization primitives is essential
    to ensure data integrity. This approach enhances the efficiency and correctness
    of multithreaded applications by minimizing the need for synchronization and ensuring
    that shared data is accessed in a controlled and consistent manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lock hierarchy** : Establishing a well-defined lock hierarchy is crucial
    for preventing deadlocks in multithreaded programming. A lock hierarchy dictates
    the order in which locks are acquired and released, ensuring a consistent locking
    pattern across threads. By acquiring locks in a hierarchical manner, from the
    coarsest to the finest granularity, the possibility of deadlocks is significantly
    reduced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The coarsest level of granularity refers to locks that control access to a large
    portion of the shared resource, while the finest granularity locks are used for
    specific, fine-grained parts of the resource. By acquiring the coarse-grained
    lock first, threads can gain exclusive access to a larger section of the resource,
    reducing the likelihood of conflicts with other threads attempting to access the
    same resource. Once the coarse-grained lock is acquired, finer-grained locks can
    be obtained to control access to specific parts of the resource, providing more
    granular control and reducing the waiting time for other threads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In some cases, lock-free data structures can be employed to eliminate the need
    for locks altogether. Lock-free data structures are designed to provide concurrent
    access to shared resources without explicit locks. Instead, they rely on atomic
    operations and non-blocking algorithms to ensure data integrity and consistency.
    By utilizing lock-free data structures, the overhead associated with lock acquisition
    and release is eliminated, resulting in improved performance and scalability in
    multithreaded applications:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Timeouts** : To prevent threads from waiting indefinitely when trying to
    acquire a lock, it is important to set timeouts for lock acquisition. This ensures
    that if a thread cannot acquire the lock within the specified timeout period,
    it will automatically give up and try again later. This helps prevent deadlocks
    and ensures that no thread is left waiting indefinitely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Thread pools** : Managing a pool of reusable threads is a key technique for
    optimizing the performance of multithreaded applications. By creating and destroying
    threads dynamically, the overhead of thread creation and termination can be reduced
    significantly. The size of the thread pool should be tuned based on the application’s
    workload and resource constraints. A too-small pool may result in tasks waiting
    for available threads, while a too-large pool may waste resources. Work queues
    are used to manage tasks and assign them to available threads in the pool. Tasks
    are added to the queue and processed by the threads in a FIFO order. This ensures
    fairness and prevents the starvation of tasks. The use of work queues also allows
    for load balancing, as tasks can be distributed evenly across the available threads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synchronization primitives** : Understand the different types of synchronization
    primitives, such as mutexes, semaphores, and condition variables. Choose the appropriate
    primitive based on the synchronization requirements of the specific scenario.
    Use synchronization primitives correctly to avoid race conditions and deadlocks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing and debugging** : Test multi-threaded applications thoroughly to
    identify and fix threading issues. Use tools such as thread sanitizers and profilers
    to detect data races and performance bottlenecks. Employ debugging techniques
    such as step-by-step execution and thread dumps to analyze and resolve threading
    problems. We will see testing and debugging in *Chapters 11* and *12* .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability and performance considerations** : Design thread-safe data structures
    and algorithms to ensure scalability and performance. Balance the number of threads
    with the available resources to avoid over-subscription. Monitor system metrics
    such as CPU utilization and thread contention to identify potential performance
    bottlenecks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communication and collaboration** : Foster collaboration among developers
    working on multi-threaded code to ensure consistency and correctness. Establish
    coding guidelines and best practices for thread management to maintain code quality
    and readability. Regularly review and update the threading strategy as the application
    evolves.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threading is a powerful tool that can be used to improve the performance and
    scalability of applications. However, it is important to understand the challenges
    of threading and to use appropriate techniques to manage these challenges. By
    doing so, developers can create multithreaded applications that are correct, efficient,
    and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the concept of processes in operating systems.
    Processes are fundamental entities that execute programs and manage resources
    on the computer. We delved into the process life cycle, examining the various
    stages a process goes through from creation to termination. Additionally, we discussed
    IPC, which is crucial for processes to interact and exchange information with
    each other.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we introduced daemons in the context of Linux operating systems.
    Daemons are special types of processes that run in the background as services
    and perform specific tasks such as managing system resources, handling network
    connections, or providing other essential services to the system. We also explored
    the concepts of system and user threads, which are lightweight processes that
    share the same address space as the parent process. We discussed the advantages
    of multithreaded applications, including improved performance and responsiveness,
    as well as the challenges associated with managing and synchronizing multiple
    threads within a single process.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing the different issues created by multithreading is fundamental to understanding
    how to fix them. In the next chapter, we will see how to create threads, and then
    in [*Chapter 4*](B22219_04.xhtml#_idTextAnchor074) and [*Chapter 5*](B22219_05.xhtml#_idTextAnchor097)
    , we will study the different synchronization primitives the standard C++ offers
    and their different applications in depth.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Butenhof, 1997] David R. Butenhof, *Programming with POSIX Threads* , Addison
    Wesley, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kerrisk, 2010] Michael Kerrisk, *The Linux Programming Interface* , No Starch
    Press, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stallings, 2018] William Stallings, *Operating Systems Internals and Design
    Principles* , Ninth Edition, Pearson Education 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Williams, 2019] Anthony Williams, *C++ Concurrency in Action* , Second Edition,
    Manning 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2: Advanced Thread Management and Synchronization Techniques'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we build upon the foundational knowledge of parallel programming
    and dive deeper into advanced techniques for managing threads and synchronizing
    concurrent operations. We will explore essential concepts such as thread creation
    and management, exception handling across threads, and efficient thread coordination,
    acquiring a solid understanding of key synchronization primitives, including mutexes,
    semaphores, condition variables, and atomic operations. All this knowledge will
    equip us with the tools needed to implement both lock-based and lock-free multithreaded
    solutions, offering a glimpse into high-performance concurrent systems, and providing
    the skills necessary to avoid common pitfalls such as race conditions, deadlocks,
    and livelocks when managing multithreaded systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B22219_03.xhtml#_idTextAnchor051) , *How to Create and Manage
    Threads in C++*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B22219_04.xhtml#_idTextAnchor074) , *Thread Synchronization with
    Locks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B22219_05.xhtml#_idTextAnchor097) , *Atomic Operations*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
