<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-196"><a id="_idTextAnchor213"/>13</h1>
<h1 id="_idParaDest-197"><a id="_idTextAnchor214"/>Revisiting Shadows with Ray Tracing</h1>
<p>In this chapter, we are going to implement<a id="_idIndexMarker675"/> shadows using <strong class="bold">ray tracing</strong>. In <a href="B18395_08.xhtml#_idTextAnchor116"><em class="italic">Chapter 8</em></a>, <em class="italic">Adding Shadows Using Mesh Shaders</em>, we used traditional shadow mapping techniques to get the visibility from each light and use that information to compute the shadow term for the final image. Using ray tracing for shadows allows us to get more detailed results and to have finer-grained control over the quality of results based on the distance and intensity of each light.</p>
<p>We are going to implement two techniques: the first one is similar to the one used in offline rendering, where we shoot rays to each light to determine visibility. While this approach gives us the best results, it can be quite expensive depending on the number of lights in the scene.</p>
<p>The second technique is based on a recent article <a id="_idIndexMarker676"/>from <strong class="bold">Ray Tracing Gems</strong>. We use some heuristics to determine how many rays we need to cast per light, and we combine the results with spatial and temporal filters to make the result stable.</p>
<p>In this chapter, we’ll cover the following main topics:</p>
<ul>
<li>Implementing simple ray-traced shadows</li>
<li>Implementing an advanced technique for ray-traced shadows</li>
</ul>
<h1 id="_idParaDest-198"><a id="_idTextAnchor215"/>Technical requirements</h1>
<p>By the end of this chapter you will learn how to implement basic ray-traced shadows. You will also become familiar with a more advanced technique that is capable of rendering multiple lights with soft shadows.</p>
<p>The code for this chapter can be found at the following URL: <a href="https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter13">https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter13</a>.</p>
<h1 id="_idParaDest-199"><a id="_idTextAnchor216"/>Implementing simple ray-traced shadows</h1>
<p>As we <a id="_idIndexMarker677"/>mentioned in the introduction, shadow mapping techniques have been a staple of real-time rendering for many years. Before the introduction of ray tracing capabilities in GPUs, using other techniques was simply too expensive.</p>
<p>This hasn’t prevented the graphics community from coming up with clever solutions to increase the quality of results while maintaining a low cost. The main issue with traditional techniques is that they are based on capturing depth buffers from the point of view of each light. This works well for objects that are near the light and camera, but as we move further away, depth discontinuities lead to artefacts in the final result.</p>
<p>Solutions to this <a id="_idIndexMarker678"/>problem include filtering the result – for<a id="_idIndexMarker679"/> instance, using <strong class="bold">Percentage Closer Filtering</strong> (<strong class="bold">PCF</strong>) or <strong class="bold">Cascade Shadow Maps</strong> (<strong class="bold">CSM</strong>). This technique requires capturing multiple depth <em class="italic">slices</em> – the casc<a id="_idTextAnchor217"/>ades to maintain enough resolution as we move further away from the light. This is usually employed only for sunlight, as it can require a lot of memory and time to re-render the scene multiple times. It can also be quite difficult to get good results on the boundaries between cascades.</p>
<p>The other main issue with shadow mapping is that it can be difficult to get hard shadows because of the resolution of the depth buffer and the discontinuities it introduces. We can alleviate these issues with ray tracing. Offline rendering has used ray and path tracing for many years to achieve photo-realistic effects, including shadows.</p>
<p>They, of course, have the luxury of being able to wait for hours or days for a single frame to complete, but we can get similar results in real time. In the previous chapter, we used the <code>vkCmdTraceRaysKHR</code> command to cast rays into a scene.</p>
<p>For this implementation, we are introducing ray queries, which allow us to traverse the Acceleration Structures we set up from a fragment and compute shaders.</p>
<p>We are going to modify the <code>calculate_point_light_contribution</code> method of our lighting pass to determine which lights each fragment can see and determine the final shadow term.</p>
<p>First, we need to enable the <code>VK_KHR_ray_query</code> device extension. We also need to enable the related shader extension:</p>
<pre class="source-code">
#extension GL_EXT_ray_query : enable</pre>
<p>Then, instead of computing the cube map from each light point of view, we simply cast a ray from the fragment world position to each light.</p>
<p>We start by initializing a <code>rayQueryEXT</code> object:</p>
<pre class="source-code">
rayQueryEXT rayQuery;
rayQueryInitializeEXT(rayQuery, as, gl_RayFlagsOpaqueEXT |
    gl_RayFlagsTerminateOnFirstHitEXT, 0xff,
        world_position, 0.001, l, d);</pre>
<p>Notice the <code>gl_RayFlagsTerminateOnFirstHitEXT</code> parameter, as we are <a id="_idTextAnchor218"/>only interested in the first hit for this ray. <code>l</code> is the direction from <code>world_position</code> to the light and we use a small offset from the ray origin to avoid self-intersection.</p>
<p>The last parameter, <code>d</code>, is the distance from <code>world_position</code> to the light position. It’s important to specify this value, as the ray query could report intersections past the light position otherwise, and we could incorrectly mark a fragment to be in shadow.</p>
<p>Now that we <a id="_idTextAnchor219"/>have initialized the ray query object, we call the following method to start scene traversal:</p>
<pre class="source-code">
rayQueryProceedEXT( rayQuery );</pre>
<p>This will<a id="_idIndexMarker680"/> return either when a hit is found or when the ray terminates. When using ray queries, we don’t have to specify a shader binding table. To determine the result of ray traversal, there are several methods we can use to query the outcome. In our case, we only want to know whether the ray hit any geometry:</p>
<pre class="source-code">
if ( rayQueryGetIntersectionTypeEXT( rayQuery, true ) ==
    gl_RayQueryCommittedIntersectionNoneEXT ) {
        shadow = 1.0;
}</pre>
<p>If not, it means we can see the light we are processing from this fragment, and we can account for this light contribution in the final computation. We repeat this for each light to obtain the overall shadow term.</p>
<p>While this implementation is really simple, it mainly works for point lights. For other types of light – area lights, for instance – we would need to cast multiple rays to determin<a id="_idTextAnchor220"/>e the visibility. As the number of lights increases, it can become too expensive to use this simple<a id="_idIndexMarker681"/> technique.</p>
<p>In this section, we have demonstrated a simple implementation to get started with real-time ray-traced shadows. In the next section, we are going to introduce a new technique that scales better and can support multiple types of light.</p>
<h1 id="_idParaDest-200"><a id="_idTextAnchor221"/>Improving ray-traced shadows</h1>
<p>In the <a id="_idIndexMarker682"/>previous section, we described a simple algorithm that can be used to compute the visibility term in our scene. As we mentioned, this doesn’t scale well for a large number of lights and can require a large number of samples for different types of light.</p>
<p>In this section, we are going to implement a different algorithm inspired by the article <em class="italic">Ray Traced Shadows</em> in the <em class="italic">Ray Tracing Gems</em> book. As will be common in this chapter and upcoming chapters, the main idea is to spread the computation cost over time.</p>
<p>This can still lead to noisy results, as we are still using a low number of samples. To achieve the quality we are looking for, we are going to make use of spatial and temporal filtering, similar to what we did in <a href="B18395_11.xhtml#_idTextAnchor178"><em class="italic">Chapter 11</em></a>, <em class="italic">Temporal Anti-Aliasing</em>.</p>
<p>The technique is implemented ove<a id="_idTextAnchor222"/>r three passes, and we are also going to leverage motion vectors. We are now going to explain each <a id="_idTextAnchor223"/>pass in detail.</p>
<h2 id="_idParaDest-201"><a id="_idTextAnchor224"/>Motion vectors</h2>
<p>As we saw in <a href="B18395_11.xhtml#_idTextAnchor178"><em class="italic">Chapter 11</em></a>, <em class="italic">Temporal Anti-Aliasing</em>, motion vectors are needed to determine how far an<a id="_idIndexMarker683"/> object at a given fragment has moved between frames. We need <a id="_idIndexMarker684"/>this information to determine which information to keep and which to discard for our computation. This helps us avoid ghosting artifacts in the final image.</p>
<p>For the technique in this chapter, we need to compute motion vectors differently compared <a id="_idIndexMarker685"/>to <strong class="bold">Temporal Anti-Aliasing</strong> (<strong class="bold">TAA</strong>). We first compute the proportional difference of depth between the two frames:</p>
<pre class="source-code">
float depth_diff = abs( 1.0 - ( previous_position_ndc.z /
    current_position_ndc.z ) );</pre>
<p>Next, we compute an epsilon value that will be used to determine acceptable changes in depth:</p>
<pre class="source-code">
float c1 = 0.003;
float c2 = 0.017;
float eps = c1 + c2 * abs( view_normal.z );</pre>
<p>Finally, we use these two values to decide whether the reprojection was successful:</p>
<pre class="source-code">
vec2 visibility_motion = depth_diff &lt; eps ? vec2(
    current_position_ndc.xy - previous_position_ndc.xy ) :
        vec2( -1, -1 );</pre>
<p>The following figure <a id="_idIndexMarker686"/>shows the result of this <a id="_idIndexMarker687"/>computation:</p>
<div><div><img alt="Figure 13.1 – The motion vector’s texture" height="588" src="img/B18395_13_01.jpg" width="1092"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – The motion vector’s texture</p>
<p>We are going to store this value in a texture for later use. The next step is to compute the variation in visibility for the past four frames.</p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor225"/>Computing visibility variance</h2>
<p>This technique uses<a id="_idIndexMarker688"/> data from the past four frames to determine how<a id="_idIndexMarker689"/> many samples are needed for each light for each fragment. We store the <code>visibility</code> values in a 3D RGBA16 texture, where each c<a id="_idTextAnchor226"/>hannel is the <code>visibility</code> value of the previous frames. Each layer stores the visibility history for individual lights.</p>
<p>This is one of the first compute shaders where we use a 3D dispatch size. It’s worth highlighting the <code>dispatch</code> call:</p>
<pre class="source-code">
gpu_commands-&gt;dispatch( x, y, render_scene-&gt;active_lights );</pre>
<p>In this pass, we simply compute the difference between the minimum and maximum value over the past four frames:</p>
<pre class="source-code">
vec4 last_visibility_values = texelFetch(
    global_textures_3d[ visibility_cache_texture_index ],
        tex_coord, 0 );
float max_v = max( max( max( last_visibility_values.x,
    last_visibility_values.y ), last_visibility_values.z ),
        last_visibility_values.w );
float min_v = min( min( min( last_visibility_values.x,
    last_visibility_values.y ), last_visibility_values.z ),
        last_visibility_values.w );
float delta = max_v - min_v;</pre>
<p>The historical values are set to <code>0</code> during the first frame. We store the delta in another 3D texture to be used in the next pass. The following figure shows the result of this pass:</p>
<div><div><img alt="Figure 13.2 – The visibility variation for the past four frames" height="589" src="img/B18395_13_02.jpg" width="1093"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – The visibility variation for the past four frames</p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor227"/>Computing visibility</h2>
<p>This pass is responsible for<a id="_idIndexMarker690"/> computing how many rays to shoot for each light <a id="_idIndexMarker691"/>depending on the variance across the past four frames.</p>
<p>This pass needs to read a lot of data from different textures. We are going to use <strong class="bold">local data storage</strong> (<strong class="bold">LDS</strong>) to <a id="_idIndexMarker692"/>cache the values across all threads within a shader invocation:</p>
<pre class="source-code">
local_image_data[ local_index.y ][ local_index.x ] =
    texelFetch( global_textures_3d[ variation_texture_index
        ], global_index, 0 ).r;</pre>
<p>As we explained in <a href="B18395_09.xhtml#_idTextAnchor143"><em class="italic">Chapter 9</em></a>, <em class="italic">Implementing Variable Rate Shading</em>, we need to be careful about synchronizing these writes by placing a <code>barrier()</code> call before accessing the data stored in <code>local_image_data</code>. Likewise, we need to populate values around the edges of the matrix. The code is the same as before and we won’t replicate it here.</p>
<p>Next, we are going to filter this data to make it more temporally stable. The first step is to compute the maximum value in a 5x5 region and store the result in another LDS matrix:</p>
<pre class="source-code">
local_max_image_data[ local_index.y ][ local_index.x ] =
    max_filter( local_index );</pre>
<p><code>max_filter</code> is implemented as follows:</p>
<pre class="source-code">
for ( int y = -2; y &lt;= 2; ++y  ) {
    for ( int x = -2; x &lt;= 2; ++x ) {
        ivec2 xy = index.xy + ivec2( x, y );
        float v = local_image_data[ xy.y ][ xy.x ];
        max_v = max( max_v, v );
    }
}</pre>
<p>After <a id="_idIndexMarker693"/>computing <a id="_idIndexMarker694"/>the <code>max</code> values, we pass them through a 13x13 tent filter:</p>
<pre class="source-code">
float spatial_filtered_value = 0.0;
for ( int y = -6; y &lt;= 6; ++y ) {
    for ( int x = -6; x &lt;= 6; ++x ) {
        ivec2 index = local_index.xy + ivec2( x, y );
        float v = local_max_image_data[ index.y ][ index.x
        ];
        float f = tent_kernel[ y + 6 ][ x + 6 ];
        spatial_filtered_value += v * f;
    }
}</pre>
<p>This is done to smooth out differences between <a id="_idIndexMarker695"/>adjacent fragments while still giving more weight to the fragment we are processing. We then combine this value with<a id="_idIndexMarker696"/> temporal data:</p>
<pre class="source-code">
vec4 last_variation_values = texelFetch(
    global_textures_3d[ variation_cache_texture_index ],
        global_index, 0 );
float filtered_value = 0.5 * ( spatial_filtered_value +
   0.25 * ( last_variation_values.x +
          last_variation_values.y +
          last_variation_values.z +
          last_variation_values.w ) );</pre>
<p>Before moving on, we update the variation cache for the next frame:</p>
<pre class="source-code">
last_variation_values.w = last_variation_values.z;
last_variation_values.z = last_variation_values.y;
last_variation_values.y = last_variation_values.x;
last_variation_values.x = texelFetch( global_textures_3d[
    variation_texture_index ], global_index, 0 ).r;</pre>
<p>We now leverage the data we just obtained to compute the visibility term. First, we need to determine the sample count. If the reprojection in the previous pass has failed, we simply use the maximum sample count:</p>
<pre class="source-code">
uint sample_count = MAX_SHADOW_VISIBILITY_SAMPLE_COUNT;
if ( motion_vectors_value.r != -1.0 ) {</pre>
<p>If the <a id="_idIndexMarker697"/>reprojection was successful, we get the sample count for the last frame and determine whether the sample count has been stable over the past four frames:</p>
<pre class="source-code">
    sample_count = sample_count_history.x;
    bool stable_sample_count = 
      ( sample_count_history.x == sample_count_history.y ) &amp;&amp;
      ( sample_count_history.x == sample_count_history.z ) &amp;&amp;
      ( sample_count_history.x == sample_count_history.w );</pre>
<p>We then <a id="_idIndexMarker698"/>combine this information with the filtered value we computed previously to determine the<a id="_idIndexMarker699"/> sample count for this frame:</p>
<pre class="source-code">
    float delta = 0.2;
    if ( filtered_value &gt; delta &amp;&amp; sample_count &lt;
        MAX_SHADOW_VISIBILITY_SAMPLE_COUNT ) {
            sample_count += 1;
    } else if ( stable_sample_count &amp;&amp;
          sample_count &gt;= 1 ) {
              sample_count -= 1;
      }<a id="_idTextAnchor228"/></pre>
<p>If the<a id="_idIndexMarker700"/> filtered value surpasses a given threshold, we are going to increase the sample count. This means we identified a high-variance value across the past four frames and we’d need more samples to converge to a better result.</p>
<p>If, on the other hand, the sample count has been stable across the past four frames, we decrease the sample count.</p>
<p>While this works well in practice, it could reach a sample count of <code>0</code> if the scene is stable – for instance, when the camera is not moving. This would lead to an unlit scene. For this reason, we force the sample count to <code>1</code> if the past four frames also had a sample count of <code>0</code>:</p>
<pre class="source-code">
    bvec4 hasSampleHistory = lessThan(
        sample_count_history, uvec4( 1 ) );
    bool zeroSampleHistory = all( hasSampleHistory );
    if ( sample_count == 0 &amp;&amp; zeroSampleHistory ) {
        sample_count = 1;
    }
}</pre>
<p>Here is an example of the sample count cache texture:</p>
<div><div><img alt="Figure 13.3 – The sample count cache texture" height="589" src="img/B18395_13_03.jpg" width="1093"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – The sample count cache texture</p>
<p>Notice that<a id="_idIndexMarker701"/> fragments that <em class="italic">see</em> the light <a id="_idIndexMarker702"/>tend to require more samples, as expected.</p>
<p>Now that we know how many samples we need, we can move on to computing the <code>visibility</code> value:</p>
<pre class="source-code">
float visibility = 0.0;
if ( sample_count &gt; 0 ) {
    // world position and normal are computed the same as
       before
    visibility = get_light_visibility(
       gl_GlobalInvocationID.z, sample_count,
       pixel_world_position, normal, frame_index );
}</pre>
<p><code>get_light_visibility</code> is the method that traces rays through the scene. It’s implemented as follows:</p>
<pre class="source-code">
const vec3 position_to_light = light.world_position –
    world_position;
const vec3 l = normalize( position_to_light );
const float NoL = clamp(dot(normal, l), 0.0, 1.0);
float d = sqrt( dot( position_to_light, position_to_light ) );</pre>
<p>We first compute a few parameters as we have done before for our lig<a id="_idTextAnchor229"/>hting implementation. In addition, we compute <code>d</code>, the distance between the world position of this fragment and the light we are processing.</p>
<p>Next, we <a id="_idIndexMarker703"/>trace rays through the scene only<a id="_idIndexMarker704"/> if this light is close enough and it’s not behind geometry at this fragment. This is achieved using the following code:</p>
<pre class="source-code">
float visiblity = 0.0;
float attenuation =
    attenuation_square_falloff(position_to_light,
        1.0f / light.radius);
const float scaled_distance = r / d;
if ( ( NoL &gt; 0.001f ) &amp;&amp; ( d &lt;= r ) &amp;&amp; ( attenuation &gt;
    0.001f ) ) {</pre>
<p>We then trace one ray per sample. To make sure the results converge over time, we compute the ray directi<a id="_idTextAnchor230"/>on by using a pre-computed Poisson disk:</p>
<pre class="source-code">
    for ( uint s = 0; s &lt; sample_count; ++s ) {
        vec2 poisson_sample = POISSON_SAMPLES[ s *
            FRAME_HISTORY_COUNT + frame_index ];
        vec3 random_dir = normalize( vec3( l.x +
            poisson_sample.x, l.y + poisson_sample.y, l.z )
            );
        vec3 random_x = x_axis * poisson_sample.x *
            (scaled_distance) * 0.01;
        vec3 random_y = y_axis * poisson_sample.y *
            (scaled_distance) * 0.01;
        vec3 random_dir = normalize(l + random_x +
            random_y);</pre>
<p>Now that we<a id="_idIndexMarker705"/> have computed our ray direction, we can start ray traversal:</p>
<pre class="source-code">
        rayQueryEXT rayQuery;
        rayQueryInitializeEXT(rayQuery, as,
           gl_RayFlagsOpaqueEXT |
           gl_RayFlagsTerminateOnFirstHitEXT,
           0xff, world_position, 0.001,
           random_dir, d);
        rayQueryProceedEXT( rayQuery );</pre>
<p>This code <a id="_idIndexMarker706"/>is very similar to the code we presented in th<a id="_idTextAnchor231"/>e first section, but in this case, we accumulate the <code>visibility</code> value for each direction the light is visible from:</p>
<pre class="source-code">
        if (rayQueryGetIntersectionTypeEXT(rayQuery, true)
            != gl_RayQueryCommittedIntersectionNoneEXT) {
                visibility +=
                    rayQueryGetIntersectionTEXT(rayQuery,
                        true) &lt; d ? 0.0f : 1.0f;
        }
        else {
            visiblity += 1.0f;
        }
    }
}</pre>
<p>Finally, we return the average of the computed <code>visibility</code> value:</p>
<pre class="source-code">
return visiblity / float( sample_count );</pre>
<p>Now that<a id="_idIndexMarker707"/> we have the <code>visibility</code> value <a id="_idIndexMarker708"/>for this frame, we need to update our visibility history cache. If the reprojection was successful, we simply add the new value:</p>
<pre class="source-code">
vec4 last_visibility_values = vec4(0);
if ( motion_vectors_value.r != -1.0 ) {
    last_visibility_values = texelFetch(
        global_textures_3d[ visibility_cache_texture_index
            ], global_index, 0 );
    last_visibility_values.w = last_visibility_values.z;
    last_visibility_values.z = last_visibility_values.y;
    last_visibility_values.y = last_visibility_values.x;</pre>
<p>If, on the other hand, the reprojection failed, we overwrite all <code>history</code> entries with the new <code>visibility</code> value:</p>
<pre class="source-code">
} else {
    last_visibility_values.w = visibility;
    last_visibility_values.z = visibility;
    last_visibility_values.y = visibility;
}
last_visibility_values.x = visibility;</pre>
<p>The last step is to also update the sample count cache:</p>
<pre class="source-code">
sample_count_history.w = sample_count_history.z;
sample_count_history.z = sample_count_history.y;
sample_count_history.y = sample_count_history.x;
sample_count_history.x = sample_count;</pre>
<p>Now that<a id="_idIndexMarker709"/> we have updated the visibility <a id="_idIndexMarker710"/>term for this frame and updated all the caches, we can move to the last pass and compute the filtered visibility that will be used in our lighting computation.</p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor232"/>Computing filtered visibility</h2>
<p>If we were<a id="_idIndexMarker711"/> to use the <code>visibility</code> value as <a id="_idIndexMarker712"/>computed in the previous section, the output would be very noisy. For each frame, we might have a different sample count and sample positions, especially if the camera or objects are moving.</p>
<p>For this reason, we need to <em class="italic">clean up</em> the result before we can use it. One common approach is to use a denoiser. A denoiser is usually implemented as a series of compute passes that, as the name implies, will reduce the noise as much as possible. Denoisers can take a significant amount of time, especially as the resolution increases.</p>
<p>In our case, we are going to use a simple temporal and spatial filter to reduce the amount of time this technique takes. As with the previous pass, we need to read data into LDS first. This is accomplished by following two lines:</p>
<pre class="source-code">
local_image_data[ local_index.y ][ local_index.x ] =
    visibility_temporal_filter( global_index );
local_normal_data[ local_index.y ][ local_index.x ] =
    get_normal( global_index );</pre>
<p><code>visibility_temporal_filter</code> is implemented as follows:</p>
<pre class="source-code">
vec4 last_visibility_values = texelFetch(
    global_textures_3d[ visibility_cache_texture_index ],
        ivec3( xy, index.z ), 0 );
float filtered_visibility = 0.25 * (
    last_visibility_values.x + last_visibility_values.y +
    last_visibility_values.z + last_visibility_values.w );</pre>
<p>We first<a id="_idIndexMarker713"/> read the historical<a id="_idIndexMarker714"/> visibility data at this fragment for the given light and simply compute the average. This is our temporal filter. Depending on your use case, you might use a different weighting function, giving more emphasis to more recent values.</p>
<p>For spatial filtering, we are going to use a Gaussian kernel. The original article uses variable-sized kernels according to visibility variance. In our implementation, we decided to use a fixed 5x5 Gaussian kernel, as it provides good enough results.</p>
<p>The loop to compute the filtered value is implemented as follows:</p>
<pre class="source-code">
vec3 p_normal = local_normal_data[ local_index.y ][
    local_index.x ];</pre>
<p>First, we store the normal in our fragment location. We then iterate over the kernel size to compute the final term:</p>
<pre class="source-code">
for ( int y = -2; y &lt;= 2; ++y ) {
    for ( int x = -2; x &lt;= 2; ++x ) {
        ivec2 index = local_index.xy + ivec2( x, y );
        vec3 q_normal = local_normal_data[ local_index.y +
            y ][ local_index.x + x ];
        if ( dot( p_normal, q_normal ) &lt;= 0.9 ) {
            continue;
        }</pre>
<p>As described<a id="_idIndexMarker715"/> in the article, if the <a id="_idIndexMarker716"/>normals of adjacent fragments diverge, we ignore this data point. This is done to prevent shadow leaking.</p>
<p>Finally, we combine the value that has already gone through the temporal filter with the kernel value:</p>
<pre class="source-code">
        float v = local_image_data[ index.y ][ index.x ];
        float k = gaussian_kernel[ y + 2 ][ x + 2 ];
        spatial_filtered_value += v * k;
    }
}</pre>
<p>The following figure illustrates the content of the filtered visibility texture:</p>
<div><div><img alt="Figure 13.4 – The filtered visibility texture" height="588" src="img/B18395_13_04.jpg" width="1093"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – The filtered visibility texture</p>
<p>This concludes<a id="_idIndexMarker717"/> the computation of the <code>visibility</code> value for each light. We can now use this information during <a id="_idIndexMarker718"/>our lighting pass, as described in the next section.</p>
<h2 id="_idParaDest-205"><a id="_idTextAnchor233"/>Using the filtered visibility</h2>
<p>Using our <code>visibility</code> <a id="_idIndexMarker719"/>term is really simple. In the <code>calculate_point_light_contribution</code> method, we simply have to read the visibility we have computed in the previous passes:</p>
<pre class="source-code">
float shadow = texelFetch( global_textures_3d[
    shadow_visibility_texture_index ], ivec3( screen_uv,
        shadow_light_index ), 0 ).r;
float attenuation =
    attenuation_square_falloff(position_to_light, 1.0f /
        light.radius) * shadow;
if ( attenuation &gt; 0.0001f &amp;&amp; NoL &gt; 0.0001f ) {
// same code as before</pre>
<p>It could be possible to combine traditional shadow maps with a ray tracing implementation similar to the one we described here. It<a id="_idTextAnchor234"/> all depends on the frame budget for the technique, the type of lights in the scene, and the desired quality.</p>
<p>In this section, we have presented a different implementation for ray-traced shadows. The first step is to compute and store the visibility variance across the past four frames. Next, we computed the sample count for each fragment and each light using a <code>max</code> filter followed by a tent filter.</p>
<p>We then used this sample<a id="_idIndexMarker720"/> count to trace rays into the scene to determine a raw <code>visibilit<a id="_idTextAnchor235"/>y</code> value. In the last pass, we passed this <code>visibility</code> value through a temporal and spatial filter to reduce noise. Finally, we used this filtered value in our lighting computation.</p>
<h1 id="_idParaDest-206"><a id="_idTextAnchor236"/>Summary</h1>
<p>In this chapter, we have presented two implementations for ray-traced shadows. In the first section, we provided a simple implementation similar to what you might find in an offline renderer. We simply shoot one ray per fragment to each light to determine whether it’s visible or not from that position.</p>
<p>While this works well for point lights, it would require many rays to support other light types and render soft shadows. For this reason, we also provided an alternative that makes use of spatial and temporal information to determine how many samples to use per light.</p>
<p>We start by computing the visibility variance of the past four frames. We then filter this value to determine how many rays to shoot for each fragment for each light. We use this count to traverse the scene and determine the <code>visibility</code> value for each fragment. Finally, we filter the visibility we obtained to reduce the noise. The filtered visibility is then used in the lighting computation to determine the final shadow term.</p>
<p>In the next chapter, we continue our ray tracing journey by implementing global illumination!</p>
<h1 id="_idParaDest-207"><a id="_idTextAnchor237"/>Further reading</h1>
<p>The technique we have implemented in this chapter is detailed in <a href="B18395_13.xhtml#_idTextAnchor213"><em class="italic">Chapter 13</em></a>, <em class="italic">Revisiting Shadows with Ray Tracing,</em> of the book <em class="italic">Ray Tracing Gems</em>. It is freely available here: <a href="http://www.realtimerendering.com/raytracinggems/rtg/index.xhtml">http://www.realtimerendering.com/raytracinggems/rtg/index.xhtml</a>.</p>
<p>We have only used a limited set of th<a id="_idTextAnchor238"/>e GLSL APIs available for ray tracing. We recommend reading the GLSL extension specification to see all the options available:</p>
<ul>
<li><a href="https://github.com/KhronosGroup/GLSL/blob/master/extensions/ext/GLSL_EXT_ray_tracing.txt">https://github.com/KhronosGroup/GLSL/blob/master/extensions/ext/GLSL_EXT_ray_tracing.txt</a></li>
<li><a href="https://github.com/KhronosGroup/GLSL/blob/master/extensions/ext/GLSL_EXT_ray_query.txt">https://github.com/KhronosGroup/GLSL/blob/master/extensions/ext/GLSL_EXT_ray_query.txt</a></li>
</ul>
<p>We used a few filters in this chapter. Signal processing is a vast and wonderful field that has more implica<a id="_idTextAnchor239"/>tions in graphics programming than people realize. To g<a id="_idTextAnchor240"/>et you started, we recommend this article by Bart Wronski: <a href="https://bartwronski.com/2021/02/15/bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset/">https://bartwronski.com/2021/02/15/bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset/</a>.</p>
</div>
</div></body></html>