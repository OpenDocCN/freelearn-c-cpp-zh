<html><head></head><body>
		<div><h1 id="_idParaDest-107"><em class="italics"><a id="_idTextAnchor114"/>Chapter 7</em></h1>
		</div>
		<div><h1 id="_idParaDest-108"><a id="_idTextAnchor115"/>Architecture</h1>
		</div>
		<div><h2 id="_idParaDest-109"><a id="_idTextAnchor116"/>Introduction</h2>
			<p>The term "software architect" has become sadly maligned of late, probably as a result of developers working with <strong class="bold">architecture astronauts</strong>—<a href="http://www.joelonsoftware.com/items/2005/10/21.html">http://www.joelonsoftware.com/items/2005/10/21.html</a> who communicate through PowerPoint-Driven Development. Simon Brown has written a book called <strong class="bold">Software Architecture for Developers</strong>—<a href="https://leanpub.com/software-architecture-for-developers">https://leanpub.com/software-architecture-for-developers</a>; check it out for a complete discussion of the responsibility of a software architect. The focus of this chapter is on the incremental differences between thinking about a problem as code and as architecture that supports the code. It's also about some of the things to think about as you'<a id="_idTextAnchor117"/>re designing your application, when to think about them, and how to communicate the results of such considerations to other people on the team.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor118"/>Non-Functional Requirements Are Essential</h2>
			<p>I'd almost go as far as to say that the <em class="italics">primary indicator of success</em> for an application architecture is whether it supports the non-functional requirements the customer has described. Anyone can, given enough patience and stubbornness, carry on gluing features together arbitrarily until all of the required functionality is present. However, making it do that coherently, in a way that combines desired attributes from the customer side (the NFRs) and the developer side (adaptability, readability, and the like) is where the art form of software architecture comes in.</p>
			<p>So, what are these non-functional requirements? It's common to say that these are the "-ility" statements made about the software. It takes a bit of squinting to accept that, but it's roughly true:</p>
			<ul>
				<li><strong class="bold">Performance</strong>: How is this an -ility? Is it speedability? Velocitility? Well, something like that anyway. It's important to understand what's meant by <em class="italics">performance</em>, as it has many different aspects. It could refer to the software's behavior with restricted resources, or large datasets. If we're talking "speed," that could be about the rate at which requests are processed, or the time to process any one request (measured in wall time or clock cycles, depending on which is more important). It could be an average, or under peak conditions. If it's an average, over what time is it measured? And is it the <em class="italics">mean</em> time or another average? Perhaps the <em class="italics">median</em>?<p><em class="italics">I worked on one project where the performance requirements were described thus: the time and memory required to complete certain operations should be within 105% of the previous versions of the software. That's easy to measure, and whether the software has succeeded is unambiguous.</em></p></li>
				<li><strong class="bold">Compatibility</strong>: What operating systems will the software have to run on? What versions? What other software components will it communicate with? Are there reasons to choose particular languages, environments, or third-party components?</li>
				<li><strong class="bold">Reliability</strong>: What happens when there's some problem? Is there a failure, a recovery, or some restricted mode of operation? How much downtime can be accepted, over what period? Or maybe there are limits on how many users may be affected simultaneously?</li>
				<li><strong class="bold">Legal or regulatory requirements</strong>: These can be requirements <em class="italics">not</em> to do things (such as don't give customer data to third parties) or mandates that the software <em class="italics">must</em> do something (such as keeping a record of any request from data).</li>
				<li><strong class="bold">Security</strong>: Such a wide topic that many books have been written, including <em class="italics">one of my own</em>. Now, I'm sure security experts will get annoyed that I've lumped security in with "other" NFRs, but that's what it is. For most software, security is not functionality that the customer wants but a property of how they want that functionality to be delivered. Notice that while security isn't directly related to other requirements, such as compliance, it can be a prerequisite to ensure that other requirements are still satisfied in the face of subversion.</li>
				<li><strong class="bold">Usability</strong>: This can cover a wide range of requirements: ease of use, obviously; but also what (human) languages should be supported, accessibility, design aesthetics, and so on. I mentioned usability, but usability by <em class="italics">whom</em>? The people who will be using it, of course; but is there anyone else who needs to be considered? Who will be deploying, installing, testing, configuring, and supporting the software? What usability requirements do those people have?</li>
				<li><strong class="bold">Adaptability</strong>: What are the most likely variations in the execution environment or the (human) system that the software's supporting? There's no need to support those things now, of course, but an architecture that makes it easier to make those changes (without causing unacceptable costs now, of course) could be beneficial.</li>
			</ul>
			<p>With a list like that, we can come up with a less hand-wavy definition of non-functional requirements: they're the <em class="italics">constraints</em> within which the product needs to provide its functionality – not the things it does, but the ways in which it must do them.</p>
			<p>That's why a successful architecture <em class="italics">must</em> support satisfaction of the non-functional requirements. If the software doesn't remain within the constraints of its operation, customers may not be able to use it at all; in which case, the software would be a failure. To support these requirements, the software architecture needs to provide a coherent, high-level structure into which developers can build the app's features. The architecture should make it clear how each feature is supposed to fit, and what limitations are imposed onto the implementation of each component. In other words, the architecture should guide developers such that the most obvious implementation of a feature is one that conforms to the NFRs. Ideally, whenever a developer has a question along the lines of "where would I add this?" or "how should I make this change?", the architect (or even the architecture) should already have an answer.</p>
			<h3 id="_idParaDest-111"><a id="_idTextAnchor119"/>When Should I Think About the NFRs?</h3>
			<p>The above discussion probably makes it seem that you need to get the architecture in place <em class="italics">before</em> any of the features are built, because the feature implementation must be constrained by the architecture. That's more or less true, though often you'll find that requirements for the app's functionality feed back into architecture decisions.</p>
			<p>I find this iteration is best handled by a series of successively high-fidelity prototypes. ("Fidelity" here refers to the technical accuracy and functional completeness of prototypes; these are for architectural evaluation, after all. I'm not talking about the prototypes' applicability to UI testing, which is <strong class="bold">a whole separate issue</strong>—<a href="http://dl.acm.org/citation.cfm?id=223514">http://dl.acm.org/citation.cfm?id=223514</a>.) The architecture is roughly defined and some of the features are roughly implemented; any identified problems are resolved and the design is refined slightly. This carries on until everything stabilizes, by which time the product is ready to ship.</p>
			<p>There's a discussion on project methodologies in <em class="italics">Chapter 13, Teamwork</em>. Those who have read that, or a similar discussion, will realize that this sounds somewhat similar to the <strong class="bold">spiral model of software development</strong>— <a href="http://dl.acm.org/citation.cfm?doid=12944.12948">http://dl.acm.org/citation.cfm?doid=12944.12948</a>, proposed by Boehm in 1986. The difference between that proposition and prototyping in stages as I practice it is the length of each iteration: days or weeks rather than the months to years Boehm was considering.</p>
			<p>People who believe in the "build one to throw away" line are at this point picking up their mortified jaws from the floor. The problem with that line is actually getting around to throwing away the one to throw away. You <em class="italics">intend</em> to throw the first one away, but somehow it manages to hang around and end up in production. You may as well accept from the beginning that this is going to happen and write a prototype that isn't ready <em class="italics">yet</em> but will be at <em class="italics">some time</em>, supported by documentation that helps the team understand the gap between the prototype and production-readiness.</p>
			<h3 id="_idParaDest-112"><a id="_idTextAnchor120"/>Performance in Low–Fidelity Prototypes</h3>
			<p>Tools for measuring the performance of an application are among some of the most capable developer tools available. Time profilers, memory managers, network packet inspectors, and others all help you to discover the performance characteristics of your application. But how do you do that when it isn't written yet?</p>
			<p>You write simulations that have the expected performance characteristics. If, for example, you estimated that an operation requested over the network would take about 0.1±0.01s to complete, using about 4 MB of heap memory, you could write a simulation that allocates about 4 MB then sleeps for the appropriate amount of time. How many of those requests can the app's architecture support at once? Is the latency to complete any one operation acceptable? Remember to consider both the <strong class="bold">normal and saturated cases</strong>—<a href="http://queue.acm.org/detail.cfm?id=2413037">http://queue.acm.org/detail.cfm?id=2413037</a> in testing.</p>
			<p>This form of simulation will not be new to many developers. Just as mock objects are simulations designed to test <em class="italics">functionality</em> when integrating two modules, these simulations are the performance equivalent.</p>
			<h3 id="_idParaDest-113"><a id="_idTextAnchor121"/>Security in Low-Fidelity Prototypes</h3>
			<p>Retrofitting a security model to an existing architecture can be intensely problematic. Finding all of the points where access control is needed (This is a key use case for aspect-oriented programming; access control can be inserted at the "join points" of the application's code), or where data should be inspected for different abuses is difficult when the data flow was designed without those aspects being considered. For critical security concerns, including access control and data protection, it's best to incorporate them in the design from the start.</p>
			<p>That doesn't necessarily mean completely polishing their implementation; it just means making sure that even the early prototypes are capable of (even prototypical) protection. As an example, on one project I was involved in, we knew that the application needed to encrypt documents that were written to disk. The early versions of the app used a <strong class="bold">Caesar cipher</strong>—<a href="http://en.wikipedia.org/wiki/Caesar_cipher">http://en.wikipedia.org/wiki/Caesar_cipher</a> to do this – far from cryptographically sound, but sufficient for showing which files were being protected and whether anything was being written through another route. You can imagine doing the same for authorization, by ensuring that even stub functionality cannot be used by unauthorized people.</p>
			<h3 id="_idParaDest-114"><a id="_idTextAnchor122"/>Reliability in Low-Fidelity Prototypes</h3>
			<p>You can easily explore how an architecture responds to failures by injecting those failures and observing what happens. In the <em class="italics">Performance</em> section in this chapter, I talked about having a stub network module that simulates the memory and time requirements of real network requests. Similarly, you could arrange for it to fail every so often and observe how the rest of the system copes with that failure. Some companies even inject random failures <strong class="bold">in production</strong>—<a href="https://github.com/Netflix/SimianArmy/wiki">https://github.com/Netflix/SimianArmy/wiki</a>) to ensure that their systems are capable of coping.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor123"/>Defer When Appropriate; Commit When Necessary</h2>
			<p>Working with successively refined prototypes means that the architecture becomes iteratively more complete; therefore, certain decisions become more "baked in" and difficult to change. Remember the notion presented earlier: that the architecture should always be ready to answer developer questions. This means that whatever the developers work on first are the things that should be solved first. But that's a tautological statement, because you can probably arrange the development work to track the architectural changes.</p>
			<p>The best things to start with are the riskiest things. They might be the exploratory aspects of the product that aren't similar to anything the team has worked on before, they could be the parts that interface with other software or other organizations, or they could be the aspects that will potentially have the highest cost. These are the aspects of the application that will most likely change, and where change will be expensive. Dealing with these issues first means a high rate of change, early on in the project before the schedule and costs have become too well-established, rather than at the end, when people have expectations about when everything will be ready. In addition, changes made before much code has been written mean less code to rework.</p>
			<p>There's an expectation management issue here. During the exploratory and experimental work, you have to be able to convince clients, managers, and anyone else who asks that the answer to "how long will it take?" is "I don't know; we're not sure what it is yet" and any progress that has been made so far is illusory. It might <em class="italics">look</em> like you've made a lot of progress, but most of it will be simulation code that doesn't really do what it looks like it does. On two separate projects I've led the development of, we've run into trouble where a stakeholder has based assumptions about the project's progress on seeing a prototype. It's not their fault; it's my responsibility to provide a realistic appraisal of the project's status on which they can base their judgements on how to proceed.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor124"/>Justify Your Decisions</h2>
			<p>So, you've chosen the technology that will be used in a particular aspect of your application. Was that because it will lead to satisfying the customer's requirements with the least effort, or because it's the new shiny thing you've wanted to use since you went to that conference?</p>
			<p>When someone asks why the team is using a particular language, framework, or pattern, a shrug of the shoulders accompanied by the phrase "right tool for the job" isn't going to be a very satisfactory answer. What is it that <em class="italics">makes</em> that tool right for the job? Does it satisfy some requirement, such as compatibility, that other alternatives don't? Is it cheaper than the alternatives? (Remember that cost is calculated holistically: a commercial tool can be cheaper than a free one if it significantly reduces effort and the likelihood of introducing bugs.)</p>
			<p>You need to convince other people that the solution you're choosing is appropriate for the task at hand. Before brushing up on your rhetoric skills (which are indeed useful – there's a section on negotiation in <em class="italics">Chapter 13, Teamwork</em>, and a whole chapter on critical thinking), the first thing to do is to make sure that it <em class="italics">is</em> an appropriate tool for the job. Think about the different considerations people will have:</p>
			<ul>
				<li>The customers: Will this technology let you build something that satisfies all of the requirements? Will you, or someone else, be able to adapt the solution as our needs change? Can we afford it? Is it compatible with our existing environment?</li>
				<li>The developers: Do I already know this, or will I have to learn it? Will it be interesting to learn? Is using this technology consistent with my career plans?</li>
				<li>Management: Is this cost-effective? Is it actually the best solution for this project, or is it just something you've always wanted to learn? What's the <strong class="bold">bus factor</strong>—<a href="http://en.wikipedia.org/wiki/Bus_factor">http://en.wikipedia.org/wiki/Bus_factor</a> going to be? Can we sell this to other customers? Can we buy support from the vendor? Does it fit well with the capabilities and goals of the company?</li>
			</ul>
			<p>If you can answer those questions honestly and your chosen technology still comes out looking like the best answer, well, I'm not going to say you won't need your skills of persuasion and negotiation – just that you'll make it easier to employ them.</p>
			<p>But remember that negotiation is one of those tangos that requires two people. In <strong class="bold">Metaphors we Live By</strong>—<a href="http://theliterarylink.com/metaphors.html">http://theliterarylink.com/metaphors.html</a>, Lakoff and Johnson propose that the way we think about argument is colored by our use of combat metaphors. Well, destroying your opponent with a deft collection of rhetorical thrusts is fine for the school debating society, but we all need to remember that we win at software by <em class="italics">building the best thing, </em>not by steamrollering dissenting arguments. It can be hard, especially under pressure, to put ego to one side and accept criticism as a way of collaborating on building better things. But it's important to do so: look back to the list of different concerns people have, think of any others I've forgotten to add, and realize that your opinion of what's best for the project only covers a part of the story.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor125"/>When to Fix and When to Replace</h2>
			<p>A particular decision you often have to justify as a software architect is the choice of whether to continue using some existing code, or whether to throw it away and replace it with something else. Well, you rarely have to justify the decision to keep what you already have; you often have to justify its replacement.</p>
			<p>This is as it should be. While it's satisfying – even calming – to think of leaving all that legacy cruft behind and starting on a greenfield implementation, there are good reasons to avoid doing so. The existing code may seem buggy and hard to understand, but your team has existing experience with it and probably knows where the problems and limitations are. The same cannot be said of the as-yet nonexistent replacement, which will probably bring its own difficulties and bugs.</p>
			<p>It's important to realize now that this argument is not the same as the sunk-cost fallacy. That would be to argue that you shouldn't throw away existing code because of the time and resources that have already been spent on it; I'm saying that you should consider carefully whether the cost of developing something new is really lower than the cost of carrying on with what you've got.</p>
			<p>It probably isn't in many cases. Here's a question: how many bugs will your replacement implementation have? What will those bugs be? How long will they take to fix? If you could predict that, you probably wouldn't leave those problems in, and you could also predict how long it'd take to fix the bugs in the existing implementation and compare the two. Experience has taught us, though, that predicting the quality of a piece of development work is really difficult. There is thus a probability that, while your new implementation will fix some bugs in the original (because you're conscious of those problems when you're developing it), it will introduce new problems, including regressions where the earlier version worked better than its replacement. You've got to factor that risk into your decision.</p>
			<p>A significant shift in the economics of this situation occurs when the replacement is not something, you're going to build in-house but is an open source or commercial module you can use. In those cases, the cost of acquiring the software will be well-known, and the fitness for purpose could probably be investigated by examining the bug database or asking the community or vendor. The cost of integration, and the extent to which you'll be responsible for fixing problems (and the costs you'll incur if you aren't) are the remaining costs to consider.</p>
			<p>Another thought on rewrites: while they're not clearly an advantage for the developers, they certainly aren't a benefit to customers. I've seen a number of applications where a new version is touted as being "a complete rewrite" and, as Danny Greg from GitHub said, this is not a good thing. If the new version of the software is a complete rewrite, then, to me as a customer, all it shares with the previous version is the name and the icon. </p>
			<p>There's a risk that things I relied on in the previous version won't work as well, or at all, in the rewritten version. This is an excellent opportunity for me to evaluate competing products.</p>
			<p>You're faced with a known, and well-understood code module, with some known problems. Using this is free, but you might have to spend some time fixing some of the problems to extend it to cope with your new project. The alternative is to spend a while building something that does the same work but has an <em class="italics">unknown</em> collection of problems. Your team doesn't have the same experience with it, though it might better conform to your team's idea of what well-designed code should look like... this month. Given the choice between those two things, and the principle that my code is a liability not an asset, I conclude that I'd rather choose the devil I know than the devil I don't.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor126"/>Know When to Nitpick, And When to Leave It</h2>
			<p>One of the attributes of a good developer is being able to pick apart the small details of a problem. Developers are good at that because computers demand it; computers are really bad at inference, so you have to predict every little case that could happen, no matter how rare, and tell the computer what to do with them. Unfortunately, this attribute, if carried too far, turns programmers into <strong class="bold">lousy conversationalists</strong>—<a href="http://tirania.org/blog/archive/2011/Feb-17.html">http://tirania.org/blog/archive/2011/Feb-17.html</a> in all other fields, including other areas of software creation.</p>
			<p>When you or someone else is designing the architecture for a software system, think of it as a low-fidelity proposal for the <em class="italics">shape</em> of the solution, not the actual solution. The answer to the question "how does this solve X?" is almost certainly "it doesn't – this is an early-stage prototype," so there's not even any point asking the question. You could demonstrate the answer by building the solution into the proposed architecture: if it works, you've built a feature; if it doesn't work, you've found something important. But often, you'll start by thinking that something isn't going to work and find out that it actually does.</p>
			<p>Similarly, "why did you do it like <em class="italics">that</em>?" is not a useful question. If the person who did it like that didn't think that doing it like that was a good idea, they wouldn't have done it like that. Many developers don't like reading other programmers' code, and I think it's because <em class="italics">developers aren't taught how to critically analyze code well</em>—<a href="http://blog.securemacprogramming.com/2012/12/can-code-be-readable/">http://blog.securemacprogramming.com/2012/12/can-code-be-readable/</a>. If you can't turn "what is <em class="italics">that</em>?" into a specific question about the proposed solution, don't ask.</p>
			<p>This is not to say that criticism is bad or unwanted. Of course it's wanted – the architecture will benefit from the input of multiple people. But the feedback has to be at the <em class="italics">same level of abstraction</em> as the architecture itself. </p>
			<p>In other words, the feedback must be in terms of the constraints placed on the solution and whether they can be met while providing the required features. Problems like "I can't see how errors from the <code>frobulator</code> interface would get into the audit component" are fine. Questions like "how does this degrade under ongoing saturation?" are fine. Suggestions like "if we use this pattern, then the database plugin can be interchangeable without much additional effort" are welcome. Comments along the lines of "this is useless – it doesn't handle the filesystem reporting a recursive link problem when you open a named pipe" can be deferred.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor127"/>Support, Don't Control</h2>
			<p>Given the definition that architecture serves to support the application's features within the constraints of its non-functional requirements, we can describe the role of architect in similar terms.</p>
			<h3 id="_idParaDest-120"><a id="_idTextAnchor128"/>What Does A Software Architect Do?</h3>
			<p>A software architect is there to identify risks that affect the technical implementation of the software product and address those risks. Preferably, before they stop or impede the development of the product.</p>
			<p>That could mean doing tests to investigate the feasibility or attributes of a proposed solution. It could mean evangelizing the developers to the clients or managers to avoid those people interrupting the development work. It could mean giving a junior developer a tutorial on a certain technology – or getting that developer to tutor the rest of the team on the thing that person is an expert on.</p>
			<h3 id="_idParaDest-121"><a id="_idTextAnchor129"/>What A Software Architect Doesn't Do</h3>
			<p>A software architect doesn't micromanage the developers who work with them. An architect doesn't rule by memos and UML diagrams. The architect doesn't prognosticate on things they have no experience of. Perhaps confusingly, the role of software architect bears very little resemblance to the profession after which it's named. If you want analogies with civil engineering, all developers are like architects. If you want to see the software analog to the builder, that's work done by the compiler and IDE.</p>
			<p>Architects don't make decisions where none is necessary. They don't ignore or belittle suggestions that come from people who aren't architects either.</p>
			<h3 id="_idParaDest-122"><a id="_idTextAnchor130"/>In one sentence</h3>
			<p>A software architect is there to make it easier for developers to develop.</p>
		</div>
	</body></html>