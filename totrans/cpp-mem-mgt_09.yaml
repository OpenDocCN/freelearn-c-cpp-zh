- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Atypical Allocation Mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are progressing in our exploration of memory management with C++. In [*Chapter
    7*](B21071_07.xhtml#_idTextAnchor116), we explored the various syntactic ways
    in which one can overload `operator new()` and `operator delete()` (as well as
    their array counterparts), and in [*Chapter 8*](B21071_08.xhtml#_idTextAnchor128),
    we wrote an actual, real-life example (a memory leak detector) relying on the
    capacity to write such overloads. It’s a nice start, showing concretely that this
    knowledge has practical uses, but you might (rightfully) wonder what else we can
    do when controlling memory management facilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will be slightly different from the others. What we will do here
    is present a non-exhaustive set of ways in which one can benefit from taking control
    of the memory allocation functions of C++. More precisely, we will show the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How placement `new` can let us drive memory-mapped hardware efficiently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How one can simplify usage of error management with the `nothrow` version of
    `operator new()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How one can install and use `std::new_handler` to make it easier to react to
    out-of-memory situations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How one can handle “exotic” memories such as shared memory or persistent memory
    through the mediation of standard C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of this chapter, we will have a broader view of what opportunities
    the basic memory allocation facilities of C++ provide us with. Later chapters
    will return to more focused topics such as arena-based allocation ([*Chapter 10*](B21071_10.xhtml#_idTextAnchor153)),
    deferred reclamation ([*Chapter 11*](B21071_11.xhtml#_idTextAnchor163)), and,
    in later chapters, how to control memory allocation with containers and allocators.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the code files for this chapter in the book’s GitHub repository
    here: [https://github.com/PacktPublishing/C-Plus-Plus-Memory-Management/tree/main/chapter9](https://github.com/PacktPublishing/C-Plus-Plus-Memory-Management/tree/main/chapter9).'
  prefs: []
  type: TYPE_NORMAL
- en: Placement new and memory-mapped hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many uses for placement `new` (an important feature discussed in [*Chapter
    7*](B21071_07.xhtml#_idTextAnchor116), as you might remember) but one use that
    is particularly interesting is that it allows us to map software objects to memory-mapped
    hardware, effectively allowing us to drive hardware as if it was software.
  prefs: []
  type: TYPE_NORMAL
- en: A working example of this feature would be tricky to write as we would find
    ourselves in “non-portable code land,” using operating-system-specific features
    to get the address of a particular device and discussing ways to get read and
    write privileges to memory locations normally accessed by software drivers. For
    that reason, we will craft an artificial yet illustrative example and ask you,
    esteemed reader, to imagine that the missing parts of this example exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, suppose that we are developing a driver for a new video card, one that
    is so wonderful that its codename is `super_video_card`. For the sake of this
    illustration, we will model this through the following class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The important aspects of this class for our purpose are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It is an uncopiable type, as it is meant to map to a specific zone of memory.
    Copying an object of this type would be counterproductive, to say the least.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has been designed in such a way that its state can conceptually be superimposed
    on its hardware equivalent. For example, given the preceding class declaration,
    starting at the beginning of the hardware’s memory layout, we expect four 32-bit
    integral registers followed by four 32-bit floating point registers. We used `<cstdint>`
    to get the aliases for fixed-width integral types on our compiler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As should be the case under such circumstances, we express our expectations
    through `static_assert` whenever possible. Also, since the state of the hardware
    registers can change through other actions than that of our program, we qualified
    the register-equivalents as `volatile` such that accesses to these member variables
    will be equivalent to I/O operations for the purpose of C++’s abstract machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why do we use volatile variables in this example?
  prefs: []
  type: TYPE_NORMAL
- en: If you are not used to `volatile` variables, you might be wondering why we used
    this qualification on the data members of our memory-mapped hardware-representing
    class. The reason why this is important is that we want to avoid our compiler
    optimizing code based on the (wrong, in this case) assumption that if our code
    does not touch these variables, then they do not change state or that if our writes
    to these variables are not followed by reads in our code, then that can be assumed
    to have no effect. Through `volatile`-qualified variables, we are effectively
    telling the compiler “*There are things you do not know happening on these objects,
    so please do not assume* *too much*.”
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, we used a constructor that zeros out the data members and a
    trivial destructor, but in practice, we could have used constructors (default
    or otherwise) to initialize the state of the memory-mapped device to match our
    needs and the destructor to reset the state of that device to some acceptable
    state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Normally, for a program to access the memory-mapped hardware, we would probably
    communicate with the operating system with services that accept as argument the
    required information to identify the device whose address we seek. In our case,
    we will simply make it look like we can access a zone of memory of the right size
    and alignment to which we can read and write. The memory address is exposed as
    raw memory (of type `void*`), which is what we can realistically expect from an
    operating system function under similar circumstances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We then arrive at how one can use placement `new` to map an object to some
    memory-mapped hardware location. Note that we need to include the `<new>` header
    as this is where placement `new` is defined. The steps to meet our objective are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, obtain the address where we want to map our carefully crafted `super_video_card`
    object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, through placement `new` at that address, construct a `super_video_card`
    object such that the data members of that object correspond to the address of
    the registers they represent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the duration of that object’s lifetime, use the object through the corresponding
    pointer (the `the_card` variable in the following code excerpt).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we are done, the one thing we do not want to do is apply `operator delete()`
    on `the_card` as we never allocated the associated memory in the first place.
    We do want to finalize the object through `~super_video_card()`, however, to make
    sure the cleanup or reset code (if any) for that object is run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We thus end up with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If the explicit destructor call is a problem, such as in code where exceptions
    could be thrown along the way, we can use a `std::unique_ptr` object with a custom
    deleter (see [*Chapter 5*](B21071_05.xhtml#_idTextAnchor079)) to finalize the
    `super_video_card` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the `std::unique_ptr` object finalizes the pointee (the `super_video_card`
    object) but does not free its memory storage, leading to more robust code in the
    presence of exceptions during the lifetime of the `the_card` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying nothrow new usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in [*Chapter 7*](B21071_07.xhtml#_idTextAnchor116), the default
    behavior of `operator new()` when unable to perform an allocation request is to
    throw an exception. This can result from such situations as running out of memory
    or otherwise being unable to service the allocation request, in which case, one
    usually throws `std::bad_alloc`; from an incorrect array length (for example,
    a negative length of one exceeding implementation-defined limits), usually leading
    to `std::bad_array_new_length` being thrown; or from failure to complete the subsequent
    construction of the object following the completion of `operator new()`, in which
    case, the exception that will be thrown will be whatever was thrown from the failing
    constructor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exceptions are the “normal” way for a C++ function to signal failure to meet
    the function’s postconditions. In some cases, such as a constructor or an overloaded
    operator, it’s the only real, workable way to do so: a constructor has no return
    value, and the signature of functions that overload operators generally does not
    leave room for additional arguments or error-reporting return values, although
    one could make a case for some types such as `std::optional` or `std::expected`
    as allowing an alternative for some overloaded operator use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, some domains typically do not use exceptions: a significant number
    of video games are compiled without exception support, for example, and the same
    goes for a lot of programs written for embedded systems. Reasons invoked go from
    the technical (fear of overhead considered undesirable in terms of memory space
    consumption, execution speed, or both) to the more philosophical (dislike for
    what is seen as hidden control paths), but no matter what the reasons are, the
    fact is that C++ code compiled without exception support exists and the `nothrow`
    version of `operator new()` is a reality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This does mean, of course, that even seemingly simple code such as the following
    can lead to **undefined** **behavior** (**UB**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The reason for this potential UB is that if the `nothrow` version of `operator
    new()` fails (unlikely but not impossible, especially in memory-constrained situations),
    then `p` will be null, and accessing the `n` data member through `p` will be…
    a very bad idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the solution is simple, and being the astute reader that you are,
    you have probably noticed it already: just test the pointer before using it! This
    works, of course, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The problem with this approach is that code quickly becomes littered with tests,
    as there is rarely only one pointer in a program, reminding us that the beauty
    of code using exceptions is that one does not need to worry about those tests.
    With exceptions, either `operator new()` and the subsequent construction both
    succeeded and one can use the resulting pointer confidently, or one of these steps
    failed and code execution did not reach the point where one could get into trouble:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Of course, one can get in trouble even with exceptions, for example, if there
    is an execution path that lets `p` remain null or uninitialized and others where
    that cannot happen (you can usually avoid this by initializing your objects on
    declaration, but that is not always possible); let us leave these code hygiene
    considerations aside for now as they would deviate from our topic of interest.
  prefs: []
  type: TYPE_NORMAL
- en: An important consideration when facing a failure-to-allocate situation is what
    to do when it happens. Whether our code base uses exceptions or not, we most probably
    do not want to let the execution of our program continue and therefore incur UB
    through such things as the improper use of a null pointer.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common way to stop execution at the point of failure-to-allocate is to wrap
    the tentative allocation and construction operation, the subsequent test on the
    resulting pointer, and the action to take if the pointer is null in some code
    construct. The code we want to wrap will be something like the following, supposing
    we want to allocate-then-construct an `int` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This code used `std::abort()` as a mechanism to end program execution; exceptions
    would provide us with potentially recoverable errors, but without exceptions,
    most standard mechanisms at our disposal will lead to program termination, and
    `std::abort()` is a reasonable choice in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Ways to conclude program execution
  prefs: []
  type: TYPE_NORMAL
- en: 'A C++ program can conclude in many different ways: reaching the end of the
    `main()` function is the most obvious one, but other examples exist. For example,
    `std::exit()` is used for normal program termination accompanied by cleanup steps;
    `std::quick_exit()` is used for program termination without cleanup steps. One
    can use `std::atexit()` and `std::at_quick_exit()` to register some functions
    to be called before exiting, and `std::abort()` is used to signal abnormal program
    termination without cleanup steps. The `std::terminate()` function is used when
    some unpleasantness in a documented list of situations occurs (this list includes
    such things as an exception being thrown from the constructor of a `static` variable
    or from the body of a `noexcept` function). In our case, the only mechanism that
    really fit was `std::abort()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One possible approach to solve this problem is to use a macro and an **immediately-invoked
    function expression** (**IIFE**), which is the name given to an expression made
    from an anonymous lambda that is at once created, executed, and discarded. To
    make our solution general, we need to be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify the type of object to create
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make the macro variadic, as we need to be able to pass any number of arguments
    of any type to the object’s constructor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A possible implementation of such a macro would be `TRY_NEW` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Not everyone is familiar with variadic macros, so let’s take it step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: The “signature” of our macro is `TRY_NEW(T,...)`, meaning `T` is mandatory and
    `...` could be any number of tokens (including none at all) separated by commas.
    Unsurprisingly, we will use `T` for the type to construct and `...` for the arguments
    to pass to the constructor that will be invoked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we wrote the macro on more than one line (for readability), each line
    but the last terminates with a space followed by a backslash to inform the preprocessor
    that it should continue parsing on the next line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The symbols on `...` are relayed through the special macro named `__VA_ARGS__`,
    which expands to what `...` contained and can be empty if `...` itself is empty.
    This works in both C and C++. Note that we use parentheses, not braces, in the
    constructor call as we want to avoid unwittingly building an initializer list
    if all elements of `__VA_ARGS__` are of the same type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We test the `p` pointer resulting from the call to a `std::nothrow` version
    of `operator new()` and call `std::abort()` if `p` is null.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This entire sequence of operations is, as announced, wrapped in an IIFE and
    the newly allocated pointer is returned. Note that we could also have returned
    a `std::unique_ptr<T>` object from that lambda if we had wanted to do so. Also,
    note that this lambda expression uses a `[&]` capture block to ensure the availability
    of tokens in `__VA_ARGS__` within the scope of the lambda.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A small but interesting side effect
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that since we used parentheses (the same would hold for braces), an empty
    `__VAR_ARGS__` will lead this macro to zero-initialize fundamental types such
    as `int` instead of leaving them uninitialized. You can compare: as of C++23,
    `new int;` yields a pointer to an uninitialized `int` object, but `new int();`
    and `new int{};` both initialize the allocated block with a value of zero. There
    is an upside to this, as with this macro, we will not end up with a pointer to
    an uninitialized object, even for trivial types. However, there is also a downside
    as we will be paying for an initialization even in cases where it might not have
    been necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach would be to use a variadic function template, which might
    lead to a better debugging experience in practice. It has slightly different-looking
    client code but is otherwise similar in usage and effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The call syntax for the variadic function version looks like a cast, and arguments
    passed to `try_new()` are perfectly forwarded to the constructor of `T` to ensure
    that the expected constructor is called in the end. As was the case with the macro,
    we could have chosen to return a `std::unique_ptr<T>` object instead of a `T*`
    object with this function.
  prefs: []
  type: TYPE_NORMAL
- en: Out-of-memory situations and new_handler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, including this chapter, we have stated that `operator new()`
    and `operator new[]()` typically throw `std::bad_alloc` when failing to allocate
    memory. It’s true to a wide extent, but there is a subtlety we have avoided so
    far and to which we will now give some time and attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a situation where user code has specialized the memory allocation functions
    to fetch memory blocks from a pre-allocated data structure with interesting performance
    characteristics. Suppose that this data structure initially allocates space for
    a small number of blocks and then goes on to allocate more space once the user
    code exhausts the blocks from the initial allocation. Expressed otherwise: in
    this situation, we have an initial, fast setting (let’s call that the “optimistic”
    state) and a secondary setting (let’s call that the “second chance” state) that
    lets user code continue allocating once the “optimistic” state’s resources have
    been consumed.'
  prefs: []
  type: TYPE_NORMAL
- en: For a scenario such as this to be seamless, with a transparent change of allocation
    strategy achievable without the explicit intervention of user code, explicitly
    throwing `std::bad_alloc` would be insufficient. Throwing would complete the execution
    of `operator new()` and client code could catch the exception and take action,
    of course, but in this (reasonable) scenario, we would like failure to allocate
    to lead to some action being taken and `operator new()` to try again with the
    updated state of things, if any.
  prefs: []
  type: TYPE_NORMAL
- en: 'In C++, scenarios such as this are handled through a `std::new_handler`, which
    is an alias for a function pointer of type `void(*)()`. What one needs to know
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a global `std::new_handler` in a program, and by default, its value
    is `nullptr`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One can set the active `std::new_handler` through the `std::set_new_handler()`
    function, and one can get the active `std::new_handler` through the `std::get_new_handler()`
    function. Note that as a convenience, `std::set_new_handler()` returns the `std::new_handler`
    that is being replaced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an allocation function such as `operator new()` fails, what it should do
    is first get the active `std::new_handler`. If that pointer is null, then the
    allocation function should throw `std::bad_alloc` as we have done so far; otherwise,
    it should call that `std::new_handler` and try again under the new conditions
    that this call installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As could be expected, your standard library should already implement this algorithm,
    but our own overloads of `operator new()` and `operator new[]()` have not done
    so, at least so far. To show how to benefit from a `std::new_handler`, we will
    now implement an artificial version of the aforementioned two-step scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'This toy implementation will use the member version of the allocation operators
    for some `X` type and behave as if we initially had enough memory for `limit`
    objects of that type (normally, we would actually manage that memory, and you
    can see an example of such management in [*Chapter 10*](B21071_10.xhtml#_idTextAnchor153)
    where we will provide a more realistic example). We will install a `std::new_handler`
    that, when called, changes `limit` to a higher number, and then resets the active
    handler to `nullptr` such that subsequent failures to allocate `X` objects will
    lead to throwing `std::bad_alloc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the way that `X::operator new()` handles failure: if it notices that it
    will not be able to meet its postconditions, it gets the active `std::new_handler`,
    and if it’s non-null, calls it before trying again. This means that the `std::new_handler`,
    when called, has to either change the situation in such a way that a subsequent
    tentative allocation could succeed or change the `std::new_handler` to `nullptr`
    such that failure will lead to an exception being thrown. Failure to respect these
    rules could lead to an infinite loop and much sadness would ensue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The handler installed in `main()` for this toy example does this: when called,
    it changes the conditions under which the allocations will be performed (it raises
    the value of `X::limit`). It then calls `std::set_new_handler()` with `nullptr`
    as we have not planned for another approach after the “optimistic” and “second
    chance” situations, so if we exhaust the second chance resources, we (as they
    say) are toast.'
  prefs: []
  type: TYPE_NORMAL
- en: A lambda as new_handler?
  prefs: []
  type: TYPE_NORMAL
- en: You might have noticed that we described the `std::new_handler` type as being
    an alias for a function pointer of the `void(*)()` type, yet in our toy example,
    we installed a lambda. Why does that work? Well, it happens that a stateless lambda—a
    lambda expression with an empty capture block—is implicitly convertible to a function
    pointer with the same calling signature. It’s a useful thing to know under many
    circumstances, such as when writing C++ code that interfaces with C code or operating
    system APIs.
  prefs: []
  type: TYPE_NORMAL
- en: We are now about to enter a strange and quite technical part of this chapter,
    where we will see how to leverage C++ to handle atypical memory.
  prefs: []
  type: TYPE_NORMAL
- en: Standard C++ and exotic memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our last example in this slightly strange chapter with examples of unusual memory
    management usage is concerned with the ways in which we can write standard C++
    programs that deal with “exotic” memory. By “exotic,” we mean memory that requires
    explicit actions to “touch” (allocate, read from, write to, deallocate, and so
    on) and that differs from a “normal” memory block under the control of our program,
    such as the one used in the illustrative example of memory-mapped usage with placement
    `new` earlier in this chapter. Examples of such memory include persistent (non-volatile)
    memory or shared memory, but anything *out of the ordinary* will do, really.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have to pick an example, we will write an example using a (fictional)
    shared memory block.
  prefs: []
  type: TYPE_NORMAL
- en: A little white lie…
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to understand that we are describing a mechanism for memory that
    would normally be shared between *processes*, but inter-process communication
    is the domain of the operating system. Standard C++ only describes the rules for
    sharing data between *threads* in a process; for that reason, we will tell a little
    white lie and write a multithreaded system, not a multiple-process one, using
    that memory to share data. Our focus is on memory management facilities, not inter-process
    communication, so that should not pose a problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the same approach as we did in previous sections of this chapter,
    we will craft a portable illustration of how to proceed in code that seeks to
    manage atypical memory, and let you map the details to the services of your chosen
    platform. Our example code will take the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: A shared memory block will be allocated. We will make it look like this memory
    is special in the sense that one needs special operating system functions to create
    it, allocate it, or deallocate it, but we will deliberately avoid using actual
    operating system functions. This means that if you want to use the code in this
    section for a real application, you will need to adapt it to your chosen platform’s
    API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will craft a “handmade” version of a toy program that uses this fictional
    API for shared memory in order to illustrate what user code would look like under
    these circumstances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we will show how understanding the memory management facilities of C++
    can help us write more pleasant and “normal looking” user code that does the same
    thing as the “handmade” one… or even better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fictional realism?
  prefs: []
  type: TYPE_NORMAL
- en: This entire section on C++ and exotic memory, which we cover next, will hopefully
    be interesting, and the code we will write will strive to be realistic with respect
    to memory management. As mentioned previously, since the C++ standard is mostly
    silent on the idea of multi-process systems, we will try to make multithreaded
    code look kind of like multi-process code. I hope you, astute reader, will accept
    this proposition.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that there will be a small amount of low-level synchronization in
    user code for this section, including some through atomic variables. I tried to
    keep it minimal yet reasonably realistic and hope you will be able to accept it
    even though I will not explain it all in detail, with this book’s focus being
    on memory management rather than on concurrent computing (another fine topic,
    of course). Feel free to use your favorite concurrent programming resource if
    you want to know more about such things as waiting on atomics or using thread
    fences.
  prefs: []
  type: TYPE_NORMAL
- en: Ready? Let’s do this!
  prefs: []
  type: TYPE_NORMAL
- en: A fictional shared memory API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will write an API that is fictional but inspired by what one finds in most
    operating systems, except that we will report errors through exceptions to simplify
    user code. Operating systems mostly report errors through error codes expressed
    from return values, but this leads to user code that is more involved. I hope
    this seems like an acceptable compromise to you, dear reader.
  prefs: []
  type: TYPE_NORMAL
- en: As most operating systems do, we will abstract the actual resource through a
    form of handle, or key; creating a “shared memory” segment of some size will yield
    a key (an integral identifier), after which, accessing that memory will require
    that key, and so will destroying that memory. As can be expected with a facility
    meant to be used to share data between processes, destroying the memory will not
    finalize the objects therein, so user code will need to ensure that objects in
    the shared memory are destroyed before releasing the shared memory segment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The signatures and types for our API will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You might notice that we are using an `enum` type for `shared_mem_id`. The
    reason for this is that `enum` types are distinct types in C++, not just aliases
    as one would get from `using` or `typedef`. Having distinct types can be useful
    when overloading functions based on the types of their arguments. It’s a useful
    trick to know: if we write two functions with the same name (one that takes an
    argument of the `shared_mem_id` type and another that takes an argument of the
    `std::size_t` type), these will be distinct functions, even though the underlying
    type of `shared_mem_id` is `std::size_t`.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we are building an artificial implementation of “shared memory” to show
    how memory allocation functions can simplify user code, the implementation for
    the functions of our API will be written to be simple, but let us write client
    code that behaves as if it were using shared memory. We will define a shared memory
    segment as a `shared_mem_block` modeled by a pair made from an array of bytes
    and a size in bytes. We will keep a `std::vector` object of that type, using the
    indices in that array as `shared_mem_id`. This means that when a `shared_mem_block`
    object is destroyed, we will not reuse its index in the `std::vector` (the container
    will eventually have “holes,” so to speak).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our implementation is as follows. Note that it is not thread-safe, but that
    does not impact our memory management-related discourse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If you want to experiment, you can replace the implementation of these functions
    with equivalent implementations that call the functions of your chosen operating
    system, adjusting the API if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Equipped with this implementation, we can now compare a “handmade” example of
    shared memory-using code with one that benefits from the facilities of C++. We
    will do this comparison with code where one allocates some chunk of data from
    a shared memory segment and then launches two threads (a writer and a reader).
    The writer will write to that shared data, and then (with minimal synchronization)
    the reader will read from it. As mentioned previously, our code will use *intra*-process
    synchronization (C++ atomic variables), but in real code, you should use *inter*-process
    synchronization mechanisms provided by the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: A note on lifetime
  prefs: []
  type: TYPE_NORMAL
- en: You might remember from [*Chapter 1*](B21071_01.xhtml#_idTextAnchor016) that
    each object has an associated lifetime, and that the compiler keeps track of this
    fact in your programs. Our fictional multiple-process example is really a single-process,
    multithreaded example, so the usual C++ lifetime rules apply.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to take the code in this section and write a real multi-process
    system to run some tests, you might want to consider using `std::start_lifetime_as()`
    from C++23 in those processes that did not explicitly create the `data` object,
    and avoid detrimental optimizations from happening based on the compiler’s reasoning
    that, in these processes, the objects have never been constructed. In earlier
    compilers, one trick that generally works is calling `std::memcpy()` of the not-officially-constructed
    object onto itself, effectively starting its lifetime.
  prefs: []
  type: TYPE_NORMAL
- en: 'In both our “handmade” and our standard-looking implementations, we will be
    using a `data` object made of an `int` value and a Boolean `ready` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In a single-process implementation, a better choice for the completion flag
    would be an `atomic<bool>` object as we want to make sure the write to the `ready`
    flag happens before the write to the value, but since we want this example to
    look like we are using inter-process shared memory, we will limit ourselves to
    a simple `bool` and ensure this synchronization through other means.
  prefs: []
  type: TYPE_NORMAL
- en: A word on synchronization
  prefs: []
  type: TYPE_NORMAL
- en: In a contemporary program, optimizing compilers will often reorder operations
    that seem independent to generate better code, and processors will do the same
    once the code has been generated in order to maximize usage of the processor’s
    internal pipeline. Concurrent code sometimes contains dependencies that are neither
    visible to the compiler nor to the processor. In our examples, we will want the
    `ready` completion flag to become `true` only after the write to `value` has been
    performed; this order is only important because the writes are performed in one
    thread but *another* thread will look at `ready` to know whether `value` can be
    read.
  prefs: []
  type: TYPE_NORMAL
- en: Not enforcing the `value`-then-`ready` sequence of writes through some form
    of synchronization would let either the compiler or the processor reorder these
    (seemingly independent) writes and break our assumptions on the meaning of `ready`.
  prefs: []
  type: TYPE_NORMAL
- en: A handmade user code example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can, of course, write user code that uses our fictional API without resorting
    to specialized memory management facilities of C++, simply relying on placement
    `new` usage as seen in [*Chapter 7*](B21071_07.xhtml#_idTextAnchor116). It might
    be tempting to think of placement `new` as a specialized facility since you might
    have learned of it from this book, but if that is your perspective, you are invited
    to reconsider: the placement `new` mechanism is a fundamental memory management
    tool used in almost every program, whether user code is aware of it or not.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, our example program will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a shared memory segment of some size (we will allocate much more than
    we need in this case).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a `data` object at the beginning of that segment, obviously through
    placement `new`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start a thread that will wait for a signal on the `go` variable of type `atomic<bool>`,
    then obtain access to the shared memory segment, write to the `value` data member
    and then only signal that the write has occurred through the `ready` data member.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start another thread that will obtain access to the shared memory segment, get
    a pointer to the shared `data` object therein, and then do some (very inefficient)
    busy waiting on the `ready` flag to change state, after which `value` will be
    read and used. Once this has been done, completion will be signaled through the
    `done` flag of type `atomic<bool>`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our program will then read a key from the keyboard, signal the threads (the
    writer thread, really) that it’s time to start working, and wait until they are
    done before freeing the shared memory segment and concluding its work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We thus end up with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We made this work: we have an infrastructure of sorts to manage shared memory
    segments, we can use these memory blocks to share data, and we can write code
    that reads from that shared data as well as writes to it. Note that we captured
    the key in each thread in a `key` variable and then obtained the memory block
    within each lambda through that key, but it would also be reasonable to simply
    capture the `p_data` pointer and use it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice, however, that we did not really manage that block: we created it and
    used a small chunk of size `sizeof(data)` at the beginning. Now, what if we had
    wanted to create multiple objects in that zone? And what if we had wanted to write
    code that both creates and destroys objects, introducing the need to manage what
    parts of that block are in use at a given time? With what we just wrote, that
    would mean doing it all in user code, a somewhat burdensome endeavor.'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping that in mind, we will now solve the same problem but with a different
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: A standard-looking user code equivalent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So, what mechanism does C++ offer us if we want to use “exotic” memory in a
    more idiomatic manner? Well, one way to do so is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To write a manager class for the “exotic” memory, encapsulating the non-portable
    interface to the operating system and exposing services that are closer to what
    C++ user code would expect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To write overloads of the memory allocation operators (`operator new()`, `operator
    delete()`, and so on) that take a reference to such a manager object as an additional
    argument
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make these overloaded memory allocation operators bridge the gap between
    portable and non-portable code through delegation on the memory manager object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This way, user code can be written essentially as “normal looking” code that
    calls `new` and `delete` operators, except that these calls will use the same
    kind of extended notation seen in [*Chapter 7*](B21071_07.xhtml#_idTextAnchor116)
    for such things as the `nothrow` or placement versions of `operator new()`.
  prefs: []
  type: TYPE_NORMAL
- en: Our `shared_mem_mgr` class will use the fictional operating system API described
    earlier in this section but, normally, one would write a class that encapsulates
    whatever operating system services are required to access the atypical memory
    one aims to use in a program.
  prefs: []
  type: TYPE_NORMAL
- en: Being an example made for simplicity, mostly to show how the feature works and
    can be used, the astute reader that you are will hopefully see much room for improvement
    and optimization… Indeed, this manager is really slow and memory consuming, keeping
    a `std::vector<bool>` object where each `bool` value indicates whether a byte
    in the memory block is taken or not and performing a naïve linear search through
    that container whenever an allocation request is made (also, it’s not thread-safe,
    which is bad!). We will examine some quality of implementation considerations
    in [*Chapter 10*](B21071_10.xhtml#_idTextAnchor153), but nothing stops you from
    taking `shared_mem_mgr` and making it significantly better in the meantime.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will notice that `shared_mem_mgr` has been expressed as an RAII type: its
    constructor creates a shared memory segment, its destructor frees that memory
    segment, and the `shared_mem_mgr` type has been made uncopiable as is often the
    case for RAII types. The key member functions to look at in the following code
    excerpt are `allocate()` and `deallocate()`; the former tries to allocate a block
    from the shared memory segment and notes that this has been done, whereas the
    latter frees the memory associated with an address within the block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `shared_mem_mgr` really is a class that manages a chunk of memory,
    and there is no magic involved. Should someone want to improve the memory management
    algorithms, one could do so without touching the interface of this class, benefiting
    from the low coupling that stems from encapsulation.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to play…
  prefs: []
  type: TYPE_NORMAL
- en: One interesting way to refine `shared_mem_mgr` would be to first make this class
    responsible for allocating and freeing the shared memory, as it already does,
    then write a different class to manage the memory within that shared memory block,
    and finally, make them work together. This way, one could use `shared_mem_mgr`
    with different memory management algorithms and pick management strategies based
    on the needs of individual programs, or sections thereof. Something to try if
    you want to have fun!
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to implement the allocation operator overloads that take an
    argument of type `shared_mem_mgr&`. This is essentially trivial since all these
    overloads need to do is delegate the work to the manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Equipped with our manager and these overloads, we can write our test program
    that performs the same task as the “handmade” one from the previous section. In
    this case, however, there are some differences:'
  prefs: []
  type: TYPE_NORMAL
- en: We do not need to manage the shared memory segment’s creation and destruction.
    These tasks are handled by the `shared_mem_mgr` object as part of its implementation
    of the RAII idiom.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do not need to manage the shared memory block at all, as this task is assigned
    to the `shared_mem_mgr` object. Finding a location in the block to put an object,
    tracking how the block is being used for objects, ensuring that it’s possible
    to distinguish used areas from unused ones, and so on are all part of that class’s
    responsibilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a corollary, in the “handmade” version, we constructed an object at the beginning
    of the shared memory block and stated that it would be a burden on user code to
    construct more objects or manage the shared memory segment to take into account
    numerous calls to the `new` and `delete` operators, but in this implementation,
    we can freely call `new` and `delete` as much as we want since this memory management
    becomes transparent to client code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The construction aspect of objects in atypical memory is rather easy: just
    pass the additional argument in the call to the `new` and `new[]`operators. The
    finalization part of objects managed through a manager such as this is slightly
    more complex though: we cannot write the equivalent of `delete p` on our pointers
    as this would try to finalize the object *and* deallocate the memory through “normal”
    means. Instead, we need to manually finalize the objects, and then manually call
    the appropriate version of the `operator delete()` function in order to do the
    exotic memory cleanup tasks. Of course, given what we have written in [*Chapter
    6*](B21071_06.xhtml#_idTextAnchor096), you could encapsulate these tasks in a
    smart pointer of your own to get simpler and safer user code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We end up with the following example program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This is still not a trivial example, but the memory management aspect is clearly
    simpler than in the “handmade” version, and the compartmentalization of tasks
    makes it easier to optimize the way in which memory is managed.
  prefs: []
  type: TYPE_NORMAL
- en: And… we’re done. Whew! That was quite the ride, once more!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter explored various ways in which one can use the C++ memory management
    facilities in unusual ways: mapping objects onto memory-mapped hardware, integrating
    basic forms of error handling with the `nothrow` version of `operator new()`,
    reacting to out-of-memory situations with a `std::exception_handler`, and accessing
    atypical memory with non-portable services through a specialization of the “normal”
    allocation operator and a manager object. This gives us a broader overview of
    memory management facilities in C++ and how one can use them to one’s advantage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing we have mentioned but not yet discussed is optimization: how to make
    memory allocation and memory allocation fast, blazingly fast even, and deterministic
    in terms of execution speed when some conditions are met. This is what we will
    do in [*Chapter 10*](B21071_10.xhtml#_idTextAnchor153) when explaining how to
    write arena-based allocation code.'
  prefs: []
  type: TYPE_NORMAL
- en: Oh, and as a bonus, we will kill Orcs.
  prefs: []
  type: TYPE_NORMAL
- en: Orcs? What are you talking about?
  prefs: []
  type: TYPE_NORMAL
- en: Orcs are fictional creatures found in numerous works of fictional fantasy, usually
    mean beasts used as foes and that have an unhealthy relation to Elves, another
    kind of fictional creature that often has a better reputation. As your friendly
    author has worked a lot with game programmers over the last few decades, Orcs
    tend to appear in his examples and will be central to the code we write in [*Chapter
    10*](B21071_10.xhtml#_idTextAnchor153).
  prefs: []
  type: TYPE_NORMAL
- en: Sounds good? Then, on to the next chapter!
  prefs: []
  type: TYPE_NORMAL
