<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-54"><a id="_idTextAnchor056"/>3</h1>
<h1 id="_idParaDest-55"><a id="_idTextAnchor057"/>Exploring C++ Concepts from A Low-Latency Application’s Perspective</h1>
<p>In this chapter, we assume that the reader has an intermediate level of understanding of C++ programming concepts, features, and so on. We will discuss how to approach low-latency application development in C++. We will move on to discussing what C++ features to avoid specifically when it comes to low-latency applications. We will then discuss the key C++ features that make it perfect for low-latency applications and how we will use them in the rest of the book. We will conclude by discussing how to maximize compiler optimizations and which C++ compiler flags are important for low-latency applications.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Approaching low-latency application development in C++</li>
<li>Avoiding pitfalls and leveraging C++ features to minimize application latency</li>
<li>Maximizing C++ compiler optimization parameters</li>
</ul>
<p>Let us start by discussing the higher-level ideas when it comes to approaching low-latency application development in C++ in the next section.</p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor058"/>Technical requirements</h1>
<p>All the code for this book can be found in the GitHub repository for this book at <a href="https://github.com/PacktPublishing/Building-Low-Latency-Applications-with-CPP">https://github.com/PacktPublishing/Building-Low-Latency-Applications-with-CPP</a>. The source code for this chapter is in the Chapter3 directory in the repository.</p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor059"/>Approaching low-latency application development in C++</h1>
<p>In this section, we <a id="_idIndexMarker370"/>will discuss the higher-level ideas to keep in mind when trying to build low-latency applications in C++. Overall, the ideas are to understand the architecture that your application runs on, your application use cases that are latency-sensitive, the programming language of your choice (C++ in this case), how to work with the development tools (the compiler, linker, etc.) and how to measure application performance in practice to understand which parts of the application to optimize first.</p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor060"/>Coding for correctness first, optimizing second</h2>
<p>For low-latency applications, correct behavior of the application under different use cases and scenarios and robust handling of edge conditions is still the primary focus. A fast application that does not do what we need is useless, so the best approach when it comes to developing a low-latency application is to first code for correctness, not speed. Once the application works correctly, only then the focus should be shifted to optimizing the critical parts of the application while maintaining correctness. This ensures that developers spend time focusing on the correct parts to optimize because it is common to find that our intuition on which pieces are critical to performance does not match what happens in practice. Optimizing the code can also take significantly longer than coding for correctness, so it is important to optimize the most important things first.</p>
<h2 id="_idParaDest-59"><a id="_idTextAnchor061"/>Designing optimal data structures and algorithms</h2>
<p>Designing custom data structures that are optimal for the application’s use cases is an important part of building low-latency applications. A good amount of thought needs to be put into each data structure used in the critical parts of the application in terms of scalability, robustness, and performance under the use cases and data encountered <em class="italic">in practice</em>. It is important to understand why we mention the term <em class="italic">in practice</em> here because different data structure choices will perform better under different use cases and input data even if the different data structures themselves have the same output or behavior. Before <a id="_idIndexMarker371"/>we discuss an example of different possible data structures and algorithms to solve the same problem, let us quickly review Big-O notation. Big-O notation is used to describe the asymptotic worst-case time complexity of performing a certain task. The term asymptotic here is used to describe the fact that we discuss cases where we measure the performance over a theoretically infinite (in practice an exceptionally large) number of data points. The asymptotic performance eliminates all the constant terms and describes the performance only as a function of the number of input data elements.</p>
<p>A simple example of using different data structures to solve the same problem would be searching for an entry in a container by a key value. We can solve this either by using a hash map implementation that has an expected <em class="italic">amortized</em> complexity of <code>O(1)</code> or using an array that has a complexity of <code>O(n)</code>, where <code>n</code> is the number of elements in the container. While on paper it might appear that the hash map is clearly the way to go, other factors such as the number of elements, the complexity of applying the hash function to the keys, and so on might change which data structure is the way to go. In this case, for a handful of elements, the array solution is faster due to better cache performance, while for many elements, the hash map solution is better. Here, we chose a suboptimal algorithm because the underlying data structure for the suboptimal algorithm performed better in practice due to cache performance.</p>
<p>Another slightly different example would be using lookup tables over recomputing values for some mathematical functions, say, trigonometric functions. While it makes complete sense that looking up the result in a precomputed lookup table <em class="italic">should</em> always be faster compared to performing some calculations, this might not always be true. For instance, if the lookup table is very large, then the cost of evaluating a floating-point expression might be less than the cost of getting a cache miss and reading the lookup table value from the main memory. The overall application performance might also be better if accessing the lookup table from the main memory leads to a lot of cache pollution, leading to performance degradation in other parts of the application code.</p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor062"/>Being mindful of the processor</h2>
<p>Modern processors have a lot of architectural and functional details that a low-latency application developer should understand, especially a C++ developer since it allows very low-level control. Modern processors have multiple cores, larger and specialized register banks, pipelined instruction processing where instructions needed next are prefetched while executing the current one, instruction level parallelism, branch predictions, extended instruction sets to facilitate faster and specialized processing, and so on. The better the application developer understands these aspects of the processor on which their applications <a id="_idIndexMarker372"/>will run, the better they can avoid sub-optimal code and/or compilation choices and make sure that the compiled machine code is optimal for their target architecture. At the very least, the developer should instruct the compiler to output code for their specific target architecture using compiler optimization flags, but we will discuss that topic later in this chapter.</p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor063"/>Understanding the cache and memory access costs</h2>
<p>Typically, a lot of effort is put into the design and development of data structures and algorithms when it comes to low-latency application development from the perspective of reducing the amount of work done or the number of instructions executed. While this is the correct approach, in this section, we would like to point out that thinking about cache and memory accesses is equally important.</p>
<p>We saw in the previous sub-section, <em class="italic">Designing optimal data structures and algorithms</em>, that it is common for data structures and algorithms that are sub-optimal on paper to outperform ones that are optimal on paper. A large reason behind that can be the higher cache and memory access costs for the optimal solution outweighing the time saved because of the reduced number of instructions the processor needs to execute. Another way to think about this is that even though the amount of work from the perspective of the number of algorithmic steps is less, in practice, it takes longer to finish with the modern processor, cache, and memory access architectures today.</p>
<p>Let us quickly review the memory hierarchy in a modern computer architecture. Note that details of what we will recap here can be found in our other book, <em class="italic">Developing High-Frequency Trading Systems</em>. The key points here are that the memory hierarchy works in such a way that if the CPU cannot find the data or instruction it needs next in the register, it goes to the L0 cache, and if it cannot find it there, goes to the L1 cache, L2, other caches, and so on, then goes to the main memory in that order. Note that the storage is accessed from fastest to slowest, which also happens to be least amount of space to most amount of space. The art of effective low-latency and cache-friendly application development relies on writing code that is cognizant of code and data access patterns to maximize the likelihood of finding data in the fastest form of storage possible. This relies on maximizing the concepts of <strong class="bold">temporal locality</strong> and <strong class="bold">spatial locality</strong>. These terms <a id="_idIndexMarker373"/>mean that data accessed recently is likely to be in the cache and data next to what <a id="_idIndexMarker374"/>we just accessed is likely to be in the cache, respectively. The following diagram visually lays out the register, cache, and memory banks and <a id="_idIndexMarker375"/>provides some data on access times from the CPU. Note that there is a good amount of variability in the access times depending on the hardware and the constant improvements being made to technologies. The key takeaway here should be that there is a significant increase in access times as we go from CPU registers to cache banks to the main memory.</p>
<div><div><img alt="Figure 3.1 – The hierarchy of memory in modern computer architectures. " src="img/Figure_3.1_B19434.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – The hierarchy of memory in modern computer architectures.</p>
<p>I would advise you to think carefully about the cache and memory access patterns for the algorithm locally, as well as the entire application globally, to make sure that your source code optimizes cache and memory access patterns, which will boost overall application performance. If you have a function that executes very quickly when it is called but causes <a id="_idIndexMarker376"/>a lot of cache pollution, that will degrade the complete application’s performance because other components will incur additional cache miss penalties. In such a case, we have failed in our objective of having an application that performs optimally even though we might have managed to make this function perform optimally locally.</p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor064"/>Understanding how C++ features work under the hood</h2>
<p>When developing low-latency applications, it is very important that the developers have an extremely good understanding of how the high-level language abstractions work at a lower level or “under the hood.” For applications that are not latency-sensitive, this is perhaps not as important since if the application behaves the way the developer intends it to, the extremely low-level details of how their source code achieves that is not relevant.</p>
<p>For low-latency applications in C++, the more knowledge the developer has of how their program gets compiled into machine code, the better they can use the programming language to achieve low-latency performance. A lot of high-level abstractions available in C++ improve the ease and speed of development, robustness and safety, maintainability, software design elegance, and so on, but not all of them might be optimal when it comes to low-latency applications.</p>
<p>Many C++ features, such as dynamic polymorphism, dynamic memory allocation, and exception handling, are great additions to the language for most applications. However, these are best avoided <a id="_idIndexMarker377"/>or used sparingly or used in a specific manner when it comes to low-latency applications since they have larger overheads.</p>
<p>Conversely, traditional programming practices suggest the developer break everything down into numerous very small functions for reusability; use recursive functions when applicable; use <strong class="bold">Object-Oriented Programming</strong> (<strong class="bold">OOP</strong>) principles, such as inheritance and virtual <a id="_idIndexMarker378"/>functions; always use smart pointers instead of raw pointers; and so on. These principles are sensible for most applications, but for low-latency applications, these need to be evaluated and used carefully because they might add non-trivial amounts of overhead and latency.</p>
<p>The key takeaway here is that it is important for low-latency application developers to understand each one of these C++ features very well to understand how they are implemented in machine code and what impact they have on the hardware resources and how they perform in practice.</p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor065"/>Leveraging the C++ compiler</h2>
<p>The modern C++ compiler is truly a fascinating piece of software. There is an immense amount of effort <a id="_idIndexMarker379"/>invested into building these compilers to be robust and correct. A lot of effort is also made to make them very intelligent in terms of the transformations and optimizations they apply to the developer’s high-level source code. Understanding how the compiler translates the developer’s code into machine instructions, how it tries to optimize the code, and when it fails is important for low-latency application developers looking to squeeze as much performance out of their applications as possible. We will discuss the workings of the compiler and optimization opportunities extensively in this chapter so that we can learn to work with the compiler instead of against it when it comes to optimizing our final application’s representation (machine code executable).</p>
<h2 id="_idParaDest-64"><a id="_idTextAnchor066"/>Measuring and improving performance</h2>
<p>We mentioned that the ideal application development journey involves first building the application for correctness and then worrying about optimizing it after that. We also mentioned that it is not uncommon for a developer’s intuition to be incorrect when it comes to identifying performance bottlenecks.</p>
<p>Finally, we also mentioned that the task of optimizing an application can take significantly longer than the task of developing it to perform correctly. For that reason, it is advisable that before embarking on an optimization journey, the developer try to run the application under practical constraints and inputs to check performance. It is important to add instrumentation <a id="_idIndexMarker380"/>to the application in different forms to measure the performance and find bottlenecks to understand and prioritize the optimization opportunities. This is also an important step since as the application evolves, measuring and improving performance continues to be part of the workflow, that is, measuring and improving performance is a part of the application’s evolution. In the last section of this book, <em class="italic">Analyzing and improving performance</em>, we will discuss this idea with a real case study to understand this better.</p>
<h1 id="_idParaDest-65"><a id="_idTextAnchor067"/>Avoiding pitfalls and leveraging C++ features to minimize application latency</h1>
<p>In this section, we will look at different C++ features that, if used correctly, can minimize application <a id="_idIndexMarker381"/>latency. We will <a id="_idIndexMarker382"/>also discuss the details of using these features in a manner that optimizes application performance throughout this sub-section. Now, let us start learning about how to use these features correctly to maximize application performance and avoid the pitfalls to minimize latency. Note that all the code snippets for this chapter are in the <code>Chapter3</code> directory in the GitHub repository for this book.</p>
<h2 id="_idParaDest-66"><a id="_idTextAnchor068"/>Choosing storage</h2>
<p>Local variables <a id="_idIndexMarker383"/>created within a function are stored on the stack by default and the stack memory is also used to store function return values. Assuming no large objects are created, the same range of stack storage space is reused a lot, resulting in great cache performance due to locality of reference.</p>
<p>Register variables are closest to the processor and are the fastest possible form of storage available. They are extremely limited, and the compiler will try to use them for the local variables that are used the most, another reason to prefer <em class="italic">local variables</em>.</p>
<p>Static variables are inefficient from the perspective of cache performance since that memory cannot be re-used for other variables and accessing static variables is likely a small fraction of all memory accesses. So, it is best to avoid static variables as well as global variables, which have similarly inefficient cache performance.</p>
<p>The <code>volatile</code> keyword instructs the compiler to disable a lot of optimizations that rely on the assumption that the variable value does not change without the compiler’s knowledge. This should only ever be used carefully in multi-threaded use cases since it prevents optimizations such as storing the variables in registers and force-flushing them to the main memory from the cache every time the value changes.</p>
<p>Dynamically allocated memory is inefficient to allocate and deallocate and, depending on how it is used, can suffer from poor cache performance. More on dynamically allocated memory inefficiencies will be discussed later in this section in the <em class="italic">Dynamically allocating </em><em class="italic">memory</em> sub-section.</p>
<p>An example of C++ optimization technique that leverages storage choice optimization is <strong class="bold">Small String Optimization</strong> (<strong class="bold">SSO</strong>). SSO attempts to use local storage for short strings if they are smaller than <a id="_idIndexMarker384"/>a certain size (typically 32 characters) instead of the default of dynamically allocated memory for string content storage.</p>
<p>In summary, you should think carefully about where the data gets stored during the execution of your <a id="_idIndexMarker385"/>program, especially in the critical sections. We should try to use registers and local variables as much as possible and optimize cache performance. Use volatile, static, global, and dynamic memory only when necessary or when it does not affect performance on the critical path.</p>
<h2 id="_idParaDest-67"><a id="_idTextAnchor069"/>Choosing data types</h2>
<p>C++ integer operations <a id="_idIndexMarker386"/>are typically super-fast as long as the size of the largest register is larger than the integer size. Integers smaller or larger than the register size are sometimes slightly slower than regular integers. This is because the processor must use multiple registers for a single variable and apply some carry-over logic for large integers. Conversely, handling integers smaller than a register size is usually handled by using a regular register, zeroing out the upper bits, using only the lower bits, and possibly invoking a type conversion operation. Note that the extra overhead is very small and generally not something to worry about. Signed and unsigned integers are equally fast, but in some cases unsigned integers are faster than signed integers. The only cases where signed integer operations are a tiny bit slower is where the processor needs to check and adjust for the sign bit. Again, the extra overhead is extremely small when present and not necessarily something we <a id="_idIndexMarker387"/>need to worry about in most cases. We will look at the cost of different operations – addition, subtraction, comparison, bit operations, and so on typically take a single clock cycle. Multiplication operations take longer, and division operations take longest.</p>
<h2 id="_idParaDest-68"><a id="_idTextAnchor070"/>Using casting and conversion operations</h2>
<p>Converting between signed and unsigned integers is free. Converting integers from a smaller size into a <a id="_idIndexMarker388"/>larger one can take a single clock cycle but sometimes can be optimized to be free. Converting integer sizes down from a larger size into a smaller one has no additional cost.</p>
<p>Conversion between floats, doubles, and long doubles is typically free except under very few conditions. Conversion of signed and unsigned integers into floats or doubles takes a few clock cycles. Conversion from unsigned integers can take longer than signed integers.</p>
<p>Conversion from floating-point values into integers can be extremely expensive – 50 to 100 clock cycles or more. If these conversions are on the critical path, it is common for low-latency application developers to try and make these more efficient by enabling special instruction sets, avoiding or refactoring these conversions, if possible, using special assembly language rounding implementations, and so on.</p>
<p>Converting pointers from one type into another type is completely free; whether the conversions are safe or not is the developer’s responsibility. Type-casting a pointer to an object to a pointer to a different object violates the strict aliasing rule stating that <em class="italic">two pointers of different types cannot point to the same memory location</em>, which really means that it is possible the compiler might not use the same register to store the two different pointers, even though they point to the same address. Remember that the CPU registers are the fastest form of storage available to the processor but are extremely limited in storage capacity. So, when an extra register gets used to store the same variable, it is an inefficient use of the registers and negatively impacts performance overall.</p>
<p>An example of type-casting a pointer to be a different object is presented here. This example uses a conversion from <code>double *</code> into <code>uint64_t *</code> and modifies the sign bit using the <code>uint64_t</code> pointer. This is nothing more than a convoluted and more efficient method <a id="_idIndexMarker389"/>of achieving <code>x = -std::abs(x)</code> but demonstrates how this violates the strict aliasing rule (<code>strict_alias.cpp</code> in <code>Chapter3</code> on GitHub):</p>
<pre class="source-code">
#include &lt;cstdio&gt;
#include &lt;cstdint&gt;
int main() {
  double x = 100;
  const auto orig_x = x;
  auto x_as_ui = (uint64_t *) (&amp;x);
  *x_as_ui |= 0x8000000000000000;
  printf(“orig_x:%0.2f x:%0.2f &amp;x:%p &amp;x_as_ui:%p\n”,
       orig_x, x, &amp;x, x_as_ui);
}</pre>
<p>It yields something like this:</p>
<pre class="source-code">
orig_x:100.00 x:-100.00 &amp;x:0x7fff1e6b00d0 &amp;x_as_ui:0x7fff1e6b00d0</pre>
<p>Using modern C++ casting operations, <code>const_cast</code>, <code>static_cast</code>, and <code>reinterpret_cast</code> do not incur any additional overhead when used. However, when it comes to <code>dynamic_cast</code>, which converts an object of a certain class into an object of a different class, this can be expensive at runtime. <code>dynamic_cast</code> checks whether the conversion is valid using <strong class="bold">Run-Time Type Information</strong> (<strong class="bold">RTTI</strong>), which is slow and possibly throws <a id="_idIndexMarker390"/>an exception if the conversion is invalid – this makes it safer but increases latency.</p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor071"/>Optimizing numerical operations</h2>
<p>Typically, double-precision <a id="_idIndexMarker391"/>calculations take about the same time as single-precision operations. In general, for integers and floating values, additions <a id="_idIndexMarker392"/>are fast, multiplications are slightly more expensive than additions, and division is quite a bit more expensive than multiplication. Integer multiplications take around 5 clock cycles and floating-point multiplications take around 8 clock cycles. Integer additions take a single clock cycle on most processors and floating-point additions take around 2-5 clock cycles. Floating-point divisions and integer divisions both take about the same amount of time around 20-80 clock cycles, depending on the processor and depending on whether it has special floating-point operations or not.</p>
<p>Compilers will try to rewrite and reduce expressions wherever possible to prefer faster operations such as rewriting divisions to be multiplications by reciprocals. Multiplication and division by values that are powers of 2 are significantly faster because the compiler rewrites them to be bit-shift operations, which are much faster. There is additional overhead when the compiler uses this optimization since it must handle signs and rounding errors. Obviously, this only applies when the expressions involve values that can be determined to be powers of 2 at compile time. When dealing with multi-dimensional arrays, for instance, the compiler converts multiplications into bitwise shift operations wherever possible.</p>
<p>Mixing single- and double-precision operations in the same expression and expressions involving floating and integer values should be avoided because they implicitly force type conversions. We saw before that type conversions are not always free, so these expressions can take longer to compute than we would guess. For instance, when mixing single- and double-precision values in an expression, the single-precision values must first be converted into double-precision values, which can consume a few clock cycles before the expression is computed. Similarly, when mixing integers and floating-point values in an expression, either the floating-point value has to be converted into an integer or the integer must be converted into a floating-point value, which adds a few clock cycles to the final calculation time.</p>
<h2 id="_idParaDest-70"><a id="_idTextAnchor072"/>Optimizing boolean and bitwise operations</h2>
<p>Boolean operations <a id="_idIndexMarker393"/>such as <code>&amp;&amp;</code>) and <code>||</code>) are evaluated such that for <code>&amp;&amp;</code>, if the first <a id="_idIndexMarker394"/>operand is false, then the second one is not evaluated, and, for <code>||</code>, if the first <a id="_idIndexMarker395"/>operand is true, then the second one is not <a id="_idIndexMarker396"/>evaluated. A simple optimization technique is to <a id="_idIndexMarker397"/>order the operands of <code>&amp;&amp;</code> in order from lowest to highest probability of being evaluated to true.</p>
<p>Similarly, for <code>||</code>, ordering <a id="_idIndexMarker398"/>the operands from highest to lowest probability of being true is best. This technique is referred to as <code>&amp;&amp;</code> boolean operation, the second operand should not be evaluated if the first one is false. Or for an <code>||</code> boolean operation, the second operand should not be evaluated if the first one is true, and so on.</p>
<p>Another aspect of using boolean variables is understanding the way they are stored. Boolean variables are stored as 8 bits and not a single bit, as might match our intuition from the way they are used. What this means is that operations involving boolean values have to be implemented such that any 8-bit values other than 0 are treated as 1, which leads to implementations with branches in them with comparisons against 0. For example, the <code>c = a &amp;&amp; b;</code> expression is implemented as follows:</p>
<pre class="source-code">
if(a != 0) {
 if(b != 0) {
   c = true;
 } else {
   c = false;
 }
} else {
 c = false;
}</pre>
<p>If there was a guarantee that <code>a</code> and <code>b</code> could not have values other than 0 or 1, then <code>c = a &amp;&amp; b;</code> would simply be <code>c = a &amp; b;</code>, which is super-fast and avoids branching and branching-related overheads.</p>
<p>Bitwise operations can also help speed up other cases of boolean expressions by treating each bit of an integer as a single boolean variable and then rewriting expressions involving comparisons of multiple booleans with bit-masking operations. For instance, take an expression such as this, where <code>market_state</code> is <code>uint64_t</code> and <code>PreOpen</code>, <code>Opening</code>, and <code>Trading</code> are enum values that reflect different market states:</p>
<pre class="source-code">
if(market_state == PreOpen ||
   market_state == Opening ||
   market_state == Trading) {
  // do something...
}</pre>
<p>It can be rewritten as follows:</p>
<pre class="source-code">
if(market_state &amp; (PreOpen | Opening | Trading)) {
  // do something...
}</pre>
<p>If the enum values are <a id="_idIndexMarker403"/>chosen such that each bit in the <code>market_state</code> variable <a id="_idIndexMarker404"/>represents a state of true <a id="_idIndexMarker405"/>or false, one choice would be for the <code>PreOpen</code>, <code>Opening</code>, and <code>Trading</code> enums to <a id="_idIndexMarker406"/>be set to <code>0x001</code>, <code>0x010</code>, and <code>0x100</code>.</p>
<h2 id="_idParaDest-71"><a id="_idTextAnchor073"/>Initializing, destroying, copying, and moving objects</h2>
<p>Constructors <a id="_idIndexMarker407"/>and destructors <a id="_idIndexMarker408"/>for developer-defined classes should be kept as light and efficient as possible since they can be <a id="_idIndexMarker409"/>called without the developer <a id="_idIndexMarker410"/>expecting it. Keeping these methods super-simple and small also allows the compiler to <em class="italic">inline</em> these methods to improve performance. The same applies to copy and move constructors, which should be kept simple, with using move constructors preferred over using copy <a id="_idIndexMarker411"/>constructors <a id="_idIndexMarker412"/>wherever possible. In <a id="_idIndexMarker413"/>many cases <a id="_idIndexMarker414"/>where high levels of optimization are required, the developer can delete the default constructor and the copy constructor to make sure unnecessary or unexpected copies of their objects are not being made.</p>
<h2 id="_idParaDest-72"><a id="_idTextAnchor074"/>Using references and pointers</h2>
<p>A lot of C++ features are built around implicitly accessing class members through the <code>this</code> pointer, so <a id="_idIndexMarker415"/>access through <a id="_idIndexMarker416"/>references and pointers occurs very frequently regardless of whether the developer explicitly does so or not. Accessing objects through pointers and references is mostly as efficient as directly accessing the objects. This is because most modern processors have support to efficiently fetch the pointer values and dereference them. The big disadvantage of using references and pointers is that they take up an extra register for the pointer themselves and the other one consists of the extra dereference instructions to access the variable pointed to by the pointer value.</p>
<p>Pointer arithmetic is just as fast as integer arithmetic except computing the differences between pointers requires a division by the size of the object, which can potentially be very slow. This is not necessarily a problem if the size of the type of object is a multiple of 2, which is quite often the case with primitive types and optimized structures.</p>
<p>Smart pointers are an important feature of modern C++ that offers safety, life cycle management, automatic memory management, and clear ownership control for dynamically allocated <a id="_idIndexMarker417"/>objects. Smart pointers such as <code>std::unique_ptr</code>, <code>std::shared_ptr</code>, and <code>std::weak_ptr</code> use the <code>std::shared_ptr</code> due to the reference counting overhead but generally, smart pointers are expected to add very little overhead to the entire program unless there are a lot of them.</p>
<p>Another important aspect of using pointers is that it can prevent compiler optimizations due to <code>a[0]</code> to <code>a[n-1]</code> and <code>b</code>. That means that this optimization is valid because <code>*b</code> is a constant for the entire loop and can be computed once:</p>
<pre class="source-code">
void func(int* a, int* b, int n) {
  for(int i = 0; i &lt; n; ++i) {
    a[i] = *b;
  }
}</pre>
<p>There are really two options for instructing the compiler to assume no pointer aliasing in cases where the developer is confident that there is no behavior that is dependent on the side effects <a id="_idIndexMarker419"/>of pointer aliasing. Use <code>__restrict__</code>, or <code>__restrict</code>, a similar specifier keyword, for your compiler <a id="_idIndexMarker420"/>on the function arguments or functions to specify no aliasing on the pointers. However, this is a hint, and the compiler does not guarantee that this will make a difference. The other option is to specify the <code>-fstrict-aliasing</code> compiler option to assume no pointer aliasing globally. The following code block demonstrates the use of the <code>restrict</code> specifier for the preceding <code>func()</code> function (<code>pointer_alias.cpp</code> in <code>Chapter3</code> on GitHub):</p>
<pre class="source-code">
void func(int *__restrict a, int *__restrict b, int n) {
  for (int i = 0; i &lt; n; ++i) {
    a[i] = *b;
  }
}</pre>
<h2 id="_idParaDest-73"><a id="_idTextAnchor075"/>Optimizing jumping and branching</h2>
<p>In modern processor pipelines, instructions and data are fetched and decoded in stages. When there <a id="_idIndexMarker421"/>is a branch instruction, the processor tries to predict which of the branches will be taken and fetches and decodes instructions from that branch. However, when the processor has mispredicted the branch taken, it takes 10 or more clock cycles before it detects the misprediction. After that, it must spend a bunch of clock cycles fetching the instructions and data from the correct branch and evaluate it. The key takeaway here is that a branch misprediction wastes many clock cycles every time it happens.</p>
<p>Let us discuss some of the most used forms of jumps and branches in C++:</p>
<ul>
<li><code>if-else</code> branching is the most common thing that comes to mind when discussing branching. Long chains of <code>if-else</code> conditionals are best avoided, if possible, because it is difficult to predict these correctly as they grow. Keeping the number of conditions small and trying to structure them so they are more predictable is the way to optimize them.</li>
<li><code>for</code> and <code>while</code> loops are also types of branching that are typically predicted well if the loop count is relatively small. This, of course, gets complicated with nested loops and loops containing hard-to-predict exit conditions.</li>
<li><code>switch</code> statements are branches with multiple jump targets, so they can be very difficult to predict. When label values are widely spread out, the compiler must use <code>switch</code> statements as a long sequence of <code>if-else</code> branching trees. An optimization technique that works well with <code>switch</code> statements is to assign case label values that increment by one and are arranged in ascending order because there is a very good chance they will get implemented as jump tables, which is significantly more efficient.</li>
</ul>
<p>Replacing branching with table lookups containing different output values in the source code is a good optimization wherever possible. We can also create a table of function pointers indexed by jump conditions but beware that function pointers are not necessarily much more efficient than the branching itself.</p>
<p><code>loop_unroll.cpp</code> in <code>Chapter3</code> on GitHub):</p>
<pre class="source-code">
   int a[5]; a[0] = 0;
    for(int i = 1; i &lt; 5; ++i)
      a[i] = a[i-1] + 1;</pre>
<p>The compiler can unroll the loop into the following code shown here. Note that it is more than likely <a id="_idIndexMarker423"/>that for such a simple example, the compiler will use additional optimizations and reduce this loop even further. But for now, we limit ourselves to only present the impact of loop unrolling:</p>
<pre class="source-code">
    int a[5];
    a[0] = 0;
    a[1] = a[0] + 1; a[2] = a[1] + 1;
    a[3] = a[2] + 1; a[4] = a[3] + 1;</pre>
<p>Compile-time branching using an <code>if constexpr (condition-expression) {}</code> format can obviously help a lot by moving the overhead of branching to compile time, but this requires that <code>condition-expression</code> be something that can be evaluated at compile time. This is <a id="_idIndexMarker424"/>technically part of the <strong class="bold">Compile time Polymorphism</strong> or <strong class="bold">Template Metaprogramming</strong> paradigm, which we will <a id="_idIndexMarker425"/>discuss more in the <em class="italic">Using compile-time polymorphism</em> sub-section in this section.</p>
<p>It is possible to provide the compiler with branch prediction hints in the source code since the developer has a better idea of the expected use cases. These do not make a significant difference overall since modern processors are good at learning which branches are most likely to be taken after a few iterations through the branches. For GNU C++, these are traditionally implemented as follows using <code>__builtin_expect</code>:</p>
<pre class="source-code">
#define LIKELY_CONDITION(x) __builtin_expect(!!(x), 1)
#define UNLIKELY_CONDITION (x) __builtin_expect(!!(x), 0)</pre>
<p>For C++ 20, these <a id="_idIndexMarker426"/>are standardized as the <code>[[likely]]</code> and <code>[[</code><code>unlikely]]</code> attributes.</p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor076"/>Calling functions efficiently</h2>
<p>There are numerous overheads associated with calling functions – the overhead of fetching the <a id="_idIndexMarker427"/>function address and jumping to it, passing the parameters to it and returning the results, setting up the stack frame, saving and restoring registers, exception handling, possible latency in the code cache misses, and so on.</p>
<p>When breaking up the code base into functions, some general things to consider to maximize the performance would be the following.</p>
<h3>Thinking before creating an excessive number of functions</h3>
<p>Functions should <a id="_idIndexMarker428"/>only be created if there is enough re-usability to justify them. The criteria for creating functions should be logical program flow and re-usability and not the length of code because, as we saw, calling functions is not free, and creating excessive functions is not a good idea.</p>
<h3>Grouping related functions together</h3>
<p>Class member and non-class member functions typically get assigned memory addresses in the order in which they are created, so it is generally a good idea to group together performance-critical functions that call each other frequently or operate on the same datasets. This facilitates better code and data cache performance.</p>
<h3>Link Time Optimization (LTO) or Whole Program Optimization (WPO)</h3>
<p>When writing <a id="_idIndexMarker429"/>performance-critical functions, it is important to place <a id="_idIndexMarker430"/>them in the same module where they are used if possible. Doing so unlocks a lot of compiler optimizations, the most important of which is the ability to inline the function call.</p>
<p>Using the <code>static</code> keyword to <a id="_idIndexMarker431"/>declare a function does the equivalent <a id="_idIndexMarker432"/>of putting it in an <code>inline</code> keyword achieves this as well, but <a id="_idIndexMarker433"/>we will explore that in the next section.</p>
<p>Specifying WPO <a id="_idIndexMarker434"/>and LTO parameters for the compiler instructs it to treat the <a id="_idIndexMarker435"/>entire code base as a single module and enable compiler optimizations across modules. Without enabling these <a id="_idIndexMarker436"/>compiler options, optimizations occur across functions in the same module but not between modules which can be quite sub-optimal for large code bases which typically have a lot of source files and modules.</p>
<h3>Macros, inline functions, and template metaprogramming</h3>
<p><strong class="bold">Macro expressions</strong> are a pre-processor directive and are expanded even before compilation begins. This eliminates <a id="_idIndexMarker437"/>the overhead associated with calling and returning from functions at runtime. Macros have several disadvantages though, such as namespace collision, cryptic compilation errors, unnecessary evaluation of conditions and expressions, and so on.</p>
<p>Inlined functions, whether they are part of a class or not, are similar to macros but solve a lot of the problems <a id="_idIndexMarker438"/>associated with macros. Inlined functions are expanded at their usage during compilation and link times and eliminate the overhead associated with function calls.</p>
<p>Using template metaprogramming, it is possible to move a lot of the computation load from runtime to <a id="_idIndexMarker439"/>compile time. This involves using partial and full template specialization and recursive loop templates. However, template metaprogramming can be clumsy and difficult to use, compile, and debug and should only really be used where the pe<a id="_idTextAnchor077"/>rformance improvements justify the increased development discomfort. We will explore templates and template metaprogramming shortly.</p>
<h3>Avoiding function pointers</h3>
<p>Calling a function through a function pointer has a larger overhead than directly calling the function. For one, if the pointer changes, then the compiler cannot predict which function will be called and cannot pre-fetch the instructions and data. Additionally, this also prevents a lot of compiler optimizations since these cannot be inlined at compile time.</p>
<p>The <code>std::function</code> is a much more powerful construct available in modern C++ but should be used only if necessary since there is potential for misuse and extra overhead of a few clock cycles compared to direct inlined functions. <code>std::bind</code> is another construct to be very careful about when using and should also only be used if absolutely necessary. If <code>std::function</code> must be used, try to see whether you can use a lambda expression instead of <code>std::bind</code> since that is typically a few clock cycles faster to invoke. Overall, be careful when using <code>std::function</code> and/or <code>std::bind</code> since a lot of developers are surprised that these constructs can perform virtual function calls and invoke dynamic memory allocations under the hood.</p>
<h3>Passing function parameters by reference or pointers</h3>
<p>For primitive types, passing parameters by value is super-efficient. For composite types that are function parameters, the preferred way of passing them would be const references. The <strong class="bold">constness</strong> means that the object cannot be modified and allows the compiler to apply optimizations <a id="_idIndexMarker440"/>based on that and the reference allows the compiler to possibly inline the object itself. If the function needs to modify the object passed to it, then obviously a non-const reference or pointer is the way to go.</p>
<h3>Returning simple types from functions</h3>
<p>Functions that return primitive types are very efficient. Returning composite types is much more inefficient and can lead to a couple of copies being created in some cases, which is quite sub-optimal especially if these are large and/or have slow copy constructors and assignment <a id="_idIndexMarker441"/>operators. When the compiler can apply <strong class="bold">Return Value Optimization</strong> (<strong class="bold">RVO</strong>), it can eliminate the temporary copy created and just write the result to the caller’s object directly. The optimal way to return a composite type is to have the caller create an object of that type and pass it to the function using a reference or a pointer for the function to modify.</p>
<p>Let us look at an example to explain what happens with RVO; let us say we have the following function definition and call to the function (<code>rvo.cpp</code> in <code>Chapter3</code> on GitHub):</p>
<pre class="source-code">
#include &lt;iostream&gt;
struct LargeClass {
  int i;
  char c;
  double d;
};
auto rvoExample(int i, char c, double d) {
  return LargeClass{i, c, d};
}
int main() {
  LargeClass lc_obj = rvoExample(10, ‘c’, 3.14);
}</pre>
<p>With RVO, instead of creating a temporary <code>LargeClass</code> object inside <code>rvoExample()</code> and then copying it into the <code>LargeClass lc_obj</code> object in <code>main()</code>, the <code>rvoExample()</code> function can directly update <code>lc_obj</code> and avoid the temporary object and copy.</p>
<h3>Avoiding recursive functions or replacing them with a loop</h3>
<p>Recursive functions are inefficient because of the overhead of calling themselves repeatedly. Additionally, recursive functions can go very deep in the stack and take up a lot of stack space, and, in <a id="_idIndexMarker442"/>worst-case scenarios, even cause a stack overflow. This causes a lot of cache misses due to the new memory areas and makes predicting the return address difficult and inefficient. In such cases, replacing recursive functions with a loop is significantly more efficient since it avoids a lot of the cache performance issues that recursive functions encounter.</p>
<h2 id="_idParaDest-75"><a id="_idTextAnchor078"/>Using bitfields</h2>
<p><strong class="bold">Bitfields</strong> are just structs where the developer controls the number of bits assigned to each member. This <a id="_idIndexMarker443"/>makes the data as compact as possible and greatly improves <a id="_idIndexMarker444"/>cache performance for many objects. Bitfield members are also usually modified using bitmask operations, which are very efficient, as we have seen before. Accessing the members of bitfields is <a id="_idIndexMarker445"/>less efficient than accessing the members of a regular structure, so it is important to carefully <a id="_idIndexMarker446"/>assess whether using bitfields and improving the cache <strong class="bold">performance</strong> is worthwhile.</p>
<h2 id="_idParaDest-76"><a id="_idTextAnchor079"/>Using runtime polymorphism</h2>
<p><code>Virtual</code> functions are the key to implementing runtime polymorphism, but they have an additional overhead compared to non-virtual function calls.</p>
<p>Usually, the compiler cannot determine at compile time which implementation of a virtual function will be called. At runtime, this causes many branch mispredictions unless the same version of the virtual function gets called most of the time. It is possible for the compiler to determine the virtual function implementation called at compile time using <code>virtual</code> functions is <a id="_idIndexMarker449"/>that the compiler cannot apply many of the compile-time optimizations in the presence of <code>virtual</code> functions, the most important one being inlining.</p>
<p>Inheritance in C++ is another important OOP concept but be careful when the inheritance structure gets too complicated since there are many subtle inefficiencies that can be introduced. Child classes inherit every single data member from their parent class, so the size of the child classes can become quite large and lead to poor cache performance.</p>
<p>In general, instead of inheriting from multiple parent classes, we can consider using the <code>composition.cpp</code> in <code>Chapter3</code> on GitHub) builds <code>OrderBook</code>, which basically holds a vector of <code>Order</code> objects, in two different ways. The benefit (if used properly) of the inheritance model is that it now inherits all the methods that <code>std::vector</code> provides while the composition model would need to implement them. In this example, we demonstrate this by implementing a <code>size()</code> method in <code>CompositionOrderBook</code>, which calls the <code>size()</code> method on the <code>std::vector</code> object, while <code>InheritanceOrderBook</code> inherits it directly from <code>std::vector</code>:</p>
<pre class="source-code">
#include &lt;cstdio&gt;
#include &lt;vector&gt;
struct Order { int id; double price; };
class InheritanceOrderBook : public std::vector&lt;Order&gt; { };
class CompositionOrderBook {
  std::vector&lt;Order&gt; orders_;
public:
  auto size() const noexcept {
    return orders_.size();
  }
};
int main() {
  InheritanceOrderBook i_book;
  CompositionOrderBook c_book;
  printf(“InheritanceOrderBook::size():%lu Composi
       tionOrderBook:%lu\n”, i_book.size(), c_book.size());
}</pre>
<p>C++ <code>dynamic_cast</code>, as we discussed before, usually uses the RTTI information to perform the cast and should also be avoided.</p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor080"/>Using compile-time polymorphism</h2>
<p>Let us discuss an alternative to using runtime polymorphism, which is to use templates to achieve compile-time <a id="_idIndexMarker453"/>polymorphism. Templates are <a id="_idIndexMarker454"/>similar to macros, meaning they are expanded before compilation, and because of this, not only is the runtime overhead eliminated but it also unlocks additional compiler optimization opportunities. Te<a id="_idTextAnchor081"/>mplates make the compiler machine code super-efficient but they come at the cost of additional source code complexity, as well as larger executable sizes.</p>
<p>The <code>virtual</code> functions and the base class and derived class relationships are similar but slightly different using the <em class="italic">CRTP</em>. A simple example of converting runtime polymorphism into compile-time polymorphism is shown here. In both cases, the derived classes, <code>SpecificRuntimeExample</code> and <code>SpecificCRTPExample</code>, override the <code>placeOrder()</code> method. The code discussed in this sub-section is<a id="_idTextAnchor082"/> in the <code>crtp.cpp</code> file in the GitHub repo for this book under the <code>Chapter3</code> directory.</p>
<h3>Runtime polymorphism using virtual functions</h3>
<p>Here, we have an <a id="_idIndexMarker456"/>example of implementing runtime polymorphism where <code>SpecificRuntimeExample</code> derives <code>RuntimeExample</code> and overrides the <code>placeOrder()</code> method:</p>
<pre class="source-code">
#include &lt;cstdio&gt;
class RuntimeExample {
public:
  virtual void placeOrder() {
    printf(“RuntimeExample::placeOrder()\n”);
  }
};
class SpecificRuntimeExample : public RuntimeExample {
public:
  void placeOrder() override {
    printf(“SpecificRuntimeExample::placeOrder()\n”);
  }
};</pre>
<h3>Compile-time polymorphism using the CRTP</h3>
<p>Now we implement similar functionality as discussed in the previous section, but instead of using <a id="_idIndexMarker457"/>runtime polymorphism, we use compile-time polymorphism. Here, we use the CRTP pattern and <code>SpecificCRTPExample</code> specializes/implements the <code>CRTPExample</code> interface and has a different implementation of <code>placeOrder()</code> via <code>actualPlaceOrder()</code>:</p>
<pre class="source-code">
template &lt;typename actual_type&gt;
class CRTPExample {
public:
  void placeOrder() {
    static_cast&lt;actual_type*&gt;(this)-&gt;actualPlaceOrder();
  }
  void actualPlaceOrder() {
    printf(“CRTPExample::actualPlaceOrder()\n”);
  }
};
class SpecificCRTPExample : public CRTPExample&lt;Specific
     CRTPExample&gt; {
public:
  void actualPlaceOrder() {
    printf(“SpecificCRTPExample::actualPlaceOrder()\n”);
  }
};</pre>
<h3>Invoking polymorphic methods in the two cases</h3>
<p>Finally, in the <a id="_idIndexMarker458"/>following snippet presented, we show how we would create <code>SpecificRuntimeExample</code> and <code>SpecificCRTPExample</code> objects. We then invoke runtime and compile-time polymorphism respectively using the <code>placeOrder()</code> method:</p>
<pre class="source-code">
int main(int, char **) {
  RuntimeExample* runtime_example = new SpecificRuntimeEx
       ample();
  runtime_example-&gt;placeOrder();
  CRTPExample&lt;SpecificCRTPExample&gt; crtp_example;
  crtp_example.placeOrder();
  return 0;
}</pre>
<p>Running this yields the following output, the first line using runtime polymorphism and the second line using compile time polymorphism:</p>
<pre class="source-code">
SpecificRuntimeExample::placeOrder()
SpecificCRTPExample::actualPlaceOrder()</pre>
<h2 id="_idParaDest-78"><a id="_idTextAnchor083"/>Using additional compile-time processing</h2>
<p><strong class="bold">Template metaprogramming</strong> is a more general term that means writing code that itself yields <a id="_idIndexMarker459"/>more code. The benefit here is also to move computations <a id="_idIndexMarker460"/>from runtime to compile time and maximize compiler optimization opportunities and runtime performance. It is possible to write almost anything with template metaprogramming, but it can get extremely complicated and difficult to understand, maintain, and debug, lead to very long compilation times, and increase the binary size to a very large size.</p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor084"/>Handling exceptions</h2>
<p>The C++ exception handling system is designed to detect unexpected error conditions at runtime and either gracefully recover or shut down from that point. When it comes to low-latency <a id="_idIndexMarker461"/>applications, it is important to evaluate the use of exception handling since while it is true that exception handling incurs the largest latencies during these rare error cases, there can still be some overhead even when exceptions are not raised. There is some bookkeeping overhead related to the logic used to recover gracefully when exceptions are raised under various scenarios. With nested functions, exceptions need to be propagated all the way up to the top-most caller function and each stack frame needs to be cleaned up. This is known as <strong class="bold">stack unwinding</strong> and requires the exception handler to track all the <a id="_idIndexMarker462"/>information it needs to walk backward during an exception.</p>
<p>For low-latency applications, exceptions are either disabled per function using the <code>throw()</code> or <code>noexcept</code> specifications or disabled across the entire program using compiler flags. This allows the compiler to assume that some or all methods will not throw an exception and hence the processor does not have to worry about saving and tracking recovery information. Note that using <code>noexcept</code> or disabling the C++ exception handling system is not without some disadvantages. For one, usually, the C++ exception handling system does not typically add a lot of extra overhead unless an exception is thrown, so this decision must be made with careful consideration. Another point is that if a method marked as <code>noexcept</code> throws an exception for some reason, the exception can no longer be propagated up the stack and instead the program is terminated right there. What this means is that disabling the C++ exception handling system either partially or fully makes handling failures and exceptions harder and completely the developer’s responsibility. Usually, what this means is that the developer will still need to make sure that exceptional error conditions are not encountered or handled elsewhere, but the point is that now the developer has explicit control over this and can move it out <a id="_idIndexMarker463"/>of the critical hot path. For this reason, it is common that during the development and testing phases, the C++ exception handling system is not disabled, but only during the very last optimization steps do we consider removing exception handling.</p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor085"/>Accessing cache and memory</h2>
<p>We have frequently referred to cache performance while discussing different uses of C++ features since <a id="_idIndexMarker464"/>accessing the main memory is significantly slower than the clock cycles used to execute CPU instructions or access registers or cache storage. Here are some general points to keep in mind when trying to optimize cache and memory access.</p>
<h3>Aligning data</h3>
<p>Variables that are aligned, in that they are placed at memory locations that are multiples of the size <a id="_idIndexMarker465"/>of the variable, are accessed most efficiently. The term <strong class="bold">word size</strong> for processors describes the number of bits read by and processed by processors, which for modern processors is <a id="_idIndexMarker466"/>either 32-bits or 64-bits. This is because the processor can read a variable from memory up to the word size in a single read operation. If the variable is aligned in memory, then the processor does not have to do any extra work to get it into the required register to be processed.</p>
<p>For these reasons, aligned variables are more efficient to handle, and the compiler will take care of automatically aligning variables. This includes adding padding in between member variables in a class or a struct to keep those variables aligned. When adding member variables to structures where we expect to have a lot of objects, it is important to consider the extra padding added carefully because the size of the struct will be larger than expected. The extra space in each instance of this struct’s or class’s objects means that they can have worse cache performance if there are a lot of them. The recommended approach here would be to reorder the members of the struct so that minimal extra padding is added to keep the members aligned.</p>
<p>We will see an example that orders the same members inside a structure in three different ways – one where there is a lot of additional padding added to keep each variable aligned, another where the developer reorders the member variables to minimize space waste due to compiler-added padding, and, finally, where we use the <code>pack()</code> pragma to eliminate <a id="_idIndexMarker467"/>all padding. This code is available in the <code>Chapter3/alignment.cpp</code> file in the GitHub repository for this book:</p>
<pre class="source-code">
#include &lt;cstdio&gt;
#include &lt;cstdint&gt;
#include &lt;cstddef&gt;
struct PoorlyAlignedData {
  char c;
  uint16_t u;
  double d;
  int16_t i;
};
struct WellAlignedData {
  double d;
  uint16_t u;
  int16_t i;
  char c;
};
#pragma pack(push, 1)
struct PackedData {
  double d;
  uint16_t u;
  int16_t i;
  char c;
};
#pragma pack(pop)
int main() {
  printf(“PoorlyAlignedData c:%lu u:%lu d:%lu i:%lu
       size:%lu\n”,
         offsetof(struct PoorlyAlignedData,c), offsetof
              (struct PoorlyAlignedData,u), offsetof(struct
              PoorlyAlignedData,d), offsetof(struct PoorlyA
              lignedData,i), sizeof(PoorlyAlignedData));
  printf(“WellAlignedData d:%lu u:%lu i:%lu c:%lu
       size:%lu\n”,
         offsetof(struct WellAlignedData,d), offsetof
              (struct WellAlignedData,u), offsetof(struct
              WellAlignedData,i), offsetof(struct WellAligned
              Data,c), sizeof(WellAlignedData));
  printf(“PackedData d:%lu u:%lu i:%lu c:%lu size:%lu\n”,
         offsetof(struct PackedData,d), offsetof(struct
              PackedData,u), offsetof(struct PackedData,i),
              offsetof(struct PackedData,c), sizeof
              (PackedData));
}</pre>
<p>This code outputs the following on my system, displaying the offsets of the different data members <a id="_idIndexMarker468"/>in each of the three designs of the same structure. Note that the first version has an extra 11 bytes of padding, the second one only has an extra 3 bytes of padding due to the reordering, and the last version has no extra padding:</p>
<pre class="source-code">
PoorlyAlignedData c:0 u:2 d:8 i:16 size:24
WellAlignedData d:0 u:8 i:10 c:12 size:16
PackedData d:0 u:8 i:10 c:12 size:13</pre>
<h3>Accessing data</h3>
<p>Cache-friendly data access (read and/or write) is when the data is accessed sequentially or somewhat <a id="_idIndexMarker469"/>sequentially. If the data is accessed backward, it is less efficient than this, and cache performance is worse if the data is accessed randomly. This is something to consider, especially when accessing multi-dimensional arrays of objects and/or objects residing in a container with a non-trivial underlying storage of the objects.</p>
<p>For instance, accessing elements in an array is significantly more efficient than accessing elements in a linked list, tree, or hash-map container because of the contiguous memory storage versus random memory storage locations. From the perspective of algorithmic complexity, searching linearly in an array is less efficient than using a hash map since the array search has <code>O(n)</code> and the hash map has <code>O(1)</code> theoretical algorithmic complexity.  However, if the number of elements is small enough, then using the array still yields better performance, a large reason being due to cache performance and algorithm overhead.</p>
<h3>Using large data structures</h3>
<p>When dealing with large multi-dimensional matrix datasets, for instance, with linear algebra <a id="_idIndexMarker470"/>operations, cache access performance dominates the performance of the operation. Often, the actual algorithm implementation for matrix operations is different from that used in classic texts to reorder the matrix access operations for cache performance. The best approach here is to measure the performance of different algorithms and access patterns and find the one that performs best under different matrix dimensions, cache contention conditions, and so on.</p>
<h3>Grouping variables together</h3>
<p>When designing <a id="_idIndexMarker471"/>classes and method or non-method functions, grouping variables that are accessed together greatly improves cache performance by reducing the number of cache misses. We discussed that preferring local variables over global, static, and dynamically allocated memory leads <a id="_idIndexMarker472"/>to better cache performance.</p>
<h3>Grouping functions together</h3>
<p>Grouping class member functions and non-member functions together so that functions that are used <a id="_idIndexMarker473"/>together are close together in memory also leads to better cache performance. This is because functions are placed in memory addresses depending on where they are in the developer’s source code and functions next to each other get assigned addresses close to each other.</p>
<h2 id="_idParaDest-81"><a id="_idTextAnchor086"/>Dynamically allocating memory</h2>
<p>Dynamically allocated memory has several good use cases, specifically when the size of containers is not <a id="_idIndexMarker474"/>known at compile time and when they can grow or shrink in size during the application instance’s life cycle. Dynamically allocated memory is also important for objects that are<a id="_idIndexMarker475"/> very large and take up a lot of stack space. Dynamically allocated memory can have a place in low-latency applications if allocation and deallocation are not done on the critical path and an allocated block of memory is used so that the cache performance is not hurt.</p>
<p>A disadvantage of dynamically allocated memory is that the process of allocating and deallocating memory blocks is <a id="_idIndexMarker476"/>awfully slow. The repeated allocation and deallocation of memory blocks of varied sizes fragments the heap, that is, it creates free memory blocks of different sizes interspersed with allocated memory blocks.</p>
<p>A fragmented heap makes the allocation and deallocation process even slower. Allocated memory blocks might not be optimally aligned unless the developer is careful about it. Dynamically allocated memory accessed through pointers causes pointer aliasing and prevents compiler optimizations, as we have seen before. There are other disadvantages of dynamically allocated memory, but these are the biggest ones for low-latency applications. Hence, it is best to avoid dynamically allocated memory completely when it comes to low-latency applications, or at the very least use it carefully and sparingly.</p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor087"/>Multi-threading</h2>
<p>If low-latency applications use multi-threading, the threads and the interactions between these threads should be designed carefully. Starting and stopping threads takes time, so it is best to avoid launching new threads when they are needed and instead use a thread pool of worker <a id="_idIndexMarker477"/>threads. Task switching or context switching is when one thread is paused or blocked, and another thread starts executing in its place. Context switching is very expensive since it requires the OS to save the state of the current thread, load the state of the next thread, start the processing, and so on, and is usually accompanied by memory reads and writes, cache misses, instruction pipeline stalls, and so on.</p>
<p>Synchronization using locks and mutexes between threads is also expensive and involves additional checks around concurrent access and context-switching overhead. When multiple threads access shared resources, they need to use the <code>volatile</code> keyword and that also prevents several compiler optimizations. Additionally, different threads can compete for the same cache lines and invalidate each other’s caches and this contention leads to terrible cache performance. Each thread gets its own stack, so it’s best to keep the shared data to a minimum and allocate variables locally on the thread’s stack.</p>
<h1 id="_idParaDest-83"><a id="_idTextAnchor088"/>Maximizing C++ compiler optimization parameters</h1>
<p>In this last section, we will understand how advanced and amazing modern C++ compilers are at <a id="_idIndexMarker478"/>optimizing the C++ code that the developers write. We will understand how compilers optimize the C++ code during the compilation, linking, and optimization stages to generate the most efficient machine code possible. We will understand how compilers optimize high-level C++ code and when they fail to do the best job. We will follow that up with a discussion on what the application developer can do to aid the compilers in their optimization task. Finally, we will look at different options available in modern C++ compilers <a id="_idIndexMarker479"/>by looking specifically at the <strong class="bold">GNU compiler</strong> (<strong class="bold">GCC</strong>). Let us start by understanding how compilers optimize our C++ program.</p>
<h2 id="_idParaDest-84"><a id="_idTextAnchor089"/>Understanding how compilers optimize</h2>
<p>In this sub-section, we will understand the different compiler optimization techniques that the compiler employs during its many passes over the high-level C++ code. The compiler typically first performs local optimizations and then tries to globally optimize these smaller code sections. It does so over several passes through the translated machine code during the pre-processing, compilation, linking, and optimization stages. Broadly, most compiler optimization techniques have some common themes, some of which overlap and some of which conflict with each other, which we will look at next.</p>
<h3>Optimizing the common cases</h3>
<p>This concept applies to software development too and helps the compiler optimize the code better. If the <a id="_idIndexMarker480"/>compiler can understand which code paths the program execution will spend most of its time in, it can optimize the common path to be faster even if it slows down the paths that are rarely taken. This results in better performance overall, but typically this is harder for the compiler to achieve at compilation time since it is not obvious which code paths are expected to be more likely unless the developer adds directives to specify this. We will discuss the hints that a developer can provide to the compiler to help specify which code paths are expected to be more likely during runtime.</p>
<h3>Minimizing branching</h3>
<p>Modern processors typically prefetch data and instructions before they are required so that the processors can execute instructions as quickly as possible. However, when there are jumps and <a id="_idIndexMarker481"/>branches (conditional and unconditional), the processor cannot know which instructions and data will be needed ahead of time with 100% certainty. What this means is that sometimes the processor incorrectly predicts the branch taken and thus the instructions and data prefetched are incorrect. When this happens, there is an extra penalty incurred since now the processor must remove the instructions and data that were fetched incorrectly and replace them with the correct instructions and data and then execute them after that. Techniques such as loop unrolling, inlining, and branch prediction hints help reduce branching and the misprediction of branching and improve performance. We will explore these concepts in more detail later in this section.</p>
<p>There are several cases in which a developer can refactor code in such a way that they avoid branching and achieve the same behavior. Sometimes, these optimization opportunities are only available to the developer, who understands the code and behavior at a deeper level than the compiler. A very simple example of how to convert a code block that uses branching and transform it to avoid branching is presented next. Here we have an enumeration to track side for an execution and we track the last bought/sold quantity, as well as updating <a id="_idIndexMarker482"/>the position in two different ways. The first way uses a branch on the <code>fill_side</code> variable and the second method avoids that branching by assuming that the <code>fill_side</code> variable can only have <code>BUY</code>/<code>SELL</code> values and can be cast to integers to be indexed into an array. This code can be found in the <code>Chapter3/branch.cpp</code> file:</p>
<pre class="source-code">
#include &lt;cstdio&gt;
#include &lt;cstdint&gt;
#include &lt;cstdlib&gt;
enum class Side : int16_t { BUY = 1, SELL = -1 };
int main() {
  const auto fill_side = (rand() % 2 ? Side::BUY : Side
       ::SELL);
  const int fill_qty = 10;
  printf(“fill_side:%s fill_qty:%d.\n”, (fill_side == Side
       ::BUY ? “BUY” : (fill_side == Side::SELL ? “SELL” :
         “INVALID”)), fill_qty);
  { // with branching
    int last_buy_qty = 0, last_sell_qty = 0, position = 0;
    if (fill_side == Side::BUY) {
      position += fill_qty; last_buy_qty = fill_qty;
    } else if (fill_side == Side::SELL) {
      position -= fill_qty; last_sell_qty = fill_qty; }
    printf(“With branching - position:%d last-buy:%d last-
         sell:%d.\n”, position, last_buy_qty,
           last_sell_qty);
  }
  { // without branching
    int last_qty[3] = {0, 0, 0}, position = 0;
    auto sideToInt = [](Side side) noexcept { return
         static_cast&lt;int16_t&gt;(side); };
    const auto int_fill_side = sideToInt(fill_side);
    position += int_fill_side * fill_qty;
    last_qty[int_fill_side + 1] = fill_qty;
    printf(“Without branching - position:%d last-buy:%d
         last-sell:%d.\n”, position, last_qty[sideToInt
           (Side::BUY) + 1], last_qty[side
             ToInt(Side::SELL)+
             1]);
  }
}</pre>
<p>And both the <a id="_idIndexMarker483"/>branching and branchless implementations compute the same values:</p>
<pre class="source-code">
fill_side:BUY fill_qty:10.
With branching - position:10 last-buy:10 last-sell:0.
Without branching - position:10 last-buy:10 last-sell:0.</pre>
<h3>Reordering and scheduling instructions</h3>
<p>The compiler can take advantage of advanced processors by re-ordering instructions in such a way that <a id="_idIndexMarker484"/>parallel processing <a id="_idIndexMarker485"/>can happen at the instruction, memory, and thread levels. The compiler can detect dependencies between code blocks and re-order them so that the program still works correctly but executes faster by executing instructions and processing data in parallel at the processor level. Modern processors can reorder instructions even without the compiler doing so, but it helps if the compiler can make it easier for the processors to do so as well. The main objective here is to prevent stalls and bubbles in modern processors, which have multiple pipelined processors, by choosing and ordering instructions in such a way as to preserve the original logical flow.</p>
<p>A simple example of how an expression can be reordered to take advantage of parallelism is shown here. Note that this is somewhat hypothetical since the actual implementation of this will vary greatly depending on the processor and the compiler:</p>
<pre class="source-code">
x = a + b + c + d + e + f;</pre>
<p>As it is written, this expression has a data dependency and would be executed sequentially, roughly as follows, and cost 5 clock cycles:</p>
<pre class="source-code">
x = a + b;
x = x + c;
x = x + d;
x = x +e;
x = x + f;</pre>
<p>It can be re-ordered into the following instructions, and assuming the advanced processor can perform two additions at a time, can be reduced to three clock cycles. This is because two operations such as <code>x = a + b;</code> and <code>p = c +d;</code> can be performed in parallel since they are independent of each other:</p>
<pre class="source-code">
x = a + b; p = c + d;
q = e + f; x = x + p;
x = x + q;</pre>
<h3>Using special instructions depending on the architecture</h3>
<p>During the compilation process, the compiler can choose which CPU instructions to use to implement <a id="_idIndexMarker486"/>the high-level program logic. When the compiler generates an executable for a specific architecture, it can use special instructions that the architecture supports. This means there is an opportunity to generate even more efficient instruction sequences, which leverage the special instructions that the architecture provides. We will look at how to specify this in the <em class="italic">Learning about compiler optimization </em><em class="italic">flags</em> section.</p>
<h3>Vectorization</h3>
<p>Modern processors can use vector registers to perform multiple calculations on multiple pieces of <a id="_idIndexMarker487"/>data in parallel. For instance, the SSE2 instruction set has 128-bit vector registers, which can be used to perform multiple operations on multiple integers or floating values depending on the size of these types. Extending this further, the AVX2 instruction set, for example, has 256-bit vector registers and can support a higher degree of vectorized operations. This optimization can be technically considered as part of the discussion in the <em class="italic">Using special instructions depending on the architecture</em> section from before.</p>
<p>To understand vectorization even better, let us present the following very simple example of a loop that operates on two arrays and stores the result in another array (<code>vector.cpp</code> in <code>Chapter3</code> in GitHub):</p>
<pre class="source-code">
  const size_t size = 1024;
  float x[size], a[size], b[size];
  for (size_t i = 0; i &lt; size; ++i) {
    x[i] = a[i] + b[i];
  }</pre>
<p>For architectures that support special vector registers such as the SSE2 instruction set we discussed before, it can hold 4 4-byte float values simultaneously and perform 4 additions at a time. In this case, the compiler can leverage the vectorization optimization technique and re-write <a id="_idIndexMarker488"/>this as the following with loop unrolling to use the SSE2 instruction set:</p>
<pre class="source-code">
  for (size_t i = 0; i &lt; size; i += 4) {
    x[i] = a[i] + b[i];
    x[i + 1] = a[i + 1] + b[i + 1];
    x[i + 2] = a[i + 2] + b[i + 2];
    x[i + 3] = a[i + 3] + b[i + 3];</pre>
<h3>Strength reduction</h3>
<p><strong class="bold">Strength reduction</strong> is a term used to describe compiler optimizations where complex operations that are <a id="_idIndexMarker489"/>quite expensive are replaced by instructions that are simpler <a id="_idIndexMarker490"/>and cheaper to improve performance. A classic example is one in which the compiler replaces operations involving division by some value with multiplication by the reciprocal of that value. Another example would be replacing multiplication by a loop index with an addition operation.</p>
<p>The simplest example we could think of is presented here, where we try to convert a price from its double notation into its integer notation by dividing the floating value by its minimum valid price increment. The variant that demonstrates the strength reduction that a compiler would perform is a simple multiplication instead of a division. Note that <code>inv_min_price_increment = 1 / min_price_increment;</code> is a <code>constexpr</code> expression, so it is not evaluated at runtime. This code is available in the <code>Chapter3/strength.cpp</code> file:</p>
<pre class="source-code">
#include &lt;cstdint&gt;
int main() {
  const auto price = 10.125; // prices are like: 10.125,
       10.130, 10.135...
  constexpr auto min_price_increment = 0.005;
  [[maybe_unused]] int64_t int_price = 0;
  // no strength reduction
  int_price = price / min_price_increment;
  // strength reduction
  constexpr auto inv_min_price_increment = 1 /
       min_price_increment;
  int_price = price * inv_min_price_increment;
}</pre>
<h3>Inlining</h3>
<p>Calling functions is expensive, <a id="_idIndexMarker491"/>as we have already seen before. There are several steps:</p>
<ul>
<li>Saving the current state of variables and execution</li>
<li>Loading the variables and instructions from the function being called</li>
<li>Executing them and possibly returning back values and resuming execution after the function call</li>
</ul>
<p>The compiler tries to replace a call to a function with the body of the function where possible to remove this overhead associated with calling functions and optimize performance. Not only that but now that it has replaced a call to a function with the actual body of the function, that opens room for more optimizations since the compiler can inspect this new larger code block.</p>
<h3>Constant folding and constant propagation</h3>
<p><strong class="bold">Constant folding</strong> is a no-brainer optimization technique and applies when there are expressions whose <a id="_idIndexMarker492"/>output can be computed entirely at compile time that do not depend <a id="_idIndexMarker493"/>on runtime branches or variables. Then, the compiler computes these expressions at compile time and replaces the evaluation of these expressions with the compile-time constant output value.</p>
<p>A similar and closely related compiler optimization tracks values in the code that are known to be compile-time constants and tries to propagate those constant values and unlock additional optimization opportunities. This optimization technique is known as <strong class="bold">constant propagation</strong>. An example <a id="_idIndexMarker494"/>would be loop unrolling if the <a id="_idIndexMarker495"/>compiler can determine the starting value, incremental value, or stopping value of the loop iterator.</p>
<h3>Dead Code Elimination (DCE)</h3>
<p><strong class="bold">DCE</strong> applies when the compiler can detect code blocks that have no impact on the program behavior. This can <a id="_idIndexMarker496"/>be due to code blocks that are never needed or <a id="_idIndexMarker497"/>code blocks where the calculations do not end up being used or affect the outcome. Once the compiler detects such <em class="italic">dead</em> code blocks, it can remove them and boost program performance. Modern compilers emit warnings when the outcome of running some code ends up not being used to help developers find such cases, but the compiler cannot detect all of these cases at compile time and there are still opportunities for DOE once it is translated into machine code instructions.</p>
<h3>Common Subexpression Elimination (CSE)</h3>
<p><strong class="bold">CSE</strong> is a specific optimization technique where the compiler finds duplicated sets of instructions or <a id="_idIndexMarker498"/>calculations. Here, the compiler restructures <a id="_idIndexMarker499"/>the code to remove this redundancy by computing the result only once and then using the value where it is required.</p>
<h3>Peephole optimizations</h3>
<p><strong class="bold">Peephole optimization</strong> is a relatively generic compiler optimization term that refers to a compiler optimization <a id="_idIndexMarker500"/>technique where the compiler tries to search for <a id="_idIndexMarker501"/>local optimizations in short sequences of instructions. We use the term local because the compiler does not necessarily try to understand the entire program and optimize it globally. Of course, however, by repeatedly and iteratively performing peephole optimizations, the compiler can achieve a decent degree of optimization at a global scale.</p>
<h3>Tail call optimization</h3>
<p>We know that function calls are not cheap because they have overhead associated with passing parameters and results and affect the cache performance and processor pipeline. <code>__attribute__ ((noinline))</code> attribute, which is there to explicitly prevent the compiler from inlining the <code>factorial()</code> function directly into <code>main()</code>. You can find this example in the <code>Chapter3/tail_call.cpp</code> source file on GitHub:</p>
<pre class="source-code">
auto __attribute__ ((noinline)) factorial(unsigned n) -&gt;
     unsigned {
  return (n ? n * factorial(n - 1) : 1);
}
int main() {
  [[maybe_unused]] volatile auto res = factorial(100);
}</pre>
<p>For this implementation, we would expect that in the machine code for the <code>factorial()</code> function, we would find a call to itself, but when compiled with optimization turned on, the compiler performs tail call optimization and implements the <code>factorial()</code> function as a loop and not a recursion. To observe that machine code, you can compile this code with something like this:</p>
<pre class="source-code">
g++ -S -Wall -O3 tail_call.cpp ; cat tail_call.s</pre>
<p>And in that <code>tail_call.s</code> file, you will see the call to <code>factorial()</code> in <code>main()</code> to be something like the following example. If this is your first time looking at assembly code, then let us quickly describe the instructions you will encounter.</p>
<ul>
<li>The <code>movl</code> instruction moves a value into a register (100 in the following block)</li>
<li>The <code>call</code> instruction calls a function (<code>factorial()</code> with name mangling (step where the C++ compiler changes the function names in intermediate code) and the parameter is passed in the <code>edi</code> register)</li>
<li>The <code>testl</code> instruction compares two registers and sets the zero flag if they’re equal</li>
<li><code>je</code> and <code>jne</code> check <a id="_idIndexMarker504"/>whether the <a id="_idIndexMarker505"/> zero flag is set and jump to the specified memory address if it is (<code>je</code>) or jump to the specified memory address if it is not (<code>jne</code>)</li>
<li>The <code>ret</code> instruction returns from the function and the return value is in the <code>eax</code> register:<pre class="source-code">
main:</pre><pre class="source-code">
.LFB1</pre><pre class="source-code">
    Movl    $100, %edi</pre><pre class="source-code">
    Call    _Z9factorialj</pre></li>
</ul>
<p>When you look at the <code>factorial()</code> function itself, you will find a loop (the <code>je</code> and <code>jne</code> instructions) instead of an additional <code>call</code> instruction to itself:</p>
<pre class="source-code">
_Z9factorialj:
.LFB0:
    Movl    $1, %eax
    testl    %edi, %edi
    je    .L4
.L3:
    Imull    %edi, %eax
    subl    $1, %edi
    jne    .L3
    ret
.L4:
    ret</pre>
<h3>Loop unrolling</h3>
<p><strong class="bold">Loop unrolling</strong> duplicates the body of the loop multiple times. Sometimes, it is not possible for the compiler to know at compile time how many times the loop will be executed – in which case, it <a id="_idIndexMarker506"/>will partially unroll the loop. For loops where the loop <a id="_idIndexMarker507"/>body is small and/or where it can be determined that the number of times that the loop will execute is low, the compiler can completely unroll the loop. This avoids the need for checking the loop counters and the overhead associated with conditional branching or looping. This is like function inlining where the call to the function is replaced by the body of the function. For loop unrolling, the entire loop is rolled out and replaces the conditional loop body.</p>
<h3>Additional loop optimizations</h3>
<p><strong class="bold">Loop unrolling</strong> is the primary loop-related optimization technique employed by compilers but there are additional loop optimizations:</p>
<ul>
<li><strong class="bold">Loop fission</strong> breaks a <a id="_idIndexMarker508"/>loop down into multiple loops operating on smaller sets of data to improve cache reference locality.</li>
<li><strong class="bold">Loop fusion</strong> does the <a id="_idIndexMarker509"/>opposite, where if two adjacent loops are executed the same number of times, they can be merged into one to reduce the loop overhead.</li>
<li><code>while</code> loop is transformed into a <code>do-while</code> loop inside <a id="_idIndexMarker510"/>a conditional <code>if</code> statement. This reduces the total number of jumps by two when the loop is executed and is typically applied to loops that are expected to execute at least once.</li>
<li><strong class="bold">Loop interchange</strong> exchanges <a id="_idIndexMarker511"/>inner loops and outer loops especially when doing so leads to better cache reference locality – for example, in the cases of iterating over an array where accessing memory contiguously makes a huge performance difference.</li>
</ul>
<h3>Register variables</h3>
<p><strong class="bold">Registers</strong> are internal processor memory and are the fastest form of storage available for the processor on account <a id="_idIndexMarker512"/>of being the closest to them. Because of this, the compiler tries to store variables that have the highest number of accesses in the registers. Registers, however, <a id="_idIndexMarker513"/>are limited, so the compiler needs to choose <a id="_idIndexMarker514"/>the variables to store effectively, and the effectiveness of this choice can make a significant difference to performance. The compiler typically picks variables such as local variables, loop counter and iterator variables, function parameters, commonly used expressions, or <strong class="bold">induction variables</strong> (variables that change by fixed amounts on each loop iteration). There are some limitations to what the compiler can place in registers such <a id="_idIndexMarker515"/>as variables whose address needs to be taken via pointers or references that need to reside in the main memory.</p>
<p>Now, we present a very simple example of how a compiler will transform a loop expression using induction variables. See the following code (<code>Chapter3/induction.cpp</code> on GitHub):</p>
<pre class="source-code">
  for(auto i = 0; i &lt; 100; ++i)
    a[i] = i * 10 + 12;
gets transformed into something of the form presented below
     and avoids the multiplication in the loop and replaces
     it
       with an induction variable based addition.
  int temp = 12;
  for(auto i = 0; i &lt; 100; ++i) {
    a[i] = temp;
    temp += 10;
  }</pre>
<h3>Live range analysis</h3>
<p>The term <strong class="bold">live range</strong> describes the code block within which a variable is active or used. If there are multiple <a id="_idIndexMarker516"/>variables in the same code block with overlapping live ranges, then<a id="_idIndexMarker517"/> each variable needs a different storage location. However, if there are variables with live ranges that do not overlap, then <a id="_idIndexMarker518"/>the compiler can use the same register for multiple variables in each live range.</p>
<h3>Rematerialization</h3>
<p><strong class="bold">Rematerialization</strong> is a compiler technique where the compiler chooses to re-calculate a value (assuming the calculation is trivial) instead of accessing the memory location that contains the value <a id="_idIndexMarker519"/>of this calculation already. The output value of this recalculation <a id="_idIndexMarker520"/>must be stored in registers, so this technique works in tandem with <em class="italic">register allocation techniques</em>. The main objective here is to avoid accessing the caches and main memory, which are slower to access than accessing the register storage. This, of course, depends on making sure that the recalculation takes less time than a cache or memory access.</p>
<h3>Algebraic reductions</h3>
<p>The compiler can find expressions that can be further reduced and simplified using algebraic laws. While <a id="_idIndexMarker521"/>software developers do not unnecessarily complicate expressions, there are cases where simpler forms of expressions exist compared to what the developer originally wrote in C++. Opportunities for algebraic reductions also show up as the compiler optimizes code iteratively due to inlining, macro expansions, constant folding, and so on.</p>
<p>Something to note here is that compilers do not typically apply algebraic reductions to floating-point operations because, in C++, floating-point operations are not safe to reduce due to precision issues. Flags need to be turned on to force the compiler to perform unsafe floating-point algebraic reductions, but it would be preferable for developers to reduce them explicitly and correctly.</p>
<p>The simplest example we can think of here is where a compiler might rewrite this expression:</p>
<pre class="source-code">
if(!a &amp;&amp; !b) {}</pre>
<p>Here, it uses two operations instead of three previously like so:</p>
<pre class="source-code">
if(!(a || b)) {}</pre>
<h3>Induction variable analysis</h3>
<p>The idea behind <strong class="bold">induction variable</strong>-related compiler optimization techniques is that an expression that is a <a id="_idIndexMarker522"/>linear function of the loop counter variable can be reduced <a id="_idIndexMarker523"/>into an expression that is a simple addition to a previous value. The simplest possible example would be calculating the address of elements in an array where the next element is at a memory location equal to the current element’s location plus the size of the object type. This is just a simple example since in modern compilers and processors, there <a id="_idIndexMarker524"/>are special instructions to calculate addresses of array elements and induction is not really used there, but induction variable-based optimizations are still performed for other loop expressions.</p>
<h3>Loop invariant code movement</h3>
<p>When the compiler can ascertain that some code and instructions within a loop are constant for the entire <a id="_idIndexMarker525"/>duration of the loop, that expression can be moved out of the loop. If there are expressions within the loop that conditionally yield one value or the other depending on branching conditions, those can also be moved out of the loop. Also, if there are expressions executed on each branch within a loop, these can be moved out of the branches and possibly the loop. There are many such optimization possibilities, but the fundamental idea is that code that does not need to be executed on each loop iteration or can be evaluated once before the loop falls under the umbrella of loop invariant code refactoring. Here is a hypothetical example of how loop invariant code movement implemented by the compiler would work. The first block is what the developer originally wrote, but the compiler can understand that the call to <code>doSomething()</code> and the expression involving the <code>b</code> variable are loop invariants and only need to be computed once. You will find this code in the <code>Chapter3/loop_invariant.cpp</code> file:</p>
<pre class="source-code">
#include &lt;cstdlib&gt;
int main() {
  auto doSomething = [](double r) noexcept { return 3.14 *
       r * r; };
  [[maybe_unused]] int a[100], b = rand();
  // original
  for(auto i = 0; i &lt; 100; ++i)
    a[i] = (doSomething(50) + b * 2) + 1;
  // loop invariant code movement
  auto temp = (doSomething(50) + b * 2) + 1;
  for(auto i = 0; i &lt; 100; ++i)
    a[i] = temp;
}</pre>
<h3>Static Single Assignment (SSA)-based optimizations</h3>
<p>SSA is a transformed form of the original program where instructions are re-ordered such that every variable is <a id="_idIndexMarker526"/>assigned in a <a id="_idIndexMarker527"/>single place. After this transformation, the compiler can apply many additional optimizations, leveraging the property that every variable is assigned in only a single place.</p>
<h3>Devirtualization</h3>
<p><strong class="bold">Devirtualization</strong> is a compiler <a id="_idIndexMarker528"/>optimization technique, especially for C++, that tries to avoid <strong class="bold">Virtual Table</strong> (<strong class="bold">vtable</strong>) lookups when calling virtual functions. This optimization <a id="_idIndexMarker529"/>technique boils down to the compiler figuring <a id="_idIndexMarker530"/>out the correct method to call at compile time. This can happen even when using virtual functions because in some cases, the object type is known at compile time, such as when there is only a single implementation of pure virtual functions.</p>
<p>Another case is where the compiler can determine that only a single derived class is created and used in some contexts or code branches, and it can replace the indirect functional call using vtable to be a direct call to the correct derived type’s method.</p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor090"/>Understanding when compilers fail to optimize</h2>
<p>In this section, we will discuss the different scenarios under which a compiler cannot apply some of the optimization techniques we discussed in the previous section. Understanding when compilers fail to optimize will help us develop C++ code that avoids these failures so that the code can be highly optimized by the compiler to yield highly efficient machine code.</p>
<h3>Failure to optimize across modules</h3>
<p>When the compiler <a id="_idIndexMarker531"/>compiles the entire program, it compiles modules independently of each other on a file-by-file basis. So, the compiler does not have information about functions in a module other than the one it is currently compiling. This prevents it from being able to optimize functions across modules and a lot of the techniques we saw cannot be applied since the compiler does not understand the whole program. Modern compilers solve such issues by using <strong class="bold">LTO</strong>, where, after the individual modules are compiled, the linker treats the different modules as if they were part of the same translation unit at compile time. This activates all the optimizations we have discussed so far, so it is important to enable LTO when trying to optimize the entire application.</p>
<h3>Dynamic memory allocation</h3>
<p>We already know that dynamic memory allocation is slow at runtime and introduces non-deterministic latency into your applications. They also have another side effect and that is <strong class="bold">pointer aliasing</strong> in the pointers that point to these dynamically allocated memory blocks. We will look at pointer aliasing in more detail next, but with dynamically allocated memory blocks, the compiler cannot ascertain that the pointers will necessarily point to different and non-overlapping memory areas, even though for the programmer it might seem obvious. This prevents various compiler optimizations that depend on aligning data or assuming alignment, as well as pointer aliasing-related inefficiencies, which we will see next. Local storage and declarations are also more cache-efficient because the memory space gets reused frequently as new functions are called and local objects are created. Dynamically allocated memory blocks can be randomly scattered in memory and yield poor cache performance.</p>
<h3>Pointer aliasing</h3>
<p>When accessing variables through pointers or references, while it might be obvious to the developer which pointers point to different and non-overlapping memory locations, the compiler cannot be 100% sure. To put it another way, the compiler cannot guarantee that a pointer is not pointing to another variable in the code block or different pointers are not pointing to overlapping memory locations. Since the compiler must assume this possibility, this prevents a lot of the compiler optimizations we discussed before since they can no longer be applied safely. There are ways to specify which pointers the compiler can safely assume are not aliases in C++ code. Another way would be to instruct the compiler to assume no pointer aliasing across the entire code, but that would require the developer to analyze all pointers and references and make sure there is never any aliasing, which is not trivial to do. Finally, the last option is to optimize the code explicitly keeping these hindrances to compiler optimizations in mind, which is not trivial either.</p>
<p>Our advice on dealing with pointer aliasing would be to do the following:</p>
<ol>
<li>Use the <code>__restrict</code> keyword in the function declarations when passing pointers to functions to instruct the compiler to assume no pointer aliasing for the pointers marked with that specifier</li>
<li>If additional optimization is required, we recommend explicitly optimizing code paths, being aware of pointer aliasing considerations</li>
<li>Finally, if additional <a id="_idIndexMarker532"/>optimizations are still required, we can instruct the compiler to assume no pointer aliasing across the entire code base, but this is a dangerous option and should only be used as a last resort</li>
</ol>
<h3>Floating-point induction variables</h3>
<p>Compilers typically do not use induction variable optimizations for floating-point expressions and variables. This is because of the rounding errors and issues with precision that we have discussed before. This prevents compiler optimizations when dealing with floating-point expressions and values. There are compiler options that can enable unsafe floating-point optimizations, but the developer must make sure to check each expression and formulate them in such a way that these precision issues due to compiler optimizations do not have unintended side effects. This is not a trivial task; hence, developers should be careful to either optimize floating-point expressions explicitly or analyze side effects from unsafe compiler optimizations.</p>
<h3>Virtual functions and function pointers</h3>
<p>We have already <a id="_idIndexMarker533"/>discussed that when it comes to virtual functions and function pointers, the compiler cannot perform optimizations at compile time since in many cases it is not possible for the compiler to determine which method will be called at runtime.</p>
<h2 id="_idParaDest-86"><a id="_idTextAnchor091"/>Learning about compiler optimization flags</h2>
<p>So far, we have discussed the different optimization techniques that the compiler uses, as well as the different cases where the compiler fails to optimize our C++ code. There are two fundamental <a id="_idIndexMarker534"/>keys to generating optimized low-latency code. The first is to write efficient C++ code and optimize manually in cases where the compiler might not be able to do so. Secondly, you can provide the compiler with as much visibility and information as possible so it can make the correct and best optimization decisions. We can convey our intent to the compiler through the compiler flags we use to configure it.</p>
<p>In this section, we will learn about the compiler flags for the GCC since that is the compiler we will use in this book. However, most modern compilers have flags to configure optimizations like the ones we will discuss in this section.</p>
<h3>Approaching compiler optimization flags</h3>
<p>At a high level, the general <a id="_idIndexMarker535"/>approach toward GCC compiler optimization flags is the following:</p>
<ul>
<li>The highest optimization level is typically preferred so <code>–O3</code> is a good starting point and enables a lot of optimizations, which we will see shortly.</li>
<li>Measuring the performance of the application in practice is the best way to measure and optimize the most critical code paths. GCC itself can perform <code>-fprofile-generate</code> option is enabled. The <a id="_idIndexMarker536"/>compiler determines the flow of the program and counts how many times each function and code branch is executed to find optimizations for the critical code paths.</li>
<li>Enabling <code>–flto</code> parameter enables LTO for our applications. The <code>-fwhole-program</code> option enables <strong class="bold">WPO</strong> to enable inter-procedural optimizations, treating the entire code base as a whole program.</li>
<li>Allowing the compiler to generate a build for a specific architecture where the application will run is a good idea. This lets the compiler use special instruction sets specific to that architecture and maximize optimization opportunities. For GCC, this is enabled using the <code>–</code><code>march</code> parameter.</li>
<li>It is recommended to disable <code>-</code><code>no-rtti</code> parameter.</li>
<li>It is possible to instruct the GCC compiler to enable fast floating-point value optimizations and even enable unsafe floating-point optimizations. GCC has the <code>-ffp-model=fast</code>, <code>-funsafe-math-optimizations</code> and <code>-ffinite-math-only</code> options to enable these unsafe floating-point optimizations. When using these flags, it is important that the developer carefully thinks about the order of operations and the precision resulting from these operations. When using a parameter such as <code>-ffinite-math-only</code>, make sure that all floating-point variables and expressions are finite because this optimization depends on that property. <code>-fno-trapping-math</code> and <code>-fno-math-errno</code> allow the compiler to <a id="_idIndexMarker537"/>vectorize loops containing floating-point operations by assuming that there will be no reliance on exception handling or the <code>errno</code> global variable for error signaling.</li>
</ul>
<h3>Understanding the details of GCC optimization flags</h3>
<p>In this section, we <a id="_idIndexMarker538"/>will provide additional details on <a id="_idIndexMarker539"/>the GCC optimization flags available. The complete list of optimization flags available is exceptionally large and out of the scope of this book. First, we will describe what turning on the higher-level optimization directives, <code>–O1</code>, <code>–O2</code>, and <code>–O3</code>, enables in GCC, and we encourage interested readers to learn about each one of these in greater detail from the GCC manual.</p>
<h4>Optimization level -O1</h4>
<p><code>–O1</code> is the first level of <a id="_idIndexMarker540"/>optimization and enables the following flags presented in the following table. At this level, the compiler tries to reduce the code size and execution time without incurring a very large increase in compilation, linking, and optimization times. These are the most important levels of optimization and provide tremendous optimization opportunities based on the ones we discussed in this chapter. We will discuss a few of the flags next.</p>
<p><code>-fdce</code> and <code>–fdse</code> perform <a id="_idIndexMarker541"/>DCE and <strong class="bold">Dead Store </strong><strong class="bold">Elimination</strong> (<strong class="bold">DSE</strong>).</p>
<p><code>-fdelayed-branch</code> is supported on many architectures and tries to reorder instructions to try and maximize the throughput of the pipeline after delayed branch instructions.</p>
<p><code>-fguess-branch-probability</code> tries to guess branch probabilities based on heuristics for branches that the developer has not provided any hints.</p>
<p><code>-fif-conversion</code> and <code>-fif-conversion2</code> try to eliminate branching by changing them into branchless equivalents using tricks similar to what we discussed in this chapter.</p>
<p><code>-fmove-loop-invariants</code> enables loop invariant code movement optimization.</p>
<p>If you are interested, you <a id="_idIndexMarker542"/>should investigate the details of these flags since discussing every parameter is outside the scope of this book.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fauto-inc-dec</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fshrink-wrap</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fbranch-count-reg</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fshrink-wrap-separate</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fcombine-stack-adjustments</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fsplit-wide-types</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fcompare-elim</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fssa-backprop</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fcprop-registers</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fssa-phiopt</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fdce</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-bit-ccp</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fdefer-pop</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-ccp</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fdelayed-branch</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-ch</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fdse</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-coalesce-vars</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fforward-propagate</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-copy-prop</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fguess-branch-probability</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-dce</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fif-conversion</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-dominator-opts</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fif-conversion2</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-dse</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>finline-functions-called-once</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-forwprop</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fipa-modref</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-fre</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fipa-profile</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-phiprop</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fipa-pure-const</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-pta</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fipa-reference</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-scev-cprop</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fipa-reference-addressable</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-sink</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fmerge-constants162</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-slsr</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fmove-loop-invariants</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-sra</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fmove-loop-stores</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-ter</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fomit-frame-pointer</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>funit-at-a-time</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>freorder-blocks</code></p>
</td>
<td class="No-Table-Style"/>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.1 – GCC optimization flags enabled when -O1 is enabled</p>
<h4>Optimization level -O2</h4>
<p><code>-O2</code> is the next optimization level and at this level, GCC will perform a lot more optimizations and will <a id="_idIndexMarker543"/>lead to longer compilation and linking times. <code>-O2</code> adds the flags in the following table in addition to the flags enabled by <code>–O1</code>. We will quickly discuss a few of these flags and leave a detailed discussion of each flag up to interested readers to pursue.</p>
<p><code>-falign-functions</code>, <code>-falign-labels</code>, and <code>-falign-loops</code> align the starting address of functions, jump targets, and loop locations so that the processor can access them as efficiently as possible. The principles we discussed on optimal data alignment in this chapter apply to the instruction addresses as well.</p>
<p><code>-fdelete-null-pointer-checks</code> lets the program assume that dereferencing null pointers is not safe and leverages that assumption to perform constant folding, eliminate null pointer checks, and so on.</p>
<p><code>-fdevirtualize</code> and <code>-fdevirtualize-speculatively</code> attempt to convert virtual function <a id="_idIndexMarker544"/>calls into direct function calls wherever possible. This, in turn, can lead to even more optimization due to inlining.</p>
<p><code>-fgcse</code> enables <strong class="bold">Global Common Subexpression Elimination</strong> (<strong class="bold">GCSE</strong>) and constant <a id="_idIndexMarker545"/>propagation.</p>
<p><code>-finline-functions</code>, <code>-finline-functions-called-once</code>, and <code>-findirect-inlining</code> increase the aggressiveness of the compiler in its attempts to inline functions and look for indirect inline opportunities due to previous optimization passes.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>falign-functions -falign-jumps</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>foptimize-sibling-calls</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>falign-labels -falign-loops</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>foptimize-strlen</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fcaller-saves</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fpartial-inlining</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fcode-hoisting</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fpeephole2</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fcrossjumping</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>freorder-blocks-algorithm=stc</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fcse-follow-jumps -fcse-skip-blocks</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>freorder-blocks-and-partition -freorder-functions</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fdelete-null-pointer-checks</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>frerun-cse-after-loop</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fdevirtualize -fdevirtualize-speculatively</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fschedule-insns -fschedule-insns2</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fexpensive-optimizations</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fsched-interblock -fsched-spec</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>ffinite-loops</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fstore-merging</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fgcse -fgcse-lm</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fstrict-aliasing</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fhoist-adjacent-loads</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fthread-jumps</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>finline-functions</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-builtin-call-dce</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>finline-small-functions</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-loop-vectorize</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>findirect-inlining</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-pre</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-fipa-bit-cp -</code><code>fipa-cp -fipa-icf</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-slp-vectorize</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-fipa-ra -</code><code>fipa-sra -fipa-vrp</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-switch-conversion -ftree-tail-merge</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fisolate-erroneous-paths-dereference</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-vrp</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>flra-remat</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fvect-cost-model=very-cheap</code></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.2 – GCC optimization flags enabled in addition to the ones from -O1 when -O2 is enabled</p>
<h4>Optimization level –O3</h4>
<p><code>–O3</code> is the most aggressive optimization option in GCC and it will optimize even when it leads to larger <a id="_idIndexMarker546"/>executable sizes as long as the program performs better. <code>-O3</code> enables the following flags presented in the next table beyond <code>–O2</code>. We quickly discuss a few important ones first and then provide the complete list.</p>
<p><code>-fipa-cp-clone</code> creates function clones to make interprocedural constant propagation and other forms of optimization stronger by trading execution speed at the cost of higher executable sizes.</p>
<p><code>-fsplit-loops</code> attempts to split a loop if it can avoid branching within the loop by having the loop for one side and then the other side – for instance, in a case where we check the side of execution in a trading algorithm within a loop and execute two different code blocks within the loop.</p>
<p><code>-funswitch-loops</code> moves loop invariant branches out of the loop to minimize branching.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table003">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fgcse-after-reload</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fsplit-paths</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fipa-cp-clone -floop-interchange</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-loop-distribution</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>floop-unroll-and-jam</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>ftree-partial-pre</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fpeel-loops</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>funswitch-loops</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fpredictive-commoning</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fvect-cost-model=dynamic</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>-</code><code>fsplit-loops</code></p>
</td>
<td class="No-Table-Style">
<p><code>-</code><code>fversion-loops-for-strides</code></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.3 – GCC optimization flags enabled in addition to the ones from -O2 when -O3 is enabled</p>
<p>We will discuss <a id="_idIndexMarker547"/>some additional compiler optimization flags we have found useful when it comes to optimizing low-latency applications.</p>
<h4>Static linkage</h4>
<p>The <code>–l library</code> option is passed to the linker to specify which library to link the executables with. However, if <a id="_idIndexMarker548"/>the linker finds a static library <a id="_idIndexMarker549"/>that has a name such as <code>liblibrary.a</code> and a shared library that has a name such as <code>liblibrary.so</code>, then we must specify the <code>–static</code> parameter to prevent linking with shared libraries and opt for the static library instead. We have discussed before why static linkage is preferred over shared library linkage for low-latency applications.</p>
<h4>Target architecture</h4>
<p>The <code>–march</code> parameter is used to specify the target architecture for which the compiler should build the final <a id="_idIndexMarker550"/>executable binary. For example, <code>–march=native</code> specifies that the compiler should build the executable for the <a id="_idIndexMarker551"/>architecture that it is being built on. We reiterate here that when the compiler knows the target architecture that the application is being built to run on, it can leverage information about that architecture, such as extended instruction sets and so on, to improve optimization.</p>
<h4>Warnings</h4>
<p>The<code>–Wall</code>, <code>–Wextra</code>, and <code>–Wpendantic</code> parameters control the number of warnings that are <a id="_idIndexMarker552"/>generated by the compiler when it detects a <a id="_idIndexMarker553"/>variety of different cases that are not <a id="_idIndexMarker554"/>technically errors but could be unsafe. It is advisable to turn these on for most applications because they detect potential bugs and typos in developers’ code. While these do not directly affect the compiler’s ability to optimize the application, sometimes, the warnings force developers to inspect cases of ambiguity or sub-optimal code, such as unexpected or implicit type conversions, which can be inefficient. The <code>–Werror</code> parameter turns these warnings into errors and <a id="_idIndexMarker555"/>will force the developer to inspect and fix each case that generates a compiler warning before compilation can succeed.</p>
<h4>Unsafe fast math</h4>
<p>This category of compiler optimization flags should not be enabled without a lot of consideration and due <a id="_idIndexMarker556"/>diligence. In C++, the compiler cannot apply a lot of floating-point optimizations that depend on properties such as floating-point operations yielding valid values, floating-point expressions being associative, and so on. To recap, this is because of the way floating-point values are represented in hardware, and a lot of these optimizations can lead to precision loss and different (and possibly incorrect) results. Enabling the <code>–ffast-math</code> parameter in turn enables the following parameters:</p>
<ul>
<li><code>–</code><code>fno-math-errno</code></li>
<li><code>–</code><code>funsafe-math-optimizations</code></li>
<li><code>–</code><code>ffinite-math-only</code></li>
<li><code>–</code><code>fno-rounding-math</code></li>
<li><code>–</code><code>fno-signaling-nans</code></li>
<li><code>–</code><code>fcx-limited-range</code></li>
<li><code>–</code><code>fexcess-precision=fast</code></li>
</ul>
<p>These parameters will allow the compiler to apply optimizations to floating-point expressions even <a id="_idIndexMarker557"/>if they are unsafe. These are not automatically enabled in any of the three optimization levels because they are unsafe and should only be enabled if the developer is confident that there are no errors or side effects that show up because of these.</p>
<h1 id="_idParaDest-87"><a id="_idTextAnchor092"/>Summary</h1>
<p>In this chapter, first, we discussed general advice that applies to developing low-latency applications in any programming language. We discussed the ideal software engineering approach when it comes to these applications and how to think about, design, develop, and evaluate building blocks such as the data structures and algorithms to use.</p>
<p>We emphasized that when it comes to low-latency application development specifically, the depth of knowledge on topics such as processor architecture, cache and memory layout and access, how the C++ programming language works under the hood, and how the compiler works to optimize your code will dictate your success. Measuring and improving performance is also a critical component for low-latency applications but we will dive into those details at the end of this book.</p>
<p>We spent a lot of time discussing different C++ principles, constructs, and features with the objective of understanding how they are implemented at a lower level. The goal here was to unlearn sub-optimal practices and emphasize some of the ideal aspects of using C++ for low-latency application development.</p>
<p>In the remainder of this book, as we build our low-latency electronic trading exchange ecosystem (collection of applications that interact with each other), we will reinforce and build on these ideas we discussed here as we avoid certain C++ features and use others instead.</p>
<p>In the last section of this chapter, we discussed many aspects of the C++ compiler in detail. We tried to build an understanding of how compilers optimize developers’ high-level code, as in, what techniques they have at their disposal. We also investigated scenarios in which the compiler fails to optimize a developer’s code. The goal there was for you to understand how to use a compiler to your advantage when trying to output the most optimal machine code possible and help the compiler help you avoid conditions where the compiler fails to optimize. Finally, we looked at the different compiler optimization flags available for the GNU GCC compiler, which is what we will use in the rest of this book.</p>
<p>We will put our theoretical knowledge into practice in the next chapter where we jump into implementing some common building blocks of low-latency applications in C++. We will keep our goal of building these components to be low-latency and highly performant. We will carefully use the principles and techniques we discussed in this chapter to build these high-performance components. In later chapters, we will use these components to build an electronic trading ecosystem.</p>
</div>
</body></html>