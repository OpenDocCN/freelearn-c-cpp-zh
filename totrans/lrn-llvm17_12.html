<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer026">&#13;
			<h1 id="_idParaDest-141" class="chapter-number"><a id="_idTextAnchor145"/>9</h1>&#13;
			<h1 id="_idParaDest-142"><a id="_idTextAnchor146"/>JIT Compilation</h1>&#13;
			<p>The LLVM core libraries come with the <strong class="bold">ExecutionEngine</strong> component<a id="_idIndexMarker517"/> that allows the compilation and execution of <strong class="bold">intermediate representation</strong> (<strong class="bold">IR</strong>) code<a id="_idIndexMarker518"/> in memory. Using this component, we can <a id="_idIndexMarker519"/>build <strong class="bold">just-in-time</strong> (<strong class="bold">JIT</strong>) compilers, which allows for direct execution of IR code. A JIT compiler works more like an interpreter because no object code needs to be stored on <span class="No-Break">secondary storage.</span></p>&#13;
			<p>In this chapter, you will learn about applications for JIT compilers, and how the LLVM JIT compiler works in principle. You will explore the LLVM dynamic compiler and interpreter and learn how to implement JIT compiler tools on your own. Furthermore, you will also learn how to use a JIT compiler as part of a static compiler, and the <span class="No-Break">associated challenges.</span></p>&#13;
			<p>This chapter will cover the <span class="No-Break">following topics:</span></p>&#13;
			<ul>&#13;
				<li>Getting an overview of LLVM’s JIT implementation and <span class="No-Break">use cases</span></li>&#13;
				<li>Using JIT compilation for <span class="No-Break">direct execution</span></li>&#13;
				<li>Implementing your own JIT compiler from <span class="No-Break">existing classes</span></li>&#13;
				<li>Implementing your own JIT compiler <span class="No-Break">from scratch</span></li>&#13;
			</ul>&#13;
			<p>By the end of the chapter, you will understand and know how to develop a JIT compiler, either using a preconfigured class or a customized version fitting for <span class="No-Break">your needs.</span></p>&#13;
			<h1 id="_idParaDest-143"><a id="_idTextAnchor147"/>Technical requirements</h1>&#13;
			<p>You can find the code used in this chapter at <a href="https://github.com/PacktPublishing/Learn-LLVM-17/tree/main/Chapter09"><span class="No-Break">https://github.com/PacktPublishing/Learn-LLVM-17/tree/main/Chapter09</span></a><span class="No-Break"><span class="hidden"> </span></span><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-144"><a id="_idTextAnchor148"/>LLVM’s overall JIT implementation and use cases</h1>&#13;
			<p>So far, we have only looked <a id="_idIndexMarker520"/>at <strong class="bold">ahead-of-time</strong> (<strong class="bold">AOT</strong>) compilers. These compilers compile the whole application. The application can only run after the compilation is finished. If the compilation is performed at the runtime of the application, then the compiler is a JIT compiler. A <a id="_idIndexMarker521"/>JIT compiler has<a id="_idIndexMarker522"/> interesting <span class="No-Break">use cases:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Implementation of a virtual machine</strong>: A programming language can be translated to byte code with an <a id="_idIndexMarker523"/>AOT compiler. At runtime, a JIT compiler is used to compile the byte code to machine code. The advantage of this approach is that the byte code is hardware-independent, and thanks to the JIT compiler, there is no performance penalty compared to an AOT compiler. Java and C# use this model today, but this is not a new idea: the USCD Pascal compiler from 1977 already used a <span class="No-Break">similar approach.</span></li>&#13;
				<li><strong class="bold">Expression evaluation</strong>: A <a id="_idIndexMarker524"/>spreadsheet application can compile often-executed expressions with a JIT compiler. For example, this can speed up financial simulations. The <strong class="source-inline">lldb</strong> LLVM debugger uses this approach to evaluate source expressions at <span class="No-Break">debug time.</span></li>&#13;
				<li><strong class="bold">Database queries</strong>: A <a id="_idIndexMarker525"/>database creates an execution plan from a database query. The execution plan describes operations on tables and columns, which leads to a query answer when executed. A JIT compiler can be used to translate the execution plan into machine code, which speeds up the execution of <span class="No-Break">the query.</span></li>&#13;
			</ul>&#13;
			<p>The static compilation model of LLVM is not as far away from the JIT model as one may think. The <strong class="source-inline">llc</strong> LLVM static compiler compiles LLVM IR into machine code and saves the result as an object file on disk. If the object file is not stored on disk but in memory, would the code be executable? Not directly, as references to global functions and global data use relocations instead of absolute addresses. Conceptually, a <strong class="bold">relocation</strong> describes how to calculate the address – for <a id="_idIndexMarker526"/>example, as an offset to a known address. If we resolve relocations into addresses, as the linker and the dynamic loader do, then we can execute the object code. Running the static compiler to compile IR code into an object file in memory, performing a link step on the in-memory object file, and running the code gives us a JIT compiler. The JIT implementation in the LLVM core libraries is based on <span class="No-Break">this idea.</span></p>&#13;
			<p>During the<a id="_idIndexMarker527"/> development history of LLVM, there were several JIT implementations, with different feature sets. The latest JIT API is the <strong class="bold">On-Request Compilation</strong> (<strong class="bold">ORC</strong>) engine. In case<a id="_idIndexMarker528"/> you were curious about the acronym, it was the lead developer’s intention<a id="_idIndexMarker529"/> to invent yet another acronym based on Tolkien’s universe, after <strong class="bold">Executable and Linking Format</strong> (<strong class="bold">ELF</strong>) and <strong class="bold">Debugging Standard</strong> (<strong class="bold">DWARF</strong>) were<a id="_idIndexMarker530"/> <span class="No-Break">already present.</span></p>&#13;
			<p>The ORC engine builds on and extends the idea of using the static compiler and a dynamic linker on the in-memory object file. The implementation uses a layered approach. The two basic levels are the compile layer and the link layer. On top of this sits a layer providing support for lazy compilation. A transformation layer can be stacked on top or below the lazy compilation layer, allowing the developer to add arbitrary transformations or simply to be notified of certain events. Moreover, this layered approach has the advantage that the JIT engine is customizable for diverse requirements. For example, a high-performance virtual machine may choose to compile everything upfront and make no use of the lazy compilation layer. On the other hand, other virtual machines will emphasize startup time and responsiveness to the user and will achieve this with the help of the lazy <span class="No-Break">compilation layer.</span></p>&#13;
			<p>The older MCJIT engine is still available, and its API is derived from an even older, already-removed JIT engine. Over time, this API gradually became bloated, and it lacks the flexibility of the ORC API. The goal is to remove this implementation, as the ORC engine now provides all the functionality of the MCJIT engine, and new developments should use the <a id="_idIndexMarker531"/><span class="No-Break">ORC API.</span></p>&#13;
			<p>In the next section, we look at <strong class="source-inline">lli</strong>, the LLVM interpreter, and the dynamic compiler, before we dive into implementing a <span class="No-Break">JIT compiler.</span></p>&#13;
			<h1 id="_idParaDest-145"><a id="_idTextAnchor149"/>Using JIT compilation for direct execution</h1>&#13;
			<p>Running LLVM IR directly is the first <a id="_idIndexMarker532"/>idea that comes to mind when thinking about a JIT compiler. This is what the <strong class="source-inline">lli</strong> tool, the LLVM interpreter, and the dynamic compiler do. We will explore the <strong class="source-inline">lli</strong> tool in the <span class="No-Break">next section.</span></p>&#13;
			<h2 id="_idParaDest-146"><a id="_idTextAnchor150"/>Exploring the lli tool</h2>&#13;
			<p>Let’s try the <strong class="source-inline">lli</strong> tool <a id="_idIndexMarker533"/>with a very simple example. The following LLVM IR can be stored as a file called <strong class="source-inline">hello.ll</strong>, which is the equivalent of a C hello world application. This file declares a prototype for the <strong class="source-inline">printf()</strong> function from the C library. The <strong class="source-inline">hellostr</strong> constant contains the message to be printed. Inside the <strong class="source-inline">main()</strong> function, a call to the <strong class="source-inline">printf()</strong> function is generated, and this function contains a <strong class="source-inline">hellostr</strong> message that will be printed. The application always <span class="No-Break">returns </span><span class="No-Break"><strong class="source-inline">0</strong></span><span class="No-Break">.</span></p>&#13;
			<p>The complete source code is <span class="No-Break">as follows:</span></p>&#13;
			<pre class="source-code">&#13;
declare i32 @printf(ptr, ...)&#13;
@hellostr = private unnamed_addr constant [13 x i8] c"Hello world\0A\00"&#13;
define dso_local i32 @main(i32 %argc, ptr %argv) {&#13;
  %res = call i32 (ptr, ...) @printf(ptr @hellostr)&#13;
  ret i32 0&#13;
}</pre>			<p>This LLVM IR file is generic enough that it is valid for all platforms. We can directly execute the IR using the <strong class="source-inline">lli</strong> tool with the <span class="No-Break">following command:</span></p>&#13;
			<pre class="console">&#13;
$ lli hello.ll&#13;
Hello world</pre>			<p>The interesting point here is how the <strong class="source-inline">printf()</strong> function is found. The IR code is compiled to machine code, and a lookup for the <strong class="source-inline">printf</strong> symbol is triggered. This symbol is not found in the IR, so the current process is searched for it. The <strong class="source-inline">lli</strong> tool dynamically links against the C library, and the symbol is <span class="No-Break">found there.</span></p>&#13;
			<p>Of course, the <strong class="source-inline">lli</strong> tool does not link against the libraries you created. To enable the use of such functions, the <strong class="source-inline">lli</strong> tool supports the loading of shared libraries and objects. The following C source just prints a <span class="No-Break">friendly message:</span></p>&#13;
			<pre class="source-code">&#13;
#include &lt;stdio.h&gt;&#13;
void greetings() {&#13;
  puts("Hi!");&#13;
}</pre>			<p>Stored in <strong class="source-inline">greetings.c</strong>, we use this to explore loading objects with <strong class="source-inline">lli</strong>. The following command will compile this source into a shared library. The <strong class="source-inline">–fPIC</strong> option instructs <strong class="source-inline">clang</strong> to generate position-independent code, which is required for shared libraries. Moreover, the compiler creates a <strong class="source-inline">greetings.so</strong> shared library with             <strong class="source-inline">–</strong><span class="No-Break"><strong class="source-inline">shared</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="console">&#13;
$ clang greetings.c -fPIC -shared -o greetings.so</pre>			<p>We also compile the file into the <strong class="source-inline">greetings.o</strong> <span class="No-Break">object file:</span></p>&#13;
			<pre class="console">&#13;
$ clang greetings.c -c -o greetings.o</pre>			<p>We now have two<a id="_idIndexMarker534"/> files, the <strong class="source-inline">greetings.so</strong> shared library and the <strong class="source-inline">greetings.o</strong> object file, which we will load into the <span class="No-Break"><strong class="source-inline">lli</strong></span><span class="No-Break"> tool.</span></p>&#13;
			<p>We also need an LLVM IR file that calls the <strong class="source-inline">greetings()</strong> function. For this, create a <strong class="source-inline">main.ll</strong> file that contains a single call to <span class="No-Break">the function:</span></p>&#13;
			<pre class="source-code">&#13;
declare void @greetings(...)&#13;
define dso_local i32 @main(i32 %argc, i8** %argv) {&#13;
  call void (...) @greetings()&#13;
  ret i32 0&#13;
}</pre>			<p>Notice that on executing, the previous IR crashes, as <strong class="source-inline">lli</strong> cannot locate the <span class="No-Break">greetings symbol:</span></p>&#13;
			<pre class="console">&#13;
$ lli main.ll&#13;
JIT session error: Symbols not found: [ _greetings ]&#13;
lli: Failed to materialize symbols: { (main, { _main }) }</pre>			<p>The <strong class="source-inline">greetings()</strong> function is defined in an external file, and to fix the crash, we have to tell the <strong class="source-inline">lli</strong> tool which additional file needs to be loaded. In order to use the shared library, you must use the <strong class="source-inline">–load</strong> option, which takes the path to the shared library as <span class="No-Break">an argument:</span></p>&#13;
			<pre class="console">&#13;
$ lli –load ./greetings.so main.ll&#13;
Hi!</pre>			<p>It is important to specify the path to the shared library if the directory containing the shared library is not in the search path for the dynamic loader. If omitted, then the library will not <span class="No-Break">be found.</span></p>&#13;
			<p>Alternatively, we can<a id="_idIndexMarker535"/> instruct <strong class="source-inline">lli</strong> to load the object file <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">–extra-object</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="console">&#13;
$ lli –extra-object greetings.o main.ll&#13;
Hi!</pre>			<p>Other supported options are <strong class="source-inline">–extra-archive</strong>, which loads an archive, and                <strong class="source-inline">–extra-module</strong>, which loads another bitcode file. Both options require the path to the file as <span class="No-Break">an argument.</span></p>&#13;
			<p>You now know how you can use the <strong class="source-inline">lli</strong> tool to directly execute LLVM IR. In the next section, we will implement our own <span class="No-Break">JIT tool.</span></p>&#13;
			<h1 id="_idParaDest-147"><a id="_idTextAnchor151"/>Implementing our own JIT compiler with LLJIT</h1>&#13;
			<p>The <strong class="source-inline">lli</strong> tool is<a id="_idIndexMarker536"/> nothing more than a thin<a id="_idIndexMarker537"/> wrapper around LLVM APIs. In the first section, we learned that the ORC engine uses a layered approach. The <strong class="source-inline">ExecutionSession</strong> class represents a running JIT program. Besides other items, this class holds information such as used <strong class="source-inline">JITDylib</strong> instances. A <strong class="source-inline">JITDylib</strong> instance is a symbol table that maps symbol names to addresses. For example, these can be symbols defined in an LLVM IR file or the symbols of a loaded <span class="No-Break">shared library.</span></p>&#13;
			<p>For<a id="_idIndexMarker538"/> executing LLVM IR, we do not <a id="_idIndexMarker539"/>need to create a JIT stack on our own, as the <strong class="source-inline">LLJIT</strong> class provides this functionality. You can also make use of this class when migrating from the older MCJIT implementation, as this class essentially provides the <span class="No-Break">same functionality.</span></p>&#13;
			<p>To illustrate the functions of the <strong class="source-inline">LLJIT</strong> utility, we will be creating an interactive calculator application while incorporating JIT functionality. The main source code of our JIT calculator will be extended from the <strong class="source-inline">calc</strong> example from <a href="B19561_02.xhtml#_idTextAnchor037"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">The Structure of </em><span class="No-Break"><em class="italic">a Compiler</em></span><span class="No-Break">.</span></p>&#13;
			<p>The primary idea behind our interactive JIT calculator will be <span class="No-Break">as follows:</span></p>&#13;
			<ol>&#13;
				<li>Allow the user to input a function definition, such as <strong class="source-inline">def f(x) = </strong><span class="No-Break"><strong class="source-inline">x*2</strong></span><span class="No-Break">.</span></li>&#13;
				<li>The function inputted by the user is then compiled by the <strong class="source-inline">LLJIT</strong> utility into a function – in this <span class="No-Break">case, </span><span class="No-Break"><strong class="source-inline">f</strong></span><span class="No-Break">.</span></li>&#13;
				<li>Allow the user to call the function they have defined with a numerical <span class="No-Break">value: </span><span class="No-Break"><strong class="source-inline">f(3)</strong></span><span class="No-Break">.</span></li>&#13;
				<li>Evaluate the function with the provided argument, and print the result to the <span class="No-Break">console: </span><span class="No-Break"><strong class="source-inline">6</strong></span><span class="No-Break">.</span></li>&#13;
			</ol>&#13;
			<p>Before we discuss incorporating JIT functionality into the calculator source code, there are a few main differences to point out with respect to the original <span class="No-Break">calculator example:</span></p>&#13;
			<ul>&#13;
				<li>Firstly, we previously only input and parsed functions beginning with the <strong class="source-inline">with</strong> keyword, rather than the <strong class="source-inline">def</strong> keyword described previously. For this chapter, we instead only accept function definitions beginning with <strong class="source-inline">def</strong>, and this is represented as a particular node in<a id="_idIndexMarker540"/> our <strong class="bold">abstract syntax tree</strong> (<strong class="bold">AST</strong>) class, known as <strong class="source-inline">DefDecl</strong>. The <strong class="source-inline">DefDecl</strong> class is aware of the arguments and their names it is defined with, and the function name is also stored within <span class="No-Break">this class.</span></li>&#13;
				<li>Secondly, we also need our AST to be aware of function calls, to represent the functions that the <strong class="source-inline">LLJIT</strong> utility has consumed or JIT’ted. Whenever a user inputs the name of a function, followed by arguments enclosed in parentheses, the AST recognizes these as <strong class="source-inline">FuncCallFromDef</strong> nodes. This class essentially is aware of the same information as the <span class="No-Break"><strong class="source-inline">DefDecl</strong></span><span class="No-Break"> class.</span></li>&#13;
			</ul>&#13;
			<p>Due to the <a id="_idIndexMarker541"/>addition of these two AST classes, it <a id="_idIndexMarker542"/>is obvious to expect that the semantic analysis, parser, and code generation classes will be adapted accordingly to handle the changes in our AST. One additional thing to note is the addition of a new data structure, called <strong class="source-inline">JITtedFunctions</strong>, which all these classes are aware of. This data structure is a map with the defined function names as keys, and the number of arguments a function is defined with is stored as values within the map. We will see later how this data structure will be utilized in our <span class="No-Break">JIT calculator.</span></p>&#13;
			<p>For more details on the changes we have made to the <strong class="source-inline">calc</strong> example, the full source containing the changes from <strong class="source-inline">calc</strong> and this section’s JIT implementation can be found within the <strong class="source-inline">lljit</strong> <span class="No-Break">source directory.</span></p>&#13;
			<h2 id="_idParaDest-148"><a id="_idTextAnchor152"/>Integrating the LLJIT engine into the calculator</h2>&#13;
			<p>Firstly, let’s discuss<a id="_idIndexMarker543"/> how to set up the JIT engine in our interactive calculator. All of the implementation pertaining to the JIT engine exists within <strong class="source-inline">Calc.cpp</strong>, and this file has one <strong class="source-inline">main()</strong> loop for the execution of <span class="No-Break">the program:</span></p>&#13;
			<ol>&#13;
				<li>We must include several header files, aside from the headers including our code generation, semantic analyzer, and parser implementation. The <strong class="source-inline">LLJIT.h</strong> header defines the <strong class="source-inline">LLJIT</strong> class and the core classes of the ORC API. Next, the <strong class="source-inline">InitLLVM.h</strong> header is needed for the basic initialization of the tool, and the <strong class="source-inline">TargetSelect.h</strong> header is needed for the initialization of the native target. Finally, we also include the <strong class="source-inline">&lt;iostream&gt;</strong> C++ header to allow for user input into our <span class="No-Break">calculator application:</span><pre class="source-code">&#13;
#include "CodeGen.h"&#13;
#include "Parser.h"&#13;
#include "Sema.h"&#13;
#include "llvm/ExecutionEngine/Orc/LLJIT.h"&#13;
#include "llvm/Support/InitLLVM.h"&#13;
#include "llvm/Support/TargetSelect.h"&#13;
#include &lt;iostream&gt;</pre></li>				<li>Next, we <a id="_idIndexMarker544"/>add the <strong class="source-inline">llvm</strong> and <strong class="source-inline">llvm::orc</strong> namespaces to the <span class="No-Break">current scope:</span><pre class="source-code">&#13;
using namespace llvm;&#13;
using namespace llvm::orc;</pre></li>				<li>Many of the calls from our <strong class="source-inline">LLJIT</strong> instance that we will be creating return an error type, <strong class="source-inline">Error</strong>. The <strong class="source-inline">ExitOnError</strong> class allows us to discard <strong class="source-inline">Error</strong> values that are returned by the calls from the <strong class="source-inline">LLJIT</strong> instance while logging to <strong class="source-inline">stderr</strong> and exiting the application. We declare a global <strong class="source-inline">ExitOnError</strong> variable <span class="No-Break">as follows:</span><pre class="source-code">&#13;
ExitOnError ExitOnErr;</pre></li>				<li>Then, we add the <strong class="source-inline">main()</strong> function, which initializes the tool and the <span class="No-Break">native target:</span><pre class="source-code">&#13;
int main(int argc, const char **argv{&#13;
  InitLLVM X(argc, argv);&#13;
  InitializeNativeTarget();&#13;
  InitializeNativeTargetAsmPrinter();&#13;
  InitializeNativeTargetAsmParser();</pre></li>				<li>We use the <strong class="source-inline">LLJITBuilder</strong> class to create an <strong class="source-inline">LLJIT</strong> instance, wrapped in the previously declared <strong class="source-inline">ExitOnErr</strong> variable in case an error occurs. A possible source of error would be that the platform does not yet support <span class="No-Break">JIT compilation:</span><pre class="source-code">&#13;
auto JIT = ExitOnErr(LLJITBuilder().create());</pre></li>				<li>Next, we declare our <strong class="source-inline">JITtedFunctions</strong> map that keeps track of the function definitions, as we have <span class="No-Break">previously described:</span><pre class="source-code">&#13;
StringMap&lt;size_t&gt; JITtedFunctions;</pre></li>				<li>To facilitate an environment that waits for user input, we add a <strong class="source-inline">while()</strong> loop and allow the user to type in an expression, saving the line that the user typed within a string <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">calcExp</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
  while (true) {&#13;
    outs() &lt;&lt; "JIT calc &gt; ";&#13;
    std::string calcExp;&#13;
    std::getline(std::cin, calcExp);</pre></li>				<li>Afterward, the <a id="_idIndexMarker545"/>LLVM context class is initialized, along with a new LLVM module. The module’s data layout is also set accordingly, and we also declare a code generator, which will be used to generate IR for the function that the user has defined on the <span class="No-Break">command line:</span><pre class="source-code">&#13;
    std::unique_ptr&lt;LLVMContext&gt; Ctx = std::make_unique&lt;LLVMContext&gt;();&#13;
    std::unique_ptr&lt;Module&gt; M = std::make_unique&lt;Module&gt;("JIT calc.expr", *Ctx);&#13;
    M-&gt;setDataLayout(JIT-&gt;getDataLayout());&#13;
    CodeGen CodeGenerator;</pre></li>				<li>We must interpret the line that was entered by the user to determine if the user is defining a new function or calling a previous function that they have defined with an argument. A <strong class="source-inline">Lexer</strong> class is defined while taking in the line of input that the user has given. We will see that there are two main cases that the lexer <span class="No-Break">cares about:</span><pre class="source-code">&#13;
    Lexer Lex(calcExp);&#13;
    Token::TokenKind CalcTok = Lex.peek();</pre></li>				<li>The lexer can<a id="_idIndexMarker546"/> check the first token of the user input. If the user is defining a new function (represented by the <strong class="source-inline">def</strong> keyword, or the <strong class="source-inline">Token::KW_def</strong> token), then we parse it and check its semantics. If the parser or the semantic analyzer detects any issues with the user-defined function, errors will be emitted accordingly, and the calculator program will halt. If no errors are detected from either the parser or the semantic analyzer, this means we have a valid AST data <span class="No-Break">structure, </span><span class="No-Break"><strong class="source-inline">DefDecl</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
   if (CalcTok == Token::KW_def) {&#13;
      Parser Parser(Lex);&#13;
      AST *Tree = Parser.parse();&#13;
      if (!Tree || Parser.hasError()) {&#13;
        llvm::errs() &lt;&lt; "Syntax errors occured\n";&#13;
        return 1;&#13;
      }&#13;
      Sema Semantic;&#13;
      if (Semantic.semantic(Tree, JITtedFunctions)) {&#13;
        llvm::errs() &lt;&lt; "Semantic errors occured\n";&#13;
        return 1;&#13;
      }</pre></li>				<li>We then can pass our newly constructed AST into our code generator to compile the IR for the function that the user has defined. The specifics of IR generation will be discussed afterward, but this function that compiles to the IR needs to be aware of the module and our <strong class="source-inline">JITtedFunctions</strong> map. After generating the IR, we can add this information to our <strong class="source-inline">LLJIT</strong> instance by calling <strong class="source-inline">addIRModule()</strong> and wrapping our module and context in a <strong class="source-inline">ThreadSafeModule</strong> class, to prevent these from being accessed by other <span class="No-Break">concurrent threads:</span><pre class="source-code">&#13;
      CodeGenerator.compileToIR(Tree, M.get(), JITtedFunctions);&#13;
      ExitOnErr(&#13;
          JIT-&gt;addIRModule(ThreadSafeModule(std::move(M),           std::move(Ctx))));</pre></li>				<li>Instead, if the <a id="_idIndexMarker547"/>user is calling a function with parameters, which is represented by the <strong class="source-inline">Token::ident</strong> token, we also need to parse and semantically check if the user input is valid prior to converting the input into a valid AST. The parsing and checking here are slightly different compared to before, as it can include checks such as ensuring the number of parameters that the user has supplied to the function call matches the number of parameters that the function was originally <span class="No-Break">defined with:</span><pre class="source-code">&#13;
   } else if (CalcTok == Token::ident) {&#13;
      outs() &lt;&lt; "Attempting to evaluate expression:\n";&#13;
      Parser Parser(Lex);&#13;
      AST *Tree = Parser.parse();&#13;
      if (!Tree || Parser.hasError()) {&#13;
        llvm::errs() &lt;&lt; "Syntax errors occured\n";&#13;
        return 1;&#13;
      }&#13;
      Sema Semantic;&#13;
      if (Semantic.semantic(Tree, JITtedFunctions)) {&#13;
        llvm::errs() &lt;&lt; "Semantic errors occured\n";&#13;
        return 1;&#13;
      }</pre></li>				<li>Once a valid AST is constructed for a function call, <strong class="source-inline">FuncCallFromDef</strong>, we get the name of the function from the AST, and then the code generator prepares to generate the call to the function that was previously added to the <strong class="source-inline">LLJIT</strong> instance. What occurs under the cover is that the user-defined function is regenerated as an LLVM call within a separate function that will be created that does the actual evaluation of the original function. This step requires the AST, the module, the function call name, and our map of <span class="No-Break">function definitions:</span><pre class="source-code">&#13;
      llvm::StringRef FuncCallName = Tree-&gt;getFnName();&#13;
      CodeGenerator.prepareCalculationCallFunc(Tree, M.get(),       FuncCallName, JITtedFunctions);</pre></li>				<li>After the<a id="_idIndexMarker548"/> code generator has completed its work to regenerate the original function and to create a separate evaluation function, we must add this information to the <strong class="source-inline">LLJIT</strong> instance. We create a <strong class="source-inline">ResourceTracker</strong> instance to track the memory that is allocated to the functions that have been added to <strong class="source-inline">LLJIT</strong>, as well as another <strong class="source-inline">ThreadSafeModule</strong> instance of the module and context. These two instances are then added to the JIT as an <span class="No-Break">IR module:</span><pre class="source-code">&#13;
      auto RT = JIT-&gt;getMainJITDylib().createResourceTracker();&#13;
      auto TSM = ThreadSafeModule(std::move(M), std::move(Ctx));&#13;
      ExitOnErr(JIT-&gt;addIRModule(RT, std::move(TSM)));</pre></li>				<li>The separate evaluation function is then queried for within our <strong class="source-inline">LLJIT</strong> instance through the <strong class="source-inline">lookup()</strong> method, by supplying the name of our evaluation function, <strong class="source-inline">calc_expr_func</strong>, into the function. If the query is successful, the address for the <strong class="source-inline">calc_expr_func</strong> function is cast to the appropriate type, which is a function that takes no arguments and returns a single integer. Once the function’s address is acquired, we call the function to generate the result of the user-defined function with the parameters they have supplied and then print the result to <span class="No-Break">the console:</span><pre class="source-code">&#13;
      auto CalcExprCall = ExitOnErr(JIT-&gt;lookup("calc_expr_func"));&#13;
      int (*UserFnCall)() = CalcExprCall.toPtr&lt;int (*)()&gt;();&#13;
      outs() &lt;&lt; "User defined function evaluated to:       " &lt;&lt; UserFnCall() &lt;&lt; "\n";</pre></li>				<li>After the<a id="_idIndexMarker549"/> function call is completed, the memory that was previously associated with our functions is then freed <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">ResourceTracker</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
ExitOnErr(RT-&gt;remove());</pre></li>			</ol>&#13;
			<h2 id="_idParaDest-149"><a id="_idTextAnchor153"/>Code generation changes to support JIT compilation via LLJIT</h2>&#13;
			<p>Now, let’s take a brief look at some of the <a id="_idIndexMarker550"/>changes we have made within <strong class="source-inline">CodeGen.cpp</strong> to support our <span class="No-Break">JIT-based calculator:</span></p>&#13;
			<ol>&#13;
				<li>As previously mentioned, the code generation class has two important methods: one to compile the user-defined function into LLVM IR and print the IR to the console, and another to prepare the calculation evaluation function, <strong class="source-inline">calc_expr_func</strong>, which contains a call to the original user-defined function for evaluation. This second function also prints the resulting IR to <span class="No-Break">the user:</span><pre class="source-code">&#13;
void CodeGen::compileToIR(AST *Tree, Module *M,&#13;
                    StringMap&lt;size_t&gt; &amp;JITtedFunctions) {&#13;
  ToIRVisitor ToIR(M, JITtedFunctions);&#13;
  ToIR.run(Tree);&#13;
  M-&gt;print(outs(), nullptr);&#13;
}&#13;
void CodeGen::prepareCalculationCallFunc(AST *FuncCall,&#13;
           Module *M, llvm::StringRef FnName,&#13;
           StringMap&lt;size_t&gt; &amp;JITtedFunctions) {&#13;
  ToIRVisitor ToIR(M, JITtedFunctions);&#13;
  ToIR.genFuncEvaluationCall(FuncCall);&#13;
  M-&gt;print(outs(), nullptr);&#13;
}</pre></li>				<li>As noted in the<a id="_idIndexMarker551"/> preceding source, these code generation functions define a <strong class="source-inline">ToIRVisitor</strong> instance that takes in our module and a <strong class="source-inline">JITtedFunctions</strong> map to be used in its constructor <span class="No-Break">upon initialization:</span><pre class="source-code">&#13;
class ToIRVisitor : public ASTVisitor {&#13;
  Module *M;&#13;
  IRBuilder&lt;&gt; Builder;&#13;
  StringMap&lt;size_t&gt; &amp;JITtedFunctionsMap;&#13;
. . .&#13;
public:&#13;
  ToIRVisitor(Module *M,&#13;
              StringMap&lt;size_t&gt; &amp;JITtedFunctions)&#13;
      : M(M), Builder(M-&gt;getContext()),       JITtedFunctionsMap(JITtedFunctions) {</pre></li>				<li>Ultimately, this information is used to either generate IR or evaluate the function that the IR was previously generated for. When generating the IR, the code generator expects to see a <strong class="source-inline">DefDecl</strong> node, which represents defining a new function. The function name, along with the number of arguments it is defined with, is stored within the function <span class="No-Break">definitions map:</span><pre class="source-code">&#13;
virtual void visit(DefDecl &amp;Node) override {&#13;
    llvm::StringRef FnName = Node.getFnName();&#13;
    llvm::SmallVector&lt;llvm::StringRef, 8&gt; FunctionVars =     Node.getVars();&#13;
    (JITtedFunctionsMap)[FnName] = FunctionVars.size();</pre></li>				<li>Afterward, the actual function definition is created by the <span class="No-Break"><strong class="source-inline">genUserDefinedFunction()</strong></span><span class="No-Break"> call:</span><pre class="source-code">&#13;
    Function *DefFunc = genUserDefinedFunction(FnName);</pre></li>				<li>Within <strong class="source-inline">genUserDefinedFunction()</strong>, the first step is to check if the function exists within the module. If it does not, we ensure that the function prototype exists within our map data structure. Then, we use the name and the number of <a id="_idIndexMarker552"/>arguments to construct a function that has the number of arguments that were defined by the user, and make the function return a single <span class="No-Break">integer value:</span><pre class="source-code">&#13;
Function *genUserDefinedFunction(llvm::StringRef Name) {&#13;
    if (Function *F = M-&gt;getFunction(Name))&#13;
      return F;&#13;
    Function *UserDefinedFunction = nullptr;&#13;
    auto FnNameToArgCount = JITtedFunctionsMap.find(Name);&#13;
    if (FnNameToArgCount != JITtedFunctionsMap.end()) {&#13;
      std::vector&lt;Type *&gt; IntArgs(FnNameToArgCount-&gt;second,       Int32Ty);&#13;
      FunctionType *FuncType = FunctionType::get(Int32Ty,       IntArgs, false);&#13;
      UserDefinedFunction =&#13;
          Function::Create(FuncType,           GlobalValue::ExternalLinkage, Name, M);&#13;
    }&#13;
    return UserDefinedFunction;&#13;
  }</pre></li>				<li>After <a id="_idIndexMarker553"/>generating the user-defined function, a new basic block is created, and we insert our function into the basic block. Each function argument is also associated with a name that is defined by the user, so we also set the names for all function arguments accordingly, as well as generate mathematical operations that operate on the arguments within <span class="No-Break">the function:</span><pre class="source-code">&#13;
    BasicBlock *BB = BasicBlock::Create(M-&gt;getContext(),     "entry", DefFunc);&#13;
    Builder.SetInsertPoint(BB);&#13;
    unsigned FIdx = 0;&#13;
    for (auto &amp;FArg : DefFunc-&gt;args()) {&#13;
      nameMap[FunctionVars[FIdx]] = &amp;FArg;&#13;
      FArg.setName(FunctionVars[FIdx++]);&#13;
    }&#13;
    Node.getExpr()-&gt;accept(*this);&#13;
  };</pre></li>				<li>When evaluating the user-defined function, the AST that is expected in our example is called a <strong class="source-inline">FuncCallFromDef</strong> node. First, we define the evaluation function and name it <strong class="source-inline">calc_expr_func</strong> (taking in zero arguments and returning <span class="No-Break">one result):</span><pre class="source-code">&#13;
  virtual void visit(FuncCallFromDef &amp;Node) override {&#13;
    llvm::StringRef CalcExprFunName = "calc_expr_func";&#13;
    FunctionType *CalcExprFunTy = FunctionType::get(Int32Ty, {},     false);&#13;
    Function *CalcExprFun = Function::Create(&#13;
        CalcExprFunTy, GlobalValue::ExternalLinkage,         CalcExprFunName, M);</pre></li>				<li>Next, we <a id="_idIndexMarker554"/>create a new basic block to insert <span class="No-Break"><strong class="source-inline">calc_expr_func</strong></span><span class="No-Break"> into:</span><pre class="source-code">&#13;
    BasicBlock *BB = BasicBlock::Create(M-&gt;getContext(),     "entry", CalcExprFun);&#13;
    Builder.SetInsertPoint(BB);</pre></li>				<li>Similar to before, the user-defined function is retrieved by <strong class="source-inline">genUserDefinedFunction()</strong>, and we pass the numerical parameters of the function call into the original function that we have <span class="No-Break">just regenerated:</span><pre class="source-code">&#13;
    llvm::StringRef CalleeFnName = Node.getFnName();&#13;
    Function *CalleeFn = genUserDefinedFunction(CalleeFnName);</pre></li>				<li>Once we have the actual <strong class="source-inline">llvm::Function</strong> instance available, we utilize <strong class="source-inline">IRBuilder</strong> to create a call to the defined function and also return the result so that it is accessible when the result is printed to the user in <span class="No-Break">the end:</span><pre class="source-code">&#13;
    auto CalleeFnVars = Node.getArgs();&#13;
    llvm::SmallVector&lt;Value *&gt; IntParams;&#13;
    for (unsigned i = 0, end = CalleeFnVars.size(); i != end;     ++i) {&#13;
      int ArgsToIntType;&#13;
      CalleeFnVars[i].getAsInteger(10, ArgsToIntType);&#13;
      Value *IntParam = ConstantInt::get(Int32Ty, ArgsToIntType,       true);&#13;
      IntParams.push_back(IntParam);&#13;
    }&#13;
    Builder.CreateRet(Builder.CreateCall(CalleeFn, IntParams,     "calc_expr_res"));&#13;
  };</pre></li>			</ol>&#13;
			<h2 id="_idParaDest-150"><a id="_idTextAnchor154"/>Building an LLJIT-based calculator</h2>&#13;
			<p>Finally, to compile our JIT <a id="_idIndexMarker555"/>calculator source, we also need to create a <strong class="source-inline">CMakeLists.txt</strong> file with the build description, saved beside <strong class="source-inline">Calc.cpp</strong> and our other <span class="No-Break">source files:</span></p>&#13;
			<ol>&#13;
				<li>We set the minimal required CMake version to the number required by LLVM and give the project <span class="No-Break">a name:</span><pre class="source-code">&#13;
cmake_minimum_required (VERSION 3.20.0)&#13;
project ("jit")</pre></li>				<li>The LLVM package needs to be loaded, and we add the directory of the CMake modules provided by LLVM to the search path. Then, we include the <strong class="source-inline">DetermineGCCCompatible</strong> and <strong class="source-inline">ChooseMSVCCRT</strong> modules, which check if the compiler has GCC-compatible command-line syntax and ensure that the same C runtime is used as by <span class="No-Break">LLVM, respectively:</span><pre class="source-code">&#13;
find_package(LLVM REQUIRED CONFIG)&#13;
list(APPEND CMAKE_MODULE_PATH ${LLVM_DIR})&#13;
include(DetermineGCCCompatible)&#13;
include(ChooseMSVCCRT)</pre></li>				<li>We also need to add definitions and the <strong class="source-inline">include</strong> path from LLVM. The used LLVM components are mapped to the library names with a <span class="No-Break">function call:</span><pre class="source-code">&#13;
add_definitions(${LLVM_DEFINITIONS})&#13;
include_directories(SYSTEM ${LLVM_INCLUDE_DIRS})&#13;
llvm_map_components_to_libnames(llvm_libs Core OrcJIT&#13;
                                          Support native)</pre></li>				<li>Afterward, if it is<a id="_idIndexMarker556"/> determined that the compiler has GCC-compatible command-line syntax, we also check if runtime type information and exception handling are enabled. If they are not enabled, C++ flags to turn off these features are added to our <span class="No-Break">compilation accordingly:</span><pre class="source-code">&#13;
if(LLVM_COMPILER_IS_GCC_COMPATIBLE)&#13;
  if(NOT LLVM_ENABLE_RTTI)&#13;
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fno-rtti")&#13;
  endif()&#13;
  if(NOT LLVM_ENABLE_EH)&#13;
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fno-exceptions")&#13;
  endif()&#13;
endif()</pre></li>				<li>Lastly, we define the name of the executable, the source files to compile, and the library to <span class="No-Break">link against:</span><pre class="source-code">&#13;
add_executable (calc&#13;
  Calc.cpp CodeGen.cpp Lexer.cpp Parser.cpp Sema.cpp)&#13;
target_link_libraries(calc PRIVATE ${llvm_libs})</pre></li>			</ol>&#13;
			<p>The preceding steps are all that is required for our JIT-based interactive calculator tool. Next, create and change into a build directory, and then run the following command to create and compile <span class="No-Break">the application:</span></p>&#13;
			<pre class="console">&#13;
$ cmake –G Ninja &lt;path to source directory&gt;&#13;
$ ninja</pre>			<p>This compiles the <strong class="source-inline">calc</strong> tool. We<a id="_idIndexMarker557"/> can then launch the calculator, start defining functions, and see how our calculator is able to evaluate the functions that <span class="No-Break">we define.</span></p>&#13;
			<p>The following example invocations show the IR of the function that is first defined, and then the <strong class="source-inline">calc_expr_func</strong> function that is created to generate a call to our originally defined function in order to evaluate the function with whichever parameter passed <span class="No-Break">into it:</span></p>&#13;
			<pre class="console">&#13;
$ ./calc&#13;
JIT calc &gt; def f(x) = x*2&#13;
define i32 @f(i32 %x) {&#13;
entry:&#13;
  %0 = mul nsw i32 %x, 2&#13;
  ret i32 %0&#13;
}&#13;
JIT calc &gt; f(20)&#13;
Attempting to evaluate expression:&#13;
define i32 @calc_expr_func() {&#13;
entry:&#13;
  %calc_expr_res = call i32 @f(i32 20)&#13;
  ret i32 %calc_expr_res&#13;
}&#13;
declare i32 @f(i32)&#13;
User defined function evaluated to: 40&#13;
JIT calc &gt; def g(x,y) = x*y+100&#13;
define i32 @g(i32 %x, i32 %y) {&#13;
entry:&#13;
  %0 = mul nsw i32 %x, %y&#13;
  %1 = add nsw i32 %0, 100&#13;
  ret i32 %1&#13;
}&#13;
JIT calc &gt; g(8,9)&#13;
Attempting to evaluate expression:&#13;
define i32 @calc_expr_func() {&#13;
entry:&#13;
  %calc_expr_res = call i32 @g(i32 8, i32 9)&#13;
  ret i32 %calc_expr_res&#13;
}&#13;
declare i32 @g(i32, i32)&#13;
User defined function evaluated to: 172</pre>			<p>That’s it! We have just created a JIT-based <span class="No-Break">calculator application!</span></p>&#13;
			<p>As our JIT calculator <a id="_idIndexMarker558"/>is meant to be a simple example that describes how to incorporate <strong class="source-inline">LLJIT</strong> into our projects, it is worth noting that there are <span class="No-Break">some limitations:</span></p>&#13;
			<ul>&#13;
				<li>This calculator does not accept negatives of <span class="No-Break">decimal values</span></li>&#13;
				<li>We cannot redefine the same function more <span class="No-Break">than once</span></li>&#13;
			</ul>&#13;
			<p>For the second limitation, this<a id="_idIndexMarker559"/> occurs by design, and so is expected and enforced by the ORC <span class="No-Break">API itself:</span></p>&#13;
			<pre class="console">&#13;
$ ./calc&#13;
JIT calc &gt; def f(x) = x*2&#13;
define i32 @f(i32 %x) {&#13;
entry:&#13;
  %0 = mul nsw i32 %x, 2&#13;
  ret i32 %0&#13;
}&#13;
JIT calc &gt; def f(x,y) = x+y&#13;
define i32 @f(i32 %x, i32 %y) {&#13;
entry:&#13;
  %0 = add nsw i32 %x, %y&#13;
  ret i32 %0&#13;
}&#13;
Duplicate definition of symbol '_f'</pre>			<p>Keep in mind that there are numerous other possibilities to expose names, besides exposing the symbols for the current process or from a shared library. For example, the <strong class="source-inline">StaticLibraryDefinitionGenerator</strong> class exposes the symbols found in a static archive and can be used in the <span class="No-Break"><strong class="source-inline">DynamicLibrarySearchGenerator</strong></span><span class="No-Break"> class.</span></p>&#13;
			<p>Furthermore, the <strong class="source-inline">LLJIT</strong> class has also an <strong class="source-inline">addObjectFile()</strong> method to expose the symbols of an object file. You can also provide your own <strong class="source-inline">DefinitionGenerator</strong> implementation if the <a id="_idIndexMarker560"/>existing implementations do not fit <span class="No-Break">your needs.</span></p>&#13;
			<p>As we can see, using the <a id="_idIndexMarker561"/>predefined <strong class="source-inline">LLJIT</strong> class is convenient, but it can limit our flexibility. In the next section, we’ll look at how to implement a JIT compiler using the layers provided by the <span class="No-Break">ORC API.</span></p>&#13;
			<h1 id="_idParaDest-151"><a id="_idTextAnchor155"/>Building a JIT compiler class from scratch</h1>&#13;
			<p>Using the layered <a id="_idIndexMarker562"/>approach of ORC, it is very easy to build a JIT compiler customized for the requirements. There is no one-size-fits-all JIT compiler, and the first section of this chapter gave some examples. Let’s have a look at how to set up a JIT compiler <span class="No-Break">from scratch.</span></p>&#13;
			<p>The ORC API uses layers that are stacked together. The lowest level is the object-linking layer, represented by the <strong class="source-inline">llvm::orc::RTDyldObjectLinkingLayer</strong> class. It is responsible for linking in-memory objects and turning them into executable code. The memory required for this task is managed by an instance of the <strong class="source-inline">MemoryManager</strong> interface. There is a default implementation, but we can also use a custom version if <span class="No-Break">we need.</span></p>&#13;
			<p>Above the object-linking layer is the compile layer, which is responsible for creating an in-memory object file. The <strong class="source-inline">llvm::orc::IRCompileLayer</strong> class takes an IR module as input and compiles it to an object file. The <strong class="source-inline">IRCompileLayer</strong> class is a subclass of the <strong class="source-inline">IRLayer</strong> class, which is a generic class for layer implementations accepting <span class="No-Break">LLVM IR.</span></p>&#13;
			<p>Both of these layers already form the core of a JIT compiler: they add an LLVM IR module as input, which is compiled and linked in memory. To add extra functionality, we can incorporate more layers on top of <span class="No-Break">both layers.</span></p>&#13;
			<p>For example, the <strong class="source-inline">CompileOnDemandLayer</strong> class splits a module so that only the requested functions are compiled. This can be used to implement lazy compilation. Moreover, the <strong class="source-inline">CompileOnDemandLayer</strong> class is also a subclass of the <strong class="source-inline">IRLayer</strong> class. In a very generic way, the <strong class="source-inline">IRTransformLayer</strong> class, also a subclass of the <strong class="source-inline">IRLayer</strong> class, allows us to apply a transformation to <span class="No-Break">the module.</span></p>&#13;
			<p>Another important class is the <strong class="source-inline">ExecutionSession</strong> class. This class represents a running JIT program. Essentially, this means that the class manages <strong class="source-inline">JITDylib</strong> symbol tables, provides lookup functionality for symbols, and keeps track of used <span class="No-Break">resource managers.</span></p>&#13;
			<p>The generic recipe for a JIT compiler is <span class="No-Break">as follows:</span></p>&#13;
			<ol>&#13;
				<li>Initialize an instance of the <span class="No-Break"><strong class="source-inline">ExecutionSession</strong></span><span class="No-Break"> class.</span></li>&#13;
				<li>Initialize the layer, at least consisting of an <strong class="source-inline">RTDyldObjectLinkingLayer</strong> class and an <span class="No-Break"><strong class="source-inline">IRCompileLayer</strong></span><span class="No-Break"> class.</span></li>&#13;
				<li>Create the first <strong class="source-inline">JITDylib</strong> symbol table, usually with <strong class="source-inline">main</strong> or a <span class="No-Break">similar name.</span></li>&#13;
			</ol>&#13;
			<p>The general usage of the JIT compiler is also <span class="No-Break">very straightforward:</span></p>&#13;
			<ol>&#13;
				<li>Add an IR module to the <span class="No-Break">symbol table.</span></li>&#13;
				<li>Look up a symbol, triggering the compilation of the associated function, and possibly the <span class="No-Break">whole module.</span></li>&#13;
				<li>Execute <a id="_idIndexMarker563"/><span class="No-Break">the function.</span></li>&#13;
			</ol>&#13;
			<p>In the next subsection, we implement a JIT compiler class following the <span class="No-Break">generic recipe.</span></p>&#13;
			<h2 id="_idParaDest-152"><a id="_idTextAnchor156"/>Creating a JIT compiler class</h2>&#13;
			<p>To keep the implementation of the<a id="_idIndexMarker564"/> JIT compiler class simple, everything is placed in <strong class="source-inline">JIT.h</strong>, within a source directory we can create called <strong class="source-inline">jit</strong>. However, the initialization of the class is a bit more complex compared to using <strong class="source-inline">LLJIT</strong>. Due to handling possible errors, we need a factory method to create some objects upfront before we can call the constructor. The steps to create the class are <span class="No-Break">as follows:</span></p>&#13;
			<ol>&#13;
				<li>We begin with guarding the header file against multiple inclusion with the <strong class="source-inline">JIT_H</strong> <span class="No-Break">preprocessor definition:</span><pre class="source-code">&#13;
#ifndef JIT_H&#13;
#define JIT_H</pre></li>				<li>Firstly, a number of <strong class="source-inline">include</strong> files are required. Most of them provide a class with the same name as the header file. The <strong class="source-inline">Core.h</strong> header provides a couple of basic classes, including the <strong class="source-inline">ExecutionSession</strong> class. Additionally, the <strong class="source-inline">ExecutionUtils.h</strong> header provides the <strong class="source-inline">DynamicLibrarySearchGenerator</strong> class to search libraries for symbols. Furthermore, the <strong class="source-inline">CompileUtils.h</strong> header provides the <span class="No-Break"><strong class="source-inline">ConcurrentIRCompiler</strong></span><span class="No-Break"> class:</span><pre class="source-code">&#13;
#include "llvm/Analysis/AliasAnalysis.h"&#13;
#include "llvm/ExecutionEngine/JITSymbol.h"&#13;
#include "llvm/ExecutionEngine/Orc/CompileUtils.h"&#13;
#include "llvm/ExecutionEngine/Orc/Core.h"&#13;
#include "llvm/ExecutionEngine/Orc/ExecutionUtils.h"&#13;
#include "llvm/ExecutionEngine/Orc/IRCompileLayer.h"&#13;
#include "llvm/ExecutionEngine/Orc/IRTransformLayer.h"&#13;
#include&#13;
     "llvm/ExecutionEngine/Orc/JITTargetMachineBuilder.h"&#13;
#include "llvm/ExecutionEngine/Orc/Mangling.h"&#13;
#include&#13;
    "llvm/ExecutionEngine/Orc/RTDyldObjectLinkingLayer.h"&#13;
#include&#13;
        "llvm/ExecutionEngine/Orc/TargetProcessControl.h"&#13;
#include "llvm/ExecutionEngine/SectionMemoryManager.h"&#13;
#include "llvm/Passes/PassBuilder.h"&#13;
#include "llvm/Support/Error.h"</pre></li>				<li>Declare a new<a id="_idIndexMarker565"/> class. Our new class will be <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">JIT</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
class JIT {</pre></li>				<li>The private data members reflect the ORC layers and some helper classes. The <strong class="source-inline">ExecutionSession</strong>, <strong class="source-inline">ObjectLinkingLayer</strong>, <strong class="source-inline">CompileLayer</strong>, <strong class="source-inline">OptIRLayer</strong>, and <strong class="source-inline">MainJITDylib</strong> instances represent the running JIT program, the layers, and the symbol table, as already described. Moreover, the <strong class="source-inline">TargetProcessControl</strong> instance is used for interaction with the JIT target process. This can be the same process, another process on the same machine, or a remote process on a different machine, possibly with a different architecture. The <strong class="source-inline">DataLayout</strong> and <strong class="source-inline">MangleAndInterner</strong> classes are required to mangle symbols’ names in the correct way. Additionally, the symbol names are internalized, which means that all equal names have the same address. This means that to check if two symbol names are equal, it is then sufficient to<a id="_idIndexMarker566"/> compare the addresses, which is a very <span class="No-Break">fast operation:</span><pre class="source-code">&#13;
  std::unique_ptr&lt;llvm::orc::TargetProcessControl&gt; TPC;&#13;
  std::unique_ptr&lt;llvm::orc::ExecutionSession&gt; ES;&#13;
  llvm::DataLayout DL;&#13;
  llvm::orc::MangleAndInterner Mangle;&#13;
  std::unique_ptr&lt;llvm::orc::RTDyldObjectLinkingLayer&gt;&#13;
      ObjectLinkingLayer;&#13;
  std::unique_ptr&lt;llvm::orc::IRCompileLayer&gt;&#13;
      CompileLayer;&#13;
  std::unique_ptr&lt;llvm::orc::IRTransformLayer&gt;&#13;
      OptIRLayer;&#13;
  llvm::orc::JITDylib &amp;MainJITDylib;</pre></li>				<li>The initialization is split into three parts. In C++, a constructor cannot return an error. The simple and recommended solution is to create a static factory method that can do the error handling before constructing an object. The initialization of the layers is more complex, so we introduce factory methods for <span class="No-Break">them, too.</span><p class="list-inset">In the <strong class="source-inline">create()</strong> factory method, we first create a <strong class="source-inline">SymbolStringPool</strong> instance, which is used to implement string internalization and is shared by several classes. To take control of the current process, we create a <strong class="source-inline">SelfTargetProcessControl</strong> instance. If we want to target a different process, then we need to change <span class="No-Break">this instance.</span></p><p class="list-inset">Next, we <a id="_idIndexMarker567"/>construct a <strong class="source-inline">JITTargetMachineBuilder</strong> instance, for which we need to know the target triple of the JIT process. Afterward, we query the target machine builder for the data layout. This step can fail if the builder is not able to instantiate the target machine based on the provided triple – for example, because support for this target is not compiled into the <span class="No-Break">LLVM libraries:</span></p><pre class="source-code">&#13;
public:&#13;
  static llvm::Expected&lt;std::unique_ptr&lt;JIT&gt;&gt; create() {&#13;
    auto SSP =&#13;
        std::make_shared&lt;llvm::orc::SymbolStringPool&gt;();&#13;
    auto TPC =&#13;
        llvm::orc::SelfTargetProcessControl::Create(SSP);&#13;
    if (!TPC)&#13;
      return TPC.takeError();&#13;
    llvm::orc::JITTargetMachineBuilder JTMB(&#13;
        (*TPC)-&gt;getTargetTriple());&#13;
    auto DL = JTMB.getDefaultDataLayoutForTarget();&#13;
    if (!DL)&#13;
      return DL.takeError();</pre></li>				<li>At this point, we have handled all calls that could potentially fail. We are now able to initialize the <strong class="source-inline">ExecutionSession</strong> instance. Finally, the constructor of the <strong class="source-inline">JIT</strong> class is called with all instantiated objects, and the result is returned to <span class="No-Break">the caller:</span><pre class="source-code">&#13;
    auto ES =&#13;
        std::make_unique&lt;llvm::orc::ExecutionSession&gt;(&#13;
            std::move(SSP));&#13;
    return std::make_unique&lt;JIT&gt;(&#13;
        std::move(*TPC), std::move(ES), std::move(*DL),&#13;
        std::move(JTMB));&#13;
  }</pre></li>				<li>The constructor of the <strong class="source-inline">JIT</strong> class moves the passed parameters to the private data members. Layer objects<a id="_idIndexMarker568"/> are constructed with a call to static factory names with the <strong class="source-inline">create</strong> prefix. Furthermore, each layer factory method requires a reference to the <strong class="source-inline">ExecutionSession</strong> instance, which connects the layer to the running JIT session. Except for the object-linking layer, which is at the bottom of the layer stack, each layer requires a reference to the previous layer, illustrating the <span class="No-Break">stacking order:</span><pre class="source-code">&#13;
JIT(std::unique_ptr&lt;llvm::orc::ExecutorProcessControl&gt;&#13;
          EPCtrl,&#13;
      std::unique_ptr&lt;llvm::orc::ExecutionSession&gt;&#13;
          ExeS,&#13;
      llvm::DataLayout DataL,&#13;
      llvm::orc::JITTargetMachineBuilder JTMB)&#13;
      : EPC(std::move(EPCtrl)), ES(std::move(ExeS)),&#13;
        DL(std::move(DataL)), Mangle(*ES, DL),&#13;
        ObjectLinkingLayer(std::move(&#13;
            createObjectLinkingLayer(*ES, JTMB))),&#13;
        CompileLayer(std::move(createCompileLayer(&#13;
            *ES, *ObjectLinkingLayer,&#13;
            std::move(JTMB)))),&#13;
        OptIRLayer(std::move(&#13;
            createOptIRLayer(*ES, *CompileLayer))),&#13;
        MainJITDylib(&#13;
            ES-&gt;createBareJITDylib("&lt;main&gt;")) {</pre></li>				<li>In the body of the constructor, we add a generator to search the current process for symbols. The <strong class="source-inline">GetForCurrentProcess()</strong> method is special, as the return value is wrapped in an <strong class="source-inline">Expected&lt;&gt;</strong> template, indicating that an <strong class="source-inline">Error</strong> object can also be returned. However, since we know that no error can occur, the current <a id="_idIndexMarker569"/>process will eventually run! Thus, we unwrap the result with the <strong class="source-inline">cantFail()</strong> function, which terminates the application if an error <span class="No-Break">occurred anyway:</span><pre class="source-code">&#13;
    MainJITDylib.addGenerator(llvm::cantFail(&#13;
        llvm::orc::DynamicLibrarySearchGenerator::&#13;
            GetForCurrentProcess(DL.getGlobalPrefix())));&#13;
  }</pre></li>				<li>To create an object-linking layer, we need to provide a memory manager. Here, we stick to the default <strong class="source-inline">SectionMemoryManager</strong> class, but we could also provide a different implementation <span class="No-Break">if needed:</span><pre class="source-code">&#13;
  static std::unique_ptr&lt;&#13;
      llvm::orc::RTDyldObjectLinkingLayer&gt;&#13;
  createObjectLinkingLayer(&#13;
      llvm::orc::ExecutionSession &amp;ES,&#13;
      llvm::orc::JITTargetMachineBuilder &amp;JTMB) {&#13;
    auto GetMemoryManager = []() {&#13;
      return std::make_unique&lt;&#13;
          llvm::SectionMemoryManager&gt;();&#13;
    };&#13;
    auto OLLayer = std::make_unique&lt;&#13;
        llvm::orc::RTDyldObjectLinkingLayer&gt;(&#13;
        ES, GetMemoryManager);</pre></li>				<li>A slight <a id="_idIndexMarker570"/>complication exists for the <strong class="bold">Common Object File Format</strong> (<strong class="bold">COFF</strong>) object file<a id="_idIndexMarker571"/> format, which is used on Windows. This file format does not allow functions to be marked as exported. This subsequently leads to failures in checks inside the object-linking layer: flags stored in the symbol are compared with the flags from IR, which leads to a mismatch because of the missing export marker. The solution is to override the flags only for this file format. This finishes the construction of the object layer, and the object is returned to <span class="No-Break">the caller:</span><pre class="source-code">&#13;
    if (JTMB.getTargetTriple().isOSBinFormatCOFF()) {&#13;
      OLLayer&#13;
         -&gt;setOverrideObjectFlagsWithResponsibilityFlags(&#13;
              true);&#13;
      OLLayer&#13;
         -&gt;setAutoClaimResponsibilityForObjectSymbols(&#13;
              true);&#13;
    }&#13;
    return OLLayer;&#13;
  }</pre></li>				<li>To initialize the compiler layer, an <strong class="source-inline">IRCompiler</strong> instance is required. The <strong class="source-inline">IRCompiler</strong> instance is responsible for compiling an IR module into an object file. If our JIT compiler does not use threads, then we can use the <strong class="source-inline">SimpleCompiler</strong> class, which compiles the IR module using a given target machine. The <strong class="source-inline">TargetMachine</strong> class is not threadsafe, and therefore the <strong class="source-inline">SimpleCompiler</strong> class <a id="_idIndexMarker572"/>is not, either. To support compilation with multiple threads, we use the <strong class="source-inline">ConcurrentIRCompiler</strong> class, which creates a new <strong class="source-inline">TargetMachine</strong> instance for each module to compile. This approach solves the problem with <span class="No-Break">multiple threads:</span><pre class="source-code">&#13;
  static std::unique_ptr&lt;llvm::orc::IRCompileLayer&gt;&#13;
  createCompileLayer(&#13;
      llvm::orc::ExecutionSession &amp;ES,&#13;
      llvm::orc::RTDyldObjectLinkingLayer &amp;OLLayer,&#13;
      llvm::orc::JITTargetMachineBuilder JTMB) {&#13;
    auto IRCompiler = std::make_unique&lt;&#13;
        llvm::orc::ConcurrentIRCompiler&gt;(&#13;
        std::move(JTMB));&#13;
    auto IRCLayer =&#13;
        std::make_unique&lt;llvm::orc::IRCompileLayer&gt;(&#13;
            ES, OLLayer, std::move(IRCompiler));&#13;
    return IRCLayer;&#13;
  }</pre></li>				<li>Instead of compiling the IR module directly to machine code, we install a layer that optimizes the IR first. This is a deliberate design decision: we turn our JIT compiler into an optimizing JIT compiler, which produces faster code that takes longer to produce, meaning a delay for the user. We do not add lazy compilation, so whole modules are compiled when just a symbol is looked up. This can add up to <a id="_idIndexMarker573"/>a significant amount of time before the user sees the <span class="No-Break">code executing.</span></li>&#13;
			</ol>&#13;
			<p class="callout-heading">Note</p>&#13;
			<p class="callout">Introducing lazy compilation is not a proper solution in all circumstances. Lazy compilation is realized by moving each function into a module of its own, which is compiled when the function name is looked up. This prevents inter-procedural optimizations such as <em class="italic">inlining</em> because <a id="_idIndexMarker574"/>the inliner pass needs access to the body of called functions to inline them. As a result, users see a faster startup with lazy compilation, but the produced code is not as optimal as it can be. These design decisions depend on the intended use. Here, we decide on fast code, accepting a slower startup time. Furthermore, this means that the optimization layer is essentially a <span class="No-Break">transformation layer.</span></p>&#13;
			<p class="list-inset">The <strong class="source-inline">IRTransformLayer</strong> class delegates the transformation to a function – in our case, to the <span class="No-Break"><strong class="source-inline">optimizeModule</strong></span><span class="No-Break"> function:</span></p>&#13;
			<pre class="source-code">&#13;
  static std::unique_ptr&lt;llvm::orc::IRTransformLayer&gt;&#13;
  createOptIRLayer(&#13;
      llvm::orc::ExecutionSession &amp;ES,&#13;
      llvm::orc::IRCompileLayer &amp;CompileLayer) {&#13;
    auto OptIRLayer =&#13;
        std::make_unique&lt;llvm::orc::IRTransformLayer&gt;(&#13;
            ES, CompileLayer,&#13;
            optimizeModule);&#13;
    return OptIRLayer;&#13;
  }</pre>			<ol>&#13;
				<li value="13">The <strong class="source-inline">optimizeModule()</strong> function is an example of a transformation on an IR module. The function gets the module to transform as a parameter and returns the transformed version of the IR module. Since the JIT compiler can potentially run with multiple <a id="_idIndexMarker575"/>threads, the IR module is wrapped in a <span class="No-Break"><strong class="source-inline">ThreadSafeModule</strong></span><span class="No-Break"> instance:</span><pre class="source-code">&#13;
  static llvm::Expected&lt;llvm::orc::ThreadSafeModule&gt;&#13;
  optimizeModule(&#13;
      llvm::orc::ThreadSafeModule TSM,&#13;
      const llvm::orc::MaterializationResponsibility&#13;
          &amp;R) {</pre></li>				<li>To optimize the IR, we recall some information from <a href="B19561_07.xhtml#_idTextAnchor117"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Optimizing IR</em>, in the <em class="italic">Adding an optimization pipeline to your compiler</em> section. We need a <strong class="source-inline">PassBuilder</strong> instance to create an optimization pipeline. First, we define a couple of analysis managers and register them afterward at the pass builder. Afterward, we populate a <strong class="source-inline">ModulePassManager</strong> instance with the default optimization pipeline for the <strong class="source-inline">O2</strong> level. This is again a design decision: the <strong class="source-inline">O2</strong> level produces already fast machine code, but it produces even faster code at the <strong class="source-inline">O3</strong> level. Next, we run the pipeline on the module, and finally, the optimized module is returned to <span class="No-Break">the caller:</span><pre class="source-code">&#13;
    TSM.withModuleDo([](llvm::Module &amp;M) {&#13;
      bool DebugPM = false;&#13;
      llvm::PassBuilder PB(DebugPM);&#13;
      llvm::LoopAnalysisManager LAM(DebugPM);&#13;
      llvm::FunctionAnalysisManager FAM(DebugPM);&#13;
      llvm::CGSCCAnalysisManager CGAM(DebugPM);&#13;
      llvm::ModuleAnalysisManager MAM(DebugPM);&#13;
      FAM.registerPass(&#13;
          [&amp;] { return PB.buildDefaultAAPipeline(); });&#13;
      PB.registerModuleAnalyses(MAM);&#13;
      PB.registerCGSCCAnalyses(CGAM);&#13;
      PB.registerFunctionAnalyses(FAM);&#13;
      PB.registerLoopAnalyses(LAM);&#13;
      PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);&#13;
      llvm::ModulePassManager MPM =&#13;
          PB.buildPerModuleDefaultPipeline(&#13;
              llvm::PassBuilder::OptimizationLevel::O2,&#13;
              DebugPM);&#13;
      MPM.run(M, MAM);&#13;
    });&#13;
    return TSM;&#13;
  }</pre></li>				<li>The client of the <strong class="source-inline">JIT</strong> class needs a way to add an IR module, which we provide with the <strong class="source-inline">addIRModule()</strong> function. Recall the layer stack we created: we must add the IR module to the top layer; otherwise, we would accidentally bypass some of the layers. This would be a programming error that is not easily spotted: if the <strong class="source-inline">OptIRLayer</strong> member is replaced by the <strong class="source-inline">CompileLayer</strong> member, then our <strong class="source-inline">JIT</strong> class <a id="_idIndexMarker576"/>still works, but not as an optimizing JIT because we have bypassed this layer. This is no concern for this small implementation, but in a large JIT optimization, we would introduce a function to return the <span class="No-Break">top-level layer:</span><pre class="source-code">&#13;
  llvm::Error addIRModule(&#13;
      llvm::orc::ThreadSafeModule TSM,&#13;
      llvm::orc::ResourceTrackerSP RT = nullptr) {&#13;
    if (!RT)&#13;
      RT = MainJITDylib.getDefaultResourceTracker();&#13;
    return OptIRLayer-&gt;add(RT, std::move(TSM));&#13;
  }</pre></li>				<li>Likewise, a client of our JIT class needs a way to look up a symbol. We delegate this to the <strong class="source-inline">ExecutionSession</strong> instance, passing in a reference to the main symbol table and the mangled and internalized name of the <span class="No-Break">requested symbol:</span><pre class="source-code">&#13;
  llvm::Expected&lt;llvm::orc::ExecutorSymbolDef&gt;&#13;
  lookup(llvm::StringRef Name) {&#13;
    return ES-&gt;lookup({&amp;MainJITDylib},&#13;
                      Mangle(Name.str()));&#13;
  }</pre></li>			</ol>&#13;
			<p>As we can see, the <a id="_idIndexMarker577"/>initialization of this JIT class can be tricky, as it involves a factory method and a constructor call for the <strong class="source-inline">JIT</strong> class, and factory methods for each layer. Although this distribution is caused by limitations in C++, the code itself <span class="No-Break">is straightforward.</span></p>&#13;
			<p>Next, we are going to use the new JIT compiler class to implement a simple command-line utility that takes an LLVM IR file <span class="No-Break">as input.</span></p>&#13;
			<h2 id="_idParaDest-153"><a id="_idTextAnchor157"/>Using our new JIT compiler class</h2>&#13;
			<p>We start off by <a id="_idIndexMarker578"/>creating a file called <strong class="source-inline">JIT.cpp</strong>, in the same directory as the <strong class="source-inline">JIT.h</strong> file, and add the following to this <span class="No-Break">source file:</span></p>&#13;
			<ol>&#13;
				<li>Firstly, several header files are included. We must include <strong class="source-inline">JIT.h</strong> to use our new class, and the <strong class="source-inline">IRReader.h</strong> header because it defines a function to read LLVM IR files. The <strong class="source-inline">CommandLine.h</strong> header allows us to parse the command-line options in the LLVM style. Next, <strong class="source-inline">InitLLVM.h</strong> is needed for the basic initialization of the tool. Finally, <strong class="source-inline">TargetSelect.h</strong> is needed for the initialization of the <span class="No-Break">native target:</span><pre class="source-code">&#13;
#include "JIT.h"&#13;
#include "llvm/IRReader/IRReader.h"&#13;
#include "llvm/Support/CommandLine.h"&#13;
#include "llvm/Support/InitLLVM.h"&#13;
#include "llvm/Support/TargetSelect.h"</pre></li>				<li>Next, we add the <strong class="source-inline">llvm</strong> namespace to the <span class="No-Break">current scope:</span><pre class="source-code">&#13;
using namespace llvm;</pre></li>				<li>Our JIT tool expects exactly one input file on the command line, which we declare with the <span class="No-Break"><strong class="source-inline">cl::opt&lt;&gt;</strong></span><span class="No-Break"> class:</span><pre class="source-code">&#13;
static cl::opt&lt;std::string&gt;&#13;
    InputFile(cl::Positional, cl::Required,&#13;
              cl::desc("&lt;input-file&gt;"));</pre></li>				<li>To read the IR file, we call the <strong class="source-inline">parseIRFile()</strong> function. The file can be a textual IR representation or a bitcode file. The function returns a pointer to the created module. Additionally, the error handling is a bit different, because a textual IR file <a id="_idIndexMarker579"/>can be parsed, which is not necessarily syntactically correct. Finally, the <strong class="source-inline">SMDiagnostic</strong> instance holds the error information in case of a syntax error. In the event of an error, an error message is printed, and the application <span class="No-Break">is exited:</span><pre class="source-code">&#13;
std::unique_ptr&lt;Module&gt;&#13;
loadModule(StringRef Filename, LLVMContext &amp;Ctx,&#13;
           const char *ProgName) {&#13;
  SMDiagnostic Err;&#13;
  std::unique_ptr&lt;Module&gt; Mod =&#13;
      parseIRFile(Filename, Err, Ctx);&#13;
  if (!Mod.get()) {&#13;
    Err.print(ProgName, errs());&#13;
    exit(-1);&#13;
  }&#13;
  return Mod;&#13;
}</pre></li>				<li>The <strong class="source-inline">jitmain()</strong> function is placed after the <strong class="source-inline">loadModule()</strong> method. This function sets up our JIT engine and compiles an LLVM IR module. The function needs the LLVM module with the IR to execute. The LLVM context class is also required for this module because the context class contains important type information. The <a id="_idIndexMarker580"/>goal is to call the <strong class="source-inline">main()</strong> function, so we also pass the usual <strong class="source-inline">argc</strong> and <span class="No-Break"><strong class="source-inline">argv</strong></span><span class="No-Break"> parameters:</span><pre class="source-code">&#13;
Error jitmain(std::unique_ptr&lt;Module&gt; M,&#13;
              std::unique_ptr&lt;LLVMContext&gt; Ctx,&#13;
              int argc, char *argv[]) {</pre></li>				<li>Next, we create an instance of our JIT class that we constructed earlier. If an error occurs, then we return an error <span class="No-Break">message accordingly:</span><pre class="source-code">&#13;
  auto JIT = JIT::create();&#13;
  if (!JIT)&#13;
    return JIT.takeError();</pre></li>				<li>Then, we add the module to the main <strong class="source-inline">JITDylib</strong> instance, wrapping the module and a context in a <strong class="source-inline">ThreadSafeModule</strong> instance yet again. If an error occurs, then we return an <span class="No-Break">error message:</span><pre class="source-code">&#13;
  if (auto Err = (*JIT)-&gt;addIRModule(&#13;
          orc::ThreadSafeModule(std::move(M),&#13;
                                std::move(Ctx))))&#13;
    return Err;</pre></li>				<li>Following this, we look up the <strong class="source-inline">main</strong> symbol. This symbol must be in the IR module given on the command line. The lookup triggers the compilation of that IR module. If other symbols are referenced inside the IR module, then they are resolved using the<a id="_idIndexMarker581"/> generator added in the previous step. The result is of the <strong class="source-inline">ExecutorAddr</strong> class, where it represents the address of the <span class="No-Break">executor process:</span><pre class="source-code">&#13;
  llvm::orc::ExecutorAddr MainExecutorAddr = MainSym-&gt;getAddress();&#13;
  auto *Main = MainExecutorAddr.toPtr&lt;int(int, char**)&gt;();</pre></li>				<li>Now, we can call the <strong class="source-inline">main()</strong> function in the IR module, and pass the <strong class="source-inline">argc</strong> and <strong class="source-inline">argv</strong> parameters that the function expects. We ignore the <span class="No-Break">return value:</span><pre class="source-code">&#13;
  (void)Main(argc, argv);</pre></li>				<li>We report success after the execution of <span class="No-Break">the function:</span><pre class="source-code">&#13;
  return Error::success();&#13;
}</pre></li>				<li>After implementing a <strong class="source-inline">jitmain()</strong> function, we add a <strong class="source-inline">main()</strong> function, which initializes the tool and the native target and parses the <span class="No-Break">command line:</span><pre class="source-code">&#13;
int main(int argc, char *argv[]) {&#13;
  InitLLVM X(argc, argv);&#13;
  InitializeNativeTarget();&#13;
  InitializeNativeTargetAsmPrinter();&#13;
  InitializeNativeTargetAsmParser();&#13;
  cl::ParseCommandLineOptions(argc, argv, "JIT\n");</pre></li>				<li>Afterward, the LLVM context class is initialized, and we load the IR module named on the <span class="No-Break">command line:</span><pre class="source-code">&#13;
  auto Ctx = std::make_unique&lt;LLVMContext&gt;();&#13;
  std::unique_ptr&lt;Module&gt; M =&#13;
      loadModule(InputFile, *Ctx, argv[0]);</pre></li>				<li>After loading the<a id="_idIndexMarker582"/> IR module, we can call the <strong class="source-inline">jitmain()</strong> function. To handle errors, we use the <strong class="source-inline">ExitOnError</strong> utility class to print an error message and exit the application when an error is encountered. We also set a banner with the name of the application, which is printed before the <span class="No-Break">error message:</span><pre class="source-code">&#13;
  ExitOnError ExitOnErr(std::string(argv[0]) + ": ");&#13;
  ExitOnErr(jitmain(std::move(M), std::move(Ctx),&#13;
                    argc, argv));</pre></li>				<li>If the control flow reaches this point, then the IR was successfully executed. We return <strong class="source-inline">0</strong> to <span class="No-Break">indicate success:</span><pre class="source-code">&#13;
  return 0;&#13;
}</pre></li>			</ol>&#13;
			<p>We can now test our newly implemented JIT compiler by compiling a simple example that prints <strong class="source-inline">Hello World!</strong> to the console. Under the hood, the new class uses a fixed optimization level, so with large enough modules, we can note differences in the startup <span class="No-Break">and runtime.</span></p>&#13;
			<p>To build our JIT compiler, we can follow the same CMake steps as we did near the end of the <em class="italic">Implementing our own JIT compiler with LLJIT</em> section, and we just need to ensure that the <strong class="source-inline">JIT.cpp</strong> source file is being compiled with the correct libraries to <span class="No-Break">link against:</span></p>&#13;
			<pre class="source-code">&#13;
add_executable(JIT JIT.cpp)&#13;
include_directories(${CMAKE_SOURCE_DIR})&#13;
target_link_libraries(JIT ${llvm_libs})</pre>			<p>We then change into the <strong class="source-inline">build</strong> directory and compile <span class="No-Break">the application:</span></p>&#13;
			<pre class="console">&#13;
$ cmake –G Ninja &lt;path to jit source directory&gt;&#13;
$ ninja</pre>			<p>Our <strong class="source-inline">JIT</strong> tool is now ready to be used. A simple <strong class="source-inline">Hello World!</strong> program can be written in C, like <span class="No-Break">the following:</span></p>&#13;
			<pre class="console">&#13;
$ cat main.c&#13;
#include &lt;stdio.h&gt;&#13;
int main(int argc, char** argv) {&#13;
  printf("Hello world!\n");&#13;
  return 0;&#13;
}</pre>			<p>Next, we can compile the Hello World C source into LLVM IR with the <span class="No-Break">following command:</span></p>&#13;
			<pre class="console">&#13;
$ clang -S -emit-llvm main.c</pre>			<p>Remember – we compile<a id="_idIndexMarker583"/> the C source into LLVM IR because our JIT compiler accepts an IR file as input. Finally, we can invoke our JIT compiler with our IR example, <span class="No-Break">as follows:</span></p>&#13;
			<pre class="console">&#13;
$ JIT main.ll&#13;
Hello world!</pre>			<h1 id="_idParaDest-154"><a id="_idTextAnchor158"/>Summary</h1>&#13;
			<p>In this chapter, you learned how to develop a JIT compiler. You began with learning about the possible applications of JIT compilers, and you explored <strong class="source-inline">lli</strong>, the LLVM dynamic compiler and interpreter. Using the predefined <strong class="source-inline">LLJIT</strong> class, you built an interactive JIT-based calculator tool and learned about looking up symbols and adding IR modules to <strong class="source-inline">LLJIT</strong>. To be able to take advantage of the layered structure of the ORC API, you also implemented an optimizing <span class="No-Break"><strong class="source-inline">JIT</strong></span><span class="No-Break"> class.</span></p>&#13;
			<p>In the next chapter, you will learn how to utilize LLVM tools for <span class="No-Break">debugging purposes.</span></p>&#13;
		</div>&#13;
	</div></body></html>