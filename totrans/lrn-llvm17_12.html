<html><head></head><body><div><p>&#13;
			<h1 id="_idParaDest-141" class="chapter-number"><a id="_idTextAnchor145"/>9</h1>&#13;
			<h1 id="_idParaDest-142"><a id="_idTextAnchor146"/>JIT Compilation</h1>&#13;
			<p>The LLVM core libraries come with the <strong class="bold">ExecutionEngine</strong> component<a id="_idIndexMarker517"/> that allows the compilation and execution of <strong class="bold">intermediate representation</strong> (<strong class="bold">IR</strong>) code<a id="_idIndexMarker518"/> in memory. Using this component, we can <a id="_idIndexMarker519"/>build <strong class="bold">just-in-time</strong> (<strong class="bold">JIT</strong>) compilers, which allows for direct execution of IR code. A JIT compiler works more like an interpreter because no object code needs to be stored on secondary storage.</p>&#13;
			<p>In this chapter, you will learn about applications for JIT compilers, and how the LLVM JIT compiler works in principle. You will explore the LLVM dynamic compiler and interpreter and learn how to implement JIT compiler tools on your own. Furthermore, you will also learn how to use a JIT compiler as part of a static compiler, and the associated challenges.</p>&#13;
			<p>This chapter will cover the following topics:</p>&#13;
			<ul>&#13;
				<li>Getting an overview of LLVM’s JIT implementation and use cases</li>&#13;
				<li>Using JIT compilation for direct execution</li>&#13;
				<li>Implementing your own JIT compiler from existing classes</li>&#13;
				<li>Implementing your own JIT compiler from scratch</li>&#13;
			</ul>&#13;
			<p>By the end of the chapter, you will understand and know how to develop a JIT compiler, either using a preconfigured class or a customized version fitting for your needs.</p>&#13;
			<h1 id="_idParaDest-143"><a id="_idTextAnchor147"/>Technical requirements</h1>&#13;
			<p>You can find the code used in this chapter at <a href="https://github.com/PacktPublishing/Learn-LLVM-17/tree/main/Chapter09">https://github.com/PacktPublishing/Learn-LLVM-17/tree/main/Chapter09</a> .</p>&#13;
			<h1 id="_idParaDest-144"><a id="_idTextAnchor148"/>LLVM’s overall JIT implementation and use cases</h1>&#13;
			<p>So far, we have only looked <a id="_idIndexMarker520"/>at <strong class="bold">ahead-of-time</strong> (<strong class="bold">AOT</strong>) compilers. These compilers compile the whole application. The application can only run after the compilation is finished. If the compilation is performed at the runtime of the application, then the compiler is a JIT compiler. A <a id="_idIndexMarker521"/>JIT compiler has<a id="_idIndexMarker522"/> interesting use cases:</p>&#13;
			<ul>&#13;
				<li><strong class="bold">Implementation of a virtual machine</strong>: A programming language can be translated to byte code with an <a id="_idIndexMarker523"/>AOT compiler. At runtime, a JIT compiler is used to compile the byte code to machine code. The advantage of this approach is that the byte code is hardware-independent, and thanks to the JIT compiler, there is no performance penalty compared to an AOT compiler. Java and C# use this model today, but this is not a new idea: the USCD Pascal compiler from 1977 already used a similar approach.</li>&#13;
				<li><code>lldb</code> LLVM debugger uses this approach to evaluate source expressions at debug time.</li>&#13;
				<li><strong class="bold">Database queries</strong>: A <a id="_idIndexMarker525"/>database creates an execution plan from a database query. The execution plan describes operations on tables and columns, which leads to a query answer when executed. A JIT compiler can be used to translate the execution plan into machine code, which speeds up the execution of the query.</li>&#13;
			</ul>&#13;
			<p>The static compilation model of LLVM is not as far away from the JIT model as one may think. The <code>llc</code> LLVM static compiler compiles LLVM IR into machine code and saves the result as an object file on disk. If the object file is not stored on disk but in memory, would the code be executable? Not directly, as references to global functions and global data use relocations instead of absolute addresses. Conceptually, a <strong class="bold">relocation</strong> describes how to calculate the address – for <a id="_idIndexMarker526"/>example, as an offset to a known address. If we resolve relocations into addresses, as the linker and the dynamic loader do, then we can execute the object code. Running the static compiler to compile IR code into an object file in memory, performing a link step on the in-memory object file, and running the code gives us a JIT compiler. The JIT implementation in the LLVM core libraries is based on this idea.</p>&#13;
			<p>During the<a id="_idIndexMarker527"/> development history of LLVM, there were several JIT implementations, with different feature sets. The latest JIT API is the <strong class="bold">On-Request Compilation</strong> (<strong class="bold">ORC</strong>) engine. In case<a id="_idIndexMarker528"/> you were curious about the acronym, it was the lead developer’s intention<a id="_idIndexMarker529"/> to invent yet another acronym based on Tolkien’s universe, after <strong class="bold">Executable and Linking Format</strong> (<strong class="bold">ELF</strong>) and <strong class="bold">Debugging Standard</strong> (<strong class="bold">DWARF</strong>) were<a id="_idIndexMarker530"/> already present.</p>&#13;
			<p>The ORC engine builds on and extends the idea of using the static compiler and a dynamic linker on the in-memory object file. The implementation uses a layered approach. The two basic levels are the compile layer and the link layer. On top of this sits a layer providing support for lazy compilation. A transformation layer can be stacked on top or below the lazy compilation layer, allowing the developer to add arbitrary transformations or simply to be notified of certain events. Moreover, this layered approach has the advantage that the JIT engine is customizable for diverse requirements. For example, a high-performance virtual machine may choose to compile everything upfront and make no use of the lazy compilation layer. On the other hand, other virtual machines will emphasize startup time and responsiveness to the user and will achieve this with the help of the lazy compilation layer.</p>&#13;
			<p>The older MCJIT engine is still available, and its API is derived from an even older, already-removed JIT engine. Over time, this API gradually became bloated, and it lacks the flexibility of the ORC API. The goal is to remove this implementation, as the ORC engine now provides all the functionality of the MCJIT engine, and new developments should use the <a id="_idIndexMarker531"/>ORC API.</p>&#13;
			<p>In the next section, we look at <code>lli</code>, the LLVM interpreter, and the dynamic compiler, before we dive into implementing a JIT compiler.</p>&#13;
			<h1 id="_idParaDest-145"><a id="_idTextAnchor149"/>Using JIT compilation for direct execution</h1>&#13;
			<p>Running LLVM IR directly is the first <a id="_idIndexMarker532"/>idea that comes to mind when thinking about a JIT compiler. This is what the <code>lli</code> tool, the LLVM interpreter, and the dynamic compiler do. We will explore the <code>lli</code> tool in the next section.</p>&#13;
			<h2 id="_idParaDest-146"><a id="_idTextAnchor150"/>Exploring the lli tool</h2>&#13;
			<p>Let’s try the <code>lli</code> tool <a id="_idIndexMarker533"/>with a very simple example. The following LLVM IR can be stored as a file called <code>hello.ll</code>, which is the equivalent of a C hello world application. This file declares a prototype for the <code>printf()</code> function from the C library. The <code>hellostr</code> constant contains the message to be printed. Inside the <code>main()</code> function, a call to the <code>printf()</code> function is generated, and this function contains a <code>hellostr</code> message that will be printed. The application always returns <code>0</code>.</p>&#13;
			<p>The complete source code is as follows:</p>&#13;
			<pre class="source-code">&#13;
declare i32 @printf(ptr, ...)&#13;
@hellostr = private unnamed_addr constant [13 x i8] c"Hello world\0A\00"&#13;
define dso_local i32 @main(i32 %argc, ptr %argv) {&#13;
  %res = call i32 (ptr, ...) @printf(ptr @hellostr)&#13;
  ret i32 0&#13;
}</pre>			<p>This LLVM IR file is generic enough that it is valid for all platforms. We can directly execute the IR using the <code>lli</code> tool with the following command:</p>&#13;
			<pre class="console">&#13;
$ lli hello.ll&#13;
Hello world</pre>			<p>The interesting point here is how the <code>printf()</code> function is found. The IR code is compiled to machine code, and a lookup for the <code>printf</code> symbol is triggered. This symbol is not found in the IR, so the current process is searched for it. The <code>lli</code> tool dynamically links against the C library, and the symbol is found there.</p>&#13;
			<p>Of course, the <code>lli</code> tool does not link against the libraries you created. To enable the use of such functions, the <code>lli</code> tool supports the loading of shared libraries and objects. The following C source just prints a friendly message:</p>&#13;
			<pre class="source-code">&#13;
#include &lt;stdio.h&gt;&#13;
void greetings() {&#13;
  puts("Hi!");&#13;
}</pre>			<p>Stored in <code>greetings.c</code>, we use this to explore loading objects with <code>lli</code>. The following command will compile this source into a shared library. The <code>–fPIC</code> option instructs <code>clang</code> to generate position-independent code, which is required for shared libraries. Moreover, the compiler creates a <code>greetings.so</code> shared library with             <code>–</code><code>shared</code>:</p>&#13;
			<pre class="console">&#13;
$ clang greetings.c -fPIC -shared -o greetings.so</pre>			<p>We also compile the file into the <code>greetings.o</code> object file:</p>&#13;
			<pre class="console">&#13;
$ clang greetings.c -c -o greetings.o</pre>			<p>We now have two<a id="_idIndexMarker534"/> files, the <code>greetings.so</code> shared library and the <code>greetings.o</code> object file, which we will load into the <code>lli</code> tool.</p>&#13;
			<p>We also need an LLVM IR file that calls the <code>greetings()</code> function. For this, create a <code>main.ll</code> file that contains a single call to the function:</p>&#13;
			<pre class="source-code">&#13;
declare void @greetings(...)&#13;
define dso_local i32 @main(i32 %argc, i8** %argv) {&#13;
  call void (...) @greetings()&#13;
  ret i32 0&#13;
}</pre>			<p>Notice that on executing, the previous IR crashes, as <code>lli</code> cannot locate the greetings symbol:</p>&#13;
			<pre class="console">&#13;
$ lli main.ll&#13;
JIT session error: Symbols not found: [ _greetings ]&#13;
lli: Failed to materialize symbols: { (main, { _main }) }</pre>			<p>The <code>greetings()</code> function is defined in an external file, and to fix the crash, we have to tell the <code>lli</code> tool which additional file needs to be loaded. In order to use the shared library, you must use the <code>–load</code> option, which takes the path to the shared library as an argument:</p>&#13;
			<pre class="console">&#13;
$ lli –load ./greetings.so main.ll&#13;
Hi!</pre>			<p>It is important to specify the path to the shared library if the directory containing the shared library is not in the search path for the dynamic loader. If omitted, then the library will not be found.</p>&#13;
			<p>Alternatively, we can<a id="_idIndexMarker535"/> instruct <code>lli</code> to load the object file with <code>–extra-object</code>:</p>&#13;
			<pre class="console">&#13;
$ lli –extra-object greetings.o main.ll&#13;
Hi!</pre>			<p>Other supported options are <code>–extra-archive</code>, which loads an archive, and                <code>–extra-module</code>, which loads another bitcode file. Both options require the path to the file as an argument.</p>&#13;
			<p>You now know how you can use the <code>lli</code> tool to directly execute LLVM IR. In the next section, we will implement our own JIT tool.</p>&#13;
			<h1 id="_idParaDest-147"><a id="_idTextAnchor151"/>Implementing our own JIT compiler with LLJIT</h1>&#13;
			<p>The <code>lli</code> tool is<a id="_idIndexMarker536"/> nothing more than a thin<a id="_idIndexMarker537"/> wrapper around LLVM APIs. In the first section, we learned that the ORC engine uses a layered approach. The <code>ExecutionSession</code> class represents a running JIT program. Besides other items, this class holds information such as used <code>JITDylib</code> instances. A <code>JITDylib</code> instance is a symbol table that maps symbol names to addresses. For example, these can be symbols defined in an LLVM IR file or the symbols of a loaded shared library.</p>&#13;
			<p>For<a id="_idIndexMarker538"/> executing LLVM IR, we do not <a id="_idIndexMarker539"/>need to create a JIT stack on our own, as the <code>LLJIT</code> class provides this functionality. You can also make use of this class when migrating from the older MCJIT implementation, as this class essentially provides the same functionality.</p>&#13;
			<p>To illustrate the functions of the <code>LLJIT</code> utility, we will be creating an interactive calculator application while incorporating JIT functionality. The main source code of our JIT calculator will be extended from the <code>calc</code> example from <a href="B19561_02.xhtml#_idTextAnchor037"><em class="italic">Chapter 2</em></a>, <em class="italic">The Structure of </em><em class="italic">a Compiler</em>.</p>&#13;
			<p>The primary idea behind our interactive JIT calculator will be as follows:</p>&#13;
			<ol>&#13;
				<li>Allow the user to input a function definition, such as <code>def f(x) = </code><code>x*2</code>.</li>&#13;
				<li>The function inputted by the user is then compiled by the <code>LLJIT</code> utility into a function – in this case, <code>f</code>.</li>&#13;
				<li>Allow the user to call the function they have defined with a numerical value: <code>f(3)</code>.</li>&#13;
				<li>Evaluate the function with the provided argument, and print the result to the console: <code>6</code>.</li>&#13;
			</ol>&#13;
			<p>Before we discuss incorporating JIT functionality into the calculator source code, there are a few main differences to point out with respect to the original calculator example:</p>&#13;
			<ul>&#13;
				<li>Firstly, we previously only input and parsed functions beginning with the <code>with</code> keyword, rather than the <code>def</code> keyword described previously. For this chapter, we instead only accept function definitions beginning with <code>def</code>, and this is represented as a particular node in<a id="_idIndexMarker540"/> our <code>DefDecl</code>. The <code>DefDecl</code> class is aware of the arguments and their names it is defined with, and the function name is also stored within this class.</li>&#13;
				<li>Secondly, we also need our AST to be aware of function calls, to represent the functions that the <code>LLJIT</code> utility has consumed or JIT’ted. Whenever a user inputs the name of a function, followed by arguments enclosed in parentheses, the AST recognizes these as <code>FuncCallFromDef</code> nodes. This class essentially is aware of the same information as the <code>DefDecl</code> class.</li>&#13;
			</ul>&#13;
			<p>Due to the <a id="_idIndexMarker541"/>addition of these two AST classes, it <a id="_idIndexMarker542"/>is obvious to expect that the semantic analysis, parser, and code generation classes will be adapted accordingly to handle the changes in our AST. One additional thing to note is the addition of a new data structure, called <code>JITtedFunctions</code>, which all these classes are aware of. This data structure is a map with the defined function names as keys, and the number of arguments a function is defined with is stored as values within the map. We will see later how this data structure will be utilized in our JIT calculator.</p>&#13;
			<p>For more details on the changes we have made to the <code>calc</code> example, the full source containing the changes from <code>calc</code> and this section’s JIT implementation can be found within the <code>lljit</code> source directory.</p>&#13;
			<h2 id="_idParaDest-148"><a id="_idTextAnchor152"/>Integrating the LLJIT engine into the calculator</h2>&#13;
			<p>Firstly, let’s discuss<a id="_idIndexMarker543"/> how to set up the JIT engine in our interactive calculator. All of the implementation pertaining to the JIT engine exists within <code>Calc.cpp</code>, and this file has one <code>main()</code> loop for the execution of the program:</p>&#13;
			<ol>&#13;
				<li>We must include several header files, aside from the headers including our code generation, semantic analyzer, and parser implementation. The <code>LLJIT.h</code> header defines the <code>LLJIT</code> class and the core classes of the ORC API. Next, the <code>InitLLVM.h</code> header is needed for the basic initialization of the tool, and the <code>TargetSelect.h</code> header is needed for the initialization of the native target. Finally, we also include the <code>&lt;iostream&gt;</code> C++ header to allow for user input into our calculator application:<pre class="source-code">&#13;
#include "CodeGen.h"&#13;
#include "Parser.h"&#13;
#include "Sema.h"&#13;
#include "llvm/ExecutionEngine/Orc/LLJIT.h"&#13;
#include "llvm/Support/InitLLVM.h"&#13;
#include "llvm/Support/TargetSelect.h"&#13;
#include &lt;iostream&gt;</pre></li>				<li>Next, we <a id="_idIndexMarker544"/>add the <code>llvm</code> and <code>llvm::orc</code> namespaces to the current scope:<pre class="source-code">&#13;
using namespace llvm;&#13;
using namespace llvm::orc;</pre></li>				<li>Many of the calls from our <code>LLJIT</code> instance that we will be creating return an error type, <code>Error</code>. The <code>ExitOnError</code> class allows us to discard <code>Error</code> values that are returned by the calls from the <code>LLJIT</code> instance while logging to <code>stderr</code> and exiting the application. We declare a global <code>ExitOnError</code> variable as follows:<pre class="source-code">&#13;
ExitOnError ExitOnErr;</pre></li>				<li>Then, we add the <code>main()</code> function, which initializes the tool and the native target:<pre class="source-code">&#13;
int main(int argc, const char **argv{&#13;
  InitLLVM X(argc, argv);&#13;
  InitializeNativeTarget();&#13;
  InitializeNativeTargetAsmPrinter();&#13;
  InitializeNativeTargetAsmParser();</pre></li>				<li>We use the <code>LLJITBuilder</code> class to create an <code>LLJIT</code> instance, wrapped in the previously declared <code>ExitOnErr</code> variable in case an error occurs. A possible source of error would be that the platform does not yet support JIT compilation:<pre class="source-code">&#13;
auto JIT = ExitOnErr(LLJITBuilder().create());</pre></li>				<li>Next, we declare our <code>JITtedFunctions</code> map that keeps track of the function definitions, as we have previously described:<pre class="source-code">&#13;
StringMap&lt;size_t&gt; JITtedFunctions;</pre></li>				<li>To facilitate an environment that waits for user input, we add a <code>while()</code> loop and allow the user to type in an expression, saving the line that the user typed within a string called <code>calcExp</code>:<pre class="source-code">&#13;
  while (true) {&#13;
    outs() &lt;&lt; "JIT calc &gt; ";&#13;
    std::string calcExp;&#13;
    std::getline(std::cin, calcExp);</pre></li>				<li>Afterward, the <a id="_idIndexMarker545"/>LLVM context class is initialized, along with a new LLVM module. The module’s data layout is also set accordingly, and we also declare a code generator, which will be used to generate IR for the function that the user has defined on the command line:<pre class="source-code">&#13;
    std::unique_ptr&lt;LLVMContext&gt; Ctx = std::make_unique&lt;LLVMContext&gt;();&#13;
    std::unique_ptr&lt;Module&gt; M = std::make_unique&lt;Module&gt;("JIT calc.expr", *Ctx);&#13;
    M-&gt;setDataLayout(JIT-&gt;getDataLayout());&#13;
    CodeGen CodeGenerator;</pre></li>				<li>We must interpret the line that was entered by the user to determine if the user is defining a new function or calling a previous function that they have defined with an argument. A <code>Lexer</code> class is defined while taking in the line of input that the user has given. We will see that there are two main cases that the lexer cares about:<pre class="source-code">&#13;
    Lexer Lex(calcExp);&#13;
    Token::TokenKind CalcTok = Lex.peek();</pre></li>				<li>The lexer can<a id="_idIndexMarker546"/> check the first token of the user input. If the user is defining a new function (represented by the <code>def</code> keyword, or the <code>Token::KW_def</code> token), then we parse it and check its semantics. If the parser or the semantic analyzer detects any issues with the user-defined function, errors will be emitted accordingly, and the calculator program will halt. If no errors are detected from either the parser or the semantic analyzer, this means we have a valid AST data structure, <code>DefDecl</code>:<pre class="source-code">&#13;
   if (CalcTok == Token::KW_def) {&#13;
      Parser Parser(Lex);&#13;
      AST *Tree = Parser.parse();&#13;
      if (!Tree || Parser.hasError()) {&#13;
        llvm::errs() &lt;&lt; "Syntax errors occured\n";&#13;
        return 1;&#13;
      }&#13;
      Sema Semantic;&#13;
      if (Semantic.semantic(Tree, JITtedFunctions)) {&#13;
        llvm::errs() &lt;&lt; "Semantic errors occured\n";&#13;
        return 1;&#13;
      }</pre></li>				<li>We then can pass our newly constructed AST into our code generator to compile the IR for the function that the user has defined. The specifics of IR generation will be discussed afterward, but this function that compiles to the IR needs to be aware of the module and our <code>JITtedFunctions</code> map. After generating the IR, we can add this information to our <code>LLJIT</code> instance by calling <code>addIRModule()</code> and wrapping our module and context in a <code>ThreadSafeModule</code> class, to prevent these from being accessed by other concurrent threads:<pre class="source-code">&#13;
      CodeGenerator.compileToIR(Tree, M.get(), JITtedFunctions);&#13;
      ExitOnErr(&#13;
          JIT-&gt;addIRModule(ThreadSafeModule(std::move(M),           std::move(Ctx))));</pre></li>				<li>Instead, if the <a id="_idIndexMarker547"/>user is calling a function with parameters, which is represented by the <code>Token::ident</code> token, we also need to parse and semantically check if the user input is valid prior to converting the input into a valid AST. The parsing and checking here are slightly different compared to before, as it can include checks such as ensuring the number of parameters that the user has supplied to the function call matches the number of parameters that the function was originally defined with:<pre class="source-code">&#13;
   } else if (CalcTok == Token::ident) {&#13;
      outs() &lt;&lt; "Attempting to evaluate expression:\n";&#13;
      Parser Parser(Lex);&#13;
      AST *Tree = Parser.parse();&#13;
      if (!Tree || Parser.hasError()) {&#13;
        llvm::errs() &lt;&lt; "Syntax errors occured\n";&#13;
        return 1;&#13;
      }&#13;
      Sema Semantic;&#13;
      if (Semantic.semantic(Tree, JITtedFunctions)) {&#13;
        llvm::errs() &lt;&lt; "Semantic errors occured\n";&#13;
        return 1;&#13;
      }</pre></li>				<li>Once a valid AST is constructed for a function call, <code>FuncCallFromDef</code>, we get the name of the function from the AST, and then the code generator prepares to generate the call to the function that was previously added to the <code>LLJIT</code> instance. What occurs under the cover is that the user-defined function is regenerated as an LLVM call within a separate function that will be created that does the actual evaluation of the original function. This step requires the AST, the module, the function call name, and our map of function definitions:<pre class="source-code">&#13;
      llvm::StringRef FuncCallName = Tree-&gt;getFnName();&#13;
      CodeGenerator.prepareCalculationCallFunc(Tree, M.get(),       FuncCallName, JITtedFunctions);</pre></li>				<li>After the<a id="_idIndexMarker548"/> code generator has completed its work to regenerate the original function and to create a separate evaluation function, we must add this information to the <code>LLJIT</code> instance. We create a <code>ResourceTracker</code> instance to track the memory that is allocated to the functions that have been added to <code>LLJIT</code>, as well as another <code>ThreadSafeModule</code> instance of the module and context. These two instances are then added to the JIT as an IR module:<pre class="source-code">&#13;
      auto RT = JIT-&gt;getMainJITDylib().createResourceTracker();&#13;
      auto TSM = ThreadSafeModule(std::move(M), std::move(Ctx));&#13;
      ExitOnErr(JIT-&gt;addIRModule(RT, std::move(TSM)));</pre></li>				<li>The separate evaluation function is then queried for within our <code>LLJIT</code> instance through the <code>lookup()</code> method, by supplying the name of our evaluation function, <code>calc_expr_func</code>, into the function. If the query is successful, the address for the <code>calc_expr_func</code> function is cast to the appropriate type, which is a function that takes no arguments and returns a single integer. Once the function’s address is acquired, we call the function to generate the result of the user-defined function with the parameters they have supplied and then print the result to the console:<pre class="source-code">&#13;
      auto CalcExprCall = ExitOnErr(JIT-&gt;lookup("calc_expr_func"));&#13;
      int (*UserFnCall)() = CalcExprCall.toPtr&lt;int (*)()&gt;();&#13;
      outs() &lt;&lt; "User defined function evaluated to:       " &lt;&lt; UserFnCall() &lt;&lt; "\n";</pre></li>				<li>After the<a id="_idIndexMarker549"/> function call is completed, the memory that was previously associated with our functions is then freed by <code>ResourceTracker</code>:<pre class="source-code">&#13;
ExitOnErr(RT-&gt;remove());</pre></li>			</ol>&#13;
			<h2 id="_idParaDest-149"><a id="_idTextAnchor153"/>Code generation changes to support JIT compilation via LLJIT</h2>&#13;
			<p>Now, let’s take a brief look at some of the <a id="_idIndexMarker550"/>changes we have made within <code>CodeGen.cpp</code> to support our JIT-based calculator:</p>&#13;
			<ol>&#13;
				<li>As previously mentioned, the code generation class has two important methods: one to compile the user-defined function into LLVM IR and print the IR to the console, and another to prepare the calculation evaluation function, <code>calc_expr_func</code>, which contains a call to the original user-defined function for evaluation. This second function also prints the resulting IR to the user:<pre class="source-code">&#13;
void CodeGen::compileToIR(AST *Tree, Module *M,&#13;
                    StringMap&lt;size_t&gt; &amp;JITtedFunctions) {&#13;
  ToIRVisitor ToIR(M, JITtedFunctions);&#13;
  ToIR.run(Tree);&#13;
  M-&gt;print(outs(), nullptr);&#13;
}&#13;
void CodeGen::prepareCalculationCallFunc(AST *FuncCall,&#13;
           Module *M, llvm::StringRef FnName,&#13;
           StringMap&lt;size_t&gt; &amp;JITtedFunctions) {&#13;
  ToIRVisitor ToIR(M, JITtedFunctions);&#13;
  ToIR.genFuncEvaluationCall(FuncCall);&#13;
  M-&gt;print(outs(), nullptr);&#13;
}</pre></li>				<li>As noted in the<a id="_idIndexMarker551"/> preceding source, these code generation functions define a <code>ToIRVisitor</code> instance that takes in our module and a <code>JITtedFunctions</code> map to be used in its constructor upon initialization:<pre class="source-code">&#13;
class ToIRVisitor : public ASTVisitor {&#13;
  Module *M;&#13;
  IRBuilder&lt;&gt; Builder;&#13;
  StringMap&lt;size_t&gt; &amp;JITtedFunctionsMap;&#13;
. . .&#13;
public:&#13;
  ToIRVisitor(Module *M,&#13;
              StringMap&lt;size_t&gt; &amp;JITtedFunctions)&#13;
      : M(M), Builder(M-&gt;getContext()),       JITtedFunctionsMap(JITtedFunctions) {</pre></li>				<li>Ultimately, this information is used to either generate IR or evaluate the function that the IR was previously generated for. When generating the IR, the code generator expects to see a <code>DefDecl</code> node, which represents defining a new function. The function name, along with the number of arguments it is defined with, is stored within the function definitions map:<pre class="source-code">&#13;
virtual void visit(DefDecl &amp;Node) override {&#13;
    llvm::StringRef FnName = Node.getFnName();&#13;
    llvm::SmallVector&lt;llvm::StringRef, 8&gt; FunctionVars =     Node.getVars();&#13;
    (JITtedFunctionsMap)[FnName] = FunctionVars.size();</pre></li>				<li>Afterward, the actual function definition is created by the <code>genUserDefinedFunction()</code> call:<pre class="source-code">&#13;
    Function *DefFunc = genUserDefinedFunction(FnName);</pre></li>				<li>Within <code>genUserDefinedFunction()</code>, the first step is to check if the function exists within the module. If it does not, we ensure that the function prototype exists within our map data structure. Then, we use the name and the number of <a id="_idIndexMarker552"/>arguments to construct a function that has the number of arguments that were defined by the user, and make the function return a single integer value:<pre class="source-code">&#13;
Function *genUserDefinedFunction(llvm::StringRef Name) {&#13;
    if (Function *F = M-&gt;getFunction(Name))&#13;
      return F;&#13;
    Function *UserDefinedFunction = nullptr;&#13;
    auto FnNameToArgCount = JITtedFunctionsMap.find(Name);&#13;
    if (FnNameToArgCount != JITtedFunctionsMap.end()) {&#13;
      std::vector&lt;Type *&gt; IntArgs(FnNameToArgCount-&gt;second,       Int32Ty);&#13;
      FunctionType *FuncType = FunctionType::get(Int32Ty,       IntArgs, false);&#13;
      UserDefinedFunction =&#13;
          Function::Create(FuncType,           GlobalValue::ExternalLinkage, Name, M);&#13;
    }&#13;
    return UserDefinedFunction;&#13;
  }</pre></li>				<li>After <a id="_idIndexMarker553"/>generating the user-defined function, a new basic block is created, and we insert our function into the basic block. Each function argument is also associated with a name that is defined by the user, so we also set the names for all function arguments accordingly, as well as generate mathematical operations that operate on the arguments within the function:<pre class="source-code">&#13;
    BasicBlock *BB = BasicBlock::Create(M-&gt;getContext(),     "entry", DefFunc);&#13;
    Builder.SetInsertPoint(BB);&#13;
    unsigned FIdx = 0;&#13;
    for (auto &amp;FArg : DefFunc-&gt;args()) {&#13;
      nameMap[FunctionVars[FIdx]] = &amp;FArg;&#13;
      FArg.setName(FunctionVars[FIdx++]);&#13;
    }&#13;
    Node.getExpr()-&gt;accept(*this);&#13;
  };</pre></li>				<li>When evaluating the user-defined function, the AST that is expected in our example is called a <code>FuncCallFromDef</code> node. First, we define the evaluation function and name it <code>calc_expr_func</code> (taking in zero arguments and returning one result):<pre class="source-code">&#13;
  virtual void visit(FuncCallFromDef &amp;Node) override {&#13;
    llvm::StringRef CalcExprFunName = "calc_expr_func";&#13;
    FunctionType *CalcExprFunTy = FunctionType::get(Int32Ty, {},     false);&#13;
    Function *CalcExprFun = Function::Create(&#13;
        CalcExprFunTy, GlobalValue::ExternalLinkage,         CalcExprFunName, M);</pre></li>				<li>Next, we <a id="_idIndexMarker554"/>create a new basic block to insert <code>calc_expr_func</code> into:<pre class="source-code">&#13;
    BasicBlock *BB = BasicBlock::Create(M-&gt;getContext(),     "entry", CalcExprFun);&#13;
    Builder.SetInsertPoint(BB);</pre></li>				<li>Similar to before, the user-defined function is retrieved by <code>genUserDefinedFunction()</code>, and we pass the numerical parameters of the function call into the original function that we have just regenerated:<pre class="source-code">&#13;
    llvm::StringRef CalleeFnName = Node.getFnName();&#13;
    Function *CalleeFn = genUserDefinedFunction(CalleeFnName);</pre></li>				<li>Once we have the actual <code>llvm::Function</code> instance available, we utilize <code>IRBuilder</code> to create a call to the defined function and also return the result so that it is accessible when the result is printed to the user in the end:<pre class="source-code">&#13;
    auto CalleeFnVars = Node.getArgs();&#13;
    llvm::SmallVector&lt;Value *&gt; IntParams;&#13;
    for (unsigned i = 0, end = CalleeFnVars.size(); i != end;     ++i) {&#13;
      int ArgsToIntType;&#13;
      CalleeFnVars[i].getAsInteger(10, ArgsToIntType);&#13;
      Value *IntParam = ConstantInt::get(Int32Ty, ArgsToIntType,       true);&#13;
      IntParams.push_back(IntParam);&#13;
    }&#13;
    Builder.CreateRet(Builder.CreateCall(CalleeFn, IntParams,     "calc_expr_res"));&#13;
  };</pre></li>			</ol>&#13;
			<h2 id="_idParaDest-150"><a id="_idTextAnchor154"/>Building an LLJIT-based calculator</h2>&#13;
			<p>Finally, to compile our JIT <a id="_idIndexMarker555"/>calculator source, we also need to create a <code>CMakeLists.txt</code> file with the build description, saved beside <code>Calc.cpp</code> and our other source files:</p>&#13;
			<ol>&#13;
				<li>We set the minimal required CMake version to the number required by LLVM and give the project a name:<pre class="source-code">&#13;
cmake_minimum_required (VERSION 3.20.0)&#13;
project ("jit")</pre></li>				<li>The LLVM package needs to be loaded, and we add the directory of the CMake modules provided by LLVM to the search path. Then, we include the <code>DetermineGCCCompatible</code> and <code>ChooseMSVCCRT</code> modules, which check if the compiler has GCC-compatible command-line syntax and ensure that the same C runtime is used as by LLVM, respectively:<pre class="source-code">&#13;
find_package(LLVM REQUIRED CONFIG)&#13;
list(APPEND CMAKE_MODULE_PATH ${LLVM_DIR})&#13;
include(DetermineGCCCompatible)&#13;
include(ChooseMSVCCRT)</pre></li>				<li>We also need to add definitions and the <code>include</code> path from LLVM. The used LLVM components are mapped to the library names with a function call:<pre class="source-code">&#13;
add_definitions(${LLVM_DEFINITIONS})&#13;
include_directories(SYSTEM ${LLVM_INCLUDE_DIRS})&#13;
llvm_map_components_to_libnames(llvm_libs Core OrcJIT&#13;
                                          Support native)</pre></li>				<li>Afterward, if it is<a id="_idIndexMarker556"/> determined that the compiler has GCC-compatible command-line syntax, we also check if runtime type information and exception handling are enabled. If they are not enabled, C++ flags to turn off these features are added to our compilation accordingly:<pre class="source-code">&#13;
if(LLVM_COMPILER_IS_GCC_COMPATIBLE)&#13;
  if(NOT LLVM_ENABLE_RTTI)&#13;
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fno-rtti")&#13;
  endif()&#13;
  if(NOT LLVM_ENABLE_EH)&#13;
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fno-exceptions")&#13;
  endif()&#13;
endif()</pre></li>				<li>Lastly, we define the name of the executable, the source files to compile, and the library to link against:<pre class="source-code">&#13;
add_executable (calc&#13;
  Calc.cpp CodeGen.cpp Lexer.cpp Parser.cpp Sema.cpp)&#13;
target_link_libraries(calc PRIVATE ${llvm_libs})</pre></li>			</ol>&#13;
			<p>The preceding steps are all that is required for our JIT-based interactive calculator tool. Next, create and change into a build directory, and then run the following command to create and compile the application:</p>&#13;
			<pre class="console">&#13;
$ cmake –G Ninja &lt;path to source directory&gt;&#13;
$ ninja</pre>			<p>This compiles the <code>calc</code> tool. We<a id="_idIndexMarker557"/> can then launch the calculator, start defining functions, and see how our calculator is able to evaluate the functions that we define.</p>&#13;
			<p>The following example invocations show the IR of the function that is first defined, and then the <code>calc_expr_func</code> function that is created to generate a call to our originally defined function in order to evaluate the function with whichever parameter passed into it:</p>&#13;
			<pre class="console">&#13;
$ ./calc&#13;
JIT calc &gt; def f(x) = x*2&#13;
define i32 @f(i32 %x) {&#13;
entry:&#13;
  %0 = mul nsw i32 %x, 2&#13;
  ret i32 %0&#13;
}&#13;
JIT calc &gt; f(20)&#13;
Attempting to evaluate expression:&#13;
define i32 @calc_expr_func() {&#13;
entry:&#13;
  %calc_expr_res = call i32 @f(i32 20)&#13;
  ret i32 %calc_expr_res&#13;
}&#13;
declare i32 @f(i32)&#13;
User defined function evaluated to: 40&#13;
JIT calc &gt; def g(x,y) = x*y+100&#13;
define i32 @g(i32 %x, i32 %y) {&#13;
entry:&#13;
  %0 = mul nsw i32 %x, %y&#13;
  %1 = add nsw i32 %0, 100&#13;
  ret i32 %1&#13;
}&#13;
JIT calc &gt; g(8,9)&#13;
Attempting to evaluate expression:&#13;
define i32 @calc_expr_func() {&#13;
entry:&#13;
  %calc_expr_res = call i32 @g(i32 8, i32 9)&#13;
  ret i32 %calc_expr_res&#13;
}&#13;
declare i32 @g(i32, i32)&#13;
User defined function evaluated to: 172</pre>			<p>That’s it! We have just created a JIT-based calculator application!</p>&#13;
			<p>As our JIT calculator <a id="_idIndexMarker558"/>is meant to be a simple example that describes how to incorporate <code>LLJIT</code> into our projects, it is worth noting that there are some limitations:</p>&#13;
			<ul>&#13;
				<li>This calculator does not accept negatives of decimal values</li>&#13;
				<li>We cannot redefine the same function more than once</li>&#13;
			</ul>&#13;
			<p>For the second limitation, this<a id="_idIndexMarker559"/> occurs by design, and so is expected and enforced by the ORC API itself:</p>&#13;
			<pre class="console">&#13;
$ ./calc&#13;
JIT calc &gt; def f(x) = x*2&#13;
define i32 @f(i32 %x) {&#13;
entry:&#13;
  %0 = mul nsw i32 %x, 2&#13;
  ret i32 %0&#13;
}&#13;
JIT calc &gt; def f(x,y) = x+y&#13;
define i32 @f(i32 %x, i32 %y) {&#13;
entry:&#13;
  %0 = add nsw i32 %x, %y&#13;
  ret i32 %0&#13;
}&#13;
Duplicate definition of symbol '_f'</pre>			<p>Keep in mind that there are numerous other possibilities to expose names, besides exposing the symbols for the current process or from a shared library. For example, the <code>StaticLibraryDefinitionGenerator</code> class exposes the symbols found in a static archive and can be used in the <code>DynamicLibrarySearchGenerator</code> class.</p>&#13;
			<p>Furthermore, the <code>LLJIT</code> class has also an <code>addObjectFile()</code> method to expose the symbols of an object file. You can also provide your own <code>DefinitionGenerator</code> implementation if the <a id="_idIndexMarker560"/>existing implementations do not fit your needs.</p>&#13;
			<p>As we can see, using the <a id="_idIndexMarker561"/>predefined <code>LLJIT</code> class is convenient, but it can limit our flexibility. In the next section, we’ll look at how to implement a JIT compiler using the layers provided by the ORC API.</p>&#13;
			<h1 id="_idParaDest-151"><a id="_idTextAnchor155"/>Building a JIT compiler class from scratch</h1>&#13;
			<p>Using the layered <a id="_idIndexMarker562"/>approach of ORC, it is very easy to build a JIT compiler customized for the requirements. There is no one-size-fits-all JIT compiler, and the first section of this chapter gave some examples. Let’s have a look at how to set up a JIT compiler from scratch.</p>&#13;
			<p>The ORC API uses layers that are stacked together. The lowest level is the object-linking layer, represented by the <code>llvm::orc::RTDyldObjectLinkingLayer</code> class. It is responsible for linking in-memory objects and turning them into executable code. The memory required for this task is managed by an instance of the <code>MemoryManager</code> interface. There is a default implementation, but we can also use a custom version if we need.</p>&#13;
			<p>Above the object-linking layer is the compile layer, which is responsible for creating an in-memory object file. The <code>llvm::orc::IRCompileLayer</code> class takes an IR module as input and compiles it to an object file. The <code>IRCompileLayer</code> class is a subclass of the <code>IRLayer</code> class, which is a generic class for layer implementations accepting LLVM IR.</p>&#13;
			<p>Both of these layers already form the core of a JIT compiler: they add an LLVM IR module as input, which is compiled and linked in memory. To add extra functionality, we can incorporate more layers on top of both layers.</p>&#13;
			<p>For example, the <code>CompileOnDemandLayer</code> class splits a module so that only the requested functions are compiled. This can be used to implement lazy compilation. Moreover, the <code>CompileOnDemandLayer</code> class is also a subclass of the <code>IRLayer</code> class. In a very generic way, the <code>IRTransformLayer</code> class, also a subclass of the <code>IRLayer</code> class, allows us to apply a transformation to the module.</p>&#13;
			<p>Another important class is the <code>ExecutionSession</code> class. This class represents a running JIT program. Essentially, this means that the class manages <code>JITDylib</code> symbol tables, provides lookup functionality for symbols, and keeps track of used resource managers.</p>&#13;
			<p>The generic recipe for a JIT compiler is as follows:</p>&#13;
			<ol>&#13;
				<li>Initialize an instance of the <code>ExecutionSession</code> class.</li>&#13;
				<li>Initialize the layer, at least consisting of an <code>RTDyldObjectLinkingLayer</code> class and an <code>IRCompileLayer</code> class.</li>&#13;
				<li>Create the first <code>JITDylib</code> symbol table, usually with <code>main</code> or a similar name.</li>&#13;
			</ol>&#13;
			<p>The general usage of the JIT compiler is also very straightforward:</p>&#13;
			<ol>&#13;
				<li>Add an IR module to the symbol table.</li>&#13;
				<li>Look up a symbol, triggering the compilation of the associated function, and possibly the whole module.</li>&#13;
				<li>Execute <a id="_idIndexMarker563"/>the function.</li>&#13;
			</ol>&#13;
			<p>In the next subsection, we implement a JIT compiler class following the generic recipe.</p>&#13;
			<h2 id="_idParaDest-152"><a id="_idTextAnchor156"/>Creating a JIT compiler class</h2>&#13;
			<p>To keep the implementation of the<a id="_idIndexMarker564"/> JIT compiler class simple, everything is placed in <code>JIT.h</code>, within a source directory we can create called <code>jit</code>. However, the initialization of the class is a bit more complex compared to using <code>LLJIT</code>. Due to handling possible errors, we need a factory method to create some objects upfront before we can call the constructor. The steps to create the class are as follows:</p>&#13;
			<ol>&#13;
				<li>We begin with guarding the header file against multiple inclusion with the <code>JIT_H</code> preprocessor definition:<pre class="source-code">&#13;
#ifndef JIT_H&#13;
#define JIT_H</pre></li>				<li>Firstly, a number of <code>include</code> files are required. Most of them provide a class with the same name as the header file. The <code>Core.h</code> header provides a couple of basic classes, including the <code>ExecutionSession</code> class. Additionally, the <code>ExecutionUtils.h</code> header provides the <code>DynamicLibrarySearchGenerator</code> class to search libraries for symbols. Furthermore, the <code>CompileUtils.h</code> header provides the <code>ConcurrentIRCompiler</code> class:<pre class="source-code">&#13;
#include "llvm/Analysis/AliasAnalysis.h"&#13;
#include "llvm/ExecutionEngine/JITSymbol.h"&#13;
#include "llvm/ExecutionEngine/Orc/CompileUtils.h"&#13;
#include "llvm/ExecutionEngine/Orc/Core.h"&#13;
#include "llvm/ExecutionEngine/Orc/ExecutionUtils.h"&#13;
#include "llvm/ExecutionEngine/Orc/IRCompileLayer.h"&#13;
#include "llvm/ExecutionEngine/Orc/IRTransformLayer.h"&#13;
#include&#13;
     "llvm/ExecutionEngine/Orc/JITTargetMachineBuilder.h"&#13;
#include "llvm/ExecutionEngine/Orc/Mangling.h"&#13;
#include&#13;
    "llvm/ExecutionEngine/Orc/RTDyldObjectLinkingLayer.h"&#13;
#include&#13;
        "llvm/ExecutionEngine/Orc/TargetProcessControl.h"&#13;
#include "llvm/ExecutionEngine/SectionMemoryManager.h"&#13;
#include "llvm/Passes/PassBuilder.h"&#13;
#include "llvm/Support/Error.h"</pre></li>				<li>Declare a new<a id="_idIndexMarker565"/> class. Our new class will be called <code>JIT</code>:<pre class="source-code">&#13;
class JIT {</pre></li>				<li>The private data members reflect the ORC layers and some helper classes. The <code>ExecutionSession</code>, <code>ObjectLinkingLayer</code>, <code>CompileLayer</code>, <code>OptIRLayer</code>, and <code>MainJITDylib</code> instances represent the running JIT program, the layers, and the symbol table, as already described. Moreover, the <code>TargetProcessControl</code> instance is used for interaction with the JIT target process. This can be the same process, another process on the same machine, or a remote process on a different machine, possibly with a different architecture. The <code>DataLayout</code> and <code>MangleAndInterner</code> classes are required to mangle symbols’ names in the correct way. Additionally, the symbol names are internalized, which means that all equal names have the same address. This means that to check if two symbol names are equal, it is then sufficient to<a id="_idIndexMarker566"/> compare the addresses, which is a very fast operation:<pre class="source-code">&#13;
  std::unique_ptr&lt;llvm::orc::TargetProcessControl&gt; TPC;&#13;
  std::unique_ptr&lt;llvm::orc::ExecutionSession&gt; ES;&#13;
  llvm::DataLayout DL;&#13;
  llvm::orc::MangleAndInterner Mangle;&#13;
  std::unique_ptr&lt;llvm::orc::RTDyldObjectLinkingLayer&gt;&#13;
      ObjectLinkingLayer;&#13;
  std::unique_ptr&lt;llvm::orc::IRCompileLayer&gt;&#13;
      CompileLayer;&#13;
  std::unique_ptr&lt;llvm::orc::IRTransformLayer&gt;&#13;
      OptIRLayer;&#13;
  llvm::orc::JITDylib &amp;MainJITDylib;</pre></li>				<li>The initialization is split into three parts. In C++, a constructor cannot return an error. The simple and recommended solution is to create a static factory method that can do the error handling before constructing an object. The initialization of the layers is more complex, so we introduce factory methods for them, too.<p class="list-inset">In the <code>create()</code> factory method, we first create a <code>SymbolStringPool</code> instance, which is used to implement string internalization and is shared by several classes. To take control of the current process, we create a <code>SelfTargetProcessControl</code> instance. If we want to target a different process, then we need to change this instance.</p><p class="list-inset">Next, we <a id="_idIndexMarker567"/>construct a <code>JITTargetMachineBuilder</code> instance, for which we need to know the target triple of the JIT process. Afterward, we query the target machine builder for the data layout. This step can fail if the builder is not able to instantiate the target machine based on the provided triple – for example, because support for this target is not compiled into the LLVM libraries:</p><pre class="source-code">&#13;
public:&#13;
  static llvm::Expected&lt;std::unique_ptr&lt;JIT&gt;&gt; create() {&#13;
    auto SSP =&#13;
        std::make_shared&lt;llvm::orc::SymbolStringPool&gt;();&#13;
    auto TPC =&#13;
        llvm::orc::SelfTargetProcessControl::Create(SSP);&#13;
    if (!TPC)&#13;
      return TPC.takeError();&#13;
    llvm::orc::JITTargetMachineBuilder JTMB(&#13;
        (*TPC)-&gt;getTargetTriple());&#13;
    auto DL = JTMB.getDefaultDataLayoutForTarget();&#13;
    if (!DL)&#13;
      return DL.takeError();</pre></li>				<li>At this point, we have handled all calls that could potentially fail. We are now able to initialize the <code>ExecutionSession</code> instance. Finally, the constructor of the <code>JIT</code> class is called with all instantiated objects, and the result is returned to the caller:<pre class="source-code">&#13;
    auto ES =&#13;
        std::make_unique&lt;llvm::orc::ExecutionSession&gt;(&#13;
            std::move(SSP));&#13;
    return std::make_unique&lt;JIT&gt;(&#13;
        std::move(*TPC), std::move(ES), std::move(*DL),&#13;
        std::move(JTMB));&#13;
  }</pre></li>				<li>The constructor of the <code>JIT</code> class moves the passed parameters to the private data members. Layer objects<a id="_idIndexMarker568"/> are constructed with a call to static factory names with the <code>create</code> prefix. Furthermore, each layer factory method requires a reference to the <code>ExecutionSession</code> instance, which connects the layer to the running JIT session. Except for the object-linking layer, which is at the bottom of the layer stack, each layer requires a reference to the previous layer, illustrating the stacking order:<pre class="source-code">&#13;
JIT(std::unique_ptr&lt;llvm::orc::ExecutorProcessControl&gt;&#13;
          EPCtrl,&#13;
      std::unique_ptr&lt;llvm::orc::ExecutionSession&gt;&#13;
          ExeS,&#13;
      llvm::DataLayout DataL,&#13;
      llvm::orc::JITTargetMachineBuilder JTMB)&#13;
      : EPC(std::move(EPCtrl)), ES(std::move(ExeS)),&#13;
        DL(std::move(DataL)), Mangle(*ES, DL),&#13;
        ObjectLinkingLayer(std::move(&#13;
            createObjectLinkingLayer(*ES, JTMB))),&#13;
        CompileLayer(std::move(createCompileLayer(&#13;
            *ES, *ObjectLinkingLayer,&#13;
            std::move(JTMB)))),&#13;
        OptIRLayer(std::move(&#13;
            createOptIRLayer(*ES, *CompileLayer))),&#13;
        MainJITDylib(&#13;
            ES-&gt;createBareJITDylib("&lt;main&gt;")) {</pre></li>				<li>In the body of the constructor, we add a generator to search the current process for symbols. The <code>GetForCurrentProcess()</code> method is special, as the return value is wrapped in an <code>Expected&lt;&gt;</code> template, indicating that an <code>Error</code> object can also be returned. However, since we know that no error can occur, the current <a id="_idIndexMarker569"/>process will eventually run! Thus, we unwrap the result with the <code>cantFail()</code> function, which terminates the application if an error occurred anyway:<pre class="source-code">&#13;
    MainJITDylib.addGenerator(llvm::cantFail(&#13;
        llvm::orc::DynamicLibrarySearchGenerator::&#13;
            GetForCurrentProcess(DL.getGlobalPrefix())));&#13;
  }</pre></li>				<li>To create an object-linking layer, we need to provide a memory manager. Here, we stick to the default <code>SectionMemoryManager</code> class, but we could also provide a different implementation if needed:<pre class="source-code">&#13;
  static std::unique_ptr&lt;&#13;
      llvm::orc::RTDyldObjectLinkingLayer&gt;&#13;
  createObjectLinkingLayer(&#13;
      llvm::orc::ExecutionSession &amp;ES,&#13;
      llvm::orc::JITTargetMachineBuilder &amp;JTMB) {&#13;
    auto GetMemoryManager = []() {&#13;
      return std::make_unique&lt;&#13;
          llvm::SectionMemoryManager&gt;();&#13;
    };&#13;
    auto OLLayer = std::make_unique&lt;&#13;
        llvm::orc::RTDyldObjectLinkingLayer&gt;(&#13;
        ES, GetMemoryManager);</pre></li>				<li>A slight <a id="_idIndexMarker570"/>complication exists for the <strong class="bold">Common Object File Format</strong> (<strong class="bold">COFF</strong>) object file<a id="_idIndexMarker571"/> format, which is used on Windows. This file format does not allow functions to be marked as exported. This subsequently leads to failures in checks inside the object-linking layer: flags stored in the symbol are compared with the flags from IR, which leads to a mismatch because of the missing export marker. The solution is to override the flags only for this file format. This finishes the construction of the object layer, and the object is returned to the caller:<pre class="source-code">&#13;
    if (JTMB.getTargetTriple().isOSBinFormatCOFF()) {&#13;
      OLLayer&#13;
         -&gt;setOverrideObjectFlagsWithResponsibilityFlags(&#13;
              true);&#13;
      OLLayer&#13;
         -&gt;setAutoClaimResponsibilityForObjectSymbols(&#13;
              true);&#13;
    }&#13;
    return OLLayer;&#13;
  }</pre></li>				<li>To initialize the compiler layer, an <code>IRCompiler</code> instance is required. The <code>IRCompiler</code> instance is responsible for compiling an IR module into an object file. If our JIT compiler does not use threads, then we can use the <code>SimpleCompiler</code> class, which compiles the IR module using a given target machine. The <code>TargetMachine</code> class is not threadsafe, and therefore the <code>SimpleCompiler</code> class <a id="_idIndexMarker572"/>is not, either. To support compilation with multiple threads, we use the <code>ConcurrentIRCompiler</code> class, which creates a new <code>TargetMachine</code> instance for each module to compile. This approach solves the problem with multiple threads:<pre class="source-code">&#13;
  static std::unique_ptr&lt;llvm::orc::IRCompileLayer&gt;&#13;
  createCompileLayer(&#13;
      llvm::orc::ExecutionSession &amp;ES,&#13;
      llvm::orc::RTDyldObjectLinkingLayer &amp;OLLayer,&#13;
      llvm::orc::JITTargetMachineBuilder JTMB) {&#13;
    auto IRCompiler = std::make_unique&lt;&#13;
        llvm::orc::ConcurrentIRCompiler&gt;(&#13;
        std::move(JTMB));&#13;
    auto IRCLayer =&#13;
        std::make_unique&lt;llvm::orc::IRCompileLayer&gt;(&#13;
            ES, OLLayer, std::move(IRCompiler));&#13;
    return IRCLayer;&#13;
  }</pre></li>				<li>Instead of compiling the IR module directly to machine code, we install a layer that optimizes the IR first. This is a deliberate design decision: we turn our JIT compiler into an optimizing JIT compiler, which produces faster code that takes longer to produce, meaning a delay for the user. We do not add lazy compilation, so whole modules are compiled when just a symbol is looked up. This can add up to <a id="_idIndexMarker573"/>a significant amount of time before the user sees the code executing.</li>&#13;
			</ol>&#13;
			<p class="callout-heading">Note</p>&#13;
			<p class="callout">Introducing lazy compilation is not a proper solution in all circumstances. Lazy compilation is realized by moving each function into a module of its own, which is compiled when the function name is looked up. This prevents inter-procedural optimizations such as <em class="italic">inlining</em> because <a id="_idIndexMarker574"/>the inliner pass needs access to the body of called functions to inline them. As a result, users see a faster startup with lazy compilation, but the produced code is not as optimal as it can be. These design decisions depend on the intended use. Here, we decide on fast code, accepting a slower startup time. Furthermore, this means that the optimization layer is essentially a transformation layer.</p>&#13;
			<p class="list-inset">The <code>IRTransformLayer</code> class delegates the transformation to a function – in our case, to the <code>optimizeModule</code> function:</p>&#13;
			<pre class="source-code">&#13;
  static std::unique_ptr&lt;llvm::orc::IRTransformLayer&gt;&#13;
  createOptIRLayer(&#13;
      llvm::orc::ExecutionSession &amp;ES,&#13;
      llvm::orc::IRCompileLayer &amp;CompileLayer) {&#13;
    auto OptIRLayer =&#13;
        std::make_unique&lt;llvm::orc::IRTransformLayer&gt;(&#13;
            ES, CompileLayer,&#13;
            optimizeModule);&#13;
    return OptIRLayer;&#13;
  }</pre>			<ol>&#13;
				<li value="13">The <code>optimizeModule()</code> function is an example of a transformation on an IR module. The function gets the module to transform as a parameter and returns the transformed version of the IR module. Since the JIT compiler can potentially run with multiple <a id="_idIndexMarker575"/>threads, the IR module is wrapped in a <code>ThreadSafeModule</code> instance:<pre class="source-code">&#13;
  static llvm::Expected&lt;llvm::orc::ThreadSafeModule&gt;&#13;
  optimizeModule(&#13;
      llvm::orc::ThreadSafeModule TSM,&#13;
      const llvm::orc::MaterializationResponsibility&#13;
          &amp;R) {</pre></li>				<li>To optimize the IR, we recall some information from <a href="B19561_07.xhtml#_idTextAnchor117"><em class="italic">Chapter 7</em></a>, <em class="italic">Optimizing IR</em>, in the <em class="italic">Adding an optimization pipeline to your compiler</em> section. We need a <code>PassBuilder</code> instance to create an optimization pipeline. First, we define a couple of analysis managers and register them afterward at the pass builder. Afterward, we populate a <code>ModulePassManager</code> instance with the default optimization pipeline for the <code>O2</code> level. This is again a design decision: the <code>O2</code> level produces already fast machine code, but it produces even faster code at the <code>O3</code> level. Next, we run the pipeline on the module, and finally, the optimized module is returned to the caller:<pre class="source-code">&#13;
    TSM.withModuleDo([](llvm::Module &amp;M) {&#13;
      bool DebugPM = false;&#13;
      llvm::PassBuilder PB(DebugPM);&#13;
      llvm::LoopAnalysisManager LAM(DebugPM);&#13;
      llvm::FunctionAnalysisManager FAM(DebugPM);&#13;
      llvm::CGSCCAnalysisManager CGAM(DebugPM);&#13;
      llvm::ModuleAnalysisManager MAM(DebugPM);&#13;
      FAM.registerPass(&#13;
          [&amp;] { return PB.buildDefaultAAPipeline(); });&#13;
      PB.registerModuleAnalyses(MAM);&#13;
      PB.registerCGSCCAnalyses(CGAM);&#13;
      PB.registerFunctionAnalyses(FAM);&#13;
      PB.registerLoopAnalyses(LAM);&#13;
      PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);&#13;
      llvm::ModulePassManager MPM =&#13;
          PB.buildPerModuleDefaultPipeline(&#13;
              llvm::PassBuilder::OptimizationLevel::O2,&#13;
              DebugPM);&#13;
      MPM.run(M, MAM);&#13;
    });&#13;
    return TSM;&#13;
  }</pre></li>				<li>The client of the <code>JIT</code> class needs a way to add an IR module, which we provide with the <code>addIRModule()</code> function. Recall the layer stack we created: we must add the IR module to the top layer; otherwise, we would accidentally bypass some of the layers. This would be a programming error that is not easily spotted: if the <code>OptIRLayer</code> member is replaced by the <code>CompileLayer</code> member, then our <code>JIT</code> class <a id="_idIndexMarker576"/>still works, but not as an optimizing JIT because we have bypassed this layer. This is no concern for this small implementation, but in a large JIT optimization, we would introduce a function to return the top-level layer:<pre class="source-code">&#13;
  llvm::Error addIRModule(&#13;
      llvm::orc::ThreadSafeModule TSM,&#13;
      llvm::orc::ResourceTrackerSP RT = nullptr) {&#13;
    if (!RT)&#13;
      RT = MainJITDylib.getDefaultResourceTracker();&#13;
    return OptIRLayer-&gt;add(RT, std::move(TSM));&#13;
  }</pre></li>				<li>Likewise, a client of our JIT class needs a way to look up a symbol. We delegate this to the <code>ExecutionSession</code> instance, passing in a reference to the main symbol table and the mangled and internalized name of the requested symbol:<pre class="source-code">&#13;
  llvm::Expected&lt;llvm::orc::ExecutorSymbolDef&gt;&#13;
  lookup(llvm::StringRef Name) {&#13;
    return ES-&gt;lookup({&amp;MainJITDylib},&#13;
                      Mangle(Name.str()));&#13;
  }</pre></li>			</ol>&#13;
			<p>As we can see, the <a id="_idIndexMarker577"/>initialization of this JIT class can be tricky, as it involves a factory method and a constructor call for the <code>JIT</code> class, and factory methods for each layer. Although this distribution is caused by limitations in C++, the code itself is straightforward.</p>&#13;
			<p>Next, we are going to use the new JIT compiler class to implement a simple command-line utility that takes an LLVM IR file as input.</p>&#13;
			<h2 id="_idParaDest-153"><a id="_idTextAnchor157"/>Using our new JIT compiler class</h2>&#13;
			<p>We start off by <a id="_idIndexMarker578"/>creating a file called <code>JIT.cpp</code>, in the same directory as the <code>JIT.h</code> file, and add the following to this source file:</p>&#13;
			<ol>&#13;
				<li>Firstly, several header files are included. We must include <code>JIT.h</code> to use our new class, and the <code>IRReader.h</code> header because it defines a function to read LLVM IR files. The <code>CommandLine.h</code> header allows us to parse the command-line options in the LLVM style. Next, <code>InitLLVM.h</code> is needed for the basic initialization of the tool. Finally, <code>TargetSelect.h</code> is needed for the initialization of the native target:<pre class="source-code">&#13;
#include "JIT.h"&#13;
#include "llvm/IRReader/IRReader.h"&#13;
#include "llvm/Support/CommandLine.h"&#13;
#include "llvm/Support/InitLLVM.h"&#13;
#include "llvm/Support/TargetSelect.h"</pre></li>				<li>Next, we add the <code>llvm</code> namespace to the current scope:<pre class="source-code">&#13;
using namespace llvm;</pre></li>				<li>Our JIT tool expects exactly one input file on the command line, which we declare with the <code>cl::opt&lt;&gt;</code> class:<pre class="source-code">&#13;
static cl::opt&lt;std::string&gt;&#13;
    InputFile(cl::Positional, cl::Required,&#13;
              cl::desc("&lt;input-file&gt;"));</pre></li>				<li>To read the IR file, we call the <code>parseIRFile()</code> function. The file can be a textual IR representation or a bitcode file. The function returns a pointer to the created module. Additionally, the error handling is a bit different, because a textual IR file <a id="_idIndexMarker579"/>can be parsed, which is not necessarily syntactically correct. Finally, the <code>SMDiagnostic</code> instance holds the error information in case of a syntax error. In the event of an error, an error message is printed, and the application is exited:<pre class="source-code">&#13;
std::unique_ptr&lt;Module&gt;&#13;
loadModule(StringRef Filename, LLVMContext &amp;Ctx,&#13;
           const char *ProgName) {&#13;
  SMDiagnostic Err;&#13;
  std::unique_ptr&lt;Module&gt; Mod =&#13;
      parseIRFile(Filename, Err, Ctx);&#13;
  if (!Mod.get()) {&#13;
    Err.print(ProgName, errs());&#13;
    exit(-1);&#13;
  }&#13;
  return Mod;&#13;
}</pre></li>				<li>The <code>jitmain()</code> function is placed after the <code>loadModule()</code> method. This function sets up our JIT engine and compiles an LLVM IR module. The function needs the LLVM module with the IR to execute. The LLVM context class is also required for this module because the context class contains important type information. The <a id="_idIndexMarker580"/>goal is to call the <code>main()</code> function, so we also pass the usual <code>argc</code> and <code>argv</code> parameters:<pre class="source-code">&#13;
Error jitmain(std::unique_ptr&lt;Module&gt; M,&#13;
              std::unique_ptr&lt;LLVMContext&gt; Ctx,&#13;
              int argc, char *argv[]) {</pre></li>				<li>Next, we create an instance of our JIT class that we constructed earlier. If an error occurs, then we return an error message accordingly:<pre class="source-code">&#13;
  auto JIT = JIT::create();&#13;
  if (!JIT)&#13;
    return JIT.takeError();</pre></li>				<li>Then, we add the module to the main <code>JITDylib</code> instance, wrapping the module and a context in a <code>ThreadSafeModule</code> instance yet again. If an error occurs, then we return an error message:<pre class="source-code">&#13;
  if (auto Err = (*JIT)-&gt;addIRModule(&#13;
          orc::ThreadSafeModule(std::move(M),&#13;
                                std::move(Ctx))))&#13;
    return Err;</pre></li>				<li>Following this, we look up the <code>main</code> symbol. This symbol must be in the IR module given on the command line. The lookup triggers the compilation of that IR module. If other symbols are referenced inside the IR module, then they are resolved using the<a id="_idIndexMarker581"/> generator added in the previous step. The result is of the <code>ExecutorAddr</code> class, where it represents the address of the executor process:<pre class="source-code">&#13;
  llvm::orc::ExecutorAddr MainExecutorAddr = MainSym-&gt;getAddress();&#13;
  auto *Main = MainExecutorAddr.toPtr&lt;int(int, char**)&gt;();</pre></li>				<li>Now, we can call the <code>main()</code> function in the IR module, and pass the <code>argc</code> and <code>argv</code> parameters that the function expects. We ignore the return value:<pre class="source-code">&#13;
  (void)Main(argc, argv);</pre></li>				<li>We report success after the execution of the function:<pre class="source-code">&#13;
  return Error::success();&#13;
}</pre></li>				<li>After implementing a <code>jitmain()</code> function, we add a <code>main()</code> function, which initializes the tool and the native target and parses the command line:<pre class="source-code">&#13;
int main(int argc, char *argv[]) {&#13;
  InitLLVM X(argc, argv);&#13;
  InitializeNativeTarget();&#13;
  InitializeNativeTargetAsmPrinter();&#13;
  InitializeNativeTargetAsmParser();&#13;
  cl::ParseCommandLineOptions(argc, argv, "JIT\n");</pre></li>				<li>Afterward, the LLVM context class is initialized, and we load the IR module named on the command line:<pre class="source-code">&#13;
  auto Ctx = std::make_unique&lt;LLVMContext&gt;();&#13;
  std::unique_ptr&lt;Module&gt; M =&#13;
      loadModule(InputFile, *Ctx, argv[0]);</pre></li>				<li>After loading the<a id="_idIndexMarker582"/> IR module, we can call the <code>jitmain()</code> function. To handle errors, we use the <code>ExitOnError</code> utility class to print an error message and exit the application when an error is encountered. We also set a banner with the name of the application, which is printed before the error message:<pre class="source-code">&#13;
  ExitOnError ExitOnErr(std::string(argv[0]) + ": ");&#13;
  ExitOnErr(jitmain(std::move(M), std::move(Ctx),&#13;
                    argc, argv));</pre></li>				<li>If the control flow reaches this point, then the IR was successfully executed. We return <code>0</code> to indicate success:<pre class="source-code">&#13;
  return 0;&#13;
}</pre></li>			</ol>&#13;
			<p>We can now test our newly implemented JIT compiler by compiling a simple example that prints <code>Hello World!</code> to the console. Under the hood, the new class uses a fixed optimization level, so with large enough modules, we can note differences in the startup and runtime.</p>&#13;
			<p>To build our JIT compiler, we can follow the same CMake steps as we did near the end of the <em class="italic">Implementing our own JIT compiler with LLJIT</em> section, and we just need to ensure that the <code>JIT.cpp</code> source file is being compiled with the correct libraries to link against:</p>&#13;
			<pre class="source-code">&#13;
add_executable(JIT JIT.cpp)&#13;
include_directories(${CMAKE_SOURCE_DIR})&#13;
target_link_libraries(JIT ${llvm_libs})</pre>			<p>We then change into the <code>build</code> directory and compile the application:</p>&#13;
			<pre class="console">&#13;
$ cmake –G Ninja &lt;path to jit source directory&gt;&#13;
$ ninja</pre>			<p>Our <code>JIT</code> tool is now ready to be used. A simple <code>Hello World!</code> program can be written in C, like the following:</p>&#13;
			<pre class="console">&#13;
$ cat main.c&#13;
#include &lt;stdio.h&gt;&#13;
int main(int argc, char** argv) {&#13;
  printf("Hello world!\n");&#13;
  return 0;&#13;
}</pre>			<p>Next, we can compile the Hello World C source into LLVM IR with the following command:</p>&#13;
			<pre class="console">&#13;
$ clang -S -emit-llvm main.c</pre>			<p>Remember – we compile<a id="_idIndexMarker583"/> the C source into LLVM IR because our JIT compiler accepts an IR file as input. Finally, we can invoke our JIT compiler with our IR example, as follows:</p>&#13;
			<pre class="console">&#13;
$ JIT main.ll&#13;
Hello world!</pre>			<h1 id="_idParaDest-154"><a id="_idTextAnchor158"/>Summary</h1>&#13;
			<p>In this chapter, you learned how to develop a JIT compiler. You began with learning about the possible applications of JIT compilers, and you explored <code>lli</code>, the LLVM dynamic compiler and interpreter. Using the predefined <code>LLJIT</code> class, you built an interactive JIT-based calculator tool and learned about looking up symbols and adding IR modules to <code>LLJIT</code>. To be able to take advantage of the layered structure of the ORC API, you also implemented an optimizing <code>JIT</code> class.</p>&#13;
			<p>In the next chapter, you will learn how to utilize LLVM tools for debugging purposes.</p>&#13;
		</p>&#13;
	</div></body></html>