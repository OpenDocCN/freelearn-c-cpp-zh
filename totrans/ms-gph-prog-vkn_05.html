<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-76"><a id="_idTextAnchor075"/>5</h1>
<h1 id="_idParaDest-77"><a id="_idTextAnchor076"/>Unlocking Async Compute</h1>
<p>In this chapter, we are going to improve our renderer by allowing compute work to be done in parallel with graphics tasks. So far, we have been recording and submitting all of our work to a single queue. We can still submit compute tasks to this queue to be executed alongside graphics work: in this chapter, for instance, we have started using a compute shader for the fullscreen lighting rendering pass. We don’t need a separate queue in this case as we want to reduce the amount of synchronization between separate queues.</p>
<p>However, it might be beneficial to run other compute workloads on a separate queue and allow the GPU to fully utilize its compute units. In this chapter, we are going to implement a simple cloth simulation using compute shaders that will run on a separate compute queue. To unlock this new functionality, we will need to make some changes to our engine.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>Using a single timeline semaphore to avoid multiple fences</li>
<li>Adding a separate queue for async compute</li>
<li>Implementing cloth simulation using async compute</li>
</ul>
<h1 id="_idParaDest-78"><a id="_idTextAnchor077"/>Technical requirements</h1>
<p>The code for this chapter can be found at the following URL: <a href="https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter5">https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter5</a></p>
<h1 id="_idParaDest-79"><a id="_idTextAnchor078"/>Replacing multiple fences with a single timeline semaphore</h1>
<p>In this section, we are going to explain how fences and semaphores are currently used in our <a id="_idIndexMarker237"/>renderer and how to reduce <a id="_idIndexMarker238"/>the number of objects we must use by taking advantage of timeline semaphores.</p>
<p>Our engine already supports rendering multiple frames in parallel using fences. Fences must be used to ensure the GPU has finished using resources for a given frame. This is accomplished by waiting on the CPU before submitting a new batch of commands to the GPU.</p>
<div><div><img alt="Figure 5.1 – The CPU is working on the current frame while the GPU is rendering the previous frame" height="244" src="img/B18395_05_01.jpg" width="718"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – The CPU is working on the current frame while the GPU is rendering the previous frame</p>
<p>There is a downside, however; we need to create a fence for each frame in flight. This means we will have to manage at least two fences for double buffering and three if we want to support triple buffering.</p>
<p>We also need multiple semaphores to ensure the GPU waits for certain operations to complete before moving on. For instance, we need to signal a semaphore once rendering is complete and pass that same semaphore to the present command. This is needed to guarantee that rendering is complete before we try to present the swap chain image.</p>
<p>The following diagram illustrates two scenarios; in the first one, no semaphore is present, and the swapchain image could be presented to the screen while rendering is still in progress.</p>
<p>In the second scenario, we have added a semaphore that is signaled in the render submission and is waited on before presenting. This ensures the correct behavior of the application. If we didn’t have this semaphore, we would risk presenting an image that is still being rendered and displaying corrupted data.</p>
<div><div><img alt="Figure 5.2 – Two scenarios illustrating the need for a semaphore between rendering and presentation" height="564" src="img/B18395_05_02.jpg" width="829"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Two scenarios illustrating the need for a semaphore between rendering and presentation</p>
<p>The situation worsens when we start to consider multiple queues. In this chapter, we are <a id="_idIndexMarker239"/>going to add a separate <a id="_idIndexMarker240"/>compute queue. This means that we will need to add more fences to wait on the CPU for compute work to complete. We will also need new semaphores to synchronize the compute and graphics queue to ensure the data produced by the compute queue is ready to be used by the graphics queue.</p>
<p>Even if we weren’t using a compute queue, we might want to break our rendering work into multiple submissions. Each submission would need its own signal and wait for semaphores according to the dependencies of each workload. This can get out of hand quickly for large scenes that have tens, possibly hundreds, of submissions.</p>
<p>Luckily for us, there is a solution. If we think about it, the fence and the semaphore hold the same information; they get signaled once a submission is complete. What if there was a way to use a single object both on the CPU and the GPU? This exact functionality is provided by a timeline semaphore.</p>
<p>As the <a id="_idIndexMarker241"/>name suggests, a <a id="_idIndexMarker242"/>timeline semaphore holds a monotonically increasing value. We can define what value we want the semaphore to be signaled with and what value we want to wait for. This object can be waited on by both the GPU and the CPU, greatly reducing the number of objects needed to implement correct synchronization.</p>
<p>We are now going to show how to use timeline semaphores in Vulkan.</p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor079"/>Enabling the timeline semaphore extension</h2>
<p>The timeline semaphore feature has been promoted to core in Vulkan 1.2. However, it’s not a <a id="_idIndexMarker243"/>mandatory extension, so we first need to query for support before using it. This is done, as usual, by enumerating the extension the device exposes and looking for the extension name:</p>
<pre class="source-code">
vkEnumerateDeviceExtensionProperties( 
    vulkan_physical_device, nullptr, 
        &amp;device_extension_count, extensions );
for ( size_t i = 0; i &lt; device_extension_count; i++ ) {
    if ( !strcmp( extensions[ i ].extensionName, 
         VK_KHR_TIMELINE_SEMAPHORE_EXTENSION_NAME ) ) {
             timeline_semaphore_extension_present = true;
             continue;
         }
}</pre>
<p>If the extension is present, we need to populate an additional structure that will be used at device creation, as shown in the following code:</p>
<pre class="source-code">
VkPhysicalDeviceFeatures2 physical_features2 { 
VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_FEATURES_2 };
void* current_pnext = nullptr;
VkPhysicalDeviceTimelineSemaphoreFeatures timeline_sempahore_features{ VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_TIMELINE_SEMAPHORE_FEATURES  };
if ( timeline_semaphore_extension_present ) {
    timeline_sempahore_features.pNext = current_pnext;
    current_pnext = &amp;timeline_sempahore_features;
}
physical_features2.pNext = current_pnext;
vkGetPhysicalDeviceFeatures2( vulkan_physical_device, 
    &amp;physical_features2 );</pre>
<p>We also <a id="_idIndexMarker244"/>need to add the extension name to the list of enabled extensions:</p>
<pre class="source-code">
if ( timeline_semaphore_extension_present ) {
    device_extensions.push( 
        VK_KHR_TIMELINE_SEMAPHORE_EXTENSION_NAME );
}</pre>
<p>Finally, we use the data we just retrieved when creating the device:</p>
<pre class="source-code">
VkDeviceCreateInfo device_create_info { 
    VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO };
device_create_info.enabledExtensionCount = 
    device_extensions.size;  
device_create_info.ppEnabledExtensionNames = 
    device_extensions.data; 
device_create_info.pNext = &amp;physical_features2;
vkCreateDevice( vulkan_physical_device, 
    &amp;device_create_info, vulkan_allocation_callbacks, 
        &amp;vulkan_device );</pre>
<p>We are <a id="_idIndexMarker245"/>now ready to use a timeline semaphore in our code! We will see how to create a timeline semaphore in the next section.</p>
<h2 id="_idParaDest-81"><a id="_idTextAnchor080"/>Creating a timeline semaphore</h2>
<p>Creating a <a id="_idIndexMarker246"/>timeline semaphore is quite simple. We start by defining the standard creation structure:</p>
<pre class="source-code">
VkSemaphoreCreateInfo semaphore_info{ 
    VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO };</pre>
<p>We then need to pass an extra structure to tell the API that we want to create a timeline semaphore:</p>
<pre class="source-code">
VkSemaphoreTypeCreateInfo semaphore_type_info{ 
    VK_STRUCTURE_TYPE_SEMAPHORE_TYPE_CREATE_INFO };
semaphore_type_info.semaphoreType = 
    VK_SEMAPHORE_TYPE_TIMELINE;
semaphore_info.pNext = &amp;semaphore_type_info;
Finally, we call the create function:
vkCreateSemaphore( vulkan_device, &amp;semaphore_info, 
    vulkan_allocation_callbacks, &amp;vulkan_timeline_semaphore );</pre>
<p>This is it! We now have a timeline semaphore that can be used in our renderer. In the next section, we will look at a few examples of how to use this type of semaphore.</p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor081"/>Waiting for a timeline semaphore on the CPU</h2>
<p>As mentioned <a id="_idIndexMarker247"/>previously, we can wait for a timeline semaphore to be signaled on the CPU. The following code does just that:</p>
<pre class="source-code">
u64 timeline_value = …;
 
VkSemaphoreWaitInfo semaphore_wait_info{ 
    VK_STRUCTURE_TYPE_SEMAPHORE_WAIT_INFO };
semaphore_wait_info.semaphoreCount = 1;
semaphore_wait_info.pSemaphores = 
    &amp;vulkan_timeline_semaphore;
semaphore_wait_info.pValues = &amp;timeline_value;
 
vkWaitSemaphores( vulkan_device, &amp;semaphore_wait_info, 
                  timeout );</pre>
<p>As you probably noticed, it’s possible to wait for multiple semaphores at once and specify a different value for each semaphore. This could be useful, for instance, when rendering to multiple windows, and each window uses a different semaphore. The <code>VkSemaphoreWaitInfo</code> structure also has a <code>flags</code> field.</p>
<p>Using the <code>VK_SEMAPHORE_WAIT_ANY_BIT</code> value in this field will terminate the wait as soon as one of the semaphores reaches the value we are waiting for. Otherwise, the wait will terminate only when all semaphores have reached their respective value.</p>
<p>The last important aspect of the preceding code is the timeout value. This value is specified in nanoseconds. If, after the given time, the wait condition is not satisfied, the call will return <code>VK_TIMEOUT</code>. We usually set the timeout to infinity, as we absolutely need the semaphore to be signaled.</p>
<p>However, there is a risk that the wait call might never return, for instance, if the combination of wait and signal values leads to a deadlock on the GPU. An alternative approach <a id="_idIndexMarker248"/>would be to set the timeout to a relatively large value – 1 second, for example. If the wait is not completed within this time span, there is likely an issue with our submission, and we can communicate the error to the user.</p>
<p>In this section, we have shown how to wait for a timeline semaphore on the CPU. In the next section, we are going to cover how to use a timeline semaphore on the GPU.</p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor082"/>Using a timeline semaphore on the GPU</h2>
<p>In this section, we are going to show how to update a timeline semaphore value and how to <a id="_idIndexMarker249"/>wait for a given value on the GPU.</p>
<p class="callout-heading">Note</p>
<p class="callout">Before we begin, we’d like to point out that we are using the <code>VK_KHR_synchronization2</code> extension. This extension simplifies writing code for barriers and semaphores. Please refer to the full code to see how this is implemented using the old APIs.</p>
<p>We start by defining the list of semaphores we want to wait for:</p>
<pre class="source-code">
VkSemaphoreSubmitInfoKHR wait_semaphores[]{
    { VK_STRUCTURE_TYPE_SEMAPHORE_SUBMIT_INFO_KHR, nullptr, 
       vulkan_image_acquired_semaphore, 0, 
       VK_PIPELINE_STAGE_2_COLOR_ATTACHMENT_OUTPUT_BIT_KHR,
       0 },
    { VK_STRUCTURE_TYPE_SEMAPHORE_SUBMIT_INFO_KHR, nullptr, 
       vulkan_timeline_semaphore, absolute_frame - ( 
       k_max_frames - 1 ), 
       VK_PIPELINE_STAGE_2_TOP_OF_PIPE_BIT_KHR , 0 }
};</pre>
<p>This list can contain both standard semaphores and timeline semaphores. For standard semaphores, the <code>signal</code> value is ignored.</p>
<p>Similarly, we need to define a list of semaphores to wait on:</p>
<pre class="source-code">
VkSemaphoreSubmitInfoKHR signal_semaphores[]{
    { VK_STRUCTURE_TYPE_SEMAPHORE_SUBMIT_INFO_KHR, nullptr, 
       *render_complete_semaphore, 0, 
       VK_PIPELINE_STAGE_2_COLOR_ATTACHMENT_OUTPUT_BIT_KHR, 
       0 },
    { VK_STRUCTURE_TYPE_SEMAPHORE_SUBMIT_INFO_KHR, nullptr, 
       vulkan_timeline_semaphore, absolute_frame + 1, 
       VK_PIPELINE_STAGE_2_COLOR_ATTACHMENT_OUTPUT_BIT_KHR 
       , 0 }
};</pre>
<p>As before, we can <a id="_idIndexMarker250"/>use different semaphore types and the signal value is ignored for standard semaphores. It’s important the signal value for a timeline semaphore is always increased. If we were to submit the same value twice or a smaller value, we would get a validation error.</p>
<p>We also need to be careful with the values we use for waiting and signaling. If we were to wait for a value that is set within the same submission, we would deadlock the GPU. As a rule of thumb, always try to use a value that is guaranteed to have been set by a previous submission. The validation layers will also help you catch this type of error.</p>
<p>The last step is to pass the two lists to the submit info structure:</p>
<pre class="source-code">
VkSubmitInfo2KHR submit_info{ 
    VK_STRUCTURE_TYPE_SUBMIT_INFO_2_KHR };
submit_info.waitSemaphoreInfoCount = 2;
submit_info.pWaitSemaphoreInfos = wait_semaphores;
submit_info.commandBufferInfoCount = 
    num_queued_command_buffers;
submit_info.pCommandBufferInfos = command_buffer_info;
submit_info.signalSemaphoreInfoCount = 2;
submit_info.pSignalSemaphoreInfos = signal_semaphores;
 
queue_submit2( vulkan_main_queue, 1, &amp;submit_info, 
    VK_NULL_HANDLE );</pre>
<p>As you <a id="_idIndexMarker251"/>probably noticed, we can now wait for and signal the same timeline semaphore in a submission. We also no longer need a fence. This greatly simplifies the code and reduces the number of synchronization objects needed.</p>
<p>In this section, we have shown how to enable the extension to use timeline semaphores and how to create and use them to wait on the CPU. Finally, we have shown how to wait and signal timeline semaphores on the GPU.</p>
<p>In the next section, we are going to use this newly acquired knowledge to add a separate queue for async compute work.</p>
<h1 id="_idParaDest-84"><a id="_idTextAnchor083"/>Adding a separate queue for async compute</h1>
<p>In this section, we are going to illustrate how to use separate queues for graphics and compute <a id="_idIndexMarker252"/>work to make full use of our GPU. Modern <a id="_idIndexMarker253"/>GPUs have many generic compute units that can be used both for graphics and compute work. Depending on the workload for a given frame (shader complexity, screen resolution, dependencies between rendering passes, and so on), it’s possible that the GPU might not be fully utilized.</p>
<p>Moving some of the computation done on the CPU to the GPU using compute shaders can increase performance and lead to better GPU utilization. This is possible because the GPU scheduler can determine if any of the compute units are idle and assign work to them to overlap existing work:</p>
<div><div><img alt="Figure 5.3 – Top: graphics workload is not fully utilizing the GPU; Bottom: compute workload can take advantage of unused resources for optimal GPU utilization" height="449" src="img/B18395_05_03.jpg" width="1106"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Top: graphics workload is not fully utilizing the GPU; Bottom: compute workload can take advantage of unused resources for optimal GPU utilization</p>
<p>In the <a id="_idIndexMarker254"/>remainder of this section, we are going to demonstrate <a id="_idIndexMarker255"/>how to use the timeline semaphore introduced in the previous section to synchronize access to data between the two queues.</p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor084"/>Submitting work on separate queues</h2>
<p>We have already set up multiple queues in <a href="B18395_03.xhtml#_idTextAnchor045"><em class="italic">Chapter 3</em></a>, <em class="italic">Unlocking Multi-Threading</em>. We now need <a id="_idIndexMarker256"/>to ensure that access to data from two queues is correctly synchronized; otherwise, we might access data that is out of date or, worse, data that hasn’t been initialized yet.</p>
<p>The first step in this process is to create a separate command buffer. A different command buffer must be used for compute work, as the same command buffer can’t be submitted to different queues. This is easily achieved by requesting a new command buffer from our <code>GpuDevice</code> implementation:</p>
<pre class="source-code">
CommandBuffer* cb = gpu.get_command_buffer( 0, 
gpu.current_frame, true );</pre>
<p>Next, we need to create a new timeline semaphore to be used by the compute queue. This is the same code we have shown in the previous section, and we won’t be duplicating it here.</p>
<p>We then need <a id="_idIndexMarker257"/>to increment the value of our timeline semaphore with each compute submission:</p>
<pre class="source-code">
bool has_wait_semaphore = last_compute_semaphore_value &gt; 0;
VkSemaphoreSubmitInfoKHR wait_semaphores[]{
    { VK_STRUCTURE_TYPE_SEMAPHORE_SUBMIT_INFO_KHR, nullptr, 
       vulkan_compute_semaphore, 
       last_compute_semaphore_value, 
       VK_PIPELINE_STAGE_2_COMPUTE_SHADER_BIT_KHR, 0 }
};
 
last_compute_semaphore_value++;
 
VkSemaphoreSubmitInfoKHR signal_semaphores[]{
    { VK_STRUCTURE_TYPE_SEMAPHORE_SUBMIT_INFO_KHR, nullptr, 
       vulkan_compute_semaphore, 
       last_compute_semaphore_value, 
       VK_PIPELINE_STAGE_2_COMPUTE_SHADER_BIT_KHR, 0 },
};</pre>
<p>This code is similar to the code we showed before in relation to submitting timeline semaphores. The main difference is the wait stage, which must now be <code>VK_PIPELINE_STAGE_2_COMPUTE_SHADER_BIT_KHR</code>. Now that we have the list of wait and signal semaphores, they are ready to be used for our submission:</p>
<pre class="source-code">
VkCommandBufferSubmitInfoKHR command_buffer_info{ 
    VK_STRUCTURE_TYPE_COMMAND_BUFFER_SUBMIT_INFO_KHR };
command_buffer_info.commandBuffer = 
    command_buffer-&gt;vk_command_buffer;
 
VkSubmitInfo2KHR submit_info{ 
    VK_STRUCTURE_TYPE_SUBMIT_INFO_2_KHR };
submit_info.waitSemaphoreInfoCount = 
    has_wait_semaphore ? 1 : 0;
submit_info.pWaitSemaphoreInfos = wait_semaphores;
submit_info.commandBufferInfoCount = 1;
submit_info.signalSemaphoreInfoCount = 1;
submit_info.pSignalSemaphoreInfos = signal_semaphores;
 
queue_submit2( vulkan_compute_queue, 1, &amp;submit_info, 
    VK_NULL_HANDLE );</pre>
<p>Again, this should be familiar code. We want to highlight that we only add the wait semaphore <a id="_idIndexMarker258"/>after the first submission. If we were to wait for the semaphore on the first submission, we would deadlock the GPU, as the semaphore will never be signaled. Luckily, the validation layers will highlight this problem, and it can be easily corrected.</p>
<p>Now that we have submitted our compute workload, we need to make sure the graphics queue waits until the data is ready. We can achieve this by adding the compute semaphore to the list of wait semaphores when submitting the graphics queue. We are going to highlight only the new code:</p>
<pre class="source-code">
bool wait_for_compute_semaphore = ( 
    last_compute_semaphore_value &gt; 0 ) &amp;&amp; has_async_work; 
VkSemaphoreSubmitInfoKHR wait_semaphores[]{
    { VK_STRUCTURE_TYPE_SEMAPHORE_SUBMIT_INFO_KHR, nullptr, 
       vulkan_image_acquired_semaphore, 0, 
       VK_PIPELINE_STAGE_2_COLOR_ATTACHMENT_OUTPUT_BIT_KHR, 
       0 },
    { VK_STRUCTURE_TYPE_SEMAPHORE_SUBMIT_INFO_KHR, nullptr, 
       vulkan_compute_semaphore, 
       last_compute_semaphore_value, 
       VK_PIPELINE_STAGE_2_VERTEX_ATTRIBUTE_INPUT_BIT_KHR, 
       0 },
    { VK_STRUCTURE_TYPE_SEMAPHORE_SUBMIT_INFO_KHR, nullptr, 
       vulkan_graphics_semaphore, 
       absolute_frame - ( k_max_frames - 1 ), 
       VK_PIPELINE_STAGE_2_TOP_OF_PIPE_BIT_KHR , 0 },
};</pre>
<p>The same <a id="_idIndexMarker259"/>care must be taken when adding the compute semaphore to the list. We want to wait only if at least one compute submission has been performed. For some frames, we might not have any compute work pending. We don’t want to wait for the compute semaphore in this case, either.</p>
<p>In our case, we have set the wait stage to <code>VK_PIPELINE_STAGE_2_VERTEX_ATTRIBUTE_INPUT_BIT_KHR</code>, as we are modifying the vertices of our mesh. This will need adjusting if, for instance, you are using the compute queue to update a texture that won’t be used until the fragment shader stage. Using the right wait stage is important to obtain the best performance.</p>
<p>In this section, we have demonstrated how to retrieve a separate queue for compute work. We then explained how to use the newly created queue to submit compute work and correctly synchronize data access from different queues to ensure correct results.</p>
<p>In the next section, we are going to show a concrete example by implementing a simple cloth simulation using compute shaders.</p>
<h1 id="_idParaDest-86"><a id="_idTextAnchor085"/>Implementing cloth simulation using async compute</h1>
<p>In this section, we are going to implement a simple cloth simulation on the GPU as an example <a id="_idIndexMarker260"/>use case of a compute <a id="_idIndexMarker261"/>workload. We start by explaining why running some tasks on the GPU might be beneficial. Next, we provide an overview of compute shaders. Finally, we show how to port code from the CPU to the GPU and highlight some of the differences between the two platforms.</p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor086"/>Benefits of using compute shaders</h2>
<p>In the past, physics simulations mainly ran on the CPU. GPUs only had enough compute capacity for <a id="_idIndexMarker262"/>graphics work, and most stages in the pipeline were implemented by dedicated hardware blocks that could only perform one task. As GPUs evolved, pipeline stages moved to generic compute blocks that could perform different tasks.</p>
<p>This increase both in flexibility and compute capacity has allowed engine developers to move some workloads on the GPU. Aside from raw performance, running some computations on the GPU avoids expensive copies from CPU memory to GPU memory. Memory speed hasn’t evolved as fast as processor speed, and moving data as little as possible between devices is key to application performance.</p>
<p>In our example, the cloth simulation has to update the position of all vertices and copy the updated data to the GPU. Depending on the size of the mesh and the number of meshes to update, this could amount to a significant percentage of frame time.</p>
<p>These workloads can also scale better on the GPU, as we can update a larger number of meshes in parallel.</p>
<p>We are now going to provide an overview of how compute shaders are executed. If you are familiar with compute shaders or have worked with CUDA or OpenCL before, feel free to skim the next section.</p>
<h2 id="_idParaDest-88"><a id="_idTextAnchor087"/>Compute shaders overview</h2>
<p>The GPU <a id="_idIndexMarker263"/>execution model is called <strong class="bold">Single Instruction, Multiple Threads</strong> (<strong class="bold">SIMT</strong>). It <a id="_idIndexMarker264"/>is similar to the <strong class="bold">Single Instruction, Multiple Data</strong> (<strong class="bold">SIMD</strong>) offered by modern <a id="_idIndexMarker265"/>CPUs to operate on multiple data entries with a single instruction.</p>
<p>However, GPUs operate on a larger number of data points within a single instruction. The other main difference is that each thread on the GPU is more flexible compared to a SIMD instruction. GPU architecture is a fascinating topic, but its scope is outside this book. We will provide references for further reading at the end of the chapter.</p>
<p class="callout-heading">Note</p>
<p class="callout">A group of threads has different names depending on the GPU vendor. You might see the term warp or wave being mentioned in their documentation. We are going to use thread group to avoid confusion.</p>
<p>Each compute shader invocation can use multiple threads within a compute unit, and it’s possible to control how many threads are used. In Vulkan, this is achieved with the following directive inside a compute shader:</p>
<pre class="source-code">
layout (local_size_x = 8, local_size_y = 8, 
local_size_z = 1) in;</pre>
<p>This defines the local group size; we are going to explain what it does in just a moment. For now, the main point is that we are telling the GPU that we want to execute 64 threads (8x8). Each GPU has an optimal thread group size. You should check the documentation from each vendor and, if possible, adjust the thread group size for optimal performance.</p>
<p>We also have to define a global group size when invoking a compute shader:</p>
<pre class="source-code">
gpu_commands-&gt;dispatch( ceilu32( renderer-&gt;
    gpu-&gt;swapchain_width * 1.f / 8 ), 
      ceilu32( renderer-&gt;gpu-&gt;swapchain_height * 1.f / 8 ), 
      1 );</pre>
<p>This code is taken from our lighting pass implementation. In this case, we want to process all the pixels in our render target texture. As you probably noticed, we divide the size by 8. This is needed to ensure we don’t process the same pixel multiple times. Let’s walk through an example to clarify how the local and global group size works.</p>
<p>Let’s say our render target is 1280x720. Multiplying the width by the height will give us the total <a id="_idIndexMarker266"/>number of pixels in the image. When we define the local group size, we determine how many pixels are going to be processed by each shader invocation (again, 64 in our case). The number of shader invocations is computed as follows:</p>
<pre class="source-code">
shader_invocation_count = total_pixels / 64</pre>
<p>The <code>dispatch</code> command requires three values, though, as both the local and global group size are defined as a vector of three values. This is why we divide each dimension by <code>8</code>:</p>
<pre class="source-code">
global_group_size_x = width / 8
global_group_size_y = height / 8</pre>
<p>Since we are operating on a 2D texture, we are not modifying the <code>z</code> value. We can verify that we are processing the right number of pixels with this code:</p>
<pre class="source-code">
local_thread_group_count = 64
shader_invocation_count = global_group_size_x * 
    global_group_size_y
total_pixels =  shader_invocation_count * 
    local_thread_group_count</pre>
<p>We can determine which invocation is being run inside the shader by using this variable provided by GLSL:</p>
<pre class="source-code">
ivec3 pos = ivec3( gl_GlobalInvocationID.xyz );</pre>
<p>Each thread will see a unique position value, which we can use to access our texture.</p>
<p>This was only a brief overview of the compute shader execution model. We are going to provide <a id="_idIndexMarker267"/>more in-depth resources in the <em class="italic">Further </em><em class="italic">reading</em> section.</p>
<p>Now that we have a better understanding of how compute shaders are executed, we are going to demonstrate how to convert CPU code to a GPU compute shader.</p>
<h2 id="_idParaDest-89"><a id="_idTextAnchor088"/>Writing compute shaders</h2>
<p>Writing code for compute shaders is similar to writing vertex or fragment shaders. The main difference <a id="_idIndexMarker268"/>is that we have more flexibility in compute shaders to define which data to access. For instance, in vertex shaders, we usually access a single entry in an attribute buffer. The same applies to fragment shaders, where the fragment being shaded by a shader invocation is determined by the GPU.</p>
<p>Because of the added flexibility, we also need to think more carefully about our access patterns and synchronization between threads. If, for instance, more than one thread has to write to the same memory location, we need to add memory barriers to ensure previous writes to that memory have completed and all threads see the correct value. In pseudo-code, this translates to this:</p>
<pre class="source-code">
// code
MemoryBarrier()
// all threads have run the code before the barrier</pre>
<p>GLSL also provides atomic operations in case the same memory location has to be accessed across shader invocations.</p>
<p>With that in mind, let’s have a look at the pseudo-code for the CPU version of the cloth simulation:</p>
<pre class="source-code">
for each physics mesh in the scene:
    for each vertex in the mesh:
        compute the force applied to the vertex
    // We need two loops because each vertex references 
       other vertices position
    // First we need to compute the force applied to each 
       vertex, 
    // and only after update each vertex position
       for each vertex in the mesh:
    update the vertex position and store its velocity
 
    update the mesh normals and tangents
    copy the vertices to the GPU</pre>
<p>We used a common spring model for the cloth simulation, but its implementation is outside the <a id="_idIndexMarker269"/>scope of this chapter. We suggest looking at the code for more detail, and we also reference the paper we used in the <em class="italic">Further </em><em class="italic">reading</em> section.</p>
<p>As you notice, at the end of the loop, we have to copy the updated vertex, normal, and tangent buffers to the GPU. Depending on the number of meshes and their complexity, this could be a costly operation. This step could be even more costly if the cloth simulation were to rely on data from other systems that run on the GPU.</p>
<p>If, for instance, the animation system runs on the GPU while the cloth simulation runs on the CPU, we now have two copies to perform, in addition to extra synchronization points in the pipeline. For these reasons, it can be beneficial to move the cloth simulation to the GPU. </p>
<p>Let’s start by looking at the vertex buffer setup:</p>
<pre class="source-code">
BufferCreation creation{ };
sizet buffer_size = positions.size * sizeof( vec3s );
creation.set( flags, ResourceUsageType::Immutable, 
    buffer_size ).set_data( positions.data )
        .set_name( nullptr ).set_persistent( true );
 
BufferResource* cpu_buffer = renderer-&gt;
    create_buffer( creation );
cpu_buffers.push( *cpu_buffer );</pre>
<p>This is the only buffer we needed before. Because we had to update the data on the CPU, we could only use a host coherent buffer so that write on the CPU would be visible on the CPU. Using this type of buffer has performance implications on the GPU, as this type of memory <a id="_idIndexMarker270"/>can be slower to access, especially when the buffer size is large.</p>
<p>Since we are now going to perform the update on the GPU, we can use a buffer that is marked as <code>device_only</code>. This is how we create the buffer:</p>
<pre class="source-code">
creation.reset().set( flags, ResourceUsageType::Immutable, 
    buffer_size ).set_device_only( true )
        .set_name( "position_attribute_buffer" );
 
BufferResource* gpu_buffer = renderer-&gt;
    create_buffer( creation );
gpu_buffers.push( *gpu_buffer );</pre>
<p>Finally, we copy the data from the CPU to the GPU only once. After the copy is done, we can free the CPU buffer:</p>
<pre class="source-code">
async_loader-&gt;request_buffer_copy( cpu_buffer-&gt;handle, 
                                   gpu_buffer-&gt;handle );</pre>
<p>We have shown an example of the position buffer. All the other buffers (normal, tangent, texture coordinates, and indices) are managed in the same way.</p>
<p>Now that we have our buffers, we need to create a descriptor set that will be used by our compute shader:</p>
<pre class="source-code">
DescriptorSetLayoutHandle physics_layout = renderer-&gt;
    gpu-&gt;get_descriptor_set_layout
        ( cloth_technique-&gt;passes[ 0 ].pipeline, 
            k_material_descriptor_set_index );
ds_creation.reset().buffer( physics_cb, 0 )
    .buffer( mesh.physics_mesh-&gt;gpu_buffer, 1 )
    .buffer( mesh.position_buffer, 2 )
    .buffer( mesh.normal_buffer, 3 )
    .buffer( mesh.index_buffer, 4 )
    .set_layout( physics_layout );
 
mesh.physics_mesh-&gt;descriptor_set = renderer-&gt;
    gpu-&gt;create_descriptor_set( ds_creation );</pre>
<p>We can <a id="_idIndexMarker271"/>match the binding of the preceding buffers with the following shader code:</p>
<pre class="source-code">
layout ( std140, set = MATERIAL_SET, binding = 0 ) uniform 
    PhysicsData {
    ...
};
 
layout ( set = MATERIAL_SET, binding = 1 ) buffer 
    PhysicsMesh {
        uint index_count;
        uint vertex_count;
 
    PhysicsVertex physics_vertices[];
};
 
layout ( set = MATERIAL_SET, binding = 2 ) buffer 
    PositionData {
        float positions[];
};
 
layout ( set = MATERIAL_SET, binding = 3 ) buffer 
    NormalData {
        float normals[];
};
 
layout ( set = MATERIAL_SET, binding = 4 ) readonly buffer 
    IndexData {
        uint indices[];
};</pre>
<p>It’s important to notice a couple of points. Because we don’t know the size of each buffer at runtime, we have to use separate storage blocks. We can only have one runtime array per storage block, and it must be the last member of the block.</p>
<p>We also have <a id="_idIndexMarker272"/>to use float arrays instead of <code>vec3</code> arrays; otherwise, each entry in the vector would be padded to 16 bytes and the data on the GPU will no longer match the data layout on the CPU. We could use <code>vec4</code> as type, but we would be wasting 4 bytes for each vertex. When you have millions, if not billions, of vertices, it adds up!</p>
<p>Finally, we marked the <code>IndexData</code> block as <code>readonly</code>. This is because we never modify the index buffer in this shader. It’s important to mark each block with the right attributes as this will give more opportunities for optimization to the shader compiler.</p>
<p>We could reduce the number of blocks by arranging our data differently, for example:</p>
<pre class="source-code">
struct MeshVertex {
    vec3 position;
    vec3 normal;
    vec3 tangent;
};
 
layout ( set = MATERIAL_SET, binding = 2 ) buffer MeshData {
    MeshVertex mesh_vertices[];
};</pre>
<p>This solution <a id="_idIndexMarker273"/>is usually referred to as <strong class="bold">Array of Structures</strong> (<strong class="bold">AoS</strong>), while <a id="_idIndexMarker274"/>the code we presented before used <strong class="bold">Structure of Arrays</strong> (<strong class="bold">SoA</strong>). While <a id="_idIndexMarker275"/>the AoS solution simplifies the bindings, it also makes it impossible to use each array individually. In our depth pass, for instance, we only need the positions. For this reason, we preferred the SoA approach.</p>
<p>We have already shown how to dispatch a compute shader and how to synchronize access between the compute and graphics queue, so we won’t repeat that code here. We can now move to the shader implementation. We are only going to show the relevant section; you can refer to the code for the full listing.</p>
<p>We start by computing the force applied to each vertex:</p>
<pre class="source-code">
vec3 spring_force = vec3( 0, 0, 0 );
 
for ( uint j = 0; j &lt; physics_vertices[ v ]
    .joint_count; ++j ) {
        pull_direction = ...;
        spring_force += pull_direction;
}
 
vec3 viscous_damping = physics_vertices[ v ]
    .velocity * -spring_damping;
 
vec3 viscous_velocity = ...;
 
vec3 force = g * m;
force -= spring_force;
force += viscous_damping;
force += viscous_velocity;
 
physics_vertices[ v ].force = force;</pre>
<p>Notice how we access the <code>physics_vertices</code> array each time. In the CPU code, we could simply <a id="_idIndexMarker276"/>get a reference to the struct, and each field would be updated correctly. However, GLSL doesn’t support references, so we need to be really careful that we are not writing to a local variable.</p>
<p>As in the CPU code, after computing the force vector for each vertex, we need to update its position:</p>
<pre class="source-code">
vec3 previous_position = physics_vertices[ v ]
    .previous_position;
vec3 current_position = physics_vertices[ v ].position;
 
vec3 new_position = ...;
 
physics_vertices[ v ].position = new_position;
physics_vertices[ v ].previous_position = current_position;
 
physics_vertices[ v ].velocity = new_position - current_position;</pre>
<p>Again, notice that we always read from the buffer each time. Finally, we update the vertex positions of the mesh:</p>
<pre class="source-code">
for ( uint v = 0; v &lt; vertex_count; ++v ) {
     positions[ v * 3 + 0 ] = physics_vertices[ v ]
         .position.x;
     positions[ v * 3 + 1 ] = physics_vertices[ v ]
         .position.y;
     positions[ v * 3 + 2 ] = physics_vertices[ v ]
         .position.z;
}</pre>
<p>Because this is all performed on the GPU, the positions could have been updated first by another system, such as animation, but we no longer need costly copy operations to and from the GPU.</p>
<p>Before we conclude, we’d like to point out that we have one shader invocation per mesh and that performance is achieved by updating the cloth simulation for multiple meshes in the same dispatch. Another approach could have been to have one dispatch per mesh where each shader invocation updates an individual vertex.</p>
<p>While technically <a id="_idIndexMarker277"/>a valid approach, it requires a lot more synchronization within the thread group and across shader invocations. As we mentioned, we first have to compute the force for each vertex before updating their position. Another solution could be to split the update into two shaders, one that computes the force and a second one that updates the positions.</p>
<p>This still requires pipeline barriers between each shader dispatch. While the GPU must guarantee that each command is executed in the same order it has been recorded; it doesn’t guarantee the order of completion. For these reasons, we have decided to use one thread per mesh.</p>
<p>In this section, we have explained the execution model of compute shaders and the benefits of running selected computations on the GPU to improve performance and avoid extra memory copies. We then demonstrated how to port code written for the CPU to the GPU and some of the aspects we need to pay attention to when working with compute shaders.</p>
<p>We suggest looking at the code for more details. Try to make changes to the cloth simulation to implement a different simulation technique or add your own compute shaders to the engine!</p>
<h1 id="_idParaDest-90"><a id="_idTextAnchor089"/>Summary</h1>
<p>In this chapter, we have built the foundations to support compute shaders in our renderer. We started by introducing timeline semaphores and how they can be used to replace multiple semaphores and fences. We have shown how to wait for a timeline semaphore on the CPU and how a timeline semaphore can be used as part of a queue submission, either for it to be signaled or to be waited on.</p>
<p>Next, we demonstrated how to use the newly introduced timeline semaphore to synchronize execution across the graphics and compute queue.</p>
<p>In the last section, we showed an example of how to approach porting code written for the CPU to the GPU. We first explained some of the benefits of running computations on the GPU. Next, we gave an overview of the execution model for compute shaders and the configuration of local and global workgroup sizes. Finally, we gave a concrete example of a compute shader for cloth simulation and highlighted the main differences with the same code written for the CPU.</p>
<p>In the next chapter, we are going to improve our pipeline by adding mesh shaders, and for the devices that don’t support them, we are going to write a compute shader alternative.</p>
<h1 id="_idParaDest-91"><a id="_idTextAnchor090"/>Further reading</h1>
<p>Synchronization is likely one of the most complex aspects of Vulkan. We have mentioned some of the concepts in this and previous chapters. If you want to improve your understanding, we suggest reading the following resources:</p>
<ul>
<li><a href="https://www.khronos.org/registry/vulkan/specs/1.3-extensions/html/vkspec.xhtml#synchronization">https://www.khronos.org/registry/vulkan/specs/1.3-extensions/html/vkspec.xhtml#synchronization</a></li>
<li><a href="https://www.khronos.org/blog/understanding-vulkan-synchronization">https://www.khronos.org/blog/understanding-vulkan-synchronization</a></li>
<li><a href="https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples">https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples</a></li>
</ul>
<p>We only touched the surface when it comes to compute shaders. The following resources go more in depth and also provide suggestions to get the most out of individual devices:</p>
<ul>
<li><a href="https://www.khronos.org/opengl/wiki/Compute_Shader">https://www.khronos.org/opengl/wiki/Compute_Shader</a></li>
<li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.xhtml#programming-model">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.xhtml#programming-model</a></li>
<li><a href="https://github.com/KhronosGroup/OpenCL-Guide/blob/main/chapters/opencl_programming_model.md">https://github.com/KhronosGroup/OpenCL-Guide/blob/main/chapters/opencl_programming_model.md</a></li>
</ul>
<p>Real-time cloth simulation for computer graphics has been a subject of study for many years. We have based our implementation on this paper: <a href="http://graphics.stanford.edu/courses/cs468-02-winter/Papers/Rigidcloth.pdf">http://graphics.stanford.edu/courses/cs468-02-winter/Papers/Rigidcloth.pdf</a>.</p>
<p>Another popular approach is presented in this paper: <a href="http://www.cs.cmu.edu/~baraff/papers/sig98.pdf">http://www.cs.cmu.edu/~baraff/papers/sig98.pdf</a>.</p>
<p>Finally, this GDC talk gave us the idea of using cloth simulation to demonstrate how to use compute shaders:</p>
<p><a href="https://www.gdcvault.com/play/1022350/Ubisoft-Cloth-Simulation-Performance-Postmortem">https://www.gdcvault.com/play/1022350/Ubisoft-Cloth-Simulation-Performance-Postmortem</a></p>
</div>
</div>

<div><div><h1 id="_idParaDest-92"><a id="_idTextAnchor091"/>Part 2: GPU-Driven Rendering</h1>
<p>Starting with this part, we are going to focus on modern rendering techniques. We will cover the following chapters in this section:</p>
<ul>
<li><a href="B18395_06.xhtml#_idTextAnchor092"><em class="italic">Chapter 6</em></a><em class="italic">, GPU-Driven Rendering</em></li>
<li><a href="B18395_07.xhtml#_idTextAnchor105"><em class="italic">Chapter 7</em></a><em class="italic">, Rendering Many Lights with Clustered Deferred Rendering</em></li>
<li><a href="B18395_08.xhtml#_idTextAnchor116"><em class="italic">Chapter 8</em></a><em class="italic">, Adding Shadows Using Mesh Shaders</em></li>
<li><a href="B18395_09.xhtml#_idTextAnchor143"><em class="italic">Chapter 9</em></a><em class="italic">, Implementing Variable Rate Shading</em></li>
<li><a href="B18395_10.xhtml#_idTextAnchor152"><em class="italic">Chapter 10</em></a><em class="italic">, Adding Volumetric Fog</em></li>
</ul>
</div>
<div><div></div>
</div>
</div></body></html>