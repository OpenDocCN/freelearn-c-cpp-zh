<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Working with Geometry Data</title>
<link href="../styles/stylesheet1.css" rel="stylesheet" type="text/css"/>
<link href="../styles/stylesheet2.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="sbo-rt-content"><section class="level1 pkt" data-number="6" id="working-with-geometry-data">
<h1 data-number="6">5 Working with Geometry Data</h1>
<section class="level2" data-number="6.1" id="join-our-book-community-on-discord-4">
<h2 data-number="6.1">Join our book community on Discord</h2>
<p>
<img height="301" src="../media/file40.png" style="width:15rem" width="301"/>
</p>
<p><a href="https://packt.link/unitydev">https://packt.link/unitydev</a></p>
<p>Previously, we tried different ad hoc approaches to storing and handling 3D geometry data in our graphical applications. The mesh data layout for vertex and index buffers was hardcoded in each of our demo apps. This way it was easier to focus on other important parts of the graphics pipeline. As we go into the territory of more complex graphics applications, we require additional control over the storage of different 3D meshes in system memory and GPU buffers. However, our focus still remains on guiding you through the main principles and practices rather than on pure efficiency.</p>
<p>In this chapter, we will learn how to store and handle mesh geometry data in a more organized way. We will cover the following recipes:</p>
<ul>
<li>Generating level-of-detail meshes using MeshOptimizer</li>
<li>Implementing programmable vertex pulling</li>
<li>Rendering instanced geometry</li>
<li>Implementing instanced meshes with compute shaders</li>
<li>Implementing an infinite grid GLSL shader</li>
<li>Integrating tessellation into the graphics pipeline</li>
<li>Organizing the mesh data storage</li>
<li>Implementing automatic geometry conversion</li>
<li>Indirect rendering in Vulkan</li>
<li>Generating textures in Vulkan using compute shaders</li>
<li>Implementing computed meshes</li>
</ul>
</section>
<section class="level2" data-number="6.2" id="technical-requirements-3">
<h2 data-number="6.2">Technical requirements</h2>
<p>To run the code from this chapter on your Linux or Windows PC, you’ll require a GPU with up-to-date drivers that support Vulkan 1.3. The source code can be downloaded from <a href="https://github.com/PacktPublishing/3D-Graphics-Rendering-Cookbook">https://github.com/PacktPublishing/3D-Graphics-Rendering-Cookbook</a>.</p>
<p>To run the demo applications from this chapter, you are advised to download and unpack the entire Amazon Lumberyard Bistro dataset from the McGuire Computer Graphics Archive <a href="http://casual-effects.com/data/index.xhtml">http://casual-effects.com/data/index.xhtml</a>. You can do it automatically by running the <code>deploy_deps.py</code> script.</p>
</section>
<section class="level2" data-number="6.3" id="generating-level-of-detail-meshes-using-meshoptimizer">
<h2 data-number="6.3">Generating level-of-detail meshes using MeshOptimizer</h2>
<p>To get started with geometry manipulations, let’s implement a mesh geometry simplification demo using the MeshOptimizer library that, besides mesh optimizations, can generate simplified meshes for real-time discrete <strong>level-of-detail</strong> (<strong>LOD</strong>) algorithms we might want to use later. Simplification is an efficient way to improve rendering performance.</p>
<p>For GPUs to render a mesh efficiently, all vertices in the vertex buffer should be unique and without duplicates. Solving this problem efficiently can be a complicated and computationally intensive task in any modern 3D content pipeline. MeshOptimizer is an open-source C++ library developed by Arseny Kapoulkine, which provides algorithms to help optimize meshes for modern GPU vertex and index processing pipelines. It can reindex an existing index buffer or generate an entirely new set of indices from an unindexed vertex buffer.</p>
<p>Let’s learn how to optimize and generate simplified meshes with MeshOptimizer.</p>
<section class="level3" data-number="6.3.1" id="getting-ready-26">
<h3 data-number="6.3.1">Getting ready</h3>
<p>It is recommended that you revisit <em>Chapter 3</em>, <em>Working with Vulkan Objects</em>. The complete source code for this recipe can be found in <code>Chapter05/01_MeshOptimizer</code>.</p>
</section>
<section class="level3" data-number="6.3.2" id="how-to-do-it...-25">
<h3 data-number="6.3.2">How to do it...</h3>
<p><code>MeshOptimizer</code> can generate all necessary LOD meshes for a specified set of indices and vertices. Once we have our loaded mesh with <code>Assimp</code>, we can pass it to <code>MeshOptimizer</code>. Here’s how to do it:</p>
<ol>
<li>Let’s load a mesh from a <code>.gltf</code> file using <code>Assimp</code>. For this demo, we need only vertex positions and indices:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  const aiScene* scene = aiImportFile(
    “data/rubber_duck/scene.gltf”, aiProcess_Triangulate);
  const aiMesh* mesh = scene-&gt;mMeshes[0];
  std::vector&lt;vec3&gt; positions;
  std::vector&lt;uint32_t&gt; indices;
  for (unsigned int i = 0; i != mesh-&gt;mNumVertices; i++) {
    const aiVector3D v = mesh-&gt;mVertices[i];
    positions.push_back(vec3(v.x, v.y, v.z));
  }
  for (unsigned int i = 0; i != mesh-&gt;mNumFaces; i++) {
    for (int j = 0; j != 3; j++)
      indices.push_back(mesh-&gt;mFaces[i].mIndices[j]);
  }
  aiReleaseImport(scene);</code></pre>
</div>
<ol>
<li>The LOD meshes are represented as a collection of indices that construct a new simplified mesh from the same vertices that are used for the original mesh. This way we have to store only one set of vertices and can render corresponding LODs just by switching index buffers data. As done previously, we store all indices as unsigned 32-bit integers for simplicity. Now we should generate a remap table for our existing vertex and index data:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  std::vector&lt;uint32_t&gt; remap(indices.size());
  const size_t vertexCount =
    meshopt_generateVertexRemap(remap.data(), indices.data(),
      indices.size(), positions.data(), indices.size(), sizeof(vec3));</code></pre>
</div>
<p>The MeshOptimizer documentation (<a href="https://github.com/zeux/meshoptimizer">https://github.com/zeux/meshoptimizer</a>) tells us the following:</p>
<blockquote>
<em>“…the remap table is generated based on binary equivalence of the input vertices, so the resulting mesh will be rendered in the same way.”</em>
</blockquote>
<ol>
<li>The returned <code>vertexCount</code> value corresponds to the number of unique vertices that have remained after remapping. Let’s allocate space and generate new vertex and index buffers:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  std::vector&lt;uint32_t&gt; remappedIndices(indices.size());
  std::vector&lt;vec3&gt; remappedVertices(vertexCount);
  meshopt_remapIndexBuffer(remappedIndices.data(), indices.data(),
    indices.size(), remap.data());
  meshopt_remapVertexBuffer(remappedVertices.data(), positions.data(),
    positions.size(), sizeof(vec3), remap.data());</code></pre>
</div>
<p>Now we can use other <code>MeshOptimizer</code> algorithms to optimize these buffers even further. The official documentation is pretty straightforward.</p>
<ol>
<li>When we want to render a mesh, the GPU has to transform each vertex via a vertex shader. GPUs can reuse transformed vertices by means of a small built-in cache, usually storing between 16 and 32 vertices inside it. In order to use this small cache effectively, we need to reorder the triangles to maximize the locality of vertex references. How to do this with <code>MeshOptimizer</code> in place is shown next. Pay attention to how only the indices data is being touched here:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  meshopt_optimizeVertexCache(remappedIndices.data(),
    remappedIndices.data(), indices.size(), vertexCount);</code></pre>
</div>
<ol>
<li>Transformed vertices form triangles that are sent for rasterization to generate fragments. Usually, each fragment is run through a depth test first, and fragments that pass the depth test get the fragment shader executed to compute the final color. As fragment shaders get more and more expensive, it becomes increasingly important to reduce the number of fragment shader invocations. This can be achieved by reducing pixel overdraw in a mesh, and, in general, it requires the use of view-dependent algorithms. However, <code>MeshOptimizer</code> implements heuristics to reorder triangles and minimize overdraw from all directions. We can use it as follows:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  meshopt_optimizeOverdraw(
    remappedIndices.data(), remappedIndices.data(),
    indices.size(), glm::value_ptr(remappedVertices[0]), vertexCount,
    sizeof(vec3), 1.05f);</code></pre>
</div>
<p>The last parameter, <code>1.05</code>, is the threshold that determines how much the algorithm can compromise the vertex cache hit ratio. We use the recommended default value from the documentation.</p>
<ol>
<li>Once we have optimized the mesh to reduce pixel overdraw, the vertex buffer access pattern can still be optimized for memory efficiency. The GPU has to fetch specified vertex attributes from the vertex buffer and pass this data into the vertex shader. To speed up this fetch, a memory cache is used, which means optimizing the locality of vertex buffer access is very important. We can use MeshOptimizer to optimize our index and vertex buffers for vertex fetch efficiency, as follows:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  meshopt_optimizeVertexFetch(
    remappedVertices.data(), remappedIndices.data(), indices.size(),
    remappedVertices.data(), vertexCount, sizeof(vec3));</code></pre>
</div>
<p>This function will reorder vertices in the vertex buffer and regenerate indices to match the new contents of the vertex buffer.</p>
<ol>
<li>The last thing we will do in this recipe is simplify the mesh. MeshOptimizer can generate a new index buffer that uses existing vertices from the vertex buffer with a reduced number of triangles. This new index buffer can be used to render LOD meshes. The following code snippet shows you how to do this using the default threshold and target error values:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  const float threshold           = 0.2f;
  const size_t target_index_count =
    size_t(remappedIndices.size() * threshold);
  const float target_error        = 0.01f;
  std::vector&lt;uint32_t&gt; indicesLod;
  indicesLod.resize(remappedIndices.size());
  indicesLod.resize(meshopt_simplify(
    &amp;indicesLod[0], remappedIndices.data(), remappedIndices.size(),
    &amp;remappedVertices[0].x, vertexCount, sizeof(vec3),
    target_index_count, target_error));
  indices   = remappedIndices;
  positions = remappedVertices;</code></pre>
</div>
<p>Now let’s take a look at how rendering of the LOD meshes works.</p>
</section>
<section class="level3" data-number="6.3.3" id="how-it-works-8">
<h3 data-number="6.3.3">How it works…</h3>
<p>In order to render the mesh and its lower-level LOD, we need to store a vertex buffer and two index buffers – one for the mesh and one for the LOD:</p>
<ol>
<li>Here’s the vertex buffer for storing vertex positions:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  lvk::Holder&lt;lvk::BufferHandle&gt; vertexBuffer = ctx-&gt;createBuffer({
    .usage     = lvk::BufferUsageBits_Vertex,
    .storage   = lvk::StorageType_Device,
    .size      = sizeof(vec3) * positions.size(),
    .data      = positions.data(),
    .debugName = “Buffer: vertex” }, nullptr);</code></pre>
</div>
<ol>
<li>We use two index buffers to store both sets of indices:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  lvk::Holder&lt;lvk::BufferHandle&gt; indexBuffer = ctx-&gt;createBuffer({
    .usage     = lvk::BufferUsageBits_Index,
    .storage   = lvk::StorageType_Device,
    .size      = sizeof(uint32_t) * indices.size(),
    .data      = indices.data(),
    .debugName = “Buffer: index” }, nullptr);
  lvk::Holder&lt;lvk::BufferHandle&gt; indexBufferLod = ctx-&gt;createBuffer({
    .usage     = lvk::BufferUsageBits_Index,
    .storage   = lvk::StorageType_Device,
    .size      = sizeof(uint32_t) * indicesLod.size(),
    .data      = indicesLod.data(),
    .debugName = “Buffer: index LOD” }, nullptr);</code></pre>
</div>
<ol>
<li>The rendering part is trivial, and the graphics pipeline setup is skipped here for the sake of brevity. We render the main mesh using the first index buffer:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  buf.cmdBindVertexBuffer(0, vertexBuffer, 0);
  buf.cmdBindRenderPipeline(pipeline);
  buf.cmdBindDepthState(dState);
  buf.cmdPushConstants(p * v1 * m);
  buf.cmdBindIndexBuffer(indexBuffer, lvk::IndexFormat_UI32);
  buf.cmdDrawIndexed(indices.size());</code></pre>
</div>
<ol>
<li>Then we render the LOD mesh using the second index buffer:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  buf.cmdPushConstants(p * v2 * m);
  buf.cmdBindIndexBuffer(indexBufferLod, lvk::IndexFormat_UI32);
  buf.cmdDrawIndexed(indicesLod.size());</code></pre>
</div>
<p>Here’s a screenshot from the running demo application.</p>
<figure>
<img alt="Figure 5.1: A mesh with a discrete LOD" height="756" src="../media/file31.png" width="1430"/><figcaption aria-hidden="true">Figure 5.1: A mesh with a discrete LOD</figcaption>
</figure>
<p>Try changing the <code>threshold</code> parameter in the code to generate meshes with different LOD.</p>
</section>
<section class="level3" data-number="6.3.4" id="theres-more...-8">
<h3 data-number="6.3.4">There’s more...</h3>
<p>The <code>MeshOptimizer</code> library contains many other useful algorithms, such as triangle strip generation, index and vertex buffer compression, and mesh animation data compression. All of these algorithms might be very useful for your geometry preprocessing stage depending on the kind of graphics software you are writing. Check out the official documentation and the releases page to get the latest features at <a href="https://github.com/zeux/meshoptimizer">https://github.com/zeux/meshoptimizer</a>.</p>
<p>In <em>Chapter 9</em>, <em>Advanced Rendering Techniques and Optimization</em>, we will learn how to render LODs in a GPU-friendly and performant way.</p>
</section>
</section>
<section class="level2" data-number="6.4" id="implementing-programmable-vertex-pulling">
<h2 data-number="6.4">Implementing programmable vertex pulling</h2>
<p>The concept of <strong>programmable vertex pulling</strong> (<strong>PVP</strong>) was proposed in an article called <em>Introducing the Programmable Vertex Pulling Rendering Pipeline</em> by Daniel Rákos, published in the amazing book <em>OpenGL Insights</em> in 2012. That article goes deep into the architecture of GPUs of that time and why it was beneficial to use this data storage approach. Initially, the idea of vertex pulling was to store vertex data inside one-dimensional buffer textures and, instead of setting up standard vertex input bindings. Then read the data using <code>texelFetch()</code> and GLSL <code>samplerBuffer</code> in the vertex shader. The built-in OpenGL GLSL <code>gl_VertexID</code> variable was used as an index to calculate texture coordinates for texel fetching. The reason for this trick was that because developers were hitting CPU limits with many draw calls, it was beneficial to combine multiple meshes inside a single buffer and render them in a single draw call without rebinding any vertex arrays or buffer objects to improve the batching of draw calls.</p>
<p>Nowadays, buffer textures are no longer required and the vertex data can be fetched directly from storage or uniform buffers using offsets calculated via the built-in Vulkan GLSL <code>gl_VertexIndex</code> variable.</p>
<p>This technique opens up possibilities for merge-instancing, where many small meshes can be merged into a bigger one to be handled as a part of the same batch. We will extensively use this technique in our examples starting from <em>Chapter 7</em>, <em>Graphics Rendering Pipeline</em>.</p>
<p>In this recipe, we will use storage buffers to implement a similar technique with Vulkan 1.3 and <em>LightweightVK</em>.</p>
<section class="level3" data-number="6.4.1" id="getting-ready-27">
<h3 data-number="6.4.1">Getting ready</h3>
<p>The complete source code for this recipe can be found in the source code bundle under the name <code>Chapter05/02_VertexPulling</code>.</p>
</section>
<section class="level3" data-number="6.4.2" id="how-to-do-it...-26">
<h3 data-number="6.4.2">How to do it...</h3>
<p>Let’s render the rubber duck 3D model <code>data/rubber_duck/scene.gltf</code> from the previous recipe. However, this time, instead of using vertex attributes, we will be using the programmable vertex-pulling technique. The idea is to allocate two buffers, one buffer for indices and another storage buffer for the vertex data, and access them in the vertex shader to fetch vertex positions. This is how we can do it:</p>
<ol>
<li>First, we load the 3D model via <code>Assimp</code>, as in the previous recipe:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const aiScene* scene = aiImportFile(
  “data/rubber_duck/scene.gltf”, aiProcess_Triangulate);
const aiMesh* mesh = scene-&gt;mMeshes[0];</code></pre>
</div>
<ol>
<li>Convert per-vertex data into a format suitable for our GLSL shaders. We are going to use <code>vec3</code> for positions and <code>vec2</code> for texture coordinates:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>struct Vertex {
  vec3 pos;
  vec2 uv;
};
std::vector&lt;Vertex&gt; positions;
for (unsigned int i = 0; i != mesh-&gt;mNumVertices; i++) {
  const aiVector3D v = mesh-&gt;mVertices[i];
  const aiVector3D t = mesh-&gt;mTextureCoords[0][i];
  positions.push_back({
   .pos = vec3(v.x, v.y, v.z), .uv = vec2(t.x, t.y) });
}</code></pre>
</div>
<ol>
<li>For simplicity, we store indices as unsigned 32-bit integers. In real-world applications, consider using 16-bit indices for small meshes and be capable of switching between them:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>std::vector&lt;uint32_t&gt; indices;
for (unsigned int i = 0; i != mesh-&gt;mNumFaces; i++) {
  for (int j = 0; j != 3; j++)
    indices.push_back(mesh-&gt;mFaces[i].mIndices[j]);
}
aiReleaseImport(scene);</code></pre>
</div>
<ol>
<li>Once the index and vertex data are ready, we can upload them into the Vulkan buffers. We should create two buffers, one for the vertices and one for the indices. Not that here, despite calling it a vertex buffer, we set the usage flag to <code>lvk::BufferUsageBits_Storage</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::BufferHandle&gt; vertexBuffer = ctx-&gt;createBuffer({
  .usage     = lvk::BufferUsageBits_Storage,
  .storage   = lvk::StorageType_Device,
  .size      = sizeof(Vertex) * positions.size(),
  .data      = positions.data(),
  .debugName = “Buffer: vertex” }, nullptr);
lvk::Holder&lt;lvk::BufferHandle&gt; indexBuffer = ctx-&gt;createBuffer({
  .usage     = lvk::BufferUsageBits_Index,
  .storage   = lvk::StorageType_Device,
  .size      = sizeof(uint32_t) * indices.size(),
  .data      = indices.data(),
  .debugName = “Buffer: index” }, nullptr);</code></pre>
</div>
<ol>
<li>Now we can create a render pipeline for our mesh. We will look into the shader code in a few moments:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::ShaderModuleHandle&gt; vert =
  loadShaderModule(ctx, “Chapter05/02_VertexPulling/src/main.vert”);
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; geom =
  loadShaderModule(ctx, “Chapter05/02_VertexPulling/src/main.geom”);
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; frag =
  loadShaderModule(ctx, “Chapter05/02_VertexPulling/src/main.frag”);
lvk::Holder&lt;lvk::RenderPipelineHandle&gt; pipelineSolid =
  ctx-&gt;createRenderPipeline({
        .smVert      = vert,
        .smGeom      = geom,
        .smFrag      = frag,
        .color       = { { .format = ctx-&gt;getSwapchainFormat() } },
        .depthFormat = app.getDepthFormat(),
        .cullMode    = lvk::CullMode_Back,
    });</code></pre>
</div>
<ol>
<li>Let’s load a texture for our mesh and create a proper depth state:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::TextureHandle&gt; texture =
  loadTexture(ctx, “data/rubber_duck/textures/Duck_baseColor.png”);
const lvk::DepthState dState =
  { .compareOp = lvk::CompareOp_Less, .isDepthWriteEnabled = true };</code></pre>
</div>
<ol>
<li>Before we can proceed with the actual rendering, we should pass the texture ID and storage buffer address to our GLSL shader. We can do it using Vulkan push constants. Model-view-projection matrix calculations are reused from the previous recipe.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const struct PushConstants {
  mat4 mvp;
  uint64_t vertices;
  uint32_t texture;
} pc {
  .mvp      = p * v * m,
  .vertices = ctx-&gt;gpuAddress(vertexBuffer),
  .texture  = texture.index(),
};
lvk::ICommandBuffer&amp; buf = ctx-&gt;acquireCommandBuffer();
buf.cmdBeginRendering(renderPass, framebuffer);
buf.cmdPushConstants(pc);</code></pre>
</div>
<ol>
<li>Now, the mesh rendering can done as follows.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>buf.cmdBindIndexBuffer(indexBuffer, lvk::IndexFormat_UI32);
buf.cmdBindRenderPipeline(pipelineSolid);
buf.cmdBindDepthState(dState);
buf.cmdDrawIndexed(indices.size());
buf.cmdEndRendering();</code></pre>
</div>
<p>The rest of the C++ code can be found in <code>Chapter05/02_VertexPulling/src/main.cpp</code>. Now, we have to look into the GLSL vertex shader to understand how to read the vertex data from buffers. The vertex shader can be found in <code>Chapter05/02_VertexPulling/src/main.vert</code>:</p>
<ol>
<li>First, we have some declarations shared between all shaders. The reason for this sharing is that our fragment shader needs to access push constants to retrieve the texture ID. Note that the <code>Vertex</code> structure does not use <code>vec2</code> and <code>vec3</code> member fields to maintain tight padding and prevent any GPU alignment issues. This structure reflects how our C++ code writes vertex data into the buffer. The buffer holds an unbounded array <code>in_Vertices[]</code>. Each element corresponds to exactly one vertex.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>struct Vertex {
  float x, y, z;
  float u, v;
};
layout(std430, buffer_reference) readonly buffer Vertices {
  Vertex in_Vertices[];
};
layout(push_constant) uniform PerFrameData {
  mat4 MVP;
  Vertices vtx;
  uint texture;
};</code></pre>
</div>
<ol>
<li>Let’s introduce two accessor functions to make the shader code more readable. The assemble <code>vec3</code> and <code>vec2</code> values are from the raw float values in the storage buffer.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>vec3 getPosition(int i) {
  return vec3(vtx.in_Vertices[i].x,
              vtx.in_Vertices[i].y,
              vtx.in_Vertices[i].z);
}
vec2 getTexCoord(int i) {
  return vec2(vtx.in_Vertices[i].u,
              vtx.in_Vertices[i].v);
}</code></pre>
</div>
<ol>
<li>The rest of the shader is trivial. The previously mentioned functions are used to load the vertex positions and texture coordinates, which are passed further into the graphics pipeline.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout (location=0) out vec2 uv;
void main() {
  gl_Position = MVP * vec4(getPosition(gl_VertexIndex), 1.0);
  uv = getTexCoord(gl_VertexIndex);
}</code></pre>
</div>
<p>That’s it for the PVP part. The fragment shader applies the texture and uses the barycentric coordinates trick for wireframe rendering as was described in the previous chapter. The resulting output from the program should look like the following screenshot:</p>
<figure>
<img alt="Figure 5.2: Textured mesh rendering using PVP" height="756" src="../media/file32.png" width="1430"/><figcaption aria-hidden="true">Figure 5.2: Textured mesh rendering using PVP</figcaption>
</figure>
</section>
<section class="level3" data-number="6.4.3" id="theres-more...-9">
<h3 data-number="6.4.3">There’s more...</h3>
<p>PVP is a complex topic and has different performance implications. There is an open-source project that does an in-depth analysis and run-time metrics of PVP performance based on different vertex data layouts and access methods, such as storing data as an array of structures or a structure of arrays, reading data as multiple floats or a single vector type, and so on. Check it out at <a href="https://github.com/nlguillemot/ProgrammablePulling">https://github.com/nlguillemot/ProgrammablePulling</a>. It should become one of your go-to tools when designing PVP pipelines in your applications.</p>
</section>
</section>
<section class="level2" data-number="6.5" id="rendering-instanced-geometry">
<h2 data-number="6.5">Rendering instanced geometry</h2>
<p>A common task in geometry rendering is drawing multiple meshes that share the same geometry but have different transformations and materials. This can lead to additional CPU overhead in generating all the necessary commands to instruct the GPU to draw each mesh individually. It occurs even though the Vulkan API already has significantly lower CPU overhead. One possible solution to this problem, provided by modern graphics APIs such as Vulkan, is instanced rendering. API draw commands can take a number of instances as a parameter and a vertex shader has access to the current instance number <code>gl_InstanceIndex</code>. Combined with the PVP approach demonstrated in the previous recipe, this technique can become incredibly flexible. Indeed, <code>gl_InstanceIndex</code> can be used to read all necessary material properties, transforms, and other data from buffers. Let’s take a look at a basic instanced geometry demo to learn how to do it in Vulkan.</p>
<section class="level3" data-number="6.5.1" id="getting-ready-28">
<h3 data-number="6.5.1">Getting ready</h3>
<p>Make sure to read the previous recipe, <em>Implementing programmable vertex pulling</em>, to understand the notion of generating vertices data inside vertex shaders. The source code for this recipe can be found in <code>Chapter05/03_MillionCubes</code>.</p>
</section>
<section class="level3" data-number="6.5.2" id="how-to-do-it-3">
<h3 data-number="6.5.2">How to do it…</h3>
<p>To demonstrate how instanced rendering can work, let’s render 1 million colored rotating cubes. Each cube should have its own distinct rotation angle around its diagonal and should be textured with an overlay of one of a few different colors. Let’s take a look at the C++ code in <code>03_MillionCubes/src/main.cpp</code>:</p>
<ol>
<li>First, let’s generate a procedural texture for our cubes. An XOR pattern texture looks quite interesting. It is generated by XOR-ing the <code>x</code> and <code>y</code> coordinates of the current texel and then applying the result to all three BGR channels by bitshifts.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const uint32_t texWidth  = 256;
const uint32_t texHeight = 256;
std::vector&lt;uint32_t&gt; pixels(texWidth * texHeight);
for (uint32_t y = 0; y != texHeight; y++)
  for (uint32_t x = 0; x != texWidth; x++)
    pixels[y * texWidth + x] =
      0xFF000000 + ((x^y) &lt;&lt; 16) + ((x^y) &lt;&lt; 8) + (x^y);
lvk::Holder&lt;lvk::TextureHandle&gt; texture = ctx-&gt;createTexture({
   .type       = lvk::TextureType_2D,
   .format     = lvk::Format_BGRA_UN8,
   .dimensions = {texWidth, texHeight},
   .usage      = lvk::TextureUsageBits_Sampled,
   .data       = pixels.data(),
   .debugName  = “XOR pattern”,
});</code></pre>
</div>
<ol>
<li>Let’s create <code>vec3</code> positions and <code>float</code> initial rotation angles for 1 million cubes. We can organize this data into <code>vec4</code> containers and store them in an immutable storage buffer. The GLSL shader code will then perform calculations based on the elapsed time.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const uint32_t kNumCubes = 1024 * 1024;
std::vector&lt;vec4&gt; centers(kNumCubes);
for (vec4&amp; p : centers)
  p = vec4(glm::linearRand(-vec3(500.0f), +vec3(500.0f)),
           glm::linearRand(0.0f, 3.14159f));
lvk::Holder&lt;lvk::BufferHandle&gt; bufferPosAngle = ctx-&gt;createBuffer({
  .usage   = lvk::BufferUsageBits_Storage,
  .storage = lvk::StorageType_Device,
  .size    = sizeof(vec4) * kNumCubes,
  .data    = centers.data(),
});</code></pre>
</div>
<ol>
<li>We skip the traditional framebuffer and pipeline creation code and jump straight into the main rendering loop. The camera motion is hardcoded so it moves back and forth through the swarm of the cubes.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>buf.cmdBeginRendering(renderPass, framebuffer);
const mat4 view = translate(mat4(1.0f),
  vec3(0.0f, 0.0f,
       -1000.0f + 500.0f * (1.0f - cos(-glfwGetTime() * 0.5f))));</code></pre>
</div>
<ol>
<li>We pass all the necessary data to shaders using push constants. Our vertex shader is going to need the current time to do the calculations based on the initial cube positions and rotations.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const struct {
  mat4 viewproj;
  uint32_t textureId;
  uint64_t bufferPosAngle;
  float time;
} pc {
  .viewproj       = proj * view,
  .textureId      = texture.index(),
  .bufferPosAngle = ctx-&gt;gpuAddress(bufferPosAngle),
  .time           = (float)glfwGetTime(),
};</code></pre>
</div>
<ol>
<li>Rendering is started via <code>vkCmdDraw()</code>, which hides inside <code>cmdDraw()</code>. The first parameter is the number of vertices we need to generate a cube using triangle primitives. We will look at how it is handled in the vertex shader in a moment. The second parameter, <code>kNumCubes</code>, is the number of instances to be rendered.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>buf.cmdPushConstants(pc);
buf.cmdBindRenderPipeline(pipelineSolid);
buf.cmdBindDepthState(dState);
buf.cmdDraw(36, kNumCubes);
buf.cmdEndRendering();</code></pre>
</div>
<p>Now let’s take a look at the GLSL code to understand how this instancing demo works under the hood.</p>
</section>
<section class="level3" data-number="6.5.3" id="how-it-works-9">
<h3 data-number="6.5.3">How it works…</h3>
<ol>
<li>Our vertex shader starts by declaring the same <code>PerFrameData</code> structure as in our C++ code mentioned above. The shader outputs per-vertex color and texture coordinates. The storage buffer contains positions and initial angles for all the cubes.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout(push_constant) uniform PerFrameData {
  mat4 viewproj;
  uint textureId;
  uvec2 bufId;
  float time;
};
layout (location=0) out vec3 color;
layout (location=1) out vec2 uv;
layout(std430, buffer_reference) readonly buffer Positions {
  vec4 pos[]; // pos, initialAngle
};</code></pre>
</div>
<ol>
<li>As you may have noticed, the C++ code in the <em>How to do it…</em> section did not provide any index data to shaders. Instead, we a going to generate vertex data in the vertex shader. Let’s declare indices mapping right here. We need indices to construct <code>6</code> cube faces using triangles. Two triangles per face gives <code>6</code> points per face and <code>36</code> indices in total. That is the number passed to <code>vkCmdDraw()</code>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const int indices[36] = int[36](
  0,  2,  1,  2,  3,  1,  5,  4,  1,  1,  4,  0,
  0,  4,  6,  0,  6,  2,  6,  5,  7,  6,  4,  5,
  2,  6,  3,  6,  7,  3,  7,  1,  3,  7,  5,  1);</code></pre>
</div>
<ol>
<li>Here are the per-instance colors for our cubes.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const vec3 colors[7] = vec3[7](
  vec3(1.0, 0.0, 0.0),
  vec3(0.0, 1.0, 0.0),
  vec3(0.0, 0.0, 1.0),
  vec3(1.0, 1.0, 0.0),
  vec3(0.0, 1.0, 1.0),
  vec3(1.0, 0.0, 1.0),
  vec3(1.0, 1.0, 1.0));</code></pre>
</div>
<ol>
<li>As there are no translation and rotation matrices passed to the vertex shader, we have to generate everything ourselves right here. Here’s a GLSL function to apply a translation by a vector <code>v</code> to the current transformation <code>m</code>. This function is a counterpart to the C++ function <code>glm::translate()</code>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>mat4 translate(mat4 m, vec3 v) {
  mat4 Result = m;
  Result[3] = m[0] * v[0] + m[1] * v[1] + m[2] * v[2] + m[3];
  return Result;
}</code></pre>
</div>
<ol>
<li>Rotations are handled in a similar way. This is an analogue of <code>glm::rotate()</code> ported to GLSL.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>mat4 rotate(mat4 m, float angle, vec3 v) {
  float a = angle;
  float c = cos(a);
  float s = sin(a);
  vec3 axis = normalize(v);
  vec3 temp = (float(1.0) - c) * axis;
  mat4 r;
  r[0][0] = c + temp[0] * axis[0];
  r[0][1] = temp[0] * axis[1] + s * axis[2];
  r[0][2] = temp[0] * axis[2] - s * axis[1];
  r[1][0] = temp[1] * axis[0] - s * axis[2];
  r[1][1] = c + temp[1] * axis[1];
  r[1][2] = temp[1] * axis[2] + s * axis[0];
  r[2][0] = temp[2] * axis[0] + s * axis[1];
  r[2][1] = temp[2] * axis[1] - s * axis[0];
  r[2][2] = c + temp[2] * axis[2];
  mat4 res;
  res[0] = m[0] * r[0][0] + m[1] * r[0][1] + m[2] * r[0][2];
  res[1] = m[0] * r[1][0] + m[1] * r[1][1] + m[2] * r[1][2];
  res[2] = m[0] * r[2][0] + m[1] * r[2][1] + m[2] * r[2][2];
  res[3] = m[3];
  return res;
}</code></pre>
</div>
<ol>
<li>With this extensive arsenal at our disposal, we can now write the <code>main()</code> function of our vertex shader. The built-in <code>gl_InstanceIndex</code> variable is used to index the storage buffer and retrieve positions and angles. Then, a model matrix for the current cube is computed using the <code>rotate()</code> and <code>translate()</code> helper functions.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void main() {
  vec4 center = Positions(bufId).pos[gl_InstanceIndex];
  mat4 model = rotate(translate(mat4(1.0f), center.xyz),
                 time + center.w, vec3(1.0f, 1.0f, 1.0f));</code></pre>
</div>
<ol>
<li>The built-in <code>gl_VertexIndex</code> variable ranges from <code>0</code> to <code>35</code>, helping us extract the specific index for <code>8</code> of our vertices. Then we use this simple binary formula to generate <code>vec3</code> positions for each of those <code>8</code> vertices.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  uint idx = indices[gl_VertexIndex];
  vec3 xyz = vec3(idx &amp; 1, (idx &amp; 4) &gt;&gt; 2, (idx &amp; 2) &gt;&gt; 1);</code></pre>
</div>
<ol>
<li>Remap the <code>0...1</code> vertex coordinates into the <code>-1…+1</code> coordinates and scale by the desired <code>edge</code> length:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  const float edge = 1.0;
  gl_Position =
    viewproj * model * vec4(edge * (xyz - vec3(0.5)), 1.0);</code></pre>
</div>
<ol>
<li>The UV coordinates are selected on a per-face basis, and the colors are per instance:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  int face = gl_VertexIndex / 6;
  if (face == 0 || face == 3) uv = vec2(xyz.x, xyz.z);
  if (face == 1 || face == 4) uv = vec2(xyz.x, xyz.y);
  if (face == 2 || face == 5) uv = vec2(xyz.y, xyz.z);
  color = colors[gl_InstanceIndex % 7];
}</code></pre>
</div>
<ol>
<li>That’s all the magic that happens in the vertex shader. The fragment shader is pretty trivial and short:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout (location=0) in vec3 color;
layout (location=1) in vec2 uv;
layout (location=0) out vec4 out_FragColor;
layout(push_constant) uniform PerFrameData {
  mat4 proj;
  uint textureId;
};
void main() {
  out_FragColor = textureBindless2D(
    textureId, 0, uv) * vec4(color, 1.0);
}</code></pre>
</div>
<p>The running demo should look as in the following screenshot. You are flying through a swarm of 1 million cubes:</p>
<figure>
<img alt="Figure 5.3: One million cubes using instanced rendering" height="756" src="../media/file33.png" width="1430"/><figcaption aria-hidden="true">Figure 5.3: One million cubes using instanced rendering</figcaption>
</figure>
</section>
<section class="level3" data-number="6.5.4" id="theres-more-5">
<h3 data-number="6.5.4">There’s more…</h3>
<p>While this example is self-contained and very fast compared to non-instanced rendering, it can be made even faster if the indices are moved out of the vertex shader and stored in a dedicated index buffer to take advantage of the hardware vertex cache and model matrices are calculated per-instance and not per-vertex. We will cover this in the next recipe.</p>
<p>Now let’s extend this instancing example a bit further and draw some meshes using real mesh data.</p>
</section>
</section>
<section class="level2" data-number="6.6" id="implementing-instanced-meshes-with-compute-shaders">
<h2 data-number="6.6">Implementing instanced meshes with compute shaders</h2>
<p>In the last recipe, we learned the fundamentals of instanced rendering. Although that approach covers various aspects of rendering geometry instances, such as handling model matrices and materials, it’s not yet a practical implementation. Let’s expand on that example and demonstrate how to render instanced meshes loaded from a <code>.gltf</code> file.</p>
<p>To add a bit more complexity to this example, we’ll enhance it by precalculating per-instance model matrices using a compute shader.</p>
<section class="level3" data-number="6.6.1" id="getting-ready-29">
<h3 data-number="6.6.1">Getting ready</h3>
<p>Make sure you read the previous recipe, <em>Rendering instanced geometry</em>. The source code for this recipe can be found in <code>Chapter05/04_InstancedMeshes</code>.</p>
</section>
<section class="level3" data-number="6.6.2" id="how-to-do-it-4">
<h3 data-number="6.6.2">How to do it…</h3>
<p>Let’s skim the C++ code to understand the big picture</p>
<ol>
<li>First, we generate random positions and initial rotation angles for our meshes. We use 32,000 meshes because our GPU cannot handle 1 million meshes with this naïve brute-force approach. Pushing it to 1 million meshes is possible, and we will show some tricks to approach that number in <em>Chapter 11</em>, <em>Advanced Rendering Techniques and Optimizations</em>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const uint32_t kNumMeshes = 32 * 1024;
std::vector&lt;vec4&gt; centers(kNumMeshes);
for (vec4&amp; p : centers)
   p = vec4(glm::linearRand(-vec3(500.0f), +vec3(500.0f)),
            glm::linearRand(0.0f, 3.14159f));</code></pre>
</div>
<ol>
<li>The center points and angles are loaded into a storage buffer in exactly the same way as in the previous recipe:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::BufferHandle&gt; bufferPosAngle   = ctx-&gt;createBuffer({
  .usage     = lvk::BufferUsageBits_Storage,
  .storage   = lvk::StorageType_Device,
  .size      = sizeof(vec4) * kNumMeshes,
  .data      = centers.data(),
  .debugName = “Buffer: angles &amp; positions”,
});</code></pre>
</div>
<ol>
<li>To store model matrices for our instances, we’ll require two buffers. We’ll alternate between them in a round-robin fashion during even and odd frames to prevent unnecessary synchronization.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::BufferHandle&gt; bufferMatrices[] = {
  ctx-&gt;createBuffer({ .usage     = lvk::BufferUsageBits_Storage,
                      .storage   = lvk::StorageType_Device,
                      .size      = sizeof(mat4) * kNumMeshes,
                      .debugName = “Buffer: matrices 1” }),
  ctx-&gt;createBuffer({ .usage     = lvk::BufferUsageBits_Storage,
                      .storage   = lvk::StorageType_Device,
                      .size      = sizeof(mat4) * kNumMeshes,
                      .debugName = “Buffer: matrices 2” }),
};.</code></pre>
</div>
<ol>
<li>The rubber duck 3D model is loaded from <code>.gltf</code> the following way. This time, besides vertex positions and texture coordinates, we require normal vectors to do some improvized lighting. We are going to need it when we render so many meshes.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const aiScene* scene =
  aiImportFile(“data/rubber_duck/scene.gltf”, aiProcess_Triangulate);
struct Vertex {
  vec3 pos;
  vec2 uv;
  vec3 n;
};
const aiMesh* mesh = scene-&gt;mMeshes[0];
std::vector&lt;Vertex&gt; vertices;
std::vector&lt;uint32_t&gt; indices;
for (unsigned int i = 0; i != mesh-&gt;mNumVertices; i++) {
  const aiVector3D v = mesh-&gt;mVertices[i];
  const aiVector3D t = mesh-&gt;mTextureCoords[0][i];
  const aiVector3D n = mesh-&gt;mNormals[i];
  vertices.push_back({ .pos = vec3(v.x, v.y, v.z),
                       .uv  = vec2(t.x, t.y),
                       .n   = vec3(n.x, n.y, n.z) });
}
for (unsigned int i = 0; i != mesh-&gt;mNumFaces; i++)
  for (int j = 0; j != 3; j++)
    indices.push_back(mesh-&gt;mFaces[i].mIndices[j]);
aiReleaseImport(scene);</code></pre>
</div>
<ol>
<li>The mesh data is uploaded into index and vertex buffers:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::BufferHandle&gt; vertexBuffer = ctx-&gt;createBuffer({
  .usage     = lvk::BufferUsageBits_Storage,
  .storage   = lvk::StorageType_Device,
  .size      = sizeof(Vertex) * vertices.size(),
  .data      = vertices.data(),
  .debugName = “Buffer: vertex” }, nullptr);
lvk::Holder&lt;lvk::BufferHandle&gt; indexBuffer = ctx-&gt;createBuffer({
  .usage     = lvk::BufferUsageBits_Index,
  .storage   = lvk::StorageType_Device,
  .size      = sizeof(uint32_t) * indices.size(),
  .data      = indices.data(),
  .debugName = “Buffer: index” }, nullptr);</code></pre>
</div>
<ol>
<li>Let’s load the texture and create compute and rendering pipelines. The compute shader will generate model matrices for our instances based on the elapsed time, following the approach used in the vertex shader in the previous recipe, <em>Rendering instanced meshes</em>. However, this time, we’ll do it on a per-instance basis instead of per vertex.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::TextureHandle&gt; texture =
  loadTexture(ctx, “data/rubber_duck/textures/Duck_baseColor.png”);
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; comp =
  loadShaderModule(ctx, “Chapter05/04_InstancedMeshes/src/main.comp”);
lvk::Holder&lt;lvk::ComputePipelineHandle&gt; pipelineComputeMatrices =
  ctx-&gt;createComputePipeline({ smComp = comp });
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; vert =
  loadShaderModule(ctx, “Chapter05/04_InstancedMeshes/src/main.vert”);
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; frag =
  loadShaderModule(ctx, “Chapter05/04_InstancedMeshes/src/main.frag”);
lvk::Holder&lt;lvk::RenderPipelineHandle&gt; pipelineSolid =
  ctx-&gt;createRenderPipeline({
    .smVert      = vert,
    .smFrag      = frag,
    .color       = { { .format = ctx-&gt;getSwapchainFormat() } },
    .depthFormat = app.getDepthFormat(),
    .cullMode    = lvk::CullMode_Back });</code></pre>
</div>
<ol>
<li>The main loop goes like this. We use the <code>frameId</code> counter to facilitate the switching of buffers containing model matrices between even and odd frames.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>uint32_t frameId = 0;
app.run([&amp;](uint32_t width, uint32_t height,
            float aspectRatio, float deltaSeconds) {
  const mat4 proj =
    glm::perspective(45.0f, aspectRatio, 0.2f, 1500.0f);
  const lvk::RenderPass renderPass = {
    .color = { { .loadOp = lvk::LoadOp_Clear,
                .clearColor = { 1.0f, 1.0f, 1.0f, 1.0f } } },
    .depth = { .loadOp = lvk::LoadOp_Clear, .clearDepth = 1.0f }
  };
  const lvk::Framebuffer framebuffer = {
    .color = { { .texture = ctx-&gt;getCurrentSwapchainTexture() } },
    .depthStencil = { .texture = app.getDepthTexture() },
  };
  lvk::ICommandBuffer&amp; buf = ctx-&gt;acquireCommandBuffer();</code></pre>
</div>
<ol>
<li>For convenience, push constants are shared between compute and rendering pipelines:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  const mat4 view = translate(mat4(1.0f), vec3(0.0f, 0.0f,
    -1000.0f + 500.0f * (1.0f - cos(-glfwGetTime() * 0.5f))));
  const struct {
    mat4 viewproj;
    uint32_t textureId;
    uint64_t bufferPosAngle;
    uint64_t bufferMatrices;
    uint64_t bufferVertices;
    float time;
  } pc {
    .viewproj       = proj * view,
    .textureId      = texture.index(),
    .bufferPosAngle = ctx-&gt;gpuAddress(bufferPosAngle),
    .bufferMatrices = ctx-&gt;gpuAddress(bufferMatrices[frameId]),
    .bufferVertices = ctx-&gt;gpuAddress(vertexBuffer),
    .time           = (float)glfwGetTime(),
  };
  buf.cmdPushConstants(pc);</code></pre>
</div>
<ol>
<li>Dispatch the compute shader. Each local workgroup handles 32 meshes – a common portable baseline supported by many GPUs:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  buf.cmdBindComputePipeline(pipelineComputeMatrices);
  buf.cmdDispatchThreadGroups({ .width = kNumMeshes / 32 } });</code></pre>
</div>
<ol>
<li>After the compute shader has finished updating model matrices, we can start rendering. Note that we have a non-empty dependencies parameter here, which refers to the buffer with model matrices. This is necessary to make sure a proper Vulkan buffer memory barrier is issued by <em>LightweightVK</em> to prevent race conditions between the compute shader and the vertex shader.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  buf.cmdBeginRendering(renderPass, framebuffer,
    { .buffers = { lvk::BufferHandle(bufferMatrices[frameId]) } });</code></pre>
</div>
<p>Now let’s look at the barrier. The source and destination stages, respectively, are:</p>
<div class="C1-SHCodePACKT">
<pre><code>VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT</code></pre>
</div>
<p>And:</p>
<div class="C1-SHCodePACKT">
<pre><code>VK_PIPELINE_STAGE_VERTEX_SHADER_BIT | VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT</code></pre>
</div>
<p>The underlying Vulkan barrier looks as follows:</p>
<div class="C1-SHCodePACKT">
<pre><code>void lvk::CommandBuffer::bufferBarrier(BufferHandle handle,
  VkPipelineStageFlags srcStage, VkPipelineStageFlags dstStage)
{
  lvk::VulkanBuffer* buf = ctx_-&gt;buffersPool_.get(handle);
  const VkBufferMemoryBarrier barrier = {
    .sType = VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER,
    .srcAccessMask = VK_ACCESS_SHADER_READ_BIT |
                     VK_ACCESS_SHADER_WRITE_BIT,
    .dstAccessMask = VK_ACCESS_SHADER_READ_BIT |
                     VK_ACCESS_SHADER_WRITE_BIT,
    .srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED,
    .dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED,
    .buffer = buf-&gt;vkBuffer_,
    .offset = 0,
    .size = VK_WHOLE_SIZE,
  };
  vkCmdPipelineBarrier(wrapper_-&gt;cmdBuf_, srcStage, dstStage,
    VkDependencyFlags{}, 0, nullptr, 1, &amp;barrier, 0, nullptr);
}</code></pre>
</div>
<ol>
<li>The rest of the rendering code is pretty standard. The <em>LightweightVK</em> draw call command, <code>cmdDrawIndexed()</code>, takes the number of indices in our mesh and the number of instances <code>kNumMeshes</code>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  buf.cmdBindRenderPipeline(pipelineSolid);
  buf.cmdBindDepthState({
    .compareOp = lvk::CompareOp_Less, .isDepthWriteEnabled = true });
  buf.cmdBindIndexBuffer(indexBuffer, lvk::IndexFormat_UI32);
  buf.cmdDrawIndexed(indices.size(), kNumMeshes);
  buf.cmdEndRendering();
  ctx-&gt;submit(buf, ctx-&gt;getCurrentSwapchainTexture());
  frameId = (frameId + 1) &amp; 1;
});</code></pre>
</div>
<p>Now, let’s delve into the GLSL implementation details to understand how it works internally.</p>
</section>
<section class="level3" data-number="6.6.3" id="how-it-works-10">
<h3 data-number="6.6.3">How it works…</h3>
<p>The first part is the compute shader, which prepares data for rendering. Let’s take a look at <code>Chapter05/04_InstancedMeshes/src/main.comp</code>:</p>
<ol>
<li>The compute shader processes 32 meshes in one local workgroup. Push constants are shared between the compute shader and the graphics pipeline. They are declared in an include file, <code>Chapter05/04_InstancedMeshes/src/common.sp</code>. We provide that file here for your convenience:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout(local_size_x = 32, local_size_y = 1, local_size_z = 1) in;
// included from &lt;Chapter05/04_InstancedMeshes/src/common.sp&gt;
layout(push_constant) uniform PerFrameData {
  mat4 viewproj;
  uint textureId;
  uvec2 bufPosAngleId;
  uvec2 bufMatricesId;
  uvec2 bufVerticesId;
  float time;
};
layout(std430, buffer_reference) readonly buffer Positions {
  vec4 pos[]; // pos, initialAngle
};
// end of #include</code></pre>
</div>
<ol>
<li>The matrices buffer reference declaration is not shared. Here, in the compute shader, it is declared as <code>writeonly</code>, while the vertex shader will declare it as <code>readonly</code>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout(std430, buffer_reference) writeonly buffer Matrices {
  mat4 mtx[];
};</code></pre>
</div>
<ol>
<li>Helper functions <code>translate()</code> and <code>rotate()</code> mimic the <code>glm::translate()</code> and <code>glm::rotate()</code> C++ functions in GLSL. They are reused from the previous recipe, <em>Rendering instanced geometry</em>, in their entirety. As they are quite long, we will not duplicate them here.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>mat4 translate(mat4 m, vec3 v);
mat4 rotate(mat4 m, float angle, vec3 v);</code></pre>
</div>
<ol>
<li>The <code>main()</code> function reads a <code>vec4</code> value containing the center point and initial angle, and calculates a model matrix. This is exactly the same computation we did in the vertex shader in the previous recipe. The model matrix is then stored in a storage buffer referenced by <code>bufMatricesId</code>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void main() {
  uint idx = gl_GlobalInvocationID.x;
  vec4 center = Positions(bufPosAngleId).pos[idx];
  mat4 model = rotate(translate(mat4(1.0f),
    center.xyz), time + center.w, vec3(1.0f, 1.0f, 1.0f));
  Matrices(bufMatricesId).mtx[idx] = model;
}</code></pre>
</div>
<p>As we moved most of the calculations into the compute shader, the shaders for the rendering pipeline became significantly shorter.</p>
<ol>
<li>The vertex shader uses the same shared declarations for push constants and buffer references:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>#include &lt;Chapter05/04_InstancedMeshes/src/common.sp&gt;
layout (location=0) out vec2 uv;
layout (location=1) out vec3 normal;
layout (location=2) out vec3 color;
layout(std430, buffer_reference) readonly buffer Matrices {
  mat4 mtx[];
};</code></pre>
</div>
<ol>
<li>The vertex data contains normal vectors, as was declared in the C++ code earlier in this recipe:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>struct Vertex {
  float x, y, z;
  float u, v;
  float nx, ny, nz;
};
layout(std430, buffer_reference) readonly buffer Vertices {
  Vertex in_Vertices[];
};</code></pre>
</div>
<ol>
<li>The vertex data is retrieved from the “vertex” storage buffer, and the model matrix is obtained from the matrices buffer, which was updated by the compute shader:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const vec3 colors[3] = vec3[3](vec3(1.0, 0.0, 0.0),
                               vec3(0.0, 1.0, 0.0),
                               vec3(1.0, 1.0, 1.0));
void main() {
  Vertex vtx = Vertices(bufVerticesId).in_Vertices[gl_VertexIndex];
  mat4 model = Matrices(bufMatricesId).mtx[gl_InstanceIndex];</code></pre>
</div>
<ol>
<li>Now we can calculate the value of <code>gl_Position</code> and pass the normal vector and texture coordinates to our fragment shader:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  const float scale = 10.0;
  gl_Position = viewproj * model *
    vec4(scale * vtx.x, scale * vtx.y, scale * vtx.z, 1.0);
  mat3 normalMatrix = transpose( inverse(mat3(model)) );
  uv = vec2(vtx.u, vtx.v);
  normal = normalMatrix * vec3(vtx.nx, vtx.ny, vtx.nz);
  color = colors[gl_InstanceIndex % 3];
}</code></pre>
</div>
<ol>
<li>The fragment shader is quite straightforward. We perform some improvized diffuse lighting calculations to enhance the distinctiveness of the meshes:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout (location=0) in vec2 uv;
layout (location=1) in vec3 normal;
layout (location=2) in vec3 color;
layout (location=0) out vec4 out_FragColor;
layout(push_constant) uniform PerFrameData {
  mat4 viewproj;
  uint textureId;
};
void main() {
  vec3 n = normalize(normal);
  vec3 l = normalize(vec3(1.0, 0.0, 1.0));
  float NdotL = clamp(dot(n, l), 0.3, 1.0);
  out_FragColor =
    textureBindless2D(textureId, 0, uv) * NdotL * vec4(color, 1.0);
};</code></pre>
</div>
<p>The running demo application should render a swarm of rotating rubber ducks, as shown in the following screenshot, while the camera flies through them.</p>
<figure>
<img alt="Figure 5.4: A swarm of rotating rubber ducks using instanced rendering" height="756" src="../media/file34.png" width="1430"/><figcaption aria-hidden="true">Figure 5.4: A swarm of rotating rubber ducks using instanced rendering</figcaption>
</figure>
</section>
<section class="level3" data-number="6.6.4" id="theres-more-6">
<h3 data-number="6.6.4">There’s more…</h3>
<p>As you may have noticed, this demo utilizes only <code>32,768</code> instances compared to <code>1</code> million instances in the previous recipe. The reason for this difference is that the cube used in the previous example had only <code>36</code> indices, while the rubber duck model in this case has <code>33,216</code>, which is almost 1,000 times more.</p>
<p>The naive brute force approach won’t suffice for this dataset. We need to employ additional tricks to render <code>1</code> million duckies, such as culling and GPU-level-of-detail management. We’ll delve into some of these topics in <em>Chapter 11</em>, <em>Advanced Rendering Techniques and Optimizations</em>.</p>
<p>Now, let’s switch gears and learn how to render some debug grid geometry before proceeding with examples of more complex mesh rendering.</p>
</section>
</section>
<section class="level2" data-number="6.7" id="implementing-an-infinite-grid-glsl-shader">
<h2 data-number="6.7">Implementing an infinite grid GLSL shader</h2>
<p>In the previous recipes of this chapter, we learned how to approach geometry rendering. To debug our applications, it is useful to have a visible representation of the coordinate system so that a viewer can quickly infer the camera orientation and position just by looking at a rendered image. A natural way to represent a coordinate system in the image is to render an infinite grid where the grid plane is aligned with one of the coordinate planes. Let’s learn how to implement a decent-looking grid in GLSL.</p>
<section class="level3" data-number="6.7.1" id="getting-ready-30">
<h3 data-number="6.7.1">Getting ready</h3>
<p>The full C++ source code for this recipe can be found in <code>Chapter05/05_Grid</code>. The corresponding GLSL shaders will be reused in subsequent recipes, so they are located in the shared data folder in the <code>data/shaders/Grid.vert</code> and <code>data/shaders/Grid.frag</code> files.</p>
</section>
<section class="level3" data-number="6.7.2" id="how-to-do-it...-27">
<h3 data-number="6.7.2">How to do it...</h3>
<p>To parametrize our grid, we should introduce some constants. They can be found and tweaked in the <code>data/shaders/GridParameters.h</code> GLSL include file. Let’s take a look inside:</p>
<ol>
<li>First of all, we need to define the size of our grid extents in the world coordinates. That is how far from the camera the grid is visible:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>float gridSize = 100.0;</code></pre>
</div>
<ol>
<li>The size of one grid cell is specified in the same units as the grid size:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>float gridCellSize = 0.025;</code></pre>
</div>
<ol>
<li>Let’s define the colors of the grid lines. We will use two different colors, one for regular thin lines and the other for thick lines, which are rendered in every tenth line. Since we render everything against a white background, we are good with black and 50% gray.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>vec4 gridColorThin = vec4(0.5, 0.5, 0.5, 1.0);
vec4 gridColorThick = vec4(0.0, 0.0, 0.0, 1.0);</code></pre>
</div>
<ol>
<li>Our grid implementation will change the number of rendered lines based on the grid LOD. We will switch LOD when the number of pixels between two adjacent grid cell lines drops below this value, as calculated in the fragment shader:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const float gridMinPixelsBetweenCells = 2.0;</code></pre>
</div>
<ol>
<li>Let’s take a look at a simple vertex shader we use to generate and transform grid vertices. It takes in the current model-view-projection matrix, the current camera position, and the grid origin. The origin is in world space and can be used to move the grid around.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout(push_constant) uniform PerFrameData {
  mat4 MVP;
  vec4 cameraPos;
  vec4 origin;
};
layout (location=0) out vec2 uv;
layout (location=1) out vec2 out_camPos;
const vec3 pos[4] = vec3[4](
  vec3(-1.0, 0.0, -1.0),
  vec3( 1.0, 0.0, -1.0),
  vec3( 1.0, 0.0,  1.0),
  vec3(-1.0, 0.0,  1.0)
);
const int indices[6] = int[6]( 0, 1, 2, 2, 3, 0 );</code></pre>
</div>
<ol>
<li>The built-in <code>gl_VertexIndex</code> variable is used to access hardcoded quad indices and vertices <code>pos[]</code>. The <code>-1…+1</code> points are scaled by the desired grid size. The resulting vertex position is translated by the 2D camera in the horizontal plane and, then, by the 3D origin position:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void main() {
  int idx = indices[gl_VertexIndex];
  vec3 position = pos[idx] * gridSize;
  position.x += cameraPos.x;
  position.z += cameraPos.z;
  position += origin.xyz;
  out_camPos = cameraPos.xz;
  gl_Position = MVP * vec4(position, 1.0);
  uv = position.xz;
}</code></pre>
</div>
<p>The fragment shader is somewhat more complex. It will calculate a programmatic texture that looks like a grid. The grid lines are rendered based on how fast the <code>uv</code> coordinates change in the screen space to avoid the Moiré pattern, hence we are going to need screen space derivatives. The screen space derivative of a variable in your shader measures how much that variable changes from one pixel to the next. The GLSL function <code>dFdx()</code> represents the horizontal change, while <code>dFdy()</code> represents the vertical change. It measures how fast a GLSL variable changes as you move across the screen, approximating its partial derivatives in calculus terms. This approximation is due to relying on discrete samples at each fragment, rather than doing a mathematical evaluation of the change:</p>
<ol>
<li>First, we introduce a bunch of GLSL helper functions to aid our calculations. They can be found in <code>data/shaders/GridCalculation.h</code>. The function names <code>satf()</code> and <code>satv()</code> stand for saturate-float and saturate-vector respectively:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>float log10(float x) {
  return log(x) / log(10.0);
}
float satf(float x) {
  return clamp(x, 0.0, 1.0);
}
vec2 satv(vec2 x) {
  return clamp(x, vec2(0.0), vec2(1.0));
}
float max2(vec2 v) {
  return max(v.x, v.y);
}</code></pre>
</div>
<ol>
<li>Let’s look into the <code>gridColor()</code> function, which is invoked from <code>main()</code>, and start by calculating the screen space length of the derivatives of the <code>uv</code> coordinates we previously generated in the vertex shader. We use built-in <code>dFdx()</code> and <code>dFdy()</code> functions to calculate the required derivatives:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>vec2 dudv = vec2( length(vec2(dFdx(uv.x), dFdy(uv.x))),
                  length(vec2(dFdx(uv.y), dFdy(uv.y))) );</code></pre>
</div>
<ol>
<li>Knowing the derivatives, the current LOD of our grid can be calculated in the following way. The <code>gridMinPixelsBetweenCells</code> value controls how fast we want our LOD to increase. In this case, it is the minimum number of pixels between two adjacent cell lines of the grid:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>float lodLevel = max(0.0, log10((length(dudv) *
  gridMinPixelsBetweenCells) / gridCellSize) + 1.0);
float lodFade = fract(lodLevel);</code></pre>
</div>
<p>Besides the LOD value, we are going to need a fading factor to render smooth transitions between the adjacent levels. It can be obtained by taking a fractional part of the floating point LOD level. The logarithm base <code>10</code> is used to ensure each LOD covers <code>pow(10, lodLevel)</code> more cells than the previous size.</p>
<ol>
<li>The LOD levels are blended with each other. To render them, we have to calculate the cell size for each LOD. Here, instead of calculating <code>pow()</code> three times, which is done purely for the sake of explanation, we can calculate it only for <code>lod0</code> and multiply each subsequent LOD cell size by <code>10.0</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>float lod0 = gridCellSize * pow(10.0, floor(lodLevel));
float lod1 = lod0 * 10.0;
float lod2 = lod1 * 10.0;</code></pre>
</div>
<ol>
<li>To be able to draw anti-aliased lines using alpha transparency, we need to increase the screen coverage of our lines. Let’s make sure each line covers up to <code>4</code> pixels. Shift grid coordinates to the centers of anti-aliased lines for subsequent alpha calculations:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>dudv *= 4.0;
uv += dudv * 0.5;</code></pre>
</div>
<ol>
<li>Now we should get coverage alpha value corresponding to each calculated LOD level. To do that, we calculate absolute distances to cell line centers for each LOD and pick the maximum coordinate:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>float lod0a = max2( vec2(1.0) - abs(
  satv(mod(uv, lod0) / dudv) * 2.0 - vec2(1.0)) );
float lod1a = max2( vec2(1.0) - abs(
  satv(mod(uv, lod1) / dudv) * 2.0 - vec2(1.0)) );
float lod2a = max2( vec2(1.0) - abs(
  satv(mod(uv, lod2) / dudv) * 2.0 - vec2(1.0)) );</code></pre>
</div>
<ol>
<li>Non-zero alpha values represent non-empty transition areas of the grid. Let’s blend between them using two colors to handle LOD transitions:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>vec4 c = lod2a &gt; 0.0 ?
  gridColorThick :
  lod1a &gt; 0.0 ?
    mix(gridColorThick, gridColorThin, lodFade) : gridColorThin;</code></pre>
</div>
<ol>
<li>Last but not least, make the grid disappear when it is far away from the camera. Use the <code>gridSize</code> value to calculate the opacity falloff:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>uv -= camPos;
float opacityFalloff = (1.0 - satf(length(uv) / gridSize));</code></pre>
</div>
<ol>
<li>Now we can blend between the LOD level alpha values and scale the result with the opacity falloff factor. The resulting pixel color value can be stored in the framebuffer:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>c.a *= 
  lod2a &gt; 0.0 ? lod2a : lod1a &gt; 0.0 ? lod1a : (lod0a * (1.0-lodFade));
c.a *= opacityFalloff;
out_FragColor = c;</code></pre>
</div>
<ol>
<li>The abovementioned shaders in <code>data/shaders/GridCalculation.h</code> should be rendered using the following render pipeline state, which is created in <code>Chapter05/05_Grid/src/main.cpp</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::RenderPipelineHandle&gt; pipeline =
  ctx-&gt;createRenderPipeline({
    .smVert      = vert,
    .smFrag      = frag,
    .color       = { {
      .format            = ctx-&gt;getSwapchainFormat(),
      .blendEnabled      = true,
      .srcRGBBlendFactor = lvk::BlendFactor_SrcAlpha,
      .dstRGBBlendFactor = lvk::BlendFactor_OneMinusSrcAlpha,
    } },
    .depthFormat = app.getDepthFormat() });</code></pre>
</div>
<ol>
<li>The C++ rendering code in the same file looks as follows:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>buf.cmdBindRenderPipeline(pipeline);
buf.cmdBindDepthState({});
struct {
  mat4 mvp;
  vec4 camPos;
  vec4 origin;
} pc = {
  .mvp    = glm::perspective(
    45.0f, aspectRatio, 0.1f, 1000.0f) * app.camera_.getViewMatrix(),
  .camPos = vec4(app.camera_.getPosition(), 1.0f),
  .origin = vec4(0.0f),
};
buf.cmdPushConstants(pc);
buf.cmdDraw(6);</code></pre>
</div>
<p>Check out the complete <code>Chapter05/05_Grid</code> for a self-contained demo app. The camera can be controlled with the WASD keys and a mouse. The resulting image should look like in the following screenshot:</p>
<figure>
<img alt="Figure 5.5: GLSL grid" height="756" src="../media/file35.png" width="1430"/><figcaption aria-hidden="true">Figure 5.5: GLSL grid</figcaption>
</figure>
</section>
<section class="level3" data-number="6.7.3" id="theres-more...-10">
<h3 data-number="6.7.3">There’s more...</h3>
<p>Besides considering only the distance to the camera to calculate the antialiasing falloff factor, we can use the angle between the viewing vector and the grid line. This will make the overall look and feel of the grid more visually pleasing and can be an interesting improvement if you want to implement a grid not only as an internal debugging tool but also as a part of a customer-facing product, like an editor.</p>
<p>This implementation was inspired by the <em>Our Machinery</em> blog. Unfortunately, it is not available anymore. However, there are some other advanced materials available on the internet showing how to render a more complex grid suitable for customer-facing rendering. Make sure you read the blog post <em>The Best Darn Grid Shader (Yet)</em> by Ben Golus <a href="https://bgolus.medium.com/the-best-darn-grid-shader-yet-727f9278b9d8">https://bgolus.medium.com/the-best-darn-grid-shader-yet-727f9278b9d8</a>, which takes grid rendering a lot further.</p>
<p>Before we move on to the next recipe, we want to mention that grid rendering is quite handy, and we’ve included it in most of our subsequent demo applications. You can use the <code>VulkanApp::drawGrid()</code> function to render this grid anywhere you want.</p>
</section>
</section>
<section class="level2" data-number="6.8" id="integrating-tessellation-into-the-graphics-pipeline">
<h2 data-number="6.8">Integrating tessellation into the graphics pipeline</h2>
<p>Let’s switch gears and learn how to integrate hardware tessellation into the Vulkan graphics rendering pipeline.</p>
<p>Hardware tessellation is implemented as a set of two new shader stage types in the graphics pipeline. The first shader stage is called the <strong>tessellation control shader</strong>, and the second stage is called the <strong>tessellation evaluation shader</strong>. The tessellation control shader operates on a set of vertices, which are called control points and define a geometric surface called a patch. The shader can manipulate the control points and calculate the required tessellation level. The tessellation evaluation shader can access the barycentric coordinates of the tessellated triangles and can use them to interpolate any required per-vertex attributes such as texture coordinates and colors. Let’s go through the code to see how these new shader stages can be used to triangulate a mesh depending on the distance to the camera.</p>
<p>Using tessellation shaders for hardware tessellation might not be as efficient on modern GPUs as using mesh shaders. Regrettably, as of the time this book was written, there is no standardized API for mesh shaders in core Vulkan. So, for now, let’s stick with the old tessellation approach.</p>
<section class="level3" data-number="6.8.1" id="getting-ready-31">
<h3 data-number="6.8.1">Getting ready</h3>
<p>The complete source code for this recipe is located in <code>Chapter05/06_Tessellation</code>.</p>
</section>
<section class="level3" data-number="6.8.2" id="how-to-do-it...-28">
<h3 data-number="6.8.2">How to do it...</h3>
<p>What we want to do now is write shaders that will calculate per-vertex tessellation levels based on the distance to the camera. This way, we can render more geometrical details in the areas that are closer to the viewer. To do that, we should start with the <code>Chapter05/06_Tessellation/src/main.vert</code> vertex shader, which will compute the world positions of vertices and pass them down to the tessellation control shader:</p>
<ol>
<li>Our per-frame data consist of the usual view and projection matrices, together with the current camera position in the world space, and the tessellation scaling factor, which is user-controllable and comes from an ImGui widget. This data does not fit into <code>128</code> bytes of push constants, so we put everything into a buffer. Geometry is accessed using the PVP technique and stored in the following format using <code>vec3</code> for vertex positions and <code>vec2</code> for texture coordinates:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>// included from &lt;Chapter05/06_Tessellation/src/common.sp&gt;
struct Vertex {
  float x, y, z;
  float u, v;
};
layout(std430, buffer_reference) readonly buffer Vertices {
  Vertex in_Vertices[];
};
layout(std430, buffer_reference) readonly buffer PerFrameData {
  mat4 model;
  mat4 view;
  mat4 proj;
  vec4 cameraPos;
  uint texture;
  float tesselationScale;
  Vertices vtx;
};
layout(push_constant) uniform PushConstants {
  PerFrameData pc;
};</code></pre>
</div>
<ol>
<li>Let’s write some helper functions to access vertex positions and texture coordinates using the traditional GLSL data types:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>vec3 getPosition(int i) {
  return vec3(pc.vtx.in_Vertices[i].x,
              pc.vtx.in_Vertices[i].y,
              pc.vtx.in_Vertices[i].z);
}
vec2 getTexCoord(int i) {
  return vec2(pc.vtx.in_Vertices[i].u,
              pc.vtx.in_Vertices[i].v);
}</code></pre>
</div>
<ol>
<li>The vertex shader outputs UV texture coordinates and per-vertex world positions. The actual calculation is done as follows:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout (location=0) out vec2 uv_in;
layout (location=1) out vec3 worldPos_in;
void main() {
  vec4 pos = vec4(getPosition(gl_VertexIndex), 1.0);
  gl_Position = pc.proj * pc.view * pc.model * pos;
  uv_in = getTexCoord(gl_VertexIndex);
  worldPos_in = (pc.model * pos).xyz;
}</code></pre>
</div>
<p>Now we can go further to the next shader stage and take a look at the tessellation control shader <code>Chapter05/06_Tessellation/src/main.tesc</code>:</p>
<ol>
<li>The shader operates on a group of <code>3</code> vertices that correspond to a single triangle in the input data. The <code>uv_in</code> and <code>worldPos_in</code> variables correspond to the ones in the vertex shader. Note how here we have arrays instead of single solitary values. The <code>PerFrameData</code> structure should be exactly the same for all shader stages in this example and comes from <code>common.sp</code>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>#include &lt;Chapter05/06_Tessellation/src/common.sp&gt;
layout (vertices = 3) out;
layout (location = 0) in vec2 uv_in[];
layout (location = 1) in vec3 worldPos_in[];</code></pre>
</div>
<ol>
<li>Let’s describe the input and output data structures that correspond to each individual vertex. Besides the required vertex position, we store <code>vec2</code> texture coordinates:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>in gl_PerVertex {
  vec4 gl_Position;
} gl_in[];
out gl_PerVertex {
  vec4 gl_Position;
} gl_out[];
struct vertex {
  vec2 uv;
};
struct vertex {
  vec2 uv;
};
layout(location = 0) out vertex Out[];</code></pre>
</div>
<ol>
<li>The <code>getTessLevel()</code> function calculates the desired tessellation level based on the distance of two adjacent vertices from the camera. The hardcoded distance values that are used to switch the levels are scaled using the <code>tessellationScale</code> uniform coming from the UI:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>float getTessLevel(float distance0, float distance1) {
  const float distanceScale1 = 1.2;
  const float distanceScale2 = 1.7;
  const float avgDistance =
    (distance0 + distance1) / (2.0 * pc.tesselationScale);
  if (avgDistance &lt;= distanceScale1) return 5.0;
  if (avgDistance &lt;= distanceScale2) return 3.0;
  return 1.0;
}</code></pre>
</div>
<ol>
<li>The <code>main()</code> function is straightforward. It passes the positions and UV coordinates as-is and then calculates the distance from each vertex in the triangle to the camera:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void main() {  
  gl_out[gl_InvocationID].gl_Position =
    gl_in[gl_InvocationID].gl_Position;
  Out[gl_InvocationID].uv = uv_in[gl_InvocationID];
  vec3 c = pc.cameraPos.xyz;
  float eyeToVertexDistance0 = distance(c, worldPos_in[0]);
  float eyeToVertexDistance1 = distance(c, worldPos_in[1]);
  float eyeToVertexDistance2 = distance(c, worldPos_in[2]);</code></pre>
</div>
<ol>
<li>Based on these distances, we can calculate the required inner and outer tessellation levels in the following way. The inner tessellation level defines how the inner part of a triangle is subdivided into smaller triangles. The outer level defines how the outer edges of the triangle are subdivided so that they can be correctly connected to adjacent triangles:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  gl_TessLevelOuter[0] =
    getTessLevel(eyeToVertexDistance1, eyeToVertexDistance2);
  gl_TessLevelOuter[1] =
    getTessLevel(eyeToVertexDistance2, eyeToVertexDistance0);
  gl_TessLevelOuter[2] =
    getTessLevel(eyeToVertexDistance0, eyeToVertexDistance1);
  gl_TessLevelInner[0] = gl_TessLevelOuter[2];
};</code></pre>
</div>
<p>Let’s take a look at the tessellation evaluation shader <code>Chapter05/06_Tessellation/src/main.tese</code>:</p>
<ol>
<li>We should specify triangles as the input. The <code>equal_spacing</code> spacing mode tells Vulkan that the tessellation level <code>n</code> should be clamped to the range <code>0...64</code> and rounded to the nearest integer. After that, the corresponding edge should be divided into <code>n</code> equal segments. When the tessellation primitive generator produces triangles, the orientation of triangles can be specified by an input layout declaration using the identifiers <code>cw</code> and <code>ccw</code>. We use the counter-clockwise orientation:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout(triangles, equal_spacing, ccw) in;
struct vertex {
  vec2 uv;
};
in gl_PerVertex {
  vec4 gl_Position;
} gl_in[];
layout(location = 0) in vertex In[];
out gl_PerVertex {
  vec4 gl_Position;
};
layout (location=0) out vec2 uv;</code></pre>
</div>
<ol>
<li>These two helper functions are useful for interpolating between the <code>vec2</code> and <code>vec4</code> attribute values at the corners of the original triangle using barycentric coordinates of the current vertex. The built-in <code>gl_TessCoord</code> variable contains the required barycentric coordinates, <code>0…1</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>vec2 interpolate2(in vec2 v0, in vec2 v1, in vec2 v2) {
  return v0 * gl_TessCoord.x +
         v1 * gl_TessCoord.y +
         v2 * gl_TessCoord.z;
}
vec4 interpolate4(in vec4 v0, in vec4 v1, in vec4 v2) {
  return v0 * gl_TessCoord.x +
         v1 * gl_TessCoord.y +
         v2 * gl_TessCoord.z;
}</code></pre>
</div>
<ol>
<li>The actual interpolation code in <code>main()</code> is straightforward and can be written in the following way:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void main() {
  gl_Position = interpolate4(gl_in[0].gl_Position,
                             gl_in[1].gl_Position,
                             gl_in[2].gl_Position);
  uv = interpolate2(In[0].uv, In[1].uv, In[2].uv);
};</code></pre>
</div>
<p>The next stage of our hardware tessellation graphics pipeline is the geometry shader <code>Chapter05/06_Tessellation/src/main.geom</code>. We use it to generate barycentric coordinates for all the small tessellated triangles. It is used to render a nice antialiased wireframe overlay on top of our colored mesh, as we did earlier in this chapter in the <em>Generating LODs using MeshOptimizer</em> recipe:</p>
<ol>
<li>The geometry shader consumes triangles generated by the hardware tessellator and outputs triangle strips, each consisting of a single triangle:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>#version 460 core
layout(triangles) in;
layout(triangle_strip, max_vertices = 3) out;
layout(location=0) in vec2 uv[];
layout(location=0) out vec2 uvs;
layout(loc
ation=1) out vec3 barycoords;</code></pre>
</div>
<ol>
<li>Barycentric coordinates are assigned per vertex using these hardcoded constants:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void main() {
  const vec3 bc[3] = vec3[]( vec3(1.0, 0.0, 0.0),
                             vec3(0.0, 1.0, 0.0),
                             vec3(0.0, 0.0, 1.0) );
  for ( int i = 0; i &lt; 3; i++ ) {
    gl_Position = gl_in[i].gl_Position;
    uvs = uv[i];
    barycoords = bc[i];
    EmitVertex();
  }
  EndPrimitive();
}</code></pre>
</div>
<p>The final stage of this rendering pipeline is the fragment shader <code>Chapter05/06_Tessellation/src/main.frag</code>:</p>
<ol>
<li>We take in the barycentric coordinates from the geometry shader and use them to calculate a wireframe overlay covering our mesh:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>#include &lt;Chapter05/06_Tessellation/src/common.sp&gt;
layout(location=0) in vec2 uvs;
layout(location=1) in vec3 barycoords;
layout(location=0) out vec4 out_FragColor;</code></pre>
</div>
<ol>
<li>A helper function returns the blending factor based on the distance to the edge and the desired thickness of the wireframe contour. Essentially, when one of the <code>3</code> barycentric coordinate values is close to <code>0</code>, it indicates that the current fragment is near one of the triangle’s edges. The distance to zero controls the visible thickness of a rendered edge:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>float edgeFactor(float thickness) {
  vec3 a3 = smoothstep( vec3(0.0),
              fwidth(barycoords) * thickness, barycoords);
  return min( min( a3.x, a3.y ), a3.z );
}</code></pre>
</div>
<ol>
<li>Let’s sample the texture using the provided UV values and call it a day:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void main() {
  vec4 color = textureBindless2D(pc.texture, 0, uvs);
  out_FragColor = mix( vec4(0.1), color, edgeFactor(0.75) );
}</code></pre>
</div>
<p>The GLSL shader part of our Vulkan hardware tessellation pipeline is over, and it is time to look into the C++ code. The source code is located in the <code>Chapter05/06_Tessellation/src/main.cpp</code> file:</p>
<ol>
<li>The shaders for the tessellated mesh rendering are loaded the following way:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::ShaderModuleHandle&gt; vert =
  loadShaderModule(ctx, “Chapter05/06_Tessellation/src/main.vert”);
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; tesc =
  loadShaderModule(ctx, “Chapter05/06_Tessellation/src/main.tesc”);
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; geom =
  loadShaderModule(ctx, “Chapter05/06_Tessellation/src/main.geom”);
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; tese =
  loadShaderModule(ctx, “Chapter05/06_Tessellation/src/main.tese”);
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; frag =
  loadShaderModule(ctx, “Chapter05/06_Tessellation/src/main.frag”);</code></pre>
</div>
<ol>
<li>Now we create a corresponding rendering pipeline:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::RenderPipelineHandle&gt; pipelineSolid =
  ctx-&gt;createRenderPipeline({
    .topology    = lvk::Topology_Patch,
    .smVert      = vert,
    .smTesc      = tesc,
    .smTese      = tese,
    .smGeom      = geom,
    .smFrag      = frag,
    .color       = { { .format = ctx-&gt;getSwapchainFormat() } },
    .depthFormat = app.getDepthFormat(),
    .patchControlPoints = 3,
  });;</code></pre>
</div>
<p>The <code>data/rubber_duck/scene.gltf</code> mesh loading code is identical to that from the previous recipes, so we’ll skip it here. What’s more important is how we render the mesh and ImGui widget to control the tessellation scale factor. Let’s take a look at the body of the rendering loop:</p>
<ol>
<li>First, we calculate model-view-projection matrices for our mesh:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const mat4 m = glm::rotate(mat4(1.0f),
  glm::radians(-90.0f), vec3(1, 0, 0));
const mat4 v = glm::rotate(glm::translate(mat4(1.0f),
  vec3(0.0f, -0.5f, -1.5f)),
  (float)glfwGetTime(), vec3(0.0f, 1.0f, 0.0f));
const mat4 p = glm::perspective(45.0f, aspectRatio, 0.1f, 1000.0f);</code></pre>
</div>
<ol>
<li>Per-frame data is uploaded into a buffer.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const PerFrameData pc = {
  .model             = v * m,
  .view              = app.camera_.getViewMatrix(),
  .proj              = p,
  .cameraPos         = vec4(app.camera_.getPosition(), 1.0f),
  .texture           = texture.index(),
  .tessellationScale = tessellationScale,
  .vertices          = ctx-&gt;gpuAddress(vertexBuffer),
};
ctx-&gt;upload(bufferPerFrame, &amp;pc, sizeof(pc));</code></pre>
</div>
<ol>
<li>Push constants are used to pass an address of the buffer into shaders. Then we can render the mesh using our tessellation pipeline.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::ICommandBuffer&amp; buf = ctx-&gt;acquireCommandBuffer();
buf.cmdBeginRendering(renderPass, framebuffer);
buf.cmdPushConstants(ctx-&gt;gpuAddress(bufferPerFrame));
buf.cmdBindIndexBuffer(indexBuffer, lvk::IndexFormat_UI32);
buf.cmdBindRenderPipeline(pipelineSolid);
buf.cmdBindDepthState({ .compareOp = lvk::CompareOp_Less,
                        .isDepthWriteEnabled = true });
buf.cmdDrawIndexed(indices.size());</code></pre>
</div>
<ol>
<li>We add a grid on top, as was described earlier in this chapter in the <em>Implementing an infinite grid GLSL shader</em> recipe. The origin is used to place the grid just under the duck model. Inside our frame rendering loop, we can access all the ImGui rendering functionality as usual. Here, we just render a single slider containing a floating-point value for the tessellation scale factor:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>app.drawGrid(buf, p, vec3(0, -0.5f, 0));
app.imgui_-&gt;beginFrame(framebuffer);
ImGui::Begin(“Camera Controls”, nullptr,
  ImGuiWindowFlags_AlwaysAutoResize);
ImGui::SliderFloat(
  “Tessellation scale”, &amp;tessellationScale, 0.7f, 1.2f, “%.1f”);
ImGui::End();
app.imgui_-&gt;endFrame(buf);
buf.cmdEndRendering();</code></pre>
</div>
<p>Here is a screenshot from the running demo application:</p>
<figure>
<img alt="Figure 5.6: Tessellated duck" height="756" src="../media/file36.png" width="1430"/><figcaption aria-hidden="true">Figure 5.6: Tessellated duck</figcaption>
</figure>
<p>Note how the different tessellation level varies based on the distance to the camera. Try playing with the control slider to emphasize the effect.</p>
</section>
<section class="level3" data-number="6.8.3" id="theres-more...-11">
<h3 data-number="6.8.3">There’s more...</h3>
<p>This recipe can be used as a cornerstone of hardware mesh tessellation techniques in your Vulkan applications. One natural step forward would be to apply a displacement map to the fine-grained tessellated vertices using the direction of normal vectors. Check out this page for inspiration: <a href="https://www.geeks3d.com/20100804/test-opengl-4-tessellation-with-displacement-mapping">https://www.geeks3d.com/20100804/test-opengl-4-tessellation-with-displacement-mapping</a>. For those who want to go serious on adaptive tessellation of subdivision surfaces, there is a chapter in the <em>GPU Gems 2</em> book covering this advanced topic in great detail. It is now available online at <a href="https://developer.nvidia.com/gpugems/gpugems2/part-i-geometric-complexity/chapter-7-adaptive-tessellation-subdivision-surfaces">https://developer.nvidia.com/gpugems/gpugems2/part-i-geometric-complexity/chapter-7-adaptive-tessellation-subdivision-surfaces</a>.</p>
</section>
</section>
<section class="level2" data-number="6.9" id="organizing-mesh-data-storage">
<h2 data-number="6.9">Organizing mesh data storage</h2>
<p>In the previous chapters, we used fixed hardcoded vertex formats for our meshes, which changed between demos, and also implicitly included the material description. For example, a hardcoded texture was used to provide color information. A triangle mesh is defined by indices and vertices. Each vertex is defined as a set of attributes with distinct data formats corresponding to the <code>lvk::VertexInput</code> vertex input description. All auxiliary physical properties of an object, such as collision detection data, mass, and moments of inertia, can be represented by a mesh, while other information, such as surface material properties, can be stored outside of the mesh as external metadata. It’s worth noting that small 3D models, like the rubber duck we used before, can be loaded pretty quickly. However, larger and more intricate real-world 3D models, especially when using transmission formats like <code>.gltf</code>, may take several minutes to load. Using a run-time mesh format can address this issue by eliminating any parsing and replacing it with flat buffer loading, which matches internal rendering data structures. This involves employing fast functions like <code>fread()</code> and similar mechanisms.</p>
<p>Let’s define a unified mesh storage format covering all the use cases for the remaining part of this book.</p>
<section class="level3" data-number="6.9.1" id="getting-ready-32">
<h3 data-number="6.9.1">Getting ready</h3>
<p>This recipe describes the basic data structures we will use to store mesh data throughout the rest of this book. The full corresponding source code is located in the header file <code>shared/Scene/VtxData.h</code>. Make sure you’ve read <em>Chapter 2</em>, <em>Getting Started with Vulkan</em> before going forward.</p>
</section>
<section class="level3" data-number="6.9.2" id="how-to-do-it...-29">
<h3 data-number="6.9.2">How to do it...</h3>
<p>A vector of homogeneous <strong>vertex attributes</strong> stored contiguously is called a <strong>vertex stream</strong>. Examples of such attributes are vertex positions, texture coordinates, and normal vectors, each of the three representing one attribute. Each stream must have a format. Vertex positions are <code>vec3</code>, texture coordinates can be <code>vec2</code>, normal vectors can use the packed format <code>Int_2_10_10_10_REV</code>, and so on.</p>
<p>Let’s define a <strong>LOD</strong> as an index buffer of reduced size that uses existing vertices and hence can be used directly for rendering with the original vertex buffer. We learned how to create LODs earlier in this chapter in the <em>Generating LODs using MeshOptimizer</em> recipe.</p>
<p>We define a <strong>mesh</strong> as a collection of all vertex data streams and a collection of all index buffers, one for each LOD. The number of elements in each vertex data stream is the same and is called the “vertex count”, which we will encounter in the instructions below. To make things a bit simpler, we always use 32-bit offsets and indices for our data.</p>
<p>All the vertex data streams and LOD index buffers are packed into a single blob. This allows us to load the data in a single <code>fread()</code> call or even use memory mapping to allow direct data access. This simple vertex data representation also enables direct upload of meshes into the GPU. What makes it particularly interesting is the ability to merge data for multiple meshes into a single file. Alternatively, this can be achieved by consolidating the data into two large buffers — one for indices and the other for vertex attributes. This will come in very handy later when we learn how to implement a LOD switching technique on GPU.</p>
<p>In this recipe, we will deal only with geometrical data. The LOD creation process is covered in the recipe <em>Generating LODs using MeshOptimizer</em> and the material data export process is covered in the subsequent chapters. Let’s take a look at <code>shared/Scene/VtxData.h</code> and declare the main data structures for our mesh:</p>
<ol>
<li>First, we need to define an individual mesh description. We deliberately avoid using pointers, which hide memory allocations and prohibit simple saving and loading of the data. We store offsets to individual data streams and LOD index buffers, which are equivalent to pointers, but are more flexible and, most importantly, more GPU-friendly. All the offsets in the <code>Mesh</code> structure are given relative to the beginning of the data block. Let’s declare our main data structure for a mesh. It contains the number of LODs and vertex data streams. The LOD count, where the original mesh counts as one of the LODs, must be strictly less than <code>kMaxLODs</code>, because we do not store LOD index buffer sizes but calculate them from offsets. To calculate these sizes, we store one extra empty LOD level at the end. The number of vertex data streams is stored directly with no modifications.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>constexpr const uint32_t kMaxLODs = 8;
struct Mesh final {
  uint32_t lodCount = 1;
  uint32_t indexOffset = 0;
  uint32_t vertexOffset = 0;</code></pre>
</div>
<ol>
<li>The <code>vertexCount</code> field contains the total number of vertices in this mesh. This number is described the content of the vertex buffer and can be greater than the number of vertices on any individual level of detail. We postpone the question of material data storage to the next chapters. To do this in an elegant manner, let’s introduce a level of indirection. The <code>materialID</code> field contains an abstract identifier that allows us to reference any material data stored elsewhere:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  uint32_t vertexCount = 0;
  uint32_t materialID = 0;</code></pre>
</div>
<ol>
<li>Each mesh can potentially be displayed at different LODs. The file contains all indices for all levels of detail and offsets for the beginning of each LOD are stored in the <code>lodOffset</code> array. This array contains one extra item at the end, which serves as a marker to calculate the size of the last LOD:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  uint32_t lodOffset[kMaxLODs+1] = { 0 };</code></pre>
</div>
<ol>
<li>Instead of storing the number of indices of each LOD, we define a little helper function to calculate that number:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  inline uint32_t getLODIndicesCount(uint32_t lod) const {
    return lod &lt; lodCount ? lodOffset[lod + 1] - lodOffset[lod] : 0;
  }
};</code></pre>
</div>
<p>As you might have noticed, the <code>Mesh</code> structure is just a sort of index into other buffers containing data, such as index and vertex buffers. Let’s take a look at that data container:</p>
<ol>
<li>The format of the vertex streams is described by the structure <code>lvk::VertexInput</code>. We already used it in <em>Chapter 2</em>, <em>Getting Started with Vulkan</em>. This form of vertex stream description allows very flexible storage.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>struct MeshData {
  lvk::VertexInput streams = {};</code></pre>
</div>
<ol>
<li>The actual index and vertex buffer data are stored in these containers. They can accommodate multiple meshes. We use only 32-bit indices in our book for the sake of simplicity.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  std::vector&lt;uint32_t&gt; indexData;
  std::vector&lt;uint8_t&gt; vertexData;</code></pre>
</div>
<ol>
<li>Another <code>std::vector</code> stores each individual mesh description:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  std::vector&lt;Mesh&gt; meshes;</code></pre>
</div>
<ol>
<li>For completeness, we’ll also store a bounding box for each mesh right here. Bounding boxes are extremely useful for culling, and having them precalculated can significantly speed up the loading process.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  std::vector&lt;BoundingBox&gt; boxes;
};</code></pre>
</div>
<blockquote>
<p><strong>Note</strong></p>
<blockquote>
<p>For this book, we are solely focusing on tightly-packed (non-interleaved) vertex attribute streams. However, it is not difficult to extend the proposed schema to support interleaved data storage by making use of the stride parameter in <code>lvk::VertexInput::VertexInputBinding</code>. One major drawback is that such data reorganization would require us to change all the vertex pulling code in shaders. If you are developing production code, measure which storage format works faster on your target hardware before committing to one particular approach.</p>
</blockquote>
</blockquote>
<p>Before we can store these mesh data structures in a file, we need some sort of a file header:</p>
<ol>
<li>To ensure data integrity and to check the validity of the header, a magic hexadecimal value of <code>0x12345678</code> is stored in the first 4 bytes of the header:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>struct MeshFileHeader {
  uint32_t magicValue;</code></pre>
</div>
<ol>
<li>The number of individual <code>Mesh</code> descriptors in this file is stored in the <code>meshCount</code> field:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  uint32_t meshCount;</code></pre>
</div>
<ol>
<li>The last two member fields store the sizes of index and vertex data in bytes, respectively. These values come in handy when checking the integrity of a mesh file as well:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  uint32_t indexDataSize;
  uint32_t vertexDataSize;
};</code></pre>
</div>
<p>The file continues with the list of <code>Mesh</code> structures. After the header and a list of individual mesh descriptors, we store a large index and vertex data block, which can be loaded all at once.</p>
</section>
<section class="level3" data-number="6.9.3" id="how-it-works...-7">
<h3 data-number="6.9.3">How it works...</h3>
<p>Let’s go through the pseudocode from <code>shared/Scene/VtxData.cpp</code> for loading such a file is just a few <code>fread()</code> calls that look as follows. Error checks are omitted from the book text but are present in the actual code:</p>
<ol>
<li>First, we read the file header with the mesh count. Error checks are skipped here in the book but are present in the bundled source code:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>MeshFileHeader loadMeshData(const char* meshFile, MeshData&amp; out) {
  FILE* f = fopen(meshFile, “rb”);
  SCOPE_EXIT { fclose(f); };
  MeshFileHeader header;
  fread(&amp;header, 1, sizeof(header), f)
  fread(&amp;out.streams, 1, sizeof(out.streams), f);</code></pre>
</div>
<ol>
<li>Having read the header, we resize the mesh descriptors array and read in all <code>Mesh</code> descriptions:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  out.meshes.resize(header.meshCount);
  fread(out.meshes.data(), sizeof(Mesh), header.meshCount, f);
  out.boxes.resize(header.meshCount);
  fread(out.boxes.data(), sizeof(BoundingBox), header.meshCount, f);</code></pre>
</div>
<ol>
<li>Then we read the main geometry data blocks for this mesh, which contain the actual index and vertex data:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  out.indexData.resize(header.indexDataSize / sizeof(uint32_t));
  out.vertexData.resize(header.vertexDataSize);
  fread(out.indexData.data(), 1, header.indexDataSize, f);
  fread(out.vertexData.data(), 1, header.vertexDataSize, f);
  return header;
};</code></pre>
</div>
<p>Alternatively, index and vertex buffers can be combined into a single large byte buffer. We leave this as an exercise for our readers.</p>
<p>Later on, the <code>indexData</code> and <code>vertexData</code> containers can be directly uploaded to the GPU. We’ll revisit this idea in the subsequent recipes in this chapter.</p>
<p>Although you can see the result of this code in the demo app <code>Chapter05/07_MeshRenderer</code> for this chapter, there are some additional functionalities that need to be implemented. Let’s cover a few more topics before we can run and witness the demo.</p>
</section>
<section class="level3" data-number="6.9.4" id="theres-more...-12">
<h3 data-number="6.9.4">There’s more...</h3>
<p>This geometry data format is pretty simple and straightforward for the purpose of storing static mesh data. If the meshes may be changed, reloaded, or loaded asynchronously, we may store separate meshes in dedicated files.</p>
<p>Since it is impossible to predict all the use cases and since this book is all about the rendering and not some general gaming engine creation, it is up to the reader to make decisions on adding extra features such as mesh skinning or others. One simple example of such a decision is the addition of material data directly into the mesh file. Technically, all we need to do is add a <code>materialCount</code> field to <code>MeshFileHeader</code> and store a list of material descriptions right after the list of meshes. Even such a simple thing immediately raises more questions. Should we pack texture data in the same file? If yes, how complex should the texture format be? What material model should we use? And so on, and so forth. For now, we just leave mesh geometry data separated from material descriptions. We will come back to materials in subsequent chapters.</p>
</section>
</section>
<section class="level2" data-number="6.10" id="implementing-automatic-geometry-conversion">
<h2 data-number="6.10">Implementing automatic geometry conversion</h2>
<p>In the previous chapters, we learned how to use the <code>Assimp</code> library to load and render 3D models stored in different file formats. In real-world graphics applications, the process of loading a model can be tedious and multistage. Besides just loading, we might want to optimize a mesh in some specific way, such as optimizing geometry and computing multiple LOD meshes. This process might become slow for sizable meshes, so it makes perfect sense to preprocess meshes offline, before an application starts, and load them later in the app as described in the previous recipe, <em>Organizing mesh data storage</em>. Let’s learn how to implement a simple framework for automatic geometry preprocessing and conversion.</p>
<blockquote>
<p>In the previous edition of this book, we created a standalone tool for geometry conversion that needs to be executed before subsequent demo apps can load the converted data. It turned out to be a significant oversight on our part because many readers dove straight into running the demo apps and then reported issues when things didn’t work out of the box as they expected. Here, we have rectified that mistake. If an app requires converted data and cannot find it, it triggers all the necessary code to load the data from storage assets and convert it into our run-time format.</p>
</blockquote>
<section class="level3" data-number="6.10.1" id="getting-ready-33">
<h3 data-number="6.10.1">Getting ready</h3>
<p>The source code for our geometry conversion framework is in <code>Chapter05/07_MeshRenderer</code>. Low-level loader functions are defined in <code>shared/Scene/VtxData.cpp</code>. The entire demo application is covered by multiple recipes from this chapter, including <em>Organizing mesh data storage</em>, <em>Implementing automatic geometry conversion</em>, and <em>Indirect rendering in Vulkan</em>.</p>
</section>
<section class="level3" data-number="6.10.2" id="how-to-do-it...-30">
<h3 data-number="6.10.2">How to do it...</h3>
<p>Let us see how the <code>Assimp</code> library is used to export the mesh data and save it into the binary file using the data structures defined in the <em>Organizing mesh data storage</em> recipe:</p>
<ol>
<li>We start by exploring a function called <code>convertAIMesh()</code>, which converts the <code>Assimp</code> mesh representation into our run-time format and appends it to the referenced <code>MeshData</code> parameter. Global index and vertex offsets are updated as well. The function is quite lengthy, but we will explore it in full detail here. Error checks are omitted:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>Mesh convertAIMesh(const aiMesh* m, MeshData&amp; meshData,
  uint32_t&amp; indexOffset, uint32_t&amp; vertexOffset)
{
  const bool hasTexCoords = m-&gt;HasTextureCoords(0);</code></pre>
</div>
<ol>
<li>The actual mesh geometry data are stored in two following arrays. We cannot output converted meshes one by one, at least not in a single-pass tool, because we do not know the total size of data in advance, so we allocate in-memory storage for all the data and then write these data blobs into the output file. We also need a reference to the global vertex buffer, where we would append new vertices from this <code>aiMesh</code>. The <code>outLods</code> container is for per-LOD index buffers. Then we just go through all vertices of <code>aiMesh</code> and convert them:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  std::vector&lt;float&gt; srcVertices;
  std::vector&lt;uint32_t&gt; srcIndices;
  std::vector&lt;uint8_t&gt;&amp; vertices = meshData.vertexData;
  std::vector&lt;std::vector&lt;uint32_t&gt;&gt; outLods;</code></pre>
</div>
<ol>
<li>For this recipe, we assume there is a single LOD and that all the vertex data is stored as a continuous data stream. In other words, we have an interleaved storage of the data. We also ignore all the material information and deal exclusively with the index and vertex data for now.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  for (size_t i = 0; i != m-&gt;mNumVertices; i++) {
    const aiVector3D v = m-&gt;mVertices[i];
    const aiVector3D n = m-&gt;mNormals[i];
    const aiVector2D t = !hasTexCoords ? aiVector2D() : aiVector2D(
      m-&gt;mTextureCoords[0][i].x,
      m-&gt;mTextureCoords[0][i].y);
    if (g_calculateLODs) {
      srcVertices.push_back(v.x);
      srcVertices.push_back(v.y);
      srcVertices.push_back(v.z);
    }</code></pre>
</div>
<ol>
<li>Once we have the stream data for the vertex, we can output it into the vertex buffer. The position <code>v</code> is stored as <code>vec3</code>. The texture coordinates <code>uv</code> are stored as half-float <code>vec2</code> to save some space. The normal vector is converted into <code>2_10_10_10_REV</code>, which has the size of <code>uint32_t</code> – not bad for <code>3</code> floats.</li>
</ol>
<blockquote>
<p><code>put()</code> is a templated function, which mem-copies the value from its second argument into a vector of <code>uint8_t</code>:</p>
</blockquote>
<div class="C1-SHCodePACKT">
<pre><code>    put(vertices, v);
    put(vertices, glm::packHalf2x16(vec2(t.x, t.y)));
    put(vertices, glm::packSnorm3x10_1x2(vec4(n.x, n.y, n.z, 0)));
  }</code></pre>
</div>
<ol>
<li>Describe the vertex streams for our demo: positions, texture coordinates, and normal vectors. The stride comes from the size of <code>vec3</code> positions, half-float <code>vec2</code> texture coordinates packed into <code>uint32_t</code>, and <code>2_10_10_10_REV</code> normals packed into <code>uint32_t</code>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  meshData.streams = {
    .attributes = {{ .location = 0,
                     .format = lvk::VertexFormat::Float3,
                     .offset = 0 },
                   { .location = 1,
                     .format = lvk::VertexFormat::HalfFloat2,
                     .offset = sizeof(vec3) },
                   { .location = 2,
                     .format = lvk::VertexFormat::Int_2_10_10_10_REV,
                     .offset = sizeof(vec3) + sizeof(uint32_t) } },
    .inputBindings = { { .stride =
      sizeof(vec3) + sizeof(uint32_t) + sizeof(uint32_t) } },
  };</code></pre>
</div>
<ol>
<li>Go through all faces and create an index buffer:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  for (unsigned int i = 0; i != m-&gt;mNumFaces; i++) {
    if (m-&gt;mFaces[i].mNumIndices != 3) continue;
    for (unsigned j = 0; j != m-&gt;mFaces[i].mNumIndices; j++)
      srcIndices.push_back(m-&gt;mFaces[i].mIndices[j]);
  }</code></pre>
</div>
<ol>
<li>If no LOD calculation is required, we can just store <code>srcIndices</code> as LOD 0. Otherwise, we call the processLods() function, which calculates LOD levels for this mesh, as described in the <em>Generating LODs using MeshOptimizer</em> recipe.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  if (!g_calculateLODs)
     outLods.push_back(srcIndices);
  else
     processLods(srcIndices, srcVertices, outLods);</code></pre>
</div>
<ol>
<li>Before updating the <code>indexOffset</code> and <code>vertexOffset</code> parameters, let’s store their values in the resulting <code>Mesh</code> structure. Their values represent where all the previous index and vertex data ended before we started converting this <code>aiMesh</code>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  Mesh result = {
    .indexOffset  = indexOffset,
    .vertexOffset = vertexOffset,
    .vertexCount  = m-&gt;mNumVertices,
  };</code></pre>
</div>
<ol>
<li>Stream out all the indices for all the LOD levels one after another:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  uint32_t numIndices = 0;
  for (size_t l = 0; l &lt; outLods.size(); l++) {
    for (size_t i = 0; i &lt; outLods[l].size(); i++)
      meshData.indexData.push_back(outLods[l][i]);
    result.lodOffset[l] = numIndices;
    numIndices += (int)outLods[l].size();
  }
  result.lodOffset[outLods.size()] = numIndices;
  result.lodCount                  = (uint32_t)outLods.size();</code></pre>
</div>
<ol>
<li>After processing the input mesh, we increment offset counters for indices and the current starting vertex:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  indexOffset += numIndices;
  vertexOffset += m-&gt;mNumVertices;
  return result;
}</code></pre>
</div>
<p>Processing a 3D asset file by Assimp comprises loading the scene and converting each mesh into the internal format. Let’s take a look at the <code>loadMeshFile()</code> function to see how to do it:</p>
<ol>
<li>The list of flags for the <code>aiImportFile()</code> function includes options that allow further usage of imported data without any extra processing on our side. For example, all the transformation hierarchies are flattened, and the resulting transformation matrices are applied to mesh vertices.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void loadMeshFile(const char* fileName, MeshData&amp; meshData)
{
  const unsigned int flags = aiProcess_JoinIdenticalVertices |
                             aiProcess_Triangulate |
                             aiProcess_GenSmoothNormals |
                             aiProcess_LimitBoneWeights | 
                             aiProcess_SplitLargeMeshes |
                             aiProcess_ImproveCacheLocality |
                             aiProcess_RemoveRedundantMaterials |
                             aiProcess_FindDegenerates |
                             aiProcess_FindInvalidData |
                             aiProcess_GenUVCoords;
  const aiScene* scene = aiImportFile(fileName, flags);</code></pre>
</div>
<ol>
<li>After importing an Assimp scene, we resize the mesh descriptor container accordingly and call <code>convertAIMesh()</code> for each mesh in the scene. The <code>indexOffset</code> and <code>vertexOffset</code> offsets are accumulated incrementally:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  meshData.meshes.reserve(scene-&gt;mNumMeshes);
  meshData.boxes.reserve(scene-&gt;mNumMeshes);
  uint32_t indexOffset = 0;
  uint32_t vertexOffset = 0;
  for (unsigned int i = 0; i != scene-&gt;mNumMeshes; i++)
    meshData.meshes.push_back(
      convertAIMesh(scene-&gt;mMeshes[i], meshData,
        indexOffset, vertexOffset));</code></pre>
</div>
<ol>
<li>In the end, we precalculate axis-aligned bounding boxes for our mesh data:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  recalculateBoundingBoxes(meshData);
}</code></pre>
</div>
<p>Although loading and preprocessing of data should be customizable to accommodate needs of each individual demo app, saving is pretty much standard because <code>MeshData</code> contains all the information we need. As such, the saving function <code>saveMeshData()</code> is defined in <code>shared/Scene/VtxData.cpp</code>.</p>
<p>Saving converted meshes into our file format is the reverse process of reading meshes from the file described in the <em>Organizing mesh data storage</em> recipe:</p>
<ol>
<li>First, we fill the file header structure using the mesh number and offsets:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void saveMeshData(const char* fileName, const MeshData&amp; m) {
  FILE* f = fopen(fileName, “wb”);</code></pre>
</div>
<ol>
<li>We calculate byte sizes of index and vertex data buffers and store them in the header:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  const MeshFileHeader header = {
    .magicValue    = 0x12345678,
    .meshCount     = (uint32_t)m.meshes.size(),
    .indexDataSize =
      (uint32_t)(m.indexData.size() * sizeof(uint32_t)),
    .vertexDataSize = (uint32_t)(m.vertexData.size()),
  };</code></pre>
</div>
<ol>
<li>Once all the sizes are known, we save the header and the list of mesh descriptions:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  fwrite(&amp;header, 1, sizeof(header), f);
  fwrite(&amp;m.streams, 1, sizeof(m.streams), f);
  fwrite(m.meshes.data(), sizeof(Mesh), header.meshCount, f);
  fwrite(m.boxes.data(), sizeof(BoundingBox), header.meshCount, f);</code></pre>
</div>
<ol>
<li>After the header and other metadata, two blobs with index and vertex data are stored.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  fwrite(m.indexData.data(), 1, header.indexDataSize, f);
  fwrite(m.vertexData.data(), 1, header.vertexDataSize, f);
  fclose(f);
}</code></pre>
</div>
<p>Let’s put all this code to work in our demo app.</p>
</section>
<section class="level3" data-number="6.10.3" id="how-it-works...-8">
<h3 data-number="6.10.3">How it works...</h3>
<p>The mesh conversion framework is a part of our demo app. Let’s take a look at that part in the <code>Chapter05/07_MeshRenderer/src/main.cpp</code> file:</p>
<ol>
<li>First, we check if there’s any valid cached mesh data available. The <code>isMeshDataValid()</code> function checks if the specified cached mesh file exists and does some routine sanity checks on the data sizes.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>bool isMeshDataValid(const char* meshFile) {
  FILE* f = fopen(meshFile, “rb”);
  if (!f)  false;
  SCOPE_EXIT { fclose(f); };
  MeshFileHeader header;
  if (fread(&amp;header, 1, sizeof(header), f) != sizeof(header))
    return false;
  if (fseek(f, sizeof(Mesh) * header.meshCount, SEEK_CUR))
    return false;
  if (fseek(f, sizeof(BoundingBox) * header.meshCount, SEEK_CUR))
    return false;
  if (fseek(f, header.indexDataSize, SEEK_CUR))
    return false;
  if (fseek(f, header.vertexDataSize, SEEK_CUR))
    return false;
  return true;
}</code></pre>
</div>
<ol>
<li>Then, we use the <code>isMeshDataValid()</code> function to load the Lumberyard Bistro dataset:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const char* meshMeshes = “.cache/ch05_bistro.meshes”;
int main() {
  if (!isMeshDataValid(meshMeshes)) {
    printf(“No cached mesh data found. Precaching...\n\n”);
    MeshData meshData;
    loadMeshFile(“deps/src/bistro/Exterior/exterior.obj”, meshData);
    saveMeshData(meshMeshes, meshData);
  }</code></pre>
</div>
<ol>
<li>If the data is already precached, we just load it using the <code>loadMeshData()</code> described earlier in this recipe:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  MeshData meshData;
  const MeshFileHeader header = loadMeshData(meshMeshes, meshData);</code></pre>
</div>
<p>The output mesh data is saved into the file <code>.cache/ch05_bistro.meshes</code>. Let’s go through the rest of this chapter to learn how to render this mesh with Vulkan.</p>
</section>
</section>
<section class="level2" data-number="6.11" id="indirect-rendering-in-vulkan">
<h2 data-number="6.11">Indirect rendering in Vulkan</h2>
<p>Indirect rendering is the process of issuing drawing commands to the graphics API, where most of the parameters to those commands come from GPU buffers. It is a part of many modern GPU usage paradigms and exists in all contemporary rendering APIs in some form. For example, we can do indirect rendering with Vulkan using the <code>vkCmdDraw*Indirect*()</code> family of functions. Instead of dealing with low-level Vulkan here, let’s get more technical and learn how to combine indirect rendering in Vulkan with the mesh data format we introduced in the <em>Organizing mesh data storage</em> recipe.</p>
<section class="level3" data-number="6.11.1" id="getting-ready-34">
<h3 data-number="6.11.1">Getting ready</h3>
<p>In the earlier recipes, we covered building a mesh preprocessing pipeline and converting 3D meshes from transmission formats such as <code>.gltf2</code> into our run-time mesh data format. To wrap up this chapter, let’s demonstrate how to render this data. To delve into something new, let’s explore how to achieve this using the indirect rendering technique.</p>
<p>Once we have defined the mesh data structures, we also need to render them. To do so, we allocate GPU buffers for vertex and index data using the previously described functions, upload all the data to the GPU, and finally, render all the meshes.</p>
<p>The whole point of the previously defined <code>Mesh</code> data structure is the ability to render multiple meshes in a single Vulkan command. Since version 1.0 of the API, Vulkan supports the technique of indirect rendering. This means we do not need to issue the <code>vkCmdDraw()</code> command for each and every mesh. Instead, we create a GPU buffer and fill it with an array of <code>VkDrawIndirectCommand</code> structures, then fill these structures with appropriate offsets into our index and vertex data buffers, and finally emit a single <code>vkCmdDrawIndirect()</code> call. The <code>Mesh</code> structure described in the <em>Organizing mesh data storage</em> recipe contains the data required to fill in <code>VkDrawIndirectCommand</code>.</p>
<p>The full source code for this recipe is located in <code>Chapter05/07_MeshRenderer</code>. It is recommended to revisit the <em>Organizing mesh data storage</em> and <em>Implementing automatic geometry conversion</em> recipes before reading further.</p>
</section>
<section class="level3" data-number="6.11.2" id="how-to-do-it...-31">
<h3 data-number="6.11.2">How to do it...</h3>
<p>Let’s implement a simple helper class called <code>VKMesh</code> to render our mesh using <em>LightweightVK</em>:</p>
<ol>
<li>We need three buffers, an index buffer, a vertex buffer, and an indirect buffer, as well as three shaders and a rendering pipeline:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>class VKMesh final {
  lvk::Holder&lt;lvk::BufferHandle&gt; bufferIndices_;
  lvk::Holder&lt;lvk::BufferHandle&gt; bufferVertices_;
  lvk::Holder&lt;lvk::BufferHandle&gt; bufferIndirect_;
  lvk::Holder&lt;lvk::ShaderModuleHandle&gt; vert_;
  lvk::Holder&lt;lvk::ShaderModuleHandle&gt; geom_;
  lvk::Holder&lt;lvk::ShaderModuleHandle&gt; frag_;
  lvk::Holder&lt;lvk::RenderPipelineHandle&gt; pipeline_;</code></pre>
</div>
<ol>
<li>The constructor accepts references to <code>MeshFileHeader</code> and <code>MeshData</code>, which we loaded in the previous recipe, <em>Implemented automatic geometry conversion</em>. The data buffers are used as-is and uploaded directly into the respective Vulkan buffers. The number of indices is inferred from the indices buffer size, assuming indices are stored as 32-bit unsigned integers. The depth format specification is required to create a corresponding rendering pipeline right here in this class:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>public:
  uint32_t numIndices_ = 0;
  VKMesh(const std::unique_ptr&lt;lvk::IContext&gt;&amp; ctx,
         const MeshFileHeader&amp; header,
         const MeshData&amp; meshData,
         lvk::Format depthFormat)
  : numIndices_(header.indexDataSize / sizeof(uint32_t)) {
    const uint32_t* indices = meshData.indexData.data();
    const uint8_t* vertexData = meshData.vertexData.data();</code></pre>
</div>
<ol>
<li>Create vertex and index buffers and upload the data into them:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>    bufferVertices_ = ctx-&gt;createBuffer({
      .usage     = lvk::BufferUsageBits_Vertex,
      .storage   = lvk::StorageType_Device,
      .size      = header.vertexDataSize,
      .data      = vertexData,
      .debugName = “Buffer: vertex” }, nullptr);
    bufferIndices_ = ctx-&gt;createBuffer({
      .usage     = lvk::BufferUsageBits_Index,
      .storage   = lvk::StorageType_Device,
      .size      = header.indexDataSize,
      .data      = indices,
      .debugName = “Buffer: index” }, nullptr);</code></pre>
</div>
<ol>
<li>Allocate the data storage for our indirect buffer:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>    std::vector&lt;uint8_t&gt; drawCommands;
    const uint32_t numCommands = header.meshCount;
    drawCommands.resize(
      sizeof(DrawIndexedIndirectCommand) * numCommands +
        sizeof(uint32_t));</code></pre>
</div>
<ol>
<li>Store the number of draw commands at the very beginning of the indirect buffer. This approach is not used in this demo but can be useful for GPU-driven rendering when the GPU calculates the number of draw commands and stores it in a buffer.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>    memcpy(drawCommands.data(), &amp;numCommands, sizeof(numCommands));
    DrawIndexedIndirectCommand* cmd = std::launder(
      reinterpret_cast&lt;DrawIndexedIndirectCommand*&gt;(
        drawCommands.data() + sizeof(uint32_t)));</code></pre>
</div>
<ol>
<li>Fill in the content of our indirect commands buffer. Each command corresponds to a single <code>Mesh</code> structure. The indirect buffer should be allocated with the <code>BufferUsageBits_Indirect</code> usage flag.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>    for (uint32_t i = 0; i != numCommands; i++)
      *cmd++ = {
        .count         = meshData.meshes[i].getLODIndicesCount(0),
        .instanceCount = 1,
        .firstIndex    = meshData.meshes[i].indexOffset,
        .baseVertex    = meshData.meshes[i].vertexOffset,
        .baseInstance  = 0,
      };</code></pre>
</div>
<blockquote>
<p><code>DrawIndexedIndirectCommand</code> is just our mirror-image of <code>VkDrawIndexedIndirectCommand</code> to prevent including Vulkan headers into our app. While this might be an exaggeration for a Vulkan book, this type of separation might be useful in real-world apps considering this data structure is compatible in Vulkan, Metal, and OpenGL.</p>
<blockquote>
<p><code>struct DrawIndexedIndirectCommand {  uint32_t count;  uint32_t instanceCount;  uint32_t firstIndex;  uint32_t baseVertex;  uint32_t baseInstance;};</code></p>
</blockquote>
</blockquote>
<div class="C1-SHCodePACKT">
<pre><code>    bufferIndirect_ = ctx-&gt;createBuffer({
      .usage     = lvk::BufferUsageBits_Indirect,
      .storage   = lvk::StorageType_Device,
      .size      = sizeof(DrawIndexedIndirectCommand) *
        numCommands + sizeof(uint32_t),
      .data      = drawCommands.data(),
      .debugName = “Buffer: indirect” }, nullptr);</code></pre>
</div>
<ol>
<li>The rendering pipeline is created using a set of vertex, geometry, and fragment shaders. The geometry shader is used for barycentric coordinates generation to render beautiful wireframes. The vertex stream descriptions from <code>meshData.streams</code> can be used directly to initialize the pipeline’s vertex input:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>    vert_ = loadShaderModule(
      ctx, “Chapter05/07_MeshRenderer/src/main.vert”);
    geom_ = loadShaderModule(
      ctx, “Chapter05/07_MeshRenderer/src/main.geom”);
    frag_ = loadShaderModule(
      ctx, “Chapter05/07_MeshRenderer/src/main.frag”);
    pipeline_ = ctx-&gt;createRenderPipeline({
      .vertexInput = meshData.streams,
      .smVert      = vert_,
      .smGeom      = geom_,
      .smFrag      = frag_,
      .color       = { { .format = ctx-&gt;getSwapchainFormat() } },
      .depthFormat = depthFormat,
      .cullMode    = lvk::CullMode_Back,
    });
  }</code></pre>
</div>
<ol>
<li>The <code>draw()</code> method fills in a corresponding Vulkan command buffer to render the entire mesh. Note how <code>cmdDrawIndexedIndirect()</code> skips the first 32 bits of the indirect buffer where the number of commands is located. We will put that number to use in <em>Chapter 11</em>, <em>Advanced Rendering Techniques and Optimizations</em>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  void draw(lvk::ICommandBuffer&amp; buf, const MeshFileHeader&amp; header) {
    buf.cmdBindIndexBuffer(bufferIndices_, lvk::IndexFormat_UI32);
    buf.cmdBindVertexBuffer(0, bufferVertices_);
    buf.cmdBindRenderPipeline(pipeline_);
    buf.cmdBindDepthState({ .compareOp = lvk::CompareOp_Less,
                            .isDepthWriteEnabled = true });
    buf.cmdDrawIndexedIndirect(
      bufferIndirect_, sizeof(uint32_t), header.meshCount);
  }
};</code></pre>
</div>
<ol>
<li>This class is used as follows after the Bistro mesh is loaded:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>MeshData meshData;
const MeshFileHeader header = loadMeshData(meshMeshes, meshData);
const VKMesh mesh(ctx, header, meshData, app.getDepthFormat());</code></pre>
</div>
<p>This completes the description of our initialization process. Now, let’s turn to the GLSL source code:</p>
<ol>
<li>The vertex shader <code>Chapter05/07_MeshRenderer/src/main.vert</code> is quite simple. We do not use programmable vertex fetching here for the sake of simplicity. Only standard per-vertex attributes are used to simplify the example.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout(push_constant) uniform PerFrameData {
  mat4 MVP;
};
layout (location=0) in vec3 in_pos;
layout (location=1) in vec2 in_tc;
layout (location=2) in vec3 in_normal;
layout (location=0) out vec2 uv;
layout (location=1) out vec3 normal;
void main() {
  gl_Position = MVP * vec4(in_pos, 1.0);
  uv = in_tc;
  normal = in_normal;
};</code></pre>
</div>
<ol>
<li>The geometry shader, <code>Chapter05/07_MeshRenderer/src/main.geom</code>, provides the necessary barycentric coordinates, as described earlier in this chapter:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>#version 460 core
layout(triangles) in;
layout(triangle_strip, max_vertices = 3) out;
layout(location=0) in vec2 uv[];
layout(location=1) in vec3 normal[];
layout(location=0) out vec2 uvs;
layout(location=1) out vec3 barycoords;
layout(location=2) out vec3 normals;
void main() {
  const vec3 bc[3] = vec3[](vec3(1.0, 0.0, 0.0),
                            vec3(0.0, 1.0, 0.0),
                            vec3(0.0, 0.0, 1.0) );
  for ( int i = 0; i &lt; 3; i++ ) {
    gl_Position = gl_in[i].gl_Position;
    uvs = uv[i];
    barycoords = bc[i];
    normals = normal[i];
    EmitVertex();
  }
  EndPrimitive();
}</code></pre>
</div>
<ol>
<li>The fragment shader, <code>Chapter05/07_MeshRenderer/src/main.frag</code>, calculates some improvized lighting and applies a wireframe outline based on the barycentric coordinates generated by the geometry shader, as described in the <em>Integrating tessellation into the graphics pipeline</em> recipe:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout (location=0) in vec2 uvs;
layout (location=1) in vec3 barycoords;
layout (location=2) in vec3 normal;
layout (location=0) out vec4 out_FragColor;
float edgeFactor(float thickness) {
  vec3 a3 = smoothstep(
    vec3( 0.0 ), fwidth(barycoords) * thickness, barycoords);
  return min( min( a3.x, a3.y ), a3.z );
}
void main() {
  float NdotL = clamp(dot(normalize(normal),
                          normalize(vec3(-1,1,-1))), 0.5, 1.0);
  vec4 color = vec4(1.0, 1.0, 1.0, 1.0) * NdotL;
  out_FragColor = mix( vec4(0.1), color, edgeFactor(1.0) );
};</code></pre>
</div>
<ol>
<li>The final C++ rendering code snippet is straightforward because all the heavy lifting is already done inside the <code>VKMesh</code> helper class:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>buf.cmdBeginRendering(renderPass, framebuffer);
buf.cmdPushConstants(mvp);
mesh.draw(buf, header);
app.drawGrid(buf, p, vec3(0, -0.0f, 0));
app.imgui_-&gt;beginFrame(framebuffer);
app.drawFPS();
app.imgui_-&gt;endFrame(buf);
buf.cmdEndRendering();</code></pre>
</div>
<p>The running application will render the following image, if the image is loaded with the Lumberyard Bistro mesh:</p>
<figure>
<img alt="Figure 5.7: Amazon Lumberyard Bistro mesh geometry loaded and rendered" height="756" src="../media/file37.png" width="1430"/><figcaption aria-hidden="true">Figure 5.7: Amazon Lumberyard Bistro mesh geometry loaded and rendered</figcaption>
</figure>
</section>
</section>
<section class="level2" data-number="6.12" id="generating-textures-in-vulkan-using-compute-shaders">
<h2 data-number="6.12">Generating textures in Vulkan using compute shaders</h2>
<p>We learned how to use basic compute shaders earlier in this chapter, in the <em>Implementing instanced meshes with compute shaders</em> recipe. It is time to go through a few examples of how to use them. Let’s start with some basic procedural texture generation. In this recipe, we implement a small program to display animated textures whose texel values are calculated in real time inside our custom compute shader. To add even more value to this recipe, we will port a GLSL shader from <a href="https://www.shadertoy.com">https://www.shadertoy.com</a> to our Vulkan compute shader.</p>
<section class="level3" data-number="6.12.1" id="getting-ready-35">
<h3 data-number="6.12.1">Getting ready</h3>
<p>The compute pipeline creation code and Vulkan application initialization are the same as in the <em>Implementing instanced meshes with compute shaders</em> recipe. Make sure you read it before proceeding further. To use and display the generated texture, we need a textured full-screen quad renderer. Its GLSL source code can be found in <code>data/shaders/Quad.vert</code> and <code>data/shaders/Quad.frag</code>. However, the geometry used is actually a triangle covering the entire screen. We will not focus on its internals here because at this point, it should be easy for you to render a full-screen quad on your own using the material from the previous chapters.</p>
<p>The original shader, “Industrial Complex,” that we are going to use here to generate a Vulkan texture was created by Gary “Shane” Warne (<a href="https://www.rhomboid.com">rhomboid.com</a>) and can be downloaded from ShaderToy: <a href="https://www.shadertoy.com/view/MtdSWS">https://www.shadertoy.com/view/MtdSWS</a>.</p>
</section>
<section class="level3" data-number="6.12.2" id="how-to-do-it...-32">
<h3 data-number="6.12.2">How to do it...</h3>
<p>Let’s start by discussing the process of writing a texture-generating GLSL compute shader. The simplest shader to generate an RGBA image without using any input data outputs an image by using the <code>gl_GlobalInvocationID</code> built-in variable to calculate which pixel to output. This maps directly to how ShaderToy shaders operate, thus we can transform them into a compute shader just by adding some input and output parameters and layout modifiers that are specific to compute shaders and Vulkan. Let’s take a look at a minimalistic compute shader that creates a red-green gradient texture:</p>
<ol>
<li>As in all other compute shaders, one mandatory line at the beginning tells the driver how to distribute the workload on the GPU. In our case, we are processing tiles of 16x16 pixels – the local workgroup size, 16x16, is supported on many GPUs, and we will use it for compatibility purposes:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout (local_size_x = 16, local_size_y = 16) in;</code></pre>
</div>
<ol>
<li>The only buffer binding that we need to specify is the output image. This is the first time we have used the image type <code>image2D</code> in this book. Here, it means that the array <code>kTextures2DOut</code> contains an array of 2D images whose elements are nothing but pixels of a texture. The <code>writeonly</code> layout qualifier instructs the compiler to assume we will not read from this image in the shader. The binding is updated from the C++ code in the <em>Using Vulkan descriptor indexing</em> recipe in <em>Chapter 2, Getting Started with Vulkan</em>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout (set = 0, binding = 2, rgba8)
  uniform writeonly image2D kTextures2DOut[];</code></pre>
</div>
<ol>
<li>The GLSL compute shading language provides a set of helper functions to retrieve various image attributes. We use the built-in <code>imageSize()</code> function to determine the size of an image in pixels, and the image ID is loaded from push constants:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout(push_constant) uniform uPushConstant {
  uint tex;
  float time;
} pc;
void main() {
  ivec2 dim = imageSize(kTextures2DOut[pc.tex]);</code></pre>
</div>
<ol>
<li>The built-in <code>gl_GlobalInvocationID</code> variable tells us which global element of our compute grid we are processing. To convert its value into 2D image coordinates, we divide it by the image dimensions. As we are dealing with 2D textures, only <code>x</code> and <code>y</code> components matter. The calling code from the C++ side executes the <code>vkCmdDispatch()</code> function and passes the output image size as the X and Y numbers of local workgroups:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  vec2 uv = vec2(gl_GlobalInvocationID.xy) / dim;</code></pre>
</div>
<ol>
<li>The actual real work we do in this shader is to call the <code>imageStore()</code> GLSL function:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  imageStore(kTextures2DOut[pc.tex],
    ivec2(gl_GlobalInvocationID.xy), vec4(uv, 0.0, 1.0));
}</code></pre>
</div>
<p>Now, this example is rather limited, and all you get is a red-and-green gradient image. Let’s change it a little bit to use the actual shader code from ShaderToy. The compute shader that renders a Vulkan version of the “Industrial Complex” shader from ShaderToy, available via the following URL <a href="https://shadertoy.com/view/MtdSWS">https://shadertoy.com/view/MtdSWS</a>, can be found in the <code>Chapter05/08_ComputeTexture/src/main.comp</code> file.</p>
<ol>
<li>First, let’s copy the entire original ShaderToy GLSL code into our new compute shader. There is a function called <code>mainImage()</code> in there, which is declared as follows:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void mainImage(out vec4 fragColor, in vec2 fragCoord)</code></pre>
</div>
<ol>
<li>We should replace it with a function that returns a <code>vec4</code> color instead of storing it in the output parameter:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>vec4 mainImage(in vec2 fragCoord)</code></pre>
</div>
<p>Don’t forget to add an appropriate <code>return</code> statement at the end.</p>
<ol>
<li>Now, let’s change the <code>main()</code> function of our compute shader to invoke <code>mainImage()</code> properly and do 5x5 accumulative antialiasing. It is a pretty neat trick:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void main() {
  ivec2 dim = imageSize(kTextures2DOut[pc.tex]);
  vec4 c = vec4(0.0);
  for (int dx = -2; dx != 3; dx++)
    for (int dy = -2; dy != 3; dy++) {
      vec2 uv = vec2(gl_GlobalInvocationID.xy) / dim +
                vec2(dx, dy) / (3.0 * dim);
      c += mainImage(uv * dim);
    }
  imageStore(kTextures2DOut[pc.tex],
    ivec2(gl_GlobalInvocationID.xy), c / 25.0);
}</code></pre>
</div>
<ol>
<li>There is still one issue that needs to be resolved before we can run this code. The ShaderToy code uses two custom input variables, <code>iTime</code> for the elapsed time and <code>iResolution</code>, which contains the size of the resulting image. To prevent any search and replace in the original GLSL code, we mimic these variables, one as a push constant and the other with a hardcoded value for simplicity.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout(push_constant) uniform uPushConstant {
  uint tex;
  float time;
} pc;
vec2 iResolution = vec2( 1280.0, 720.0 );
float iTime = pc.time;</code></pre>
</div>
<blockquote>
<p><strong>Note</strong></p>
<blockquote>
<p>The GLSL <code>imageSize()</code> function can be used to obtain the <code>iResolution</code> value based on the actual size of our texture. We leave it as an exercise to the reader.</p>
</blockquote>
</blockquote>
<p>The C++ code is rather short and consists of creating a texture and invoking the previously mentioned compute shader, inserting a Vulkan pipeline barrier, and rendering a textured full-screen quad:</p>
<ol>
<li>A texture is created using the <code>TextureUsageBits_Storage</code> usage flag to make it accessible to compute shaders:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::TextureHandle&gt; texture = ctx-&gt;createTexture({
  .type       = lvk::TextureType_2D,
  .format     = lvk::Format_RGBA_UN8,
  .dimensions = {1024, 720},
  .usage      = lvk::TextureUsageBits_Sampled |
                lvk::TextureUsageBits_Storage,
  .debugName  = “Texture: compute”,
});</code></pre>
</div>
<ol>
<li>Compute and rendering pipelines are created the following way:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::ShaderModuleHandle&gt; comp = loadShaderModule(
  ctx, “Chapter05/08_ComputeTexture/src/main.comp”);
lvk::Holder&lt;lvk::ComputePipelineHandle&gt; pipelineComputeMatrices =
  ctx-&gt;createComputePipeline({ smComp = comp });
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; vert = loadShaderModule(
  ctx, “data/shaders/Quad.vert”);
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; frag = loadShaderModule(
  ctx, “data/shaders/Quad.frag”);
lvk::Holder&lt;lvk::RenderPipelineHandle&gt; pipelineFullScreenQuad =
  ctx-&gt;createRenderPipeline({
    .smVert = vert,
    .smFrag = frag,
    .color  = { { .format = ctx-&gt;getSwapchainFormat() } },
});</code></pre>
</div>
<ol>
<li>When doing rendering, we provide the texture ID to both shaders and the current time to the compute shader using push constants:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::ICommandBuffer&amp; buf = ctx-&gt;acquireCommandBuffer();
const struct {
  uint32_t textureId;
  float time;
} pc {
  .textureId = texture.index(),
  .time      = (float)glfwGetTime(),
};
buf.cmdPushConstants(pc);
buf.cmdBindComputePipeline(pipelineComputeMatrices);
buf.cmdDispatchThreadGroups(
  { .width = 1024 / 16, .height = 720 / 16 });</code></pre>
</div>
<ol>
<li>A pipeline barrier that ensures that the compute shader finishes before texture sampling happens is created by LightweightVK when we specify a required texture dependency. Take a look at <code>lvk::CommandBuffer::transitionToShaderReadOnly()</code> for the low-level image barrier code. As our fullscreen shader uses a triangle covering the entire screen, draw 3 vertices of that triangle.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>buf.cmdBeginRendering(renderPass, framebuffer,
  { .textures = { { lvk::TextureHandle(texture) } } });
buf.cmdBindRenderPipeline(pipelineFullScreenQuad);
buf.cmdDraw(3);
buf.cmdEndRendering();
ctx-&gt;submit(buf, ctx-&gt;getCurrentSwapchainTexture());</code></pre>
</div>
<p>The running application should render the following image, which is similar to the output of <a href="https://www.shadertoy.com/view/MtdSWS">https://www.shadertoy.com/view/MtdSWS</a>:</p>
<figure>
<img alt="Figure 5.8: Using compute shaders to generate textures" height="730" src="../media/file38.jpg" width="1429"/><figcaption aria-hidden="true">Figure 5.8: Using compute shaders to generate textures</figcaption>
</figure>
<p>In the next recipe, we will continue learning the Vulkan compute pipeline and implement a mesh generation compute shader.</p>
</section>
</section>
<section class="level2" data-number="6.13" id="implementing-computed-meshes">
<h2 data-number="6.13">Implementing computed meshes</h2>
<p>In the <em>Generating textures in Vulkan using compute shaders</em> recipe, we learned how to write pixel data into textures from compute shaders. We are going to need that data in the next chapter to implement a BRDF precomputation tool for our physically-based rendering pipeline. But before that, let’s learn a few simple and interesting ways to use compute shaders in Vulkan and combine this feature with mesh geometry generation on the GPU.</p>
<p>We are going to run a compute shader to create the triangulated geometry of a 3D torus knot shape with different <code>P</code> and <code>Q</code> parameters.</p>
<blockquote>
<p><strong>Note</strong></p>
<blockquote>
<p>A torus knot is a special kind of knot that lies on the surface of an unknotted torus in 3D space. Each torus knot is specified by a pair of coprime integers, p and q. To find out more, check out the Wikipedia page: <a href="https://en.wikipedia.org/wiki/Torus_knot">https://en.wikipedia.org/wiki/Torus_knot</a>.</p>
</blockquote>
</blockquote>
<p>A compute shader generates vertex data, including positions, texture coordinates, and normal vectors. This data is stored in a buffer and later utilized as a vertex buffer to render a mesh in a graphics pipeline. To make the results more visually pleasing, we will implement real-time morphing between two different torus knots that is controllable from an ImGui widget. Let’s get started.</p>
<section class="level3" data-number="6.13.1" id="getting-ready-36">
<h3 data-number="6.13.1">Getting ready</h3>
<p>The source code for this example is located in <code>Chapter05/09_ComputeMesh</code>.</p>
</section>
<section class="level3" data-number="6.13.2" id="how-to-do-it...-33">
<h3 data-number="6.13.2">How to do it...</h3>
<p>The application consists of multiple parts: the C++ part (which drives the UI and Vulkan commands), the mesh generation compute shader, the texture generations compute shader, and a rendering pipeline with simple vertex and fragment shaders. The C++ part in <code>Chapter05/09_ComputeMesh/src/main.cpp</code> is quite short, so let’s tackle it first:</p>
<ol>
<li>We store a queue of P-Q pairs, which defines the order of morphing. The queue always has at least two elements, which define the current and the next torus knot. We also store a floating point value, <code>g_MorphCoef</code>, which is the morphing factor <code>0...1</code> between the two adjacent P-Q pairs in the queue. The mesh is regenerated in every frame, and the morphing coefficient is increased until it reaches <code>1.0</code>. At this point, we will either stop morphing or, if there are more than two elements in the queue, remove the top element from it, reset <code>g_MorphCoef</code> to <code>zero</code>, and repeat. The <code>g_AnimationSpeed</code> value defines how fast one torus knot mesh morphs into another. The <code>g_UseColoredMesh</code> Boolean flag is used to switch between colored and textured shading of the mesh:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>std::deque&lt;std::pair&lt;uint32_t, uint32_t&gt;&gt; morphQueue =
  { { 5, 8 }, { 5, 8 } };
float morphCoef = 0.0f;
float animationSpeed = 1.0f;
bool g_UseColoredMesh = false;</code></pre>
</div>
<ol>
<li>Two global constants define the tessellation level of a torus knot. Feel free to play around with them:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>constexpr uint32_t kNumU = 1024;
constexpr uint32_t kNumV = 1024;</code></pre>
</div>
<ol>
<li>Regardless of the <code>P</code> and <code>Q</code> parameter values, we have a single order in which we should traverse vertices to produce torus knot triangles. The <code>generateIndices()</code> function prepares index buffer data for this purpose. Here, <code>6</code> is the number of indices generated for each rectangular grid element consisting of <code>2</code> triangles:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void generateIndices(uint32_t* indices) {
  for (uint32_t j = 0; j &lt; kNumV - 1; j++) {
    for (uint32_t i = 0; i &lt; kNumU - 1; i++) {
      uint32_t ofs = (j * (kNumU - 1) + i) * 6;
      uint32_t i1 = (j + 0) * kNumU + (i + 0);
      uint32_t i2 = (j + 0) * kNumU + (i + 1);
      uint32_t i3 = (j + 1) * kNumU + (i + 1);
      uint32_t i4 = (j + 1) * kNumU + (i + 0);
      indices[ofs + 0] = i1;
      indices[ofs + 1] = i2;
      indices[ofs + 2] = i4;
      indices[ofs + 3] = i2;
      indices[ofs + 4] = i3;
      indices[ofs + 5] = i4;
    }
  }
}</code></pre>
</div>
<p>Additionally, our C++ code operates an ImGui UI for selecting a configuration of a torus knot and managing various parameters. This offers insights into the code’s flow, so let’s examine it more closely:</p>
<ol>
<li>Each torus knot is specified by a pair of coprime integers, <code>P</code> and <code>Q</code>. Here, we have preselected a few pairs that produce visually interesting results.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void renderGUI(lvk::TextureHandle texture) {
  static const std::vector&lt;std::pair&lt;uint32_t, uint32_t&gt;&gt; PQ = {
    {1, 1}, {2, 3}, {2, 5}, {2, 7}, {3, 4},
    {2, 9}, {3, 5}, {5, 8}, {8, 9} };
  ImGui::SetNextWindowPos(ImVec2(0, 0), ImGuiCond_Appearing);
  ImGui::Begin(“Torus Knot params”, nullptr,
    ImGuiWindowFlags_AlwaysAutoResize | ImGuiWindowFlags_NoCollapse);</code></pre>
</div>
<ol>
<li>We can control the morphing animation speed. That is how fast one mesh morphs into another. We can also switch between colored and textured shading of the mesh.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  ImGui::Checkbox(“Use colored mesh”, &amp;g_UseColoredMesh);
  ImGui::SliderFloat(
    “Morph animation speed”, &amp;g_AnimationSpeed, 0.0f, 2.0f);</code></pre>
</div>
<ol>
<li>When we click a button with a different set of <code>P</code>-<code>Q</code> parameters, we don’t regenerate the mesh right away. Instead, we let any ongoing animations complete by adding a new pair to a queue. In the main loop, after the current morphing animation concludes, we remove the front element from the queue and initiate the animation again.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  for (size_t i = 0; i != PQ.size(); i++) {
    const std::string title = std::to_string(PQ[i].first) + “, “ +
      std::to_string(PQ[i].second);
    if (ImGui::Button(title.c_str(), ImVec2(128, 0))) {
      if (PQ[i] != g_MorphQueue.back())
        g_MorphQueue.push_back(PQ[i]);
    }
  }</code></pre>
</div>
<ol>
<li>The content of the morph queue is printed here for you. The current <code>P</code>-<code>Q</code> pair is marked with <code>“&lt;---”</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  ImGui::Text(“Morph queue:”);
  for (size_t i = 0; i != g_MorphQueue.size(); i++) {
    const bool isLastElement = (i + 1) == g_MorphQueue.size();
    ImGui::Text(“  P = %u, Q = %u %s”, g_MorphQueue[i].first,
      g_MorphQueue[i].second, isLastElement ? “&lt;---” : ““);
  }
  ImGui::End();</code></pre>
</div>
<ol>
<li>If we apply an animated texture for shading the mesh, let’s also showcase it using <code>ImGui::Image()</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  if (!g_UseColoredMesh) {
    const ImVec2 size = ImGui::GetIO().DisplaySize;
    const float  dim  = std::max(size.x, size.y);
    const ImVec2 sizeImg(0.25f * dim, 0.25f * dim);
    ImGui::SetNextWindowPos(ImVec2(size.x - sizeImg.x - 25, 0),
      ImGuiCond_Appearing);
    ImGui::Begin(“Texture”, nullptr,
      ImGuiWindowFlags_AlwaysAutoResize);
    ImGui::Image(texture.indexAsVoid(), sizeImg);
    ImGui::End();
  }
};</code></pre>
</div>
<p>Now, let’s examine the C++ code that’s responsible for creating buffers and populating them with initial data:</p>
<ol>
<li>The indices are immutable and are generated using the previously mentioned <code>generateIndices()</code> function. The vertex buffer size is calculated based on <code>12</code> <code>float</code> elements per vertex. That is <code>vec4</code> positions, <code>vec4</code> texture coordinates, and <code>vec4</code> normal vectors. This padding is used to simplify the compute shader that writes to the vertex buffer.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>std::vector&lt;uint32_t&gt; indicesGen((kNumU - 1) * (kNumV - 1) * 6);
generateIndices(indicesGen.data());
const uint32_t vertexBufferSize = 12 * sizeof(float) * kNumU * kNumV;
const uint32_t indexBufferSize  =
  sizeof(uint32_t) * (kNumU - 1) * (kNumV - 1) * 6;
lvk::Holder&lt;lvk::BufferHandle&gt; bufferIndex  = ctx-&gt;createBuffer({
  .usage     = lvk::BufferUsageBits_Index,
  .storage   = lvk::StorageType_Device,
  .size      = indicesGen.size() * sizeof(uint32_t),
  .data      = indicesGen.data(),
  .debugName = “Buffer: index” });</code></pre>
</div>
<ol>
<li>Let’s create a texture that will be generated by a compute shader. This is similar to the previous recipe, <em>Generating textures in Vulkan using compute shaders</em>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::TextureHandle&gt; texture = ctx-&gt;createTexture({
  .type       = lvk::TextureType_2D,
  .format     = lvk::Format_RGBA_UN8,
  .dimensions = {1024, 1024},
  .usage      = lvk::TextureUsageBits_Sampled |
                lvk::TextureUsageBits_Storage,
  .debugName  = “Texture: compute” });</code></pre>
</div>
<ol>
<li>The vertex buffer should be created with the <code>BufferUsageBits_Storage</code> flag to allow vertex shader usage:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::BufferHandle&gt; bufferVertex = ctx-&gt;createBuffer({
  .usage     = lvk::BufferUsageBits_Vertex |
               lvk::BufferUsageBits_Storage,
  .storage   = lvk::StorageType_Device,
  .size      = vertexBufferSize,
  .debugName = “Buffer: vertex” });</code></pre>
</div>
<ol>
<li>We create two compute pipelines: one for mesh generation and another for texture generation.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::ShaderModuleHandle&gt; compMesh = loadShaderModule(ctx,
  “Chapter05/09_ComputeMesh/src/main_mesh.comp”);
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; compTexture =
  loadShaderModule(ctx,
  “Chapter05/09_ComputeMesh/src/main_texture.comp”);
lvk::Holder&lt;lvk::ComputePipelineHandle&gt; pipelineComputeMesh =
  ctx-&gt;createComputePipeline({ .smComp = compMesh });
lvk::Holder&lt;lvk::ComputePipelineHandle&gt; pipelineComputeTexture =
  ctx-&gt;createComputePipeline({ .smComp = compTexture });</code></pre>
</div>
<ol>
<li>Shader modules are loaded as usual. The geometry shader generates barycentric coordinates for wireframe outlines. Wireframe outline rendering will be enabled if you set <code>kNumU</code> and <code>kNumV</code> to <code>64</code> or lower values.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::Holder&lt;lvk::ShaderModuleHandle&gt; vert =
  loadShaderModule(ctx, “Chapter05/09_ComputeMesh/src/main.vert”);
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; geom =
  loadShaderModule(ctx, “Chapter05/09_ComputeMesh/src/main.geom”);
lvk::Holder&lt;lvk::ShaderModuleHandle&gt; frag =
  loadShaderModule(ctx, “Chapter05/09_ComputeMesh/src/main.frag”);</code></pre>
</div>
<ol>
<li>For vertex input, we employ <code>vec4</code> to simplify padding concerns—or, to be more precise, to eliminate them entirely:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const lvk::VertexInput vdesc = {
  .attributes  = { { .location = 0,
                     .format = VertexFormat::Float4,
                     .offset = 0 },
                   { .location = 1,
                     .format = VertexFormat::Float4,
                     .offset = sizeof(vec4) },
                   { .location = 2,
                     .format = VertexFormat::Float4,
                     .offset = sizeof(vec4)+sizeof(vec4) } },
  .inputBindings = { { .stride = 3 * sizeof(vec4) },
};</code></pre>
</div>
<ol>
<li>The same shader code is specialized for textures and colored shading using specialization constants:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>const uint32_t specColored = 1;
const uint32_t specNotColored = 0;
lvk::Holder&lt;lvk::RenderPipelineHandle&gt; pipelineMeshColored =
  ctx-&gt;createRenderPipeline({
    .vertexInput = vdesc,
    .smVert      = vert,
    .smGeom      = geom,
    .smFrag      = frag,
    .specInfo    = { .entries = { { .constantId = 0,
                                    .size = sizeof(uint32_t) } },
                     .data = &amp;specColored,
                     .dataSize = sizeof(specColored) },
    .color       = { { .format = ctx-&gt;getSwapchainFormat() } },
    .depthFormat = app.getDepthFormat() });
lvk::Holder&lt;lvk::RenderPipelineHandle&gt; pipelineMeshTextured =
  ctx-&gt;createRenderPipeline({
    .vertexInput = vdesc,
    .smVert      = vert,
    .smGeom      = geom,
    .smFrag      = frag,
    .specInfo    = { .entries = { { .constantId = 0,
                                    .size = sizeof(uint32_t) } },
                     .data = &amp;specNotColored,
                     .dataSize = sizeof(specNotColored) },
    .color       = { { .format = ctx-&gt;getSwapchainFormat() } },
    .depthFormat = app.getDepthFormat() });</code></pre>
</div>
<p>The final segment of the C++ code is the rendering loop. Let’s explore how to populate a command buffer:</p>
<ol>
<li>First, we need to access the current <code>P</code>-<code>Q</code> pair from the morph queue:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>lvk::ICommandBuffer&amp; buf = ctx-&gt;acquireCommandBuffer();
const mat4 m = glm::translate(mat4(1.0f), vec3(0.0f, 0.0f, -18.f));
const mat4 p = glm::perspective(45.0f, aspectRatio, 0.1f, 1000.0f);
auto iter = g_MorphQueue.begin();</code></pre>
</div>
<ol>
<li>The number of parameters for the shaders is larger than usual, precisely <code>128</code> bytes. Push constants are shared across all shaders, compute, and graphics, for convenience purposes and to simplify the code.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>struct PerFrame {
  mat4 mvp;
  uint64_t buffer;
  uint32_t textureId;
  float time;
  uint32_t numU, numV;
  float minU, maxU;
  float minV, maxV;
  uint32_t p1, p2;
  uint32_t q1, q2;
  float morph;
} pc = {
  .mvp       = p * m,
  .buffer    = ctx-&gt;gpuAddress(bufferVertex),
  .textureId = texture.index(),
  .time      = (float)glfwGetTime(),
  .numU      = kNumU,
  .numV      = kNumU,
  .minU      = -1.0f,
  .maxU      = +1.0f,
  .minV      = -1.0f,
  .maxV      = +1.0f,
  .p1        = iter-&gt;first,
  .p2        = (iter + 1)-&gt;first,
  .q1        = iter-&gt;second,
  .q2        = (iter + 1)-&gt;second,
  .morph     = easing(g_MorphCoef),
};
buf.cmdPushConstants(pc);</code></pre>
</div>
<ol>
<li>When we dispatch the mesh generation compute shader, we need to specify proper memory barriers to make sure the previous frame has finished rendering:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>buf.cmdBindComputePipeline(pipelineComputeMesh);
buf.cmdDispatchThreadGroups(
  { .width = (kNumU * kNumV) / 2 },
  { .buffers = { { lvk::BufferHandle(bufferVertex) } } });</code></pre>
</div>
<ol>
<li>Do the same for the texture generation shader. We must ensure the texture regeneration is properly synchronized with rendering by issuing a Vulkan image layout transition with appropriate pipeline stages and masks. This is done by <code>LightweightVK</code> inside <code>lvk::CommandBuffer::cmdDispatchThreadGroups()</code> using a few empirical rules.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>if (!g_UseColoredMesh) {
  buf.cmdBindComputePipeline(pipelineComputeTexture);
  buf.cmdDispatchThreadGroups(
    { .width = 1024 / 16, .height = 1024 / 16 },
    { .textures = { { lvk::TextureHandle(texture) } } });
}</code></pre>
</div>
<ol>
<li>When we start rendering, it’s essential to specify both dependencies—the texture and the vertex buffer. The choice of the rendering pipeline depends on whether we aim to render a colored or textured mesh.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>buf.cmdBeginRendering(
  renderPass, framebuffer,
  { .textures = { { lvk::TextureHandle(texture) } },
    .buffers  = { { lvk::BufferHandle(bufferVertex) } } });
buf.cmdBindRenderPipeline(
  g_UseColoredMesh ? pipelineMeshColored : pipelineMeshTextured);
buf.cmdBindDepthState({ .compareOp = lvk::CompareOp_Less, .isDepthWriteEnabled = true });
buf.cmdBindVertexBuffer(0, bufferVertex);
buf.cmdBindIndexBuffer(bufferIndex, lvk::IndexFormat_UI32);
buf.cmdDrawIndexed(indicesGen.size());
app.imgui_-&gt;beginFrame(framebuffer);</code></pre>
</div>
<ol>
<li>The <code>renderGUI()</code> function renders the ImGui UI, as described earlier in this recipe. In this case, we pass our generated texture to it for presentation.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>renderGUI(texture);
app.drawFPS();
app.imgui_-&gt;endFrame(buf);
buf.cmdEndRendering();</code></pre>
</div>
<p>That covers the C++ part. Now, let’s delve into the GLSL shaders to understand how the whole demo works.</p>
</section>
<section class="level3" data-number="6.13.3" id="how-it-works-11">
<h3 data-number="6.13.3">How it works…</h3>
<p>Let’s start with the compute shader, <code>Chapter05/09_ComputeMesh/src/main_mesh.comp</code>, which is responsible for generating vertex data:</p>
<ol>
<li>The push constants are declared in <code>Chapter05/09_ComputeMesh/src/common.sp</code>. They are shared between all shaders, and we paste them here for convenience:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>layout (local_size_x = 2, local_size_y = 1, local_size_z = 1) in;
// included from &lt;Chapter05/09_ComputeMesh/src/common.sp&gt;
layout(push_constant) uniform PerFrameData {
  mat4 MVP;
  uvec2 bufferId;
  uint textureId;
  float time;
  uint numU, numV;
  float minU, maxU, minV, maxV;
  uint P1, P2, Q1, Q2;
  float morph;
} pc;</code></pre>
</div>
<ol>
<li>This is the structure containing per-vertex data that we aim to generate and write into a buffer referenced by <code>VertexBuffer</code>, which is stored in <code>pc.bufferId</code> a few lines above the buffer.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>struct VertexData {
  vec4 pos;
  vec4 tc;
  vec4 norm;
};
layout (buffer_reference) buffer VertexBuffer {
  VertexData vertices[];
} vbo;</code></pre>
</div>
<ol>
<li>The heart of our mesh generation algorithm is the <code>torusKnot()</code> function, which uses the following parametrization to triangulate a torus knot <a href="https://en.wikipedia.org/wiki/Torus_knot">https://en.wikipedia.org/wiki/Torus_knot</a>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>x = r * cos(u)
y = r * sin(u)
z = -sin(v)</code></pre>
</div>
<ol>
<li>The <code>torusKnot()</code> function is rather long and is implemented directly from the previously mentioned parametrization. Feel free to play with the <code>baseRadius</code>, <code>segmentRadius</code>, and <code>tubeRadius</code> values:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>VertexData torusKnot(vec2 uv, float p, float q) {
  const float baseRadius    = 5.0;
  const float segmentRadius = 3.0;
  const float tubeRadius    = 0.5;
  float ct = cos( uv.x );
  float st = sin( uv.x );
  float qp = q / p;
  float qps = qp * segmentRadius;
  float arg = uv.x * qp;
  float sqp = sin( arg );
  float cqp = cos( arg );
  float BSQP = baseRadius + segmentRadius * cqp;
  float dxdt = -qps * sqp * ct - st * BSQP;
  float dydt = -qps * sqp * st + ct * BSQP;
  float dzdt =  qps * cqp;
  vec3 r    = vec3(BSQP * ct, BSQP * st, segmentRadius * sqp);
  vec3 drdt = vec3(dxdt, dydt, dzdt);
  vec3 v1 = normalize(cross(r, drdt));
  vec3 v2 = normalize(cross(v1, drdt));
  float cv = cos( uv.y );
  float sv = sin( uv.y );
  VertexData res;
  res.pos  = vec4(r + tubeRadius * ( v1 * sv + v2 * cv ), 1);
  res.norm = vec4(cross(v1 * cv - v2 * sv, drdt ), 0);
  return res;
}</code></pre>
</div>
<ol>
<li>We are running this compute shader in each frame, so, instead of generating a static set of vertices, we can actually pre-transform them to make the mesh look like it is rotating. Here is a couple of helper functions to compute the appropriate rotation matrices:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>mat3 rotY(float angle) {
  float c = cos(angle), s = sin(angle);
  return mat3(c, 0, -s, 0, 1, 0, s, 0, c);
}
mat3 rotZ(float angle) {
  float c = cos(angle), s = sin(angle);
  return mat3(c, -s, 0, s, c, 0, 0, 0, 1);
}</code></pre>
</div>
<ol>
<li>Using the previously mentioned helpers, the <code>main()</code> function of our compute shader is now straightforward, and the only interesting thing worth mentioning here is the real-time morphing that blends two torus knots with different <code>P</code> and <code>Q</code> parameters. This is pretty easy because the total number of vertices always remains the same:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>void main() {
  uint index = gl_GlobalInvocationID.x;
  vec2 numUV = vec2(pc.numU, pc.numV);
  vec2 ij = vec2(float(index / pc.numV), float(index % pc.numV));</code></pre>
</div>
<ol>
<li>Two sets of UV coordinates for parametrization need to be computed:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  const vec2 maxUV1 = 2.0 * 3.141592653 * vec2(pc.P1, 1.0);
  vec2 uv1 = ij * maxUV1 / (numUV - vec2(1));
  const vec2 maxUV2 = 2.0 * 3.141592653 * vec2(pc.P2, 1.0);
  vec2 uv2 = ij * maxUV2 / (numUV - vec2(1));</code></pre>
</div>
<ol>
<li>Compute the model matrix for our mesh by combining two rotation matrices:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  mat3 modelMatrix = rotY(0.5 * pc.time) * rotZ(0.5 * pc.time);</code></pre>
</div>
<ol>
<li>Compute two vertex positions for two different torus knots defined by the two sets of <code>P</code>-<code>Q</code> parameters: <code>P1</code>-<code>Q1</code> and <code>P2</code>-<code>Q2</code>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  VertexData v1 = torusKnot(uv1, pc.P1, pc.Q1);
  VertexData v2 = torusKnot(uv2, pc.P2, pc.Q2);</code></pre>
</div>
<ol>
<li>Perform a linear blend between them using the <code>pc.morph</code> coefficient. We only need to blend the position and the normal vector. While normal vectors can be interpolated more gracefully, we leave that as another exercise for our readers:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  vec3 pos = mix(v1.pos.xyz, v2.pos.xyz, pc.morph);
  vec3 norm = mix(v1.norm.xyz, v2.norm.xyz, pc.morph);</code></pre>
</div>
<ol>
<li>Fill in the resulting <code>VertexData</code> structure and store it in the output vertex buffer:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  VertexData vtx;
  vtx.pos  = vec4(modelMatrix * pos, 1);
  vtx.tc   = vec4(ij / numUV, 0, 0);
  vtx.norm = vec4(modelMatrix * norm, 0);
  VertexBuffer(pc.bufferId).vertices[index] = vtx;
}</code></pre>
</div>
<p>The vertex shader looks as follows. Just note that the vertex attributes have proper types here:</p>
<div class="C1-SHCodePACKT">
<pre><code>layout(push_constant) uniform PerFrameData {
  mat4 MVP;
};
layout (location=0) in vec4 in_pos;
layout (location=1) in vec2 in_uv;
layout (location=2) in vec3 in_normal;
layout (location=0) out vec2 uv;
layout (location=1) out vec3 normal;
void main() {
  gl_Position = MVP * in_pos;
  uv = in_uv;
  normal = in_normal;
}</code></pre>
</div>
<p>The geometry shader is trivial because the only thing it does is generate barycentric coordinates as described in <em>Integrating tessellation into the graphics pipeline</em>. So, let’s jump straight to the fragment shader, <code>Chapter05/09_ComputeMesh/src/main.frag</code>:</p>
<ol>
<li>The specialization constant is used to switch between the colored and textured versions of the shader:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>#include &lt;Chapter05/09_ComputeMesh/src/common.sp&gt;
layout (location=0) in vec2 uv;
layout (location=1) in vec3 normal;
layout (location=2) in vec3 barycoords;
layout (location=0) out vec4 out_FragColor;
layout (constant_id = 0) const bool isColored = false;</code></pre>
</div>
<ol>
<li>Edge factor calculation for wireframe outlines as described in the <em>Integrating tessellation into the graphics pipeline</em> recipe:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>float edgeFactor(float thickness) {
  vec3 a3 = smoothstep( vec3( 0.0 ),
    fwidth(barycoords) * thickness, barycoords);
  return min( min( a3.x, a3.y ), a3.z );
}</code></pre>
</div>
<ol>
<li>A function to calculate an RGB color for our mesh based on a floating point “hue” value:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>vec3 hue2rgb(float hue) {
  float h = fract(hue);
  float r = abs(h * 6 - 3) - 1;
  float g = 2 - abs(h * 6 - 2);
  float b = 2 - abs(h * 6 - 4);
  return clamp(vec3(r,g,b), vec3(0), vec3(1));
}
void main() {
  float NdotL = dot(normalize(normal), normalize(vec3(0, 0, +1)));
  float intensity = 1.0 * clamp(NdotL, 0.75, 1);
  vec3 color = isColored ?
    intensity * hue2rgb(uv.x) :
    textureBindless2D(pc.textureId, 0, vec2(8,1) * uv).xyz;
  out_FragColor = vec4(color, 1.0);</code></pre>
</div>
<ol>
<li>For high values of <code>numU </code>and <code>numV</code>, the tessellation level is so dense that no distinct wireframe edges are visible—everything collapses into a black Moiré mess. We disable wireframe overlays for values greater than <code>64</code>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>  if (isColored &amp;&amp; pc.numU &lt;= 64 &amp;&amp; pc.numV &lt;= 64)
    out_FragColor =
      vec4( mix( vec3(0.0), color, edgeFactor(1.0) ), 1.0 );
}</code></pre>
</div>
<p>Last but not least, there’s a compute shader responsible for generating an animated texture. You can find it in <code>Chapter05/09_ComputeMesh/src/main_texture.comp</code>, and the idea is identical to the approach described in the previous recipe, <em>Generating textures in Vulkan using compute shaders</em>. We do not copy and paste that shader here.</p>
<p>The demo application will produce a variety of torus knots similar to the one in the following screenshot. Each time you select a new pair of <code>P</code>-<code>Q</code> parameters from the UI, the morphing animation will kick in and transform one knot into another. Checking the <em>Use colored mesh</em> box will apply colors to the mesh instead of a computed texture:</p>
<figure>
<img alt="Figure 5.9: Computed mesh with real-time animation" height="756" src="../media/file39.png" width="1430"/><figcaption aria-hidden="true">Figure 5.9: Computed mesh with real-time animation</figcaption>
</figure>
</section>
<section class="level3" data-number="6.13.4" id="theres-more-7">
<h3 data-number="6.13.4">There’s more…</h3>
<p>Refer to the Wikipedia page <a href="https://en.wikipedia.org/wiki/Torus_knot">https://en.wikipedia.org/wiki/Torus_knot</a> for additional explanation of the math details. Try setting the values of <code>kNumU</code> and <code>kNumV</code> to <code>32</code> while checking “Use colored mesh”.</p>
<p>This is the initial recipe that involves an explicit synchronization process between two independent compute shaders and a rendering pass. The <em>LightweightVK</em> implementation aims to make this synchronization as seamless as possible through explicit dependency specification. In more complex real-world 3D applications, more fine-grained synchronization mechanisms would be desirable. Refer to the guide on Vulkan synchronization from Khronos for valuable insights on how to handle it effectively: <a href="https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples">https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples</a>.</p>
</section>
</section>
</section>
</div></body>
</html>