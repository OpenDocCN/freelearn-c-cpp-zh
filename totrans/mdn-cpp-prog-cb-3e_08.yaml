- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leveraging Threading and Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most computers contain multiple processors or at least multiple cores, and leveraging
    this computational power is key to many categories of applications. Unfortunately,
    many developers still have a mindset of sequential code execution, even though
    operations that do not depend on each other could be executed concurrently. This
    chapter presents standard library support for threads, asynchronous tasks, and
    related components, as well as some practical examples at the end.
  prefs: []
  type: TYPE_NORMAL
- en: Most modern processors (except those dedicated to types of applications that
    do not require great computing power, such as Internet of Things applications)
    have two, four, or more cores that enable you to concurrently execute multiple
    threads of execution. Applications must be explicitly written to leverage the
    multiple processing units that exist; you can write such applications by executing
    functions on multiple threads at the same time. Since C++11, the standard library
    provides support for working with threads, synchronization of shared data, thread
    communication, and asynchronous tasks. In this chapter, we’ll explore the most
    important topics related to threads and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter includes the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronizing access to shared data with mutexes and locks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding alternatives for recursive mutexes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling exceptions from thread functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending notifications between threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using promises and futures to return values from threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing functions asynchronously
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using atomic types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing parallel `map` and `fold` with threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing parallel `map` and `fold` with tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing parallel `map` and `fold` with standard parallel algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using joinable threads and cancellation mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronizing threads with latches, barriers, and semaphores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronizing writing to output streams from multiple threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first part of this chapter, we will look at the various threading objects
    and mechanisms that have built-in support in the library, such as threads, locking
    objects, condition variables, exception handling, and others.
  prefs: []
  type: TYPE_NORMAL
- en: Working with threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A thread is a sequence of instructions that can be managed independently by
    a scheduler, such as the operating system. Threads could be software or hardware.
    Software threads are threads of execution that are managed by the operating system.
    They can run on single processing units, usually by time slicing. This is a mechanism
    where each thread gets a time slot of execution (in the range of milliseconds)
    on the processing unit before the operating system schedules another software
    thread to run on the same processing unit. Hardware threads are threads of execution
    at the physical level. They are, basically, a CPU or a CPU core. They can run
    simultaneously, that is, in parallel, on systems with multiprocessors or multicores.
    Many software threads can run concurrently on a hardware thread, usually by using
    time slicing. The C++ library provides support for working with software threads.
    In this recipe, you will learn how to create and perform operations with threads.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A thread of execution is represented by the `thread` class, available in the
    `std` namespace in the `<thread>` header. Additional thread utilities are available
    in the same header but in the `std::this_thread` namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following examples, the `print_time()` function is used. This function
    prints the local time to the console. Its implementation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will see how to perform common operations with threads.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following solutions to manage threads:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a `std::thread` object without starting the execution of a new thread,
    use its default constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the execution of a function on another thread by constructing a `std::thread`
    object and passing the function as an argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the execution of a function with arguments on another thread by constructing
    a `std::thread` object, and then passing the function as an argument to the constructor,
    followed by its arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To wait for a thread to finish its execution, use the `join()` method on the
    `thread` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To allow a thread to continue its execution independently of the current `thread`
    object, use the `detach()` method. This means the thread will continue its execution
    until it finishes without being managed by the `std::thread` object, which will
    no longer own any thread:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To pass arguments by reference to a function thread, wrap them in either `std::ref`
    or `std::cref` (if the reference is constant):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To stop the execution of a thread for a specified duration, use the `std::this_thread::sleep_for()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To stop the execution of a thread until a specified moment in time, use the
    `std::this_thread::sleep_until()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To suspend the execution of the current thread and provide an opportunity for
    another thread to perform the execution, use `std::this_thread::yield()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `std::thread` class, which represents a single thread of execution, has
    several constructors:'
  prefs: []
  type: TYPE_NORMAL
- en: A default constructor that only creates the thread object but does not start
    the execution of a new thread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A move constructor that creates a new thread object to represent a thread of
    execution previously represented by the object it was constructed from. After
    the construction of the new object, the other object is no longer associated with
    the execution thread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A constructor with a variable number of arguments: the first being a function
    that represents the top-level thread function and the others being arguments to
    be passed to the thread function. Arguments need to be passed to the thread function
    by value. If the thread function takes parameters by reference or by constant
    reference, they must be wrapped in either a `std::ref` or `std::cref` object.
    These are helper function templates that generate objects of the type `std::reference_wrapper`,
    which wraps a reference in a copyable and assignable object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The thread function, in this case, cannot return a value. It is not illegal
    for the function to actually have a return type other than `void`, but it ignores
    any value that is directly returned by the function. If it has to return a value,
    it can do so using a shared variable or a function argument. In the *Using promises
    and futures to return values from threads* recipe, later in this chapter, we will
    see how a thread function returns a value to another thread using a *promise*.
  prefs: []
  type: TYPE_NORMAL
- en: If the function terminates with an exception, the exception cannot be caught
    with a `try...catch` statement in the context where a thread was started and the
    program terminates abnormally with a call to `std::terminate()`. All exceptions
    must be caught within the executing thread, but they can be transported across
    threads via a `std::exception_ptr` object. We’ll discuss this topic in a later
    recipe, called *Handling exceptions from thread functions*.
  prefs: []
  type: TYPE_NORMAL
- en: After a thread has started its execution, it is both joinable and detachable.
    Joining a thread implies blocking the execution of the current thread until the
    joined thread ends its execution. Detaching a thread means decoupling the thread
    object from the thread of execution it represents, allowing both the current thread
    and the detached thread to be executed at the same time. Detached threads are
    sometimes called background threads or daemon threads. When a program terminates
    (by returning from the main function), the detached threads that are still running
    are not waited for. That means the stack of those threads is not unwound. Because
    of this, the destructor of the objects on the stack is not called, which may lead
    to resource leaks or corrupted resources (files, shared memory, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Joining a thread is done with `join()` and detaching a thread is done with `detach()`.
    Once you call either of these two methods, the thread is said to be non-joinable
    and the thread object can be safely destroyed. When a thread is detached, the
    shared data it may need to access must be available throughout its execution.
  prefs: []
  type: TYPE_NORMAL
- en: When you detach a thread, you cannot join it anymore. An attempt to do so will
    result in a runtime error. You can prevent this by checking whether the thread
    can be joined or not by using the `joinable()` member function.
  prefs: []
  type: TYPE_NORMAL
- en: If a thread object goes out of scope and is destroyed but neither `join()` or
    `detach()` has been called, then `std::terminate()` is invoked.
  prefs: []
  type: TYPE_NORMAL
- en: Each thread has an identifier that can be retrieved. For the current thread,
    call the `std::this_thread::get_id()` function. For another thread of execution
    represented by a `thread` object, call its `get_id()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several additional utility functions available in the `std::this_thread`
    namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: The `yield()` method hints at the scheduler to activate another thread. This
    is useful when implementing a busy-waiting routine, as in the last example from
    the previous section. However, the actual behavior is implementation-specific.
    A call to this function, in fact, may have no effect on the execution of threads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sleep_for()` method blocks the execution of the current thread for at least
    the specified period of time (the actual time the thread is put to sleep may be
    longer than the requested period due to scheduling).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sleep_until()` method blocks the execution of the current thread until
    at least the specified time point (the actual duration of the sleep may be longer
    than requested due to scheduling).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `std::thread` class requires the `join()` method to be called explicitly
    to wait for the thread to finish. This can lead to programming errors (as detailed
    above). The C++20 standard provides a new thread class, called `std::jthread`,
    that solves this inconvenience. This will be the topic of the *Using joinable
    threads and cancellation mechanisms* recipe, later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Synchronizing access to shared data with mutexes and locks*, to see what mechanisms
    are available for synchronizing thread access to shared data and how they work'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Finding alternatives for recursive mutexes*, to learn why recursive mutexes
    should be avoided, and also how to transform a thread-safe type using a recursive
    mutex into a thread-safe type using a non-recursive mutex'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Handling exceptions from thread functions*, to understand how to handle exceptions
    thrown in a worker thread from the main thread or the thread where it was joined'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sending notifications between threads*, to see how to use condition variables
    to send notifications between producer and consumer threads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using promises and futures to return values from threads*, to learn how to
    use a `std::promise` object to return a value or an exception from a thread'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronizing access to shared data with mutexes and locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads allow you to execute multiple functions at the same time, but it is
    often necessary that these functions access shared resources. Access to shared
    resources must be synchronized so that only one thread can read or write from
    or to the shared resource at a time. In this recipe, we will see what mechanisms
    the C++ standard defines for synchronizing thread access to shared data and how
    they work.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `mutex` and `lock` classes discussed in this recipe are available in the
    `std` namespace in the `<mutex>` header, and, respectively, `<shared_mutex>` for
    C++14 shared mutexes and locks.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following pattern for synchronizing access with a single shared resource:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a `mutex` in the appropriate context (class or global scope):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Acquire a `lock` on this `mutex` before accessing the shared resource in each
    thread:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following pattern for synchronizing access to multiple shared resources
    at the same time to avoid deadlocks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a mutex for each shared resource in the appropriate context (global
    or class scope):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lock the mutexes at the same time using a deadlock avoidance algorithm with
    `std::lock()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After locking them, adopt the ownership of each mutex into a `std::lock_guard`
    class to ensure they are safely released at the end of the function (or scope):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **mutex** (**mutual exclusion**) is a synchronization primitive that allows
    us to protect simultaneous access to shared resources from multiple threads. The
    C++ standard library provides several implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::mutex` is the most commonly used mutex type; it is illustrated in the
    preceding code snippet. It provides methods to acquire and release the mutex.
    `lock()` tries to acquire the mutex and blocks it if it is not available, `try_lock()`
    tries to acquire the mutex and returns it without blocking if the mutex is not
    available, and `unlock()` releases the mutex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::timed_mutex` is similar to `std::mutex` but provides two more methods
    to acquire the mutex using a timeout: `try_lock_for()` tries to acquire the mutex
    and returns it if the mutex is not made available during the specified duration,
    and `try_lock_until()` tries to acquire the mutex and returns it if the mutex
    is not made available until a specified time point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::recursive_mutex` is similar to `std::mutex`, but the mutex can be acquired
    multiple times from the same thread without being blocked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::recursive_timed_mutex` is a combination of a recursive mutex and a timed
    mutex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::shared_timed_mutex`, since C++14, is to be used in scenarios when multiple
    readers can access the same resource at the same time without causing data races,
    while only one writer is allowed to do so. It implements locking with two levels
    of access – *shared* (several threads can share the ownership of the same mutex)
    and *exclusive* (only one thread can own the mutex) – and provides timeout facilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::shared_mutex`, since C++17, is similar to the `shared_timed_mutex` but
    without the timeout facilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first thread that locks an available mutex takes ownership of it and continues
    with the execution. All consecutive attempts to lock the mutex from any thread
    fail, including the thread that already owns the mutex, and the `lock()` method
    blocks the thread until the mutex is released with a call to `unlock()`. If a
    thread needs to be able to lock a mutex multiple times without blocking it and
    therefore enter a deadlock, a `recursive_mutex` class template should be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical use of a mutex to protect access to a shared resource comprises
    locking the mutex, using the shared resource, and then unlocking the mutex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This method of using the mutex is, however, prone to error. This is because
    each call to `lock()` must be paired with a call to `unlock()` on all execution
    paths; that is, both normal return paths and exception return paths. In order
    to safely acquire and release a mutex, regardless of the way the execution of
    a function goes, the C++ standard defines several locking classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::lock_guard` is the locking mechanism seen earlier; it represents a mutex
    wrapper implemented in an RAII manner. It attempts to acquire the mutex at the
    time of its construction and release it upon destruction. This is available in
    C++11\. The following is a typical implementation of `lock_guard`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`std::unique_lock` is a mutex ownership wrapper that provides support for deferred
    locking, time locking, recursive locking, transfer of ownership, and using it
    with condition variables. This is available in C++11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::shared_lock` is a mutex-shared ownership wrapper that provides support
    for deferred locking, time locking, and transfer of ownership. This is available
    in C++14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::scoped_lock` is a wrapper for multiple mutexes implemented in an RAII
    manner. Upon construction, it attempts to acquire ownership of the mutexes in
    a deadlock avoidance manner as if it is using `std::lock()`, and upon destruction,
    it releases the mutexes in reverse order of the way they were acquired. This is
    available in C++17.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RAII**, which stands for **Resource Acquisition Is Initialization**, is a
    programming technique used in some programming languages, including C++, that
    simplifies resource management, ensures program correctness, and reduces code
    size. This technique binds the life cycle of a resource to an object. The allocation,
    also referred to as the acquisition, of a resource is done during the creation
    of the object (in the constructor) and the release of the resource (deallocation)
    is done when the object is destroyed (in the destructor). This ensures resources
    do not leak, provided that the bound objects are not themselves leaked. For more
    information about RAII, see [https://en.cppreference.com/w/cpp/language/raii](https://en.cppreference.com/w/cpp/language/raii).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first example in the *How to do it...* section, we used `std::mutex`
    and `std::lock_guard` to protect access to the `std::cout` stream object, which
    is shared between all the threads in a program. The following example shows how
    the `thread_func()` function can be executed concurrently on several threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'A possible output for this program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When a thread needs to take ownership of multiple mutexes that are meant to
    protect multiple shared resources, acquiring them one by one may lead to deadlocks.
    Let’s consider the following example (where `container` is the class shown in
    the *How to do it...* section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `container` class holds data that may be accessed simultaneously
    from different threads; therefore, it needs to be protected by acquiring a mutex.
    The `move_between()` function is a thread-safe function that removes an element
    from a container and adds it to a second container. To do so, it acquires the
    mutexes of the two containers sequentially, then erases the element from the first
    container and adds it to the end of the second container.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is, however, prone to deadlocks because a race condition might
    be triggered while acquiring the locks. Suppose we have a scenario where two different
    threads execute this function, but with different arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The first thread starts executing with the arguments `c1` and `c2` in this order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first thread is suspended after it acquires the lock for the `c1` container.
    The second thread starts executing with the arguments `c2` and `c1` in this order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second thread is suspended after it acquires the lock for the `c2` container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first thread continues the execution and tries to acquire the mutex for
    `c2`, but the mutex is unavailable. Therefore, a deadlock occurs (this can be
    simulated by putting the thread to sleep for a short while after it acquires the
    first mutex).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To avoid possible deadlocks such as these, mutexes should be acquired in a
    deadlock avoidance manner, and the standard library provides a utility function
    called `std::lock()` that does that. The `move_between()` function needs to change
    by replacing the two locks with the following code (as shown in the *How to do
    it...* section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The ownership of the mutexes must still be transferred to a lock guard object
    so they are properly released after the execution of the function ends (or, depending
    on the case, when a particular scope ends).
  prefs: []
  type: TYPE_NORMAL
- en: 'In C++17, a new mutex wrapper is available, `std::scoped_lock`, that can be
    used to simplify code, such as the one in the preceding example. This type of
    lock can acquire the ownership of multiple mutexes in a deadlock-free manner.
    These mutexes are released when the scoped lock is destroyed. The preceding code
    is equivalent to the following single line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `scoped_lock` class provides a simplified mechanism for owning one or more
    mutexes for the duration of a scoped block and also helps with writing simple
    and more robust code.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Working with threads*, to learn about the `std::thread` class and the basic
    operations for working with threads in C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using joinable threads and cancellation mechanisms*, to learn about the C++20
    `std::jthread` class, which manages a thread of execution and automatically joins
    during its destruction, as well as the improved mechanisms for stopping the execution
    of threads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Finding alternatives for recursive mutexes*, to learn why recursive mutexes
    should be avoided and how to transform a thread-safe type using a recursive mutex
    into a thread-safe type using a non-recursive mutex'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding alternatives for recursive mutexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The standard library provides several mutex types for protecting access to shared
    resources. `std::recursive_mutex` and `std::recursive_timed_mutex` are two implementations
    that allow you to use multiple locking in the same thread. A typical use for a
    recursive mutex is to protect access to a shared resource from a recursive function.
    A `std::recursive_mutex` class may be locked multiple times from a thread, either
    with a call to `lock()` or `try_lock()`. When a thread locks an available recursive
    mutex, it acquires its ownership; as a result of this, consecutive attempts to
    lock the mutex from the same thread do not block the execution of the thread,
    creating a deadlock. The recursive mutex is, however, released only when an equal
    number of calls to `unlock()` are made. Recursive mutexes may also have a greater
    overhead than non-recursive mutexes. For these reasons, when possible, they should
    be avoided. This recipe presents a use case for transforming a thread-safe type
    using a recursive mutex into a thread-safe type using a non-recursive mutex.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to be familiar with the various mutexes and locks available in the
    standard library. I recommend that you read the previous recipe, *Synchronizing
    access to shared data with mutex and locks*, to get an overview of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this recipe, we will consider the following class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The purpose of this recipe is to transform the `foo_rec` class so we can avoid
    using `std::recursive_mutex`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To transform the preceding implementation into a thread-safe type using a non-recursive
    mutex, do this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Replace `std::recursive_mutex` with `std::mutex`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define private non-thread-safe versions of the public methods or helper functions
    to be used in thread-safe public methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rewrite the public methods to use the newly defined non-thread-safe private
    methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `foo_rec` class we just discussed uses a recursive mutex to protect access
    to shared data; in this case, it is an integer member variable that is accessed
    from two thread-safe public functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`update()` sets a new value in the private variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update_and_return()` sets a new value in the private variable and returns
    the previous value to the called function. This function calls `update()` to set
    the new value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementation of `foo_rec` was probably intended to avoid duplication of
    code, yet this particular approach is rather a design error that can be improved,
    as shown in the *How to do it...* section. Rather than reusing public thread-safe
    functions, we can provide private non-thread-safe functions that could then be
    called from the public interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same solution can be applied to other similar problems: define a non-thread-safe
    version of the code and then provide perhaps lightweight, thread-safe wrappers.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Working with threads*, to learn about the `std::thread` class and the basic
    operations for working with threads in C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Synchronizing access to shared data with mutexes and locks*, to see what mechanisms
    are available for synchronizing thread access to shared data and how they work'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling exceptions from thread functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first recipe, we introduced the thread support library and saw how to
    do some basic operations with threads. In that recipe, we briefly discussed exception
    handling in thread functions and mentioned that exceptions cannot be caught with
    a `try…catch` statement in the context where the thread was started. On the other
    hand, exceptions can be transported between threads within a `std::exception_ptr`
    wrapper. In this recipe, we will see how to handle exceptions from thread functions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are now familiar with the thread operations we discussed in the previous
    recipe, *Working with threads*. The `exception_ptr` class is available in the
    `std` namespace, which is in the `<exception>` header; `mutex` (which we discussed
    in more detail previously) is also available in the same namespace but in the
    `<mutex>` header.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To properly handle exceptions thrown in a worker thread from the main thread
    or the thread where it was joined, do the following (assuming multiple exceptions
    can be thrown from multiple threads):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use a global container to hold instances of `std::exception_ptr`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use a global `mutex` to synchronize access to the shared container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use a `try...catch` block for the code that is being executed in the top-level
    thread function. Use `std::current_exception()` to capture the current exception
    and wrap a copy or its reference into a `std::exception_ptr` pointer, which is
    added to the shared container for exceptions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Clear the container from the main thread before you start the threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the main thread, after the execution of all the threads has finished, inspect
    the caught exceptions and handle each of them appropriately:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the example in the preceding section, we assumed that multiple threads could
    throw exceptions and therefore need a container to hold them all. If there is
    a single exception from a single thread at a time, then you do not need a shared
    container and a mutex to synchronize access to it. You can use a single global
    object of the type `std::exception_ptr` to hold the exception that’s transported
    between threads.
  prefs: []
  type: TYPE_NORMAL
- en: '`std::current_exception()` is a function that is typically used in a `catch`
    clause to capture the current exception and create an instance of `std::exception_ptr`.
    This is done to hold a copy or reference (depending on the implementation) to
    the original exception, which remains valid as long as there is a `std::exception_ptr`
    pointer available that refers to it. If this function is called when no exception
    is being handled, then it creates an empty `std::exception_ptr`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `std::exception_ptr` pointer is a wrapper for an exception captured with
    `std::current_exception()`. If default constructed, it does not hold any exception;
    it is, in this case, a null pointer. Two objects of this type are equal if they
    are both empty or point to the same exception object. The `std::exception_ptr`
    objects can be passed to other threads, where they can be rethrown and caught
    in a `try...catch` block.
  prefs: []
  type: TYPE_NORMAL
- en: '`std::rethrow_exception()` is a function that takes `std::exception_ptr` as
    an argument and throws the exception object referred to by its argument.'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::current_exception()`, `std::rethrow_exception()`, and `std::exception_ptr`
    are all available in C++11.'
  prefs: []
  type: TYPE_NORMAL
- en: In the example from the previous section, each thread function uses a `try...catch`
    statement for the entire code it executes so that no exception may leave the function
    uncaught. When an exception is handled, a lock on the global `mutex` object is
    acquired and the `std::exception_ptr` object holding the current exception is
    added to the shared container. With this approach, the thread function stops at
    the first exception; however, in other circumstances, you may need to execute
    multiple operations, even if the previous one throws an exception. In this case,
    you will have multiple `try...catch` statements and perhaps transport only some
    of the exceptions outside the thread.
  prefs: []
  type: TYPE_NORMAL
- en: In the main thread, after all the threads have finished executing, the container
    is iterated, and each non-empty exception is rethrown and caught with a `try...catch`
    block and handled appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Working with threads*, to learn about the `std::thread` class and the basic
    operations for working with threads in C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Synchronizing access to shared data with mutexes and locks*, to see what mechanisms
    are available for synchronizing thread access to shared data and how they work'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending notifications between threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mutexes are synchronization primitives that can be used to protect access to
    shared data. However, the standard library provides a synchronization primitive,
    called a *condition variable*, that enables a thread to signal to others that
    a certain condition has occurred. The thread or threads that are waiting on the
    condition variable are blocked until the condition variable is signaled or until
    a timeout or a spurious wakeup occurs. In this recipe, we will see how to use
    condition variables to send notifications between thread-producing data and thread-consuming
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, you need to be familiar with threads, mutexes, and locks. Condition
    variables are available in the `std` namespace in the `<condition_variable>` header.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following pattern for synchronizing threads with notifications on condition
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a condition variable (in the appropriate context):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a mutex for threads to lock on. A second mutex should be used for synchronizing
    access to the standard console from different threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the shared data used between the threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the producing thread, lock the mutex before you modify the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the producing thread, signal the condition variable with a call to `notify_one()`
    or `notify_all()` (do this after the mutex used to protect the shared data is
    unlocked):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the consuming thread, acquire a unique lock on the data mutex and use it
    to wait on the condition variable. Beware that spurious wakeups may occur, which
    is a subject we’ll discuss in detail in the *How it works…* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the consuming thread, use the shared data after the condition is notified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The preceding example represents two threads that share common data (in this
    case, an integer variable). One thread produces data after a lengthy computation
    (simulated with a sleep), while the other consumes it only after it is produced.
    To do so, they use a synchronization mechanism that uses a mutex and a condition
    variable that blocks the consuming thread until a notification arises from the
    producer thread, indicating that data has been made available. The key in this
    communication channel is the condition variable that the consuming thread waits
    on until the producing thread notifies it. Both threads start at about the same
    time. The producer thread begins a long computation that is supposed to produce
    data for the consuming thread. At the same time, the consuming thread cannot actually
    proceed until the data is made available; it must remain blocked until it is notified
    that the data has been produced. Once notified, it can continue its execution.
    The entire mechanism works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: There must be at least one thread waiting on the condition variable to be notified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There must be at least one thread that is signaling the condition variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The waiting threads must first acquire a lock on a mutex (`std::unique_lock<std::mutex>`)
    and pass it to the `wait()`, `wait_for()`, or `wait_until()` method of the condition
    variable. All the waiting methods atomically release the mutex and block the thread
    until the condition variable is signaled. At this point, the thread is unblocked
    and the mutex is atomically acquired again (that means the operations involved
    are treated as a whole and the thread cannot be interrupted while performing them).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The thread that signals the condition variable can do so with either `notify_one()`,
    where one blocked thread is unblocked, or `notify_all()`, where all the blocked
    threads waiting for the condition variable are unblocked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Condition variables cannot be made completely predictable on multiprocessor
    systems. Therefore, *spurious wakeups* may occur, and a thread is unlocked even
    if nobody signals the condition variable. So, it is necessary to check whether
    the condition is true after the thread has been unblocked. However, spurious wakeups
    may occur multiple times and, therefore, it is necessary to check the condition
    variable in a loop. You can learn more about spurious wakeups at [https://en.wikipedia.org/wiki/Spurious_wakeup](https://en.wikipedia.org/wiki/Spurious_wakeup).
  prefs: []
  type: TYPE_NORMAL
- en: 'The C++ standard provides two implementations of condition variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::condition_variable`, used in this recipe, defines a condition variable
    associated with `std::unique_lock`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::condition_variable_any` represents a more general implementation that
    works with any lock that meets the requirements of a basic lock (implements the
    `lock()` and `unlock()` methods). A possible use of this implementation is providing
    interruptible waits, as explained by Anthony Williams in *C++ Concurrency In Action*
    (2012):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A custom lock operation would both lock the associated mutex as expected and
    also perform the necessary job of notifying this condition variable when the interrupting
    signal is received.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'All the waiting methods of the condition variable have two overloads:'
  prefs: []
  type: TYPE_NORMAL
- en: The first overload takes `std::unique_lock<std::mutex>` (based on the type;
    that is, duration or time point) and causes the thread to remain blocked until
    the condition variable is signaled. This overload atomically releases the mutex
    and blocks the current thread, and then adds it to the list of threads waiting
    on the condition variable. The thread is unblocked when the condition is notified
    with either `notify_one()` or `notify_all()`, a spurious wakeup occurs, or a timeout
    occurs (depending on the function overload). When this happens, the mutex is atomically
    acquired again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second overload takes a predicate in addition to the arguments of the other
    overloads. This predicate can be used to avoid spurious wakeups while waiting
    for a condition to become `true`. This overload is equivalent to the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, the consumer thread’s implementation is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The consumer thread does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Loops until it is signaled that the process of producing data is finished.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acquires a unique lock on the `mutex` object associated with the condition variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses the `wait_for()` overload, which takes a predicate, checking that the buffer
    is not empty when a wakeup occurs (to avoid spurious wakeups). This method uses
    a timeout of 1 second and returns after the timeout has occurred, even if the
    condition is signaled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consumes all of the data from the queue after it is signaled through the condition
    variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To test this, we can start several producing threads and one consuming thread.
    Producer threads generate random data and, therefore, share the pseudo-random
    generator engines and distributions. All of this is shown in the following code
    sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'A possible output of this program is as follows (the actual output would be
    different for each execution):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The standard also features a helper function called `notify_all_at_thread_exit()`,
    which provides a way for a thread to notify other threads through a `condition_variable`
    object that it’s completely finished execution, including destroying all `thread_local`
    objects. This function has two parameters: a `condition_variable` and a `std::unique_lock<std::mutex>`
    associated with the condition variable (that it takes ownership of). The typical
    use case for this function is running a detached thread that calls this function
    just before finishing.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Working with threads*, to learn about the `std::thread` class and the basic
    operations for working with threads in C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Synchronizing access to shared data with mutexes and locks*, to see what mechanisms
    are available for synchronizing thread access to shared data and how they work'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using promises and futures to return values from threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first recipe of this chapter, we discussed how to work with threads.
    You also learned that thread functions cannot return values and that threads should
    use other means, such as shared data, to do so; however, for this, synchronization
    is required. An alternative to communicating a return value or an exception with
    either the main or another thread is using `std::promise`. This recipe will explain
    how this mechanism works.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `promise` and `future` classes used in this recipe are available in the
    `std` namespace in the `<future>` header.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To communicate a value from one thread to another through promises and futures,
    do this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make a promise available to the thread function through a parameter; for example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call `set_value()` on the promise to set the result to represent a value or
    `set_exception()` to set the result to indicate an exception:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make the future associated with the promise available to the other thread function
    through a parameter; for example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call `get()` on the `future` object to get the result set to the promise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the calling thread, use `get_future()` on the promise to get the `future`
    associated with the promise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The promise-future pair is basically a communication channel that enables a
    thread to communicate a value or exception with another thread through a shared
    state. `promise` is an asynchronous provider of the result and has an associated
    `future` that represents an asynchronous return object. To establish this channel,
    you must first create a promise. This, in turn, creates a shared state that can
    be later read through the future associated with the promise.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set a result to a promise, you can use any of the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: The `set_value()` or `set_value_at_thread_exit()` method is used to set a return
    value; the latter function stores the value in the shared state but only makes
    it available through the associated future if the thread exits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `set_exception()` or `set_exception_at_thread_exit()` method is used to
    set an exception as a return value. The exception is wrapped in a `std::exception_ptr`
    object. The latter function stores the exception in the shared state but only
    makes it available when the thread exits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To retrieve the `future` object associated with `promise`, use the `get_future()`
    method. To get the value from the `future` value, use the `get()` method. This
    blocks the calling thread until the value from the shared state is made available.
    The future class has several methods for blocking the thread until the result
    from the shared state is made available:'
  prefs: []
  type: TYPE_NORMAL
- en: '`wait()` only returns when the result is available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wait_for()` returns either when the result is available or when the specified
    timeout expires.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wait_until()` returns either when the result is available or when the specified
    time point is reached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If an exception is set to the `promise` value, calling the `get()` method on
    the `future` object will throw this exception. The example from the previous section
    has been rewritten as follows to throw an exception instead of setting a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: You can see here that, in the `consume_value()` function, the call to `get()`
    is put in a `try...catch` block. If an exception is caught – and in this particular
    implementation, it is – its message is printed to the console.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Establishing a promise-future channel in this manner is a rather explicit operation
    that can be avoided by using the `std::async()` function; this is a higher-level
    utility that runs a function asynchronously, creates an internal promise and a
    shared state, and returns a future associated with the shared state. We will see
    how `std::async()` works in the next recipe, *Executing functions asynchronously*.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Working with threads*, to learn about the `std::thread` class and the basic
    operations for working with threads in C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Handling exceptions from thread functions*, to understand how to handle exceptions
    thrown in a worker thread from the main thread or the thread where it was joined'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing functions asynchronously
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads enable us to run multiple functions at the same time; this helps us
    take advantage of the hardware facilities in multiprocessor or multicore systems.
    However, threads require explicit, lower-level operations. An alternative to threads
    is tasks, which are units of work that run in a particular thread. The C++ standard
    does not provide a complete task library, but it enables developers to execute
    functions asynchronously on different threads and communicate results back through
    a promise-future channel, as seen in the previous recipe. In this recipe, we will
    see how to do this using `std::async()` and `std::future`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the examples in this recipe, we will use the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, we will use futures; therefore, you are advised to read the
    previous recipe to get a quick overview of how they work. Both `async()` and `future`
    are available in the `std` namespace in the `<future>` header.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To execute a function asynchronously on another thread when the current thread
    is continuing with the execution without expecting a result, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `std::async()` to start a new thread to execute the specified function.
    This will create an asynchronous provider and return a `future` associated with
    it. Use the `std::launch::async` policy for the first argument to the `std::async()`
    function in order to make sure the function will run asynchronously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Continue with the execution of the current thread:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the `wait()` method on the `future` object returned by `std::async()`
    when you need to make sure the asynchronous operation is completed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To execute a function asynchronously on a worker thread while the current thread
    continues its execution, until the result from the asynchronous function is needed
    in the current thread, do the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use `std::async()` to start a new thread to execute the specified function,
    create an asynchronous provider, and return a `future` associated with it. Use
    the `std::launch::async` policy of the first argument to the function to make
    sure the function does run asynchronously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Continue the execution of the current thread:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the `get()` method on the `future` object returned by `std::async()` when
    you need the result from the function to be executed asynchronously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`std::async()` is a variadic function template that has two overloads: one
    that specifies a launch policy as the first argument and another that does not.
    The other arguments to `std::async()` are the function to execute and its arguments,
    if any. The launch policy is defined by a scoped enumeration called `std::launch`,
    available in the `<future>` header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The two available launch policies specify the following:'
  prefs: []
  type: TYPE_NORMAL
- en: With `async`, a new thread is launched to execute the task asynchronously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With `deferred`, the task is executed on the calling thread the first time its
    result is requested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When both flags are specified (`std::launch::async | std::launch::deferred`),
    it is an implementation decision regarding whether to run the task asynchronously
    on a new thread or synchronously on the current thread. This is the behavior of
    the other `std::async()` overload that does not specify a launch policy. This
    behavior is not deterministic.
  prefs: []
  type: TYPE_NORMAL
- en: Do not use the non-deterministic overload of `std::async()` to run tasks asynchronously.
    For this purpose, always use the overload that requires a launch policy, and always
    use only `std::launch::async`.
  prefs: []
  type: TYPE_NORMAL
- en: Both overloads of `std::async()` return a `future` object that refers to the
    shared state created internally by `std::async()` for the promise-future channel
    it establishes. When you need the result of the asynchronous operation, call the
    `get()` method on the future. This blocks the current thread until either the
    result value or an exception is made available. If the future does not transport
    any value or if you are not actually interested in that value, but you want to
    make sure the asynchronous operation will be completed at some point, use the
    `wait()` method; it blocks the current thread until the shared state is made available
    through the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'The future class has two more waiting methods: `wait_for()` specifies a duration
    after which the call ends and returns even if the shared state is not yet available
    through the future, while `wait_until()` specifies a time point after which the
    call returns, even if the shared state is not yet available. These methods could
    be used to create a polling routine and display a status message to the user,
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of running this program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Using promises and futures to return values from threads*, to learn how to
    use a `std::promise` object to return a value or an exception from a thread'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using atomic types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The thread support library offers functionalities for managing threads and synchronizing
    access to shared data with mutexes and locks, and, as of C++20, with latches,
    barriers, and semaphores. The standard library provides support for the complementary,
    lower-level atomic operations on data, which are indivisible operations that can
    be executed concurrently from different threads on shared data, without the risk
    of producing race conditions and without the use of locks. The support it provides
    includes atomic types, atomic operations, and memory synchronization ordering.
    In this recipe, we will see how to use some of these types and functions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the atomic types and operations are defined in the `std` namespace in the
    `<atomic>` header.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are a series of typical operations that use atomic types:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `std::atomic` class template to create atomic objects that support
    atomic operations, such as loading, storing, or performing arithmetic or bitwise
    operations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In C++20, use the `std::atomic_ref` class template to apply atomic operations
    to a referenced object, which can be a reference or pointer to an integral type,
    a floating-point type, or a user-defined type:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `std::atomic_flag` class for an atomic Boolean type:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use the atomic type’s members – `load()`, `store()`, and `exchange()` – or non-member
    functions – `atomic_load()`/`atomic_load_explicit()`, `atomic_store()`/`atomic_store_explicit()`,
    and `atomic_exchange()`/`atomic_exchange_explicit()` – to atomically read, set,
    or exchange the value of an atomic object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use its member functions `fetch_add()` and `fetch_sub()` or non-member functions
    `atomic_fetch_add()`/`atomic_fetch_add_explicit()` and `atomic_fetch_sub()`/`atomic_fetch_sub_explicit()`
    to atomically add or subtract a value to/from an atomic object and return its
    value before the operation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use its member functions `fetch_and()`, `fetch_or()`, and `fetch_xor()` or non-member
    functions `atomic_fetch_and()`/`atomic_fetch_and_explicit()`, `atomic_fetch_or()`/
    `atomic_fetch_or_explicit()`, and `atomic_fetch_xor()`/`atomic_fetch_xor_explicit()`
    to perform AND, OR, and XOR atomic operations, respectively, with the specified
    argument and return the value of the atomic object before the operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `std::atomic_flag` member functions `test_and_set()` and `clear()` or
    non-member functions `atomic_flag_test_and_set()`/`atomic_flag_test_and_set_explicit()`
    and `atomic_flag_clear()`/`atomic_flag_clear_explicit()` to set or reset an atomic
    flag. In addition, in C++20, you can use the member function `test()` and the
    non-member function `atomic_flag_test()`/`atomic_flag_test_explicit()` to atomically
    return the value of the flag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In C++20, perform thread synchronization with member functions `wait()`, `notify_one()`,
    and `notify_all()`, available to `std::atomic`, `std::atomic_ref`, and `std::atomic_flag`,
    as well as the non-member functions `atomic_wait()`/`atomic_wait_explicit()`,
    `atomic_notify_one()`, and `atomic_notify_all()`. These functions provide a more
    efficient mechanism for waiting for the value of an atomic object to change than
    polling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`std::atomic` is a class template that defines (including its specializations)
    an atomic type. The behavior of an object of an atomic type is well defined when
    one thread writes to the object and the other reads data, without using locks
    to protect access. The operations on atomic variables are treated as single, uninterruptable
    actions. If two threads want to write on the same atomic variable, the first to
    take hold of it will write, while the other will wait for the atomic write to
    complete before it writes. This is a deterministic behavior and does not require
    additional locking.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `std::atomic` class provides several specializations:'
  prefs: []
  type: TYPE_NORMAL
- en: Full specialization for `bool`, with a typedef called `atomic_bool`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full specialization for all integral types, with type aliases (typedefs) called
    `atomic_bool` (for `std::atomic<bool>`), `atomic_int` (for `std::atomic<int>`),
    `atomic_long` (for `std::atomic<long>`), `atomic_char` (for `std::atomic<char>`),
    `atomic_size_t` (for `std::atomic<std::size_t>`), and many others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partial specialization for pointer types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In C++20, full specializations for the floating-point types `float`, `double`,
    and `long double`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In C++20, partial specializations such as `std::atomic<std::shared_ptr<U>>`
    for `std::shared_ptr` and `std::atomic<std::weak_ptr<U>>` for `std::weak_ptr`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `atomic` class template has various member functions that perform atomic
    operations, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`load()` to atomically load and return the value of the object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`store()` to atomically store a non-atomic value in the object; this function
    does not return anything.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exchange()` to atomically store a non-atomic value in the object and return
    the previous value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`operator=`, which has the same effect as `store(arg)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fetch_add()` to atomically add a non-atomic argument to the atomic value and
    return the value stored previously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fetch_sub()` to atomically subtract a non-atomic argument from the atomic
    value and return the value stored previously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fetch_and()`, `fetch_or()`, and `fetch_xor()` to atomically perform a bitwise
    AND, OR, or XOR operation between the argument and the atomic value; store the
    new value in the atomic object; and return the previous value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prefixing and postfixing `operator++` and `operator--` to atomically increment
    and decrement the value of the atomic object with 1\. These operations are equivalent
    to using `fetch_add()` or `fetch_sub()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`operator +=`, `-=`, `&=`, `|=`, and `ˆ=` to add, subtract, or perform bitwise
    AND, OR, or XOR operations between the argument and the atomic value and store
    the new value in the atomic object. These operations are equivalent to using `fetch_add()`,
    `fetch_sub()`, `fetch_and()`, `fetch_or()`, and `fetch_xor()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider you have an atomic variable, such as `std::atomic<int> a`; the following
    is not an atomic operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'This involves a series of operations, some of which are atomic:'
  prefs: []
  type: TYPE_NORMAL
- en: Atomically load the value of the atomic object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add 42 to the value that was loaded (which is not an atomic operation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atomically store the result in the atomic object `a`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, the following operation, which uses the member operator
    `+=`, is atomic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This operation has the same effect as either of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Though `std::atomic` has a full specialization for the `bool` type, called
    `std::atomic<bool>`, the standard defines yet another atomic type called `std::atomic_flag`,
    which is guaranteed to be lock-free. This atomic type, however, is very different
    than `std::atomic<bool>`, and it has only the following member functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`test_and_set()` atomically sets the value to `true` and returns the previous
    value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clear()` atomically sets the value to `false`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In C++20, there’s `test()`, which atomically returns the value of the flag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prior to C++20, the only way to initialize a `std::atomic_flag` to a definite
    value was by using the `ATOMIC_FLAG_INIT` macro. This initializes the atomic flag
    to the clear (`false`) value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: In C++20, this macro has been deprecated because the default constructor of
    `std::atomic_flag` initializes it to the clear state.
  prefs: []
  type: TYPE_NORMAL
- en: All member functions mentioned earlier, for both `std::atomic` and `std::atomic_flag`,
    have non-member equivalents that are prefixed with `atomic_` or `atomic_flag_`,
    depending on the type they refer to. For instance, the equivalent of `std::atomic::fetch_add()`
    is `std::atomic_fetch_add()`, and the first argument of these non-member functions
    is always a pointer to a `std::atomic` object. Internally, the non-member function
    calls the equivalent member function on the provided `std::atomic` argument. Similarly,
    the equivalent of `std::atomic_flag::test_and_set()` is `std::atomic_flag_test_and_set()`,
    and its first parameter is a pointer to a `std::atomic_flag` object.
  prefs: []
  type: TYPE_NORMAL
- en: All these member functions of `std::atomic` and `std::atomic_flag` have two
    sets of overloads; one of them has an extra argument representing a memory order.
    Similarly, all non-member functions – such as `std::atomic_load()`, `std::atomic_fetch_add()`,
    and `std::atomic_flag_test_and_set()` – have a companion with the suffix `_explicit`
    – `std::atomic_load_explicit()`, `std::atomic_fetch_add_explicit()`, and `std::atomic_flag_test_and_set_explicit()`;
    these functions have an extra argument that represents the memory order.
  prefs: []
  type: TYPE_NORMAL
- en: The memory order specifies how non-atomic memory accesses are to be ordered
    around atomic operations. By default, the memory order of all atomic types and
    operations is *sequential consistency*.
  prefs: []
  type: TYPE_NORMAL
- en: Additional ordering types are defined in the `std::memory_order` enumeration
    and can be passed as an argument to the member functions of `std::atomic` and
    `std::atomic_flag`, or the non-member functions with the suffix `_explicit()`.
  prefs: []
  type: TYPE_NORMAL
- en: '*Sequential consistency* is a consistency model that requires that, in a multiprocessor
    system, all instructions are executed in some order and all writes become instantly
    visible throughout the system. This model was first proposed by Leslie Lamport
    in the 70s, and is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“the results of any execution is the same as if the operations of all the
    processors were executed in some sequential order, and the operations of each
    individual processor appear in this sequence in the order specified by its program.”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Various types of memory ordering functions are described in the following table,
    taken from the C++ reference website ([http://en.cppreference.com/w/cpp/atomic/memory_order](http://en.cppreference.com/w/cpp/atomic/memory_order)).
    The details of how each of these works is beyond the scope of this book and can
    be looked up in the standard C++ reference (see the previous link):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| `memory_order_relaxed` | This is a relaxed operation. There are no synchronization
    or ordering constraints; only atomicity is required from this operation. |'
  prefs: []
  type: TYPE_TB
- en: '| `memory_order_consume` | A load operation with this memory order performs
    a consume operation on the affected memory location; no reads or writes in the
    current thread that are dependent on the value currently loaded can be reordered
    before this load operation. Writes to data-dependent variables in other threads
    that release the same atomic variable are visible in the current thread. On most
    platforms, this affects compiler optimizations only. |'
  prefs: []
  type: TYPE_TB
- en: '| `memory_order_acquire` | A load operation with this memory order performs
    the acquire operation on the affected memory location; no reads or writes in the
    current thread can be reordered before this load. All writes in other threads
    that release the same atomic variable are visible in the current thread. |'
  prefs: []
  type: TYPE_TB
- en: '| `memory_order_release` | A store operation with this memory order performs
    the release operation; no reads or writes in the current thread can be reordered
    after this store. All writes in the current thread are visible in other threads
    that acquire the same atomic variable, and writes that carry a dependency to the
    atomic variable become visible in other threads that consume the same atomic variable.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `memory_order_acq_rel` | A read-modify-write operation with this memory order
    is both an acquire operation and a release operation. No memory reads or writes
    in the current thread can be reordered before or after this store. All writes
    in other threads that release the same atomic variable are visible before the
    modification, and the modification is visible in other threads that acquire the
    same atomic variable. |'
  prefs: []
  type: TYPE_TB
- en: '| `memory_order_seq_cst` | Any operation with this memory order is both an
    acquire operation and a release operation; a single total order exists in which
    all threads observe all modifications in the same order. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8.1: std::memory_order members that describe how memory access is ordered
    for an atomic operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first example in the *How to do it...* section shows several threads repeatedly
    modifying a shared resource – a counter – by incrementing it concurrently. This
    example can be refined further by implementing a class to represent an atomic
    counter with methods such as `increment()` and `decrement()`, which modify the
    value of the counter, and `get()`, which retrieves its current value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'With this class template, the first example can be rewritten in the following
    form with the same result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: If you need to perform atomic operations on references, you cannot use `std::atomic`.
    However, in C++20, you can use the new `std::atomic_ref` type. This is a class
    template that applies atomic operations to the object it references. This object
    must outlive the `std::atomic_ref` object and, as long as any `std::atomic_ref`
    instance referencing this object exists, the object must be accessed only through
    the `std::atomic_ref` instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `std::atomic_ref` type has the following specializations:'
  prefs: []
  type: TYPE_NORMAL
- en: The primary template can be instantiated with any trivially copyable type `T`,
    including `bool`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partial specialization for all pointer types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specializations for integral types (character types, signed and unsigned integer
    types, and any additional integral types needed by the typedefs in the `<cstdint>`
    header).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specializations for the floating-point types `float`, `double`, and `long double`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When using `std::atomic_ref`, you must keep in mind that:'
  prefs: []
  type: TYPE_NORMAL
- en: It is not thread-safe to access any sub-object of the object referenced by a
    `std::atomic_ref`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to modify the referenced value through a const `std::atomic_ref`
    object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, in C++20, there are new member and non-member functions that provide
    an efficient thread-synchronization mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: The member function `wait()` and non-member functions `atomic_wait()`/`atomic_wait_explicit()`
    and `atomic_flag_wait()`/`atomic_flag_wait_explicit()` perform atomic wait operations,
    blocking a thread until notified and the atomic value changes. Its behavior is
    similar to repeatedly comparing the provided argument with the value returned
    by `load()` and, if equal, blocks until notified by `notify_one()` or `notify_all()`,
    or the thread is unblocked spuriously. If the compared values are not equal, then
    the function returns without blocking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The member function `notify_one()` and non-member functions `atomic_notify_one()`
    and `atomic_flag_notify_one()` notify, atomically, at least one thread blocked
    in an atomic waiting operation. If there is no such thread blocked, the function
    does nothing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The member function `notify_all()` and the non-member functions `atomic_notify_all()`
    and `atomic_flag_notify_all()` unblock all the threads blocked in an atomic waiting
    operation or do nothing if no such thread exists.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, it should be mentioned that all the atomic objects from the standard
    atomic operations library – `std::atomic`, `std::atomic_ref`, and `std::atomic_flag`
    – are free of data races.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Working with threads*, to learn about the `std::thread` class and the basic
    operations for working with threads in C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Synchronizing access to shared data with mutexes and locks*, to see what mechanisms
    are available for synchronizing thread access to shared data and how they work'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Executing functions asynchronously*, to learn how to use the `std::future`
    class and the `std::async()` function to execute functions asynchronously on different
    threads and communicate the result back'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing parallel map and fold with threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Chapter 3*, *Exploring Functions*, we discussed two higher-order functions:
    `map`, which applies a function to the elements of a range by either transforming
    the range or producing a new range, and `fold` (also referred to as `reduce`),
    which combines the elements of a range into a single value. The various implementations
    we did were sequential. However, in the context of concurrency, threads, and asynchronous
    tasks, we can leverage the hardware and run parallel versions of these functions
    to speed up their execution for large ranges, or when the transformation and aggregation
    are time-consuming. In this recipe, we will see a possible solution for implementing
    `map` and `fold` using threads.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to be familiar with the concepts of the `map` and `fold` functions.
    It is recommended that you read the *Implementing higher-order functions map and
    fold* recipe from *Chapter 3*, *Exploring Functions*. In this recipe, we will
    use the various thread functionalities presented in the *Working with threads*
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: To measure the execution time of these functions and compare it with sequential
    alternatives, we will use the `perf_timer` class template, which we introduced
    in the *Measuring function execution time with a standard clock* recipe in *Chapter
    6*, *General-Purpose Utilities*.
  prefs: []
  type: TYPE_NORMAL
- en: A parallel version of an algorithm can potentially speed up execution time,
    but this is not necessarily true in all circumstances. Context switching for threads
    and synchronized access to shared data can introduce a significant overhead. For
    some implementations and particular datasets, this overhead could make a parallel
    version actually take a longer time to execute than a sequential version.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine the number of threads required to split the work, we will use
    the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: We’ll explore the first possible implementation for a parallel version of the
    `map` and `fold` functions in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To implement a parallel version of the `map` function, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function template that takes the `begin` and `end` iterators of a
    range and a function to apply to all the elements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the size of the range. If the number of elements is smaller than a predefined
    threshold (for this implementation, the threshold is 10,000), execute the mapping
    in a sequential manner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For larger ranges, split the work on multiple threads and let each thread map
    be a part of the range. These parts should not overlap to avoid the need to synchronize
    access to the shared data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the threads, and on each thread, run a sequential version of the mapping:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Wait until all the threads have finished their execution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding steps, when put together, result in the following implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'To implement a parallel version of the left `fold` function, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function template that takes a `begin` and an `end` iterator for a
    range, an initial value, and a binary function to apply to the elements of the
    range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the size of the range. If the number of elements is smaller than a predefined
    threshold (for this implementation, it is 10,000), execute the folding in a sequential
    manner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For larger ranges, split the work into multiple threads and let each thread
    fold a part of the range. These parts should not overlap in order to avoid thread
    synchronization of shared data. The result can be returned through a reference
    passed to the thread function in order to avoid data synchronization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the threads, and on each thread, execute a sequential version of the
    folding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Wait until all the threads have finished execution and fold the partial results
    into the final result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The steps we just put together result in the following implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These parallel implementations of `map` and `fold` are similar in several aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: They both fall back to a sequential version if the number of elements in the
    range is smaller than 10,000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They both start the same number of threads. These threads are determined using
    the static function `std::thread::hardware_concurrency()`, which returns the number
    of concurrent threads supported by the implementation. However, this value is
    more of a hint than an accurate value and should be used with that in mind.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No shared data is used to avoid synchronization of access. Even though all the
    threads work on the elements from the same range, they all process parts of the
    range that do not overlap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both of these functions are implemented as function templates that take a begin
    and an end iterator to define the range to be processed. In order to split the
    range into multiple parts to be processed independently by different threads,
    use additional iterators in the middle of the range. For this, we use `std::advance()`
    to increment an iterator with a particular number of positions. This works well
    for vectors or arrays but is very inefficient for containers such as lists. Therefore,
    this implementation is suited only for ranges that have random access iterators.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sequential versions of `map` and `fold` can be simply implemented in C++
    with `std::transform()` and `std::accumulate()`. In fact, to verify the correctness
    of the parallel algorithms and check whether they provide any execution speedup,
    we can compare them with the execution of these general-purpose algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To put this to the test, we will use `map` and `fold` on a vector with sizes
    varying from 10,000 to 50 million elements. The range is first mapped (that is,
    transformed) by doubling the value of each element, and then the result is folded
    into a single value by adding together all the elements of the range. For simplicity,
    each element in the range is equal to its 1-based index (the first element is
    1, the second element is 2, and so on). The following sample runs both the sequential
    and parallel versions of `map` and `fold` on vectors of different sizes and prints
    the execution time in a tabular format:'
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise, you can vary the number of elements, as well as the number of
    threads, and see how the parallel version performs compared to the sequential
    version.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'A possible output of this program is shown in the following chart (executed
    on a machine running Windows 64-bit with an Intel Core i7 processor and 4 physical
    and 8 logical cores). The parallel version, especially the `fold` implementation,
    performs better than the sequential version. But this is true only when the length
    of the vector exceeds a certain size. In the following table, we can see that
    for up to 1 million elements, the sequential version is still faster. The parallel
    version executes faster when there are 2 million or more elements in the vector.
    Notice that the actual times vary slightly from one run to another, even on the
    same machine, and they can be very different on different machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'To better visualize these results, we can represent the speedup of the parallel
    version in the form of a bar chart. In the following chart, the blue bars represent
    the speedup of a parallel `map` implementation, while the orange bars show the
    speedup of the parallel `fold` implementation. A positive value indicates that
    the parallel version is faster; a negative version indicates that the sequential
    version is faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21549_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: The speedup of the parallel implementation for map (in blue) and
    fold (in orange) for various processed elements'
  prefs: []
  type: TYPE_NORMAL
- en: This chart makes it easier to see that only when the number of elements exceeds
    a certain threshold (which is about 2 million in my benchmarks) is the parallel
    implementation faster than the sequential version.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Chapter 3*, *Implementing higher-order functions map and fold*, to learn about
    higher-order functions in functional programming and see how to implement the
    widely used `map` and `fold` (or reduce) functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Implementing parallel map and fold with tasks*, to see how to implement the
    `map` and `fold` functions from functional programming using asynchronous functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Implementing parallel map and fold with standard parallel algorithms*, to
    see how to implement the `map` and `fold` functions from functional programming
    using parallel algorithms from C++17'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Working with threads*, to learn about the `std::thread` class and the basic
    operations for working with threads in C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing parallel map and fold with tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tasks are a higher-level alternative to threads for performing concurrent computations.
    `std::async()` enables us to execute functions asynchronously, without the need
    to handle lower-level threading details. In this recipe, we will take the same
    task of implementing a parallel version of the `map` and `fold` functions, as
    in the previous recipe, but we will use tasks and see how it compares with the
    thread version.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The solution presented in this recipe is similar in many aspects to the one
    that uses threads in the previous recipe, *Implementing parallel map and fold
    with threads*. Make sure you read that one before continuing with the current
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To implement a parallel version of the `map` function, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function template that takes a begin and end iterator for a range
    and a function to apply to all the elements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the size of the range. For a number of elements smaller than the predefined
    threshold (for this implementation, the threshold is 10,000), execute the mapping
    in a sequential manner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For larger ranges, split the work into multiple tasks and let each task map
    a part of the range. These parts should not overlap to avoid synchronizing thread
    access to shared data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the asynchronous functions and run a sequential version of the mapping
    on each of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Wait until all the asynchronous functions have finished their execution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These steps, when put together, result in the following implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'To implement a parallel version of the left `fold` function, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function template that takes a begin and end iterator for a range,
    an initial value, and a binary function to apply to the elements of the range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the size of the range. For a number of elements smaller than the predefined
    threshold (for this implementation, the threshold is 10,000), execute the folding
    in a sequential manner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For larger ranges, split the work into multiple tasks and let each task fold
    a part of the range. These parts should not overlap to avoid synchronizing thread
    access to the shared data. The result can be returned through a reference passed
    to the asynchronous function to avoid synchronization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the asynchronous functions and execute a sequential version of folding
    on each one of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Wait until all the asynchronous functions have finished execution and fold
    the partial results into the final result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These steps, when put together, result in the following implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The implementation just proposed is only slightly different than what we did
    in the previous recipe. Threads were replaced with asynchronous functions, starting
    with `std::async()`, and results were made available through the returned `std::future`.
    The number of asynchronous functions that are launched concurrently is equal to
    the number of threads the implementation can support. This is returned by the
    static method `std::thread::hardware_concurrency()`, but this value is only a
    hint and should not be considered very reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are mainly two reasons for taking this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Seeing how a function implemented for parallel execution with threads can be
    modified to use asynchronous functions and, therefore, avoid lower-level details
    of threading.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a number of asynchronous functions equal to the number of supported
    threads can potentially run one function per thread; this could provide the fastest
    execution time for the parallel function because there is a minimum overhead of
    context switching and waiting time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can test the performance of the new `map` and `fold` implementations using
    the same method as in the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'A possible output of the preceding program, which can vary slightly from one
    execution to another and greatly from one machine to another, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the illustration of the solution with threads, the speedup of the
    parallel `map` and `fold` implementations can be seen in the following chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'Negative values indicate that the sequential version was faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21549_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: The speedup of the parallel implementation of map (in blue) and
    fold (in orange) using asynchronous functions, compared to the sequential implementation'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we compare this with the results from the parallel version using threads,
    we will find that these are faster execution times and that the speedup is significant,
    especially for the `fold` function. The following chart shows the speedup of the
    task’s implementation over the thread’s implementation. In this chart, a value
    smaller than 1 means that the thread’s implementation was faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21549_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: The speedup of the parallel implementation using asynchronous functions
    over the parallel implementation using threads for map (in blue) and fold (in
    orange)'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The implementation shown earlier is only one of the possible approaches we
    can take for parallelizing the `map` and `fold` functions. A possible alternative
    uses the following strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the range to process into two equal parts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively call the parallel function asynchronously to process the first part
    of the range.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively call the parallel function synchronously to process the second part
    of the range.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the synchronous recursive call is finished, wait for the asynchronous
    recursive call to end too before finishing the execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This divide-and-conquer algorithm can potentially create a lot of tasks. Depending
    on the size of the range, the number of asynchronous calls can greatly exceed
    the number of threads, and in this case, there will be lots of waiting time that
    will affect the overall execution time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `map` and `fold` functions can be implemented using a divide-and-conquer
    algorithm, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'The execution times for this implementation are listed here, next to the ones
    for the previous implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: When we compare these execution times, we can see that this version (indicated
    by `p2` in the preceding output) is similar to the sequential version for both
    `map` and `fold` and much worse than the first parallel version shown earlier
    (indicated by `p1`).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Implementing parallel map and fold with threads*, to see how to implement
    the `map` and `fold` functions from functional programming using raw threads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Implementing parallel map and fold with standard parallel algorithms*, to
    see how to implement the `map` and `fold` functions from functional programming
    using parallel algorithms from C++17'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Executing functions asynchronously*, to learn how to use the `std::future`
    class and the `std::async()` function to execute functions asynchronously on different
    threads and communicate the result back'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing parallel map and fold with standard parallel algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two recipes, we implemented parallel versions of the `map` and
    `fold` functions (which are called `std::transform()` and `std::accumulate()`
    in the standard library) using threads and tasks. However, these implementations
    required manual handling of parallelization details, such as splitting data into
    chunks to be processed in parallel and creating threads or tasks, synchronizing
    their execution, and merging the results.
  prefs: []
  type: TYPE_NORMAL
- en: In C++17, many of the standard generic algorithms have been parallelized. In
    fact, the same algorithm can execute sequentially or in parallel, depending on
    a provided execution policy. In this recipe, we will learn how to implement `map`
    and `fold` in parallel with standard algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you continue with this recipe, it is recommended that you read the previous
    two to make sure you understand the differences between various parallel implementations.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use the standard algorithms with parallel execution, you should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Find a good candidate for an algorithm to parallelize. Not every algorithm runs
    faster in parallel. Make sure you correctly identify the parts of the program
    that can be improved with parallelization. Use profilers for this purpose and,
    in general, look at operations that have *O(n)* or worse complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include the header `<execution>` for the execution policies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide the parallel execution policy (`std::execution::par`) as the first argument
    to the overloaded algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A parallel implementation of the map function using the parallel overload of
    `std::transform()` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'A parallel implementation of the fold function using the parallel overload
    of `std::reduce()` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In C++17, 69 of the standard generic algorithms have been overloaded to support
    parallel execution. These overloads take an execution policy as the first parameter.
    The available execution policies, from header `<execution>`, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Policy** | **Since** | **Description** | **Global object** |'
  prefs: []
  type: TYPE_TB
- en: '| `std::execution::sequenced_policy` | C++17 | Indicates that the algorithm
    may not execute in parallel. | `std::execution::seq` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::execution::parallel_policy` | C++17 | Indicates that the algorithm’s
    execution may be parallelized. | `std::execution::par` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::execution::parallel_unsequenced_policy` | C++17 | Indicates that the
    algorithm’s execution may be parallelized and vectorized. | `std::execution::par_unseq`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `std::execution::unsequenced_policy` | C++20 | Indicates that the algorithm’s
    execution may be vectorized. | `std::execution::unseq` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8.2: Execution policies from the <execution> header'
  prefs: []
  type: TYPE_NORMAL
- en: Vectorization is the process of transforming an algorithm so that instead of
    working on a single value at a time it would work on a set of values (vector)
    at the same time. Modern processors provide this at a hardware level through **SIMD**
    (**Single Instruction, Multiple Data**) units.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the existing algorithms that have been overloaded, seven new algorithms
    have been added:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `std::for_each_n` | Applies a given function to the first *N* elements of
    the specified range, according to the specified execution policy. |'
  prefs: []
  type: TYPE_TB
- en: '| `std::exclusive_scan` | Computes the partial sum of a range of elements (using
    either `std::plus<>` or a binary operation) but excludes the *i*th element from
    the *i*th sum. If the binary operation is associative, the result is the same
    as when using `std::partial_sum()`. |'
  prefs: []
  type: TYPE_TB
- en: '| `std::inclusive_scan` | Computes the partial sum of a range of elements (using
    either `std::plus<>` or a binary operation) but includes the *i*th element in
    the *i*th sum. |'
  prefs: []
  type: TYPE_TB
- en: '| `std::transform_exclusive_scan` | Applies a unary function to each element
    of a range and then calculates an exclusive scan on the resulting range. |'
  prefs: []
  type: TYPE_TB
- en: '| `std::transform_inclusive_scan` | Applies a unary function to each element
    of a range and then calculates an inclusive scan on the resulting range. |'
  prefs: []
  type: TYPE_TB
- en: '| `std::reduce` | An out-of-order version of `std::accumulate()`. |'
  prefs: []
  type: TYPE_TB
- en: '| `std::transform_reduce` | Applies a function to the elements of a range then
    accumulates the elements of the resulting range out of order (that is, reduces).
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8.2: New algorithms in C++17 from the <algorithm> and <numeric> headers'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding examples, we used `std::transform()` and `std::reduce()` with
    an execution policy – in our case, `std::execution::par`. The algorithm `std::reduce()`
    is similar to `std::accumulate()` but it processes the elements out of order.
    `std::accumulate()` does not have an overload for specifying an execution policy,
    so it can only execute sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that, just because an algorithm supports parallelization,
    it doesn’t mean that it will run faster than the sequential version. Execution
    depends on the actual hardware, datasets, and algorithm particularities. In fact,
    some of these algorithms may never, or hardly ever, execute faster when parallelized
    than sequentially. For this reason, for instance, the Microsoft implementation
    of several algorithms that permute, copy, or move elements does not perform parallelization
    but falls back to sequential execution in all cases. These algorithms are `copy()`,
    `copy_n()`, `fill()`, `fill_n()`, `move()`, `reverse()`, `reverse_copy()`, `rotate()`,
    `rotate_copy()`, and `swap_ranges()`. Moreover, the standard does not guarantee
    a particular execution; specifying a policy is actually a request for an execution
    strategy but with no guarantees implied.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the standard library allows parallel algorithms to allocate
    memory. When this cannot be done, an algorithm throws `std::bad_alloc`. However,
    again, the Microsoft implementation differs and instead of throwing an exception,
    it falls back to the sequential version of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect that must be known is that the standard algorithms
    work with different kinds of iterators. Some require forward iterators, some input
    iterators. However, all the overloads that allow specifying an execution policy
    restrict the use of the algorithm with forward iterators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21549_08_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: A comparison of execution times for sequential and parallel implementations
    of the map and reduce functions'
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can see a comparison of execution times for sequential and parallel
    implementations of the `map` and `reduce` functions. Highlighted are the versions
    of the functions implemented in this recipe. These times may vary slightly from
    execution to execution. These values were obtained by running a 64-bit released
    version compiled with Visual C++ 2019 16.4.x on a machine with an Intel Xeon CPU
    with four cores. Although the parallel versions perform better than the sequential
    version for these datasets, which one is actually better varies with the size
    of the dataset. This is why profiling is key when you optimize by parallelizing
    work.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we have seen separate implementations for `map` and `fold`
    (which is also called reduce). However, in C++17, there is a standard algorithm
    called `std::transform_reduce()`, which composes the two operations into a single
    function call. This algorithm has overloads for sequential execution, as well
    as policy-based execution for parallelism and vectorization. We can, therefore,
    utilize this algorithm instead of the handwritten implementation we did in these
    previous three recipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the sequential and parallel versions of the algorithm used
    to compute the sum of the doubles of all the elements of a range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'If we compare the execution time of these two calls, seen in the following
    table in the last two columns, with the total time for separately calling `map`
    and `reduce`, as seen in the other implementations, you can see that `std::transform_reduce()`,
    especially the parallel version, executes better in most cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21549_08_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: A comparison of execution times for the transform/reduce pattern
    with a highlight of the times for the std::transform_reduce() standard algorithm
    from C++17'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Chapter 3*, *Implementing higher-order functions map and fold*, to learn about
    higher-order functions in functional programming and see how to implement the
    widely used `map` and `fold` (or reduce) functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Implementing parallel map and fold with threads*, to see how to implement
    the `map` and `fold` functions from functional programming using raw threads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Implementing parallel map and fold with tasks*, to see how to implement the
    `map` and `fold` functions from functional programming using asynchronous functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using joinable threads and cancellation mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The C++11 class `std::thread` represents a single thread of execution and allows
    multiple functions to execute concurrently. However, it has a major inconvenience:
    you must explicitly invoke the `join()` method to wait for the thread to finish
    execution. This can lead to problems because if a `std::thread` object is destroyed
    while it is still joinable, then `std::terminate()` is called. C++20 provides
    an improved thread class called `std::jthread` (from *joinable thread*) that automatically
    calls `join()` if the thread is still joinable when the object is destroyed. Moreover,
    this type supports cancellation through `std::stop_source`/`std::stop_token` and
    its destructor also requests the thread to stop before joining. In this recipe,
    you will learn how to use these new C++20 types.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you continue with this, you should read the first recipe of this chapter,
    *Working with threads*, to make sure you are familiar with `std::thread`. To use
    `std::jthread`, you need to include the same `<thread>` header. For `std::stop_source`
    and `std::stop_token`, you need to include the header `<stop_token>`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The typical scenarios for using joinable threads and a cooperative cancellation
    mechanism are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to automatically join a thread object when it goes out of scope,
    use `std::jthread` instead of `std::thread`. You can still use all the methods
    that `std::thread` has, such as explicitly joining with `join()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you need to be able to cancel the execution of a thread, you should do the
    following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure the first parameter of the thread function is a `std::stop_token`
    object.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the thread function, periodically check if stopping was requested using the
    `stop_requested()` method of the `std::stop_token` object and stop when signaled.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `std::jthread` for executing the function on a separate thread.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From the calling thread, use the `request_stop()` method of the `std::jthread`
    object to request the thread function to stop and return:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'If you need to cancel the work of multiple threads, then you can do the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All thread functions must take a `std::stop_token` object as the first argument.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: All thread functions should periodically check if a stop was requested by calling
    the `stop_requested()` method of `std::stop_token` and, if a stop was requested,
    abort the execution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `std::jthread` to execute functions on different threads.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the calling thread, create a `std::stop_source` object.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a `std::stop_token` object by calling the `get_token()` method of the `std::stop_source`
    object and pass it as the first argument for the thread function when creating
    `std::jthread` objects.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When you want to stop the execution of the thread functions, call the `request_stop()`
    method of the `std::stop_source` object.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'If you need to execute a piece of code when a stop source is requesting cancellation,
    you can use a `std::stop_callback` created with the `std::stop_token` object,
    which signals the stop request and a callback function that is invoked when the
    stop is requested (through the `std::stop_source` object associated with `std::stop_token`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`std::jthread` is very similar to `std::thread`. It is, in fact, an attempt
    to fix what was missing for threads in C++11\. Its public interface is very similar
    to `std::thread`. All the methods `std::thread` has are also present in `std::thread`.
    However, it differs in the following key aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Internally, it maintains, at least logically, a shared stop state, which allows
    for the request of the thread function to stop execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It has several methods for handling cooperative cancellation: `get_stop_source()`,
    which returns a `std::stop_source` object associated with the shared stop state
    of the thread, `get_stop_token()`, which returns a `std::stop_token` associated
    with the shared stop state of the thread, and `request_stop()`, which requests
    the cancellation of the execution of the thread function via the shared stop state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The behavior of its destructor, which, when the thread is joinable, calls `request_stop()`
    and then `join()` to first signal the request to stop execution and then wait
    until the thread has finished its execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can create `std::jthread` objects just as you would create `std::thread`
    objects. However, the callable function that you pass to a `std::jthread` can
    have a first argument of the type `std::stop_token`. This is necessary when you
    want to be able to cooperatively cancel the thread’s execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typical scenarios include graphical user interfaces where user interaction
    may cancel work in progress, but many other situations can be envisioned. The
    invocation of such a function thread happens as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If the first argument for the thread function, supplied when constructing `std::jthread`,
    is a `std::stop_token`, it is forwarded to the callable function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the first argument, when there are arguments, for the callable function is
    not a `std::stop_token` object, then the `std::stop_token` object associated with
    the `std::jthread` object’s internal shared stop state is passed to the function.
    This token is obtained with a call to `get_stop_token()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function thread must periodically check the status of the `std::stop_token`
    object. The `stop_requested()` method checks if a stop was requested. The request
    to stop comes from a `std::stop_source` object.
  prefs: []
  type: TYPE_NORMAL
- en: If multiple stop tokens are associated with the same stop source, a stop request
    is visible to all the stop tokens. If a stop is requested, it cannot be withdrawn,
    and successive stop requests have no meaning. To request a stop, you should call
    the `request_stop()` method. You can check if a `std::stop_source` is associated
    with a stop state and can be requested to stop by calling the `stop_possible()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to invoke a callback function when a stop source is requested to
    stop, then you can use the `std::stop_callback` class. This associates a `std::stop_token`
    object with a callback function. When the stop source of the stop token is requested
    to stop the callback is invoked. Callback functions are invoked as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the same thread that invoked `request_stop()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the thread constructing the `std::stop_callback` object, if the stop has
    already been requested before the stop callback object has been constructed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can create any number of `std::stop_callback` objects for the same stop
    token. However, the order the callbacks are invoked in is unspecified. The only
    guarantee is that they will be executed synchronously, provided that the stop
    has been requested after the `std::stop_callback` objects have been created.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to note that, if any callback function returns via an exception,
    then `std::terminate()` will be invoked.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Working with threads*, to learn about the `std::thread` class and the basic
    operations for working with threads in C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sending notifications between threads*, to see how to use condition variables
    to send notifications between producer and consumer threads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronizing threads with latches, barriers, and semaphores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The thread support library from C++11 includes mutexes and condition variables
    that enable thread-synchronization to shared resources. A mutex allows only one
    thread of multiple processes to execute, while other threads that want to access
    a shared resource are put to sleep. Mutexes can be expensive to use in some scenarios.
    For this reason, the C++20 standard features several new, simpler synchronization
    mechanisms: latches, barriers, and semaphores. Although these do not provide new
    use cases, they are simpler to use and can be more performant because they may
    internally rely on lock-free mechanisms.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The new C++20 synchronization mechanisms are defined in new headers. You have
    to include `<latch>` for `std::latch`, `<barrier>`, or `std::barrier`, and `<semaphore>`
    for `std::counting_semaphore` and `std::binary_semaphore`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code snippets in this recipe will use the following two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the C++20 synchronization mechanisms as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `std::latch` when you need threads to wait until a counter, decreased by
    other threads, reaches zero. The latch must be initialized with a non-zero count
    and multiple threads can decrease it, while others wait for the count to reach
    zero. When that happens, all waiting threads are awakened and the latch can no
    longer be used. If the latch count does not decrease to zero (not enough threads
    decrease it) the waiting threads will be blocked forever. In the following example,
    four threads are creating data (stored in a vector of integers) and the main thread
    waits for the completion of them all by utilizing a `std::latch`, decremented
    by each thread after completing its work:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `std::barrier` when you need to perform loop synchronization between parallel
    tasks. You construct a barrier with a count and, optionally, a completion function.
    Threads arrive at the barrier, decrease the internal counter, and block. When
    the counter reaches zero, the completion function is invoked, all blocked threads
    are awakened, and a new cycle begins. In the following example, four threads are
    creating data that they store in a vector of integers. When all the threads have
    completed a cycle, the data is processed in the main thread, by a completion function.
    Each thread blocks after completing a cycle until they are awakened through the
    use of a `std::barrier` object, which also stores the completion function. This
    process is repeated 10 times:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `std::counting_semaphore<N>` or `std::binary_semaphore` when you want to
    restrict a number of *N* threads (a single thread, in the case of `binary_semaphore`)
    to access a shared resource, or when you want to pass notifications between different
    threads. In the following example, four threads are creating data that is added
    to the end of a vector of integers. To avoid race conditions, a `binary_semaphore`
    object is used to restrict the access to the vector to a single thread:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `std::latch` class implements a counter that can be used to synchronize
    threads. It is a race-free class that works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The counter is initialized when the latch is created and can only be decreased.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A thread may decrease the value of the latch and can do so multiple times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A thread may block by waiting until the latch counter reaches zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the counter reaches zero, the latch becomes permanently signaled and all
    the threads that are blocked on the latch are awakened.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `std::latch` class has the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Methods** | **Descriptions** |'
  prefs: []
  type: TYPE_TB
- en: '| `count_down()` | Decrements the internal counter by *N* (which is `1` by
    default) without blocking the caller. This operation is performed atomically.
    *N* must be a positive value no greater than the value of the internal counter;
    otherwise, the behavior is undefined. |'
  prefs: []
  type: TYPE_TB
- en: '| `try_wait()` | Indicates whether the internal counter reaches zero, in which
    case it returns `true`. There is a very low probability that, although the counter
    has reached zero, the function may still return `false`. |'
  prefs: []
  type: TYPE_TB
- en: '| `wait()` | Blocks the calling thread until the internal counter reaches zero.
    If the internal counter is already zero, the function returns immediately without
    blocking. |'
  prefs: []
  type: TYPE_TB
- en: '| `arrive_and_wait()` | This function is equivalent to calling `count_down()`,
    followed by `wait()`. It decrements the internal counter with *N* (which is `1`
    by default) and then blocks the calling thread until the internal counter reaches
    zero. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8.3: std::memory_order members that describe how memory access is ordered
    for an atomic operation'
  prefs: []
  type: TYPE_NORMAL
- en: In the first example in the previous section, we have a `std::latch`, called
    `work_done`, initialized with the number of threads (or jobs) that perform work.
    Each thread produces data that is then written in a shared resource, a vector
    of integers. Although this is shared, there is no race condition because each
    thread writes to a different place; therefore, there is no need for a synchronization
    mechanism. After completing its work, each thread decrements the counter of the
    latch. The main thread waits until the counter of the latch reaches zero, after
    which it processes the data from the threads.
  prefs: []
  type: TYPE_NORMAL
- en: Because the internal counter of `std::latch` cannot be incremented or reset,
    this synchronization mechanism can be used only once. A similar but reusable synchronization
    mechanism is `std::barrier`. A barrier allows threads to block until an operation
    is completed and is useful for managing repeated tasks performed by multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'A barrier works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A barrier contains a counter that is initialized during its creation and can
    be decreased by threads arriving at the barrier. When the counter reaches zero,
    it is reset to its initial value and the barrier can be reused.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A barrier also contains a completion function that is called when the counter
    reaches zero. If a default completion function is used, it is invoked as part
    of the call to `arrive_and_wait()` or `arrive_and_drop()`. Otherwise, the completion
    function is invoked on one of the threads that participate in the completion phase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process through which a barrier goes from start to reset is called the **completion
    phase**. This starts with a so-called **synchronization point** and ends with
    the **completion step**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first *N* threads that arrive at the synchronization point after the construction
    of the barrier are said to be the **set of participating threads**. Only these
    threads are allowed to arrive at the barrier during each of the following cycles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A thread that arrives at the synchronization point may decide to participate
    in the completion phase by calling `arrive_and_wait()`. However, a thread may
    remove itself from the participation set by calling `arrive_and_drop()`. In this
    case, another thread must take its place in the participation set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When all the threads in the participation set have arrived at the synchronization
    point, the completion phase is executed. There are three steps that occur: first,
    the completion function is invoked. Second, all the threads that are blocked are
    awakened. Third, and last, the barrier count is reset and a new cycle begins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `std::barrier` class has the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Methods** | **Descriptions** |'
  prefs: []
  type: TYPE_TB
- en: '| `arrive()` | Arrives at the barrier’s synchronization point and decrements
    the expected count by a value n. The behavior is undefined if the value of n is
    greater than the expected count, or equal to or less than zero. The function executes
    atomically. |'
  prefs: []
  type: TYPE_TB
- en: '| `wait()` | Blocks at the synchronization point until the completion step
    is executed. |'
  prefs: []
  type: TYPE_TB
- en: '| `arrive_and_wait()` | Arrives at the barrier’s synchronization point and
    blocks. The calling thread must be in the participating set; otherwise, the behavior
    is undefined. This function only returns after the completion phase ends. |'
  prefs: []
  type: TYPE_TB
- en: '| `arrive_and_drop()` | Arrives at the barrier’s synchronization point and
    removes the thread from the participation set. It is an implementation detail
    whether the function blocks or not until the end of the completion phase. The
    calling thread must be in the participation set; otherwise, the behavior is undefined.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8.4: The member functions of the std::barrier class'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: The last synchronization mechanism available in the thread support library in
    C++20 is represented by semaphores. A semaphore contains an internal counter that
    can be both decreased and increased by multiple threads. When the counter reaches
    zero, further attempts to decrease it will block the thread, until another thread
    increases the counter.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two semaphore classes: `std::counting_semaphore<N>` and `std::binary_semaphore`.
    The latter is actually just an alias for `std::counting_semaphore<1>`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `counting_semaphore` allows *N* threads to access a shared resource, unlike
    a mutex, which only allows one. `binary_semaphore`, is, in this matter, similar
    to the mutex, because only one thread can access the shared resource. On the other
    hand, a mutex is bound to a thread: the thread that locked the mutex must unlock
    it. However, this is not the case for semaphores. A semaphore can be released
    by threads that did not acquire it, and a thread that acquired a semaphore does
    not have to also release it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `std::counting_semaphore` class has the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Methods** | **Descriptions** |'
  prefs: []
  type: TYPE_TB
- en: '| `acquire()` | Decrements the internal counter by 1 if it is greater than
    0\. Otherwise, it blocks until the counter becomes greater than 0. |'
  prefs: []
  type: TYPE_TB
- en: '| `try_acquire()` | Tries to decrement the counter by 1 if it is greater than
    0\. It returns `true` if it succeeds, or `false` otherwise. This method does not
    block. |'
  prefs: []
  type: TYPE_TB
- en: '| `try_acquire_for()` | Tries to decrease the counter by 1 if it is greater
    than 0\. Otherwise, it blocks either until the counter becomes greater than 0
    or a specified timeout occurs. The function returns `true` if it succeeds in decreasing
    the counter. |'
  prefs: []
  type: TYPE_TB
- en: '| `try_acquire_until()` | Tries to decrease the counter by 1 if it is greater
    than 0\. Otherwise, it blocks either until the counter becomes greater than 0
    or a specified time point has been passed. The function returns `true` if it succeeds
    in decreasing the counter. |'
  prefs: []
  type: TYPE_TB
- en: '| `release()` | Increments the internal counter by the specified value (which
    is 1 by default). Any thread that was blocked waiting for the counter to become
    greater than 0 is awakened. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8.5: The member functions of the std::counting_semaphore class'
  prefs: []
  type: TYPE_NORMAL
- en: All the increment and decrement operations performed on the counter by the methods
    listed here are executed atomically.
  prefs: []
  type: TYPE_NORMAL
- en: The last example in the *How to do it...* section shows how a `binary_semaphore`
    can be used. A number of threads (four, in this example) produce work in a loop
    and write to a shared resource. Unlike the previous examples, they simply add
    to the end of a vector of integers. Therefore, the access to this vector must
    be synchronized between the threads, and this is where the binary semaphore is
    used. In each loop, the thread function creates a new value (which may take some
    time). This value is then appended to the end of the vector. However, the thread
    must call the `acquire()` method of the semaphore to make sure it is the only
    thread that can continue execution and access the shared resource. After the write
    operation completes, the thread calls the `release()` method of the semaphore
    in order to increment the internal counter and allow another thread to access
    the shared resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'Semaphores can be used for multiple purposes: to block access to shared resources
    (similar to mutexes), to signal or pass notifications between threads (similar
    to condition variables), or to implement barriers, often with better performance
    than similar mechanisms.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Working with threads*, to learn about the `std::thread` class and the basic
    operations for working with threads in C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Synchronizing access to shared data with mutexes and locks*, to see what mechanisms
    are available for synchronizing thread access to shared data and how they work'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sending notifications between threads*, to see how to use condition variables
    to send notifications between producer and consumer threads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronizing writing to output streams from multiple threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`std::cout` is a global object of the `std::ostream` type. It is used to write
    text to the standard output console. Although writing to it is guaranteed to be
    thread-safe, this applies to just one invocation of the `operator<<`. Multiple
    such sequenced calls to `operator<<` can be interrupted and resumed later, making
    it necessary to employ synchronization mechanisms to avoid corrupted results.
    This applies to all scenarios where multiple threads operate on the same output
    stream. To simplify this scenario, C++20 introduced `std::basic_osyncstream` to
    provide a mechanism to synchronize threads writing to the same output stream.
    In this recipe, you will learn how to use this new utility.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To synchronize access to an output stream for writing from multiple threads,
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Include the `<syncstream>` header.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a variable of the `std::osyncstream` type to wrap the shared output stream,
    such as `std::cout`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the wrapper variable exclusively to write to the output stream.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following snippet shows an example for this pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By default, the standard C++ stream objects `std::cin`/`std::wcin`, `std::cout`/`std::wcout`,
    `std::cerr`/`std::wcerr`, and `std::clog`/`std::wclog` are synchronized with their
    respective C streams, `stdin`, `stdout`, and `stderr` (unless a call to `std::ios_base::sync_with_stdio()`
    disables this synchronization). What this means is that any operation applied
    to a C++ stream object is immediately applied to the corresponding C stream. Moreover,
    accessing these streams is guaranteed to be thread-safe. This means that calls
    to `operator <<` or `>>` are atomic; another thread cannot access the stream until
    the call completes. However, multiple calls can be interrupted, as shown in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'The output differs on different executions, but it looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: There are three different invocations to `operator <<` in the thread function.
    Although each executes atomically, the thread can be suspended in between calls
    for another thread to get a chance to execute. This is why we see the output having
    the shape shown earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be solved in several ways. One can use a synchronization mechanism,
    such as a mutex. However, in this particular case, a simpler solution is to use
    a local `std::stringstream` object to build the text to be displayed on the console
    and make a single invocation to `operator<<`, as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'With these changes, the output has the form that was expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: In C++20, you can use a `std::osyncstream`/`std::wosyncstream` object to wrap
    an output stream to synchronize access, as shown in the *How to do it…* section.
    The `osyncstream` class guarantees there are no data races if all the write operations
    from different threads occur through instances of this class. The `std::basic_osyncstream`
    class wraps an instance of `std::basic_syncbuf`, which, in turn, wraps an output
    buffer but also contains a separate internal buffer. This class accumulates output
    in an internal buffer and transmits it to the wrapped buffer when the object is
    destructed or when an explicit call to the `emit()` member function occurs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sync stream wrappers can be used to synchronize access to any output stream,
    not just `std::ostream`/`std::wostream` (the type of `std::cout`/`std::wcout`).
    For instance, it can be used to synchronize access to a string stream, as shown
    in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we define a `std::ostringstream` object called `str`. In the
    inner block, this is wrapped by a `std::osyncstream` object and then we write
    the text `"sync stream demo"` through this wrapper to the string stream. On the
    line marked with **[1]**, we print the content of the string stream to the console.
    However, the content of the stream’s buffer is empty because the sync stream has
    not been destroyed, nor has a call to `emit()` occurred. When the sync stream
    goes out of scope, the content of its inner buffer is transferred to the wrapped
    stream. Therefore, on the line marked with **[2]**, the `str` string stream contains
    the text `"sync stream demo"`. This results in the following output for the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'We can elaborate on this example to show how the `emit()` member function affects
    the behavior of the streams. Let’s consider the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'The first part of this second example is the same. On line **[1]**, the content
    of the string buffer is empty. However, after the call to `emit()`, the sync stream
    transfers the content of its inner buffer to the wrapped output stream. Therefore,
    on line **[2]**, the string buffer contains the text `"sync stream demo"`. New
    text, `"demo part 2"`, is written to the string stream through the sync stream,
    but this is not transferred to the string stream before the line marked with **[3]**
    executes; therefore, at this point the content of the string stream is unchanged.
    Upon going out of scope at the end of the inner block, the new content of the
    sync stream’s inner buffer is again transferred to the wrapped string stream,
    which will now contain the text `"sync stream demodemo part 2"`. As a result,
    the output of this second example is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'The `std::basic_syncstream` class has a member function called `get_wrapped()`,
    which returns a pointer to the wrapped stream buffer. This can be used to construct
    a new instance of the `std::basic_syncstream` class so that you can sequence content
    to the same output stream through different instances of `std::basic_osyncstream`.
    The next snippet demonstrates how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, the first part of the example is unchanged. However, here we have a
    second inner block where a second instance of `std::osyncstream` is constructed
    with a pointer to the stream buffer returned by the call to `syncstr`''s `get_wrapped()`
    member function. At the line marked with **[2]**, none of the two instances of
    `std::osyncstream` has been destroyed; therefore, the content of the `str` string
    stream is still empty. The first sync stream to be destroyed is `syncstr2`, at
    the end of the second inner block. Therefore, on the line marked with **[3]**,
    the content of the string stream will be `"demo part 3"`. Then, the first sync
    stream object, `syncstr`, goes out of scope at the end of the first inner block,
    adding the text `"sync stream demo"` to the string stream. The output of running
    this program is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'Although in all these examples we defined named variables, you can write to
    an output stream using a temporary sync stream too, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Working with threads*, to learn about the `std::thread` class and the basic
    operations for working with threads in C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using joinable threads and cancellation mechanisms*, to learn about the C++20
    `std::jthread` class, which manages a thread of execution and automatically joins
    during its destruction, as well as the improved mechanisms for stopping the execution
    of threads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://discord.gg/7xRaTCeEhx](Chapter_08.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code2659294082093549796.png)'
  prefs: []
  type: TYPE_IMG
