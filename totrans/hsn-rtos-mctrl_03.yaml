- en: Understanding RTOS Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The super loop programming paradigm is typically one of the first programming
    methods that an embedded systems engineer will encounter. A program implemented
    with a super loop has a single top-level loop that cycles through the various
    functions the system needs to perform. These simple `while` loops are easy to
    create and understand (when they are small). In FreeRTOS, tasks are very similar
    to super loops – the main difference is that the system can have more than one
    task, but only one super loop.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will take a closer look at super loops and different ways
    of achieving a degree of parallelism with them. After that, a comparison between
    super loops and tasks will be made and a theoretical way of thinking about task
    execution will be introduced. Finally, we'll take a look at how tasks are *actually* executed
    with an RTOS kernel and compare two basic scheduling algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing super loop programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving parallel operations with super loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing RTOS tasks to super loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving parallel operations with RTOS tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RTOS tasks versus super loops – pros and cons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are no software or hardware requirements for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing super loop programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is one common property that all embedded systems share – they don't have
    an exit point. Because of its nature, embedded code is generally expected to always
    be available – silently running in the background, taking care of housekeeping
    tasks, and ready for user input at any time. Unlike desktop environments that
    are meant to start and stop programs, there isn't anything for a micro-controller
    to do if it exits the `main()` function. If this happens, it is likely that the
    entire device has stopped functioning. For this reason the `main()` function in
    an embedded system never returns. Unlike application programs, which are started
    and stopped by their host OS, most embedded MCU-based applications start at power
    on and end abruptly when the system is powered off. Because of this abrupt shutdown,
    embedded applications typically don't have any of the shutdown tasks normally
    associated with applications, such as freeing memory and resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code represents the basic idea of a super loop. Take a look at
    this before moving on to the more detailed explanations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: While extremely simple, the preceding code has a number of features worth pointing
    out. The `while` loop never returns – it goes on forever executing the same three
    functions (this is intended). The three innocent-looking function calls can hide
    some nasty surprises in a real-time system.
  prefs: []
  type: TYPE_NORMAL
- en: The basic super loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This main loop that never returns is generally referred to as a *super loop*.
    It''s always fun to think *super* because it has control over most things in the
    system – nothing gets done in the following diagram unless the super loop makes
    it happen. This type of setup is perfect for very simple systems that need to
    perform just a few tasks that don''t take a considerable amount of time. Basic
    super loop structures are extremely easy to write and understand; if the problem
    you''re trying to solve can be done with a simple super loop, then use a simple
    super loop. Here is the execution flow of the code presented previously – each
    function is called sequentially and the loop never exits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc1de34f-1936-496b-bdf2-84cbb1d49776.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's have a look at what this execution looks like in a real-time system
    and some of the drawbacks associated with this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Super loops in real-time systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When simple super loops are operating quickly (usually because they have limited
    functionality/responsibility), they are quite responsive. However, the simplicity
    of the super loop can be a blessing and a curse. Since each function always follows
    the preceding function, they are always called in the same sequence and fully
    dependent on one another. Any delay introduced by one function propagates to the
    next function, which causes the total amount of time it takes to execute that
    iteration of the loop to increase (as seen in the following diagram). If `func1`
    takes 10 us to execute one time through the loop, and then 100 ms the next, `func2`
    isn''t going to be called nearly as quickly the second time through the loop as
    it was the first time through:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50b20b51-d2ed-46c5-9d06-858b44634a4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at this in a little bit more depth. In the preceding diagram,
    `func3` is responsible for checking the state of a flag representing an external
    event (this event signals a rising edge of a signal). The frequency of how often `func3`
    checks the flag is dependent on how long `func1` and `func2` take to execute.
    A well designed and responsive super loop will typically execute very rapidly,
    checking for events more often than they occur (callout B). When an external event
    does occur, the loop doesn''t detect the event until the next time `func3` executes
    (callouts A, C, and D). Notice that there is a delay between when the event is
    generated and when it is detected by `func3`. Also note that the delay isn''t
    always consistent: this difference in time is referred to as jitter.'
  prefs: []
  type: TYPE_NORMAL
- en: In many super loop-based systems, the execution speed of the super loop is extremely
    high compared to slowly occurring events being polled. We don't have enough room
    on the page to show a loop executing hundreds (or thousands) of iterations between
    detecting an event!
  prefs: []
  type: TYPE_NORMAL
- en: If a system has a known maximum amount of jitter when responding to an event,
    it is considered to be deterministic. That is, it will reliably respond to an
    event within the specified amount of time after that event occurs. A high level
    of determinism is crucial for time-critical components in a real-time system because,
    without it, the system could fail to respond to important events in a timely manner.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the case of a loop checking a hardware flag repeatedly for an event
    (this is referred to as polling). The tighter the loop, the faster the flag is
    checked – when the flag is checked often, the code will be more responsive to
    the event of interest. If we have an event that needs to be acted upon in a timely
    manner, we could just write a really tight loop and wait for the important event
    to occur. This approach works – but *only* if that event is the only thing of
    interest for the system. If the *only* responsibility the entire system has is
    watching for that event (no background I/O, communication, and so on), then this
    is a valid approach. This type of situation rarely occurs in today's complex real-world
    systems. Poor responsiveness is the limitation of solely polled-based systems.
    Next up, we'll take a look at how to get a bit more parallelism in our super loop.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving parallel operations with super loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though a basic super loop can only step through functions sequentially,
    there are still ways to achieve parallelism. MCUs have a few different types of
    specialized hardware designed to take some of the burden away from the CPU, while
    still enabling a highly responsive system. This section will introduce those systems
    and how they can be used within the context of a super loop style program.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing interrupts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Polling for a single event is not only wasteful in terms of CPU cycles and power
    – it also results in a system that isn't responsive to anything else, which should
    generally be avoided. So then, how can we get a single core processor to do things
    in parallel? Well, we can't – there's only one processor after all. . . but since
    our processor is likely to be running millions of instructions per second, it
    is possible to get it to perform things that are close enough to parallel. MCUs
    also include dedicated hardware for generating interrupts. Interrupts provide
    signals to the MCU that allow it to jump directly to an **interrupt service routine**
    (**ISR**) when the event occurs. This is such a critical piece of functionality
    that ARM Cortex-M cores provide a standardized peripheral for it, called the **nested
    vector in****terrupt controller** (**NVIC**). The NVIC provides a common way of
    dealing with interrupts. The *nested* portion of this term signifies that even
    interrupts can be interrupted by other interrupts with a higher priority. This
    is quite convenient since it allows us to minimize the amount of latency and jitter
    for the most time-critical pieces of the system.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do interrupts fit into a super loop in a way that better achieves the
    illusion of parallel activity? The code inside an ISR is generally kept as short
    as possible, in order to minimize the amount of time spent in the interrupt. This
    is important for a few reasons. If the interrupt occurs very often and the ISR
    contains a *lot* of instructions, there is a chance that the ISR won't return
    before being called again. For communication peripherals such as UART or SPI,
    this will mean dropped data (which obviously isn't desirable). Another reason
    to keep the code short is because other interrupts also need to be serviced, which
    is why it's a good idea to push off any responsibility to the code that isn't
    running inside an ISR context.
  prefs: []
  type: TYPE_NORMAL
- en: 'To quickly get an idea of how ISRs contribute to jitter, let''s take a look
    at a simple example of an external **analog to digital converter** (**ADC**) signaling
    to an MCU that a reading has been taken and the conversion is ready to be transferred
    to the MCU (refer to the hardware diagram shown here):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddf5caac-58f0-435e-9286-9efceea5f8e0.png)'
  prefs: []
  type: TYPE_IMG
- en: In the ADC hardware, a pin is dedicated to signaling that a reading of an analog
    value has been converted to a digital representation and is ready for transfer
    to the MCU. The MCU would then initiate a transfer over the communication medium
    (COM in the diagram).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s have a look at how the ISR calls might stack up against one another
    over time, relative to the rising edge on the conversion ready line. The following
    diagram shows six different instances of ISR being called in response to a rising
    edge of a signal. The small amount of time between when the rising edge occurs
    in the hardware versus when the ISR in firmware is invoked is the minimum latency.
    The jitter in the response of the ISR is the difference in the latency over many
    different cycles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fe1639a-87e9-4bc4-9a6c-0506b974a644.png)'
  prefs: []
  type: TYPE_IMG
- en: There are different ways to minimize latency and jitter for critical ISRs. In
    ARM Cortex-M-based MCUs, interrupt priorities are flexible – a single interrupt
    source can be assigned different priorities at runtime. The ability to reprioritize
    interrupts is one way of making sure the most important parts of a system get
    the CPU when they need it.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, it is important to keep the amount of code executing in
    interrupts as short as possible, since code that is inside an ISR will take precedence
    over any code that is not in an ISR (for example `main()`). Additionally, lower
    priority ISRs won't be executed until all of the code in a higher priority ISR
    has been executed and the ISR exits – which is why it is important to keep ISRs
    short. It is always a good idea to try and limit how much *responsibility* (and
    therefore code) an ISR has.
  prefs: []
  type: TYPE_NORMAL
- en: When multiple interrupts are nested, they don't fully return – there's actually
    a really useful feature of ARM Cortex M processors called interrupt-tail chaining.
    If the processor detects that an interrupt is about to exit, but another one is
    pending, the next ISR will be executed without the processor totally restoring
    the pre-interrupt state, which further reduces latency.
  prefs: []
  type: TYPE_NORMAL
- en: Interrupts and super loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way of achieving minimal instructions and responsibility in the ISR is to
    do the smallest amount of work possible inside the ISR and then set a flag that
    is checked by code running in the super loop. This way, the interrupt can be serviced
    as soon as possible, without the entire system being dedicated to waiting on the
    event. In the following diagram, notice how the interrupt is being generated multiple
    times before finally being dealt with by `func3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on what exactly that interrupt is trying to achieve, it will typically
    take a value from the associated peripheral and push it into an array (or take
    a value from an array and feed it to the peripheral registers). In the case of
    our external ADC, the ISR (triggered each time the ADC performs a conversion)
    would go out to the ADC, transfer the digitized reading, and store it in RAM,
    setting a flag indicating that one or more values are ready for processing. This
    allows for the interrupt to be serviced multiple times without involving the higher-level
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b0bf6dd0-d0a7-4e8b-9636-245b27bc1f85.png)'
  prefs: []
  type: TYPE_IMG
- en: In the case of a communication peripheral that is transmitting large blocks
    of data, an array can be used as a queue for storing items to be transmitted.
    At the end of the entire transmission, a flag can be set to notify the main loop
    of the completion. There are many examples of situations where queuing values
    are appropriate. For instance, if some processing is required to be performed
    on a block of data, it is often advantageous to collect the data first and then
    process the entire block together outside of the interrupt. An interrupt-driven
    approach isn't the only way to achieve this blocked-data approach. In the next
    section, we'll take a look at a piece of hardware that can make moving large blocks
    of data both easier for the programmer, and more efficient for the processor.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing DMA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Remember the assertion that the processor couldn''t *really *do things truly
    in parallel? This is still true. *However *. . . modern MCUs contain more than
    just a processing core. While our processing core is chugging along dealing with
    instructions, there are many other hardware subsystems hard at work inside the
    MCU. One of these hard working subsystems is called a **Direct Memory Access Controller**
    (**DMA**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a6eab36-865d-4565-8043-b8eb88bad6bd.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram presents a very simplified hardware block diagram that
    shows a view of two different data paths available from RAM to a UART peripheral.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of receiving a stream of bytes from a UART without DMA, information
    from the UART will move into the UART registers, be read by the CPU, and then
    pushed out to RAM for storage:'
  prefs: []
  type: TYPE_NORMAL
- en: The CPU must detect when an individual byte (or word) has been received, either
    by polling the UART register flags, or by setting up an interrupt service routine
    that will be fired when a byte is ready.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the byte is transferred from the UART, the CPU can then place it into
    RAM for further processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps 1 and 2 are repeated until the entire message is received.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When DMA is used in the same scenario, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: The CPU configures the DMA controller and peripheral for the transfer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DMA controller takes care of ALL transfers between the UART peripheral and
    RAM. This requires no intervention from the CPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The CPU will be notified when the entire transfer is complete and it can go
    directly to processing the entire byte stream.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most programmers find DMA to be nearly magical if they're accustomed to dealing
    with super loops and ISRs. The controller is configured to transfer a block of
    memory to the peripheral, as the peripheral needs it, and then provide a notification
    (typically via an interrupt) when the transfer is complete – that's it!
  prefs: []
  type: TYPE_NORMAL
- en: This convenience does come at a price, of course. It does take some time to
    set up the DMA transfer initially, so for small transfers, it might actually take
    more CPU time to set up the transfer than if an interrupt or polled method was
    used.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also some caveats to be aware of: each MCU has specific limitations,
    so be sure to read the details of the datasheet, reference manual, and errata
    before counting on the availability of DMA for a critical design component of
    the system:'
  prefs: []
  type: TYPE_NORMAL
- en: The bandwidth of the MCU's internal buses limits the number of bandwidth-hungry
    peripherals that can be reliably placed on a single bus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Occasionally, limited availability of mapped DMA channels to peripherals also
    complicates the design process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These types of reasons are why it is important to get all team members involved
    with the early-stage design of embedded systems, rather than just *throwing it
    over the wall*.
  prefs: []
  type: TYPE_NORMAL
- en: DMA is great for accessing a large number of peripherals efficiently, giving
    us the ability to add more and more functionality to the system. However, as we
    start adding more and more modules of code to the super loop, inter-dependencies
    between subsystems become more complex as well. In the next section, we'll discuss
    the challenges of scaling a super loop for complex systems.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling a super loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, we've now got a responsive system that is able to reliably process interrupts.
    Perhaps we've configured a DMA controller to take care of the heavy lifting for
    the communication peripherals as well. Why do we even need an RTOS? Well, it is
    entirely possible you don't! If the system is dealing with a limited number of
    responsibilities and none of them are especially complicated or time-consuming,
    then there may be no need for anything more sophisticated than a super loop.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the system is also responsible for generating a **User Interface**
    (**UI**), running complex time-consuming algorithms, or dealing with complex communication
    stacks, it is very likely that these tasks will take a non-trivial amount of time.
    If a glitzy eye-catching UI with lots of animation starts to stutter a little
    bit because the MCU is dealing with collecting data from a critical sensor, that
    is no big deal. Either the animation can be dialed back or eliminated and the
    important part of the real-time system is left intact. But what happens if that
    animation still looks perfectly good, even though there was some missed data from
    the sensor?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are all sorts of different ways in which this problem plays out every
    day in our industry. Sometimes, if the system was designed well enough, the missing
    data will be detected and flagged (but it can''t be recovered: it is gone forever).
    If the design team is really lucky, it may even have failed in this way during
    in-house testing. However, in many cases, the missed sensor data will go completely
    unnoticed until somebody notices one of the readings seems to be a little bit
    off ... sometimes. If everyone is lucky, the bug report for the sketchy reading
    might include a hint that it only seems to happen when someone is at the front
    panel (playing with those fancy animations). This would at least give the poor
    firmware engineer assigned to debug the issue a hint – but we''re often not even
    that lucky.'
  prefs: []
  type: TYPE_NORMAL
- en: These are the types of systems where an RTOS is needed. Guaranteeing that the
    most time-critical tasks are always running when necessary and scheduling lower
    priority tasks to run whenever spare time is available is a strong point of preemptive
    schedulers. In this type of setup, the critical sensor readings could be pushed
    into their own task and assigned a high priority – effectively interrupting anything
    else in the system (except ISRs) when it was time to deal with the sensor. That
    complex communication stack could be assigned a lower priority than the critical
    sensor. Finally, the glitzy UI with the fancy animations gets the left-over processor
    cycles. It is free to perform as many sliding alpha-blending animations as it
    wants, but only when the processor doesn't have anything else better to do.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing RTOS tasks to super loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve only mentioned tasks very casually, but what is a task, really?
    An easy way to think about a task is that it is *just another main loop*. In a
    preemptive RTOS, there are two main differences between tasks and super loops:'
  prefs: []
  type: TYPE_NORMAL
- en: Each task receives its own private stack. Unlike a super loop in main, which
    was sharing the system stack, tasks receive their own stack that no other task
    in the system will use. This allows each task to have its own call stack without
    interfering with other tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each task has a priority assigned to it. This priority allows the scheduler
    to make decisions on which task should be running (the goal is to make sure the
    highest priority task in the system is always doing useful work).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given these two features, each task may be programmed as if it is the only
    thing the processor has to do. Do you have a single flag you''d like to watch
    AND some calculations for flashy animations to churn through? No problem: simply
    program the task and assign it a reasonable priority, relative to the rest of
    the system''s functionality. The preemptive scheduler will always ensure that
    the most important task is executing when it has work to do. When a higher priority
    task no longer has useful work to perform and it is waiting on something else
    in the system, a lower priority task will be switched into context and allowed
    to run.'
  prefs: []
  type: TYPE_NORMAL
- en: The FreeRTOS scheduler will be discussed in more detail in [Chapter 7](2fa909fe-91a6-48c1-8802-8aa767100b8f.xhtml),
    *The FreeRTOS Scheduler*.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving parallel operations with RTOS tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Earlier, we had looked at a super loop that was looping through three functions.
    Now, for a very simple example, let''s move each one of the three functions into
    its own task. We''ll use these three simple tasks to examine the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Theoretical task programming model**: How the three tasks can be described theoretically'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actual round-robin scheduling**: What the tasks look like when executed using
    a round-robin scheduling algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actual preemptive scheduling**: What the tasks look like when executed using
    preemptive scheduling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In real-world programs, there is almost never a single function per task; we're
    only using this as an analog to the overly simplistic super loop from earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical task programming model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s some pseudo-code that uses a super loop to execute three functions.
    The same three functions are also included in a task-based system – each RTOS
    task (on the right) contains the same functionality as the functions from the
    super loop on the left. This will be used moving forward as we discuss the differences
    in how the code is executed when using a super loop versus using a task-driven
    approach with a scheduler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad3dfae0-42ea-4a9f-ad2c-2ca7d38fc3f7.png)'
  prefs: []
  type: TYPE_IMG
- en: One of the immediate differences you might notice between the super loop implementation
    and the RTOS implementation is the number of infinite `while` loops. There is
    only a single infinite `while` loop (in `main()`) for the super loop implementation,
    but each task has its own infinite `while` loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the super loop, the three functions being executed by a super loop are each
    run to completion before the next function is called, and then the cycle continues
    onto the next iteration (illustrated by the following diagram):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bb60f4a-fad2-4597-abae-6ca87409c6d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the RTOS implementation, each task is essentially its own little infinite
    `while` loop. Whereas functions in the super loop were always sequentially called
    one after the other (orchestrated by the logic in the super loop), tasks can simply
    be thought of as all executing in parallel after the scheduler has been started.
    Here''s a diagram of an RTOS executing three tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a6161d5-f95f-4367-999f-6aabbcb9e90a.png)'
  prefs: []
  type: TYPE_IMG
- en: In the diagram, you'll notice that the size of each `while` loop is not the
    same. This is one of the many benefits of using a scheduler that is executing
    the tasks in *parallel* versus a super loop – the programmer doesn't need to be
    immediately concerned with the length of the longest executing loop slowing down
    the other tighter loops. The diagram depicts `Task 2` having a much longer loop
    than `Task 1`. In a super loop system, this would cause the functionality in `func1`
    to execute less frequently (since the super loop would need to execute `func1`,
    then `func2`, and then `func3`). In a task-based programming model, this isn't
    the case – the loop of each task can be thought of as being isolated from the
    other tasks in the system – and they all run in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: This isolation and perceived parallel execution are some of the benefits of
    using an RTOS; it alleviates some of the complexity for the programmer. So – that's
    the easiest way of conceptualizing tasks – they're simply independent infinite
    `while` loops that all execute in parallel . . . in theory. In reality, things
    aren't quite this simple. In the next two sections, we'll take a glimpse into
    what goes on behind the scenes to make it *seem* like tasks are executing in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Round-robin scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the easiest ways to conceptualize actual task execution is with round-robin
    scheduling. In round-robin scheduling, each task gets a small slice of time to
    use the processor, which is controlled by the scheduler. As long as the task has
    work to perform, it will execute. As far as the task is concerned, it has the
    processor entirely to itself. The scheduler takes care of all of the complexity
    of switching in the appropriate context for the next task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf10b772-0a6c-4bef-b257-43d452369a0a.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the same three tasks that were shown previously, except that instead
    of a theoretical conceptualization, each iteration through the tasks' loops are
    enumerated over time. Because the round-robin scheduler assigns equal time slices
    to each task, the shortest task (`Task 1`) has executed nearly six iterations
    of its loop, whereas the task with the slowest loop (`Task 2`) has only made it
    through the first iteration. `Task 3` has executed three iterations of its loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'An extremely important distinction between a super loop executing the same
    functions versus a round-robin scheduling routine executing them is this: `Task
    3` completed its moderately tight loop before `Task 2`. When the super loop was
    running functions in a serial fashion, `Function 3` wouldn''t even have started
    until `Function 2` had run to completion. So, while the scheduler isn''t providing
    us with true parallelism, each task is getting it''s *fair share* of CPU cycles.
    So, with this scheduling scheme, if a task has a shorter loop, it will execute
    more often than a task with a longer loop.'
  prefs: []
  type: TYPE_NORMAL
- en: All of this switching does come at a (slight) cost – the scheduler needs to
    be invoked any time there is a context switch. In this example, the tasks are
    not explicitly calling the scheduler to run. In the case of FreeRTOS running on
    an ARM Cortex-M, the scheduler will be called from the SysTick interrupt (more
    details can be found in [Chapter 7](2fa909fe-91a6-48c1-8802-8aa767100b8f.xhtml)*, The
    FreeRTOS Scheduler*). A considerable amount of effort is put into making sure
    the scheduler kernel is extremely efficient and takes as little time to run as
    possible. However, the fact remains that it will run at some point and consume
    CPU cycles. On most systems, the small amount of overhead is generally not noticed
    (or significant), but it can become an issue in some systems. For example, if
    a design is on the extreme edge of feasibility because it has extremely tight
    timing requirements and very few spare CPU cycles, the added overhead may not
    be desirable (or completely necessary) if the super loop/interrupt approach has
    been carefully characterized and optimized. However, it is best to avoid this
    type of situation wherever possible, since the likelihood of overlooking a combination
    of interrupt stack-up (or nested conditionals taking longer *every once in a while) *and
    causing the system to miss a deadline is extremely high on even moderately complex
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Preemptive-based scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preemptive scheduling provides a mechanism for ensuring that the system is always
    performing its most important task. A preemptive scheduling algorithm will give
    priority to the most important task, regardless of what else in the system is
    happening – except for interrupts, since they occur *underneath* the scheduler
    and always have a higher priority. This sounds very straightforward – and it is
    – except that there are some details that need to be taken into consideration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the same three tasks. These three tasks all have the
    same functionality: a simple `while` loop that endlessly increments a volatile
    variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, consider the following three scenarios to figure out which of the three
    tasks will get context. The following diagram has the same tasks as previously
    presented with round-robin scheduling. Each of the three tasks has more than enough
    work to do, which will prevent the task from going out of context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9f432c6-63c5-4991-8e1b-b43ebe54be13.png)'
  prefs: []
  type: TYPE_IMG
- en: So, what happens when three different tasks are set up with three different
    sets of priorities (A, B, and C)?
  prefs: []
  type: TYPE_NORMAL
- en: '**A (top left)**: `Task 1` has the highest priority in the system – it gets
    *all* of the processor time! Regardless of how many iterations `Task 1` performs,
    if it is the highest priority task in the system and it has work to do (without
    waiting on anything else in the system), it will be given context and run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**B (top right)**: `Task 2` is the highest priority task in the system. Since
    it has more than enough work to do, not needing to wait on anything else in the
    system, `Task 2` will be given context. Since `Task 2` is configured as the highest
    priority in the system, it will execute until it needs to wait on something else
    in the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**C (bottom left)**: `Task 3` is configured as the highest priority task in
    the system. No other tasks run because they are lower priority.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, obviously, if you were actually designing a system that required multiple
    tasks to run in parallel, a preemptive scheduler wouldn't be much use if all of
    the tasks in the system required 100% CPU time and didn't need to wait on anything.
    This setup also wouldn't be a great design for a real-time system since it was
    completely overloaded (and ignoring two of the three primary functions the system
    was meant to perform)! The situation presented is referred to as **task starvation**,
    since only the highest priority task in the system is getting CPU time and the
    other tasks are being *starved* of processor time.
  prefs: []
  type: TYPE_NORMAL
- en: Another detail worth pointing out is that the scheduler is still running at
    predetermined intervals. No matter what is going on in the system, the scheduler
    will diligently run at its predetermined tick rate.
  prefs: []
  type: TYPE_NORMAL
- en: There is an exception to this. FreeRTOS has a *tick-less* scheduler mode designed
    for use in extremely low power devices, which prevents the scheduler from running
    on the same predetermined intervals.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more realistic use case where a preemptive scheduler is used is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be80720d-75c8-4919-81b4-ffa20e63c7d6.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, `Task 1` is the highest priority task in the system (it also happens
    to finish executing very quickly) – the only time `Task 1` has context taken from
    it is when the scheduler needs to run; otherwise, it will keep context until it
    doesn't have any additional work to perform.
  prefs: []
  type: TYPE_NORMAL
- en: '`Task 2` is the next highest priority – you''ll also notice that this task
    is set up to execute once per RTOS scheduler tick (indicated by the downward arrows).
    `Task 3` is the lowest priority task in the system: it only gets context when
    there is nothing else worth doing in the system. There are three main points worth
    looking at in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A**: `Task 2` has context. Even though it is interrupted by the scheduler,
    it immediately gets context again after the scheduler has run (because it still
    has work to perform).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**B**: `Task 2` has finished its work for iteration 0\. The scheduler has run
    and determined that (since no other tasks in the system are required to run) `Task
    3` could have processor time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**C**: `Task 2` has started running iteration 4, but `Task 1` now has some
    work to do – even though `Task 2` hasn''t finished the work for that iteration.
    `Task 1` is immediately switched in by the scheduler to perform its higher priority
    work. After `Task 1` is finished with what it needs to do, `Task 2` is switched
    back in to finish iteration 4\. This time, the iteration runs until the next tick
    and `Task 2` runs again (iteration 5). After `Task 2` iteration 5 has completed,
    there is no higher priority work to perform, so the lowest priority task in the
    system (`Task 3`) runs again. It looks as if `Task 3` has finally completed iteration
    0, so it moves on to iteration 1 and chugs along . . .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopefully you're still with me! If not, that's OK, given that this is a very
    abstract example. The key takeaway is that the highest priority task in the system
    takes precedence.
  prefs: []
  type: TYPE_NORMAL
- en: This is only a brief introduction to the relevant scheduling concepts covered
    in detail in [Chapter 7](2fa909fe-91a6-48c1-8802-8aa767100b8f.xhtml),* The FreeRTOS
    Scheduler*, to put the concept of tasks into context, showing the different ways
    in which they can be run and scheduled. Many more details and strategies for achieving
    desired system performance are discussed there, along with real-world examples.
  prefs: []
  type: TYPE_NORMAL
- en: RTOS tasks versus super loops – pros and cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Super loops are great for simple systems with limited responsibilities. If a
    system is simple enough, they can provide very low jitter in response to an event,
    but only if the loop is tight enough. As a system grows more complex and acquires
    more responsibility, polling rates decrease. This decreased polling rate causes
    much larger jitter in response to events. Interrupts can be introduced into the
    system to combat the increased jitter. As a super loop-based system becomes more
    complex, it becomes harder to track and guarantee responsiveness to events.
  prefs: []
  type: TYPE_NORMAL
- en: An RTOS becomes very valuable with more complex systems that have not only time-consuming
    tasks, but also require good responsiveness to external events. With an RTOS,
    an increase in system complexity, ROM, RAM, and initial setup time is the trade-off
    for a more easily understood system, which can more easily guarantee responsiveness
    to external events in a timely manner.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've covered quite a few concepts in this chapter in relation to super loops
    and tasks. At this point, you should have a good understanding of how super loops
    can be combined with interrupts and DMA to provide parallel processing to keep
    a system responsive, without the use of an RTOS. We introduced task-based architectures
    at a theoretical level and the two main types of scheduling you'll encounter when
    using FreeRTOS (round-robin and preemptive). You also had a very brief glimpse
    at how a preemptive scheduler schedules tasks of different priorities. All of
    these concepts are important to grasp, so feel free to refer back to these simplistic
    examples as we move forward and discuss more advanced topics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you'll be introduced to the various inter-task communication
    mechanisms that will cause context switches like the ones covered in this chapter.
    As we progress through the book and move onto interrupt and task communication
    mechanisms, many real-world examples will be discussed and we'll take a deep dive
    into the code that you'll need to write in order to create reliable real-time
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter''s material. You will find the answers in the *Assessments*
    section of the Appendix:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a super loop?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An infinite `while` loop
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A loop that oversees all function calls in an embedded system
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both of the preceding options
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RTOS tasks should *always *be preferred over super loops.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Name a drawback to complex super loops.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can the responsiveness of a super loop-based application be improved?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List two ways in which super loops differ from RTOS tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What features do RTOS tasks possess to help ensure that the most time-critical
    task gets CPU time before less time-critical tasks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Time slicing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Prioritization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Round-robin scheduling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What type of scheduler attempts to execute the most critical tasks before less
    critical tasks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If interrupts and DMA are new to you, here are two resources that describe
    their use (relative to MCUs) fairly well:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For interrupts: [https://www.renesas.com/eu/en/support/technical-resources/engineer-school/mcu-programming-peripherals-04-interrupts.html](https://www.renesas.com/eu/en/support/technical-resources/engineer-school/mcu-programming-peripherals-04-interrupts.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'STM application note AN4031 – Using DMA on the STM32F7: [https://www.st.com/content/ccc/resource/technical/document/application_note/27/46/7c/ea/2d/91/40/a9/DM00046011.pdf/files/DM00046011.pdf/jcr:content/translations/en.DM00046011.pdf](https://www.st.com/content/ccc/resource/technical/document/application_note/27/46/7c/ea/2d/91/40/a9/DM00046011.pdf/files/DM00046011.pdf/jcr:content/translations/en.DM00046011.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
