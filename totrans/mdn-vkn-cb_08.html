<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-273"><a id="_idTextAnchor310"/>8</h1>
<h1 id="_idParaDest-274"><a id="_idTextAnchor311"/>Extended Reality with OpenXR</h1>
<p>Similar to what Vulkan is for graphics, OpenXR, an integral part of the world of <strong class="bold">Extended Reality</strong> (<strong class="bold">XR</strong>), is an API that serves as a powerful tool for implementing XR applications. This <a id="_idIndexMarker570"/>chapter provides an overview of OpenXR and how to use it in conjunction with Vulkan. We start with a basic introduction to OpenXR, explaining its role and significance in XR applications, and follow with recipes that may be <a id="_idIndexMarker571"/>used to improve your XR applications, such as <strong class="bold">single pass multiview</strong> rendering, a technique that optimizes the rendering of stereo scenes. The chapter further expands into the realm of foveated rendering, a method that significantly <a id="_idIndexMarker572"/>bolsters <strong class="bold">Frames Per Second</strong> (<strong class="bold">FPS</strong>) by rendering different sections of the screen at diverse resolutions. We delve into the implementation <a id="_idIndexMarker573"/>of this technique using the <strong class="bold">fragment shading rate</strong> feature of the Vulkan extension, providing you with a practical understanding of <a id="_idIndexMarker574"/>its application. Lastly, we delve into the use of <strong class="bold">half floats</strong>, a practical aid in conserving memory space on <strong class="bold">Head-Mounted Displays</strong> (<strong class="bold">HMDs</strong>). By the <a id="_idIndexMarker575"/>end of this chapter, you will have gained an understanding of these concepts and will be equipped with the skills to apply them effectively in your XR projects.</p>
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Getting started with OpenXR</li>
<li>How to implement single pass multiview rendering</li>
<li>Implementing static foveated rendering with a fragment density map</li>
<li>Retrieving eye gaze information from OpenXR in your app</li>
<li>Implementing dynamic foveated rendering using Qualcomm’s fragment density map Offset extension</li>
<li>Using half floats to reduce memory load</li>
</ul>
<h1 id="_idParaDest-275"><a id="_idTextAnchor312"/>Technical requirements</h1>
<p>For this chapter, you will need to install Android Studio and will also need a Meta Quest 2 or Meta Quest <a id="_idIndexMarker576"/>Pro to run the <strong class="bold">Virtual Reality</strong> (<strong class="bold">VR</strong>) sample application provided in the repository. Please follow these steps to install the tools needed to build, install, and run an application on the device:</p>
<ul>
<li>Download and install the Android Studio Hedgehog version from <a href="https://developer.android.com/studio/releases">https://developer.android.com/studio/releases</a>.</li>
<li>We also recommend installing the Meta Quest developer hub from <a href="https://developer.oculus.com/downloads/package/oculus-developer-hub-win">https://developer.oculus.com/downloads/package/oculus-developer-hub-win</a>. This tool provides several features that help the development of XR applications.</li>
<li>Please follow the steps outlined in the following link to make sure that your device is developer-ready—that is, you can debug, deploy, and test VR apps: <a href="https://developer.oculus.com/documentation/native/android/mobile-device-setup/">https://developer.oculus.com/documentation/native/android/mobile-device-setup/</a>.</li>
</ul>
<p>To launch the project, simply launch Android Studio and open this chapter's <code>project</code> folder located in <code>source/chapter8</code> directory.</p>
<h1 id="_idParaDest-276"><a id="_idTextAnchor313"/>Getting started with OpenXR</h1>
<p>Before we dive <a id="_idIndexMarker577"/>into how our application code is structured, let’s talk about some important OpenXR concepts:</p>
<ul>
<li><code>XrInstance</code>: This is <a id="_idIndexMarker578"/>the starting point for an OpenXR application. It represents the application’s connection to an OpenXR runtime. It is the first object you create and the last thing you destroy.</li>
<li><code>XrSystemId</code>: After <a id="_idIndexMarker579"/>creating an instance, the application queries for a system ID, which represents a specific device or group of devices, such as a VR headset.</li>
<li><code>XrViewConfigurationType</code>: This is used to select a view configuration that the <a id="_idIndexMarker580"/>application will use to display images. Different configurations can represent different display setups, such as monoscopic, stereoscopic, and so on.</li>
<li><code>XrSession</code>: Once the instance is set up and the system ID and view configuration <a id="_idIndexMarker581"/>are determined, a session is created. A session represents the application’s interaction with a device. The session manages the life cycle, rendering parameters, and input data for the device.</li>
<li><code>XrSpace</code>: Spaces <a id="_idIndexMarker582"/>represent coordinate systems within the XR environment. They are used to position objects in 3D space.</li>
<li><code>XrSwapchain</code>: A swapchain is <a id="_idIndexMarker583"/>a collection of textures used to buffer images for display. After the session has been established, the swapchain is created to handle the rendering.</li>
<li><code>xrBeginFrame</code> and <code>xrEndFrame</code>: These are functions used to start and end the <a id="_idIndexMarker584"/>rendering of a frame. The <code>xrBeginFrame</code> function <a id="_idIndexMarker585"/>signals the start of a rendering frame, and <code>xrEndFrame</code> signals the end of a frame. They are called for each frame in the render loop.<p class="list-inset"><em class="italic">Figure 8</em><em class="italic">.1</em> depicts the basic idea about how to use OpenXR:</p></li>
</ul>
<div><div><img alt="Figure 8.1 – OpenXR object interaction diagram" src="img/B18491_08_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – OpenXR object interaction diagram</p>
<p>In this recipe, we will learn about the main OpenXR initialization events, and which functions we need to use to render a frame and display them on a device. The recipe will also cover how the OpenXR code is handled in the repository.</p>
<h2 id="_idParaDest-277"><a id="_idTextAnchor314"/>Getting ready</h2>
<p>The first step in creating an OpenXR application involves setting up an <code>XrInstance</code>. This instance <a id="_idIndexMarker586"/>is the primary connection between your application and the OpenXR runtime. To create an <code>XrInstance</code>, you’ll need to call the <code>xrCreateInstance</code> function. Before you do this, you will need to decide which extensions your application requires. At the very least, your application will need to enable a graphics binding extension, which specifies the graphics API that will be used. You can also use <code>xrEnumerateInstanceExtensionProperties</code> to enumerate all extensions supported by the platform. Additionally, before calling <code>xrCreateInstance</code>, you will need to populate the <code>XrApplicationInfo</code> structure. This structure holds essential details about your application, such as the application’s name, engine name, and version information.</p>
<p>After these details are set, you can call <code>xrCreateInstance</code>, which will return an instance handle upon successful creation. Following the creation of the <code>XrInstance</code>, the next step involves querying for a <code>SystemId</code> and selecting an <code>XrViewConfigurationView</code>. The <code>SystemId</code> represents a specific XR device or a group of devices, such as a VR headset, and it can be retrieved using the <code>xrGetSystem</code> function. <code>XrViewConfigurationView</code>, on the other hand, allows you to choose the view configuration that your application will use for displaying images. This could range from monoscopic to stereoscopic configurations, depending on your device type. In the recipes in this chapter, we will be using the stereo view by specifying <code>XR_VIEW_CONFIGURATION_TYPE_PRIMARY_STEREO</code>.</p>
<p>The next step <a id="_idIndexMarker587"/>is to create a instance of <code>XrSession</code>. A <code>XrSession</code> represents your application’s active interaction with the XR device. It handles the rendering parameters, input data, and overall life cycle of the application’s interaction with the device. To create an <code>XrSession</code>, we will need to fill the graphics binding information in <code>XrSessionCreateInfo</code>. Since we are using Vulkan, we will specify graphics binding using the <code>XrGraphicsBindingVulkanKHR</code> structure.</p>
<p>Tracking spatial relationships is very important in XR platforms. An instance of <code>XrSpace</code> class represent something that is being tracked by the XR system. To interact with tracked objects, we will use <code>XrSpace</code> handles. Several spaces are known as reference spaces, which can be accessed using sessions and enumerations. There are three types of reference spaces in OpenXR:</p>
<ul>
<li><code>XR_REFERENCE_SPACE_TYPE_LOCAL</code>: Seated or static space</li>
<li><code>XR_REFERENCE_SPACE_TYPE_VIEW</code>: Head locked space</li>
<li><code>XR_REFERENCE_SPACE_TYPE_STAGE</code>: An area bounded by an environment in which the user can move around</li>
</ul>
<p>To get an <code>XrSpace</code> from these enumerations, you will use <code>xrCreateReferenceSpace</code>. Another kind of space you can create is <code>xrCreateActionSpace</code>, which is used when you need to create a space from a pose action. For instance, we use it to create an <code>XrSpace</code> for gaze location and orientation. <code>xrLocateSpace</code> is an API which is used to determine transform relative to other spaces.</p>
<p>To render graphics, we will need to create a swapchain, just like in Vulkan. To create one, you need to call <code>xrCreateSwapchain</code>. Next, we will use <code>xrEnumerateSwapchainImages</code> to acquire multiple <code>XrSwapchainImageVulkanKHR</code> instances that hold a reference to <code>vkImage</code>.</p>
<p>In OpenXR, a key concept is that of layers. Imagine layers as distinct sections or elements of the final rendered scene in a virtual or augmented reality experience. Rather than presenting a flat, single-image view, OpenXR creates a multi-dimensional perspective by <a id="_idIndexMarker588"/>independently rendering each layer and then compositing them to form the final image. The most frequently used layer is <code>XrCompositionLayerProjection</code>. This layer is responsible for rendering the main scene. To create a sense of depth and immersion typical of VR experiences, this layer incorporates multiple views—one for each eye in a VR headset. This arrangement produces a stereoscopic 3D effect. But <code>XrCompositionLayerProjection</code> isn’t the only layer at work. OpenXR also employs layers such as <code>XrCompositionLayerQuad</code>, <code>XrCompositionLayerCubeKHR</code>, and <code>XrCompositionLayerEquirectKHR</code>. Each of these plays a unique role in enhancing the rendering of the final image.</p>
<p>Now we will move to the render loop; the application render loop consists of three main functions:</p>
<ol>
<li><code>xrWaitFrame</code> blocks until the OpenXR runtime determines that it’s the right time to <a id="_idIndexMarker589"/>start the next frame. This includes computations and rendering based on the user’s head pose.</li>
<li><code>xrBeginFrame</code> is called <a id="_idIndexMarker590"/>by the application to mark the start of rendering for the given frame.</li>
<li><code>xrEndFrame</code> submits <a id="_idIndexMarker591"/>the frame for display.</li>
</ol>
<p>The next part is acquiring and releasing swapchain images: <code>xrAcquireSwapchainImage</code> gives the index of the current swapchain image but it doesn’t give you permission to write to the image. To write to the swapchain image, you will need to call <code>xrWaitSwapchainImage</code>. <code>xrReleaseSwapchainImage</code> is called just before <code>xrEndFrame</code>, before the rendering is done. <code>xrEndFrame</code> will use the most recently released swapchain image for displaying to the device.</p>
<p>The last important call is <code>xrPollEvents</code>, which is used to retrieve events from the event queue. Events in OpenXR represent various types of occurrences, such as changes in session state, input from the user, or changes in the environment. For instance, an event might be generated when the user puts on or takes off their headset, when they press a button on a controller, or when the tracking system loses or regains sight of a tracked object. It’s usually called once a frame.</p>
<p>In the repository, the code for OpenXR is encapsulated in the <code>OXR::Context</code> and <code>OXR::OXRSwapchain</code> classes.</p>
<h2 id="_idParaDest-278"><a id="_idTextAnchor315"/>How to do it…</h2>
<p>The <code>OXR::Context</code> class in the repository manages most of OpenXR calls and states. In this <a id="_idIndexMarker592"/>recipe, we will show you the details of these functions and how to use them to initialize the OpenXR sample app in the repository:</p>
<ol>
<li>The <code>OXR::Context::initializeExtensions</code> method finds the extensions available in the OpenXR runtime and filters out the requested extensions that aren’t supported. Once the available extensions are fetched, it iterates through the requested extensions, eliminating any that aren’t available. That results in a list of extensions that are both requested and supported:<pre class="source-code">
void Context::initializeExtensions() {
   uint32_t numExtensions = 0;
  xrEnumerateInstanceExtensionProperties(
    nullptr, 0, &amp;numExtensions, nullptr);
  availableExtensions_.resize(
    numExtensions,
    {XR_TYPE_EXTENSION_PROPERTIES});
  xrEnumerateInstanceExtensionProperties(
    nullptr, numExtensions, &amp;numExtensions,
    availableExtensions_.data());
  requestedExtensions_.erase(
    std::remove_if(
      requestedExtensions_.begin(),
      requestedExtensions_.end(),
      [this](const char *ext) {
        return std::none_of(
          availableExtensions_.begin(),
          availableExtensions_.end(),
          [ext](
            const XrExtensionProperties &amp;props) {
            return strcmp(props.extensionName,
                          ext) == 0;
          });
      }),
    requestedExtensions_.end());
}</pre></li> <li>The <code>Context::createInstance()</code> method is responsible for creating an OpenXR instance <a id="_idIndexMarker593"/>with basic application information and the extension details:<pre class="source-code">
bool Context::createInstance() {
  const XrApplicationInfo appInfo = {
    .applicationName = "OpenXR Example",
    .applicationVersion = 0,
    .engineName = "OpenXR Example",
    .engineVersion = 0,
    .apiVersion = XR_CURRENT_API_VERSION,
  };
  const XrInstanceCreateInfo instanceCreateInfo =
    {
      .type = XR_TYPE_INSTANCE_CREATE_INFO,
      .createFlags = 0,
      .applicationInfo = appInfo,
      .enabledApiLayerCount = 0,
      .enabledApiLayerNames = nullptr,
      .enabledExtensionCount =
        static_cast&lt;uint32_t&gt;(
          requestedExtensions_.size()),
      .enabledExtensionNames =
        requestedExtensions_.data(),
    };
  XR_CHECK(xrCreateInstance(&amp;instanceCreateInfo,
                            &amp;instance_));
  XR_CHECK(xrGetInstanceProperties(
    instance_, &amp;instanceProps_));
}</pre></li> <li>The <code>Context::systemInfo</code> method retrieves and stores the properties of the OpenXR system for a head-mounted display. It fetches the system ID and its properties, including <a id="_idIndexMarker594"/>system name, vendor ID, graphics properties, tracking properties, and eye gaze support:<pre class="source-code">
void Context::systemInfo() {
  const XrSystemGetInfo systemGetInfo = {
    .type = XR_TYPE_SYSTEM_GET_INFO,
    .formFactor =
      XR_FORM_FACTOR_HEAD_MOUNTED_DISPLAY,
  };
  XR_CHECK(xrGetSystem(instance_, &amp;systemGetInfo,
                       &amp;systemId_));
  XR_CHECK(xrGetSystemProperties(
    instance_, systemId_, &amp;systemProps_));
}</pre></li> <li>The <code>Context::enumerateViewConfigurations</code> function enumerates all the view configurations supported by the system and then selects and stores properties of the one that matches the predefined supported configuration. If the selected configuration supports the required number of viewports, it stores the configuration properties and the view configuration views.</li>
<li>The <code>Context::initGraphics</code> function is designed to initialize the graphics requirements for Vulkan. It achieves this by obtaining key components such as the Vulkan instance and device extensions. <code>xrGetVulkanInstanceExtensionsKHR</code> and <code>xrGetVulkanDeviceExtensionsKHR</code> are functions used in the OpenXR API to retrieve the names of Vulkan instance and device extensions, respectively, that are needed by a particular OpenXR runtime:<pre class="source-code">
void Context::initGraphics() {
  uint32_t bufferSize = 0;
  pfnGetVulkanInstanceExtensionsKHR(
    instance_, systemId_, 0, &amp;bufferSize, NULL);
  requiredVkInstanceExtensionsBuffer_.resize(
    bufferSize);
  pfnGetVulkanInstanceExtensionsKHR(
    instance_, systemId_, bufferSize, &amp;bufferSize,
    requiredVkInstanceExtensionsBuffer_.data());
  pfnGetVulkanDeviceExtensionsKHR(
    instance_, systemId_, 0, &amp;bufferSize, NULL);
  requiredVkDeviceExtensionsBuffer_.resize(
    bufferSize);
  pfnGetVulkanDeviceExtensionsKHR(
    instance_, systemId_, bufferSize, &amp;bufferSize,
    requiredVkDeviceExtensionsBuffer_.data());
}</pre></li> <li>The <code>Context::initializeSession</code> function creates a new OpenXR session. It begins by creating an <code>XrGraphicsBindingVulkanKHR</code> object, which is used to bind Vulkan to the XR session. This object is populated with the Vulkan instance, physical device, and device, as well as the queue family index. This information allows the OpenXR runtime to interface with the Vulkan API. Then, an <code>XrSessionCreateInfo</code> object is created, which is used to specify the parameters for creating a new session. This object attributes are populated with the <a id="_idIndexMarker595"/>nature of the session to be created, the graphics binding, and the system ID. Finally, the <code>xrCreateSession</code> function is called to create the session:<pre class="source-code">
bool Context::initializeSession(
  VkInstance vkInstance,
  VkPhysicalDevice vkPhysDevice,
  VkDevice vkDevice, uint32_t queueFamilyIndex) {
  // Bind Vulkan to XR session
  const XrGraphicsBindingVulkanKHR
    graphicsBinding = {
      XR_TYPE_GRAPHICS_BINDING_VULKAN_KHR,
      NULL,
      vkInstance,
      vkPhysDevice,
      vkDevice,
      queueFamilyIndex,
      0,
    };
  const XrSessionCreateInfo sessionCreateInfo = {
    .type = XR_TYPE_SESSION_CREATE_INFO,
    .next = &amp;graphicsBinding,
    .createFlags = 0,
    .systemId = systemId_,
  };
  XR_CHECK(xrCreateSession(
    instance_, &amp;sessionCreateInfo, &amp;session_));
  return true;
}</pre></li> <li>The <code>Context::enumerateReferenceSpaces</code> function retrieves the types of <a id="_idIndexMarker596"/>reference spaces available for the current OpenXR session. It calls <code>xrEnumerateReferenceSpaces</code> to fill a vector of <code>XrReferenceSpaceType</code> structures with the available reference space types. Finally, it checks whether the <code>XR_REFERENCE_SPACE_TYPE_STAGE</code> type is available and stores this information in the <code>stageSpaceSupported_</code> variable. The <code>XR_REFERENCE_SPACE_TYPE_STAGE</code> type represents a standing-scale experience where the user has a small amount of room to move around.</li>
<li>The <code>Context::createSwapchains</code> function is responsible for creating the swapchains needed for rendering. Based on the value of <code>useSinglePassStereo_</code>, it either creates a single swapchain that will be used for both views (in case of single-pass stereo rendering), or separate swapchains for each view. For each swapchain, it creates a new <code>OXRSwapchain</code> instance. The <code>OXRSwapchain</code> constructor is called with the Vulkan context, the OpenXR session, the viewport for the swapchain, and the number of views per swapchain. We call the <code>initialize</code> function to initialize the <code>OXRSwapchain</code> instance. The <code>initialize</code> function in the <code>OXRSwapchain</code> class sets up the color and depth swapchains for an OpenXR session by <a id="_idIndexMarker597"/>calling the <code>xrCreateSwapchain</code> function. Once <code>XrSwapchain</code> is created, we call <code>enumerateSwapchainImages</code> in <code>OXRSwapchain</code>, which is responsible for creating a vector of <code>XrSwapchainImageVulkanKHR</code>:<pre class="source-code">
void Context::createSwapchains(
  VulkanCore::Context &amp;ctx) {
  const uint32_t numSwapchainProviders =
    useSinglePassStereo_ ? 1 : kNumViews;
  const uint32_t numViewsPerSwapchain =
    useSinglePassStereo_ ? kNumViews : 1;
  swapchains_.reserve(numSwapchainProviders);
  for (uint32_t i = 0; i &lt; numSwapchainProviders;
       i++) {
    swapchains_.emplace_back(
      std::make_unique&lt;OXRSwapchain&gt;(
        ctx, session_, viewports_[i],
        numViewsPerSwapchain));
    swapchains_.back()-&gt;initialize();
  }
}</pre></li> <li><code>OXRSwapchain</code> also provides functions such as <code>getSurfaceTexture</code> and <code>releaseSwapchainImages</code>. <code>getSurfaceTexture</code> is responsible for acquiring a swapchain by calling <code>xrAcquireSwapchainImage</code> and <code>xrWaitSwapchainImage</code>.</li>
<li>Before starting to render, <code>OXR::Context::beginFrame</code> first synchronizes frame submission with the display by calling <code>xrWaitFrame</code>, which returns an <code>XrFrameState</code> structure. The frame state specifies a predicted display time when the runtime predicts a frame will be displayed. The function also calls <code>xrBeginFrame</code>, which must be called before rendering starts, and retrieves some other <a id="_idIndexMarker598"/>important information, such as the head and view poses, and calculates view and camera transformations:<pre class="source-code">
XrFrameState Context::beginFrame() {
  const XrFrameWaitInfo waitFrameInfo = {
    XR_TYPE_FRAME_WAIT_INFO};
  XrFrameState frameState = {XR_TYPE_FRAME_STATE};
  XR_CHECK(xrWaitFrame(session_, &amp;waitFrameInfo,
                       &amp;frameState));
  XrFrameBeginInfo beginFrameInfo = {
    XR_TYPE_FRAME_BEGIN_INFO};
  XR_CHECK(
    xrBeginFrame(session_, &amp;beginFrameInfo));
  XrSpaceLocation loc = {
    loc.type = XR_TYPE_SPACE_LOCATION};
  XR_CHECK(xrLocateSpace(
    headSpace_, stageSpace_,
    frameState.predictedDisplayTime, &amp;loc));
  XrPosef headPose = loc.pose;
  XrViewState viewState = {XR_TYPE_VIEW_STATE};
  const XrViewLocateInfo projectionInfo = {
    .type = XR_TYPE_VIEW_LOCATE_INFO,
    .viewConfigurationType =
      viewConfigProps_.viewConfigurationType,
    .displayTime =
      frameState.predictedDisplayTime,
    .space = headSpace_,
  };
  uint32_t numViews = views_.size();
  views_[0].type = XR_TYPE_VIEW;
  views_[1].type = XR_TYPE_VIEW;
  XR_CHECK(xrLocateViews(
    session_, &amp;projectionInfo, &amp;viewState,
    views_.size(), &amp;numViews, views_.data()));
}</pre></li> <li>Once rendering <a id="_idIndexMarker599"/>has been completed, the application must call the <code>OXR::endFrame</code> method, which in turn calls <code>xrEndFrame</code>. The <code>XrFrameEndInfo</code> structure specifies the type of layer being presented (and its flags) and its associated spaces (with its poses and field of view angles and maybe depth information) and how the image(s) should be blended with underlying layers. Note that, for the sake of conciseness, only the critical sections of the code are displayed here. For a comprehensive understanding, please refer to the full code in the original source:<pre class="source-code">
void Context::endFrame(XrFrameState frameState) {
  const XrFrameEndInfo endFrameInfo = {
    .type = XR_TYPE_FRAME_END_INFO,
    .displayTime =
      frameState.predictedDisplayTime,
    .environmentBlendMode =
      XR_ENVIRONMENT_BLEND_MODE_OPAQUE,
    .layerCount = 1,
    .layers = layers,
  };
  XR_CHECK(xrEndFrame(session_, &amp;endFrameInfo));
}</pre></li> <li>The <code>android_main</code> function, outside of the <code>OXR::Context</code> class, serves as the main <a id="_idIndexMarker600"/>entry point for a native Android activity. It initializes both OpenXR (<code>oxrContext</code>) and Vulkan (<code>vkContext</code>) contexts and sets up their required extensions and features. After creating an instance, it establishes a session and creates swapchains for rendering. Shader modules for vertex and fragment shaders are also created. The function then enters a loop where it handles OpenXR events, begins a frame, carries out rendering actions, and ends the frame. This loop continues until the app is requested to be destroyed. Please note that, for brevity, a significant amount of detail has been omitted from this summary. You are encouraged to review the actual code in the repository for a comprehensive understanding:<pre class="source-code">
void android_main(struct android_app *pApp) {
  OXR::Context oxrContext(pApp);
  oxrContext.initializeExtensions();
  oxrContext.createInstance();
  VulkanCore::Context vkContext(
    VkApplicationInfo{});
  vkContext.createVkDevice(
    oxrContext.findVkGraphicsDevice(
      vkContext.instance()),
    oxrContext.vkDeviceExtensions(),
    VK_QUEUE_GRAPHICS_BIT);
  oxrContext.initializeSession(
    vkContext.instance(),
    vkContext.physicalDevice().vkPhysicalDevice(),
    vkContext.device(),
    vkContext.physicalDevice()
      .graphicsFamilyIndex()
      .value());
  oxrContext.createSwapchains(vkContext);
  auto commandMgr =
    vkContext.createGraphicsCommandQueue(3, 3);
  do {
    auto frameState = oxrContext.beginFrame();
    if (frameState.shouldRender == XR_FALSE) {
      oxrContext.endFrame(frameState);
      continue;
    }
    auto commandBuffer =
      commandMgr.getCmdBufferToBegin();
    vkCmdDrawIndexedIndirect(
      commandBuffer, buffers[3]-&gt;vkBuffer(), 0,
      numMeshes,
      sizeof(EngineCore::
               IndirectDrawCommandAndMeshData));
    commandMgr.submit(
      &amp;vkContext.swapchain()-&gt;createSubmitInfo(
        &amp;commandBuffer,
        &amp;VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT,
        false, false));
    commandMgr.goToNextCmdBuffer();
    oxrContext.swapchain(0)
      -&gt;releaseSwapchainImages();
    oxrContext.endFrame(frameState);
  } while (!pApp-&gt;destroyRequested);
}</pre></li> </ol>
<p>This recipe involves a sequence of steps starting from initializing the OpenXR and Vulkan contexts to <a id="_idIndexMarker601"/>entering a game event loop for handling OpenXR events and rendering. The process is intricate and involves enabling specific features, handling graphics commands, and managing frames. This guide has provided a simplified overview, and we strongly recommend reviewing the full code in the repository for a complete understanding.</p>
<h2 id="_idParaDest-279"><a id="_idTextAnchor316"/>See also</h2>
<p>For more details, please refer to the OpenXR guide by Khronos:</p>
<ul>
<li><a href="https://www.khronos.org/files/openxr-10-reference-guide.pdf">https://www.khronos.org/files/openxr-10-reference-guide.pdf</a></li>
</ul>
<h1 id="_idParaDest-280"><a id="_idTextAnchor317"/>How to implement single pass multiview rendering</h1>
<p>XR devices must render scenes at least twice for each frame, generating one image for each eye. Single pass multiview rendering is a technique used to enhance the performance <a id="_idIndexMarker602"/>of XR applications by allowing the rendering of multiple views in a single pass. This effectively enables the rendering of the scene from both eye’s perspectives with one draw call.</p>
<p>In this recipe, we will navigate how to enable the Multiview rendering feature in Vulkan and how to use it to render the scene for both eyes in one render pass.</p>
<h2 id="_idParaDest-281"><a id="_idTextAnchor318"/>Getting ready</h2>
<p>In the context of Vulkan, the <code>VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_MULTIVIEW_FEATURES</code> extension specifies whether multiple views are supported in a single rendering pass. Once the feature is enabled, you can specify multiple viewports and scissor rectangles for your rendering pass. The graphics pipeline will then render the scene from different perspectives in a single pass, reducing the need for duplicate operations.</p>
<p>Besides enabling a Vulkan extension, you will also need to enable the <code>GL_EXT_multiview</code> extension in your shader code. <code>GL_EXT_multiview</code> is a GLSL extension that allows multiple views to be rendered in a single pass. <code>GL_EXT_multiview</code> introduces a new built-in variable, <code>gl_ViewIndex</code>, that can be used in your shaders to determine which view is being rendered. It contains the index of the <a id="_idIndexMarker603"/>current view being processed and can be used to adjust your drawing based on the view index (for instance, index <code>0</code> may represent the left eye, while index <code>1</code> can represent the right eye).</p>
<p>We also need the ability to query whether multiview is supported by hardware or not using <code>VkPhysicalDeviceMultiviewFeatures</code>. Additionally, we need to specify that we will be using multiple views when creating the render pass. This is done by adding an instance of the <code>VkRenderPassMultiviewCreateInfo</code> structure to the <code>pNext</code> chain of the <code>VkRenderPassCreateInfo</code> structure. One other important part is that swapchain images need to have multiple layers (in our case, two—one for each eye), and the results of the rendering go to different layers of the attachments. You may think that we could have rendered the same scene twice (one for left and one for right), but that would mean we build a command buffer that sends all the geometry and textures twice. This extension helps us send data only once and only shaders are fired twice (for each view ID). The only difference between these two executions is uniform data for the camera.</p>
<p>To support multiview, code changes are required in various areas of the code base. In this case, we needed to change <code>Texture</code>, <code>RenderPass</code>, <code>Context</code> classes, and shader files.</p>
<h2 id="_idParaDest-282"><a id="_idTextAnchor319"/>How to do it…</h2>
<p>In the following steps, we will go through details on how to implement this recipe:</p>
<ol>
<li>Extend <code>VulkanCore::Texture</code> to support <code>vkImageView</code> created using <code>VK_IMAGE_VIEW_TYPE_2D_ARRAY</code>; this is necessary if we have multiple layers in the same texture.</li>
<li>Add support for multiview in <code>VulkanCore::RenderPass</code>; this is achieved by connecting <code>VkRenderPassMultiviewCreateInfo</code> to <code>VkRenderPassCreateInfo</code>.</li>
<li>Add support to enable multiview extension in <code>VulkanCore::Context</code>; this is abstracted in a function named <code>enableMultiView</code>, which simply enables <code>VkPhysicalDeviceMultiviewFeatures</code> if it is supported by a physical device.</li>
<li>The vertex <a id="_idIndexMarker604"/>shader is now passed two <code>Context::mvp(index)</code> was introduced, so that we can query MVP for the left and right eye.</li>
<li>We also introduced a constant named <code>kUseSinglePassStereo</code> that can be used to control whether we want to use a single pass or not.</li>
</ol>
<p>Given that the code is distributed across various files, we strongly suggest delving into the repository for a comprehensive review of the implementation. Specifically, the file located at <code>source/chapter8/app/src/main/cpp/main.cpp</code> should warrant your particular attention.</p>
<h1 id="_idParaDest-283"><a id="_idTextAnchor320"/>Implementing static foveated rendering with a fragment density map</h1>
<p><strong class="bold">Foveated rendering</strong> is a <a id="_idIndexMarker606"/>cutting-edge graphics rendering technique <a id="_idIndexMarker607"/>that leverages the human eye’s natural tendency to focus on specific regions of a scene, optimizing computational resources by allocating higher detail and resolution to the <a id="_idIndexMarker608"/>central, foveal vision, and progressively reducing it toward the peripheral vision. This mimics the way the human eye perceives detail, offering a substantial performance boost in graphics rendering without sacrificing visual quality.</p>
<p>In this <a id="_idIndexMarker609"/>recipe, we will see how to implement fixed foveated rendering by using the <strong class="bold">fragment density map</strong> (<strong class="bold">FDM</strong>) extension.</p>
<h2 id="_idParaDest-284"><a id="_idTextAnchor321"/>Getting ready</h2>
<p>The FDM device extension in Vulkan (<code>VK_EXT_fragment_density</code>) enables an application to specify different levels of detail to use in different areas of the render target <a id="_idIndexMarker610"/>by means of a texture that encodes how many times a fragment shader will be invoked for <a id="_idIndexMarker611"/>that area. The FDM may be modified on each frame to accommodate the user’s eye gaze direction. This recipe only works with HMDs that provide eye gaze detection, such as Meta’s Quest Pro. The recipe presented here works for a single-pass stereo rendering approach.</p>
<h2 id="_idParaDest-285"><a id="_idTextAnchor322"/>How to do it…</h2>
<p>Before creating and using an FDM and the FDM Offset extension, we need to enable the extensions:</p>
<ol>
<li>Before enabling the feature, it is necessary to check whether the physical device supports it. Doing so requires appending an instance of the <code>VkPhysicalDeviceFragmentDensityMapFeaturesEXT</code> structure to the <code>pNext</code> chain of <code>VkPhysicalDeviceFeatures2</code> passed to the <code>vkGetPhysicalDeviceFeatures2</code> function.<p class="list-inset"><code>VkPhysicalDeviceFragmentDensityMapFeaturesEXT:: fragmentDensityMap</code> specifies whether the device supports the FDM extension.</p></li>
<li>The extension has properties that need to be queried to be used properly. To do that, also include an instance of the <code>VkPhysicalDeviceFragmentDensityMapPropertiesEXT</code> structure to the <code>pNext</code> chain of <code>VkPhysicalDeviceProperties2</code> and query those properties with <code>vkGetPhysicalDeviceProperties2</code>. We will use these properties in <em class="italic">step 4</em>.</li>
<li>The FDM extension is a device extension and its name needs to be passed in during the creation of the <code>VkDevice</code> object: <code>"VK_EXT_fragment_density_map"</code> (or the definition, <code>VK_EXT_FRAGMENT_DENSITY_MAP_EXTENSION_NAME</code>).</li>
<li>An FDM’s size doesn’t map to the framebuffers on a one-to-one ratio. One texel of the <a id="_idIndexMarker612"/>map affects an <em class="italic">area</em> of the render target. This area’s size can be queried from <code>VkPhysicalDeviceFragmentDensityMapPropertiesEXT</code>, from the <code>minFragmentDensityTexelSize</code> and <code>maxFragmentDensityTexelSize</code> properties.<p class="list-inset">In <a id="_idIndexMarker613"/>our recipe, we will create an FDM with texels that map to an area that is at least 32 x 32 of the render target, bounded by <code>minFragmentDensityTexelSize</code>:</p><pre class="source-code">
const glm::vec2 mapSize = glm::vec2(
    std::ceilf(
        oxrContext.swapchain(0)
            -&gt;viewport()
            .recommendedImageRectWidth /
        std::max(
            32u,
            vkContext.physicalDevice()
                .fragmentDensityMapProperties()
                .minFragmentDensityTexelSize.width)),
    std::ceilf(
        oxrContext.swapchain(0)
            -&gt;viewport()
            .recommendedImageRectHeight /
        std::max(
            32u,
            vkContext.physicalDevice()
                .fragmentDensityMapProperties()
                .minFragmentDensityTexelSize.height)));</pre></li> <li>An <a id="_idIndexMarker614"/>FDM is <a id="_idIndexMarker615"/>a regular texture with some special usage flags:<pre class="source-code">
std::shared_ptr&lt;VulkanCore::Texture&gt; =
    std::make_shared&lt;VulkanCore::Texture&gt;(
        vkContext, VK_IMAGE_TYPE_2D,
        VK_FORMAT_R8G8_UNORM,
        static_cast&lt;VkImageCreateFlags&gt;(0),
        VK_IMAGE_USAGE_FRAGMENT_DENSITY_MAP_BIT_EXT,
        VkExtent3D{static_cast&lt;uint32_t&gt;(mapSize.x),
                   static_cast&lt;uint32_t&gt;(mapSize.y),
                   1},
        1, 2, VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT,
        false, VK_SAMPLE_COUNT_1_BIT,
        "fragment density map", true,
        VK_IMAGE_TILING_LINEAR);</pre></li> <li>The format of the texture is <code>VK_FORMAT_R8G8_UNORM</code>. Each pixel stored in the map specifies the density of fragments to be used for that area of the render target, where <code>255</code> means the density should be the highest (or the default: one fragment per render target’s pixel; <code>128</code> for half the density, and so on). In our recipe, our map is initialized to <code>128</code> (half density) and then manipulated to have an area in the center with a radius equal to <code>2</code> texels with full density:<pre class="source-code">
std::vector&lt;uint8_t&gt; fdmData(mapSize.x *mapSize.y * 2,
                             255);
constexpr uint16_t high_res_radius = 8;
const glm::vec2 center = mapSize / 2.f;
for (uint32_t x = 0; x &lt; mapSize.x; ++x) {
  for (uint32_t y = 0; y &lt; mapSize.y; ++y) {
    const float length =
        glm::length(glm::vec2(x, y) - center);
    if (length &lt; high_res_radius) {
      const uint32_t index =
          (y * mapSize.x * 2) + x * 2;
      fdmData[index] = 255;     // full density
      fdmData[index + 1] = 255; // full density</pre><p class="list-inset">Note <a id="_idIndexMarker616"/>that the image has two layers, one for each eye. The data is uploaded to the <a id="_idIndexMarker617"/>device twice, once for each layer of the image.</p></li> <li>Once the data has been uploaded for each layer of the map, the texture’s layout needs to be transitioned to the special layout, <code>VK_IMAGE_LAYOUT_FRAGMENT_DENSITY_MAP_OPTIMAL_EXT</code>.</li>
<li>The FDM needs to be specified and referenced by the render pass in a <code>VkAttachmentDescription</code> structure, just like any other attachment used in the render pass:<pre class="source-code">
const auto fdmAttachDesc = VkAttachmentDescription{
    .format = VK_FORMAT_R8G8_UNORM,
    .samples = VK_SAMPLE_COUNT_1_BIT,
    .loadOp = VK_ATTACHMENT_LOAD_OP_DONT_CARE,
    .storeOp = VK_ATTACHMENT_STORE_OP_DONT_CARE,
    .initialLayout =
        VK_IMAGE_LAYOUT_FRAGMENT_DENSITY_MAP_OPTIMAL_EXT,
    .finalLayout =
        VK_IMAGE_LAYOUT_FRAGMENT_DENSITY_MAP_OPTIMAL_EXT,
};</pre></li> <li>The <a id="_idIndexMarker618"/>FDM must not appear as a color or depth stencil attachment in the <code>VkSubpassDescription::pColorAttachments</code> or <code>VkSubpassDescription::pDepthStencilAttachment</code> arrays. Instead, it <a id="_idIndexMarker619"/>must be referenced in an instance of the special <code>VkRenderPassFragmentDensityMapCreateInfoEXT</code> structure:<pre class="source-code">
const VkRenderPassFragmentDensityMapCreateInfoEXT
    fdmAttachmentci = {
        .sType =
            VK_STRUCTURE_TYPE_RENDER_PASS_FRAGMENT_DENSITY_MAP_CREATE_INFO_EXT,
        .fragmentDensityMapAttachment =
            {
                .attachment =
                    <strong class="bold">fragmentDensityAttachmentReference</strong>,
                .layout =
                    VK_IMAGE_LAYOUT_FRAGMENT_DENSITY_MAP_OPTIMAL_EXT,
            },
};
refers to the <em class="italic">index</em> of the <code>VkAttachmentDescription</code> structure <a id="_idIndexMarker621"/>that mentions the FDM in the attachment description array passed to <code>VkRenderPassCreateInfo::pAttachments</code>.</pre></li> </ol>
<p class="callout-heading">Bug prevention notice</p>
<p class="callout">The order in which this structure appears in the <code>VkRenderPassCreateInfo:: pAttachments</code> array must match the index of the <code>VkImage</code> array passed to <code>VkFramebufferCreateInfo::pAttachments</code>.</p>
<ol>
<li value="10">The instance of the <code>VkRenderPassFragmentDensityMapCreateInfoEXT</code> structure needs to be added to the <code>pNext</code> chain property of the <code>VkRenderPassCreateInfo</code> structure:<pre class="source-code">
const VkRenderPassCreateInfo rpci = {
    .sType = VK_STRUCTURE_TYPE_RENDER_PASS_CREATE_INFO,
    .pNext = <strong class="bold">&amp;fdmAttachmentci</strong>,
    ...
};</pre></li> <li>The image view of the FDM needs to be part of the framebuffer as well. Its image view must be added to the <code>VkFramebufferCreateInfo::pAttachments</code> array and its index into this array must match that of the <code>VkAttachmentDescription</code> structure passed to the creation of the render pass.</li>
</ol>
<p>This marks <a id="_idIndexMarker622"/>the end of our guide on static foveated rendering. In the upcoming sections, we’ll <a id="_idIndexMarker623"/>expand our exploration into the realm of dynamic foveated rendering.</p>
<h2 id="_idParaDest-286"><a id="_idTextAnchor323"/>See also</h2>
<p>For more information, check out the extension information at the following links:</p>
<ul>
<li>https://registry.khronos.org/vulkan/specs/1.3-extensions/man/html/VK_EXT_fragment_density_map.html</li>
<li>https://registry.khronos.org/vulkan/specs/1.3-extensions/man/html/VK_QCOM_fragment_density_map_offset.html.</li>
</ul>
<h1 id="_idParaDest-287"><a id="_idTextAnchor324"/>Retrieving eye gaze information from OpenXR in your app</h1>
<p>The realm of VR has evolved to the extent that some HMDs are now equipped with the capability <a id="_idIndexMarker624"/>to track the user’s eye gaze. This feature, which identifies the direction in which the user is looking, can be harnessed <a id="_idIndexMarker625"/>for a variety of tasks, enhancing the interactivity and immersion of VR experiences. In this recipe, we will guide you through the process of enabling and retrieving eye gaze data from OpenXR in your application. Additionally, we will illustrate how to calculate the focal region—the specific area the user is looking at—in pixel coordinates on the render target used for display.</p>
<h2 id="_idParaDest-288"><a id="_idTextAnchor325"/>Getting ready</h2>
<p>For this recipe, you will need an HMD that supports eye-tracking features, such as Meta’s Quest Pro. You will also need to provide permission to the app to track the user’s eye, which can be achieved through the <strong class="bold">Settings</strong> menu on most devices.</p>
<p>Also, get acquainted with how spaces and actions are supported and used in OpenXR (see the <em class="italic">Getting started with </em><em class="italic">OpenXR</em> recipe).</p>
<p>This recipe <a id="_idIndexMarker626"/>was authored and tested <a id="_idIndexMarker627"/>with Meta’s Quest Pro device, so some of the code shown here is specific to that platform. Your implementation might require small tweaks to work on your device.</p>
<h2 id="_idParaDest-289"><a id="_idTextAnchor326"/>How to do it…</h2>
<p>Adding eye gaze support requires allowing the device to track the user’s eyes. This requires executing the following steps:</p>
<ol>
<li>Before using the eye-tracking feature in your app, you need to request permission by adding the following lines to the <code>AndroidManifest.xml</code> file of your app:<pre class="source-code">
&lt;uses-permission android:name="com.oculus.permission.EYE_TRACKING" /&gt;
&lt;uses-permission android:name="oculus.software.eye_tracking" /&gt;
&lt;uses-feature android:name="oculus.software.eye_tracking"/&gt;</pre></li> <li>Grant permission to your app to track the user’s eye with the following:<pre class="source-code">
adb shell pm grant <code>com.example.openxrsample</code> and you might need to change it to your app’s name.</p></li> <li>Enable <a id="_idIndexMarker628"/>the OpenXR extension <a id="_idIndexMarker629"/>when creating your OpenXR instance by adding <code>XR_EXT_EYE_GAZE_INTERACTION_EXTENSION_NAME</code> to the <code>XrInstanceCreateInfo::enableExtensionNames</code> array:<pre class="source-code">
const XrApplicationInfo appInfo = {
    .applicationName = "OpenXR Example",
    .applicationVersion = 0,
    .engineName = "OpenXR Example",
    .engineVersion = 0,
    .apiVersion = XR_CURRENT_API_VERSION,
};
std::vector&lt;const char *&gt; requestedExtensions = {
    XR_KHR_VULKAN_ENABLE_EXTENSION_NAME,
   XR_FB_SWAPCHAIN_UPDATE_STATE_VULKAN_EXTENSION_NAME,
    <strong class="bold">XR_EXT_EYE_GAZE_INTERACTION_EXTENSION_NAME</strong>,
};
const XrInstanceCreateInfo instanceCreateInfo = {
    .type = XR_TYPE_INSTANCE_CREATE_INFO,
    .createFlags = 0,
    .applicationInfo = appInfo,
    .enabledApiLayerCount = 0,
    .enabledApiLayerNames = nullptr,
    .enabledExtensionCount = static_cast&lt;uint32_t&gt;(
        requestedExtensions_.size()),
    .enabledExtensionNames =
        requestedExtensions_.data(),
};
XR_CHECK(xrCreateInstance(&amp;instanceCreateInfo,
                          &amp;instance_));</pre></li> <li>We begin <a id="_idIndexMarker630"/>by adding a few member <a id="_idIndexMarker631"/>variables to the <code>OXR:Context</code> class:<pre class="source-code">
XrActionSet eyegazeActionSet_ = XR_NULL_HANDLE;
XrAction eyeGazeAction_ = XR_NULL_HANDLE;
XrSpace gazeActionSpace_ = XR_NULL_HANDLE;
XrSpace localReferenceSpace_ = XR_NULL_HANDLE;</pre></li> <li>Eye tracking is considered an input action in OpenXR, so we create an action set to store the eye-tracking action (<code>OXR::Context::eyegazeActionSet_</code>):<pre class="source-code">
const XrActionSetCreateInfo actionSetInfo{
    .type = XR_TYPE_ACTION_SET_CREATE_INFO,
    .actionSetName = "gameplay",
    .localizedActionSetName = "Eye Gaze Action Set",
    .priority = 0,
};
XR_CHECK(xrCreateActionSet(instance_, &amp;actionSetInfo,
                           &amp;eyegazeActionSet_));</pre></li> <li>We then create an action that represents the eye gaze input:<pre class="source-code">
const XrActionCreateInfo actionInfo{
    .type = XR_TYPE_ACTION_CREATE_INFO,
    .actionName = "user_intent",
    .actionType = XR_ACTION_TYPE_POSE_INPUT,
    .localizedActionName = "Eye Gaze Action",
};
XR_CHECK(xrCreateAction(eyegazeActionSet_, &amp;actionInfo,
                        &amp;eyegazeAction_));</pre></li> <li>We’ll <a id="_idIndexMarker632"/>need paths that identify <a id="_idIndexMarker633"/>the input action and its pose:<pre class="source-code">
XrPath eyeGazeInteractionProfilePath;
XR_CHECK(xrStringToPath(
    instance_,
    "/interaction_profiles/ext/eye_gaze_interaction",
    &amp;eyeGazeInteractionProfilePath));
XrPath gazePosePath;
XR_CHECK(xrStringToPath(
    instance_, "/user/eyes_ext/input/gaze_ext/pose",
    &amp;gazePosePath));</pre></li> <li>The action and its pose need to be bound together using an instance of the <code>XrActionSuggestedBinding</code> structure:<pre class="source-code">
const XrActionSuggestedBinding bindings{
    .action = eyegazeAction_,
    .binding = gazePosePath,
};
const XrInteractionProfileSuggestedBinding
    suggestedBindings{
        .type =
            XR_TYPE_INTERACTION_PROFILE_SUGGESTED_BINDING,
        .interactionProfile =
            eyeGazeInteractionProfilePath,
        .countSuggestedBindings = 1,
        .suggestedBindings = &amp;bindings,
    };
XR_CHECK(xrSuggestInteractionProfileBindings(
    instance_, &amp;suggestedBindings));</pre></li> <li>Actions <a id="_idIndexMarker634"/>need to be attached <a id="_idIndexMarker635"/>to a session to work, which can be done by calling <code>xrAttachSessionActionSets</code> with the action set that stores the eye gaze action:<pre class="source-code">
const XrSessionActionSetsAttachInfo attachInfo{
    .type = XR_TYPE_SESSION_ACTION_SETS_ATTACH_INFO,
    .countActionSets = 1,
    .actionSets = &amp;eyegazeActionSet_,
};
XR_CHECK(xrAttachSessionActionSets(session_,
                                   &amp;attachInfo));</pre></li> <li>We also need to create an action space for the eye gaze action to define a position and orientation of the new space’s origin within a natural reference frame of the pose action:<pre class="source-code">
const XrActionSpaceCreateInfo createActionSpaceInfo{
    .type = XR_TYPE_ACTION_SPACE_CREATE_INFO,
    .action = eyegazeAction_,
    .poseInActionSpace = poseIdentity_,
};
XR_CHECK(xrCreateActionSpace(session_,
                             &amp;createActionSpaceInfo,
                             &amp;gazeActionSpace_));</pre></li> <li>The last <a id="_idIndexMarker636"/>initialization step is to <a id="_idIndexMarker637"/>create a local reference space, which we’ll use to base the eye gaze position and orientation. The type of the reference space is <code>XR_REFERENCE_SPACE_TYPE_VIEW</code> as the eye gaze is locked to the eye or headset location and orientation. The <code>eyePoseIdentity</code> variable is initialized with the identity orientation, at a height of <code>1.8</code> meters:<pre class="source-code">
const XrPosef eyePoseIdentity = {
  .orientation = {.x = 0,
                  .y = 0,
                  .z = 0,
                  .w = 1.f},
  .position = {0, 1.8f, 0},
};
const XrReferenceSpaceCreateInfo
    createReferenceSpaceInfo{
        .type = XR_TYPE_REFERENCE_SPACE_CREATE_INFO,
        .referenceSpaceType =
            XR_REFERENCE_SPACE_TYPE_VIEW,
        .poseInReferenceSpace = <strong class="bold">eyePoseIdentity</strong>,
    };
XR_CHECK(xrCreateReferenceSpace(
    session_, &amp;createReferenceSpaceInfo,
    &amp;localReferenceSpace_));</pre></li> <li>In the <code>OXR::Context::beginFrame</code> method, we update the current state of the <a id="_idIndexMarker638"/>eye gaze action, but only if <a id="_idIndexMarker639"/>the current state of the app is <code>focused</code>. We can then get the action’s state pose with <code>xrGetActionStatePose</code>:<pre class="source-code">
if (currentState_ == XR_SESSION_STATE_FOCUSED) {
  XrActiveActionSet activeActionSet{
      .actionSet = eyegazeActionSet_,
      .subactionPath = XR_NULL_PATH,
  };
  const XrActionsSyncInfo syncInfo{
      .type = XR_TYPE_ACTIONS_SYNC_INFO,
      .countActiveActionSets = 1,
      .activeActionSets = &amp;activeActionSet,
  };
  XR_CHECK(xrSyncActions(session_, &amp;syncInfo));
  XrActionStatePose actionStatePose{
      XR_TYPE_ACTION_STATE_POSE};
  const XrActionStateGetInfo getActionStateInfo{
      .type = XR_TYPE_ACTION_STATE_GET_INFO,
      .action = eyegazeAction_,
  };
  XR_CHECK(xrGetActionStatePose(session_,
                                &amp;getActionStateInfo,
                                &amp;actionStatePose));</pre></li> <li>If <code>actionStatePose</code> is <code>active</code>, that means we can go ahead and locate the action <a id="_idIndexMarker640"/>in <code>localReferenceSpace</code> at the <a id="_idIndexMarker641"/>predicted time from the frame state queried before:<pre class="source-code">
  if (actionStatePose.isActive)
    XrEyeGazeSampleTimeEXT eyeGazeSampleTime{
        XR_TYPE_EYE_GAZE_SAMPLE_TIME_EXT};
    XrSpaceLocation gazeLocation{
        XR_TYPE_SPACE_LOCATION, &amp;eyeGazeSampleTime};
    XR_CHECK(xrLocateSpace(
        gazeActionSpace_, localReferenceSpace_,
        frameState.predictedDisplayTime,
        &amp;gazeLocation));</pre></li> <li>If both the gaze’s orientation and position are valid, we can use them to calculate where, in pixel coordinates, the user is looking at the image being presented on the device:<pre class="source-code">
    const bool orientationValid =
        gazeLocation.locationFlags &amp;
        XR_SPACE_LOCATION_ORIENTATION_VALID_BIT;
    const bool positionValid =
        gazeLocation.locationFlags &amp;
        XR_SPACE_LOCATION_POSITION_VALID_BIT;
    if (orientationValid &amp;&amp; positionValid) {
      eyeGazePositionScreen_[0] =
        screenCoordinatesFromEyeGazePose(gazeLocation,
                                         0, 0);
      eyeGazePositionScreen_[1] =
        screenCoordinatesFromEyeGazePose(gazeLocation,
                                         1, 0);
    }</pre></li> <li>Calculating <a id="_idIndexMarker642"/>the screen coordinates <a id="_idIndexMarker643"/>of the user’s gaze is simple. The following function performs all the math to convert from an <code>XrPosef</code> structure (the eye gaze location) to the coordinates on the screen. It uses the swapchain dimensions to convert the canonical view direction in OpenXR, which points to the <em class="italic">-Z</em> direction, to screen space:<pre class="source-code">
glm::vec3
Context::screenCoordinatesFromEyeGazePose(
  XrSpaceLocation gazeLocation, int eye,
  float offset) {
  XrVector3f canonicalViewDirection{0, 0, -1.f};
  // Reset the position. We won't need it
  gazeLocation.pose.position = {0, 0, 0};
  XrVector3f transformedViewDirection;
  XrPosef_TransformVector3f(
    &amp;transformedViewDirection, &amp;gazeLocation.pose,
    &amp;canonicalViewDirection);
  XrMatrix4x4f proj;
  XrMatrix4x4f_CreateProjectionFov(
    &amp;proj, GRAPHICS_OPENGL, views_[eye].fov,
    near_, far_);
  const XrVector4f tanAngle = {
    -transformedViewDirection.x /
      transformedViewDirection.z,
    -transformedViewDirection.y /
      transformedViewDirection.z,
    -1.f, 0};
  const auto width = swapchain(0)
                       -&gt;viewport()
                       .recommendedImageRectWidth;
  const auto height =
    swapchain(0)
      -&gt;viewport()
      .recommendedImageRectHeight;
  XrMatrix4x4f scalem;
  XrMatrix4x4f_CreateScale(&amp;scalem, 0.5f, 0.5f,
                           1.f);
  XrMatrix4x4f biasm;
  XrMatrix4x4f_CreateTranslation(&amp;biasm, 0.5f,
                                 0.5f, 0);
  XrMatrix4x4f rectscalem;
  XrMatrix4x4f_CreateScale(&amp;rectscalem, width,
                           height, 1.f);
  XrMatrix4x4f rectbiasm;
  XrMatrix4x4f_CreateTranslation(&amp;rectbiasm, 0, 0,
                                 0);
  XrMatrix4x4f rectfromclipm;
  XrMatrix4x4f_Multiply(&amp;rectfromclipm,
                        &amp;rectbiasm, &amp;rectscalem);
  XrMatrix4x4f_Multiply(&amp;rectfromclipm,
                        &amp;rectfromclipm, &amp;biasm);
  XrMatrix4x4f_Multiply(&amp;rectfromclipm,
                        &amp;rectfromclipm, &amp;scalem);
  XrMatrix4x4f rectfromeyem;
  XrMatrix4x4f_Multiply(&amp;rectfromeyem,
                        &amp;rectfromclipm, &amp;proj);
  rectfromeyem.m[11] = -1.f;
  XrVector4f texCoords;
  XrMatrix4x4f_TransformVector4f(
    &amp;texCoords, &amp;rectfromeyem, &amp;tanAngle);
  return glm::vec3(texCoords.x,
                   height - texCoords.y - offset,
                   texCoords.y);
}</pre><p class="list-inset">The function uses the helper types and functions defined in <code>xr_linear.h</code>. The projection matrix is calculated in the function, and not cached at the class level, to allow it to be modified while the app is running.</p></li> </ol>
<p>The sample <a id="_idIndexMarker644"/>app in the repository displays <a id="_idIndexMarker645"/>a washed-out round cursor, about 10 pixels in radius, for each eye if eye-tracking is supported by the device to help you see how the eye gaze behaves in the final output.</p>
<h1 id="_idParaDest-290"><a id="_idTextAnchor327"/>Implementing dynamic foveated rendering using Qualcomm’s fragment density map Offset extension</h1>
<p>In the <em class="italic">Implementing static foveated rendering with a fragment density map</em> recipe, we <a id="_idIndexMarker646"/>discussed how to render fragments at a lower density than one fragment per pixel using a map that dictates the fragment density for regions of the render <a id="_idIndexMarker647"/>target. Although useful, the application of a static map is limited because the user’s gaze changes as they look around to inspect the scene displayed on the device. Recomputing and modifying the map for each frame, based on the user’s input, may be computationally expensive and tax the CPU with extra work, making the performance gained with the FDM moot.</p>
<p>Another option is to apply an offset to the static FDM and let the GPU perform the heavy lifting of translating the densities from the map to the rendered scene. Thanks to Qualcomm’s FDM Offset device extension, this is possible.</p>
<p>In this recipe, we will show you how to use this extension to dynamically translate the FDM based on the user’s gaze direction.</p>
<h2 id="_idParaDest-291"><a id="_idTextAnchor328"/>Getting ready</h2>
<p>For this recipe, you will need an HMD that supports eye-tracking features, such as Meta’s Quest Pro. This recipe was authored and tested with Meta’s Quest Pro device, so some of the code shown here is specific to that platform. This recipe assumes you have already implemented static foveated rendering using a fragment density map. If not, you <a id="_idIndexMarker648"/>might want to refer to our previous guide on that topic to understand the foundational concepts.</p>
<h2 id="_idParaDest-292"><a id="_idTextAnchor329"/>How to do it…</h2>
<p>This <a id="_idIndexMarker649"/>extension simplifies the application code by applying an offset to the FDM at render time, inside the render loop:</p>
<ol>
<li>All attachments used in the render pass where the offset is applied to the FDM must be created with the <code>VK_IMAGE_CREATE_FRAGMENT_DENSITY_MAP_OFFSET_BIT_QCOM</code> flag. Since we are rendering directly to swapchain images, the swapchain images need to be created with that flag. Swapchain images are created by OpenXR. Thankfully, Meta devices provide the ability to provide additional Vulkan flags to be used during the creation of swapchain images. For that, create an instance of the <code>XrVulkanSwapchainCreateInfoMETA</code> structure and add the flag mentioned before to its <code>addditionalCreateFlags</code> property:<pre class="source-code">
const XrVulkanSwapchainCreateInfoMETA
  vulkanImageAdditionalFlags{
    .type =
      XR_TYPE_VULKAN_SWAPCHAIN_CREATE_INFO_META,
    .next = nullptr,
    .additionalCreateFlags =
      VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT |
      VK_IMAGE_CREATE_FRAGMENT_DENSITY_MAP_OFFSET_BIT_QCOM,
  };
Const XrSwapchainCreateInfo swapChainCreateInfo = {
  .type = XR_TYPE_SWAPCHAIN_CREATE_INFO,
  .next = &amp;vulkanImageAdditionalFlags,
  ...
};</pre><p class="list-inset">The instance of the <code>XrVulkanSwapchainCreateInfoMETA</code> structure must be added to the <code>pNext</code> chain of the <code>XrSwapchainCreateInfo</code> structure.</p></li> <li>Before <a id="_idIndexMarker650"/>enabling the FDM Offset feature, it is necessary <a id="_idIndexMarker651"/>to check whether the physical device supports it. Doing so requires appending an instance of the <code>VkPhysicalDeviceFragmentDensityMapOffsetFeaturesQCOM</code> structure to the <code>pNext</code> chain of <code>VkPhysicalDeviceFeatures2</code> passed to the <code>vkGetPhysicalDeviceFeatures2</code> function.<p class="list-inset"><code>VkPhysicalDeviceFragmentDensityMapOffsetFeaturesQCOM:: fragmentDensityMapOffset</code> specifies whether the FDM Offset extension is supported.</p></li>
<li>The extension has properties that need to be queried to be used properly. To do that, also include an instance of the <code>VkPhysicalDeviceFragmentDensityMapOffsetPropertiesQCOM</code> structure to the <code>pNext</code> chain of <code>VkPhysicalDeviceProperties2</code> and query those properties with <code>vkGetPhysicalDeviceProperties2</code>. We will use them later.</li>
<li>The FDM Offset extension is a device extension and its name needs to be passed in during the creation of the <code>VkDevice</code> object: <code>"VK_QCOM_fragment_density_map_offset"</code> (or <code>VK_QCOM_FRAGMENT_DENSITY_MAP_OFFSET_EXTENSION_NAME</code>).</li>
<li>The FDM texture needs to be created with the <code>VK_IMAGE_CREATE_FRAGMENT_DENSITY_MAP_OFFSET_BIT_QCOM</code> creation flag.</li>
<li>The <a id="_idIndexMarker652"/>offsets are applied to the FDM by creating an instance <a id="_idIndexMarker653"/>of the <code>VkSubpassFragmentDensityMapOffsetEndInfoQCOM</code> structure and adding it to the <code>pNext</code> chain of the <code>VkSubpassEndInfo</code> structure. Note that, in this case, you need to call <code>vkCmdEndRenderPass2</code>. <code>vkCmdEndRenderPass</code> isn’t extensible (we’ll see how to calculate the offsets in the next step):<pre class="source-code">
const std::array&lt;VkOffset2D, 2&gt; offsets = {
  leftEyeOffset,
  rightEyeOffset,
};
const VkSubpassFragmentDensityMapOffsetEndInfoQCOM
  offsetInfo = {
    .sType =
      VK_STRUCTURE_TYPE_SUBPASS_FRAGMENT_DENSITY_MAP_OFFSET_END_INFO_QCOM,
    .fragmentDensityOffsetCount =
      offsets.size(), // 1 for each
                      // layer/multiview view
    .pFragmentDensityOffsets =
      offsets
        .data(), // aligned to
                 // fragmentDensityOffsetGranularity
};
const VkSubpassEndInfo subpassEndInfo = {
  .sType = VK_STRUCTURE_TYPE_SUBPASS_END_INFO,
  .pNext = &amp;offsetInfo,
};
vkCmdEndRenderPass2KHR(commandBuffer,
                       &amp;subpassEndInfo);</pre></li> <li>The <code>eyeGazeScreenPosLeft</code> and <code>eyeGazeScreenPosRight</code> offsets can <a id="_idIndexMarker654"/>be calculated using the previous recipe, <em class="italic">Retrieving eye gaze information from OpenXR in your app</em>. In the sample app <a id="_idIndexMarker655"/>provided in the repository, they can be retrieved from the context with the <code>OXR::Context::eyeGazeScreenPos(int </code><code>eye)</code> function:<pre class="source-code">
const glm::vec2 swapchainImageCenter =
  glm::vec2(oxrContext.swapchain(0)
                -&gt;viewport()
                .recommendedImageRectWidth /
              2.f,
            oxrContext.swapchain(0)
                -&gt;viewport()
                .recommendedImageRectHeight /
              2.f);
const glm::vec2 offsetInPixelsLeft =
  glm::vec2(<strong class="bold">eyeGazeScreenPosLeft</strong>) -
  swapchainImageCenter;
const glm::vec2 offsetInPixelsRight =
  glm::vec2(<strong class="bold">eyeGazeScreenPosRight</strong>) -
  swapchainImageCenter;
const glm::vec2 fdmOffsetGranularity = glm::vec2(
  vkContext.physicalDevice()
    .fragmentDensityMapOffsetProperties()
    .fragmentDensityOffsetGranularity.width,
  vkContext.physicalDevice()
    .fragmentDensityMapOffsetProperties()
    .fragmentDensityOffsetGranularity.height);
const VkOffset2D leftEyeOffset{
  offsetInPixelsLeft.x,
  offsetInPixelsLeft.y,
};
const VkOffset2D rightEyeOffset{
  offsetInPixelsRight.x,
  offsetInPixelsRight.y,
};</pre><p class="list-inset">This <a id="_idIndexMarker656"/>extension is powerful because it allows static <a id="_idIndexMarker657"/>FDM to be used to achieve dynamic foveation without the need to impose an extra CPU load of recalculating the map every frame. <em class="italic">Figure 8</em><em class="italic">.2</em> shows the result of rendering the bistro scene on a Quest Pro with an FDM plus Qualcomm’s FDM Offset extension. The white circle is the cursor used to help visualize the eye gaze direction.</p></li> </ol>
<div><div><img alt="Figure 8.2 – The bistro scene rendered on a Quest Pro with an FDM applied to the eye gaze direction" src="img/B18491_08_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – The bistro scene rendered on a Quest Pro with an FDM applied to the eye gaze direction</p>
<p>This <a id="_idIndexMarker658"/>concludes our recipe on dynamic foveated rendering. In the next recipe, we <a id="_idIndexMarker659"/>will learn how we can reduce memory load since the VR devices have limited GPU memory.</p>
<h1 id="_idParaDest-293"><a id="_idTextAnchor330"/>Using half floats to reduce memory load</h1>
<p>A <strong class="bold">half float</strong>, also <a id="_idIndexMarker660"/>known as a <strong class="bold">half-precision floating point</strong>, is a binary <a id="_idIndexMarker661"/>floating-point format that occupies 16 bits. It plays a crucial role specifically for its application in VR devices and other low-performance hardware. A half-precision floating point has a smaller memory footprint and <a id="_idIndexMarker662"/>requires less bandwidth, which can significantly improve the performance and efficiency of such devices. They are ideal for scenarios where the precision of full single-precision floating-point numbers is not necessary, such as storing pixel values in graphics, performing large but simple computations in machine learning models, and certain calculations in 3D graphics. Employing 16 bits not only bolsters throughput but also diminishes register usage, a key determinant of GPU performance. The quantity of shaders that can run concurrently is directly contingent upon the available registers, thus making their efficient usage crucial. In this recipe, we demonstrate how to use half floats in Vulkan and how we can reduce memory consumption by storing vertex data in 16-bit floats instead of 32-bit.</p>
<h2 id="_idParaDest-294"><a id="_idTextAnchor331"/>Getting ready</h2>
<p>To implement a half float in your application, there are several Vulkan and GLSL features that you need to be aware of. Vulkan supports half floats by enabling the <code>storageBuffer16BitAccess</code> and <code>shaderFloat16</code> features. The <code>storageBuffer16BitAccess</code> feature allows you to use a 16-bit format for storage buffers, which can save memory and bandwidth. The <code>shaderFloat16</code> feature enables the use of 16-bit floating-point types in your shaders, which can improve performance by reducing the amount of data that needs to be processed.</p>
<p>On the GLSL side, you would need to enable the <code>GL_EXT_shader_explicit_arithmetic_types_float16</code> and <code>GL_EXT_shader_16bit_storage</code> extensions. The <code>GL_EXT_shader_explicit_arithmetic_types_float16</code> extension allows you to perform arithmetic operations with half-precision floating-point numbers directly in your shaders. Meanwhile, the <code>GL_EXT_shader_16bit_storage</code> extension enables you to store half-precision floating-point numbers in your shader storage blocks and interface blocks.</p>
<p>By leveraging these Vulkan and GLSL features, you can effectively incorporate a half float in your application, optimizing performance, especially for low-performance devices.</p>
<h2 id="_idParaDest-295"><a id="_idTextAnchor332"/>How to do it…</h2>
<p>Follow <a id="_idIndexMarker663"/>these steps to effectively implement the 16-bit float, starting with the activation of specific features and then modifying the shader code:</p>
<ol>
<li>Initially, we must activate two specific features: <code>storageBuffer16BitAccess</code> (found in <code>VkPhysicalDeviceVulkan11Features</code>) and <code>shaderFloat16</code> (located in <code>VkPhysicalDeviceVulkan12Features</code>). To facilitate this, we have incorporated a function within the <code>VulkanCore::Context</code> class:<pre class="source-code">
void Context::enable16bitFloatFeature() {
  enable11Features_.storageBuffer16BitAccess = VK_TRUE;
  enable12Features_.shaderFloat16 = VK_TRUE;
}</pre></li> <li>Next, we change our shader code and add GLSL extensions to it. This is done inside the <code>app/src/main/assets/shaders/Common.glsl</code> file. We also change the vertex structure inside this file to use <code>float16_t</code> instead of <code>float</code>. We also use <code>glm::packHalf1x16</code> to convert a 32-bit float to 16-bit when loading GLB assets:<pre class="source-code">
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require
#extension GL_EXT_shader_16bit_storage : require
struct Vertex {
  float16_t posX;
  float16_t posY;
  float16_t posZ;
  float16_t normalX;
  float16_t normalY;
  float16_t normalZ;
  float16_t tangentX;
  float16_t tangentY;
  float16_t tangentZ;
  float16_t tangentW;
  float16_t uvX;
  float16_t uvY;
  float16_t uvX2;
  float16_t uvY2;
  int material;
};</pre></li> </ol>
<p>In conclusion, implementing a 16-bit float offers significant improvement in GPU performance, especially in the context of VR and other low-performance devices. By activating <a id="_idIndexMarker664"/>the necessary features in Vulkan and making the appropriate adjustments in our GLSL shaders, we can take advantage of the benefits that a 16-bit float has to offer. It’s a relatively straightforward process that involves enabling specific features, adjusting shader code, and modifying data structures to accommodate the half-precision format.</p>
<p>In this chapter, you embarked on a journey through the world of OpenXR. You started by grasping the fundamentals and swiftly moved on to mastering advanced techniques. You learned how to implement single pass multiview rendering and how to utilize the fragment density map for static foveated rendering. You also gained the skills to retrieve eye gaze information for your app. Further, you unlocked the secrets of implementing dynamic foveated rendering using Qualcomm’s fragment density map Offset extension. Lastly, you discovered the power of using half floats to significantly reduce memory load in your applications.</p>
</div>
</body></html>