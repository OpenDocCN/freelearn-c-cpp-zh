- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Techniques for Lighting, Shading, and Shadows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Welcome to an exploration of lighting and shading techniques designed to infuse
    realism into your scenes. In the world of graphics, both lighting and shading
    play an integral role in enhancing the aesthetic appeal and realism of 3D visuals.
    This chapter delves into these topics, presenting a spectrum of algorithms ranging
    from the fundamental to the complex which can add realism to your scenes. In this
    chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing G-buffer for deferred rendering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing screen space reflections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing shadow maps for real-time shadows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing screen space ambient occlusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a lighting pass for illuminating the scene
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a comprehensive understanding of these
    techniques, enabling you to adeptly implement them in your rendering projects.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you will need to make sure you have VS 2022 installed along
    with the Vulkan SDK. Basic familiarity with the C++ programming language and an
    understanding of OpenGL or any other graphics API will be useful. Please revisit
    [*Chapter 1*](B18491_01.xhtml#_idTextAnchor019)*, Vulkan Core Concepts*, under
    the T*echnical requirements* section for details on setting up and building executables
    for this chapter. We also assume that by now you are familiar with how to use
    the Vulkan API and various concepts that were introduced in previous chapters.
    All recipes for this chapter are encapsulated in a single executable and can be
    launched using `Chapter04_Deferred_Renderer.exe` executable.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing G-buffer for deferred rendering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deferred rendering** is a technique that adds an additional render pass at
    the beginning of the scene rendering that accumulates various information about
    the scene in screen space, such as position, surface normal, surface color, and
    others. This extra information is stored in a buffer called the **geometry buffer**
    (**G-buffer**), where each one of the values computed during this step is stored
    for each pixel. Once this initial pass has finished, the final scene rendering
    can take place, and the extra information to improve the rendering quality by
    computing things such as reflections, ambient occlusion, atmospheric effects,
    and others can be used. The benefit of using deferred rendering is that it provides
    more efficient handling of complex scenes with many lights, as each light only
    needs to be calculated once per pixel, rather than once per object. We have essentially
    decoupled geometry and shading, which allows for more flexibility in the rendering
    pipeline. The technique also has some disadvantages, such as increased memory
    usage (for the G-buffer itself), and difficulty handling transparency and anti-aliasing.'
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, you will gain an understanding of the implementation of G-buffer
    for deferred rendering, its advantages in managing complex scenes with multiple
    lights, and the challenges it may present, such as increased memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating a G-buffer in Vulkan is somewhat straightforward. The bulk of the technique
    relies on creating a framebuffer that contains references to all render targets
    (textures) that will store the scene’s information, such as position, normal,
    and material data. The render pass also needs to dictate how those render targets
    should be loaded and stored at the end of the pass. Finally, in the fragment shader,
    each render target is specified as an output variable and the value of each render
    target is written to the output that refers to the correct texture or storage
    buffer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – G-buffer textures](img/B18491_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – G-buffer textures
  prefs: []
  type: TYPE_NORMAL
- en: In the repository, the G-buffer generation is encapsulated in the `GBufferPass`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To generate a G-buffer and its artifacts, we need to first create a framebuffer
    and a corresponding `RenderPass`. In the following steps, we will show you how
    to create targets for the base color of the material, the normal, and the depth
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before creating theFrambuffer object, it is necessary to create the textures
    (render targets) that will store the output of the G-buffer pass:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `Framebuffer` object references the preceding targets. The order is important
    here and should be mirrored in the shader where the outputs are specified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `RenderPass` object describes how each render target should be loaded and
    stored. The operations should match the order of the targets used by the framebuffer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the fragment shader, besides the input data originating from the previous
    stages in the pipeline, the output data is directed to each one of the targets
    using the layout keyword and the location qualifier. The location index must match
    the render target index on the framebuffer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code snippet, the world normal is calculated based on the normal
    and tangent values and stored in the `outgBufferWorldNormal` location, which corresponds
    to the attachment with `index 1` (see code fragment in *step 2*).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing screen space reflections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Physically correct reflections involve tracing the path of light rays as they
    bounce off surfaces. This process accounts for the geometry, material properties,
    and light sources in the scene, as well as the view angle. However, it is a very
    computationally intensive process, often too demanding for real-time rendering,
    especially in complex scenes or on less powerful hardware. To achieve a balance
    between visual quality and performance, an approximation technique known as **screen
    space reflection** (**SSR**) can be used. SSR is a method that approximates reflections
    by reusing data that has already been rendered to the screen. By utilizing a screen-space
    variant, the heavy computational cost associated with physically correct reflections
    can be significantly reduced, making it a viable technique for real-time rendering.
    In this recipe, we will explain how to compute reflections using buffers derived
    from the previous section, such as the normal and depth buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SSR uses the depth buffer to find intersections between a reflected ray and
    the geometry’s depth. The reflection ray is computed in world space based on the
    surface normal and the view direction and is marched in small increments until
    it leaves the screen bounds. For every step, the ray’s location is projected onto
    the screen and its coordinates are compared against the depth buffer. If the difference
    between the ray’s location and the depth buffer’s depth is less than a small threshold,
    then the ray has collided with some geometry, and the ray’s originating point
    on the surface is obscured. This reflection vector is then used to look up the
    color of the pixel in the already-rendered image at the reflected position. This
    color is then used as the reflected color, creating the illusion of a reflection.
    SSR can produce visually pleasing reflections that come close to those produced
    by much more computationally expensive physically correct reflection models; however,
    it can only reflect what’s already visible on the screen, and it may produce inaccurate
    results for complex surfaces or at screen edges.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the depth and normal buffers have been calculated, the SSR can be easily
    computed in a render or compute pass:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following SSR code is used by a compute pass and specifies the buffers
    used as input, generated by the deferred rendering step, as well as the transformation
    data it needs to perform the intersection in screen space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Two auxiliary functions are defined in the shader. They perform the projection
    from a point in world space to screen space and calculate the projection from
    a screen space along with depth coordinate to world space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, the data needed for the reflection calculations from the G-buffer
    is fetched. This includes the world normal, specular data, and base color for
    the current pixel. The UV coordinates are calculated, which are used to sample
    the base color from the G-buffer. The roughness, which controls how blurry or
    sharp the reflection is, is also extracted from the specular data. We also check
    the metalness value from the G-buffer specular data. If the material is not metallic
    (`metalness < 0.01`), it assumes it doesn’t reflect and simply writes the base
    color to the result and exits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following snippet fetches the depth of the current pixel from the depth
    buffer and generates the world position of the pixel using the UV and depth. The
    view direction is calculated from the camera position to the pixel’s world position.
    The reflection direction is then calculated using the view direction and the normal.
    The shader then performs ray marching along the reflection direction in screen
    space. It steps along the reflection ray and at each step, it checks whether the
    ray has intersected with any geometry, based on the depth difference between the
    current position of the ray and the depth at the corresponding screen position:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If an intersection is found, the code fetches the color at the intersection
    point and blends it with the base color. The blending is based on the roughness
    value, which represents a characteristic of the surface at the intersection point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we learned how SSR is computed using the depth and normal
    buffers.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are some references that go into more detail on how to implement
    SSR; we suggest you go through these references to gain a more thorough understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://interplayoflight.wordpress.com/2022/09/28/notes-on-screenspace-reflections-with-fidelityfx-sssr/](https://interplayoflight.wordpress.com/2022/09/28/notes-on-screenspace-reflections-with-fidelityfx-sssr/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://roar11.com/2015/07/screen-space-glossy-reflections/](http://roar11.com/2015/07/screen-space-glossy-reflections/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://interplayoflight.wordpress.com/2019/09/07/hybrid-screen-space-reflections/](https://interplayoflight.wordpress.com/2019/09/07/hybrid-screen-space-reflections/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing shadow maps for real-time shadows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name implies, **shadow maps** are used to simulate shadows. The goal
    of shadow mapping is to determine which parts of a scene are in shadow and which
    parts are illuminated by a light source by first rendering the scene from the
    light’s perspective, generating a depth map.
  prefs: []
  type: TYPE_NORMAL
- en: This depth map (also known as a shadow map) serves as a spatial record, storing
    the shortest distance from the light source to any point in the scene. By encapsulating
    the scene from the vantage point of the light source, the depth map effectively
    captures the areas of the scene that are directly visible to the light source
    and those that are occluded.
  prefs: []
  type: TYPE_NORMAL
- en: This depth map is then used during the main render pass to determine if the
    fragment can’t be reached from the light by comparing its depth value with the
    one in the depth map. For each fragment in the scene, we perform a test to evaluate
    whether it lies in shadow. This is achieved by comparing the depth value of the
    fragment from the light source, derived from the main camera’s perspective, with
    the corresponding depth value stored in the depth map.
  prefs: []
  type: TYPE_NORMAL
- en: If the fragment’s depth value exceeds the value recorded in the depth map, it
    implies that the fragment is occluded by another object in the scene and is, therefore,
    in shadow. Conversely, if the fragment’s depth value is less than or equal to
    the depth map value, it signifies that the fragment is directly visible to the
    light source and is thus illuminated.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to implement shadow maps to create real-time
    shadows in your 3D scene. This involves understanding the theory of shadow mapping,
    generating a depth map from the light’s perspective, and finally using this depth
    map in the main render pass to accurately determine which fragments of the scene
    are in shadow and which are illuminated.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To obtain the shadow map, we first need to render the scene from the light’s
    perspective and retain the depth map. This render pass needs a depth texture that
    will store the depth information and simple vertex and fragment shaders. The main
    render pass, in which the scene is rendered, is where the depth map is used as
    a reference to determine if a pixel is lit or not and needs to refer to the shadow
    map generated in the previous step, along with a special sampler to access the
    depth map in the shader code. It also has code to perform the comparison between
    the fragment and the value stored in the depth map.
  prefs: []
  type: TYPE_NORMAL
- en: In the repository, a shadow map generation is encapsulated in the `ShadowPass`
    class, and usage of shadow depth texture is encapsulated in the `LightingPass`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll start with a walk-through of the shadow map pass first:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The shadow map is a regular texture with a format that supports depth values.
    Our depth texture is 4x the resolution of the normal texture and uses the `VK_FORMAT_D24_UNORM_S8_UINT`
    format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The render pass needs to clear the depth attachment at the beginning and then
    store it at the end. There are no color attachments in the shadow map render pass
    or in the framebuffer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This render pass’s pipeline definition needs to match the size of the viewport
    to the size of the shadow map and use the special vertex and fragment shaders
    for this pass. The fragment and vertex shader are conceptually identical to the
    G-buffer pass but they just need to output the depth buffer instead of multiple
    geometry buffers; it also needs a light view projection matrix instead of the
    camera’s one. As a future optimization, you could use the specialization constant
    with the G-buffer pass instead of using a separate shader:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The vertex shader needs the light’s transformation matrix, which is set into
    the `depthTestEnable`, `depthWriteEnable`, and `depthCompareOperation` – will
    govern how we evaluate and store depth information during this process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The fragment shader is empty, as it doesn’t need to output any color information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The main render (lighting) pass uses the shadow map calculated before as a reference
    to determine if a fragment is lit or not. There is no special setup for the scene,
    except for the sampler used with the shadow map, which needs to enable a comparison
    function. The vertex and fragment shaders also need some special treatment to
    perform the depth comparison against the shadow map.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The sampler used to access the shadow map in the shader needs to enable the
    comparison function. We use the `VK_COMPARE_OP_LESS_OR_EQUAL` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The fragment shader needs a shadow map as well as a light’s view projection
    matrix. The following code includes the uniform `sampler2Dshadow`, which holds
    the depth map or shadow map. The uniform `Lights` structure contains information
    about the light source, including its position, direction, color, and the view
    projection matrix from the light’s perspective:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We introduce a `computeShadow` auxiliary function that takes as input a position
    in light-projective space. It first converts this position into **normalized device
    coordinates** (**NDCs**), then it looks up the shadow map at the corresponding
    position and returns the shadow intensity at that point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we introduce another auxiliary function, the `PCF` function. `16` different
    offsets around the pixel. Then, it would calculate the average of these samples
    to determine the final shadow value for the pixel. This averaging process results
    in pixels on the edge of shadows having intermediate shadow values between fully
    lit and fully shadowed, creating a soft transition that makes the shadow look
    more natural:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the main function, we first retrieve the world position and base color of
    the fragment from the G-buffer. If the world position is zero (indicating no meaningful
    information), we simply set the output color to the base color and return early.
    As a next step, the world position is transformed into light-projective space
    using the light’s view projection matrix and passes this position to the `PCF`
    function to compute the visibility factor, which represents how much the fragment
    is in shadow. If the visibility factor is below a threshold (meaning the fragment
    is in deep shadow), it sets the visibility to a fixed value for a minimum amount
    of ambient light. Finally, we multiply the computed `outColor` by the visibility
    factor to create the final color, which will be darker if the fragment is in shadow
    and lighter if it’s lit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding section, we illustrated how to implement shadow pass, but there
    are limitations to this technique that will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more …
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have demonstrated the use of the basic shadow mapping technique; however,
    this technique has some limitations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aliasing and pixelation**: One of the primary issues with simple shadow mapping
    is the problem of **aliasing**, where shadows can appear pixelated or blocky.
    This is because the resolution of the shadow map directly affects the shadow’s
    quality. If the shadow map’s resolution is too low, the resulting shadows will
    be pixelated. While increasing the shadow map’s resolution can mitigate this,
    it comes at the cost of increasing memory usage and computational load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hard shadow edges**: Basic shadow mapping produces shadows with hard edges
    since it uses a binary lit or unlit test for shadow determination. Shadows often
    have soft edges due to light scattering (penumbra).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shadow acne or self-shadowing artifacts**: This problem arises when a surface
    incorrectly shadows itself due to precision errors in the depth test. Techniques
    such as biasing are used to handle this issue but choosing the right bias can
    be challenging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of these challenges can be overcome with more advanced techniques such
    as the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Cascade shadow maps**: This technique addresses the issue of resolution by
    dividing the camera’s view frustum into multiple **cascades** or sections, each
    with its own shadow map. This allows for higher-resolution shadow maps to be used
    close to the camera, where detail is more important, and lower-resolution shadow
    maps to be used farther away.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Moment shadow maps**: This technique uses statistical moments to store more
    information about the depth distribution within a pixel, which can handle transparency
    and provide anti-aliased, soft shadows. Moment shadow maps require more memory
    and computation than basic shadow maps but can provide higher-quality shadows.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are references that discuss and provide implementation details
    about advanced techniques such as cascade shadow maps and moment shadows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://learn.microsoft.com/en-us/windows/win32/dxtecharts/cascaded-shadow-maps](https://learn.microsoft.com/en-us/windows/win32/dxtecharts/cascaded-shadow-maps)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://momentsingraphics.de/I3D2015.html](https://momentsingraphics.de/I3D2015.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing screen space ambient occlusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Screen space ambient occlusion** (**SSAO**) can be used to approximate the
    effect of ambient occlusion in real time. Ambient occlusion is a shading and rendering
    method used to calculate how exposed each point in a scene is to ambient lighting.
    This technique adds more realistic shadows where two surfaces or objects meet,
    or where an object blocks light from reaching another object.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to implement SSAO to realistically estimate
    ambient occlusion in real time. You will grasp how to use this shading and rendering
    technique to calculate the exposure of each point in a scene to ambient lighting.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The algorithm described in this recipe calculates the difference between the
    depth of a pixel and its neighbors (samples) in a circular fashion. If a sample
    is closer to the camera than the central pixel, it contributes to the occlusion
    factor, making the pixel darker.
  prefs: []
  type: TYPE_NORMAL
- en: A depiction of the algorithm is shown in *Figure 4**.2*. The code loops over
    several *rings* around a central point, the pixel being processed. Within each
    ring, it takes several samples, as seen in item (a).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – SSAO sampling pattern](img/B18491_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – SSAO sampling pattern
  prefs: []
  type: TYPE_NORMAL
- en: A small amount of noise is applied to each sample’s location to avoid banding
    effects, as shown in item (b). Additionally, a weight is applied to samples on
    the same ring, with rings farther from the center with the smallest weights, as
    depicted in item (c).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The entire SSAO algorithm is implemented as a compute shader that writes its
    output to an image:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by declaring the inputs and outputs. The input is the depth buffer.
    The output is an image in which we’ll store the result of the algorithm. We also
    need a function to generate noise in 2D. A small amount of noise is applied to
    each sample’s location to avoid banding effects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need a function to convert the depth value from the depth buffer to
    a linear scale, as the values are not stored in a linear fashion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The comparison between the depth value of the pixel being processed and the
    surrounding samples is done by the `compareDepths` function, which returns the
    difference between the samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The main part of the algorithm starts by collecting the pixel’s position and
    its depth value, which is converted to a linear scale. It also calculates the
    size of the buffers and calculates the noise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The size of the area inspected for samples is proportional to the depth value
    of the pixel: the farther away the pixel is from the camera, the smaller the area
    is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The bulk of the algorithm is where the ring radiuses and the number of samples
    are calculated. The number of samples is proportional to the ring’s diameter.
    For each sample, we compare their depths, apply the ring weight, and accumulate
    the output, which is averaged out at the end of the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This concludes our recipe for SSAO. For a deeper understanding and further exploration,
    we highly recommend visiting the various resources provided in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For further understanding and an exploration of the topic of SSAO, you may
    find the following resources helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/NVIDIAGameWorks/HBAOPlus](https://github.com/NVIDIAGameWorks/HBAOPlus)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.gamedevs.org/uploads/comparative-study-of-ssao-methods.pdf](https://www.gamedevs.org/uploads/comparative-study-of-ssao-methods.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://research.nvidia.com/sites/default/files/pubs/2012-06_Scalable-Ambient-Obscurance/McGuire12SAO.pdf](https://research.nvidia.com/sites/default/files/pubs/2012-06_Scalable-Ambient-Obscurance/McGuire12SAO.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.ppsloan.org/publications/vo.pdf](https://www.ppsloan.org/publications/vo.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/GameTechDev/XeGTAO](https://github.com/GameTechDev/XeGTAO)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a lighting pass for illuminating the scene
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last recipe in the book you how to implement a lighting pass; this is where
    we calculate the lighting for the scene. For each light in the scene, we draw
    a volume (for point lights, this would be a sphere; for directional lights, a
    full-screen quad; for spotlights, we would draw a cone) and for each pixel in
    that volume, we fetch the data from the G-buffer and calculate the lighting contribution
    of that light to the pixel. The results are then usually added together (blended)
    to a final render target to get the final image. In the demo, we only have one
    spotlight that is used as a demonstration, but we can easily add multiple lights.
    For each light in the scene, we will need to consider the area affected by the
    light (i.e., we use a shader that fetches the relevant data for each pixel from
    the G-buffer, which then uses this data to calculate how much this light source
    contributes to the final color of each pixel). For example, if we’re dealing with
    a spotlight, this volume is a cone centered at the light’s position, oriented
    in the light’s direction, and with an angle that matches the spread of the spotlight.
    The length or height of the cone should be equal to the range of the spotlight.
    Lastly, we use a physically based lighting model (the **Cook-Torrance lighting
    model**), which is applied in the fragment shader. The inputs to the lighting
    model include the light’s properties (color, intensity, position, etc.) and the
    surface properties (material color, shininess, normal, etc.), which are fetched
    from the G-buffer.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recipe is implemented in the `LightingPass` class and the `Lighting.frag`
    shader. It simply uses a full-screen vertex shader to draw a full-screen quad.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the introduction of this recipe, we use the Cook-Torrance lighting
    model, which is a physically based rendering model that simulates how light interacts
    with a surface. It considers various factors such as the angle of incidence, surface
    roughness, and microfacet distribution to render realistic lighting effects. The
    algorithm uses the `Fresnel-Schlick` function, which is used to determine the
    proportion of light reflected versus refracted depending on the view angle. The
    GGX distribution function calculates the distribution of microfacets on the surface,
    which influences how rough or smooth a surface appears.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The entire algorithm is implemented as a full-screen space quad shader that
    writes its output to a final color texture:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by declaring the inputs and outputs. The inputs include the G-buffer
    data (normal, specular, base color, depth, and position), ambient occlusion map,
    shadow map, camera, and light data. The output is simply `fragColor`, which is
    written to the color attachment as specified by the render pass:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Afterward, we define a few auxiliary functions; these will be used in the main
    function. These are defined in the `brdf.glsl` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `fresnelSchlick` function calculates the `Fresnel-Schlick` approximation,
    which models the amount of reflection and refraction based on the angle at which
    light hits the surface. The result is used to determine the specular color.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `distributionGGX` function calculates the distribution of microfacets on
    a surface. The result models how rough or smooth a surface appears, influencing
    the spread of the specular highlight.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `geometrySchlickGGX` function calculates the geometric attenuation term
    using `geometrySmith` function calculates the total geometric attenuation considering
    both the view direction and the light direction. It calculates geometric attenuation
    for both view and light direction and multiplies both to get the final geometric
    attenuation, this function assumes that microfacet distribution is the same in
    all directions. These functions are combined in the `Cook-Torrance BRDF` model
    to account for microfacet occlusion and shadowing effects:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The shader first retrieves the base color, world position, camera position,
    specular data, and normal data from the G-buffer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the next steps, it calculates the view vector (`V`), light vector (`L`),
    and half vector (`H`). These vectors are used in lighting calculations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'During this part, the `Fresnel-Schlick`, `distributionGGX`, and `geometrySmith`
    functions are called to calculate the specular reflection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, the shader calculates the diffuse reflection. It’s a simple model
    based on Lambert’s cosine law, but it’s modified by applying an energy conservation
    principle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In these last few steps, ambient light is calculated by simply multiplying
    the ambient light color by the base color. Here, we also calculate the attenuation
    based on the distance to the light, and the spot attenuation based on the angle
    between the light direction and the direction to the fragment. These are used
    to calculate the light intensity. The shader calculates the final color by adding
    the ambient, diffuse, and specular components together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the last step, the final color is then multiplied by the ambient occlusion
    factor (sampled from ambient occlusion texture calculated in the *Implementing
    screen space ambient occlusion* recipe) and the shadow visibility factor to account
    for shadows and ambient occlusion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following screenshot, we present an image that shows a shadow created
    using the shadow map technique:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3 – A shadow using a shadow map](img/B18491_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – A shadow using a shadow map
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot demonstrates the result of the SSR technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – SSR](img/B18491_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – SSR
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we embarked on a journey to comprehend and implement some of
    the most influential techniques in 3D graphics for achieving real-time physically
    based effects using the Vulkan API. We began our exploration with the principles
    of G-buffer generation, a foundational concept of deferred rendering. This technique
    allows us to manage the complexity of modern lighting and shading, paving the
    way for the implementation of more advanced rendering effects. We then described
    techniques such as SSR and shadows, which are needed for simulating realism in
    rendered scenes. We also explored the complexities of lighting and shading with
    a deep dive into SSAO. This technique provided us with the tools to simulate the
    intricate ways light radiates in real life, adding depth and detail to corners
    in our 3D world. Finally, our exploration ended with the implementation of a lighting
    pass. By calculating the contributions of various light sources on each object
    in our scene, we successfully illuminated our 3D environment. We hope that you
    have gained a comprehensive understanding of several core techniques in modern
    lighting, shading, and shadows, which will empower you to create stunning and
    realistic 3D graphics with Vulkan.
  prefs: []
  type: TYPE_NORMAL
