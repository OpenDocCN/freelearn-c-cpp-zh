<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-86"><a id="_idTextAnchor086"/>6</h1>
<h1 id="_idParaDest-87"><a id="_idTextAnchor087"/> Concurrent System Programming with C++</h1>
<p>In this chapter, we will look at what concurrency means and how it is different from parallelism. We will go through the fundamentals and the theory behind processes and threads. We will look at the changes in the C++ memory model, which enforce native concurrency support in the language. We will also familiarize ourselves with what a race condition is, how it can lead to a data race, and how to prevent data races. Next, we will get acquainted with the C++20 <code>std::jthread</code> primitive, which enables multithreading support. We will learn about the specifics of the <code>std::jthread</code> class and how we can stop already running <code>std::jthread</code> instances by using the <code>std::stop_source</code> primitive. Finally, we will learn how to synchronize the execution of concurrent code and how to report calculation results from executed tasks. We will learn how to use C++ synchronization primitives such as <em class="italic">barriers</em> and <em class="italic">latches</em> to synchronize the execution of concurrent tasks, and how to properly report the result of these tasks using <em class="italic">promises</em> and <em class="italic">futures</em>.</p>
<p>To sum up, we will be covering the following topics in this chapter:</p>
<ul>
<li>What is concurrency?</li>
<li>Thread versus process</li>
<li>Concurrency with C++</li>
<li>Demystifying race conditions and data races</li>
<li>Practical multithreading</li>
<li>Sharing data during parallel execution</li>
</ul>
<p>So, let’s get started!</p>
<h1 id="_idParaDest-88"><a id="_idTextAnchor088"/>Technical requirements</h1>
<p> All examples in this chapter have been tested in an environment with the following configuration:</p>
<ul>
<li>Linux Mint 21 Cinnamon Edition</li>
<li>GCC 12.2 with compiler flags – <code>-</code><code>std=c++20 -pthread</code></li>
<li>A stable internet connection</li>
<li>Please make sure your environment is at least this recent. For all the examples, you can alternatively use <a href="https://godbolt.org/">https://godbolt.org/</a>.</li>
<li>All code examples in this chapter are available to download from <a href="https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%206">https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%206</a>.</li>
</ul>
<h1 id="_idParaDest-89"><a id="_idTextAnchor089"/>What is concurrency?</h1>
<p>Modern cars <a id="_idIndexMarker464"/>have become highly intricate machines that provide not only transportation but also various other functionalities. These functionalities include infotainment systems, which allow users to play music and videos, and heating and air conditioning systems, which regulate the temperature for passengers. Consider a scenario in which these features did not work simultaneously. In such a case, the driver would have to choose between driving the car, listening to music, or staying in a comfortable climate. This is not what we expect from a car, right? We expect all of these features to be available at the same time, enhancing our driving experience and providing a comfortable trip. To achieve this, these features must operate in parallel.</p>
<p>But do they really run in parallel, or do they just run concurrently? Is there any difference?</p>
<p>In computer systems, <strong class="bold">concurrency</strong> and <strong class="bold">parallelism</strong> are <a id="_idIndexMarker465"/>similar in certain ways, but they are not the same. Imagine you have some work to do, but this work can be done in separate smaller chunks. Concurrency refers to the situation where multiple chunks of the work begin, execute, and finish during overlapping time intervals, without a guaranteed specific order of execution. On the other hand, parallelism is an execution policy where these chunks execute simultaneously on hardware with multiple computing resources, such as a multi-core processor.</p>
<p>Concurrency happens when multiple chunks of work, which we <a id="_idIndexMarker466"/>call <strong class="bold">tasks</strong>, are executed in an unspecified order for a certain period of time. The operating system could run some of the tasks and force the rest to wait. In concurrent execution, the task <a id="_idIndexMarker467"/>continuously strives for an execution slot because the operating system does not guarantee that it will execute all of them at once. Furthermore, it is highly possible that while a task is being executed, it is suddenly suspended, and another task starts executing. This is called <strong class="bold">preemption</strong>. It is clear that in concurrent task execution, the order of how the tasks will be executed is not guaranteed.</p>
<p>Let’s get back to our car example. In modern cars, the infotainment system is responsible for performing many activities simultaneously. For example, it can run the navigation part while allowing you to listen to music. This is possible because the system runs these tasks concurrently. It runs the tasks related to route calculation while processing the music content. If the hardware system has a single core, then these tasks should run concurrently:</p>
<div><div><img alt="Figure 6.1 – Concurrent task execution" height="300" src="img/Figure_6.01_B20833.jpg" width="961"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Concurrent task execution</p>
<p>From the preceding figure, you can see that each task gets a non-deterministic execution time in an unpredictable order. In addition, there is no guarantee that your task will be finished before the next one is started. This is where the preemption happens. While your task is running, it is suddenly suspended, and another task is scheduled for execution. Keep in mind that task switching is not a cheap process. The system consumes the processor’s computation resource to perform this action – to make the context switch. The conclusion should be the following: we have to design our systems to respect these limitations.</p>
<p>On the other hand, parallelism is a form of concurrency that involves executing multiple operations simultaneously on <em class="italic">separate processing units</em>. For example, a computer with multiple CPUs can execute multiple tasks in parallel, which can lead to significant performance improvements. You don’t have to worry about the context switching and the preemption. It has its drawbacks, though, and we will discuss them thoroughly.</p>
<div><div><img alt="Figure 6.2 – Parallel task execution" height="624" src="img/Figure_6.02_B20833.jpg" width="703"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Parallel task execution</p>
<p>Going back to our car example, if the CPU of the infotainment system is multi-core, then the tasks related to the navigation system could be executed on one core, and the tasks for the music processing on some of the other cores. Therefore, you don’t have to take any action to design your code to support preemption. Of course, this is only true if you are sure that your code will be executed in such an environment.</p>
<p>The fundamental connection between concurrency and parallelism lies in the fact that parallelism can be applied to concurrent computations without affecting the accuracy of the outcome, but the presence of concurrency alone does not guarantee parallelism.</p>
<p>In summary, concurrency is an important concept in computing that allows multiple tasks to be executed simultaneously, even though that is not guaranteed. This could lead to improved performance and efficient resource utilization but at the cost of more complicated code respecting the pitfalls that concurrency brings. On the other hand, truly parallel execution of code is easier to handle from a software perspective but must be supported by the underlying system.</p>
<p>In the next section, we will get familiar with the difference between execution threads and processes in Linux.</p>
<h1 id="_idParaDest-90"><a id="_idTextAnchor090"/>Threads versus processes</h1>
<p>In Linux, a <strong class="bold">process</strong> is an<a id="_idIndexMarker468"/> instance of a program in execution. A process can have one or more threads of execution. A <strong class="bold">thread</strong> is a <a id="_idIndexMarker469"/>sequence of instructions that<a id="_idIndexMarker470"/> can proceed independently of other threads within the same process.</p>
<p>Each process has its own memory space, system resources, and execution context. Processes are isolated from each other and do not share memory by default. They can only <a id="_idIndexMarker471"/>communicate through files and <strong class="bold">inter-process communication</strong> (<strong class="bold">IPC</strong>) mechanisms, such as pipes, queues, sockets, shared memory, and so on.</p>
<p>A thread, on <a id="_idIndexMarker472"/>the other hand, is a lightweight unit of execution within a process. The overhead of loading the instructions from non-volatile memory to RAM or even the cache is already paid for by the process creating the thread – the parent process. Each thread has its own stack and register values but shares the memory space and system resources of the parent process. Because threads share memory within the process, they can easily communicate with each other and synchronize their own execution. In general, this makes them more efficient than processes for concurrent execution.</p>
<div><div><img alt="Figure 6.3 – IPC" height="312" src="img/Figure_6.03_B20833.jpg" width="929"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – IPC</p>
<p>The main differences between processes and threads are as follows:</p>
<ul>
<li><strong class="bold">Resource allocation</strong>: Processes are independent entities that have their own memory space, system resources, and scheduling priority. On the other hand, threads share the same memory space and system resources as the process they belong to.</li>
<li><strong class="bold">Creation and destruction</strong>: Processes are created and destroyed by the operating system, while threads are created and managed by the process that they belong to.</li>
<li><strong class="bold">Context switching</strong>: When a context switch occurs, the operating system switches the entire process context, including all its threads. In contrast, a thread context switch only requires switching the state of the current thread, which, in general, is faster and less resource-intensive.</li>
<li><strong class="bold">Communication and synchronization</strong>: IPC mechanisms such as pipes, queues, sockets, and<a id="_idIndexMarker473"/> shared memory are used to enable communication between processes. Threads, on the other hand, can communicate <a id="_idIndexMarker474"/>directly by sharing memory within the same process. This also enables efficient synchronization between threads, as they can use locks and other synchronization primitives to coordinate their access to shared resources.</li>
</ul>
<p class="callout-heading">Important note</p>
<p class="callout">Linux schedules tasks in the kernel, which are either threads or single-threaded processes. Each task is represented through a kernel thread; thus, the scheduler does not differentiate between a thread and a process.</p>
<p>Processes and threads have their analogy in real life. Let’s say you are working on a project with a group of people, and the project is divided into different tasks. Each task represents a unit of work that needs to be completed. You can think of the project as a process, and each task as a thread.</p>
<p>In this analogy, the process (project) is a collection of related tasks that need to be completed to achieve a common goal. Each task (thread) is a separate unit of work that can be assigned to a specific person to complete.</p>
<p>When you assign a task to <a id="_idIndexMarker475"/>someone, you are creating a new thread within the project (process). The person who is assigned the task (thread) can work on it independently, without interfering with the work of others. They may also communicate with other team members (threads) to coordinate their work, just as threads within a process can communicate with each other. They also need to use the common project resource to finish their tasks.</p>
<p>In contrast, if<a id="_idIndexMarker476"/> you divide the project into different projects, you create multiple processes. Each process has its own resources, team members, and goals. It is harder to ensure that both processes share a resource needed for the project to finish.</p>
<p>So, processes and threads in computing are like real-life projects and tasks, respectively. A process represents a collection of related tasks that need to be completed to achieve a common goal, while a thread is a separate unit of work that can be assigned to a specific person to complete.</p>
<p>In Linux, processes are separate instances of a program with their own memory and resources, while threads are lightweight execution units within a process that share the same memory and resources. Threads can communicate more efficiently and are more suitable for tasks that require parallel execution, while processes provide better isolation and fault tolerance.</p>
<p>Having all this in mind, let’s see how to write concurrent code in C++.</p>
<h1 id="_idParaDest-91"><a id="_idTextAnchor091"/>Concurrency with C++</h1>
<p>The C++ language has <a id="_idIndexMarker477"/>had built-in support for managing and executing concurrent threads since C++11. But it doesn’t have any native support for managing concurrent processes. The C++ Standard Library <a id="_idIndexMarker478"/>provides various classes for thread management, synchronization and communication between threads, protection of shared data, atomic operations, and parallel algorithms. The <strong class="bold">C++ memory model</strong> is <a id="_idIndexMarker479"/>also designed with thread awareness in mind. This makes it a great choice for developing concurrent applications.</p>
<p>Multithreading with C++ is the ability to have multiple threads of execution running concurrently within a single program. This allows a program to take advantage of multiple CPU cores and perform tasks in parallel, leading to faster completion of tasks and improved overall performance.</p>
<p>The C++ Standard Library<a id="_idIndexMarker480"/> introduced the <code>std::thread</code> thread management class. Once it is instantiated, it is the responsibility of the user to take care of the thread’s objective. The users have to choose to either join the thread or detach it from its parent thread. If they don’t take care of it, the program terminates.</p>
<p>With the <a id="_idIndexMarker481"/>release of C++20, a brand-new thread management class, <code>std::jthread</code>, was introduced. It makes it relatively easy to create and manage threads. To create a new thread, you can create an instance of the <code>std::jthread</code> class, passing the function or callable object that you want to run as a separate thread. A key advantage of <code>std::jthread</code> compared to <code>std::thread</code> is that you don’t have to explicitly worry about joining it. It will be done automatically during the <code>std::jthread</code> destruction. Later in the chapter, we will have a deeper look into <code>std::jthread</code> and how to use it.</p>
<p>Bear in mind that multithreading will also make a program more complex, as it requires careful management of shared resources and synchronization of threads. If not properly managed, multithreading can lead to issues such as deadlocks and race conditions, which can cause a program to hang or produce unexpected results.</p>
<p>Additionally, multithreading <a id="_idIndexMarker482"/>requires the developers to ensure that the code is thread-safe, which can be a challenging task. Not all tasks are suitable for multithreading; some tasks may actually run slower if attempted to be parallelized.</p>
<p>Overall, multithreading with C++ can provide significant benefits in terms of performance and resource utilization, but it also requires careful consideration of the potential challenges and pitfalls.</p>
<p>Now, let’s get familiar with the most common pitfalls of writing concurrent code.</p>
<h1 id="_idParaDest-92"><a id="_idTextAnchor092"/>Demystifying race conditions and data races</h1>
<p>In C++, multithreading <a id="_idIndexMarker483"/>support was first introduced with the C++11 version of the language. One of the key elements provided by the C++11 standard<a id="_idIndexMarker484"/> to facilitate multithreading is the memory model. The memory model tackles two problems: the layout of objects in memory and the concurrent access to these objects. In C++, all data is represented by objects, which are blocks of memory that have various properties such as type, size, alignment, lifetime, value, and an optional name. Each object remains in memory for a specific period of time and is stored in one or more memory locations, depending on whether it is a simple scalar object or a more complex type.</p>
<p>In the context of <a id="_idIndexMarker485"/>multithreaded<a id="_idIndexMarker486"/> programming in C++, it is crucial to consider how to tackle concurrent access by multiple threads to shared objects. If two or more threads try to access different memory locations, there<a id="_idIndexMarker487"/> is usually no problem. However, when threads attempt to write in the same memory location simultaneously, it can lead to data races, which can cause unexpected behaviors and errors in the program.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Data races occur when multiple threads try to access data and at least one of them attempts to modify it, and no precautions are taken to synchronize the memory access. Data races can cause undefined behavior in your program and are a source of trouble.</p>
<p>But how does your program come to a <em class="italic">data race</em>? This happens when there is a <em class="italic">race condition</em> that hasn’t been properly handled. Let’s have a look into the difference between data races and race conditions:</p>
<ul>
<li><strong class="bold">Race condition</strong>: A <a id="_idIndexMarker488"/>situation where the correctness of a code depends on specific timing or a strict sequence of operation</li>
<li><strong class="bold">Data race</strong>: When <a id="_idIndexMarker489"/>two or more threads access one object and at least one of these threads modifies it</li>
</ul>
<p>Based on these definitions, we can deduce that every data race that occurs in your program comes as a result of not correctly handling race conditions. But the opposite is not always true: not every race condition leads to a data race.</p>
<p>There is no better way to understand race conditions and data races than by looking at an example. Let’s imagine a primitive banking system, really primitive, which we hope doesn’t exist anywhere.</p>
<p>Bill and John have accounts in a bank. Bill has $100 in his account and John has $50. Bill owes John a total of $30. To pay off his debt, Bill decides to make two transfers to John’s account. The first is worth $10 and the second is $20. So de facto, Bill will repay John. After both transfers are complete, Bill will have $70 left in his account, while John will have accumulated a total of $80.</p>
<p>Let’s define <a id="_idIndexMarker490"/>an <code>Account</code> structure that contains the name of the owner of the account together with their account balance at a certain moment:</p>
<pre class="source-code">
struct Account {
    Account(std::string_view the_owner, unsigned
      the_amount) noexcept :
        balance{the_amount}, owner{the_owner} {}
    std::string GetBalance() const {
        return "Current account balance of " + owner +
                " is " + std::to_string(balance) + '\n';
    }
private:
    unsigned balance;
    std::string owner;
};</pre> <p>In the <code>Account</code> structure, we will also add the overloaded operator methods for <code>+=</code> and <code>-=</code>. These are responsible for depositing or withdrawing a specific amount of money to the corresponding account, respectively. Before and after each of the operations, the current balance of the account is printed. Here is the definition of these operators, which are part of the <code>Account</code> structure:</p>
<pre class="source-code">
Account&amp; operator+=(unsigned amount) noexcept {
        Print(" balance before depositing: ", balance,
          owner);
        auto temp{balance}; // {1}
        std::this_thread::sleep_for(1ms);
        balance = temp + amount; // {2}
        Print(" balance after depositing: ", balance,
          owner);
        return *this;
    }
    Account&amp; operator-=(unsigned amount) noexcept {
        Print(" balance before withdrawing: ", balance,
          owner);
        auto temp{balance}; // {1}
        balance = temp - amount; // {2}
        Print(" balance after withdrawing: ", balance,
          owner);
        return *this;
    }</pre> <p>Looking into <a id="_idIndexMarker491"/>the implementation of the operator functions shows that they first read the current balance of the account, then store it in a local object (marker <code>{1}</code>), and finally, using the value of the local object, they increment or decrement with the specified amount.</p>
<p>As simple as it gets!</p>
<p>The resulting value of the new balance of the account is written back into the <code>balance</code> member of the <code>Account</code> structure (marker <code>{2}</code>).</p>
<p>We also need to define a method that will be responsible for the actual money transfer:</p>
<pre class="source-code">
void TransferMoney(unsigned amount, Account&amp; from, Account&amp; to) {
    from -= amount; // {1}
    to += amount; // {2}
}</pre> <p>The only thing it does is withdraw the desired amount from one account (marker <code>{1}</code>) and deposit <a id="_idIndexMarker492"/>it to the other account (marker <code>{2}</code>), which is exactly what we need to successfully transfer money between accounts.</p>
<p>Now, let’s have a look at our <code>main</code> program method, which will execute our example:</p>
<pre class="source-code">
int main() {
    Account bill_account{"Bill", 100}; // {1}
    Account john_account{"John", 50}; // {2}
    std::jthread first_transfer{[&amp;](){ TransferMoney(10,
      bill_account, john_account); }}; // {3}
    std::jthread second_transfer{[&amp;](){ TransferMoney(20,
      bill_account, john_account); }}; // {4}
    std::this_thread::sleep_for(100ms); // {5}
    std::cout &lt;&lt; bill_account.GetBalance(); // {6}
    std::cout &lt;&lt; john_account.GetBalance(); // {7}
    return 0;
}</pre> <p>First, we need to create accounts for Bill and John and deposit $100 and $70 into them, respectively (markers <code>{1}</code> and <code>{2}</code>). Then, we have to do the actual money transfers: one transfer for $10 and one for $20 (markers <code>{3}</code> and <code>{4}</code>). I know that this code may look unfamiliar to you but don’t worry, we will deep-dive into <code>std::jthread</code> shortly in this chapter.</p>
<p>The only important detail you have to know so far is that we try to make both transfers <em class="italic">concurrently</em> with the help of the C++ multithreading library. At the end of the process, we set some time for both execution threads to finish the money transfers (marker <code>{5}</code>) and print the result (markers <code>{6}</code> and <code>{7}</code>). As we already discussed, after the transfers are<a id="_idIndexMarker493"/> finished, Bill should have $70 in his account while John should have $80.</p>
<p>Let’s see the program output:</p>
<pre class="console">
140278035490560 Bill balance before withdrawing: 100
140278027097856 Bill balance before withdrawing: 100
140278027097856 Bill balance after withdrawing: 80
140278035490560 Bill balance after withdrawing: 90
140278027097856 John balance before depositing: 50
140278035490560 John balance before depositing: 50
140278027097856 John balance after depositing: 70
140278035490560 John balance after depositing: 60
Current account balance of Bill is 80
Current account balance of John is 60</pre> <p>Wait, what? Bill has $80 while John has $60! How is that possible?</p>
<p>It’s possible because we created a <em class="italic">race condition</em> that led to a <em class="italic">data race</em>! Let’s explain. Having a deeper look into the implementation of the <code>operator+=</code> method reveals the problem. By the way, the situation is absolutely the same with the other operator method as well:</p>
<pre class="source-code">
Account&amp; operator+=(unsigned amount) noexcept {
    Print(" balance before withdrawing: ", balance, owner);
    auto temp{balance}; // {1}
    std::this_thread::sleep_for(1ms); // {2}
    balance = temp + amount; // {3}
    Print(" balance after withdrawing: ", balance, owner);
    return *this;
}</pre> <p>At marker <code>{1}</code>, we cache the current balance of the account into a local object living on the stack.</p>
<p class="callout-heading">Important note</p>
<p class="callout">The C++ memory model guarantees that each thread has its own copy of all objects with automatic storage duration – the stack objects.</p>
<p>Next, we give the<a id="_idIndexMarker494"/> current execution thread some rest time of at least <code>1ms</code> (marker <code>{2}</code>). With this statement, we put our thread to sleep, allowing other threads (if any) to take processor time and start executing. Nothing to worry about so far, right? Once the thread is back on executing, it uses its cached value of the account’s balance and increments it with the new amount. Finally, it stores the newly calculated value back to the <code>balance</code> member of the <code>Account</code> structure.</p>
<p>Having a closer look into the output of the program, we observe the following:</p>
<pre class="console">
140278035490560 Bill balance before withdrawing: 100
140278027097856 Bill balance before withdrawing: 100
140278027097856 Bill balance after withdrawing: 80
140278035490560 Bill balance after withdrawing: 90</pre> <p>The first transfer starts executing. It is running as part of a thread with the <code>140278035490560</code> identifier. We see that before the withdrawal is finished, the second transfer is started too. Its identifier is <code>140278027097856</code>. The second transfer finishes the withdrawal first, leaving Bill’s bank account with a balance of $80. Then, the first withdrawal is back in action. But what happens then? Instead of taking $10 more from Bill’s account, it actually returns $10! This happens because the first thread was suspended when it had already cached the initial account balance of $100. A <em class="italic">race condition</em> was created. Meanwhile, the second transfer has changed the account balance, and now, when the first transfer is back to execution, it already works with outdated cached values. This results in blindly overriding the newer account balance with the outdated value. A <em class="italic">data </em><em class="italic">race</em> happened.</p>
<h2 id="_idParaDest-93"><a id="_idTextAnchor093"/>How do we avoid them?</h2>
<p>Luckily, the C++ programming language provides various concurrency control mechanisms to address these challenges, such as atomic operations, locks, semaphores, condition variables, barriers, and others. These mechanisms help ensure that shared resources are accessed in a predictable and safe manner and that threads are coordinated effectively to avoid a data race. In the next sections, we will get deeper into some of these synchronization primitives.</p>
<h1 id="_idParaDest-94"><a id="_idTextAnchor094"/>Practical multithreading</h1>
<p>In computer science, a thread of <a id="_idIndexMarker495"/>execution is a sequence of code instructions that can be managed independently by a scheduler of the operating system. On a Linux system, the thread is always part of a process. The C++ threads<a id="_idIndexMarker496"/> could be executed concurrently with each other via the multithreading capabilities provided by the standard. During execution, threads share common memory space, unlike processes, where each has its own. Specifically, the threads of a process share its executable code, the dynamically and globally allocated objects, which are not defined as <code>thread_local</code>.</p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor095"/>Hello C++ jthread</h2>
<p>Every C++ program contains at least <a id="_idIndexMarker497"/>one thread, and this is the thread that runs the <code>int main()</code> method. Multithreaded programs have additional threads started at some point in the execution of the main thread. Let’s have a <a id="_idIndexMarker498"/>look at a simple C++ program that uses multiple threads to print to the standard output:</p>
<pre class="source-code">
#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;syncstream&gt;
#include &lt;array&gt;
int main() {
    std::array&lt;std::jthread, 5&gt; my_threads; // Just an
      array of 5 jthread objects which do nothing.
    const auto worker{[]{
        const auto thread_id = std::
           this_thread::get_id();  // 3
        std::osyncstream sync_cout{std::cout};
        sync_cout &lt;&lt; "Hello from new jthread with id:"
                  &lt;&lt; thread_id &lt;&lt; '\n';
    }};
    for (auto&amp; thread : my_threads) {
        thread = std::jthread{worker}; // This moves the
          new jthread on the place of the placeholder
    }
    std::osyncstream{std::cout} &lt;&lt; "Hello Main program
      thread with id:" &lt;&lt; std::this_thread::get_id() &lt;&lt;
        '\n';
    return 0; // jthread dtors join them here.
}</pre> <p>When the program <a id="_idIndexMarker499"/>starts, the <code>int main()</code> method is entered. Nothing surprising so far. At the beginning of the execution, we create a variable on the method stack, called <code>my_threads</code>. It is a type of <code>std::array</code>, which <a id="_idIndexMarker500"/>contains five elements in it. The <code>std::array</code> type represents a container from the Standard Library, encapsulating C-style, fixed-sized arrays. It has the advantages of a standard container, such as being aware of its own size, supporting assignment, random access iterators, and so on. As with any other array type in C++, we need to specify what kind of elements it contains. In our example, <code>my_threads</code> contains five <code>std::jthread</code> objects. The <code>std::jthread</code> class <a id="_idIndexMarker501"/>was introduced in the C++ Standard Library with the C++20 standard release. It represents a single thread of execution, just like <code>std::thread</code>, which was introduced with the release of C++11. Some advantages of <code>std::jthread</code> compared to <code>std::thread</code> are <a id="_idIndexMarker502"/>that it automatically rejoins on destruction and it can be canceled or stopped in some specific cases. It is defined in the <code>&lt;thread&gt;</code> header; therefore, we must include it in order to compile successfully.</p>
<p>Yes, you are asking the<a id="_idIndexMarker503"/> right question! If we already defined an array of <code>jthread</code> objects, what job do they really perform? The expectation is that every thread is associated with some job that needs to be done. But here, the simple answer is <em class="italic">nothing</em>. Our array contains five <code>jthread</code> objects, which don’t actually represent an execution thread. They are used more like placeholders because, when <code>std::array</code> is instantiated, it also creates the objects it contains using their default constructors if no other arguments are passed.</p>
<p>Let’s now define<a id="_idIndexMarker504"/> some workers that our threads can be associated with. The <code>std::jthread</code> class accepts, as a worker, any <em class="italic">callable</em> type. Such types provide a single operation that can be invoked. Widely known examples of such types are function objects and lambda expressions, which we already covered in detail in <a href="B20833_04.xhtml#_idTextAnchor060"><em class="italic">Chapter 4</em></a>. In our example, we will use lambda expressions because they provide a way of creating anonymous function objects (functors) that can be utilized in-line or passed as an argument. The introduction of lambda expressions in C++11 simplifies the process of creating anonymous functors, making it more efficient and straightforward. The following code shows our worker method defined as a lambda expression:</p>
<pre class="source-code">
const auto worker{[]{
    const auto thread_id = std::this_thread::get_id();
    std::osyncstream sync_cout{std::cout};
    sync_cout &lt;&lt; "Hello from new jthread with id:" &lt;&lt;
      thread_id &lt;&lt; '\n';
}};</pre> <p>The defined lambda expression, <code>const auto worker{…};</code>, is pretty simple. It is instantiated on the function stack. It has no input parameters, and it doesn’t capture any state from outside. The only work it does is to print to the standard output the <code>jthread</code> object’s ID. Every thread in C++ provided by the standard concurrency support library has a unique identifier associated with it. The <code>std::this_thread::get_id()</code> method returns the ID of the specific thread in which it has been invoked. This means that if this lambda expression is passed to several different threads, it should print a different thread ID.</p>
<p>Printing to <code>std::cout</code> by <a id="_idIndexMarker505"/>many concurrent threads could bring surprising results. The <code>std::cout</code> object is defined as a global, thread-safe object, which<a id="_idIndexMarker506"/> ensures that each character written to it is done so atomically. However, no guarantees are made about a sequence of characters such as strings, and it is likely that the output when multiple threads are concurrently writing strings to <code>std::cout</code> will be a mixture of these strings. Well, this is not what we really want here. We expect that each thread will be able to fully print its messages. Therefore, we need a synchronization mechanism, which ensures that writing a string to <code>std::cout</code> is fully atomic. Luckily, C++20 introduces a whole new family of class templates defined in the <code>&lt;syncstream&gt;</code> standard library header, which provides mechanisms to synchronize threads writing to one and the same stream. One of them is <code>std::osyncstream</code>. You can use it as a regular stream. Just create an instance of it by passing <code>std::cout</code> as a parameter. Then, with the help of its <code>std::basic_ostream&amp; operator&lt;&lt;(...)</code> class method, you can insert data, just like a regular stream. It is guaranteed that all of the inserted data will be flushed atomically to the output once the <code>std::osyncstream</code> object goes out of scope and is destroyed. In our example, the <code>sync_cout</code> object will be destroyed when the lambda is about to finish its execution and leave its scope. This is exactly the behavior we want.</p>
<p>Finally, we are ready <a id="_idIndexMarker507"/>to give some work to our threads to do. This means that we need to associate worker lambdas with the five threads we have in the <code>my_threads</code> array. But the <code>std::jthread</code> type supports adding a worker method only as part of its construction. That’s why we need to create other <code>jthread</code> objects and replace them with the placeholders in the <code>my_threads</code> array:</p>
<pre class="source-code">
for (auto&amp; thread : my_threads) {
    thread = jthread{worker}; // This moves the new jthread
      on the place of the placeholder
}</pre> <p>Being a standard container, <code>std::array</code> natively supports range-based for loops. Therefore, we can easily iterate through all elements in <code>my_threads</code> and replace them with new <code>jthread</code> objects that already have associated workers with them. Firstly, we create new <code>jthread</code> objects with automatic storage duration and assign them a worker object. In our case, for every newly created thread, we assign one and the same worker object. This is possible because, in the current case, the <code>jthread</code> class makes a copy of the worker instance in the <code>jthread</code> objects and, therefore, each <code>jthread</code> object gets its own copy of the worker lambda. When constructing these objects, the process is carried out within the context of the caller. This means that any exceptions that occur during the evaluation and copying or movement of the arguments are thrown in the current <code>main</code> thread.</p>
<p>An important<a id="_idIndexMarker508"/> detail is that the newly created <code>jthread</code> objects are not copied to the existing elements of the array, but they are moved. Therefore, the <code>std::jthread</code> class has implicitly deleted its copy constructor and assignment operator because it doesn’t make much sense to copy a thread to an already existing thread. In our case, the newly created <code>jthread</code> objects will be created in the storage of the existing array elements.</p>
<p>When a <code>jthread</code> object is<a id="_idIndexMarker509"/> constructed, the associated thread starts immediately, although there may be some delays due to Linux scheduling specifics. The thread begins executing at the function specified as an argument to the constructor. In our example, this is the worker lambda associated with each thread. If the worker returns a result, it will be ignored, and if it ends by throwing an exception, the <code>std::terminate</code> function is executed. Therefore, we need to make sure that either our worker code doesn’t throw or we catch everything throwable.</p>
<p>When a thread is started, it begins executing its dedicated worker. Each thread has its own function stack space, which guarantees that any local variable defined in the worker will have a separate instance per thread. Therefore, <code>const auto thread_id</code> in the worker is initialized with a different ID depending on the thread it is run by. We do not need to take any precautions to ensure that the data stored in <code>thread_id</code> is consistent. It is guaranteed by the Standard that data with automatic storage duration is not shared between the threads.</p>
<p>Once all the <code>jthread</code> objects have been created, the <code>main</code> thread concurrently prints its ID along with the rest of the threads. There is no guaranteed order of execution for each thread, and it is possible for one thread to be interrupted by another. As a result, it is important to ensure that the code is written in a manner that can handle potential preemption and remains robust in all scenarios:</p>
<pre class="source-code">
std::osyncstream{std::cout} &lt;&lt; "Hello Main program thread
  with id:" &lt;&lt; std::this_thread::get_id() &lt;&lt; '\n';</pre> <p>All threads are now running<a id="_idIndexMarker510"/> concurrently with the <code>main</code> thread. We need to make sure that the <code>main</code> thread is also printing to the standard output in a thread-safe manner. We again use an instance of <code>std::osyncstream</code>, but this time, we don’t create a named variable – instead, we create a temporary one. This approach is favored due to its ease of use, similar <a id="_idIndexMarker511"/>to using the <code>std::cout</code> object. The standard guarantees that the output will be flushed at the end of each statement, as the temporary ones persist until the end of the statement and their destructor is invoked, resulting in the flushing of the output.</p>
<p>Here is a sample output from the program:</p>
<pre class="console">
Hello from new jthread with id:1567180544
Hello from new jthread with id:1476392704
Hello from new jthread with id:1468000000
Hello Main program thread with id:1567184704
Hello from new jthread with id:1558787840
Hello from new jthread with id:1459607296</pre> <p>The <code>std::jthread</code> name refers to a <em class="italic">joining</em> thread. Unlike <code>std::thread</code>, <code>std::jthread</code> also has the ability to <em class="italic">automatically</em> join the thread that it has been started by. The behavior of <code>std::thread</code> can be confusing at times. If <code>std::thread</code> has not been joined or detached, and it is still considered <em class="italic">joinable</em>, the <code>std::terminate</code> function will be called upon its destruction. A thread is considered joinable if neither the <code>join()</code> nor the <code>detach()</code> method has been called. In our example, all the <code>jthread</code> objects automatically join during their destruction and do not result in the termination of the program.</p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor096"/>Canceling threads – is this really possible?</h2>
<p>Before C++ 20 was<a id="_idIndexMarker512"/> released, this wasn’t quite possible. It was not guaranteed that <code>std::thread</code> was stoppable in the sense that there wasn’t a standard utility to halt the thread’s execution. Different mechanisms were used instead. Stopping <code>std::thread</code> required cooperation between the <code>main</code> and worker threads, typically using a flag or atomic variable or some kind of messaging system.</p>
<p>With the release of C++20, there is now a standardized utility for requesting <code>std::jthread</code> objects to stop their execution. The stop tokens come to help. Looking at the C++ standard reference page about the<a id="_idIndexMarker513"/> definition of <code>std::jthread</code> (<a href="https://en.cppreference.com/w/cpp/thread/jthread">https://en.cppreference.com/w/cpp/thread/jthread</a>), we find the following:</p>
<p class="author-quote">“The class jthread represents a single thread of execution. It has the same general behavior as std::thread, except that jthread automatically rejoins on destruction, and can be canceled/stopped in certain situations.”</p>
<p>We already saw that <code>jthread</code> objects automatically join on destruction, but what about canceling/stopping and what does “certain situations” mean? Let’s dig deeper into this.</p>
<p>First of all, don’t <a id="_idIndexMarker514"/>expect that <code>std::jthread</code> exposes some magical mechanism, some red button that stops the running thread when it is pressed. It is always a matter of implementation, how exactly your worker function is implemented. If you want your thread to be cancelable, you have to make sure that you have implemented it in the right way in order to allow cancellation:</p>
<pre class="source-code">
#include &lt;iostream&gt;
#include &lt;syncstream&gt;
#include &lt;thread&gt;
#include &lt;array&gt;
using namespace std::literals::chrono_literals;
int main() {
    const auto worker{[](std::stop_token token, int num){
      // {1}
        while (!token.stop_requested()) { // {2}
            std::osyncstream{std::cout} &lt;&lt; "Thread with id
              " &lt;&lt; num &lt;&lt; " is currently working.\n";
            std::this_thread::sleep_for(200ms);
        }
        std::osyncstream{std::cout} &lt;&lt; "Thread with id " &lt;&lt;
          num &lt;&lt; " is now stopped!\n";
    }};
    std::array&lt;std::jthread, 3&gt; my_threads{
        std::jthread{worker, 0},
        std::jthread{worker, 1},
        std::jthread{worker, 2}
    };
    // Give some time to the other threads to start
      executing …
    std::this_thread::sleep_for(1s);
    // 'Let's stop them
    for (auto&amp; thread : my_threads) {
        thread.request_stop(); // {3} - this is not a
          blocking call, it is just a request.
    }
    std::osyncstream{std::cout} &lt; "Main thread just
      requested stop!\n";
    return 0; // jthread dtors join them here.
}</pre> <p>Looking at the <a id="_idIndexMarker515"/>definition of our worker lambda function, we observe that it is now slightly reworked (marker <code>{1}</code>). It accepts two new parameters – <code>std::stop_token token</code> and <code>int num</code>. The stop token reflects the shared stop state that a <code>jthread</code> object has. If the worker method accepts many parameters, then the stop token must always be the first parameter passed.</p>
<p>It is imperative to ensure that the worker method is designed to be able to handle cancellation. This is what the stop token is used for. Our logic should be implemented in such a way that it regularly checks whether a stop request has been received. This is done with a call to the <code>stop_requested()</code> method of the <code>std::stop_token</code> object. Every specific implementation decides where and when these checks are to be done. If the code doesn’t respect the stop token state, then the thread can’t be canceled gracefully. So, it’s up to you to correctly design your code.</p>
<p>Luckily, our worker <a id="_idIndexMarker516"/>lambda respects the state of the thread’s stop token. It continuously checks whether a stop is requested (marker <code>{2}</code>). If not, it prints the thread’s ID and goes to sleep for <code>200ms</code>. This loop continues until the parent thread decides to send stop requests to its worker threads (marker <code>{3}</code>). This is done by invoking the <code>request_stop()</code> method of the <code>std::jthread</code> object.</p>
<p>Here is the output of the program:</p>
<pre class="console">
Thread with id 0 is currently working.
Thread with id 1 is currently working.
Thread with id 2 is currently working.
Thread with id 1 is currently working.
Thread with id 2 is currently working.
Thread with id 0 is currently working.
Thread with id 1 is currently working.
Thread with id 2 is currently working.
Thread with id 0 is currently working.
Thread with id 2 is currently working.
Thread with id 1 is currently working.
Thread with id 0 is currently working.
Thread with id 1 is currently working.
Thread with id 0 is currently working.
Thread with id 2 is currently working.
Main thread just requested stop!
Thread with id 1 is now stopped!
Thread with id 0 is now stopped!
Thread with id 2 is now stopped!</pre> <p>Now that we know how<a id="_idIndexMarker517"/> we can stop the execution of a specific <code>std::jthread</code> using <code>std::stop_token</code>, let’s see how we can stop the execution of multiple <code>std::jthread</code> objects using a single stop source.</p>
<h2 id="_idParaDest-97"><a id="_idTextAnchor097"/>std::stop_source</h2>
<p>The <code>std::stop_source</code> class <a id="_idIndexMarker518"/>enables you to<a id="_idIndexMarker519"/> signal a cancellation request for <code>std::jthread</code>. When a stop request is issued through a <code>stop_source</code> object, it becomes visible to all other <code>stop_source</code> and <code>std::stop_token</code> objects associated with the same stop state. You just need to signal it, and any thread worker that consumes it will be notified.</p>
<p>By utilizing <code>std::stop_token</code> and <code>std::stop_source</code>, threads can signal or check for a request to stop their execution asynchronously. The request to stop is made through <code>std::stop_source</code>, which affects all related <code>std::stop_token</code> objects. These tokens can be passed to the worker functions and used to monitor stop requests. Both <code>std::stop_source</code> and <code>std::stop_token</code> share ownership of the stop state. The method of the <code>std::stop_source</code> class – <code>request_stop()</code> – and the methods in <code>std::stop_token</code> – <code>stop_requested()</code> and <code>stop_possible()</code> – are all atomic operations to ensure that no data race will occur.</p>
<p>Let’s have a look at how our previous example could be reworked with the help of the stop tokens:</p>
<pre class="source-code">
#include &lt;iostream&gt;
#include &lt;syncstream&gt;
#include &lt;thread&gt;
#include &lt;array&gt;
using namespace std::literals::chrono_literals;
int main() {
    std::stop_source source;
    const auto worker{[](std::stop_source sr, int num){
        std::stop_token token = sr.get_token();
        while (!token.stop_requested()) {
            std::osyncstream{std::cout} &lt;&lt; "Thread with id
              " &lt;&lt; num &lt;&lt; " is currently working.\n";
            std::this_thread::sleep_for(200ms);
        }
        std::osyncstream{std::cout} &lt;&lt; "Thread with id " &lt;&lt;
          num &lt;&lt; " is now stopped!\n";
    }};
    std::array&lt;std::jthread, 3&gt; my_threads{
        std::jthread{worker, source, 0},
        std::jthread{worker, source, 1},
        std::jthread{worker, source, 2}
    };
    std::this_thread::sleep_for(1s);
    source.request_stop(); // this is not a blocking call,
      it is just a request. {1}
    Std::osyncstream{std::cout} &lt;&lt; "Main thread just
      requested stop!\n";
    return 0; // jthread dtors join them here.
}</pre> <p>The <code>main</code> method starts with the declaration of the <code>std::stop_source</code> source, which will be<a id="_idIndexMarker520"/> used by the <code>main</code> thread to signal all child worker threads and request them to stop. The worker lambda is slightly reworked in order to accept <code>std::stop_source sr</code> as an input. This is in fact the communication channel through which the worker is notified for a stop request. The <code>std::stop_source</code> object is copied in all workers associated with the started threads.</p>
<p>Rather than<a id="_idIndexMarker521"/> iterating through all the threads and invoking on each of them a stop request, the only operation that we need to invoke is to directly call <code>request_stop()</code> on the source instance in the <code>main</code> thread (marker <code>{1}</code>). This will broadcast stop requests to all workers that consume it.</p>
<p>As the name suggests, the call to the <code>request_stop()</code> method on the stop source object is just a request rather than a blocking call. So, don’t expect your threads to stop immediately once the call is finished.</p>
<p>Here is the sample output from the program:</p>
<pre class="console">
Thread with id 0 is currently working.
Thread with id 1 is currently working.
Thread with id 2 is currently working.
Thread with id 1 is currently working.
Thread with id 2 is currently working.
Thread with id 0 is currently working.
Thread with id 1 is currently working.
Thread with id 2 is currently working.
Thread with id 0 is currently working.
Thread with id 1 is currently working.
Thread with id 0 is currently working.
Thread with id 2 is currently working.
Thread with id 1 is currently working.
Thread with id 0 is currently working.
Thread with id 2 is currently working.
Main thread just requested stop!
Thread with id 1 is now stopped!
Thread with id 0 is now stopped!
Thread with id 2 is now stopped!</pre> <p>We are <a id="_idIndexMarker522"/>now familiar with two mechanisms for halting <a id="_idIndexMarker523"/>thread execution in C++. Now, it’s time to see how we can share data between multiple threads.</p>
<h1 id="_idParaDest-98"><a id="_idTextAnchor098"/>Sharing data during parallel execution</h1>
<p><em class="italic">Think in terms of tasks rather than </em><em class="italic">threads</em> (<a href="https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#cp4-think-in-terms-of-tasks-rather-than-threads">https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#cp4-think-in-terms-of-tasks-rather-than-threads</a>).</p>
<p>Referring <a id="_idIndexMarker524"/>back to the <em class="italic">C++ Core Guidelines</em>, they advise us that it is better to stick to tasks rather than threads. A thread is a technical implementation idea, a perspective on how the machine works. On the<a id="_idIndexMarker525"/> other hand, a task is a practical concept for work that you want to do, ideally alongside other tasks. In general, practical concepts are simpler to understand and provide better abstraction, and we prefer them.</p>
<p>But what is a task in C++? Is it another standard library primitive or what? Let’s have a look!</p>
<p>In C++, besides threads, tasks are also available to perform work asynchronously. A task consists of a worker and two <a id="_idIndexMarker526"/>associated components: a <strong class="bold">promise</strong> and a <strong class="bold">future</strong>. These components are connected through a shared state, which is a kind of data channel. The promise <a id="_idIndexMarker527"/>does the work and places the result in the shared state, while the future retrieves<a id="_idIndexMarker528"/> the result. Both the promise and the future can run in separate threads. One unique aspect of the future is that it can retrieve the result at a later time, making the calculation of the result by the promise independent from the retrieval of the result by the associated future.</p>
<div><div><img alt="Figure 6.4 – Inter-thread communication" height="205" src="img/Figure_6.04_B20833.jpg" width="588"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Inter-thread communication</p>
<p>The <code>&lt;future&gt;</code> header, defined<a id="_idIndexMarker529"/> in the<a id="_idIndexMarker530"/> Standard Library, is necessary for utilizing tasks. It provides the capability to obtain the results of functions executed in separate threads, also<a id="_idIndexMarker531"/> referred to as <code>std::promise</code> class, these results are communicated through a shared state, where the asynchronous task can store its return value or an exception. This shared state can then be accessed using <code>std::future</code> to retrieve the return value or the stored exception.</p>
<p>Let’s have a look at a simple example where a thread reports a string as a result to its parent thread:</p>
<pre class="source-code">
#include &lt;future&gt;
#include &lt;thread&gt;
#include &lt;iostream&gt;
using namespace std::literals::chrono_literals;
int main() {
    std::promise&lt;std::string&gt; promise; // {1}
    std::future&lt;std::string&gt; future{promise.get_future()};
      // {2} – Get the future from the promise.
    std::jthread th1{[p{std::move(promise)}]() mutable { //
      {3} – Move the promise inside the worker thread.
        std::this_thread::sleep_for(20ms);
        p.set_value_at_thread_exit("I promised to call you
          back once I am ready!\n"); // {4}
    }};
    std::cout &lt;&lt; "Main thread is ready.\n";
    std::cout &lt;&lt; future.get(); // {5} – This is a blocking
      call!
    return 0;
}</pre> <p>As we already<a id="_idIndexMarker532"/> discussed, threads <a id="_idIndexMarker533"/>communicate with each other using a shared state. In the <code>int main()</code> method, we declare <code>std::promise&lt;std::string&gt; promise</code>, which is our de facto data source (marker <code>{1}</code>). The <code>std::promise</code> class is a template class that needs to be parameterized once it is instantiated. In our example, we want our worker thread, <code>std::thread th1</code>, to return a string as a result. Therefore, we instantiate <code>std::promise</code> with the <code>std::string</code> type. We also need a way for the <code>main</code> thread to be able to get the result that will be set by the worker thread. In order to do so, we need to get a <code>std::future</code> object from the promise we already instantiated. This is possible because the <code>std::promise</code> type has a method that returns its associated future – <code>std::future&lt;...&gt; get_future()</code>. In our example, we instantiate a future object, <code>future</code>, which is initialized by the <code>get_future()</code> method of the promise (marker <code>{2}</code>).</p>
<p>Since we already have a promise and its associated future, we are now ready to move the promise as part of the worker thread. We are moving it in order to be sure that it won’t be used by the <code>main</code> thread anymore (marker <code>{3}</code>). Our worker thread is quite simple, and it just sleeps for <code>20ms</code> and sets the result in the promise (marker <code>{4}</code>). The <code>std::promise</code> type provides several ways to set a result. The result could be either a value of type by which the promise is parameterized or it could be an exception thrown during worker execution. The value is set by the <code>set_value()</code> and <code>set_value_at_thread_exit()</code>methods. The main difference between both methods is that <code>set_value()</code> immediately notifies the shared state that the value is ready, whereas <code>set_value_at_thread_exit()</code> does it when the thread execution is finished.</p>
<p>Meanwhile, the <code>main</code> thread execution has been blocked waiting for the result of the worker <a id="_idIndexMarker534"/>thread. This<a id="_idIndexMarker535"/> is done on the call to the <code>future.get()</code> method. This is a blocking call on which the waiting thread is blocked until the shared state is notified that the result of the future is set. In our example, this happens after the completion of the worker thread because the shared state is only notified when the worker is finished (marker <code>{5}</code>).</p>
<p>The expected output from the program is as follows:</p>
<pre class="console">
Main thread is ready.
I promised to call you back once I am ready!</pre> <h2 id="_idParaDest-99"><a id="_idTextAnchor099"/>Barriers and latches</h2>
<p>New thread <a id="_idIndexMarker536"/>synchronization primitives were introduced with the C++20 standard. Barriers and latches are straightforward thread synchronization primitives that block threads to wait until a counter reaches zero. These primitives are offered by the standard library in the form of the <code>std::latch</code> and <code>std::barrier</code> classes.</p>
<p>What distinguishes these two synchronization mechanisms? The key difference is that <code>std::latch</code> can<a id="_idIndexMarker537"/> only be used once, while <code>std::barrier</code> can be used multiple times by multiple threads.</p>
<p>What advantages do barriers and latches offer over other synchronization primitives that the C++ standard provides, such as condition variables and locks? Barriers and latches are easier to use, more intuitive, and, in some circumstances, may provide better performance.</p>
<p>Let’s have a<a id="_idIndexMarker538"/> look at <a id="_idIndexMarker539"/>the following example:</p>
<pre class="source-code">
#include &lt;thread&gt;
#include &lt;iostream&gt;
#include &lt;array&gt;
#include &lt;latch&gt;
#include &lt;syncstream&gt;
using namespace std::literals::chrono_literals;
int main() {
    std::latch progress{2}; // {1}
    std::array&lt;std::jthread, 2&gt; threads {
        std::jthread{[&amp;](int num){
            std::osyncstream{std::cout} &lt;&lt; "Starting thread
              " &lt;&lt; num &lt;&lt; " and go to sleep.\n";
            std::this_thread::sleep_for(100ms);
            std::osyncstream{std::cout} &lt;&lt; "Decrementing
              the latch for thread " &lt;&lt; num &lt;&lt; '\n';
            progress.count_down(); // {2}
            std::osyncstream{std::cout} &lt;&lt; "Thread " &lt;&lt; num
              &lt;&lt; " finished!\n";
        }, 0},
        std::jthread{[&amp;](int num){
            std::osyncstream{std::cout} &lt;&lt; "Starting thread
              " &lt;&lt; num &lt;&lt; ". Arrive on latch and wait to
                 become zero.\n";
            progress.arrive_and_wait(); // {3}
            std::osyncstream{std::cout} &lt;&lt; "Thread " &lt;&lt; num
              &lt;&lt; " finished!\n";
        }, 1}
    };
    std::osyncstream{std::cout} &lt;&lt; "Main thread waiting
      workers to finish.\n";
    progress.wait(); // {4} wait for all threads to finish.
    std::cout &lt;&lt; "Main thread finished!\n";
    return 0;
}</pre> <p>We have an array of two threads that are synchronized on a latch. This means that each thread starts its execution and does its work until it reaches the latch.</p>
<p>The <code>std::latch</code> class <a id="_idIndexMarker540"/>is a synchronization mechanism that utilizes a downward-counting <a id="_idIndexMarker541"/>counter to coordinate threads. The counter is set at initialization and passed as an argument to the constructor. The threads can then wait until the counter reaches zero. It is not possible to increase or reset the counter once it is initialized. Access to the member functions of <code>std::latch</code> from multiple threads concurrently is guaranteed to be thread-safe and free from data races.</p>
<p>In our example (marker <code>{1}</code>), we have initialized the latch with a value of <code>2</code> because we have two worker threads that need to be synchronized with the main one. Once the worker reaches the latch, it has three options:</p>
<ul>
<li>Decrement it and continue (marker <code>{2}</code>). This is done using the member of the <code>std::latch</code> class – <code>void count_down(n = 1)</code>. This call is non-blocking and automatically decrements the latch’s internal counter value by <code>n</code>. It is undefined behavior if you try to decrement with a negative value or with a value greater than the value that the internal counter currently has. In our example, this is a worker thread with an ID of <code>0</code>, which, once it is ready, decrements the latch counter and finishes.</li>
<li>Decrement it<a id="_idIndexMarker542"/> and wait until the latch becomes zero (marker <code>{3}</code>). In order to do so, you have to use another method of the <code>std::latch</code> class – <code>void arrive_and_wait(n = 1)</code>. This method, once it is invoked, decrements the latch by <code>n</code> and blocks it until the latch’s internal counter hits <code>0</code>. In our example, this is a worker thread with an ID of <code>1</code>, which, once it is ready, starts waiting until the other worker is finished.</li>
<li>Just block and <a id="_idIndexMarker543"/>wait until the internal counter of the latch becomes zero (marker <code>{4}</code>). This is possible because <code>std::latch</code> provides a method – <code>void wait() const</code>. This is a blocking call on which the invoking thread is blocked until the internal counter of the latch hits zero. In our example, the <code>main</code> thread blocks and starts waiting for the worker threads to finish their execution.</li>
</ul>
<p>The result of our program is that the <code>main</code> thread execution is suspended until the worker threads finish their jobs. The <code>std::latch</code> class provides a convenient way to synchronize the execution of several threads:</p>
<pre class="console">
Main thread waiting workers to finish.
Starting thread 1. Arrive on latch and wait to become zero.
Starting thread 0 and go to sleep.
Decrementing the latch for thread 0
Thread 0 finished!
Main thread finished!
Thread 1 finished!</pre> <p>Another very similar synchronization primitive to <code>std::latch</code> is <code>std::barrier</code>. Barriers are thread synchronization primitives that permit a group of threads to wait until all of them reach a specific synchronization point. Unlike a latch, a barrier can be used multiple times. Once the threads have been released from the synchronization point, they can reuse the barrier. A synchronization point is a specific moment where a thread can pause its execution until a specific condition has been met. This makes barriers ideal for synchronizing repeated tasks or executing different phases from the same bigger task by many threads.</p>
<p>In order to get a<a id="_idIndexMarker544"/> better understanding of what barriers are, let’s use an<a id="_idIndexMarker545"/> example. Imagine that you have a network of temperature sensors installed in your home. In each room, there is a sensor installed. Each sensor takes a temperature measurement at a specific time period and the result is buffered in its memory. When the sensor does 10 measurements, it sends them as a chunk to a server. This server is responsible for collecting all measurements from all sensors in your home and calculating temperature mean values – the mean temperature for each room and the mean temperature for your entire home.</p>
<p>Let’s discuss the algorithm now. In order to calculate the mean temperature of your entire home, we first need to process the temperature measurements done by the sensors that are sent to the server at some specific time period. This means that we need to process all the temperature samples received for a specific room to calculate the mean temperature for that room, and we need to do this for all the rooms in your home. Finally, with the calculated mean temperatures for each room, we can calculate the mean temperature for the entire home.</p>
<p>It sounds like we need to process a lot of data. It makes sense to try to parallelize the data processing wherever possible. Yes, you are right: not all of the data processing can be parallelized! There is a strict sequence of actions we need to respect. Firstly, we need to calculate the mean temperature in each room. There are no dependencies between the rooms, so we can execute these calculations in parallel. Once we have all the room temperatures calculated, we can continue to the calculation of the mean temperature of the entire home. This is exactly where <code>std::barrier</code> will come to the rescue.</p>
<p>The <code>std::barrier</code> synchronization primitive blocks the threads at a specific synchronization point (the barrier) until all of them arrive. Then, it allows a callback to be invoked and a specific action to be performed. In our example, we need to wait for all room calculations to be finished – to wait on the barrier. Then, a callback will be executed where we will calculate the mean temperature for the entire home:</p>
<pre class="source-code">
using Temperature =
    std::tuple&lt;std::string, // The name of the room
               std::vector&lt;double&gt;, // Temperature
                 measurements
               double&gt;; // Calculated mean temperature
                        // value for a specific room
std::vector&lt;Temperature&gt; room_temperatures {
    {"living_room",{}, 0.0},
    {"bedroom", {}, 0.0},
    {"kitchen", {}, 0.0},
    {"closet", {}, 0.0}
};</pre> <p>Let’s start with<a id="_idIndexMarker546"/> the<a id="_idIndexMarker547"/> definition of our data container where we will store the temperature measurements done for each room, together with their calculated mean values by our worker threads. We will use a vector of rooms, <code>room_temperature</code>, in which we will store the room name, a vector of measurements, and the mean value.</p>
<p>Now, we need to define the workers that will, in parallel, calculate the mean values for each room:</p>
<pre class="source-code">
std::stop_source message;
std::barrier measurementBarrier{ // {1}
    static_cast&lt;int&gt;(room_temperatures.size()), // {2}
    [&amp;message]() noexcept { // {3}
        // 1. Compute the mean temperature of the entire
          home.
        // 2. Push new temperature data
        // 3. After 5 measurement cycles request stop.
    }
};
std::vector&lt;std::jthread&gt; measurementSensors;
for (auto&amp; temp : room_temperatures) {
    measurementSensors.emplace_back([&amp;measurementBarrier,
      &amp;message, &amp;temp](){
        const auto&amp; token = message.get_token();
        while(!token.stop_requested()) {
            ProcessMeasurement(temp);
            measurementBarrier.arrive_and_wait(); // {4}
        }
    });
}</pre> <p>We create the same count of <code>jthread</code> instances as the count of the rooms. Each <code>jthread</code> instance is created and a worker lambda is assigned to it. As you can see, the worker lambda captures a <code>std::stop_source</code> object, which will be used to notify it that no other work is pending and the thread execution should be finished. The lambda also captures <code>std::barrier measurementBarrier</code>, which will be used to block each thread that is ready with its computation until all other threads are also ready (marker <code>{1}</code>).</p>
<p>The <code>std::barrier</code> instance<a id="_idIndexMarker548"/> needs to be initialized with the count of the synchronization points (marker <code>{2}</code>). This means that the barrier will be raised when the count of<a id="_idIndexMarker549"/> threads reaching the barrier is equal to the initialized value. In our example, we initialize the barrier with the count of the worker threads that will concurrently compute the mean temperatures for each room. An optional initialization parameter that the barrier accepts is a callback function (marker <code>{3}</code>). This function must not throw and, therefore, we mark it as <code>noexcept</code>. It will be invoked when all threads in a certain cycle arrive at the barrier and before the barrier is raised. Keep in mind that the standard doesn’t specify which thread this callback will be executed on. We will use this callback to do the following:</p>
<ul>
<li>Iterate through all already computed mean temperatures for the rooms and compute the mean temperature of the entire home. This is the result we expect our program to deliver.</li>
<li>Feed the worker threads with new temperature data for the next computation cycle. In contrast to <code>std::latch</code>, <code>std::barrier</code> allows us to use the same barrier as many times as we need.</li>
<li>Check whether we have already calculated five times the mean temperature of the entire home and, if so, notify the workers that they need to gracefully stop and exit the program.</li>
</ul>
<p>When a thread<a id="_idIndexMarker550"/> starts<a id="_idIndexMarker551"/> working and it is ready with its computation, it hits the barrier (marker <code>{4}</code>). This is possible because <code>std::barrier</code> exposes a method: <code>void arrive_and_wait()</code>. This call effectively decrements the internal counter of the barrier, which notifies it that the thread has arrived and blocks the thread until the counter hits zero and the barrier’s callback is triggered.</p>
<p>In the following code, you can find the methods responsible for generating example temperature values and calculating the mean temperature value:</p>
<pre class="source-code">
void GetTemperatures(Temperature&amp; temp) {
    std::mt19937 gen{std::random_device{}()};
    // Add normal distribution with mean = 20
    // and standard deviation of 8
    std::normal_distribution&lt;&gt; d{20, 8};
    auto&amp; input_data{std::get&lt;1&gt;(temp)};
    input_data.clear();
    for (auto n{0}; n &lt; 10; ++n) {
        // Add input data
        input_data.emplace_back(d(gen));
    }
}
void ProcessMeasurement(Temperature&amp; temp){
    const auto&amp; values{std::get&lt;1&gt;(temp)};
    auto&amp; mean{std::get&lt;2&gt;(temp)};
    mean = std::reduce(values.begin(), values.end()) /
      values.size();
}</pre> <p>Once we<a id="_idIndexMarker552"/> have <a id="_idIndexMarker553"/>all the code pieces available, let’s see the <code>main</code> method implementation of our program:</p>
<pre class="source-code">
int main() {
    // Init data
    std::ranges::for_each(room_temperatures,
      GetTemperatures);
    std::stop_source message;
    std::barrier measurementBarrier{
        static_cast&lt;int&gt;(room_temperatures.size()),
        [&amp;message]() noexcept {
            // Get all results
            double mean{0.0};
            for (const auto&amp; room_t : room_temperatures) {
                std::cout &lt;&lt; "Mean temperature in "
                          &lt;&lt; std::get&lt;0&gt;(room_t)
                          &lt;&lt; " is " &lt;&lt; std::get&lt;2&gt;(room_t)
                            &lt;&lt; ".\n";
                mean += std::get&lt;2&gt;(room_t);
            }
            mean /= room_temperatures.size();
            std::cout &lt;&lt; "Mean temperature in your home is
              " &lt;&lt; mean &lt;&lt; " degrees Celsius.\n";
            std::cout &lt;&lt; "=======================
              ======================\n";
            // Add new input data
            std::ranges::for_each(room_temperatures,
              GetTemperatures);
            // Make 4 measurements and request stop.
            static unsigned timer{0};
            if (timer &gt;= 3) {
                message.request_stop();
            }
            ++timer;
        }
    };
    std::vector&lt;std::jthread&gt; measurementSensors;
    for (auto&amp; temp : room_temperatures) {
        measurementSensors.emplace_back
          ([&amp;measurementBarrier, &amp;message, &amp;temp](){
            const auto&amp; token = message.get_token();
            while(!token.stop_requested()) {
                ProcessMeasurement(temp);
                measurementBarrier.arrive_and_wait();
            }
        });
    }
    return 0;
}</pre> <p>For the<a id="_idIndexMarker554"/> input<a id="_idIndexMarker555"/> temperature data of our example, we use a random number generator, which produces data with normal distribution. As a result, we get the following output:</p>
<pre class="console">
Mean temperature in living_room is 18.7834.
Mean temperature in bedroom is 16.9559.
Mean temperature in kitchen is 22.6351.
Mean temperature in closet is 20.0296.
Mean temperature in your home is 19.601 degrees Celsius.
=============================================
Mean temperature in living_room is 19.8014.
Mean temperature in bedroom is 20.4068.
Mean temperature in kitchen is 19.3223.
Mean temperature in closet is 21.2223.
Mean temperature in your home is 20.1882 degrees Celsius.
=============================================
Mean temperature in living_room is 17.9305.
Mean temperature in bedroom is 22.6204.
Mean temperature in kitchen is 17.439.
Mean temperature in closet is 20.3107.
Mean temperature in your home is 19.5752 degrees Celsius.
=============================================
Mean temperature in living_room is 19.4584.
Mean temperature in bedroom is 19.0377.
Mean temperature in kitchen is 16.3529.
Mean temperature in closet is 20.1057.
Mean temperature in your home is 18.7387 degrees Celsius.
=============================================</pre> <p>With the <a id="_idIndexMarker556"/>preceding example, we have demonstrated how you can use <a id="_idIndexMarker557"/>synchronization primitives with <code>std::jthread</code> to provide inter-thread synchronization for your program.</p>
<h1 id="_idParaDest-100"><a id="_idTextAnchor100"/>Summary</h1>
<p>In this chapter, we explored several topics related to concurrency and parallelism in C++. We began by discussing the terminology and differences between concurrency and parallelism, including preemption. We then delved into how programs execute on single and multiple processing units, distinguishing between processes and execution threads and briefly exploring communication mechanisms such as pipes, sockets, and shared memory.</p>
<p>In the context of C++, we examined how the language supports concurrency, specifically through the <code>std::thread</code> class and the new <code>std::jthread</code> primitive introduced in C++20. We also discussed the risks associated with race conditions and data races, including an example of a money transfer operation. To avoid these issues, we examined mechanisms such as locks, atomic operations, and memory barriers.</p>
<p>Moving on, we looked closely at the <code>std::jthread</code> class, exploring its functionality and proper usage. Additionally, we learned about a new synchronized stream wrapper delivered in C++20 for printing in concurrent environments. We also covered how to cancel running threads using <code>std::stop_token</code> and how to request a stop to several threads using <code>std::stop_source</code>.</p>
<p>We then shifted our focus to returning results from threads using <code>std::future</code> and <code>std::promise</code>. Additionally, we discussed the use of <code>std::latch</code> and <code>std::barrier</code>, using an example of a temperature station to demonstrate how the latter can be used to synchronize threads.</p>
<p>Overall, we explored a range of topics related to concurrency and parallelism in C++, from basic terminology and concepts to more advanced techniques and mechanisms for avoiding data races and synchronizing threads. But please stay tuned because, in the next chapter, you will get familiar with some mechanisms for IPC that are widely used in software programming.</p>
</div>
</div></body></html>