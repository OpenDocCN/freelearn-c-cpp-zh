<html><head></head><body>
		<div><h1 id="_idParaDest-93"><em class="italics"><a id="_idTextAnchor099"/>Chapter 6</em></h1>
		</div>
		<div><h1 id="_idParaDest-94"><a id="_idTextAnchor100"/>Testing</h1>
		</div>
		<div><h2 id="_idParaDest-95"><a id="_idTextAnchor101"/>Introduction</h2>
			<p>One of my earliest jobs in IT was in software testing. I discovered that developers and testers have separate communities, with separate techniques and bodies of knowledge. I also found that, in some companies, the developers had an antagonistic relationship with the testers: developers resented testers for being proud of poking holes in their hard work. In return, testers resented the slapdash and inconsistent way in which the developers had written and released the software. Of course, neither of these extremist positions was actually grounded in reality.</p>
			<p>This chapter lays out a way of thinking about making software that puts developers and testers in the same position: that of wanting to make a valuable product. It then includes an introduction to the field of systematic software testing, as understood by software testers, and as apparently given little attention by developers.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor102"/>A Philosophy of Testing</h2>
			<p>Imagine plotting the various dimensions of your software: the functionality, performance, user interface, and so on, on a multidimensional chart (for the diagrams in this section, I'll stick to two dimensions; even if you're viewing them on some mad future reader, my graphics tool doesn't support more than that).</p>
			<p>The first thing to notice is that you can't draw a point on <em class="italics">Figure 6.1</em> that represents the "target" product to develop. The most important reason is that the target may not exist. Depending on your philosophical approach to software, there may not be a <em class="italics">true</em> collection of requirements that is universally understood to be the <em class="italics">correct</em> thing to build. Consider the people who are using the software as part of the system the software is supporting, so the "right thing" depends on those people and their interactions with each other. The thing you "should" build depends on the context and varies with time. (Manny Lehman wrote a more complete description of this philosophy, in which he describes software systems embedded in real-world interactions and processes as "E-type" systems (E for <strong class="bold">Evolving</strong>). In exploring the properties of E-type systems, he formulated eight <strong class="bold">laws of software evolution</strong>—<a href="http://en.wikipedia.org/wiki/Lehman’s_laws_of_software_evolution">http://en.wikipedia.org/wiki/Lehman's_laws_of_software_evolution</a>. I find it ironic that these came to be described as laws as if they were intrinsic to nature, when the lesson is that there are no universal truths when it comes to software.)</p>
			<p>What you <em class="italics">could</em> graph are many fuzzy blobs representing various <em class="italics">perceptions</em> of the software: what customers think it does, what customers think it <em class="italics">should</em> do, and what various members of the project team thinks it does. Then there's another blob, representing what the software <em class="italics">actually</em> does.</p>
			<div><div><img alt="" src="img/B15099_06_01.jpg"/>
				</div>
			</div>
			<p><a id="_idTextAnchor103"/></p>
			<h6>Figure 6.1: Software behavior Venn diagram</h6>
			<p>The behavior of a software system and the opinions different people have of what that behavior is or should be are different regions in the space of possible behaviors. Software testing is the practice of identifying these differences so they can be reconciled.</p>
			<p>The various practices that comprise software testing can be seen, alongside some marketing and requirements gathering activities, as part of the effort to catalog these perceptions and the gaps between them. The effort to reconcile these different perceptions and to close the gaps is then not <em class="italics">solely</em> a debugging effort, implying that testers will find problems the developers missed. It's a whole-team effort where debugging is just one of the reconciliation activities. Marketing (changing the customers' perceptions to match the capability of the software), extra-sales engineering (changing the deployment environment to match that expected by the software), and other techniques are all ways to close these gaps.</p>
			<p>With this mindset, testers are not working to "show up" developers; everybody is working to create both a valuable software system, and a common understanding of what that system does. The goal of testing is to identify <em class="italics">opportunities</em> for the project team to exploit.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor104"/>Black and White Boxes</h2>
			<p>One thing that I've found can infuriate developers is when a problem report is written from a black-box perspective – the tester has reported a bug with no other information than what can be discovered through the user interface: "I tried this and it didn't work." I know it's infuriating, because I've been on the receiving end of these reports.</p>
			<p>From the perspective outlined in the previous section, though, black-box test reports are the most valuable reports. (Here, "black-box" refers to the format of the test report, where the inputs to the software are described along with the difference between the expected and the actual output. In test <em class="italics">planning</em>, testers use the phrases "black-box" and "white-box" to refer to whether the software's source code was used in designing the tests; such tests are still likely to be executed via the software's interfaces.) Anything that doesn't work as expected via the UI represents one of the gaps that was described: a gap between the customer's perception of what the software does and the capability it actually demonstrates.</p>
			<p>The reason it's often frustrating to receive this kind of report is that it can be incredibly difficult and time-consuming to replicate the reported issue and to isolate the cause. Often, this process takes longer than fixing the problem when it's been located; why are the testers giving you so much extra work when they could be using white-box techniques, using internal knowledge of the software, to test components in isolation and go straight to where the bug is?</p>
			<p>This is another example of one of those perception gaps. Because we spend all of our time working with methods and functions that group instructions into sequences of 10 or so, the natural view the programmer has of the system is in terms of those instructions and methods. Black-box problem reports bear a strong resemblance to the old puzzle game of black-box, where you shine a light from one edge and see that it gets absorbed or deflected. You want to be thinking about mirrors and other features of the box's innards, but you're forced to infer them from what happens to the light beams.</p>
			<p>The tester, meanwhile, is acting on behalf of the customer and therefore has no emotional attachment toward the guts of the system. The customers will think "I have <em class="italics">this</em> problem, and I believe the software can help me to solve it if I do <em class="italics">that</em>" – a naturally black-box view that only interacts with the external interface of the software. In other words, they (and the testers on their behalf) have no opinion on whether a particular method returns <code>true</code> or <code>false</code> when the parameter is <code>3</code>; they care whether the software's output is a useful solution to the problem expressed as its input. Remember that the tester is trying to find differences between the expected and the actual behavior; discovering their causes is something that only needs to be done once the team has decided a code fix is appropriate.</p>
			<h3 id="_idParaDest-98"><a id="_idTextAnchor105"/>Shining Light on The Black-Box</h3>
			<p>Evidently then, if the effort of locating and diagnosing a code problem is only needed when it's decided that the code must be fixed, it's the programmer and not the tester who needs to go from a black-box problem definition to a root cause. Like it or not, it's the developer's responsibility to isolate the fault – whether or not the testers are able to help out.</p>
			<p>Obviously, it would be possible to isolate the fault by going through the reproduction steps in the problem report, stepping through the code in a debugger from start to finish until the problem shows itself. That's neither very fast, nor very enjoyable though. It'd be much quicker to diagnose problems if you could hypothesize the likely cause and rapidly demonstrate whether or not that hypothesis is valid.</p>
			<p>This is where component and integration testing become useful, but as part of a larger picture: knowing (or being able to find out) the conditions under which the various modules that comprise the whole system work successfully, and whether those conditions are being satisfied for each of the modules taking part in the buggy behavior.</p>
			<p>Help in constructing these hypotheses can come from the software's behavior. A common device used in problem diagnosis is a configurable level of logging output: messages are tagged with differing levels of severity and users choose what levels get recorded in the logs. When reproducing a bug, the logging is set to show everything, giving a clearer view of the flow of the code. The downsides to this approach depend on the specific application but can include noise from unrelated parts of the software, and changes to the overall behavior if the problem is timing related.</p>
			<p>Problem diagnosis also benefits from having a scriptable interface onto an application; for example, a command-line or AppleScript interface. The first benefit is that it gives you a second UI onto the same functionality, making it possible to quickly determine whether a problem is in the UI or the application logic. Secondly, it gives you a repeatable and storable test that can be added to a regression test suite. Finally, such interfaces are usually much simpler than GUIs, so only the code that's relevant to the problem is exercised, making isolation a quicker task.</p>
			<p>Otherwise, going from observable behavior to likely cause is largely still a matter of intuition and system-specific knowledge. Knowing which modules are responsible for which parts of the application's external behavior (or being able to find out – see <em class="italics">Chapter 8, Documentation</em>) and reasoning about which is most likely to have caused the problem cuts down debugging time greatly. I therefore prefer to organize my projects along those lines, so that all of the code that goes into one feature is in one group or folder, and is only broken out into another folder when it gets shared with another feature. <strong class="bold">Eclipse's Mylyn task manager</strong>—<a href="http://eclipse.org/mylyn/start/">http://eclipse.org/mylyn/start/</a> is a richer way of providing a problem-specific view of your project.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor106"/>Test Case Design</h2>
			<p>Random, undirected testing (otherwise known as playing about with the user interface) is an inefficient way to test software. A long-established technique (documented in Myer's <strong class="bold">The Art of Software Testing</strong>—<a href="http://books.google.co.uk/books/about/The_art_of_software_testing.html?id=86rz6UExDEEC&amp;redir_esc=y">http://books.google.co.uk/books/about/The_art_of_software_testing.html?id=86rz6UExDEEC&amp;redir_esc=y</a>) seeks to cover all possible conditions with the minimum number of tests. For each input variable or state, the tester discovers the ranges of values that represent distinct conditions in the software. As an example, an age field may have the following ranges:</p>
			<ul>
				<li>[0,18[ : child</li>
				<li>[18, 150[ : adult</li>
				<li>0[ : too small</li>
				<li>[150 : too large</li>
				<li>NaN : not a number</li>
			</ul>
			<p>The tester then tabulates these various ranges for all the inputs and creates the minimum number of tests required to exercise all of them. This is called <strong class="bold">equivalence partitioning</strong>: the behavior at age 36 and the behavior at age 38 are probably the same, so it's reasonable to expect that if you test one of them, the residual risk associated with not testing the other is small – specifically, smaller than the cost of also having <em class="italics">that</em> test.</p>
			<p>In fact, testers will not quite produce the minimum number of tests; they will probably choose to pay extra attention to boundary values (maybe writing tests that use the ages 17, 18, and 19). Boundaries are likely to be a fecund source of ambiguity: did everybody understand the phrases "up to 18" and "over 18" to mean the same thing? Does the software use a rounding scheme appropriate to age in years?</p>
			<p>Such a technique was first created with the assumption that the "true" behavior of a software system was to be found in its functional specification; that all tests could be derived by applying the above analysis to the functional specification; and that any difference between observed behavior and the specification is a bug. According to the philosophy of testing described at the beginning of the chapter, these assumptions are not valid: even if a functional specification exists, it is as much an incomplete and ambiguous description of the software system as any other. The technique described here is still useful, as ferreting out these ambiguities and misunderstandings is a part of the value testers bring to a project. It just means that their role has grown from verification to include being a (verbal) language lawyer.</p>
			<h3 id="_idParaDest-100"><a id="_idTextAnchor107"/>Code-Directed Tests</h3>
			<p>Remembering that the phrase "white-box testing" has contextual meaning, I've chosen to refer to code-directed tests. This means tests that are <em class="italics">designed</em> with reference to the application's source code, however they're run.</p>
			<p>When testers design these tests, they typically have one of two goals: either ensuring 100% statement coverage or 100% branch coverage. Maximizing branch coverage will yield more tests. Consider this function:</p>
			<pre>    void f(int x)
    {
        if (x&gt;3)
        {
            // do some work...
        }
    }</pre>
			<p>A tester who wants to execute every statement need only test the case where <code>x</code> is greater than 3; a tester who wants to execute every branch will need to consider the other case too (and a diligent tester will try to discover what people think will happen when <code>x</code> <em class="italics">is equal to</em> 3).</p>
			<p>Because the tests are derived from the source code, which by definition is a format suitable for manipulation by software tools, tool support is right for code-directed test design. Plenty of platforms have tools for measuring and reporting the code coverage. There are even automatic test-case generators that can ensure 100% branch coverage; a good example is the <strong class="bold">Klee</strong>—<a href="http://klee.llvm.org/">http://klee.llvm.org/</a>, symbolic virtual machine.</p>
			<h3 id="_idParaDest-101"><a id="_idTextAnchor108"/>Testing For Non-Functional Requirements</h3>
			<p>In principle, testing the non-functional properties of a system should be the same as testing its functional behavior. You find out what the system does, what various parties think it should do, and compare those. In practice, non-functional requirements can be tacit (someone might want the system to work in a particular way, but they either doesn't know how to say that or considers it too obvious to make explicit) or defined in ambiguous terms ("the system must be fast").</p>
			<p>The first step in addressing these problems is to get them into discussion, so testing these aspects of the software and reporting the results is a good idea. As an example, the customer might not have expressed any system requirements because they don't know it's important; a report saying "the application doesn't run properly on 32-bit systems and requires at least Service Pack 2" will uncover whether or not that's an issue, leading to a better mutual understanding of the system.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor109"/>Automate All The Things</h2>
			<p>Testing software and writing software share the following property in common: it's not <em class="italics">doing</em> them that's beneficial, it's <em class="italics">having done</em> them. Having access to finished, working software is a useful thing, so a project that's in progress is only as valuable as one that hasn't started (although the in-progress one has already cost more). Therefore, as much of the testing procedure itself should be automated as possible to let testers get on with the more creative tasks of defining tests and discovering/reporting issues.</p>
			<p>This automation starts with setting up the test environment into a known, initial state. Virtual machines are increasingly being used for this task (at least in server and desktop environments) because they offer a quick way to create an environment of known configuration into which the test harness and the software it's testing can be deployed. At the end of a test run, the state of the virtual machine is reset and it's ready to start again.</p>
			<p>Automated driving of the software under test can be done through dedicated scripting interfaces, as already described, but these do not test the behavior of the UI buttons and widgets. Developers tend not to like automatic GUI driving tests as there's a finite chance the test will fail due to unimportant properties of the GUI changing, such as the location or design of a control. There are two things to notice here:</p>
			<ul>
				<li>The location and design of a control <em class="italics">are</em> important; if a test driver cannot find the same control between two versions of the software, there's a likelihood that customers won't be able to either.</li>
				<li>While there's a risk of such tests failing due to innocuous changes, if you drop the tests completely, then there's a risk that you'll ship undetected problems with the GUI. These conflicting risks must be resolved. The impact of the test failure scenario is that, on those occasions, when the GUI is updated, there will be a brief flurry of false negatives from the test suite until someone realizes what's happened and spends some time updating the tests. The impact of the broken GUI scenario is that your software <em class="italics">definitely</em> won't do what your customers expect, which will lead to dissatisfaction, low reviews, maybe loss of revenue, <em class="italics">and</em> someone will have to spend some time releasing a fixed version of the software. The second scenario seems a lot less desirable than the first, so accepting the cost of keeping the tests up to date is the better choice.</li>
			</ul>
			<p>Automation is particularly helpful if you have a "smoke test" procedure for determining whether a build is stable enough to be subjected to further testing or treated as a release candidate. Going through the smoke test suite is almost the definition of repetitive drudgery, so give it to a computer to do. Then, developers can go back to planning and working on the next build, and testers can work on providing valuable tests. Additionally, automated smoke test suites will be faster than manual smoke tests, so the build can be subjected to greater rigor. You could go as far as to add all automatic tests to the smoke test battery, so that each build contains no known regressions over previous builds.</p>
			<p>Some teams <em class="italics">allow a build to be deployed automatically</em>—<a href="https://github.com/blog/1241-deploying-at-github">https://github.com/blog/1241-deploying-at-github</a> as soon as it passes the automatic tests.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor110"/>Getting Someone Else In</h2>
			<p>Much of the literature on testing makes reference to the fact that an external tester has less emotional attachment to the software under test than the developer, will be more dispassionate in their evaluation of that software, and therefore will uncover more problems. The fact is that a developer <em class="italics">can</em> systematically test their own software, but the inclination to do so is often lacking (particularly as we tend to see writing code as the valuable thing we do, and everything else as overhead). Getting some form of external input, whether it's a third-party tester or a consultant to examine whether our own testing covered the relevant cases, is a valuable check on our work.</p>
			<p>Notice that beta testers are <em class="italics">not</em> likely to provide such systematic reviews. Typically, a beta tester is an interested user who can be given access to the software for free while it's still under development. They are likely to approach testing in a random fashion, and to only use the parts of the software that are of interest to them. Beta testing is useful for discovering the gap between how you think software will be used and how you expect it to be used, but statistical techniques must be employed in analyzing reports from beta testers. The temptation to change something reported by one beta tester because "the customer is always right" is high but remember that the other <code>n-1</code> testers did not report the same problem, and that <em class="italics">none</em> of them has tested the alternative.</p>
			<p><em class="italics">On one project I worked on, the thing we called "beta testing" was really customer environment testing. We gave the software to customers in the hope that their setups would be different from ours and might uncover problems that were configuration specific. Being large businesses, those customers did not actually test the beta versions on their real networks but in "different" environments set up expressly for testing. Therefore, the team still did not know whether the software worked in the customers' setups.</em></p>
			<p>Getting external involvement is also useful when the testing procedures require specialized knowledge. Security testing, performance testing, and testing localized versions of the software are situations where this applies.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor111"/>Other Benefits Of Testing</h2>
			<p>I have shown throughout this chapter that software testing has an important role to play in identifying the gaps between your software's actual behavior, apparent behavior, and expected behavior among the various people who interact with it. Additionally, I've described the benefits of using automated tests as a regression suite, so that a problem fixed once will be detected if it's accidentally reintroduced. There are other benefits that result from investing in testing your software, too.</p>
			<h3 id="_idParaDest-105"><a id="_idTextAnchor112"/>Accessibility</h3>
			<p>Traditionally, in the world of software, accessibility (or a11y, after the eleven letters that have been elided) refers to making a software's interface usable by people with certain disabilities or impairments. Often, it's narrowly applied to considerations for just the visually impaired.</p>
			<p>Indeed, an automated user interface test suite can improve the accessibility of an application. Some UI test frameworks (including <strong class="bold">Apple's UI Automation</strong>—<a href="http://developer.apple.com/library/ios/#documentation/DeveloperTools/Reference/UIAutomationRef/_index.html">http://developer.apple.com/library/ios/#documentation/DeveloperTools/Reference/UIAutomationRef/_index.html</a> and <strong class="bold">Microsoft's UI Automation</strong>—<a href="http://msdn.microsoft.com/en-us/library/ms747327.aspx">http://msdn.microsoft.com/en-us/library/ms747327.aspx</a>) use the metadata supplied for screen readers and other assistive devices to find and operate the controls on an application's display. Testing at this level ensures that the tests can still find controls that have had their labels changed or have been moved on the screen, which image-detection-based test frameworks have difficulty coping with.</p>
			<p>Some developers who have difficulty arguing for making their products accessible on other a11y-grounds find that testing is a handy device for doing it anyway. In my experience, first the ethical approach is taken ("it's the right thing to do"), then the legal approach ("are we bound by the Disability Discrimination Act?"), then the financial approach ("we'd get more customers – ones that our competitors probably aren't selling to"). Even <strong class="bold">vociferous promoters of accessible software</strong> (<a href="http://mattgemmell.com/2010/12/19/accessibility-for-iphone-and-ipad-apps/">http://mattgemmell.com/2010/12/19/accessibility-for-iphone-and-ipad-apps/</a>) admit that the financial justification is shaky: <em class="italics">I'm not going to try to make a convincing commercial argument for supporting accessibility; I'm not even sure that I could</em>—<a href="http://mattgemmell.com/2012/10/26/ios-accessibility-heroes-and-villains/">http://mattgemmell.com/2012/10/26/ios-accessibility-heroes-and-villains/</a>). Managers tend to love reduced cost and risk: automating user interface tests, then keeping them as part of a regression battery can provide these two reductions. </p>
			<h3 id="_idParaDest-106"><a id="_idTextAnchor113"/>Structure</h3>
			<p>From unit tests to system tests, whatever level your tests are operating at, the object under test must be extractable from your application to execute in the test harness. This requirement enforces a separation of concerns: at each level, modules must be capable of operating in isolation or with external dependencies substituted. It also strongly suggests a single responsibility for each module: if you want to find the tests for the logging facility, it's easier to look in the "Logging Tests" fixture than the "Amortization Calculation (also does logging, BTW)" fixture.</p>
			<p>Admittedly, such a rigorous separation of concerns is not <em class="italics">always</em> the appropriate solution, but it <em class="italics">usually</em> is until you discover otherwise. It will simplify many aspects of development: particularly the assignment of work to different developers. If each problem is solved in an entirely separate module, then different programmers need only agree on the interfaces between those modules and can build the internals as they see fit. If they need combining for some reason later, then the fact that you <em class="italics">have</em> tested them as separate standalone components lends confidence to their integration, even if you have to remove some of the regression tests to get everything to work.</p>
			<p>I've seen this case primarily in optimization for performance. As I was writing the visualization for a particular feature, another developer wrote the functionality. Those pieces each worked in isolation, but the interface made them too slow. We took the decision to couple them together, which made them fast but introduced tight dependencies between the modules. Certain things that could previously be tested in isolation then required the other parts to be present; but we <em class="italics">had</em> tested them in isolation, so had some idea of how they worked and what was assumed.</p>
		</div>
	</body></html>