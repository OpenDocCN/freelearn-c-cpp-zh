<html><head></head><body>
		<div><h1 id="_idParaDest-151"><em class="italic"><a id="_idTextAnchor158"/>Chapter 11</em>: Gearing Up with Support Utilities</h1>
			<p>In the previous chapter, we learned the basics of <strong class="bold">Low-Level Virtual Machine</strong> (<strong class="bold">LLVM</strong>) <strong class="bold">intermediate representation</strong> (<strong class="bold">IR</strong>)—the target-independent intermediate representations in LLVM—and how to inspect and manipulate this with C++ <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>). These are the core techniques for doing program analysis and transformation in LLVM. In addition to those skill sets, LLVM also provides many support utilities to improve compiler developers' productivity when working with LLVM IR. We are going to cover those topics in this chapter.</p>
			<p>A compiler is a complex piece of software. It not only needs to handle thousands of different cases— including input programs with different shapes and a wide variety of target architectures—but the <strong class="bold">correctness</strong> of a compiler is also an important topic: namely, the compiled code needs to have the same behavior as the original one. LLVM, a large-scale compiler framework (and probably one of the biggest), is not an exception. </p>
			<p>To tackle these complexities, LLVM has provided a crate of gadgets to improve the development experience. In this chapter, we are going to show you how to gear up to use those tools. The utilities covered here can assist you in diagnosing problems that occur from the LLVM code you are developing. This includes more efficient debugging, error handling, and profiling abilities; for instance, one of the tools can collect statistical numbers on key components—such as the number of basic blocks being processed by a specific Pass—and automatically generate a summary report. Another example is LLVM's own error-handling framework, which prevents as many unhandled errors (a common programming mistake) as possible.</p>
			<p>Here is a list of the topics we are going to cover in this chapter:</p>
			<ul>
				<li>Printing diagnostic messages</li>
				<li>Collecting statistics</li>
				<li>Adding time measurements</li>
				<li>Error-handling utilities in LLVM</li>
				<li>Learning about the <code>Expected</code> and <code>ErrorOr</code> classes</li>
			</ul>
			<p>With the help of these utilities, you will have a better time debugging and diagnosing the LLVM code, letting you focus on the core logic you want to implement with LLVM.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor159"/>Technical requirements</h1>
			<p>In this section, we are also going to use LLVM Pass as the platform to show different API usages. Therefore, please make sure you have built the <code>opt</code> command-line tool, as follows:</p>
			<pre>$ ninja opt</pre>
			<p>Note that some of the content in this chapter only works with a <strong class="bold">debug build</strong> version of LLVM. Please check the first chapter, <a href="B14590_01_Final_JC_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Saving Resources When Building LLVM,</em> for a recap on how to build LLVM in debug mode.</p>
			<p>You can also go back to <a href="B14590_09_Final_JC_ePub.xhtml#_idTextAnchor127"><em class="italic">Chapter 9</em></a>, <em class="italic">Working with PassManager and AnalysisManager,</em> if you are not sure how to create a new LLVM Pass.</p>
			<p>The sample code for this chapter can be found here: </p>
			<p><a href="https://github.com/PacktPublishing/LLVM-Techniques-Tips-and-Best-Practices-Clang-and-Middle-End-Libraries/tree/main/Chapter11">https://github.com/PacktPublishing/LLVM-Techniques-Tips-and-Best-Practices-Clang-and-Middle-End-Libraries/tree/main/Chapter11</a></p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor160"/>Printing diagnostic messages</h1>
			<p>In software development, there are many ways to diagnose a bug—for instance, using a debugger, inserting a sanitizer <a id="_idIndexMarker568"/>into your program (to catch invalid memory access, for example), or simply using one of the simplest yet most effective ways: adding <em class="italic">print statements</em>. While the last option doesn't sound really smart, it is actually pretty useful in many cases where other options cannot unleash their full potential (for example, release mode binaries with poor debug information quality or multithread programs).</p>
			<p>LLVM provides a small utility that not only helps you to print out debug messages but also <em class="italic">filters</em> which messages to show. Let's say we have an LLVM Pass, <code>SimpleMulOpt</code>, which replaces multiplication by power-of-two constants with left-shifting operations (which is what we did in the last section of the previous chapter, <em class="italic">Processing LLVM IR</em>). Here is part of its <code>run</code> method:</p>
			<pre>PreservedAnalyses
SimpleMulOpt::run(Function &amp;F, FunctionAnalysisManager &amp;FAM) {
  for (auto &amp;I : <strong class="bold">instructions</strong>(F)) {
    if (auto *BinOp = dyn_cast&lt;<strong class="bold">BinaryOperator</strong>&gt;(&amp;I) &amp;&amp;
        BinOp-&gt;getOpcode() == Instruction::Mul) {
      auto *<strong class="bold">LHS</strong> = BinOp-&gt;getOperand(0),
           *<strong class="bold">RHS</strong> = BinOp-&gt;getOperand(1);
      // `BinOp` is a multiplication, `LHS` and `RHS` are its
      // operands, now trying to optimize this instruction…
      …
    }
  }
  …
}</pre>
			<p>The preceding code iterates <a id="_idIndexMarker569"/>through all instructions in the given function before looking for instructions that represent arithmetic multiplication. If there are any, the Pass will then work with the <code>LHS</code> and <code>RHS</code> operands (which appear in the rest of the code—these are not shown here).</p>
			<p>Let's assume that we want to print out the operand variables during our development. The most naïve way will be by using our old friend <code>errs()</code>, which streams arbitrary messages to <code>stderr</code>, as shown in the following code snippet:</p>
			<pre>// (extracted from the previous snippet)
…
auto *LHS = BinOp-&gt;getOperand(0),
     *RHS = BinOp-&gt;getOperand(1);
<strong class="bold">errs()</strong> &lt;&lt; "Found a multiplication with operands ";
LHS-&gt;<strong class="bold">printAsOperand(errs())</strong>;
<strong class="bold">errs()</strong> &lt;&lt; " and ";
RHS-&gt;<strong class="bold">printAsOperand(errs())</strong>;
…</pre>
			<p>The <code>printAsOperand</code> used in the preceding code snippet prints the textual representation of a <code>Value</code> to the given stream (<code>errs()</code>, in this case).</p>
			<p>Everything looks normal, except the <a id="_idIndexMarker570"/>fact that these messages will be printed out anyway even in a production environment, which is not what we want. Either we need to remove these codes before we ship our products, adding some macro guard around these codes (for example, <code>#ifndef NDEBUG</code>), or we can use the debug utility provided by LLVM. Here is an example of this:</p>
			<pre>#include "<strong class="bold">llvm/Support/Debug.h</strong>"
#define <strong class="bold">DEBUG_TYPE "simple-mul-opt"</strong>
…
auto *LHS = BinOp-&gt;getOperand(0),
     *RHS = BinOp-&gt;getOperand(1);
<strong class="bold">LLVM_DEBUG</strong>(<strong class="bold">dbgs()</strong> &lt;&lt; "Found a multiplication with operands ");
LLVM_DEBUG(LHS-&gt;printAsOperand(<strong class="bold">dbgs()</strong>));
LLVM_DEBUG(dbgs() &lt;&lt; " and ");
LLVM_DEBUG(RHS-&gt;printAsOperand(dbgs()));
…</pre>
			<p>The preceding code is basically doing the following three things:</p>
			<ul>
				<li>Replacing any usage of <code>errs()</code> with <code>dbgs()</code>. These two streams are basically doing the same thing, but the latter one will add a nice banner (<code>Debug Log Output</code>) to the output message.</li>
				<li>Wrapping all lines related to debug printing with the <code>LLVM_DEBUG(…)</code> macro function. The use of this macro ensures that the enclosing line is only compiled in development mode. It also encodes the debug message category, which we will introduce shortly.</li>
				<li>Before using any <code>LLVM_DEBUG(…)</code> macro functions, please make sure you define <code>DEBUG_TYPE</code> to the desired debug category string (<code>simple-mul-opt</code>, in this case).</li>
			</ul>
			<p>In addition to the aforementioned code modification, we also need to use an additional command-line flag, <code>-debug</code>, with <code>opt</code> to print those debug messages. Here is an example of this:</p>
			<pre>$ opt -O3 <strong class="bold">-debug</strong> -load-pass-plugin=… …</pre>
			<p>But then, you'll find the <a id="_idIndexMarker571"/>output to be pretty noisy. There are tons of debug messages from <em class="italic">other</em> LLVM Passes. In this case, we're only interested in the messages from our Pass.</p>
			<p>To filter out unrelated messages, we can use the <code>-debug-only</code> command-line flag. Here is an example of this:</p>
			<pre>$ opt -O3 <strong class="bold">-debug-only=simple-mul-opt</strong> -load-pass-plugin=… …</pre>
			<p>The value after <code>-debug-only</code> is the <code>DEBUG_TYPE</code> value we defined in the previous code snippet. In other words, we can use <code>DEBUG_TYPE</code> defined by each Pass to filter the desired debug messages. We can also select <em class="italic">multiple</em> debug categories to print. For instance, check out the following command:</p>
			<pre>$ opt -O3 <strong class="bold">-debug-only=sroa,simple-mul-opt</strong> -load-pass-plugin=… …</pre>
			<p>This command not only prints debug messages from our <code>SimpleMulOpt</code> Pass, but also those coming from the <code>SROA</code> Pass—an LLVM Pass included in the <code>O3</code> optimization pipeline.</p>
			<p>In addition to defining a single debug category (<code>DEBUG_TYPE</code>) for an LLVM Pass, you are in fact free to use as many categories as you like inside a Pass. This is useful, for instance, when you want to use separate debug categories for different parts of a Pass. For example, we can use separate categories for each of the operands in our <code>SimpleMulOpt</code> Pass. Here is how we can do this:</p>
			<pre>…
<strong class="bold">#define DEBUG_TYPE "simple-mul-opt"</strong>
auto *LHS = BinOp-&gt;getOperand(0),
     *RHS = BinOp-&gt;getOperand(1);
LLVM_DEBUG(dbgs() &lt;&lt; "Found a multiplication instruction");
<strong class="bold">DEBUG_WITH_TYPE</strong>("<strong class="bold">simple-mul-opt-lhs</strong>",
               LHS-&gt;printAsOperand(dbgs() &lt;&lt; "LHS operand: "));
<strong class="bold">DEBUG_WITH_TYPE</strong>("<strong class="bold">simple-mul-opt-rhs</strong>",
               RHS-&gt;printAsOperand(dbgs() &lt;&lt; "RHS operand: "));
…</pre>
			<p><code>DEBUG_WITH_TYPE</code> is a <a id="_idIndexMarker572"/>special version of <code>LLVM_DEBUG</code>. It executes code at the second argument, with the first argument as the debug category, which can be different from the currently defined <code>DEBUG_TYPE</code> value. In the preceding code snippet, in addition to printing <code>Found a multiplication instruction</code> using the original <code>simple-mul-opt</code> category, we are using <code>simple-mul-opt-lhs</code> to print messages <a id="_idIndexMarker573"/>related to the <code>simple-mul-opt-rhs</code> to print messages for the other operand. With this feature, we can have a finer granularity to select debug message categories via the <code>opt</code> command.</p>
			<p>You have now learned how to use the utility provided by LLVM to print out debug messages in the development environment only, and how to filter them if needed. In the next section, we are going to learn how to collect key statistics while running an LLVM Pass.</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor161"/>Collecting statistics</h1>
			<p>As mentioned in the previous section, a compiler <a id="_idIndexMarker574"/>is a complex piece of <a id="_idIndexMarker575"/>software. Collecting <strong class="bold">statistical numbers</strong>—for example, the number of basic blocks processed by a specific optimization—is one of the easiest and most efficient ways to get a quick portrait on the runtime behaviors of a compiler.</p>
			<p>There are several ways to collect statistics in LLVM. In this section, we are going to learn three of the most common and useful options for doing this, and these methods are outlined here:</p>
			<ul>
				<li>Using the <code>Statistic</code> class</li>
				<li>Using an optimization remark</li>
				<li>Adding time measurements</li>
			</ul>
			<p>The first option is a general <a id="_idIndexMarker576"/>utility that collects statistics via simple counters; the second option is specifically designed to profile <em class="italic">compiler optimizations</em>; and the last option is used for collecting timing information in the compiler.</p>
			<p>Let's start with the first one.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor162"/>Using the Statistic class</h2>
			<p>In this section, we are going to demonstrate <a id="_idIndexMarker577"/>new features by amending them to the <code>SimpleMulOpt</code> LLVM Pass from the previous section. First, let's assume that we don't only want to print out the operand <code>Value</code> from multiplication instructions but <a id="_idIndexMarker578"/>that we also want to <em class="italic">count</em> how many multiplication instructions have been processed by our Pass. First, let's try to implement this feature using the <code>LLVM_DEBUG</code> infrastructure we just learned about, as follows:</p>
			<pre>#define DEBUG_TYPE "simple-mul-opt"
PreservedAnalyses
SimpleMulOpt::run(Function &amp;F, FunctionAnalysisManager &amp;FAM) {
  <strong class="bold">unsigned NumMul = 0</strong>;
  for (auto &amp;I : instructions(F)) {
    if (auto *BinOp = dyn_cast&lt;BinaryOperator&gt;(&amp;I) &amp;&amp;
        BinOp-&gt;getOpcode() == Instruction::Mul) {
      <strong class="bold">++NumMul;</strong>
      …
    }
  }
  <strong class="bold">LLVM_DEBUG(dbgs() &lt;&lt; "Number of multiplication: " &lt;&lt; NumMul);</strong>
  …
}</pre>
			<p>This approach seems <a id="_idIndexMarker579"/>pretty straightforward. But it comes with a drawback—the statistical numbers we are interested in are mixed with other debug messages. We need to take additional actions to parse or filter the value we want because although you might argue that these problems could be <a id="_idIndexMarker580"/>tackled by using a separate <code>DEBUG_TYPE</code> tag for each counter variable, when the number of counter variables increases, you might find yourself creating lots of redundant code.</p>
			<p>One elegant solution is to use the <code>Statistic</code> class (and related utilities) provided by LLVM. Here is a version rewritten using this solution:</p>
			<pre>#include "llvm/ADT/Statistic.h"
#define <strong class="bold">DEBUG_TYPE</strong> "simple-mul-opt"
<strong class="bold">STATISTIC(NumMul, "Number of multiplications processed");</strong>
PreservedAnalyses
SimpleMulOpt::run(Function &amp;F, FunctionAnalysisManager &amp;FAM) {
  for (auto &amp;I : instructions(F)) {
    if (auto *BinOp = dyn_cast&lt;BinaryOperator&gt;(&amp;I) &amp;&amp;
        BinOp-&gt;getOpcode() == Instruction::Mul) {
      <strong class="bold">++NumMul;</strong>
      …
    }
  }
  …
}</pre>
			<p>The preceding code snippet shows the usage of <code>Statistic</code>, calling the <code>STATISTIC</code> macro function to create a <code>Statistic</code> type variable (with a textual description) and simply using it like a normal integer counter variable.</p>
			<p>This solution only <a id="_idIndexMarker581"/>needs to modify a few lines in the original code, plus it collects all counter values and prints them in a table view at the end of the optimization. For example, if you run the <code>SimpleMulOpt</code> Pass using the <code>-stats</code> flag with <code>opt</code>, you will get the following output:</p>
			<pre>$ opt <strong class="bold">-stats</strong> –load-pass-plugin=… …
===-------------------------------===
      … Statistics Collected …
===-------------------------------===
<strong class="bold">87</strong> simple-mul-opt - Number of multiplications processed
$</pre>
			<p><code>87</code> is the number of <a id="_idIndexMarker582"/>multiplication instructions processed in <code>SimpleMulOpt</code>. Of course, you are free to add as many <code>Statistic</code> counters as you want in order to collect different statistics. If you run more than one Pass in the pipeline, all of the statistical numbers will be presented in the same table. For instance, if we add another <code>Statistic</code> counter into <code>SimpleMulOpt</code> to collect a number of <code>none-power-of-two constant operands</code> from the multiplication instructions and run <a id="_idIndexMarker583"/>the Pass with <strong class="bold">Scalar Replacement of Aggregates</strong> (<strong class="bold">SROA</strong>), we can get an output similar to the one shown next:</p>
			<pre>$ opt -stats –load-pass-plugin=… <strong class="bold">--passes="sroa,simple-mult-opt"</strong> …
===-------------------------------===
      … Statistics Collected …
===-------------------------------===
94  simple-mul-opt - Number of multiplications processed
87  simple-mul-opt - Number of none-power-of-two constant operands
100 sroa           - Number of alloca partition uses rewritten
34  sroa           - Number of instructions deleted
…
$</pre>
			<p>The second <a id="_idIndexMarker584"/>column in the preceding code snippet is the name of the origin Pass, which is designated by the <code>DEBUG_TYPE</code> value defined prior to any calls to <code>STATISTIC</code>.</p>
			<p>Alternatively, you can <a id="_idIndexMarker585"/>output the result in <code>-stats-json</code> flag to <code>opt</code>. For example, look at the following code snippet:</p>
			<pre>$ opt -stats <strong class="bold">-stats-json</strong> –load-pass-plu<a id="_idTextAnchor163"/>gin=… …
{
        <strong class="bold">"simple-mul-opt.NumMul": 87</strong>
}
$</pre>
			<p>In this JSON format, instead of <a id="_idIndexMarker586"/>printing statistic values with a textual description, the field name of a statistic entry has this format: <code>"&lt;Pass name&gt;.&lt;Statistic variable name&gt;"</code> (the Pass name here is also the value of <code>DEBUG_TYPE</code>). Furthermore, you can print statistic results (either in default or JSON format) into a file using the <code>-info-output-file=&lt;file name&gt;</code> command-line option. The following code snippet shows an example of this:</p>
			<pre>$ opt -stats -stats-json <strong class="bold">-info-output-file=my_stats.json</strong> …
$ cat <strong class="bold">my_stats.json</strong>
{
        "simple-mul-opt.NumMul": 87
}
$</pre>
			<p>You have now <a id="_idIndexMarker587"/>learned how to collect simple statistic values <a id="_idIndexMarker588"/>using the <code>Statistic</code> class. In the next section, we are going to learn a statistic collecting method that is unique to compiler optimization.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor164"/>Using an optimization remark</h2>
			<p>A typical compiler optimization usually <a id="_idIndexMarker589"/>consists of two stages: <em class="italic">searching</em> for the desired patterns from the input code, followed by <em class="italic">modifying</em> the code. Take our <code>SimpleMulOpt</code> Pass as an example: the first stage is to look for multiplication instructions (<code>BinaryOperator</code> with the <code>Instruction::Mul</code> <code>IRBuilder::CreateShl(…)</code> and replace all old usages of multiplication instructions with these.</p>
			<p>There are many cases, however, where the optimization algorithm simply "bails out" during the first stage due to <em class="italic">infeasible</em> input code. For example, in <code>SimpleMulOpt</code>, we are looking for a multiplication instruction, but if the incoming instruction is not <code>BinaryOperator</code>, the Pass will not proceed to the second stage (and continue on to the next instruction). Sometimes, we want to know the <em class="italic">reason</em> behind this bailout, which can help us to improve the optimization algorithm or diagnose incorrect/suboptimal <a id="_idIndexMarker591"/>compiler optimization. LLVM provides a nice utility called an <strong class="bold">optimization remarks</strong> to collect and report this kind of bailout (or any kind of information) occurring in optimization Passes.</p>
			<p>For example, let's assume we have the following input code:</p>
			<pre>int foo(int *a, int N) {
  int <strong class="bold">x = a[5]</strong>;
  for (int i = 0; i &lt; N; <strong class="bold">i += 3</strong>) {
    a[i] += 2;
    <strong class="bold">x = a[5];</strong>
  }
  return x;
}</pre>
			<p>Theoretically, we <a id="_idIndexMarker592"/>can use <strong class="bold">loop-invariant code motion</strong> (<strong class="bold">LICM</strong>) to optimize this code into an equivalent code base such as this one:</p>
			<pre>int foo(int *a, int N) {
  for (int i = 0; i &lt; N; i += 3) {
    a[i] += 2;
  }
  <strong class="bold">return a[5];</strong>
}</pre>
			<p>We can do this as the <a id="_idIndexMarker593"/>fifth array element, <code>a[5]</code>, never changed its value inside the loop. However, if we run LLVM's LICM Pass over the <a id="_idIndexMarker594"/>original code, it fails to perform the expected optimization.</p>
			<p>To diagnose this problem, we can invoke the <code>opt</code> command with an additional option: <code>--pass-remarks-output=&lt;filename&gt;</code>. The filename will be a <strong class="bold">YAML Ain't Markup Language</strong> (<strong class="bold">YAML</strong>) file in <a id="_idIndexMarker595"/>which optimization remarks print out the possible reasons why LICM failed to optimize. Here is an example of this:</p>
			<pre>$ opt -licm input.ll <strong class="bold">–pass-remarks-output=licm_remarks.yaml</strong> …
$ cat licm_remarks.yaml
…
--- <strong class="bold">!Missed</strong>
Pass:            licm
Name:            LoadWithLoopInvariantAddressInvalidated
Function:        foo
Args:
  - String:          <strong class="bold">failed to move load with loop-invariant address because the loop may invalidate its value</strong>
...
$</pre>
			<p>The <code>cat</code> command in the <a id="_idIndexMarker596"/>preceding output shows one of the optimization remark entries in <code>licm_remarks.yaml</code>. This entry tells us that there was a <em class="italic">missed</em> optimization that happened in the LICM Pass when it was processing the <code>foo</code> function. It also tells us the reason: LICM was not sure if a particular memory address was invalidated by the loop. Though this message doesn't provide fine-grained details, we can still infer that the problematic memory address concerning LICM was probably <code>a[5]</code>. LICM was not sure if the <code>a[i] += 2</code> statement modified the content of <code>a[5]</code>. </p>
			<p>With this knowledge, compiler developers <a id="_idIndexMarker597"/>can get hands-on in improving LICM—for example, teaching LICM to recognize induction variables (that is, the <code>i</code> variable in this loop) with a step value greater than 1 (in this case, it was 3, since <code>i += 3</code>).</p>
			<p>To generate optimization remarks such as the one shown in the preceding output, compiler developers need to integrate a specific utility API into their optimization Pass. To show you how to do that in your own Pass, we are going to reuse our <code>SimpleMulOpt</code> Pass as the sample. Here is part of the code that performs the first stage—<em class="italic">searching for multiplications</em> with power-of-two constant operands—in <code>SimpleMulOpt</code>:</p>
			<pre>…
for (auto &amp;I : instructions(F)) {
  if (auto *BinOp = dyn_cast&lt;BinaryOperator&gt;(&amp;I))
    if (BinOp-&gt;getOpcode() == Instruction::Mul) {
      auto *LHS = BinOp-&gt;getOperand(0),
           *RHS = BinOp-&gt;getOperand(1);
      // Has no constant operand
      <strong class="bold">if (!isa&lt;Constant&gt;(RHS)) continue;</strong>
      const APInt &amp;Const = cast&lt;ConstantInt&gt;(RHS)-&gt;getValue();
      // Constant operand is not power of two
      <strong class="bold">if (!Const.isPowerOf2()) continue;</strong>
      …
    }
}</pre>
			<p>The preceding code <a id="_idIndexMarker598"/>checks if the operand is constant before making sure it's also a power-of-two operand. If either of these checks fails, the algorithm will bail out by continuing on to the next instruction in the function.</p>
			<p>We intentionally <a id="_idIndexMarker599"/>inserted a small flaw into this code to make it less powerful, and we are going to show you how to find that problem by using an optimization remark. Here are the steps to do this:</p>
			<ol>
				<li>First, we need to have an <code>OptimizationRemarkEmitter</code> instance, which can help you to emit remark messages. This can be obtained from its parent analyzer, <code>OptimizationRemarkEmitterAnalysis</code>. Here is how we include it at the beginning of the <code>SimpleMulOpt::run</code> method:<pre>#include "llvm/Analysis/OptimizationRemarkEmitter.h"
PreservedAnalyses
SimpleMulOpt::run(Function &amp;F, FunctionAnalysisManager &amp;<strong class="bold">FAM</strong>) {
  <strong class="bold">OptimizationRemarkEmitter</strong> &amp;ORE
    = FAM.getResult&lt;<strong class="bold">OptimizationRemarkEmitterAnalysis</strong>&gt;(F);
  …
}</pre></li>
				<li>Then, we are going to use this <code>OptimizationRemarkEmitter</code> instance to emit an <a id="_idIndexMarker600"/>optimization remark if the <a id="_idIndexMarker601"/>multiplication instruction lacks a constant operand, as follows:<pre>#include "<code>OptimizationRemarkEmitter::emit</code> method takes a lambda function as the argument. This lambda function will be invoked to emit an optimization remark object if the <a id="_idIndexMarker602"/>optimization remark feature is turned on (via the <code>–pass-remarks-output</code> command-line option we've seen previously, for example).</li><li>The <code>OptimizationRemarkMissed</code> class (note that it is not declared in <code>OptimizationRemarkEmitter.h</code> but in the <code>DiagnosticInfo.h</code> header file) represents the remark of a missed <code>I</code> does not have any constant operand. The <a id="_idIndexMarker603"/>constructor of <code>OptimizationRemarkMissed</code> takes three arguments: the name of the Pass, the name of the missed optimization opportunity, and the enclosing IR unit (in this case, we use the enclosing <code>Function</code>). In addition to constructing a <code>OptimizationRemarkMissed</code> object, we also concatenate several objects via the stream operator (<code>&lt;&lt;</code>) at the tail. These objects will eventually be put under the <code>Args</code> section of each optimization remark entry in the YAML file we saw previously.<p>In addition to using <code>OptimizationRemarkMissed</code> to notify you of missed optimization opportunities, you can also use other classes derived from <code>DiagnosticInfoOptimizationBase</code> to present different kinds of information—for example, use <code>OptimizationRemark</code> to find out which optimization has been <em class="italic">successfully</em> applied, and use <code>OptimizationRemarkAnalysis</code> to keep a log of analysis data/facts.</p></li><li>Among objects concatenated by the stream operator, <code>ore::NV(…)</code> seems to be a special case. Recall that in the optimization remark YAML file, each line under the <code>Args</code> section was a key-value pair (for example, <code>String:  failed to move load with….</code>, where <code>String</code> was the key). The <code>ore::NV</code> object allows you to customize the key-value pair. In this case, we are using <code>Inst</code> as the key and <code>SS.str()</code> as the value. This feature provides more flexibility to parse the <a id="_idIndexMarker604"/>optimization remark YAML file—for instance, if you want to write a little tool to visualize the optimization remarks, custom <code>Args</code> keys can give you an easier time (during the parsing stage) by distinguishing critical data from other strings.</li></ul></li>
				<li>Now that you have <a id="_idIndexMarker605"/>inserted the code to emit the optimization remark, it's time to test it. This time, we are going to use the following <code>IR</code> function as the input code: <pre>define i32 @bar(i32 %0) {
  %2 = mul nsw i32 %0, 3
  %3 = mul nsw i32 8, %3
  ret %3
}</pre><p>You can rebuild the <code>SimpleMulOpt</code> Pass and run it using a command such as this:</p><pre>$ opt –load-pass-plugin=… –passes="simple-mul-opt" \
      <code>SimpleMulOpt</code> bailed out because it couldn't find a constant operand on one of the (multiplication) instructions. The <code>Args</code> section shows a detailed reason for this.</p><p>With this <a id="_idIndexMarker606"/>information, we realize that <code>SimpleMulOpt</code> is unable to optimize a multiplication whose <em class="italic">first</em> operand (LHS operand) is a <a id="_idIndexMarker607"/>power-of-two constant, albeit a proper optimization opportunity. Thus, we can now fix the implementation of <code>SimpleMulOpt</code> to check if <em class="italic">either</em> of the operands is constant, as follows:</p><pre>…
if (BinOp-&gt;getOpcode() == Instruction::Mul) {
  auto *LHS = BinOp-&gt;getOperand(0),
       *RHS = BinOp-&gt;getOperand(1);
  // Has no constant operand
  if (<strong class="bold">!isa&lt;ConstantInt&gt;(RHS) &amp;&amp; !isa&lt;ConstantInt&gt;(LHS)</strong>) {
    ORE.emit([&amp;]() {
      return …
    });
    continue;
  }
  …
}
…</pre><p>You have now learned how to emit optimization remarks in an LLVM Pass and how to use the generated report to discover potential optimization opportunities.</p></li>
			</ol>
			<p>So far, we have only <a id="_idIndexMarker608"/>studied the generated optimization remark YAML file. Though it has provided valuable diagnostic information, it would be great if we <a id="_idIndexMarker609"/>could have more fine-grained and intuitive location information to know where exactly these remarks happened. Luckily, Clang and LLVM have provided a way to achieve that.</p>
			<p>With the help of Clang, we can actually generate optimization remarks with <strong class="bold">source location</strong> (that is, line and column numbers in the original source file) attached. Furthermore, LLVM provides you with a small utility that can associate an optimization remark with its corresponding source location and visualize the result on a web page. Here's how to do this:</p>
			<ol>
				<li value="1">Let's reuse the following code as the input:<pre>int foo(int *a, int N) {
  for (int i = 0; i &lt; N; i += 3) {
    a[i] += 2;
  }
  return a[5];
}</pre><p>First, let's generate optimization remarks using this <code>clang</code> command: </p><pre>$ clang -O3 <code>-foptimization-record-file</code> is the command-line option used to generate an optimization remark file with the given filename.</p></li>
				<li>After <code>licm.remark.yaml</code> is generated, let's use a utility called <code>opt-viewer.py</code> to <a id="_idIndexMarker610"/>visualize the remarks. The <code>opt-viewer.py</code> script is not installed in the typical location by default—instead of putting it in <code>&lt;install path&gt;/bin</code> (for example <code>/usr/bin</code>), it is installed in <code>&lt;install path&gt;/share/opt-viewer</code> (<code>/usr/share/opt-viewer</code>). We are going to invoke this script with the following command-line options:<pre>$ opt-viewer.py --source-dir=$PWD \ 
--target-dir=licm_remark licm.remark.yaml</pre><p>(Note that <code>opt-viewer.py</code> depends on several Python packages such as <code>pyyaml</code> and <code>pygments</code>. Please install them before you use <code>opt-viewer.py</code>.)</p></li>
				<li>There <a id="_idIndexMarker611"/>will be a HTML file—<code>index.html</code>—generated inside the <code>licm_remark</code> folder. Before you open the web page, please copy the original source code—<code>opt_remark_licm.c</code>—into that folder as well. After that, you will be able to see a web page like this:<div><img src="img/B14590_Figure_11.1.jpg" alt="Figure 11.1 – Web page of optimization remarks combined with the source file&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 11.1 – Web page of optimization remarks combined with the source file</p>
			<p>We are particularly <a id="_idIndexMarker612"/>interested in two of these columns: <code>Missed</code>, <code>Passed</code>, or <code>Analyzed</code> rendered in red, green, and white, respectively—attached on a given line shown at the <strong class="bold">Source Location</strong> column.</p>
			<p>If we click on a link in the <strong class="bold">Source Location</strong> column, this will navigate you to a page that looks like this:</p>
			<div><div><img src="img/B14590_Figure_11.2.jpg" alt="Figure 11.2 – Details of an optimization remark&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – Details of an optimization remark</p>
			<p>This page gives you a nice view of optimization remark details, interleaved with the originating source code line. For example, on <em class="italic">line 3</em>, <code>loop-vectorize</code> Pass said it couldn't vectorize this loop because its cost model didn't think it was beneficial to do so.</p>
			<p>You have now learned how to use <a id="_idIndexMarker614"/>optimization remarks to gain insights into the optimization Pass, which is especially useful when you're <a id="_idIndexMarker615"/>debugging a missing optimization opportunity or fixing a mis-compilation bug.</p>
			<p>In the next section, we are going to learn some useful skills to profile the execution time of LLVM.</p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor165"/>Adding time measurements</h1>
			<p>LLVM is an enormous software, with <a id="_idIndexMarker616"/>hundreds of components working closely together. Its ever-increasing running time is slowly becoming an issue. This affects many use cases that are sensitive to compilation time—for example, the <strong class="bold">Just-in-Time</strong> (<strong class="bold">JIT</strong>) compiler. To diagnose this <a id="_idIndexMarker617"/>problem in a systematic way, LLVM provides some useful utilities for <strong class="bold">profiling</strong> the execution time.</p>
			<p>Time profiling has always been an important topic in software development. With the running time collected from individual software components, we can spot performance bottlenecks more easily. In this section, we are going to learn about two tools provided by LLVM: the <code>Timer</code> class and the <code>TimeTraceScope</code> class. Let's start with the <code>Timer</code> class first.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor166"/>Using the Timer class</h2>
			<p>The <code>Timer</code> class, as suggested by its <a id="_idIndexMarker618"/>name, can measure the execution time of a code region. Here is an example of this:</p>
			<pre>#include "llvm/Support/Timer.h"
…
Timer T("MyTimer", "A simple timer");
T.<strong class="bold">startTimer</strong>();
// Do some time-consuming works…
T.<strong class="bold">stopTimer</strong>();</pre>
			<p>In the preceding snippet, <code>Timer</code> instance <code>T</code> measures the time spent in the region, enclosed by the <code>startTimer</code> and <code>stopTimer</code> method calls.</p>
			<p>Now that we have collected the <a id="_idIndexMarker619"/>timing data, let's try to print it out. Here is an example of this:</p>
			<pre>Timer T(…);
…
TimeRecord TR = T.<strong class="bold">getTotalTime</strong>();
TR.<strong class="bold">print</strong>(TR, errs());</pre>
			<p>In the previous code snippet, a <code>TimeRecord</code> instance encapsulates the data collected by the <code>Timer</code> class. We can then use <code>TimeRecord::print</code> to print it to a stream—in this case, the <code>errs()</code> stream. In addition, we assigned another <code>TimeRecord</code> instance—via the first argument of <code>print</code>—as the <em class="italic">total</em> time interval we want to compare it against. Let's look at the output of this code, as follows:</p>
			<pre>===---------------------------------------------------------===
                     Miscellaneous Ungrouped Timers
===---------------------------------------------------------===
   ---User Time---   --User+System--   ---Wall Time---  --- Name ---
   0.0002 (100.0%)   0.0002 (100.0%)   0.0002 (100.0%)  A simple timer
   0.0002 (100.0%)   0.0002 (100.0%)   0.0002 (100.0%)  Total
   0.0002 (100.0%)   0.0002 (100.0%)   0.0002 (100.0%)</pre>
			<p>In the preceding output, the <a id="_idIndexMarker620"/>first row shows the <code>TimeRecord</code> instance collected from our previous <code>Timer</code> instance, whereas the second row shows the total time—the first argument of <code>TimeRecord::print</code>.</p>
			<p>We now know how to print the timing data collected by a single <code>Timer</code> instance, but what about multiple timers? LLVM provides another support utility for the <code>Timer</code> class: the <code>TimerGroup</code> class. Here's an example usage of the <code>TimerGroup</code> class:</p>
			<pre>TimerGroup TG("MyTimerGroup", "My collection of timers");
Timer T("MyTimer", "A simple timer", <strong class="bold">TG</strong>);
T.startTimer();
// Do some time-consuming works…
T.stopTimer();
Timer T2("MyTimer2", "Yet another simple timer", <strong class="bold">TG</strong>);
T2.startTimer();
// Do some time-consuming works…
T2.stopTimer();
<strong class="bold">TG.print(errs());</strong></pre>
			<p>In the preceding code snippet, we declare a <code>TimerGroup</code> instance, <code>TG</code>, and use it as the third constructor argument for each <code>Timer</code> instance we create. Finally, we print them using <code>TimerGroup::print</code>. Here is the output of this code:</p>
			<pre>===---------------------------------------------------------===
                    My collection of timers
===---------------------------------------------------------===
  Total Execution Time: 0.0004 seconds (0.0004 wall clock)
   ---User Time---   --User+System--   ---Wall Time---  --- Name ---
   0.0002 ( 62.8%)   0.0002 ( 62.8%)   0.0002 ( 62.8%)  A simple timer
   0.0001 ( 37.2%)   0.0001 ( 37.2%)   0.0001 ( 37.2%)  Yet another simple timer
   0.0004 (100.0%)   0.0004 (100.0%)   0.0004 (100.0%)  Total</pre>
			<p>Each row in the output (except the last one) is the <code>TimeRecord</code> instance for each <code>Timer</code> instance in this group.</p>
			<p>So far, we have <a id="_idIndexMarker621"/>been using <code>Timer::startTimer</code> and <code>Timer::stopTimer</code> to toggle the timer. To make measuring the time interval within a code block—namely, the region enclosed with curly brackets <code>{}</code>—easier without manually calling those two methods, LLVM provides another utility that automatically starts the timer upon entering a code block and turns it off when exiting. Let's see how to use the <code>TimeRegion</code> class with the following sample code:</p>
			<pre>TimerGroup TG("MyTimerGroup", "My collection of timers");
{
  <strong class="bold">Timer T</strong>("MyTimer", "A simple timer", TG);
  <strong class="bold">TimeRegion TR(T);</strong>
  // Do some time-consuming works…
}
{
  <strong class="bold">Timer T</strong>("MyTimer2", "Yet another simple timer", TG);
  <strong class="bold">TimeRegion TR(T);</strong>
  // Do some time-consuming works…
}
TG.print(errs());</pre>
			<p>As you can see in the preceding snippet, instead of calling <code>startTimer</code>/<code>stopTimer</code>, we put the to-be-measured code into a separate code block and use a <code>TimeRegion</code> variable to automatically toggle the timer. This code will print out the same content as the previous example. With the help of <code>TimeRegion</code>, we can have a more concise syntax and avoid any mistakes where we <em class="italic">forget</em> to turn off the timer.</p>
			<p>You have now <a id="_idIndexMarker622"/>learned how to use <code>Timer</code> and its supporting utilities to measure the execution time of a certain code region. In the next section, we are going to learn a more advanced form of time measurement that captures the hierarchical structure of the program.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor167"/>Collecting the time trace</h2>
			<p>In the previous section, we learned how to use <code>Timer</code> to collect the execution time of a small range of code regions. Although <a id="_idIndexMarker623"/>that gave us a portrait of the compiler's runtime performance, we sometimes need a more <em class="italic">structural</em> timing profile in order to fully understand any systematic issues.</p>
			<p><code>TimeTraceScope</code> is a class <a id="_idIndexMarker624"/>provided by LLVM to perform global-scope time profiling. Its usage is pretty simple: similar to <code>TimeRegion</code>, which we saw in the previous section, a <code>TimeTraceScope</code> instance automatically turns the time profiler on and off upon entering and exiting a code block. Here is an example of this:</p>
			<pre>TimeTraceScope OuterTimeScope("<strong class="bold">TheOuterScope</strong>");
for (int i = 0; i &lt; 50; ++i) {
  {
    TimeTraceScope InnerTimeScope("<strong class="bold">TheInnerScope</strong>");
    <strong class="bold">foo();</strong>
  }
  <strong class="bold">bar();</strong>
}</pre>
			<p>In the preceding code snippet, we create two <code>TimeTraceScope</code> instances: <code>OuterTimeScope</code> and <code>InnerTimeScope</code>. These try to profile the execution time of the whole region and the time spent on function <code>foo</code>, respectively.</p>
			<p>Normally, if we use <code>Timer</code> rather than <code>TimeTraceScope</code>, it can <a id="_idIndexMarker625"/>only give us the aggregate duration collected from each timer. However, in this case, we are more interested in how different parts of the code allocate themselves on the <em class="italic">timeline</em>. For example, does the <code>foo</code> function always <a id="_idIndexMarker626"/>spend the same amount of time ion every loop iteration? If that's not the case, which iterations spend more time than others?</p>
			<p>To see the result, we need to add additional command-line options to the <code>opt</code> command when running the Pass (assuming you use <code>TimeTraceScope</code> within a Pass). Here is an example of this:</p>
			<pre>$ opt –passes="…" <strong class="bold">-time-trace</strong> <strong class="bold">-time-trace-file=my_trace.json</strong> …</pre>
			<p>The additional <code>-time-trace</code> flag is asking <code>opt</code> to export all the traces collected by <code>TimeTraceScope</code> to the file designated by the <code>-time-trace-file</code> option.</p>
			<p>After running this command, you will get a new file, <code>my_trace.json</code>. The content of this file is basically non-human-readable, but guess what? You can visualize it using the <strong class="bold">Chrome</strong> web browser. Here are the steps to do this:</p>
			<ol>
				<li value="1">Open your Chrome web browser and type in <code>chrome://tracing</code> in the <strong class="bold">Uniform Resource Locator</strong> (<strong class="bold">URL</strong>) bar. You <a id="_idIndexMarker627"/>will see an interface that looks like this:<div><img src="img/B14590_Figure_11.3.jpg" alt="Figure 11.3 – The trace visualizer in Chrome&#13;&#10;"/></div><p class="figure-caption">Figure 11.3 – The trace visualizer in Chrome</p></li>
				<li>Click on the <code>my_trace.json</code> file. You will see a page like this:<div><img src="img/B14590_Figure_11.4.jpg" alt="Figure 11.4 – The view after opening my_trace.json&#13;&#10;"/></div><p class="figure-caption">Figure 11.4 – The view after opening my_trace.json</p><p>Each color block represents a time interval <a id="_idIndexMarker628"/>collected by a <code>TimeTraceScope</code> instance. </p></li>
				<li>Let's take a closer look: please press the number key <em class="italic">3</em> to switch to zoom mode. After that, you should be <a id="_idIndexMarker629"/>able to zoom in or out by clicking and dragging the mouse up or down. In the meantime, you can use the arrow keys to scroll the timeline left or right. Here is part of the timeline after we zoom in:<div><img src="img/B14590_Figure_11.5.jpg" alt="Figure 11.5 – Part of the trace timeline&#13;&#10;"/></div><p class="figure-caption">Figure 11.5 – Part of the trace timeline</p><p>As we can see from <em class="italic">Figure 11.5</em>, there are several layers stacking together. This layout reflects how different <code>TimeTraceScope</code> instances are organized in <code>opt</code> (and in our Pass). For example, our  <code>TimeTraceScope</code> instance entitled <code>TheOuterScope</code> is stacked above multiple <code>TheInnerScope</code> blocks. Each of the <code>TheInnerScope</code> blocks represents the time spent on the <code>foo</code> function in each loop iteration we saw earlier.</p></li>
				<li>We can further inspect the properties of a block by clicking on it. For example, if we click one of the <code>TheInnerScope</code> blocks, its timing properties will be shown in the lower half of the screen. Here is an example of this:</li>
			</ol>
			<div><div><img src="img/B14590_Figure_11.6.jpg" alt="Figure 11.6 – Details of a time interval block&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – Details of a time interval block</p>
			<p>This gives us information such as the time interval and the starting time in this timeline.</p>
			<p>With this visualization, we can combine timing information with the structure of the compiler, which will help us to find out performance bottlenecks more rapidly.</p>
			<p>In addition to <code>opt</code>, <code>clang</code> can also generate the same trace JSON file. Please consider adding a <code>-ftime-trace</code> flag. Here is an example of this:</p>
			<pre>$ clang -O3 <strong class="bold">-ftime-trace</strong> -c foo.c</pre>
			<p>This will generate a JSON trace file with the same name as the input file. In this case, it will be <code>foo.json</code>. You can use the skills we just learned to visualize it.</p>
			<p>In this section, we have <a id="_idIndexMarker630"/>learned some useful skills to collect statistics from LLVM. The <code>Statistic</code> class can be used as an integer counter to record the number of events occurring in the optimization. Optimization remarks, on the other hand, can give us insights into some of the decision-making process inside the optimization Pass, making it easier for compiler developers to diagnose missing optimization opportunities. With <code>Timer</code> and <code>TimeTraceScope</code>, developers can monitor LLVM's execution time in a more manageable way and handle compilation-speed regressions with confidence. These techniques can improve an LLVM developer's productivity when creating new inventions or fixing a challenging problem.</p>
			<p>In the next section of this chapter, we are going to learn how to write error-handling code in an efficient way, using utilities provided by LLVM.</p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor168"/>Error-handling utilities in LLVM</h1>
			<p>Error handling has <a id="_idIndexMarker631"/>always been a widely discussed topic in software development. It can be as simple as returning an error code—such as in many of the Linux APIs (for example, the <code>open</code> function)—or using an advanced mechanism such as throwing an exception, which has been widely adopted by many modern programming languages such as Java and C++.</p>
			<p>Although C++ has built-in <a id="_idIndexMarker632"/>support for exception handling, LLVM does <em class="italic">not</em> adopt it in its code base at all. The rationale behind this decision is that despite its convenience and expressive syntax, exception handling in C++ comes at a high cost in terms of performance. Simply speaking, exception handling makes the original code more complicated and hinders a compiler's ability to optimize it. Furthermore, during runtime, the program usually needs to spend more time recovering from an exception. Therefore, LLVM disables exception handling by default in its code base and falls back to other ways of error handling—for example, carrying an error with the return value or using the utilities we are going to learn about in this section.</p>
			<p>In the first half of this section, we are going to talk about the <code>Error</code> class, which—as the name suggests—represents an error. This is unlike conventional error representations—when using an integer as the error code, for instance, you cannot <em class="italic">ignore</em> the generated <code>Error</code> instances without handling it. We will explain this shortly.</p>
			<p>In addition to the <code>Error</code> class, developers found that in LLVM's code base a common pattern was shared by much of the error-handling code: an API may return a result <em class="italic">or</em> an error, but not both (at the same time). For instance, when we call a file-reading API, we are expecting to get the content of that file (the result) or an error when something goes wrong (for example, there is no such file). In the second part of this section, we are going to learn two utility classes that implement this pattern. </p>
			<p>Let's start with an introduction to the <code>Error</code> class first.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor169"/>Introducing the Error class</h2>
			<p>The concept <a id="_idIndexMarker633"/>represented by the <code>Error</code> class is pretty simple: it's an <a id="_idIndexMarker634"/>error with supplementary descriptions such as an error message or error code. It is designed to be passed by a value (as a function argument) or returned from a function. Developers are free to create their custom <code>Error</code> instance, too. For example, if we want to create a <code>FileNotFoundError</code> instance to tell users <a id="_idIndexMarker635"/>that a certain file does not exist, we can write the following code:</p>
			<pre>#include "llvm/Support/Error.h"
#include &lt;system_error&gt;
// In the header file…
struct FileNotFoundError : public <strong class="bold">ErrorInfo&lt;FileNoteFoundError&gt;</strong> {
  StringRef FileName;
  explicit FileNotFoundError(StringRef Name) : FileName(Name)    {}
  <strong class="bold">static char ID;</strong>
  <strong class="bold">std::error_code convertToErrorCode()</strong> const override {
    return std::errc::no_such_file_or_directory;
  }
  <strong class="bold">void log(raw_ostream &amp;OS)</strong> const override {
    OS &lt;&lt; FileName &lt;&lt; ": No such file";
  }
};
// In the CPP file…
char FileNotFoundError::ID = 0;</pre>
			<p>There are several <a id="_idIndexMarker636"/>requirements for implementing a custom <code>Error</code> instance. These are listed next:</p>
			<ul>
				<li>Derive from the <code>ErrorInfo&lt;T&gt;</code> class, where <code>T</code> is your custom class.</li>
				<li>Declare a unique <code>ID</code> variable. In this case, we use a static class member variable.</li>
				<li>Implement the <code>convertToErrorCode</code> method. This method designates a <code>std::error_code</code> instance for this <code>Error</code> instance. <code>std::error_code</code> is the error type used in the C++ standard library (since C++11). Please refer to the C++ reference documentation for available (predefined) <code>std::error_code</code> instances.</li>
				<li>Implement the <code>log</code> method to print out error messages.</li>
			</ul>
			<p>To create an <code>Error</code> instance, we can leverage a <code>make_error</code> utility function. Here is an example usage of this:</p>
			<pre>Error NoSuchFileErr = <strong class="bold">make_error</strong>&lt;FileNotFoundError&gt;(<strong class="bold">"foo.txt"</strong>);</pre>
			<p>The <code>make_error</code> function takes an error class—in this case, our <code>FileNotFoundError</code> class—as the template argument and function arguments (in this case, <code>foo.txt</code>), if there are any. These will then be passed to its constructor.</p>
			<p>If you try to run the <a id="_idIndexMarker637"/>preceding code (in debug build) without doing anything to the <code>NoSuchFileErr</code> variable, the program will simply crash and show an <a id="_idIndexMarker638"/>error message such as this:</p>
			<pre>Program aborted due to an unhandled Error:
foo.txt: No such file</pre>
			<p>It turns out that every <code>Error</code> instance is required to be <strong class="bold">checked</strong> and <strong class="bold">handled</strong> before the end of its lifetime (that is, when its destructor method is called). </p>
			<p>Let me first explain what <em class="italic">checking an</em> <code>Error</code> <em class="italic">instance</em> means. In addition to representing a real error, the <code>Error</code> class can also represent a <em class="italic">success</em> state—that is, no error. To give you a more concrete idea of this, many of the LLVM APIs have the following error-handling structure:</p>
			<pre><strong class="bold">Error</strong> readFile(StringRef FileName) {
  if (openFile(FileName)) {
    // Success
    // Read the file content…
    return <strong class="bold">ErrorSuccess();</strong>
  } else
    return <strong class="bold">make_error&lt;FileNotFoundError&gt;(FileName);</strong>
}</pre>
			<p>In other words, they return an <code>ErrorSuccess</code> instance in the case of success or an <code>ErrorInfo</code> instance otherwise. When the program returns from <code>readFile</code>, we need to <em class="italic">check</em> if the returned <code>Error</code> instance represents a success result or not by treating it as a Boolean variable, as follows:</p>
			<pre>Error E = readFile(…);
if (E) {
  <strong class="bold">// TODO: Handle the error</strong>
} else {
  // Success!
}</pre>
			<p>Note that you <em class="italic">always</em> need to <a id="_idIndexMarker639"/>check an <code>Error</code> instance even if you are 100% sure that it is in a <code>Success</code> state, otherwise the program will still abort.</p>
			<p>The preceding code snippet <a id="_idIndexMarker640"/>provides a good segue into the topic of handling <code>Error</code> instances. If an <code>Error</code> instance represents a real error, we need to use a special API to handle it: <code>handleErrors</code>. Here's how to use it:</p>
			<pre>Error E = readFile(…);
if (E) {
  Error UnhandledErr = <strong class="bold">handleErrors</strong>(
    <strong class="bold">std::move(E)</strong>,
    <strong class="bold">[&amp;](const FileNotFoundError &amp;NotFound)</strong> {
      NotFound.log(errs() &lt;&lt; "Error occurred: ");
      errs() &lt;&lt; "\n";
    });
  …
}</pre>
			<p>The <code>handleErrors</code> function takes ownership of the <code>Error</code> instance (by <code>std::move(E)</code>) and uses the provided lambda function to handle the error. You might notice that <code>handleErrors</code> returns another <code>Error</code> instance, which represents the <em class="italic">unhandled</em> error. What does that mean?</p>
			<p>In the <a id="_idIndexMarker641"/>previous example of the <code>readFile</code> function, the returned <code>Error</code> instance can represent either a <code>Success</code> state or a <code>FileNotFoundError</code> state. We <a id="_idIndexMarker642"/>can slightly modify the function to return a <code>FileEmptyError</code> instance when the opened file is empty, as follows:</p>
			<pre><strong class="bold">Error</strong> readFile(StringRef FileName) {
  if (openFile(FileName)) {
    // Success
    …
    if (Buffer.empty())
      return <strong class="bold">make_error&lt;FileEmptyError&gt;()</strong>;
    else
      return <strong class="bold">ErrorSuccess();</strong>
  } else
    return <strong class="bold">make_error&lt;FileNotFoundError&gt;(FileName);</strong>
}</pre>
			<p>Now, the <code>Error</code> instance returned from <code>readFile</code> can either be a <code>Success</code> state, a <code>FileNotFoundError</code> instance, <em class="italic">or</em> a <code>FileEmptyError</code> instance. However, the <code>handleErrors</code> code we wrote previously only handled the case of <code>FileNotFoundError</code>.</p>
			<p>Therefore, we need to use the following code to handle the case of <code>FileEmptyError</code>:</p>
			<pre>Error E = readFile(…);
if (E) {
  Error UnhandledErr = <strong class="bold">handleErrors</strong>(
    std::move(E),
    [&amp;](const <strong class="bold">FileNotFoundError</strong> &amp;NotFound) {…});
  UnhandledErr = <strong class="bold">handleErrors</strong>(
    std::move(UnhandledErr),
    [&amp;](const <strong class="bold">FileEmptyError</strong> &amp;IsEmpty) {…});
  …
}</pre>
			<p>Be aware that you always need to take <a id="_idIndexMarker643"/>ownership of an <code>Error</code> instance when using <code>handleErrors</code>.</p>
			<p>Alternatively, you can <em class="italic">coalesce</em> two <code>handleErrors</code> function calls into one by using multiple lambda <a id="_idIndexMarker644"/>function arguments for each of the error types, as follows:</p>
			<pre>Error E = readFile(…);
if (E) {
  Error UnhandledErr = <strong class="bold">handleErrors</strong>(
    std::move(E),
    [&amp;](const <strong class="bold">FileNotFoundError</strong> &amp;NotFound) {…},
    [&amp;](const <strong class="bold">FileEmptyError</strong> &amp;IsEmpty) {…});
  …
}</pre>
			<p>In other words, the <code>handleErrors</code> function is acting like a switch-case statement for an <code>Error</code> instance. It is effectively working like the following pseudocode:</p>
			<pre>Error E = readFile(…);
if (E) {
  <strong class="bold">switch</strong> (E) {
  case <strong class="bold">FileNotFoundError</strong>: …
  case <strong class="bold">FileEmptyError</strong>: …
  default:
    // generate the <strong class="bold">UnhandledError</strong>
  }
}</pre>
			<p>Now, you might be <a id="_idIndexMarker645"/>wondering: <em class="italic">Since</em> <code>handleErrors</code> <em class="italic">will always return an</em> <code>Error</code> <em class="italic">representing the unhandled error, and I can't just ignore the returned instance, otherwise the program will abort, how should we end this "chain of error handling"?</em> There are two ways to do that, so let's have a look at each, as follows:</p>
			<ul>
				<li>If you are 100% sure that <a id="_idIndexMarker646"/>you have handled all possible error types—which means that the unhandled <code>Error</code> variable is in a <code>Success</code> state—you can call the <code>cantFail</code> function to make an assertion, as illustrated in the following code snippet:<pre>if (E) {
  Error UnhandledErr = handleErrors(
    std::move(E),
    [&amp;](const FileNotFoundError &amp;NotFound) {…},
    [&amp;](const FileEmptyError &amp;IsEmpty) {…});
  <code>UnhandledErr</code> still contains an error, the <code>cantFail</code> function will abort the program execution and print an error message.</p></li>
				<li>A more elegant solution would be to use the <code>handleAllErrors</code> function, as follows:<pre>if (E) {
  <code>handleAllErrors</code> will still abort the program execution, just like what we have seen previously.</p></li>
			</ul>
			<p>You have now learned how to use the <code>Error</code> class and how to properly handle errors. Though the design of <code>Error</code> seems a little annoying at first glance (that is, we need to handle <em class="italic">all</em> possible error types or the execution will just abort halfway), these restrictions can decrease the number of mistakes made by programmers and create a more <strong class="bold">robust</strong> program.</p>
			<p>Next, we are going to introduce two other utility classes that can further improve the error-handling expressions in LLVM.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor170"/>Learning about the Expected and ErrorOr classes</h1>
			<p>As we briefly mentioned in the introduction of this section, in LLVM's code base it's pretty common to see a coding pattern where an API wants to return a result or an error if something goes wrong. LLVM tries to make this pattern more accessible by creating utilities that <em class="italic">multiplex</em> results and errors in a single object—they are the <code>Expected</code> and <code>ErrorOr</code> classes. Let's begin with the first one.</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor171"/>The Expected class</h2>
			<p>The <code>Expected</code> class carries either a <code>Success</code> result or an <a id="_idIndexMarker649"/>error—for instance, the <a id="_idIndexMarker650"/>JSON library in LLVM uses it to represent the outcome of parsing an incoming string, as shown next:</p>
			<pre>#include "llvm/Support/JSON.h"
 using namespace llvm;
…
// `InputStr` has the type of `StringRef`
<strong class="bold">Expected&lt;json::Value&gt;</strong> JsonOrErr = json::parse(InputStr);
if (JsonOrErr) {
  // Success!
  <strong class="bold">json::Value &amp;Json = *JsonOrErr;</strong>
  …
} else {
  // Something goes wrong…
  <strong class="bold">Error Err = JsonOrErr.takeError();</strong>
  // Start to handle `Err`…
}</pre>
			<p>The preceding <code>JsonOrErr</code> class has a type of <code>Expected&lt;json::Value&gt;</code>. This means that this <code>Expected</code> variable either carries a <code>json::Value</code>-type <code>Success</code> result or an error, represented by the <code>Error</code> class we just learned about in the previous section.</p>
			<p>Just as with the <code>Error</code> class, every <code>Expected</code> instance needs to be <em class="italic">checked</em>. If it represents an error, that <code>Error</code> instance needs to be <em class="italic">handled</em> as well. To check the status of an <code>Expected</code> instance, we <a id="_idIndexMarker651"/>can also cast it to a Boolean type. However, unlike with <code>Error</code>, if an <code>Expected</code> instance contains a <code>Success</code> result, it will be <code>true</code> after being casted into a Boolean.</p>
			<p>If the <code>Expected</code> instance <a id="_idIndexMarker652"/>represents a <code>Success</code> result, you can fetch the result using either the <code>*</code> operator (as shown in the preceding code snippet), the <code>-&gt;</code> operator, or the <code>get</code> method. Otherwise, you can retrieve the error by calling the <code>takeError</code> method before handling the <code>Error</code> instance, using the skills we learned in the previous section.</p>
			<p>Optionally, if you are sure that an <code>Expected</code> instance is in an <code>Error</code> state, you can check the underlying error type by calling the <code>errorIsA</code> method without retrieving the underlying <code>Error</code> instance first. For example, the following code checks if an error is a <code>FileNotFoundError</code> instance, which we <a id="_idIndexMarker653"/>created in the previous section:</p>
			<pre>if (JsonOrErr) {
  // Success!
  …
} else {
  // Something goes wrong…
  if (<strong class="bold">JsonOrErr.errorIsA&lt;FileNotFoundError&gt;()</strong>) {
    …
  }
}</pre>
			<p>These are tips for <a id="_idIndexMarker654"/>consuming an <code>Expected</code> variable. To create an <code>Expected</code> instance, the most common way is to leverage the <em class="italic">implicit</em> type conversion to <code>Expected</code>. Here is an example of this:</p>
			<pre><strong class="bold">Expected&lt;std::string&gt;</strong> readFile(StringRef FileName) {
  if (openFile(FileName)) {
    <strong class="bold">std::string Content</strong>;
    // Reading the file…
    return <strong class="bold">Content</strong>;
  } else
    return <strong class="bold">make_error&lt;FileNotFoundError&gt;(FileName)</strong>;
}</pre>
			<p>The preceding code shows that in cases where something goes wrong, we can simply return an <code>Error</code> instance, which will be implicitly converted into an <code>Expected</code> instance representing that error. Similarly, if everything goes pretty smoothly, the <code>Success</code> result—in this case, the <code>std::string</code> type variable, <code>Content</code>—will also be implicitly <a id="_idIndexMarker655"/>converted into an <code>Expected</code> instance with a <code>Success</code> state.</p>
			<p>You have now learned <a id="_idIndexMarker656"/>how to use the <code>Expected</code> class. The last part of this section will show you how to use one of its sibling classes: <code>ErrorOr</code>.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor172"/>The ErrorOr class</h2>
			<p>The <code>ErrorOr</code> class <a id="_idIndexMarker657"/>uses a model that is <a id="_idIndexMarker658"/>nearly identical to the <code>Expected</code> class—it is either a <code>Success</code> result or an error. Unlike the <code>Expected</code> class, <code>ErrorOr</code> uses <code>std::error_code</code> to represent the error. Here is an example of using the <code>MemoryBuffer</code> API to read a file—<code>foo.txt</code>— and storing its content into a <code>MemoryBuffer</code> object:</p>
			<pre>#include "llvm/Support/MemoryBuffer.h"
…
<strong class="bold">ErrorOr&lt;std::unique_ptr&lt;MemoryBuffer&gt;&gt;</strong> ErrOrBuffer
  = MemoryBuffer::getFile("foo.txt");
if (ErrOrBuffer) {
  // Success!
  <strong class="bold">std::unique_ptr&lt;MemoryBuffer&gt; &amp;MB = *ErrOrBuffer</strong>;
} else {
  // Something goes wrong…
  <strong class="bold">std::error_code EC = ErrOrBuffer.getError()</strong>;
  …
}</pre>
			<p>The previous code snippet shows a similar structure, with the sample code for <code>Expected</code> we saw previously: the <code>std::unique_ptr&lt;MemoryBuffer&gt;</code> instance is the type of success result here. We can also retrieve it using the <code>*</code> operator after checking the state of <code>ErrOrBuffer</code>.</p>
			<p>The only difference here is that if <code>ErrOrBuffer</code> is in an <code>Error</code> state, the error is represented by a <code>std::error_code</code> instance rather than <code>Error</code>. Developers are not <em class="italic">obliged</em> to handle a <code>std::error_code</code> instance—in other words, they can just ignore that error, which might increase the chances of other developers making mistakes in the code. Nevertheless, using the <code>ErrorOr</code> class <a id="_idIndexMarker659"/>can give you better <em class="italic">interoperability</em> with C++ standard library APIs, as many of them use <code>std::error_code</code> to represent errors. For details about how to use <code>std::error_code</code>, please refer to the C++ reference documentation.</p>
			<p>Finally, to create an <code>ErrorOr</code> instance, we are <a id="_idIndexMarker660"/>using the same trick we used on the <code>Expected</code> class—leveraging implicit conversion, as shown in the following code snippet:</p>
			<pre>#include &lt;system_error&gt;
<strong class="bold">ErrorOr&lt;std::string&gt;</strong> readFile(StringRef FileName) {
  if (openFile(FileName)) {
    std::string Content;
    // Reading the file…
    <strong class="bold">return Content;</strong>
  } else
    <strong class="bold">return std::errc::no_such_file_or_directory;</strong>
}</pre>
			<p>The <code>std::errc::no_such_file_or_directory</code> object is one of the predefined <code>std::error_code</code> objects from the <code>system_error</code> header file.</p>
			<p>In this section, we learned how to use some error-handling utilities provided by LLVM—the important <code>Error</code> class that imposes strict rules on unhandled errors, and the <code>Expected</code> and <code>ErrorOr</code> classes that provide you with a handy way of multiplexing the program result and error state in a single object. These tools can help you to write expressive yet robust error-handling code when developing with LLVM.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor173"/>Summary</h1>
			<p>In this chapter, we learned lots of useful utilities that can improve our productivity when developing with LLVM. Some of them—such as optimization remarks or timers—are useful for diagnosing problems raised by LLVM, while others—the <code>Error</code> class, for instance—help you to build more robust code that scales well with the complexity of your own compiler.</p>
			<p>In the final chapter of this book, we are going to learn about <strong class="bold">profile-guided optimization</strong> (<strong class="bold">PGO</strong>) and sanitizer development, which are advanced topics that you can't miss.</p>
		</div>
	</body></html>