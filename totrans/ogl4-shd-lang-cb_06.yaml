- en: Image Processing and Screen Space Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying an edge detection filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying a Gaussian blur filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing HDR lighting with tone mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a bloom effect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using gamma correction to improve image quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multisample anti-aliasing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using deferred shading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Screen space ambient occlusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring the depth test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing order-independent transparency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus on techniques that work directly with the pixels
    in a framebuffer. These techniques typically involve multiple passes. An initial
    pass produces the pixel data and subsequent passes apply effects or further processes
    those pixels. To implement this, we often make use of the ability provided in
    OpenGL for rendering directly to a texture or set of textures (refer to the *Rendering
    to a texture* recipe in [Chapter 5](a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml),
    *Using Textures*).
  prefs: []
  type: TYPE_NORMAL
- en: The ability to render to a texture, combined with the power of the fragment
    shader, opens up a huge range of possibilities. We can implement image processing
    techniques such as brightness, contrast, saturation, and sharpness by applying
    an additional process in the fragment shader prior to output. We can apply **convolution**
    filters such as edge detection, smoothing (blur), or sharpening. We'll take a
    closer look at convolution filters in the recipe on edge detection.
  prefs: []
  type: TYPE_NORMAL
- en: A related set of techniques involves rendering additional information to textures
    beyond the traditional color information and then, in a subsequent pass, further
    processing that information to produce the final rendered image. These techniques
    fall under the general category that is often called **deferred shading**.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll look at some examples of each of the preceding techniques.
    We'll start off with examples of convolution filters for edge detection, blur,
    and bloom. Then, we'll move on to the important topics of gamma correction and
    multisample anti-aliasing. Finally, we'll finish with a full example of deferred
    shading.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the recipes in this chapter involve multiple passes. In order to apply
    a filter that operates on the pixels of the final rendered image, we start by
    rendering the scene to an intermediate buffer (a texture). Then, in a final pass,
    we render the texture to the screen by drawing a single fullscreen quad, applying
    the filter in the process. You'll see several variations on this theme in the
    following recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Applying an edge detection filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Edge detection** is an image processing technique that identifies regions
    where there is a significant change in the brightness of the image. It provides
    a way to detect the boundaries of objects and changes in the topology of the surface.
    It has applications in the field of computer vision, image processing, image analysis,
    and image pattern recognition. It can also be used to create some visually interesting
    effects. For example, it can make a 3D scene look similar to a 2D pencil sketch,
    as shown in the following image. To create this image, a teapot and torus were
    rendered normally, and then an edge detection filter was applied in a second pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9eef129b-0de4-4aab-addf-f86a80d9aa4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The edge detection filter that we''ll use here involves the use of a convolution
    filter, or convolution kernel (also called a **filter kernel**). A convolution
    filter is a matrix that defines how to transform a pixel by replacing it with
    the sum of the products between the values of nearby pixels and a set of pre-determined
    weights. As a simple example, consider the following convolution filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e329a4b-f800-4746-a66e-3fcd27c13cb8.png)'
  prefs: []
  type: TYPE_IMG
- en: The 3 x 3 filter is shaded in gray, superimposed over a hypothetical grid of
    pixels. The numbers in bold represent the values of the filter kernel (weights),
    and the non-bold values are the pixel values. The values of the pixels could represent
    grayscale intensity or the value of one of the RGB components. Applying the filter
    to the center pixel in the gray area involves multiplying the corresponding cells
    together and summing the results. The result would be the new value for the center
    pixel (**25**). In this case, the value would be (*17 + 19 + 2 * 25 + 31 + 33*),
    or 150.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in order to apply a convolution filter, we need access to the pixels
    of the original image and a separate buffer to store the results of the filter.
    We'll achieve this here by using a two-pass algorithm. In the first pass, we'll
    render the image to a texture, and then in the second pass, we'll apply the filter
    by reading from the texture and send the filtered results to the screen.
  prefs: []
  type: TYPE_NORMAL
- en: One of the simplest convolution-based techniques for edge detection is the so-called
    **Sobel operator**. The Sobel operator is designed to approximate the gradient
    of the image intensity at each pixel. It does so by applying two 3 x 3 filters.
    The results of the two are the vertical and horizontal components of the gradient.
    We can then use the magnitude of the gradient as our edge trigger. When the magnitude
    of the gradient is above a certain threshold, we assume that the pixel is on an
    edge.
  prefs: []
  type: TYPE_NORMAL
- en: 'The 3 x 3 filter kernels used by the Sobel operator are shown in the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e867208d-05ba-4cd1-aad4-3e340f580e28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the result of applying *S[x]* is *s[x]* and the result of applying *S[y]*
    is *s[y]*, then an approximation of the magnitude of the gradient is given by
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e91beb5-9542-4843-b4fc-c2b8623d842c.png)'
  prefs: []
  type: TYPE_IMG
- en: If the value of *g* is above a certain threshold, we consider the pixel to be
    an edge pixel and we highlight it in the resulting image.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we'll implement this filter as the second pass of a two-pass
    algorithm. In the first pass, we'll render the scene using an appropriate lighting
    model, but we'll send the result to a texture. In the second pass, we'll render
    the entire texture as a screen-filling quad, and apply the filter to the texture.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Set up a framebuffer object (refer to the *Rendering to a texture* recipe in
    [Chapter 5](a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml), *Using Textures*) that
    has the same dimensions as the main window. Connect the first color attachment
    of the FBO to a texture object in texture unit zero. During the first pass, we'll
    render directly to this texture. Make sure that the `mag` and `min` filters for
    this texture are set to `GL_NEAREST`. We don't want any interpolation for this
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Provide vertex information in vertex attribute zero, normals in vertex attribute
    one, and texture coordinates in vertex attribute two.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following uniform variables need to be set from the OpenGL application:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Width`: This is used to set the width of the screen window in pixels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Height`: This is used to set the height of the screen window in pixels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EdgeThreshold`: This is the minimum value of `g` squared required to be considered
    *on an edge*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RenderTex`: This is the texture associated with the FBO'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any other uniforms associated with the shading model should also be set from
    the OpenGL application.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create a shader program that applies the Sobel edge detection filter, perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The vertex shader just converts the position and normal to camera coordinates
    and passes them along to the fragment shader.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The fragment shader applies the reflection model in the first pass, and applies
    the edge detection filter in the second pass:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the render function of your OpenGL application, follow these steps for pass
    #1:'
  prefs: []
  type: TYPE_NORMAL
- en: Select FBO, and clear the color/depth buffers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `Pass` uniform to `1`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the model, view, and projection matrices, and draw the scene
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For pass #2, carry out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Deselect the FBO (revert to the default framebuffer) and clear the color/depth
    buffers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `Pass` uniform to `2`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the model, view, and projection matrices to the identity matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a single quad (or two triangles) that fills the screen (-1 to +1 in *x*
    and *y*), with texture coordinates that range from 0 to 1 in each dimension.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first pass renders all of the scene's geometry of sending the output to
    a texture. We select the function `pass1`, which simply computes and applies the
    Blinn-Phong reflection model (refer to [Chapter 3](74703f9d-f69a-4b08-bb38-6e1066371207.xhtml),
    *The Basics of GLSL Shaders*).
  prefs: []
  type: TYPE_NORMAL
- en: In the second pass, we select the function `pass2`, and render only a single
    quad that covers the entire screen. The purpose of this is to invoke the fragment
    shader once for every pixel in the image. In the `pass2` function, we retrieve
    the values of the eight neighboring pixels of the texture containing the results
    from the first pass, and compute their brightness by calling the `luminance` function.
    The horizontal and vertical Sobel filters are then applied and the results are
    stored in `sx` and `sy`.
  prefs: []
  type: TYPE_NORMAL
- en: The `luminance` function determines the brightness of an RGB value by computing
    a weighted sum of the intensities. The weights are from the ITU-R Recommendation
    Rec. 709\. For more details on this, see the Wikipedia entry for *luma*.
  prefs: []
  type: TYPE_NORMAL
- en: We then compute the squared value of the magnitude of the gradient (in order
    to avoid the square root) and store the result in `g`. If the value of `g` is
    greater than `EdgeThreshold`, we consider the pixel to be on an edge and we output
    a white pixel. Otherwise, we output a solid black pixel.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Sobel operator is somewhat crude and tends to be sensitive to high frequency
    variations in the intensity. A quick look at Wikipedia will guide you to a number
    of other edge detection techniques that may be more accurate. It is also possible
    to reduce the amount of high frequency variation by adding a *blur pass* between
    the render and edge detection passes. The blur pass will smooth out the high frequency
    fluctuations and may improve the results of the edge detection pass.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The technique discussed here requires eight texture fetches. Texture accesses
    can be somewhat slow, and reducing the number of accesses can result in substantial
    speed improvements. Chapter 24 of *GPU Gems: Programming Techniques, Tips and
    Tricks for Real-Time Graphics*, edited by Randima Fernando (Addison-Wesley Professional
    2004), has an excellent discussion of ways to reduce the number of texture fetches
    in a filter operation by making use of so-called *helper* textures.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `chapter06/sceneedge.cpp` file in the example code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'D. Ziou and S. Tabbone (*1998*), *Edge detection techniques: An overview*,
    *International Journal of Computer Vision*, *Vol 24*, *Issue 3*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Frei-Chen edge detector*: [http://rastergrid.com/blog/2011/01/frei-chen-edge-detector/](http://rastergrid.com/blog/2011/01/frei-chen-edge-detector/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Rendering to a texture* recipe in [Chapter 5](a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml),
    *Using Textures*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying a Gaussian blur filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A blur filter can be useful in many different situations where the goal is to
    reduce the amount of noise in the image. As mentioned in the previous recipe,
    applying a blur filter prior to the edge detection pass may improve the results
    by reducing the amount of high frequency fluctuation across the image. The basic
    idea of any blur filter is to mix the color of a pixel with that of nearby pixels
    using a weighted sum. The weights typically decrease with the distance from the
    pixel (in 2D screen space) so that pixels that are far away contribute less than
    those closer to the pixel being blurred.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **Gaussian blur** uses the two-dimensional Gaussian function to weight the
    contributions of the nearby pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/176d86f8-545b-483b-a71d-f039f8430dad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sigma squared term is the **variance** of the Gaussian, and determines
    the width of the Gaussian curve. The Gaussian function is maximum at (0,0), which
    corresponds to the location of the pixel being blurred and its value decreases
    as *x* or *y* increases. The following graph shows the two-dimensional Gaussian
    function with a sigma squared value of 4.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58b45fc2-0640-456d-8bb7-1f92ae40e81b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following images show a portion of an image before (left) and after (right)
    the Gaussian blur operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b0930f16-21ef-4581-83db-3285266192ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To apply a Gaussian blur, for each pixel, we need to compute the weighted sum
    of all pixels in the image scaled by the value of the Gaussian function at that
    pixel (where the *x* and *y* coordinates of each pixel are based on an origin
    located at the pixel being blurred). The result of that sum is the new value for
    the pixel. However, there are two problems with the algorithm so far:'
  prefs: []
  type: TYPE_NORMAL
- en: As this is a *O(n²)* process (where *n* is the number of pixels in the image),
    it is likely to be too slow for real-time use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights must sum to one in order to avoid changing the overall brightness
    of
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the image
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we sampled the Gaussian function at discrete locations, and didn't sum over
    the entire (infinite) bounds of the function, the weights almost certainly do
    not sum to one.
  prefs: []
  type: TYPE_NORMAL
- en: We can deal with both of the preceding problems by limiting the number of pixels
    that we blur with a given pixel (instead of the entire image), and by normalizing
    the values of the Gaussian function. In this example, we'll use a 9 x 9 Gaussian
    blur filter. That is, we'll only compute the contributions of the 81 pixels in
    the neighborhood of the pixel being blurred.
  prefs: []
  type: TYPE_NORMAL
- en: Such a technique would require 81 texture fetches in the fragment shader, which
    is executed once for each pixel. The total number of texture fetches for an image
    of size 800 x 600 would be *800 * 600 * 81 = 38,880,000*. This seems like a lot,
    doesn't it? The good news is that we can substantially reduce the number of texture
    fetches by doing the Gaussian blur in two passes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two-dimensional Gaussian function can be decomposed into the product of
    two one-dimensional Gaussians:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/049816d2-d752-4b1f-ae5d-fce015f42579.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where the one-dimensional Gaussian function is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9559cd7f-1c79-44d0-8fdd-6a0f1d61cb84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So if *C[ij]* is the color of the pixel at pixel location (i, j), the sum that
    we need to compute is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/677564d4-341f-4106-ad15-2724de2feb47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be re-written using the fact that the two-dimensional Gaussian is
    a product of two one-dimensional Gaussians:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e10f9ad-b403-481f-a342-8bc9b63d63ec.png)'
  prefs: []
  type: TYPE_IMG
- en: This implies that we can compute the Gaussian blur in two passes. In the first
    pass, we can compute the sum over *j* (the vertical sum) in the preceding equation
    and store the results in a temporary texture. In the second pass, we compute the
    sum over *i* (the horizontal sum) using the results from the previous pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, before we look at the code, there is one important point that has to be
    addressed. As we mentioned previously, the Gaussian weights must sum to one in
    order to be a true weighted average. Therefore, we need to normalize our Gaussian
    weights, as in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a66eea6d-85ae-4dac-a260-644243f99225.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The value of *k* in the preceding equation is just the sum of the raw Gaussian
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a92509c-23af-4070-815d-1f9bba693e15.png)'
  prefs: []
  type: TYPE_IMG
- en: Phew! We've reduced the *O(n^([2]))* problem to one that is *O(n)*. OK, with
    that, let's move on to the code.
  prefs: []
  type: TYPE_NORMAL
- en: We'll implement this technique using three passes and two textures. In the first
    pass, we'll render the entire scene to a texture. Then, in the second pass, we'll
    apply the first (vertical) sum to the texture from the first pass and store the
    results in another texture. Finally, in the third pass, we'll apply the horizontal
    sum to the texture from the second pass, and send the results to the default framebuffer.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Set up two framebuffer objects (refer to the *Rendering to a texture* recipe
    in [Chapter 5](a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml), *Using Textures*),
    and two corresponding textures. The first FBO should have a depth buffer because
    it will be used for the first pass. The second FBO need not have a depth buffer
    because, in the second and third passes, we'll only render a single screen-filling
    quad in order to execute the fragment shader once for each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the previous recipe, we''ll use a uniform variable to select the functionality
    of each pass. The OpenGL program should also set the following uniform variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Width`: This is used to set the width of the screen in pixels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Height`: This is used to set the height of the screen in pixels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Weight[]`: This is the array of normalized Gaussian weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Texture0`: This is to set this to texture unit zero'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PixOffset[]`: This is the array of offsets from the pixel being blurred'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the fragment shader, we apply the Blinn-Phong reflection model in the first
    pass. In the second pass, we compute the vertical sum. In the third, we compute
    the horizontal sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the OpenGL application, compute the Gaussian weights for the offsets found
    in the uniform variable `PixOffset`, and store the results in the array `Weight`.
    You could use the following code to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the main render function, implement the following steps for pass #1:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the render framebuffer, enable the depth test, and clear the color/depth
    buffers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `Pass` to `1`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw the scene
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the following steps for pass #2:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the intermediate framebuffer, disable the depth test, and clear the color
    buffer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `Pass` to `2`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the view, projection, and model matrices to the identity matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bind the texture from pass #1 to texture unit zero'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a fullscreen quad
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the following steps for pass #3:'
  prefs: []
  type: TYPE_NORMAL
- en: Deselect the framebuffer (revert to the default), and clear the color buffer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `Pass` to `3`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bind the texture from pass #2 to texture unit zero'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a fullscreen quad
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding code for computing the Gaussian weights (code segment 3), the
    function named `gauss` computes the one-dimensional Gaussian function where the
    first argument is the value for `x` and the second argument is sigma squared.
    Note that we only need to compute the positive offsets because the Gaussian is
    symmetric about zero. As we are only computing the positive offsets, we need to
    carefully compute the sum of the weights. We double all of the non-zero values
    because they will be used twice (for the positive and negative offsets).
  prefs: []
  type: TYPE_NORMAL
- en: The first pass (function `pass1`) renders the scene to a texture using the Blinn-Phong
    reflection model.
  prefs: []
  type: TYPE_NORMAL
- en: The second pass (function `pass2`) applies the weighted vertical sum of the
    Gaussian blur operation, and stores the results in yet another texture. We read
    pixels from the texture created in the first pass, offset in the vertical direction
    by the amounts in the `PixOffset` array. We sum using weights from the `Weight`
    array. (The `dy` term is the height of a texel in texture coordinates.) We sum
    in both directions at the same time, a distance of four pixels in each vertical
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: The third pass (`pass3`) is very similar to the second pass. We accumulate the
    weighted, horizontal sum using the texture from the second pass. By doing so,
    we are incorporating the sums produced in the second pass into our overall weighted
    sum, as described earlier. Thereby, we are creating a sum over a 9 x 9 pixel area
    around the destination pixel. For this pass, the output color goes to the default
    framebuffer to make up the final result.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, we can also adapt the preceding technique to blur a larger range
    of texels by increasing the size of the arrays `Weight` and `PixOffset` and re-computing
    the weights, and/or we could use different values of `sigma2` to vary the shape
    of the Gaussian.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `chapter06/sceneblur.cpp` file in the example code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bilateral filtering: [http://people.csail.mit.edu/sparis/bf_course/](http://people.csail.mit.edu/sparis/bf_course/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Rendering to a texture* recipe in [Chapter 5](a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml),
    *Using Textures*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Applying an edge detection filter* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing HDR lighting with tone mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When rendering for most output devices (monitors or televisions), the device
    only supports a typical color precision of 8-bits per color component, or 24-bits
    per pixel. Therefore, for a given color component, we're limited to a range of
    intensities between 0 and 255\. Internally, OpenGL uses floating-point values
    for color intensities, providing a wide range of both values and precision. These
    are eventually converted to 8-bit values by mapping the floating-point range [0.0,
    1.0] to the range of an unsigned byte [0, 255] before rendering.
  prefs: []
  type: TYPE_NORMAL
- en: Real scenes, however, have a much wider range of luminance. For example, light
    sources that are visible in a scene, or direct reflections of them, can be hundreds
    to thousands of times brighter than the objects that are illuminated by the source.
    When we're working with 8-bits per channel, or the floating-point range [0.0,
    -1.0], we can't represent this range of intensities. If we decide to use a larger
    range of floating point values, we can do a better job of internally representing
    these intensities, but in the end, we still need to compress down to the 8-bit
    range.
  prefs: []
  type: TYPE_NORMAL
- en: The process of computing the lighting/shading using a larger dynamic range is
    often referred to as **High Dynamic Range rendering** (**HDR rendering**). Photographers
    are very familiar with this concept. When a photographer wants to capture a larger
    range of intensities than would normally be possible in a single exposure, he/she
    might take several images with different exposures to capture a wider range of
    values. This concept, called **High Dynamic Range imaging** (**HDR imaging**),
    is very similar in nature to the concept of HDR rendering. A post-processing pipeline
    that includes HDR is now considered a fundamentally essential part of any game
    engine.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tone mapping** is the process of taking a wide dynamic range of values and
    compressing them into a smaller range that is appropriate for the output device.
    In computer graphics, generally, tone mapping is about mapping to the 8-bit range
    from some arbitrary range of values. The goal is to maintain the dark and light
    parts of the image so that both are visible and neither is completely *washed
    out*.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, a scene that includes a bright light source might cause our shading
    model to produce intensities that are greater than 1.0\. If we were to simply
    send that to the output device, anything greater than 1.0 would be clamped to
    255 and would appear white. The result might be an image that is mostly white,
    similar to a photograph that is over exposed.
  prefs: []
  type: TYPE_NORMAL
- en: Or, if we were to linearly compress the intensities to the [0, 255] range, the
    darker parts might be too dark or completely invisible. With tone mapping, we
    want to maintain the brightness of the light source and also maintain detail in
    the darker areas.
  prefs: []
  type: TYPE_NORMAL
- en: This description just scratches the surface when it comes to tone mapping and
    HDR rendering/imaging. For more details, I recommend the book *High Dynamic Range
    Imaging* by Reinhard et al.
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical function used to map from one dynamic range to a smaller range
    is called the **Tone Mapping Operator** (**TMO**). These generally come in two
    flavors, local operators and global operators. A local operator determines the
    new value for a given pixel by using its current value and perhaps the value of
    some nearby pixels. A global operator needs some information about the entire
    image in order to do its work. For example, it might need to have the overall
    average luminance of all pixels in the image. Other global operators use a histogram
    of luminance values over the entire image to help fine-tune the mapping.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll use the simple global operator described in the book
    *Real Time Rendering*. This operator uses the log-average luminance of all pixels
    in the image. The log-average is determined by taking the logarithm of the luminance
    and averaging those values, then converting back, as shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4829befe-6c06-4f97-b736-ff0be2195116.png)'
  prefs: []
  type: TYPE_IMG
- en: '*L[w](x, y)* is the luminance of the pixel at *(x, y)*. The *0.0001* term is
    included in order to avoid taking the logarithm of zero for black pixels. This
    log-average is then used as part of the tone mapping operator shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4d7308f-de54-4830-b68f-f947084806dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *a* term in this equation is the key. It acts in a similar way to the exposure
    level in a camera. The typical values for *a* range from 0.18 to 0.72\. Since
    this tone mapping operator compresses the dark and light values a bit too much,
    we''ll use a modification of the previous equation that doesn''t compress the
    dark values as much, and includes a maximum luminance (*L[white]*), a configurable
    value that helps to reduce some of the extremely bright pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4be147f8-a6dd-4961-b002-348bbb5437df.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the tone mapping operator that we'll use in this example. We'll render
    the scene to a high-resolution buffer, compute the log-average luminance, and
    then apply the previous tone-mapping operator in a second pass.
  prefs: []
  type: TYPE_NORMAL
- en: However, there's one more detail that we need to deal with before we can start
    implementing. The previous equations all deal with luminance. Starting with an
    RGB value, we can compute its luminance, but once we modify the luminance, how
    do we modify the RGB components to reflect the new luminance without changing
    the hue (or chromaticity)?
  prefs: []
  type: TYPE_NORMAL
- en: The **chromaticity** is the perceived color, independent of the brightness of
    that color. For example, grey and white are two brightness levels for the same
    color.
  prefs: []
  type: TYPE_NORMAL
- en: The solution involves switching color spaces. If we convert the scene to a color
    space that separates out the luminance from the chromaticity, then we can change
    the luminance value independently. The **CIE XYZ** color space has just what we
    need. The CIE XYZ color space was designed so that the *Y* component describes
    the luminance of the color and the chromaticity can be determined by two derived
    parameters (*x* and *y*). The derived color space is called the **CIE xyY** space,
    and is exactly what we're looking for. The *Y* component contains the luminance
    and the *x* and *y* components contain the chromaticity. By converting to the
    *CIE xyY* space, we've factored out the luminance from the chromaticity allowing
    us to change the luminance without affecting the perceived color.
  prefs: []
  type: TYPE_NORMAL
- en: So the process involves converting from RGB to CIE XYZ, then converting to CIE
    xyY, modifying the luminance, and reversing the process to get back to RGB. Converting
    from RGB to CIE XYZ (and vice-versa) can be described as a transformation matrix
    (refer to the code or the *See also* section for the matrix).
  prefs: []
  type: TYPE_NORMAL
- en: 'The conversion from XYZ to xyY involves the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36de4b90-038e-4504-93da-cae3032f2530.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, converting from xyY back to XYZ is done using the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5eced92f-b4a5-4b83-8757-5be68a3558d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following images show an example of the results of this tone mapping operator.
    The left image shows the scene rendered without any tone mapping. The shading
    was deliberately calculated with a wide dynamic range using three strong light
    sources. The scene appears *blown out* because any values that are greater than
    1.0 simply get clamped to the maximum intensity. The image on the right uses the
    same scene and the same shading, but with the previous tone mapping operator applied.
    Note the recovery of the specular highlights from the *blown out* areas on the
    sphere and teapot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f514e34-78dc-4e56-8b09-e45613529447.png)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps involved are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Render the scene to a high-resolution texture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the log-average luminance (on the CPU).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Render a screen-filling quad to execute the fragment shader for each screen
    pixel. In the fragment shader, read from the texture created in step 1, apply
    the tone mapping operator, and send the results to the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To get set up, create a high-res texture (using `GL_RGB32F` or a similar format)
    attached to a framebuffer with a depth attachment. Set up your fragment shader
    with a uniform to select the pass. The vertex shader can simply pass through the
    position and normal in eye coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To implement HDR tone mapping, we''ll perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first pass, we want to just render the scene to the high-resolution texture.
    Bind to the framebuffer that has the texture attached and render the scene normally.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the log average luminance of the pixels in the texture. To do so, we''ll
    pull the data from the texture and loop through the pixels on the CPU side. We
    do this on the CPU for simplicity; a GPU implementation, perhaps with a compute
    shader, would be faster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the `AveLum` uniform variable using `logAve`. Switch back to the default
    frame buffer, and draw a screen-filling quad. In the fragment shader, apply the
    tone mapping operator to the values from the texture produced in step 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first step, we render the scene to an HDR texture. In step 2, we compute
    the log-average luminance by retrieving the pixels from the texture and doing
    the computation on the CPU (OpenGL side).
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we render a single screen-filling quad to execute the fragment shader
    for each screen pixel. In the fragment shader, we retrieve the HDR value from
    the texture and apply the tone-mapping operator. There are two *tunable* variables
    in this calculation. The `Exposure` variable corresponds to the *a* term in the
    tone mapping operator, and the variable `White` corresponds to *L[white]*. For
    the previous image, we used values of `0.35` and `0.928`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tone mapping is not an exact science. Often, it is the process of experimenting
    with the parameters until you find something that works well and looks good.
  prefs: []
  type: TYPE_NORMAL
- en: We could improve the efficiency of the previous technique by implementing step
    2 on the GPU using compute shaders (refer to [Chapter 11](d67e01c8-8212-4d49-937f-6b1c62a57744.xhtml),
    *Using Compute Shaders*) or some other clever technique. For example, we could
    write the logarithms to a texture, then iteratively downsample the full frame
    to a 1 x 1 texture. The final result would be available in that single pixel.
    However, with the flexibility of the compute shader, we could optimize this process
    even more.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `chapter06/scenetonemap.cpp` file in the example code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bruce Justin Lindbloom has provided a useful web resource for conversion between
    color spaces. It includes among other things the transformation matrices needed
    to convert from RGB to XYZ. Visit: [http://www.brucelindbloom.com/index.html?Eqn_XYZ_to_RGB.html](http://www.brucelindbloom.com/index.html?Eqn_XYZ_to_RGB.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Rendering to a texture* recipe in [Chapter 5](a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml),
    *Using Textures*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a bloom effect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **bloom** is a visual effect where the bright parts of an image seem to have
    fringes that extend beyond the boundaries into the darker parts of the image.
    This effect has its basis in the way that cameras and the human visual system
    perceive areas of high contrast. Sources of bright light *bleed* into other areas
    of the image due to the so-called **airy disc**, which is a diffraction pattern
    produced by light that passes through an aperture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows a bloom effect in the animated film Elephant''s Dream
    (© 2006, Blender Foundation / Netherlands Media Art Institute / [www.elephantsdream.org](https://orange.blender.org/)).
    The bright white color from the light behind the door *bleeds* into the darker
    parts of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5a4b8cd-0e82-4ad9-bc80-d092d4ca280f.png)'
  prefs: []
  type: TYPE_IMG
- en: Producing such an effect within an artificial CG rendering requires determining
    which parts of the image are bright enough, extracting those parts, blurring,
    and re-combining with the original image. Typically, the bloom effect is associated
    with HDR rendering. With HDR rendering, we can represent a larger range of intensities
    for each pixel (without quantizing artifacts). The bloom effect is more accurate
    when used in conjunction with HDR rendering due to the fact that a wider range
    of brightness values can be represented.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the fact that HDR produces higher quality results, it is still possible
    to produce a bloom effect when using standard (non-HDR) color values. The result
    may not be as effective, but the principles involved are similar for either situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we''ll implement a bloom effect using five passes,
    consisting of four major steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first pass, we will render the scene to an HDR texture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second pass will extract the parts of the image that are brighter than a
    certain threshold value. We'll refer to this as the **bright-pass filter**. We'll
    also downsample to a lower resolution buffer when applying this filter. We do
    so because we will gain additional blurring of the image when we read back from
    this buffer using a linear sampler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The third and fourth passes will apply the Gaussian blur to the bright parts
    (refer to the *Applying a Gaussian blur filter* recipe in this chapter).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the fifth pass, we'll apply tone mapping and add the tone-mapped result to
    the blurred bright-pass filter results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following diagram summarizes this process. The upper-left image shows the
    scene rendered to an HDR buffer, with some of the colors out of gamut, causing
    much of the image to be *blown-out*. The bright-pass filter produces a smaller
    (about a quarter or an eighth of the original size) image with only pixels that
    correspond to a luminance that is above a threshold. The pixels are shown as white
    because they have values that are greater than one in this example. A two-pass
    Gaussian blur is applied to the downsampled image, and tone mapping is applied
    to the original image. The final image is produced by combining the tone-mapped
    image with the blurred bright-pass filter image. When sampling the latter, we
    use a linear filter to get additional blurring.The final result is shown at the
    bottom.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the bloom on the bright highlights on the sphere and the back wall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7265b5a6-cb42-4b43-b879-96d34ee8589b.png)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this recipe, we'll need two framebuffer objects, each associated with a
    texture. The first will be used for the original HDR render, and the second will
    be used for the two passes of the Gaussian blur operation. In the fragment shader,
    we'll access the original render via the variable `HdrTex`, and the two stages
    of the Gaussian blur will be accessed via `BlurTex`.
  prefs: []
  type: TYPE_NORMAL
- en: The uniform variable `LumThresh` is the minimum luminance value used in the
    second pass. Any pixels greater than that value will be extracted and blurred
    in the following passes.
  prefs: []
  type: TYPE_NORMAL
- en: Use a vertex shader that passes through the position and normal in eye coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To generate a bloom effect, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first pass, render the scene to the framebuffer with a high-res backing
    texture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the second pass, switch to a framebuffer containing a high-res texture that
    is smaller than the size of the full render. In the example code, we use a texture
    that is one-eighth the size. Draw a fullscreen quad to initiate the fragement
    shader for each pixel, and in the fragment shader sample from the high-res texture,
    and write only those values that are larger than `LumThresh`. Otherwise, color
    the pixel black:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the third and fourth passes, apply the Gaussian blur to the results of the
    second pass. This can be done with a single framebuffer and two textures. Ping-pong
    between them, reading from one and writing to the other. For details, refer to
    the *Applying a Gaussian blur filter* recipe in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the fifth and final pass, switch to linear filtering from the texture that
    was produced in the fourth pass. Switch to the default frame buffer (the screen).
    Apply the tone-mapping operator from the *Implementing HDR lighting with tone
    mapping* recipe to the original image texture (`HdrTex`), and combine the results
    with the blurred texture from step 3\. The linear filtering and magnification
    should provide an additional blur:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to space constraints, the entire fragment shader code isn't shown here.
    The full code is available from the GitHub repository. The fragment shader is
    implemented with five methods, one for each pass. The first pass renders the scene
    normally to the HDR texture. During this pass, the active framebuffer object is
    the one associated with the texture corresponding to `HdrTex`, so the output is
    sent directly to that texture.
  prefs: []
  type: TYPE_NORMAL
- en: The second pass reads from `HdrTex`, and writes out only pixels that have a
    luminance above the threshold value `LumThresh`. The value is (0,0,0,0) for pixels
    that have a brightness (luma) value below `LumThresh`. The output goes to the
    second framebuffer, which contains a much smaller texture (one-eighth the size
    of the original).
  prefs: []
  type: TYPE_NORMAL
- en: The third and fourth passes apply the basic Gaussian blur operation (refer to
    the *Applying a Gaussian blur filter* recipe in this chapter). In these passes,
    we ping-pong between `BlurTex1` and `BlurTex2`, so we must be careful to swap
    the appropriate texture into the framebuffer.
  prefs: []
  type: TYPE_NORMAL
- en: In the fifth pass, we switch back to the default framebuffer, and read from
    `HdrTex` and `BlurTex1`. `BlurTex1` contains the final blurred result from step
    four, and `HdrTex` contains the original render. We apply tone mapping to the
    results of `HdrTex` and add to `BlurTex1`. When pulling from `BlurTex1`, we are
    applying a linear filter, gaining additional blurring.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that we applied the tone-mapping operator to the original rendered image,
    but not to the blurred bright-pass filter image. One could choose to apply the
    TMO to the blurred image as well, but in practice, it is often not necessary.
    We should keep in mind that the bloom effect can also be visually distracting
    if it is overused. A little goes a long way.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `chapter06/scenehdrbloom.cpp` file in the example code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HDR meets Black & White 2* by Francesco Caruzzi in *Shader X6*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Rendering to a texture* recipe in [Chapter 5](a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml),
    *Using Textures*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Applying an edge detection filter* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using gamma correction to improve image quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is common for many books about OpenGL and 3D graphics to somewhat neglect
    the subject of gamma correction. Lighting and shading calculations are performed,
    and the results are sent directly to the output buffer without modification. However,
    when we do this, we may produce results that don't quite end up looking the way
    we might expect. This may be due to the fact that computer monitors (both the
    old CRT and the newer LCD) have a non-linear response to pixel intensity. For
    example, without gamma correction, a grayscale value of 0.5 will not appear half
    as bright as a value of 1.0\. Instead, it will appear to be darker than it should.
  prefs: []
  type: TYPE_NORMAL
- en: 'The lower curve in the following graph shows the response curves of a typical
    monitor (gamma of **2.2**). The *x* axis is the intensity and the *y* axis is
    the perceived intensity. The dashed line represents a linear set of intensities.
    The upper curve represents gamma correction applied to linear values. The lower
    curve represents the response of a typical monitor. A grayscale value of **0.5**
    would appear to have a value of **0.218** on a screen that had a similar response
    curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50cc84d4-57b2-4ad6-8c16-6deeed516115.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The non-linear response of a typical monitor can usually be modeled using a
    simple power function. The perceived intensity (*P*) is proportional to the pixel
    intensity (*I*) raised to a power that is usually called gamma:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef0da10a-863e-47ba-b6bb-698d998a9f89.png)'
  prefs: []
  type: TYPE_IMG
- en: Depending on the display device, the value of *γ* is usually somewhere between
    2.0 and 2.4\. Some kind of monitor calibration is often needed to determine a
    precise value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to compensate for this non-linear response, we can apply **gamma correction**
    before sending our results to the output framebuffer. Gamma correction involves
    raising the pixel intensities to a power that will compensate for the monitor''s
    non-linear response to achieve a perceived result that appears linear. Raising
    the linear-space values to the power of *1/γ* will do the trick:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4221d774-e06e-46f9-ba66-e9f8d89c78ea.png)'
  prefs: []
  type: TYPE_IMG
- en: When rendering, we can do all of our lighting and shading computations, ignoring
    the fact that the monitor's response curve is non-linear. This is sometimes referred
    to as working in *linear space*. When the final result is to be written to the
    output framebuffer, we can apply the gamma correction by raising the pixel to
    the power of 1/γ just before writing. This is an important step that will help
    to improve the look of the rendered result.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider the following images. The image on the left is the
    mesh rendered without any consideration of gamma at all. The reflection model
    is computed and the results are directly sent to the framebuffer. On the right
    is the same mesh with gamma correction applied to the color just prior to output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fd4aef2-aafe-476b-bcd5-e22fc760592c.png)'
  prefs: []
  type: TYPE_IMG
- en: The obvious difference is that the left image appears much darker than the image
    on the right. However, the more important distinction is the variations from light
    to dark across the face. While the transition at the shadow terminator seems stronger
    than before, the variations within the lighted areas are less extreme.
  prefs: []
  type: TYPE_NORMAL
- en: Applying gamma correction is an important technique, and can be effective in
    improving the results of a lighting model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adding gamma correction to an OpenGL program can be as simple as carrying out
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up a uniform variable named `Gamma` and set it to an appropriate value for
    your system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the following code or something similar in a fragment shader:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If your shader involves texture data, care must be taken to make sure that the
    texture data is not already gamma-corrected so that you don't apply gamma correction
    twice (refer to the *There's more...* section of this recipe).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The color determined by the lighting/shading model is computed and stored in
    the variable `color`. We think of this as computing the color in linear space.
    There is no consideration of the monitor's response during the calculation of
    the shading model (assuming that we don't access any texture data that might already
    be gamma-corrected).
  prefs: []
  type: TYPE_NORMAL
- en: To apply the correction, in the fragment shader, we raise the color of the pixel
    to the power of `1.0 / Gamma`, and apply the result to the output variable `FragColor`.
    Of course, the inverse of `Gamma` could be computed outside the fragment shader
    to avoid the division operation.
  prefs: []
  type: TYPE_NORMAL
- en: We do not apply the gamma correction to the alpha component because it is typically
    not desired.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The application of gamma correction is a good idea in general; however, some
    care must be taken to make sure that computations are done within the correct
    space. For example, textures could be photographs or images produced by other
    imaging applications that apply gamma correction before storing the data within
    the image file.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, if we use a texture in our application as a part of the lighting
    model and then apply gamma correction, we will be effectively applying gamma correction
    twice to the data from the texture. Instead, we need to be careful to "decode"
    the texture data, by raising to the power of gamma prior to using the texture
    data in our lighting model.
  prefs: []
  type: TYPE_NORMAL
- en: There is a very detailed discussion about these and other issues surrounding
    gamma correction in Chapter 24, *The Importance of Being Linear* in the book *GPU
    Gems 3*, edited by Hubert Nguyen (Addison-Wesley Professional 2007), and this
    is highly recommended supplemental reading.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `chapter06/scenegamma.cpp` file in the example code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multisample anti-aliasing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Anti-aliasing** is the technique of removing or reducing the visual impact
    of **aliasing artifacts** that are present whenever high-resolution or continuous
    information is presented at a lower resolution. In real-time graphics, aliasing
    often reveals itself in the jagged appearance of polygon edges, or the visual
    distortion of textures that have a high degree of variation.'
  prefs: []
  type: TYPE_NORMAL
- en: The following images show an example of aliasing artifacts at the edge of an
    object. On the left, we can see that the edge appears jagged. This occurs because
    each pixel is determined to lie either completely inside the polygon or completely
    outside it. If the pixel is determined to be inside, it is shaded, otherwise it
    is not. Of course, this is not entirely accurate. Some pixels lie directly on
    the edge of the polygon. Some of the screen area that the pixel encompasses actually
    lies within the polygon and some lies outside. Better results could be achieved
    if we were to modify the shading of a pixel based upon the amount of the pixel's
    area that lies within the polygon. The result could be a mixture of the shaded
    surface's color with the color outside the polygon, where the area that is covered
    by the pixel determines the proportions. You might be thinking that this sounds
    like it would be prohibitively expensive to do. That may be true; however, we
    can approximate the results by using multiple **samples** per pixel.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multisample anti-aliasing** involves evaluating multiple samples per pixel
    and combining the results of those samples to determine the final value for the
    pixel. The samples are located at various points within the pixel''s extent. Most
    of these samples will fall inside the polygon, but for pixels near a polygon''s
    edge, some will fall outside. The fragment shader will typically execute only
    once for each pixel as usual. For example, with 4x **multisample anti-aliasing**
    (**MSAA**), rasterization happens at four times the frequency. For each pixel,
    the fragment shader is executed once and the result is scaled based on how many
    of the four samples fall within the polygon.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image on the right shows the results when multisample anti-aliasing
    is used. The inset image is a zoomed portion of the inside edge of a torus. On
    the left, the torus is rendered without MSAA. The right-hand image shows the results
    with MSAA enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48ab445e-7ee5-4686-b867-0b6bb88f129f.png)'
  prefs: []
  type: TYPE_IMG
- en: OpenGL has supported multisampling for some time now, and it is nearly transparent
    to use. It is simply a matter of turning it on or off. It works by using additional
    buffers to store the subpixel samples as they are processed. Then, the samples
    are combined together to produce a final color for the fragment. Nearly all of
    this is automatic, and there is little that a programmer can do to fine-tune the
    results. However, at the end of this recipe, we'll discuss the interpolation qualifiers
    that can affect the results.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll see the code needed to enable multisample anti-aliasing
    in an OpenGL application.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The technique for enabling multisampling is unfortunately dependent on the window
    system API. In this example, we'll demonstrate how it is done using GLFW. The
    steps will be similar in GLUT or other APIs that support OpenGL.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make sure that the multisample buffers are created and available, use the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating your OpenGL window, you need to select an OpenGL context that
    supports MSAA. The following is how one would do so in GLFW:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To determine whether multisample buffers are available and how many samples
    per-pixel are actually being used, you can use the following code (or something
    similar):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To enable multisampling, use the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To disable multisampling, use the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we just mentioned, the technique for creating an OpenGL context with multisample
    buffers is dependent on the API used for interacting with the window system. The
    preceding example demonstrates how it might be done using GLFW. Once the OpenGL
    context is created, it is easy to enable multisampling by simply using the `glEnable`
    call shown in the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: Stay tuned, because in the next section, we'll discuss a subtle issue surrounding
    interpolation of shader variables when multisample anti-aliasing is enabled.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two interpolation qualifiers within the GLSL that allow the programmer
    to fine-tune some aspects of multisampling. They are `sample` and `centroid`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can get into how `sample` and `centroid` work, we need a bit of background.
    Let''s consider the way that polygon edges are handled without multisampling.
    A fragment is determined to be inside or outside of a polygon by determining where
    the center of that pixel lies. If the center is within the polygon, the pixel
    is shaded, otherwise it is not. The following image represents this behavior.
    It shows pixels near a polygon edge without MSAA. The line represents the edge
    of the polygon. Gray pixels are considered to be inside the polygon. White pixels
    are outside and are not shaded. The dots represent the pixel centers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/749a3226-d018-4f8f-8944-265097202507.png)'
  prefs: []
  type: TYPE_IMG
- en: The values for the interpolated variables (the fragment shader's input variables)
    are interpolated with respect to the center of each fragment, which will always
    be inside the polygon.
  prefs: []
  type: TYPE_NORMAL
- en: When multisample anti-aliasing is enabled, multiple samples are computed per
    fragment at various locations within the fragment's extent. If any of those samples
    lie within the polygon, then the shader is executed at least once for that pixel
    (but not necessarily for each sample).
  prefs: []
  type: TYPE_NORMAL
- en: 'As a visual example, the following image represents pixels near a polygon''s
    edge. The dots represent the samples. The dark samples lie within the polygon
    and the white samples lie outside the polygon. If any sample lies within the polygon,
    the fragment shader is executed (usually only once) for that pixel. Note that
    for some pixels, the pixel center lies outside the polygon. So, with MSAA, the
    fragment shader may execute slightly more often near the edges of polygons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bba2698a-8fc3-40ce-8440-9f9a3db25a3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, here's the important point. The values of the fragment shader's input variables
    are normally interpolated to the center of the pixel rather than to the location
    of any particular sample. In other words, the value that is used by the fragment
    shader is determined by interpolating to the location of the fragment's center,
    which may lie outside the polygon! If we are relying on the fact that the fragment
    shader's input variables are interpolated strictly between their values at the
    vertices (and not outside that range), then this might lead to unexpected results.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider the following portion of a fragment shader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This shader is designed to color the polygon black unless the `s` component
    of the texture coordinate is greater than one. In that case, the fragment gets
    a yellow color. If we render a square with texture coordinates that range from
    zero to one in each direction, we may get the results shown in the following image
    on the left. The following images show the enlarged edge of a polygon where the
    `s` texture coordinate is about `1.0`. Both images were rendered using the preceding
    shader. The right-hand image was created using the `centroid` qualifier (more
    on this later in this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/434513f5-5d71-4f11-9093-e3de16063b00.png)'
  prefs: []
  type: TYPE_IMG
- en: The left image shows that some pixels along the edge have a lighter color (yellow,
    if the image is in full color). This is due to the fact that the texture coordinate
    is interpolated to the pixel's center, rather than to any particular sample's
    location. Some of the fragments along the edge have a center that lies outside
    of the polygon and therefore end up with a texture coordinate that is greater
    than one!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can ask OpenGL to instead compute the value for the input variable by interpolating
    to some location that is not only within the pixel but also within the polygon.
    We can do so by using the `centroid` qualifier, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: (The qualifier needs to also be included with the corresponding output variable
    in the vertex shader.) When `centroid` is used with the preceding shader, we get
    the preceding image shown on the right.
  prefs: []
  type: TYPE_NORMAL
- en: In general, we should use `centroid` or `sample` when we know that the interpolation
    of the input variables should not extend beyond the values of those variables
    at the vertices.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sample` qualifier forces OpenGL to interpolate the shader''s input variables
    to the actual location of the sample itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This, of course, requires that the fragment shader be executed once for each
    sample. This will produce the most accurate results, but the performance hit may
    not be worthwhile, especially if the visual results produced by `centroid` (or
    without the default) are good enough.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `chapter06/scenemsaa.cpp` file in the example code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using deferred shading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deferred shading** is a technique that involves postponing (or *deferring*)
    the lighting/shading step to a second pass. We do this (among other reasons) in
    order to avoid shading a pixel more than once. The basic idea is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first pass, we render the scene, but instead of evaluating the reflection
    model to determine a fragment color, we simply store all of the geometry information
    (position, normal, texture coordinate, reflectivity, and so on) in an intermediate
    set of buffers, collectively called the **g-buffer** (g for geometry).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the second pass, we simply read from the g-buffer, evaluate the reflection
    model, and produce a final color for each pixel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When deferred shading is used, we avoid evaluating the reflection model for
    a fragment that will not end up being visible. For example, consider a pixel located
    in an area where two polygons overlap. The fragment shader may be executed once
    for each polygon that covers that pixel; however, the resulting color of only
    one of the two executions will end up being the final color for that pixel (assuming
    that blending is not enabled). The cycles spent in evaluating the reflection model
    for one of the two fragments are effectively wasted. With deferred shading, the
    evaluation of the reflection model is postponed until all the geometry has been
    processed, and the visible geometry is known at each pixel location. Hence, the
    reflection model is evaluated only once for each pixel on the screen. This allows
    us to do lighting in a more efficient fashion. For example, we could use even
    hundreds of light sources because we are only evaluating the lighting once per
    screen pixel.
  prefs: []
  type: TYPE_NORMAL
- en: Deferred shading is fairly simple to understand and work with. It can therefore
    help with the implementation of complex lighting/reflection models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll go through a simple example of deferred shading. We''ll
    store the following information in our g-buffer: the position, normal, and diffuse
    color (the diffuse reflectivity). In the second pass, we''ll simply evaluate the
    diffuse lighting model using the data stored in the g-buffer.'
  prefs: []
  type: TYPE_NORMAL
- en: This recipe is meant to be a starting point for deferred shading. If we were
    to use deferred shading in a more substantial (real-world) application, we'd probably
    need more components in our g-buffer. It should be straightforward to extend this
    example to use more complex lighting/shading models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The g-buffer will contain three textures for storing the position, normal,
    and diffuse color. There are three uniform variables that correspond to these
    three textures: `PositionTex`, `NormalTex`, and `ColorTex`; these textures should
    be assigned to texture units `0`, `1`, and `2`, respectively. Likewise, the vertex
    shader assumes that position information is provided in vertex attribute `0`,
    the normal is provided in attribute `1`, and the texture coordinate in attribute
    `2`.'
  prefs: []
  type: TYPE_NORMAL
- en: The fragment shader has several uniform variables related to light and material
    properties that must be set from the OpenGL program. Specifically, the structures
    `Light` and `Material` apply to the shading model used here.
  prefs: []
  type: TYPE_NORMAL
- en: You'll need a variable named `deferredFBO` (type `GLuint`) to store the handle
    to the FBO.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create the framebuffer object that contains our g-buffer(s) use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: During the first pass, the fragment shader writes to the G-buffers.  In the
    second pass, it reads from them and applies the shading model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `render` function of the OpenGL application, use the following steps
    for pass #1:'
  prefs: []
  type: TYPE_NORMAL
- en: Bind to the framebuffer object `deferredFBO`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clear the color/depth buffers, set `Pass` to `1`, and enable the depth test
    (if necessary)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Render the scene normally
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the following steps for pass #2:'
  prefs: []
  type: TYPE_NORMAL
- en: Revert to the default FBO (bind to framebuffer 0)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clear the color buffer, set `Pass` to `2`, and disable the depth test (if desired)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Render a screen-filling quad (or two triangles) with texture coordinates that
    range from zero to one in each direction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When setting up the FBO for the g-buffer, we use textures with the internal
    format `GL_RGB32F` for the position and normal components. As we are storing geometry
    information, rather than simply color information, there is a need to use a higher
    resolution (that is more bits per pixel). The buffer for the diffuse reflectivity
    just uses `GL_RGB8` since we don't need the extra resolution for these values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three textures are then attached to the framebuffer at color attachments
    `0`, `1`, and `2` using `glFramebufferTexture2D`. They are then connected to the
    fragment shader''s output variables with the call to `glDrawBuffers`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The array `drawBuffers` indicates the relationship between the framebuffer's
    components and the fragment shader's output variable locations. The *i*^(th) item
    in the array corresponds to the *i*^(th) output variable location. This call sets
    color attachments `0`, `1`, and `2` to output variable locations `1`, `2`, and
    `3`, respectively. (Note that the fragment shader's corresponding variables are
    `PositionData`, `NormalData`, and `ColorData`.)
  prefs: []
  type: TYPE_NORMAL
- en: During pass 2, it is not strictly necessary to convert and pass through the
    normal and position, as they will not be used in the fragment shader at all. However,
    to keep things simple, this optimization is not included. It would be a simple
    matter to add a subroutine to the vertex shader in order to *switch off* the conversion
    during pass 2\. (Of course, we need to set `gl_Position` regardless.)
  prefs: []
  type: TYPE_NORMAL
- en: In the fragment shader, the functionality depends on the value of the variable
    `Pass`. It will either call `pass1` or `pass2`, depending on its value. In the
    `pass1` function, we store the values of `Position`, `Normal`, and `Material.Kd`
    in the appropriate output variables, effectively storing them in the textures
    that we just talked about.
  prefs: []
  type: TYPE_NORMAL
- en: In the `pass2` function, the values of the position, normal, and color are retrieved
    from the textures, and used to evaluate the diffuse lighting model. The result
    is then stored in the output variable `FragColor`. In this pass, `FragColor` should
    be bound to the default framebuffer, so the results of this pass will appear on
    the screen.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the graphics community, the relative advantages and disadvantages of deferred
    shading are a source of some debate. Deferred shading is not ideal for all situations.
    It depends greatly on the specific requirements of your application, and one needs
    to carefully evaluate the benefits and drawbacks before deciding whether or not
    to use deferred shading.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-sample anti-aliasing with deferred shading is possible in recent versions
    of OpenGL by making use of `GL_TEXTURE_2D_MULTISAMPLE`.
  prefs: []
  type: TYPE_NORMAL
- en: Another consideration is that deferred shading can't do blending/transparency
    very well. In fact, blending is impossible with the basic implementation we saw
    some time ago. Additional buffers with depth-peeling can help by storing additional
    layered geometry information in the g-buffer.
  prefs: []
  type: TYPE_NORMAL
- en: One notable advantage of deferred shading is that one can retain the depth information
    from the first pass and access it as a texture during the shading pass. Having
    access to the entire depth buffer as a texture can enable algorithms such as depth
    of field (depth blur), screen space ambient occlusion, volumetric particles, and
    other similar techniques.
  prefs: []
  type: TYPE_NORMAL
- en: For much more information about deferred shading, refer to Chapter 9 in *GPU
    Gems 2* edited by Matt Pharr and Randima Fernando (Addison-Wesley Professional
    2005) and Chapter 19 of *GPU Gems 3* edited by Hubert Nguyen (Addison-Wesley Professional
    2007). Both combined, provide an excellent discussion of the benefits and drawbacks
    of deferred shading, and how to make the decision whether or not to use it in
    your application.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `chapter06/scenedeferred.cpp` file in the example code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Rendering to a texture* recipe in [Chapter 5](a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml),
    *Using Textures*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Screen space ambient occlusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ambient occlusion** is a rendering technique that is based on the assumption
    that a surface receives uniform illumination from all directions. Some surface
    positions will receive less light than others due to objects nearby that occlude
    some of the light. If a surface point has a lot of local geometry nearby, some
    of this ambient illumination will be blocked causing the point to be darker.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this is shown in the following image (generated using Blender):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56affbb3-efb5-4155-acfc-31954f85d841.png)'
  prefs: []
  type: TYPE_IMG
- en: This image is rendered using ambient occlusion only, without light sources.
    Note how the result looks like shadows in areas that have local geometry occluding
    the ambient illumination. The result is quite pleasing to the eye and adds a significant
    amount of realism to an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ambient occlusion is calculated by testing the visibility of a surface point
    from the upper hemisphere centered at the surface point. Consider the two points
    **A** and **B** in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/192adb51-45a0-4f2c-9f50-ea51deedf14d.png)'
  prefs: []
  type: TYPE_IMG
- en: Point **A** is near a corner of the surface and point **B** is located on a
    flat area. The arrows represent directions for visibility testing. All directions
    in the hemisphere above point **B** are unoccluded, meaning that the rays do not
    intersect with any geometry. However, in the hemisphere above point **A**, roughly
    half of the directions are occluded (arrows with dashed lines). Therefore, **A**
    should receive less illumination and appear darker than point **B**.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, ambient occlusion boils down to the following process. Sample as
    many directions as possible in the upper hemisphere around the surface point.
    Test each direction for visibility (occlusion). The fraction of rays that are
    unoccluded gives the ambient occlusion factor at that point.
  prefs: []
  type: TYPE_NORMAL
- en: This process generally requires a large number of samples to produce acceptable
    results. To do this for every vertex of a mesh would be impractical in real time
    for complex scenes. However, the results can be precomputed and stored in a texture
    for static scenes. If the geometry can move, we need some approximation that is
    independent of the complexity of the scene.
  prefs: []
  type: TYPE_NORMAL
- en: '**Screen space ambient occlusion** (or **SSAO**) is the name for a class of
    algorithms that attempt to approximate ambient occlusion in real time using screen
    space information. In other words, with SSAO, we compute the ambient occlusion
    in a post process after the scene has been rendered using the data stored in the
    depth buffer and/or geometry buffers. SSAO works naturally in conjunction with
    deferred shading (see the recipe *Using deferred shading*), but has been implemented
    with forward (non-deferred) renderers as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll implement SSAO as part of a deferred rendering process.
    We''ll compute the ambient occlusion factor at each screen-space pixel, rather
    than on the surface of each object in the scene, ignoring any geometry that is
    occluded from the camera. After the first pass of a deferred shading renderer,
    we have position, normal, and color information for the visible surface locations
    at each screen pixel in our g-buffers (see the *Using deferred shading* recipe).
    For each pixel, we''ll use the position and normal vector to define the hemisphere
    above the surface point. Then, we will randomly choose locations (samples) within
    that hemisphere and test each location for visibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f5d01ef-48b5-4df2-944a-7bdaf107dc60.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding image represents occlusion testing for the surface point **P**.
    The filled and hollow circles are random sample points chosen within the hemisphere
    above **P**, centered along the normal vector. Hollow circles fail the visibility
    test and the filled circles pass.
  prefs: []
  type: TYPE_NORMAL
- en: To accurately test for visibility, we would need to trace a ray from the surface
    point toward all sample points and check each ray for intersection with a surface.
    However, we can avoid that expensive process. Rather than tracing rays, we'll
    estimate the visibility by defining a point as visible from the surface point
    in the following way. If the point is visible from the camera, we'll assume that
    it is also visible from the surface point. This can be inaccurate for some cases,
    but is a good approximation for a wide variety of typical scenes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, we''ll use one additional approximation. We won''t trace a ray from
    the camera to the surface point; instead, we''ll just compare the *z* coordinates
    of the point being tested and the surface point at the same (x,y) position in
    camera space. This introduces another small amount of error, but not enough to
    be objectionable in practice. The following diagram illustrates this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61ce5241-0d1c-464f-be2c-4a4418e977dd.png)'
  prefs: []
  type: TYPE_IMG
- en: For each sample that we test within the hemisphere, we find the corresponding
    location on the camera-visible surface at the same (x,y) position in camera coordinates.
    This position is simply the value in the position g-buffer at the (x,y) location
    of the sample. We then compare the *z* coordinates of the sample and the surface
    point. If the *z* coordinate of the surface point is larger than the sample's
    *z* coordinate (remember, we're in camera coordinates so all *z* coordinates will
    be negative), then we consider the sample to be occluded by the surface.
  prefs: []
  type: TYPE_NORMAL
- en: As the preceding diagram shows, this is an approximation of what a trace of
    the eye-ray might find. What precedes is a basic overview of the process and there's
    not much more to it than that. This algorithm boils down to testing a number of
    random samples in the hemisphere above each point.
  prefs: []
  type: TYPE_NORMAL
- en: The proportion of visible samples is the ambient occlusion factor. Of course,
    there are a bunch of details to be worked out. Let's start with an overview of
    the process. We'll implement this algorithm with four passes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first pass renders the data to the g-buffers: camera space position, normal,
    and base color.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second pass computes the ambient occlusion factor for each screen pixel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The third pass is a simple blur of the ambient occlusion data to remove high
    frequency artifacts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final pass is a lighting pass. The reflection model is evaluated, integrating
    the ambient occlusion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last three of these passes employs a screen-space technique, meaning that
    we invoke the fragment shader once for each pixel on the screen by rendering just
    a single screen filling quad. The actual geometry for the scene is only rendered
    during the first pass. The bulk of the interesting stuff here happens in pass
    2\. There we need to generate a number of random points in the hemisphere above
    the surface at each point. Random number generation within a shader is challenging
    for a number of reasons that we won't go into here. So, instead of trying to generate
    random numbers, we'll pre-generate a set of random points in a hemisphere centered
    around the *z* axis. We'll refer to this as our random kernel. We'll re-use this
    kernel at each point by transforming the points to camera space, aligning the
    kernel's *z* axis with the normal vector at the surface point. To squeeze out
    a bit more randomness, we'll also rotate the kernel around the normal vector by
    a random amount.
  prefs: []
  type: TYPE_NORMAL
- en: We'll cover the details in the steps presented in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s build our random kernel. We need a set of points in the positive-z
    hemisphere centered at the origin. We''ll use a hemisphere with a radius of 1.0
    so that we can scale it to any size as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `uniformHemisphere` function chooses a random point on the surface of the
    hemisphere in a uniform fashion. The details of how to do this were covered in
    an earlier recipe (see *Diffuse image based lighting*). To get a point within
    the hemisphere, we scale the point by the variable `scale`. This value will vary
    from 0 to 1 and is non-linear. It will produce more points close to the origin
    and fewer as we move away from the origin. We do this because we want to give
    slightly more weight to things that are close to the surface point.
  prefs: []
  type: TYPE_NORMAL
- en: We assign the values of the kernel points to a uniform variable (array) in our
    shader named `SampleKernel`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, we want to re-use this kernel for each surface point,
    but with a random rotation. To do so, we''ll build a small texture containing
    random rotation vectors. Each vector will be a unit vector in the x-y plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `uniformCircle` function gives a random point on the unit circle in the
    x-y plane. We're using a 4 x 4 texture here, but you could use a larger size.
    We'll tile this texture across the screen, and we'll make it available to the
    shader (uniform variable `RandTex`).
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking that a 4 x 4 texture is too small to give us enough randomness.
    Yes, it will produce high-frequency patterns, but the blur pass will help to smooth
    that noise out.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we'll use a single shader and a single framebuffer. You could,
    of course, use several if desired. We'll need framebuffer textures for the camera
    space position, camera space normal, base color, and ambient occlusion. The AO
    buffer can be a single channel texture (for example, format `R_16F`). We'll also
    need one additional AO texture for the blur pass. We will swap each one into the
    framebuffer as necessary.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first pass, render the scene to the geometry buffers (see *Using deferred
    shading* for details).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the second pass, we'll use this fragment shader code to compute the AO factor.
    To do so, we first compute a matrix for converting the kernel points into camera
    space. When doing so, we use a vector from `RandTex` to rotate the kernel. This
    process is similar to computing the tangent space matrix in normal mapping. For
    more on this, see *Using normal maps:*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we compute the ambient occlusion factor by looping over all of the kernel
    points, transforming them into camera coordinates, and then finding the surface
    point at the same (x,y) position and comparing the *z* values. We write the result
    to the AO buffer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the third pass, we do a simple blur, using a unweighted average of the nine
    nearest pixels. We read from the texture that was written in the previous pass,
    and write the results to our second AO buffer texture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The fourth pass applies the reflection model using the ambient occlusion value
    from the previous pass. We scale the ambient portion by the value in the AO buffer
    raised to the fourth power (to slightly exaggerate the effect):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the second pass, we compute the ambient occlusion factor. To do so, the first
    step is to find the matrix that converts our kernel into camera space. We want
    a matrix that converts the kernel's *z* axis to the normal vector at the surface
    point, and applies a random rotation using a vector from `RandTex`. The columns
    of the matrix are the three ortho-normal vectors that define the tangent coordinate
    system in screen space. Since we want the kernel's *z* axis to be transformed
    to the normal vector, the third of these three vectors is the normal vector itself.
    The other two (`tang` and `biTang`) are determined by using cross products. To
    find `biTang`, we take the cross product of the normal vector (`n`) and the random
    rotation vector retrieved from the texture (`randDir`). As long as the two are
    not parallel, this will give us a vector that is perpendicular to both `n` and
    `randDir`. However, there is a small possibility that the two might be parallel.
    If so, the normal vector is in the x-y plane of camera space (because all of the
    rotation vectors in the texture are in the x-y plane). So in this case, we compute
    `biTang` by taking the cross product of `n` and the *z* axis. Next, we normalize
    `biTang`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we scale the texture coordinates when accessing the random texture
    to get the random rotation vector. We do this because the texture is smaller than
    the size of the screen and we want to tile it to fill the screen so that a texel
    matches the size of a screen pixel.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have two ortho-normal vectors, we can compute the third with the
    cross product of the two. The three vectors `tang`, `biTang`, and `n` make up
    the axes of the tangent space coordinate system. The matrix that converts from
    the tangent system to camera space (`toCamSpace`) has these three vectors as its
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the `toCamSpace` matrix, we can loop over the 64 kernel points
    and test each of them. Note that we''re not actually treating these as points.
    Instead, they are treated as vectors that define an offset from the surface point.
    So a sample point is determined by the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, we take a vector from the sample kernel, convert it to camera space, scale
    it by `Radius`, and add it to the position of the surface point. The scale factor
    (`Radius`) is an important term that defines the size of the hemisphere around
    the point. It is a camera space value and may need to be adjusted for different
    scenes. In the example code, a value of `0.55` is used.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to find the visible surface at the sample point's (x,y) position.
    To do so, we need to look up the value of the position in the g-buffer at the
    sample position. We need the texture coordinates that correspond to that position.
    To find that, we first project the point to clip space, divide by the homogeneous
    *w* coordinate, and scale/translate to the texture space. Using that value, we
    then access the position g-buffer and retrieve the *z* coordinate of the surface
    position at that location (`surfaceZ` ). We don't need the *x* and *y* coordinates
    here as they are the same as the sample's.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the *z* coordinate of the projected point on the surface near
    the sample (`sampleZ`), we compute the difference (`dz`) between it and the *z*
    coordinate of the *original* surface point (the point being shaded, `pos`). If
    this value is less than zero or greater than `Radius`, then we know that the projected
    point on the surface at the sample location is outside of the hemisphere. In that
    case, we assume the sample is unoccluded. If that is not the case, we assume the
    projected point is within the hemisphere and we compare the *z* values. If `surfaceZ`
    is greater than `samplePos.z`, we know that the sample point is behind the surface.
  prefs: []
  type: TYPE_NORMAL
- en: This may seem strange, but remember, we're working in camera coordinates here.
    All of the *z* coordinates will be negative. These are not depth values—they are
    the camera space *z* coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: We add `1.0` to `occlusionSum` if we determine that the point is occluded. The
    final result in `occlusionSum` after the loop will be the total number of points
    that were occluded. Since we want the opposite—the fraction of points that *are
    not* occluded—we subtract one from the average before writing to the output variable
    `AoData`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image (on the left) shows the results of this pass. Note that
    if you look closely, you can see some high frequency grid-like artifacts due to
    the re-use of the random rotation vectors throughout the image. This is smoothed
    out by the blur pass (right-hand image):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a235afc-2b5d-45da-b3b2-c18f929d7894.png)'
  prefs: []
  type: TYPE_IMG
- en: The third pass is just a simple average of the nine texels near each texel.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth pass applies the reflection model. In this example, we just compute
    the diffuse and ambient components of the Blinn-Phong model, scaling the ambient
    term by the blurred ambient occlusion value (`aoVal`). In this example, it is
    raised to a power of `4.0` to make it a bit darker and increase the effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following images show the scene rendered without ambient occlusion (on
    the left) and with ambient occlusion (on the right). The ambient term is increased
    substantially to demonstrate the effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62902435-c18e-4df6-9939-08daeb89d21b.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `chapter06/scenessao.cpp` file in the example code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Using deferred shading* recipe in this chapter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring the depth test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GLSL 4 provides the ability to configure how the depth test is performed. This
    gives us additional control over how and when fragments are tested against the
    depth buffer.
  prefs: []
  type: TYPE_NORMAL
- en: Many OpenGL implementations automatically provide an optimization known as the
    early depth test or early fragment test. With this optimization, the depth test
    is performed before the fragment shader is executed. Since fragments that fail
    the depth test will not appear on the screen (or the framebuffer), there is no
    point in executing the fragment shader at all for those fragments and we can save
    some time by avoiding the execution.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenGL specification, however, states that the depth test must appear to
    be performed *after* the fragment shader. This means that if an implementation
    wishes to use the early depth test optimization, it must be careful. The implementation
    must make sure that if anything within the fragment shader might change the results
    of the depth test, then it should avoid using the early depth test.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a fragment shader can change the depth of a fragment by writing
    to the output variable, `gl_FragDepth`. If it does so, then the early depth test
    cannot be performed because, of course, the final depth of the fragment is not
    known prior to the execution of the fragment shader. However, the GLSL provides
    ways to notify the pipeline roughly how the depth will be modified, so that the
    implementation may determine when it might be okay to use the early depth test.
  prefs: []
  type: TYPE_NORMAL
- en: Another possibility is that the fragment shader might conditionally discard
    the fragment using the `discard` keyword. If there is any possibility that the
    fragment may be discarded, some implementations may not perform the early depth
    test.
  prefs: []
  type: TYPE_NORMAL
- en: There are also certain situations where we want to rely on the early depth test.
    For example, if the fragment shader writes to memory other than the framebuffer
    (with image load/store, shader storage buffers, or other incoherent memory writing),
    we might not want the fragment shader to execute for fragments that fail the depth
    test. This would help us to avoid writing data for fragments that fail. The GLSL
    provides a technique for forcing the early depth test optimization.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To ask the OpenGL pipeline to always perform the early depth test optimization,
    use the following layout qualifier in your fragment shader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If your fragment shader will modify the fragment''s depth, but you still would
    like to take advantage of the early depth test when possible, use the following
    layout qualifier in a declaration of `gl_FragDepth` within your fragment shader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this, `depth_*` is one of the following: `depth_any`, `depth_greater`, `depth_less`, or `depth_unchanged`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following statement forces the OpenGL implementation to always perform
    the early depth test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We must keep in mind that if we attempt to modify the depth anywhere within
    the shader by writing to `gl_FragDepth`, the value that is written will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: If your fragment shader needs to modify the depth value, then we can't force
    early fragment tests. However, we can help the pipeline to determine when it can
    still apply the early test. We do so by using one of the layout qualifiers for `gl_FragDepth` as
    shown before. This places some limits on how the value will be modified. The OpenGL
    implementation can then determine if the fragment shader can be skipped. If it
    can be determined that the depth will not be changed in such a way that it would
    cause the result of the test to change, the implementation can still use the optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The layout qualifier for the output variable `gl_FragDepth` tells the OpenGL
    implementation specifically how the depth might change within the fragment shader.
    The qualifier `depth_any` indicates that it could change in any way. This is the
    default.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other qualifiers describe how the value may change with respect to `gl_FragCoord.z`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`depth_greater`: This fragment shader promises to only increase the depth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`depth_less`: This fragment shader promises to only decrease the depth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`depth_unchanged`: This fragment shader promises not to change the depth. If
    it writes to `gl_FragDepth`, the value will be equal to `gl_FragCoord.z`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you use one of these qualifiers, but then go on to modify the depth in an
    incompatible way, the results are undefined. For example, if you declare `gl_FragDepth` with `depth_greater`,
    but decrease the depth of the fragment, the code will compile and execute, but
    you shouldn't expect to see accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: If your fragment shader writes to `gl_FragDepth`, then it must be sure to write
    a value in all circumstances. In other words, it must write a value no matter
    which branches are taken within the code.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Implementing order-independent transparency* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing order-independent transparency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transparency can be a difficult effect to do accurately in pipeline architectures
    like OpenGL. The general technique is to draw opaque objects first, with the depth
    buffer enabled, then to make the depth buffer read-only (using `glDepthMask`),
    disable the depth test, and draw the transparent geometry. However, care must
    be taken to ensure that the transparent geometry is drawn from *back to front*.
    That is, objects farther from the viewer should be drawn before the objects that
    are closer. This requires some sort of depth-sorting to take place prior to rendering.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following images show an example of a block of small, semi-transparent
    spheres with some semi-transparent cubes placed evenly within them. On the right-hand
    side, the objects are rendered in an arbitrary order, using standard OpenGL blending.
    The result looks incorrect because objects are blended in an improper order. The
    cubes, which were drawn last, appear to be on top of the spheres, and the spheres
    look jumbled, especially in the middle of the block. On the left, the scene is
    drawn using proper ordering, so objects appear to be oriented correctly with respect
    to depth, and the overall look is more realistic looking:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3e8ea27-c9d9-4059-8f83-5a1c7bdadbbb.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Order Independent Transparency** (**OIT**) means that we can draw objects
    in any order and still get accurate results. Depth sorting is done at some other
    level, perhaps within the fragment shader, so that the programmer need not sort
    objects before rendering. There are a variety of techniques for doing this; one
    of the most common technique is to keep a list of colors for each pixel, sort
    them by depth, and then blend them together in the fragment shader. In this recipe
    we''ll use this technique to implement OIT, making use of some of the newest features
    in OpenGL 4.3.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shader storage buffer objects** (**SSBO**) and **image load/store** are some
    of the newest features in OpenGL, introduced in 4.3 and 4.2, respectively. They
    allow arbitrary read/write access to data from within a shader. Prior to this,
    shaders were very limited in terms of what data they could access. They could
    read from a variety of locations (textures, uniforms, and so on), but writing
    was very limited. Shaders could only write to controlled, isolated locations such
    as fragment shader outputs and transform feedback buffers. This was for a very
    good reason. Since shaders can execute in parallel and in a seemingly arbitrary
    order, it is very difficult to ensure that data is consistent between instantiations
    of a shader. Data written by one shader instance might not be visible to another
    shader instance whether or not that instance is executed after the other. Despite
    this, there are good reasons for wanting to read and write to shared locations.
    With the advent of SSBOs and image load/store, that capability is now available
    to us. We can create buffers and textures (called images) with read/write access
    to any shader instance. This is especially important for compute shaders, the
    subject of [Chapter 11](d67e01c8-8212-4d49-937f-6b1c62a57744.xhtml), *Using Compute
    Shaders*. However, this power comes at a price. The programmer must now be very
    careful to avoid the types of memory consistency errors that come along with writing
    to memory that is shared among parallel threads. Additionally, the programmer
    must be aware of the performance issues that come with synchronization between
    shader invocations.'
  prefs: []
  type: TYPE_NORMAL
- en: For a more thorough discussion of the issues involved with memory consistency
    and shaders, refer to Chapter 11, of *The OpenGL Programming Guide*, 8th Edition.
    That chapter also includes another similar implementation of OIT.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll use SSBOs and image load/store to implement order-independent
    transparency. We'll use two passes. In the first pass, we'll render the scene
    geometry and store a linked list of fragments for each pixel. After the first
    pass, each pixel will have a corresponding linked list containing all fragments
    that were written to that pixel, including their depth and color. In the second
    pass, we'll draw a fullscreen quad to invoke the fragment shader for each pixel.
    In the fragment shader, we'll extract the linked list for the pixel, sort the
    fragments by depth (largest to smallest), and blend the colors in that order.
    The final color will then be sent to the output device.
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s the basic idea, so let''s dig into the details. We''ll need three memory
    objects that are shared among the fragment shader instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '**An atomic counter:** This is just an unsigned integer that we''ll use to
    keep track of'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the size of our linked list buffer. Think of this as the index of the first
    unused slot in the buffer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**A head-pointer texture that corresponds to the size of the screen**: The
    texture will store a single unsigned integer in each texel. The value is the index
    of the head of the linked list for the corresponding pixel.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**A buffer containing all of our linked lists:** Each item in the buffer will
    correspond to a fragment, and contains a struct with the color and depth of the
    fragment as well as an integer, which is the index of the next fragment in the
    linked list.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In order to understand how all of this works together, let''s consider a simple
    example. Suppose that our screen is three pixels wide and three pixels high. We''ll
    have a head pointer texture that is the same dimensions, and we''ll initialize
    all of the texels to a special value that indicates the end of the linked list
    (an empty list). In the following diagram, that value is shown as an **x**, but
    in practice, we''ll use `0xffffffff`. The initial value of the counter is zero,
    and the linked list buffer is allocated to a certain size but treated as empty
    initially. The initial state of our memory is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8dd60ca-4750-4864-8662-e9f409b4c153.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now suppose that a fragment is rendered at the position (0,1) with a depth
    of 0.75\. The fragment shader will take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Increment the atomic counter. The new value will be 1, but we'll use the previous
    value (**0**) as the index for our new node in the linked list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the head pointer texture at (0,1) with the previous value of the counter
    (**0**). This is the index of the new head of the linked list at that pixel. Hold
    on to the previous value that was stored there (**x**), as we'll need that in
    the next step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a new value into the linked list buffer at the location corresponding to
    the previous value of the counter (**0**). Store the color of the fragment and
    its depth here. Store in the next component the previous value of the head pointer
    texture at (0,1) that we held on to in step 2\. In this case, it is the special
    value indicating the end of the list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After processing this fragment, the memory layout looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b62cb1aa-bced-41f8-b460-6fad6245100e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, suppose another fragment is rendered at (0,1), with a depth of 0.5\. The
    fragment shader will execute the same steps as the previous ones, resulting in
    the following memory layout:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d36be185-a17f-4146-838d-1981ed3f55f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now have a two-element linked list starting at index 1 and ending at index
    0\. Suppose, now that we have three more fragments in the following order: a fragment
    at (1,1) with a depth of 0.2, a fragment at (0,1) with a depth of 0.3, and a fragment
    at (1,1) with a depth of 0.4\. Following the same steps for each fragment, we
    get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/269f6ef8-f08d-4037-9c41-a8ad6989a9e9.png)'
  prefs: []
  type: TYPE_IMG
- en: The linked list at (0,1) consists of fragments {3, 1, 0} and the linked list
    at (1,1) contains fragments {4, 2}.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we must keep in mind that due to the highly parallel nature of GPUs, fragments
    can be rendered in virtually any order. For example, fragments from two different
    polygons might proceed through the pipeline in the opposite order as to when the
    draw instructions for polygons were issued. As a programmer, we must not expect
    any specific ordering of fragments. Indeed, instructions from separate instances
    of the fragment shader may interleave in arbitrary ways. The only thing that we
    can be sure of is that the statements within a particular instance of the shader
    will execute in order. Therefore, we need to convince ourselves that any interleaving
    of the previous three steps will still result in a consistent state. For example,
    suppose instance one executes steps 1 and 2, then another instance (another fragment,
    perhaps at the same fragment coordinates) executes steps 1, 2, and 3, before the
    first instance executes step 3\. Will the result still be consistent? I think
    you can convince yourself that it will be, even though the linked list will be
    broken for a short time during the process. Try working through other interleavings
    and convince yourself that we're OK.
  prefs: []
  type: TYPE_NORMAL
- en: Not only can statements within separate instances of a shader interleave with
    each other, but the sub-instructions that make up the statements can interleave.
    (For example, the sub-instructions for an increment operation consist of a load,
    increment, and a store.) What's more, they could actually execute at exactly the
    same time. Consequently, if we aren't careful, nasty memory consistency issues
    can crop up. To help avoid this, we need to make careful use of the GLSL support
    for atomic operations.
  prefs: []
  type: TYPE_NORMAL
- en: Recent versions of OpenGL (4.2 and 4.3) have introduced the tools that we need
    to make this algorithm possible. OpenGL 4.2 introduced atomic counters and the
    ability to read and write to arbitrary locations within a texture (called image
    load/store). OpenGL 4.3 introduced shader storage buffer objects. We'll make use
    of all three of these features in this example, as well as the various atomic
    operations and memory barriers that go along with them.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There''s a bunch of setup needed here, so we''ll go into a bit of detail with
    some code segments. First, we''ll set up a buffer for our atomic counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create a buffer for our linked list storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`nodeSize` in the previous code is the size of `struct NodeType` used in the
    fragment shader (in the latter part of the code). This is computed based on the `std430` layout.
    For details on the `std430` layout, see the OpenGL specification document. For
    this example, `nodeSize` is `5 * sizeof(GLfloat) + sizeof(GLuint)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to create a texture to hold the list head pointers. We''ll use
    32-bit unsigned integers, and bind it to image unit 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'After we render each frame, we need to clear the texture by setting all texels
    to a value of `0xffffffff`. To help with that, we''ll create a buffer of the same
    size as the texture, with each value set to our clear value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: That's all the buffers we'll need. Note the fact that we've bound the head pointer
    texture to image unit 0, the atomic counter buffer to index 0 of the `GL_ATOMIC_COUNTER_BUFFER`
    binding point (`glBindBufferBase`), and the linked list storage buffer to index
    0 of the `GL_SHADER_STORAGE_BUFFER` binding point. We'll refer back to that later.
    Use a pass-through vertex shader that sends the position and normal along in eye
    coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With all of the buffers set up, we need two render passes. Before the first
    pass, we want to clear our buffers to default values (that is, empty lists), and
    to reset our atomic counter buffer to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first pass, we''ll render the full scene geometry. Generally, we should
    render all the opaque geometry first and store the results in a texture. However,
    we''ll skip that step for this example to keep things simple and focused. Instead,
    we''ll render only transparent geometry. When rendering the transparent geometry,
    we need to make sure to put the depth buffer in read-only mode (use `glDepthMask`).
    In the fragment shader, we add each fragment to the appropriate linked list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Before rendering the second pass, we need to be sure that all of the data has
    been written to our buffers. In order to ensure that is indeed the case, we can
    use a memory barrier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the second pass, we don''t render the scene geometry, just a single, screen-filling
    quad in order to invoke the fragment shader for each screen pixel. In the fragment
    shader, we start by copying the linked list for the fragment into a temporary
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we sort the fragments using insertion sort:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we blend the fragments manually, and send the result to the output
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To clear our buffers, prior to the first pass, we bind `clearBuf` to the `GL_PIXEL_UNPACK_BUFFER`
    binding point, and call `glTexSubImage2D` to copy data from `clearBuf` to the
    the head pointer texture. Note that when a non-zero buffer is bound to `GL_PIXEL_UNPACK_BUFFER`,
    `glTexSubImage2D` treats the last parameter as an offset into the buffer that
    is bound there. Therefore, this will initiate a copy from `clearBuf` into `headPtrTex`.
    Clearing the atomic counter is straightforward, but the use of `glBindBufferBase`
    may be a bit confusing. If there can be several buffers bound to the binding point
    (at different indices), how does `glBufferSubData` know which buffer to target?
    It turns out that when we bind a buffer using `glBindBufferBase`, it is also bound
    to the generic binding point as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the fragment shader during the first pass, we start with the layout specification
    enabling the early fragment test optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This is important because if any fragments are obscured by the opaque geometry,
    we don't want to add them to a linked list. If the early fragment test optimization
    is not enabled, the fragment shader may be executed for fragments that will fail
    the depth test and hence will get added to the linked list. The previous statement
    ensures that the fragment shader will not execute for those fragments.
  prefs: []
  type: TYPE_NORMAL
- en: The definition of `struct NodeType` specifies the type of data that is stored
    in our linked list buffer. We need to store color, depth, and a pointer to the
    next node in the linked list.
  prefs: []
  type: TYPE_NORMAL
- en: The next three statements declare the objects related to our linked list storage.
  prefs: []
  type: TYPE_NORMAL
- en: The first, `headPointers`, is the image object that stores the locations of
    the heads of each linked list. The layout qualifier indicates that it is located
    at image unit 0 (refer to the *Getting ready* section of this recipe), and the
    data type is `r32ui` (red, 32-bit unsigned integer).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second object is our atomic counter `nextNodeCounter`. The layout qualifier
    indicates the index within the `GL_ATOMIC_COUTER_BUFFER` binding point (refer
    to the *Getting ready* section of this recipe) and the offset within the buffer
    at that location.  Since we only have a single value in the buffer, the offset
    is 0, but in general, you might have several atomic counters located within a
    single buffer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Third is our linked-list storage buffer `linkedLists`. This is a shader storage
    buffer object. The organization of the data within the object is defined within
    the curly braces here. In this case, we just have an array of `NodeType` structures.
    The bounds of the array can be left undefined, the size being limited by the underlying
    buffer object that we created. The layout qualifiers define the binding and memory
    layout. The first, binding, indicates that the buffer is located at index 0 within
    the `GL_SHADER_STORAGE_BUFFER` binding point. The second, `std430`, indicates
    how memory is organized within the buffer. This is mainly important when we want
    to read the data back from the OpenGL side. As mentioned previously, this is documented
    in the OpenGL specification document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first step in the fragment shader during the first pass is to increment
    our atomic counter using `atomicCounterIncrement`. This will increment the counter
    in such a way that there is no possibility of memory consistency issues if another
    shader instance is attempting to increment the counter at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: An atomic operation is one that is isolated from other threads and can be considered
    to be a single, uninterruptable operation. Other threads cannot interleave with
    an atomic operation. It is always a good idea to use atomic operations when writing
    to shared data within a shader.
  prefs: []
  type: TYPE_NORMAL
- en: 'The return value of `atomicCounterIncrement` is the previous value of the counter.
    It is the next unused location in our linked list buffer. We''ll use this value
    as the location where we''ll store this fragment, so we store it in a variable
    named `nodeIdx`. It will also become the new head of the linked list, so the next
    step is to update the value in the `headPointers` image at this pixel''s location
    `gl_FragCoord.xy`. We do so using another atomic operation: `imageAtomicExchange`.
    This replaces the value within the image at the location specified by the second
    parameter with the value of the third parameter. The return value is the previous
    value of the image at that location. This is the previous head of our linked list.
    We hold on to this value in `prevHead`, because we want to link our new head to
    that node, thereby restoring the consistency of the linked list with our new node
    at the head.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we update the node at `nodeIdx` with the color and depth of the fragment,
    and set the `next` value to the previous head of the list (`prevHead`). This completes
    the insertion of this fragment into the linked list at the head of the list.
  prefs: []
  type: TYPE_NORMAL
- en: After the first pass is complete, we need to make sure that all changes are
    written to our shader storage buffer and image object before proceeding. The only
    way to guarantee this is to use a memory barrier. The call to `glMemoryBarrier`
    will take care of this for us. The parameter to `glMemoryBarrier` is the type
    of barrier. We can fine tune the type of barrier to specifically target the kind
    of data that we want to read. However, just to be safe, and for simplicity, we'll
    use `GL_ALL_BARRIER_BITS`, which ensures that all possible data has been written.
  prefs: []
  type: TYPE_NORMAL
- en: In the second pass, we start by copying the linked list for the fragment into
    a temporary array. We start by getting the location of the head of the list from
    the `headPointers` image using `imageLoad`. Then we traverse the linked list with
    the `while` loop, copying the data into the `array` frags.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we sort the array by depth from largest to smallest, using the insertion
    sort algorithm. Insertion sort works well on small arrays, so should be a fairly
    efficient choice here.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we combine all the fragments in order, using the `mix` function to
    blend them together based on the value of the alpha channel. The final result
    is stored in the output variable `FragColor`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, we've skipped anything that deals with opaque geometry.
    In general, one would probably want to render any opaque geometry first, with
    the depth buffer enabled, and store the rendered fragments in a texture. Then,
    when rendering the transparent geometry, one would disable writing to the depth
    buffer, and build the linked list as shown previously. Finally, you could use
    the value of the opaque texture as the background color when blending the linked
    lists.
  prefs: []
  type: TYPE_NORMAL
- en: This is the first example in this book that makes use of reading and writing
    from/to arbitrary (shared) storage from a shader. This capability, has given us
    much more flexibility, but that comes at a price. As indicated previously, we
    have to be very careful to avoid memory consistency and coherence issues. The
    tools to do so include atomic operations and memory barriers, and this example
    has just scratched the surface. There's much more to come in [Chapter 11](d67e01c8-8212-4d49-937f-6b1c62a57744.xhtml),
    *Using Compute Shaders* when we look at compute shaders, and I recommend you read
    through the memory chapter in the *OpenGL Programming Guide* for much more detail
    than is provided here.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `chapter06/sceneoit.cpp` file in the example code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 11](d67e01c8-8212-4d49-937f-6b1c62a57744.xhtml), *Using Compute Shaders*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenGL Development Cookbook* by Muhammad Mobeen Movania has several recipes
    in Chapter 6, *GPU-Based Alpha Blending and Global Illumination*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
