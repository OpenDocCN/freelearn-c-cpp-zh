<html><head></head><body>
<div><h1 class="chapterNumber">8</h1>
<h1 class="chapterTitle" id="_idParaDest-493">Leveraging Threading and Concurrency</h1>
<p class="normal">Most computers contain multiple processors or at least multiple cores, and leveraging this computational power is key to many categories of applications. Unfortunately, many developers still have a mindset of sequential code execution, even though operations that do not depend on each other could be executed concurrently. This chapter presents standard library support for threads, asynchronous tasks, and related components, as well as some practical examples at the end.</p>
<p class="normal">Most modern processors (except those dedicated to types of applications that do not require great computing power, such as Internet of Things applications) have two, four, or more cores that enable you to concurrently execute multiple threads of execution. Applications must be explicitly written to leverage the multiple processing units that exist; you can write such applications by executing functions on multiple threads at the same time. Since C++11, the standard library provides support for working with threads, synchronization of shared data, thread communication, and asynchronous tasks. In this chapter, we’ll explore the most important topics related to threads and tasks.</p>
<p class="normal">This chapter includes the following recipes:</p>
<ul>
<li class="bulletList">Working with threads</li>
<li class="bulletList">Synchronizing access to shared data with mutexes and locks</li>
<li class="bulletList">Finding alternatives for recursive mutexes</li>
<li class="bulletList">Handling exceptions from thread functions</li>
<li class="bulletList">Sending notifications between threads</li>
<li class="bulletList">Using promises and futures to return values from threads</li>
<li class="bulletList">Executing functions asynchronously</li>
<li class="bulletList">Using atomic types</li>
<li class="bulletList">Implementing parallel <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> with threads</li>
<li class="bulletList">Implementing parallel <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> with tasks</li>
<li class="bulletList">Implementing parallel <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> with standard parallel algorithms</li>
<li class="bulletList">Using joinable threads and cancellation mechanisms</li>
<li class="bulletList">Synchronizing threads with latches, barriers, and semaphores</li>
<li class="bulletList">Synchronizing writing to output streams from multiple threads</li>
</ul>
<p class="normal">In the first part of this chapter, we will look at the various threading objects and mechanisms that have built-in support in the library, such as threads, locking objects, condition variables, exception handling, and others.</p>
<h1 class="heading-1" id="_idParaDest-494">Working with threads</h1>
<p class="normal">A thread is a <a id="_idIndexMarker947"/>sequence of instructions that can be managed independently by a scheduler, such as the operating system. Threads could be software or hardware. Software threads are threads of execution that are managed by the operating system. They can run on single processing units, usually by time slicing. This is a mechanism where each thread gets a time slot of execution (in the range of milliseconds) on the processing unit before the operating system schedules another software thread to run on the same processing unit. Hardware threads are threads of execution at the physical level. They are, basically, a CPU or a CPU core. They can run simultaneously, that is, in parallel, on systems<a id="_idIndexMarker948"/> with multiprocessors or multicores. Many software threads can run concurrently on a hardware thread, usually by using time slicing. The C++ library provides support for working with software threads. In this recipe, you will learn how to create and perform operations with threads.</p>
<h2 class="heading-2" id="_idParaDest-495">Getting ready</h2>
<p class="normal">A thread of execution is represented by the <code class="inlineCode">thread</code> class, available in the <code class="inlineCode">std</code> namespace in the <code class="inlineCode">&lt;thread&gt;</code> header. Additional thread utilities are available in the same header but in the <code class="inlineCode">std::this_thread</code> namespace.</p>
<p class="normal">In the following examples, the <code class="inlineCode">print_time()</code> function is used. This function prints the local time to the console. Its implementation is as follows:</p>
<pre class="programlisting code"><code class="hljs-code">inline void print_time()
{
  auto now = std::chrono::system_clock::now();
  auto stime = std::chrono::system_clock::to_time_t(now);
  auto ltime = std::localtime(&amp;stime);
  std::cout &lt;&lt; std::put_time(ltime, "%c") &lt;&lt; '\n';
}
</code></pre>
<p class="normal">In the next <a id="_idIndexMarker949"/>section, we will see how to perform common operations with threads.</p>
<h2 class="heading-2" id="_idParaDest-496">How to do it...</h2>
<p class="normal">Use the following solutions to manage threads:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">To create a <code class="inlineCode">std::thread</code> object without starting the execution of a new thread, use its default constructor:
        <pre class="programlisting code"><code class="hljs-code">std::thread t;
</code></pre>
</li>
<li class="numberedList">Start the execution of a function on another thread by constructing a <code class="inlineCode">std::thread</code> object and passing the function as an argument:
        <pre class="programlisting code"><code class="hljs-code">void func1()
{
  std::cout &lt;&lt; "thread func without params" &lt;&lt; '\n';
}
std::thread t1(func1);
std::thread t2([]() {
  std::cout &lt;&lt; "thread func without params" &lt;&lt; '\n'; });
</code></pre>
</li>
<li class="numberedList">Start the execution of a function with arguments on another thread by constructing a <code class="inlineCode">std::thread</code> object, and then passing the function as an argument to the constructor, followed by its arguments:
        <pre class="programlisting code"><code class="hljs-code">void func2(int const i, double const d, std::string const s)
{
  std::cout &lt;&lt; i &lt;&lt; ", " &lt;&lt; d &lt;&lt; ", " &lt;&lt; s &lt;&lt; '\n';
}
std::thread t(func2, 42, 42.0, "42");
</code></pre>
</li>
<li class="numberedList">To wait for a thread to finish its execution, use the <code class="inlineCode">join()</code> method on the <code class="inlineCode">thread</code> object:
        <pre class="programlisting code"><code class="hljs-code">t.join();
</code></pre>
</li>
<li class="numberedList">To allow a thread to continue its execution independently of the current <code class="inlineCode">thread</code> object, use the <code class="inlineCode">detach()</code> method. This means the thread will continue its execution until it finishes without being managed by the <code class="inlineCode">std::thread</code> object, which will no longer own any thread:
        <pre class="programlisting code"><code class="hljs-code">t.detach();
</code></pre>
</li>
<li class="numberedList">To pass<a id="_idIndexMarker950"/> arguments by reference to a function thread, wrap them in either <code class="inlineCode">std::ref</code> or <code class="inlineCode">std::cref</code> (if the reference is constant):
        <pre class="programlisting code"><code class="hljs-code">void func3(int &amp; i)
{
  i *= 2;
}
int n = 42;
std::thread t(func3, std::ref(n));
t.join();
std::cout &lt;&lt; n &lt;&lt; '\n'; // 84
</code></pre>
</li>
<li class="numberedList">To stop the execution of a thread for a specified duration, use the <code class="inlineCode">std::this_thread::sleep_for()</code> function:
        <pre class="programlisting code"><code class="hljs-code">void func4()
{
  using namespace std::chrono;
  print_time();
  std::this_thread::sleep_for(2s);
  print_time();
}
std::thread t(func4);
t.join();
</code></pre>
</li>
<li class="numberedList">To stop the execution of a thread until a specified moment in time, use the <code class="inlineCode">std::this_thread::sleep_until()</code> function:
        <pre class="programlisting code"><code class="hljs-code">void func5()
{
  using namespace std::chrono;
  print_time();
  std::this_thread::sleep_until(
    std::chrono::system_clock::now() + 2s);
  print_time();
}
std::thread t(func5);
t.join();
</code></pre>
</li>
<li class="numberedList">To <a id="_idIndexMarker951"/>suspend the execution of the current thread and provide an opportunity for another thread to perform the execution, use <code class="inlineCode">std::this_thread::yield()</code>:
        <pre class="programlisting code"><code class="hljs-code">void func6(std::chrono::seconds timeout)
{
  auto now = std::chrono::system_clock::now();
  auto then = now + timeout;
  do
  {
    std::this_thread::yield();
  } while (std::chrono::system_clock::now() &lt; then);
}
std::thread t(func6, std::chrono::seconds(2));
t.join();
print_time();
</code></pre>
</li>
</ol>
<h2 class="heading-2" id="_idParaDest-497">How it works...</h2>
<p class="normal">The <code class="inlineCode">std::thread</code> class, which represents a single thread of execution, has several constructors:</p>
<ul>
<li class="bulletList">A default constructor that only creates the thread object but does not start the execution of a new thread.</li>
<li class="bulletList">A move constructor that creates a new thread object to represent a thread of execution previously represented by the object it was constructed from. After the construction of the new object, the other object is no longer associated with the execution thread.</li>
<li class="bulletList">A constructor with a variable number of arguments: the first being a function that represents the top-level thread function and the others being arguments to be passed to the thread function. Arguments need to be passed to the thread function by value. If the thread function takes parameters by reference or by constant reference, they must be wrapped in either a <code class="inlineCode">std::ref</code> or <code class="inlineCode">std::cref</code> object. These are helper function templates that generate objects of the type <code class="inlineCode">std::reference_wrapper</code>, which wraps a reference in a copyable and assignable object.</li>
</ul>
<p class="normal">The thread function, in <a id="_idIndexMarker952"/>this case, cannot return a value. It is not illegal for the function to actually have a return type other than <code class="inlineCode">void</code>, but it ignores any value that is directly returned by the function. If it has to return a value, it can do so using a shared variable or a function argument. In the <em class="italic">Using promises and futures to return values from threads</em> recipe, later in this chapter, we will see how a thread function returns a value to another thread using a <em class="italic">promise</em>.</p>
<p class="normal">If the function terminates with an exception, the exception cannot be caught with a <code class="inlineCode">try...catch</code> statement in the context where a thread was started and the program terminates abnormally with a call to <code class="inlineCode">std::terminate()</code>. All exceptions must be caught within the executing thread, but they can be transported across threads via a <code class="inlineCode">std::exception_ptr</code> object. We’ll discuss this topic in a later recipe, called <em class="italic">Handling exceptions from thread functions</em>.</p>
<p class="normal">After a thread has started its execution, it is both joinable and detachable. Joining a thread implies blocking the execution of the current thread until the joined thread ends its execution. Detaching a thread means decoupling the thread object from the thread of execution it represents, allowing both the current thread and the detached thread to be executed at the same time. Detached threads are sometimes called background threads or daemon threads. When a program terminates (by returning from the main function), the detached threads that are still running are not waited for. That means the stack of those threads is not unwound. Because of this, the destructor of the objects on the stack is not called, which may lead to resource leaks or corrupted resources (files, shared memory, etc.).</p>
<p class="normal">Joining a thread is done with <code class="inlineCode">join()</code> and detaching a thread is done with <code class="inlineCode">detach()</code>. Once you call either of these two methods, the thread is said to be non-joinable and the thread object can be safely destroyed. When a thread is detached, the shared data it may need to access must be available throughout its execution. </p>
<p class="normal">When you detach a thread, you cannot join it anymore. An attempt to do so will result in a runtime error. You can prevent this by checking whether the thread can be joined or not by using the <code class="inlineCode">joinable()</code> member function.</p>
<p class="normal">If a thread<a id="_idIndexMarker953"/> object goes out of scope and is destroyed but neither <code class="inlineCode">join()</code> or <code class="inlineCode">detach()</code> has been called, then <code class="inlineCode">std::terminate()</code> is invoked.</p>
<p class="normal">Each thread has an identifier that can be retrieved. For the current thread, call the <code class="inlineCode">std::this_thread::get_id()</code> function. For another thread of execution represented by a <code class="inlineCode">thread</code> object, call its <code class="inlineCode">get_id()</code> method.</p>
<p class="normal">There are several additional utility functions available in the <code class="inlineCode">std::this_thread</code> namespace:</p>
<ul>
<li class="bulletList">The <code class="inlineCode">yield()</code> method hints at the scheduler to activate another thread. This is useful when implementing a busy-waiting routine, as in the last example from the previous section. However, the actual behavior is implementation-specific. A call to this function, in fact, may have no effect on the execution of threads.</li>
<li class="bulletList">The <code class="inlineCode">sleep_for()</code> method blocks the execution of the current thread for at least the specified period of time (the actual time the thread is put to sleep may be longer than the requested period due to scheduling).</li>
<li class="bulletList">The <code class="inlineCode">sleep_until()</code> method blocks the execution of the current thread until at least the specified time point (the actual duration of the sleep may be longer than requested due to scheduling).</li>
</ul>
<p class="normal">The <code class="inlineCode">std::thread</code> class requires the <code class="inlineCode">join()</code> method to be called explicitly to wait for the thread to finish. This can lead to programming errors (as detailed above). The C++20 standard provides a new thread class, called <code class="inlineCode">std::jthread</code>, that solves this inconvenience. This will be the topic of the <em class="italic">Using joinable threads and cancellation mechanisms</em> recipe, later in this chapter.</p>
<h2 class="heading-2" id="_idParaDest-498">See also</h2>
<ul>
<li class="bulletList"><em class="italic">Synchronizing access to shared data with mutexes and locks</em>, to see what mechanisms are available for synchronizing thread access to shared data and how they work</li>
<li class="bulletList"><em class="italic">Finding alternatives for recursive mutexes</em>, to learn why recursive mutexes should be avoided, and also how to transform a thread-safe type using a recursive mutex into a thread-safe type using a non-recursive mutex</li>
<li class="bulletList"><em class="italic">Handling exceptions from thread functions</em>, to understand how to handle exceptions thrown in a worker thread from the main thread or the thread where it was joined</li>
<li class="bulletList"><em class="italic">Sending notifications between threads</em>, to see how to use condition variables to send notifications between producer and consumer threads</li>
<li class="bulletList"><em class="italic">Using promises and futures to return values from threads</em>, to learn how to use a <code class="inlineCode">std::promise</code> object to return a value or an exception from a thread</li>
</ul>
<h1 class="heading-1" id="_idParaDest-499">Synchronizing access to shared data with mutexes and locks</h1>
<p class="normal">Threads<a id="_idIndexMarker954"/> allow you to execute multiple functions at the same time, but it is often necessary that these functions access shared resources. Access to shared resources must be synchronized so that only one thread can read or write from or to the shared resource at a time. In this recipe, we will see what mechanisms the C++ standard defines for synchronizing thread access to shared data and how they work.</p>
<h2 class="heading-2" id="_idParaDest-500">Getting ready</h2>
<p class="normal">The <code class="inlineCode">mutex</code> and <code class="inlineCode">lock</code> classes discussed in this recipe are available in the <code class="inlineCode">std</code> namespace in the <code class="inlineCode">&lt;mutex&gt;</code> header, and, respectively, <code class="inlineCode">&lt;shared_mutex&gt;</code> for C++14 shared mutexes and locks.</p>
<h2 class="heading-2" id="_idParaDest-501">How to do it...</h2>
<p class="normal">Use the following pattern for synchronizing access with a single shared resource:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Define a <code class="inlineCode">mutex</code> in the appropriate context (class or global scope):
        <pre class="programlisting code"><code class="hljs-code">std::mutex g_mutex;
</code></pre>
</li>
<li class="numberedList">Acquire a <code class="inlineCode">lock</code> on this <code class="inlineCode">mutex</code> before accessing the shared resource in each thread:
        <pre class="programlisting code"><code class="hljs-code">void thread_func()
{
  using namespace std::chrono_literals;
  {
    std::lock_guard&lt;std::mutex&gt; lock(g_mutex);
    std::cout &lt;&lt; "running thread " 
              &lt;&lt; std::this_thread::get_id() &lt;&lt; '\n';
  }
  std::this_thread::yield();
  std::this_thread::sleep_for(2s);
  {
    std::lock_guard&lt;std::mutex&gt; lock(g_mutex);
    std::cout &lt;&lt; "done in thread " 
              &lt;&lt; std::this_thread::get_id() &lt;&lt; '\n';
  }
}
</code></pre>
</li>
</ol>
<p class="normal">Use the following pattern for synchronizing access to multiple shared resources at the same time to avoid deadlocks:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Define a <a id="_idIndexMarker955"/>mutex for each shared resource in the appropriate context (global or class scope):
        <pre class="programlisting code"><code class="hljs-code">template &lt;typename T&gt;
struct container
{
  std::mutex     mutex;
  std::vector&lt;T&gt; data;
};
</code></pre>
</li>
<li class="numberedList">Lock the mutexes at the same time using a deadlock avoidance algorithm with <code class="inlineCode">std::lock()</code>:
        <pre class="programlisting code"><code class="hljs-code">template &lt;typename T&gt;
void move_between(container&lt;T&gt; &amp; c1, container&lt;T&gt; &amp; c2, 
                  T const value)
{
  std::lock(c1.mutex, c2.mutex);
  // continued at 3.
}
</code></pre>
</li>
<li class="numberedList">After locking them, adopt the ownership of each mutex into a <code class="inlineCode">std::lock_guard</code> class to ensure they are safely released at the end of the function (or scope):
        <pre class="programlisting code"><code class="hljs-code">// continued from 2.
std::lock_guard&lt;std::mutex&gt; l1(c1.mutex, std::adopt_lock);
std::lock_guard&lt;std::mutex&gt; l2(c2.mutex, std::adopt_lock);
c1.data.erase(
  std::remove(c1.data.begin(), c1.data.end(), value), 
  c1.data.end());
c2.data.push_back(value);
</code></pre>
</li>
</ol>
<h2 class="heading-2" id="_idParaDest-502">How it works...</h2>
<p class="normal">A <strong class="keyWord">mutex </strong>(<strong class="keyWord">mutual exclusion</strong>) is a <a id="_idIndexMarker956"/>synchronization primitive that allows us to protect simultaneous access to shared resources from multiple threads. The C++ standard library provides<a id="_idIndexMarker957"/> several implementations:</p>
<ul>
<li class="bulletList"><code class="inlineCode">std::mutex</code> is the most commonly used mutex type; it is illustrated in the preceding code snippet. It provides methods to acquire and release the mutex. <code class="inlineCode">lock()</code> tries to acquire the mutex and blocks it if it is not available, <code class="inlineCode">try_lock()</code> tries to acquire the mutex and returns it without blocking if the mutex is not available, and <code class="inlineCode">unlock()</code> releases the mutex.</li>
<li class="bulletList"><code class="inlineCode">std::timed_mutex</code> is similar to <code class="inlineCode">std::mutex</code> but provides two more methods to acquire the mutex using a timeout: <code class="inlineCode">try_lock_for()</code> tries to acquire the mutex and returns it if the mutex is not made available during the specified duration, and <code class="inlineCode">try_lock_until()</code> tries to acquire the mutex and returns it if the mutex is not made available until a specified time point.</li>
<li class="bulletList"><code class="inlineCode">std::recursive_mutex</code> is similar to <code class="inlineCode">std::mutex</code>, but the mutex can be acquired multiple times from the same thread without being blocked.</li>
<li class="bulletList"><code class="inlineCode">std::recursive_timed_mutex</code> is a combination of a recursive mutex and a timed mutex.</li>
<li class="bulletList"><code class="inlineCode">std::shared_timed_mutex</code>, since C++14, is to be used in scenarios when multiple readers can access the same resource at the same time without causing data races, while only one writer is allowed to do so. It implements locking with two levels of access – <em class="italic">shared</em> (several threads can share the ownership of the same mutex) and <em class="italic">exclusive</em> (only one thread can own the mutex) – and provides timeout facilities.</li>
<li class="bulletList"><code class="inlineCode">std::shared_mutex</code>, since C++17, is similar to the <code class="inlineCode">shared_timed_mutex</code> but without the timeout facilities.</li>
</ul>
<p class="normal">The first <a id="_idIndexMarker958"/>thread that locks an available mutex takes ownership of it and continues with the execution. All consecutive attempts to lock the mutex from any thread fail, including the thread that already owns the mutex, and the <code class="inlineCode">lock()</code> method blocks the thread until the mutex is released with a call to <code class="inlineCode">unlock()</code>. If a thread needs to be able to lock a mutex multiple times without blocking it and therefore enter a deadlock, a <code class="inlineCode">recursive_mutex</code> class template should be used.</p>
<p class="normal">The typical use of a mutex to<a id="_idIndexMarker959"/> protect access to a shared resource comprises locking the mutex, using the shared resource, and then unlocking the mutex:</p>
<pre class="programlisting code"><code class="hljs-code">g_mutex.lock();
// use the shared resource such as std::cout
std::cout &lt;&lt; "accessing shared resource" &lt;&lt; '\n';
g_mutex.unlock();
</code></pre>
<p class="normal">This method of using the mutex is, however, prone to error. This is because each call to <code class="inlineCode">lock()</code> must be paired with a call to <code class="inlineCode">unlock()</code> on all execution paths; that is, both normal return paths and exception return paths. In order to safely acquire and release a mutex, regardless of the way the execution of a function goes, the C++ standard defines several locking classes:</p>
<ul>
<li class="bulletList"><code class="inlineCode">std::lock_guard</code> is the locking mechanism seen earlier; it represents a mutex wrapper implemented in an RAII manner. It attempts to acquire the mutex at the time of its construction and release it upon destruction. This is available in C++11. The following is a typical implementation of <code class="inlineCode">lock_guard</code>:
        <pre class="programlisting code"><code class="hljs-code">template &lt;class M&gt;
class lock_guard
{
public:
  typedef M mutex_type;
  explicit lock_guard(M&amp; Mtx) : mtx(Mtx)
  {
    mtx.lock();
  }
  lock_guard(M&amp; Mtx, std::adopt_lock_t) : mtx(Mtx)
  { }
  ~lock_guard() noexcept
  {
    mtx.unlock();
  }
  lock_guard(const lock_guard&amp;) = delete;
  lock_guard&amp; operator=(const lock_guard&amp;) = delete;
private:
  M&amp; mtx;
};
</code></pre>
</li>
<li class="bulletList"><code class="inlineCode">std::unique_lock</code> is a mutex ownership wrapper that provides support for deferred locking, time locking, recursive locking, transfer of ownership, and using it with condition variables. This is available in C++11.</li>
<li class="bulletList"><code class="inlineCode">std::shared_lock</code> is a mutex-shared ownership wrapper that provides support for deferred locking, time locking, and transfer of ownership. This is available in C++14.</li>
<li class="bulletList"><code class="inlineCode">std::scoped_lock</code> is a wrapper for multiple mutexes implemented in an RAII manner. Upon construction, it attempts to acquire ownership of the mutexes in a deadlock avoidance manner as if it is using <code class="inlineCode">std::lock()</code>, and upon destruction, it releases the mutexes in reverse order of the way they were acquired. This is available in C++17.</li>
</ul>
<div><p class="normal"><strong class="keyWord">RAII</strong>, which stands for <strong class="keyWord">Resource Acquisition Is Initialization</strong>, is a programming technique <a id="_idIndexMarker960"/>used in some programming languages, including C++, that simplifies resource management, ensures program correctness, and reduces code size. This technique binds the life cycle of a resource to an object. The allocation, also referred to as the acquisition, of a resource is done during the creation of the object (in the constructor) and the release of the resource (deallocation) is done when the object is destroyed (in the destructor). This ensures resources do not leak, provided that the bound objects are not themselves leaked. For more information about RAII, see <a href="https://en.cppreference.com/w/cpp/language/raii">https://en.cppreference.com/w/cpp/language/raii</a>.</p>
</div>
<p class="normal">In the first example in the <em class="italic">How to do it...</em> section, we used <code class="inlineCode">std::mutex</code> and <code class="inlineCode">std::lock_guard</code> to protect access to the <code class="inlineCode">std::cout</code> stream object, which is shared between <a id="_idIndexMarker961"/>all the threads in a program. The following example shows how the <code class="inlineCode">thread_func()</code> function can be executed concurrently on several threads:</p>
<pre class="programlisting code"><code class="hljs-code">std::vector&lt;std::thread&gt; threads;
for (int i = 0; i &lt; 5; ++i)
  threads.emplace_back(thread_func);
for (auto &amp; t : threads)
  t.join();
</code></pre>
<p class="normal">A possible output for this program is as follows:</p>
<pre class="programlisting con"><code class="hljs-con">running thread 140296854550272
running thread 140296846157568
running thread 140296837764864
running thread 140296829372160
running thread 140296820979456
done in thread 140296854550272
done in thread 140296846157568
done in thread 140296837764864
done in thread 140296820979456
done in thread 140296829372160
</code></pre>
<p class="normal">When a thread needs to take ownership of multiple mutexes that are meant to protect multiple shared resources, acquiring them one by one may lead to deadlocks. Let’s consider the following example (where <code class="inlineCode">container</code> is the class shown in the <em class="italic">How to do it...</em> section):</p>
<pre class="programlisting code"><code class="hljs-code">template &lt;typename T&gt;
void move_between(container&lt;T&gt; &amp; c1, container&lt;T&gt; &amp; c2, T const value)
{
  std::lock_guard&lt;std::mutex&gt; l1(c1.mutex);
  std::lock_guard&lt;std::mutex&gt; l2(c2.mutex);
  c1.data.erase(
    std::remove(c1.data.begin(), c1.data.end(), value), 
    c1.data.end());
  c2.data.push_back(value);
}
container&lt;int&gt; c1;
c1.data.push_back(1);
c1.data.push_back(2);
c1.data.push_back(3);
container&lt;int&gt; c2;
c2.data.push_back(4);
c2.data.push_back(5);
c2.data.push_back(6);
std::thread t1(move_between&lt;int&gt;, std::ref(c1), std::ref(c2), 3);
std::thread t2(move_between&lt;int&gt;, std::ref(c2), std::ref(c1), 6);
t1.join();
t2.join();
</code></pre>
<p class="normal">In this <a id="_idIndexMarker962"/>example, the <code class="inlineCode">container</code> class holds data that may be accessed simultaneously from different threads; therefore, it needs to be protected by acquiring a mutex. The <code class="inlineCode">move_between()</code> function is a thread-safe function that removes an element from a container and adds it to a second container. To do so, it acquires the mutexes of the two containers sequentially, then erases the element from the first container and adds it to the end of the second container.</p>
<p class="normal">This function is, however, prone to deadlocks because a race condition might be triggered while acquiring the locks. Suppose we have a scenario where two different threads execute this function, but with different arguments:</p>
<ul>
<li class="bulletList">The first thread starts executing with the arguments <code class="inlineCode">c1</code> and <code class="inlineCode">c2</code> in this order.</li>
<li class="bulletList">The first thread is suspended after it acquires the lock for the <code class="inlineCode">c1</code> container. The second thread starts executing with the arguments <code class="inlineCode">c2</code> and <code class="inlineCode">c1</code> in this order.</li>
<li class="bulletList">The second thread is suspended after it acquires the lock for the <code class="inlineCode">c2</code> container.</li>
<li class="bulletList">The first thread continues the execution and tries to acquire the mutex for <code class="inlineCode">c2</code>, but the mutex is unavailable. Therefore, a deadlock occurs (this can be simulated by putting the thread to sleep for a short while after it acquires the first mutex).</li>
</ul>
<p class="normal">To avoid possible deadlocks such as these, mutexes should be acquired in a deadlock avoidance manner, and the standard library provides a utility function called <code class="inlineCode">std::lock()</code> that does that. The <code class="inlineCode">move_between()</code> function needs to change by replacing the two locks with the following code (as shown in the <em class="italic">How to do it...</em> section):</p>
<pre class="programlisting code"><code class="hljs-code">std::lock(c1.mutex, c2.mutex);
std::lock_guard&lt;std::mutex&gt; l1(c1.mutex, std::adopt_lock);
std::lock_guard&lt;std::mutex&gt; l2(c2.mutex, std::adopt_lock);
</code></pre>
<p class="normal">The <a id="_idIndexMarker963"/>ownership of the mutexes must still be transferred to a lock guard object so they are properly released after the execution of the function ends (or, depending on the case, when a particular scope ends).</p>
<p class="normal">In C++17, a new mutex wrapper is available, <code class="inlineCode">std::scoped_lock</code>, that can be used to simplify code, such as the one in the preceding example. This type of lock can acquire the ownership of multiple mutexes in a deadlock-free manner. These mutexes are released when the scoped lock is destroyed. The preceding code is equivalent to the following single line of code:</p>
<pre class="programlisting code"><code class="hljs-code">std::scoped_lock lock(c1.mutex, c2.mutex);
</code></pre>
<p class="normal">The <code class="inlineCode">scoped_lock</code> class provides a simplified mechanism for owning one or more mutexes for the duration of a scoped block and also helps with writing simple and more robust code.</p>
<h2 class="heading-2" id="_idParaDest-503">See also</h2>
<ul>
<li class="bulletList"><em class="italic">Working with threads</em>, to learn about the <code class="inlineCode">std::thread</code> class and the basic operations for working with threads in C++</li>
<li class="bulletList"><em class="italic">Using joinable threads and cancellation mechanisms</em>, to learn about the C++20 <code class="inlineCode">std::jthread</code> class, which manages a thread of execution and automatically joins during its destruction, as well as the improved mechanisms for stopping the execution of threads</li>
<li class="bulletList"><em class="italic">Finding alternatives for recursive mutexes</em>, to learn why recursive mutexes should be avoided and how to transform a thread-safe type using a recursive mutex into a thread-safe type using a non-recursive mutex</li>
</ul>
<h1 class="heading-1" id="_idParaDest-504">Finding alternatives for recursive mutexes</h1>
<p class="normal">The <a id="_idIndexMarker964"/>standard library provides several mutex types for protecting access to shared resources. <code class="inlineCode">std::recursive_mutex</code> and <code class="inlineCode">std::recursive_timed_mutex</code> are two implementations that allow you to use multiple locking in the same thread. A typical use for a recursive mutex is to protect access to a shared resource from a recursive function. A <code class="inlineCode">std::recursive_mutex</code> class may be locked multiple times from a thread, either with a call to <code class="inlineCode">lock()</code> or <code class="inlineCode">try_lock()</code>. When a thread locks an available recursive mutex, it acquires its ownership; as a result of this, consecutive attempts to lock the mutex from the same thread do not block the execution of the thread, creating a deadlock. The recursive mutex is, however, released only when an equal number of calls to <code class="inlineCode">unlock()</code> are made. Recursive mutexes may also have a greater overhead than non-recursive mutexes. For these reasons, when possible, they should be avoided. This recipe presents a use case for transforming a thread-safe type using a recursive mutex into a thread-safe type using a non-recursive mutex.</p>
<h2 class="heading-2" id="_idParaDest-505">Getting ready</h2>
<p class="normal">You need to be familiar with the various mutexes and locks available in the standard library. I recommend that you read the previous recipe, <em class="italic">Synchronizing access to shared data with mutex and locks</em>, to get an overview of them.</p>
<p class="normal">For this recipe, we will consider the following class:</p>
<pre class="programlisting code"><code class="hljs-code">class foo_rec
{
  std::recursive_mutex m;
  int data;
public:
  foo_rec(int const d = 0) : data(d) {}
  void update(int const d)
 {
    std::lock_guard&lt;std::recursive_mutex&gt; lock(m);
    data = d;
  }
  int update_with_return(int const d)
 {
    std::lock_guard&lt;std::recursive_mutex&gt; lock(m);
    auto temp = data;
    update(d);
    return temp;
  }
};
</code></pre>
<p class="normal">The purpose of this recipe is to transform the <code class="inlineCode">foo_rec</code> class so we can avoid using <code class="inlineCode">std::recursive_mutex</code>.</p>
<h2 class="heading-2" id="_idParaDest-506">How to do it...</h2>
<p class="normal">To transform <a id="_idIndexMarker965"/>the preceding implementation into a thread-safe type using a non-recursive mutex, do this:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Replace <code class="inlineCode">std::recursive_mutex</code> with <code class="inlineCode">std::mutex</code>:
        <pre class="programlisting code"><code class="hljs-code">class foo
{
  std::mutex m;
  int        data;
  // continued at 2.
};
</code></pre>
</li>
<li class="numberedList">Define private non-thread-safe versions of the public methods or helper functions to be used in thread-safe public methods:
        <pre class="programlisting code"><code class="hljs-code">void internal_update(int const d) { data = d; }
// continued at 3.
</code></pre>
</li>
<li class="numberedList">Rewrite the public methods to use the newly defined non-thread-safe private methods:
        <pre class="programlisting code"><code class="hljs-code">public:
  foo(int const d = 0) : data(d) {}
  void update(int const d)
 {
    std::lock_guard&lt;std::mutex&gt; lock(m);
    internal_update(d);
  }
  int update_with_return(int const d)
 {
    std::lock_guard&lt;std::mutex&gt; lock(m);
    auto temp = data;
    internal_update(d);
    return temp;
  }
</code></pre>
</li>
</ol>
<h2 class="heading-2" id="_idParaDest-507">How it works...</h2>
<p class="normal">The <code class="inlineCode">foo_rec</code> class we<a id="_idIndexMarker966"/> just discussed uses a recursive mutex to protect access to shared data; in this case, it is an integer member variable that is accessed from two thread-safe public functions:</p>
<ul>
<li class="bulletList"><code class="inlineCode">update()</code> sets a new value in the private variable.</li>
<li class="bulletList"><code class="inlineCode">update_and_return()</code> sets a new value in the private variable and returns the previous value to the called function. This function calls <code class="inlineCode">update()</code> to set the new value.</li>
</ul>
<p class="normal">The implementation of <code class="inlineCode">foo_rec</code> was probably intended to avoid duplication of code, yet this particular approach is rather a design error that can be improved, as shown in the <em class="italic">How to do it...</em> section. Rather than reusing public thread-safe functions, we can provide private non-thread-safe functions that could then be called from the public interface.</p>
<p class="normal">The same solution can be applied to other similar problems: define a non-thread-safe version of the code and then provide perhaps lightweight, thread-safe wrappers.</p>
<h2 class="heading-2" id="_idParaDest-508">See also</h2>
<ul>
<li class="bulletList"><em class="italic">Working with threads</em>, to learn about the <code class="inlineCode">std::thread</code> class and the basic operations for working with threads in C++</li>
<li class="bulletList"><em class="italic">Synchronizing access to shared data with mutexes and locks</em>, to see what mechanisms are available for synchronizing thread access to shared data and how they work</li>
</ul>
<h1 class="heading-1" id="_idParaDest-509">Handling exceptions from thread functions</h1>
<p class="normal">In the first <a id="_idIndexMarker967"/>recipe, we introduced the thread support library and saw how to do some basic operations with threads. In that recipe, we briefly discussed exception handling in thread functions and mentioned that exceptions<a id="_idIndexMarker968"/> cannot be caught with a <code class="inlineCode">try…catch</code> statement in the context where the thread was started. On the other hand, exceptions can be transported between threads within a <code class="inlineCode">std::exception_ptr</code> wrapper. In this recipe, we will see how to handle exceptions from thread functions.</p>
<h2 class="heading-2" id="_idParaDest-510">Getting ready</h2>
<p class="normal">You are now familiar with the thread operations we discussed in the previous recipe, <em class="italic">Working with threads</em>. The <code class="inlineCode">exception_ptr</code> class is available in the <code class="inlineCode">std</code> namespace, which is in the <code class="inlineCode">&lt;exception&gt;</code> header; <code class="inlineCode">mutex</code> (which we discussed in more detail previously) is also available in the same namespace but in the <code class="inlineCode">&lt;mutex&gt;</code> header.</p>
<h2 class="heading-2" id="_idParaDest-511">How to do it...</h2>
<p class="normal">To properly handle exceptions thrown in a worker thread from the main thread or the thread where it was joined, do the following (assuming multiple exceptions can be thrown from multiple threads):</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Use a global container to hold instances of <code class="inlineCode">std::exception_ptr</code>:
        <pre class="programlisting code"><code class="hljs-code">std::vector&lt;std::exception_ptr&gt; g_exceptions;
</code></pre>
</li>
<li class="numberedList">Use a global <code class="inlineCode">mutex</code> to synchronize access to the shared container:
        <pre class="programlisting code"><code class="hljs-code">std::mutex g_mutex;
</code></pre>
</li>
<li class="numberedList">Use a <code class="inlineCode">try...catch</code> block for the code that is being executed in the top-level thread function. Use <code class="inlineCode">std::current_exception()</code> to capture the current exception and wrap a copy or its reference into a <code class="inlineCode">std::exception_ptr</code> pointer, which is added to the shared container for exceptions:
        <pre class="programlisting code"><code class="hljs-code">void func1()
{
  throw std::runtime_error("exception 1");
}
void func2()
{
  throw std::runtime_error("exception 2");
}
void thread_func1()
{
  try
  {
    func1();
  }
  catch (...)
  {
    std::lock_guard&lt;std::mutex&gt; lock(g_mutex);
    g_exceptions.push_back(std::current_exception());
  }
}
void thread_func2()
{
  try
  {
    func2();
  }
  catch (...)
  {
    std::lock_guard&lt;std::mutex&gt; lock(g_mutex);
    g_exceptions.push_back(std::current_exception());
  }
}
</code></pre>
</li>
<li class="numberedList">Clear <a id="_idIndexMarker969"/>the container from the main thread before you start the threads:
        <pre class="programlisting code"><code class="hljs-code">g_exceptions.clear();
</code></pre>
</li>
<li class="numberedList">In the <a id="_idIndexMarker970"/>main thread, after the execution of all the threads has finished, inspect the caught exceptions and handle each of them appropriately:
        <pre class="programlisting code"><code class="hljs-code">std::thread t1(thread_func1);
std::thread t2(thread_func2);
t1.join();
t2.join();
for (auto const &amp; e : g_exceptions)
{
  try
  {
    if(e)
      std::rethrow_exception(e);
  }
  catch(std::exception const &amp; ex)
  {
    std::cout &lt;&lt; ex.what() &lt;&lt; '\n';
  }
}
</code></pre>
</li>
</ol>
<h2 class="heading-2" id="_idParaDest-512">How it works...</h2>
<p class="normal">For the <a id="_idIndexMarker971"/>example in the preceding section, we assumed that multiple threads could throw exceptions and therefore need a container to hold them all. If there is a single exception from a single thread at a time, then you do not need a shared container and a mutex to synchronize access to it. You can use a single global object of the type <code class="inlineCode">std::exception_ptr</code> to hold the exception that’s transported between threads.</p>
<p class="normal"><code class="inlineCode">std::current_exception()</code> is a function that is typically used in a <code class="inlineCode">catch</code> clause to capture the <a id="_idIndexMarker972"/>current exception and create an instance of <code class="inlineCode">std::exception_ptr</code>. This is done to hold a copy or reference (depending on the implementation) to the original exception, which remains valid as long as there is a <code class="inlineCode">std::exception_ptr</code> pointer available that refers to it. If this function is called when no exception is being handled, then it creates an empty <code class="inlineCode">std::exception_ptr</code>.</p>
<p class="normal">The <code class="inlineCode">std::exception_ptr</code> pointer is a wrapper for an exception captured with <code class="inlineCode">std::current_exception()</code>. If default constructed, it does not hold any exception; it is, in this case, a null pointer. Two objects of this type are equal if they are both empty or point to the same exception object. The <code class="inlineCode">std::exception_ptr</code> objects can be passed to other threads, where they can be rethrown and caught in a <code class="inlineCode">try...catch</code> block.</p>
<p class="normal"><code class="inlineCode">std::rethrow_exception()</code> is a function that takes <code class="inlineCode">std::exception_ptr</code> as an argument and throws the exception object referred to by its argument.</p>
<div><p class="normal"><code class="inlineCode">std::current_exception()</code>, <code class="inlineCode">std::rethrow_exception()</code>, and <code class="inlineCode">std::exception_ptr</code> are all available in C++11.</p>
</div>
<p class="normal">In the <a id="_idIndexMarker973"/>example from the previous section, each thread function uses a <code class="inlineCode">try...catch</code> statement for the entire code it executes so that no exception may leave the function uncaught. When an exception is handled, a lock on the global <code class="inlineCode">mutex</code> object is acquired and the <code class="inlineCode">std::exception_ptr</code> object holding the current exception is added to the shared container. With this approach, the thread function stops at the first exception; however, in other circumstances, you may need to execute multiple operations, even if the previous one throws an exception. In this case, you will have multiple <code class="inlineCode">try...catch</code> statements <a id="_idIndexMarker974"/>and perhaps transport only some of the exceptions outside the thread.</p>
<p class="normal">In the main thread, after all the threads have finished executing, the container is iterated, and each non-empty exception is rethrown and caught with a <code class="inlineCode">try...catch</code> block and handled appropriately.</p>
<h2 class="heading-2" id="_idParaDest-513">See also</h2>
<ul>
<li class="bulletList"><em class="italic">Working with threads</em>, to learn about the <code class="inlineCode">std::thread</code> class and the basic operations for working with threads in C++</li>
<li class="bulletList"><em class="italic">Synchronizing access to shared data with mutexes and locks</em>, to see what mechanisms are available for synchronizing thread access to shared data and how they work</li>
</ul>
<h1 class="heading-1" id="_idParaDest-514">Sending notifications between threads</h1>
<p class="normal">Mutexes are <a id="_idIndexMarker975"/>synchronization primitives that can be used to <a id="_idIndexMarker976"/>protect access to shared data. However, the standard library provides a synchronization primitive, called a <em class="italic">condition variable</em>, that enables a thread to signal to others that a certain condition has occurred. The thread or threads that are waiting on the condition variable are blocked until the condition variable is signaled or until a timeout or a spurious wakeup occurs. In this recipe, we will see how to use condition variables to send notifications between thread-producing data and thread-consuming data.</p>
<h2 class="heading-2" id="_idParaDest-515">Getting ready</h2>
<p class="normal">For this recipe, you need to be familiar with threads, mutexes, and locks. Condition variables are available in the <code class="inlineCode">std</code> namespace in the <code class="inlineCode">&lt;condition_variable&gt;</code> header.</p>
<h2 class="heading-2" id="_idParaDest-516">How to do it...</h2>
<p class="normal">Use the<a id="_idIndexMarker977"/> following pattern for synchronizing threads with <a id="_idIndexMarker978"/>notifications on condition variables:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Define a condition variable (in the appropriate context):
        <pre class="programlisting code"><code class="hljs-code">std::condition_variable cv;
</code></pre>
</li>
<li class="numberedList">Define a mutex for threads to lock on. A second mutex should be used for synchronizing access to the standard console from different threads:
        <pre class="programlisting code"><code class="hljs-code">std::mutex data_mutex; // data mutex
std::mutex io_mutex;   // I/O mutex
</code></pre>
</li>
<li class="numberedList">Define the shared data used between the threads:
        <pre class="programlisting code"><code class="hljs-code">int data = 0;
</code></pre>
</li>
<li class="numberedList">In the producing thread, lock the mutex before you modify the data:
        <pre class="programlisting code"><code class="hljs-code">std::thread producer([&amp;](){
 // simulate long running operation
  {
 using namespace std::chrono_literals;
    std::this_thread::sleep_for(2s);
  }
 // produce
  {
    std::unique_lock lock(data_mutex);
    data = 42;
  }
 // print message
  {
    std::lock_guard l(io_mutex);
    std::cout &lt;&lt; "produced " &lt;&lt; data &lt;&lt; '\n';
  }
 // continued at 5.
});
</code></pre>
</li>
<li class="numberedList">In the producing thread, signal the condition variable with a call to <code class="inlineCode">notify_one()</code> or <code class="inlineCode">notify_all()</code> (do this after the mutex used to protect the shared data is unlocked):
        <pre class="programlisting code"><code class="hljs-code">// continued from 4.
cv.notify_one();
</code></pre>
</li>
<li class="numberedList">In the <a id="_idIndexMarker979"/>consuming thread, acquire a unique <a id="_idIndexMarker980"/>lock on the data mutex and use it to wait on the condition variable. Beware that spurious wakeups may occur, which is a subject we’ll discuss in detail in the <em class="italic">How it works…</em> section:
        <pre class="programlisting code"><code class="hljs-code">std::thread consumer([&amp;](){
 // wait for notification
  {
    std::unique_lock lock(data_mutex);
    cv.wait(lock);
  }
 // continued at 7.
});
</code></pre>
</li>
<li class="numberedList">In the consuming thread, use the shared data after the condition is notified:
        <pre class="programlisting code"><code class="hljs-code">// continued from 6.
{
  std::lock_guard lock(io_mutex);
  std::cout &lt;&lt; "consumed " &lt;&lt; data &lt;&lt; '\n';
}
</code></pre>
</li>
</ol>
<h2 class="heading-2" id="_idParaDest-517">How it works...</h2>
<p class="normal">The preceding example represents two threads that share common data (in this case, an integer variable). One thread produces data after a lengthy computation (simulated with a sleep), while the other consumes it only after it is produced. To do so, they use a synchronization mechanism that uses a mutex and a condition variable that blocks the consuming thread until a notification arises from the producer thread, indicating that data has been made available. The key in this communication channel is the condition variable that the consuming thread waits on until the producing thread notifies it. Both threads start at about the same time. The producer thread begins a long computation that is supposed <a id="_idIndexMarker981"/>to produce data for the consuming thread. At the same time, the consuming thread cannot actually proceed until the data is made available; it must remain <a id="_idIndexMarker982"/>blocked until it is notified that the data has been produced. Once notified, it can continue its execution. The entire mechanism works as follows:</p>
<ul>
<li class="bulletList">There must be at least one thread waiting on the condition variable to be notified.</li>
<li class="bulletList">There must be at least one thread that is signaling the condition variable.</li>
<li class="bulletList">The waiting threads must first acquire a lock on a mutex (<code class="inlineCode">std::unique_lock&lt;std::mutex&gt;</code>) and pass it to the <code class="inlineCode">wait()</code>, <code class="inlineCode">wait_for()</code>, or <code class="inlineCode">wait_until()</code> method of the condition variable. All the waiting methods atomically release the mutex and block the thread until the condition variable is signaled. At this point, the thread is unblocked and the mutex is atomically acquired again (that means the operations involved are treated as a whole and the thread cannot be interrupted while performing them).</li>
<li class="bulletList">The thread that signals the condition variable can do so with either <code class="inlineCode">notify_one()</code>, where one blocked thread is unblocked, or <code class="inlineCode">notify_all()</code>, where all the blocked threads waiting for the condition variable are unblocked.</li>
</ul>
<div><p class="normal">Condition variables cannot be made completely predictable on multiprocessor systems. Therefore, <em class="italic">spurious wakeups</em> may occur, and a thread is unlocked even if nobody signals the condition variable. So, it is necessary to check whether the condition is true after the thread has been unblocked. However, spurious wakeups may occur multiple times and, therefore, it is necessary to check the condition variable in a loop. You can learn more about spurious wakeups at <a href="https://en.wikipedia.org/wiki/Spurious_wakeup">https://en.wikipedia.org/wiki/Spurious_wakeup</a>.</p>
</div>
<p class="normal">The C++ standard provides two implementations of condition variables:</p>
<ul>
<li class="bulletList"><code class="inlineCode">std::condition_variable</code>, used in this recipe, defines a condition variable associated with <code class="inlineCode">std::unique_lock</code>.</li>
<li class="bulletList"><code class="inlineCode">std::condition_variable_any</code> represents a more general implementation that works with any lock that meets the requirements of a basic lock (implements the <code class="inlineCode">lock()</code> and <code class="inlineCode">unlock()</code> methods). A possible use of this implementation is providing interruptible waits, as explained by Anthony Williams in <em class="italic">C++ Concurrency In Action</em> (2012):</li>
</ul>
<blockquote class="packt_quote">
<p class="quote">A custom lock operation would both lock the associated mutex as expected and also perform the necessary job of notifying this condition variable when the interrupting signal is received.</p>
</blockquote>
<p class="normal">All the <a id="_idIndexMarker983"/>waiting <a id="_idIndexMarker984"/>methods of the condition variable have two overloads:</p>
<ul>
<li class="bulletList">The first overload takes <code class="inlineCode">std::unique_lock&lt;std::mutex&gt;</code> (based on the type; that is, duration or time point) and causes the thread to remain blocked until the condition variable is signaled. This overload atomically releases the mutex and blocks the current thread, and then adds it to the list of threads waiting on the condition variable. The thread is unblocked when the condition is notified with either <code class="inlineCode">notify_one()</code> or <code class="inlineCode">notify_all()</code>, a spurious wakeup occurs, or a timeout occurs (depending on the function overload). When this happens, the mutex is atomically acquired again.</li>
<li class="bulletList">The second overload takes a predicate in addition to the arguments of the other overloads. This predicate can be used to avoid spurious wakeups while waiting for a condition to become <code class="inlineCode">true</code>. This overload is equivalent to the following:
        <pre class="programlisting code"><code class="hljs-code">while(!pred())
  wait(lock);
</code></pre>
</li>
</ul>
<pre>producer</code> thread:</pre>
<pre class="programlisting code"><code class="hljs-code">std::mutex g_lockprint;
std::mutex g_lockqueue;
std::condition_variable g_queuecheck;
std::queue&lt;int&gt; g_buffer;
bool g_done;
void producer(
 int const id, 
  std::mt19937&amp; generator,
  std::uniform_int_distribution&lt;int&gt;&amp; dsleep,
  std::uniform_int_distribution&lt;int&gt;&amp; dcode)
{
  for (int i = 0; i &lt; 5; ++i)
  {
    // simulate work
    std::this_thread::sleep_for(
      std::chrono::seconds(dsleep(generator)));
    // generate data
    {
      std::unique_lock&lt;std::mutex&gt; locker(g_lockqueue);
      int value = id * 100 + dcode(generator);
      g_buffer.push(value);
      {
        std::unique_lock&lt;std::mutex&gt; locker(g_lockprint);
        std::cout &lt;&lt; "[produced(" &lt;&lt; id &lt;&lt; ")]: " &lt;&lt; value &lt;&lt; '\n';
      }
    }
    // notify consumers 
    g_queuecheck.notify_one();
  }
}
</code></pre>
<p class="normal">On the<a id="_idIndexMarker985"/> other <a id="_idIndexMarker986"/>hand, the consumer thread’s implementation is listed here:</p>
<pre class="programlisting code"><code class="hljs-code">void consumer()
{
  // loop until end is signaled
while (!g_done)
  {
    std::unique_lock&lt;std::mutex&gt; locker(g_lockqueue);
    g_queuecheck.wait_for(
      locker, 
      std::chrono::seconds(1),
      [&amp;]() {return !g_buffer.empty(); });
    // if there are values in the queue process them
while (!g_done &amp;&amp; !g_buffer.empty())
    {
      std::unique_lock&lt;std::mutex&gt; locker(g_lockprint);
      std::cout &lt;&lt; "[consumed]: " &lt;&lt; g_buffer.front() &lt;&lt; '\n';
      g_buffer.pop();
    }
  }
}
</code></pre>
<p class="normal">The<a id="_idIndexMarker987"/> consumer thread does the following:</p>
<ul>
<li class="bulletList">Loops until it is signaled that the process of producing data is finished.</li>
<li class="bulletList">Acquires a unique lock on the <code class="inlineCode">mutex</code> object associated with the condition variable.</li>
<li class="bulletList">Uses the <code class="inlineCode">wait_for()</code> overload, which takes a predicate, checking that the buffer is not empty when a wakeup occurs (to avoid spurious wakeups). This method uses a timeout of 1 second and returns after the timeout has occurred, even if the condition is signaled.</li>
<li class="bulletList">Consumes all of the data from the queue after it is signaled through the condition variable.</li>
</ul>
<p class="normal">To test this, we <a id="_idIndexMarker988"/>can start several producing threads and one consuming thread. Producer threads generate random data and, therefore, share the pseudo-random generator engines and distributions. All of this is shown in the following code sample:</p>
<pre class="programlisting code"><code class="hljs-code">auto seed_data = std::array&lt;int, std::mt19937::state_size&gt; {};
std::random_device rd {};
std::generate(std::begin(seed_data), std::end(seed_data),
              std::ref(rd));
std::seed_seq seq(std::begin(seed_data), std::end(seed_data));
auto generator = std::mt19937{ seq };
auto dsleep = std::uniform_int_distribution&lt;&gt;{ 1, 5 };
auto dcode = std::uniform_int_distribution&lt;&gt;{ 1, 99 };
std::cout &lt;&lt; "start producing and consuming..." &lt;&lt; '\n';
std::thread consumerthread(consumer);
std::vector&lt;std::thread&gt; threads;
for (int i = 0; i &lt; 5; ++i)
{
  threads.emplace_back(producer, 
                       i + 1, 
                       std::ref(generator),
                       std::ref(dsleep),
                       std::ref(dcode));
}
// work for the workers to finish
for (auto&amp; t : threads)
  t.join();
// notify the logger to finish and wait for it
g_done = true;
consumerthread.join();
std::cout &lt;&lt; "done producing and consuming" &lt;&lt; '\n';
</code></pre>
<p class="normal">A possible<a id="_idIndexMarker989"/> output<a id="_idIndexMarker990"/> of this program is as follows (the actual output would be different for each execution):</p>
<pre class="programlisting con"><code class="hljs-con">start producing and consuming...
[produced(5)]: 550
[consumed]: 550
[produced(5)]: 529
[consumed]: 529
[produced(5)]: 537
[consumed]: 537
[produced(1)]: 122
[produced(2)]: 224
[produced(3)]: 326
[produced(4)]: 458
[consumed]: 122
[consumed]: 224
[consumed]: 326
[consumed]: 458
...
done producing and consuming
</code></pre>
<p class="normal">The standard also features a helper function called <code class="inlineCode">notify_all_at_thread_exit()</code>, which provides a way for a thread to notify other threads through a <code class="inlineCode">condition_variable</code> object that it’s completely finished execution, including destroying all <code class="inlineCode">thread_local</code> objects. This <a id="_idIndexMarker991"/>function has two parameters: a <code class="inlineCode">condition_variable</code> and a <code class="inlineCode">std::unique_lock&lt;std::mutex&gt;</code> associated with the condition variable (that it takes ownership of). The typical use case for this function is running a detached thread that calls this function<a id="_idIndexMarker992"/> just before finishing.</p>
<h2 class="heading-2" id="_idParaDest-518">See also</h2>
<ul>
<li class="bulletList"><em class="italic">Working with threads</em>, to learn about the <code class="inlineCode">std::thread</code> class and the basic operations for working with threads in C++</li>
<li class="bulletList"><em class="italic">Synchronizing access to shared data with mutexes and locks</em>, to see what mechanisms are available for synchronizing thread access to shared data and how they work</li>
</ul>
<h1 class="heading-1" id="_idParaDest-519">Using promises and futures to return values from threads</h1>
<p class="normal">In the first<a id="_idIndexMarker993"/> recipe of this chapter, we discussed how to work with threads. You also learned that thread functions cannot return values and that threads should use other means, such as shared data, to do so; however, for this, synchronization is required. An alternative to communicating a return value or an exception with either the main or another thread is using <code class="inlineCode">std::promise</code>. This recipe will explain how this mechanism works.</p>
<h2 class="heading-2" id="_idParaDest-520">Getting ready</h2>
<p class="normal">The <code class="inlineCode">promise</code> and <code class="inlineCode">future</code> classes used in this recipe are available in the <code class="inlineCode">std</code> namespace in the <code class="inlineCode">&lt;future&gt;</code> header.</p>
<h2 class="heading-2" id="_idParaDest-521">How to do it...</h2>
<p class="normal">To communicate a value from one thread to another through promises and futures, do this:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Make a promise available to the thread function through a parameter; for example:
        <pre class="programlisting code"><code class="hljs-code">void produce_value(std::promise&lt;int&gt;&amp; p)
{
  // simulate long running operation
  {
    using namespace std::chrono_literals;
    std::this_thread::sleep_for(2s);
  }
  // continued at 2.
}
</code></pre>
</li>
<li class="numberedList">Call <code class="inlineCode">set_value()</code> on <a id="_idIndexMarker994"/>the promise to set the result to represent a value or <code class="inlineCode">set_exception()</code> to set the result to indicate an exception:
        <pre class="programlisting code"><code class="hljs-code">// continued from 1.
p.set_value(42);
</code></pre>
</li>
<li class="numberedList">Make the future associated with the promise available to the other thread function through a parameter; for example:
        <pre class="programlisting code"><code class="hljs-code">void consume_value(std::future&lt;int&gt;&amp; f)
{
  // continued at 4.
}
</code></pre>
</li>
<li class="numberedList">Call <code class="inlineCode">get()</code> on the <code class="inlineCode">future</code> object to get the result set to the promise:
        <pre class="programlisting code"><code class="hljs-code">// continued from 3.
auto value = f.get();
</code></pre>
</li>
<li class="numberedList">In the calling thread, use <code class="inlineCode">get_future()</code> on the promise to get the <code class="inlineCode">future</code> associated with the promise:
        <pre class="programlisting code"><code class="hljs-code">std::promise&lt;int&gt; p;
std::thread t1(produce_value, std::ref(p));
std::future&lt;int&gt; f = p.get_future();
std::thread t2(consume_value, std::ref(f));
t1.join();
t2.join();
</code></pre>
</li>
</ol>
<h2 class="heading-2" id="_idParaDest-522">How it works...</h2>
<p class="normal">The <a id="_idIndexMarker995"/>promise-future pair is basically a communication channel that enables a thread to communicate a value or exception with another thread through a shared state. <code class="inlineCode">promise</code> is an asynchronous provider of the result and has an associated <code class="inlineCode">future</code> that represents an asynchronous return object. To establish this channel, you must first create a promise. This, in turn, creates a shared state that can be later read through the future associated with the promise.</p>
<p class="normal">To set a result to a promise, you can use any of the following methods:</p>
<ul>
<li class="bulletList">The <code class="inlineCode">set_value()</code> or <code class="inlineCode">set_value_at_thread_exit()</code> method is used to set a return value; the latter function stores the value in the shared state but only makes it available through the associated future if the thread exits.</li>
<li class="bulletList">The <code class="inlineCode">set_exception()</code> or <code class="inlineCode">set_exception_at_thread_exit()</code> method is used to set an exception as a return value. The exception is wrapped in a <code class="inlineCode">std::exception_ptr</code> object. The latter function stores the exception in the shared state but only makes it available when the thread exits.</li>
</ul>
<p class="normal">To retrieve the <code class="inlineCode">future</code> object associated with <code class="inlineCode">promise</code>, use the <code class="inlineCode">get_future()</code> method. To get the value from the <code class="inlineCode">future</code> value, use the <code class="inlineCode">get()</code> method. This blocks the calling thread until the value from the shared state is made available. The future class has several methods for blocking the thread until the result from the shared state is made available:</p>
<ul>
<li class="bulletList"><code class="inlineCode">wait()</code> only returns when the result is available.</li>
<li class="bulletList"><code class="inlineCode">wait_for()</code> returns either when the result is available or when the specified timeout expires.</li>
<li class="bulletList"><code class="inlineCode">wait_until()</code> returns either when the result is available or when the specified time point is reached.</li>
</ul>
<p class="normal">If an exception is set to the <code class="inlineCode">promise</code> value, calling the <code class="inlineCode">get()</code> method on the <code class="inlineCode">future</code> object will throw this exception. The example from the previous section has been rewritten as follows to throw an exception instead of setting a result:</p>
<pre class="programlisting code"><code class="hljs-code">void produce_value(std::promise&lt;int&gt;&amp; p)
{
  // simulate long running operation
  {
    using namespace std::chrono_literals;
    std::this_thread::sleep_for(2s);
  }
  try
  {
    throw std::runtime_error("an error has occurred!");
  }
  catch(...)
  {
    p.set_exception(std::current_exception());
  }
}
void consume_value(std::future&lt;int&gt;&amp; f)
{
  std::lock_guard&lt;std::mutex&gt; lock(g_mutex);
  try
  {
    std::cout &lt;&lt; f.get() &lt;&lt; '\n';
  }
  catch(std::exception const &amp; e)
  {
    std::cout &lt;&lt; e.what() &lt;&lt; '\n';
  } 
}
</code></pre>
<p class="normal">You can<a id="_idIndexMarker996"/> see here that, in the <code class="inlineCode">consume_value()</code> function, the call to <code class="inlineCode">get()</code> is put in a <code class="inlineCode">try...catch</code> block. If an exception is caught – and in this particular implementation, it is – its message is printed to the console.</p>
<h2 class="heading-2" id="_idParaDest-523">There’s more...</h2>
<p class="normal">Establishing a promise-future channel in this manner is a rather explicit operation that can be avoided by using the <code class="inlineCode">std::async()</code> function; this is a higher-level utility that runs a function asynchronously, creates an internal promise and a shared state, and returns a future associated with the shared state. We will see how <code class="inlineCode">std::async()</code> works in the next recipe, <em class="italic">Executing functions asynchronously</em>.</p>
<h2 class="heading-2" id="_idParaDest-524">See also</h2>
<ul>
<li class="bulletList"><em class="italic">Working with threads</em>, to learn about the <code class="inlineCode">std::thread</code> class and the basic operations for working with threads in C++</li>
<li class="bulletList"><em class="italic">Handling exceptions from thread functions</em>, to understand how to handle exceptions thrown in a worker thread from the main thread or the thread where it was joined</li>
</ul>
<h1 class="heading-1" id="_idParaDest-525">Executing functions asynchronously</h1>
<p class="normal">Threads <a id="_idIndexMarker997"/>enable us to run multiple functions at the same time; this helps us take advantage of the hardware facilities in multiprocessor or multicore systems. However, threads require explicit, lower-level operations. An alternative to threads is tasks, which are units of work that run in a particular thread. The C++ standard does not provide a complete task library, but it enables developers to execute functions asynchronously on different threads and communicate results back through a promise-future channel, as seen in the previous recipe. In this recipe, we will see how to do this using <code class="inlineCode">std::async()</code> and <code class="inlineCode">std::future</code>.</p>
<h2 class="heading-2" id="_idParaDest-526">Getting ready</h2>
<p class="normal">For the examples in this recipe, we will use the following functions:</p>
<pre class="programlisting code"><code class="hljs-code">void do_something()
{
  // simulate long running operation
  {
    using namespace std::chrono_literals;
    std::this_thread::sleep_for(2s);
  } 
  std::lock_guard&lt;std::mutex&gt; lock(g_mutex);
  std::cout &lt;&lt; "operation 1 done" &lt;&lt; '\n'; 
}
void do_something_else()
{
  // simulate long running operation
  {
    using namespace std::chrono_literals;
    std::this_thread::sleep_for(1s);
  } 
  std::lock_guard&lt;std::mutex&gt; lock(g_mutex);
  std::cout &lt;&lt; "operation 2 done" &lt;&lt; '\n'; 
}
int compute_something()
{
  // simulate long running operation
  {
    using namespace std::chrono_literals;
    std::this_thread::sleep_for(2s);
  } 
  return 42;
}
int compute_something_else()
{
  // simulate long running operation
  {
    using namespace std::chrono_literals;
    std::this_thread::sleep_for(1s);
  }
  return 24;
}
</code></pre>
<p class="normal">In this recipe, we<a id="_idIndexMarker998"/> will use futures; therefore, you are advised to read the previous recipe to get a quick overview of how they work. Both <code class="inlineCode">async()</code> and <code class="inlineCode">future</code> are available in the <code class="inlineCode">std</code> namespace in the <code class="inlineCode">&lt;future&gt;</code> header.</p>
<h2 class="heading-2" id="_idParaDest-527">How to do it...</h2>
<p class="normal">To execute a function asynchronously on another thread when the current thread is continuing with the execution without expecting a result, do the following:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Use <code class="inlineCode">std::async()</code> to start a new thread to execute the specified function. This will create an asynchronous provider and return a <code class="inlineCode">future</code> associated with it. Use the <code class="inlineCode">std::launch::async</code> policy for the first argument to the <code class="inlineCode">std::async()</code> function in order to make sure the function will run asynchronously:
        <pre class="programlisting code"><code class="hljs-code">auto f = std::async(std::launch::async, do_something);
</code></pre>
</li>
<li class="numberedList">Continue with the execution of the current thread:
        <pre class="programlisting code"><code class="hljs-code">do_something_else();
</code></pre>
</li>
<li class="numberedList">Call the <code class="inlineCode">wait()</code> method on the <code class="inlineCode">future</code> object returned by <code class="inlineCode">std::async()</code> when you need to make sure the asynchronous operation is completed:
        <pre class="programlisting code"><code class="hljs-code">f.wait();
</code></pre>
<p class="normal">To <a id="_idIndexMarker999"/>execute a function asynchronously on a worker thread while the current thread continues its execution, until the result from the asynchronous function is needed in the current thread, do the following:</p> </li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="4">Use <code class="inlineCode">std::async()</code> to start a new thread to execute the specified function, create an asynchronous provider, and return a <code class="inlineCode">future</code> associated with it. Use the <code class="inlineCode">std::launch::async</code> policy of the first argument to the function to make sure the function does run asynchronously:
        <pre class="programlisting code"><code class="hljs-code">auto f = std::async(std::launch::async, compute_something);
</code></pre>
</li>
<li class="numberedList">Continue the execution of the current thread:
        <pre class="programlisting code"><code class="hljs-code">auto value = compute_something_else();
</code></pre>
</li>
<li class="numberedList">Call the <code class="inlineCode">get()</code> method on the <code class="inlineCode">future</code> object returned by <code class="inlineCode">std::async()</code> when you need the result from the function to be executed asynchronously:
        <pre class="programlisting code"><code class="hljs-code">value += f.get();
</code></pre>
</li>
</ol>
<h2 class="heading-2" id="_idParaDest-528">How it works...</h2>
<p class="normal"><code class="inlineCode">std::async()</code> is a<a id="_idIndexMarker1000"/> variadic function template that has two overloads: one that specifies a launch policy as the first argument and another that does not. The other arguments to <code class="inlineCode">std::async()</code> are the function to execute and its arguments, if any. The launch policy is defined by a scoped enumeration called <code class="inlineCode">std::launch</code>, available in the <code class="inlineCode">&lt;future&gt;</code> header:</p>
<pre class="programlisting code"><code class="hljs-code">enum class launch : /* unspecified */ 
{
  async = /* unspecified */,
  deferred = /* unspecified */,
  /* implementation-defined */
};
</code></pre>
<p class="normal">The two available launch policies specify the following:</p>
<ul>
<li class="bulletList">With <code class="inlineCode">async</code>, a new thread is launched to execute the task asynchronously.</li>
<li class="bulletList">With <code class="inlineCode">deferred</code>, the task is executed on the calling thread the first time its result is requested.</li>
</ul>
<p class="normal">When both<a id="_idIndexMarker1001"/> flags are specified (<code class="inlineCode">std::launch::async | std::launch::deferred</code>), it is an implementation decision regarding whether to run the task asynchronously on a new thread or synchronously on the current thread. This<a id="_idIndexMarker1002"/> is the behavior of the other <code class="inlineCode">std::async()</code> overload that does not specify a launch policy. This behavior is not deterministic.</p>
<div><p class="normal">Do not use the non-deterministic overload of <code class="inlineCode">std::async()</code> to run tasks asynchronously. For this purpose, always use the overload that requires a launch policy, and always use only <code class="inlineCode">std::launch::async</code>.</p>
</div>
<p class="normal">Both overloads of <code class="inlineCode">std::async()</code> return a <code class="inlineCode">future</code> object that refers to the shared state created internally by <code class="inlineCode">std::async()</code> for the promise-future channel it establishes. When you need the result of the asynchronous operation, call the <code class="inlineCode">get()</code> method on the future. This blocks the current thread until either the result value or an exception is made available. If the future does not transport any value or if you are not actually interested in that value, but you want to make sure the asynchronous operation will be completed at some point, use the <code class="inlineCode">wait()</code> method; it blocks the current thread until the shared state is made available through the future.</p>
<p class="normal">The future class has two more waiting methods: <code class="inlineCode">wait_for()</code> specifies a duration after which the call ends and returns even if the shared state is not yet available through the future, while <code class="inlineCode">wait_until()</code> specifies a time point after which the call returns, even if the shared state is not yet available. These methods could be used to create a polling routine and display a status message to the user, as shown in the following example:</p>
<pre class="programlisting code"><code class="hljs-code">auto f = std::async(std::launch::async, do_something);
while(true)
{
  std::cout &lt;&lt; "waiting...\n";
  using namespace std::chrono_literals;
  auto status = f.wait_for(500ms);
  if(status == std::future_status::ready) 
    break;
}
std::cout &lt;&lt; "done!\n";
</code></pre>
<p class="normal">The result of<a id="_idIndexMarker1003"/> running this program is as follows:</p>
<pre class="programlisting con"><code class="hljs-con">waiting...
waiting...
waiting...
operation 1 done
done!
</code></pre>
<h2 class="heading-2" id="_idParaDest-529">See also</h2>
<ul>
<li class="bulletList"><em class="italic">Using promises and futures to return values from threads</em>, to learn how to use a <code class="inlineCode">std::promise</code> object to return a value or an exception from a thread</li>
</ul>
<h1 class="heading-1" id="_idParaDest-530">Using atomic types</h1>
<p class="normal">The thread<a id="_idIndexMarker1004"/> support library offers functionalities for managing threads and synchronizing access to shared data with mutexes and locks, and, as of C++20, with latches, barriers, and semaphores. The standard library provides support for the complementary, lower-level atomic operations on data, which are indivisible operations that can be executed concurrently from different threads on shared data, without the risk of producing race conditions and without the use of locks. The support it provides includes atomic types, atomic operations, and memory synchronization ordering. In this recipe, we will see how to use some of these types and functions.</p>
<h2 class="heading-2" id="_idParaDest-531">Getting ready</h2>
<p class="normal">All the atomic types and operations are defined in the <code class="inlineCode">std</code> namespace in the <code class="inlineCode">&lt;atomic&gt;</code> header.</p>
<h2 class="heading-2" id="_idParaDest-532">How to do it...</h2>
<p class="normal">The following are a series <a id="_idIndexMarker1005"/>of typical operations that use atomic types:</p>
<ul>
<li class="bulletList">Use the <code class="inlineCode">std::atomic</code> class template to create atomic objects that support atomic operations, such as loading, storing, or performing arithmetic or bitwise operations:
        <pre class="programlisting code"><code class="hljs-code">std::atomic&lt;int&gt; counter {0};
std::vector&lt;std::thread&gt; threads;
for(int i = 0; i &lt; 10; ++i)
{
  threads.emplace_back([&amp;counter](){
    for(int j = 0; j &lt; 10; ++j)
      ++counter;
    });
}
for(auto &amp; t : threads) t.join();
std::cout &lt;&lt; counter &lt;&lt; '\n'; // prints 100
</code></pre>
</li>
<li class="bulletList">In C++20, use the <code class="inlineCode">std::atomic_ref</code> class template to apply atomic operations to a referenced object, which can be a reference or pointer to an integral type, a floating-point type, or a user-defined type:
        <pre class="programlisting code"><code class="hljs-code">void do_count(int&amp; c)
{
  std::atomic_ref&lt;int&gt; counter{ c };
  std::vector&lt;std::thread&gt; threads;
  for (int i = 0; i &lt; 10; ++i)
  {
    threads.emplace_back([&amp;counter]() {
      for (int j = 0; j &lt; 10; ++j)
        ++counter;
      });
  }
  for (auto&amp; t : threads) t.join();
}
int main()
{
  int c = 0;
  do_count(c);
  std::cout &lt;&lt; c &lt;&lt; '\n'; // prints 100
}
</code></pre>
</li>
<li class="bulletList">Use <a id="_idIndexMarker1006"/>the <code class="inlineCode">std::atomic_flag</code> class for an atomic Boolean type:
        <pre class="programlisting code"><code class="hljs-code">std::atomic_flag lock = ATOMIC_FLAG_INIT;
int counter = 0;
std::vector&lt;std::thread&gt; threads;
for(int i = 0; i &lt; 10; ++i)
{
  threads.emplace_back([&amp;](){
    while(lock.test_and_set(std::memory_order_acquire));
      ++counter;
      lock.clear(std::memory_order_release);
  });
}
for(auto &amp; t : threads) t.join();
std::cout &lt;&lt; counter &lt;&lt; '\n'; // prints 10
</code></pre>
</li>
<li class="bulletList">Use the atomic type’s members – <code class="inlineCode">load()</code>, <code class="inlineCode">store()</code>, and <code class="inlineCode">exchange()</code> – or non-member functions – <code class="inlineCode">atomic_load()</code>/<code class="inlineCode">atomic_load_explicit()</code>, <code class="inlineCode">atomic_store()</code>/<code class="inlineCode">atomic_store_explicit()</code>, and <code class="inlineCode">atomic_exchange()</code>/<code class="inlineCode">atomic_exchange_explicit()</code> – to atomically read, set, or exchange the value of an atomic object.</li>
<li class="bulletList">Use its member functions <code class="inlineCode">fetch_add()</code> and <code class="inlineCode">fetch_sub()</code> or non-member functions <code class="inlineCode">atomic_fetch_add()</code>/<code class="inlineCode">atomic_fetch_add_explicit()</code> and <code class="inlineCode">atomic_fetch_sub()</code>/<code class="inlineCode">atomic_fetch_sub_explicit()</code> to atomically add or subtract a value to/from an atomic object and return its value before the operation:
        <pre class="programlisting code"><code class="hljs-code">std::atomic&lt;int&gt; sum {0};
std::vector&lt;int&gt; numbers = generate_random();
size_t size = numbers.size();
std::vector&lt;std::thread&gt; threads;
for(int i = 0; i &lt; 10; ++i)
{
  threads.emplace_back([&amp;sum, &amp;numbers](size_t const start,
                                        size_t const end) {
  for(size_t j = start; j &lt; end; ++j)
  {
    std::atomic_fetch_add_explicit(
      &amp;sum, numbers[j], 
      std::memory_order_acquire);
    // same as 
// sum.fetch_add(numbers[i], std::memory_order_acquire);
  }},
  i*(size/10),
  (i+1)*(size/10));
}
for(auto &amp; t : threads) t.join();
</code></pre>
</li>
<li class="bulletList">Use its <a id="_idIndexMarker1007"/>member functions <code class="inlineCode">fetch_and()</code>, <code class="inlineCode">fetch_or()</code>, and <code class="inlineCode">fetch_xor()</code> or non-member functions <code class="inlineCode">atomic_fetch_and()</code>/<code class="inlineCode">atomic_fetch_and_explicit()</code>, <code class="inlineCode">atomic_fetch_or()</code>/ <code class="inlineCode">atomic_fetch_or_explicit()</code>, and <code class="inlineCode">atomic_fetch_xor()</code>/<code class="inlineCode">atomic_fetch_xor_explicit()</code> to perform AND, OR, and XOR atomic operations, respectively, with the specified argument and return the value of the atomic object before the operation.</li>
<li class="bulletList">Use the <code class="inlineCode">std::atomic_flag</code> member functions <code class="inlineCode">test_and_set()</code> and <code class="inlineCode">clear()</code> or non-member functions <code class="inlineCode">atomic_flag_test_and_set()</code>/<code class="inlineCode">atomic_flag_test_and_set_explicit()</code> and <code class="inlineCode">atomic_flag_clear()</code>/<code class="inlineCode">atomic_flag_clear_explicit()</code> to set or reset an atomic flag. In addition, in C++20, you can use the member function <code class="inlineCode">test()</code> and the non-member function <code class="inlineCode">atomic_flag_test()</code>/<code class="inlineCode">atomic_flag_test_explicit()</code> to atomically return the value of the flag.</li>
<li class="bulletList">In C++20, perform thread synchronization with member functions <code class="inlineCode">wait()</code>, <code class="inlineCode">notify_one()</code>, and <code class="inlineCode">notify_all()</code>, available to <code class="inlineCode">std::atomic</code>, <code class="inlineCode">std::atomic_ref</code>, and <code class="inlineCode">std::atomic_flag</code>, as well as the non-member functions <code class="inlineCode">atomic_wait()</code>/<code class="inlineCode">atomic_wait_explicit()</code>, <code class="inlineCode">atomic_notify_one()</code>, and <code class="inlineCode">atomic_notify_all()</code>. These functions provide a more efficient mechanism for waiting for the value of an atomic object to change than polling.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-533">How it works...</h2>
<p class="normal"><code class="inlineCode">std::atomic</code> is a <a id="_idIndexMarker1008"/>class template that defines (including its specializations) an atomic type. The behavior of an object of an atomic type is well defined when one thread writes to the object and the other reads data, without using locks to protect access. The operations on atomic variables are treated as single, uninterruptable actions. If two threads want to write on the same atomic variable, the first to take hold of it will write, while the other will wait for the atomic write to complete before it writes. This is a deterministic behavior and does not require additional locking.</p>
<p class="normal">The <code class="inlineCode">std::atomic</code> class<a id="_idIndexMarker1009"/> provides several specializations:</p>
<ul>
<li class="bulletList">Full specialization for <code class="inlineCode">bool</code>, with a typedef called <code class="inlineCode">atomic_bool</code>.</li>
<li class="bulletList">Full specialization for all integral types, with type aliases (typedefs) called <code class="inlineCode">atomic_bool</code> (for <code class="inlineCode">std::atomic&lt;bool&gt;</code>), <code class="inlineCode">atomic_int</code> (for <code class="inlineCode">std::atomic&lt;int&gt;</code>), <code class="inlineCode">atomic_long</code> (for <code class="inlineCode">std::atomic&lt;long&gt;</code>), <code class="inlineCode">atomic_char</code> (for <code class="inlineCode">std::atomic&lt;char&gt;</code>), <code class="inlineCode">atomic_size_t</code> (for <code class="inlineCode">std::atomic&lt;std::size_t&gt;</code>), and many others.</li>
<li class="bulletList">Partial specialization for pointer types.</li>
<li class="bulletList">In C++20, full specializations for the floating-point types <code class="inlineCode">float</code>, <code class="inlineCode">double</code>, and <code class="inlineCode">long double</code>.</li>
<li class="bulletList">In C++20, partial specializations such as <code class="inlineCode">std::atomic&lt;std::shared_ptr&lt;U&gt;&gt;</code> for <code class="inlineCode">std::shared_ptr</code> and <code class="inlineCode">std::atomic&lt;std::weak_ptr&lt;U&gt;&gt;</code> for <code class="inlineCode">std::weak_ptr</code>.</li>
</ul>
<p class="normal">The <code class="inlineCode">atomic</code> class template has various member functions that perform atomic operations, such as the following:</p>
<ul>
<li class="bulletList"><code class="inlineCode">load()</code> to atomically load and return the value of the object.</li>
<li class="bulletList"><code class="inlineCode">store()</code> to atomically store a non-atomic value in the object; this function does not return anything.</li>
<li class="bulletList"><code class="inlineCode">exchange()</code> to atomically store a non-atomic value in the object and return the previous value.</li>
<li class="bulletList"><code class="inlineCode">operator=</code>, which has the same effect as <code class="inlineCode">store(arg)</code>.</li>
<li class="bulletList"><code class="inlineCode">fetch_add()</code> to atomically add a non-atomic argument to the atomic value and return the value stored previously.</li>
<li class="bulletList"><code class="inlineCode">fetch_sub()</code> to atomically subtract a non-atomic argument from the atomic value and return the value stored previously.</li>
<li class="bulletList"><code class="inlineCode">fetch_and()</code>, <code class="inlineCode">fetch_or()</code>, and <code class="inlineCode">fetch_xor()</code> to atomically perform a bitwise AND, OR, or XOR operation between the argument and the atomic value; store the new value in the atomic object; and return the previous value.</li>
<li class="bulletList">Prefixing and postfixing <code class="inlineCode">operator++</code> and <code class="inlineCode">operator--</code> to atomically increment and decrement the value of the atomic object with 1. These operations are equivalent to using <code class="inlineCode">fetch_add()</code> or <code class="inlineCode">fetch_sub()</code>.</li>
<li class="bulletList"><code class="inlineCode">operator +=</code>, <code class="inlineCode">-=</code>, <code class="inlineCode">&amp;=</code>, <code class="inlineCode">|=</code>, and <code class="inlineCode">ˆ=</code> to add, subtract, or perform bitwise AND, OR, or XOR operations between the argument and the atomic value and store the new value in the atomic object. These operations are equivalent to using <code class="inlineCode">fetch_add()</code>, <code class="inlineCode">fetch_sub()</code>, <code class="inlineCode">fetch_and()</code>, <code class="inlineCode">fetch_or()</code>, and <code class="inlineCode">fetch_xor()</code>.</li>
</ul>
<p class="normal">Consider you <a id="_idIndexMarker1010"/>have an atomic variable, such as <code class="inlineCode">std::atomic&lt;int&gt; a</code>; the following is not an atomic operation:</p>
<pre class="programlisting code"><code class="hljs-code">a = a + 42;
</code></pre>
<p class="normal">This involves a series of operations, some of which are atomic:</p>
<ul>
<li class="bulletList">Atomically load the value of the atomic object</li>
<li class="bulletList">Add 42 to the value that was loaded (which is not an atomic operation)</li>
<li class="bulletList">Atomically store the result in the atomic object <code class="inlineCode">a</code></li>
</ul>
<p class="normal">On the other hand, the following operation, which uses the member operator <code class="inlineCode">+=</code>, is atomic:</p>
<pre class="programlisting code"><code class="hljs-code">a += 42;
</code></pre>
<p class="normal">This operation has the same effect as either of the following:</p>
<pre class="programlisting code"><code class="hljs-code">a.fetch_add(42);               // using member function
std::atomic_fetch_add(&amp;a, 42); // using non-member function
</code></pre>
<p class="normal">Though <code class="inlineCode">std::atomic</code> has a full specialization for the <code class="inlineCode">bool</code> type, called <code class="inlineCode">std::atomic&lt;bool&gt;</code>, the standard defines yet another atomic type called <code class="inlineCode">std::atomic_flag</code>, which is guaranteed to be lock-free. This atomic type, however, is very different than <code class="inlineCode">std::atomic&lt;bool&gt;</code>, and it has only the following member functions:</p>
<ul>
<li class="bulletList"><code class="inlineCode">test_and_set()</code> atomically sets the value to <code class="inlineCode">true</code> and returns the previous value.</li>
<li class="bulletList"><code class="inlineCode">clear()</code> atomically sets the value to <code class="inlineCode">false</code>.</li>
<li class="bulletList">In C++20, there’s <code class="inlineCode">test()</code>, which atomically returns the value of the flag.</li>
</ul>
<p class="normal">Prior to C++20, the<a id="_idIndexMarker1011"/> only way to initialize a <code class="inlineCode">std::atomic_flag</code> to a definite value was by using the <code class="inlineCode">ATOMIC_FLAG_INIT</code> macro. This initializes the atomic flag to the clear (<code class="inlineCode">false</code>) value:</p>
<pre class="programlisting code"><code class="hljs-code">std::atomic_flag lock = ATOMIC_FLAG_INIT;
</code></pre>
<p class="normal">In C++20, this macro has been deprecated because the default constructor of <code class="inlineCode">std::atomic_flag</code> initializes it to the clear state.</p>
<p class="normal">All member functions mentioned earlier, for both <code class="inlineCode">std::atomic</code> and <code class="inlineCode">std::atomic_flag</code>, have non-member equivalents that are prefixed with <code class="inlineCode">atomic_</code> or <code class="inlineCode">atomic_flag_</code>, depending on the type they refer to. For instance, the equivalent of <code class="inlineCode">std::atomic::fetch_add()</code> is <code class="inlineCode">std::atomic_fetch_add()</code>, and the first argument of these non-member functions is always a pointer to a <code class="inlineCode">std::atomic</code> object. Internally, the non-member function calls the equivalent member function on the provided <code class="inlineCode">std::atomic</code> argument. Similarly, the equivalent of <code class="inlineCode">std::atomic_flag::test_and_set()</code> is <code class="inlineCode">std::atomic_flag_test_and_set()</code>, and its first parameter is a pointer to a <code class="inlineCode">std::atomic_flag</code> object.</p>
<p class="normal">All these member functions of <code class="inlineCode">std::atomic</code> and <code class="inlineCode">std::atomic_flag</code> have two sets of overloads; one of them has an extra argument representing a memory order. Similarly, all non-member functions – such as <code class="inlineCode">std::atomic_load()</code>, <code class="inlineCode">std::atomic_fetch_add()</code>, and <code class="inlineCode">std::atomic_flag_test_and_set()</code> – have a companion with the suffix <code class="inlineCode">_explicit</code> – <code class="inlineCode">std::atomic_load_explicit()</code>, <code class="inlineCode">std::atomic_fetch_add_explicit()</code>, and <code class="inlineCode">std::atomic_flag_test_and_set_explicit()</code>; these functions have an extra argument that represents the memory order.</p>
<p class="normal">The memory order specifies how non-atomic memory accesses are to be ordered around atomic operations. By default, the memory order of all atomic types and operations is <em class="italic">sequential consistency</em>.</p>
<p class="normal">Additional ordering types are defined in the <code class="inlineCode">std::memory_order</code> enumeration and can be passed as an argument to the member functions of <code class="inlineCode">std::atomic</code> and <code class="inlineCode">std::atomic_flag</code>, or the non-member functions with the suffix <code class="inlineCode">_explicit()</code>.</p>
<div><p class="normal"><em class="italic">Sequential consistency</em> is a consistency model that requires that, in a multiprocessor system, all instructions are executed in some order and all writes become instantly visible throughout the system. This model was first proposed by Leslie Lamport in the 70s, and is described as follows:</p>
<p class="normal"><em class="italic">“the results of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.”</em></p>
</div>
<p class="normal">Various types <a id="_idIndexMarker1012"/>of memory ordering functions are described in the following table, taken from the C++ reference website (<a href="http://en.cppreference.com/w/cpp/atomic/memory_order">http://en.cppreference.com/w/cpp/atomic/memory_order</a>). The details of how each of these works is beyond the scope of this book and can be looked up in the standard C++ reference (see the previous link):</p>
<table class="table-container" id="table001-5">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Model</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Explanation</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">memory_order_relaxed</code></p>
</td>
<td class="table-cell">
<p class="normal">This is a relaxed operation. There are no synchronization or ordering constraints; only atomicity is required from this operation.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">memory_order_consume</code></p>
</td>
<td class="table-cell">
<p class="normal">A load operation with this memory order performs a consume operation on the affected memory location; no reads or writes in the current thread that are dependent on the value currently loaded can be reordered before this load operation. Writes to data-dependent variables in other threads that release the same atomic variable are visible in the current thread. On most platforms, this affects compiler optimizations only.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">memory_order_acquire</code></p>
</td>
<td class="table-cell">
<p class="normal">A load operation with this memory order performs the acquire operation on the affected memory location; no reads or writes in the current thread can be reordered before this load. All writes in other threads that release the same atomic variable are visible in the current thread.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">memory_order_release</code></p>
</td>
<td class="table-cell">
<p class="normal">A store operation with this memory order performs the release operation; no reads or writes in the current thread can be reordered after this store. All writes in the current thread are visible in other threads that acquire the same atomic variable, and writes that carry a dependency to the atomic variable become visible in other threads that consume the same atomic variable.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">memory_order_acq_rel</code></p>
</td>
<td class="table-cell">
<p class="normal">A read-modify-write operation with this memory order is both an acquire operation and a release operation. No memory reads or writes in the current thread can be reordered before or after this store. All writes in other threads that release the same atomic variable are visible before the modification, and the modification is visible in other threads that acquire the same atomic variable.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">memory_order_seq_cst</code></p>
</td>
<td class="table-cell">
<p class="normal">Any operation with this memory order is both an acquire operation and a release operation; a single total order exists in which all threads observe all modifications in the same order.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 8.1: std::memory_order members that describe how memory access is ordered for an atomic operation</p>
<p class="normal">The first <a id="_idIndexMarker1013"/>example in the <em class="italic">How to do it...</em> section shows several threads repeatedly modifying a shared resource – a counter – by incrementing it concurrently. This example can be refined further by implementing a class to represent an atomic counter with methods such as <code class="inlineCode">increment()</code> and <code class="inlineCode">decrement()</code>, which modify the value of the counter, and <code class="inlineCode">get()</code>, which retrieves its current value:</p>
<pre class="programlisting code"><code class="hljs-code">template &lt;typename T, 
          typename I = 
            typename std::enable_if&lt;std::is_integral_v&lt;T&gt;&gt;::type&gt;
class atomic_counter
{
  std::atomic&lt;T&gt; counter {0};
public:
  T increment()
 {
    return counter.fetch_add(1);
  }
  T decrement()
 {
    return counter.fetch_sub(1);
  }
  T get()
 {
    return counter.load();
  }
};
</code></pre>
<p class="normal">With this class<a id="_idIndexMarker1014"/> template, the first example can be rewritten in the following form with the same result:</p>
<pre class="programlisting code"><code class="hljs-code">atomic_counter&lt;int&gt; counter;
std::vector&lt;std::thread&gt; threads;
for(int i = 0; i &lt; 10; ++i)
{
  threads.emplace_back([&amp;counter](){
    for(int j = 0; j &lt; 10; ++j)
      counter.increment();
  });
}
for(auto &amp; t : threads) t.join();
std::cout &lt;&lt; counter.get() &lt;&lt; '\n'; // prints 100
</code></pre>
<p class="normal">If you need to perform atomic operations on references, you cannot use <code class="inlineCode">std::atomic</code>. However, in C++20, you can use the new <code class="inlineCode">std::atomic_ref</code> type. This is a class template that applies atomic operations to the object it references. This object must outlive the <code class="inlineCode">std::atomic_ref</code> object and, as long as any <code class="inlineCode">std::atomic_ref</code> instance referencing this object exists, the object must be accessed only through the <code class="inlineCode">std::atomic_ref</code> instances.</p>
<p class="normal">The <code class="inlineCode">std::atomic_ref</code> type has the following specializations:</p>
<ul>
<li class="bulletList">The primary template can be instantiated with any trivially copyable type <code class="inlineCode">T</code>, including <code class="inlineCode">bool</code>.</li>
<li class="bulletList">Partial specialization for all pointer types.</li>
<li class="bulletList">Specializations for integral types (character types, signed and unsigned integer types, and any additional integral types needed by the typedefs in the <code class="inlineCode">&lt;cstdint&gt;</code> header).</li>
<li class="bulletList">Specializations for the floating-point types <code class="inlineCode">float</code>, <code class="inlineCode">double</code>, and <code class="inlineCode">long double</code>.</li>
</ul>
<p class="normal">When using <code class="inlineCode">std::atomic_ref</code>, you must keep in mind that:</p>
<ul>
<li class="bulletList">It is not thread-safe to access any sub-object of the object referenced by a <code class="inlineCode">std::atomic_ref</code>.</li>
<li class="bulletList">It is possible to modify the referenced value through a const <code class="inlineCode">std::atomic_ref</code> object.</li>
</ul>
<p class="normal">Also, in C++20, there <a id="_idIndexMarker1015"/>are new member and non-member functions that provide an efficient thread-synchronization mechanism:</p>
<ul>
<li class="bulletList">The member function <code class="inlineCode">wait()</code> and non-member functions <code class="inlineCode">atomic_wait()</code>/<code class="inlineCode">atomic_wait_explicit()</code> and <code class="inlineCode">atomic_flag_wait()</code>/<code class="inlineCode">atomic_flag_wait_explicit()</code> perform atomic wait operations, blocking a thread until notified and the atomic value changes. Its behavior is similar to repeatedly comparing the provided argument with the value returned by <code class="inlineCode">load()</code> and, if equal, blocks until notified by <code class="inlineCode">notify_one()</code> or <code class="inlineCode">notify_all()</code>, or the thread is unblocked spuriously. If the compared values are not equal, then the function returns without blocking.</li>
<li class="bulletList">The member function <code class="inlineCode">notify_one()</code> and non-member functions <code class="inlineCode">atomic_notify_one()</code> and <code class="inlineCode">atomic_flag_notify_one()</code> notify, atomically, at least one thread blocked in an atomic waiting operation. If there is no such thread blocked, the function does nothing.</li>
<li class="bulletList">The member function <code class="inlineCode">notify_all()</code> and the non-member functions <code class="inlineCode">atomic_notify_all()</code> and <code class="inlineCode">atomic_flag_notify_all()</code> unblock all the threads blocked in an atomic waiting operation or do nothing if no such thread exists.</li>
</ul>
<p class="normal">Finally, it should be mentioned that all the atomic objects from the standard atomic operations library – <code class="inlineCode">std::atomic</code>, <code class="inlineCode">std::atomic_ref</code>, and <code class="inlineCode">std::atomic_flag</code> – are free of data races.</p>
<h2 class="heading-2" id="_idParaDest-534">See also</h2>
<ul>
<li class="bulletList"><em class="italic">Working with threads</em>, to learn about the <code class="inlineCode">std::thread</code> class and the basic operations for working with threads in C++</li>
<li class="bulletList"><em class="italic">Synchronizing access to shared data with mutexes and locks</em>, to see what mechanisms are available for synchronizing thread access to shared data and how they work</li>
<li class="bulletList"><em class="italic">Executing functions asynchronously</em>, to learn how to use the <code class="inlineCode">std::future</code> class and the <code class="inlineCode">std::async()</code> function to execute functions asynchronously on different threads and communicate the result back</li>
</ul>
<h1 class="heading-1" id="_idParaDest-535">Implementing parallel map and fold with threads</h1>
<p class="normal">In <em class="italic">Chapter 3</em>, <em class="italic">Exploring Functions</em>, we discussed two higher-order functions: <code class="inlineCode">map</code>, which applies a function to the elements of a range by either transforming the range or producing a <a id="_idIndexMarker1016"/>new range, and <code class="inlineCode">fold</code> (also referred to as <code class="inlineCode">reduce</code>), which combines the elements of a range into a single value. The <a id="_idIndexMarker1017"/>various implementations we did were sequential. However, in the context of concurrency, threads, and asynchronous tasks, we can leverage the hardware and run parallel versions of these functions to speed up their execution for large ranges, or when the transformation and aggregation are time-consuming. In this recipe, we will see a possible solution for implementing <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> using threads.</p>
<h2 class="heading-2" id="_idParaDest-536">Getting ready</h2>
<p class="normal">You need to be familiar with the concepts of the <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> functions. It is recommended that you read the <em class="italic">Implementing higher-order functions map and fold</em> recipe from <em class="chapterRef">Chapter 3</em>, <em class="italic">Exploring Functions</em>. In this recipe, we will use the various thread functionalities presented in the <em class="italic">Working with threads</em> recipe.</p>
<p class="normal">To measure the execution time of these functions and compare it with sequential alternatives, we will use the <code class="inlineCode">perf_timer</code> class template, which we introduced in the <em class="italic">Measuring function execution time with a standard clock</em> recipe in <em class="chapterRef">Chapter 6</em>, <em class="italic">General-Purpose Utilities</em>.</p>
<div><p class="normal">A parallel version of an algorithm can potentially speed up execution time, but this is not necessarily true in all circumstances. Context switching for threads and synchronized access to shared data can introduce a significant overhead. For some implementations and particular datasets, this overhead could make a parallel version actually take a longer time to execute than a sequential version.</p>
</div>
<p class="normal">To determine the number of threads required to split the work, we will use the following function:</p>
<pre class="programlisting code"><code class="hljs-code">unsigned get_no_of_threads()
{
  return std::thread::hardware_concurrency();
}
</code></pre>
<p class="normal">We’ll explore <a id="_idIndexMarker1018"/>the first possible implementation for a parallel version of the <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> functions in the next section.</p>
<h2 class="heading-2" id="_idParaDest-537">How to do it...</h2>
<p class="normal">To <a id="_idIndexMarker1019"/>implement a parallel version of the <code class="inlineCode">map</code> function, do the following:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Define a function template that takes the <code class="inlineCode">begin</code> and <code class="inlineCode">end</code> iterators of a range and a function to apply to all the elements:
        <pre class="programlisting code"><code class="hljs-code">template &lt;typename Iter, typename F&gt;
void parallel_map(Iter begin, Iter end, F&amp;&amp; f)
{
}
</code></pre>
</li>
<li class="numberedList">Check the size of the range. If the number of elements is smaller than a predefined threshold (for this implementation, the threshold is 10,000), execute the mapping in a sequential manner:
        <pre class="programlisting code"><code class="hljs-code">auto size = std::distance(begin, end);
if(size &lt;= 10000)
  std::transform(begin, end, begin, std::forward&lt;F&gt;(f));
</code></pre>
</li>
<li class="numberedList">For larger ranges, split the work on multiple threads and let each thread map be a part of the range. These parts should not overlap to avoid the need to synchronize access to the shared data:
        <pre class="programlisting code"><code class="hljs-code">else
{
  auto no_of_threads = get_no_of_threads();
  auto part = size / no_of_threads;
  auto last = begin;
  // continued at 4. and 5.
}
</code></pre>
</li>
<li class="numberedList">Start the threads, and on each thread, run a sequential version of the mapping:
        <pre class="programlisting code"><code class="hljs-code">std::vector&lt;std::thread&gt; threads;
for(unsigned i = 0; i &lt; no_of_threads; ++i)
{
  if(i == no_of_threads - 1) last = end;
  else std::advance(last, part);
  threads.emplace_back(
    [=,&amp;f]{std::transform(begin, last, 
                          begin, std::forward&lt;F&gt;(f));});
  begin = last;
}
</code></pre>
</li>
<li class="numberedList">Wait until all the threads have finished their execution:
        <pre class="programlisting code"><code class="hljs-code">for(auto &amp; t : threads) t.join();
</code></pre>
</li>
</ol>
<p class="normal">The <a id="_idIndexMarker1020"/>preceding steps, when put together, result <a id="_idIndexMarker1021"/>in the following implementation:</p>
<pre class="programlisting code"><code class="hljs-code">template &lt;typename Iter, typename F&gt;
void parallel_map(Iter begin, Iter end, F&amp;&amp; f)
{
  auto size = std::distance(begin, end);
  if(size &lt;= 10000)
    std::transform(begin, end, begin, std::forward&lt;F&gt;(f)); 
  else
  {
    auto no_of_threads = get_no_of_threads();
    auto part = size / no_of_threads;
    auto last = begin;
    std::vector&lt;std::thread&gt; threads;
    for(unsigned i = 0; i &lt; no_of_threads; ++i)
    {
      if(i == no_of_threads - 1) last = end;
      else std::advance(last, part);
      threads.emplace_back(
        [=,&amp;f]{std::transform(begin, last, 
                              begin, std::forward&lt;F&gt;(f));});
      begin = last;
    }
    for(auto &amp; t : threads) t.join();
  }
}
</code></pre>
<p class="normal">To implement a <a id="_idIndexMarker1022"/>parallel version of<a id="_idIndexMarker1023"/> the left <code class="inlineCode">fold</code> function, do the following:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Define a function template that takes a <code class="inlineCode">begin</code> and an <code class="inlineCode">end</code> iterator for a range, an initial value, and a binary function to apply to the elements of the range:
        <pre class="programlisting code"><code class="hljs-code">template &lt;typename Iter, typename R, typename F&gt;
auto parallel_fold(Iter begin, Iter end, R init, F&amp;&amp; op)
{
}
</code></pre>
</li>
<li class="numberedList">Check the size of the range. If the number of elements is smaller than a predefined threshold (for this implementation, it is 10,000), execute the folding in a sequential manner:
        <pre class="programlisting code"><code class="hljs-code">auto size = std::distance(begin, end);
if(size &lt;= 10000)
  return std::accumulate(begin, end, 
                         init, std::forward&lt;F&gt;(op));
</code></pre>
</li>
<li class="numberedList">For larger ranges, split the work into multiple threads and let each thread fold a part of the range. These parts should not overlap in order to avoid thread synchronization of shared data. The result can be returned through a reference passed to the thread function in order to avoid data synchronization:
        <pre class="programlisting code"><code class="hljs-code">else
{
  auto no_of_threads = get_no_of_threads();
  auto part = size / no_of_threads;
  auto last = begin;
  // continued with 4. and 5.
}
</code></pre>
</li>
<li class="numberedList">Start the threads, and on each thread, execute a sequential version of the folding:
        <pre class="programlisting code"><code class="hljs-code">std::vector&lt;std::thread&gt; threads;
std::vector&lt;R&gt; values(no_of_threads);
for(unsigned i = 0; i &lt; no_of_threads; ++i)
{
  if(i == no_of_threads - 1) last = end;
  else std::advance(last, part);
  threads.emplace_back(
    [=,&amp;op](R&amp; result){
      result = std::accumulate(begin, last, R{}, 
                               std::forward&lt;F&gt;(op));},
    std::ref(values[i]));
  begin = last;
}
</code></pre>
</li>
<li class="numberedList">Wait<a id="_idIndexMarker1024"/> until all the threads have finished execution and fold the partial results into the final result:
        <pre class="programlisting code"><code class="hljs-code">for(auto &amp; t : threads) t.join();
return std::accumulate(std::begin(values), std::end(values),
                       init, std::forward&lt;F&gt;(op));
</code></pre>
</li>
</ol>
<p class="normal">The steps<a id="_idIndexMarker1025"/> we just put together result in the following implementation:</p>
<pre class="programlisting code"><code class="hljs-code">template &lt;typename Iter, typename R, typename F&gt;
auto parallel_fold(Iter begin, Iter end, R init, F&amp;&amp; op)
{
  auto size = std::distance(begin, end);
  if(size &lt;= 10000)
    return std::accumulate(begin, end, init, std::forward&lt;F&gt;(op));
  else
  {
    auto no_of_threads = get_no_of_threads();
    auto part = size / no_of_threads;
    auto last = begin;
    std::vector&lt;std::thread&gt; threads;
    std::vector&lt;R&gt; values(no_of_threads);
    for(unsigned i = 0; i &lt; no_of_threads; ++i)
    {
      if(i == no_of_threads - 1) last = end;
      else std::advance(last, part);
      threads.emplace_back(
        [=,&amp;op](R&amp; result){
          result = std::accumulate(begin, last, R{}, 
                                   std::forward&lt;F&gt;(op));},
        std::ref(values[i]));
      begin = last;
    }
    for(auto &amp; t : threads) t.join();
    return std::accumulate(std::begin(values), std::end(values), 
                           init, std::forward&lt;F&gt;(op));
  }
}
</code></pre>
<h2 class="heading-2" id="_idParaDest-538">How it works...</h2>
<p class="normal">These <a id="_idIndexMarker1026"/>parallel implementations<a id="_idIndexMarker1027"/> of <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> are similar in several aspects:</p>
<ul>
<li class="bulletList">They both fall back to a sequential version if the number of elements in the range is smaller than 10,000.</li>
<li class="bulletList">They both start the same number of threads. These threads are determined using the static function <code class="inlineCode">std::thread::hardware_concurrency()</code>, which returns the number of concurrent threads supported by the implementation. However, this value is more of a hint than an accurate value and should be used with that in mind.</li>
<li class="bulletList">No shared data is used to avoid synchronization of access. Even though all the threads work on the elements from the same range, they all process parts of the range that do not overlap.</li>
<li class="bulletList">Both of these functions are implemented as function templates that take a begin and an end iterator to define the range to be processed. In order to split the range into multiple parts to be processed independently by different threads, use additional iterators in the middle of the range. For this, we use <code class="inlineCode">std::advance()</code> to increment an iterator with a particular number of positions. This works well for vectors or arrays but is very inefficient for containers such as lists. Therefore, this implementation is suited only for ranges that have random access iterators.</li>
</ul>
<p class="normal">The <a id="_idIndexMarker1028"/>sequential versions of <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> can be simply implemented in C++ with <code class="inlineCode">std::transform()</code> and <code class="inlineCode">std::accumulate()</code>. In fact, to verify the correctness of the parallel algorithms and check whether they provide any execution speedup, we can compare them with the execution of these general-purpose algorithms.</p>
<p class="normal">To put this to <a id="_idIndexMarker1029"/>the test, we will use <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> on a vector with sizes varying from 10,000 to 50 million elements. The range is first mapped (that is, transformed) by doubling the value of each element, and then the result is folded into a single value by adding together all the elements of the range. For simplicity, each element in the range is equal to its 1-based index (the first element is 1, the second element is 2, and so on). The following sample runs both the sequential and parallel versions of <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> on vectors of different sizes and prints the execution time in a tabular format:</p>
<div><p class="normal">As an exercise, you can vary the number of elements, as well as the number of threads, and see how the parallel version performs compared to the sequential version.</p>
</div>
<pre class="programlisting code"><code class="hljs-code">std::vector&lt;int&gt; sizes
{
  10000, 100000, 500000, 
  1000000, 2000000, 5000000, 
  10000000, 25000000, 50000000
};
std::cout
  &lt;&lt; std::right &lt;&lt; std::setw(8) &lt;&lt; std::setfill(' ') &lt;&lt; "size"
  &lt;&lt; std::right &lt;&lt; std::setw(8) &lt;&lt; "s map"
  &lt;&lt; std::right &lt;&lt; std::setw(8) &lt;&lt; "p map"
  &lt;&lt; std::right &lt;&lt; std::setw(8) &lt;&lt; "s fold"
  &lt;&lt; std::right &lt;&lt; std::setw(8) &lt;&lt; "p fold"
  &lt;&lt; '\n';
for (auto const size : sizes)
{
  std::vector&lt;int&gt; v(size);
  std::iota(std::begin(v), std::end(v), 1);
  auto v1 = v;
  auto s1 = 0LL;
  auto tsm = perf_timer&lt;&gt;::duration([&amp;] {
    std::transform(std::begin(v1), std::end(v1), std::begin(v1), 
                   [](int const i) {return i + i; }); });
  auto tsf = perf_timer&lt;&gt;::duration([&amp;] {
    s1 = std::accumulate(std::begin(v1), std::end(v1), 0LL,
                         std::plus&lt;&gt;()); });
  auto v2 = v;
  auto s2 = 0LL;
  auto tpm = perf_timer&lt;&gt;::duration([&amp;] {
    parallel_map(std::begin(v2), std::end(v2), 
                 [](int const i) {return i + i; }); });
  auto tpf = perf_timer&lt;&gt;::duration([&amp;] {
    s2 = parallel_fold(std::begin(v2), std::end(v2), 0LL,
                       std::plus&lt;&gt;()); });
  assert(v1 == v2);
  assert(s1 == s2);
  std::cout
    &lt;&lt; std::right &lt;&lt; std::setw(8) &lt;&lt; std::setfill(' ') &lt;&lt; size
    &lt;&lt; std::right &lt;&lt; std::setw(8) 
    &lt;&lt; std::chrono::duration&lt;double, std::micro&gt;(tsm).count()
    &lt;&lt; std::right &lt;&lt; std::setw(8) 
    &lt;&lt; std::chrono::duration&lt;double, std::micro&gt;(tpm).count()
    &lt;&lt; std::right &lt;&lt; std::setw(8) 
    &lt;&lt; std::chrono::duration&lt;double, std::micro&gt;(tsf).count()
    &lt;&lt; std::right &lt;&lt; std::setw(8) 
    &lt;&lt; std::chrono::duration&lt;double, std::micro&gt;(tpf).count()
    &lt;&lt; '\n';
}
</code></pre>
<p class="normal">A possible output of this program is shown in the following chart (executed on a machine running Windows 64-bit with an Intel Core i7 processor and 4 physical and 8 logical cores). The parallel version, especially the <code class="inlineCode">fold</code> implementation, performs better than the sequential version. But this<a id="_idIndexMarker1030"/> is true only when the length of the vector exceeds a certain size. In the following table, we can see that for up to 1 million elements, the sequential version is still faster. The parallel version executes faster when there are 2 million or more elements in the vector. Notice that the actual times vary slightly from one run to another, even on the same machine, and they can be very different on different machines:</p>
<pre class="programlisting con"><code class="hljs-con">    size   s map   p map  s fold  p fold
   10000      11      10       7      10
  100000     108    1573      72     710
  500000     547    2006     361     862
 1000000    1146    1163     749     862
 2000000    2503    1527    1677    1289
 5000000    5937    3000    4203    2314
10000000   11959    6269    8269    3868
25000000   29872   13823   20961    9156
50000000   60049   27457   41374   19075
</code></pre>
<p class="normal">To better <a id="_idIndexMarker1031"/>visualize these results, we can represent the speedup of the parallel version in the form of a bar chart. In the following chart, the blue bars represent the speedup of a parallel <code class="inlineCode">map</code> implementation, while the orange bars show the speedup of the parallel <code class="inlineCode">fold</code> implementation. A positive value indicates that the parallel version is faster; a negative version indicates that the sequential version is faster:</p>
<figure class="mediaobject"><img alt="" src="img/B21549_08_01.png"/></figure>
<p class="packt_figref">Figure 8.1: The speedup of the parallel implementation for map (in blue) and fold (in orange) for various processed elements</p>
<p class="normal">This <a id="_idIndexMarker1032"/>chart makes it easier to see that only when the number of elements exceeds a certain threshold (which is about 2 million in my benchmarks) is the parallel implementation <a id="_idIndexMarker1033"/>faster than the sequential version.</p>
<h2 class="heading-2" id="_idParaDest-539">See also</h2>
<ul>
<li class="bulletList"><em class="chapterRef">Chapter 3</em>, <em class="italic">Implementing higher-order functions map and fold</em>, to learn about higher-order functions in functional programming and see how to implement the widely used <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> (or reduce) functions</li>
<li class="bulletList"><em class="italic">Implementing parallel map and fold with tasks</em>, to see how to implement the <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> functions from functional programming using asynchronous functions</li>
<li class="bulletList"><em class="italic">Implementing parallel map and fold with standard parallel algorithms</em>, to see how to implement the <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> functions from functional programming using parallel algorithms from C++17</li>
<li class="bulletList"><em class="italic">Working with threads</em>, to learn about the <code class="inlineCode">std::thread</code> class and the basic operations for working with threads in C++</li>
</ul>
<h1 class="heading-1" id="_idParaDest-540">Implementing parallel map and fold with tasks</h1>
<p class="normal">Tasks <a id="_idIndexMarker1034"/>are a<a id="_idIndexMarker1035"/> higher-level alternative to threads for performing concurrent computations. <code class="inlineCode">std::async()</code> enables us to execute functions asynchronously, without the need to handle lower-level threading details. In this recipe, we will take the same task of implementing a parallel version of the <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> functions, as in the previous recipe, but we will use tasks and see how it compares with the thread version.</p>
<h2 class="heading-2" id="_idParaDest-541">Getting ready</h2>
<p class="normal">The <a id="_idIndexMarker1036"/>solution presented in this recipe is similar in many aspects to the one that uses threads in the previous recipe, <em class="italic">Implementing parallel map and fold with threads</em>. Make sure you read that one before continuing with the current recipe.</p>
<h2 class="heading-2" id="_idParaDest-542">How to do it...</h2>
<p class="normal">To implement a parallel version of the <code class="inlineCode">map</code> function, do the following:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Define a function template that takes a begin and end iterator for a range and a function to apply to all the elements:
        <pre class="programlisting code"><code class="hljs-code">template &lt;typename Iter, typename F&gt;
void parallel_map(Iter begin, Iter end, F&amp;&amp; f)
{
}
</code></pre>
</li>
<li class="numberedList">Check the size of the range. For a number of elements smaller than the predefined threshold (for this implementation, the threshold is 10,000), execute the mapping in a sequential manner:
        <pre class="programlisting code"><code class="hljs-code">auto size = std::distance(begin, end);
if(size &lt;= 10000)
  std::transform(begin, end, begin, std::forward&lt;F&gt;(f));
</code></pre>
</li>
<li class="numberedList">For larger ranges, split the work into multiple tasks and let each task map a part of the range. These parts should not overlap to avoid synchronizing thread access to shared data:
        <pre class="programlisting code"><code class="hljs-code">else
{
  auto no_of_tasks = get_no_of_threads();
  auto part = size / no_of_tasks;
  auto last = begin;
  // continued at 4. and 5.
}
</code></pre>
</li>
<li class="numberedList">Start the<a id="_idIndexMarker1037"/> asynchronous functions and run a sequential version of the mapping on each of them:
        <pre class="programlisting code"><code class="hljs-code">std::vector&lt;std::future&lt;void&gt;&gt; tasks;
for(unsigned i = 0; i &lt; no_of_tasks; ++i)
{
  if(i == no_of_tasks - 1) last = end;
  else std::advance(last, part);
  tasks.emplace_back(std::async(
    std::launch::async, 
      [=,&amp;f]{std::transform(begin, last, begin, 
                            std::forward&lt;F&gt;(f));}));
    begin = last;
}
</code></pre>
</li>
<li class="numberedList">Wait until all the asynchronous functions have finished their execution:
        <pre class="programlisting code"><code class="hljs-code">for(auto &amp; t : tasks) t.wait();
</code></pre>
</li>
</ol>
<p class="normal">These steps, when <a id="_idIndexMarker1038"/>put together, result in the following implementation:</p>
<pre class="programlisting code"><code class="hljs-code">template &lt;typename Iter, typename F&gt;
void parallel_map(Iter begin, Iter end, F&amp;&amp; f)
{
  auto size = std::distance(begin, end);
  if(size &lt;= 10000)
    std::transform(begin, end, begin, std::forward&lt;F&gt;(f)); 
  else
  {
    auto no_of_tasks = get_no_of_threads();
    auto part = size / no_of_tasks;
    auto last = begin;
    std::vector&lt;std::future&lt;void&gt;&gt; tasks;
    for(unsigned i = 0; i &lt; no_of_tasks; ++i)
    {
      if(i == no_of_tasks - 1) last = end;
      else std::advance(last, part);
      tasks.emplace_back(std::async(
        std::launch::async, 
          [=,&amp;f]{std::transform(begin, last, begin, 
                                std::forward&lt;F&gt;(f));}));
      begin = last;
    }
    for(auto &amp; t : tasks) t.wait();
  }
}
</code></pre>
<p class="normal">To implement a <a id="_idIndexMarker1039"/>parallel version of the left <code class="inlineCode">fold</code> function, do the following:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Define a function template that takes a begin and end iterator for a range, an initial value, and a binary function to apply to the elements of the range:
        <pre class="programlisting code"><code class="hljs-code">template &lt;typename Iter, typename R, typename F&gt;
auto parallel_fold(Iter begin, Iter end, R init, F&amp;&amp; op)
{
}
</code></pre>
</li>
<li class="numberedList">Check the size of the range. For a number of elements smaller than the predefined threshold (for this implementation, the threshold is 10,000), execute the folding in a sequential manner:
        <pre class="programlisting code"><code class="hljs-code">auto size = std::distance(begin, end);
if(size &lt;= 10000)
  return std::accumulate(begin, end, init, std::forward&lt;F&gt;(op));
</code></pre>
</li>
<li class="numberedList">For <a id="_idIndexMarker1040"/>larger ranges, split the work into multiple tasks and let each task fold a part of the range. These parts should not overlap to avoid synchronizing thread access to the shared data. The result can be returned through a reference passed to the asynchronous function to avoid synchronization:
        <pre class="programlisting code"><code class="hljs-code">else
{
  auto no_of_tasks = get_no_of_threads();
  auto part = size / no_of_tasks;
  auto last = begin;
  // continued at 4. and 5.
}
</code></pre>
</li>
<li class="numberedList">Start the<a id="_idIndexMarker1041"/> asynchronous functions and execute a sequential version of folding on each one of them:
        <pre class="programlisting code"><code class="hljs-code">std::vector&lt;std::future&lt;R&gt;&gt; tasks;
for(unsigned i = 0; i &lt; no_of_tasks; ++i)
{
  if(i == no_of_tasks - 1) last = end;
  else std::advance(last, part);
  tasks.emplace_back(
    std::async(
      std::launch::async,
      [=,&amp;op]{return std::accumulate(
                          begin, last, R{}, 
                          std::forward&lt;F&gt;(op));}));
  begin = last;
}
</code></pre>
</li>
<li class="numberedList">Wait until <a id="_idIndexMarker1042"/>all the asynchronous functions have finished execution and fold the partial results into the final result:
        <pre class="programlisting code"><code class="hljs-code">std::vector&lt;R&gt; values;
for(auto &amp; t : tasks)
  values.push_back(t.get());
return std::accumulate(std::begin(values), std::end(values), 
                       init, std::forward&lt;F&gt;(op));
</code></pre>
</li>
</ol>
<p class="normal">These steps, when put together, result in the following implementation:</p>
<pre class="programlisting code"><code class="hljs-code">template &lt;typename Iter, typename R, typename F&gt;
auto parallel_fold(Iter begin, Iter end, R init, F&amp;&amp; op)
{
  auto size = std::distance(begin, end);
  if(size &lt;= 10000)
    return std::accumulate(begin, end, init, std::forward&lt;F&gt;(op));
  else
  {
    auto no_of_tasks = get_no_of_threads();
    auto part = size / no_of_tasks;
    auto last = begin;
    std::vector&lt;std::future&lt;R&gt;&gt; tasks;
    for(unsigned i = 0; i &lt; no_of_tasks; ++i)
    {
      if(i == no_of_tasks - 1) last = end;
      else std::advance(last, part);
      tasks.emplace_back(
        std::async(
          std::launch::async,
          [=,&amp;op]{return std::accumulate(
                            begin, last, R{}, 
                            std::forward&lt;F&gt;(op));}));
      begin = last;
    }
    std::vector&lt;R&gt; values;
    for(auto &amp; t : tasks)
      values.push_back(t.get());
    return std::accumulate(std::begin(values), std::end(values), 
                           init, std::forward&lt;F&gt;(op));
  }
}
</code></pre>
<h2 class="heading-2" id="_idParaDest-543">How it works...</h2>
<p class="normal">The <a id="_idIndexMarker1043"/>implementation <a id="_idIndexMarker1044"/>just proposed is only slightly different than what we did in the previous recipe. Threads were replaced with asynchronous functions, starting with <code class="inlineCode">std::async()</code>, and results were made available through the returned <code class="inlineCode">std::future</code>. The number of asynchronous functions that are launched concurrently is equal to the <a id="_idIndexMarker1045"/>number of threads the implementation can support. This is returned by the static method <code class="inlineCode">std::thread::hardware_concurrency()</code>, but this value is only a hint and should not <a id="_idIndexMarker1046"/>be considered very reliable.</p>
<p class="normal">There are mainly two reasons for taking this approach:</p>
<ul>
<li class="bulletList">Seeing how a function implemented for parallel execution with threads can be modified to use asynchronous functions and, therefore, avoid lower-level details of threading.</li>
<li class="bulletList">Running a number of asynchronous functions equal to the number of supported threads can potentially run one function per thread; this could provide the fastest execution time for the parallel function because there is a minimum overhead of context switching and waiting time.</li>
</ul>
<p class="normal">We can test the performance of the new <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> implementations using the same method as in the previous recipe:</p>
<pre class="programlisting code"><code class="hljs-code">std::vector&lt;int&gt; sizes
{
  10000, 100000, 500000,
  1000000, 2000000, 5000000,
  10000000, 25000000, 50000000
};
std::cout
  &lt;&lt; std::right &lt;&lt; std::setw(8) &lt;&lt; std::setfill(' ') &lt;&lt; "size"
  &lt;&lt; std::right &lt;&lt; std::setw(8) &lt;&lt; "s map"
  &lt;&lt; std::right &lt;&lt; std::setw(8) &lt;&lt; "p map"
  &lt;&lt; std::right &lt;&lt; std::setw(8) &lt;&lt; "s fold"
  &lt;&lt; std::right &lt;&lt; std::setw(8) &lt;&lt; "p fold"
  &lt;&lt; '\n';
for(auto const size : sizes)
{
  std::vector&lt;int&gt; v(size);
  std::iota(std::begin(v), std::end(v), 1);
  auto v1 = v;
  auto s1 = 0LL;
  auto tsm = perf_timer&lt;&gt;::duration([&amp;] {
    std::transform(std::begin(v1), std::end(v1), std::begin(v1), 
                   [](int const i) {return i + i; }); });
  auto tsf = perf_timer&lt;&gt;::duration([&amp;] {
    s1 = std::accumulate(std::begin(v1), std::end(v1), 0LL,
                         std::plus&lt;&gt;()); });
auto v2 = v;
auto s2 = 0LL;
auto tpm = perf_timer&lt;&gt;::duration([&amp;] {
  parallel_map(std::begin(v2), std::end(v2), 
               [](int const i) {return i + i; }); });
auto tpf = perf_timer&lt;&gt;::duration([&amp;] {
  s2 = parallel_fold(std::begin(v2), std::end(v2), 0LL, 
                       std::plus&lt;&gt;()); });
assert(v1 == v2);
assert(s1 == s2);
std::cout
  &lt;&lt; std::right &lt;&lt; std::setw(8) &lt;&lt; std::setfill(' ') &lt;&lt; size
  &lt;&lt; std::right &lt;&lt; std::setw(8) 
  &lt;&lt; std::chrono::duration&lt;double, std::micro&gt;(tsm).count()
  &lt;&lt; std::right &lt;&lt; std::setw(8) 
  &lt;&lt; std::chrono::duration&lt;double, std::micro&gt;(tpm).count()
  &lt;&lt; std::right &lt;&lt; std::setw(8) 
  &lt;&lt; std::chrono::duration&lt;double, std::micro&gt;(tsf).count()
  &lt;&lt; std::right &lt;&lt; std::setw(8) 
  &lt;&lt; std::chrono::duration&lt;double, std::micro&gt;(tpf).count()
  &lt;&lt; '\n';
}
</code></pre>
<p class="normal">A <a id="_idIndexMarker1047"/>possible output of the preceding program, which<a id="_idIndexMarker1048"/> can vary slightly from one execution to another and greatly from one machine to another, is as follows:</p>
<pre class="programlisting con"><code class="hljs-con">    size   s map   p map  s fold  p fold
   10000      11      11      11      11
  100000     117     260     113      94
  500000     576     303     571     201
 1000000    1180     573    1165     283
 2000000    2371     911    2330     519
 5000000    5942    2144    5841    1886
10000000   11954    4999   11643    2871
25000000   30525   11737   29053    9048
50000000   59665   22216   58689   12942
</code></pre>
<p class="normal">Similar to the illustration of the solution with threads, the speedup of the parallel <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> implementations can be seen in the following chart.</p>
<p class="normal">Negative values indicate that the sequential version was faster:</p>
<figure class="mediaobject"><img alt="" src="img/B21549_08_02.png"/></figure>
<p class="packt_figref">Figure 8.2: The speedup of the parallel implementation of map (in blue) and fold (in orange) using asynchronous functions, compared to the sequential implementation</p>
<p class="normal">If we <a id="_idIndexMarker1049"/>compare this with the results from <a id="_idIndexMarker1050"/>the parallel version using threads, we will find that these are faster execution times and that the speedup is significant, especially for the <code class="inlineCode">fold</code> function. The following chart shows the speedup of the task’s implementation over the thread’s implementation. In this chart, a value smaller than 1 means that the thread’s implementation was faster:</p>
<figure class="mediaobject"><img alt="" src="img/B21549_08_03.png"/></figure>
<p class="packt_figref">Figure 8.3: The speedup of the parallel implementation using asynchronous functions over the parallel implementation using threads for map (in blue) and fold (in orange)</p>
<h2 class="heading-2" id="_idParaDest-544">There’s more...</h2>
<p class="normal">The<a id="_idIndexMarker1051"/> implementation<a id="_idIndexMarker1052"/> shown earlier is only one of the possible approaches we can take for parallelizing the <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> functions. A possible alternative uses the following strategy:</p>
<ul>
<li class="bulletList">Divide the range to process into two equal parts.</li>
<li class="bulletList">Recursively call the parallel function asynchronously to process the first part of the range.</li>
<li class="bulletList">Recursively call the parallel function synchronously to process the second part of the range.</li>
<li class="bulletList">After the synchronous recursive call is finished, wait for the asynchronous recursive call to end too before finishing the execution.</li>
</ul>
<p class="normal">This divide-and-conquer algorithm can potentially create a lot of tasks. Depending on the size of the range, the number of asynchronous calls can greatly exceed the number of threads, and in this case, there will be lots of waiting time that will affect the overall execution time.</p>
<p class="normal">The <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> functions <a id="_idIndexMarker1053"/>can be implemented<a id="_idIndexMarker1054"/> using a divide-and-conquer algorithm, as follows:</p>
<pre class="programlisting code"><code class="hljs-code">template &lt;typename Iter, typename F&gt;
void parallel_map(Iter begin, Iter end, F f)
{ 
  auto size = std::distance(begin, end);
  if(size &lt;= 10000)
  {
    std::transform(begin, end, begin, std::forward&lt;F&gt;(f)); 
  }
  else
  {
    auto middle = begin;
    std::advance(middle, size / 2);
    auto result = std::async(
      std::launch::deferred, 
      parallel_map&lt;Iter, F&gt;, 
      begin, middle, std::forward&lt;F&gt;(f));
    parallel_map(middle, end, std::forward&lt;F&gt;(f));
    result.wait();
  }
}
template &lt;typename Iter, typename R, typename F&gt;
auto parallel_fold(Iter begin, Iter end, R init, F op)
{
  auto size = std::distance(begin, end);
  if(size &lt;= 10000)
    return std::accumulate(begin, end, init, std::forward&lt;F&gt;(op));
  else
  {
    auto middle = begin;
    std::advance(middle, size / 2);
    auto result1 = std::async(
      std::launch::async, 
      parallel_reduce&lt;Iter, R, F&gt;, 
      begin, middle, R{}, std::forward&lt;F&gt;(op));
    auto result2 = parallel_fold(middle, end, init, 
                                 std::forward&lt;F&gt;(op));
    return result1.get() + result2;
  }
}
</code></pre>
<p class="normal">The execution times for this implementation are listed here, next to the ones for the previous implementations:</p>
<pre class="programlisting con"><code class="hljs-con">    size   s map p1 map  p2 map  s fold p1 fold p2 fold
   10000      11     11      10       7      10      10
  100000     111    275     120      72      96     426
  500000     551    230     596     365     210    1802
 1000000    1142    381    1209     753     303    2378
 2000000    2411    981    2488    1679     503    4190
 5000000    5962   2191    6237    4177    1969    7974
10000000   11961   4517   12581    8384    2966   15174
</code></pre>
<p class="normal">When we<a id="_idIndexMarker1055"/> compare these execution times, we <a id="_idIndexMarker1056"/>can see that this version (indicated by <code class="inlineCode">p2</code> in the preceding output) is similar to the sequential version for both <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> and much worse than the first parallel version shown earlier (indicated by <code class="inlineCode">p1</code>).</p>
<h2 class="heading-2" id="_idParaDest-545">See also</h2>
<ul>
<li class="bulletList"><em class="italic">Implementing parallel map and fold with threads</em>, to see how to implement the <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> functions from functional programming using raw threads</li>
<li class="bulletList"><em class="italic">Implementing parallel map and fold with standard parallel algorithms</em>, to see how to implement the <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> functions from functional programming using parallel algorithms from C++17</li>
<li class="bulletList"><em class="italic">Executing functions asynchronously</em>, to learn how to use the <code class="inlineCode">std::future</code> class and the <code class="inlineCode">std::async()</code> function to execute functions asynchronously on different threads and communicate the result back</li>
</ul>
<h1 class="heading-1" id="_idParaDest-546">Implementing parallel map and fold with standard parallel algorithms</h1>
<p class="normal">In the <a id="_idIndexMarker1057"/>previous two recipes, we implemented parallel versions of the <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> functions (which are called <code class="inlineCode">std::transform()</code> and <code class="inlineCode">std::accumulate()</code> in the standard library) using threads <a id="_idIndexMarker1058"/>and tasks. However, these implementations required manual handling of parallelization details, such as splitting data into chunks to be processed in parallel and creating threads or tasks, synchronizing their execution, and merging the results.</p>
<p class="normal">In C++17, many of the standard generic algorithms have been parallelized. In fact, the same algorithm can execute sequentially or in parallel, depending on a provided execution policy. In this recipe, we will learn how to implement <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> in parallel with standard algorithms.</p>
<h2 class="heading-2" id="_idParaDest-547">Getting ready</h2>
<p class="normal">Before you continue with this recipe, it is recommended that you read the previous two to make sure you understand the differences between various parallel implementations.</p>
<h2 class="heading-2" id="_idParaDest-548">How to do it...</h2>
<p class="normal">To use the standard algorithms with parallel execution, you should do the following:</p>
<ul>
<li class="bulletList">Find a good candidate for an algorithm to parallelize. Not every algorithm runs faster in parallel. Make sure you correctly identify the parts of the program that can be improved with parallelization. Use profilers for this purpose and, in general, look at operations that have <em class="italic">O(n)</em> or worse complexity.</li>
<li class="bulletList">Include the header <code class="inlineCode">&lt;execution&gt;</code> for the execution policies.</li>
<li class="bulletList">Provide the parallel execution policy (<code class="inlineCode">std::execution::par</code>) as the first argument to the overloaded algorithm.</li>
</ul>
<p class="normal">A parallel implementation of the map function using the parallel overload of <code class="inlineCode">std::transform()</code> is as follows:</p>
<pre class="programlisting code"><code class="hljs-code">template &lt;typename Iter, typename F&gt;
void parallel_map(Iter begin, Iter end, F&amp;&amp; f)
{
   std::transform(std::execution::par,
                  begin, end,
                  begin,
                  std::forward&lt;F&gt;(f));
}
</code></pre>
<p class="normal">A <a id="_idIndexMarker1059"/>parallel implementation of the fold function using the parallel overload of <code class="inlineCode">std::reduce()</code> is as follows:</p>
<pre class="programlisting code"><code class="hljs-code">template &lt;typename Iter, typename R, typename F&gt;
auto parallel_fold(Iter begin, Iter end, R init, F&amp;&amp; op)
{
   return std::reduce(std::execution::par,
                      begin, end,
                      init,
                      std::forward&lt;F&gt;(op));
}
</code></pre>
<h2 class="heading-2" id="_idParaDest-549">How it works...</h2>
<p class="normal">In C++17, 69 of <a id="_idIndexMarker1060"/>the standard generic algorithms have been overloaded to support parallel execution. These overloads take an execution policy as the first parameter. The available execution policies, from header <code class="inlineCode">&lt;execution&gt;</code>, are as follows:</p>
<table class="table-container" id="table002-4">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Policy</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Since</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Description</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Global object</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::execution::sequenced_policy</code></p>
</td>
<td class="table-cell">
<p class="normal">C++17</p>
</td>
<td class="table-cell">
<p class="normal">Indicates that the algorithm may not execute in parallel.</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::execution::seq</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::execution::parallel_policy</code></p>
</td>
<td class="table-cell">
<p class="normal">C++17</p>
</td>
<td class="table-cell">
<p class="normal">Indicates that the algorithm’s execution may be parallelized.</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::execution::par</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::execution::parallel_unsequenced_policy</code></p>
</td>
<td class="table-cell">
<p class="normal">C++17</p>
</td>
<td class="table-cell">
<p class="normal">Indicates that the algorithm’s execution may be parallelized and vectorized.</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::execution::par_unseq</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::execution::unsequenced_policy</code></p>
</td>
<td class="table-cell">
<p class="normal">C++20</p>
</td>
<td class="table-cell">
<p class="normal">Indicates that the algorithm’s execution may be vectorized.</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::execution::unseq</code></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 8.2: Execution policies from the &lt;execution&gt; header</p>
<div><p class="normal">Vectorization is the process of transforming an algorithm so that instead of working on a single value at a time it would work on a set of values (vector) at the same time. Modern processors provide this at a hardware level through <strong class="keyWord">SIMD</strong> (<strong class="keyWord">Single Instruction, Multiple Data</strong>) units.</p>
</div>
<p class="normal">Apart<a id="_idIndexMarker1061"/> from the existing <a id="_idIndexMarker1062"/>algorithms that have been overloaded, seven new algorithms have been added:</p>
<table class="table-container" id="table003-4">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Algorithm</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Description</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::for_each_n</code></p>
</td>
<td class="table-cell">
<p class="normal">Applies a given function to the first <em class="italic">N</em> elements of the specified range, according to the specified execution policy.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::exclusive_scan</code></p>
</td>
<td class="table-cell">
<p class="normal">Computes the partial sum of a range of elements (using either <code class="inlineCode">std::plus&lt;&gt;</code> or a binary operation) but excludes the <em class="italic">i</em>th element from the <em class="italic">i</em>th sum. If the binary operation is associative, the result is the same as when using <code class="inlineCode">std::partial_sum()</code>.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::inclusive_scan</code></p>
</td>
<td class="table-cell">
<p class="normal">Computes the partial sum of a range of elements (using either <code class="inlineCode">std::plus&lt;&gt;</code> or a binary operation) but includes the <em class="italic">i</em>th element in the <em class="italic">i</em>th sum.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::transform_exclusive_scan</code></p>
</td>
<td class="table-cell">
<p class="normal">Applies a unary function to each element of a range and then calculates an exclusive scan on the resulting range.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::transform_inclusive_scan</code></p>
</td>
<td class="table-cell">
<p class="normal">Applies a unary function to each element of a range and then calculates an inclusive scan on the resulting range.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::reduce</code></p>
</td>
<td class="table-cell">
<p class="normal">An out-of-order version of <code class="inlineCode">std::accumulate()</code>.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">std::transform_reduce</code></p>
</td>
<td class="table-cell">
<p class="normal">Applies a function to the elements of a range then accumulates the elements of the resulting range out of order (that is, reduces).</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 8.2: New algorithms in C++17 from the &lt;algorithm&gt; and &lt;numeric&gt; headers</p>
<p class="normal">In the preceding examples, we used <code class="inlineCode">std::transform()</code> and <code class="inlineCode">std::reduce()</code> with an execution policy – in our case, <code class="inlineCode">std::execution::par</code>. The algorithm <code class="inlineCode">std::reduce()</code> is similar to <code class="inlineCode">std::accumulate()</code> but it processes the elements out of order. <code class="inlineCode">std::accumulate()</code> does not have an<a id="_idIndexMarker1063"/> overload for specifying an execution policy, so it can only execute sequentially.</p>
<div><p class="normal">It is important to note that, just because an algorithm supports parallelization, it doesn’t mean that it will run faster than the sequential version. Execution depends on the actual hardware, datasets, and algorithm particularities. In fact, some of these algorithms may never, or hardly ever, execute faster when parallelized than sequentially. For this reason, for instance, the Microsoft implementation of several algorithms that permute, copy, or move elements does not perform parallelization but falls back to sequential execution in all cases. These algorithms are <code class="inlineCode">copy()</code>, <code class="inlineCode">copy_n()</code>, <code class="inlineCode">fill()</code>, <code class="inlineCode">fill_n()</code>, <code class="inlineCode">move()</code>, <code class="inlineCode">reverse()</code>, <code class="inlineCode">reverse_copy()</code>, <code class="inlineCode">rotate()</code>, <code class="inlineCode">rotate_copy()</code>, and <code class="inlineCode">swap_ranges()</code>. Moreover, the standard does not guarantee a particular execution; specifying a policy is actually a request for an execution strategy but with no guarantees implied.</p>
</div>
<p class="normal">On the <a id="_idIndexMarker1064"/>other hand, the standard library allows parallel algorithms to allocate memory. When this cannot be done, an algorithm throws <code class="inlineCode">std::bad_alloc</code>. However, again, the Microsoft implementation differs and instead of throwing an exception, it falls back to the sequential version of the algorithm.</p>
<p class="normal">Another important aspect that must be known is that the standard algorithms work with different kinds of iterators. Some require forward iterators, some input iterators. However, all the overloads that allow specifying an execution policy restrict the use of the algorithm with forward iterators.</p>
<p class="normal">Take a look at the following table:</p>
<figure class="mediaobject"><img alt="" src="img/B21549_08_04.png"/></figure>
<p class="packt_figref">Figure 8.4: A comparison of execution times for sequential and parallel implementations of the map and reduce functions</p>
<p class="normal">Here, you can <a id="_idIndexMarker1065"/>see a comparison of execution times for sequential and parallel implementations of the <code class="inlineCode">map</code> and <code class="inlineCode">reduce</code> functions. Highlighted are the versions of the functions implemented in this recipe. These times may vary slightly from execution to execution. These values were obtained by running a 64-bit released version compiled with Visual C++ 2019 16.4.x on a machine with an Intel Xeon CPU with four cores. Although the<a id="_idIndexMarker1066"/> parallel versions perform better than the sequential version for these datasets, which one is actually better varies with the size of the dataset. This is why profiling is key when you optimize by parallelizing work.</p>
<h2 class="heading-2" id="_idParaDest-550">There’s more...</h2>
<p class="normal">In this example, we have seen separate implementations for <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> (which is also called reduce). However, in C++17, there is a standard algorithm called <code class="inlineCode">std::transform_reduce()</code>, which composes the two operations into a single function call. This algorithm has overloads for sequential execution, as well as policy-based execution for parallelism and vectorization. We can, therefore, utilize this algorithm instead of the handwritten implementation we did in these previous three recipes.</p>
<p class="normal">The following are the sequential and parallel versions of the algorithm used to compute the sum of the doubles of all the elements of a range:</p>
<pre class="programlisting code"><code class="hljs-code">std::vector&lt;int&gt; v(size);
std::iota(std::begin(v), std::end(v), 1);
// sequential
auto sums = std::transform_reduce(
    std::begin(v), std::end(v), 
    0LL,
    std::plus&lt;&gt;(),
    [](int const i) {return i + i; } );
// parallel
auto sump = std::transform_reduce(
    std::execution::par,
    std::begin(v), std::end(v),
    0LL,
    std::plus&lt;&gt;(),
    [](int const i) {return i + i; });  
</code></pre>
<p class="normal">If we compare<a id="_idIndexMarker1067"/> the execution time of these two calls, seen in the following table in the last two columns, with the total time for separately calling <code class="inlineCode">map</code> and <code class="inlineCode">reduce</code>, as seen in the other implementations, you can see that <code class="inlineCode">std::transform_reduce()</code>, especially the parallel <a id="_idIndexMarker1068"/>version, executes better in most cases:</p>
<figure class="mediaobject"><img alt="" src="img/B21549_08_05.png"/></figure>
<p class="packt_figref">Figure 8.5: A comparison of execution times for the transform/reduce pattern with a highlight of the times for the std::transform_reduce() standard algorithm from C++17</p>
<h2 class="heading-2" id="_idParaDest-551">See also</h2>
<ul>
<li class="bulletList"><em class="chapterRef">Chapter 3</em>, <em class="italic">Implementing higher-order functions map and fold</em>, to learn about higher-order functions in functional programming and see how to implement the widely used <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> (or reduce) functions</li>
<li class="bulletList"><em class="italic">Implementing parallel map and fold with threads</em>, to see how to implement the <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> functions from functional programming using raw threads</li>
<li class="bulletList"><em class="italic">Implementing parallel map and fold with tasks</em>, to see how to implement the <code class="inlineCode">map</code> and <code class="inlineCode">fold</code> functions from functional programming using asynchronous functions</li>
</ul>
<h1 class="heading-1" id="_idParaDest-552">Using joinable threads and cancellation mechanisms</h1>
<p class="normal">The C++11 class <code class="inlineCode">std::thread</code> represents a single thread of execution and allows multiple functions to execute concurrently. However, it has a major inconvenience: you must explicitly<a id="_idIndexMarker1069"/> invoke the <code class="inlineCode">join()</code> method to wait for the thread to finish execution. This can lead to problems because if a <code class="inlineCode">std::thread</code> object is destroyed while it is still joinable, then <code class="inlineCode">std::terminate()</code> is called. C++20 provides an improved thread <a id="_idIndexMarker1070"/>class called <code class="inlineCode">std::jthread</code> (from <em class="italic">joinable thread</em>) that automatically calls <code class="inlineCode">join()</code> if the thread is still joinable when the object is destroyed. Moreover, this type supports cancellation through <code class="inlineCode">std::stop_source</code>/<code class="inlineCode">std::stop_token</code> and its destructor also requests the thread to stop before joining. In this recipe, you will learn how to use these new C++20 types.</p>
<h2 class="heading-2" id="_idParaDest-553">Getting ready</h2>
<p class="normal">Before you continue with this, you should read the first recipe of this chapter, <em class="italic">Working with threads</em>, to make sure you are familiar with <code class="inlineCode">std::thread</code>. To use <code class="inlineCode">std::jthread</code>, you need to include the same <code class="inlineCode">&lt;thread&gt;</code> header. For <code class="inlineCode">std::stop_source</code> and <code class="inlineCode">std::stop_token</code>, you need to include the header <code class="inlineCode">&lt;stop_token&gt;</code>.</p>
<h2 class="heading-2" id="_idParaDest-554">How to do it...</h2>
<p class="normal">The typical scenarios for using joinable threads and a cooperative cancellation mechanism are as follows:</p>
<ul>
<li class="bulletList">If you want to automatically join a thread object when it goes out of scope, use <code class="inlineCode">std::jthread</code> instead of <code class="inlineCode">std::thread</code>. You can still use all the methods that <code class="inlineCode">std::thread</code> has, such as explicitly joining with <code class="inlineCode">join()</code>:
        <pre class="programlisting code"><code class="hljs-code">void thread_func(int i)
{
    while(i-- &gt; 0)
    {
        std::cout &lt;&lt; i &lt;&lt; '\n';
    }
}
int main()
{
    std::jthread t(thread_func, 10);
}
</code></pre>
</li>
<li class="bulletList">If you <a id="_idIndexMarker1071"/>need to be able to cancel the execution of a thread, you<a id="_idIndexMarker1072"/> should do the following:<ul>
<li class="bulletList">Make sure the first parameter of the thread function is a <code class="inlineCode">std::stop_token</code> object.</li>
<li class="bulletList">In the thread function, periodically check if stopping was requested using the <code class="inlineCode">stop_requested()</code> method of the <code class="inlineCode">std::stop_token</code> object and stop when signaled.</li>
<li class="bulletList">Use <code class="inlineCode">std::jthread</code> for executing the function on a separate thread.</li>
<li class="bulletList">From the calling thread, use the <code class="inlineCode">request_stop()</code> method of the <code class="inlineCode">std::jthread</code> object to request the thread function to stop and return:
            <pre class="programlisting code"><code class="hljs-code">void thread_func(std::stop_token st, int&amp; i)
{
    while(!st.stop_requested() &amp;&amp; i &lt; 100)
    {
        using namespace std::chrono_literals;
        std::this_thread::sleep_for(200ms);
        i++;
    }
}
int main()
{
    int a = 0;
    
    std::jthread t(thread_func, std::ref(a));
    
    using namespace std::chrono_literals;
    std::this_thread::sleep_for(1s);
    
    t.request_stop();
    
    std::cout &lt;&lt; a &lt;&lt; '\n';       // prints 4
}
</code></pre>
</li>
</ul>
</li>
<li class="bulletList">If you <a id="_idIndexMarker1073"/>need to cancel the work of multiple threads, then<a id="_idIndexMarker1074"/> you can do the following:<ul>
<li class="bulletList">All thread functions must take a <code class="inlineCode">std::stop_token</code> object as the first argument.</li>
<li class="bulletList">All thread functions should periodically check if a stop was requested by calling the <code class="inlineCode">stop_requested()</code> method of <code class="inlineCode">std::stop_token</code> and, if a stop was requested, abort the execution.</li>
<li class="bulletList">Use <code class="inlineCode">std::jthread</code> to execute functions on different threads.</li>
<li class="bulletList">In the calling thread, create a <code class="inlineCode">std::stop_source</code> object.</li>
<li class="bulletList">Get a <code class="inlineCode">std::stop_token</code> object by calling the <code class="inlineCode">get_token()</code> method of the <code class="inlineCode">std::stop_source</code> object and pass it as the first argument for the thread function when creating <code class="inlineCode">std::jthread</code> objects.</li>
<li class="bulletList">When you want to stop the execution of the thread functions, call the <code class="inlineCode">request_stop()</code> method of the <code class="inlineCode">std::stop_source</code> object.
            <pre class="programlisting code"><code class="hljs-code">void thread_func(std::stop_token st, int&amp; i)
{
    while(!st.stop_requested() &amp;&amp; i &lt; 100)
    {
        using namespace std::chrono_literals;
        std::this_thread::sleep_for(200ms);
        i++;
    }
}
int main()
{
    int a = 0;
    int b = 10;
    
    std::stop_source st;
    
    std::jthread t1(thread_func, st.get_token(),
                    std::ref(a));
    std::jthread t2(thread_func, st.get_token(),
                    std::ref(b));
        
    using namespace std::chrono_literals;
    std::this_thread::sleep_for(1s);
    
    st.request_stop();
    
    std::cout &lt;&lt; a &lt;&lt; ' ' &lt;&lt; b &lt;&lt; '\n';       // prints 4
// and 14
}
</code></pre>
</li>
</ul>
</li>
<li class="bulletList">If you <a id="_idIndexMarker1075"/>need to execute a piece of code when a stop source is <a id="_idIndexMarker1076"/>requesting cancellation, you can use a <code class="inlineCode">std::stop_callback</code> created with the <code class="inlineCode">std::stop_token</code> object, which signals the stop request and a callback function that is invoked when the stop is requested (through the <code class="inlineCode">std::stop_source</code> object associated with <code class="inlineCode">std::stop_token</code>):
        <pre class="programlisting code"><code class="hljs-code">void thread_func(std::stop_token st, int&amp; i)
{
    while(!st.stop_requested() &amp;&amp; i &lt; 100)
    {
        using namespace std::chrono_literals;
        std::this_thread::sleep_for(200ms);
        i++;
    }
}
int main()
{
    int a = 0;
    
    std::stop_source src;
    std::stop_token token = src.get_token();
    std::stop_callback cb(token, []{std::cout &lt;&lt; "the end\n";});
    
    std::jthread t(thread_func, token, std::ref(a));
        
    using namespace std::chrono_literals;
    std::this_thread::sleep_for(1s);
    
    src.request_stop();
    
    std::cout &lt;&lt; a &lt;&lt; '\n';       // prints "the end" and 4
}
</code></pre>
</li>
</ul>
<h2 class="heading-2" id="_idParaDest-555">How it works...</h2>
<p class="normal"><code class="inlineCode">std::jthread</code> is very <a id="_idIndexMarker1077"/>similar to <code class="inlineCode">std::thread</code>. It is, in fact, an attempt to fix<a id="_idIndexMarker1078"/> what was missing for threads in C++11. Its public interface is very similar to <code class="inlineCode">std::thread</code>. All<a id="_idIndexMarker1079"/> the methods <code class="inlineCode">std::thread</code> has are also present in <code class="inlineCode">std::thread</code>. However, it differs in the following key aspects:</p>
<ul>
<li class="bulletList">Internally, it maintains, at least logically, a shared stop state, which allows for the request of the thread function to stop execution.</li>
<li class="bulletList">It has several methods for handling cooperative cancellation: <code class="inlineCode">get_stop_source()</code>, which returns a <code class="inlineCode">std::stop_source</code> object associated with the shared stop state of the thread, <code class="inlineCode">get_stop_token()</code>, which returns a <code class="inlineCode">std::stop_token</code> associated with the shared stop state of the thread, and <code class="inlineCode">request_stop()</code>, which requests the cancellation of the execution of the thread function via the shared stop state.</li>
<li class="bulletList">The behavior of its destructor, which, when the thread is joinable, calls <code class="inlineCode">request_stop()</code> and then <code class="inlineCode">join()</code> to first signal the request to stop execution and then wait until the thread has finished its execution.</li>
</ul>
<p class="normal">You can create <code class="inlineCode">std::jthread</code> objects just as you would create <code class="inlineCode">std::thread</code> objects. However, the callable function that you pass to a <code class="inlineCode">std::jthread</code> can have a first argument of the type <code class="inlineCode">std::stop_token</code>. This is necessary when you want to be able to cooperatively cancel the thread’s execution. </p>
<p class="normal">Typical scenarios include graphical user interfaces where user interaction may cancel work in progress, but many other situations can be envisioned. The invocation of such a function thread happens as follows:</p>
<ul>
<li class="bulletList">If the first argument for the thread function, supplied when constructing <code class="inlineCode">std::jthread</code>, is a <code class="inlineCode">std::stop_token</code>, it is forwarded to the callable function.</li>
<li class="bulletList">If the first argument, when there are arguments, for the callable function is not a <code class="inlineCode">std::stop_token</code> object, then the <code class="inlineCode">std::stop_token</code> object associated with the <code class="inlineCode">std::jthread</code> object’s internal shared stop state is passed to the function. This token is obtained with a call to <code class="inlineCode">get_stop_token()</code>.</li>
</ul>
<p class="normal">The function thread <a id="_idIndexMarker1080"/>must periodically check the status of the <code class="inlineCode">std::stop_token</code> object. The <code class="inlineCode">stop_requested()</code> method checks if a stop was requested. The request to stop comes from a <code class="inlineCode">std::stop_source</code> object.</p>
<p class="normal">If multiple stop <a id="_idIndexMarker1081"/>tokens are associated with the same stop source, a stop request is visible to all the stop tokens. If a stop is requested, it cannot be withdrawn, and successive stop requests have no meaning. To request a stop, you should call the <code class="inlineCode">request_stop()</code> method. You can check if a <code class="inlineCode">std::stop_source</code> is associated with a stop state and can be requested to stop by calling the <code class="inlineCode">stop_possible()</code> method.</p>
<p class="normal">If you need to invoke a callback function when a stop source is requested to stop, then you can use the <code class="inlineCode">std::stop_callback</code> class. This associates a <code class="inlineCode">std::stop_token</code> object with a callback function. When the stop source of the stop token is requested to stop the callback is invoked. Callback functions are invoked as follows:</p>
<ul>
<li class="bulletList">In the same thread that invoked <code class="inlineCode">request_stop()</code>.</li>
<li class="bulletList">In the thread constructing the <code class="inlineCode">std::stop_callback</code> object, if the stop has already been requested before the stop callback object has been constructed.</li>
</ul>
<p class="normal">You can create any number of <code class="inlineCode">std::stop_callback</code> objects for the same stop token. However, the order the callbacks are invoked in is unspecified. The only guarantee is that they will be executed synchronously, provided that the stop has been requested after the <code class="inlineCode">std::stop_callback</code> objects have been created.</p>
<p class="normal">It is also important to note that, if any callback function returns via an exception, then <code class="inlineCode">std::terminate()</code> will be invoked.</p>
<h2 class="heading-2" id="_idParaDest-556">See also</h2>
<ul>
<li class="bulletList"><em class="italic">Working with threads</em>, to learn about the <code class="inlineCode">std::thread</code> class and the basic operations for working with threads in C++</li>
<li class="bulletList"><em class="italic">Sending notifications between threads</em>, to see how to use condition variables to send notifications between producer and consumer threads</li>
</ul>
<h1 class="heading-1" id="_idParaDest-557">Synchronizing threads with latches, barriers, and semaphores </h1>
<p class="normal">The<a id="_idIndexMarker1082"/> thread support library from C++11 includes mutexes and condition variables that enable thread-synchronization to shared resources. A mutex allows only one thread of multiple processes to execute, while other threads that want to access a shared resource are put to sleep. Mutexes can be expensive to use in some scenarios. For this reason, the C++20 standard features several new, simpler synchronization mechanisms: latches, barriers, and semaphores. Although these do not provide new use cases, they are simpler to use and can be more performant because they may internally rely on lock-free mechanisms.</p>
<h2 class="heading-2" id="_idParaDest-558">Getting ready</h2>
<p class="normal">The new<a id="_idIndexMarker1083"/> C++20 synchronization mechanisms are defined in new headers. You have to include <code class="inlineCode">&lt;latch&gt;</code> for <code class="inlineCode">std::latch</code>, <code class="inlineCode">&lt;barrier&gt;</code>, or <code class="inlineCode">std::barrier</code>, and <code class="inlineCode">&lt;semaphore&gt;</code> for <code class="inlineCode">std::counting_semaphore</code> and <code class="inlineCode">std::binary_semaphore</code>.</p>
<p class="normal">The code snippets in this recipe will use the following two functions:</p>
<pre class="programlisting code"><code class="hljs-code">void process(std::vector&lt;int&gt; const&amp; data) noexcept
{
   for (auto const e : data)
      std::cout &lt;&lt; e &lt;&lt; ' ';  
   std::cout &lt;&lt; '\n';
}
int create(int const i, int const factor) noexcept
{
   return i * factor;
}
</code></pre>
<h2 class="heading-2" id="_idParaDest-559">How to do it...</h2>
<p class="normal">Use the C++20 synchronization mechanisms<a id="_idIndexMarker1084"/> as follows:</p>
<ul>
<li class="bulletList">Use <code class="inlineCode">std::latch</code> when you need threads to wait until a counter, decreased by other<a id="_idIndexMarker1085"/> threads, reaches zero. The latch must be initialized with a non-zero count and multiple threads can decrease it, while others wait for the count to reach zero. When that happens, all waiting threads are awakened and the latch can no longer be used. If the latch count does not decrease to zero (not enough threads decrease it) the waiting threads will be blocked forever. In the following example, four threads are creating data (stored in a vector of integers) and the main thread waits for the completion of them all by utilizing a <code class="inlineCode">std::latch</code>, decremented by each thread after completing its work:
        <pre class="programlisting code"><code class="hljs-code">int const jobs = 4;
std::latch work_done(jobs);
std::vector&lt;int&gt; data(jobs);
std::vector&lt;std::jthread&gt; threads;
for(int i = 1; i &lt;= jobs; ++i)
{   
   threads.push_back(std::jthread([&amp;data, i, &amp;work_done]{
      using namespace std::chrono_literals;
      std::this_thread::sleep_for(1s); // simulate work
      data[i-1] = create(i, 1);        // create data
      
      work_done.count_down();          // decrement counter
   }));   
}
work_done.wait();             // wait for all jobs to finish
process(data);                // process data from all jobs
</code></pre>
</li>
<li class="bulletList">Use <code class="inlineCode">std::barrier</code> when you need to perform loop synchronization between parallel tasks. You construct a barrier with a count and, optionally, a completion function. Threads arrive at the barrier, decrease the internal counter, and block. When the counter reaches zero, the completion function is invoked, all blocked threads are awakened, and a new cycle begins. In the following example, four threads are creating data that they store in a vector of integers. When all the threads have completed a cycle, the data is processed in the main thread, by a completion function. Each thread blocks after completing a cycle until they are awakened through the use of a <code class="inlineCode">std::barrier</code> object, which also stores the completion function. This <a id="_idIndexMarker1086"/>process is repeated 10 times:
        <pre class="programlisting code"><code class="hljs-code">int const jobs = 4;
std::vector&lt;int&gt; data(jobs);
int cycle = 1;
std::stop_source st;
// completion function
auto on_completion = [&amp;data, &amp;cycle, &amp;st]() noexcept {
   process(data);          // process data from all jobs
   cycle++;
   if (cycle == 10)        // stop after ten cycles
      st.request_stop();
   };
std::barrier work_done(jobs, on_completion);
std::vector&lt;std::jthread&gt; threads;
for (int i = 1; i &lt;= jobs; ++i)
{
   threads.push_back(std::jthread(
      [&amp;data, &amp;cycle, &amp;work_done](std::stop_token st, 
                                  int const i)
      {
         while (!st.stop_requested())
         {
            using namespace std::chrono_literals;
            // simulate work
            std::this_thread::sleep_for(200ms); 
            data[i-1] = create(i, cycle); // create data 
            work_done.arrive_and_wait();  // decrement counter
         }
      },
      st.get_token(),
      i));
}
for (auto&amp; t : threads) t.join();
</code></pre>
</li>
<li class="bulletList">Use <code class="inlineCode">std::counting_semaphore&lt;N&gt;</code> or <code class="inlineCode">std::binary_semaphore</code> when you want to restrict a number of <em class="italic">N</em> threads (a single thread, in the case of <code class="inlineCode">binary_semaphore</code>) to access a shared resource, or when you want to pass notifications between different threads. In the following example, four threads <a id="_idIndexMarker1087"/>are creating data that is added to the end of a vector of integers. To avoid race conditions, a <code class="inlineCode">binary_semaphore</code> object is used to restrict the access to the vector to a single thread:
        <pre class="programlisting code"><code class="hljs-code">int const jobs = 4;
std::vector&lt;int&gt; data;
std::binary_semaphore sem(1);
std::vector&lt;std::jthread&gt; threads;
for (int i = 1; i &lt;= jobs; ++i)
{
   threads.push_back(std::jthread([&amp;data, i, &amp;sem] {
      for (int k = 1; k &lt; 5; ++k)
      {
         // simulate work
using namespace std::chrono_literals;
         std::this_thread::sleep_for(200ms);
         // create data
int value = create(i, k);
         // acquire the semaphore
         sem.acquire();
         // write to the shared resource
         data.push_back(value);
         // release the semaphore
         sem.release();
      }
   }));
}
for (auto&amp; t : threads) t.join();
process(data); // process data from all jobs
</code></pre>
</li>
</ul>
<h2 class="heading-2" id="_idParaDest-560">How it works...</h2>
<p class="normal">The <code class="inlineCode">std::latch</code> class implements a counter that can be used to synchronize threads. It is a race-free class <a id="_idIndexMarker1088"/>that works as follows:</p>
<ul>
<li class="bulletList">The counter is initialized when the latch is created and can only be decreased.</li>
<li class="bulletList">A thread may decrease the value of the latch and can do so multiple times.</li>
<li class="bulletList">A thread may block by waiting until the latch counter reaches zero.</li>
<li class="bulletList">When the counter reaches zero, the latch becomes permanently signaled and all the threads that are blocked on the latch are awakened.</li>
</ul>
<p class="normal">The <code class="inlineCode">std::latch</code> class has the<a id="_idIndexMarker1089"/> following methods:</p>
<table class="table-container" id="table004-3">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Methods</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Descriptions</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">count_down()</code></p>
</td>
<td class="table-cell">
<p class="normal">Decrements the internal counter by <em class="italic">N</em> (which is <code class="inlineCode">1</code> by default) without blocking the caller. This operation is performed atomically. <em class="italic">N</em> must be a positive value no greater than the value of the internal counter; otherwise, the behavior is undefined.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">try_wait()</code></p>
</td>
<td class="table-cell">
<p class="normal">Indicates whether the internal counter reaches zero, in which case it returns <code class="inlineCode">true</code>. There is a very low probability that, although the counter has reached zero, the function may still return <code class="inlineCode">false</code>.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">wait()</code></p>
</td>
<td class="table-cell">
<p class="normal">Blocks the calling thread until the internal counter reaches zero. If the internal counter is already zero, the function returns immediately without blocking.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">arrive_and_wait()</code></p>
</td>
<td class="table-cell">
<p class="normal">This function is equivalent to calling <code class="inlineCode">count_down()</code>, followed by <code class="inlineCode">wait()</code>. It decrements the internal counter with <em class="italic">N</em> (which is <code class="inlineCode">1</code> by default) and then blocks the calling thread until the internal counter reaches zero.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 8.3: std::memory_order members that describe how memory access is ordered for an atomic operation</p>
<p class="normal">In the first example in the previous section, we have a <code class="inlineCode">std::latch</code>, called <code class="inlineCode">work_done</code>, initialized with the number of threads (or jobs) that perform work. Each thread produces data that is then written in a shared resource, a vector of integers. Although this is shared, there is no race condition because each thread writes to a different place; therefore, there is no need for a synchronization mechanism. After completing its work, each thread decrements the counter of the latch. The main thread waits until the counter of the latch reaches zero, after which it processes the data from the threads.</p>
<p class="normal">Because <a id="_idIndexMarker1090"/>the internal counter of <code class="inlineCode">std::latch</code> cannot be incremented or reset, this synchronization mechanism can be used only once. A similar but reusable synchronization mechanism is <code class="inlineCode">std::barrier</code>. A barrier <a id="_idIndexMarker1091"/>allows threads to block until an operation is completed and is useful for managing repeated tasks performed by multiple threads.</p>
<p class="normal">A barrier works as follows:</p>
<ul>
<li class="bulletList">A barrier contains a counter that is initialized during its creation and can be decreased by threads arriving at the barrier. When the counter reaches zero, it is reset to its initial value and the barrier can be reused.</li>
<li class="bulletList">A barrier also contains a completion function that is called when the counter reaches zero. If a default completion function is used, it is invoked as part of the call to <code class="inlineCode">arrive_and_wait()</code> or <code class="inlineCode">arrive_and_drop()</code>. Otherwise, the completion function is invoked on one of the threads that participate in the completion phase.</li>
<li class="bulletList">The process through<a id="_idIndexMarker1092"/> which a barrier goes from start to reset is called the <strong class="keyWord">completion phase</strong>. This <a id="_idIndexMarker1093"/>starts with a <a id="_idIndexMarker1094"/>so-called <strong class="keyWord">synchronization point</strong> and ends with the <strong class="keyWord">completion step</strong>.</li>
<li class="bulletList">The first <em class="italic">N</em> threads that arrive at the synchronization point after the construction of the barrier are said to be<a id="_idIndexMarker1095"/> the <strong class="keyWord">set of participating threads</strong>. Only these threads are allowed to arrive at the barrier during each of the following cycles.</li>
<li class="bulletList">A thread that arrives at the synchronization point may decide to participate in the completion phase by calling <code class="inlineCode">arrive_and_wait()</code>. However, a thread may remove itself from the participation set by calling <code class="inlineCode">arrive_and_drop()</code>. In this case, another thread must take its place in the participation set.</li>
<li class="bulletList">When all the threads in the participation set have arrived at the synchronization point, the completion phase is executed. There are three steps that occur: first, the completion function is invoked. Second, all the threads that are blocked are awakened. Third, and last, the barrier count is reset and a new cycle begins.</li>
</ul>
<p class="normal">The <code class="inlineCode">std::barrier</code> class<a id="_idIndexMarker1096"/> has the following methods:</p>
<table class="table-container" id="table005-2">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Methods</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Descriptions</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">arrive()</code></p>
</td>
<td class="table-cell">
<p class="normal">Arrives at the barrier’s synchronization point and decrements the expected count by a value n. The behavior is undefined if the value of n is greater than the expected count, or equal to or less than zero. The function executes atomically.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">wait()</code></p>
</td>
<td class="table-cell">
<p class="normal">Blocks at the synchronization point until the completion step is executed.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">arrive_and_wait()</code></p>
</td>
<td class="table-cell">
<p class="normal">Arrives at the barrier’s synchronization point and blocks. The calling thread must be in the participating set; otherwise, the behavior is undefined. This function only returns after the completion phase ends.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">arrive_and_drop()</code></p>
</td>
<td class="table-cell">
<p class="normal">Arrives at the barrier’s synchronization point and removes the thread from the participation set. It is an implementation detail whether the function blocks or not until the end of the completion phase. The calling thread must be in the participation set; otherwise, the behavior is undefined.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 8.4: The member functions of the std::barrier class</p>
<pre>How to do it...</em> section. In this example, a <code class="inlineCode">std::barrier</code> is created and initialized with a counter, which represents the number of threads, and a completion function. This function processes the data produced by all the threads, then increments a loop counter, and requests threads to stop after 10 loops. This basically means that the barrier will perform 10 cycles before the threads will finish their work. Each thread loops until a stop is requested, and, in each iteration, they produce some data, written to the shared vector of integers. At the end of the loop, each thread arrives at the barrier synchronization point, decrements the counter, and waits for it to reach zero and the completion function to execute. This is done with a call to the <code class="inlineCode">arrive_and_wait()</code> method of the <code class="inlineCode">std::barrier</code> class.</pre>
<p class="normal">The last synchronization mechanism available in the thread support library in C++20 is represented by semaphores. A semaphore contains an internal counter that can be both decreased and increased by multiple threads. When the counter reaches zero, further attempts to decrease it will block the thread, until another thread increases the counter.</p>
<p class="normal">There are two<a id="_idIndexMarker1098"/> semaphore classes: <code class="inlineCode">std::counting_semaphore&lt;N&gt;</code> and <code class="inlineCode">std::binary_semaphore</code>. The latter is actually just an alias for <code class="inlineCode">std::counting_semaphore&lt;1&gt;</code>.</p>
<p class="normal">A <code class="inlineCode">counting_semaphore</code> allows <a id="_idIndexMarker1099"/><em class="italic">N</em> threads to access a shared resource, unlike a mutex, which only allows one. <code class="inlineCode">binary_semaphore</code>, is, in this matter, similar to the mutex, because only one thread can access the shared resource. On the other hand, a mutex is bound to a thread: the thread that locked the mutex must unlock it. However, this is not the case for semaphores. A semaphore can be released by threads that did not acquire it, and a <a id="_idIndexMarker1100"/>thread that acquired a semaphore does not have to also release it.</p>
<p class="normal">The <code class="inlineCode">std::counting_semaphore</code> class<a id="_idIndexMarker1101"/> has the following methods:</p>
<table class="table-container" id="table006-2">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Methods</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Descriptions</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">acquire()</code></p>
</td>
<td class="table-cell">
<p class="normal">Decrements the internal counter by 1 if it is greater than 0. Otherwise, it blocks until the counter becomes greater than 0.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">try_acquire()</code></p>
</td>
<td class="table-cell">
<p class="normal">Tries to decrement the counter by 1 if it is greater than 0. It returns <code class="inlineCode">true</code> if it succeeds, or <code class="inlineCode">false</code> otherwise. This method does not block.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">try_acquire_for()</code></p>
</td>
<td class="table-cell">
<p class="normal">Tries to decrease the counter by 1 if it is greater than 0. Otherwise, it blocks either until the counter becomes greater than 0 or a specified timeout occurs. The function returns <code class="inlineCode">true</code> if it succeeds in decreasing the counter.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">try_acquire_until()</code></p>
</td>
<td class="table-cell">
<p class="normal">Tries to decrease the counter by 1 if it is greater than 0. Otherwise, it blocks either until the counter becomes greater than 0 or a specified time point has been passed. The function returns <code class="inlineCode">true</code> if it succeeds in decreasing the counter.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">release()</code></p>
</td>
<td class="table-cell">
<p class="normal">Increments the internal counter by the specified value (which is 1 by default). Any thread that was blocked waiting for the counter to become greater than 0 is awakened.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 8.5: The member functions of the std::counting_semaphore class</p>
<p class="normal">All the increment and decrement operations performed on the counter by the methods listed here are executed atomically.</p>
<p class="normal">The last example in the <em class="italic">How to do it...</em> section shows how a <code class="inlineCode">binary_semaphore</code> can be used. A number of threads (four, in this example) produce work in a loop and write to a shared resource. Unlike the previous examples, they simply add to the end of a vector of integers. Therefore, the access to this vector must be synchronized between the threads, and this is where the binary semaphore is used. In each loop, the thread function creates a new value (which may take some time). This value is then appended to the end of the vector. However, the thread must call the <code class="inlineCode">acquire()</code> method of the semaphore <a id="_idIndexMarker1102"/>to make sure it is the only thread that can continue execution and access the shared resource. After the write operation completes, the thread calls the <code class="inlineCode">release()</code> method of the semaphore in order to increment the internal counter and allow another thread to access the shared resource.</p>
<p class="normal">Semaphores can <a id="_idIndexMarker1103"/>be used for multiple purposes: to block access to shared resources (similar to mutexes), to signal or pass notifications between threads (similar to condition variables), or to implement barriers, often with better performance than similar mechanisms.</p>
<h2 class="heading-2" id="_idParaDest-561">See also</h2>
<ul>
<li class="bulletList"><em class="italic">Working with threads</em>, to learn about the <code class="inlineCode">std::thread</code> class and the basic operations for working with threads in C++</li>
<li class="bulletList"><em class="italic">Synchronizing access to shared data with mutexes and locks</em>, to see what mechanisms are available for synchronizing thread access to shared data and how they work</li>
<li class="bulletList"><em class="italic">Sending notifications between threads</em>, to see how to use condition variables to send notifications between producer and consumer threads</li>
</ul>
<h1 class="heading-1" id="_idParaDest-562">Synchronizing writing to output streams from multiple threads</h1>
<p class="normal"><code class="inlineCode">std::cout</code> is a<a id="_idIndexMarker1104"/> global object of the <code class="inlineCode">std::ostream</code> type. It is used to write text to the standard output console. Although writing to it is guaranteed to be thread-safe, this applies to just one invocation of the <code class="inlineCode">operator&lt;&lt;</code>. Multiple such sequenced calls to <code class="inlineCode">operator&lt;&lt;</code> can be interrupted and resumed later, making it necessary to employ synchronization mechanisms to avoid corrupted results. This applies to all scenarios where multiple threads operate on the same output stream. To simplify this scenario, C++20 introduced <code class="inlineCode">std::basic_osyncstream</code> to provide a mechanism to synchronize threads writing to the same output stream. In this recipe, you will learn how to use this new utility.</p>
<h2 class="heading-2" id="_idParaDest-563">How to do it…</h2>
<p class="normal">To<a id="_idIndexMarker1105"/> synchronize access to an output stream for writing from multiple threads, do the following:</p>
<ul>
<li class="bulletList">Include the <code class="inlineCode">&lt;syncstream&gt;</code> header.</li>
<li class="bulletList">Define a variable of the <code class="inlineCode">std::osyncstream</code> type to wrap the shared output stream, such as <code class="inlineCode">std::cout</code>.</li>
<li class="bulletList">Use the wrapper variable exclusively to write to the output stream.</li>
</ul>
<p class="normal">The following snippet shows an example for this pattern:</p>
<pre class="programlisting code"><code class="hljs-code">std::vector&lt;std::jthread&gt; threads;
for (int i = 1; i &lt;= 10; ++i)
{
   threads.push_back(
      std::jthread([](const int id)
         {
            std::osyncstream scout{ std::cout };
            scout &lt;&lt; "thread " &lt;&lt; id &lt;&lt; " running\n";
         }, i));
}
</code></pre>
<h2 class="heading-2" id="_idParaDest-564">How it works…</h2>
<p class="normal">By default, the standard C++ stream objects <code class="inlineCode">std::cin</code>/<code class="inlineCode">std::wcin</code>, <code class="inlineCode">std::cout</code>/<code class="inlineCode">std::wcout</code>, <code class="inlineCode">std::cerr</code>/<code class="inlineCode">std::wcerr</code>, and <code class="inlineCode">std::clog</code>/<code class="inlineCode">std::wclog</code> are synchronized with their respective C streams, <code class="inlineCode">stdin</code>, <code class="inlineCode">stdout</code>, and <code class="inlineCode">stderr</code> (unless a call to <code class="inlineCode">std::ios_base::sync_with_stdio()</code> disables this synchronization). What this means is that any operation applied to a C++ stream object is immediately applied to the corresponding C stream. Moreover, accessing these streams is guaranteed to be thread-safe. This means that calls to <code class="inlineCode">operator &lt;&lt;</code> or <code class="inlineCode">&gt;&gt;</code> are atomic; another thread cannot access the stream until the call completes. However, multiple calls can be interrupted, as shown in the following example:</p>
<pre class="programlisting code"><code class="hljs-code">std::vector&lt;std::jthread&gt; threads;
for (int i = 1; i &lt;= 10; ++i)
{
   threads.push_back(
      std::jthread([](const int id)
         {
            std::cout &lt;&lt; "thread " &lt;&lt; id &lt;&lt; " running\n";
         }, i));
}
</code></pre>
<p class="normal">The output<a id="_idIndexMarker1106"/> differs on different executions, but it looks like the following:</p>
<pre class="programlisting con"><code class="hljs-con">thread thread thread 6 running
thread 2 running
1 running
thread 3 running
5 running
thread 4thread 7 running
thread 10 running
thread 9 running
 running
thread 8 running
</code></pre>
<p class="normal">There are three different invocations to <code class="inlineCode">operator &lt;&lt;</code> in the thread function. Although each executes atomically, the thread can be suspended in between calls for another thread to get a chance to execute. This is why we see the output having the shape shown earlier. </p>
<p class="normal">This can be solved in several ways. One can use a synchronization mechanism, such as a mutex. However, in this particular case, a simpler solution is to use a local <code class="inlineCode">std::stringstream</code> object to build the text to be displayed on the console and make a single invocation to <code class="inlineCode">operator&lt;&lt;</code>, as shown next:</p>
<pre class="programlisting code"><code class="hljs-code">std::vector&lt;std::jthread&gt; threads;
for (int i = 1; i &lt;= 10; ++i)
{
   threads.push_back(
      std::jthread([](const int id)
         {
            std::stringstream ss;
            ss &lt;&lt; "thread " &lt;&lt; id &lt;&lt; " running\n";
            std::cout &lt;&lt; ss.str();
         }, i));
}
</code></pre>
<p class="normal">With these changes, the output has the form that was expected:</p>
<pre class="programlisting con"><code class="hljs-con">thread 1 running
thread 2 running
thread 3 running
thread 4 running
thread 5 running
thread 6 running
thread 7 running
thread 8 running
thread 9 running
thread 10 running
</code></pre>
<p class="normal">In C++20, you can use a <code class="inlineCode">std::osyncstream</code>/<code class="inlineCode">std::wosyncstream</code> object to wrap an output stream to synchronize access, as shown in the <em class="italic">How to do it…</em> section. The <code class="inlineCode">osyncstream</code> class guarantees there are no data races if all the write operations from different threads occur through instances of this class. The <code class="inlineCode">std::basic_osyncstream</code> class wraps an instance of <code class="inlineCode">std::basic_syncbuf</code>, which, in turn, wraps an output buffer but also <a id="_idIndexMarker1107"/>contains a separate internal buffer. This class accumulates output in an internal buffer and transmits it to the wrapped buffer when the object is destructed or when an explicit call to the <code class="inlineCode">emit()</code> member function occurs.</p>
<p class="normal">The sync stream wrappers can be used to synchronize access to any output stream, not just <code class="inlineCode">std::ostream</code>/<code class="inlineCode">std::wostream</code> (the type of <code class="inlineCode">std::cout</code>/<code class="inlineCode">std::wcout</code>). For instance, it can be used to synchronize access to a string stream, as shown in the following snippet:</p>
<pre class="programlisting code"><code class="hljs-code">int main()
{
   std::ostringstream str{ };
   {
      std::osyncstream syncstr{ str };
      syncstr &lt;&lt; "sync stream demo";
      std::cout &lt;&lt; "A:" &lt;&lt; str.str() &lt;&lt; '\n'; // [1]
   }
   std::cout &lt;&lt; "B:" &lt;&lt; str.str() &lt;&lt; '\n';    // [2]
}
</code></pre>
<p class="normal">In this example, we define a <code class="inlineCode">std::ostringstream</code> object called <code class="inlineCode">str</code>. In the inner block, this is wrapped by a <code class="inlineCode">std::osyncstream</code> object and then we write the text <code class="inlineCode">"sync stream demo"</code> through this wrapper to the string stream. On the line marked with <strong class="keyWord">[1]</strong>, we print the content of the string stream to the console. However, the content of the stream’s buffer is empty because the sync stream has not been destroyed, nor has a call to <code class="inlineCode">emit()</code> occurred. When the sync stream goes out of scope, the content of its inner buffer is <a id="_idIndexMarker1108"/>transferred to the wrapped stream. Therefore, on the line marked with <strong class="keyWord">[2]</strong>, the <code class="inlineCode">str</code> string stream contains the text <code class="inlineCode">"sync stream demo"</code>. This results in the following output for the program:</p>
<pre class="programlisting con"><code class="hljs-con">A:
B:sync stream demo
</code></pre>
<p class="normal">We can elaborate on this example to show how the <code class="inlineCode">emit()</code> member function affects the behavior of the streams. Let’s consider the following snippet:</p>
<pre class="programlisting code"><code class="hljs-code">int main()
{
   std::ostringstream str{ };
   {
      std::osyncstream syncstr{ str };
      syncstr &lt;&lt; "sync stream demo";
      std::cout &lt;&lt; "A:" &lt;&lt; str.str() &lt;&lt; '\n'; // [1]
      syncstr.emit();
      std::cout &lt;&lt; "B:" &lt;&lt; str.str() &lt;&lt; '\n'; // [2]
      syncstr &lt;&lt; "demo part 2";
      std::cout &lt;&lt; "C:" &lt;&lt; str.str() &lt;&lt; '\n'; // [3]
   }
   std::cout &lt;&lt; "D:" &lt;&lt; str.str() &lt;&lt; '\n';    // [4]
}
</code></pre>
<p class="normal">The first part of this second example is the same. On line <strong class="keyWord">[1]</strong>, the content of the string buffer is empty. However, after the call to <code class="inlineCode">emit()</code>, the sync stream transfers the content of its inner buffer to the wrapped output stream. Therefore, on line <strong class="keyWord">[2]</strong>, the string buffer contains the text <code class="inlineCode">"sync stream demo"</code>. New text, <code class="inlineCode">"demo part 2"</code>, is written to the string stream through the sync stream, but this is not transferred to the string stream before the line marked with <strong class="keyWord">[3]</strong> executes; therefore, at this point the content of the string stream is unchanged. Upon going out of scope at the end of the inner block, the new content of the sync stream’s inner buffer is again transferred to the wrapped string stream, which will now contain the text <code class="inlineCode">"sync stream demodemo part 2"</code>. As a result, the output of this second example is the following:</p>
<pre class="programlisting con"><code class="hljs-con">A:
B:sync stream demo
C:sync stream demo
D:sync stream demodemo part 2
</code></pre>
<p class="normal">The <code class="inlineCode">std::basic_syncstream</code> class has a member function called <code class="inlineCode">get_wrapped()</code>, which returns a <a id="_idIndexMarker1109"/>pointer to the wrapped stream buffer. This can be used to construct a new instance of the <code class="inlineCode">std::basic_syncstream</code> class so that you can sequence content to the same output stream through different instances of <code class="inlineCode">std::basic_osyncstream</code>. The next snippet demonstrates how this works:</p>
<pre class="programlisting code"><code class="hljs-code">int main()
{
   std::ostringstream str{ };
   {
      std::osyncstream syncstr{ str };
      syncstr &lt;&lt; "sync stream demo";
      std::cout &lt;&lt; "A:" &lt;&lt; str.str() &lt;&lt; '\n';    // [1]
      {
         std::osyncstream syncstr2{ syncstr.get_wrapped() };
         syncstr2 &lt;&lt; "demo part 3";
         std::cout &lt;&lt; "B:" &lt;&lt; str.str() &lt;&lt; '\n'; // [2]
      }
      std::cout &lt;&lt; "C:" &lt;&lt; str.str() &lt;&lt; '\n';    // [3]
   }
   std::cout &lt;&lt; "D:" &lt;&lt; str.str() &lt;&lt; '\n';       // [4]
}
</code></pre>
<p class="normal">Again, the first part of the example is unchanged. However, here we have a second inner block where a second instance of <code class="inlineCode">std::osyncstream</code> is constructed with a pointer to the stream buffer returned by the call to <code class="inlineCode">syncstr</code>'s <code class="inlineCode">get_wrapped()</code> member function. At the line marked with <strong class="keyWord">[2]</strong>, none of the two instances of <code class="inlineCode">std::osyncstream</code> has been destroyed; therefore, the content of the <code class="inlineCode">str</code> string stream is still empty. The first sync stream to be destroyed is <code class="inlineCode">syncstr2</code>, at the end of the second inner block. Therefore, on the line marked with <strong class="keyWord">[3]</strong>, the content of the string stream will be <code class="inlineCode">"demo part 3"</code>. Then, the first sync stream object, <code class="inlineCode">syncstr</code>, goes out of scope at the end of the first inner block, adding the text <code class="inlineCode">"sync stream demo"</code> to the string stream. The output of running this program is the following:</p>
<pre class="programlisting con"><code class="hljs-con">A:
B:
C:demo part 3
D:demo part 3sync stream demo
</code></pre>
<p class="normal">Although in <a id="_idIndexMarker1110"/>all these examples we defined named variables, you can write to an output stream using a temporary sync stream too, as shown below:</p>
<pre class="programlisting code"><code class="hljs-code">threads.push_back(
   std::jthread([](const int id)
      {
         std::osyncstream{ std::cout } &lt;&lt; "thread " &lt;&lt; id 
                                       &lt;&lt; " running\n";
      }, i));
</code></pre>
<h2 class="heading-2" id="_idParaDest-565">See also</h2>
<ul>
<li class="bulletList"><em class="italic">Working with threads</em>, to learn about the <code class="inlineCode">std::thread</code> class and the basic operations for working with threads in C++</li>
<li class="bulletList"><em class="italic">Using joinable threads and cancellation mechanisms</em>, to learn about the C++20 <code class="inlineCode">std::jthread</code> class, which manages a thread of execution and automatically joins during its destruction, as well as the improved mechanisms for stopping the execution of threads</li>
</ul>
<h1 class="heading-1">Learn more on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
<p class="normal"><a href="Chapter_08.xhtml">https://discord.gg/7xRaTCeEhx</a></p>
<p class="normal"><img alt="" src="img/QR_Code2659294082093549796.png"/></p>
</div>
</body></html>