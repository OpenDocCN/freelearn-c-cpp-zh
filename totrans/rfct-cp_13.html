<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-261"><a id="_idTextAnchor260"/>13</h1>
<h1 id="_idParaDest-262"><a id="_idTextAnchor261"/>Modern Approach to Managing Third Parties</h1>
<p>In modern software development, the reliance on third-party libraries is virtually inescapable. From foundational components such as OpenSSL for secure communication, and Boost for extensive C++ libraries, to even the standard library that forms the bedrock of C++ programming, external libraries are integral to building functional and efficient applications. This dependency underscores the importance of understanding how third-party libraries are managed within the C++ ecosystem.</p>
<p>Given the complexity and diversity of these libraries, it’s crucial for developers to grasp the basics of third-party library management. This knowledge not only aids in the seamless integration of these libraries into projects but also influences deployment strategies. The compilation method of a library, whether static or dynamic, directly impacts the number of files deployed and the overall footprint of the application.</p>
<p>Unlike some other programming languages that benefit from a standardized library ecosystem, C++ presents a unique challenge due to the absence of such a unified system. This chapter delves into the existing solutions for third-party library management in C++, exploring tools such as vcpkg, Conan, and others. By examining these tools, we aim to provide insights into which solution might best fit your project’s needs, considering factors such as platform compatibility, ease of use, and the scope of the library catalog.</p>
<p>As we navigate through these solutions, our goal is to equip you with the knowledge to make informed decisions about integrating and managing third-party libraries in your C++ projects, thereby enhancing your development workflow and the quality of your software.</p>
<h1 id="_idParaDest-263"><a id="_idTextAnchor262"/>Overview of linking and shared V threads::ThreadsS static libraries</h1>
<p>In the context of C and C++ development, third-party entities are external libraries or frameworks that developers integrate into their projects. These entities serve to improve functionality or utilize existing solutions. These third-party components can vary significantly in scope, from minimal utility libraries to comprehensive frameworks offering a broad range of features.</p>
<p>The <a id="_idIndexMarker755"/>process of integrating third-party libraries into a project involves using header files that outline the interfaces of these libraries. These header files contain the declarations of classes, functions, and variables provided by the library, allowing the compiler to understand the required signatures and structures for successful compilation. Including a header file in a C++ source file essentially concatenates the contents of the header file to the point of inclusion, enabling access to the library’s interfaces without embedding the actual implementation within the source file.</p>
<p>The implementation of these libraries is supplied through compiled object code, typically distributed as either static libraries or shared libraries. Static libraries are archives of object files that are directly incorporated into the final executable by the linker, resulting in a larger executable size due to the embedding of the library code. On the other hand, shared libraries, known<a id="_idIndexMarker756"/> as <strong class="bold">Dynamic Link Libraries (DLLs)</strong> on Windows or <strong class="bold">Shared Objects (SOs)</strong> on Unix-like systems, are not embedded into the <a id="_idIndexMarker757"/>executable. Instead, references to these libraries are included, and the operating system loads them into memory at runtime. This mechanism allows multiple applications to utilize the same library code, conserving memory.</p>
<p>Shared libraries <a id="_idIndexMarker758"/>were designed to facilitate the sharing of common libraries, such as libc or C++ standard libraries, among multiple applications. This practice is especially advantageous for frequently utilized libraries. This design also theoretically allows users to update shared libraries without needing to upgrade the entire application. However, in practice, this is not always seamless and can introduce compatibility issues, making it less advantageous for applications to provide their dependencies as shared libraries. Furthermore, opting for shared libraries over static ones can reduce linker time, as the linker does not need to embed the library code into the executable, which can speed up the build process.</p>
<p>The linker plays a pivotal role in this process, merging various object files and libraries into a single executable or library, and resolving symbol references along the way to ensure the final binary is complete and executable.</p>
<p>The choice between static and dynamic linking significantly affects application performance, size, and deployment strategies. Static linking simplifies deployment by creating self-contained executables but at the cost of larger file sizes and the necessity to recompile for library updates. Dynamic linking, while reducing memory usage by sharing library code among <a id="_idIndexMarker759"/>applications and facilitating easier library updates, introduces complexities in deployment to ensure all dependencies are met.</p>
<p>Given the complexities associated with linking external shared objects and the widespread use of templated code in C++, many library developers have started to prefer supplying their libraries as “header-only” libraries. A <a id="_idIndexMarker760"/>header-only library is a library that is entirely contained within header files, with no separate implementation files or precompiled binaries. This means that all the code, including function and class definitions, is included in the header files.</p>
<p>This approach simplifies the integration process significantly. When a developer includes a header file from a header-only library, they are not just including interface declarations but the entire implementation. Consequently, there is no need for separate compilation or linking of the library’s implementation; the compiler includes and compiles the library’s code directly into the developer’s source code when the header file is included. This direct inclusion can lead to more efficient inlining and optimizations by the compiler, potentially resulting in faster executable code due to the elimination of function call overheads.</p>
<p>However, it’s worth noting that while header-only libraries offer convenience and ease of integration, they also have some downsides. Since the entire library is included and compiled with each source file that includes it, this can lead to increased compilation times, especially for large libraries or projects that include the library in multiple files. Furthermore, any change in the header file necessitates recompiling all source files that include it, which can further increase development time.</p>
<p>Despite its drawbacks, the header-only approach in C++ is highly attractive to many developers and users due to its simplicity of distribution and use. Additionally, it helps avoid linking issues and offers benefits for template-heavy libraries. This model is especially prevalent in libraries where heavy use of templates is made, such as those providing metaprogramming facilities, because templates must be available in their entirety to the compiler at compile time, making the header-only model a natural fit.</p>
<p>In essence, the management of third-party dependencies in C++ projects involves a deep understanding of header files, static and shared libraries, and the intricacies of the linking process. Developers must carefully consider the trade-offs between static and dynamic linking in the context of application requirements and deployment environments, balancing factors such as performance, size, and ease of maintenance.</p>
<h1 id="_idParaDest-264"><a id="_idTextAnchor263"/>Managing third-party libraries in C++</h1>
<p>Managing <a id="_idIndexMarker761"/>third-party libraries is a critical aspect of C++ development. While there is no standardized package manager for C++, various methods and tools have been adopted to streamline this process, each with its own set of practices and supported platforms.</p>
<h2 id="_idParaDest-265"><a id="_idTextAnchor264"/>Installing libraries with OS package managers</h2>
<p>Many <a id="_idIndexMarker762"/>developers rely on the operating system’s package manager to install third-party libraries. On Ubuntu and other Debian-based systems, <code>apt</code> is commonly used:</p>
<pre class="source-code">
sudo apt install libboost-all-dev</pre>
<p>For Red Hat-based systems, <code>yum</code> or its successor <code>dnf</code> is the go-to option:</p>
<pre class="source-code">
sudo yum install boost-devel</pre>
<p>On macOS, Homebrew is a popular choice for managing packages:</p>
<pre class="source-code">
brew install boost</pre>
<p>Windows users often turn to Chocolatey or <code>vcpkg</code> (the latter also functions as a general C++ library manager beyond just Windows):</p>
<pre class="source-code">
choco install boost</pre>
<p>These OS package managers are convenient for common libraries but might not always offer the latest version or specific configurations needed for development.</p>
<h2 id="_idParaDest-266"><a id="_idTextAnchor265"/>Using Git as a third-party manager via submodules</h2>
<p>Git submodules<a id="_idIndexMarker763"/> allow developers to include and manage the source code of third-party libraries directly within their repositories. This method is advantageous for ensuring all team members and the build system use an exact version of a library. A typical workflow for adding a submodule and integrating it with CMake might look like this:</p>
<pre class="source-code">
git submodule add https://github.com/google/googletest.git external/googletest
git submodule update --init</pre>
<p>In <code>CMakeLists.txt</code>, you’d include the submodule:</p>
<pre class="source-code">
add_subdirectory(external/googletest)
include_directories(${gtest_SOURCE_DIR}/include ${gtest_SOURCE_DIR})</pre>
<p>This <a id="_idIndexMarker764"/>method tightly couples your project with specific versions of the library and facilitates tracking updates through Git.</p>
<h2 id="_idParaDest-267"><a id="_idTextAnchor266"/>Using CMake FetchContent to download libraries</h2>
<p>CMake’s <code>FetchContent</code> module<a id="_idIndexMarker765"/> provides a more<a id="_idIndexMarker766"/> flexible alternative to submodules by downloading dependencies at configure time, without the need to include them directly in your repository:</p>
<pre class="source-code">
include(FetchContent)
FetchContent_Declare(
  json
  GIT_REPOSITORY https://github.com/nlohmann/json.git
  GIT_TAG v3.7.3
)
FetchContent_MakeAvailable(json)</pre>
<p>This approach differs from Git submodules by not requiring the library’s source code to be present in your repository or updating it manually. <code>FetchContent</code> dynamically retrieves the specified version, making it easier to manage and update dependencies.</p>
<h1 id="_idParaDest-268"><a id="_idTextAnchor267"/>Conan – advanced dependency management</h1>
<p>Conan is a<a id="_idIndexMarker767"/> powerful package manager for C and C++ that simplifies the process of integrating third-party libraries and managing dependencies across various platforms and configurations. It<a id="_idIndexMarker768"/> stands out for its ability to handle multiple versions of libraries, complex dependency graphs, and different build configurations, making it an essential tool for modern C++ development.</p>
<h2 id="_idParaDest-269"><a id="_idTextAnchor268"/>Conan configuration and features</h2>
<p>Conan’s <a id="_idIndexMarker769"/>configuration is stored in <code>conanfile.txt</code> or <code>conanfile.py</code>, where developers specify the required libraries, versions, settings, and options. This file serves as the manifest for project dependencies, enabling precise control over the libraries <a id="_idIndexMarker770"/>used in a project.</p>
<p><strong class="bold">Key features</strong>:</p>
<ul>
<li><strong class="bold">Multi-platform support</strong>: Conan is designed to work on Windows, Linux, macOS, and FreeBSD, offering a consistent experience across different operating systems</li>
<li><strong class="bold">Build configuration management</strong>: Developers can specify settings such as compiler version, architecture, and build type (debug, release) to ensure compatibility and optimal builds for their projects</li>
<li><strong class="bold">Version handling</strong>: Conan can manage multiple versions of the same library, allowing projects to depend on specific versions as needed</li>
<li><strong class="bold">Dependency resolution</strong>: It automatically resolves and downloads transitive dependencies, ensuring that all required libraries are available for the build process</li>
</ul>
<h2 id="_idParaDest-270"><a id="_idTextAnchor269"/>Library locations and Conan Center</h2>
<p>The <a id="_idIndexMarker771"/>primary<a id="_idIndexMarker772"/> repository for Conan packages is <strong class="bold">Conan Center</strong>, an extensive collection of open source C and C++ libraries. Conan Center is the go-to place to find and download packages, but developers can also specify custom or private repositories for their projects.</p>
<p>In addition to<a id="_idIndexMarker773"/> Conan Center, companies and development teams can host their own Conan servers or use services such as Artifactory to manage private or proprietary packages, enabling a centralized approach to dependency management within an organization.</p>
<h2 id="_idParaDest-271"><a id="_idTextAnchor270"/>Configuring static or dynamic linking</h2>
<p>Conan <a id="_idIndexMarker774"/>allows developers to specify whether to use static or dynamic linking for libraries. This is typically done through options in <code>conanfile.txt</code> or <code>conanfile.py</code>. Here’s an example:</p>
<pre class="source-code">
[options]
Poco:shared=True  # Use dynamic linking for Poco
Or in <code>conanfile.py</code>:
class MyProject(ConanFile):
    requires = “poco/1.10.1”
    default_options = {“poco:shared”: True}</pre>
<p>These settings instruct Conan to download and use the dynamic version of the specified libraries. Similarly, setting the option to <code>False</code> would favor static libraries. It’s essential to note that not all packages will support both linking options, depending on how they were packaged for Conan.</p>
<h2 id="_idParaDest-272"><a id="_idTextAnchor271"/>Extending Conan with custom packages</h2>
<p>One of the <a id="_idIndexMarker775"/>strengths of Conan is its extensible nature. If a required library is not available in Conan Center or does not meet specific needs, developers can create and contribute their own packages. Conan provides a Python-based development kit for creating packages, which includes tools for defining build processes, dependencies, and package metadata.</p>
<p>To create a Conan package, developers define <code>conanfile.py</code> that describes how to source, build, and package the library. This file includes methods such as <code>source()</code>, <code>build()</code>, and <code>package()</code> that Conan executes during the package creation process.</p>
<p>Once a package is developed, it can be shared through Conan Center by submitting it for inclusion, or it can be distributed through private repositories to maintain control over distribution and usage.</p>
<p>Conan’s flexibility, support <a id="_idIndexMarker776"/>for multiple platforms and configurations, and its comprehensive package repository make it an invaluable tool for C++ developers. By leveraging Conan, teams can streamline their dependency management process, ensuring consistent, reproducible builds across different environments. The ability to configure static or dynamic linking, coupled with the option to extend the repository with custom packages, underscores Conan’s adaptability to diverse project requirements. Whether working with widely-used open source libraries or specialized proprietary code, Conan provides a robust framework for managing C++ dependencies efficiently and effectively.</p>
<p>Conan is a dedicated C++ package manager that excels in managing different versions of libraries and their dependencies. It operates independently of the operating system’s package manager and provides a high level of control and flexibility. A typical Conan workflow involves creating <code>conanfile.txt</code> or <code>conanfile.py</code> to declare dependencies.</p>
<h1 id="_idParaDest-273"><a id="_idTextAnchor272"/>CMake integration</h1>
<p>CMake is <a id="_idIndexMarker777"/>widely used in C++ projects for its powerful scripting capabilities and cross-platform support. Integrating Conan with CMake can significantly streamline the process of managing dependencies. Here’s how<a id="_idIndexMarker778"/> you can achieve this integration:</p>
<ul>
<li><code>conanbuildinfo.cmake</code> file generated by Conan in your project’s <code>CMakeLists.txt</code>:<pre class="source-code">
include(${CMAKE_BINARY_DIR}/conanbuildinfo.cmake)</pre><pre class="source-code">
conan_basic_setup(TARGETS)</pre><p class="list-inset">This script sets up the necessary <code>include</code> paths and library paths, and defines the dependencies managed by Conan, making them available to your CMake project.</p></li>
<li><code>TARGETS</code> option in <code>conan_basic_setup()</code> generates CMake targets for your Conan dependencies, allowing you to link against them using the <code>target_link_libraries()</code> function in CMake:<pre class="source-code">
target_link_libraries(my_project_target CONAN_PKG::poco)</pre><p class="list-inset">This approach<a id="_idIndexMarker780"/> provides a clean and explicit way to link your project’s targets against the libraries managed by Conan.</p></li>
</ul>
<h2 id="_idParaDest-274"><a id="_idTextAnchor273"/>Other build system integration</h2>
<p>Conan’s flexibility <a id="_idIndexMarker781"/>extends to other build systems as well, making it adaptable to various project requirements:</p>
<ul>
<li><code>include</code> paths, library paths, and flags that can be included in a Makefile:<pre class="source-code">
<code>include conanbuildinfo.mak</code></pre></li>
<li><code>.props</code> files that can be imported into Visual Studio projects, providing a seamless integration with the MSBuild ecosystem.</li>
<li><strong class="bold">Bazel, Meson, and others</strong>: While <a id="_idIndexMarker784"/>direct support for some build systems such as Bazel or <a id="_idIndexMarker785"/>Meson might require custom integration scripts or tools, the Conan community often contributes generators and tools to bridge these gaps, extending Conan’s reach to virtually any build system.</li>
</ul>
<h2 id="_idParaDest-275"><a id="_idTextAnchor274"/>Custom integration</h2>
<p>For <a id="_idIndexMarker786"/>build systems without direct support or for projects with unique requirements, Conan offers the ability to customize the generated files or even write custom generators. This allows developers to tailor the integration to their specific build<a id="_idIndexMarker787"/> process, making Conan a highly adaptable tool for dependency management.</p>
<h2 id="_idParaDest-276"><a id="_idTextAnchor275"/>Conclusion</h2>
<p>The integration of Conan with CMake and other build systems underscores its versatility as a package manager for C++ projects. By providing straightforward mechanisms to incorporate dependencies into various build environments, Conan not only simplifies dependency management but also enhances build reproducibility and consistency across different platforms and configurations. Whether you’re working with a widely used build system such as CMake or a more specialized setup, Conan’s flexible integration options ensure that you can maintain an efficient and streamlined development workflow.</p>
<h1 id="_idParaDest-277"><a id="_idTextAnchor276"/>vcpkg</h1>
<p>vcpkg, developed<a id="_idIndexMarker788"/> by Microsoft, is a cross-platform C++ package manager that simplifies the process of acquiring and building C++ open source libraries. It is designed to work seamlessly with CMake and other build systems, providing a straightforward and consistent way to manage C++ library dependencies.</p>
<h2 id="_idParaDest-278"><a id="_idTextAnchor277"/>Key differences from Conan</h2>
<p>While both <a id="_idIndexMarker789"/>vcpkg and Conan are aimed at simplifying dependency management in C++ projects, there are notable differences in their approach and ecosystem:</p>
<ul>
<li><strong class="bold">Origin and backing</strong>: vcpkg was created and is maintained by Microsoft, which ensures tight integration with Visual Studio and the MSBuild system, although it remains fully functional and useful across different platforms and development environments.</li>
<li><strong class="bold">Package sources</strong>: vcpkg focuses on compiling from source, ensuring that libraries are built with the same compiler and settings as the consuming project. This approach contrasts with Conan, which can manage precompiled binaries, allowing for quicker integration but potentially leading to binary incompatibility issues.</li>
<li><strong class="bold">Integration</strong>: vcpkg <a id="_idIndexMarker790"/>integrates natively with CMake and Visual Studio, providing manifest files for project-level integration. This can make it particularly attractive for projects already using these tools, offering a more seamless integration experience.</li>
<li><strong class="bold">Ecosystem and libraries</strong>: Both package managers boast a large collection of available libraries, but their ecosystems might differ slightly due to the community and backing of each project.</li>
</ul>
<h2 id="_idParaDest-279"><a id="_idTextAnchor278"/>Operating system support</h2>
<p>vcpkg is designed<a id="_idIndexMarker791"/> to be cross-platform, with support for the following:</p>
<ul>
<li>Windows</li>
<li>Linux</li>
<li>macOS</li>
</ul>
<p>This wide range of support makes it a versatile option for developers working in diverse development environments.</p>
<h2 id="_idParaDest-280"><a id="_idTextAnchor279"/>Example of configuring a project with vcpkg</h2>
<p>To<a id="_idIndexMarker792"/> illustrate the use of vcpkg in a project, let’s go through a simple example of integrating a library, such as the JSON for Modern C++ library (<code>nlohmann-json</code>), into a C++ project using CMake.</p>
<p>Clone the vcpkg repository and run the bootstrap script:</p>
<pre class="source-code">
git clone https://github.com/Microsoft/vcpkg.git
cd vcpkg
./bootstrap-vcpkg.sh  # Use bootstrap-vcpkg.bat on Windows
Install <code>nlohmann-json</code> using vcpkg:
./vcpkg install nlohmann-json</pre>
<p><code>vcpkg</code> will download and compile the library, making it available for projects.</p>
<p>To use vcpkg<a id="_idIndexMarker793"/> with a CMake project, you can set the <code>CMAKE_TOOLCHAIN_FILE</code> variable to the path of the <code>vcpkg.cmake</code> toolchain file when configuring your project:</p>
<pre class="source-code">
cmake -B build -S . -DCMAKE_TOOLCHAIN_FILE=[vcpkg root]/scripts/buildsystems/vcpkg.cmake
Replace [vcpkg root] with the path to your <code>vcpkg</code> installation.
In your CMakeLists.txt, find and link against the <code>nlohmann-json</code> package:
cmake_minimum_required(VERSION 3.0)
project(MyVcpkgProject)
find_package(nlohmann_json CONFIG REQUIRED)
add_executable(my_app main.cpp)
target_link_libraries(my_app PRIVATE nlohmann_json::nlohmann_json)
In your main.cpp, you can now use the <code>nlohmann-json</code> library:
#include &lt;nlohmann/json.hpp&gt;
int main() {
    nlohmann::json j;
    j[“message”] = “Hello, world!”;
    std::cout &lt;&lt; j &lt;&lt; std::endl;
    return 0;
}</pre>
<p><code>vcpkg</code>, with its emphasis on source-based distribution and integration with CMake and Visual Studio, offers a robust solution for C++ developers looking to manage library dependencies effectively. Its<a id="_idIndexMarker794"/> simplicity, coupled with the backing of Microsoft, makes it a compelling choice for projects that prioritize consistency with the build environment and seamless integration with existing Microsoft tools. While it shares common goals with Conan in simplifying dependency management, the choice between vcpkg and Conan may come down to specific project requirements, preferred workflow, and the development ecosystem.</p>
<h2 id="_idParaDest-281"><a id="_idTextAnchor280"/>Utilizing Docker for C++ builds</h2>
<p>A <a id="_idIndexMarker795"/>notable shortfall within C++ is its absence of an inherent mechanism for managing dependencies. Consequently, the incorporation of third-party elements is achieved through a heterogeneous array of methodologies: the utilization of package managers provided by Linux distributions (for instance, <code>apt-get</code>), the direct installation via <code>make install</code>, the inclusion of third-party libraries as Git submodules and their subsequent compilation within the project’s source tree, or the adoption of package management solutions such as Conan or Vcpkgvcpkg.</p>
<p>Regrettably, each of these methods comes with its own set of drawbacks:</p>
<ul>
<li>The installation of dependencies directly on a development machine tends to compromise the cleanliness of the environment, rendering it dissimilar to those of CI/CD pipelines or production environments – a discrepancy that becomes more pronounced with each update of third-party components.</li>
<li>It is often a formidable task to ensure uniformity in the versions of compilers, debuggers, and other tools utilized by all developers. This lack of standardization can culminate in a scenario where a build executes successfully on an individual developer’s machine yet fails within the CI/CD environment.</li>
<li>The practice of integrating third-party libraries as Git submodules and compiling them within the project’s source directory poses a challenge, particularly when dealing with substantial libraries (such as Boost, Protobuf, Thrift, etc.). This method can lead to a significant deceleration of the build process, to the extent that developers may hesitate to clear the build directory or to alternate between branches.</li>
<li>Package <a id="_idIndexMarker796"/>management solutions such as Conan may not always offer the desired version of a specific dependency, and the inclusion of such a version necessitates the authoring of additional code in Python, which, in my opinion, is unduly burdensome.</li>
</ul>
<h3>A single isolated and reproducible build environment</h3>
<p>The <a id="_idIndexMarker797"/>optimal resolution for the aforementioned challenges involves the formulation of a Docker image, embedded with all requisite dependencies and tools, such as compilers and debuggers, to facilitate the project’s compilation within a container derived from this image.</p>
<p>This particular image serves as the cornerstone for a <strong class="bold">singular</strong> build environment that is uniformly employed by developers on their respective workstations as well as on CI/CD servers, effectively eliminating the all-too-common discrepancy of “it works on my machine but fails at CI!”.</p>
<p>Owing to the encapsulated nature of the build process within the container, it remains impervious to any external variables, tools, or configurations peculiar to an individual developer’s local setup, thereby rendering the build environment <strong class="bold">isolated</strong>.</p>
<p>In an ideal scenario, Docker images are meticulously labeled with meaningful version identifiers, enabling users to seamlessly transition between different environments by retrieving the appropriate image from the registry. Furthermore, in the event that an image is no longer available in the registry, it’s worth noting that Docker images are constructed from Dockerfiles, which are typically maintained within Git repositories. This ensures that, should the need arise, there is always the feasibility to reconstruct the image from a previous version of the Dockerfile. This attribute of the Dockerized build framework lends it a characteristic of being <strong class="bold">reproducible</strong>.</p>
<h3>Creating the build image</h3>
<p>We will embark on developing a straightforward application and compile it within a container. The<a id="_idIndexMarker798"/> essence of the application is to display its size utilizing <code>boost::filesystem</code>. The selection of Boost for this demonstration is intentional, aiming to illustrate the integration of Docker with a “heavy” third-party library:</p>
<pre class="source-code">
#include &lt;boost/filesystem/operations.hpp&gt;
#include &lt;iostream&gt;
int main(int argc, char *argv[]) {
    std::cout &lt;&lt; “The path to the binary is: “
              &lt;&lt; boost::filesystem::absolute(argv[0])
              &lt;&lt; “, the size is:” &lt;&lt; boost::filesystem::file_size(argv[0]) &lt;&lt; ‘\n’;
    return 0;
}</pre>
<p>The CMake file is quite simple:</p>
<pre class="source-code">
cmake_minimum_required(VERSION 3.10.2)
project(a.out)
set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)
# Remove for compiler-specific features
set(CMAKE_CXX_EXTENSIONS OFF)
string(APPEND CMAKE_CXX_FLAGS “ -Wall”)
string(APPEND CMAKE_CXX_FLAGS “ -Wbuiltin-macro-redefined”)
string(APPEND CMAKE_CXX_FLAGS “ -pedantic”)
string(APPEND CMAKE_CXX_FLAGS “ -Werror”)
# clangd completion
set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
include_directories(${CMAKE_SOURCE_DIR})
file(GLOB SOURCES “${CMAKE_SOURCE_DIR}/*.cpp”)
add_executable(${PROJECT_NAME} ${SOURCES})
set(Boost_USE_STATIC_LIBS        ON) # only find static libs
set(Boost_USE_MULTITHREADED      ON)
set(Boost_USE_STATIC_RUNTIME    OFF) # do not look for boost libraries linked against static C++ std lib
find_package(Boost REQUIRED COMPONENTS filesystem)
target_link_libraries(${PROJECT_NAME}
    Boost::filesystem
)</pre>
<p class="callout-heading">Note </p>
<p class="callout">In this example, Boost is linked statically since it is required if the target machine does not have the right version of Boost pre-installed; this recommendation applies to all dependencies pre-installed in the Docker image.</p>
<p>The <a id="_idIndexMarker799"/>Dockerfile employed for this task is notably uncomplicated:</p>
<pre class="source-code">
FROM ubuntu:18.04
LABEL Description=”Build environment”
ENV HOME /root
SHELL [“/bin/bash”, “-c”]
RUN apt-get update &amp;&amp; apt-get -y --no-install-recommends install \
    build-essential \
    clang \
    cmake \
    gdb \
    wget
# Let us add some heavy dependency
RUN cd ${HOME} &amp;&amp; \
    wget --no-check-certificate --quiet \
        https://boostorg.jfrog.io/artifactory/main/release/1.77.0/source/boost_1_77_0.tar.gz &amp;&amp; \
        tar xzf ./boost_1_77_0.tar.gz &amp;&amp; \
        cd ./boost_1_77_0 &amp;&amp; \
        ./bootstrap.sh &amp;&amp; \
        ./b2 install &amp;&amp; \
        cd .. &amp;&amp; \
<code>        rm -rf ./boost_1_77_0</code></pre>
<p>To<a id="_idIndexMarker800"/> ensure that its name is distinctive and does not overlap with existing Dockerfiles, while also clearly conveying its purpose, I have named it <code>DockerfileBuildEnv</code>:</p>
<pre class="source-code">
$ docker build -t example/example_build:0.1 -f DockerfileBuildEnv .
<code>Here is supposed to be a long output of boost build</code></pre>
<p>*Note that the version is not the “latest” but has a meaningful name (e.g., 0.1).</p>
<p>Once the image has been successfully constructed, we are positioned to proceed with the project’s build process. The initial step involves initiating a Docker container that is based on our crafted image, followed by the execution of the Bash shell within this container:</p>
<pre class="source-code">
$ cd project
$ docker run -it --rm --name=example \
 --mount type=bind,source=${PWD},target=/src \
 example/example_build:0.1 \
<code> bash</code></pre>
<p>The parameter of particular importance in this context is <code>--</code><code>mount type=bind,source=$</code><strong class="source-inline">
{PWD},target=/src</strong>. This directive instructs Docker to bind mount the current directory, which houses the source code, to the <code>/src</code> directory within the container. This approach circumvents the need to copy source files into the container. Moreover, as will be demonstrated subsequently, it enables the storage of output binaries directly on the host’s file system, thereby eliminating the need for redundant copying. For an understanding of the remaining flags and options, it is advisable to consult the official Docker documentation.</p>
<p>Within the <a id="_idIndexMarker801"/>container, we will proceed to compile the project:</p>
<pre class="source-code">
root@3abec58c9774:/# cd src
root@3abec58c9774:/src# mkdir build &amp;&amp; cd build
root@3abec58c9774:/src/build# cmake ..
-- The C compiler identification is GNU 7.5.0
-- The CXX compiler identification is GNU 7.5.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Boost  found.
-- Found Boost components:
   filesystem
-- Configuring done
-- Generating done
-- Build files have been written to: /src/build
root@3abec58c9774:/src/build# make
Scanning dependencies of target a.out
[ 50%] Building CXX object CMakeFiles/a.out.dir/main.cpp.o
[100%] Linking CXX executable a.out
<code>[100%] Built target a.out</code></pre>
<p>Et voilà, the<a id="_idIndexMarker802"/> project was built successfully!</p>
<p>The resulting binary runs successfully, both in the container and on the host, because Boost is linked <em class="italic">statically</em>:</p>
<pre class="source-code">
$ build/a.out
The size of “/home/dima/dockerized_cpp_build_example/build/a.out” is 177320</pre>
<h3>Making the environment usable</h3>
<p>At this<a id="_idIndexMarker803"/> juncture, it’s reasonable to feel overwhelmed by the multitude of Docker commands and wonder how one is expected to memorize them all. It’s important to emphasize that developers are not expected to retain every detail of these commands for project-building purposes. To streamline this process, I propose encapsulating the Docker commands within a tool that is widely familiar to most developers – <code>make</code>.</p>
<p>To facilitate this, I have established a GitHub repository (<a href="https://github.com/f-squirrel/dockerized_cpp">https://github.com/f-squirrel/dockerized_cpp</a>) that contains a versatile Makefile. This Makefile is designed to be easily adaptable and can typically be employed for nearly any project that utilizes CMake without necessitating modifications. Users have the option to either directly download it from this repository or integrate it into their project as a Git submodule, ensuring access to the most recent updates. I advocate for the latter approach and will provide further details on this.</p>
<p>The Makefile is configured to support fundamental commands. Users can display the available command options by executing <code>make help</code> in the terminal:</p>
<pre class="source-code">
$ make help
gen_cmake                      Generate cmake files, used internally
build                          Build source. In order to build a specific target run: make TARGET=&lt;target name&gt;.
test                           Run all tests
clean                          Clean build directory
login                          Login to the container. Note: if the container is already running, login into the existing one
<code>build-docker-deps-image        Build the deps image.</code></pre>
<p>To<a id="_idIndexMarker804"/> integrate the Makefile into our sample project, we’ll begin by adding it as a Git submodule within the <code>build_tools</code> directory:</p>
<pre class="source-code">
git submodule add  https://github.com/f-squirrel/dockerized_cpp.git build_tools/</pre>
<p>The next step is to create another Makefile in the root of the repository and include the Makefile that we have just checked out:</p>
<pre class="source-code">
include build_tools/Makefile</pre>
<p>Before the project compilation, it’s prudent to adjust certain default settings to better suit the specific needs of your project. This can be efficiently achieved by declaring variables in the top-level Makefile prior to the inclusion of <code>build_tools/Makefile</code>. Such preemptive declarations allow for the customization of various parameters, ensuring that the build environment and process are optimally configured for your project’s requirements:</p>
<pre class="source-code">
PROJECT_NAME=example
DOCKER_DEPS_VERSION=0.1
<code>include build_tools/Makefile</code>
By defining the project name, we automatically set the build image name as <code>example/example_build</code>.</pre>
<p>Make is <a id="_idIndexMarker805"/>now ready to build the image:</p>
<pre class="source-code">
$ make build-docker-deps-image
docker build  -t example/example_build:latest \
 -f ./DockerfileBuildEnv .
Sending build context to Docker daemon  1.049MB
Step 1/6 : FROM ubuntu:18.04
&lt; long output of docker build &gt;
Build finished. Docker image name: “example/example_build:latest”.
Before you push it to Docker Hub, please tag it(DOCKER_DEPS_VERSION + 1).
If you want the image to be the default, please update the following variables:
<code>/home/dima/dockerized_cpp_build_example/Makefile: DOCKER_DEPS_VERSION</code></pre>
<p>The Makefile, by default, assigns the latest tag to the Docker image. For better version control and to align with our project’s current stage, it is advisable to tag the image with a specific version. In this context, we shall tag the image as <code>0.1</code>.</p>
<p>Finally, let us build the project:</p>
<pre class="source-code">
$ make
docker run -it --init --rm --memory-swap=-1 --ulimit core=-1 --name=”example_build” --workdir=/example --mount type=bind,source=/home/dima/dockerized_cpp_build_example,target=/example  example/example_build:0.1 \
 bash -c \
 “mkdir -p /example/build &amp;&amp; \
 cd build &amp;&amp; \
 CC=clang CXX=clang++ \
 cmake  ..”
-- The C compiler identification is Clang 6.0.0
-- The CXX compiler identification is Clang 6.0.0
-- Check for working C compiler: /usr/bin/clang
-- Check for working C compiler: /usr/bin/clang -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/clang++
-- Check for working CXX compiler: /usr/bin/clang++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Boost  found.
-- Found Boost components:
   filesystem
-- Configuring done
-- Generating done
-- Build files have been written to: /example/build
CMake finished.
docker run -it --init --rm --memory-swap=-1 --ulimit core=-1 --name=”example_build” --workdir=/example --mount type=bind,source=/home/dima/dockerized_cpp_build_example,target=/example  example/example_build:latest \
 bash -c \
 “cd build &amp;&amp; \
 make -j $(nproc) “
Scanning dependencies of target a.out
[ 50%] Building CXX object CMakeFiles/a.out.dir/main.cpp.o
[100%] Linking CXX executable a.out
[100%] Built target a.out
<code>Build finished. The binaries are in /home/dima/dockerized_cpp_build_example/build</code></pre>
<p>Upon inspecting the build directory on the host, you’ll observe that the output binary has been seamlessly placed there, facilitating easy access and management.</p>
<p>Both the <a id="_idIndexMarker806"/>Makefile and an example of a project that utilizes it with its default values can be found on GitHub. This provides a practical demonstration of how the Makefile can be integrated into a project, offering a turnkey solution for developers seeking to implement a Dockerized build environment in their C++ projects:</p>
<ul>
<li>Makefile repository: <a href="https://github.com/f-squirrel/dockerized_cpp">https://github.com/f-squirrel/dockerized_cpp</a></li>
<li>Example project: <a href="https://github.com/f-squirrel/dockerized_cpp">https://github.com/f-squirrel/dockerized_cpp</a></li>
</ul>
<h3>Enhancements for user management within Dockerized builds</h3>
<p>The initial iteration of the Docker-based build system executed operations under the root user’s <a id="_idIndexMarker807"/>privileges. While this setup typically doesn’t pose immediate problems—developers have the option to modify file permissions using <code>chmod</code>—executing Docker containers as the root user is generally discouraged from a security standpoint. More critically, this approach can lead to complications if any of the build targets modify the source code, such as code formatting or applying <code>clang-tidy</code> corrections through <code>make</code> commands. Such modifications could result in source files being owned by the root user, thereby restricting the ability to edit these files directly from the host.</p>
<p>To address this concern, modifications have been made to the Dockerized build’s source code, enabling the container to execute as the host user by specifying the current user’s ID and group ID. This adjustment is now the standard configuration to enhance security and usability. Should there be a need to revert to running the container as the root user, the following command can be utilized:</p>
<pre class="source-code">
make DOCKER_USER_ROOT=ON</pre>
<p>It is important to recognize that the Docker image does not replicate the host user’s environment in its entirety—there is no corresponding home directory, nor are the user’s name and group replicated within the container. This implies that if the build process relies on accessing the home directory, this modified approach may not be suitable.</p>
<h1 id="_idParaDest-282"><a id="_idTextAnchor281"/>Summary</h1>
<p>In this chapter, we explored various strategies and tools for managing third-party dependencies in C++ projects, a critical aspect that significantly impacts the efficiency and reliability of the development process. We delved into traditional methods, such as utilizing operating system package managers and incorporating dependencies directly via Git submodules, each with its unique advantages and limitations.</p>
<p>We then transitioned to more specialized C++ package managers, highlighting Conan and vcpkg. Conan, with its robust ecosystem, extensive library support through Conan Center, and flexible configuration options, offers a comprehensive solution for managing complex dependencies, integrating seamlessly with multiple build systems, and supporting both static and dynamic linking. Its ability to handle multiple versions of libraries and the ease with which developers can extend the repository with custom packages make it an invaluable tool for modern C++ development.</p>
<p>vcpkg, developed by Microsoft, presents a slightly different approach, focusing on source-based distribution and ensuring libraries are built with the same compiler and settings as the consuming project. Its tight integration with CMake and Visual Studio, coupled with the backing of Microsoft, ensures a smooth experience, particularly for projects within the Microsoft ecosystem. The emphasis on compiling from source addresses potential binary incompatibility issues, making vcpkg a reliable choice for managing dependencies.</p>
<p>Lastly, we discussed the adoption of Dockerized builds as an advanced strategy for creating consistent, reproducible build environments, particularly beneficial in Linux systems. This approach, while more complex, offers significant advantages in terms of isolation, scalability, and consistency across development, testing, and deployment stages.</p>
<p>Throughout the chapter, we aimed to equip you with the knowledge and tools necessary to navigate the landscape of dependency management in C++ projects. By understanding the strengths and limitations of each method and tool, developers can make informed decisions tailored to their project’s specific needs, leading to more efficient and reliable software development processes.</p>
</div>
</body></html>