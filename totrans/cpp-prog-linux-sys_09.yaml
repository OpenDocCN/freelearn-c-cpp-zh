- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Understanding the C++ Memory Model
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 C++ 内存模型
- en: This chapter is a continuation of the discussion from [*Chapter 7*](B20833_07.xhtml#_idTextAnchor101),
    where we discussed a few multiprocess and multi-threaded techniques; this chapter
    will enhance their usage. We will guide you through various techniques while narrowing
    down to the main focus of the chapter – the C++ memory model. But in order to
    discuss this, you will start first with a brief examination of memory robustness
    through the smart pointer and the optional objects. We will use them later to
    implement *lazy initialization* and handle *shared memory* regions safely. An
    improved memory access analysis of *cache-friendly* code follows. You will learn
    when and why using multi-threaded execution could be a trap, even though you did
    everything right in the software design.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是延续第[*第7章*](B20833_07.xhtml#_idTextAnchor101)的讨论，在第7章中，我们讨论了一些多进程和多线程技术；本章将增强它们的用法。我们将引导您了解各种技术，同时缩小到本章的主要焦点——C++内存模型。但为了讨论这一点，您将首先通过智能指针和可选对象对内存健壮性进行简要检查。我们将在稍后使用它们来实现**延迟初始化**并安全地处理**共享内存**区域。接下来是针对**缓存友好**代码的改进内存访问分析。您将了解为什么即使在软件设计中做得一切正确，使用多线程执行也可能成为陷阱。
- en: This chapter gives you the opportunity to broaden your understanding of the
    synchronization primitives. While learning about the *condition variables*, you
    will also understand the benefits of the *read-write locks*. We will use the *ranges*
    from C++20 to visualize the same shared data differently. Combining these mechanisms
    one by one, we will finalize our analysis with the biggest topic – instruction
    ordering. Through the C++ *memory order*, you will learn more about the significance
    of the correct atomic routine setup. The *spinlock* implementation will be used
    to summarize all techniques at the end.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为您提供了扩展对同步原语理解的机会。在学习**条件变量**的同时，您还将了解**读写锁**的好处。我们将使用 C++20 的**范围**来以不同的方式可视化相同共享数据。逐一结合这些机制，我们将以最大的主题——指令排序来最终完成我们的分析。通过
    C++ 的**内存顺序**，您将了解正确原子例程设置的重要性。最后，我们将使用**自旋锁**实现来总结所有技术。
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Getting to know smart pointers and optionals in C++
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 C++ 中的智能指针和可选
- en: Learning about condition variables, read-write locks, and ranges in C++
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 C++ 中的条件变量、读写锁和范围
- en: Discussing multiprocessor systems – cache locality and cache friendliness in
    C++
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论多处理器系统——C++ 中的缓存局部性和缓存友好性
- en: Revisiting shared resources through the C++ memory model via the spinlock implementation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过自旋锁实现回顾 C++ 内存模型中的共享资源
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In order to run the code examples, the reader must prepare the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行代码示例，读者必须准备以下内容：
- en: A Linux-based system capable of compiling and executing C++20 (for example,
    **Linux** **Mint 21**)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够编译和执行 C++20 的基于 Linux 的系统（例如，**Linux** **Mint 21**）
- en: 'The GCC12.2 compiler: [https://gcc.gnu.org/git/gcc.git gcc-source](https://gcc.gnu.org/git/gcc.gitgcc-source)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCC12.2 编译器：[https://gcc.gnu.org/git/gcc.git gcc-source](https://gcc.gnu.org/git/gcc.gitgcc-source)
- en: With the `-std=c++2a`, `-lpthread`, and `-``lrt` flags
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `-std=c++2a`，`-lpthread` 和 `-``lrt` 标志
- en: For all the examples, you can alternatively use [https://godbolt.org/](https://godbolt.org/).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有示例，您还可以使用 [https://godbolt.org/](https://godbolt.org/)。
- en: All code examples in this chapter are available for download from [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章中所有代码示例均可从 [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209)
    下载。
- en: Getting to know smart pointers and optionals in C++
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 C++ 中的智能指针和可选
- en: In [*Chapter 4*](B20833_04.xhtml#_idTextAnchor060), we revisited the C++ fundamentals
    in order to be on the same page when it comes to the language. One instrument
    that is also considered a *must* is smart pointers. Through these, we are able
    to improve the safety of the program and also manage our resources more effectively.
    And as discussed in the earlier chapters, this is one of our main goals as system
    programmers. Remember the **RAII** principle? Smart pointers are based on this,
    helping the C++ developer reduce and even eliminate *memory leaks*. They could
    also help with shared memory management as you will see later in the chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B20833_04.xhtml#_idTextAnchor060)中，我们回顾了C++基础知识，以便在语言方面保持一致。另一个也被认为是*必需品*的工具是智能指针。通过这些工具，我们能够提高程序的安全性，并更有效地管理我们的资源。正如前面章节所讨论的，这是我们作为系统程序员的主要目标之一。还记得**RAII**原则吗？智能指针基于这一点，帮助C++开发者减少甚至消除*内存泄漏*。它们还可以帮助管理共享内存，正如你将在本章后面看到的那样。
- en: '*Memory leaks* appear when we allocate memory but fail to free it. This could
    happen not only because we forgot to call the object’s destructor, but also when
    we lose the pointer to that memory address. In addition to these, there are also
    the *wild* and *dangling pointers* to consider as well. The first one happens
    when the pointer is there on the *stack*, but it’s never associated with the real
    object (or address). The second one happens when we free the memory, used by the
    object, but the value of the pointer remains *dangling* around, and we reference
    an already-deleted object. Altogether, these errors can lead not only to **memory
    fragmentation**, but also to **buffer** **overflow** vulnerabilities.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*内存泄漏*发生在我们分配内存但未能释放它的时候。这种情况不仅可能是因为我们忘记了调用对象的析构函数，还可能是因为我们失去了对该内存地址的指针。除此之外，还需要考虑*野指针*和*悬挂指针*。第一种情况是当指针存在于*栈*上，但它从未与实际对象（或地址）相关联。第二种情况是当我们释放了对象使用的内存，但指针的值仍然*悬挂*在周围，我们引用了一个已经被删除的对象。总的来说，这些错误不仅可能导致**内存碎片化**，还可能导致**缓冲区溢出**漏洞。'
- en: These issues are hard to catch and reproduce, especially on large systems. System
    programmers and software integration engineers use tools such as address sanitizers,
    static and dynamic code analyzers, and profilers, among others, relying on them
    to predict future defects. But such tools are expensive and consume a lot of computational
    power, so we cannot rely on them constantly for higher code quality. That said,
    what can we do, then? The answer is simple – use smart pointers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题很难捕捉和重现，尤其是在大型系统中。系统程序员和软件集成工程师使用诸如地址清理器、静态和动态代码分析器以及性能分析器等工具，依赖它们来预测未来的缺陷。但是，这样的工具成本高昂，消耗大量的计算能力，因此我们不能始终依赖它们来保证更高的代码质量。话虽如此，我们还能做什么呢？答案是简单的——使用智能指针。
- en: Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can read more on the subject of smart pointers in the standard, or refer
    to [https://en.cppreference.com/w/cpp/memory](https://en.cppreference.com/w/cpp/memory).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在标准中了解更多关于智能指针的信息，或者参考[https://en.cppreference.com/w/cpp/memory](https://en.cppreference.com/w/cpp/memory)。
- en: Retracing RAII via smart pointers
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过智能指针重审RAII
- en: 'Even experienced C++ developers make mistakes when it comes to the right time
    for memory deallocation. Other languages use garbage collection techniques to
    handle memory management, but it’s important to mention that memory leaks happen
    there as well. Multiple algorithms are implemented for detecting such cases in
    the code but are not always successful. For example, the cycle dependency between
    objects is sometimes difficult to resolve – should two objects pointing to each
    other be deleted, or should they remain allocated? If they remain allocated, does
    this constitute a leak or not? So, it’s our job to be cautious about memory usage.
    In addition, garbage collectors work to free up memory, but do not manage opened
    files, network connections, locks, and so on. To this end, C++ implements its
    own instrument for control – wrapper classes over the pointers, helping us free
    the memory at the right time, usually when the object goes out of scope (the object
    life cycle was discussed already in [*Chapter 4*](B20833_04.xhtml#_idTextAnchor060)).
    Smart pointers are efficient in terms of memory and performance, meaning they
    don’t cost (much) more than raw pointers. At the same time, they give us robustness
    in memory management. There are three types of smart pointers in the C++ standard:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是有经验的C++开发者，在处理内存释放的正确时机时也会犯错误。其他语言使用垃圾回收技术来处理内存管理，但重要的是要提到，那里也会发生内存泄漏。代码中实现了多种算法来检测此类情况，但并不总是成功。例如，对象之间的循环依赖有时很难解决——指向彼此的两个对象应该被删除，还是应该保持分配？如果它们保持分配，这构成泄漏吗？因此，我们必须谨慎对待内存使用。此外，垃圾回收器努力释放内存，但不管理打开的文件、网络连接、锁等。为此，C++实现了自己的控制工具——指针的包装类，帮助我们正确地释放内存，通常是在对象超出作用域时（对象生命周期已在[*第4章*](B20833_04.xhtml#_idTextAnchor060)中讨论）。智能指针在内存和性能方面效率高，这意味着它们（几乎）不会比原始指针多花费（多少）成本。同时，它们在内存管理方面提供了稳健性。C++标准中有三种类型的智能指针：
- en: '`unique_ptr`: This is a pointer that is allowed one owner only. It cannot be
    copied or shared, but the ownership can be transferred. It has the size of a single
    raw pointer. It is destroyed and the object deallocated when it goes out of the
    scope.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unique_ptr`: 这是一个只能有一个所有者的指针。它不能被复制或共享，但所有权可以被转让。它的大小与单个原始指针相同。当它超出作用域时，它将被销毁，对象将被释放。'
- en: '`shared_ptr`: This can have multiple owners and is destroyed when all owners
    have given up ownership on it or all go out of scope. It uses a reference counter
    to the pointer of an object. Its size is two raw pointers – one for the allocated
    object, and one for the shared control block containing the reference count.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shared_ptr`: 它可以有多个所有者，当所有所有者都放弃对该对象的所有权或所有所有者都超出作用域时，它将被销毁。它使用一个指向对象的引用计数器。它的大小是两个原始指针——一个用于分配的对象，一个用于包含引用计数的共享控制块。'
- en: '`weak_ptr`: This provides access to an object owned by one or more shared pointers,
    but doesn’t count references. It is used for observing an object, but not for
    managing its life cycle. It consists of two pointers – one for the control block,
    and one for pointing to the shared pointer it was constructed from. Through `weak_ptr`
    you can learn whether the underlying `shared_ptr` is still valid – just call the
    `expired()` method.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weak_ptr`: 这提供了对一个或多个共享指针拥有的对象的访问，但不计算引用。它用于观察一个对象，但不用于管理其生命周期。它由两个指针组成——一个用于控制块，一个用于指向它从中构建的共享指针。通过`weak_ptr`你可以了解底层的`shared_ptr`是否仍然有效——只需调用`expired()`方法。'
- en: 'Let’s demonstrate their initial roles through the following example:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下示例演示它们的初始作用：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you can see, we use the heap as we call `new` for the creation of the `Book`
    objects. But as the smart pointer handles memory management, we don’t need to
    call the destructor explicitly:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们在创建`Book`对象时使用堆，因为我们调用`new`。但是，由于智能指针处理内存管理，我们不需要显式调用析构函数：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: First, we move the ownership of `book1`’s object to another `unique_ptr` – `book1_new`
    (marker `{1}`). We print out its `title` through the second `unique_ptr` as the
    first one is already invalid. We do the same operation for another `Book` object,
    but through a `shared_ptr` object (marker `{2}`). This time the `title` variable
    can be accessed from both pointers. We also print the reference count, and we
    see there are two references to that object.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将`book1`对象的拥有权移动到另一个`unique_ptr` – `book1_new`（标记{1}）。我们通过第二个`unique_ptr`打印其`title`，因为第一个已经无效。我们对另一个`Book`对象执行相同的操作，但通过一个`shared_ptr`对象（标记{2}）。这次`title`变量可以从两个指针访问。我们还打印了引用计数，我们看到有两个对该对象的引用。
- en: '`weak_ptr` has useful strengths in system programming, too. You can use `weak_ptr`
    to check for pointer validity. `weak_ptr` could also resolve the issue of cyclic
    dependency between objects. Let’s consider an example of a list node of a doubly
    linked list. The next example illustrates the benefits of `weak_ptr`. This is
    a good time to advise you not to implement such data structures yourself, especially
    when they are already a part of the C++ standard.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`weak_ptr`在系统编程中也有有用的优势。你可以使用`weak_ptr`来检查指针的有效性。`weak_ptr`还可以解决对象之间的循环依赖问题。让我们考虑一个双链表列表节点的例子。下一个例子将说明`weak_ptr`的好处。现在是时候建议你不要自己实现这样的数据结构了，尤其是当它们已经是C++标准的一部分时。'
- en: 'Now, let’s use the `Book` object as content of the `ListNode` `struct`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将`Book`对象用作`ListNode` `struct`的内容：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We also add two member variables for the previous and following nodes, but
    one of them will be `weak_ptr`. One remark is that the `weak_ptr` reference is
    not counted as such in the `shared_ptr` control block. Now, we have both access
    to the objects and the opportunity to count the references to zero with each deallocation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还添加了两个成员变量用于前一个和后一个节点，但其中一个将是`weak_ptr`。一个需要注意的是，`weak_ptr`引用在`shared_ptr`控制块中不被计为引用。现在，我们既有了对对象的访问，又有机会在每次分配时将引用计数归零：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'From the output, it’s clear that all objects were removed successfully:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，所有对象都已成功删除：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`weak_ptr` is also useful for cache implementation. Think about it – if you
    lose all references to an object, you will lose the object itsel; but with smart
    pointers, it will certainly be destroyed. So, imagine that recently accessed objects
    or objects with higher importance are kept through `shared_ptr` in the current
    code scope. But `weak_ptr` allows us to keep a reference to an object in the same
    scope if we need to reference the object later in that same scope. We would create
    a `weak_ptr` object to it in this case. But imagine that meanwhile, some other
    code scope holds a reference to the object through `shared_ptr`, thus keeping
    it allocated. In other words, we know about the object, but we don’t need to be
    concerned about its management. Thus, that object is accessible if it’s still
    required later, but removed when nothing else needs it. The following diagram
    shows how `shared_ptr` could be incorrectly used on the left-hand side, along
    with the implementation just described on the right-hand side:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`weak_ptr`在缓存实现中也非常有用。想想看 – 如果你失去了对一个对象的全部引用，你将失去该对象本身；但是，使用智能指针，它肯定会销毁。所以，想象一下，最近访问的对象或重要性较高的对象通过`shared_ptr`在当前代码作用域中保持。但是`weak_ptr`允许我们在需要在该作用域中稍后引用该对象时，在该作用域中保持对对象的引用。在这种情况下，我们会为它创建一个`weak_ptr`对象。但是想象一下，与此同时，其他代码作用域通过`shared_ptr`持有对该对象的引用，从而保持其分配。换句话说，我们知道关于该对象的信息，但不需要担心其管理。因此，如果以后还需要，该对象仍然是可访问的，但如果不再需要，它将被删除。以下图表显示了`shared_ptr`在左侧可能的不正确使用，以及右侧描述的实现：'
- en: '![Figure 9.1 – Cyclic dependency through shared_ptr and resolving through weak_ptr](img/Figure_9.1_B20833.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – 通过shared_ptr产生的循环依赖和通过weak_ptr解决](img/Figure_9.1_B20833.jpg)'
- en: Figure 9.1 – Cyclic dependency through shared_ptr and resolving through weak_ptr
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 通过shared_ptr产生的循环依赖和通过weak_ptr解决
- en: We are not going to dive further into other design solutions where smart pointers
    could come in handy in this section, but we will return to them in the realm of
    system programming later in the chapter. In the next section, we discuss a technique
    that’s the opposite to `weak_ptr`, where we retain the awareness of an object
    that hasn’t been created in memory yet.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们不会深入探讨其他可能需要智能指针的设计解决方案，但稍后在本章的系统编程领域，我们将回到这些解决方案。在下一节中，我们将讨论一种与`weak_ptr`相反的技术，其中我们保留了对尚未在内存中创建的对象的意识。
- en: Doing a lazy initialization in C++
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在C++中进行懒初始化
- en: Do you play video games? Have you ever seen a missing texture somewhere in the
    graphics while playing? Has a graphical resource appeared suddenly when you moved
    close to it with your character? Have you observed such behavior in other UIs
    as well? If your answers are mostly in the positive, then you have probably encountered
    **lazy initialization** already. It’s easy to figure out that its purpose is to
    postpone the construction of an object until it’s really needed. By doing so,
    we allow the system to allocate the required resources only. We also use it to
    speed up our code, especially if it’s run during high CPU loads, such as at system
    startup. Instead of wasting CPU cycles to create large objects that won’t be needed
    until (much) later, we free up the CPU to handle other requests. On the negative
    side, we might end up failing to load the object on time, as you have likely observed
    in video games. As we discussed in [*Chapter 2*](B20833_02.xhtml#_idTextAnchor029),
    this is also used when a program is loaded, and the kernel allocates virtual memory
    in a lazy fashion – a page of executable code is not loaded until referenced.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你玩电子游戏吗？在玩游戏的时候，你是否曾在图形中看到过缺失的纹理？当你用角色靠近时，是否有图形资源突然出现？你在其他UI中也观察到这种行为吗？如果你的回答大多是肯定的，那么你可能已经遇到了**延迟初始化**。很容易理解它的目的是将对象的构建推迟到真正需要的时候。通过这样做，我们只允许系统分配所需的资源。我们还用它来加速我们的代码，尤其是在高CPU负载期间运行时，比如在系统启动时。我们不会浪费CPU周期去创建那些（在很久以后）才需要的大的对象，而是让CPU腾出来处理其他请求。从负面来看，我们可能会遇到无法及时加载对象的情况，正如你在游戏中可能观察到的。正如我们在[*第2章*](B20833_02.xhtml#_idTextAnchor029)中讨论的那样，这也用于程序加载时，内核以延迟方式分配虚拟内存——直到被引用，可执行代码的页面才被加载。
- en: 'As with every other pattern, **lazy initialization** cannot solve all of the
    problems. So, the system programmer has to choose whether it should be applied
    for the given application’s functions or not. Usually, it is preferred that parts
    of the graphical and network storage resources remain lazily initialized as they
    are loaded on demand either way. In other words, the user doesn’t see the UI in
    its entirety all the time. Therefore, it’s not required to store it in memory
    a priori. C++ has features that allow us to easily implement this approach. We
    present **lazy initialization** in the following example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 就像其他任何模式一样，**延迟初始化**不能解决所有问题。因此，系统程序员必须选择是否应该将其应用于给定应用程序的功能。通常，图形和网络存储资源的一部分保持延迟初始化是首选的，因为它们无论如何都是按需加载的。换句话说，用户并不总是看到UI的全部内容。因此，事先存储在内存中不是必需的。C++有特性允许我们轻松实现这种方法。以下是我们展示**延迟初始化**的示例：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We propose a `Settings` class that will help us simulate the loading and updating
    of a list of settings from the disk. Note that we pass it by value and not by
    reference:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出一个`Settings`类，它将帮助我们模拟从磁盘加载和更新设置列表的过程。请注意，我们是通过值传递而不是引用传递：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This technique saves time due to reduced loading from memory. In C++, pass-by-value
    (or pass-by-copy) is the default argument passing technique, except for in the
    case of arrays. It is cheap and optimal for small types, such as `int`. Pass-by-reference
    is an alternative to pass-by-value and the `string_view` object is handled in
    the same manner as `int`, using a cheaper copy constructor than other standard
    objects such as `string`. Getting back to our example, we’re creating a configuration
    object, `Config`, which will consist of the settings file (which could be more
    than one file in real-world scenarios) and will allow settings to be changed in
    that configuration. Our `main()` method simulates an application’s startup. The
    `Config` object will be constructed, but the settings file will be loaded only
    when the startup is finished, and the process resources are available:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术由于减少了从内存中的加载而节省了时间。在C++中，除了数组之外，按值传递（或按拷贝传递）是默认的参数传递技术，对于小型类型，如`int`来说，这是便宜且最优的。按引用传递是按值传递的替代方案，`string_view`对象的处理方式与`int`相同，使用比其他标准对象（如`string`）更便宜的拷贝构造函数。回到我们的例子，我们正在创建一个配置对象`Config`，它将包含设置文件（在现实场景中可能不止一个文件）并允许在配置中更改设置。我们的`main()`方法模拟应用程序的启动。`Config`对象将被构建，但设置文件只有在启动完成后、进程资源可用时才会被加载：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We observe that the file is loaded after the startup has finished, as we expected:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到文件是在启动完成后加载的，正如我们所预期的：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `optional` class template is designed so that functions can return *nothing*
    when they fail, or a valid result when they succeed. We could also use it to handle
    objects whose construction is expensive. It also manages a value that may or may
    not be present at a given time. It is also readable, and its intent is clear.
    If an `optional` object contains a value, the value is guaranteed to be allocated
    as part of the `optional` object, and no dynamic memory allocation happens. Thus,
    an `optional` object models a *reservation* to an object, not a pointer. This
    is a key difference between `optional` and the smart pointer. Although using a
    smart pointer to handle large and complex objects might be a better idea, `optional`
    gives you the opportunity to construct an object at a later point in time when
    all parameters are known, if they weren’t known earlier in the execution. Both
    of them will work well in implementing **lazy initialization** – it’s a matter
    of your preference.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`optional` 类模板的设计是为了让函数在失败时可以返回 *nothing*，或者在成功时返回一个有效结果。我们也可以用它来处理构建成本高昂的对象。它还管理一个在特定时间可能存在也可能不存在的值。它易于阅读，意图明确。如果一个
    `optional` 对象包含一个值，那么这个值保证是作为 `optional` 对象的一部分分配的，并且不会发生动态内存分配。因此，`optional`
    对象模拟了一个对对象的 *reservation*，而不是一个指针。这是 `optional` 和智能指针之间的一个关键区别。尽管使用智能指针来处理大型和复杂对象可能是一个更好的主意，但
    `optional` 给你提供了一个在所有参数都已知时（如果它们在执行早期未知）稍后构建对象的机会。两者在实现 **延迟初始化** 方面都能很好地工作——这取决于你的偏好。'
- en: Later in the chapter, we will return to smart pointers and their usability for
    managing shared memory. First, though, we will use the next section to present
    some useful mechanisms for synchronization.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将回到智能指针及其在管理共享内存方面的可用性。不过，首先，我们将使用下一节来介绍一些有用的同步机制。
- en: Learning about condition variables, read-write locks, and ranges in C++
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 C++ 中的条件变量、读写锁和范围
- en: Let’s now start our discussion of synchronization primitives, a fundamental
    one of which is the **condition variable**. Its purpose is to allow multiple threads
    to remain blocked until an event occurs (i.e., a condition is satisfied). The
    implementation of **condition variables** requires an additional Boolean variable
    to indicate whether the condition is met or not, a *mutex* to serialize the access
    to the Boolean variable, and the condition variable itself.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始讨论同步原语，其中一个是 **条件变量**。它的目的是允许多个线程在事件发生（即条件满足）之前保持阻塞。**条件变量** 的实现需要一个额外的布尔变量来指示条件是否满足，一个
    *互斥锁* 来序列化对布尔变量的访问，以及条件变量本身。
- en: 'POSIX provides an interface for multiple use cases. Do you remember the producer-consumer
    example in [*Chapter 7*](B20833_07.xhtml#_idTextAnchor101)*, Using Shared Memory*?
    So, `pthread_cond_timedwait()` is used to block a thread for a given period of
    time. Or simply wait for a condition through `pthread_cond_wait ()` and signal
    with `pthread_cond_signal()` to one thread, or `pthread_cond_broadcast()` to all
    threads. Typically, the condition is checked periodically in the scope of a mutex
    lock:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: POSIX 为多个用例提供了一个接口。你还记得在 [*第7章*](B20833_07.xhtml#_idTextAnchor101) 中关于使用共享内存的
    **生产者-消费者** 示例吗？所以，`pthread_cond_timedwait()` 用于在给定时间内阻塞一个线程。或者简单地通过 `pthread_cond_wait()`
    等待一个条件，并通过 `pthread_cond_signal()` 向一个线程或 `pthread_cond_broadcast()` 向所有线程发出信号。通常，条件是在互斥锁的作用域内定期检查的：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we level up the abstraction, as we did in [*Chapter 7*](B20833_07.xhtml#_idTextAnchor101),
    C++ gives us access to the same technique, but a bit simpler and safer to use
    – we are guarded by the RAII principle. Let’s check the following snippet in C++:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将抽象级别提升到与我们在 [*第7章*](B20833_07.xhtml#_idTextAnchor101) 中所做的一样，C++ 会给我们提供相同的技巧，但使用起来更简单、更安全——我们受到
    RAII 原则的保护。让我们检查以下 C++ 代码片段：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this form, the code is not correct. There is no condition to be checked,
    and the shared resource itself is missing. We are simply setting the stage for
    the following examples, which are a continuation of what we covered in [*Chapter
    7*](B20833_07.xhtml#_idTextAnchor101). But observe the use of a `{4}`), while
    the first one was waiting (marker `{2}`). As you see, we rely on a *mutex* to
    lock the shared resource in the scope (marker `{1}`) and the condition variable
    is triggered through it in order to continue to work (markers `{2}` and `{3}`).
    Thus, the CPU is not busy waiting, as there’s no endless loop to wait for a condition,
    freeing up access to the CPU for other processes and threads. But the thread remains
    blocked, because the `wait()` method of the **condition variable** unlocks the
    **mutex** and the thread is put to sleep atomically. When the thread is signaled,
    it will be resumed and will re-acquire the **mutex**. This is not always useful
    as you will see in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种形式中，代码是不正确的。没有要检查的条件，共享资源本身也缺失。我们只是在为以下示例做准备，这些示例是我们之前在[*第7章*](B20833_07.xhtml#_idTextAnchor101)中讨论的内容的延续。但请注意，使用了一个`{4}`)，而第一个是在等待时（标记`{2}`）。正如你所看到的，我们依赖于一个*互斥锁*来锁定作用域内的共享资源（标记`{1}`），并通过它触发条件变量以继续工作（标记`{2}`和`{3}`）。因此，CPU不会忙于等待，因为没有无限循环等待条件，从而为其他进程和线程释放了对CPU的访问。但是线程仍然保持阻塞，因为**条件变量**的`wait()`方法会解锁**互斥锁**，线程被原子性地置于睡眠状态。当线程被信号时，它将被恢复并重新获取**互斥锁**。这并不总是有用的，你将在下一节中看到。
- en: Cooperative cancellation through condition variables
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过条件变量进行协作取消
- en: 'An important remark is that the condition variable should wait only with a
    condition and through a predicate. If not, the thread waiting on it will remain
    blocked. Do you remember the thread cancellation example from [*Chapter 6*](B20833_06.xhtml#_idTextAnchor086)?
    We used `jthread` and sent *stop notifications* between threads through the `stop_token`
    class and the `stop_requested` method. This mechanism is known as `jthread` technique
    is considered safe and easy to apply, but it might not be an option for your software
    design, or it might not be enough. Canceling threads could be directly related
    to waiting for an event. In that case, **condition variables** could come in handy
    as no endless loops or polling will be required. Revisiting the thread cancellation
    example from [*Chapter 6*](B20833_06.xhtml#_idTextAnchor086)*, Canceling Threads,
    Is This Really Possible?*, we have the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的注意事项是，条件变量应该只通过条件和谓词来等待。如果不这样做，等待该条件的线程将保持阻塞状态。你还记得来自[*第6章*](B20833_06.xhtml#_idTextAnchor086)的线程取消示例吗？我们使用了`jthread`并在线程之间通过`stop_token`类和`stop_requested`方法发送*停止通知*。这种机制被称为`jthread`技术，被认为是安全且易于应用的，但它可能不是你的软件设计选项，或者可能不足以满足需求。取消线程可能与等待事件直接相关。在这种情况下，**条件变量**可能会派上用场，因为不需要无限循环或轮询。回顾来自[*第6章*](B20833_06.xhtml#_idTextAnchor086)的线程取消示例“取消线程，这真的可能吗？”，我们有以下内容：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We are doing polling as the thread worker checks periodically whether the cancellation
    has been sent while doing something else in the meantime. But if the cancellation
    is the only thing we care about, then instead of polling, we could simply *subscribe*
    to the cancellation event using the `stop_requested` function. C++20 allows us
    to define a `stop_callback` function, so together with the condition variable
    and `get_stop_token()`, we can do the cooperative cancellation without endless
    loops:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在执行轮询，因为线程工作线程在同时做其他事情时定期检查是否已发送取消信号。但如果取消是我们唯一关心的事情，那么我们就可以使用`stop_requested`函数简单地*订阅*取消事件。C++20允许我们定义一个`stop_callback`函数，因此，结合条件变量和`get_stop_token()`，我们可以进行协作取消，而不需要无限循环：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'So, let’s finish the work from the example in the previous section and add
    a predicate to the **condition variable** in a worker thread:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们完成上一节中的示例工作，并在工作线程中的**条件变量**添加一个谓词：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So, the worker thread remains in execution, but the `stopper` thread gets the
    stop token in the `stop_callback` function. When the stop is requested through
    the stopper function, the **condition variable** is signaled through the token.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，工作线程仍然在执行，但`stopper`线程在`stop_callback`函数中获得了停止令牌。当通过停止函数请求停止时，**条件变量**将通过令牌被信号。
- en: Now that we have another mechanism besides the **semaphore** to signal between
    threads, we can get the **shared memory** back in the game. Let’s see how this
    can work together with the condition variables and smart pointers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们有了除了**信号量**之外的另一种线程间通信的机制，我们就可以让**共享内存**重新回到游戏中。让我们看看它是如何与条件变量和智能指针一起工作的。
- en: Combining smart pointers, condition variables, and shared memory
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合智能指针、条件变量和共享内存
- en: We already explored the concept of **shared memory** in [*Chapter 7*](B20833_07.xhtml#_idTextAnchor101)*,
    Using Shared Memory*. Let’s use the knowledge from the earlier sections in this
    chapter to enhance the code safety through some C++ techniques. We’re simplifying
    the scenario a little bit. The full example can be found at [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[*第7章*](B20833_07.xhtml#_idTextAnchor101)*，使用共享内存*中探讨了**共享内存**的概念。现在，让我们利用本章前面部分的知识，通过一些C++技术来增强代码的安全性。我们稍微简化了场景。完整的示例可以在[https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209)找到。
- en: 'We use the `unique_ptr` argument to provide a specific deallocator:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`unique_ptr`参数提供一个特定的析构函数：
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We rely on the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们依赖于以下：
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you see, we are also using templates in order to provide the possibility
    of storing any type of objects in the **shared memory**. It is easy to keep complex
    objects with large hierarchies and members in the heap, but storing and accessing
    their data is not trivial. Multiple processes will have access to those objects
    in the **shared memory**, but are the processes able to reference the memory behind
    the pointers? If the referenced memory is not in there or the shared virtual address
    space, then a memory access violation exception will be thrown. So, approach this
    with caution.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们也在使用模板来提供在**共享内存**中存储任何类型对象的可能性。在堆中保持具有大型层次结构和成员的复杂对象很容易，但存储和访问它们的数据并不简单。多个进程将能够访问**共享内存**中的这些对象，但这些进程能否引用指针后面的内存？如果引用的内存不在那里或者共享虚拟地址空间中，那么将会抛出一个内存访问违规异常。因此，要小心处理。
- en: 'We proceed with the next example. The already-known condition variable technique
    is used, but this time we add a real predicate to wait for:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续进行下一个示例。使用已知的条件变量技术，但这次我们添加了一个真实的谓词来等待：
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Our `producer()` method creates and maps the `{1}`). This technique is known
    as `new` operator does these two operations together. Additionally, the object
    itself is wrapped by a `unique_ptr` object with the respective deallocator. As
    soon as the scope is left, that portion of the memory will be reset through the
    `munmap()` method. A **condition variable** is used to signal to the consumer
    that the data has been prepared:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`producer()`方法创建并映射了`{1}`。这种技术被称为`new`运算符同时执行这两个操作。此外，对象本身被一个带有相应析构函数的`unique_ptr`对象包装。一旦离开作用域，该内存部分将通过`munmap()`方法重置。使用**条件变量**向消费者发出信号，表明数据已准备就绪：
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `shm` region is created and sized. Now, let us use it to store the data:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 创建并调整了`shm`区域的大小。现在，让我们使用它来存储数据：
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The consumer is implemented similarly, just waiting for the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者的实现方式类似，只是等待以下情况：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, two threads are started and joined as a producer and consumer to provide
    the following output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，启动并连接两个线程作为生产者和消费者，以提供以下输出：
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Of course, the example could be much more complex, adding periodic production
    and consumption. We encourage you to try it out, just using another type of buffer
    – as you may remember, the `string_view` object is a constant. Be sure that the
    deallocator is correctly implemented and called. It is used to make the code safer
    and discard the possibility of memory leaks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，示例可以更加复杂，添加周期性的生产和消费。我们鼓励你尝试一下，只需使用另一种类型的缓冲区——如你所记得的，`string_view`对象是常量。确保析构函数被正确实现和调用。它用于使代码更安全并排除内存泄漏的可能性。
- en: As you may have observed, throughout our work in this book, we often want to
    access an object just to read it, without modifying its data. In that case, we
    don’t need full-scale locking, but something to make a difference between just
    reading data or modifying it. This technique is the *read-write lock* and we present
    it in the following section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能观察到的，在我们这本书的工作中，我们经常只想访问一个对象来读取它，而不修改其数据。在这种情况下，我们不需要全面的锁定，但需要某种方法来区分仅仅是读取数据还是修改它。这种技术是*读写锁*，我们将在下一节中介绍。
- en: Implementing read-write locks and ranges with C++
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用C++实现读写锁和ranges
- en: POSIX provides the read-write locks mechanism directly, while C++ hides it under
    different names – `shared_mutex` and `shared_timed_mutex`. Let’s see how it works
    traditionally in POSIX. We have the *read-write lock* object (`rwlock`) with the
    expected POSIX interface, where a thread could hold multiple concurrent read locks
    on it. The goal is to allow multiple readers to access the data until a thread
    decides to modify it. That thread locks the resource through a write lock. Most
    implementations favor the write lock over the read lock in order to avoid write
    starvation. Such behavior is not necessary when it comes to data races, but it
    definitely causes a minimal application execution bottleneck.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: POSIX直接提供了读写锁机制，而C++则将其隐藏在不同的名称下——`shared_mutex`和`shared_timed_mutex`。让我们看看在POSIX中它是如何传统上工作的。我们有*读写锁*对象（`rwlock`），它具有预期的POSIX接口，其中线程可以对其持有多个并发读锁。目标是允许多个读取器访问数据，直到一个线程决定修改它。那个线程通过写入锁来锁定资源。大多数实现更倾向于写入锁而不是读锁，以避免写入饥饿。当涉及到数据竞争时，这种行为不是必要的，但它确实会导致应用程序执行的最小瓶颈。
- en: This is especially true when dealing with large-scale systems’ data readers
    – for example, multiple read-only UIs. The C++ features again give us a simple
    and robust instrument for this task. Therefore, we will not devote time to studying
    examples of POSIX. We advise you to take a look yourself if interested, starting
    with https://linux.die.net/man/3/pthread_rwlock_rdlock.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这尤其适用于处理大规模系统的数据读取器——例如，多个只读UI。C++的特性再次为我们提供了这个任务的简单而强大的工具。因此，我们不会花时间研究POSIX的示例。如果你感兴趣，我们建议你自己查看，从https://linux.die.net/man/3/pthread_rwlock_rdlock开始。
- en: 'Proceeding with the C++ example, let’s consider the following scenario – a
    small number of threads want to modify a shared resource – a vector of numbers
    – and a larger number of threads wants to visualize the data. What we want to
    use here is `shared_timed_mutex`. It allows two levels of access: *exclusive*,
    where only one thread can own the mutex; and *shared*, where multiple threads
    share ownership of the mutex.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续C++示例之前，让我们考虑以下场景——少数线程想要修改一个共享资源——一个数字向量，而更多的线程想要可视化数据。我们在这里想要使用的是`shared_timed_mutex`。它允许两种级别的访问：*独占*，其中只有一个线程可以拥有互斥锁；和*共享*，其中多个线程共享互斥锁的所有权。
- en: Important note
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Keep in mind that both the `shared_timed_mutex` and `shared_mutex` types are
    heavier than a simple `mutex`, although `shared_mutex` is considered more efficient
    on some platforms than `shared_timed_mutex`. You’re expected to use them when
    your read operations are really resource-hungry, slow, and frequent. For short
    operation bursts it would be preferable to stick with just the mutex. You’ll need
    to measure your resource usage specifically for your system in order to work out
    which to choose.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`shared_timed_mutex`和`shared_mutex`类型比简单的`mutex`更重，尽管在某些平台上`shared_mutex`被认为比`shared_timed_mutex`更高效。当你确实需要大量的读取操作时，你应该使用它们。对于短时间的操作爆发，坚持使用互斥锁会更好。你需要具体测量你系统的资源使用情况，以便确定选择哪一个。
- en: The following example illustrates the usage of `shared_mutex`. We’ll also use
    the opportunity to present the `ranges` library in C++. This feature comes with
    C++20 and together with `string_views` provides an agile way to visualize, filter,
    transform, and slice C++ containers, among other things. Through this example,
    you’ll learn about some useful techniques with the `ranges` library, which will
    be explained along with the code. The full example can be found at https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例说明了`shared_mutex`的使用方法。我们还将利用这个机会介绍C++中的`ranges`库。这个特性是C++20的一部分，并且与`string_views`一起提供了一种灵活的方式来可视化、过滤、转换和切片C++容器，以及其他功能。通过这个示例，你将了解一些关于`ranges`库的有用技术，这些技术将伴随着代码进行解释。完整的示例可以在https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209找到。
- en: 'Let’s have a `Book` struct with a shared resource – `vector` of books. We are
    going to use `shared_mutex` to handle read-write locking:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们有一个带有共享资源——书籍 `vector` 的 `Book` 结构体。我们将使用 `shared_mutex` 来处理读写锁：
- en: '[PRE23]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We implement the method for adding a book to the shared resource with the `wr_`
    prefix in order to distinguish its role from the other methods. We also execute
    a write lock on the resource (marker `{1}`):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `wr_` 前缀实现向共享资源添加书籍的方法，以区分其与其他方法的角色。我们还在资源上执行写锁（标记 `{1}`）：
- en: '[PRE24]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we start with the implementation of multiple reader routines. They are
    marked with the `rd_` prefix, and each of them executes a read lock, meaning that
    the resource will be available for multiple readers at a time:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们开始实现多个读取例程。它们以 `rd_` 前缀标记，并且每个例程执行一个读取锁，这意味着资源将同时可供多个读取者使用：
- en: '[PRE25]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Observe the `for` loop after marker `{2}`. It not only iterates through the
    shared resource, but with the pipe (|) character we filter out portions of it,
    which is similar to piping and `grep` as covered in [*Chapter 3*](B20833_03.xhtml#_idTextAnchor047),
    except here, it’s not a pipe. We are creating a *range view* through the pipe
    operator, thus providing additional logic to the iteration. In other words, we
    manipulate the view to the container. This approach can be used not only for `vectors`,
    but for the other C++ iterable objects as well. Why? *Ranges* are used to extend
    and generalize the algorithms with iterators so the code becomes tighter and less
    error prone.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 观察标记 `{2}` 之后的 `for` 循环。它不仅遍历共享资源，而且通过管道（|）字符过滤掉其部分内容，这与在[*第3章*](B20833_03.xhtml#_idTextAnchor047)中介绍的管道和
    `grep` 类似，但在这里，它不是一个管道。我们通过管道操作符创建了一个*范围视图*，从而为迭代提供了额外的逻辑。换句话说，我们操作了容器的视图。这种方法不仅适用于
    `vectors`，也适用于其他 C++ 可迭代对象。为什么？*范围*用于通过迭代器扩展和泛化算法，使代码更加紧凑且错误率更低。
- en: 'It’s easy to see the intention of the *range* here, too. Additionally, the
    *range view* is a lightweight object, similar to `string_view`. It represents
    an iterable sequence – the *range* itself, created on top of the containers’ iterators.
    It is based on the *Curiously Recurring Template Pattern*. Through the *range*
    interface, we can change the presentation of a container, present its values as
    transformed in a given manner, filter out values, split and combine sequences,
    present unique elements, shuffle elements, slide a window through the values,
    and so on. All of this is done via the simple syntax of already-implemented *range
    adapters*. In our example, `rd_applyYearFilter` has a `for` loop wherein books
    older than `yearKey` are filtered out. We could also print out the shared resource’s
    elements in reverse order:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出这里的*范围*意图。此外，*范围视图*是一个轻量级对象，类似于 `string_view`。它表示一个可迭代的序列——*范围*本身，是在容器的迭代器之上创建的。它基于*Curiously
    Recurring Template Pattern*。通过*范围*接口，我们可以改变容器的表示，以某种方式转换其值，过滤掉值，分割和组合序列，展示唯一元素，打乱元素，滑动窗口通过值，等等。所有这些操作都是通过已实现的简单*范围适配器*语法完成的。在我们的示例中，`rd_applyYearFilter`
    有一个 `for` 循环，其中过滤掉了比 `yearKey` 更早的书籍。我们也可以以逆序打印共享资源的元素：
- en: '[PRE26]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We could even combine views, as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以将视图组合起来，如下所示：
- en: '[PRE27]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The previous snippet iterates through the elements in reverse order, but it
    also filters out those books where the length of the author’s name is longer than
    a given value. With the next snippet, we demonstrate how to simply drop a portion
    of the container during iteration:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码片段以逆序遍历元素，但它还过滤掉了作者姓名长度超过给定值的书籍。在下一个代码片段中，我们展示了如何在迭代过程中简单地丢弃容器的一部分：
- en: '[PRE28]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If this is too generic, you could instead use a specific subrange, which will
    create a `range` object. The `range` object can be used like any other, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这太通用，你可以使用特定的子范围，这将创建一个 `range` 对象。`range` 对象可以像任何其他对象一样使用，如下所示：
- en: '[PRE29]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'With all of this complete, we create threads to execute all of these actions
    in a concurrent manner and see how the *read-write lock* manages them. Running
    the example will produce different output orders depending on the thread’s scheduling:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有这些操作后，我们创建线程以并发方式执行所有这些操作，并观察*读写锁*如何管理它们。运行示例将根据线程的调度产生不同的输出顺序：
- en: '[PRE30]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is per the described *range views* (the following has been rearranged
    slightly for easier reading):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出按照描述的*范围视图*（以下内容略有调整以便于阅读）：
- en: '[PRE31]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You have now learned about another combination of techniques with which you
    can scale up a system with multiple threads that handle presentation tasks. Let’s
    now take a step back and discuss the possible traps arising from concurrent execution
    that are not directly related to data races. We continue with cache-friendly code.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经了解了另一种技术组合，你可以使用它来扩展一个由多个线程处理展示任务的系统。现在让我们退一步，讨论一下与数据竞争无关的并发执行可能出现的陷阱。我们继续讨论缓存友好的代码。
- en: Discussing multiprocessor systems – cache locality and cache friendliness in
    C++
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论多处理器系统——C++中的缓存局部性和缓存友好性
- en: You probably recall [*Chapter 2*](B20833_02.xhtml#_idTextAnchor029) at this
    point, where we discussed multi-thread and multi-core processors. The respective
    computational units were presented as processors. We also visualized the transport
    of instructions from the **NVM** (the disk) to the processors, through which we
    explained the creation of processes and *software* threads.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得此时此刻的[*第二章*](B20833_02.xhtml#_idTextAnchor029)，在那里我们讨论了多线程和多核处理器。相应的计算单元被表示为处理器。我们还可视化了指令从**NVM**（磁盘）传输到处理器的过程，通过这个过程我们解释了进程和*软件*线程的创建。
- en: We want our code to be as performant as required. The most important aspect
    of getting the code to perform well is the choice of appropriate algorithms and
    data structures. With a bit of thought, you can try to squeeze the most out of
    every last CPU cycle. One of the most common examples of misusing algorithms is
    sorting a large, unordered array with bubble sort. So, make sure to learn your
    algorithms and data structures – together with the knowledge from this section
    and beyond, it will make you a really powerful developer.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的代码能够满足性能要求。让代码表现良好的最重要的方面是选择合适的算法和数据结构。经过一些思考，你可以尝试从每个CPU周期中榨取最大价值。算法误用的最常见例子之一就是使用冒泡排序对大型无序数组进行排序。所以，确保学习你的算法和数据结构——结合本节及以后的知识，这将使你成为一个真正强大的开发者。
- en: As you already know, the further we get from the RAM and the closer we get to
    the processor registers, the faster the operations and the smaller the memory
    capacity becomes. Each time the processor loads data from the RAM to the cache,
    it will either just sit and wait for that data to show up, or execute other non-related
    tasks. Thus, from the perspective of the current task, the CPU cycles are wasted.
    Of course, reaching 100% CPU utilization might be impossible, but we should at
    least be aware when it’s doing needless work. All of this might sound meaningless
    to you at this point, but concurrent systems will suffer if we act carelessly.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所知，我们离RAM越远，接近处理器寄存器的程度越高，操作就越快，内存容量就越小。每次处理器从RAM中加载数据到缓存时，它要么只是坐着等待数据出现，要么执行其他非相关任务。因此，从当前任务的角度来看，CPU周期被浪费了。当然，达到100%的CPU利用率可能是不可能的，但我们应该至少意识到它在做无谓的工作。所有这些可能在你现在看来似乎没有意义，但如果我们粗心大意，并发系统将会受到影响。
- en: The C++ language provides access to multiple tools for even better performance
    improvements, including **prefetching mechanisms** through hardware instructions
    and **branch prediction** **optimization**. Even without doing anything in particular,
    modern compilers and CPUs do a great job with these techniques. Still, we could
    improve this performance further by providing the right hints, options, and instructions.
    It’s also a good idea to be aware of the data in the cache to help reduce the
    time taken when accessing it. Remember that the cache is just a type of fast,
    temporary storage for data and instructions. So, we can use the features of C++
    to our advantage when we treat the cache in a good manner, known as **cache-friendly
    code**. An important remark to note is the inverse of this statement – misusing
    C++ features will lead to poor cache performance, or at least not the best performance
    possible. You’ve probably already guessed that this is related to the system’s
    scale and the requirement for fast data access. Let’s discuss this further in
    the next section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: C++ 语言提供了多种工具来进一步提高性能，包括通过硬件指令的**预取机制**和**分支预测优化**。即使不做任何特别的事情，现代编译器和 CPU 也会使用这些技术做得很好。然而，通过提供正确的提示、选项和指令，我们还可以进一步提高性能。了解缓存中的数据也有助于减少访问它所需的时间。请记住，缓存只是数据和指令的一种快速、临时的存储类型。因此，当我们以良好的方式处理缓存时，即所谓的**缓存友好代码**，我们可以利用
    C++ 的特性。需要注意的是，这个陈述的反面——误用 C++ 特性会导致缓存性能不佳，或者至少不是最佳性能。您可能已经猜到这与系统的规模和快速数据访问的需求有关。让我们在下一节中进一步讨论这个问题。
- en: Considering cache locality through cache-friendly code
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过缓存友好代码考虑缓存局部性
- en: We mentioned the concept of cache-friendly code already, but what does it truly
    mean? First of all, you need to be aware of the `int` or even an unsigned `char`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到了缓存友好代码的概念，但它究竟意味着什么呢？首先，您需要了解 `int` 或甚至无符号的 `char`。
- en: As a result, caching has become a major aspect of almost every system. Earlier
    in the book we mentioned that slower hardware, such as disks, sometimes has its
    own cache memory to reduce the time taken to access frequently opened files. OSs
    can cache frequently used data, for example, files, as chunks of virtual address
    space, thus improving performance even more. This is also known as **temporal
    locality**.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，缓存已经成为几乎所有系统的主要方面。在本书的前面，我们提到较慢的硬件，如磁盘，有时有自己的缓存内存来减少访问频繁打开的文件所需的时间。操作系统可以缓存频繁使用的数据，例如文件，作为虚拟地址空间的部分，从而进一步提高性能。这也被称为**时间局部性**。
- en: 'Consider the following scenario: a piece of data is not found in the cache
    on the first try – this is known as a **cache miss**. Then it is looked up in
    the RAM, is found, and is loaded into the cache as one or multiple **cache blocks**
    or **cache lines**. Afterwards, if this data is requested a number of subsequent
    times and is still found in the cache, known as a **cache hit**, it will remain
    in the cache and guarantee faster access, or at least faster than the first **cache
    miss**. You can observe this in the following diagram:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下场景：第一次尝试在缓存中找不到数据，这被称为**缓存未命中**。然后它在 RAM 中查找，找到后，作为一个或多个**缓存块**或**缓存行**被加载到缓存中。之后，如果此数据被多次请求并且仍然在缓存中找到，这被称为**缓存命中**，它将保留在缓存中并保证更快的访问，或者至少比第一次**缓存未命中**更快。您可以在以下图中观察到这一点：
- en: '![Figure 9.2 – Representation of temporal locality on the hardware level](img/Figure_9.2_B20833.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 硬件级别上时间局部性的表示](img/Figure_9.2_B20833.jpg)'
- en: Figure 9.2 – Representation of temporal locality on the hardware level
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 硬件级别上时间局部性的表示
- en: As we mentioned with the **prefetching mechanisms** earlier, it’s a known fact
    that having an object with multiple **cache hits** means that the data around
    it might also be referenced soon. This causes the processor to *request* or *prefetch*
    that additional nearby data from the RAM and load it a priori, so it will be there
    in the cache when it is eventually needed. This causes **spatial locality**, meaning
    accessing nearby memory and benefiting from the fact that caching is done in chunks,
    known as **cache lines, t**hus paying for a single transfer and using several
    bytes of memory. The prefetching technique assumes that the code already has **spatial
    locality** in order to improve performance.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的**预取机制**，有一个已知的事实，即具有多个**缓存命中**的对象意味着其周围的数据也可能很快被引用。这导致处理器**请求**或**预取**从RAM中额外的附近数据，并提前将其加载到缓存中，以便在最终需要时它将在缓存中。这导致了**空间局部性**，意味着访问附近的内存并从缓存以块的形式（称为**缓存行**）进行的事实中受益，从而只需支付一次传输费用并使用几个字节的内存。预取技术假设代码已经具有**空间局部性**，以提高性能。
- en: Both locality principles are based on assumptions. But code branching requires
    good design. The simpler the branch tree, the simpler to predict. Again, you need
    to consider carefully the data structures and algorithms to be used. You also
    need to aim at contiguous memory access and reduce the code to simple loops and
    small functions; for example, switching from using linked lists to arrays or matrices.
    For small-sized objects, the `std::vector` container is still the optimal choice.
    Additionally, we ideally seek a data structure object that can fit into one **cache
    line** – but sometimes this is just not possible because of the application’s
    requirements.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个局部性原则都是基于假设的。但代码分支需要良好的设计。分支树越简单，预测就越简单。再次强调，你需要仔细考虑要使用的数据结构和算法。你还需要旨在连续内存访问，将代码简化为简单的循环和小函数；例如，从使用链表切换到数组或矩阵。对于小型对象，`std::vector`容器仍然是最佳选择。此外，我们理想地寻求一个可以适应一个**缓存行**的数据结构对象——但有时这因为应用程序的要求而根本不可能。
- en: Our process should access the data in contiguous blocks, where each one has
    the size of a cache line (typically 64 bytes but depends on the system). But if
    we want to do parallel evaluations, then it would be preferable for each CPU core
    (processor) to handle data in different **cache lines** from other cores’ data.
    If not, the cache hardware will have to move data back and forth between cores
    and the CPU will waste time on meaningless work again and the performance will
    worsen, instead of being improved. This term is known as **false sharing**, which
    we’ll now have a look at in the following section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的过程应该以连续块的形式访问数据，其中每个块的大小与缓存行的大小相同（通常是64字节，但取决于系统）。但如果我们要进行并行评估，那么每个CPU核心（处理器）处理的数据最好与其他核心的数据在不同的**缓存行**中。如果不是这样，缓存硬件将不得不在核心和CPU之间来回移动数据，CPU将再次浪费在无意义的工作上的时间，性能将下降，而不是提高。这个术语被称为**伪共享**，我们将在下一节中对其进行探讨。
- en: A glance at false sharing
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简要了解伪共享
- en: As a rule, small pieces of data will be put together in a single **cache line**
    unless the programmer instructs otherwise, as we will see in the following examples.
    This is the way processors work in order to keep latency low – they handle one
    cache line for each core at a time. Even if it’s not full, the **cache line**’s
    size will be allocated as the smallest possible block for the CPU to handle. As
    mentioned earlier, if the data in that **cache line** is requested by two or more
    threads independently, then this will slow down the multi-threaded execution.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，除非程序员另有指示，否则小数据块将组合在一个单独的**缓存行**中，正如我们将在以下示例中看到的那样。这是处理器为了保持低延迟而工作的方式——一次处理每个核心的一个缓存行。即使它不是满的，**缓存行**的大小也将被分配为CPU可以处理的最小块。如前所述，如果两个或更多线程独立地请求该**缓存行**中的数据，那么这将减慢多线程执行的效率。
- en: 'Dealing with the effects of **false sharing** means getting predictability.
    Just as code branching can be predicted, so can the system programmer predict
    if an object is of the size of a cache line, and thus each separate object can
    reside in its own memory block. In addition, all computations can happen in the
    local scope and the shared data modifications take place at the end of a given
    procedure. Of course, such activities will lead to the wasting of resources at
    some point, but it’s a matter of design and preferences. Nowadays, we can use
    compiler optimizations to improve this predictability and performance, too, but
    we shouldn’t always rely on this. Let’s first check the size of our cache line:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 处理**伪共享**的影响意味着获得可预测性。就像代码分支可以被预测一样，系统程序员也可以预测一个对象是否是缓存行的大小，因此每个单独的对象可以驻留在自己的内存块中。此外，所有计算都可以在局部作用域内进行，共享数据修改发生在给定过程的末尾。当然，这种活动最终会导致资源的浪费，但这是一个设计和偏好的问题。如今，我们可以使用编译器优化来提高这种可预测性和性能，但我们不应该总是依赖于此。让我们首先检查我们的缓存行大小：
- en: '[PRE32]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The expected output is as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的输出如下：
- en: '[PRE33]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now that we know how to get the `std::atomic` to guarantee a single modifier
    to a shared resource, but we also emphasized that this is not the full picture.
    Let’s enrich the previous example with three atomic variables:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何使用`std::atomic`来保证对共享资源的单一修改，但我们同时也强调了这并不是全部。让我们用三个原子变量丰富之前的例子：
- en: '[PRE34]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Printing the addresses out gives the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 打印地址给出以下结果：
- en: '[PRE35]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This means that even when we have atomic variables, they can be fitted into
    a single `atomic_ref<T>::required_alignment`, which allows the programmer to align
    atomics as per the current cache line size, thus keeping them well apart. Let’s
    apply it for all atomics as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着即使我们有原子变量，它们也可以适应单个`atomic_ref<T>::required_alignment`，这允许程序员根据当前的缓存行大小对齐原子，从而保持它们之间的良好距离。让我们如下应用它到所有原子变量：
- en: '[PRE37]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE38]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In the preceding snippet, you can see that the differences in the addresses
    are as expected and the variables are well aligned, which was always the system
    programmer’s responsibility. Now, let’s apply the `increment()` method that you
    might remember from [*Chapter 7*](B20833_07.xhtml#_idTextAnchor101):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，你可以看到地址的差异正如预期的那样，变量对齐良好，这始终是系统程序员的职责。现在，让我们应用你可能从[*第7章*](B20833_07.xhtml#_idTextAnchor101)中记得的`increment()`方法：
- en: '[PRE39]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We increment an atomic resource, and as covered in [*Chapter 8*](B20833_08.xhtml#_idTextAnchor116),
    we know how to measure the duration of a procedure. So, we can analyze the performance
    for the next four scenarios. One remark – if you feel so inclined, you could play
    with the compiler optimization levels to spot the difference in the following
    values, as we are not using any of the optimization flags. The full code example
    could be found at [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209).
    Our scenarios are as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们增加一个原子资源，正如在[*第8章*](B20833_08.xhtml#_idTextAnchor116)中所述，我们知道如何测量过程的持续时间。因此，我们可以分析以下四种场景的性能。有一点需要注意——如果你有兴趣，可以调整编译器的优化级别来观察以下值的差异，因为我们没有使用任何优化标志。完整的代码示例可以在[https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209)找到。我们的场景如下：
- en: A single-threaded application, calling `increment()` 3 times, doing 300,000
    increments of an atomic variable, which takes 2,744 microseconds
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单线程应用程序，调用`increment()` 3次，对一个原子变量进行300,000次增加，耗时2,744微秒
- en: Direct sharing with one atomic variable, incremented 100,000 times by each of
    3 threads in parallel, taking 5,796 microseconds
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接与一个原子变量进行共享，由3个线程并行地各自增加100,000次，耗时5,796微秒
- en: False sharing with three unaligned atomic variables, incremented 100,000 times
    by each of the 3 threads in parallel, taking 3,545 microseconds
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与三个未对齐的原子变量发生伪共享，由3个线程并行地各自增加100,000次，耗时3,545微秒
- en: No sharing with three aligned atomic variables, incremented 100,000 times by
    each of 3 threads in parallel, taking 1,044 microseconds
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不与三个对齐的原子变量共享，由3个线程并行地各自增加100,000次，耗时1,044微秒
- en: 'As we are not using a benchmarking tool, we cannot measure the number of cache
    misses or hits. We simply do the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有使用基准测试工具，我们无法测量缓存未命中或命中的次数。我们只是做以下操作：
- en: '[PRE40]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The **no-sharing** work is presented in the following diagram:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**无共享**工作在以下图中展示：'
- en: '![Figure 9.3 – Representation of no-sharing (correct sharing) of data on multiple
    cores/threads](img/Figure_9.3_B20833.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 多核心/线程上无共享（正确共享）数据的表示](img/Figure_9.3_B20833.jpg)'
- en: Figure 9.3 – Representation of no-sharing (correct sharing) of data on multiple
    cores/threads
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 多核心/线程上无共享（正确共享）数据的表示
- en: Important note
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It’s obvious that we either have to align our atomic resources before we modify
    them in parallel, or use single-threaded applications for small procedures. The
    time metric could differ, depending on the system and the compiler optimization
    flags. Keep in mind that these speed-ups are great when you get the best out of
    your hardware, but going into so much detail might also lead to complex code,
    harder debugging, and time wasted on maintenance. It’s a balancing act.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们或者必须在并行修改它们之前对原子资源进行对齐，或者为小过程使用单线程应用程序。时间度量可能因系统和编译器优化标志而异。记住，当你从硬件中获得最佳性能时，这些加速是非常好的，但深入这么多细节也可能导致代码复杂、调试困难以及维护上的时间浪费。这是一个平衡行为。
- en: False sharing happens during multi-threading and can be fixed if the shared
    object is fitted into one cache line. But what happens if the object is larger
    than one cache line in size?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在多线程期间发生**伪共享**，如果共享对象适合一个缓存行，则可以修复。但如果对象的大小超过一个缓存行呢？
- en: Sharing resources larger than a cache line in C++
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在C++中共享大于缓存行的资源
- en: The analysis here is relatively simple, as it is not so dependent on the language.
    Large objects, representing large data structures, are just... large. They don’t
    fit into single **cache lines** and therefore they are not **cache friendly**
    by nature. Data-oriented design deals with this issue. For example, you could
    think about using smaller objects or share only small parts of them for parallel
    work. Additionally, it is good to think about optimizations in algorithms. Making
    them linear leads to better **branch predictions**. This means making conditional
    statements depend on predictable, not random, data. Complex conditional statements
    can be replaced with arithmetic solutions and templates, or chained differently,
    so it is easier for the CPU to predict which branch has a higher probability of
    occurring. Such operations, again, could lead to unreadable code and complex debugging,
    so proceed with them only when the code is not fast enough for your requirements.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的分析相对简单，因为它并不那么依赖于语言。代表大型数据结构的大对象只是...很大。它们无法适应单个**缓存行**，因此它们在本质上不是**缓存友好**的。面向数据的设计处理这个问题。例如，你可以考虑使用更小的对象，或者只为并行工作共享它们的小部分。此外，考虑算法的优化也是好的。使它们线性化会导致更好的**分支预测**。这意味着使条件语句依赖于可预测的而不是随机的数据。复杂的条件语句可以用算术解决方案和模板替换，或者以不同的方式链接，这样CPU就更容易预测哪个分支有更高的发生概率。这些操作，再次强调，可能会导致难以阅读的代码和复杂的调试，因此只有在代码不够快以满足你的要求时才进行这些操作。
- en: As **branch misprediction** could be expensive and remain well hidden, another
    proposal is the so-called **conditional move**. It is not based on predictions,
    but on data. The data dependencies include both *condition true* and *condition
    false* cases. After an instruction that conditionally moves data from one register
    to another, the contents of the second depend on both their previous values and
    the values from the first register. As mentioned, well-designed branching allows
    better performance. But data dependencies require one or two CPU cycles to arrive,
    sometimes making them a safer bet. A probable trap is when the condition is such
    that the value taken from the memory is not assigned to the register – then it’s
    just meaningless waiting. Luckily for the system programmer, the **conditional
    move** instructions in the instruction sets are typically close register-wise.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于**分支预测错误**可能代价高昂且隐藏得很好，因此另一个建议是所谓的**条件移动**。它不是基于预测，而是基于数据。数据依赖包括*条件为真*和*条件为假*的情况。在执行将数据从寄存器之一移动到另一个寄存器的条件指令之后，第二个寄存器的内容取决于它们的先前值和第一个寄存器的值。正如提到的，良好的分支设计允许更好的性能。但是数据依赖需要一到两个CPU周期才能到达，有时使它们成为一个更安全的赌注。一个可能的陷阱是当条件是这样的，从内存中取出的值没有被分配给寄存器时——那么它就只是无意义的等待。幸运的是，对于系统程序员来说，指令集中的**条件移动**指令通常在寄存器上很接近。
- en: '`std::array` and `std::vector`. Yes, the vector could be resized, but it’s
    still cache friendly, as the elements are next to each other in the memory. Of
    course, if you have to reallocate the vector due to constant resizing, then probably
    it’s not the data structure you need. You could consider the `std::deque` container,
    which is efficient for modifications in the middle of the collection, or `std::list`
    as an alternative, which is a linked list and is not cache friendly at all.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::array` 和 `std::vector`。是的，向量可以调整大小，但它仍然对缓存友好，因为元素在内存中是相邻的。当然，如果你因为频繁调整大小而需要重新分配向量，那么可能这不是你需要的数据结构。你可以考虑使用
    `std::deque` 容器，它在集合中间的修改中效率很高，或者作为替代的 `std::list`，它是一个链表，并且完全不友好缓存。'
- en: Important note
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Depending on the system, many reallocations (construction and destruction) of
    contiguous memory blocks could cause memory fragmentation. This can happen due
    to software algorithms for memory management, language standards, OSs, drivers,
    devices, and so on. It is hard to predict it until it happens. It might take a
    good portion of non-stop execution time for the memory allocations to start failing.
    There could be enough free space in the sum of the free memory blocks in the RAM,
    but not a single block big enough to hold the currently reallocated or created
    contiguous block. Excessive fragmentation could lead to poor performance and even
    denial of service.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 根据系统不同，许多连续内存块的重新分配（构造和析构）可能会导致内存碎片化。这种情况可能由于内存管理的软件算法、语言标准、操作系统、驱动程序、设备等原因造成。直到它发生之前很难预测。内存分配开始失败可能需要大量的连续执行时间。在
    RAM 中，所有空闲内存块的总和可能有足够的空闲空间，但可能没有足够大的单个块来容纳当前重新分配或创建的连续块。过度的碎片化可能导致性能下降，甚至服务拒绝。
- en: A final remark on the topic is that there are many articles discussing optimal
    ways of using C++’s algorithms and containers efficiently. It deserves a book
    on its own and most of the time is very CPU specific – or at least when you get
    to the absolute performance. For example, the **conditional moves** lead directly
    to assembly code, which we don’t have the opportunity to explore here. That said,
    the variety of solutions for different practical problems is enormous when it
    comes to algorithms and data structures.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个主题的最后一句话是，有许多文章讨论了高效使用 C++ 算法和容器的最佳方法。这值得一本单独的书，而且大多数情况下都是非常 CPU 特定的——或者至少当你达到绝对性能时是这样。例如，**条件移动**会直接导致汇编代码，而这在这里我们没有机会去探索。话虽如此，当涉及到算法和数据结构时，针对不同实际问题的解决方案种类繁多。
- en: Revisiting shared resources through the C++ memory model via spinlock implementation
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过自旋锁实现重新审视 C++ 内存模型中的共享资源
- en: 'We learned about atomic operations back in [*Chapter 7*](B20833_07.xhtml#_idTextAnchor101).
    In this chapter, you learned that the placement of atomic variables in the cache
    is crucial as well. Originally, atomics and locks were introduced because of correctness
    when multiple threads want to enter the same critical section. Now, our investigation
    will continue a bit deeper. There’s one last piece of the puzzle of atomic operations.
    Examine the following snippet:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第 7 章*](B20833_07.xhtml#_idTextAnchor101)中学习了原子操作。在本章中，你了解到原子变量在缓存中的放置同样至关重要。最初，原子和锁的引入是为了在多个线程想要进入相同的临界区时保证正确性。现在，我们的研究将深入一点。原子操作的最后一块拼图。请检查以下代码片段：
- en: '[PRE41]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This was an example of a non-atomic operation. Even when we make it atomic,
    we still don’t have a word about the order of the instructions. Until now, we
    used the synchronization primitives to instruct the CPU about which section of
    instructions has to be taken as a unitary context. What we need now is to instruct
    the processor about the order of those instructions. We do this through C++’s
    `memory_order`, which is a part of the C++ standard memory model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非原子操作的例子。即使我们将其变为原子操作，我们仍然没有关于指令顺序的说明。直到现在，我们使用同步原语来指示 CPU 哪些指令部分必须作为一个统一的上下文来处理。我们现在需要指示处理器这些指令的顺序。我们通过
    C++ 的 `memory_order` 来实现，它是 C++ 标准内存模型的一部分。
- en: Introducing the memory_order type in C++
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 C++ 中引入 memory_order 类型
- en: 'With the `memory_order` type, we specify how atomic and non-atomic memory accesses
    are ordered around an atomic operation. The atomic realization of the snippet
    from the preceding section and the example using read-write locks earlier in the
    chapter could both suffer from the same issue: two atomic operations are not atomic
    as a whole. The order of instructions inside the atomic scope will be kept, but
    not around it. This is usually done after optimization techniques in the CPU and
    the compiler. So, if there are many reader threads, the order in which we (and
    the threads) expect to observe changes could vary. Such an effect could appear
    even during single-threaded execution as the compiler might re-arrange instructions
    as allowed by the memory model.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`memory_order`类型，我们指定原子和非原子内存访问在原子操作周围的排序方式。前一小节中的代码片段的原子实现以及本章早期使用读写锁的示例都可能存在相同的问题：两个原子操作作为一个整体并不是原子的。原子作用域内的指令顺序将被保持，但不是在它周围。这通常是在CPU和编译器的优化技术之后完成的。因此，如果有许多读取线程，我们（和线程）期望观察到的变化顺序可能会变化。这种效果甚至可能在单线程执行期间出现，因为编译器可能会根据内存模型重新排列指令。
- en: Note
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'We encourage you to check out the full information on `memory_order` here:
    https://en.cppreference.com/w/cpp/atomic/memory_order.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励您在此处查看有关`memory_order`的完整信息：https://en.cppreference.com/w/cpp/atomic/memory_order。
- en: 'An important remark is that the default behavior of all atomic operations in
    C++ applies sequentially consistent ordering. The defined memory orders in C++20
    are as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的说明是，C++中所有原子操作默认的行为是应用顺序一致性排序。C++20中定义的内存排序如下：
- en: 'Relaxed ordering, tagged like so:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 松弛排序，标记如下：
- en: '[PRE42]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This ordering is the bare minimum. It is the cheapest option and provides no
    guarantees, except of the current operation’s atomicity. One example of this in
    action is the incrementation of the `shared_ptr` reference counter, as it needs
    to be atomic, but no ordering is required.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种排序是最基本的。这是最便宜的选择，除了当前操作的原子性外，不提供任何保证。一个例子是`shared_ptr`引用计数器的增加，因为它需要是原子的，但不需要排序。
- en: 'Release-acquire ordering, tagged as follows:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布-获取排序，标记如下：
- en: '[PRE43]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Reads and writes are prevented from reordering right after an atomic region
    when the release operation is in effect. The `acquire` operation is similar, but
    reordering is prohibited before the atomic region. The third model, `acq_rel`,
    is a combination of both. This model could really help in the creation of read-write
    locks, except there’s no locking going on. The decrementing of the `shared_ptr`
    reference count is done through this technique as it needs to be synchronized
    with the destructor.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当发布操作生效时，防止读取和写入在原子区域之后重新排序。`acquire`操作类似，但在原子区域之前禁止重新排序。第三种模型`acq_rel`是两者的组合。这种模型在创建读写锁时可能非常有帮助，但因为没有进行锁定操作。`shared_ptr`引用计数的递减是通过这种技术完成的，因为它需要与析构函数同步。
- en: 'Release-consume ordering, tagged as follows:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布-消费排序，标记如下：
- en: '[PRE44]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The `consume` operation’s requirements are still being revised to this day.
    It is designed to work as the `acquire` operation does, but only for specific
    data. That way, the compiler is more flexible in optimizing the code than the
    `acquire` operation. Obviously, getting the data dependencies right makes the
    code more complex, therefore this model is not widely used. You can see it when
    accessing rarely written concurrent data structures – configurations and settings,
    security policies, firewall rules, or publish-subscribe applications with pointer-mediated
    publication; the producer publishes a pointer through which the consumer can access
    information.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`consume`操作的要求至今仍在修订中。它被设计成像`acquire`操作一样工作，但仅针对特定数据。这样，编译器在优化代码时比`acquire`操作更灵活。显然，正确处理数据依赖会使代码更复杂，因此这种模型并不广泛使用。您可以在访问很少写入的并发数据结构时看到它——配置和设置、安全策略、防火墙规则，或者通过指针介导发布的发布-订阅应用程序；生产者通过一个指针发布，消费者可以通过该指针访问信息。'
- en: 'Sequentially consistent ordering, tagged as follows:'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序一致性排序，标记如下：
- en: '[PRE45]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This is the exact opposite of the relaxed order. All operations in and around
    the atomic region follow a strict order. Neither instruction can cross the barrier
    imposed by the atomic operation. It is considered the most expensive model as
    all optimization opportunities are lost. Sequentially consistent ordering is helpful
    for multiple producer-multiple consumer applications, where all consumers must
    observe the actions of all producers occurring in an exact order.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这与松散顺序正好相反。原子区域及其周围的所有操作都遵循严格顺序。没有任何指令可以跨越原子操作强加的障碍。它被认为是最昂贵的模型，因为所有优化机会都丢失了。顺序一致排序对多生产者-多消费者应用程序很有帮助，其中所有消费者都必须按精确顺序观察所有生产者的操作。
- en: One famous example directly benefiting from the memory order is the **spinlock**
    mechanism. We will proceed to examine this in the next section.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一个直接受益于内存顺序的著名例子是**自旋锁**机制。我们将在下一节中继续探讨这个问题。
- en: Designing spinlocks for multiprocessor systems in C++
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在C++中为多处理器系统设计自旋锁
- en: 'Operating systems often use this technique as it’s very efficient for short-period
    operations, including the ability to escape rescheduling and context switching.
    But locks held for longer periods will be at risk of being interrupted by the
    OS scheduler. The **spinlock** means that a given thread will either acquire a
    lock or will wait *spinning* (in a loop) – checking the lock’s availability. We
    discussed a similar example of *busy waiting* earlier in the chapter when we presented
    **cooperative cancellation**. The risk here is that keeping the lock acquired
    for longer periods will put the system into a **livelock** state, as described
    in [*Chapter 2*](B20833_02.xhtml#_idTextAnchor029). The thread holding the lock
    will not progress further by releasing it, and the other threads will remain *spinning*
    while trying to acquire the lock. C++ is well suited for the implementation of
    the spinlock as atomic operations can be configured in detail. In low-level programming,
    this approach is also known as test-and-set. Here’s an example:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统经常使用这种技术，因为它对短期的操作非常有效，包括逃避重新调度和上下文切换的能力。但是，长时间持有的锁可能会被操作系统调度器中断。**自旋锁**意味着一个给定的线程要么获取锁，要么会等待*自旋*（在一个循环中）——检查锁的可用性。我们在本章介绍**协作取消**时讨论了类似的*忙等待*例子。这里的危险是，长时间保持锁将使系统进入**活锁**状态，正如在[*第二章*](B20833_02.xhtml#_idTextAnchor029)中描述的那样。持有锁的线程通过释放它不会进一步进展，而其他线程将保持*自旋*状态，试图获取锁。C++非常适合实现自旋锁，因为原子操作可以详细配置。在底层编程中，这种方法也被称为测试-设置。以下是一个例子：
- en: '[PRE46]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: You’re probably wondering why we aren’t using the already-known synchronization
    techniques. Well, keep in mind that all memory order settings here cost only one
    CPU instruction. They are fast and simple, both software- and hardware-wise. You
    should limit your use of them to very short periods of time, though, since the
    CPU is prevented from doing a useful job for another process.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么我们不使用已经知道的同步技术。好吧，记住，这里所有的内存顺序设置只花费一个CPU指令。它们既快又简单，从软件和硬件的角度来看。尽管如此，你应该限制它们的使用时间非常短，因为CPU被阻止为另一个进程做有用的工作。
- en: An atomic Boolean is used to mark whether the state of `SpinLock` is locked
    or unlocked. The `unlock()` method is easy – when the critical section is released,
    the `false` value is set (`store()` is atomic) to the `state` member through the
    release order. All following read/write operations have to be ordered in an atomic
    manner. The `lock()` method firstly runs a loop, trying to access the critical
    section. The `exchange()` method will set `state` to `true` and will return the
    previous value, `false`, thus interrupting the loop. Logically, this is very similar
    to the semaphore `P(S)` and `V(S)` functions. The inner loop will execute the
    busy wait scenario without order limitations and without producing **cache misses**.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 使用原子布尔值来标记`SpinLock`的状态是锁定还是解锁。`unlock()`方法很简单——当关键部分释放时，通过释放顺序将`false`值（`store()`是原子的）设置到`state`成员。所有后续的读写操作都必须以原子方式排序。`lock()`方法首先运行一个循环，尝试访问关键部分。`exchange()`方法将`state`设置为`true`，并返回先前的值`false`，从而中断循环。从逻辑上讲，这与信号量`P(S)`和`V(S)`函数非常相似。内循环将执行无序限制的忙等待场景，并且不会产生**缓存未命中**。
- en: Important note
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The `store()`, `load()`, and `exchange()` operations have `memory_order` requirements
    and a list of supported orders. Using additional and unexpected orders leads to
    undefined behavior and keeps the CPU busy without doing useful work.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`store()`、`load()`和`exchange()`操作有`memory_order`要求，以及支持的一组顺序。使用额外的和意外的顺序会导致未定义的行为，并使CPU忙碌而不做有用的工作。'
- en: An advanced version of the **spinlock** is the ticket lock algorithm. In the
    same fashion as with queues, tickets are provided to the threads in a FIFO manner.
    That way, the order in which they enter a critical section is managed fairly.
    In contrast with spinlocks, starvation is avoided here. However, this mechanism
    does not scale well. First of all, there’s a greater number of instructions to
    read, test, and acquire the lock, as there are more instructions for managing
    the order. Secondly, as soon as the critical section is free for access, all threads
    must have their context loaded into the cache to determine whether they are allowed
    to acquire the lock and enter the critical section.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**自旋锁**的高级版本是票据锁算法。与队列类似，票据以先进先出的方式提供给线程。这样，它们进入临界区的顺序就可以公平地管理。与自旋锁相比，这里避免了饥饿。然而，这种机制的可扩展性并不好。首先，需要读取、测试和获取锁的指令更多，因为管理顺序的指令也更多。其次，一旦临界区空闲，所有线程的上下文都必须加载到缓存中，以确定它们是否被允许获取锁并进入临界区。'
- en: C++ has an advantage here thanks to its low latency. The full example is available
    at [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其低延迟，C++在这里具有优势。完整的示例可在[https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209)找到。
- en: 'First, we implement the `TicketLock` mechanism, providing the necessary `lock()`
    and `unlock()` methods. We use two helper member variables, `serving` and `next`.
    As you see, they are aligned to be in separate `lock()` and `unlock()` methods
    are implemented as in the `SpinLock` example. Additionally, an atomic increment
    is done through `fetch_add()`, allowing the lock to generate tickets. No read/write
    operations happen around it, so it is executed in a relaxed order. Instead of
    just setting the variable to `false` as with `SpinLock`, the `unlock()` method
    loads a ticket number value, again in a relaxed manner, and stores it as the currently
    served thread:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们实现`TicketLock`机制，提供必要的`lock()`和`unlock()`方法。我们使用两个辅助成员变量，`serving`和`next`。正如你所见，它们被对齐以在单独的`lock()`和`unlock()`方法中实现。此外，通过`fetch_add()`执行原子递增，允许锁生成票据。没有读写操作发生在其周围，因此它以宽松的顺序执行。与`SpinLock`不同，`unlock()`方法以宽松的方式加载一个票据号值，并将其存储为当前服务的线程：
- en: '[PRE47]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The methods for locking and unlocking of the `TicketLock` algorithm follow:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`TicketLock`算法的锁定和解锁方法如下：'
- en: '[PRE48]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, a global `spinlock` object of type `TicketLock` is created. We also create
    a `vector` that plays the role of a shared resource. The `producer()` and `consumer()`
    routines are as expected – the first will create data and the latter will consume
    it, including clearing the shared resource. As both operations will be carried
    out in parallel, the order of their execution is random. If you want instead to
    create a ping-pong-like behavior for this, **conditional variables** or **semaphores**
    could be used as signaling mechanisms. The current implementation is limited just
    to the purposes of the **ticket lock**:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，创建了一个全局的`spinlock`对象，其类型为`TicketLock`。我们还创建了一个`vector`，它充当共享资源。`producer()`和`consumer()`例程如预期的那样——前者将创建数据，后者将消费它，包括清除共享资源。由于这两个操作都将并行执行，它们的执行顺序是随机的。如果你想创建类似乒乓的行为，可以使用**条件变量**或**信号量**作为信号机制。当前的实现仅限于**票据锁**的目的：
- en: '[PRE49]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'And the consumer is similar to what you’ve already learned:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者与你已经学过的类似：
- en: '[PRE50]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Remove the contents of the vector:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 移除向量中的内容：
- en: '[PRE51]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output is as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE52]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output shows that the production and the consumption routines are treated
    as a whole, although they are not called an equal number of times, which is expected.
    As mentioned previously, instead of pausing the threads for `100ms`, you could
    also modify the code by adding a **condition variable**:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示，生产和消费例程被视为一个整体，尽管它们没有被调用相同次数，这是预期的。如前所述，除了暂停线程`100ms`外，你也可以通过添加一个**条件变量**来修改代码：
- en: '[PRE53]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Proceed with the expected critical section:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 继续进行预期的关键部分：
- en: '[PRE54]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: With all of these techniques combined – memory robustness, synchronization primitives,
    cache friendliness, and instruction ordering awareness – you have the instruments
    to really sharpen your code’s performance and tweak it to get the best performance
    on your specific system. We want to take this opportunity to remind you that such
    detailed optimizations could lead to unreadable code and hard debugging, so use
    them only when required.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 结合所有这些技术——内存健壮性、同步原语、缓存友好性和指令排序意识——你拥有了真正提升代码性能和调整以获得特定系统最佳性能的工具。我们想借此机会提醒你，这样的详细优化可能会导致代码难以阅读和调试困难，因此只有在需要时才使用它们。
- en: Summary
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we’ve gathered together the entire set of instruments required
    for optimal code performance with C++. You learned techniques on many different
    system and software levels, so it’s understandable if you want to take a breather
    now. It is true that it would be good to spend more time on some of what we covered,
    for example, **branch predictions** and **cache friendliness**, or to implement
    more algorithms through **condition variables** and memory order. We strongly
    encourage you to use this chapter as a step in the direction of system improvements
    and more efficient work.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们汇集了实现C++代码最佳性能所需的所有工具。你学习了在不同系统和软件层面上的技术，因此你现在想要休息一下是可以理解的。确实，花更多时间在一些我们覆盖的内容上会更好，例如**分支预测**和**缓存友好性**，或者通过**条件变量**和内存顺序实现更多算法。我们强烈建议你将本章作为系统改进和更高效工作的一个步骤。
- en: The next chapter is dedicated to one more significant improvement in C++’s features
    – **coroutines**. You will see that they are much lighter and, for some of the
    mechanisms discussed here, such as event waiting, they are much more preferable.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将专注于C++特性中的一个更多重要改进——**协程**。你会发现它们要轻得多，对于这里讨论的一些机制，如事件等待，它们要更受欢迎得多。
