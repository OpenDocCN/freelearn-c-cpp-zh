- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding the C++ Memory Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is a continuation of the discussion from [*Chapter 7*](B20833_07.xhtml#_idTextAnchor101),
    where we discussed a few multiprocess and multi-threaded techniques; this chapter
    will enhance their usage. We will guide you through various techniques while narrowing
    down to the main focus of the chapter – the C++ memory model. But in order to
    discuss this, you will start first with a brief examination of memory robustness
    through the smart pointer and the optional objects. We will use them later to
    implement *lazy initialization* and handle *shared memory* regions safely. An
    improved memory access analysis of *cache-friendly* code follows. You will learn
    when and why using multi-threaded execution could be a trap, even though you did
    everything right in the software design.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter gives you the opportunity to broaden your understanding of the
    synchronization primitives. While learning about the *condition variables*, you
    will also understand the benefits of the *read-write locks*. We will use the *ranges*
    from C++20 to visualize the same shared data differently. Combining these mechanisms
    one by one, we will finalize our analysis with the biggest topic – instruction
    ordering. Through the C++ *memory order*, you will learn more about the significance
    of the correct atomic routine setup. The *spinlock* implementation will be used
    to summarize all techniques at the end.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know smart pointers and optionals in C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about condition variables, read-write locks, and ranges in C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing multiprocessor systems – cache locality and cache friendliness in
    C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisiting shared resources through the C++ memory model via the spinlock implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to run the code examples, the reader must prepare the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A Linux-based system capable of compiling and executing C++20 (for example,
    **Linux** **Mint 21**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The GCC12.2 compiler: [https://gcc.gnu.org/git/gcc.git gcc-source](https://gcc.gnu.org/git/gcc.gitgcc-source)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the `-std=c++2a`, `-lpthread`, and `-``lrt` flags
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For all the examples, you can alternatively use [https://godbolt.org/](https://godbolt.org/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All code examples in this chapter are available for download from [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting to know smart pointers and optionals in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B20833_04.xhtml#_idTextAnchor060), we revisited the C++ fundamentals
    in order to be on the same page when it comes to the language. One instrument
    that is also considered a *must* is smart pointers. Through these, we are able
    to improve the safety of the program and also manage our resources more effectively.
    And as discussed in the earlier chapters, this is one of our main goals as system
    programmers. Remember the **RAII** principle? Smart pointers are based on this,
    helping the C++ developer reduce and even eliminate *memory leaks*. They could
    also help with shared memory management as you will see later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '*Memory leaks* appear when we allocate memory but fail to free it. This could
    happen not only because we forgot to call the object’s destructor, but also when
    we lose the pointer to that memory address. In addition to these, there are also
    the *wild* and *dangling pointers* to consider as well. The first one happens
    when the pointer is there on the *stack*, but it’s never associated with the real
    object (or address). The second one happens when we free the memory, used by the
    object, but the value of the pointer remains *dangling* around, and we reference
    an already-deleted object. Altogether, these errors can lead not only to **memory
    fragmentation**, but also to **buffer** **overflow** vulnerabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: These issues are hard to catch and reproduce, especially on large systems. System
    programmers and software integration engineers use tools such as address sanitizers,
    static and dynamic code analyzers, and profilers, among others, relying on them
    to predict future defects. But such tools are expensive and consume a lot of computational
    power, so we cannot rely on them constantly for higher code quality. That said,
    what can we do, then? The answer is simple – use smart pointers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can read more on the subject of smart pointers in the standard, or refer
    to [https://en.cppreference.com/w/cpp/memory](https://en.cppreference.com/w/cpp/memory).
  prefs: []
  type: TYPE_NORMAL
- en: Retracing RAII via smart pointers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even experienced C++ developers make mistakes when it comes to the right time
    for memory deallocation. Other languages use garbage collection techniques to
    handle memory management, but it’s important to mention that memory leaks happen
    there as well. Multiple algorithms are implemented for detecting such cases in
    the code but are not always successful. For example, the cycle dependency between
    objects is sometimes difficult to resolve – should two objects pointing to each
    other be deleted, or should they remain allocated? If they remain allocated, does
    this constitute a leak or not? So, it’s our job to be cautious about memory usage.
    In addition, garbage collectors work to free up memory, but do not manage opened
    files, network connections, locks, and so on. To this end, C++ implements its
    own instrument for control – wrapper classes over the pointers, helping us free
    the memory at the right time, usually when the object goes out of scope (the object
    life cycle was discussed already in [*Chapter 4*](B20833_04.xhtml#_idTextAnchor060)).
    Smart pointers are efficient in terms of memory and performance, meaning they
    don’t cost (much) more than raw pointers. At the same time, they give us robustness
    in memory management. There are three types of smart pointers in the C++ standard:'
  prefs: []
  type: TYPE_NORMAL
- en: '`unique_ptr`: This is a pointer that is allowed one owner only. It cannot be
    copied or shared, but the ownership can be transferred. It has the size of a single
    raw pointer. It is destroyed and the object deallocated when it goes out of the
    scope.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shared_ptr`: This can have multiple owners and is destroyed when all owners
    have given up ownership on it or all go out of scope. It uses a reference counter
    to the pointer of an object. Its size is two raw pointers – one for the allocated
    object, and one for the shared control block containing the reference count.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weak_ptr`: This provides access to an object owned by one or more shared pointers,
    but doesn’t count references. It is used for observing an object, but not for
    managing its life cycle. It consists of two pointers – one for the control block,
    and one for pointing to the shared pointer it was constructed from. Through `weak_ptr`
    you can learn whether the underlying `shared_ptr` is still valid – just call the
    `expired()` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s demonstrate their initial roles through the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we use the heap as we call `new` for the creation of the `Book`
    objects. But as the smart pointer handles memory management, we don’t need to
    call the destructor explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: First, we move the ownership of `book1`’s object to another `unique_ptr` – `book1_new`
    (marker `{1}`). We print out its `title` through the second `unique_ptr` as the
    first one is already invalid. We do the same operation for another `Book` object,
    but through a `shared_ptr` object (marker `{2}`). This time the `title` variable
    can be accessed from both pointers. We also print the reference count, and we
    see there are two references to that object.
  prefs: []
  type: TYPE_NORMAL
- en: '`weak_ptr` has useful strengths in system programming, too. You can use `weak_ptr`
    to check for pointer validity. `weak_ptr` could also resolve the issue of cyclic
    dependency between objects. Let’s consider an example of a list node of a doubly
    linked list. The next example illustrates the benefits of `weak_ptr`. This is
    a good time to advise you not to implement such data structures yourself, especially
    when they are already a part of the C++ standard.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s use the `Book` object as content of the `ListNode` `struct`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We also add two member variables for the previous and following nodes, but
    one of them will be `weak_ptr`. One remark is that the `weak_ptr` reference is
    not counted as such in the `shared_ptr` control block. Now, we have both access
    to the objects and the opportunity to count the references to zero with each deallocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'From the output, it’s clear that all objects were removed successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`weak_ptr` is also useful for cache implementation. Think about it – if you
    lose all references to an object, you will lose the object itsel; but with smart
    pointers, it will certainly be destroyed. So, imagine that recently accessed objects
    or objects with higher importance are kept through `shared_ptr` in the current
    code scope. But `weak_ptr` allows us to keep a reference to an object in the same
    scope if we need to reference the object later in that same scope. We would create
    a `weak_ptr` object to it in this case. But imagine that meanwhile, some other
    code scope holds a reference to the object through `shared_ptr`, thus keeping
    it allocated. In other words, we know about the object, but we don’t need to be
    concerned about its management. Thus, that object is accessible if it’s still
    required later, but removed when nothing else needs it. The following diagram
    shows how `shared_ptr` could be incorrectly used on the left-hand side, along
    with the implementation just described on the right-hand side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Cyclic dependency through shared_ptr and resolving through weak_ptr](img/Figure_9.1_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Cyclic dependency through shared_ptr and resolving through weak_ptr
  prefs: []
  type: TYPE_NORMAL
- en: We are not going to dive further into other design solutions where smart pointers
    could come in handy in this section, but we will return to them in the realm of
    system programming later in the chapter. In the next section, we discuss a technique
    that’s the opposite to `weak_ptr`, where we retain the awareness of an object
    that hasn’t been created in memory yet.
  prefs: []
  type: TYPE_NORMAL
- en: Doing a lazy initialization in C++
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Do you play video games? Have you ever seen a missing texture somewhere in the
    graphics while playing? Has a graphical resource appeared suddenly when you moved
    close to it with your character? Have you observed such behavior in other UIs
    as well? If your answers are mostly in the positive, then you have probably encountered
    **lazy initialization** already. It’s easy to figure out that its purpose is to
    postpone the construction of an object until it’s really needed. By doing so,
    we allow the system to allocate the required resources only. We also use it to
    speed up our code, especially if it’s run during high CPU loads, such as at system
    startup. Instead of wasting CPU cycles to create large objects that won’t be needed
    until (much) later, we free up the CPU to handle other requests. On the negative
    side, we might end up failing to load the object on time, as you have likely observed
    in video games. As we discussed in [*Chapter 2*](B20833_02.xhtml#_idTextAnchor029),
    this is also used when a program is loaded, and the kernel allocates virtual memory
    in a lazy fashion – a page of executable code is not loaded until referenced.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with every other pattern, **lazy initialization** cannot solve all of the
    problems. So, the system programmer has to choose whether it should be applied
    for the given application’s functions or not. Usually, it is preferred that parts
    of the graphical and network storage resources remain lazily initialized as they
    are loaded on demand either way. In other words, the user doesn’t see the UI in
    its entirety all the time. Therefore, it’s not required to store it in memory
    a priori. C++ has features that allow us to easily implement this approach. We
    present **lazy initialization** in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We propose a `Settings` class that will help us simulate the loading and updating
    of a list of settings from the disk. Note that we pass it by value and not by
    reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This technique saves time due to reduced loading from memory. In C++, pass-by-value
    (or pass-by-copy) is the default argument passing technique, except for in the
    case of arrays. It is cheap and optimal for small types, such as `int`. Pass-by-reference
    is an alternative to pass-by-value and the `string_view` object is handled in
    the same manner as `int`, using a cheaper copy constructor than other standard
    objects such as `string`. Getting back to our example, we’re creating a configuration
    object, `Config`, which will consist of the settings file (which could be more
    than one file in real-world scenarios) and will allow settings to be changed in
    that configuration. Our `main()` method simulates an application’s startup. The
    `Config` object will be constructed, but the settings file will be loaded only
    when the startup is finished, and the process resources are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We observe that the file is loaded after the startup has finished, as we expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `optional` class template is designed so that functions can return *nothing*
    when they fail, or a valid result when they succeed. We could also use it to handle
    objects whose construction is expensive. It also manages a value that may or may
    not be present at a given time. It is also readable, and its intent is clear.
    If an `optional` object contains a value, the value is guaranteed to be allocated
    as part of the `optional` object, and no dynamic memory allocation happens. Thus,
    an `optional` object models a *reservation* to an object, not a pointer. This
    is a key difference between `optional` and the smart pointer. Although using a
    smart pointer to handle large and complex objects might be a better idea, `optional`
    gives you the opportunity to construct an object at a later point in time when
    all parameters are known, if they weren’t known earlier in the execution. Both
    of them will work well in implementing **lazy initialization** – it’s a matter
    of your preference.
  prefs: []
  type: TYPE_NORMAL
- en: Later in the chapter, we will return to smart pointers and their usability for
    managing shared memory. First, though, we will use the next section to present
    some useful mechanisms for synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about condition variables, read-write locks, and ranges in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now start our discussion of synchronization primitives, a fundamental
    one of which is the **condition variable**. Its purpose is to allow multiple threads
    to remain blocked until an event occurs (i.e., a condition is satisfied). The
    implementation of **condition variables** requires an additional Boolean variable
    to indicate whether the condition is met or not, a *mutex* to serialize the access
    to the Boolean variable, and the condition variable itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'POSIX provides an interface for multiple use cases. Do you remember the producer-consumer
    example in [*Chapter 7*](B20833_07.xhtml#_idTextAnchor101)*, Using Shared Memory*?
    So, `pthread_cond_timedwait()` is used to block a thread for a given period of
    time. Or simply wait for a condition through `pthread_cond_wait ()` and signal
    with `pthread_cond_signal()` to one thread, or `pthread_cond_broadcast()` to all
    threads. Typically, the condition is checked periodically in the scope of a mutex
    lock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If we level up the abstraction, as we did in [*Chapter 7*](B20833_07.xhtml#_idTextAnchor101),
    C++ gives us access to the same technique, but a bit simpler and safer to use
    – we are guarded by the RAII principle. Let’s check the following snippet in C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this form, the code is not correct. There is no condition to be checked,
    and the shared resource itself is missing. We are simply setting the stage for
    the following examples, which are a continuation of what we covered in [*Chapter
    7*](B20833_07.xhtml#_idTextAnchor101). But observe the use of a `{4}`), while
    the first one was waiting (marker `{2}`). As you see, we rely on a *mutex* to
    lock the shared resource in the scope (marker `{1}`) and the condition variable
    is triggered through it in order to continue to work (markers `{2}` and `{3}`).
    Thus, the CPU is not busy waiting, as there’s no endless loop to wait for a condition,
    freeing up access to the CPU for other processes and threads. But the thread remains
    blocked, because the `wait()` method of the **condition variable** unlocks the
    **mutex** and the thread is put to sleep atomically. When the thread is signaled,
    it will be resumed and will re-acquire the **mutex**. This is not always useful
    as you will see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Cooperative cancellation through condition variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An important remark is that the condition variable should wait only with a
    condition and through a predicate. If not, the thread waiting on it will remain
    blocked. Do you remember the thread cancellation example from [*Chapter 6*](B20833_06.xhtml#_idTextAnchor086)?
    We used `jthread` and sent *stop notifications* between threads through the `stop_token`
    class and the `stop_requested` method. This mechanism is known as `jthread` technique
    is considered safe and easy to apply, but it might not be an option for your software
    design, or it might not be enough. Canceling threads could be directly related
    to waiting for an event. In that case, **condition variables** could come in handy
    as no endless loops or polling will be required. Revisiting the thread cancellation
    example from [*Chapter 6*](B20833_06.xhtml#_idTextAnchor086)*, Canceling Threads,
    Is This Really Possible?*, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We are doing polling as the thread worker checks periodically whether the cancellation
    has been sent while doing something else in the meantime. But if the cancellation
    is the only thing we care about, then instead of polling, we could simply *subscribe*
    to the cancellation event using the `stop_requested` function. C++20 allows us
    to define a `stop_callback` function, so together with the condition variable
    and `get_stop_token()`, we can do the cooperative cancellation without endless
    loops:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let’s finish the work from the example in the previous section and add
    a predicate to the **condition variable** in a worker thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: So, the worker thread remains in execution, but the `stopper` thread gets the
    stop token in the `stop_callback` function. When the stop is requested through
    the stopper function, the **condition variable** is signaled through the token.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have another mechanism besides the **semaphore** to signal between
    threads, we can get the **shared memory** back in the game. Let’s see how this
    can work together with the condition variables and smart pointers.
  prefs: []
  type: TYPE_NORMAL
- en: Combining smart pointers, condition variables, and shared memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already explored the concept of **shared memory** in [*Chapter 7*](B20833_07.xhtml#_idTextAnchor101)*,
    Using Shared Memory*. Let’s use the knowledge from the earlier sections in this
    chapter to enhance the code safety through some C++ techniques. We’re simplifying
    the scenario a little bit. The full example can be found at [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209).
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `unique_ptr` argument to provide a specific deallocator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We rely on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you see, we are also using templates in order to provide the possibility
    of storing any type of objects in the **shared memory**. It is easy to keep complex
    objects with large hierarchies and members in the heap, but storing and accessing
    their data is not trivial. Multiple processes will have access to those objects
    in the **shared memory**, but are the processes able to reference the memory behind
    the pointers? If the referenced memory is not in there or the shared virtual address
    space, then a memory access violation exception will be thrown. So, approach this
    with caution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We proceed with the next example. The already-known condition variable technique
    is used, but this time we add a real predicate to wait for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `producer()` method creates and maps the `{1}`). This technique is known
    as `new` operator does these two operations together. Additionally, the object
    itself is wrapped by a `unique_ptr` object with the respective deallocator. As
    soon as the scope is left, that portion of the memory will be reset through the
    `munmap()` method. A **condition variable** is used to signal to the consumer
    that the data has been prepared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `shm` region is created and sized. Now, let us use it to store the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The consumer is implemented similarly, just waiting for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, two threads are started and joined as a producer and consumer to provide
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Of course, the example could be much more complex, adding periodic production
    and consumption. We encourage you to try it out, just using another type of buffer
    – as you may remember, the `string_view` object is a constant. Be sure that the
    deallocator is correctly implemented and called. It is used to make the code safer
    and discard the possibility of memory leaks.
  prefs: []
  type: TYPE_NORMAL
- en: As you may have observed, throughout our work in this book, we often want to
    access an object just to read it, without modifying its data. In that case, we
    don’t need full-scale locking, but something to make a difference between just
    reading data or modifying it. This technique is the *read-write lock* and we present
    it in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing read-write locks and ranges with C++
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: POSIX provides the read-write locks mechanism directly, while C++ hides it under
    different names – `shared_mutex` and `shared_timed_mutex`. Let’s see how it works
    traditionally in POSIX. We have the *read-write lock* object (`rwlock`) with the
    expected POSIX interface, where a thread could hold multiple concurrent read locks
    on it. The goal is to allow multiple readers to access the data until a thread
    decides to modify it. That thread locks the resource through a write lock. Most
    implementations favor the write lock over the read lock in order to avoid write
    starvation. Such behavior is not necessary when it comes to data races, but it
    definitely causes a minimal application execution bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: This is especially true when dealing with large-scale systems’ data readers
    – for example, multiple read-only UIs. The C++ features again give us a simple
    and robust instrument for this task. Therefore, we will not devote time to studying
    examples of POSIX. We advise you to take a look yourself if interested, starting
    with https://linux.die.net/man/3/pthread_rwlock_rdlock.
  prefs: []
  type: TYPE_NORMAL
- en: 'Proceeding with the C++ example, let’s consider the following scenario – a
    small number of threads want to modify a shared resource – a vector of numbers
    – and a larger number of threads wants to visualize the data. What we want to
    use here is `shared_timed_mutex`. It allows two levels of access: *exclusive*,
    where only one thread can own the mutex; and *shared*, where multiple threads
    share ownership of the mutex.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that both the `shared_timed_mutex` and `shared_mutex` types are
    heavier than a simple `mutex`, although `shared_mutex` is considered more efficient
    on some platforms than `shared_timed_mutex`. You’re expected to use them when
    your read operations are really resource-hungry, slow, and frequent. For short
    operation bursts it would be preferable to stick with just the mutex. You’ll need
    to measure your resource usage specifically for your system in order to work out
    which to choose.
  prefs: []
  type: TYPE_NORMAL
- en: The following example illustrates the usage of `shared_mutex`. We’ll also use
    the opportunity to present the `ranges` library in C++. This feature comes with
    C++20 and together with `string_views` provides an agile way to visualize, filter,
    transform, and slice C++ containers, among other things. Through this example,
    you’ll learn about some useful techniques with the `ranges` library, which will
    be explained along with the code. The full example can be found at https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a `Book` struct with a shared resource – `vector` of books. We are
    going to use `shared_mutex` to handle read-write locking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We implement the method for adding a book to the shared resource with the `wr_`
    prefix in order to distinguish its role from the other methods. We also execute
    a write lock on the resource (marker `{1}`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we start with the implementation of multiple reader routines. They are
    marked with the `rd_` prefix, and each of them executes a read lock, meaning that
    the resource will be available for multiple readers at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Observe the `for` loop after marker `{2}`. It not only iterates through the
    shared resource, but with the pipe (|) character we filter out portions of it,
    which is similar to piping and `grep` as covered in [*Chapter 3*](B20833_03.xhtml#_idTextAnchor047),
    except here, it’s not a pipe. We are creating a *range view* through the pipe
    operator, thus providing additional logic to the iteration. In other words, we
    manipulate the view to the container. This approach can be used not only for `vectors`,
    but for the other C++ iterable objects as well. Why? *Ranges* are used to extend
    and generalize the algorithms with iterators so the code becomes tighter and less
    error prone.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s easy to see the intention of the *range* here, too. Additionally, the
    *range view* is a lightweight object, similar to `string_view`. It represents
    an iterable sequence – the *range* itself, created on top of the containers’ iterators.
    It is based on the *Curiously Recurring Template Pattern*. Through the *range*
    interface, we can change the presentation of a container, present its values as
    transformed in a given manner, filter out values, split and combine sequences,
    present unique elements, shuffle elements, slide a window through the values,
    and so on. All of this is done via the simple syntax of already-implemented *range
    adapters*. In our example, `rd_applyYearFilter` has a `for` loop wherein books
    older than `yearKey` are filtered out. We could also print out the shared resource’s
    elements in reverse order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We could even combine views, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous snippet iterates through the elements in reverse order, but it
    also filters out those books where the length of the author’s name is longer than
    a given value. With the next snippet, we demonstrate how to simply drop a portion
    of the container during iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If this is too generic, you could instead use a specific subrange, which will
    create a `range` object. The `range` object can be used like any other, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'With all of this complete, we create threads to execute all of these actions
    in a concurrent manner and see how the *read-write lock* manages them. Running
    the example will produce different output orders depending on the thread’s scheduling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is per the described *range views* (the following has been rearranged
    slightly for easier reading):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: You have now learned about another combination of techniques with which you
    can scale up a system with multiple threads that handle presentation tasks. Let’s
    now take a step back and discuss the possible traps arising from concurrent execution
    that are not directly related to data races. We continue with cache-friendly code.
  prefs: []
  type: TYPE_NORMAL
- en: Discussing multiprocessor systems – cache locality and cache friendliness in
    C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You probably recall [*Chapter 2*](B20833_02.xhtml#_idTextAnchor029) at this
    point, where we discussed multi-thread and multi-core processors. The respective
    computational units were presented as processors. We also visualized the transport
    of instructions from the **NVM** (the disk) to the processors, through which we
    explained the creation of processes and *software* threads.
  prefs: []
  type: TYPE_NORMAL
- en: We want our code to be as performant as required. The most important aspect
    of getting the code to perform well is the choice of appropriate algorithms and
    data structures. With a bit of thought, you can try to squeeze the most out of
    every last CPU cycle. One of the most common examples of misusing algorithms is
    sorting a large, unordered array with bubble sort. So, make sure to learn your
    algorithms and data structures – together with the knowledge from this section
    and beyond, it will make you a really powerful developer.
  prefs: []
  type: TYPE_NORMAL
- en: As you already know, the further we get from the RAM and the closer we get to
    the processor registers, the faster the operations and the smaller the memory
    capacity becomes. Each time the processor loads data from the RAM to the cache,
    it will either just sit and wait for that data to show up, or execute other non-related
    tasks. Thus, from the perspective of the current task, the CPU cycles are wasted.
    Of course, reaching 100% CPU utilization might be impossible, but we should at
    least be aware when it’s doing needless work. All of this might sound meaningless
    to you at this point, but concurrent systems will suffer if we act carelessly.
  prefs: []
  type: TYPE_NORMAL
- en: The C++ language provides access to multiple tools for even better performance
    improvements, including **prefetching mechanisms** through hardware instructions
    and **branch prediction** **optimization**. Even without doing anything in particular,
    modern compilers and CPUs do a great job with these techniques. Still, we could
    improve this performance further by providing the right hints, options, and instructions.
    It’s also a good idea to be aware of the data in the cache to help reduce the
    time taken when accessing it. Remember that the cache is just a type of fast,
    temporary storage for data and instructions. So, we can use the features of C++
    to our advantage when we treat the cache in a good manner, known as **cache-friendly
    code**. An important remark to note is the inverse of this statement – misusing
    C++ features will lead to poor cache performance, or at least not the best performance
    possible. You’ve probably already guessed that this is related to the system’s
    scale and the requirement for fast data access. Let’s discuss this further in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Considering cache locality through cache-friendly code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We mentioned the concept of cache-friendly code already, but what does it truly
    mean? First of all, you need to be aware of the `int` or even an unsigned `char`.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, caching has become a major aspect of almost every system. Earlier
    in the book we mentioned that slower hardware, such as disks, sometimes has its
    own cache memory to reduce the time taken to access frequently opened files. OSs
    can cache frequently used data, for example, files, as chunks of virtual address
    space, thus improving performance even more. This is also known as **temporal
    locality**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following scenario: a piece of data is not found in the cache
    on the first try – this is known as a **cache miss**. Then it is looked up in
    the RAM, is found, and is loaded into the cache as one or multiple **cache blocks**
    or **cache lines**. Afterwards, if this data is requested a number of subsequent
    times and is still found in the cache, known as a **cache hit**, it will remain
    in the cache and guarantee faster access, or at least faster than the first **cache
    miss**. You can observe this in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Representation of temporal locality on the hardware level](img/Figure_9.2_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Representation of temporal locality on the hardware level
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned with the **prefetching mechanisms** earlier, it’s a known fact
    that having an object with multiple **cache hits** means that the data around
    it might also be referenced soon. This causes the processor to *request* or *prefetch*
    that additional nearby data from the RAM and load it a priori, so it will be there
    in the cache when it is eventually needed. This causes **spatial locality**, meaning
    accessing nearby memory and benefiting from the fact that caching is done in chunks,
    known as **cache lines, t**hus paying for a single transfer and using several
    bytes of memory. The prefetching technique assumes that the code already has **spatial
    locality** in order to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: Both locality principles are based on assumptions. But code branching requires
    good design. The simpler the branch tree, the simpler to predict. Again, you need
    to consider carefully the data structures and algorithms to be used. You also
    need to aim at contiguous memory access and reduce the code to simple loops and
    small functions; for example, switching from using linked lists to arrays or matrices.
    For small-sized objects, the `std::vector` container is still the optimal choice.
    Additionally, we ideally seek a data structure object that can fit into one **cache
    line** – but sometimes this is just not possible because of the application’s
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Our process should access the data in contiguous blocks, where each one has
    the size of a cache line (typically 64 bytes but depends on the system). But if
    we want to do parallel evaluations, then it would be preferable for each CPU core
    (processor) to handle data in different **cache lines** from other cores’ data.
    If not, the cache hardware will have to move data back and forth between cores
    and the CPU will waste time on meaningless work again and the performance will
    worsen, instead of being improved. This term is known as **false sharing**, which
    we’ll now have a look at in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: A glance at false sharing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a rule, small pieces of data will be put together in a single **cache line**
    unless the programmer instructs otherwise, as we will see in the following examples.
    This is the way processors work in order to keep latency low – they handle one
    cache line for each core at a time. Even if it’s not full, the **cache line**’s
    size will be allocated as the smallest possible block for the CPU to handle. As
    mentioned earlier, if the data in that **cache line** is requested by two or more
    threads independently, then this will slow down the multi-threaded execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dealing with the effects of **false sharing** means getting predictability.
    Just as code branching can be predicted, so can the system programmer predict
    if an object is of the size of a cache line, and thus each separate object can
    reside in its own memory block. In addition, all computations can happen in the
    local scope and the shared data modifications take place at the end of a given
    procedure. Of course, such activities will lead to the wasting of resources at
    some point, but it’s a matter of design and preferences. Nowadays, we can use
    compiler optimizations to improve this predictability and performance, too, but
    we shouldn’t always rely on this. Let’s first check the size of our cache line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know how to get the `std::atomic` to guarantee a single modifier
    to a shared resource, but we also emphasized that this is not the full picture.
    Let’s enrich the previous example with three atomic variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Printing the addresses out gives the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that even when we have atomic variables, they can be fitted into
    a single `atomic_ref<T>::required_alignment`, which allows the programmer to align
    atomics as per the current cache line size, thus keeping them well apart. Let’s
    apply it for all atomics as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding snippet, you can see that the differences in the addresses
    are as expected and the variables are well aligned, which was always the system
    programmer’s responsibility. Now, let’s apply the `increment()` method that you
    might remember from [*Chapter 7*](B20833_07.xhtml#_idTextAnchor101):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We increment an atomic resource, and as covered in [*Chapter 8*](B20833_08.xhtml#_idTextAnchor116),
    we know how to measure the duration of a procedure. So, we can analyze the performance
    for the next four scenarios. One remark – if you feel so inclined, you could play
    with the compiler optimization levels to spot the difference in the following
    values, as we are not using any of the optimization flags. The full code example
    could be found at [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209).
    Our scenarios are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A single-threaded application, calling `increment()` 3 times, doing 300,000
    increments of an atomic variable, which takes 2,744 microseconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Direct sharing with one atomic variable, incremented 100,000 times by each of
    3 threads in parallel, taking 5,796 microseconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False sharing with three unaligned atomic variables, incremented 100,000 times
    by each of the 3 threads in parallel, taking 3,545 microseconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No sharing with three aligned atomic variables, incremented 100,000 times by
    each of 3 threads in parallel, taking 1,044 microseconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we are not using a benchmarking tool, we cannot measure the number of cache
    misses or hits. We simply do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The **no-sharing** work is presented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Representation of no-sharing (correct sharing) of data on multiple
    cores/threads](img/Figure_9.3_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Representation of no-sharing (correct sharing) of data on multiple
    cores/threads
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It’s obvious that we either have to align our atomic resources before we modify
    them in parallel, or use single-threaded applications for small procedures. The
    time metric could differ, depending on the system and the compiler optimization
    flags. Keep in mind that these speed-ups are great when you get the best out of
    your hardware, but going into so much detail might also lead to complex code,
    harder debugging, and time wasted on maintenance. It’s a balancing act.
  prefs: []
  type: TYPE_NORMAL
- en: False sharing happens during multi-threading and can be fixed if the shared
    object is fitted into one cache line. But what happens if the object is larger
    than one cache line in size?
  prefs: []
  type: TYPE_NORMAL
- en: Sharing resources larger than a cache line in C++
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The analysis here is relatively simple, as it is not so dependent on the language.
    Large objects, representing large data structures, are just... large. They don’t
    fit into single **cache lines** and therefore they are not **cache friendly**
    by nature. Data-oriented design deals with this issue. For example, you could
    think about using smaller objects or share only small parts of them for parallel
    work. Additionally, it is good to think about optimizations in algorithms. Making
    them linear leads to better **branch predictions**. This means making conditional
    statements depend on predictable, not random, data. Complex conditional statements
    can be replaced with arithmetic solutions and templates, or chained differently,
    so it is easier for the CPU to predict which branch has a higher probability of
    occurring. Such operations, again, could lead to unreadable code and complex debugging,
    so proceed with them only when the code is not fast enough for your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: As **branch misprediction** could be expensive and remain well hidden, another
    proposal is the so-called **conditional move**. It is not based on predictions,
    but on data. The data dependencies include both *condition true* and *condition
    false* cases. After an instruction that conditionally moves data from one register
    to another, the contents of the second depend on both their previous values and
    the values from the first register. As mentioned, well-designed branching allows
    better performance. But data dependencies require one or two CPU cycles to arrive,
    sometimes making them a safer bet. A probable trap is when the condition is such
    that the value taken from the memory is not assigned to the register – then it’s
    just meaningless waiting. Luckily for the system programmer, the **conditional
    move** instructions in the instruction sets are typically close register-wise.
  prefs: []
  type: TYPE_NORMAL
- en: '`std::array` and `std::vector`. Yes, the vector could be resized, but it’s
    still cache friendly, as the elements are next to each other in the memory. Of
    course, if you have to reallocate the vector due to constant resizing, then probably
    it’s not the data structure you need. You could consider the `std::deque` container,
    which is efficient for modifications in the middle of the collection, or `std::list`
    as an alternative, which is a linked list and is not cache friendly at all.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the system, many reallocations (construction and destruction) of
    contiguous memory blocks could cause memory fragmentation. This can happen due
    to software algorithms for memory management, language standards, OSs, drivers,
    devices, and so on. It is hard to predict it until it happens. It might take a
    good portion of non-stop execution time for the memory allocations to start failing.
    There could be enough free space in the sum of the free memory blocks in the RAM,
    but not a single block big enough to hold the currently reallocated or created
    contiguous block. Excessive fragmentation could lead to poor performance and even
    denial of service.
  prefs: []
  type: TYPE_NORMAL
- en: A final remark on the topic is that there are many articles discussing optimal
    ways of using C++’s algorithms and containers efficiently. It deserves a book
    on its own and most of the time is very CPU specific – or at least when you get
    to the absolute performance. For example, the **conditional moves** lead directly
    to assembly code, which we don’t have the opportunity to explore here. That said,
    the variety of solutions for different practical problems is enormous when it
    comes to algorithms and data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting shared resources through the C++ memory model via spinlock implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned about atomic operations back in [*Chapter 7*](B20833_07.xhtml#_idTextAnchor101).
    In this chapter, you learned that the placement of atomic variables in the cache
    is crucial as well. Originally, atomics and locks were introduced because of correctness
    when multiple threads want to enter the same critical section. Now, our investigation
    will continue a bit deeper. There’s one last piece of the puzzle of atomic operations.
    Examine the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This was an example of a non-atomic operation. Even when we make it atomic,
    we still don’t have a word about the order of the instructions. Until now, we
    used the synchronization primitives to instruct the CPU about which section of
    instructions has to be taken as a unitary context. What we need now is to instruct
    the processor about the order of those instructions. We do this through C++’s
    `memory_order`, which is a part of the C++ standard memory model.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the memory_order type in C++
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the `memory_order` type, we specify how atomic and non-atomic memory accesses
    are ordered around an atomic operation. The atomic realization of the snippet
    from the preceding section and the example using read-write locks earlier in the
    chapter could both suffer from the same issue: two atomic operations are not atomic
    as a whole. The order of instructions inside the atomic scope will be kept, but
    not around it. This is usually done after optimization techniques in the CPU and
    the compiler. So, if there are many reader threads, the order in which we (and
    the threads) expect to observe changes could vary. Such an effect could appear
    even during single-threaded execution as the compiler might re-arrange instructions
    as allowed by the memory model.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'We encourage you to check out the full information on `memory_order` here:
    https://en.cppreference.com/w/cpp/atomic/memory_order.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An important remark is that the default behavior of all atomic operations in
    C++ applies sequentially consistent ordering. The defined memory orders in C++20
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Relaxed ordering, tagged like so:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This ordering is the bare minimum. It is the cheapest option and provides no
    guarantees, except of the current operation’s atomicity. One example of this in
    action is the incrementation of the `shared_ptr` reference counter, as it needs
    to be atomic, but no ordering is required.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Release-acquire ordering, tagged as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Reads and writes are prevented from reordering right after an atomic region
    when the release operation is in effect. The `acquire` operation is similar, but
    reordering is prohibited before the atomic region. The third model, `acq_rel`,
    is a combination of both. This model could really help in the creation of read-write
    locks, except there’s no locking going on. The decrementing of the `shared_ptr`
    reference count is done through this technique as it needs to be synchronized
    with the destructor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Release-consume ordering, tagged as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `consume` operation’s requirements are still being revised to this day.
    It is designed to work as the `acquire` operation does, but only for specific
    data. That way, the compiler is more flexible in optimizing the code than the
    `acquire` operation. Obviously, getting the data dependencies right makes the
    code more complex, therefore this model is not widely used. You can see it when
    accessing rarely written concurrent data structures – configurations and settings,
    security policies, firewall rules, or publish-subscribe applications with pointer-mediated
    publication; the producer publishes a pointer through which the consumer can access
    information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Sequentially consistent ordering, tagged as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is the exact opposite of the relaxed order. All operations in and around
    the atomic region follow a strict order. Neither instruction can cross the barrier
    imposed by the atomic operation. It is considered the most expensive model as
    all optimization opportunities are lost. Sequentially consistent ordering is helpful
    for multiple producer-multiple consumer applications, where all consumers must
    observe the actions of all producers occurring in an exact order.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One famous example directly benefiting from the memory order is the **spinlock**
    mechanism. We will proceed to examine this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Designing spinlocks for multiprocessor systems in C++
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Operating systems often use this technique as it’s very efficient for short-period
    operations, including the ability to escape rescheduling and context switching.
    But locks held for longer periods will be at risk of being interrupted by the
    OS scheduler. The **spinlock** means that a given thread will either acquire a
    lock or will wait *spinning* (in a loop) – checking the lock’s availability. We
    discussed a similar example of *busy waiting* earlier in the chapter when we presented
    **cooperative cancellation**. The risk here is that keeping the lock acquired
    for longer periods will put the system into a **livelock** state, as described
    in [*Chapter 2*](B20833_02.xhtml#_idTextAnchor029). The thread holding the lock
    will not progress further by releasing it, and the other threads will remain *spinning*
    while trying to acquire the lock. C++ is well suited for the implementation of
    the spinlock as atomic operations can be configured in detail. In low-level programming,
    this approach is also known as test-and-set. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: You’re probably wondering why we aren’t using the already-known synchronization
    techniques. Well, keep in mind that all memory order settings here cost only one
    CPU instruction. They are fast and simple, both software- and hardware-wise. You
    should limit your use of them to very short periods of time, though, since the
    CPU is prevented from doing a useful job for another process.
  prefs: []
  type: TYPE_NORMAL
- en: An atomic Boolean is used to mark whether the state of `SpinLock` is locked
    or unlocked. The `unlock()` method is easy – when the critical section is released,
    the `false` value is set (`store()` is atomic) to the `state` member through the
    release order. All following read/write operations have to be ordered in an atomic
    manner. The `lock()` method firstly runs a loop, trying to access the critical
    section. The `exchange()` method will set `state` to `true` and will return the
    previous value, `false`, thus interrupting the loop. Logically, this is very similar
    to the semaphore `P(S)` and `V(S)` functions. The inner loop will execute the
    busy wait scenario without order limitations and without producing **cache misses**.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The `store()`, `load()`, and `exchange()` operations have `memory_order` requirements
    and a list of supported orders. Using additional and unexpected orders leads to
    undefined behavior and keeps the CPU busy without doing useful work.
  prefs: []
  type: TYPE_NORMAL
- en: An advanced version of the **spinlock** is the ticket lock algorithm. In the
    same fashion as with queues, tickets are provided to the threads in a FIFO manner.
    That way, the order in which they enter a critical section is managed fairly.
    In contrast with spinlocks, starvation is avoided here. However, this mechanism
    does not scale well. First of all, there’s a greater number of instructions to
    read, test, and acquire the lock, as there are more instructions for managing
    the order. Secondly, as soon as the critical section is free for access, all threads
    must have their context loaded into the cache to determine whether they are allowed
    to acquire the lock and enter the critical section.
  prefs: []
  type: TYPE_NORMAL
- en: C++ has an advantage here thanks to its low latency. The full example is available
    at [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%209).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we implement the `TicketLock` mechanism, providing the necessary `lock()`
    and `unlock()` methods. We use two helper member variables, `serving` and `next`.
    As you see, they are aligned to be in separate `lock()` and `unlock()` methods
    are implemented as in the `SpinLock` example. Additionally, an atomic increment
    is done through `fetch_add()`, allowing the lock to generate tickets. No read/write
    operations happen around it, so it is executed in a relaxed order. Instead of
    just setting the variable to `false` as with `SpinLock`, the `unlock()` method
    loads a ticket number value, again in a relaxed manner, and stores it as the currently
    served thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The methods for locking and unlocking of the `TicketLock` algorithm follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, a global `spinlock` object of type `TicketLock` is created. We also create
    a `vector` that plays the role of a shared resource. The `producer()` and `consumer()`
    routines are as expected – the first will create data and the latter will consume
    it, including clearing the shared resource. As both operations will be carried
    out in parallel, the order of their execution is random. If you want instead to
    create a ping-pong-like behavior for this, **conditional variables** or **semaphores**
    could be used as signaling mechanisms. The current implementation is limited just
    to the purposes of the **ticket lock**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'And the consumer is similar to what you’ve already learned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the contents of the vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that the production and the consumption routines are treated
    as a whole, although they are not called an equal number of times, which is expected.
    As mentioned previously, instead of pausing the threads for `100ms`, you could
    also modify the code by adding a **condition variable**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Proceed with the expected critical section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: With all of these techniques combined – memory robustness, synchronization primitives,
    cache friendliness, and instruction ordering awareness – you have the instruments
    to really sharpen your code’s performance and tweak it to get the best performance
    on your specific system. We want to take this opportunity to remind you that such
    detailed optimizations could lead to unreadable code and hard debugging, so use
    them only when required.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve gathered together the entire set of instruments required
    for optimal code performance with C++. You learned techniques on many different
    system and software levels, so it’s understandable if you want to take a breather
    now. It is true that it would be good to spend more time on some of what we covered,
    for example, **branch predictions** and **cache friendliness**, or to implement
    more algorithms through **condition variables** and memory order. We strongly
    encourage you to use this chapter as a step in the direction of system improvements
    and more efficient work.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is dedicated to one more significant improvement in C++’s features
    – **coroutines**. You will see that they are much lighter and, for some of the
    mechanisms discussed here, such as event waiting, they are much more preferable.
  prefs: []
  type: TYPE_NORMAL
