- en: Chapter 13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the course of the next two chapters we are going to talk about *concurrency*
    and the theoretical background that is required for developing concurrent programs,
    not only in C, but necessarily in other languages as well. As such, these two
    chapters won't contain any C code and instead use pseudo-code to represent concurrent
    systems and their intrinsic properties.
  prefs: []
  type: TYPE_NORMAL
- en: The topic of concurrency, due to its length, has been split into two chapters.
    In this chapter we will be looking at the basic concepts regarding concurrency
    itself, before moving to *Chapter 14*, *Synchronization*, where we will discuss
    concurrency-related issues and the *synchronization* mechanisms used in concurrent
    programs to resolve said issues. The collective end goal of these two chapters
    is to provide you with enough theoretical knowledge to proceed with the multithreading
    and multi-processing topics discussed in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The background knowledge we build in this chapter will also be useful when working
    with the *POSIX threading library*, which we use throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this first chapter on concurrency, we will be working on understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: How parallel systems differ from concurrent systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we need concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What a *task scheduler* is, and what the widely used scheduling algorithms are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How a concurrent program is run and what the interleavings are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What a shared state is and how various tasks can access it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start our look into concurrency by giving an introduction to the concept,
    and understanding broadly what it means for us.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency simply means having multiple pieces of logic within a program being
    executed simultaneously. Modern software systems are often concurrent, as programs
    need to run various pieces of logic at the same time. As such, concurrency is something
    that every program today is using to a certain extent.
  prefs: []
  type: TYPE_NORMAL
- en: We can say that concurrency is a powerful tool that lets you write programs
    that can manage different tasks at the same time, and the support for it usually
    lies in the kernel, which is at the heart of the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous examples in which an ordinary program manages multiple jobs
    simultaneously. For example, you can surf the web while downloading files. In
    this case, tasks are being executed in the context of the browser process concurrently.
    Another notable example is in a *video streaming* scenario, such as when you are
    watching a video on YouTube. The video player might be in the middle of downloading
    future chunks of the video while you are still watching previously downloaded
    chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Even simple word-processing software has several concurrent tasks running in
    the background. As I write this chapter on Microsoft Word, a spell checker and
    a formatter are running in the background. If you were to be reading this on the
    Kindle application on an iPad, what programs do you think might be running concurrently
    as part of the Kindle program?
  prefs: []
  type: TYPE_NORMAL
- en: Having multiple programs being run at the same time sounds amazing, but as with
    most technology, concurrency brings along with it several headaches in addition
    to its benefits. Indeed, concurrency brings some of the most painful headaches
    in the history of computer science! These "headaches," which we will address later
    on, can remain hidden for a long time, even for months after a release, and they
    are usually hard to find, reproduce, and resolve.
  prefs: []
  type: TYPE_NORMAL
- en: We started this section describing concurrency as having tasks being executed
    at the same time, or concurrently. This description implies that the tasks are
    being run in parallel, but that's not strictly true. Such a description is too
    simple, as well as inaccurate, because *being concurrent is different from being
    parallel*, and we have not yet explained the differences between the two. Two
    concurrent programs are different from two parallel programs, and one of our goals
    in this chapter is to shine a light on these differences and give some definitions
    used by the official literature in this field.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we are going to explain some basic concurrency-related
    concepts such as *tasks*, *scheduling*, *interleavings*, *state*, and *shared
    state*, which are some of the terms you will come across frequently in this book.
    It's worth pointing out that most of these concepts are abstract and can be applied
    to any concurrent system, not just in C.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the difference between parallel and concurrent, we are going to
    briefly touch upon parallel systems.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this chapter we stick to simple definitions. Our sole purpose is
    to give you a basic idea of how concurrent systems work, as going beyond this
    would be outside of the scope of this book on C.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallelism simply means having two tasks run at the same time, or *in parallel*.
    The phrase "in parallel" is the key element that differentiates parallelism from
    concurrency. Why is this? Because parallel implies that two things are happening
    simultaneously. This is not the case in a concurrent system; in concurrent systems,
    you need to pause one task in order to let another continue execution. Note that
    this definition can be too simple and incomplete regarding the modern concurrent
    systems, but it is sufficient for us to give you a basic idea.
  prefs: []
  type: TYPE_NORMAL
- en: We meet parallelism regularly in our daily lives. When you and your friend are
    doing two separate tasks simultaneously, those tasks are being done in parallel.
    To have a number of tasks in parallel, we need separate and isolated *processing
    units*, each of which is assigned to a certain task. For instance, in a computer
    system, each *CPU core* is a processor unit that can handle one task at a time.
  prefs: []
  type: TYPE_NORMAL
- en: For a minute, look at yourself as the sole reader of this book. You cannot read
    two books in parallel; you would have to pause in reading one of them in order
    to read the other. Yet, if you added your friend into the mix, then it's possible
    for two books to be read in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: What would happen if you had a third book that needed to be read? Since neither
    of you can read two books in parallel, then one of you would need to pause in
    reading your book to continue with the third one. This simply means that either
    you or your friend need to divide your time properly in order to read all three books.
  prefs: []
  type: TYPE_NORMAL
- en: In a computer system, there must be at least two separate and independent processing
    units in order to have two parallel tasks being executed on that system. Modern
    CPUs have a number of *cores* inside, and those cores are the actual processing
    units. For example, a 4-core CPU has 4 processing units, and therefore can support
    4 parallel tasks being run simultaneously. For simplicity, in this chapter we
    will suppose that our imaginary CPU has only one core inside and therefore cannot
    perform parallel tasks. There will be some discussion regarding multi-core CPUs
    later, within relevant sections.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you get two laptops with our imaginary CPU inside, with one playing
    a piece of music, and the other one finding the solution to a differential equation.
    Both of them are functioning in parallel, but if you want them to do both on the
    same laptop using only one CPU, and with one core, then it *cannot* be parallel
    and it is in fact concurrent.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism is about tasks that can be parallelized. This means that the actual
    algorithm can be divided and run on multiple processor units. But most of the
    algorithms we write, as of today, are *sequential* and not parallel in nature.
    Even in multithreading, each thread has a number of sequential instructions that
    cannot be broken into some parallel *execution flows*.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, a sequential algorithm cannot be easily broken into some parallel
    flows of execution automatically by the operating system, and this should be done
    by a programmer. Therefore, with having a multi-core CPU, you still need to assign
    each of the execution flows to a certain CPU core, and in that core, if you have
    more than one flow assigned, you cannot have both of them running in parallel,
    and you immediately observe a concurrent behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In short, of course having two flows, each assigned to a different core, can
    end up in two parallel flows but assigning them to just one core, would result
    in two concurrent flows. In multi-core CPUs we effectively observe a mixed behavior,
    both parallelism between the cores, and concurrency on the same core.
  prefs: []
  type: TYPE_NORMAL
- en: Despite its simple meaning and numerous everyday examples, parallelism is a complex
    and tough topic in computer architecture. In fact, it is a separate academic subject
    from concurrency, with its own theories, books, and literature. Being able to
    have an operating system that can break a sequential algorithm into some parallel
    execution flows is an open field of research and the current operating systems
    cannot do that.
  prefs: []
  type: TYPE_NORMAL
- en: As stated, the purpose of this chapter is not to go into any depth in parallelism,
    but only to provide an initial definition for the concept. Since further depth
    of discussion about parallelism is beyond the scope of this book, let's begin
    with the concept of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we'll talk about concurrent systems and what it really means in comparison
    to parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may have heard about *multitasking* – well, concurrency has the same idea.
    If your system is managing multiple tasks at the same time, you need to understand
    that it does not necessarily mean that the tasks are being run in parallel. Instead,
    there can be a *task scheduler* in the middle; this simply switches very quickly
    between the different tasks and performs a tiny bit of each of them in a fairly
    small amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: This certainly happens when you have just one processor unit. For the rest of
    our discussion in this section, we assume that we are operating on just one processor
    unit.
  prefs: []
  type: TYPE_NORMAL
- en: If a task scheduler is sufficiently *fast* and *fair*, you won't notice the
    *switching* between the tasks, and they'll appear to be running in parallel from
    your perspective. That's the magic of concurrency, and the very reason why it
    is being used in most of the widely known operating systems, including Linux,
    macOS, and Microsoft Windows.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency could be seen as a simulation of performing tasks in parallel, using
    a single processor unit. In fact, the whole idea can be referred to as a form
    of artificial parallelism. For old systems that only had a single CPU, with only
    one core, it was a huge advance when people were able to use that single core
    in a multitasking fashion.
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, *Multics* was one of the first operating systems designed to
    multitask and manage simultaneous processes. You'll remember that in *Chapter
    10*, *Unix – History and Architecture*, Unix was built based on the ideas gained
    from the Multics project.
  prefs: []
  type: TYPE_NORMAL
- en: As we've explained previously, almost all operating systems can perform concurrent
    tasks through multitasking, especially POSIX-compliant operating systems, since
    the ability is clearly exposed in the POSIX standard.
  prefs: []
  type: TYPE_NORMAL
- en: Task scheduler unit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we've said before, all multitasking operating systems are required to have
    a *task scheduler* unit, or simply a *scheduler unit*, in their kernel. In this
    section, we're going to see how this unit works and how it contributes to the
    seamless execution of some concurrent tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some facts regarding the task scheduler unit are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The scheduler has a *queue* for tasks waiting to be executed. *Tasks* or *jobs*
    are simply the pieces of work that should be performed in separate flows of execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This queue is usually *prioritized*, with the high-priority tasks being chosen
    to start first.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The processor unit is managed and shared among all the tasks by the task scheduler.
    When the processor unit is free (no task is using it), the task scheduler must
    select another task from its queue before letting it use the processor unit. When
    the task is finished, it releases the processor unit and make it available again,
    then the task scheduler selects another task. This goes on in a continuous loop.
    This is called *task scheduling*, and it is the sole responsibility of the task
    scheduler to do this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many *scheduling algorithms* that the task scheduler can operate,
    but all of them should address specific requirements. For example, all of them
    should be *fair*, and no task should be *starved* in the queue as a result of
    not being chosen for a prolonged period of time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on a chosen *scheduling strategy*, the scheduler should either dedicate
    a specific *time slice* or *time quantum* to the task in order to use the processor
    unit, or alternatively, the scheduler must wait for the task to release the processor
    unit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the scheduling strategy is *preemptive*, the scheduler should be able to
    forcefully take back the CPU core from the running task in order to give it to
    the next task. This is called *preemptive scheduling*. There is also another scheme
    in which the task releases the CPU voluntarily, which is called *cooperative scheduling*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preemptive scheduling algorithms try to share *time slices* evenly and fairly
    between different tasks. Prioritized tasks may get chosen more frequently, or
    they may even get longer time slices depending upon the implementation of the
    scheduler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A task is a general abstract concept, used to refer to any piece of work that
    should be done in a concurrent system, not necessarily a computer system. We'll
    look at what exactly these non-computer systems are shortly. Likewise, CPUs are
    not the only type of resource that can be shared between tasks. Humans have been
    scheduling and prioritizing tasks for as long as we have existed, when we are
    faced with tasks that we cannot complete simultaneously. In the next few paragraphs,
    we will consider such a situation as a good example for understanding scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose that we are at the beginning of the twentieth century and there
    is only one telephone booth in the street, and 10 people are waiting to use the
    telephone. In this case, these 10 people should follow a scheduling algorithm
    in order to share the booth fairly between themselves.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, they need to stand in a queue. This is the most basic decision
    that enters the civilized mind in such a situation – to stand in the queue and
    wait for your turn. However, this alone is not enough; we also need some regulations
    to support this method. The first person, who is currently using the phone, can't
    talk as much as they might like to when there are nine other people waiting for
    the booth. The first person must leave the booth after a certain amount of time
    in order to allow the next person in the queue their turn.
  prefs: []
  type: TYPE_NORMAL
- en: In the rare case that they have not finished their conversation yet, the first
    person should stop using the phone after a certain amount of time, leave the booth,
    and go back to the end of the queue. They must then wait for their next turn so
    that they can continue their talk. This way, each of the 10 people will need to
    continue entering the booth, until they have completed their conversation.
  prefs: []
  type: TYPE_NORMAL
- en: This is just an example. We encounter examples of sharing resources between
    a number of consumers every day, and humans have invented many ways to share these
    resources fairly between themselves – to the extent that human nature allows!
    In the next section, we return to considering scheduling within the context of
    a computer system.
  prefs: []
  type: TYPE_NORMAL
- en: Processes and threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this book, we are mainly interested in task scheduling within computer
    systems. In an operating system, tasks are either *processes* or *threads*. We''ll
    explain them and their differences in the upcoming chapters, but for now, you
    should know that most operating systems treat both in basically the same way:
    as some tasks that need to be executed concurrently.'
  prefs: []
  type: TYPE_NORMAL
- en: An operating system needs to use a task scheduler to share the CPU cores among
    the many tasks, be they processes or threads, that are willing to use the CPU
    for their execution. When a new process or a new thread is created, it enters
    the scheduler queue as a new task, and it waits to obtain a CPU core before it
    starts running.
  prefs: []
  type: TYPE_NORMAL
- en: In cases in which a *time-sharing* or *preemptive scheduler* is in place, if
    the task cannot finish its logic in a certain amount of time, then the CPU core
    will be taken back forcefully by the task scheduler and the task enters the queue
    again, just like in the telephone booth scenario.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the task should wait in the queue until it obtains the CPU core
    once more, and then it can continue running. If it cannot finish its logic in
    the second round, the same process continues until it is able to finish.
  prefs: []
  type: TYPE_NORMAL
- en: Every time a preemptive scheduler stops a process in the middle of running and puts
    another process into the running state, it is said that a *context switch* has occurred.
    The faster the context switches are, the more a user will feel as if the tasks
    are being run in parallel. Interestingly, most operating systems today use a preemptive
    scheduler, something that will be our main focus for the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: From now on, all schedulers are assumed to be preemptive. I will specify in
    instances where this is not the case.
  prefs: []
  type: TYPE_NORMAL
- en: When a task is running, it may experience hundreds or even thousands of context
    switches before being finished. However, context switches have a very bizarre
    and unique characteristic – they are not *predictable*. In other words, we are
    not able to predict when, or even at which instruction, a context switch is going
    to happen. Even in two remarkably close successive runs of a program on the same
    platform, the context switches will happen differently.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of this, and the impact it has, cannot be overstated; context
    switches cannot be predicted! Shortly, through the given examples, you'll observe
    the consequences of this for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Context switches are highly unpredictable, to such an extent that the best way
    to deal with this uncertainty is to assume that the probability of having a context
    switch on a specific instruction is the same for all instructions. In other words,
    you should expect that all instructions are subject to experiencing a context
    switch in any given run. What this means, simply, is that you may have gaps between
    the execution of any two adjacent instructions.
  prefs: []
  type: TYPE_NORMAL
- en: With that being said, let's now move on and take a look at the only certainties
    that do exist in a concurrent environment.
  prefs: []
  type: TYPE_NORMAL
- en: Happens-before constraint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We established in the previous section that context switches are not predictable;
    there is uncertainty about the time at which they are likely to occur in our programs.
    Despite that, there is certainty about the instructions that are being executed
    concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue with a simple example. To start with, we''re going to work
    on the basis that we''ve got a task like the one you see next in *Code Box 13-1*,
    which has five instructions. Note that these instructions are abstract, and they
    don''t represent any real instructions like C or machine instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Code Box 13-1: A simple task with 5 instructions'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the instructions are ordered, which means that they *must* be
    executed in that specified order in order to satisfy the purpose of the task.
    We are certain about this. In technical terms, we say that we have a *happens-before
    constraint* between every two adjacent instructions. The instruction `num++` must
    happen before `num = num - 2` and this constraint must be kept satisfied no matter
    how the context switches are happening.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we still have uncertainty about when the context switches are going
    to happen; it's key to remember that they can happen anywhere between the instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are going to present two possible executions of the preceding task,
    with different context switches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Code Box 13-2: One possible run of the above task together with the context
    switches'
  prefs: []
  type: TYPE_NORMAL
- en: 'And for the second run, it is executed as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Code Box 13-3: Another possible run together with the context switches'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Code Box 13-2*, the number of context switches and the places
    they occur can both change in each run. Yet, as we said before, there are certain
    happens-before constraints that should be followed.
  prefs: []
  type: TYPE_NORMAL
- en: This is the reason we can have an overall deterministic behavior for a specific
    task. No matter how context switches happen in different runs, the *overall state*
    of a task remains the same. By the overall state of a task, we mean the set of
    variables and their corresponding values after the execution of the last instruction
    in the task. For example, for the preceding task, we always have the final state,
    including the `num` variables with a value of `14`, and the variable `x` with
    a value of `10`, regardless of the context switches.
  prefs: []
  type: TYPE_NORMAL
- en: By knowing that the overall state of a single task does not change in different
    runs, we might be tempted to conclude that due to having to follow the order of
    execution and the happens-before constraints, concurrency cannot affect the overall
    state of a task. Yet, we should be careful about this conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume that we have a system of concurrent tasks, all having read/write
    permissions over a *shared resource*, say a variable. If all the tasks only read
    the shared variable and none of them are going to write to it (change its value),
    we can say that no matter how context switches are happening, and no matter how
    many times you run the tasks, we always get the same results. Note that this is
    also true about a system of concurrent tasks that have no shared variable at all.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, if just one of the tasks is going to write to the shared variable, then
    the context switches imposed by the task scheduler unit will affect the overall
    state of all tasks. This means that it can be different from one run to another!
    Consequently, a proper control mechanism should be employed to avoid any unwanted
    results. This is all due to the fact that context switches cannot be predicted,
    and the tasks' *intermediate states* can vary from one run to another. An intermediate
    state, as opposed to overall state, is a set of variables together with their
    values at a certain instruction. Every task has only one overall state that is
    determined when it is finished, but it has numerous intermediate states that correspond
    to the variables and their values after executing a certain instruction.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, when you have a concurrent system containing several tasks with
    a shared resource that can be written to by any of those tasks, then different
    runs of the system will yield different results. Hence, proper *synchronization*
    methods should be used in order to cancel the effect of context switches and obtain
    the same deterministic results in various runs.
  prefs: []
  type: TYPE_NORMAL
- en: We now have some of the basic concepts of concurrency, which is the dominant
    topic of this chapter. The concepts explained in this section are fundamental
    to our understanding of many topics, and you will hear them again and again in
    future sections and chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: You'll remember that we also said concurrency could be problematic and in turn,
    it can make things more complicated for us. So, you may be asking, when do we
    need it? In the next section of this chapter, we'll answer that question.
  prefs: []
  type: TYPE_NORMAL
- en: When to use concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Based on our explanations given so far, it seems that having only one task is
    less problematic than having multiple tasks do the same thing concurrently. This
    is quite right; if you can write a program that runs acceptably without introducing
    concurrency, it is highly recommended that you do so. There are some general patterns
    we can use to know when we have to use concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to walk through what these general patterns are,
    and how they lead us to split a program into concurrent flows.
  prefs: []
  type: TYPE_NORMAL
- en: A program, regardless of the programming language used, is simply a set of instructions
    that should be executed in sequence. In other words, a given instruction won't
    be executed until the preceding instruction has been executed. We call this concept
    a *sequential execution*. It doesn't matter how long the current instruction takes
    to finish; the next instruction must wait until the current one has been completed.
    It is usually said that the current instruction is *blocking* the next instruction;
    this is sometimes described as the current instruction being a *blocking instruction*.
  prefs: []
  type: TYPE_NORMAL
- en: In every program, all of the instructions are blocking, and the execution flow
    is sequential in each flow of execution. We can only say a program is running
    quickly if each instruction blocks the following instruction for a relatively
    short time in terms of a few milliseconds. Yet, what happens if a blocking instruction
    takes too much time (for example 2 seconds or 2000 milliseconds), or the time
    that it takes cannot be determined? These are two patterns that tell us we need
    to have a concurrent program.
  prefs: []
  type: TYPE_NORMAL
- en: To elaborate further, every blocking instruction consumes an amount of time
    when trying to get completed. For us, the best scenario is that a given instruction
    takes a relatively short time to complete and after that, the next instruction
    can be executed immediately. However, we are not always so fortunate.
  prefs: []
  type: TYPE_NORMAL
- en: There are certain scenarios where we cannot determine the time that a blocking
    instruction takes to complete. This usually happens when a blocking instruction
    is waiting either for a certain event to occur, or for some data to become available.
  prefs: []
  type: TYPE_NORMAL
- en: Let's continue with an example. Suppose that we have a server program that is
    serving a number of client programs. There is an instruction in the server program
    that waits for a client program to get connected. From the server program's point
    of view, no one can say for sure when a new client is about to connect. Therefore,
    the next instruction cannot be executed on the server side because we don't know
    when we will be done with the current one. It depends entirely on the time at
    which a new client tries to connect.
  prefs: []
  type: TYPE_NORMAL
- en: A simpler example is when you read a string from the user. From the program's
    point of view, no one can say for sure when the user will enter their input; hence,
    future instructions cannot be executed. This is the *first pattern* that leads
    to a concurrent system of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The first pattern for concurrency then, is when you have an instruction that
    can block the flow of execution for an indefinite amount of time. At this point
    you should split the existing flow into two separate flows or tasks. You would
    do this if you need to have the later instructions being executed, and you cannot
    wait for the current instruction to complete first. More importantly for this
    scenario, we assume that the later instructions are not dependent on the result
    of the current instructions being completed.
  prefs: []
  type: TYPE_NORMAL
- en: By splitting our preceding flow into two concurrent tasks, while one of the
    tasks is waiting for the blocking instruction to complete, the other task can
    continue and execute those instructions that were blocked in the preceding non-concurrent
    setup.
  prefs: []
  type: TYPE_NORMAL
- en: The following example that we're going to focus on in this section shows how
    the first pattern can result in a system of concurrent tasks. We will be using
    pseudo-code to represent the instructions in each task.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**:'
  prefs: []
  type: TYPE_NORMAL
- en: No prior knowledge of computer networks is needed to understand the upcoming
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example we''re going to focus on is about a server program that has three objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: It calculates the sum of two numbers read from a client and returns the result
    back to the client.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It writes the number of served clients to a file regularly, regardless of whether
    any client is being served or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It must also be able to serve multiple clients at once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before talking about the final concurrent system that satisfies preceding objectives,
    let''s first suppose that in this example we are going to use only one task (or
    flow) and then we are going to show that a single task cannot accomplish the preceding
    objectives. You can see the pseudo-code for the server program, in a single-task
    setup, in *Code Box 13-4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Code Box 13-4: A server program operating using a single task'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, our single flow waits for a client on the network to get connected.
    It then reads two numbers from the client, then calculates their sum and returns
    it to the client. Finally, it closes the client connection and writes the number
    of served clients to a file before continuing to wait for the next client to join
    in. Shortly, we'll show that the preceding code cannot satisfy our aforementioned
    objectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'This pseudo-code contains only one task, `T1`. It has 12 lines of instructions,
    and as we''ve said before, they are executed sequentially, and all the instructions
    are blocking. So, what exactly is this code showing us? Let''s walk through it:'
  prefs: []
  type: TYPE_NORMAL
- en: The first instruction, `N = 0`, is simple and finishes very quickly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second instruction, `Prepare Server`, is expected to finish in a reasonable
    time so that it won't block the execution of the server program.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third instruction is just starting the main loop and it should finish quickly
    as we proceed to go inside the loop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth command, `Wait for a client C`, is a blocking instruction with an
    unknown completion time. Therefore, commands *5*, *6*, and the rest won't be executed.
    Hence, it seems that they must wait for a new client to join in, and only after
    that, these instructions can be executed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we said before, having instructions *5* to *10* wait for a new client is
    must. In other words, those instructions are dependent on the output of instruction
    *4* and they cannot be executed without having a client accepted. However, instruction
    *11*, `Write N to file`, needs to be executed regardless of having a client or
    not. This is dictated by the second objective that we've defined for this example.
    By the preceding configuration, we write `N` to file only if we have a client,
    despite this being against our initial requirement, that is, we write `N` to file
    *regardless* of whether we have a client or not.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code has another problem in its flow of instructions; both instructions
    *6* and *7* can potentially block the flow of execution. These instructions wait
    for the client to enter two numbers, and since this is up to the client we cannot
    predict exactly when these instructions are going to finish. This prevents the
    program from continuing its execution.
  prefs: []
  type: TYPE_NORMAL
- en: More than that, these instructions potentially block the program from accepting
    new clients. This is because the flow of executions won't reach the command *4* again,
    if commands *6* and *7* are going to take a long time to complete. Therefore,
    the server program cannot serve multiple clients at once, which is again not in
    accordance with our defined objectives.
  prefs: []
  type: TYPE_NORMAL
- en: To resolve the aforementioned issues, we need to break our single task into
    three concurrent tasks that together will satisfy our requirements for the server program.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following pseudo-code in *Code Box 13-5*, you will find three flows
    of execution, `T1`, `T2`, and `T3`, that satisfy our defined objectives based
    on a concurrent solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Code Box 13-5: A server program operating using three concurrent tasks'
  prefs: []
  type: TYPE_NORMAL
- en: The program starts by executing task `T1`. `T1` is said to be the main task
    of the program because it is the first task that is going to be executed. Take
    note that each program has at least one task and that all other tasks are initiated
    by this task, either directly or indirectly.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code box, we have two other tasks that are spawned by the main
    task, `T1`. There is also a shared variable, `N`, which stores the number of served
    clients and can be accessed (read or written) by all the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The program starts with the first instruction in task `T1`; through this, it
    initializes the variable `N` to zero. Then the second instruction prepares the
    server. As part of this instruction, some preliminary steps should be taken in
    order for the server program to be able to accept the incoming connections. Note
    that so far there hasn't been any other concurrent task running next to task `T1`.
  prefs: []
  type: TYPE_NORMAL
- en: The third instruction in task `T1` creates a new *instance* of task `T2`. The
    creation of a new task is usually fast and takes no time. Therefore, task `T1`
    enters the infinite loop immediately after the creation of task `T2`, where it
    continues to write the value of the shared variable `N` to a file every 30 seconds.
    This was our first objective defined for the server program that has now been
    satisfied. Based on that, without having any interruption or blockage from other
    instructions, task `T1` writes the value of `N` to a file regularly, until the
    program finishes.
  prefs: []
  type: TYPE_NORMAL
- en: Let's talk about the spawned task. The sole responsibility of task `T2` is to
    accept the incoming clients as soon as they send the connection request. It's
    also worth remembering that all the instructions in task `T2` are run inside an
    infinite loop. The second command in task `T2` waits for a new client. Here, it
    blocks other instructions in task `T2` from executing, but this is only applied
    to the instructions in task `T2`. Note that if we had spawned two instances of
    task `T2` instead of one, having instructions blocked in one of them would not
    block the instructions in the other instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other concurrent tasks, in this case only `T1`, continue to execute their instructions
    without any blockage. This is what concurrency enables; while some tasks are blocked
    for a certain event, other tasks can continue their work without any interruption.
    As we said before, this has an important design principle at its core: *Whenever
    you have a blocking operation where either its finishing time is unknown, or it
    takes a long time to complete, then you should break the task into two concurrent
    tasks*.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, suppose that a new client joins. We've already seen in *Code Box 13-4*,
    in the non-concurrent version of the server program, that the read operations
    could block the acceptance of new clients. Based on the design principle that
    we pointed out just now, since the read instructions are blocking, we need to
    break the logic into two concurrent tasks, which is why we have introduced task
    `T3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whenever a new client joins, task `T2` spawns a new instance of task `T3` in
    order to communicate with the newly joined client. This was done by instruction
    *4* in task `T2`, which, to remind you, was the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Code Box 13-6: Instruction 4 in task T2'
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that before spawning a new task, task `T2` increments
    the value of the shared variable `N` as an indication of having a new client served.
    Again, a spawn instruction is fairly quick and doesn't block the acceptance of
    new clients.
  prefs: []
  type: TYPE_NORMAL
- en: In task `T2`, when instruction *4* is finished, the loop continues, and it goes
    back to instruction *2*, which waits for another client to join. Note that based
    on the pseudo-code that we have, while we have only one instance of task `T1`
    and one instance of task `T2`, we can have multiple instances of `T3` for every
    client.
  prefs: []
  type: TYPE_NORMAL
- en: The sole responsibility of task `T3` is to communicate to the client and read
    the input numbers. It then continues by calculating the sum and sending it back
    to the client. As pointed out before, the blocking instructions inside task `T3`
    cannot block the execution of other tasks, and its blocking behavior is limited
    to the same instance of `T3`. Even the blocking instructions in a specific instance
    of `T3` cannot block the instructions in another instance of `T3`. This way, the
    server program can satisfy all of our desired objectives in a concurrent way.
  prefs: []
  type: TYPE_NORMAL
- en: So, the next question might be, when do the tasks finish? We know that generally,
    when all the instructions within a task are executed, the task is finished. That
    being said, when we have an infinite loop wrapping all instructions inside a task,
    the task won't finish, and its lifetime is dependent on its *parent task* that
    has spawned it. We will discuss this specifically regarding processes and threads
    in future chapters. For the sake of our example, in our preceding concurrent program
    the parent task of all instances of `T3` is the only instance of task `T2`. As
    you can see, a specific instance of task `T3` finishes either when it closes the
    connection to the client after passing two blocking read instructions or when
    the only instance of task `T2` is finished.
  prefs: []
  type: TYPE_NORMAL
- en: In a rare but possible scenario, if all read operations take too much time to
    complete (and this can be either intentional or accidental), and the number of
    incoming clients increases rapidly, then there should be a moment where we have
    too many instances of task `T3` running and all of them are waiting for their
    clients to provide their input numbers. This situation would result in consuming
    a considerable amount of resources. Then, after some time, by having more and
    more incoming connections, either the server program would be terminated by the
    operating system, or it simply cannot serve any more clients.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever happens in the preceding case, the server program ceases to serve the
    clients. When this occurs, it's called a **denial of service** (**DoS**). Systems
    with concurrent tasks should be designed in such a way to overcome these extreme
    situations that stop them from serving clients in a reasonable fashion.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**:'
  prefs: []
  type: TYPE_NORMAL
- en: When under a DoS attack, congestion of resources on a server machine occurs
    in order to bring it down and make it non-responsive. DoS attacks belong to a
    group of network attacks that try to block a certain service in order to make
    it unavailable to its clients. They cover a wide range of attacks, including *exploits*,
    with the intention of stopping a service. This can even include the *flooding*
    of a network in order to bring down the network infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example of the server program, we described a situation in
    which we had a blocking instruction whose completion time could not be determined,
    and this was the first pattern for the use of concurrency. There is another pattern
    that is similar to this, but slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: If an instruction or a group of instructions take too much time to complete,
    then we can put them in a separate task and run the new task concurrent to the
    main task. This is different from the first pattern because, while we do have
    an estimate of the completion time, albeit not a very accurate one, we do know
    that it won't be soon.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing to note about the preceding example, regarding the shared variable,
    `N`, is that one of the tasks, specifically the instance of task `T2`, could change
    its value. Based on our previous discussions in this chapter, this system of concurrent
    tasks is therefore prone to concurrency problems because of it having a shared
    variable that can be modified by one of the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that the solution we proposed for the server program
    is far from perfect. In the next chapter, you'll be introduced to concurrency
    issues, and through it you will see that the preceding example suffers from a
    serious *data race* issue over the shared variable, `N`. As a result, proper control
    mechanisms should be employed to resolve the issues created by concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: In the following and final section in this chapter, we are going to talk about
    the *states* that are shared between some concurrent tasks. We will also introduce
    the concept of *interleaving* and its important consequences for a concurrent
    system with a modifiable shared state.
  prefs: []
  type: TYPE_NORMAL
- en: Shared states
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we talked about the patterns suggesting that we require
    a concurrent system of tasks. Before that, we also briefly explained how the uncertainty
    in the pattern of context switches during the execution of a number of concurrent
    tasks, together with having a modifiable shared state, can lead to non-determinism
    occurring in the overall states of all tasks. This section provides an example
    to demonstrate how this non-determinism can be problematic in a simple program.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to continue our discussion and bring in *shared
    states* to see how they contribute to the non-determinism we talked about. As
    a programmer, the term *state* should remind you of a set of variables and their
    corresponding values at a specific time. Therefore, when we are talking about
    the *overall state* of a task, as we defined it in the first section, we are referring
    to the set of all existing non-shared variables, together with their corresponding
    values, at the exact moment when the last instruction of the task has been executed.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, an *intermediate state* of a task is the set of all existing non-shared
    variables, together with their values when the task has executed a certain instruction.
    Therefore, a task has a different intermediate state for each of its instructions,
    and the number of intermediate states is equal to the number of instructions.
    According to our definitions, the last intermediate state is the same as the overall
    state of the task.
  prefs: []
  type: TYPE_NORMAL
- en: A shared state is also a set of variables together with their corresponding
    values at a specific time which can be read or modified by a system of concurrent
    tasks. A shared state is not owned by a task (it is not local to a task), and
    it can be read or modified by any of the tasks running in the system, and of course
    at any time.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, we are not interested in shared states that are read-only. They are
    usually safe to be read by many concurrent tasks, and they don't yield any problem.
    However, a shared state that is modifiable usually yields some serious problems
    if it is not protected carefully. Therefore, all the shared states covered by
    this section are considered to be modifiable by at least by one of the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ask yourself this question: what can go wrong if a shared state is modified
    by one of the concurrent tasks in a system? To answer this, we start by giving
    an example of a system of two concurrent tasks accessing a single shared variable,
    which, in this case, is a simple integer variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose that we have the following system as displayed in *Code Box
    13-7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Code Box 13-7: A system of two concurrent tasks with a modifiable shared state'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that in the preceding system, tasks `P` and `Q` are not concurrently
    run. Therefore, they become executed sequentially. Suppose that the instructions
    in `P` are executed first, before `Q`. If that was the case, then the overall
    state of the whole system, regardless of the overall state of any individual task,
    would be the shared variable, `X`, with a value of 3.
  prefs: []
  type: TYPE_NORMAL
- en: If you run the system in reverse order, first the instructions in `Q` and then
    the instructions in `P`, you will get the same overall state. However, this is
    not usually the case and running two different tasks in a reversed order probably
    leads to a different overall state.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, running these tasks sequentially produces a deterministic result
    without worrying about context switches.
  prefs: []
  type: TYPE_NORMAL
- en: Now, suppose that they are run concurrently on the same CPU core. There are
    many possible scenarios for putting the instructions of `P` and `Q` into execution
    by considering various context switches occurring at various instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a possible scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Code Box 13-8: A possible interleaving of tasks P and Q when run concurrently'
  prefs: []
  type: TYPE_NORMAL
- en: This scenario is only one of many possible scenarios with context switches happening
    at certain places. Each scenario is called an *interleaving*. So, for a system
    of concurrent tasks, there are a number of possible interleavings based on the
    various places that context switches can happen, and in each run only one of these
    many interleavings will happen. This, as a result, makes them unpredictable.
  prefs: []
  type: TYPE_NORMAL
- en: For the preceding interleaving, as you can see in the first and last column,
    the order of instructions and happens-before constraints are preserved, but there
    could be *gaps* between the executions. These gaps are not predictable, and as
    we trace the execution, the preceding interleaving leads to a surprising result.
    Process `P` prints the value `1` and process `Q` prints the value `2`, yet it
    was expected that both of them would print `3` as their final result.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the preceding example, the constraint for accepting the final result
    has been defined like this – the program should print two `3`s in the output.
    This constraint could be something else, and independent of the visible output
    of the program. More than that, there exist other critical constraints that should
    remain *invariant* when facing unpredictable context switches. These could include
    not having any *data race* or *race condition*, having no memory leak at all,
    or even not to crash. All of these constraints are far more important than the
    visible output of the program. In many real applications, a program doesn't even
    have an output at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following in *Code Box 13-9* is another interleaving with a different result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Code Box 13-9: Another possible interleaving of tasks P and Q when run concurrently'
  prefs: []
  type: TYPE_NORMAL
- en: In this interleaving, task `P` prints `3`, but task `Q` prints `2`. This occurs
    due to the fact that task `P` hasn't been lucky enough to update the value of
    the shared variable `X` before the third context switch. Therefore, task `Q` just
    printed the value of `X`, which was `2` at that moment. This condition is called
    a *data race* over the variable `X`, and we explain this further in the upcoming
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In a real C program, we usually write `X++` or `X = X + 1` instead of firstly
    copying `X` into `A` and then incrementing `A`, and finally putting it back into
    `X`. You will see an example of this in *Chapter 15*, *Thread Execution*.
  prefs: []
  type: TYPE_NORMAL
- en: This clearly shows that a simple `X++` statement in C consists of three smaller
    instructions that won't be executed in a single time slice. In other words, it
    is not an *atomic instruction*, but it has been made up of three smaller atomic
    instructions. An atomic instruction cannot be broken down into smaller operations
    and it cannot be interrupted by context switches. We will see more of this in
    later chapters regarding multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: There is another thing to consider regarding the preceding example. In the preceding
    example, the tasks `P` and `Q` were not the only running tasks in the system;
    there were also other tasks being executed concurrently to our tasks `P` and `Q`,
    but we didn't consider them in our analysis, and we only discussed those two tasks.
    Why is that?
  prefs: []
  type: TYPE_NORMAL
- en: The answer to this question relies on the fact that the different interleavings
    between any of these two tasks and the other tasks in the system could not change
    the intermediate states of the task `P` or `Q`. In other words, the other tasks
    have no shared state with `P` and `Q`, and as we have explained before, when there
    are no shared resources between some tasks, interleavings won't matter, as we
    can see in this case. Therefore, we could assume that there are no other tasks
    besides `P` and `Q` in our hypothetical system.
  prefs: []
  type: TYPE_NORMAL
- en: The only effect that the other tasks have upon `P` and `Q` is that, if there
    are too many of them, they can make `P` and `Q`'s execution slower. That's simply
    a result of having long gaps between two successive instructions in `P` or `Q`.
    In other words, the CPU core needs to be shared among more tasks. Therefore, tasks
    `P` and `Q` would need to wait in the queue more often than normally, delaying
    their execution.
  prefs: []
  type: TYPE_NORMAL
- en: Using this example, you saw how even a single shared state between only two
    concurrent tasks could lead to a lack of determinism in the overall result. We
    have shown the problems associated with a lack of determinism; we don't want to
    have a program that yields to a different result in each run. The tasks in our
    example were relatively simple, containing four trivial instructions, but real
    concurrent applications that are present in the production environment are much
    more complex than this.
  prefs: []
  type: TYPE_NORMAL
- en: More than that, we have various kinds of shared resources that don't necessarily
    reside in the memory, such as files or services that are available on the network.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, the number of tasks trying to access a shared resource can be high,
    and therefore we need to study concurrency issues in a deeper sense and find mechanisms
    to bring back determinism. In the next chapter, we'll continue our discussions
    by talking about concurrency issues and the solutions to fix them.
  prefs: []
  type: TYPE_NORMAL
- en: Before finishing this chapter, let's briefly talk about the task scheduler and
    how it works. If we only have one CPU core, then, at any given moment, we can
    only have one task using that CPU core.
  prefs: []
  type: TYPE_NORMAL
- en: We also know that the task scheduler itself is a piece of program that needs
    a slice of the CPU core to be executed. So, how does it manage different tasks
    in order to use the CPU core when another task is using it? Let's suppose that
    the task scheduler itself is using the CPU core. Firstly, it selects a task from
    its queue before it sets a timer for a *timer interrupt* to happen, and it then
    leaves the CPU core and gives its resources over to the selected task.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have assumed that the task scheduler will give each task a certain
    amount of time, there is a time that the interrupt will act, and the CPU core
    stops the execution of the current task and immediately loads the task scheduler
    back into the CPU. Now, the scheduler stores the latest status of the previous
    task and loads the next one from the queue. All of this goes on until the kernel
    is up and running. Regarding a machine having a CPU with multiple cores, this
    can change, and the kernel can use various cores while scheduling the tasks for
    other cores.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we briefly went over the concept of shared states and the way
    they participate in concurrent systems. The discussions will be continued in the
    next chapter by talking about concurrency issues and synchronization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through the basics of concurrency, and the essential
    concepts and terminology that you need to know in order to understand the upcoming
    topics of multithreading and multi-processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we discussed the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The definitions of concurrency and parallelism – the fact that each parallel
    task needs to have its own processor unit, while concurrent tasks can share a
    single processor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrent tasks use a single processor unit while a task scheduler manages
    the processor time and shares it between different tasks. This will lead to a
    number context switches and different interleavings for each task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to blocking instructions. We also explained the patterns that
    suggest when we require concurrency, and the way we could break a single task
    into two or three concurrent tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We described what a shared state is. We also showed how a shared state could
    lead to serious concurrency issues like data races when multiple tasks try to
    read and write the same shared state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following chapter, we complete our discussion on the topic of concurrency,
    and we explain the several types of issues that you will experience in a concurrent
    environment. Talking about the solutions to concurrency-related issues will also
    be a part of our discussions in the following chapter.
  prefs: []
  type: TYPE_NORMAL
