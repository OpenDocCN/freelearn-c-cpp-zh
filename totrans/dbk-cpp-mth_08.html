<html><head></head><body>
  <div><h1 class="chapter-number" id="_idParaDest-89">
    <a id="_idTextAnchor088">
    </a>
    
     8
    
   </h1>
   <h1 id="_idParaDest-90">
    <a id="_idTextAnchor089">
    </a>
    
     The Fastest C++ Code is Inline Assembly
    
   </h1>
   <p class="italic-heading">
    <em class="italic">
     
      Lower than this you should
     
    </em>
    
     <em class="italic">
      
       not get
      
     </em>
    
   </p>
   <p>
    
     In the fast-paced world of C++ developers, where efficiency is paramount, optimizing code to squeeze out every last drop of performance has always been a fascinating challenge.
    
    
     This journey often takes developers down to the very roots of computing, where C++ meets assembly language, and every CPU
    
    
     
      cycle counts.
     
    
   </p>
   <p>
    
     Circa three decades ago, during the wild 90s, programmers frequently had to manually craft every byte of executable code, often diving into the murky waters of assembly language (and even lower) to achieve the desired performance.
    
    
     These early pioneers of optimization developed techniques that, while rudimentary by today’s standards, laid the groundwork for understanding the power and limitations of both C++
    
    
     
      and assembly.
     
    
   </p>
   <p>
    
     This exploration delves into the specifics of optimizing a seemingly simple task, lighting up a pixel on the screen, by comparing handcrafted and optimized assembly routines from three decades ago with the modern-day output of advanced compilers such as Clang, GCC, and MSVC.
    
    
     As we navigate through the evolution of compilers, we’ll see how the balance between human intuition and machine-generated optimization has shifted, offering new insights into the ever-evolving relationship between the code we write and the machine that ultimately runs our programs.
    
    
     As a side note, in this chapter, we’ll focus on Intel’s x86 family of processors and delve into specific features, while leaving coverage of the ARM architecture for another book, potentially by a
    
    
     
      different author.
     
    
   </p>
   <p>
    
     In this chapter, you will learn
    
    
     
      the following:
     
    
   </p>
   <ul>
    <li>
     
      How to use assembly code to speed up
     
     
      
       your routines
      
     
    </li>
    <li>
     
      How not to use assembly code and trust your compiler’s optimizer to come up with the
     
     
      
       fastest solution
      
     
    </li>
   </ul>
   <h1 id="_idParaDest-91">
    <a id="_idTextAnchor090">
    </a>
    
     Light me a pixel
    
   </h1>
   <p>
    
     Circa 30 years ago, at the nearer end of the wild 90s, the author of these lines spent quite a significant time optimizing code that was supposed to run as fast as possible, consuming the least amount of resources while showing incredible spinning graphics on a screen (there was also scrolling involved, too, and other not
    
    
     
      relevant calculations).
     
    
   </p>
   <p>
    
     These applications were called demos (intros, etc.) and showcased some spectacular graphical effects, backed by a strong mathematical background, and had an in-house developed graphical engine; in those days, there was no DirectX to take all those nasty low-level details off your plate, so all had to be done by hand.
    
    
     Methods for pixel color calculation, color palette setting, vertical retrace of the CRT screen, and flipping of back and front buffers were all coded by hand, using C++ of the 90s and some assembly language routines for the
    
    
     
      time-critical bits.
     
    
   </p>
   <p>
    
     One of these methods was
    
    <a id="_idIndexMarker289">
    </a>
    
     putting a pixel on the screen, which, in its simplest incarnation of the method, looked
    
    
     
      like this:
     
    
   </p>
   <pre class="source-code">
void putpixel(int x, int y, unsigned char color) {
    unsigned char far* vid_mem = (unsigned char far*)0xA0000000L;
    vid_mem[(y * 320) + x] = color;
}</pre>
   <p>
    
     I’ll spare you the very low-level details such as how segment/offset memory worked 30 years ago.
    
    
     Instead, imagine that the
    
    
     
      following apply:
     
    
   </p>
   <ul>
    <li>
     
      You are using DOS (in 1994, in the wild-far-eastern part of Europe, almost everyone who had a PC used DOS – kudos to the 0.1 percent of early
     
     
      
       Linux adopters)
      
     
    </li>
    <li>
     
      You are also using a special graphic mode, 0x13 (almost all the games used this mode because it allowed 256 colors to be drawn on the screen using the mysterious 320 by 200 resolution, whose origins only IBM engineers from 40 years
     
     
      
       ago know)
      
     
    </li>
   </ul>
   <p>
    
     In this case, if you put a byte at the
    
    <strong class="source-inline">
     
      0xA000
     
    </strong>
    
     segment and a specific offset, the graphic card will light up a pixel at the specific coordinates, which can be obtained from the
    
    
     
      preceding formula.
     
    
   </p>
   <p>
    
     Now, after
    
    <a id="_idIndexMarker290">
    </a>
    
     several iterations of the code, the aforementioned programmer observed that the routine was not that optimal, and it could have benefited from
    
    
     
      some optimizations.
     
    
   </p>
   <p>
    
     Please bear with me; the code that was generated by the affordable compiler (the one you just copied from the disk that we mentioned in
    
    <a href="B22235_02.xhtml#_idTextAnchor026">
     
      <em class="italic">
       
        Chapter 2
       
      </em>
     
    </a>
    
     of the book) is in the
    
    
     
      following screenshot:
     
    
   </p>
   <div><div><img alt="Figure 8.1 – Everyone’s favorite Turbo Debugger from 30 years ago" src="img/B22235_08_01.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 8.1 – Everyone’s favorite Turbo Debugger from 30 years ago
    
   </p>
   <p>
    
     Now, this looks pretty wild, considering the age of it, but again, we need just a bit of patience, and all the mystery surrounding why it’s here will be revealed.
    
    
     You see, we were discussing how the code generated by the compiler is far from
    
    
     
      being optimal.
     
    
   </p>
   <p>
    
     Let’s take a moment to consider this piece of code.
    
    
     After giving it some thought, especially from the perspective of someone familiar with assembly language, which is becoming increasingly rare these days, it might be clear to them that the compiler didn’t struggle as much as we
    
    
     
      might expect.
     
    
   </p>
   <p>
    
     The following is
    
    <a id="_idIndexMarker291">
    </a>
    
     the assembly code that the compiler generated for the
    
    
     <strong class="source-inline">
      
       putpixel
      
     </strong>
    
    
     
      routine:
     
    
   </p>
   <pre class="source-code">
putpixel   proc   near
  push bp              ; Save the base pointer on the stack
  mov bp, sp      ; Set the BP to the current stack pointer
  sub sp, 4           ; Reserve 4 bytes for local variables
  mov word ptr [bp-2], 40960       ; Store 0xA000 at [bp-2]
  mov word ptr [bp-4], 0                ; Store 0 at [bp-4]
  mov ax, word ptr [bp+6]   ; Load the y-coordinate into AX
  mov dx, 320               ; Load the screen width into DX
  imul dx      ; Multiply AX (y-coord) by DX (screen width)
  mov bx, word ptr [bp+4]   ; Load the x-coordinate into BX
  add bx, ax      ; Add y*screen width (AX) to BX (x-coord)
  mov es, word ptr [bp-2]             ; Load 0xA000 into ES
  add bx, word ptr [bp-4]       ; Final pixel address in BX
  mov al, byte ptr [bp+8]    ; Load the color value into AL
  mov byte ptr es:[bx], al               ; Light the pixel!
  mov sp, bp                    ; Restore the stack pointer
  pop bp                         ; Restore the base pointer
  ret                           ; Return from the procedure</pre>
   <p>
    
     For those not familiar with the notation,
    
    <strong class="source-inline">
     
      []
     
    </strong>
    
     represents the data at the address given in the square parentheses, so the parameters are being passed in
    
    
     
      like this:
     
    
   </p>
   <ul>
    <li>
     
      The
     
     <strong class="source-inline">
      
       x
      
     </strong>
     
      coordinate of the pixel (
     
     
      
       from
      
     
     
      <strong class="source-inline">
       
        [bp+4]
       
      </strong>
     
     
      
       )
      
     
    </li>
    <li>
     
      The
     
     <strong class="source-inline">
      
       y
      
     </strong>
     
      coordinate of the pixel (
     
     
      
       from
      
     
     
      <strong class="source-inline">
       
        [bp+6]
       
      </strong>
     
     
      
       )
      
     
    </li>
    <li>
     
      The color value to set (
     
     
      
       from
      
     
     
      <strong class="source-inline">
       
        [bp+8]
       
      </strong>
     
     
      
       )
      
     
    </li>
   </ul>
   <p>
    
     Indeed, the
    
    <a id="_idIndexMarker292">
    </a>
    
     code as is contains a lot of unnecessary memory access to move data around, when those operations could have been kept in registers, and there is quite a lot of unnecessary access to various memory areas, which can be skipped.
    
    
     The code compiled by the compiler of the day generated code that was easy to debug, but which could have been written much neater.
    
    
     Compilers today generate the same kind of code, having a very similar performance, when compiling in Debug mode but once you switch them to optimized Release mode, they will
    
    
     
      do magic.
     
    
   </p>
   <p>
    
     Modern CPUs are highly complex beasts; when running in protected mode, they employ various techniques, such as out-of-order execution, instruction pipelining, and other techniques that make really low-level performance analysis nowadays quite difficult to nail down properly...
    
    
     but old machines were much simpler!
    
    
     Or just use DOS on a modern computer and you will get the
    
    
     
      same feeling.
     
    
   </p>
   <p>
    
     Not considering that protected mode was introduced in the early 80286 processors, DOS simply could not handle it (and still can’t), so it stuck to what it knew best: running programs in real mode.
    
    
     While running in real mode, the processor just executed one instruction after the other, and there even was an instruction table explaining how many cycles each instruction
    
    
     
      would take
     
    
    
     
      
       <a class="_idFootnoteLink _idGenColorInherit" href="B22235_08.xhtml#footnote-024">
        
         1
        
       </a>
      
     
    
    
     
      .
     
    
   </p>
   <div><p class="Footnote-text">
     <a class="_idFootnoteAnchor _idGenColorInherit" href="B22235_08.xhtml#footnote-024-backlink">
      
       1
      
     </a>
     <a href="https://zs3.me/intel.php">
      
       https://zs3.me/intel.php
      
     </a>
    </p>
   </div>
   <p>
    
     After spending a significant amount of time consulting those tables, we came to the conclusion that one
    
    <strong class="source-inline">
     
      imul
     
    </strong>
    
     can take longer than two shifts and an add on a processor of those days (the same conclusion was drawn by several other thousands of programmers all around the world after consulting those tables, but we felt that we must be some kind of local heroes for discovering
    
    
     
      this feature).
     
    
   </p>
   <p>
    
     Considering that 320 is a very nice number, as it is the sum of 256 and 64, after several rounds of optimizations, we came up with the following slightly more optimized version for
    
    
     
      the routine:
     
    
   </p>
   <pre class="source-code">
void putpixel(int x, int y, unsigned char c) {
  asm {
  mov ax, 0xA000      // Load 0xA000 (VGA mode 13h) into AX
  mov es, ax        // Set ES to the video segment (0xA000)
  mov dx, y                // Load the y-coordinate into DX
  mov di, x                // Load the x-coordinate into DI
  mov bx, y                // Copy the y-coordinate into BX
  shl dx, 8    // Multiply DX by 256 (left shift by 8 bits)
  shl bx, 6     // Multiply BX by 64 (left shift by 6 bits)
  add dx, bx // Add those, effectively multiplying y by 320
  add di, dx   // Add the calculated y to DI (pixel offset)
  mov al, c                 // Load the color value into AL
  stosb                                  // Light the pixel
} }</pre>
   <p>
    
     It is not the most optimal routine that one can come up with for this purpose, but for our specific requirements, it was more
    
    
     
      than enough.
     
    
   </p>
   <p>
    
     A significantly
    
    <a id="_idIndexMarker293">
    </a>
    
     reduced amount of direct memory access (which was considered slow even in the old days), the lengthy multiplication by 320 using
    
    <strong class="source-inline">
     
      imul
     
    </strong>
    
     changed to multiplication by 256 (this is the shift to the left by 8 operations:
    
    <strong class="source-inline">
     
      shl dx,8
     
    </strong>
    
     ), and 64 (the same by 6), then a sum, which still adds up to fewer cycles than the
    
    
     
      power-consuming multiplication.
     
    
   </p>
   <p>
    
     And thus, the foundation was laid for the myth that if you really want fast code, you have to write it yourself at the lowest
    
    
     
      possible level.
     
    
   </p>
   <p>
    
     As an interesting mental exercise, let’s jump forward in time 30 years, skipping several generations of compilers.
    
    
     If we feed the C++ routine as it is to a modern compiler (for our purpose, we have used Clang – the latest at the time of writing was version 18.1 – but using GCC will get also a very similar result, just using a different set of registers), we get the
    
    
     
      following output:
     
    
   </p>
   <pre class="source-code">
putpixel(int, int, unsigned char):
  movzx eax, byte ptr [esp + 12]
  mov ecx, dword ptr [esp + 4]
  mov edx, dword ptr [esp + 8]
  lea edx, [edx + 4*edx]
  shl edx, 6
  mov byte ptr [edx + ecx + 40960], al</pre>
   <p>
    
     This is way
    
    <a id="_idIndexMarker294">
    </a>
    
     shorter than the one we concocted for our purpose and considered optimal targeting the processors from 3 decades ago, but processors have evolved a lot in the last 30 years, and a lot more advanced features have come in, with new commands (some more words about new commands a bit late in this chapter, so stay tuned) and we find it extremely satisfying how compilers’ optimization routines have resolved the multiplication with that nice
    
    
     
      number, 320.
     
    
   </p>
   <p>
    
     C++ compilers have evolved significantly over the past few decades, from their humble beginnings as Turbo C++ or Watcom C++, becoming incredibly sophisticated and capable of performing a wide range of optimizations that were previously unimaginable due to mostly hardware constraints because, well...
    
    
     640 KB should be enough
    
    
     
      for everyone.
     
    
   </p>
   <p>
    
     Modern compilers are no longer just simple translators from human-readable code to machine code; they have become complex systems that analyze and transform code in ways that can drastically improve performance and memory usage, taking into consideration some aspects that are all meant to help developers bring out the best of
    
    
     
      their source.
     
    
   </p>
   <p>
    
     GCC, Clang, and MSVC all employ advanced optimization techniques such as inlining functions, loop unrolling, constant folding, dead code elimination, and aggressive optimizations that span across entire modules or programs, since, at their stage, they have an overview of the entire application, allowing these
    
    
     
      high-level optimizations.
     
    
   </p>
   <p>
    
     On a side note, these compilers also leverage modern hardware features, such as vectorization and parallelism, to generate highly efficient machine code that can target a specific processor.
    
    
     We will soon see how these optimizations fall into place when we present the example in the next section, where we take a mundane task and let our compilers churn
    
    
     
      through it.
     
    
   </p>
   <p>
    
     But till we reach that stage, just one more use case from 30 years ago.
    
    
     The subtitle of this chapter is
    
    <em class="italic">
     
      Lower than this you should not get
     
    </em>
    
     .
    
    
     Certainly, we meant coding at a lower level, not something else, and right now, we will proudly contradict ourselves.
    
    
     Again.
    
    
     Here is the contradiction:
    
    <em class="italic">
     
      in certain situations, you really should get to a level lower
     
    </em>
    
     <em class="italic">
      
       than assembly
      
     </em>
    
    
     
      .
     
    
   </p>
   <p>
    
     If you are familiar with graphic programming, then I suppose you are familiar with the concept of double-buffering and back-buffering.
    
    
     The back buffer is an off-screen buffer (memory area, with the same size as the screen) where all the rendering (drawing of graphics) happens first.
    
    
     When the rendering is done, the back buffer is copied onto the screen in order to show the graphics, the back buffer is cleared, and the rendering restarts.
    
    
     At
    
    <a id="_idIndexMarker295">
    </a>
    
     some time in history, Tom Duff, a Canadian programmer, invented a wonderful piece of ingenious code that was meant to accomplish exactly this task; the name of it is Duff’s device and it has been discussed several times in several forums, and we are not going to discuss it now.
    
    
     Instead, we will show you the “highly optimized” code that we used to copy data from the back buffer to
    
    
     
      the screen:
     
    
   </p>
   <pre class="source-code">
void flip(unsigned int source, unsigned int dest) {
  asm {
     push ds   // Save the current value of the DS register
     mov ax, dest   // Load the destination address into AX
     mov es, ax       // Copy the value from AX into the ES
     mov ax, source      // Load the source address into AX
     mov ds, ax         // Copy the value in AX into the DS
     xor si, si  // Zero out the SI (source index) register
     xor di, di      // Zero out the DI (destination index)
     mov cx, 64000       // Load 64000 into the CX register
                   // (this is the number of bytes to copy)
     rep movsb         // Run  the`movsb` instruction 64000
          // times (movsb copies bytes from DS:SI to ES:DI)
     pop ds         // Restore the original value of the DS
} }</pre>
   <p>
    
     The preceding trick consists of the
    
    <strong class="source-inline">
     
      rep movsb
     
    </strong>
    
     instruction, which will do the actual copying of bytes (
    
    <strong class="source-inline">
     
      movsb
     
    </strong>
    
     ), repeated (
    
    <strong class="source-inline">
     
      rep
     
    </strong>
    
     ) 64,000 times, as indicated by the CX register (we all know that 64,000 = 320 x 200; that’s why they are
    
    
     
      magic numbers).
     
    
   </p>
   <p>
    
     This code works perfectly given the circumstances.
    
    
     However, there is an opportunity for a tiny bit of optimization; you see, we are using a decent processor – at least, an 80386.
    
    
     Unlike its predecessor, the 80286, which was a pure 16-bit processor, the 80386 is a huge step forward, since it is the first 32-bit x86 processor coming from Intel.
    
    
     So, what we can do is the following: instead of copying 64,000 bytes using
    
    <strong class="source-inline">
     
      rep movsd
     
    </strong>
    
     , we can harvest the
    
    <a id="_idIndexMarker296">
    </a>
    
     opportunities given by our high-end processor and put to use the new 32-bit framework, keywords, and registers.
    
    
     What we do is move 16,000 double words (we all know that a byte is measured as 8 bits, two bytes are called a word, measuring 16 bits, and two words are called a double word, totaling 32 bits) because that is exactly what the new processor has support for: operation on 32-bit values.
    
    
     The newly introduced
    
    <strong class="source-inline">
     
      movsd
     
    </strong>
    
     command does exactly this: copies 4 bytes in one step, so that could be a speed-up of 4 times compared to our
    
    
     
      older code.
     
    
   </p>
   <p>
    
     Our anecdotical C++ compiler, introduced at the beginning of this book, is Turbo C++ Lite.
    
    
     Unfortunately for us, Turbo C++ cannot compile code for anything other than processors below 80286, so we are stuck with 16-bit registers and some really inefficient
    
    
     
      register handling.
     
    
   </p>
   <p>
    
     And here is where the lowest level of hack anyone can see in C++ code comes in – we simply add the bytes of the
    
    <strong class="source-inline">
     
      rep movsd
     
    </strong>
    
     command as hexadecimal values in
    
    
     
      the code:
     
    
   </p>
   <pre class="source-code">
xor di,di
mov cx,16000
db 0xF3,0x66,0xA5 //rep movsd
pop ds</pre>
   <p>
    
     Nothing is simpler and more eye-watering than seeing this in production code, right?
    
    
     Now, regardless that our compiler cannot compile code for 80386 because it’s stuck in the Stone Age (pretty much like half of the chapter you are reading right now), we can still produce code that runs optimally on your processor.
    
    
     Please don’t
    
    
     
      do this.
     
    
   </p>
   <h2 id="_idParaDest-92">
    <a id="_idTextAnchor091">
    </a>
    
     A note on the past
    
   </h2>
   <p>
    
     Now, you might ask why we even bother mentioning assembly language in 2024, when the major trends exhaust themselves concerning the widespread adoption of AI-driven development tools, the growth of low-code/no-code platforms, and the continued rise of the Nth iteration of various JavaScript modules that have exactly the same output as the previous one, except that the syntax
    
    
     
      is different.
     
    
   </p>
   <p>
    
     Regardless that these are the loudest happenings in the IT world nowadays, assembly language is still not obsolete.
    
    
     It might not get as much focus as everyone’s favorite Rust language (Alex will debate Rust in a later chapter if all goes according to plan), but there are still major business branches where the assembly is a must, and still essential in several hardware environments that require precise control, performance optimization, or direct hardware access, such as
    
    
     
      the following:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Embedded systems
      
     </strong>
     
      : Microcontrollers and IoT devices often use assembly for efficient, low-level programming.
     
     
      There isn’t too much power on these small devices; every
     
     
      
       bit counts.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Operating system
      
     </strong>
     
      (
     
     <strong class="bold">
      
       OS
      
     </strong>
     
      )
     
     <strong class="bold">
      
       development
      
     </strong>
     
      : Bootloaders and critical parts of OS kernels require assembly for hardware initialization and management.
     
     
      To achieve this feat, either you work for a large corporation or start your own project.
     
     
      Linux is pretty much
     
     
      
       accounted for.
      
     
    </li>
    <li>
     <strong class="bold">
      
       High-performance computing
      
     </strong>
     
      (
     
     <strong class="bold">
      
       HPC
      
     </strong>
     
      ): Assembly is used for optimizing performance-critical code, particularly in scientific computing or custom hardware (e.g., FPGAs).
     
     
      To pursue this, you must find someone wanting to pay you to
     
     
      
       pursue this.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Security and reverse engineering
      
     </strong>
     
      : Analyzing and exploiting binaries often involves understanding and writing assembly.
     
     
      This is the most lucrative of all, and the most realistic way of getting into assembly
     
     
      
       programming, unfortunately.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Firmware development
      
     </strong>
     
      : BIOS/UEFI and low-level device drivers are commonly written in assembly for direct hardware interaction.
     
     
      Here, again, you must be on the payroll of a large corporation, although there are a few open source projects too (coreboot, libreboot, or just google free bios to get a
     
     
      
       decent list).
      
     
    </li>
    <li>
     <strong class="bold">
      
       Legacy systems
      
     </strong>
     
      : Maintaining older systems or working with retro computing often requires assembly.
     
     
      This is the ideal chance to blend both fun and suffering into
     
     
      
       one experience.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Specialized hardware
      
     </strong>
     
      : DSPs and custom CPU architectures may need assembly for specialized,
     
     
      
       efficient processing.
      
     
    </li>
   </ul>
   <p>
    
     Please don’t dismiss assembly language just yet.
    
    
     It remains relevant and will continue to be as long as computers exist.
    
    
     For those who are interested in the topic, it has its place.
    
    
     Otherwise, you can stick to
    
    
     
      standard C++.
     
    
   </p>
   <h1 id="_idParaDest-93">
    <a id="_idTextAnchor092">
    </a>
    
     The sum of all numbers
    
   </h1>
   <p>
    
     Dearest esteemed reader.
    
    
     It is a truth universally acknowledged that all developers at some stage in their lives must go through a technical interview.
    
    
     There are various levels of interrogations: some just on the level of “Please tell me something about yourself” (these are the hardest), while some go deeper and might even ask you to write some code on a blackboard or even
    
    
     
      a computer.
     
    
   </p>
   <p>
    
     One of the programs that very frequently comes up in interview questions is to write some code that will calculate the sum of a series of numbers sharing a certain peculiarity, for example, the sum of all even numbers, the sum of all numbers divisible by, let’s say, five, or the sum of odd numbers in a
    
    
     
      specific interval.
     
    
   </p>
   <p>
    
     For simplicity’s sake, let’s stick to
    
    <a id="_idIndexMarker297">
    </a>
    
     something simple: the sum of all odd numbers up to 100.
    
    
     The following quick program delivers
    
    
     
      exactly this:
     
    
   </p>
   <pre class="source-code">
#include &lt;cstdio&gt;
int main() {
    int sum = 0;
    for (int i = 1; i &lt;= 100; ++i) {
        if (i % 2 != 0) {  // Check if the number is odd
            sum += i;      // Add the odd number to the sum
        }
    }
    printf("The sum is: %d\n",sum);
    return 0;
}</pre>
   <p>
    
     Not an overly complicated program: just iterate through the numbers; check whether they are odd; if yes, add their value to the final sum; and, in the end, print out the sum (for everyone interested, the sum of odd numbers from 1 to 100 is
    
    
     
      exactly 2,500).
     
    
   </p>
   <p>
    
     But our clear thinking was clouded by the well-known fact (at least, for C++ programmers) that the fastest C++ code is inline assembly, so we decided to sacrifice the portability and understandability of our program on the altar of speed and rewrite the main part of it using assembly language.
    
    
     Because, well, that is the fastest.
    
    
     Here is our attempt
    
    <a id="_idIndexMarker298">
    </a>
    
     at this, using AT&amp;T assembly syntax, just to demonstrate the widely available assembly dialects we can embed in a non-standard compliant
    
    
     
      C++ program:
     
    
   </p>
   <pre class="source-code">
#include &lt;cstdio&gt;
int main() {
    int sum = 0;
    int i = 1; // Start with the first odd number
    __asm__ (
        "movl $1, %[i]\n"   // Initialize i to 1
        "movl $0, %[sum]\n" // Initialize sum to 0
        "loop_start:\n"
        "cmpl $100, %[i]\n"   // Compare i with 100
        "jg loop_end\n"       // If i &gt; 100, exit the
        "addl %[i], %[sum]\n" // sum += i
        "addl $2, %[i]\n"     // i += 2
        "jmp loop_start\n"    // Repeat the loop
        "loop_end:\n"
        : [sum] "+r" (sum), [i] "+r" (i)
    );
    printf("The sum is: %d\n", sum);
    return 0;
}</pre>
   <p>
    
     Just a quick presentation of what the assembly code does, because I hope the other lines of code
    
    
     
      are self-explanatory.
     
    
   </p>
   <p>
    
     Here is the
    
    <a id="_idIndexMarker299">
    </a>
    
     assembly
    
    
     
      code breakdown:
     
    
   </p>
   <ol>
    <li>
     <strong class="source-inline">
      
       "movl $1, %[i]\n"
      
     </strong>
     
      : This instruction sets
     
     <strong class="source-inline">
      
       i
      
     </strong>
     
      to
     
     <strong class="source-inline">
      
       1
      
     </strong>
     
      .
     
     
      Although
     
     <strong class="source-inline">
      
       i
      
     </strong>
     
      was already initialized to
     
     <strong class="source-inline">
      
       1
      
     </strong>
     
      in the C++ code, this explicitly sets it again in assembly
     
     
      
       for clarity.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       "movl $0, %[sum]\n"
      
     </strong>
     
      : This sets the sum to
     
     <strong class="source-inline">
      
       0
      
     </strong>
     
      , ensuring that the sum starts from
     
     <strong class="source-inline">
      
       0
      
     </strong>
     
      in the assembly code.
     
     
      We have to admit that these two initializations are not required, but we wanted them to be a gentle introduction to the assembly code so as not to scare
     
     
      
       you away.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       loop_start
      
     </strong>
     
      : This is just a label, and needs no
     
     
      
       further clarification.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       "cmpl $100, %[i]\n"
      
     </strong>
     
      : Compares
     
     <strong class="source-inline">
      
       i
      
     </strong>
     
      with
     
     <strong class="source-inline">
      
       100
      
     </strong>
     
      .
     
     
      The comparison is used to check whether
     
     <strong class="source-inline">
      
       i
      
     </strong>
     
      has reached or
     
     
      
       exceeded
      
     
     
      <strong class="source-inline">
       
        100
       
      </strong>
     
     
      
       .
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       "jg loop_end\n"
      
     </strong>
     
      : If
     
     <strong class="source-inline">
      
       i
      
     </strong>
     
      is greater than
     
     <strong class="source-inline">
      
       100
      
     </strong>
     
      , the program jumps to
     
     <strong class="source-inline">
      
       loop_end
      
     </strong>
     
      , exiting
     
     
      
       the loop.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       "addl %[i], %[sum]\n"
      
     </strong>
     
      : Adds the current value of
     
     <strong class="source-inline">
      
       i
      
     </strong>
     
      to
     
     <strong class="source-inline">
      
       sum
      
     </strong>
     
      .
     
     
      This accumulates the sum of all odd numbers up
     
     
      
       to
      
     
     
      <strong class="source-inline">
       
        99
       
      </strong>
     
     
      
       .
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       "addl $2, %[i]\n"
      
     </strong>
     
      : Increments
     
     <strong class="source-inline">
      
       i
      
     </strong>
     
      by 2 to move to the next odd number (e.g., 1 → 3 →
     
     
      
       5, etc.).
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       "jmp loop_start\n"
      
     </strong>
     
      : Jumps back to the start of the loop to repeat
     
     
      
       the process.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       loop_end
      
     </strong>
     
      : This is the label where the program jumps when
     
     <strong class="source-inline">
      
       i
      
     </strong>
     
      exceeds
     
     <strong class="source-inline">
      
       100
      
     </strong>
     
      , effectively ending
     
     
      
       the loop.
      
     
    </li>
   </ol>
   <p>
    
     The weirdly looking
    
    <strong class="source-inline">
     
      "+r" (sum)
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      "+r" (i)
     
    </strong>
    
     parts are constraints that tell the compiler to treat
    
    <strong class="source-inline">
     
      sum
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      i
     
    </strong>
    
     as read-write variables, meaning their values can be both read from and written to during the
    
    
     
      assembly operations.
     
    
   </p>
   <p>
    
     As a first
    
    <a id="_idIndexMarker300">
    </a>
    
     drawback, the readability and understandability of the code have suffered exponentially.
    
    
     We intentionally use the AT&amp;T syntax for assembly because it is much more cumbersome and harder to comprehend, and we want you to suffer with it and remember never to use assembly in your code unless you know what you’re doing, and then
    
    
     
      you’re excused.
     
    
   </p>
   <p>
    
     Secondly, this code is not portable anymore because there is no such thing as
    
    <strong class="source-inline">
     
      __asm__
     
    </strong>
    
     under Visual C++; they used
    
    <strong class="source-inline">
     
      __asm
     
    </strong>
    
     back in the day (or more recently, at the beginning of this chapter, Turbo C demonstrated the introduction of the
    
    <strong class="source-inline">
     
      asm
     
    </strong>
    
     keyword).
    
    
     And since we are here, the C++ standard does not include a common assembly block identifier because assembly language syntax is compiler- and platform-specific, and inline assembly is an extension rather than a core part of the language.
    
    
     You have been warned.
    
    
     I really hope that the preceding statement managed to entirely discourage you from ever considering writing assembly code in the body of your C++ function, regardless of the presence of the non-standard keyword to enable you to
    
    
     
      do this.
     
    
   </p>
   <p>
    
     But now that we are here, courtesy of
    
    <a href="http://gcc.godbolt.org">
     
      gcc.godbolt.org
     
    </a>
    
     , we have asked the major compilers to churn through the original little C++ program (with no assembly incursion at all) at various optimization levels because we feel the urge to demonstrate to you that, indeed, entirely skipping the assembly language at this stage is the wisest decision you
    
    
     
      can take.
     
    
   </p>
   <p>
    
     The first one to demonstrate how efficient the compiler is in generating optimal C++ code is Microsoft Visual C++.
    
    
     Microsoft’s own tiny, squishy C++ compiler has several options to
    
    <a id="_idIndexMarker301">
    </a>
    
     generate and optimize the generated code
    
    
     
      <a class="_idFootnoteLink _idGenColorInherit" href="B22235_08.xhtml#footnote-023">
       
        2
       
      </a>
     
    
    
     , but we have a saying here: the shorter the code, the faster it runs.
    
    
     So, we have explicitly told the compiler to generate the shortest code (
    
    <strong class="source-inline">
     
      /O1
     
    </strong>
    
     ), which is
    
    
     
      the following:
     
    
   </p>
   <div><p class="Footnote-text">
     <a class="_idFootnoteAnchor _idGenColorInherit" href="B22235_08.xhtml#footnote-023-backlink">
      
       2
      
     </a>
     <a href="https://learn.microsoft.com/en-us/cpp/build/reference/o-options-optimize-code?view=msvc-170">
      
       https://learn.microsoft.com/en-us/cpp/build/reference/o-options-optimize-code?view=msvc-170
      
     </a>
    </p>
   </div>
   <pre class="source-code">
`string' DB 'The sum is: %d', 0aH, 00H ; `string'
_main PROC
  xor ecx, ecx  ; Clear the ECX register (set ECX to 0)
  xor edx, edx  ; Clear the EDX register (set EDX to 0)
  inc ecx       ; Increment ECX, setting it to 1
 $LL4@main:
  test cl, 1    ; Test the least significant bit of CL
                ; (ECX) to check if ECX is odd or even
  lea eax, DWORD PTR [ecx+edx] ; Load the effective
                ; address of ECX + EDX into EAX
  cmove eax, edx; If the zero flag is set
                ; (ECX was even), move EDX into EAX
  inc ecx       ; Increment ECX by 1
  mov edx, eax  ; Move the value in EAX to EDX
                ; (update EDX for the next iteration)
  cmp ecx, 100  ; Compare ECX with 100
  jle SHORT $LL4@main ; Jump to the start of the loop
                ; (loop until ECX &gt; 100)
  push edx      ; Push the final value of EDX (the sum)
                ; after the loop onto the stack
  push OFFSET `string' ; Push the offset of the string
  call _printf  ; Call the printf function
  pop ecx       ; Clean up the stack (remove string)
  pop ecx       ; Clean up the stack (remove EDX)
  ret 0         ; Return from the _main function
 _main ENDP</pre>
   <p>
    
     Interestingly, the assembly output from MSVC is very much in line with the one we concocted by hand; it has a loop, a bit differently dealing with the various registers based on whether we are currently dealing with an odd or even number, but besides this, it’s similar to the one
    
    
     
      we wrote.
     
    
   </p>
   <p>
    
     Using the other
    
    <a id="_idIndexMarker302">
    </a>
    
     combinations for optimization flags (
    
    <strong class="source-inline">
     
      /Ox
     
    </strong>
    
     ,
    
    <strong class="source-inline">
     
      /O2
     
    </strong>
    
     , and
    
    <strong class="source-inline">
     
      /Ot
     
    </strong>
    
     ) for MSVC did not produce a very different code, just a slightly different assignment of the registers, but nothing that would make us
    
    
     
      say VOW!
     
    
   </p>
   <p>
    
     After switching to GCC (14.1) in order for it to churn through our simple code, we noticed that for the optimization levels of
    
    <strong class="source-inline">
     
      –O1
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      –O2
     
    </strong>
    
     , the code generated was very similar to the one generated by MSVC: it had a variable, churned through the numbers, and made some test for oddness and sum.
    
    
     That’s it, not black magic...
    
    
     unlike the code that was generated
    
    
     
      for
     
    
    
     <strong class="source-inline">
      
       –O3
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     Using this flag, we were
    
    <a id="_idIndexMarker303">
    </a>
    
     surprised to see how the
    
    <strong class="bold">
     
      single instruction, multiple data
     
    </strong>
    
     (
    
    <strong class="bold">
     
      SIMD
     
    </strong>
    
     ) instructions were being pulled in by the compiler, in order to increase the speed, and the unexpected feature this compiler pulled in was that it calculated the sum of elements in an evolving 4-element array, starting with the values {1, 2, 3, 4} and incrementing each element by 4 over 25 iterations using SIMD instructions.
    
    
     The accumulated sum was stored in a SIMD register, and after the loop, it was reduced to a single integer, supplying the
    
    
     
      correct result.
     
    
   </p>
   <p>
    
     The assembly code produced for this was simply too long (more than three pages), and we decided not to publish it here  because it would have been useless, but as a fact of curiosity, we
    
    
     
      mentioned it.
     
    
   </p>
   <p>
    
     The next compiler that we checked for how it deals with our simple C++ program is Clang.
    
    
     At this stage (meaning after the long SIMD instruction dump from GCC with
    
    <strong class="source-inline">
     
      –O3
     
    </strong>
    
     ), we did not expect anything spectacular, but we had
    
    
     
      a surprise.
     
    
   </p>
   <p>
    
     Even at
    
    <strong class="source-inline">
     
      –O1
     
    </strong>
    
     , Clang greeted us with the following, may I say quite
    
    
     
      short code:
     
    
   </p>
   <pre class="source-code">
main:
  push rax
  lea rdi, [rip + .L.str]
  mov esi, 2500
  xor eax, eax
  call printf@PLT
  xor eax, eax
  pop rcx
  ret
.L.str:
  .asciz "The sum is: %d\n"</pre>
   <p>
    
     What a surprise!
    
    
     It
    
    <a id="_idIndexMarker304">
    </a>
    
     seems that Clang did all the calculations behind the scenes, and just simply put the result in the compiled binary.
    
    
     More optimized than this it cannot be.
    
    
     We were really thrilled that compilers have matured and grown into these clever beasts, so this tantalized us to check whether other compilers can be clever like
    
    
     
      this, too.
     
    
   </p>
   <p>
    
     GCC exposes
    
    <a id="_idIndexMarker305">
    </a>
    
     the same behavior at
    
    <strong class="source-inline">
     
      –O3
     
    </strong>
    
     , but surprisingly, only if we want to summarize odd numbers up to 71.
    
    
     At 72, something breaks inside and churns out again the long list of SIMD
    
    
     
      assembly sources.
     
    
   </p>
   <p>
    
     We could not convince MSVC under any circumstance, combinations of numbers and parameters to go the Clang way, and precalculate the number required to print out the sum of odd numbers, so we just concluded that it cannot.
    
    
     Maybe it will be implemented in the next version, what do you say Microsoft Visual
    
    
     
      C++ developers?
     
    
   </p>
   <h2 id="_idParaDest-94">
    <a id="_idTextAnchor093">
    </a>
    
     A glimpse into the future
    
   </h2>
   <p>
    
     There is a phrase circulating amongst C++ developers that goes along the lines of
    
    <em class="italic">
     
      today’s compiler optimizations are the best we’ve ever managed to cobble together and a not-so-gentle reminder of just how much better they
     
    </em>
    
     <em class="italic">
      
       could be
      
     </em>
    
    
     
      .
     
    
   </p>
   <p>
    
     Taking into
    
    <a id="_idIndexMarker306">
    </a>
    
     consideration that this book was written in 2024 (hopefully, it will be published in 2025, and if all goes according to plan, in 2027, it will be obsolete, and we will get a commission to come up with a more up-to-date version of it), we have a pretty clear overview of what is happening in the
    
    
     
      world today.
     
    
   </p>
   <p>
    
     However, if you are reading this book while someone tries to grow potatoes on a different planet and the walls of your building are covered by graffitied monkeys, then you might have had some insights on how far the compilers have come in the last 10 years.
    
    
     Actually, it might even happen that Microsoft’s own (yes, we know, tiny, squishy) C++ compiler
    
    <a id="_idIndexMarker307">
    </a>
    
     managed to grow up to the stage where it can calculate the sum of a few numbers before compilation and GCC is not
    
    <a id="_idIndexMarker308">
    </a>
    
     throwing a tantrum at 72.
    
    
     Even for a short, simple program like the one
    
    
     
      we have.
     
    
   </p>
   <p>
    
     Welcome to
    
    
     
      the future.
     
    
   </p>
   <h1 id="_idParaDest-95">
    <a id="_idTextAnchor094">
    </a>
    
     One instruction to rule them all
    
   </h1>
   <p>
    
     Dear reader.
    
    
     In our
    
    <a id="_idIndexMarker309">
    </a>
    
     previous section of this chapter, unfortunately, we exhausted the only pompous introduction we could borrow from various cultural sources concerning technical interviews, career and life choices, and whether should we take the red pill or the blue one, so let’s focus our attention on more technical questions that our candidates might face at a technical interview (the word technical appears four times in this short
    
    
     
      introductory paragraph).
     
    
   </p>
   <p>
    
     One of these questions, served to the author of these lines a few years ago, was to write a short code snippet that will count the number of 1 bits (the on bits) in a 32-bit integer.
    
    
     Let’s draft up a quick application to
    
    
     
      do this:
     
    
   </p>
   <pre class="source-code">
int countOneBits(uint32_t n) {
    int count = 0;
    while (n) {
        count += n &amp; 1;
        n &gt;&gt;= 1;
    }
    return count;
}</pre>
   <p>
    
     Here’s what happens.
    
    
     Firstly, we initialize a counter, starting with
    
    <strong class="source-inline">
     
      0
     
    </strong>
    
     .
    
    
     The next step is to loop through the bits.
    
    
     While
    
    <strong class="source-inline">
     
      n
     
    </strong>
    
     is non-zero, we add the least significant bit of
    
    <strong class="source-inline">
     
      n
     
    </strong>
    
     to the counter (
    
    <strong class="source-inline">
     
      n&amp;1
     
    </strong>
    
     gives us this value).
    
    
     Following this, we shift
    
    <strong class="source-inline">
     
      n
     
    </strong>
    
     right by one bit (discarding the least
    
    
     
      significant bit).
     
    
   </p>
   <p>
    
     Once all bits are processed (when
    
    <strong class="source-inline">
     
      n
     
    </strong>
    
     becomes
    
    <strong class="source-inline">
     
      0
     
    </strong>
    
     ), return the total count of 1 bits.
    
    
     Not a very complicated process, just
    
    
     
      raw work.
     
    
   </p>
   <p>
    
     It seems that this procedure of counting bits in numbers must be of a very peculiar interest in computing circles, such as for the purpose of error detection and correction, data compression, cryptography, algorithmic efficiency, digital signal processing, hardware design, and performance metrics, so no wonder it managed to creep itself into the STL (C++ STL, which is the standard template library) too in the form of
    
    <strong class="source-inline">
     
      std::popcount
     
    </strong>
    
     from
    
    
     
      C++ 20.
     
    
   </p>
   <p>
    
     The
    
    <a id="_idIndexMarker310">
    </a>
    
     interesting part of the story is that not only in the STL do we find this handy operation, but it was deemed so useful that it even exists at the level of the processors, under the
    
    <a id="_idIndexMarker311">
    </a>
    
     infamous
    
    <strong class="source-inline">
     
      POPCNT
     
    </strong>
    
     mnemonic.
    
    
     Infamous it is, due to the fact that in 2024, it was effectively used in hindering the installation of Windows 11 on older machines that were not officially
    
    
     
      supported
     
    
    
     
      
       <a class="_idFootnoteLink _idGenColorInherit" href="B22235_08.xhtml#footnote-022">
        
         3
        
       </a>
      
     
    
    
     
      .
     
    
   </p>
   <div><p class="Footnote-text">
     <a class="_idFootnoteAnchor _idGenColorInherit" href="B22235_08.xhtml#footnote-022-backlink">
      
       3
      
     </a>
     <a href="https://www.theregister.com/2024/04/23/windows_11_cpu_requirements/">
      
       https://www.theregister.com/2024/04/23/windows_11_cpu_requirements/
      
     </a>
    </p>
   </div>
   <p>
    
     But what that means for our candidate, who has to write code to impress the interviewers, is that they can simply replace the complicated code from before with the following very
    
    
     
      handy snippet:
     
    
   </p>
   <pre class="console">
int countOneBits(uint32_t n) {
    return std::popcount(n);
}</pre>
   <p>
    
     Not forgetting to include the
    
    <strong class="source-inline">
     
      &lt;bit&gt;
     
    </strong>
    
     header, after feeding the preceding program into
    
    <a href="http://gcc.godbolt.org">
     
      gcc.godbolt.org
     
    </a>
    
     ’s compilers, we get a strange mishmash of results.
    
    
     The code compiled by GCC, regardless of the optimization level, always generates a variation of
    
    
     
      the following:
     
    
   </p>
   <pre class="source-code">
countOneBits(unsigned int):
  sub rsp, 8
  mov edi, edi
  call __popcountdi2
  add rsp, 8
  ret</pre>
   <p>
    
     So, the code at some level disappears from our eyes into a strange call deep inside the libraries offered by GCC, called
    
    <strong class="source-inline">
     
      __popcountdi2
     
    </strong>
    
     
      <a class="_idFootnoteLink _idGenColorInherit" href="B22235_08.xhtml#footnote-021">
       
        4
       
      </a>
     
    
    
     .
    
    
     In order to convince GCC to fully utilize the power of the processor that we are running the code on, we need to utilize some of the not-so-well-known command-line options, such as
    
    <strong class="source-inline">
     
      -march
     
    </strong>
    
     (or
    
    <strong class="source-inline">
     
      -mpopcnt
     
    </strong>
    
     for this
    
    
     
      specific purpose).
     
    
   </p>
   <div><p class="Footnote-text">
     <a class="_idFootnoteAnchor _idGenColorInherit" href="B22235_08.xhtml#footnote-021-backlink">
      
       4
      
     </a>
     <a href="https://gcc.gnu.org/onlinedocs/gccint/Integer-library-routines.html">
      
       https://gcc.gnu.org/onlinedocs/gccint/Integer-library-routines.html
      
     </a>
    </p>
   </div>
   <p>
    
     According to the
    
    <a id="_idIndexMarker312">
    </a>
    
     official documentation,
    
    
     
      <a class="_idFootnoteLink _idGenColorInherit" href="B22235_08.xhtml#footnote-020">
       
        5
       
      </a>
     
    
    
     this command will select the appropriate processor instruction set in order to use the available extensions of the specific processor.
    
    
     Since, at this stage, we know that the
    
    <strong class="source-inline">
     
      POPCNT
     
    </strong>
    
     instruction was introduced in the early Core i5 and i7 processors, in the Nehalem family, we should simply specify the following to GCC:
    
    <strong class="source-inline">
     
      -march=nehalem
     
    </strong>
    
     .
    
    
     And now, not surprisingly, the compiler generates
    
    
     
      the following:
     
    
   </p>
   <div><p class="Footnote-text">
     <a class="_idFootnoteAnchor _idGenColorInherit" href="B22235_08.xhtml#footnote-020-backlink">
      
       5
      
     </a>
     <a href="https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html">
      
       https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html
      
     </a>
    </p>
   </div>
   <pre class="source-code">
countOneBits(unsigned int):
  popcnt eax, edi
  ret</pre>
   <p>
    
     Interestingly, if we provide the compiler with just the
    
    <strong class="source-inline">
     
      -mpopcnt
     
    </strong>
    
     flag, then it generates an extra
    
    <strong class="source-inline">
     
      xor eax, eax
     
    </strong>
    
     (meaning it nulls the EAX register) so maybe we have witnessed some processor-specific extra optimizations by choosing the
    
    
     
      Nehalem architecture:
     
    
   </p>
   <pre class="source-code">
countOneBits(unsigned int):
  xor eax, eax
  popcnt eax, edi
  ret</pre>
   <p>
    
     We cannot squeeze more than this out of GCC; there is simply no lower level for this functionality, so we focus our attention on the next compiler on
    
    
     
      our list.
     
    
   </p>
   <p>
    
     Without explicitly asking to optimize the code, Clang also generates a generic call to a
    
    <strong class="source-inline">
     
      std::popcount
     
    </strong>
    
     function, found somewhere in its libraries; however, explicitly asking to optimize the generated code, Clang at various levels of optimization yields
    
    
     
      the following:
     
    
   </p>
   <pre class="source-code">
countOneBits(unsigned int):
  mov eax, edi
  shr eax
  and eax, 1431655765
  sub edi, eax
  mov eax, edi
  and eax, 858993459
  shr edi, 2
  and edi, 858993459
  add edi, eax
  mov eax, edi
  shr eax, 4
  add eax, edi
  and eax, 252645135
  imul eax, eax, 16843009
  shr eax, 24
  ret</pre>
   <p>
    
     Surprising as it seems, there
    
    <a id="_idIndexMarker313">
    </a>
    
     is a perfectly logical explanation for this code, found at the bit-twiddling site
    
    
     
      <a class="_idFootnoteLink _idGenColorInherit" href="B22235_08.xhtml#footnote-019">
       
        6
       
      </a>
     
    
    
     of Sean Eron Anderson at Stanford.
    
    
     Not considering this extra detour, Clang behaves identically to GCC when it comes to handling architecture and specifying the subset of CPU extensions to use while
    
    
     
      generating code.
     
    
   </p>
   <div><p class="Footnote-text">
     <a class="_idFootnoteAnchor _idGenColorInherit" href="B22235_08.xhtml#footnote-019-backlink">
      
       6
      
     </a>
     <a href="https://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetParallel">
      
       https://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetParallel
      
     </a>
    </p>
   </div>
   <p>
    
     The last of the big three, Microsoft’s own (we know, tiny, squishy) C++ compiler handles the situation very similarly to Clang.
    
    
     When asking to optimize the code while we specify an architecture that does not support the
    
    <strong class="source-inline">
     
      POPCNT
     
    </strong>
    
     instruction, it generates code like the one generated by Clang with low-level bit hacks, while if the architecture has support for the
    
    <strong class="source-inline">
     
      POPCNT
     
    </strong>
    
     instruction, it will adjust to the correct type and will call
    
    <strong class="source-inline">
     
      POPCNT
     
    </strong>
    
     for the proper parameters (/std:c++latest /
    
    
     
      arch:SSE4.2 /O1).
     
    
   </p>
   <p>
    
     Good work, tiny,
    
    
     
      squishy compiler.
     
    
   </p>
   <h1 id="_idParaDest-96">
    <a id="_idTextAnchor095">
    </a>
    
     Summary
    
   </h1>
   <p>
    
     Myths related to C++ programming are shaped by the language’s evolving history through time, the differences and various levels of mastery between the users of the language, and psychological needs within the developer community.
    
    
     Early C++ compilers, which often generated less optimal code compared to modern compilers, contributed to myths about the language’s inefficiency and the necessity of manual optimization, such as rewriting entire routines using platform-specific
    
    
     
      assembly languages.
     
    
   </p>
   <p>
    
     As compilers and language features have advanced, these myths persist, sometimes overshadowing modern best practices.
    
    
     This, combined with a culture of elitism and a sense of mastery among C++ programmers, reinforces outdated perceptions, even as C++ continues to be seen as a powerful and versatile language for serious,
    
    
     
      performance-critical applications.
     
    
   </p>
   <p>
    
     In the upcoming chapter, we will host a beauty pageant of programming languages, quickly eliminating all but our favorite, and the process will culminate in the crowning of the undisputed queen, C++.
    
    
     Admittedly, our admiration for this language is so profound that one might suspect the contest was rigged from the
    
    
     
      very start.
     
    
   </p>
  </div>
 </body></html>