<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-166"><a id="_idTextAnchor178"/>11</h1>
<h1 id="_idParaDest-167"><a id="_idTextAnchor179"/>Temporal Anti-Aliasing</h1>
<p>In this chapter, we will expand on a concept touched on in the previous one when we talked about temporal reprojection. One of the most common ways to improve image quality is to sample more data (super-sampling) and filter it down to the needed sampling frequency.</p>
<p>The primary technique used in<a id="_idIndexMarker567"/> rendering is <strong class="bold">Multi-Sample Anti-Aliasing</strong>, or <strong class="bold">MSAA</strong>. Another technique used for super-sampling is temporal super-sampling or using the samples from two or more frames to reconstruct a higher-quality image.</p>
<p>In the Volumetric Fog technique, a similar approach is used to remove banding given by the low resolution of the Volume Texture in a very effective way. We will see how we can achieve better image quality using <strong class="bold">Temporal </strong><strong class="bold">Anti-Aliasing</strong> (<strong class="bold">TAA</strong>).</p>
<p>This technique has become widely used in recent years after more and more games started using Deferred Rendering at their core and because of the difficulty in applying MSAA on it. There were various attempts to make MSAA and Deferred Rendering work together, but performance (both time- and memory-wise) has always been proven to not be feasible at the time and thus alternative solutions started to be developed.</p>
<p>Enter <strong class="bold">Post-Process Anti-Aliasing</strong> and its <a id="_idIndexMarker568"/>plethora of acronyms. The first one<a id="_idIndexMarker569"/> to be widely used was <strong class="bold">Morphological Anti-Aliasing</strong>, or <strong class="bold">MLAA</strong>, developed by Alexander Reshetov, working at Intel at the time, and presented at High-Performance Graphics in 2009.</p>
<p>The algorithm was developed to work on the CPU using Intel’s <strong class="bold">Streaming SIMD Extensions</strong> (<strong class="bold">SSE</strong>) instructions <a id="_idIndexMarker570"/>and introduced some interesting solutions to find and improve geometrical edge rendering, which fueled successive implementations. Later, Sony Santa Monica adopted MLAA for God of War III using <a id="_idIndexMarker571"/>the Cell <strong class="bold">Synergisic Processing Unit</strong> (<strong class="bold">SPUs</strong>) to be performed with real-time performances.</p>
<p>Post-Process Anti-Aliasing finally found a GPU implementation developed by Jorge Jimenez and others in 2011, opening a new rendering research field. Various other game studios started developing custom Post Process Anti-Aliasing techniques and sharing their details.</p>
<p>All those techniques were based on geometrical edge recognition and image enhancement.</p>
<p>Another aspect that started to emerge was the reuse of information from previous frames to further enhance visual quality, such as in <strong class="bold">Sharp Morphological Anti-Aliasing</strong>, or <strong class="bold">SMAA</strong>, which <a id="_idIndexMarker572"/>started adding a temporal component to enhance the final image.</p>
<p>The most adopted anti-aliasing technique is TAA, which comes with its own set of challenges but fits nicely within the rendering pipeline and lets other techniques (such as Volumetric Fog) increase their visual quality by reducing banding with the introduction of animated dithering.</p>
<p>TAA is now the standard in most game engines, both commercial and private. It comes with its own challenges, such as handling transparent objects and image blurriness, but we will see how to tackle those problems as well.</p>
<p>In the rest of the chapter, we will first see an overview of the algorithm and then dive into the implementation. We will also create an initial, incredibly simple implementation just to show the basic building blocks of the algorithm, allowing you to understand how to write a custom TAA implementation from scratch. Finally, we will see the different areas of improvement within the algorithm.</p>
<p>Let’s see an example scene and highlight the TAA improvements:</p>
<div><div><img alt="Figure 11.1 – Temporally anti-aliased scene" height="756" src="img/B18395_11_01.jpg" width="523"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Temporally anti-aliased scene</p>
<p>The following are a couple of screenshots of the final result, with and without TAA enabled.</p>
<div><div><img alt="Figure 11.2 – Details of Figure 11.1 without (left) and with (right) TAA" height="276" src="img/B18395_11_02.jpg" width="312"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Details of Figure 11.1 without (left) and with (right) TAA</p>
<p>In this chapter, we will have a look at the following topics:</p>
<ul>
<li>Creating the simplest TAA implementation</li>
<li>Step-by-step improvement of the technique</li>
<li>Overview of image-sharpening techniques outside of TAA</li>
<li>Improving banding in different image areas with noise and TAA</li>
</ul>
<h1 id="_idParaDest-168"><a id="_idTextAnchor180"/>Technical requirements</h1>
<p>The code for this chapter can be found at the following URL: <a href="https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter11">https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter11</a>.</p>
<h1 id="_idParaDest-169"><a id="_idTextAnchor181"/>Overview</h1>
<p>In this section, we<a id="_idIndexMarker573"/> will see the algorithm overview of the TAA rendering technique.</p>
<p>TAA is based on the collection of samples over time by applying small offsets to the camera projection matrix and applying some filters to generate the final image, like so:</p>
<div><div><img alt="Figure 11.3 – Frustum jitter" height="954" src="img/B18395_11_03.jpg" width="629"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Frustum jitter</p>
<p>There are various numerical sequences that can be used to offset the camera, as we will see in the implementation section. Moving the camera is <a id="_idIndexMarker574"/>called <strong class="bold">jittering</strong>, and by jittering the camera, we gather additional data that we can use to enhance the image.</p>
<p>The following is an overview of the TAA shader:</p>
<div><div><img alt="Figure 11.4 – TAA algorithm overview" height="798" src="img/B18395_11_04.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – TAA algorithm overview</p>
<p>Based on <em class="italic">Figure 11</em><em class="italic">.4</em>, we’ve <a id="_idIndexMarker575"/>separated the algorithm into steps (blue rectangles) and texture reads (yellow ellipses:.</p>
<ol>
<li>We calculate the coordinates to <a id="_idIndexMarker576"/>read the velocity from, represented by the <strong class="bold">Velocity </strong><strong class="bold">Coordinates</strong> block.</li>
</ol>
<p>This is normally done by reading a neighborhood of 3x3 pixels around the current pixel position and finding the closest pixel, using the current frame’s <strong class="bold">Depth Texture</strong>. Reading<a id="_idIndexMarker577"/> from a 3x3 neighborhood has been proven to decrease ghosting and improve edge quality.</p>
<ol>
<li value="2">We read the velocity using the newly found coordinates from the <strong class="bold">Velocity Texture</strong> block, paying <a id="_idIndexMarker578"/>attention to use a linear sampler, as velocity is not just in increments of pixels, but can be in-between pixels.</li>
<li>We read the color information<a id="_idIndexMarker579"/> from the <strong class="bold">History Texture</strong> block. This is basically the last frame’s TAA output. We can optionally apply a filter to read the texture to further enhance the quality.</li>
<li>We will read the current scene color. In this step, we will also cache information again by reading a neighborhood around the current pixel position to constrain the history color we read previously and guide the final resolve phase.</li>
<li>History constraint. We try to limit the previous frame color inside an area of the current color to reject invalid samples coming from occlusion or disocclusion. Without doing that there would be a lot of ghosting.</li>
<li>The sixth and final step is <strong class="bold">Resolve</strong>. We combine the current color and the constraint history color to generate the final pixel color by applying some additional filters.</li>
</ol>
<p>The result of the current frame’s TAA will be the next frame history texture, so we simply switch the textures (history and TAA result) every frame without the need to copy the results <a id="_idIndexMarker580"/>over, as seen in some implementations.</p>
<p>Now that we have seen an overview of the algorithm, we can start by implementing an initial TAA shader.</p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor182"/>The simplest TAA implementation</h1>
<p>The best way to<a id="_idIndexMarker581"/> understand this technique is to build a basic implementation missing some important steps and to have a blurry or jittery rendering as it is easy to do.</p>
<p>The basic ingredients for this technique are simple if done correctly, but each must be done in a precise way. We will first add jittering to the camera so that we can render slightly different points of view of the scene and gather additional data.</p>
<p>We will then add motion vectors so that we can read the previous frame color information in the right place. Finally, we will reproject, or simply put, read the history frame color data and combine it with current f<a id="_idTextAnchor183"/>rame data.</p>
<p>Let us see the different steps.</p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor184"/>Jittering the camera</h2>
<p>The objective<a id="_idIndexMarker582"/> of this step is to translate the projection camera by a small amount in both the <em class="italic">x</em> and <em class="italic">y</em> axes.</p>
<p>We have added some utility code in the <code>GameCamera</code> class:</p>
<pre class="source-code">
void GameCamera::apply_jittering( f32 x, f32 y ) {
    // Reset camera projection
    camera.calculate_projection_matrix();
    // Calculate jittering translation matrix and modify
       projection matrix
    mat4s jittering_matrix = glms_translate_make( { x, y,
                                                  0.0f } );
    camera.projection = glms_mat4_mul( jittering_matrix,
                                       camera.projection );
    camera.calculate_view_projection();
}</pre>
<p>Every step is important and error prone, so be careful.</p>
<p>We first want to reset the projection matrix, as we will manually modify it. We then build a translation matrix with the jittering values in <code>x</code> and <code>y</code>, and we will see later how to calculate them.</p>
<p>Finally, we multiply the projection matrix by the jittering matrix and calculate the new view-projection matrix. Beware of multiplication order, as if this is wrong you will see a jittery blurry mess even when not moving the camera!</p>
<p>Having this working, we can optimize the code by removing the matrix construction and multiplication, having cleaner and less error-prone code, like so:</p>
<pre class="source-code">
void GameCamera::apply_jittering( f32 x, f32 y ) {
   camera.calculate_projection_matrix();
   // Perform the same calculations as before, with the
      observation that
   // we modify only 2 elements in the projection matrix:
   camera.projection.m20 += x;
   camera.projection.m21 += y;
   camera.calculate_view_projection();
}</pre>
<h2 id="_idParaDest-172"><a id="_idTextAnchor185"/>Choosing jittering sequences</h2>
<p>We will <a id="_idIndexMarker583"/>now build a sequence of <code>x</code> and <code>y</code> values to jitter the camera. Normally there are different sequences that are used:</p>
<ul>
<li>Halton</li>
<li>Hammersley</li>
<li>Martin Robert’s R2</li>
<li>Interleaved gradients</li>
</ul>
<p>There are all the implementations for the preceding sequences in the code, and each can give a slightly different look to the image, as it changes how we collect samples over time.</p>
<p>There is plenty of material on using the different sequences that we will provide links to at the end of the chapter; right now what is important is to know that we have a sequence of two numbers that we repeat after a few frames to jitter<a id="_idTextAnchor186"/> the camera.</p>
<p>Let us say that we choose the Halton sequence. We first want to calculate the values for <code>x</code> and <code>y</code>:</p>
<pre class="source-code">
   f32 jitter_x = halton( jitter_index, 2 );
   f32 jitter_y = halton( jitter_index, 3 );</pre>
<p>These values are in the <code>[0,1]</code> range, but we want to jitter in both directions, so we map it to the <code>[-</code><code>1.1]</code> range:</p>
<pre class="source-code">
    f32 jitter_offset_x = jitter_x * 2 - 1.0f;
    f32 jitter_offset_y = jitter_y * 2 - 1.0f;</pre>
<p>We now apply them to the apply <code>jitter</code> method, with a caveat: we want to add sub-pixel jittering, thus we need to divide these offsets by the screen resolution:</p>
<pre class="source-code">
game_camera.apply_jittering( jitter_offset_x / gpu.swapchain_width, jitter_offset_y / gpu.swapchain_height );</pre>
<p>Finally, we have a jitter period to choose after how many frames we repeat the jittering numbers, updated like this:</p>
<pre class="source-code">
jitter_index = ( jitter_index + 1 ) % jitter_period;</pre>
<p>A good period is normally four frames, but in the accompanying code, there is the possibility to change this number and see the effect on the rendering image.</p>
<p>Another fundamental thing to do is to cache previous and current jittering values and send them to<a id="_idIndexMarker584"/> the GPU, so that motion vectors take into consideration the full movement.</p>
<p>We’ve added <code>jitter_xy</code> and <code>previous_jitter_xy</code> as variables in the scene uniforms to be accessed in all shaders.</p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor187"/>Adding motion vectors</h2>
<p>Now that we<a id="_idIndexMarker585"/> correctly jittered the camera and saved the offsets, it is time to add motion vectors to properly read the color data from the previous frame. There are two sources of motion: camera motion and dynamic object motion.</p>
<p>We added a velocity texture with R16G16 format to store the per-pixel velocity. For each frame, we clear that to <code>(0,0)</code> and we calculate the diff<a id="_idTextAnchor188"/>erent motions. For camera motion, we will calculate the current and previous screen space position, considering the jitter and the motion vector.</p>
<p>We will perform this in a compute shader:</p>
<pre class="source-code">
layout (local_size_x = 8, local_size_y = 8, local_size_z =
        1) in;
void main() {
    ivec3 pos = ivec3(gl_GlobalInvocationID.xyz);
    // Read the raw depth and reconstruct NDC coordinates.
    const float raw_depth = texelFetch(global_textures[
        nonuniformEXT(depth_texture_index)], pos.xy, 0).r;
    const vec2 screen_uv = uv_nearest(pos.xy, resolution);
    vec4 current_position_ndc = vec4(
        ndc_from_uv_raw_depth( screen_uv, raw_depth ), 1.0f
        );
    // Reconstruct world position and previous NDC position
    const vec3 pixel_world_position =
        world_position_from_depth
           (screen_uv, raw_depth, inverse_view_projection);
    vec4 previous_position_ndc = previous_view_projection *
        vec4(pixel_world_position, 1.0f);
    previous_position_ndc.xyz /= previous_position_ndc.w;
    // Calculate the jittering difference.
    vec2 jitter_difference = (jitter_xy –
                              previous_jitter_xy)* 0.5f;
    // Pixel velocity is given by the NDC [-1,1] difference
       in X and Y axis
    vec2 velocity = current_position_ndc.xy –
                    previous_position_ndc.xy;
    // Take in account jittering
    velocity -= jitter_difference;
    imageStore( motion_vectors, pos.xy, vec4(velocity, 0,
                                             0) );</pre>
<p>Dynamic meshes need an additional output to be written in the vertex or mesh shaders, with similar calculations done in the camera motion shader:</p>
<pre class="source-code">
// Mesh shader version
gl_MeshVerticesNV[ i ].gl_Position = view_projection *
    (model * vec4(position, 1));
vec4 world_position = model * vec4(position, 1.0);
vec4 previous_position_ndc = previous_view_projection *
    vec4(world_position, 1.0f);
previous_position_ndc.xyz /= previous_position_ndc.w;
vec2 jitter_difference = (jitter_xy - previous_jitter_xy) *
                          0.5f;
vec2 velocity = gl_MeshVerticesNV[ i ].gl_Position.xy –
    previous_position_ndc.xy - jitter_difference;
vTexcoord_Velocity[i] = velocity;</pre>
<p>And after this, just writing the velocity to its own render target will be all that is needed.</p>
<p>Now that we <a id="_idIndexMarker586"/>have the motion vectors, we can finally see the im<a id="_idTextAnchor189"/>plementation of an extremely basic TAA shader.</p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor190"/>First implementation code</h2>
<p>We again run a <a id="_idIndexMarker587"/>compute shader to calculate TAA. The implementation of the simplest possible shader is the following:</p>
<pre class="source-code">
vec3 taa_simplest( ivec2 pos ) {
    const vec2 velocity = sample_motion_vector( pos );
    const vec2 screen_uv = uv_nearest(pos, resolution);
    const vec2 reprojected_uv = screen_uv - velocity;
    vec3 current_color = sample_color(screen_uv.xy).rgb;
    vec3 history_color =
        sample_history_color(reprojected_uv).rgb;
    // source_weight is normally around 0.9.
    return mix(current_color, previous_color,
               source_weight);
}</pre>
<p>Going through the code, the steps are simple:</p>
<ol>
<li value="1">Sample the velocity at the pixel position.</li>
<li>Sample the current color at the pixel position.</li>
<li>Sample the history color at the previous pixel position, calculated using the motion vectors.</li>
<li>Mix the colors, taking something like 10% of the current frame colors.</li>
</ol>
<p>Before moving<a id="_idIndexMarker588"/> on to any improvement it is paramount to have this working perfectly.</p>
<p>You should see a blurrier image with a big problem: ghosting when moving the camera or an object. If the camera and the scene are static, there should be no pixel movement. This is fundamental to knowing that jittering and reprojection are working properly.</p>
<p>With this implementation working, we are now ready to see the different improvement areas to have a more solid TAA.</p>
<h1 id="_idParaDest-175">Improving T<a id="_idTextAnchor191"/>AA</h1>
<p>There are <a id="_idIndexMarker589"/>five areas to improve TAA: reprojection, history sampling, scene sampling, history constraint, and resolve.</p>
<p>Each one has different parameters to be tweaked that can suit the rendering needs of a project – TAA is not exact or perfect, thus some extra care from a visual perspective needs to be taken into account.</p>
<p>Let’s see the <a id="_idIndexMarker590"/>different areas in detail so that the accompanying code will be clearer.</p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor192"/>Reprojection</h2>
<p>The first thing to<a id="_idIndexMarker591"/> do is to improve reprojection and thus calculate the coordinates to read the velocity to drive the <em class="italic">History </em><em class="italic">sampling</em> section.</p>
<p>To calculate the history texture pixel coordinates, the most common solution is to get the closest pixel in a 3x3 square around the current pixel, as an idea by Brian Karis. We will read the depth texture and use the depth value as a way to determine the closest pixel, and cache the <code>x</code> and <code>y</code> position of that pixel:</p>
<pre class="source-code">
void find_closest_fragment_3x3(ivec2 pixel, out ivec2
                               closest_position, out
                               float closest_depth) {
    closest_depth = 1.0f;
    closest_position = ivec2(0,0);
    for (int x = -1; x &lt;= 1; ++x ) {
        for (int y = -1; y &lt;= 1; ++y ) {
            ivec2 pixel_position = pixel + ivec2(x, y);
                pixel_position = clamp(pixel_position,
                    ivec2(0), ivec2(resolution.x - 1,
                        resolution.y - 1));
            float current_depth =
                texelFetch(global_textures[
                    nonuniformEXT(depth_texture_index)],
                        pixel_position, 0).r;
            if ( current_depth &lt; closest_depth ) {
                closest_depth = current_depth;
                closest_position = pixel_position;
            }
        }
    }
}</pre>
<p>By just using the found pixel position as the read coordinate for the motion vectors, ghosting will be <a id="_idIndexMarker592"/>much less visible, and edges will be smoother:</p>
<pre class="source-code">
        float closest_depth = 1.0f;
        ivec2 closest_position = ivec2(0,0);
        find_closest_fragment_3x3( pos.xy,
                                   closest_position,
                                   closest_depth );
        const vec2 velocity = sample_motion_vector
            (closest_position.xy);
        // rest of the TAA shader</pre>
<p>There can be other ways of reading the velocity, but this has proven to be the best trade-off between quality and performance. Another way to experiment would be to use the maximum velocity in a similar 3x3 neighborhood of pixels.</p>
<p>There is no perfect solution, and thus experimentation and parametrization of the rendering technique are highly encouraged. After we have calculated the pixel position of the history texture to read, we can finally sample it.</p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor193"/>History sampling</h2>
<p>In this case, the <a id="_idIndexMarker593"/>simplest thing to do is to just read the history texture at the calculated position. The reality is that we can apply a filter to enhance the visual quality of the read as well.</p>
<p>In the code, we’ve added options to try different filters, and the standard choice here is to use a Catmull-Rom filter to enhance the sampling:</p>
<pre class="source-code">
   // Sample motion vectors.
    const vec2 velocity = sample_motion_vector_point(
                          closest_position );
    const vec2 screen_uv = uv_nearest(pos.xy, resolution);
    const vec2 reprojected_uv = screen_uv - velocity;
    // History sampling: read previous frame samples and
       optionally apply a filter to it.
    vec3 history_color = vec3(0);
    history_color = sample_history_color(
                    reprojected_uv ).rgb;
    switch (history_sampling_filter) {
        case HistorySamplingFilterSingle:
            history_color = sample_history_color(
                            reprojected_uv ).rgb;
            break;
        case HistorySamplingFilterCatmullRom:
            history_color = sample_texture_catmull_rom(
                            reprojected_uv,
                            history_color_texture_index );
            break;
    }</pre>
<p>After we have the history color, we will sample the current scene color and cache information <a id="_idIndexMarker594"/>needed for both the history constraint and the final resolve phase.</p>
<p>Using the history color without further processing would result in ghosting.</p>
<h2 id="_idParaDest-178"><a id="_idTextAnchor194"/>Scene sampling</h2>
<p>At this point, ghosting<a id="_idIndexMarker595"/> is less noticeable but still present, so with a similar mentality to searching for the closest pixel, we can search around the current pixel to calculate color information and apply a filter to it.</p>
<p>Basically, we are treating a pixel like a signal instead of a simple color. The subject can be quite long and interesting and at the end of the chapter, there will be resources to dive deeper into this. Also, in this step, we will cache information used for the history boundaries used to constrain the color coming from the previous frames.</p>
<p>What we need to know is that we sample another 3x3 area around the current pixel and calculate the information necessary for the constr<a id="_idTextAnchor195"/>aint to happen. The most valuable information is the minimum and maximum color in this area, and Variance Clipping (which we will look at later on) also requires mean color and square mean color (known as <strong class="bold">moments</strong>) to be calculated to aid history constraint. Finally, we will also apply some filtering to the sampling of the color.</p>
<p>Let’s see the code:</p>
<pre class="source-code">
// Current sampling: read a 3x3 neighborhood and cache
   color and other data to process history and final
   resolve.
    // Accumulate current sample and weights.
    vec3 current_sample_total = vec3(0);
    float current_sample_weight = 0.0f;
    // Min and Max used for history clipping
    vec3 neighborhood_min = vec3(10000);
    vec3 neighborhood_max = vec3(-10000);
    // Cache of moments used in the constraint phase
    vec3 m1 = vec3(0);
    vec3 m2 = vec3(0);
    for (int x = -1; x &lt;= 1; ++x ) {
        for (int y = -1; y &lt;= 1; ++y ) {
            ivec2 pixel_position = pos + ivec2(x, y);
            pixel_position = clamp(pixel_position,
                ivec2(0), ivec2(resolution.x - 1,
                    resolution.y - 1));
            vec3 current_sample =
            sample_current_color_point(pixel_position).rgb;
            vec2 subsample_position = vec2(x * 1.f, y *
                                           1.f);
            float subsample_distance = length(
                                       subsample_position
                                       );
            float subsample_weight = subsample_filter(
                                     subsample_distance );
            current_sample_total += current_sample *
                                    subsample_weight;
            current_sample_weight += subsample_weight;
            neighborhood_min = min( neighborhood_min,
                                    current_sample );
            neighborhood_max = max( neighborhood_max,
                                     current_sample );
            m1 += current_sample;
            m2 += current_sample * current_sample;
        }
    }
vec3 current_sample = current_sample_total /
                      current_sample_weight;</pre>
<p>What all this code<a id="_idIndexMarker596"/> does is sample color, filter it, and cache information for the history constraint, and thus we are ready to move on to the next phase.</p>
<h2 id="_idParaDest-179"><a id="_idTextAnchor196"/>The history constraint</h2>
<p>Finally, we <a id="_idIndexMarker597"/>arrived at the constraint of the history sampled color. Based on previous steps we have created a range of possible color values that we consider valid. If we think of each color channel as a value, we basically created an area of valid colors that we will constraint against.</p>
<p>A constraint is a way of accepting or discarding color information coming from the history texture, reducing ghosting to almost nothing. Over time, different ways to constrain history sampled color were developed in search of better criteria to discard colors.</p>
<p>Some implementations also tried relying on depth or velocity differences, but this seems to be the more robust solution.</p>
<p>We have added four constraints to test:</p>
<ul>
<li>RGB clamp</li>
<li>RGB clip</li>
<li>Variance clip</li>
<li>Variance clip with clamped RGB</li>
</ul>
<p>The best quality is given by variance clip with the clamped RGB, but it is interesting to see the other<a id="_idIndexMarker598"/> ones, as they are the ones that were employed in the first implementations.</p>
<p>Here is the code:</p>
<pre class="source-code">
    switch (history_clipping_mode) {
        // This is the most complete and robust history
           clipping mode:
        case HistoryClippingModeVarianceClipClamp:
        default: {
            // Calculate color AABB using color moments m1
               and m2
            float rcp_sample_count = 1.0f / 9.0f;
            float gamma = 1.0f;
            vec3 mu = m1 * rcp_sample_count;
            vec3 sigma = sqrt(abs((m2 * rcp_sample_count) –
                         (mu * mu)));
            vec3 minc = mu - gamma * sigma;
            vec3 maxc = mu + gamma * sigma;
            // Clamp to new AABB
            vec3 clamped_history_color = clamp(
                                         history_color.rgb,
                                         neighborhood_min,
                                         neighborhood_max
                                         );
            history_color.rgb = clip_aabb(minc, maxc,
                                vec4(clamped_history_color,
                                1), 1.0f).rgb;
            break;
        }
    }</pre>
<p>The <code>clip_aabb</code> function<a id="_idIndexMarker599"/> is the method that constrains the sampled history color within minimum and maximum color values.</p>
<p>In brief, we are trying to build an AABB in colorspace to limit the history color to be within that range, so that the final color is more plausible compared to the current one.</p>
<p>The last step in the TAA shader is resolve, or combining current and history colors and applying some filters to generate the final pixel color.</p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor197"/>Resolve</h2>
<p>Once again, we <a id="_idIndexMarker600"/>will apply some additional filters to decide whether the previous pixel is usable or not and by how much.</p>
<p>By default, we start with using just 10% of the current frame pixel and rely on history, so without any of those filters the image will be quite blurry:</p>
<pre class="source-code">
// Resolve: combine history and current colors for final
   pixel color
    vec3 current_weight = vec3(0.1f);
    vec3 history_weight = vec3(1.0 - current_weight);</pre>
<p>The first filter we will see is the temporal one, which uses the cached neighborhood minimum and maximum colors to calculate how much to blend the current and previous colors:</p>
<pre class="source-code">
    // Temporal filtering
    if (use_temporal_filtering() ) {
        vec3 temporal_weight = clamp(abs(neighborhood_max –
                                      neighborhood_min) /
                                      current_sample,
                                      vec3(0), vec3(1));
        history_weight = clamp(mix(vec3(0.25), vec3(0.85),
                               temporal_weight), vec3(0),
                               vec3(1));
        current_weight = 1.0f - history_weight;
    }</pre>
<p>The next two filters are linked; thus, we have them together.</p>
<p>They both work with luminance, with one used to <a id="_idIndexMarker601"/>suppress so-called <strong class="bold">fireflies</strong>, or very bright single pixels that can exist in images when there is a strong source of light, while the second uses the difference in luminance to further steer the weight toward either the current or <a id="_idIndexMarker602"/>previous colors:</p>
<pre class="source-code">
    // Inverse luminance filtering
    if (use_inverse_luminance_filtering() ||
        use_luminance_difference_filtering() ) {
        // Calculate compressed colors and luminances
        vec3 compressed_source = current_sample /
            (max(max(current_sample.r, current_sample.g),
                current_sample.b) + 1.0f);
        vec3 compressed_history = history_color /
            (max(max(history_color.r, history_color.g),
                history_color.b) + 1.0f);
        float luminance_source = use_ycocg() ?
            compressed_source.r :
                luminance(compressed_source);
        float luminance_history = use_ycocg() ?
            compressed_history.r :
                luminance(compressed_history);
        if ( use_luminance_difference_filtering() ) {
            float unbiased_diff = abs(luminance_source –
            luminance_history) / max(luminance_source,
            max(luminance_history, 0.2));
            float unbiased_weight = 1.0 - unbiased_diff;
            float unbiased_weight_sqr = unbiased_weight *
                                        unbiased_weight;
            float k_feedback = mix(0.0f, 1.0f,
                                   unbiased_weight_sqr);
            history_weight = vec3(1.0 - k_feedback);
            current_weight = vec3(k_feedback);
        }
        current_weight *= 1.0 / (1.0 + luminance_source);
        history_weight *= 1.0 / (1.0 + luminance_history);
    }</pre>
<p>We combine the <a id="_idIndexMarker603"/>result using the newly calculated weights, and finally, we output the color:</p>
<pre class="source-code">
    vec3 result = ( current_sample * current_weight +
                    history_color * history_weight ) /
                    max( current_weight + history_weight,
                    0.00001 );
    return result;</pre>
<p>At this point, the shader is complete and ready to be used. In the accompanying demo, there will be many tweaking parameters to learn the differences between the different filters and steps involved.</p>
<p>One of the most common complaints about TAA is the blurriness of the image. We will see a couple of ways to improve that next.</p>
<h1 id="_idParaDest-181"><a id="_idTextAnchor198"/>Sharpening the image</h1>
<p>One thing that <a id="_idIndexMarker604"/>can be noticed in the most basic implementation, and a problem often linked to TAA, is a decrease in the sharpness of the image.</p>
<p>We have already improved it by using a filter when sampling the scene, but we can work on the final image appearance outside of TAA in different ways. We will briefly discuss three different ways to improve the sharpening of the image.</p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor199"/>Sharpness post-processing</h2>
<p>One of the<a id="_idIndexMarker605"/> ways to improve the sharpness of the image is to add a simple sharpening shader in the post-process chain.</p>
<p>The code is simple, and it is luminance based:</p>
<pre class="source-code">
    vec4 color = texture(global_textures[
                 nonuniformEXT(texture_id)], vTexCoord.xy);
    float input_luminance = luminance(color.rgb);
    float average_luminance = 0.f;
    // Sharpen
    for (int x = -1; x &lt;= 1; ++x ) {
        for (int y = -1; y &lt;= 1; ++y ) {
            vec3 sampled_color = texture(global_textures[
                nonuniformEXT(texture_id)], vTexCoord.xy +
                    vec2( x / resolution.x, y /
                        resolution.y )).rgb;
            average_luminance += luminance( sampled_color
                                          );
        }
    }
    average_luminance /= 9.0f;
    float sharpened_luminance = input_luminance –
                                average_luminance;
    float final_luminance = input_luminance +
                            sharpened_luminance *
                            sharpening_amount;
    color.rgb = color.rgb * (final_luminance /
                input_luminance);</pre>
<p>Based on this code, when<a id="_idIndexMarker606"/> the sharpening amount is <code>0</code> the image is not sharpened. The standard value is <code>1</code>.</p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor200"/>Negative mip bias</h2>
<p>A global way to <a id="_idIndexMarker607"/>reduce blurriness is to modify the <code>mipLodBias</code> field in the <code>VkSamplerCreateInfo</code> structure to be a negative number, such as <code>–0.25</code>, thus shifting the texture <strong class="bold">mip,</strong> the<a id="_idIndexMarker608"/> pyramid of progressively smaller images of a texture to higher values.</p>
<p>This should be done by considering the performance difference, as we are sampling at a higher MIP level, and if the level is too high, we could re-introduce aliasing.</p>
<p>A global engine option to tweak would be a great solution to this.</p>
<h2 id="_idParaDest-184"><a id="_idTextAnchor201"/>Unjitter texture UVs</h2>
<p>Another<a id="_idIndexMarker609"/> possible fix to sample sharper textures is to calculate the UVs as the camera was without any jittering, like so:</p>
<pre class="source-code">
vec2 unjitter_uv(float uv, vec2 jitter) {
    return uv - dFdxFine(uv) * jitter.x + dFdyFine(uv) *
        jitter.y;
}</pre>
<p>I personally did not try this method but found it interesting and something to experiment with. It was written about by Emilio Lopez in his TAA article, linked in the <em class="italic">Reference</em> section, also citing a colleague named Martin Sobek who came up with the idea.</p>
<p>The combination of TAA and sharpening drastically improves the edges of the image while retaining <a id="_idIndexMarker610"/>the details inside the objects.</p>
<p>We need to look at one last aspect of the image: banding.</p>
<h1 id="_idParaDest-185"><a id="_idTextAnchor202"/>Improving banding</h1>
<p>Banding is a <a id="_idIndexMarker611"/>problem affecting various steps in the rendering of a frame. It affects Volumetric Fog and lighting calculations, for example.</p>
<div><div><img alt="Figure 11.5 – Banding problem detail in Volumetric Fog" height="573" src="img/B18395_11_05.jpg" width="632"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Banding problem detail in Volumetric Fog</p>
<p>We can see in <em class="italic">Figure 11</em><em class="italic">.5</em> how this can be present in Volumetric Fog if no solution is implemented. A solution to remove banding in visuals is to add some dithering to various passes of the frame, but that also adds visual noise to the image.</p>
<p>Dithering is defined as the intentional addition of noise specifically to remove banding. Different type of noises can be used, as we will see in the accompaining code. Adding temporal reprojection smoothens the noise added, thus becoming one of the best ways to improve the visual quality of the image.</p>
<p>In <a href="B18395_10.xhtml#_idTextAnchor152"><em class="italic">Chapter 10</em></a>, <em class="italic">Adding Volumetric Fog</em>, we saw a very simple temporal reprojection scheme, and we have also added noise to various steps of the algorithm. We have now seen a more complex implementation of a temporal reprojection scheme to enhance the image, and it should be clearer on reasoning behind animated dithering: animating dithering gives effectively more samples, and thanks to temporal reprojection, uses them effectively. Dithering is linked to its own temporal reprojection, thus in the Volumetric Fog steps, the dithering scale can be too large to be cleaned up by TAA.</p>
<p>When applying<a id="_idIndexMarker612"/> Volumetric Fog to the scene though, we can add a small, animated dithering that increases the fog visuals while being cleaned up by TAA. Another dithering application is in the lighting shader, again at the per-pixel level and thus eligible to be cleaned up by TAA.</p>
<p class="callout-heading">Note</p>
<p class="callout">Trying to get a noise-free image is hard as the temporal reprojection uses more than one frame, thus it is not possible to show here in an image what appears banding-free in the accompanying application.</p>
<h1 id="_idParaDest-186"><a id="_idTextAnchor203"/>Summary</h1>
<p>In this chapter, we introduced the TAA rendering technique.</p>
<p>We gave an overview of the algorithm by trying to highlight the different shader steps involved. We then moved on to create the simplest possible TAA shader: an exercise to give us a deeper understanding of the technique itself.</p>
<p>Following that, we started enhancing the various steps using filters and information taken from the current scene. We encourage you to add custom filters and tweak parameters and different scenes to understand and develop the technique further.</p>
<p>An idea to experiment with could also be to apply the history constraint to the temporal reprojection phase of the Volumetric Fog, as suggested by my friend Marco Vallario a few months ago.</p>
<p>In the next chapter, we will add support for ray tracing to the Raptor Engine, a recent technological advancement that unlocks high-quality illumination techniques, which we will cover in the following chapters.</p>
<h1 id="_idParaDest-187"><a id="_idTextAnchor204"/>Further reading</h1>
<p>We touched on several topics in this chapter, from the history of post-process anti-aliasing to implementations of TAA, to banding and noise.</p>
<p>Thanks to the graphics community, which shares a lot of information on their findings, it is possible to sharpen our knowledge on this subject.</p>
<p>The following are some links to read:</p>
<ul>
<li>For an index of the evolution of Post-Process Anti-Aliasing techniques: <a href="http://www.iryoku.com/research-impact-retrospective-mlaa-from-2009-to-2017">http://www.iryoku.com/research-impact-retrospective-mlaa-from-2009-to-2017</a>.</li>
<li>The first MLAA paper: <a href="https://www.intel.com/content/dam/develop/external/us/en/documents/z-shape-arm-785403.pdf">https://www.intel.com/content/dam/develop/external/us/en/documents/z-shape-arm-785403.pdf</a>.</li>
<li>An MLAA GPU implementation: <a href="http://www.iryoku.com/mlaa/">http://www.iryoku.com/mlaa/</a>.</li>
<li>SMAA, an evolution of MLAA: <a href="http://www.iryoku.com/smaa/">http://www.iryoku.com/smaa/</a>.</li>
<li>The best article on signal processing and anti-aliasing by Matt Pettineo: <a href="https://therealmjp.github.io/posts/msaa-resolve-filters/">https://therealmjp.github.io/posts/msaa-resolve-filters/</a>.</li>
<li>Temporal Reprojection Anti-Aliasing in Inside, containing the first full documentation of a TAA technique. Includes information about history constraints and AABB clipping: <a href="http://s3.amazonaws.com/arena-attachments/655504/c5c71c5507f0f8bf344252958254fb7d.pdf?1468341463">http://s3.amazonaws.com/arena-attachments/655504/c5c71c5507f0f8bf344252958254fb7d.pdf?1468341463</a>.</li>
<li>High-Quality Temporal Supersampling, Unreal Engine TAA implementation: <a href="https://de45xmedrsdbp.cloudfront.net/Resources/files/TemporalAA_small-59732822.pdf">https://de45xmedrsdbp.cloudfront.net/Resources/files/TemporalAA_small-59732822.pdf</a>.</li>
<li>An excursion in temporal super-sampling, introducing variance clipping: <a href="https://developer.download.nvidia.com/gameworks/events/GDC2016/msalvi_temporal_supersampling.pdf">https://developer.download.nvidia.com/gameworks/events/GDC2016/msalvi_temporal_supersampling.pdf</a>.</li>
<li>A TAA article, with tips, such as UV unjittering and Mip bias: <a href="https://www.elopezr.com/temporal-aa-and-the-quest-for-the-holy-trail/">https://www.elopezr.com/temporal-aa-and-the-quest-for-the-holy-trail/</a>.</li>
<li>Another great TAA article with a full implementation: <a href="https://alextardif.com/TAA.xhtml">https://alextardif.com/TAA.xhtml</a>.</li>
<li>Banding in games: <a href="https://loopit.dk/banding_in_games.pdf">https://loopit.dk/banding_in_games.pdf</a>.</li>
</ul>
</div>
</div></body></html>