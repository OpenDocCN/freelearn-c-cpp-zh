<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-246"><a id="_idTextAnchor283"/>6</h1>
<h1 id="_idParaDest-247"><a id="_idTextAnchor284"/>Anti-Aliasing Techniques</h1>
<p>Anti-aliasing can be achieved in many ways, the most common being the one usually provided by the graphics API. In this chapter, we start by looking at how to enable and use the anti-aliasing provided by Vulkan, going over a multitude of other techniques that are more suitable for other use cases that require better anti-aliasing or that need a different algorithm altogether, such as temporal anti-aliasing. In this chapter, we will guide you through various anti-aliasing techniques, starting from enabling and using the one provided by Vulkan to exploring other more advanced and suitable methods for different use cases. The goal is to empower you with the knowledge and skills to choose and implement the most appropriate anti-aliasing technique for your specific needs, thereby <a id="_idIndexMarker456"/>improving the visual quality of your rendered <a id="_idIndexMarker457"/>graphics.</p>
<p>In this <a id="_idIndexMarker458"/>chapter, we will cover the following recipes:</p>
<ul>
<li>Enabling <a id="_idIndexMarker459"/>and using Vulkan’s MSAA</li>
<li>Applying FXAA</li>
<li>Utilizing TAA</li>
<li>Applying DLSS</li>
</ul>
<h1 id="_idParaDest-248"><a id="_idTextAnchor285"/>Technical requirements</h1>
<p>For this chapter, you will need to make sure you have VS 2022 installed along with the Vulkan SDK. Basic familiarity with the C++ programming language and an understanding of OpenGL or any other graphics API will be useful. Please revisit <a href="B18491_01.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a><em class="italic">, Vulkan Core Concepts</em>, under the <em class="italic">Technical requirements</em> section for details on setting up and building executables for this chapter. This chapter has multiple recipes, which can be launched using the following executables:</p>
<ol>
<li><code>Chapter06_MSAA.exe</code></li>
<li><code>Chapter06_FXAA.exe</code></li>
<li><code>Chapter06_TAA.exe</code></li>
<li><code>Chapter06_DLSS.exe</code></li>
</ol>
<h1 id="_idParaDest-249"><a id="_idTextAnchor286"/>Enabling and using Vulkan’s MSAA</h1>
<p>MSAA is an <a id="_idIndexMarker460"/>anti-aliasing technique that is used to reduce the jagged edges that can appear on curved lines and diagonal edges. Here’s an overview of how it works:</p>
<ol>
<li><strong class="bold">Multiple samples</strong>: Instead of sampling a pixel once (like in regular rendering), MSAA takes multiple samples within each pixel. For example, 4 x MSAA needs 4 samples while 8 x MSAA needs 8 samples. The fragment shader runs for each one of the samples, and their output is stored for processing in <em class="italic">step 3</em>.</li>
<li><strong class="bold">Edge detection</strong>: MSAA only multi-samples pixels that are at the edges of geometry. This makes it more performance-efficient compared to techniques such as super-sampling, which samples the entire image at a higher resolution.</li>
<li><strong class="bold">Combining samples</strong>: Once the samples are taken, they are averaged (or resolved) into a single-color value for the pixel. If some of the samples are within an object and some are outside it, the final pixel color will be a blend, creating a smoother transition and thereby reducing the appearance of jagged edges.</li>
</ol>
<p>In this recipe, we will describe what steps you need to take to enable MSAA in Vulkan, as it is provided by the API.</p>
<h2 id="_idParaDest-250"><a id="_idTextAnchor287"/>Getting ready</h2>
<p>Enabling MSAA <a id="_idIndexMarker461"/>in Vulkan requires changes to multiple locations in the source code. The following are high-level steps to implement MSAA:</p>
<ol>
<li>First, you need to ensure that the system supports MSAA. Additionally, you need to determine the maximum number of samples per pixel that are supported.</li>
<li>Textures need to be created with the number of samples they support.</li>
<li>Additional textures need to be created to serve as the output after combining the samples (also referred to as resolve attachments).</li>
<li>Render passes need to specify the number of samples per attachment and provide extra information about the resolve attachments.</li>
<li>Finally, framebuffers need to refer to the resolve attachments.</li>
</ol>
<p>Rendering with MSAA involves images with sample counts greater than 1. However, these multi-sampled images cannot be presented directly using <code>VK_IMAGE_LAYOUT_PRESENT_SRC_KHR</code>. The <code>VK_IMAGE_LAYOUT_PRESENT_SRC_KHR</code> layout is designed for single-sampled images that are ready for presentation, with one color value per pixel. That’s why a <em class="italic">resolve</em> operation is needed to convert the multi-sampled image into a single-sampled image. The final <a id="_idIndexMarker462"/>anti-aliased output, which is suitable for presentation, needs to be written to another image with a sample count of <code>VK_SAMPLE_COUNT_1_BIT</code>. This implies that every color attachment with a sample count greater than 1 requires an associated attachment with a sample count equal to <code>VK_SAMPLE_COUNT_1_BIT</code>. These additional attachments, known as resolve attachments, are used to store the final anti-aliased output. During the resolve operation, the values from the multi-samples are combined and written into the <em class="italic">resolve</em> attachment, creating the final single-sample image that can be presented.</p>
<h2 id="_idParaDest-251"><a id="_idTextAnchor288"/>How to do it...</h2>
<p>Enabling MSAA in Vulkan is not difficult but needs changes in multiple parts of the code. Here’s a step-by-step guide on how to do it:</p>
<ol>
<li>In the following code block, we deal with <code>VkPhysicalDeviceProperties</code> objects, specifically focusing on the <code>framebufferColorSampleCounts</code> and <code>framebufferDepthSampleCounts</code> properties. These properties help us determine the maximum number of samples per pixel supported for color and depth respectively. This capability is hardware-dependent, which makes it necessary to check it first before usage. The maximum supported value is found in the following:<pre class="source-code">
VkPhysicalDeviceProperties::limits::framebufferColorSampleCounts
VkPhysicalDeviceProperties::limits::framebufferDepthSampleCounts</pre></li> <li>The maximum number of samples is provided as a bit field of type <code>VkSampleCountFlagBits</code>, with flags such as <code>VK_SAMPLE_COUNT_1_BIT</code>, <code>VK_SAMPLE_COUNT_2_BIT</code>, <code>VK_SAMPLE_COUNT_4_BIT</code>, and so on, up to <code>VK_SAMPLE_COUNT_64_BIT</code>.</li>
<li>During image creation, the number of samples that a texture supports must be specified. This is done by setting the <code>samples</code> member of the <code>VkImageCreateInfo</code> structure, which is of type <code>VkSampleCountFlagBits</code>, such as the following:<pre class="source-code">
VkImageCreateInfo newTexture = {
  ...
  .samples = VK_SAMPLE_COUNT_8_BIT,
};</pre></li> <li>While <a id="_idIndexMarker463"/>creating a render pass, the attachment descriptions must indicate the sample count by setting the <code>VkAttachmentDescription::samples</code> field:<pre class="source-code">
VkAttachmentDescription attachment = {
  ...
  .samples = VK_SAMPLE_COUNT_8_BIT,
};</pre></li> <li>An instance of the <code>VkAttachmentDescription</code> structure needs to be added to the render pass’s list of attachments, <code>VkSubpassDescription::pColorAttachments</code>, for each resolve attachment in the render pass. The resolve attachments must have their samples field set to <code>VK_SAMPLE_COUNT_1_BIT</code>, as the resolution of the multi-sampled image results in a single sample per pixel. This is because the multiple samples from the multi-sampled image are resolved into one final color value for that pixel. Here is how you can create and configure such a <code>VkAttachmentDescription</code> instance:<pre class="source-code">
VkAttachmentDescription resolveAttachment = {
  ...
  .samples = VK_SAMPLE_COUNT_1_BIT,
}</pre></li> <li>An instance <a id="_idIndexMarker464"/>of the <code>VkAttachmentReference</code> structure must be created to reference this resolve attachment:<pre class="source-code">
VkAttachmentReference resolveAttachmentRef{
  .attachment = <strong class="bold">&lt;index of resolve texture in the attachmentDescriptor vector&gt;</strong>,
  .layout =
      VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL,
}</pre><p class="list-inset">The <code>VkAttachmentReference::attachment</code> field is an integer that points to the resolve attachment at the corresponding index of the <code>VkRenderPassCreateInfo::pAttachments</code> array.</p></li> <li>Finally, the list of attachment references that describe the resolve attachments is added to the <code>VkSubpassDescription::pResolveAttachments</code> field.<p class="list-inset"><em class="italic">Figure 6</em><em class="italic">.1</em> illustrates how each component is set up and how they are referenced by a render pass and subpass description structures. The depth/stencil attachment must have the same sample count as the color attachments, and the number of resolve attachments must be equal to the number of color attachments.</p></li>
</ol>
<div><div><img alt="Figure 6.1 – Render pass configuration" src="img/B18491_06_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Render pass configuration</p>
<p>In the <a id="_idIndexMarker465"/>preceding diagram, we showcased the configuration of texture sample count and its reference by both the render pass and subpass description structures. This arrangement is crucial for enabling MSAA in Vulkan.</p>
<h1 id="_idParaDest-252"><a id="_idTextAnchor289"/>Applying FXAA</h1>
<p>FXAA is a screen-space anti-aliasing technique that can be implemented as an extra full-screen post-process pass. FXAA works by identifying edges in the image and then smoothing <a id="_idIndexMarker466"/>them to reduce the appearance of aliasing. Without the need for any additional information from the scene, FXAA can be easily integrated into existing code. It’s also fast because it processes only the final rendered image pixels and some of their neighbors. In this recipe, you will learn about the FXAA technique. You will understand how it functions as a screen-space anti-aliasing method, how it can be applied through a post-process pass, and why it is a beneficial tool due to its ease of integration and speed. Note that FXAA is generally applied before gamma correction or any sRGB conversion. The reason for this is that FXAA works best on linear RGB data. If you apply FXAA after gamma correction or sRGB conversion, it may result in incorrect edge detection and thus less effective anti-aliasing.</p>
<h2 id="_idParaDest-253"><a id="_idTextAnchor290"/>Getting ready</h2>
<p>The FXAA algorithm is implemented in our repository by the <code>FXAAPass</code> class, found in the <code>source/enginecore/passes/FXAA.cpp</code> and <code>FXAA.hpp</code> files. The shader used by the pass is located at <code>source/enginecore/resources/shaders/fxaa.frag</code>.</p>
<h2 id="_idParaDest-254"><a id="_idTextAnchor291"/>How to do it...</h2>
<p>The algorithm can be implemented entirely in a fragment shader that uses the final render <a id="_idIndexMarker467"/>image as input. The shader also needs the size of the viewport, which can be provided as a push constant:</p>
<ol>
<li>The shader is simple in terms of input and output and only needs an input texture and the size of the viewport for processing:<pre class="source-code">
#version 460
layout(push_constant) uniform Viewport {
  uvec2 size;
}
ViewportSize;
layout(set = 0, binding = 0) uniform sampler2D
    inputTexture;
layout(location = 0) out vec4 outColor;</pre></li> <li>The FXAA algorithm operates on the luminance of the pixels, so we need a function to convert RGB values to luminance:<pre class="source-code">
float rgb2luma(vec3 rgb) {
  return dot(rgb, vec3(0.299, 0.587, 0.114));
}</pre></li> <li>Here are some constants for the edge-detection part:<pre class="source-code">
const float EDGE_THRESHOLD_MIN = (1.0 / 16.0);
const float EDGE_THRESHOLD_MAX = (1.0 / 8.0);
const float PIXEL_BLEND_LIMIT_TO_REDUCE_BLURRING =
    (3.0 / 4.0);
const float MIN_PIXEL_ALIASING_REQUIRED =
    (1.0 / 8.0);
const float NUM_LOOP_FOR_EDGE_DETECTION = 1;</pre></li> <li>To simplify the code, we’ll use an array to store the luminance and RGB values for the <a id="_idIndexMarker468"/>neighboring pixels. We’ll also use constants to help refer to elements of the vector without using just integers:<pre class="source-code">
const int Center      = 0;
const int Top         = 1;
const int Bottom      = 2;
const int Left        = 3;
const int Right       = 4;
const int TopRight    = 5;
const int BottomRight = 6;
const int TopLeft     = 7;
const int BottomLeft  = 8;
vec2 offsets[] = {
    vec2( 0, 0), vec2( 0, -1), vec2( 0,  1),
    vec2(-1, 0), vec2( 1,  0), vec2( 1, -1),
    vec2( 1, 1), vec2(-1, -1), vec2(-1,  1)};</pre></li> <li>The algorithm is encapsulated in the <code>applyFXAA</code> function that takes the screen coordinates in pixels, the rendered image to be processed, and the size of the viewport:<pre class="source-code">
vec4 applyFXAA(vec2 screenCoord,
               sampler2D inputTexture,
               uvec2 viewportSize) {</pre></li> <li>The first <a id="_idIndexMarker469"/>step is to compute the luminance and the RGB values of all eight neighboring pixels, as well as the range between the lowest and the highest luminance. If the value is below a certain threshold, we don’t perform the anti-aliasing. The <strong class="bold">threshold</strong> is used to determine whether a pixel is on an edge; its value represents the minimum difference in luminance that must exist between a pixel and its neighbors for that pixel to be considered part of an edge:<pre class="source-code">
  const vec2 viewportSizeInverse = vec2(
      1.0 / viewportSize.x, 1.0 / viewportSize.y);
  const vec2 texCoord =
      screenCoord * viewportSizeInverse;
  float minLuma = 100000000;
  float maxLuma = 0;
  float lumas[9];
  vec3 rgb[9];
  vec3 rgbSum = vec3(0, 0, 0);
  for (int i = 0; i &lt; 9; ++i) {
    rgb[i] =
        texture(inputTexture,
                texCoord +
                    offsets[i] *
                        viewportSizeInverse)
            .rgb;
    rgbSum += rgb[i];
    lumas[i] = rgb2luma(rgb[i]);
    if (i &lt; 5) {
      minLuma = min(lumas[i], minLuma);
      maxLuma = max(lumas[i], maxLuma);
    }
  }
  const float rangeLuma = maxLuma - minLuma;
  if (rangeLuma &lt;
      max(EDGE_THRESHOLD_MIN,
          EDGE_THRESHOLD_MAX * maxLuma)) {
    return vec4(rgb[Center], 1.0);
  }</pre></li> <li>The difference between the average luminance of all neighboring pixels and the center <a id="_idIndexMarker470"/>pixels tells us whether we need to perform the anti-aliasing algorithm and the amount of blending required. It also clamps the blend amount between 0 and <code>PIXEL_BLEND_LIMIT_TO_REDUCE_BLURRING</code> to reduce blurring:<pre class="source-code">
  const float lumaTopBottom =
      lumas[Top] + lumas[Bottom];
  const float lumaLeftRight =
      lumas[Left] + lumas[Right];
  const float lumaTopCorners =
      lumas[TopLeft] + lumas[TopRight];
  const float lumaBottomCorners =
      lumas[BottomLeft] + lumas[BottomRight];
  const float lumaLeftCorners =
      lumas[TopLeft] + lumas[BottomLeft];
  const float lumaRightCorners =
      lumas[TopRight] + lumas[BottomRight];
  const float lumaTBLR =
      lumaTopBottom + lumaLeftRight;
  const float averageLumaTBLR = (lumaTBLR) / 4.0;
  const float lumaSubRange =
      abs(averageLumaTBLR - lumas[Center]);
  float pixelblendAmount =
      max(0.0, (lumaSubRange / rangeLuma) -
                   MIN_PIXEL_ALIASING_REQUIRED);
  pixelblendAmount = min(
      PIXEL_BLEND_LIMIT_TO_REDUCE_BLURRING,
      pixelblendAmount *
          (1.0 /
           (1.0 - MIN_PIXEL_ALIASING_REQUIRED)));</pre></li> <li>The next step consists of determining whether the edge is more vertical than horizontal <a id="_idIndexMarker471"/>and initializing the variables that will be used to find the edge endpoints, with the <code>findEndPointPosition</code> function:<pre class="source-code">
  const vec3 averageRGBNeighbor =
      rgbSum * (1.0 / 9.0);
  const float verticalEdgeRow1 =
      abs(-2.0 * lumas[Top] + lumaTopCorners);
  const float verticalEdgeRow2 =
      abs(-2.0 * lumas[Center] + lumaLeftRight);
  const float verticalEdgeRow3 = abs(
      -2.0 * lumas[Bottom] + lumaBottomCorners);
  const float verticalEdge =
      (verticalEdgeRow1 + verticalEdgeRow2 * 2.0 +
       verticalEdgeRow3) /
      12.0;
  const float horizontalEdgeCol1 =
      abs(-2.0 * lumas[Left] + lumaLeftCorners);
  const float horizontalEdgeCol2 =
      abs(-2.0 * lumas[Center] + lumaTopBottom);
  const float horizontalEdgeCol3 =
      abs(-2.0 * lumas[Right] + lumaRightCorners);
  const float horizontalEdge =
      (horizontalEdgeCol1 +
       horizontalEdgeCol2 * 2.0 +
       horizontalEdgeCol3) /
      12.0;
  const bool isHorizontal =
      horizontalEdge &gt;= verticalEdge;
  const float luma1 =
      isHorizontal ? lumas[Top] : lumas[Left];
  const float luma2 =
      isHorizontal ? lumas[Bottom] : lumas[Right];
  const bool is1Steepest =
      abs(lumas[Center] - luma1) &gt;=
      abs(lumas[Center] - luma2);
  float stepLength =
      isHorizontal ? -screenCoordToTextureCoord.y
                   : -screenCoordToTextureCoord.x;
  float lumaHighContrastPixel;
  if (is1Steepest) {
    lumaHighContrastPixel = luma1;
  } else {
    lumaHighContrastPixel = luma2;
    // Also reverse the direction:
    stepLength = -stepLength;
  }
  vec2 outPosToFetchTexelForEdgeAntiAliasing;
  vec3 rgbEdgeAntiAliasingPixel = rgb[Center];</pre></li> <li>The <code>findEndPointPosition</code> function returns <code>1</code> if it deems antialiasing is needed and <code>0</code> otherwise. It also returns the coordinate of the texel that will be blended with the pixel being anti-aliased. We will investigate the <code>findEndPointPosition</code> function in <em class="italic">step 11</em>:<pre class="source-code">
  const float res = findEndPointPosition(
      inputTexture, texCoord,
      lumas[Center], lumaHighContrastPixel,
      stepLength, screenCoordToTextureCoord,
      isHorizontal,
      outPosToFetchTexelForEdgeAntiAliasing);</pre></li> <li>If the <a id="_idIndexMarker472"/>return value is <code>1.0</code>, we perform the antialiasing by blending the original pixel’s color with the color from the texel at the <code>outPosToFetchTexelForEdgeAntiAliasing</code> coordinate. The blending factor to use (<code>pixelblendAmount</code>) was computed previously, in <em class="italic">step 7</em>:<pre class="source-code">
  if (res == 1.0) {
    rgbEdgeAntiAliasingPixel =
        texture(
            inputTexture,
            outPosToFetchTexelForEdgeAntiAliasing)
            .rgb;
  }
  return vec4(mix(rgbEdgeAntiAliasingPixel,
                  averageRGBNeighbor,
                  pixelblendAmount),
              1.0);
}</pre></li> <li>The <code>findEndPointPosition</code> function performs an important task – it traverses the image in search of edge endpoints, moving in both directions from the central pixel that’s being processed. To accomplish this, it requires several pieces of information. First, it needs the texture that’s being processed, which is the image that the function will traverse. Next, it requires the coordinate of the pixel being processed, which serves as the starting point for the function’s traversal. The function also needs to know the luminance, or brightness, of the pixel. In addition, it must be aware of the luminance of the highest contrast pixel, an element that is determined based on whether the edge being examined <a id="_idIndexMarker473"/>is more horizontal or more vertical. Another crucial piece of information is the step length, which, like the luminance of the highest contrast pixel, is also dependent on the angle of the edge. The function needs the length of one pixel in texture coordinates for accurate image traversal. Finally, it requires a flag that indicates whether the edge is more horizontal or more vertical to correctly understand the edge’s orientation. It returns <code>1</code> if it deems anti-aliasing needs to be performed and <code>0</code> otherwise. It also returns the coordinate of the pixel that contains the RGB value to be used for the anti-aliasing:<pre class="source-code">
float findEndPointPosition(
    sampler2D inputTexture,
    vec2 textureCoordMiddle, float lumaMiddle,
    float lumaHighContrastPixel, float stepLength,
    vec2 screenCoordToTextureCoord,
    bool isHorizontal,
    out vec2
        outPosToFetchTexelForEdgeAntiAliasing) {</pre></li> <li>Depending on whether the edge is horizontal or not, the function initializes the direction <a id="_idIndexMarker474"/>and position of the high-contrast pixel:<pre class="source-code">
  vec2 textureCoordOfHighContrastPixel =
      textureCoordMiddle;
  // Direction of the edge
  vec2 edgeDir;
  if (isHorizontal) {
    textureCoordOfHighContrastPixel.y =
        textureCoordMiddle.y + stepLength;
    textureCoordOfHighContrastPixel.x =
        textureCoordMiddle.x;
    edgeDir.x = screenCoordToTextureCoord.x;
    edgeDir.y = 0.0;
  } else {
    textureCoordOfHighContrastPixel.x =
        textureCoordMiddle.x + stepLength;
    textureCoordOfHighContrastPixel.y =
        textureCoordMiddle.y;
    edgeDir.y = screenCoordToTextureCoord.y;
    edgeDir.x = 0.0;
  }</pre></li> <li>Before we proceed to start looking for the edge endpoints, we need to set up some variables used in the loop:<pre class="source-code">
  // Prepare for the search loop:
  float lumaHighContrastPixelNegDir;
  float lumaHighContrastPixelPosDir;
  float lumaMiddlePixelNegDir;
  float lumaMiddlePixelPosDir;
  bool doneGoingThroughNegDir = false;
  bool doneGoingThroughPosDir = false;
  vec2 posHighContrastNegDir =
      textureCoordOfHighContrastPixel - edgeDir;
  vec2 posHighContrastPosDir =
      textureCoordOfHighContrastPixel + edgeDir;
  vec2 posMiddleNegDir =
      textureCoordMiddle - edgeDir;
  vec2 posMiddlePosDir =
      textureCoordMiddle + edgeDir;</pre></li> <li>The loop iterates a maximum of <code>NUM_LOOP_FOR_EDGE_DETECTION</code> times. It checks <a id="_idIndexMarker475"/>for edges by looking at the luminance differences in both the positive and negative directions from the middle pixel. The edge is detected when the luminance difference between two consecutive points in one direction exceeds a threshold (we will look at the <code>processDirection</code> function in <em class="italic">step 20</em>):<pre class="source-code">
    for (int i = 0; i &lt; NUM_LOOP_FOR_EDGE_DETECTION;
       ++i) {
    // Negative direction processing
    if (!doneGoingThroughNegDir) {
      processDirection(doneGoingThroughNegDir,
                       posHighContrastNegDir,
                       posMiddleNegDir, -edgeDir,
                       lumaHighContrastPixel,
                       lumaMiddle);
    }
    // Positive direction processing
    if (!doneGoingThroughPosDir) {
      processDirection(doneGoingThroughPosDir,
                       posHighContrastPosDir,
                       posMiddlePosDir, edgeDir,
                       lumaHighContrastPixel,
                       lumaMiddle);
    }
    // If both directions are done, exit the loop
    if (doneGoingThroughNegDir &amp;&amp;
        doneGoingThroughPosDir) {
      break;
    }
  }</pre></li> <li>The function <a id="_idIndexMarker476"/>now calculates the distances from the middle pixel to the detected edge endpoints, in both the negative and positive directions:<pre class="source-code">
  float dstNeg;
  float dstPos;
  if (isHorizontal) {
    dstNeg =
        textureCoordMiddle.x - posMiddleNegDir.x;
    dstPos =
        posMiddlePosDir.x - textureCoordMiddle.x;
  } else {
    dstNeg =
        textureCoordMiddle.y - posMiddleNegDir.y;
    dstPos =
        posMiddlePosDir.y - textureCoordMiddle.y;
  }</pre></li> <li>It also <a id="_idIndexMarker477"/>checks which endpoint is closer to the middle pixel:<pre class="source-code">
  bool isMiddlePixelCloserToNeg = dstNeg &lt; dstPos;
  float dst = min(dstNeg, dstPos);
  float lumaEndPointOfPixelCloserToMiddle =
      isMiddlePixelCloserToNeg
          ? lumaMiddlePixelNegDir
          : lumaMiddlePixelPosDir;</pre></li> <li>Anti-aliasing is deemed necessary, based on the luminance difference between the endpoint that is closer to the middle pixel and the middle pixel itself:<pre class="source-code">
  bool edgeAARequired =
      abs(lumaEndPointOfPixelCloserToMiddle -
          lumaHighContrastPixel) &lt;
      abs(lumaEndPointOfPixelCloserToMiddle -
          lumaMiddle);</pre></li> <li>Using the <a id="_idIndexMarker478"/>distances to the edge endpoints, the following code snippet calculates the pixel offset that is required for anti-aliasing:<pre class="source-code">
  float negInverseEndPointsLength =
      -1.0 / (dstNeg + dstPos);
  float pixelOffset =
      dst * negInverseEndPointsLength + 0.5;
  outPosToFetchTexelForEdgeAntiAliasing =
      textureCoordMiddle;
  if (isHorizontal) {
    outPosToFetchTexelForEdgeAntiAliasing.y +=
        pixelOffset * stepLength;
  } else {
    outPosToFetchTexelForEdgeAntiAliasing.x +=
        pixelOffset * stepLength;
  }</pre></li> <li>The function returns <code>1.0</code> if edge anti-aliasing is required and <code>0.0</code> otherwise:<pre class="source-code">
  return edgeAARequired ? 1.0 : 0.0;
}</pre></li> <li>The <code>processDirection</code> inspects the luma values of pixels in a certain direction (given by <code>edgeIncrement</code>) to check for high contrast or edges. It will continue to inspect positions in this direction until a certain contrast condition is met. Once the condition is met, it will set the <code>doneGoingThroughDir</code> flag to <code>true</code>, signaling <a id="_idIndexMarker479"/>that it’s done processing in this direction:<pre class="source-code">
void processDirection(inout bool doneGoingThroughDir,
                      inout vec2 posHighContrast,
                      inout vec2 posMiddle,
                      float edgeIncrement,
                      float lumaHighContrastPixel,
                      float lumaMiddle) {
  float lumaHighContrastPixelDir = rgb2luma(
      texture(inputTexture, posHighContrast).rgb);
  float lumaMiddlePixelDir = rgb2luma(
      texture(inputTexture, posMiddle).rgb);
  doneGoingThroughDir =
      abs(lumaHighContrastPixelDir -
          lumaHighContrastPixel) &gt;
          abs(lumaHighContrastPixelDir -
              lumaMiddle) ||
      abs(lumaMiddlePixelDir - lumaMiddle) &gt;
          abs(lumaMiddlePixelDir -
              lumaHighContrastPixel);
  // Update position for next iteration if not
  // done
  if (!doneGoingThroughDir) {
    posHighContrast += edgeIncrement;
    posMiddle += edgeIncrement;
  }
}</pre></li> <li>The <a id="_idIndexMarker480"/>fragment code calls <code>applyFXAA</code>, which returns the new color to output from the shader:<pre class="source-code">
void main() {
  outColor =
      applyFXAA(gl_FragCoord.xy, inputTexture,
                ViewportSize.size);
}</pre></li> </ol>
<p>And there you have it – the recipe for applying FXAA, a powerful tool for smoothing out jaggies in your graphics. As we wrap this up, remember that the beauty of FXAA lies not just in its ability to enhance visual output but also in its flexibility and ease of integration into existing systems.</p>
<h1 id="_idParaDest-255"><a id="_idTextAnchor292"/>Utilizing TAA</h1>
<p>Unlike the previously discussed anti-aliasing methods, which only consider spatial information, TAA is based on temporal information – that is, it utilizes both the current and previous <a id="_idIndexMarker481"/>frames to smooth out these aliasing artifacts. The reason aliasing artifacts happens is because of insufficient samples; TAA solves this by sampling data over the frame sequence, significantly reducing the pressure on a single frame.</p>
<p>The basic idea is to apply subpixel jittering – that is, slightly shift the projection matrix of the camera for each new frame. This results in slightly different viewpoints for each frame, giving us more information about the scene than a static viewpoint would. When sampling textures during rendering, the resulting color value can be different due to the jitter. This creates a different aliasing pattern per frame, which, when accumulated over time, averages out and reduces the visible aliasing in the scene. This is demonstrated in the following screenshot in <em class="italic">Figure 6</em><em class="italic">.2</em>:</p>
<div><div><img alt="Figure 6.2 – A temporal anti-aliasing overview" src="img/B18491_06_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – A temporal anti-aliasing overview</p>
<p>The concept outlined here performs exceptionally well for static scenes. However, in scenarios <a id="_idIndexMarker482"/>where either the objects or the camera is in motion, consecutive frames can exhibit substantial differences. This can lead to a visual artifact where moving objects appear to leave behind a series of their <em class="italic">ghosts</em>, creating what is <a id="_idIndexMarker483"/>known as the <strong class="bold">ghosting</strong> effect.</p>
<p>To get rid of ghosting, we use <a id="_idIndexMarker484"/>what is commonly called a <strong class="bold">velocity</strong> buffer, and the motion in the scene is captured using motion vectors. For each pixel, a motion vector is calculated that represents how much a pixel has moved compared to the previous frame. The result is a velocity buffer that stores these motion vectors. The previously rendered frame is then re-projected onto the current frame using the velocity buffer. This means that for each pixel, the color of the corresponding pixel in the previous frame is looked up using the motion vector. This color is then blended with the current color of the pixel, which results in a smoothing of the colors over time.</p>
<p><em class="italic">Figure 6</em><em class="italic">.3</em> shows a high-level overview of the TAA algorithm:</p>
<div><div><img alt="Figure 6.3 – A TAA frame overview" src="img/B18491_06_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – A TAA frame overview</p>
<p>In this recipe, you will learn how to implement TAA, an advanced technique that can significantly reduce flickering and provide smoother visuals in your graphics. You’ll understand the intricacies of TAA and how to adeptly integrate it into your code, adding another powerful tool to your graphics rendering toolbox.</p>
<h2 id="_idParaDest-256"><a id="_idTextAnchor293"/>Getting ready</h2>
<p>In the repository, the TAA algorithm is implemented by the <code>TAAComputePass</code> class, located in the <code>source/enginecore/passes/TAAComputePass.hpp</code> and <code>cpp</code> files. The shaders <a id="_idIndexMarker485"/>are implemented using a compute shader, located in <code>source/enginecore/resources/shaders/taaresolve.comp</code> and<code> source/enginecore/resources/shaders/taahistorycopyandsharpen.comp</code>. TAA example can be launched by running the <code>Chapter06_TAA</code> executable.</p>
<p><em class="italic">Figure 6</em><em class="italic">.4</em> illustrates the flow of the TAA algorithm:</p>
<div><div><img alt="Figure 6.4 – A TAA algorithm in a deferred renderer" src="img/B18491_06_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – A TAA algorithm in a deferred renderer</p>
<p>TAA is implemented as a two-step compute shader:</p>
<ol>
<li>The first step is the TAA resolve shader, which takes <code>ColorTexture</code>, <code>DepthTexture</code>, <code>HistoryTexture</code>, and <code>VelocityTexture</code> as input and writes to an intermediate image. The velocity, color, and depth texture are produced from the <code>Gbuffer</code> pass in the given example; however, conceptually, the same could be produced in forward rendering as well.</li>
<li>The second <a id="_idIndexMarker486"/>step is running a compute shader that is responsible for the following:<ol><li class="Alphabets">Copying the results of the previously produced intermediate texture into a history texture.</li><li class="Alphabets">Refining these intermediate results, we don’t need to generate an additional texture to store the refined results, instead, we can utilize the provided <code>ColorTexture</code> in the TAA resolve shader. This is the same <code>ColorTexture</code> that eventually gets displayed. A known downside of TAA is the potential of causing a minor blur in the image. To mitigate this, a sharpening filter is applied post-TAA. This sharpening phase is designed to intensify the edges and intricate details in the image, thereby reinstating some of the sharpness that might have been compromised during the TAA resolve process.</li></ol></li>
</ol>
<h2 id="_idParaDest-257"><a id="_idTextAnchor294"/>How to do it...</h2>
<p>To implement TAA, we first need to construct a jitter matrix. This matrix will be used in tandem with the <strong class="bold">Model-View-Projection</strong> (<strong class="bold">MVP</strong>) matrix during the rendering process. Furthermore, we will <a id="_idIndexMarker487"/>need color, depth, and velocity buffers. Conveniently, these buffers are already generated as part of the G-buffer pipeline, which we implemented in <a href="B18491_04.xhtml#_idTextAnchor241"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Techniques for Lighting, Shading, and Shadows</em> in the <em class="italic">Implementing the G-buffer for deferred </em><em class="italic">rendering</em> recipe:</p>
<ol>
<li>The <code>TAAComputePass::init</code> method is in charge of initializing various resources. It establishes two pipelines – one for resolving to an output color, and another for transferring the output color into a history texture and enhancing the output color’s sharpness.</li>
<li>The majority of work happens in the <code>TAAComputePass::doAA</code> function. This function simply operates the resolve compute pipeline, followed by the pipeline that handles the copying of the history texture and the sharpening of the output color. We’ve highlighted key components of the <code>doAA</code> function as follows, omitting <a id="_idIndexMarker488"/>less critical parts to avoid verbosity:<pre class="source-code">
void TAAComputePass::doAA(VkCommandBuffer cmd,
                          int frameIndex,
                          int isCamMoving) {
  pipeline_-&gt;bind(cmd);
  outColorTexture_-&gt;transitionImageLayout(
      cmd, VK_IMAGE_LAYOUT_GENERAL);
  historyTexture_-&gt;transitionImageLayout(
      cmd, VK_IMAGE_LAYOUT_GENERAL);
  vkCmdDispatch(…);
  VkImageMemoryBarrier barriers[2] = {
      {
        …
  };
  vkCmdPipelineBarrier(
      cmd, VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
      VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT, 0, 0,
      nullptr, 0, nullptr, 2, barriers);
  colorTexture_-&gt;transitionImageLayout(
      cmd, VK_IMAGE_LAYOUT_GENERAL);
  sharpenPipeline_-&gt;bind(cmd);
  vkCmdDispatch(…);
  colorTexture_-&gt;transitionImageLayout(
      cmd,
      VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL);
  outColorTexture_-&gt;transitionImageLayout(
      cmd,
      VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL);
}</pre></li> <li>The actual <a id="_idIndexMarker489"/>magic happens in the two compute shaders; specifically, the resolve shader is the most important. The resolve shader is implemented in <code>taaresolve.comp</code>. Let’s look at how the shaders work.</li>
<li>First, we will expand on some auxiliary functions. <code>catmullRomTextureFiltering</code> helps smooth out the temporal aliasing by blending the colors of pixels between frames, using Catmull-Rom interpolation. Catmull-Rom interpolation is a form of cubic interpolation that provides a smoother appearance than linear interpolation. The function uses the Catmull-Rom weights (<code>w0, w1, w2, w3</code>) to calculate the weight for the center (<code>w12</code>) and the offset from the center (<code>offset12</code>). Then, the function calculates three new texture positions (<code>texPos0</code>, <code>texPos3</code>, and <code>texPos12</code>) and adjusts these positions to match the texture resolution. The function then uses these weights and texture positions to calculate the resulting pixel color, by accessing the history buffer texture at the specific positions, multiplying the retrieved color by the respective weights, and summing them together.<p class="list-inset">The <code>varianceClampColor</code> function is used in anti-aliasing TAA to deal with issues of ghosting and blurring that can occur, due to the temporal reprojection of color data. The function works by limiting the color value of a given pixel, based on the color variance of its surrounding pixels. It loops over a 3x3 neighborhood around the current pixel. For each neighboring pixel, the function retrieves its color data (<code>neighColor</code>) and calculates a weight (<code>w</code>), based on the Euclidean distance from the current pixel. This weight is designed to give closer pixels more influence over the final color result.</p><p class="list-inset">The <code>calculateBlendFactor</code> function is responsible for performing calculations to determine the blend factor of a pixel, based on its velocity and luminance. Firstly, the pixel movement is calculated on two levels, the overall motion and the tiny subpixel motion, resulting in values called <code>subpixelMotion</code> and <code>dynamicBlendFactor</code> respectively. Then, to adjust the pixel’s brightness or luminance, the difference between the current color and <a id="_idIndexMarker490"/>the previous frame color is determined. This entire process enhances the realism of the pixel’s movement and color changes over time, significantly improving the overall image quality when there’s movement of objects or camera. The implementation of <code>catmullRomTextureFiltering</code> and <code>varianceClampColor</code> are very verbose; we suggest looking at <code>taaresolve.comp</code> for implementation details.</p></li>
<li>Next, we present the <code>main</code> function; this helps to produce a smoother, more stable image by reducing flickering and ghosting artifacts that can occur, due to the rapid movement of the camera or objects in a scene. The following sub-steps will walk you through the specifics of its implementation:<ol><li class="Alphabets">Calculate the closest depth and corresponding velocity around the current pixel:</li></ol><pre class="source-code">
void main() {
  vec2 velocity;
  const float closestDepth =
      closestVelocityAndDepth(velocity);</pre><ol><li class="Alphabets" value="2">Reproject the current pixel position to its position in the previous frame using the calculated velocity:</li></ol><pre class="source-code">  vec2 reprojectedUV = uv - velocity;</pre><ol><li class="Alphabets" value="3">Calculate <code>velocityLerp</code> using the history buffer at the reprojected location. Note the use of <code>taaConstData.isFirstFrame</code>, which helps to determine whether we are dealing with the first frame of the sequence or not. If it is the first frame, <code>velocityLerp</code> is simply initialized to <code>0.0f</code>. In the context of camera cuts or teleports (i.e., a sudden change from one perspective to another), the first frame assumption is also applicable. Whenever these events occur, the scene changes dramatically from one frame to another. In such cases, it’s beneficial to treat the frame right after the cut <a id="_idIndexMarker491"/>or teleport as a first frame. This is because the data from the previous frame is no longer a good reference for the current frame, due to the drastic changes in scene content:</li></ol><pre class="source-code">  float velocityLerp =
      (taaConstData.isFirstFrame != 0)
          ? texture(inHistoryBuffer,
                    reprojectedUV)
                .w
          : 0.0f;</pre><ol><li class="Alphabets" value="4">Load the current frame color (<code>colorIn</code>) and calculate <code>colorHistory</code> using <code>catmullRomTextureFiltering</code>:</li></ol><pre class="source-code">  vec3 colorIn = getColorData(
      ivec2(gl_GlobalInvocationID.xy));
  vec3 colorHistory = catmullRomTextureFiltering(
      reprojectedUV, vec2(workSize));</pre><ol><li class="Alphabets" value="5">Define two constants, <code>boxSizeWhenMoving</code> and <code>boxSizeWhenStationary</code>. We determine value of <code>boxSize</code> based on whether the camera is moving or not and is interpolated between the stationary and moving values, based on <code>velocityLerp</code>:</li></ol><pre class="source-code">  const float boxSizeWhenMoving = 2000.0f;
  const float boxSizeWhenStationary = 100.0f;
  float boxSize =
      (taaConstData.isCameraMoving == 0)
          ? boxSizeWhenStationary
          : mix(boxSizeWhenStationary,
                boxSizeWhenMoving, velocityLerp);
  boxSize = mix(
      0.5f, boxSize,
      noGeometry ? 0.0f
                 : smoothstep(0.02f, 0.0f,
                              length(velocity)));</pre><ol><li class="Alphabets" value="6">Historical <a id="_idIndexMarker492"/>color (<code>colorHistory</code>) is then clamped using the <code>varianceClampColor</code> function to ensure that the color is within a certain range, based on the variance of surrounding pixels:</li></ol><pre class="source-code">  vec3 clampHistory =
      varianceClampColor(colorHistory, boxSize);</pre><ol><li class="Alphabets" value="7">Calculate <code>blendFactor</code>, which determines how much of the current color and the historical color should be used to get the final color:</li></ol><pre class="source-code">  float blendFactor = calculateBlendFactor(
      closestDepth, velocity, noGeometry,
      workSize, colorIn, clampHistory,
      velocityLerp);</pre><ol><li class="Alphabets" value="8">Compute the final color (<code>colorResolve</code>) as a mix of the clamped historical color and the current color, based on <code>blendFactor</code>; also, store <code>velocityLerp</code> in the <code>alpha</code> channel:</li></ol><pre class="source-code">  vec3 colorResolve =
      mix(clampHistory, colorIn, blendFactor);
  imageStore(outColorImage,
             ivec2(gl_GlobalInvocationID.xy),
             vec4(colorResolve, velocityLerp));
}</pre></li> <li>Next, we will show how <code>taahistorycopyandsharpen.comp</code> works; this shader is <a id="_idIndexMarker493"/>responsible for copying the data into history texture, as well as sharpening the results produced by <em class="italic">step 5</em> (<code>taaresolve.comp</code>). The main function is presented as follows, and the code is simple – it first copies <code>incolor</code> (which is the image produced by previous <em class="italic">step 5</em>) into the history texture. Then, the <code>sharpen</code> method is called. This method works by first loading pixel colors from the center and four directly adjacent locations (top, left, right, and bottom). It then uses an unsharp masking technique, which involves subtracting a blurred or <em class="italic">unsharp</em> version of the image from the original image to create a mask that represents the detail of the image. The function applies this mask to enhance the original image, making it appear sharper. The final color produced by the <code>sharpen</code> method is stored in <code>outColorImage</code>, which is finally copied to the swapchain image. For the sake of brevity, we’re not detailing the <code>sharpen</code> function here. However, you can review its implementation in the <code>taahistorycopyandsharpen.comp</code> file:<pre class="source-code">
void main() {
  vec4 incolor = imageLoad(inColorImage, ivec2(gl_GlobalInvocationID.xy));
  imageStore(outHistory, ivec2(gl_GlobalInvocationID.xy), incolor);
  vec3 color = sharpen();
  imageStore(outColorImage, ivec2(gl_GlobalInvocationID.xy), vec4(color, 1.0f));
}</pre><p class="list-inset">Despite its <a id="_idIndexMarker494"/>widespread use and numerous benefits, TAA isn’t without its shortcomings:</p><ul><li>When object motion uncovers new areas on a screen, these areas are either not present in the history buffer or are inaccurately depicted by the motion vectors. Additionally, camera rotation and reverse translation can lead to extensive uncovered areas at the screen’s edges.</li><li>Features with subpixel dimensions, such as wires, may be missed by a consecutive frame, leading to their absence in motion vectors in the subsequent frame. Transparent surfaces can generate pixels where the motion vectors from opaque objects don’t align with the overall movement of the objects depicted. Lastly, shadows and reflections don’t follow the direction of the motion vectors of the surfaces they shade.</li></ul></li> </ol>
<p>When TAA doesn’t work properly, it either results in ghosting (a blurring effect caused by integrating incorrect values) or it exposes the original aliasing, leading to jagged edges, flickering, and noise.</p>
<p>See also</p>
<p>For further reading and a deeper understanding of TAA, consider exploring the following resources. These references will provide you with more detailed information, practical applications, and insights into the latest advancements:</p>
<ul>
<li><a href="https://research.nvidia.com/publication/2019-03_improving-temporal-antialiasing-adaptive-ray-tracing">https://research.nvidia.com/publication/2019-03_improving-temporal-antialiasing-adaptive-ray-tracing</a></li>
<li><a href="https://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/temporal-anti-aliasing">https://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/temporal-anti-aliasing</a></li>
</ul>
<h1 id="_idParaDest-258"><a id="_idTextAnchor295"/>Applying DLSS</h1>
<p>DLSS is an AI-powered technology developed by NVIDIA for their RTX series of graphics cards. DLSS uses <a id="_idIndexMarker495"/>the power of machine learning and AI to increase the resolution of rendered frames by intelligently upscaling lower resolution images in real time. This results in a high-quality, high-resolution image that requires less computational power to produce. We can also use DLSS to render frames at a lower base resolution and then use AI to upscale the image to a higher resolution.</p>
<p>Note that to use DLSS, you must have an NVIDIA RTX series graphics card.</p>
<p>In this recipe, you’ll learn how to apply DLSS, an innovative technique for enhancing the resolution of rendered frames in real time. You will gain an understanding of how DLSS leverages machine learning and AI to upscale lower-resolution images intelligently, thereby achieving superior image quality with less computational power.</p>
<h2 id="_idParaDest-259"><a id="_idTextAnchor296"/>Getting ready</h2>
<p>In the repository, DLSS is implemented by the <code>DLSS</code> class, located in the <code>source/enginecore/DLSS.hpp</code> and <code>cpp</code> files. The DLSS example can be launched by running the <code>chapter06_DLSS</code> executable.</p>
<p>DLSS also requires color, depth and velocity textures, the same textures that were used by the TAA algorithm.</p>
<h2 id="_idParaDest-260"><a id="_idTextAnchor297"/>How to do it...</h2>
<p>The DLSS is integrated using the following steps:</p>
<ol>
<li>First, we need to query the device and instance extensions that are required for DLSS; these extensions need to be enabled before Vulkan is initialized. NVIDIA’s DLSS SDK provides <code>NVSDK_NGX_VULKAN_RequiredExtensions</code>, which needs to be used to query extensions. The following code block presents a static function that can append extensions required by DLSS; this needs to be called before initializing the Vulkan device:<pre class="source-code">
void DLSS::requiredExtensions(std::vector&lt;std::string&gt;&amp; instanceExtensions,
                              std::vector&lt;std::string&gt;&amp; deviceExtensions) {
  unsigned int instanceExtCount;
  const char** instanceExt;
  unsigned int deviceExtCount;
  const char** deviceExt;
  auto result = NVSDK_NGX_VULKAN_RequiredExtensions(
      &amp;instanceExtCount, &amp;instanceExt, &amp;deviceExtCount, &amp;deviceExt);
  for (int i = 0; i &lt; instanceExtCount; ++i) {
    if (std::find(instanceExtensions.begin(), instanceExtensions.end(),
                  instanceExt[i]) == instanceExtensions.end()) {
      instanceExtensions.push_back(instanceExt[i]);
    }
  }
  for (int i = 0; i &lt; deviceExtCount; ++i) {
    if (std::find(deviceExtensions.begin(), deviceExtensions.end(),
                  deviceExt[i]) == deviceExtensions.end()) {
      deviceExtensions.push_back(deviceExt[i]);
      if (deviceExtensions.back() ==
          "VK_EXT_buffer_device_address") {  // we are using 1.3, this extension
                                             // has been promoted
        deviceExtensions.pop_back();
      }
    }
  }
}</pre></li> <li>Next, we will look at <code>DLSS init</code> method. This method is responsible for initializing <a id="_idIndexMarker496"/>the DLSS feature provided by the NVSDK. It takes the current width and height of the viewport, an upscale factor, and a reference to a <code>CommandQueueManager</code> object. The function first sets the upscale factor and then determines the optimal settings for DLSS, based on the current viewport size and desired quality level. It then configures DLSS features such as motion vector resolution, frame sharpening, and others based on specific flags. Finally, it creates the DLSS feature and submits the command to the Vulkan command buffer:<pre class="source-code">
void DLSS::init(int currentWidth, int currentHeight, float upScaleFactor, VulkanCore::CommandQueueManager&amp; commandQueueManager) {
  NVSDK_NGX_Result result = NGX_DLSS_GET_OPTIMAL_SETTINGS(paramsDLSS_, currentWidth, currentHeight, dlssQuality, &amp;optimalRenderWidth, &amp;optimalRenderHeight, &amp;minRenderWidth, &amp;minRenderHeight, &amp;maxRenderWidth, &amp;maxRenderHeight, &amp;recommendedSharpness);
  int dlssCreateFeatureFlags = NVSDK_NGX_DLSS_Feature_Flags_None;
  dlssCreateFeatureFlags |= NVSDK_NGX_DLSS_Feature_Flags_MVLowRes;
  dlssCreateFeatureFlags |= NVSDK_NGX_DLSS_Feature_Flags_DoSharpening;
  NVSDK_NGX_DLSS_Create_Params dlssCreateParams{
      .Feature =
          {
              .InWidth = unsigned int(currentWidth),
              .InHeight = unsigned int(currentHeight),
              .InTargetWidth = unsigned int(currentWidth * upScaleFactor),
              .InTargetHeight = unsigned int(currentHeight * upScaleFactor),
              .InPerfQualityValue = NVSDK_NGX_PerfQuality_Value_MaxQuality,
          },
      .InFeatureCreateFlags = dlssCreateFeatureFlags,
  };
  auto commmandBuffer = commandQueueManager.getCmdBufferToBegin();
  constexpr unsigned int creationNodeMask = 1;
  constexpr unsigned int visibilityNodeMask = 1;
  NVSDK_NGX_Result createDlssResult = NGX_VULKAN_CREATE_DLSS_EXT(commmandBuffer, creationNodeMask, visibilityNodeMask, &amp;dlssFeatureHandle_, paramsDLSS_, &amp;dlssCreateParams);
  ASSERT(createDlssResult == NVSDK_NGX_Result_Success, "Failed to create NVSDK NGX DLSS feature");
  commandQueueManager.endCmdBuffer(commmandBuffer);
  VkSubmitInfo submitInfo{
      .sType = VK_STRUCTURE_TYPE_SUBMIT_INFO,
      .commandBufferCount = 1,
      .pCommandBuffers = &amp;commmandBuffer,
  };
  commandQueueManager.submit(&amp;submitInfo);
  commandQueueManager.waitUntilSubmitIsComplete();
}</pre></li> <li>The next step is to call DLSS’s <code>render</code> method, which is responsible for applying DLSS <a id="_idIndexMarker497"/>to the provided input textures to enhance the image quality. It takes a Vulkan command buffer and several texture objects as inputs – color, depth, motion vector, and output color texture, along with a 2D vector for the camera jitter. Firstly, we create resources for each of the input textures using the <code>NVSDK_NGX_Create_ImageView_Resource_VK</code> function; afterward, we transition the layout of the output color texture to <code>VK_IMAGE_LAYOUT_GENERAL</code> to prepare it for writing. Next, this function sets up the parameters for the DLSS evaluation, including input color and output resources, sharpness level, depth resource, motion vector resource, and camera jitter offsets. The last part is to call <code>NGX_VULKAN_EVALUATE_DLSS_EXT</code> to apply DLSS to the images, which enhances the image quality, based on the parameters provided:<pre class="source-code">
void DLSS::render(VkCommandBuffer commandBuffer, VulkanCore::Texture&amp; inColorTexture, VulkanCore::Texture&amp; inDepthTexture, VulkanCore::Texture&amp; inMotionVectorTexture, VulkanCore::Texture&amp; outColorTexture, glm::vec2 cameraJitter) {
  NVSDK_NGX_Resource_VK inColorResource = NVSDK_NGX_Create_ImageView_Resource_VK(
          inColorTexture.vkImageView(), inColorTexture.vkImage(),
          {VK_IMAGE_ASPECT_COLOR_BIT, 0, 1, 0, 1}, VK_FORMAT_UNDEFINED,
          inColorTexture.vkExtents().width, inColorTexture.vkExtents().height,
          true);
  NVSDK_NGX_Resource_VK outColorResource = NVSDK_NGX_Create_ImageView_Resource_VK(…);
  NVSDK_NGX_Resource_VK depthResource = NVSDK_NGX_Create_ImageView_Resource_VK(…);
  NVSDK_NGX_Resource_VK motionVectorResource = NVSDK_NGX_Create_ImageView_Resource_VK(…);
  outColorTexture.transitionImageLayout(commandBuffer, VK_IMAGE_LAYOUT_GENERAL);
  NVSDK_NGX_VK_DLSS_Eval_Params evalParams = {
      .Feature =
          {
              .pInColor = &amp;inColorResource,
              .pInOutput = &amp;outColorResource,
              .InSharpness = 1.0,
          },
      .pInDepth = &amp;depthResource,
      .pInMotionVectors = &amp;motionVectorResource,
      .InJitterOffsetX = cameraJitter.x,
      .InJitterOffsetY = cameraJitter.y,
      .InRenderSubrectDimensions =
          {
              .Width =
                  static_cast&lt;unsigned int&gt;(inColorTexture.vkExtents().width),
              .Height =
                  static_cast&lt;unsigned int&gt;(inColorTexture.vkExtents().height),
          },
      .InReset = 0,
      .InMVScaleX = -1.0f * inColorResource.Resource.ImageViewInfo.Width,
      .InMVScaleY = -1.0f * inColorResource.Resource.ImageViewInfo.Height,
      .pInExposureTexture = nullptr,
  };
  NVSDK_NGX_Result result = NGX_VULKAN_EVALUATE_DLSS_EXT(commandBuffer, dlssFeatureHandle_, paramsDLSS_, &amp;evalParams);
  ASSERT(result == NVSDK_NGX_Result_Success, "Failed to evaluate DLSS feature");
  if (result != NVSDK_NGX_Result_Success) {
    auto store = GetNGXResultAsString(result);
  }
}</pre></li> </ol>
<p>In the <a id="_idIndexMarker498"/>next section, we present valuable links for further reading and deeper understanding of the topic.</p>
<h2 id="_idParaDest-261"><a id="_idTextAnchor298"/>See also</h2>
<p>For more in-depth knowledge and practical insights on DLSS, the following resource will prove invaluable:</p>
<ul>
<li><a href="https://github.com/NVIDIA/DLSS/blob/main/doc/DLSS_Programming_Guide_Release.pdf">https://github.com/NVIDIA/DLSS/blob/main/doc/DLSS_Programming_Guide_Release.pdf</a></li>
</ul>
<p>In this chapter, we started with Vulkan’s MSAA. This is a method used to combat the spatial aliasing of high-contrast edges, often seen as jagged or staircase lines in the rendered image. We discussed the process of enabling MSAA in Vulkan, which involves configuring the multi-sample state during pipeline creation and allocating a separate multi-sample image. We also covered how MSAA operates by averaging the color of multiple sample points, reducing the jagged appearance, and providing a smoother, more natural look to the edges.</p>
<p>Then, we addressed FXAA. This technique is a screen-space, post-processing method, meaning that <a id="_idIndexMarker499"/>it works directly on the final image. Its primary advantage is its speed and simplicity, offering a good trade-off between performance and quality. FXAA smooths edges by finding high-contrast pixels and blending them with their surroundings. Despite being an approximation, FXAA can often provide a significant improvement in perceived image quality, particularly in scenes with many high-contrast edges.</p>
<p>The third technique we discussed was TAA. This method uses the concept of temporal reprojection, where it leverages information from previous frames to minimize aliasing artifacts in the current frame. We covered how TAA operates by accumulating samples over multiple frames and applying a filter to reduce the temporal aliasing effects, such as crawling and flickering. When implemented correctly, TAA can offer superior results over purely spatial techniques, particularly in scenes with high levels of motion and detail.</p>
<p>Lastly, we explored the cutting-edge technique of DLSS. Powered by AI, DLSS is a proprietary technology developed by NVIDIA. It works by training a deep learning model to predict high-resolution images from lower-resolution inputs. The trained model is then used to upscale images in real time. We also talked about how DLSS can maintain or even improve visual fidelity while significantly boosting performance.</p>
<p>This chapter provided a comprehensive overview of various anti-aliasing techniques, each with its strengths and use cases. By understanding these methods, you can make informed choices on which technique to implement, based on the specific needs of your Vulkan applications.</p>
</div>
</body></html>