<html><head></head><body>
  <div><h1 class="chapter-number" id="_idParaDest-75">
    <a id="_idTextAnchor074">
    </a>
    
     4
    
   </h1>
   <h1 id="_idParaDest-76">
    <a id="_idTextAnchor075">
    </a>
    
     Thread Synchronization  with Locks
    
   </h1>
   <p>
    
     In
    
    <a href="B22219_02.xhtml#_idTextAnchor035">
     
      <em class="italic">
       
        Chapter 2
       
      </em>
     
    </a>
    
     , we learned that threads can read and write memory shared by the process they belong to.
    
    
     While the operating system implements process memory access protection, there is no such protection for threads accessing shared memory in the same process.
    
    
     Concurrent memory write operations to the same memory address from multiple threads require synchronization mechanisms to avoid data races and ensure
    
    
     
      data integrity.
     
    
   </p>
   <p>
    
     In this chapter, we will describe in detail the problems created by concurrent access to shared memory by multiple threads and how to fix them.
    
    
     We are going to study in detail the
    
    
     
      following topics:
     
    
   </p>
   <ul>
    <li>
     
      Race conditions – what they are and how they
     
     
      
       can happen
      
     
    </li>
    <li>
     
      Mutual exclusion as a synchronization mechanism and how it is implemented in C++
     
     
      
       by
      
     
     
      <strong class="source-inline">
       
        std::mutex
       
      </strong>
     
    </li>
    <li>
     
      Generic
     
     
      
       lock management
      
     
    </li>
    <li>
     
      What condition variables are and how to use them
     
     
      
       with mutexes
      
     
    </li>
    <li>
     
      Implementing a fully synchronized queue using
     
     <strong class="source-inline">
      
       std::mutex
      
     </strong>
     
      
       and
      
     
     
      <strong class="source-inline">
       
        std::condition_variable
       
      </strong>
     
    </li>
    <li>
     
      The new synchronization primitives introduced with C++20 – semaphores, barriers,
     
     
      
       and latches
      
     
    </li>
   </ul>
   <p>
    
     These are all lock-based synchronization mechanisms.
    
    
     Lock-free techniques are the subject of the
    
    
     
      next chapter.
     
    
   </p>
   <h1 id="_idParaDest-77">
    <a id="_idTextAnchor076">
    </a>
    
     Technical requirements
    
   </h1>
   <p>
    
     The technical requirements for this chapter are the same as for the concepts explained in the previous chapter, and to compile and run the examples, a C++ compiler with C++20 support is required (for semaphores, latches, and barriers examples).
    
    
     Most of the examples require just C++11.
    
    
     Examples have been tested on Linux Ubuntu
    
    
     
      LTS 24.04.
     
    
   </p>
   <p>
    
     The code in this chapter can be found
    
    
     
      on GitHub:
     
    
   </p>
   <p>
    <a href="https://github.com/PacktPublishing/Asynchronous-Programming-with-CPP">
     
      
       https://github.com/PacktPublishing/Asynchronous-Programming-with-CPP
      
     
    </a>
   </p>
   <h1 id="_idParaDest-78">
    <a id="_idTextAnchor077">
    </a>
    
     Understanding race conditions
    
   </h1>
   <p>
    
     A race condition
    
    <a id="_idIndexMarker218">
    </a>
    
     happens when the outcome of running a program depends on the sequence in which its instructions are executed.
    
    
     We will begin with a very simple example to show how race conditions happen, and later in this chapter, we will learn how to resolve
    
    
     
      this problem.
     
    
   </p>
   <p>
    
     In the following code, the
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     global variable is incremented by two threads
    
    
     
      running concurrently:
     
    
   </p>
   <pre class="source-code">
#include &lt;iostream&gt;
#include &lt;thread&gt;
int counter = 0;
int main() {
    auto func = [] {
        for (int i = 0; i &lt; 1000000; ++i) {
            counter++;
        }
    };
    std::thread t1(func);
    std::thread t2(func);
    t1.join();
    t2.join();
    std::cout &lt;&lt; counter &lt;&lt; std::endl;
    return 0;
}</pre>
   <p>
    
     After running the preceding code three times, we get the following
    
    
     <strong class="source-inline">
      
       counter
      
     </strong>
    
    
     
      values:
     
    
   </p>
   <pre class="console">
1056205
1217311
1167474</pre>
   <p>
    
     We see two main issues here: first, the value of
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     is incorrect; second, every execution of the program ends with a different value of
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     .
    
    
     The results are non-deterministic and most frequently incorrect.
    
    
     If you are very lucky, you may get the right values, but that is
    
    
     
      very unlikely.
     
    
   </p>
   <p>
    
     This scenario involves two threads,
    
    <strong class="source-inline">
     
      t1
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      t2
     
    </strong>
    
     , that run concurrently and modify the same variable, which is essentially some memory region.
    
    
     It seems like it should work fine because there is only one line of code that increases the
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     value and thus modifies the memory content (by the way, it doesn’t matter if we use the post-increment operator like in
    
    <strong class="source-inline">
     
      counter++
     
    </strong>
    
     or the pre-increment operator like in
    
    <strong class="source-inline">
     
      ++counter
     
    </strong>
    
     ; the results will be
    
    
     
      equally wrong).
     
    
   </p>
   <p>
    
     Looking closer at the
    
    <a id="_idIndexMarker219">
    </a>
    
     preceding code, let’s study the following
    
    
     
      line carefully:
     
    
   </p>
   <pre class="source-code">
        counter++;</pre>
   <p>
    
     It increments
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     in
    
    
     
      three steps:
     
    
   </p>
   <ul>
    <li>
     
      The contents of the memory address where the
     
     <strong class="source-inline">
      
       counter
      
     </strong>
     
      variable is stored are loaded into a CPU register.
     
     
      In this case, an
     
     <strong class="source-inline">
      
       int
      
     </strong>
     
      data type is loaded from memory into a
     
     
      
       CPU register.
      
     
    </li>
    <li>
     
      The value in the register is incremented
     
     
      
       by one.
      
     
    </li>
    <li>
     
      The value in the register is stored in the
     
     <strong class="source-inline">
      
       counter
      
     </strong>
     
      variable
     
     
      
       memory address.
      
     
    </li>
   </ul>
   <p>
    
     Now, let us consider a possible scenario when two threads attempt to increment the counter concurrently.
    
    
     Let us look at
    
    
     <em class="italic">
      
       Table 4.1
      
     </em>
    
    
     
      :
     
    
   </p>
   <table class="No-Table-Style _idGenTablePara-1" id="table001-1">
    <colgroup>
     <col/>
     <col/>
    </colgroup>
    <thead>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         <strong class="bold">
          
           THREAD 1
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         <strong class="bold">
          
           THREAD 2
          
         </strong>
        
       </p>
      </td>
     </tr>
    </thead>
    <tbody>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         [1] Load counter value
        
        
         
          into register
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         [3] Load counter value
        
        
         
          into register
         
        
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         [2] Increment
        
        
         
          register value
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         [5] Increment
        
        
         
          register value
         
        
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         [4] Store register
        
        
         
          in counter
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         [6] Store register
        
        
         
          in counter
         
        
       </p>
      </td>
     </tr>
    </tbody>
   </table>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Table 4.1: Two threads incrementing the counter concurrently
    
   </p>
   <p>
    
     Thread 1 executes [1] and loads the current value of the counter (let’s assume it is 1) into a CPU register.
    
    
     Then, it increments the value in the register by one [2] (now, the register value
    
    
     
      is 2).
     
    
   </p>
   <p>
    
     Thread 2 is scheduled for execution and [3] loads the current value of the counter (remember – it has not been modified yet, so it is still 1) into a
    
    
     
      CPU register.
     
    
   </p>
   <p>
    
     Now, thread 1 is scheduled again for execution and [4] stores the updated value into memory.
    
    
     The value of
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     is now equal
    
    
     
      to two.
     
    
   </p>
   <p>
    
     Finally, thread 2 is scheduled again, and [5] and [6] are executed.
    
    
     The register value is incremented by one and then the value two is stored in memory.
    
    
     The
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     variable has been incremented just once when it should have been incremented twice and its value should
    
    
     
      be three.
     
    
   </p>
   <p>
    
     The previous issue happened because the increment operation on the counter is not atomic.
    
    
     If each thread could execute the three instructions required to increment the
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     variable
    
    <a id="_idIndexMarker220">
    </a>
    
     without being interrupted,
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     would be incremented twice as expected.
    
    
     However, depending on the order in which the operations are executed, the result can be different.
    
    
     This is called a
    
    
     <strong class="bold">
      
       race condition
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     To avoid race conditions, we need to ensure that shared resources are accessed and modified in a controlled manner.
    
    
     One way to achieve this is by using locks.
    
    
     A
    
    <strong class="bold">
     
      lock
     
    </strong>
    
     is a synchronization
    
    <a id="_idIndexMarker221">
    </a>
    
     primitive that allows only one thread to access a shared resource at a time.
    
    
     When a thread wants to access a shared resource, it must first acquire the lock.
    
    
     Once the thread has acquired the lock, it can access the shared resource without interference from other threads.
    
    
     When the thread has finished accessing the shared resource, it must release the lock so that other threads can
    
    
     
      access it.
     
    
   </p>
   <p>
    
     Another way to avoid race conditions is
    
    <a id="_idIndexMarker222">
    </a>
    
     by using
    
    <strong class="bold">
     
      atomic operations
     
    </strong>
    
     .
    
    
     An atomic operation is an operation that is guaranteed to be executed in a single, indivisible step.
    
    
     This means that no other thread can interfere with an atomic operation while it is being executed.
    
    
     Atomic operations are typically implemented using hardware instructions that are designed to be indivisible.
    
    
     Atomic operations will be explained in
    
    <a href="B22219_05.xhtml#_idTextAnchor097">
     
      <em class="italic">
       
        Chapter 5
       
      </em>
     
    </a>
    
     
      .
     
    
   </p>
   <p>
    
     In this section, we have seen the most common and important problem created by multithreaded code: race conditions.
    
    
     We have seen how, depending on the order of the operations performed, the
    
    <a id="_idIndexMarker223">
    </a>
    
     results can be different.
    
    
     With this problem in mind, we are going to study how to solve it in the
    
    
     
      next section.
     
    
   </p>
   <h1 id="_idParaDest-79">
    <a id="_idTextAnchor078">
    </a>
    
     Why do we need mutual exclusion?
    
   </h1>
   <p>
    <strong class="bold">
     
      Mutual exclusion
     
    </strong>
    
     is a fundamental
    
    <a id="_idIndexMarker224">
    </a>
    
     concept in concurrent programming that ensures that multiple threads or processes do not simultaneously access a shared resource such as a shared variable, a critical section of code, or a file or network connection.
    
    
     Mutual exclusion is crucial for preventing race conditions such as the one we have seen in the
    
    
     
      previous section.
     
    
   </p>
   <p>
    
     Imagine a small coffee shop with a single espresso machine.
    
    
     The machine can only make one espresso at a time.
    
    
     This means the machine is a critical resource that all baristas
    
    
     
      must share.
     
    
   </p>
   <p>
    
     The coffee shop is attended by three baristas: Alice, Bob, and Carol.
    
    
     They use the coffee machine
    
    <em class="italic">
     
      concurrently
     
    </em>
    
     , but they cannot use it simultaneously because that could create problems: Bob puts the right amount of freshly ground coffee in the machine and starts making an espresso.
    
    
     Then, Alice does the same but first removes the coffee from the machine, thinking that Bob just forgot to do it.
    
    
     Bob then takes the espresso from the machine, and after that, Alice finds that there is no espresso!
    
    
     This is a disaster – a real-life version of our
    
    
     
      counter program.
     
    
   </p>
   <p>
    
     To fix the problems in the coffee shop, they may appoint Carol as a machine manager.
    
    
     Before using the machine, both Alice and Bob ask her if they can start making a new espresso.
    
    
     That would solve
    
    
     
      the issue.
     
    
   </p>
   <p>
    
     Back to our counter program, if we could allow just one thread at a time to access
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     (what Carol did in the coffee shop), our software problem would be solved too.
    
    
     Mutual exclusion is a mechanism that can be used to control concurrent thread access to memory.
    
    
     The C++ Standard Library provides the
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     class, a synchronization primitive used to protect shared data from being simultaneously accessed by two or
    
    
     
      more threads.
     
    
   </p>
   <p>
    
     This new version of the code we saw in the previous section implements two ways of concurrently incrementing
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     : free access, as in the previous section, and synchronized
    
    <a id="_idIndexMarker225">
    </a>
    
     access using
    
    
     
      mutual exclusion:
     
    
   </p>
   <pre class="source-code">
#include &lt;iostream&gt;
#include &lt;mutex&gt;
#include &lt;thread&gt;
std::mutex mtx;
int counter = 0;
int main() {
    auto funcWithoutLocks = [] {
        for (int i = 0; i &lt; 1000000; ++i) {
            ++counter;
        };
    };
    auto funcWithLocks = [] {
        for (int i = 0; i &lt; 1000000; ++i) {
            mtx.lock();
            ++counter;
            mtx.unlock();
        };
    };
    {
        counter = 0;
        std::thread t1(funcWithoutLocks);
        std::thread t2(funcWithoutLocks);
        t1.join();
        t2.join();
        std::cout &lt;&lt; "Counter without using locks: " &lt;&lt; counter &lt;&lt; std::endl;
    }
    {
        counter = 0;
        std::thread t1(funcWithLocks);
        std::thread t2(funcWithLocks);
        t1.join();
        t2.join();
        std::cout &lt;&lt; "Counter using locks: " &lt;&lt; counter &lt;&lt; std::endl;
    }
    return 0;
}</pre>
   <p>
    
     When a thread runs
    
    <strong class="source-inline">
     
      funcWithLocks
     
    </strong>
    
     , it acquires a lock with
    
    <strong class="source-inline">
     
      mtx.lock()
     
    </strong>
    
     before incrementing
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     .
    
    
     Once
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     has been incremented, the thread releases the
    
    
     
      lock (
     
    
    
     <strong class="source-inline">
      
       mtx.unlock()
      
     </strong>
    
    
     
      ).
     
    
   </p>
   <p>
    
     The lock can only be owned by one thread.
    
    
     If, for example,
    
    <strong class="source-inline">
     
      t1
     
    </strong>
    
     acquires the lock and then
    
    <strong class="source-inline">
     
      t2
     
    </strong>
    
     tries to acquire it too,
    
    <strong class="source-inline">
     
      t2
     
    </strong>
    
     will be blocked and will wait until the lock is available.
    
    
     Because only one thread can own the lock at any time, this synchronization primitive is called a
    
    <strong class="bold">
     
      mutex
     
    </strong>
    
     (from
    
    <em class="italic">
     
      mutual exclusion
     
    </em>
    
     ).
    
    
     If you
    
    <a id="_idIndexMarker226">
    </a>
    
     run this program a few times, you will always get the correct
    
    
     
      result:
     
    
    
     <strong class="source-inline">
      
       2000000
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     In this section, we introduced the concept of mutual exclusion and learned that the C++ Standard Library
    
    <a id="_idIndexMarker227">
    </a>
    
     provides the
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     class as a primitive for thread synchronization.
    
    
     In the next section, we will study
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     
      in detail.
     
    
   </p>
   <h2 id="_idParaDest-80">
    <a id="_idTextAnchor079">
    </a>
    
     C++ Standard Library mutual exclusion implementation
    
   </h2>
   <p>
    
     In the previous section, we
    
    <a id="_idIndexMarker228">
    </a>
    
     introduced the concept of mutual exclusion and mutexes and why they are needed to synchronize concurrent memory access.
    
    
     In this section, we will see the classes provided by the C++ Standard Library to implement mutual exclusion.
    
    
     We will also see some helper classes the C++ Standard Library provides to make the use of
    
    
     
      mutexes easier.
     
    
   </p>
   <p>
    
     The following table summarizes the mutex classes provided by the C++ Standard Library and their
    
    
     
      main features:
     
    
   </p>
   <table class="No-Table-Style _idGenTablePara-1" id="table002">
    <colgroup>
     <col/>
     <col/>
     <col/>
     <col/>
    </colgroup>
    <tbody>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         <strong class="bold">
          
           Mutex Type
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         <strong class="bold">
          
           Access
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         <strong class="bold">
          
           Recursive
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         <strong class="bold">
          
           Timeout
          
         </strong>
        
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         <strong class="source-inline">
          
           std::mutex
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          EXCLUSIVE
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          NO
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          NO
         
        
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         <strong class="source-inline">
          
           std::recursive_mutex
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          EXCLUSIVE
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          YES
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          NO
         
        
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         <strong class="source-inline">
          
           std::shared_mutex
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         1 -
        
        
         
          EXCLUSIVE
         
        
       </p>
       <p>
        
         N -
        
        
         
          SHARED
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          NO
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          NO
         
        
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         <strong class="source-inline">
          
           std::timed_mutex
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          EXCLUSIVE
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          NO
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          YES
         
        
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         <strong class="source-inline">
          
           std::recursive_timed_mutex
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          EXCLUSIVE
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          YES
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          YES
         
        
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         <strong class="source-inline">
          
           std::shared_timed_mutex
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         1 -
        
        
         
          EXCLUSIVE
         
        
       </p>
       <p>
        
         N -
        
        
         
          SHARED
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          NO
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          YES
         
        
       </p>
      </td>
     </tr>
    </tbody>
   </table>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Table 4.2: Mutex classes in C++ Standard Library
    
   </p>
   <p>
    
     Let us
    
    <a id="_idIndexMarker229">
    </a>
    
     explore these classes one
    
    
     
      by one.
     
    
   </p>
   <h3>
    
     std::mutex
    
   </h3>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     class was
    
    <a id="_idIndexMarker230">
    </a>
    
     introduced
    
    <a id="_idIndexMarker231">
    </a>
    
     in C++11 and is one of the most important and most frequently used synchronization primitives provided by the C++
    
    
     
      Standard Library.
     
    
   </p>
   <p>
    
     As we have seen earlier in this chapter,
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     is a synchronization primitive that can be used to protect shared data from being simultaneously accessed by
    
    
     
      multiple threads.
     
    
   </p>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     class offers exclusive, non-recursive
    
    
     
      ownership semantics.
     
    
   </p>
   <p>
    
     The main features of
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     are
    
    
     
      the following:
     
    
   </p>
   <ul>
    <li>
     
      A calling thread owns the mutex from the time it successfully calls
     
     <strong class="source-inline">
      
       lock()
      
     </strong>
     
      or
     
     <strong class="source-inline">
      
       try_lock()
      
     </strong>
     
      until it
     
     
      
       calls
      
     
     
      <strong class="source-inline">
       
        unlock()
       
      </strong>
     
     
      
       .
      
     
    </li>
    <li>
     
      A calling thread must not own the mutex before calling
     
     <strong class="source-inline">
      
       lock()
      
     </strong>
     
      or
     
     <strong class="source-inline">
      
       try_lock(
      
     </strong>
     
      ).
     
     
      This is the non-recursive ownership semantics property
     
     
      
       of
      
     
     
      <strong class="source-inline">
       
        std::mutex
       
      </strong>
     
     
      
       .
      
     
    </li>
    <li>
     
      When a thread owns a mutex, all other threads will block (when calling
     
     <strong class="source-inline">
      
       lock())
      
     </strong>
     
      or receive a
     
     <strong class="source-inline">
      
       false
      
     </strong>
     
      return value (when calling
     
     <strong class="source-inline">
      
       try_lock()
      
     </strong>
     
      ).
     
     
      This is the exclusive ownership semantics
     
     
      
       of
      
     
     
      <strong class="source-inline">
       
        std::mutex
       
      </strong>
     
     
      
       .
      
     
    </li>
   </ul>
   <p>
    
     If a thread owning a mutex tries to acquire it again, the resulting behavior is undefined.
    
    
     Usually, an exception is thrown when this happens, but this
    
    
     
      is implementation-defined.
     
    
   </p>
   <p>
    
     If, after a thread releases a mutex, it tries to release it again, this is also undefined behavior (as in the
    
    
     
      previous case).
     
    
   </p>
   <p>
    
     A mutex being destroyed while a thread has it locked or a thread terminating without releasing the lock are also causes of
    
    
     
      undefined behavior.
     
    
   </p>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     class has
    
    
     
      three methods:
     
    
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      
       lock()
      
     </strong>
     
      : Calling
     
     <strong class="source-inline">
      
       lock()
      
     </strong>
     
      acquires the mutex.
     
     
      If the mutex is already locked, then the calling thread is blocked until the mutex is unlocked.
     
     
      From the application’s point of view, it is as if the calling thread waits for the mutex to
     
     
      
       be available.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       try_lock()
      
     </strong>
     
      : When called, this function returns either
     
     <strong class="source-inline">
      
       true
      
     </strong>
     
      , indicating that the mutex has been successfully locked, or
     
     <strong class="source-inline">
      
       false
      
     </strong>
     
      in the event of the mutex being already locked.
     
     
      Note that
     
     <strong class="source-inline">
      
       try_lock
      
     </strong>
     
      is non-blocking, and the calling thread either acquires the mutex or not, but it is not blocked like when calling
     
     <strong class="source-inline">
      
       lock()
      
     </strong>
     
      .
     
     
      The
     
     <strong class="source-inline">
      
       try_lock()
      
     </strong>
     
      method is generally used when we don’t want the thread to wait until the mutex is available.
     
     
      We will call
     
     <strong class="source-inline">
      
       try_lock()
      
     </strong>
     
      when we want the thread
     
     <a id="_idIndexMarker232">
     </a>
     
      to proceed with
     
     <a id="_idIndexMarker233">
     </a>
     
      some processing and try to acquire the
     
     
      
       mutex later.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       unlock()
      
     </strong>
     
      : Calling
     
     <strong class="source-inline">
      
       unlock()
      
     </strong>
     
      releases
     
     
      
       the mutex.
      
     
    </li>
   </ul>
   <h3>
    
     std::recursive_mutex
    
   </h3>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     class offers
    
    <a id="_idIndexMarker234">
    </a>
    
     exclusive, non-recursive
    
    <a id="_idIndexMarker235">
    </a>
    
     ownership semantics.
    
    
     While exclusive ownership semantics are always required at least for a thread (it is a mutual exclusion mechanism, after all), in some instances, we may need to recursively acquire the mutex.
    
    
     For example, a recursive function may need to acquire a mutex.
    
    
     We may also need to acquire a mutex in function
    
    <strong class="source-inline">
     
      g()
     
    </strong>
    
     called from another function
    
    <strong class="source-inline">
     
      f()
     
    </strong>
    
     , which acquired the
    
    
     
      same mutex.
     
    
   </p>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::recursive_mutex
     
    </strong>
    
     class offers exclusive, recursive semantics.
    
    
     Its main features are
    
    
     
      the following:
     
    
   </p>
   <ul>
    <li>
     
      A calling thread may acquire the same mutex more than once.
     
     
      It will own the mutex until it releases the mutex the same number of times it acquired it.
     
     
      For example, if a thread recursively acquires a mutex three times, it will own the mutex until it releases it for the
     
     
      
       third time.
      
     
    </li>
    <li>
     
      The maximum number of times a recursive mutex can be recursively acquired is unspecified and hence implementation-defined.
     
     
      Once a mutex has been acquired for the maximum number of times, calls to
     
     <strong class="source-inline">
      
       lock()
      
     </strong>
     
      will throw
     
     <strong class="source-inline">
      
       std::system_error
      
     </strong>
     
      , and calls to
     
     <strong class="source-inline">
      
       try_lock()
      
     </strong>
     
      will
     
     
      
       return
      
     
     
      <strong class="source-inline">
       
        false
       
      </strong>
     
     
      
       .
      
     
    </li>
    <li>
     
      Ownership is the same as for
     
     <strong class="source-inline">
      
       std::mutex
      
     </strong>
     
      : if a thread owns a
     
     <strong class="source-inline">
      
       std::recursive_mutex
      
     </strong>
     
      class, any other threads will block if they try to acquire it by calling
     
     <strong class="source-inline">
      
       lock()
      
     </strong>
     
      , or they will get false as a return when
     
     
      
       calling
      
     
     
      <strong class="source-inline">
       
        try_lock()
       
      </strong>
     
     
      
       .
      
     
    </li>
   </ul>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::recursive_mutex
     
    </strong>
    
     interface
    
    <a id="_idIndexMarker236">
    </a>
    
     is exactly
    
    <a id="_idIndexMarker237">
    </a>
    
     the same as
    
    
     
      for
     
    
    
     <strong class="source-inline">
      
       std::mutex
      
     </strong>
    
    
     
      .
     
    
   </p>
   <h3>
    
     std::shared_mutex
    
   </h3>
   <p>
    
     Both
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      std::shared_mutex
     
    </strong>
    
     have exclusive ownership semantics, and just one
    
    <a id="_idIndexMarker238">
    </a>
    
     thread can be
    
    <a id="_idIndexMarker239">
    </a>
    
     the mutex owner at any given time.
    
    
     There are some cases, though, when we may need to let several threads simultaneously access the protected data and give just one thread
    
    
     
      exclusive access.
     
    
   </p>
   <p>
    
     The counter example required exclusive access to a single variable for every thread because they were all updating
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     values.
    
    
     Now, if we have threads that only require reading the current value in
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     and just one thread to increment its value, it would be much better to let the reader threads access
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     concurrently and give the writer
    
    
     
      exclusive access.
     
    
   </p>
   <p>
    
     This functionality is implemented using what is called a Readers-Writer lock.
    
    
     The C++ Standard Library implements the
    
    <strong class="source-inline">
     
      std::shared_mutex
     
    </strong>
    
     class, with a similar (but not exactly the
    
    
     
      same) functionality.
     
    
   </p>
   <p>
    
     The main difference between
    
    <strong class="source-inline">
     
      std::shared_mutex
     
    </strong>
    
     and other mutex types is that it has two
    
    
     
      access levels:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Shared
      
     </strong>
     
      : Several threads can share the ownership of the same mutex.
     
     
      Shared ownership is acquired/released calling
     
     <strong class="source-inline">
      
       lock_shared()
      
     </strong>
     
      ,
     
     <strong class="source-inline">
      
       try_lock_shared()
      
     </strong>
     
      /
     
     <strong class="source-inline">
      
       unlock shared()
      
     </strong>
     
      .
     
     
      While at least one thread has acquired shared access to the lock, no other thread can get exclusive access to it, but it can acquire
     
     
      
       shared access.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Exclusive
      
     </strong>
     
      : Only one thread can own the mutex.
     
     
      Exclusive ownership is acquired/released by calling
     
     <strong class="source-inline">
      
       lock()
      
     </strong>
     
      ,
     
     <strong class="source-inline">
      
       try_lock()
      
     </strong>
     
      /
     
     <strong class="source-inline">
      
       unlock()
      
     </strong>
     
      .
     
     
      While a thread has acquired exclusive access to the lock, no other thread can acquire either shared or exclusive access
     
     
      
       to it.
      
     
    </li>
   </ul>
   <p>
    
     Let’s see a
    
    <a id="_idIndexMarker240">
    </a>
    
     simple example
    
    <a id="_idIndexMarker241">
    </a>
    
     
      using
     
    
    
     <strong class="source-inline">
      
       std::shared_mutex
      
     </strong>
    
    
     
      :
     
    
   </p>
   <pre class="source-code">
#include &lt;algorithm&gt;
#include &lt;chrono&gt;
#include &lt;iostream&gt;
#include &lt;shared_mutex&gt;
#include &lt;thread&gt;
int counter = 0;
int main() {
    using namespace std::chrono_literals;
    std::shared_mutex mutex;
    auto reader = [&amp;] {
        for (int i = 0; i &lt; 10; ++i) {
            mutex.lock_shared();
            // Read the counter and do something
            mutex.unlock_shared();
        }
    };
    auto writer = [&amp;] {
        for (int i = 0; i &lt; 10; ++i) {
            mutex.lock();
            ++counter;
            std::cout &lt;&lt; "Counter: " &lt;&lt; counter &lt;&lt; std::endl;
            mutex.unlock();
            std::this_thread::sleep_for(10ms);
        }
    };
    std::thread t1(reader);
    std::thread t2(reader);
    std::thread t3(writer);
    std::thread t4(reader);
    std::thread t5(reader);
    std::thread t6(writer);
    t1.join();
    t2.join();
    t3.join();
    t4.join();
    t5.join();
    t6.join();
    return 0;
}</pre>
   <p>
    
     The example uses
    
    <strong class="source-inline">
     
      std::shared_mutex
     
    </strong>
    
     to synchronize six threads: two threads are writers, and they increment the value of
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     and require exclusive access.
    
    
     The remaining four threads just read
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     and only require shared access.
    
    
     Also, note that in order to
    
    <a id="_idIndexMarker242">
    </a>
    
     use
    
    <strong class="source-inline">
     
      std::shared_mutex
     
    </strong>
    
     , we
    
    <a id="_idIndexMarker243">
    </a>
    
     need to include the
    
    <strong class="source-inline">
     
      &lt;shared_mutex&gt;
     
    </strong>
    
     
      header file.
     
    
   </p>
   <h3>
    
     Timed mutex types
    
   </h3>
   <p>
    
     The mutex types
    
    <a id="_idIndexMarker244">
    </a>
    
     we have seen until now behave in the same way when we want to acquire the lock for
    
    
     
      exclusive use:
     
    
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      
       std::lock()
      
     </strong>
     
      : The calling thread blocks until the lock
     
     
      
       is available
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       std::try_lock()
      
     </strong>
     
      : Returns
     
     <strong class="source-inline">
      
       false
      
     </strong>
     
      if the lock is
     
     
      
       not available
      
     
    </li>
   </ul>
   <p>
    
     In the case of
    
    <strong class="source-inline">
     
      std::lock()
     
    </strong>
    
     , the calling thread may be waiting for a long time, and we may need to just wait for a certain period of time and then let the thread proceed with some processing if it has not been able to acquire
    
    
     
      the lock.
     
    
   </p>
   <p>
    
     To achieve this goal, we can use the timed mutexes provided by the C++ Standard Library:
    
    <strong class="source-inline">
     
      std::timed_mutex
     
    </strong>
    
     ,
    
    <strong class="source-inline">
     
      std::recursive_timed_mutex
     
    </strong>
    
     ,
    
    
     
      and
     
    
    
     <strong class="source-inline">
      
       std::shared_time_mutex
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     They are similar to their non-timed counterparts and implement the following additional functions to allow waiting for the lock to be available for a specific period
    
    
     
      of time:
     
    
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      
       try_lock_for()
      
     </strong>
     
      : Tries to lock the mutex and blocks the thread until the specified time duration has elapsed (timed out).
     
     
      If the mutex is locked before the specified time duration, then it returns
     
     <strong class="source-inline">
      
       true
      
     </strong>
     
      ; otherwise, it
     
     
      
       returns
      
     
     
      <strong class="source-inline">
       
        false
       
      </strong>
     
     
      
       .
      
     
     <p class="list-inset">
      
       If the specified time duration is less than or equal to zero (
      
      <strong class="source-inline">
       
        timeout_duration.zero()
       
      </strong>
      
       ), then the function behaves exactly
      
      
       
        like
       
      
      
       <strong class="source-inline">
        
         try_lock()
        
       </strong>
      
      
       
        .
       
      
     </p>
     <p class="list-inset">
      
       This function may block for longer than the specified duration due to scheduling or
      
      
       
        contention delays.
       
      
     </p>
    </li>
    <li>
     <strong class="source-inline">
      
       try_lock_until()
      
     </strong>
     
      : Tries to lock the mutex until the specified timeout time or the mutex is locked, whichever comes first.
     
     
      In this case, we specify an instance in the future as a limit for
     
     
      
       the waiting.
      
     
    </li>
   </ul>
   <p>
    
     The following
    
    <a id="_idIndexMarker245">
    </a>
    
     example shows how to
    
    
     
      use
     
    
    
     <strong class="source-inline">
      
       std::try_lock_for()
      
     </strong>
    
    
     
      :
     
    
   </p>
   <pre class="source-code">
#include &lt;algorithm&gt;
#include &lt;chrono&gt;
#include &lt;iostream&gt;
#include &lt;mutex&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;
constexpr int NUM_THREADS = 8;
int counter = 0;
int failed = 0;
int main() {
    using namespace std::chrono_literals;
    std::timed_mutex tm;
    std::mutex m;
    auto worker = [&amp;] {
        for (int i = 0; i &lt; 10; ++i) {
            if (tm.try_lock_for(10ms)) {
                ++counter;
                std::cout &lt;&lt; "Counter: " &lt;&lt; counter &lt;&lt; std::endl;
                std::this_thread::sleep_for(10ms);
                m.unlock();
            }
            else {
                m.lock();
                ++failed;
                std::cout &lt;&lt; "Thread " &lt;&lt; std::this_thread::get_id() &lt;&lt; " failed to lock" &lt;&lt; std::endl;
                m.unlock();
            }
            std::this_thread::sleep_for(12ms);
        }
    };
    std::vector&lt;std::thread&gt; threads;
    for (int i = 0; i &lt; NUM_THREADS; ++i) {
        threads.emplace_back(worker);
    }
    for (auto&amp; t : threads) {
        t.join();
    }
    std::cout &lt;&lt; "Counter: " &lt;&lt; counter &lt;&lt; std::endl;
    std::cout &lt;&lt; "Failed: " &lt;&lt; failed &lt;&lt; std::endl;
    return 0;
}</pre>
   <p>
    
     The preceding code uses two locks:
    
    <strong class="source-inline">
     
      tm
     
    </strong>
    
     , a timed mutex, to synchronize access to
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     and writing to the screen if acquiring
    
    <strong class="source-inline">
     
      tm
     
    </strong>
    
     is successful, and
    
    <strong class="source-inline">
     
      m
     
    </strong>
    
     , a non-timed mutex, to synchronize
    
    <a id="_idIndexMarker246">
    </a>
    
     access to
    
    <strong class="source-inline">
     
      failed
     
    </strong>
    
     and writing to the screen if acquiring
    
    <strong class="source-inline">
     
      tm
     
    </strong>
    
     is
    
    
     
      not successful.
     
    
   </p>
   <h2 id="_idParaDest-81">
    <a id="_idTextAnchor080">
    </a>
    
     Problems when using locks
    
   </h2>
   <p>
    
     We have seen examples
    
    <a id="_idIndexMarker247">
    </a>
    
     using just a mutex (lock).
    
    
     If we only need one mutex and we acquire and release it properly, in general is not very difficult to write correct multithreaded code.
    
    
     Once we need more than one lock, the code complexity increases.
    
    
     Two common problems when using multiple locks are
    
    <em class="italic">
     
      deadlock
     
    </em>
    
     
      and
     
    
    
     <em class="italic">
      
       livelock
      
     </em>
    
    
     
      .
     
    
   </p>
   <h3>
    
     Deadlock
    
   </h3>
   <p>
    
     Let’s consider the
    
    <a id="_idIndexMarker248">
    </a>
    
     following scenario: to perform a certain task, a thread
    
    <a id="_idIndexMarker249">
    </a>
    
     needs to access two resources, and they cannot be accessed simultaneously by two or more threads (we need mutual exclusion to properly synchronize access to the required resources).
    
    
     Each resource is synchronized with a different
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     class.
    
    
     In this case, a thread must acquire the first resource mutex then acquire the second resource mutex, and finally process the resources and release
    
    
     
      both mutexes.
     
    
   </p>
   <p>
    
     When two threads try performing the aforementioned processing, something like this
    
    
     
      may happen:
     
    
   </p>
   <p>
    <em class="italic">
     
      Thread 1
     
    </em>
    
     and
    
    <em class="italic">
     
      thread 2
     
    </em>
    
     need to acquire two mutexes to perform the required processing.
    
    <em class="italic">
     
      Thread 1
     
    </em>
    
     acquires the first mutex and
    
    <em class="italic">
     
      thread 2
     
    </em>
    
     acquires the second mutex.
    
    
     Then,
    
    <em class="italic">
     
      thread 1
     
    </em>
    
     will be blocked forever waiting for the second mutex to be available, and
    
    <em class="italic">
     
      thread 2
     
    </em>
    
     will be blocked forever waiting for the first mutex to be available.
    
    
     This is called a
    
    <strong class="bold">
     
      deadlock
     
    </strong>
    
     because both threads will be blocked forever waiting for each other to release the
    
    
     
      required mutex.
     
    
   </p>
   <p>
    
     This is one of the most common issues in multithreaded code.
    
    
     In
    
    <a href="B22219_11.xhtml#_idTextAnchor228">
     
      <em class="italic">
       
        Chapter 11
       
      </em>
     
    </a>
    
     , about debugging, we will
    
    <a id="_idIndexMarker250">
    </a>
    
     learn how to spot this problem by inspecting the running (deadlocked) program
    
    <a id="_idIndexMarker251">
    </a>
    
     with
    
    
     
      a debugger.
     
    
   </p>
   <h3>
    
     Livelock
    
   </h3>
   <p>
    
     A possible solution for
    
    <a id="_idIndexMarker252">
    </a>
    
     deadlock could be the following: when
    
    <a id="_idIndexMarker253">
    </a>
    
     a thread tries to acquire the lock, it will block just for a limited time, and if still unsuccessful, it will release any lock it may
    
    
     
      have acquired.
     
    
   </p>
   <p>
    
     For example,
    
    <em class="italic">
     
      thread 1
     
    </em>
    
     acquires the first lock and
    
    <em class="italic">
     
      thread 2
     
    </em>
    
     acquires the second lock.
    
    
     After a certain time,
    
    <em class="italic">
     
      thread 1
     
    </em>
    
     still has not acquired the second lock, so it releases the first one.
    
    <em class="italic">
     
      Thread 2
     
    </em>
    
     may finish waiting too and release the lock it acquired (in this example, the
    
    
     
      second lock).
     
    
   </p>
   <p>
    
     This solution may work sometimes, but it is not right.
    
    
     Imagine this scenario:
    
    <em class="italic">
     
      Thread 1
     
    </em>
    
     has acquired the first lock and has acquired the second lock.
    
    
     After some time, both threads release their already acquired locks, and then they acquire the same locks again.
    
    
     Then, the threads release the locks, then acquire them again, and
    
    
     
      so on.
     
    
   </p>
   <p>
    
     The threads are unable to do anything but acquire a lock, wait, release the lock, and do the same again.
    
    
     This situation is called
    
    <strong class="bold">
     
      livelock
     
    </strong>
    
     because the threads are not just waiting forever (as in the deadlock case), but they are kind of alive and acquire and release a
    
    
     
      lock continuously.
     
    
   </p>
   <p>
    
     The most common solution for both deadlock and livelock situations is acquiring the locks in a consistent order.
    
    
     For example, if a thread needs to acquire two locks, it will always acquire the first lock first, and then it will acquire the second lock.
    
    
     The locks will be released in the opposite order (first releasing the second lock and then the first).
    
    
     If a second thread tries to acquire the first lock, it will have to wait until the first thread releases both locks, and deadlock will
    
    
     
      never happen.
     
    
   </p>
   <p>
    
     In this section, we have seen the mutex classes provided by the C++ Standard Library.
    
    
     We have studied their main features and the issues we may experience when using more than one lock.
    
    
     In the next section, we will see the mechanisms that the C++ Standard Library
    
    <a id="_idIndexMarker254">
    </a>
    
     provides to make acquiring and releasing
    
    
     
      mutexes.
     
    
    
     <a id="_idIndexMarker255">
     </a>
    
    
     
      easier.
     
    
   </p>
   <h1 id="_idParaDest-82">
    <a id="_idTextAnchor081">
    </a>
    
     Generic lock management
    
   </h1>
   <p>
    
     In the previous
    
    <a id="_idIndexMarker256">
    </a>
    
     section, we saw the different types of mutexes provided by the C++ Standard Library.
    
    
     In this section, we will see the provided classes to make the use of mutexes easier.
    
    
     This is done by using different wrapper classes.
    
    
     The following table summarizes the lock management classes and their
    
    
     
      main features:
     
    
   </p>
   <table class="No-Table-Style _idGenTablePara-1" id="table003">
    <colgroup>
     <col/>
     <col/>
     <col/>
    </colgroup>
    <thead>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <strong class="bold">
         
          Mutex
         
        </strong>
        
         <strong class="bold">
          
           Manager Class
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <strong class="bold">
         
          Supported
         
        </strong>
        
         <strong class="bold">
          
           Mutex Types
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         <strong class="bold">
          
           Mutexes Managed
          
         </strong>
        
       </p>
      </td>
     </tr>
    </thead>
    <tbody>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         <strong class="source-inline">
          
           std::lock_guard
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          All
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         1
        
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         <strong class="source-inline">
          
           std::scoped_lock
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          All
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         Zero
        
        
         
          or more
         
        
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         <strong class="source-inline">
          
           std::unique_lock
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         
          All
         
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         1
        
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        
         <strong class="source-inline">
          
           std::shared_lock
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         <strong class="source-inline">
          
           std::shared_mutex
          
         </strong>
        
       </p>
       <p>
        
         <strong class="source-inline">
          
           std::shared_timed_mutex
          
         </strong>
        
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        
         1
        
       </p>
      </td>
     </tr>
    </tbody>
   </table>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Table 4.3: Lock management classes and their features
    
   </p>
   <p>
    
     Let’s see each of the mutex management classes and their
    
    
     
      main features.
     
    
   </p>
   <h2 id="_idParaDest-83">
    <a id="_idTextAnchor082">
    </a>
    
     std::lock_guard
    
   </h2>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::lock_guard
     
    </strong>
    
     class is
    
    <a id="_idIndexMarker257">
    </a>
    
     a
    
    <strong class="bold">
     
      Resource Acquisition Is Initialization
     
    </strong>
    
     (
    
    <strong class="bold">
     
      RAII
     
    </strong>
    
     ) class
    
    <a id="_idIndexMarker258">
    </a>
    
     that
    
    <a id="_idIndexMarker259">
    </a>
    
     makes it easier to use mutexes and guarantees that a mutex will be released when the
    
    <strong class="source-inline">
     
      lock_guard
     
    </strong>
    
     destructor is called.
    
    
     This is very useful, for example, when dealing
    
    
     
      with exceptions.
     
    
   </p>
   <p>
    
     The following code shows the use of
    
    <strong class="source-inline">
     
      std::lock_guard
     
    </strong>
    
     and how it makes handling exceptions easier
    
    <a id="_idIndexMarker260">
    </a>
    
     when a lock is
    
    
     
      already
     
    
    
     <a id="_idIndexMarker261">
     </a>
    
    
     
      acquired:
     
    
   </p>
   <pre class="source-code">
#include &lt;format&gt;
#include &lt;iostream&gt;
#include &lt;mutex&gt;
#include &lt;thread&gt;
std::mutex mtx;
uint32_t counter{};
void function_throws() { throw std::runtime_error("Error"); }
int main() {
    auto worker = [] {
        for (int i = 0; i &lt; 1000000; ++i) {
            mtx.lock();
            counter++;
            mtx.unlock();
        }
    };
    auto worker_exceptions = [] {
        for (int i = 0; i &lt; 1000000; ++i) {
            try {
                std::lock_guard&lt;std::mutex&gt; lock(mtx);
                counter++;
                function_throws();
            } catch (std::system_error&amp; e) {
                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;
                return;
            } catch (...) {
                return;
            }
        }
    };
    std::thread t1(worker_exceptions);
    std::thread t2(worker);
    t1.join();
    t2.join();
    std::cout &lt;&lt; "Final counter value: " &lt;&lt; counter &lt;&lt; std::endl;
}</pre>
   <p>
    
     The
    
    <strong class="source-inline">
     
      function_throws()
     
    </strong>
    
     function is just a utility function that will throw
    
    
     
      an exception.
     
    
   </p>
   <p>
    
     In the previous code
    
    <a id="_idIndexMarker262">
    </a>
    
     example, the
    
    <strong class="source-inline">
     
      worker_exceptions()
     
    </strong>
    
     function is
    
    <a id="_idIndexMarker263">
    </a>
    
     executed by
    
    <strong class="source-inline">
     
      t1
     
    </strong>
    
     .
    
    
     In this case, the exception is handled to print meaningful messages.
    
    
     The lock is not explicitly acquired/released.
    
    
     This is delegated to
    
    <strong class="source-inline">
     
      lock
     
    </strong>
    
     , a
    
    <strong class="source-inline">
     
      std::lock_guard
     
    </strong>
    
     object.
    
    
     When the lock is constructed, it wraps the mutex and calls
    
    <strong class="source-inline">
     
      mtx.lock()
     
    </strong>
    
     , acquiring the lock.
    
    
     When
    
    <strong class="source-inline">
     
      lock
     
    </strong>
    
     is destroyed, the mutex is released automatically.
    
    
     In the event of an exception, the mutex will also be released because the scope where
    
    <strong class="source-inline">
     
      lock
     
    </strong>
    
     was defined
    
    
     
      is exited.
     
    
   </p>
   <p>
    
     There is another constructor implemented for
    
    <strong class="source-inline">
     
      std::lock_guard
     
    </strong>
    
     , receiving a parameter of type
    
    <strong class="source-inline">
     
      std::adopt_lock_t
     
    </strong>
    
     .
    
    
     Basically, this constructor makes it possible to wrap an already acquired
    
    <a id="_idIndexMarker264">
    </a>
    
     non-shared mutex, which will be
    
    <a id="_idIndexMarker265">
    </a>
    
     released automatically in the
    
    
     <strong class="source-inline">
      
       std::lock_guard
      
     </strong>
    
    
     
      destructor.
     
    
   </p>
   <h2 id="_idParaDest-84">
    <a id="_idTextAnchor083">
    </a>
    
     std::unique_lock
    
   </h2>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::lock_guard
     
    </strong>
    
     class is
    
    <a id="_idIndexMarker266">
    </a>
    
     just a simple
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     wrapper
    
    <a id="_idIndexMarker267">
    </a>
    
     that automatically acquires the mutex in its constructor (the thread will be blocked, waiting until the mutex is released by another thread) and releases the mutex in its destructor.
    
    
     This is very useful, but sometimes we need more control.
    
    
     For example,
    
    <strong class="source-inline">
     
      std::lock_guard
     
    </strong>
    
     will either call
    
    <strong class="source-inline">
     
      lock()
     
    </strong>
    
     on the mutex or assume the mutex is already acquired.
    
    
     We may prefer or really need to call
    
    <strong class="source-inline">
     
      try_lock
     
    </strong>
    
     .
    
    
     We also may want the
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     wrapper not to acquire the lock in its constructor; that is, we may want to defer the locking until a later moment.
    
    
     All this functionality is implemented
    
    
     
      by
     
    
    
     <strong class="source-inline">
      
       std::unique_lock
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::unique_lock
     
    </strong>
    
     constructor accepts a tag as its second parameter to indicate what we want to do with the underlying mutex.
    
    
     There are
    
    
     
      three options:
     
    
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      
       std::defer_lock
      
     </strong>
     
      : Does not acquire ownership of the mutex.
     
     
      The mutex is not locked in the constructor, and it will not be unlocked in the destructor if it is
     
     
      
       never acquired.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       std::adopt_lock
      
     </strong>
     
      : Assumes that the mutex has been acquired by the calling thread.
     
     
      It will be released in the destructor.
     
     
      This option is also available
     
     
      
       for
      
     
     
      <strong class="source-inline">
       
        std::lock_guard
       
      </strong>
     
     
      
       .
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       std::try_to_lock
      
     </strong>
     
      : Try to acquire the mutex
     
     
      
       without blocking.
      
     
    </li>
   </ul>
   <p>
    
     If we just pass the mutex as the only parameter to the
    
    <strong class="source-inline">
     
      std::unique_lock
     
    </strong>
    
     constructor, the behavior is the same as in
    
    <strong class="source-inline">
     
      std::lock_guard
     
    </strong>
    
     : it will block until the mutex is available and then acquire it.
    
    
     It will release the mutex in
    
    
     
      the destructor.
     
    
   </p>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::unique_lock
     
    </strong>
    
     class, unlike
    
    <strong class="source-inline">
     
      std::lock_guard
     
    </strong>
    
     , allows you to call
    
    <strong class="source-inline">
     
      lock()
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      unlock()
     
    </strong>
    
     to respectively acquire and release
    
    
     
      the mutex.
     
    
   </p>
   <h2 id="_idParaDest-85">
    <a id="_idTextAnchor084">
    </a>
    
     std::scoped_lock
    
   </h2>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::scoped_lock
     
    </strong>
    
     class, as with
    
    <strong class="source-inline">
     
      std::unique_lock
     
    </strong>
    
     , is a
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     wrapper
    
    <a id="_idIndexMarker268">
    </a>
    
     implementing
    
    <a id="_idIndexMarker269">
    </a>
    
     an RAII mechanism (remember – the mutexes will be released in the destructor if they are acquired).
    
    
     The main difference is that
    
    <strong class="source-inline">
     
      std::unique_lock
     
    </strong>
    
     , as its name implies, just wraps one mutex, but
    
    <strong class="source-inline">
     
      std::scoped_lock
     
    </strong>
    
     wraps zero or more mutexes.
    
    
     Also, the mutexes are acquired in the order they are passed to the
    
    <strong class="source-inline">
     
      std::scoped_lock
     
    </strong>
    
     constructor, hence
    
    
     
      avoiding deadlock.
     
    
   </p>
   <p>
    
     Let’s look at the
    
    
     
      following code:
     
    
   </p>
   <pre class="source-code">
std::mutex mtx1;
std::mutex mtx2;
// Acquire both mutexes avoiding deadlock
std::scoped_lock lock(mtx1, mtx2);
// Same as doing this
// std::lock(mtx1, mtx2);
// std::lock_guard&lt;std::mutex&gt; lock1(mtx1, std::adopt_lock);
// std::lock_guard&lt;std::mutex&gt; lock2(mtx2, std::adopt_lock);</pre>
   <p>
    
     The preceding code snippet shows how we can work with two mutex locks
    
    
     
      very easily.
     
    
   </p>
   <h2 id="_idParaDest-86">
    <a id="_idTextAnchor085">
    </a>
    
     std::shared_lock
    
   </h2>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::shared_lock
     
    </strong>
    
     class is
    
    <a id="_idIndexMarker270">
    </a>
    
     another
    
    <a id="_idIndexMarker271">
    </a>
    
     general-purpose mutex ownership wrapper.
    
    
     As with
    
    <strong class="source-inline">
     
      std::unique_lock
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      std::scoped_lock
     
    </strong>
    
     , it allows deferred locking and transferring the lock ownership.
    
    
     The main difference between
    
    <strong class="source-inline">
     
      std::unique_lock
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      std::shared_lock
     
    </strong>
    
     is that the latter is used to acquire/release the wrapped mutex in shared mode while the former is used to do the same in
    
    
     
      exclusive mode.
     
    
   </p>
   <p>
    
     In this section, we saw mutex wrapper classes and their main features.
    
    
     Next, we will introduce another
    
    <a id="_idIndexMarker272">
    </a>
    
     synchronization
    
    <a id="_idIndexMarker273">
    </a>
    
     mechanism:
    
    
     
      condition variables.
     
    
   </p>
   <h1 id="_idParaDest-87">
    <a id="_idTextAnchor086">
    </a>
    
     Condition variables
    
   </h1>
   <p>
    <strong class="bold">
     
      Condition variables
     
    </strong>
    
     are
    
    <a id="_idIndexMarker274">
    </a>
    
     another synchronization primitive provided by the C++ Standard Library.
    
    
     They allow multiple threads to communicate with each other.
    
    
     They also allow for several threads to wait for a notification from another thread.
    
    
     Condition variables are always associated with
    
    
     
      a mutex.
     
    
   </p>
   <p>
    
     In the following example, a thread must wait for a counter to be equal to a
    
    
     
      certain value:
     
    
   </p>
   <pre class="source-code">
#include &lt;chrono&gt;
#include &lt;condition_variable&gt;
#include &lt;iostream&gt;
#include &lt;mutex&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;
int counter = 0;
int main() {
    using namespace std::chrono_literals;
    std::mutex mtx;
    std::mutex cout_mtx;
    std::condition_variable cv;
    auto increment_counter = [&amp;] {
        for (int i = 0; i &lt; 20; ++i) {
            std::this_thread::sleep_for(100ms);
            mtx.lock();
            ++counter;
            mtx.unlock();
            cv.notify_one();
        }
    };
    auto wait_for_counter_non_zero_mtx = [&amp;] {
        mtx.lock();
        while (counter == 0) {
            mtx.unlock();
            std::this_thread::sleep_for(10ms);
            mtx.lock();
        }
        mtx.unlock();
        std::lock_guard&lt;std::mutex&gt; cout_lck(cout_mtx);
        std::cout &lt;&lt; "Counter is non-zero" &lt;&lt; std::endl;
    };
    auto wait_for_counter_10_cv = [&amp;] {
        std::unique_lock&lt;std::mutex&gt; lck(mtx);
        cv.wait(lck, [] { return counter == 10; });
        std::lock_guard&lt;std::mutex&gt; cout_lck(cout_mtx);
        std::cout &lt;&lt; "Counter is: " &lt;&lt; counter &lt;&lt; std::endl;
    };
    std::thread t1(wait_for_counter_non_zero_mtx);
    std::thread t2(wait_for_counter_10_cv);
    std::thread t3(increment_counter);
    t1.join();
    t2.join();
    t3.join();
    return 0;
}</pre>
   <p>
    
     There are two ways to wait for a certain condition: one is waiting in a loop and using a mutex as a synchronization mechanism.
    
    
     This is implemented in
    
    <strong class="source-inline">
     
      wait_for_counter_non_zero_mtx
     
    </strong>
    
     .
    
    
     The function acquires the lock, reads the value in
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     , and releases the lock.
    
    
     Then, it sleeps for 10 milliseconds, and the lock is acquired again.
    
    
     This is done in a
    
    <strong class="source-inline">
     
      while
     
    </strong>
    
     loop until
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     
      is nonzero.
     
    
   </p>
   <p>
    
     Condition variables
    
    <a id="_idIndexMarker275">
    </a>
    
     help us to simplify the previous code.
    
    
     The
    
    <strong class="source-inline">
     
      wait_for_counter_10_cv
     
    </strong>
    
     function waits until
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     is equal to 10.
    
    
     The thread will wait on the
    
    <strong class="source-inline">
     
      cv
     
    </strong>
    
     condition variable until it is notified by
    
    <strong class="source-inline">
     
      t1
     
    </strong>
    
     , the thread increasing
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     in
    
    
     
      a loop.
     
    
   </p>
   <p>
    <strong class="source-inline">
     
      The wait_for_counter_10_cv
     
    </strong>
    
     function works like this: a condition variable,
    
    <strong class="source-inline">
     
      cv
     
    </strong>
    
     , waits on a mutex,
    
    <strong class="source-inline">
     
      mtx
     
    </strong>
    
     .
    
    
     After calling
    
    <strong class="source-inline">
     
      wait()
     
    </strong>
    
     , the condition variable locks the mutex and waits until the condition is
    
    <strong class="source-inline">
     
      true
     
    </strong>
    
     (the condition is implemented in the lambda passed as a second parameter to the
    
    <strong class="source-inline">
     
      wait
     
    </strong>
    
     function).
    
    
     If the condition is not
    
    <strong class="source-inline">
     
      true
     
    </strong>
    
     , the condition variable remains in a
    
    <em class="italic">
     
      waiting
     
    </em>
    
     state until it is signaled and releases the mutex.
    
    
     Once the condition is met, the condition variable ends its waiting state and locks the mutex again to synchronize its access
    
    
     
      to
     
    
    
     <strong class="source-inline">
      
       counter
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     One important issue is that the conditional variable may be signaled by an unrelated thread.
    
    
     This is
    
    <a id="_idIndexMarker276">
    </a>
    
     called
    
    <strong class="bold">
     
      spurious wakeup
     
    </strong>
    
     .
    
    
     To avoid errors due to spurious wakeups, the condition is checked in
    
    <strong class="source-inline">
     
      wait
     
    </strong>
    
     .
    
    
     When the condition variable is signaled, the condition is checked again.
    
    
     In the event of a spurious wakeup and the counter being zero (the condition check returns
    
    <strong class="source-inline">
     
      false
     
    </strong>
    
     ), the waiting
    
    
     
      would resume.
     
    
   </p>
   <p>
    
     A different thread increments the counter by running
    
    <strong class="source-inline">
     
      increment_counter
     
    </strong>
    
     .
    
    
     Once
    
    <strong class="source-inline">
     
      counter
     
    </strong>
    
     has the desired value (in the example, this value is 10), it signals the waiting thread
    
    
     
      condition variable.
     
    
   </p>
   <p>
    
     There are two functions provided to signal a
    
    
     
      condition variable:
     
    
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      
       cv.notify_one()
      
     </strong>
     
      : Signal only one of the
     
     
      
       waiting threads
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       cv.notify_all()
      
     </strong>
     
      : Signal all of the
     
     
      
       waiting threads
      
     
    </li>
   </ul>
   <p>
    
     In this section, we have introduced condition variables, and we have seen a simple example of synchronization using condition variables and how in some cases it can simplify the
    
    <a id="_idIndexMarker277">
    </a>
    
     synchronization/waiting code.
    
    
     Now, let us turn our attention to implementing a synchronized queue using a mutex and two
    
    
     
      condition variables.
     
    
   </p>
   <h1 id="_idParaDest-88">
    <a id="_idTextAnchor087">
    </a>
    
     Implementing a multithreaded safe queue
    
   </h1>
   <p>
    
     In this section, we will see how to implement a
    
    <a id="_idIndexMarker278">
    </a>
    
     simple
    
    <strong class="bold">
     
      multithreaded safe queue
     
    </strong>
    
     .
    
    
     The queue will be accessed by multiple threads, some of them adding elements to
    
    <a id="_idIndexMarker279">
    </a>
    
     it (
    
    <strong class="bold">
     
      producer threads
     
    </strong>
    
     ) and some of them removing elements
    
    <a id="_idIndexMarker280">
    </a>
    
     from it (
    
    <strong class="bold">
     
      consumer threads
     
    </strong>
    
     ).
    
    
     For starters, we are going to assume just two threads: one producer and
    
    
     
      one consumer.
     
    
   </p>
   <p>
    
     Queues or
    
    <strong class="bold">
     
      first-in-first-outs
     
    </strong>
    
     (
    
    <strong class="bold">
     
      FIFOs
     
    </strong>
    
     ) are
    
    <a id="_idIndexMarker281">
    </a>
    
     a standard way of communication between threads.
    
    
     For example, if we need to receive packets containing data from a network connection as fast as possible, we may not have enough time in just one thread to receive all the packets and process them.
    
    
     In this case, we use a second thread to process the packets read by the first thread.
    
    
     Using just one consumer thread is simpler to synchronize (we will see how this is the case in
    
    <a href="B22219_05.xhtml#_idTextAnchor097">
     
      <em class="italic">
       
        Chapter 5
       
      </em>
     
    </a>
    
     ), and we have a guarantee that the packets will be processed in the same order as they arrived and were copied to the queue by the producer thread.
    
    
     It is true that the packets will really be read in the same order they were copied to the queue irrespective of the number of threads we have as consumers, but the consumer threads may be scheduled in and out by the operating system, and the full sequence of processed packets could be in a
    
    
     
      different order.
     
    
   </p>
   <p>
    
     In general, the easiest problem is that of a
    
    <strong class="bold">
     
      single-producer-single-consumer
     
    </strong>
    
     (
    
    <strong class="bold">
     
      SPSC
     
    </strong>
    
     ) queue.
    
    
     Different
    
    <a id="_idIndexMarker282">
    </a>
    
     problems may require multiple consumers if the processing of each item is too costly for just a thread, and we may have different sources of data to be processed and need multiple producer threads.
    
    
     The queue described in this section will work in
    
    
     
      every case.
     
    
   </p>
   <p>
    
     The first step in designing the queue is deciding what data structure we will use to store the queued items.
    
    
     We want the queue to contain elements of any type
    
    <em class="italic">
     
      T
     
    </em>
    
     , so we will implement it as a template class.
    
    
     Also, we are going to limit the capacity of the queue so that the maximum number of elements we can store in the queue will be fixed and set in the class constructor.
    
    
     It is possible, for example, to use a linked list and make the queue unbounded, or even
    
    <a id="_idIndexMarker283">
    </a>
    
     use a
    
    <strong class="bold">
     
      Standard Template Library
     
    </strong>
    
     (
    
    <strong class="bold">
     
      STL
     
    </strong>
    
     ) queue,
    
    <strong class="source-inline">
     
      std::queue
     
    </strong>
    
     , and let the queue grow to an arbitrary size.
    
    
     In this chapter, we will implement a fixed-size queue.
    
    
     We will revisit the implementation in
    
    <a href="B22219_05.xhtml#_idTextAnchor097">
     
      <em class="italic">
       
        Chapter 5
       
      </em>
     
    </a>
    
     and implement it in a very different way (we won’t be using any mutex or waiting on condition variables).
    
    
     For our current implementation, we will use an STL vector,
    
    <strong class="source-inline">
     
      std::vector&lt;T&gt;
     
    </strong>
    
     , to store the queued items.
    
    
     The vector will allocate memory for all the elements in the
    
    <a id="_idIndexMarker284">
    </a>
    
     queue class constructor, so there will be no memory allocations after that.
    
    
     When the queue is destroyed, the vector will destroy itself and will free the allocated memory.
    
    
     This is convenient and simplifies
    
    
     
      the implementation.
     
    
   </p>
   <p>
    
     We will use the vector
    
    <a id="_idIndexMarker285">
    </a>
    
     as a
    
    <strong class="bold">
     
      ring buffer
     
    </strong>
    
     .
    
    
     This means that, once we store an element at the end of the vector, the next one will be stored at the beginning, so we
    
    <em class="italic">
     
      wrap around
     
    </em>
    
     both locations to write and read elements from
    
    
     
      the vector.
     
    
   </p>
   <p>
    
     This is the first version of the queue class, quite simple and not
    
    
     
      useful yet:
     
    
   </p>
   <pre class="source-code">
template &lt;typename T&gt;
class synchronized_queue {
public:
    explicit synchronized_queue(size_t size) :
        capacity_{ size }, buffer_(capacity_)
        {}
private:
    std::size_t head_{ 0 };
    std::size_t tail_{ 0 };
    std::size_t capacity_;
    std::vector&lt;T&gt; buffer_;
};</pre>
   <p>
    
     The
    
    <strong class="source-inline">
     
      head
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      tail
     
    </strong>
    
     variables are used to indicate where to read or write the next element respectively.
    
    
     We also need to know when the queue is empty or full.
    
    
     If the queue is empty, the consumer thread won’t be able to get any item from the queue.
    
    
     If the queue is full, the producer thread will not be able to put any items in
    
    
     
      the queue.
     
    
   </p>
   <p>
    
     There are different ways to indicate when a queue is empty and when it is full.
    
    
     In this example, we follow
    
    
     
      this convention:
     
    
   </p>
   <ul>
    <li>
     
      If
     
     <strong class="source-inline">
      
       tail_ == head_
      
     </strong>
     
      , then the queue
     
     
      
       is empty
      
     
    </li>
    <li>
     
      If
     
     <strong class="source-inline">
      
       (tail_ + 1) % capacity_ == head_
      
     </strong>
     
      , then the queue
     
     
      
       is full
      
     
    </li>
   </ul>
   <p>
    
     Another way to
    
    <a id="_idIndexMarker286">
    </a>
    
     implement it would require just checking if
    
    <strong class="source-inline">
     
      tail_ == head_
     
    </strong>
    
     and using an extra flag to indicate if the queue is full or not (or using a counter to know how many items there are in the queue).
    
    
     We avoid any extra flag or counter in this example because the flag will be read and written by both the consumer and the producer threads, and we aim to minimize sharing data among threads as much as we can.
    
    
     Also, reducing sharing data will be the only option when we revisit the implementation of the queue in
    
    <a href="B22219_05.xhtml#_idTextAnchor097">
     
      <em class="italic">
       
        Chapter 5
       
      </em>
     
    </a>
    
     
      .
     
    
   </p>
   <p>
    
     There is a small issue here.
    
    
     Because of the way we check if the queue is full, we lose one slot in the buffer, so the real capacity is
    
    <strong class="source-inline">
     
      capacity_ - 1
     
    </strong>
    
     .
    
    
     We will consider the queue as full when there is just one empty slot.
    
    
     Because of this, we lose one queue slot (note that the slot will be used, but the queue will still be full when the number of items is
    
    <strong class="source-inline">
     
      capacity_ - 1
     
    </strong>
    
     ).
    
    
     In general, this is not
    
    
     
      an issue.
     
    
   </p>
   <p>
    
     The queue we are going to implement is a bounded queue (fixed size) implemented as a
    
    
     
      ring buffer.
     
    
   </p>
   <p>
    
     There is another detail to be considered here:
    
    <strong class="source-inline">
     
      head_ + 1
     
    </strong>
    
     must take into account that we wrap around the indices to the buffer (it is a ring buffer).
    
    
     So, we must do
    
    <strong class="source-inline">
     
      (head_ + 1) % capacity_
     
    </strong>
    
     .
    
    
     The modulo operator calculates the remainder of the index value divided by the
    
    
     
      queue capacity.
     
    
   </p>
   <p>
    
     The following code
    
    <a id="_idIndexMarker287">
    </a>
    
     shows the basic utility functions implemented as helper functions in the
    
    
     
      synchronized queue:
     
    
   </p>
   <pre class="source-code">
template &lt;typename T&gt;
class synchronized_queue {
public:
    explicit synchronized_queue(size_t size) :
        capacity_{ size }, buffer_(capacity_) {
    }
private:
    std::size_t next(std::size_t index) {
        return (index + 1)% capacity_;
    }
    bool is_full() const {
        return next(tail_) == head_;
    }
    bool is_empty() const {
        return tail_ == head_;
    }
    std::size_t head_{ 0 };
    std::size_t tail_{ 0 };
    std::size_t capacity_;
    std::vector&lt;T&gt; buffer_;
};</pre>
   <p>
    
     We have implemented a few useful functions to update both the head and the tail of the ring buffer and to check if the buffer is full or empty.
    
    
     Now, we can start implementing the
    
    
     
      queue functionality.
     
    
   </p>
   <p>
    
     The code for the full queue implementation is in the accompanying GitHub repo for the book.
    
    <em class="italic">
     
      Here, we only show the important bits
     
    </em>
    
     for the sake of simplicity and focus just on the synchronization aspects of the
    
    
     
      queue implementation.
     
    
   </p>
   <p>
    
     The interface to the queue has the following
    
    
     
      two functions:
     
    
   </p>
   <pre class="source-code">
void push(const T&amp; item);
void pop(T&amp; item);</pre>
   <p>
    
     The
    
    <strong class="source-inline">
     
      push
     
    </strong>
    
     function inserts an element in the queue, while
    
    <strong class="source-inline">
     
      pop
     
    </strong>
    
     gets an element from
    
    
     
      the queue.
     
    
   </p>
   <p>
    
     Let’s start with
    
    <strong class="source-inline">
     
      push
     
    </strong>
    
     .
    
    
     It inserts an item in the queue.
    
    
     If the queue is full,
    
    <strong class="source-inline">
     
      push
     
    </strong>
    
     will wait until the queue has at least an empty slot (a consumer removed an element from the queue).
    
    
     This way, the producer thread will be blocked until the queue has at least one empty slot (the not-full condition
    
    
     
      is met).
     
    
   </p>
   <p>
    
     We have seen earlier in this chapter that there is a synchronization mechanism called a condition variable that does just that.
    
    
     The
    
    <strong class="source-inline">
     
      push
     
    </strong>
    
     function will check if the condition is met, and when it is met, it will insert an item in the queue.
    
    
     If the condition is not met, the lock associated with the condition variable will be released, and the thread will wait on the condition variable until the condition
    
    
     
      is satisfied.
     
    
   </p>
   <p>
    
     It is possible for the
    
    <a id="_idIndexMarker288">
    </a>
    
     condition variable to just wait until the lock is released.
    
    
     We still need to check if the queue is full because a condition variable may end its waiting due to a spurious wakeup.
    
    
     This happens when the condition variable receives a notification not sent explicitly by any
    
    
     
      other thread.
     
    
   </p>
   <p>
    
     We add the following three member variables to the
    
    
     
      queue class:
     
    
   </p>
   <pre class="source-code">
std::mutex mtx_;
std::condition_variable not_full_;
Std::condition_variable not_empty_;</pre>
   <p>
    
     We need two condition variables – one to notify the consumers that the queue is not full (
    
    <strong class="source-inline">
     
      not_full_
     
    </strong>
    
     ) and another to notify the producers that the queue is not
    
    
     
      empty (
     
    
    
     <strong class="source-inline">
      
       not_empty_
      
     </strong>
    
    
     
      ).
     
    
   </p>
   <p>
    
     This is the code
    
    
     
      implementing
     
    
    
     <strong class="source-inline">
      
       push
      
     </strong>
    
    
     
      :
     
    
   </p>
   <pre class="source-code">
void push(const T&amp; item) {
    std::unique_lock&lt;std::mutex&gt; lock(mtx_);
    not_full_.wait(lock, [this]{ return !is_full(); });
    buffer_[tail_] = T;
    tail_ = increment(tail_);
    lock.unlock();
    not_empty_.notify_one();
}</pre>
   <p>
    
     Let’s think about a scenario with a single producer and a single consumer.
    
    
     We will see the
    
    <strong class="source-inline">
     
      pop
     
    </strong>
    
     function later, but as an advance, it also synchronizes with the mutex/condition variable.
    
    
     Both threads try to access the queue at the same time – the producer when inserting an element and the consumer when
    
    
     
      removing it.
     
    
   </p>
   <p>
    
     Let’s assume the consumer
    
    <a id="_idIndexMarker289">
    </a>
    
     acquires the lock first.
    
    
     This happens in
    
    <strong class="source-inline">
     
      [1]
     
    </strong>
    
     .
    
    
     The use of
    
    <strong class="source-inline">
     
      std::unique_lock
     
    </strong>
    
     is required by condition variables to use a mutex.
    
    
     In
    
    <strong class="source-inline">
     
      [2]
     
    </strong>
    
     , we wait on the condition variable until the condition in the predicate of the
    
    <strong class="source-inline">
     
      wait
     
    </strong>
    
     function is met.
    
    
     If it is not met, the lock is released for the consumer thread to be able to access
    
    
     
      the queue.
     
    
   </p>
   <p>
    
     Once the condition is met, the lock is acquired again, and the queue is updated in
    
    <strong class="source-inline">
     
      [3]
     
    </strong>
    
     .
    
    
     After updating the queue,
    
    <strong class="source-inline">
     
      [4]
     
    </strong>
    
     releases the lock and then
    
    <strong class="source-inline">
     
      [5]
     
    </strong>
    
     notifies one consumer thread that may be waiting on
    
    <strong class="source-inline">
     
      not_empty
     
    </strong>
    
     that the queue is effectively not
    
    
     
      empty now.
     
    
   </p>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::unique_lock
     
    </strong>
    
     class could release the mutex lock in its destructor, but we needed to release it in
    
    <strong class="source-inline">
     
      [4]
     
    </strong>
    
     because we didn’t want to release the lock after notifying the
    
    
     
      condition variable.
     
    
   </p>
   <p>
    
     The
    
    <strong class="source-inline">
     
      pop()
     
    </strong>
    
     function follows a similar logic, as shown in the
    
    
     
      following code:
     
    
   </p>
   <pre class="source-code">
void pop(T&amp; item)
{
    std::unique_lock&lt;std::mutex&gt; lock(mtx_);
    not_empty_.wait(lock, [this]{return !is_empty()});
    item = buffer_[head_];
    head_ = increment(head_);
    lock.unlock();
    not_full_.notify_one();
}</pre>
   <p>
    
     The code is very similar to that in the
    
    <strong class="source-inline">
     
      push
     
    </strong>
    
     function.
    
    <strong class="source-inline">
     
      [1]
     
    </strong>
    
     creates the
    
    <strong class="source-inline">
     
      std::unique_lock
     
    </strong>
    
     class required to use the
    
    <strong class="source-inline">
     
      not_empty_
     
    </strong>
    
     condition variable.
    
    <strong class="source-inline">
     
      [2]
     
    </strong>
    
     waits on
    
    <strong class="source-inline">
     
      not_empty_
     
    </strong>
    
     until it is notified that the queue is not empty.
    
    <strong class="source-inline">
     
      [3]
     
    </strong>
    
     reads the item from the queue, assigning it to the
    
    <strong class="source-inline">
     
      item
     
    </strong>
    
     variable, and then in
    
    <strong class="source-inline">
     
      [4]
     
    </strong>
    
     , the lock is released.
    
    
     Finally, in
    
    <strong class="source-inline">
     
      [5]
     
    </strong>
    
     , the
    
    <strong class="source-inline">
     
      not_full_
     
    </strong>
    
     condition variable is notified to indicate to the consumer that the queue is
    
    
     
      not full.
     
    
   </p>
   <p>
    
     Both
    
    <strong class="source-inline">
     
      push
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      pop
     
    </strong>
    
     functions
    
    <a id="_idIndexMarker290">
    </a>
    
     are blocking and waiting until the queue is not full or not empty respectively.
    
    
     We may need the thread to keep on running in the event of not being able to either insert or get a message to/from the queue – for example, to let it do some independent processing – and then try again to access
    
    
     
      the queue.
     
    
   </p>
   <p>
    
     The
    
    <strong class="source-inline">
     
      try_push
     
    </strong>
    
     function does exactly that.
    
    
     If the mutex is free to be acquired and the queue is not full, then the functionality is the same as the
    
    <strong class="source-inline">
     
      push
     
    </strong>
    
     function, but in this case,
    
    <strong class="source-inline">
     
      try_push
     
    </strong>
    
     doesn’t need to use any condition variable for synchronization (but it must notify the consumer).
    
    
     This is the code
    
    
     
      for
     
    
    
     <strong class="source-inline">
      
       try_push
      
     </strong>
    
    
     
      :
     
    
   </p>
   <pre class="source-code">
bool try_push(const T&amp; item) {
    std::unique_lock&lt;std::mutex&gt; lock(mtx_, std::try_to_lock);
    if (!lock || is_full()) {
        return false;
    }
    buffer_[tail_] = item;
    tail_ = next(tail_);
    lock.unlock();
    not_empty_.notify_one();
    return true;
}</pre>
   <p>
    
     The code works like this:
    
    <strong class="source-inline">
     
      [1]
     
    </strong>
    
     tries to acquire the lock and returns without blocking the calling thread.
    
    
     If the lock was already acquired, then it will evaluate to
    
    <strong class="source-inline">
     
      false
     
    </strong>
    
     .
    
    
     In
    
    <strong class="source-inline">
     
      [2]
     
    </strong>
    
     , in case the lock has not been acquired or the queue is full,
    
    <strong class="source-inline">
     
      try_push
     
    </strong>
    
     returns
    
    <strong class="source-inline">
     
      false
     
    </strong>
    
     to indicate to the caller that no item was inserted in the queue and delegates the waiting/blocking to the caller.
    
    
     Note that
    
    <strong class="source-inline">
     
      [3]
     
    </strong>
    
     returns
    
    <strong class="source-inline">
     
      false
     
    </strong>
    
     and the function terminates.
    
    
     If the lock was acquired, it will be released when the function exits and the
    
    <strong class="source-inline">
     
      std::unique_lock
     
    </strong>
    
     destructor
    
    
     
      is called.
     
    
   </p>
   <p>
    
     After the lock is
    
    <a id="_idIndexMarker291">
    </a>
    
     acquired and has checked that the queue is not full, then the item is inserted in the queue, and
    
    <strong class="source-inline">
     
      tail_
     
    </strong>
    
     is updated.
    
    
     In
    
    <strong class="source-inline">
     
      [5]
     
    </strong>
    
     , the lock is released, and in
    
    <strong class="source-inline">
     
      [6]
     
    </strong>
    
     , the consumer is notified that the queue is not empty anymore.
    
    
     This notification is required because the consumer may call
    
    <strong class="source-inline">
     
      pop
     
    </strong>
    
     instead
    
    
     
      of
     
    
    
     <strong class="source-inline">
      
       try_pop
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     Finally, the function returns
    
    <strong class="source-inline">
     
      true
     
    </strong>
    
     to indicate to the caller that the item was successfully inserted in
    
    
     
      the queue.
     
    
   </p>
   <p>
    
     The code for the corresponding
    
    <strong class="source-inline">
     
      try_pop
     
    </strong>
    
     function is shown next.
    
    
     As an exercise, try to understand how
    
    
     
      it works:
     
    
   </p>
   <pre class="source-code">
bool try_pop(T&amp; item) {
     std::unique_lock&lt;std::mutex&gt; lock(mtx_, std::try_to_lock);
     if (!lock || is_empty()) {
         return false;
     }
     item = buffer_[head_];
     head_ = next(head_);
     lock.unlock();
     not_empty_.notify_one();
     return true;
 }</pre>
   <p>
    
     This is the full code
    
    <a id="_idIndexMarker292">
    </a>
    
     for the queue we have implemented in
    
    
     
      this section:
     
    
   </p>
   <pre class="source-code">
#pragma once
#include &lt;condition_variable&gt;
#include &lt;mutex&gt;
#include &lt;vector&gt;
namespace async_prog {
template &lt;typename T&gt;
class queue {
public:
    queue(std::size_t capacity) : capacity_{capacity}, buffer_(capacity) {}
    void push(const T&amp; item) {
        std::unique_lock&lt;std::mutex&gt; lock(mtx_);
        not_full_.wait(lock, [this] { return !is_full(); });
        buffer_[tail_] = item;
        tail_ = next(tail_);
        lock.unlock();
        not_empty_.notify_one();
    }
    bool try_push(const T&amp; item) {
        std::unique_lock&lt;std::mutex&gt; lock(mtx_, std::try_to_lock);
        if (!lock || is_full()) {
            return false;
        }
        buffer_[tail_] = item;
        tail_ = next(tail_);
        lock.unlock();
        not_empty_.notify_one();
        return true;
    }
    void pop(T&amp; item) {
        std::unique_lock&lt;std::mutex&gt; lock(mtx_);
        not_empty_.wait(lock, [this] { return !is_empty(); });
        item = buffer_[head_];
        head_ = next(head_);
        lock.unlock();
        not_full_.notify_one();
    }
    bool try_pop(T&amp; item) {
        std::unique_lock&lt;std::mutex&gt; lock(mtx_, std::try_to_lock);
        if (!lock || is_empty()) {
            return false;
        }
        item = buffer_[head_];
        head_ = next(head_);
        lock.unlock();
        not_empty_.notify_one();
        return true;
    }
private:
    [[nodiscard]] std::size_t next(std::size_t idx) const noexcept {
        return ((idx + 1) % capacity_);
    }
    [[nodiscard]] bool is_empty() const noexcept { return (head_ == tail_); }
    [[nodiscard]] bool is_full() const noexcept { return (next(tail_) == head_); }
   private:
    std::mutex mtx_;
    std::condition_variable not_empty_;
    std::condition_variable not_full_;
    std::size_t head_{0};
    std::size_t tail_{0};
    std::size_t capacity_;
    std::vector&lt;T&gt; buffer_;
};
}</pre>
   <p>
    
     In this section, we have introduced condition variables and implemented a basic queue synchronized with a mutex and two condition variables, the two basic synchronization primitives provided by the C++ Standard Library
    
    
     
      since C++11.
     
    
   </p>
   <p>
    
     The queue example shows how synchronization is implemented using these synchronization
    
    <a id="_idIndexMarker293">
    </a>
    
     primitives and can be used as a basic building block for more elaborate utilities such as, for example, a
    
    
     
      thread pool.
     
    
   </p>
   <h1 id="_idParaDest-89">
    <a id="_idTextAnchor088">
    </a>
    
     Semaphores
    
   </h1>
   <p>
    
     C++20 introduces new
    
    <a id="_idIndexMarker294">
    </a>
    
     synchronization primitives to write multithreaded applications.
    
    
     In this section, we will look
    
    
     
      at semaphores.
     
    
   </p>
   <p>
    
     A
    
    <strong class="bold">
     
      semaphore
     
    </strong>
    
     is a counter that manages the number of permits available for accessing a shared resource.
    
    
     Semaphores can be classified into two
    
    
     
      main types:
     
    
   </p>
   <ul>
    <li>
     
      A
     
     <strong class="bold">
      
       binary semaphore
      
     </strong>
     
      is like a mutex.
     
     
      It has only two states: 0 and 1.
     
     
      Even though a binary semaphore is conceptually like a mutex, there are some differences between a binary semaphore and a mutex that we will see later in
     
     
      
       this section.
      
     
    </li>
    <li>
     
      A
     
     <strong class="bold">
      
       counting semaphore
      
     </strong>
     
      can have a value greater than 1 and is used to control access to a resource that has a limited number
     
     
      
       of instances.
      
     
    </li>
   </ul>
   <p>
    
     C++20 implements both binary and
    
    
     
      counting semaphores.
     
    
   </p>
   <h2 id="_idParaDest-90">
    <a id="_idTextAnchor089">
    </a>
    
     Binary semaphores
    
   </h2>
   <p>
    
     A binary semaphore is
    
    <a id="_idIndexMarker295">
    </a>
    
     a synchronization primitive
    
    <a id="_idIndexMarker296">
    </a>
    
     that can be used to control access to a shared resource.
    
    
     It has two states: 0 and 1.
    
    
     A semaphore with a value of 0 indicates that the resource is unavailable, while a semaphore with a value of 1 indicates that the resource
    
    
     
      is available.
     
    
   </p>
   <p>
    
     Binary semaphores can be used to implement mutual exclusion.
    
    
     This is achieved by using a binary semaphore to control access to the resource.
    
    
     When a thread wants to access the resource, it first checks the semaphore.
    
    
     If the semaphore is 1, the thread can access the resource.
    
    
     If the semaphore is 0, the thread must wait until the semaphore is 1 before it can access
    
    
     
      the resource.
     
    
   </p>
   <p>
    
     The most significant difference between mutexes and semaphores is that mutexes have exclusive ownership, whereas binary semaphores do not.
    
    
     Only the thread owning the mutex can release it.
    
    
     Semaphores can be signaled by any thread.
    
    
     A mutex is a locking mechanism for a critical section, and a semaphore is more like a signaling mechanism.
    
    
     In this respect, a semaphore is closer to a condition variable than a mutex.
    
    
     For this reason, semaphores are commonly used for signaling rather than for
    
    
     
      mutual exclusion.
     
    
   </p>
   <p>
    
     In C++20,
    
    <strong class="source-inline">
     
      std::binary_semaphore
     
    </strong>
    
     is an alias for the specialization of
    
    <strong class="source-inline">
     
      std::counting_semaphore
     
    </strong>
    
     , with
    
    <strong class="source-inline">
     
      LeastMaxValue
     
    </strong>
    
     
      being 1.
     
    
   </p>
   <p>
    
     Binary semaphores must be initialized with either 1 or 0, such
    
    
     
      as follows:
     
    
   </p>
   <pre class="source-code">
std::binary_semaphore sm1{ 0 };
std::binary_semaphore sm2{ 1 };</pre>
   <p>
    
     If the initial value is
    
    <strong class="source-inline">
     
      0
     
    </strong>
    
     , acquiring the semaphore will block the thread trying to acquire it, and before it can be acquired, it must be released by another thread.
    
    
     Acquiring a semaphore decreases
    
    <a id="_idIndexMarker297">
    </a>
    
     the counter, and releasing it increases the
    
    <a id="_idIndexMarker298">
    </a>
    
     counter.
    
    
     As previously stated, if the counter is
    
    <strong class="source-inline">
     
      0
     
    </strong>
    
     and a thread tries to acquire the lock (semaphore), the thread will be blocked until the semaphore counter is greater
    
    
     
      than
     
    
    
     <strong class="source-inline">
      
       0
      
     </strong>
    
    
     
      .
     
    
   </p>
   <h2 id="_idParaDest-91">
    <a id="_idTextAnchor090">
    </a>
    
     Counting semaphores
    
   </h2>
   <p>
    
     A counting semaphore
    
    <a id="_idIndexMarker299">
    </a>
    
     allows access to a shared resource by
    
    <a id="_idIndexMarker300">
    </a>
    
     more than one thread.
    
    
     The counter can be initialized to an arbitrary number, and it will be decreased every time a thread acquires the semaphore.
    
    
     As an example of how to use counting semaphores, we will modify the multithread safe queue we implemented in the previous section and use semaphores instead of condition variables to synchronize access to
    
    
     
      the queue.
     
    
   </p>
   <p>
    
     The member variables of the new class are
    
    
     
      the following:
     
    
   </p>
   <pre class="source-code">
template &lt;typename T&gt;
class queue {
 // public methods and private helper methods
private:
    std::counting_semaphore&lt;&gt; sem_empty_;
    std::counting_semaphore&lt;&gt; sem_full_;
    std::size_t head_{ 0 };
    std::size_t tail_{ 0 };
    std::size_t capacity_;
    std::vector&lt;T&gt; buffer_;
};</pre>
   <p>
    
     We still need
    
    <strong class="source-inline">
     
      head_
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      tail_
     
    </strong>
    
     to know where to read and write an element,
    
    <strong class="source-inline">
     
      capacity_
     
    </strong>
    
     for the wraparound of the indices, and
    
    <strong class="source-inline">
     
      buffer_
     
    </strong>
    
     , a
    
    <strong class="source-inline">
     
      std::vector&lt;T&gt;
     
    </strong>
    
     vector.
    
    
     But for now, we are not using a mutex, and we will use counting semaphores instead of condition variables.
    
    
     We will use two of them:
    
    <strong class="source-inline">
     
      sem_empty_
     
    </strong>
    
     to count the empty slots in the buffer (initially set to
    
    <strong class="source-inline">
     
      capacity_
     
    </strong>
    
     ) and
    
    <strong class="source-inline">
     
      sem_full_
     
    </strong>
    
     to count the non-empty slots in the buffer, initially set
    
    
     
      to 0.
     
    
   </p>
   <p>
    
     Now, let’s see
    
    <a id="_idIndexMarker301">
    </a>
    
     how
    
    <a id="_idIndexMarker302">
    </a>
    
     to implement
    
    <strong class="source-inline">
     
      push
     
    </strong>
    
     , the function used to insert items in
    
    
     
      a queue.
     
    
   </p>
   <p>
    
     In
    
    <strong class="source-inline">
     
      [1]
     
    </strong>
    
     ,
    
    <strong class="source-inline">
     
      sem_empty_
     
    </strong>
    
     is acquired, decreasing the semaphore counter.
    
    
     If the queue is full, then the thread will block until
    
    <strong class="source-inline">
     
      sem_empty_
     
    </strong>
    
     is released (signaled) by another thread.
    
    
     If the queue is not full, then the item is copied to the buffer, and
    
    <strong class="source-inline">
     
      tail_
     
    </strong>
    
     is updated in
    
    <strong class="source-inline">
     
      [2]
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      [3]
     
    </strong>
    
     .
    
    
     Finally,
    
    <strong class="source-inline">
     
      sem_full_
     
    </strong>
    
     is released in
    
    <strong class="source-inline">
     
      [4]
     
    </strong>
    
     , signaling another thread that the queue is not empty and there is at least one item in
    
    
     
      the buffer:
     
    
   </p>
   <pre class="source-code">
void push(const T&amp; item) {
    sem_empty_.acquire();
    buffer_[tail_] = item;
    tail_ = next(tail_);
    sem_full_.release();
}</pre>
   <p>
    
     The
    
    <strong class="source-inline">
     
      pop
     
    </strong>
    
     function is used to get elements from
    
    
     
      a queue:
     
    
   </p>
   <pre class="source-code">
void pop(T&amp; item) {
    sem_full_.acquire();
    item = buffer_[head_];
    head_ = next(head_);
    sem_empty_.release();
}</pre>
   <p>
    
     Here, in
    
    <strong class="source-inline">
     
      [1]
     
    </strong>
    
     , we successfully acquire
    
    <strong class="source-inline">
     
      sem_full_
     
    </strong>
    
     if the queue is not empty.
    
    
     Then, the item is read and
    
    <strong class="source-inline">
     
      head_
     
    </strong>
    
     updated in
    
    <strong class="source-inline">
     
      [2]
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      [3]
     
    </strong>
    
     respectively.
    
    
     Finally, we signal the consumer thread that the queue is not full,
    
    
     
      releasing
     
    
    
     <strong class="source-inline">
      
       sem_empty
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     There are several
    
    <a id="_idIndexMarker303">
    </a>
    
     issues in our first version of
    
    <strong class="source-inline">
     
      push
     
    </strong>
    
     .
    
    
     The first and
    
    <a id="_idIndexMarker304">
    </a>
    
     most important one is that
    
    <strong class="source-inline">
     
      sem_empty_
     
    </strong>
    
     allows more than one thread to access the critical section in the queue (
    
    <strong class="source-inline">
     
      [2]
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      [3]
     
    </strong>
    
     ).
    
    
     We need to synchronize this critical section and use
    
    
     
      a mutex.
     
    
   </p>
   <p>
    
     Here is the new version of
    
    <strong class="source-inline">
     
      push
     
    </strong>
    
     using a mutex
    
    
     
      for synchronization.
     
    
   </p>
   <p>
    
     In
    
    <strong class="source-inline">
     
      [2]
     
    </strong>
    
     , the lock is acquired (using
    
    <strong class="source-inline">
     
      std::unique_lock
     
    </strong>
    
     ), and in
    
    <strong class="source-inline">
     
      [5]
     
    </strong>
    
     , it is released.
    
    
     Using the lock will synchronize the critical section, preventing several threads from simultaneously accessing it and updating the queue concurrently without
    
    
     
      any synchronization:
     
    
   </p>
   <pre class="source-code">
void push(const T&amp; item)
{
    sem_empty_.acquire();
    std::unique_lock&lt;std::mutex&gt; lock(mtx_);
    buffer_[tail_] = item;
    tail_ = next(tail_);
    lock.unlock();
    sem_full_.release();
}</pre>
   <p>
    
     A second issue is that acquiring a semaphore is blocking, and as we have seen previously, sometimes the caller thread can do some processing instead of just waiting.
    
    
     The
    
    <strong class="source-inline">
     
      try_push
     
    </strong>
    
     function (and its corresponding
    
    <strong class="source-inline">
     
      try_pop
     
    </strong>
    
     function) implements this functionality.
    
    
     Let’s study
    
    <a id="_idIndexMarker305">
    </a>
    
     the code of
    
    <strong class="source-inline">
     
      try_push
     
    </strong>
    
     .
    
    
     Note
    
    <a id="_idIndexMarker306">
    </a>
    
     that
    
    <strong class="source-inline">
     
      try_push
     
    </strong>
    
     may still block on
    
    
     
      the mutex:
     
    
   </p>
   <pre class="source-code">
bool try_push(const T&amp; item) {
    if (!sem_empty_.try acquire()) {
        return false;
    }
    std::unique_lock&lt;std::mutex&gt; lock(mtx_);
    buffer_[tail_] = item;
    tail_ = next(tail_);
    lock.unlock();
    sem_full_.release();
    return true;
}</pre>
   <p>
    
     The only changes are
    
    <strong class="source-inline">
     
      [1]
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      [2]
     
    </strong>
    
     .
    
    
     Instead of blocking when acquiring the semaphore, we just try to acquire it, and if we fail, we return
    
    <strong class="source-inline">
     
      false
     
    </strong>
    
     .
    
    
     The
    
    <strong class="source-inline">
     
      try_acquire
     
    </strong>
    
     function may spuriously fail and return
    
    <strong class="source-inline">
     
      false
     
    </strong>
    
     even if the semaphore can be acquired (count is
    
    
     
      not zero).
     
    
   </p>
   <p>
    
     Here is the complete
    
    <a id="_idIndexMarker307">
    </a>
    
     code for the queue synchronized
    
    <a id="_idIndexMarker308">
    </a>
    
     
      with semaphores:
     
    
   </p>
   <pre class="source-code">
#pragma once
#include &lt;mutex&gt;
#include &lt;semaphore&gt;
#include &lt;vector&gt;
namespace async_prog {
template &lt;typename T&gt;
class semaphore_queue {
   public:
    semaphore_queue(std::size_t capacity)
        : sem_empty_(capacity), sem_full_(0), capacity_{capacity}, buffer_(capacity)
    {}
    void push(const T&amp; item) {
        sem_empty_.acquire();
        std::unique_lock&lt;std::mutex&gt; lock(mtx_);
        buffer_[tail_] = item;
        tail_ = next(tail_);
        lock.unlock();
        sem_full_.release();
    }
    bool try_push(const T&amp; item) {
        if (!sem_empty_.try_acquire()) {
            return false;
        }
        std::unique_lock&lt;std::mutex&gt; lock(mtx_);
        buffer_[tail_] = item;
        tail_ = next(tail_);
        lock.unlock();
        sem_full_.release();
        return true;
    }
    void pop(T&amp; item) {
        sem_full_.acquire();
        std::unique_lock&lt;std::mutex&gt; lock(mtx_);
        item = buffer_[head_];
        head_ = next(head_);
        lock.unlock();
        sem_empty_.release();
    }
    bool try_pop(T&amp; item) {
        if (!sem_full_.try_acquire()) {
            return false;
        }
        std::unique_lock&lt;std::mutex&gt; lock(mtx_);
        item = buffer_[head_];
        head_ = next(head_);
        lock.unlock();
        sem_empty_.release();
        return true;
    }
private:
    [[nodiscard]] std::size_t next(std::size_t idx) const noexcept {
        return ((idx + 1) % capacity_);
    }
private:
    std::mutex mtx_;
    std::counting_semaphore&lt;&gt; sem_empty_;
    std::counting_semaphore&lt;&gt; sem_full_;
    std::size_t head_{0};
    std::size_t tail_{0};
    std::size_t capacity_;
    std::vector&lt;T&gt; buffer_;
};</pre>
   <p>
    
     In this section, we have seen semaphores, a new synchronization primitive included in the C++ Standard Library since C++20.
    
    
     We learned how to use them to implement the same queue we implemented before but using semaphores as
    
    
     
      synchronization primitives.
     
    
   </p>
   <p>
    
     In the next section, we
    
    <a id="_idIndexMarker309">
    </a>
    
     will introduce
    
    <strong class="bold">
     
      barriers
     
    </strong>
    
     and
    
    <strong class="bold">
     
      latches
     
    </strong>
    
     , two new
    
    <a id="_idIndexMarker310">
    </a>
    
     synchronization mechanisms included in the C++ Standard Library
    
    
     
      since C++20.
     
    
   </p>
   <h1 id="_idParaDest-92">
    <a id="_idTextAnchor091">
    </a>
    
     Barriers and latches
    
   </h1>
   <p>
    
     In this section, we
    
    <a id="_idIndexMarker311">
    </a>
    
     will introduce
    
    <a id="_idIndexMarker312">
    </a>
    
     barriers and latches, two new synchronization primitives introduced in C++20.
    
    
     These mechanisms allow threads to wait for each other, thereby coordinating the execution of
    
    
     
      concurrent tasks.
     
    
   </p>
   <h2 id="_idParaDest-93">
    <a id="_idTextAnchor092">
    </a>
    
     std::latch
    
   </h2>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::latch
     
    </strong>
    
     latch is a
    
    <a id="_idIndexMarker313">
    </a>
    
     synchronization primitive that allows one or more threads to block until a specified number of operations are completed.
    
    
     It is a single-use object, and once the count reaches zero, it cannot
    
    
     
      be reset.
     
    
   </p>
   <p>
    
     The following example is a simple illustration of the use of latches in a multithreaded application.
    
    
     We want to write a function to multiply by two each element of a vector and then add all the elements of the vector.
    
    
     We will use three threads to multiply the vector elements by two and then one thread to add all the elements of the vector and obtain
    
    
     
      the result.
     
    
   </p>
   <p>
    
     We need two latches.
    
    
     The first one will be decremented by each of the three threads multiplying by two vector elements.
    
    
     The adding thread will wait for this latch to be zero.
    
    
     Then, the main thread will wait on the second latch to synchronize printing the result of adding all the vector’s elements.
    
    
     We can also wait for the thread performing the additions calling
    
    <strong class="source-inline">
     
      join
     
    </strong>
    
     on it, but this can be done with a
    
    
     
      latch too.
     
    
   </p>
   <p>
    
     Now, let’s analyze the code in functional blocks.
    
    
     We will include the full code for the latches and barriers example later in
    
    
     
      this section:
     
    
   </p>
   <pre class="source-code">
std::latch map_latch{ 3 };
auto map_thread = [&amp;](std::vector&lt;int&gt;&amp; numbers, int start, int end) {
    for (int i = start; i &lt; end; ++i) {
        numbers[i] *= 2;
    }
    map_latch.count_down();
};</pre>
   <p>
    
     Each multiplying thread will run this lambda function, multiplying by two elements of a certain range in the vector (from
    
    <strong class="source-inline">
     
      start
     
    </strong>
    
     to
    
    <strong class="source-inline">
     
      end
     
    </strong>
    
     ).
    
    
     Once the thread is done, it will decrease the
    
    <strong class="source-inline">
     
      map_latch
     
    </strong>
    
     counter by one.
    
    
     Once all the threads finish their tasks, the latch counter will be zero, and the thread blocked waiting on
    
    <strong class="source-inline">
     
      map_latch
     
    </strong>
    
     will be able to go on and add all the elements of the vector together.
    
    
     Note that the threads access different elements of the vector, so we don’t need to synchronize access to the vector itself, but we cannot start adding the numbers until all the multiplications
    
    
     
      are done.
     
    
   </p>
   <p>
    
     The code for the adding thread is
    
    
     
      the following:
     
    
   </p>
   <pre class="source-code">
std::latch reduce_latch{ 1 };
auto reduce_thread = [&amp;](const std::vector&lt;int&gt;&amp; numbers, int&amp; sum) {
    map_latch.wait();
    sum = std::accumulate(numbers.begin(), numbers.end(), 0);
    reduce_latch.count_down();
};</pre>
   <p>
    
     This thread waits until the
    
    <strong class="source-inline">
     
      map_latch
     
    </strong>
    
     counter goes down to zero, then adds all the elements of the vector, and finally decrements the
    
    <strong class="source-inline">
     
      reduce_latch
     
    </strong>
    
     counter (it will go down to zero) for the main thread to be able to print the
    
    
     
      final result:
     
    
   </p>
   <pre class="source-code">
reduce_latch.wait();
std::cout &lt;&lt; "All threads finished. The sum is: " &lt;&lt; sum &lt;&lt; '\n';</pre>
   <p>
    
     Having seen a basic
    
    <a id="_idIndexMarker314">
    </a>
    
     application of latches, next, let’s learn
    
    
     
      about barriers.
     
    
   </p>
   <h2 id="_idParaDest-94">
    <a id="_idTextAnchor093">
    </a>
    
     std::barrier
    
   </h2>
   <p>
    
     The
    
    <strong class="source-inline">
     
      std::barrier
     
    </strong>
    
     barrier is
    
    <a id="_idIndexMarker315">
    </a>
    
     another synchronization primitive used to synchronize a group of threads.
    
    
     The
    
    <strong class="source-inline">
     
      std::barrier
     
    </strong>
    
     barrier is reusable.
    
    
     Each thread reaches the barrier and waits until all participating threads reach the same barrier point (like what happens when we
    
    
     
      use latches).
     
    
   </p>
   <p>
    
     The main difference between
    
    <strong class="source-inline">
     
      std::barrier
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      std::latch
     
    </strong>
    
     is the reset capability.
    
    
     The
    
    <strong class="source-inline">
     
      std::latch
     
    </strong>
    
     latch is a single-use barrier with a countdown mechanism that cannot be reset.
    
    
     Once it reaches zero, it stays at zero.
    
    
     In contrast,
    
    <strong class="source-inline">
     
      std::barrier
     
    </strong>
    
     is reusable.
    
    
     It resets after all threads have reached the barrier, allowing the same set of threads to synchronize at the same barrier
    
    
     
      multiple times.
     
    
   </p>
   <p>
    
     When to use latches and when to use barriers?
    
    
     Use
    
    <strong class="source-inline">
     
      std::latch
     
    </strong>
    
     when you have a one-time gathering point for threads, such as waiting for multiple initializations to complete before proceeding.
    
    
     Use
    
    <strong class="source-inline">
     
      std::barrier
     
    </strong>
    
     when you need to synchronize threads repeatedly through multiple phases of a task or
    
    
     
      iterative computations.
     
    
   </p>
   <p>
    
     We will now rewrite the previous example, this time using barriers instead of latches.
    
    
     Each thread will multiply by two its corresponding range of vector elements, and then it will add them.
    
    
     The main thread will use
    
    <strong class="source-inline">
     
      join()
     
    </strong>
    
     in this example to wait for the processing to be finished and then add the results obtained by each of
    
    
     
      the threads.
     
    
   </p>
   <p>
    
     The code for the worker
    
    <a id="_idIndexMarker316">
    </a>
    
     thread is
    
    
     
      the following:
     
    
   </p>
   <pre class="source-code">
std::barrier map_barrier{ 3 };
auto worker_thread = [&amp;](std::vector&lt;int&gt;&amp; numbers, int start, int end, int id) {
    std::cout &lt;&lt; std::format("Thread {0} is starting...\n", id);
    for (int i = start; i &lt; end; ++i) {
        numbers[i] *= 2;
    }
    map_barrier.arrive_and_wait();
    for (int i = start; i &lt; end; ++i) {
        sum[id] += numbers[i];
    }
    map_barrier.arrive();
};</pre>
   <p>
    
     The code is synchronized with a barrier.
    
    
     When a worker thread finishes doing the multiplications, it decreases the
    
    <strong class="source-inline">
     
      map_barrier
     
    </strong>
    
     counter and waits for the barrier counter to be zero.
    
    
     Once it goes down to zero, the threads end their waiting and start doing the additions.
    
    
     The barrier counter is reset, and its value is again equal to three.
    
    
     Once the additions are done, the barrier counter is decremented again, but this time, the threads won’t wait because their task
    
    
     
      is done.
     
    
   </p>
   <p>
    
     Sure – each thread could have done the additions and then multiplied by two.
    
    
     They don’t need to wait for each other because the work done by any thread is independent of the work done by any other thread, but this is a good way of explaining how barriers work with an
    
    
     
      easy example.
     
    
   </p>
   <p>
    
     The main thread just
    
    <a id="_idIndexMarker317">
    </a>
    
     waits with
    
    <strong class="source-inline">
     
      join
     
    </strong>
    
     for the worker threads to finish and then prints
    
    
     
      the result:
     
    
   </p>
   <pre class="source-code">
for (auto&amp; t : workers) {
    t.join();
}
std::cout &lt;&lt; std::format("The total sum is {0}\n",
                         std::accumulate(sum.begin(), sum. End(), 0));</pre>
   <p>
    
     Here is the full code for the latches and
    
    
     
      barriers example:
     
    
   </p>
   <pre class="source-code">
#include &lt;algorithm&gt;
#include &lt;barrier&gt;
#include &lt;format&gt;
#include &lt;iostream&gt;
#include &lt;latch&gt;
#include &lt;numeric&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;
void multiply_add_latch() {
    const int NUM_THREADS{3};
    std::latch map_latch{NUM_THREADS};
    std::latch reduce_latch{1};
    std::vector&lt;int&gt; numbers(3000);
    int sum{};
    std::iota(numbers.begin(), numbers.end(), 0);
    auto map_thread = [&amp;](std::vector&lt;int&gt;&amp; numbers, int start, int end) {
        for (int i = start; i &lt; end; ++i) {
            numbers[i] *= 2;
        }
        map_latch.count_down();
    };
    auto reduce_thread = [&amp;](const std::vector&lt;int&gt;&amp; numbers, int&amp; sum) {
        map_latch.wait();
        sum = std::accumulate(numbers.begin(), numbers.end(), 0);
        reduce_latch.count_down();
    };
    for (int i = 0; i &lt; NUM_THREADS; ++i) {
        std::jthread t(map_thread, std::ref(numbers), 1000 * i, 1000 * (i + 1));
    }
    std::jthread t(reduce_thread, numbers, std::ref(sum));
    reduce_latch.wait();
    std::cout &lt;&lt; "All threads finished. The total sum is: " &lt;&lt; sum &lt;&lt; '\n';
}
void multiply_add_barrier() {
    const int NUM_THREADS{3};
    std::vector&lt;int&gt; sum(3, 0);
    std::vector&lt;int&gt; numbers(3000);
    std::iota(numbers.begin(), numbers.end(), 0);
    std::barrier map_barrier{NUM_THREADS};
    auto worker_thread = [&amp;](std::vector&lt;int&gt;&amp; numbers, int start, int end, int id) {
        std::cout &lt;&lt; std::format("Thread {0} is starting...\n", id);
        for (int i = start; i &lt; end; ++i) {
            numbers[i] *= 2;
        }
        map_barrier.arrive_and_wait();
        for (int i = start; i &lt; end; ++i) {
            sum[id] += numbers[i];
        }
        map_barrier.arrive();
    };
    std::vector&lt;std::jthread&gt; workers;
    for (int i = 0; i &lt; NUM_THREADS; ++i) {
        workers.emplace_back(worker_thread, std::ref(numbers), 1000 * i,
                             1000 * (i + 1), i);
    }
    for (auto&amp; t : workers) {
        t.join();
    }
    std::cout &lt;&lt; std::format("All threads finished. The total sum is: {0}\n",
     std::accumulate(sum.begin(), sum.end(), 0));
}
int main() {
    std::cout &lt;&lt; "Multiplying and reducing vector using barriers..." &lt;&lt; std::endl;
    multiply_add_barrier();
    std::cout &lt;&lt; "Multiplying and reducing vector using latches..." &lt;&lt; std::endl;
    multiply_add_latch();
    return 0;
}</pre>
   <p>
    
     In this section, we have seen barriers and latches.
    
    
     Though they are not so commonly used as mutexes, condition variables, and semaphores, it is always useful to know what they are.
    
    
     The simple examples presented here have illustrated a common use of barriers and latches: synchronizing
    
    <a id="_idIndexMarker318">
    </a>
    
     threads performing processing in
    
    
     
      different stages.
     
    
   </p>
   <p>
    
     Finally, we will see a mechanism to execute code just once, even if the code is called more than once from
    
    
     
      different threads.
     
    
   </p>
   <h1 id="_idParaDest-95">
    <a id="_idTextAnchor094">
    </a>
    
     Performing a task only once
    
   </h1>
   <p>
    
     Sometimes, we need to
    
    <a id="_idIndexMarker319">
    </a>
    
     perform a certain task just one time.
    
    
     For example, in a multithreaded application, several threads may run the same function to initialize a variable.
    
    
     Any of the running threads may do it, but we want the initialization to be done
    
    
     
      exactly once.
     
    
   </p>
   <p>
    
     The C++ Standard Library provides both
    
    <strong class="source-inline">
     
      std::once_flag
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      std::call_once
     
    </strong>
    
     to implement exactly that functionality.
    
    
     We will see how to implement this functionality using atomic operations in the
    
    
     
      next chapter.
     
    
   </p>
   <p>
    
     The following example will help us to understand how to use
    
    <strong class="source-inline">
     
      std::once_flag
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      std::call_once
     
    </strong>
    
     to achieve our goal of performing a task just one time when more than one thread tries to
    
    
     
      do it:
     
    
   </p>
   <pre class="source-code">
#include &lt;exception&gt;
#include &lt;iostream&gt;
#include &lt;mutex&gt;
#include &lt;thread&gt;
int main() {
    std::once_flag run_once_flag;
    std::once_flag run_once_exceptions_flag;
    auto thread_function = [&amp;] {
        std::call_once(run_once_flag, []{
            std::cout &lt;&lt; "This must run just once\n";
        });
    };
    std::jthread t1(thread_function);
    std::jthread t2(thread_function);
    std::jthread t3(thread_function);
    auto function_throws = [&amp;](bool throw_exception) {
        if (throw_exception) {
            std::cout &lt;&lt; "Throwing exception\n";
            throw std::runtime_error("runtime error");
        }
        std::cout &lt;&lt; "No exception was thrown\n";
    };
    auto thread_function_1 = [&amp;](bool throw_exception) {
        try {
            std::call_once(run_once_exceptions_flag,
                           function_throws,
                           throw_exception);
        }
        catch (...) {
        }
    };
    std::jthread t4(thread_function_1, true);
    std::jthread t5(thread_function_1, true);
    std::jthread t6(thread_function_1, false);
    return 0;
}</pre>
   <p>
    
     In the first part of the example, three threads,
    
    <strong class="source-inline">
     
      t1
     
    </strong>
    
     ,
    
    <strong class="source-inline">
     
      t2
     
    </strong>
    
     , and
    
    <strong class="source-inline">
     
      t3
     
    </strong>
    
     , run the
    
    <strong class="source-inline">
     
      thread_function
     
    </strong>
    
     function.
    
    
     This function calls a lambda from
    
    <strong class="source-inline">
     
      std::call_once
     
    </strong>
    
     .
    
    
     If you run the example, you
    
    <a id="_idIndexMarker320">
    </a>
    
     will see that the message
    
    <strong class="source-inline">
     
      This must run just once
     
    </strong>
    
     is printed only one time,
    
    
     
      as expected.
     
    
   </p>
   <p>
    
     In the second part of the example, again, three threads,
    
    <strong class="source-inline">
     
      t4
     
    </strong>
    
     ,
    
    <strong class="source-inline">
     
      t5
     
    </strong>
    
     , and
    
    <strong class="source-inline">
     
      t6
     
    </strong>
    
     , run the
    
    <strong class="source-inline">
     
      thread_function_1
     
    </strong>
    
     function.
    
    
     This function calls
    
    <strong class="source-inline">
     
      function_throws
     
    </strong>
    
     , which depending on a parameter may throw or not throw an exception.
    
    
     This code shows that, if the function called from
    
    <strong class="source-inline">
     
      std::call_once
     
    </strong>
    
     does not terminate successfully, then it doesn’t count as done and
    
    <strong class="source-inline">
     
      std::call_once
     
    </strong>
    
     should be called again.
    
    
     Only a successful function counts as a
    
    
     
      run function.
     
    
   </p>
   <p>
    
     This final section showed a simple mechanism we can use to ensure that a function is executed exactly once, even
    
    <a id="_idIndexMarker321">
    </a>
    
     if it is called more than once from the same or a
    
    
     
      different thread.
     
    
   </p>
   <h1 id="_idParaDest-96">
    <a id="_idTextAnchor095">
    </a>
    
     Summary
    
   </h1>
   <p>
    
     In this chapter, we learned how to use the lock-based synchronization primitives provided by the C++
    
    
     
      Standard Library.
     
    
   </p>
   <p>
    
     We started with an explanation of race conditions and the need for mutual exclusion.
    
    
     Then, we studied
    
    <strong class="source-inline">
     
      std::mutex
     
    </strong>
    
     and how to use it to solve race conditions.
    
    
     We also learned about the main problems when synchronizing with locks: deadlock
    
    
     
      and livelock.
     
    
   </p>
   <p>
    
     After learning about mutexes, we studied condition variables and implemented a synchronized queue using mutex and condition variables.
    
    
     Finally, we saw the new synchronization primitives introduced in C++20: semaphores, latches,
    
    
     
      and barriers.
     
    
   </p>
   <p>
    
     Finally, we studied the mechanisms provided by the C++ Standard Library to run a function just
    
    
     
      one time.
     
    
   </p>
   <p>
    
     In this chapter, we learned about the basic building blocks of thread synchronization and the foundation of asynchronous programming with multiple threads.
    
    
     Lock-based thread synchronization is the most used method to
    
    
     
      synchronize threads.
     
    
   </p>
   <p>
    
     In the next chapter, we will study lock-free thread synchronization.
    
    
     We will start with a review of atomicity, atomic operations, and atomic types provided by the C++20 Standard Library.
    
    
     We will show an implementation of a lock-free bound single-producer-single-consumer queue.
    
    
     We will also introduce the C++
    
    
     
      memory model.
     
    
   </p>
   <h1 id="_idParaDest-97">
    <a id="_idTextAnchor096">
    </a>
    
     Further reading
    
   </h1>
   <ul>
    <li>
     
      David R.
     
     
      Butenhof,
     
     <em class="italic">
      
       Programming with POSIX Threads
      
     </em>
     
      , Addison
     
     
      
       Wesley, 1997.
      
     
    </li>
    <li>
     
      Anthony Williams,
     
     <em class="italic">
      
       C++ Concurrency in Action
      
     </em>
     
      , Second Edition,
     
     
      
       Manning, 2019.
      
     
    </li>
   </ul>
  </div>
 </body></html>