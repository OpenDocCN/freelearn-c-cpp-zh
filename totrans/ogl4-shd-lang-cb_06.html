<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Image Processing and Screen Space Techniques</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will cover the following recipes:</p>
<ul>
<li>Applying an edge detection filter</li>
<li>Applying a Gaussian blur filter</li>
<li>Implementing HDR lighting with tone mapping</li>
<li>Creating a bloom effect</li>
<li>Using gamma correction to improve image quality</li>
<li>Using multisample anti-aliasing</li>
<li>Using deferred shading</li>
<li>Screen space ambient occlusion</li>
<li>Configuring the depth test</li>
<li>Implementing order-independent transparency</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will focus on techniques that work directly with the pixels in a framebuffer. These techniques typically involve multiple passes. An initial pass produces the pixel data and subsequent passes apply effects or further processes those pixels. To implement this, we often make use of the ability provided in OpenGL for rendering directly to a texture or set of textures (refer to the <em>Rendering to a texture</em> recipe in <a href="a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Using Textures</em>).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The ability to render to a texture, combined with the power of the fragment shader, opens up a huge range of possibilities. We can implement image processing techniques such as brightness, contrast, saturation, and sharpness by applying an additional process in the fragment shader prior to output. We can apply <strong>convolution</strong> filters such as edge detection, smoothing (blur), or sharpening. We'll take a closer look at convolution filters in the recipe on edge detection.</p>
<p>A related set of techniques involves rendering additional information to textures beyond the traditional color information and then, in a subsequent pass, further processing that information to produce the final rendered image. These techniques fall under the general category that is often called <strong>deferred shading</strong>.</p>
<p>In this chapter, we'll look at some examples of each of the preceding techniques. We'll start off with examples of convolution filters for edge detection, blur, and bloom. Then, we'll move on to the important topics of gamma correction and multisample anti-aliasing. Finally, we'll finish with a full example of deferred shading.</p>
<p>Most of the recipes in this chapter involve multiple passes. In order to apply a filter that operates on the pixels of the final rendered image, we start by rendering the scene to an intermediate buffer (a texture). Then, in a final pass, we render the texture to the screen by drawing a single fullscreen quad, applying the filter in the process. You'll see several variations on this theme in the following recipes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying an edge detection filter</h1>
                </header>
            
            <article>
                
<p><strong>Edge detection</strong> is an image processing technique that identifies regions where there is a significant change in the brightness of the image. It provides a way to detect the boundaries of objects and changes in the topology of the surface. It has applications in the field of computer vision, image processing, image analysis, and image pattern recognition. It can also be used to create some visually interesting effects. For example, it can make a 3D scene look similar to a 2D pencil sketch, as shown in the following image. To create this image, a teapot and torus were rendered normally, and then an edge detection filter was applied in a second pass:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/9eef129b-0de4-4aab-addf-f86a80d9aa4e.png" style="width:18.08em;height:12.75em;"/></div>
<p>The edge detection filter that we'll use here involves the use of a convolution filter, or convolution kernel (also called a <strong>filter kernel</strong>). A convolution filter is a matrix that defines how to transform a pixel by replacing it with the sum of the products between the values of nearby pixels and a set of pre-determined weights. As a simple example, consider the following convolution filter:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/3e329a4b-f800-4746-a66e-3fcd27c13cb8.png" style="width:20.00em;height:19.25em;"/></div>
<p class="mce-root"/>
<p>The 3 x 3 filter is shaded in gray, superimposed over a hypothetical grid of pixels. The numbers in bold represent the values of the filter kernel (weights), and the non-bold values are the pixel values. The values of the pixels could represent grayscale intensity or the value of one of the RGB components. Applying the filter to the center pixel in the gray area involves multiplying the corresponding cells together and summing the results. The result would be the new value for the center pixel (<strong>25</strong>). In this case, the value would be (<em>17 + 19 + 2 * 25 + 31 + 33</em>), or 150.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Of course, in order to apply a convolution filter, we need access to the pixels of the original image and a separate buffer to store the results of the filter. We'll achieve this here by using a two-pass algorithm. In the first pass, we'll render the image to a texture, and then in the second pass, we'll apply the filter by reading from the texture and send the filtered results to the screen.</p>
<p>One of the simplest convolution-based techniques for edge detection is the so-called <strong>Sobel operator</strong>. The Sobel operator is designed to approximate the gradient of the image intensity at each pixel. It does so by applying two 3 x 3 filters. The results of the two are the vertical and horizontal components of the gradient. We can then use the magnitude of the gradient as our edge trigger. When the magnitude of the gradient is above a certain threshold, we assume that the pixel is on an edge.</p>
<p>The 3 x 3 filter kernels used by the Sobel operator are shown in the following equation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e867208d-05ba-4cd1-aad4-3e340f580e28.png" style="width:20.42em;height:4.17em;"/></div>
<p>If the result of applying <em>S<sub>x</sub></em> is <em>s<sub>x</sub></em> and the result of applying <em>S<sub>y</sub></em> is <em>s<sub>y</sub></em>, then an approximation of the magnitude of the gradient is given by the following equation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3e91beb5-9542-4843-b4fc-c2b8623d842c.png" style="width:9.17em;height:3.17em;"/></div>
<p>If the value of <em>g</em> is above a certain threshold, we consider the pixel to be an edge pixel and we highlight it in the resulting image.</p>
<p>In this example, we'll implement this filter as the second pass of a two-pass algorithm. In the first pass, we'll render the scene using an appropriate lighting model, but we'll send the result to a texture. In the second pass, we'll render the entire texture as a screen-filling quad, and apply the filter to the texture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Set up a framebuffer object (refer to the <em>Rendering to a texture</em> recipe in <a href="a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml"/><a href="a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Using Textures</em>) that has the same dimensions as the main window. Connect the first color attachment of the FBO to a texture object in texture unit zero. During the first pass, we'll render directly to this texture. Make sure that the <kbd>mag</kbd> and <kbd>min</kbd> filters for this texture are set to <kbd>GL_NEAREST</kbd>. We don't want any interpolation for this algorithm.</p>
<p class="mce-root"/>
<p>Provide vertex information in vertex attribute zero, normals in vertex attribute one, and texture coordinates in vertex attribute two.</p>
<p>The following uniform variables need to be set from the OpenGL application:</p>
<ul>
<li><kbd>Width</kbd>: This is used to set the width of the screen window in pixels</li>
<li><kbd>Height</kbd>: This is used to set the height of the screen window in pixels</li>
<li><kbd>EdgeThreshold</kbd>: This is the minimum value of <kbd>g</kbd> squared required to be considered <em>on an edge</em></li>
<li><kbd>RenderTex</kbd>: This is the texture associated with the FBO</li>
</ul>
<p>Any other uniforms associated with the shading model should also be set from the OpenGL application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To create a shader program that applies the Sobel edge detection filter, perform the following steps:</p>
<ol>
<li>The vertex shader just converts the position and normal to camera coordinates and passes them along to the fragment shader.</li>
</ol>
<ol start="2">
<li>The fragment shader applies the reflection model in the first pass, and applies the edge detection filter in the second pass:</li>
</ol>
<pre style="padding-left: 60px">in vec3 Position; 
in vec3 Normal; <br/><br/>uniform int Pass; // Pass number
 
// The texture containing the results of the first pass 
layout( binding=0 ) uniform sampler2D RenderTex; 
 
uniform float EdgeThreshold; // The squared threshold 
 
// Light/material uniforms...
 
layout( location = 0 ) out vec4 FragColor; 
const vec3 lum = vec3(0.2126, 0.7152, 0.0722); 
 
vec3 blinnPhong( vec3 pos, vec3 norm ) {<br/> // ... 
} 
 
// Approximates the brightness of a RGB value. 
float luminance( vec3 color ) { 
 return dot(lum, color);
} <br/>vec4 pass1() { 
 return vec4(blinnPhong( Position, normalize(Normal) ),1.0); 
} 
<br/>vec4 pass2() { 
 ivec2 pix = ivec2(gl_FragCoord.xy); 
 float s00 = luminance( 
    texelFetchOffset(RenderTex, pix, 0, 
            ivec2(-1,1)).rgb); 
 float s10 = luminance( 
       texelFetchOffset(RenderTex, pix, 0, 
                ivec2(-1,0)).rgb); 
 float s20 = luminance( 
       texelFetchOffset(RenderTex, pix, 0, 
                ivec2(-1,-1)).rgb); 
 float s01 = luminance( 
       texelFetchOffset(RenderTex, pix, 0, 
                ivec2(0,1)).rgb); 
 float s21 = luminance( 
       texelFetchOffset(RenderTex, pix, 0, 
                ivec2(0,-1)).rgb); 
 float s02 = luminance( 
       texelFetchOffset(RenderTex, pix, 0, 
                ivec2(1,1)).rgb); 
 float s12 = luminance( 
       texelFetchOffset(RenderTex, pix, 0, 
                ivec2(1,0)).rgb); 
 float s22 = luminance( 
       texelFetchOffset(RenderTex, pix, 0, 
                ivec2(1,-1)).rgb); 
 
 float sx = s00 + 2 * s10 + s20 - (s02 + 2 * s12 + s22); 
 float sy = s00 + 2 * s01 + s02 - (s20 + 2 * s21 + s22); 
 
 float g = sx * sx + sy * sy; 
 
 if( g &gt; EdgeThreshold ) return vec4(1.0); 
 else return vec4(0.0,0.0,0.0,1.0); 
} 
 
void main() { 
  if( Pass == 1 ) FragColor = pass1();<br/>  if( Pass == 2 ) FragColor = pass2(); 
}</pre>
<p class="mce-root"/>
<p>In the render function of your OpenGL application, follow these steps for pass #1:</p>
<ol>
<li>Select FBO, and clear the color/depth buffers</li>
<li>Set the <kbd>Pass</kbd> uniform to <kbd>1</kbd></li>
<li>Set up the model, view, and projection matrices, and draw the scene</li>
</ol>
<p>For pass #2, carry out the following steps:</p>
<ol>
<li>Deselect the FBO (revert to the default framebuffer) and clear the color/depth buffers</li>
<li>Set the <kbd>Pass</kbd> uniform to <kbd>2</kbd></li>
<li>Set the model, view, and projection matrices to the identity matrix</li>
<li>Draw a single quad (or two triangles) that fills the screen (-1 to +1 in <em>x</em> and <em>y</em>), with texture coordinates that range from 0 to 1 in each dimension.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The first pass renders all of the scene's geometry of sending the output to a texture. We select the function <kbd>pass1</kbd>, which simply computes and applies the Blinn-Phong reflection model (refer to <a href="74703f9d-f69a-4b08-bb38-6e1066371207.xhtml">C<span class="ChapterrefPACKT">hapter 3</span></a>, <em>The Basics of GLSL Shaders</em>).</p>
<p>In the second pass, we select the function <kbd>pass2</kbd>, and render only a single quad that covers the entire screen. The purpose of this is to invoke the fragment shader once for every pixel in the image. In the <kbd>pass2</kbd> function, we retrieve the values of the eight neighboring pixels of the texture containing the results from the first pass, and compute their brightness by calling the <kbd>luminance</kbd> function. The horizontal and vertical Sobel filters are then applied and the results are stored in <kbd>sx</kbd> and <kbd>sy</kbd>.</p>
<div class="packt_infobox"><span>The </span><kbd>luminance</kbd><span> function determines the brightness of an RGB value by computing a weighted sum of the intensities. The weights are from the ITU-R Recommendation Rec. 709. For more details on this, see the Wikipedia entry for <em>luma</em>.</span></div>
<p>We then compute the squared value of the magnitude of the gradient (in order to avoid the square root) and store the result in <kbd>g</kbd>. If the value of <kbd>g</kbd> is greater than <kbd>EdgeThreshold</kbd>, we consider the pixel to be on an edge and we output a white pixel. Otherwise, we output a solid black pixel.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The Sobel operator is somewhat crude and tends to be sensitive to high frequency variations in the intensity. A quick look at Wikipedia will guide you to a number of other edge detection techniques that may be more accurate. It is also possible to reduce the amount of high frequency variation by adding a <em>blur pass</em> between the render and edge detection passes. The blur pass will smooth out the high frequency fluctuations and may improve the results of the edge detection pass.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimization techniques</h1>
                </header>
            
            <article>
                
<p>The technique discussed here requires eight texture fetches. Texture accesses can be somewhat slow, and reducing the number of accesses can result in substantial speed improvements. Chapter 24 of <em>GPU Gems: Programming Techniques, Tips and Tricks for Real-Time Graphics</em>, edited by Randima Fernando (Addison-Wesley Professional 2004), has an excellent discussion of ways to reduce the number of texture fetches in a filter operation by making use of so-called <em>helper</em> textures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter06/sceneedge.cpp</kbd> file in the example code</li>
<li>D. Ziou and S. Tabbone (<em>1998</em>), <em>Edge detection techniques: An overview</em>, <em>International Journal of Computer Vision</em>, <em>Vol 24</em>, <em>Issue 3</em></li>
<li><em>Frei-Chen edge detector</em>: <a href="http://rastergrid.com/blog/2011/01/frei-chen-edge-detector/"><span class="URLPACKT">http://rastergrid.com/blog/2011/01/frei-chen-edge-detector/</span></a></li>
<li>The <em>Rendering to a texture</em> <span>recipe in</span> <a href="a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a><span>,</span> <em>Using Textures</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying a Gaussian blur filter</h1>
                </header>
            
            <article>
                
<p>A blur filter can be useful in many different situations where the goal is to reduce the amount of noise in the image. As mentioned in the previous recipe, applying a blur filter prior to the edge detection pass may improve the results by reducing the amount of high frequency fluctuation across the image. The basic idea of any blur filter is to mix the color of a pixel with that of nearby pixels using a weighted sum. The weights typically decrease with the distance from the pixel (in 2D screen space) so that pixels that are far away contribute less than those closer to the pixel being blurred.</p>
<p class="mce-root"/>
<p>A <strong>Gaussian blur</strong> uses the two-dimensional Gaussian function to weight the contributions of the nearby pixels:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/176d86f8-545b-483b-a71d-f039f8430dad.png" style="width:15.92em;height:4.00em;"/></div>
<p>The sigma squared term is the <strong>variance</strong> of the Gaussian, and determines the width of the Gaussian curve. The Gaussian function is maximum at (0,0), which corresponds to the location of the pixel being blurred and its value decreases as <em>x</em> or <em>y</em> increases. The following graph shows the two-dimensional Gaussian function with a sigma squared value of 4.0:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/58b45fc2-0640-456d-8bb7-1f92ae40e81b.png" style="width:30.75em;height:19.92em;"/></div>
<p>The following images show a portion of an image before (left) and after (right) the Gaussian blur operation:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/b0930f16-21ef-4581-83db-3285266192ba.png" style="width:28.58em;height:10.08em;"/></div>
<p>To apply a Gaussian blur, for each pixel, we need to compute the weighted sum of all pixels in the image scaled by the value of the Gaussian function at that pixel (where the <em>x</em> and <em>y</em> coordinates of each pixel are based on an origin located at the pixel being blurred). The result of that sum is the new value for the pixel. However, there are two problems with the algorithm so far:</p>
<ul>
<li>As this is a <em>O(n<sup>2</sup>)</em> process (where <em>n</em> is the number of pixels in the image), it is likely to be too slow for real-time use</li>
<li>The weights must sum to one in order to avoid changing the overall brightness of<br/>
the image</li>
</ul>
<p>As we sampled the Gaussian function at discrete locations, and didn't sum over the entire (infinite) bounds of the function, the weights almost certainly do not sum to one.</p>
<p>We can deal with both of the preceding problems by limiting the number of pixels that we blur with a given pixel (instead of the entire image), and by normalizing the values of the Gaussian function. In this example, we'll use a 9 x 9 Gaussian blur filter. That is, we'll only compute the contributions of the 81 pixels in the neighborhood of the pixel being blurred.</p>
<p>Such a technique would require 81 texture fetches in the fragment shader, which is executed once for each pixel. The total number of texture fetches for an image of size 800 x 600 would be <em>800 * 600 * 81 = 38,880,000</em>. This seems like a lot, doesn't it? The good news is that we can substantially reduce the number of texture fetches by doing the Gaussian blur in two passes.</p>
<p>The two-dimensional Gaussian function can be decomposed into the product of two one-dimensional Gaussians:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/049816d2-d752-4b1f-ae5d-fce015f42579.png" style="width:12.75em;height:1.75em;"/></div>
<p>Where the one-dimensional Gaussian function is given by the following equation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9559cd7f-1c79-44d0-8fdd-6a0f1d61cb84.png" style="width:12.33em;height:3.50em;"/></div>
<p>So if <em>C<sub>ij</sub></em> is the color of the pixel at pixel location (i, j), the sum that we need to compute is given by the following equation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/677564d4-341f-4106-ad15-2724de2feb47.png" style="width:18.25em;height:4.50em;"/></div>
<p>This can be re-written using the fact that the two-dimensional Gaussian is a product of two one-dimensional Gaussians:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0e10f9ad-b403-481f-a342-8bc9b63d63ec.png" style="width:18.92em;height:4.25em;"/></div>
<p>This implies that we can compute the Gaussian blur in two passes. In the first pass, we can compute the sum over <em>j</em> (the vertical sum) in the preceding equation and store the results in a temporary texture. In the second pass, we compute the sum over <em>i</em> (the horizontal sum) using the results from the previous pass.</p>
<p>Now, before we look at the code, there is one important point that has to be addressed. As we mentioned previously, the Gaussian weights must sum to one in order to be a true weighted average. Therefore, we need to normalize our Gaussian weights, as in the following equation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a66eea6d-85ae-4dac-a260-644243f99225.png" style="width:20.58em;height:4.42em;"/></div>
<p>The value of <em>k</em> in the preceding equation is just the sum of the raw Gaussian weights:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4a92509c-23af-4070-815d-1f9bba693e15.png" style="width:7.25em;height:4.08em;"/></div>
<p>Phew! We've reduced the <em>O(n<sup><sub>2</sub></sup>)</em> problem to one that is <em>O(n)</em>. OK, with that, let's move on to the code.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We'll implement this technique using three passes and two textures. In the first pass, we'll render the entire scene to a texture. Then, in the second pass, we'll apply the first (vertical) sum to the texture from the first pass and store the results in another texture. Finally, in the third pass, we'll apply the horizontal sum to the texture from the second pass, and send the results to the default framebuffer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Set up two framebuffer objects (refer to the <em>Rendering to a texture</em> recipe in <a href="a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Using Textures</em>), and two corresponding textures. The first FBO should have a depth buffer because it will be used for the first pass. The second FBO need not have a depth buffer because, in the second and third passes, we'll only render a single screen-filling quad in order to execute the fragment shader once for each pixel.</p>
<p>As with the previous recipe, we'll use a uniform variable to select the functionality of each pass. The OpenGL program should also set the following uniform variables:</p>
<ul>
<li><kbd>Width</kbd>: This is used to set the width of the screen in pixels</li>
<li><kbd>Height</kbd>: This is used to set the height of the screen in pixels</li>
<li><kbd>Weight[]</kbd>: This is the array of normalized Gaussian weights</li>
<li><kbd>Texture0</kbd>: This is to set this to texture unit zero</li>
<li><kbd>PixOffset[]</kbd>: This is the array of offsets from the pixel being blurred</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In the fragment shader, we apply the Blinn-Phong reflection model in the first pass. In the second pass, we compute the vertical sum. In the third, we compute the horizontal sum:</p>
<pre>in vec3 Position; // Vertex position 
in vec3 Normal;  // Vertex normal <br/><br/>uniform int Pass; // Pass number<br/>layout(binding=0) uniform sampler2D Texture0; 
 
// Light/material uniforms ....<br/>layout( location = 0 ) out vec4 FragColor; 
 
uniform int PixOffset[5] = int[](0,1,2,3,4); 
uniform float Weight[5]; 
 
vec3 blinnPhong( vec3 pos, vec3 norm ) { 
  // ... 
} 
 
vec4 pass1() {
 return vec4(blinnPhong( Position, normalize(Normal) ),1.0); 
} 
 
vec4 pass2() { 
 ivec2 pix = ivec2(gl_FragCoord.xy); 
 vec4 sum = texelFetch(Texture0, pix, 0) * Weight[0]; 
 for( int i = 1; i &lt; 5; i++ ) 
 { 
  sum += texelFetchOffset( Texture0, pix, 0, 
        ivec2(0,PixOffset[i])) * Weight[i]; 
  sum += texelFetchOffset( Texture0, pix, 0, 
        ivec2(0,-PixOffset[i])) * Weight[i]; 
 } 
 return sum; 
} 
 
vec4 pass3() { 
 ivec2 pix = ivec2(gl_FragCoord.xy); 
 vec4 sum = texelFetch(Texture0, pix, 0) * Weight[0]; 
 for( int i = 1; i &lt; 5; i++ ) 
 { 
  sum += texelFetchOffset( Texture0, pix, 0, 
        ivec2(PixOffset[i],0)) * Weight[i]; 
  sum += texelFetchOffset( Texture0, pix, 0, 
        ivec2(-PixOffset[i],0)) * Weight[i]; 
 } 
 return sum; 
} 
 
void main() 
{ 
 if( Pass == 1 ) FragColor = pass1();<br/> else if( Pass == 2 ) FragColor = pass2();<br/> else if( Pass == 3 ) FragColor = pass3();
} </pre>
<p>In the OpenGL application, compute the Gaussian weights for the offsets found in the uniform variable <kbd>PixOffset</kbd>, and store the results in the array <kbd>Weight</kbd>. You could use the following code to do so:</p>
<pre>char uniName[20]; 
float weights[5], sum, sigma2 = 4.0f; 
 
// Compute and sum the weights 
weights[0] = gauss(0,sigma2); // The 1-D Gaussian function 
sum = weights[0]; 
for( int i = 1; i &lt; 5; i++ ) { 
 weights[i] = gauss(i, sigma2); 
 sum += 2 * weights[i]; 
} 
 
// Normalize the weights and set the uniform 
for( int i = 0; i &lt; 5; i++ ) { 
 snprintf(uniName, 20, "Weight[%d]", i); 
 prog.setUniform(uniName, weights[i] / sum); 
} </pre>
<p>In the main render function, implement the following steps for pass #1:</p>
<ol>
<li>Select the render framebuffer, enable the depth test, and clear the color/depth buffers</li>
<li>Set <kbd>Pass</kbd> to <kbd>1</kbd></li>
<li>Draw the scene</li>
</ol>
<p>Use the following steps for pass #2:</p>
<ol>
<li>Select the intermediate framebuffer, disable the depth test, and clear the color buffer</li>
<li>Set <kbd>Pass</kbd> to <kbd>2</kbd></li>
<li>Set the view, projection, and model matrices to the identity matrix</li>
<li>Bind the texture from pass #1 to texture unit zero</li>
<li>Draw a fullscreen quad</li>
</ol>
<p>Use the following steps for pass #3:</p>
<ol>
<li>Deselect the framebuffer (revert to the default), and clear the color buffer</li>
<li>Set <kbd>Pass</kbd> to <kbd>3</kbd></li>
<li>Bind the texture from pass #2 to texture unit zero</li>
<li>Draw a fullscreen quad</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In the preceding code for computing the Gaussian weights (code segment 3), the function named <kbd>gauss</kbd> computes the one-dimensional Gaussian function where the first argument is the value for <kbd>x</kbd> and the second argument is sigma squared. Note that we only need to compute the positive offsets because the Gaussian is symmetric about zero. As we are only computing the positive offsets, we need to carefully compute the sum of the weights. We double all of the non-zero values because they will be used twice (for the positive and negative offsets).</p>
<p>The first pass (function <kbd>pass1</kbd>) renders the scene to a texture using the Blinn-Phong reflection model.</p>
<p>The second pass (function <kbd>pass2</kbd>) applies the weighted vertical sum of the Gaussian blur operation, and stores the results in yet another texture. We read pixels from the texture created in the first pass, offset in the vertical direction by the amounts in the <kbd>PixOffset</kbd> array. We sum using weights from the <kbd>Weight</kbd> array. (The <kbd>dy</kbd> term is the height of a texel in texture coordinates.) We sum in both directions at the same time, a distance of four pixels in each vertical direction.</p>
<p>The third pass (<kbd>pass3</kbd>) is very similar to the second pass. We accumulate the weighted, horizontal sum using the texture from the second pass. By doing so, we are incorporating the sums produced in the second pass into our overall weighted sum, as described earlier. Thereby, we are creating a sum over a 9 x 9 pixel area around the destination pixel. For this pass, the output color goes to the default framebuffer to make up the final result.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Of course, we can also adapt the preceding technique to blur a larger range of texels by increasing the size of the arrays <kbd>Weight</kbd> and <kbd>PixOffset</kbd> and re-computing the weights, and/or we could use different values of <kbd>sigma2</kbd> to vary the shape of the Gaussian.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter06/sceneblur.cpp</kbd> file in the example code</li>
<li>Bilateral filtering: <a href="http://people.csail.mit.edu/sparis/bf_course/"><span class="URLPACKT">http://people.csail.mit.edu/sparis/bf_course/</span></a></li>
<li>The <em>Rendering to a texture</em> recipe in <a href="a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Using Textures</em></li>
<li>The <em>Applying an edge detection filter</em> recipe in this chapter</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing HDR lighting with tone mapping</h1>
                </header>
            
            <article>
                
<p>When rendering for most output devices (monitors or televisions), the device only supports a typical color precision of 8-bits per color component, or 24-bits per pixel. Therefore, for a given color component, we're limited to a range of intensities between 0 and 255. Internally, OpenGL uses floating-point values for color intensities, providing a wide range of both values and precision. These are eventually converted to 8-bit values by mapping the floating-point range [0.0, 1.0] to the range of an unsigned byte [0, 255] before rendering.</p>
<p>Real scenes, however, have a much wider range of luminance. For example, light sources that are visible in a scene, or direct reflections of them, can be hundreds to thousands of times brighter than the objects that are illuminated by the source. When we're working with 8-bits per channel, or the floating-point range [0.0, -1.0], we can't represent this range of intensities. If we decide to use a larger range of floating point values, we can do a better job of internally representing these intensities, but in the end, we still need to compress down to the 8-bit range.</p>
<p>The process of computing the lighting/shading using a larger dynamic range is often referred to as <strong>High Dynamic Range rendering</strong> (<strong>HDR rendering</strong>). Photographers are very familiar with this concept. When a photographer wants to capture a larger range of intensities than would normally be possible in a single exposure, he/she might take several images with different exposures to capture a wider range of values. This concept, called <strong>High Dynamic Range imaging</strong> (<strong>HDR imaging</strong>), is very similar in nature to the concept of HDR rendering. A post-processing pipeline that includes HDR is now considered a fundamentally essential part of any game engine.</p>
<p><strong>Tone mapping</strong> is the process of taking a wide dynamic range of values and compressing them into a smaller range that is appropriate for the output device. In computer graphics, generally, tone mapping is about mapping to the 8-bit range from some arbitrary range of values. The goal is to maintain the dark and light parts of the image so that both are visible and neither is completely <em>washed out</em>.</p>
<p>For example, a scene that includes a bright light source might cause our shading model to produce intensities that are greater than 1.0. If we were to simply send that to the output device, anything greater than 1.0 would be clamped to 255 and would appear white. The result might be an image that is mostly white, similar to a photograph that is over exposed.</p>
<p>Or, if we were to linearly compress the intensities to the [0, 255] range, the darker parts might be too dark or completely invisible. With tone mapping, we want to maintain the brightness of the light source and also maintain detail in the darker areas.</p>
<p class="mce-root"/>
<div class="packt_infobox"><span>This description just scratches the surface when it comes to tone mapping and HDR rendering/imaging. For more details, I recommend the book </span><em>High Dynamic Range Imaging</em><span> by Reinhard et al.</span></div>
<p>The mathematical function used to map from one dynamic range to a smaller range is called the <strong>Tone Mapping Operator</strong> (<strong>TMO</strong>). These generally come in two flavors, local operators and global operators. A local operator determines the new value for a given pixel by using its current value and perhaps the value of some nearby pixels. A global operator needs some information about the entire image in order to do its work. For example, it might need to have the overall average luminance of all pixels in the image. Other global operators use a histogram of luminance values over the entire image to help fine-tune the mapping.</p>
<p>In this recipe, we'll use the simple global operator described in the book <em>Real Time Rendering</em>. This operator uses the log-average luminance of all pixels in the image. The log-average is determined by taking the logarithm of the luminance and averaging those values, then converting back, as shown in the following equation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4829befe-6c06-4f97-b736-ff0be2195116.png" style="width:15.75em;height:1.92em;"/></div>
<p><em>L<sub>w</sub>(x, y)</em> is the luminance of the pixel at <em>(x, y)</em>. The <em>0.0001</em> term is included in order to avoid taking the logarithm of zero for black pixels. This log-average is then used as part of the tone mapping operator shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d4d7308f-de54-4830-b68f-f947084806dd.png" style="width:12.17em;height:2.75em;"/></div>
<p class="chapter-content">The <em>a</em> term in this equation is the key. It acts in a similar way to the exposure level in a camera. The typical values for <em>a</em> range from 0.18 to 0.72. Since this tone mapping operator compresses the dark and light values a bit too much, we'll use a modification of the previous equation that doesn't compress the dark values as much, and includes a maximum luminance (<em>L<sub>white</sub></em>), a configurable value that helps to reduce some of the extremely bright pixels:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4be147f8-a6dd-4961-b002-348bbb5437df.png" style="width:15.25em;height:3.92em;"/></div>
<p class="mce-root">This is the tone mapping operator that we'll use in this example. We'll render the scene to a high-resolution buffer, compute the log-average luminance, and then apply the previous tone-mapping operator in a second pass.</p>
<p class="chapter-content">However, there's one more detail that we need to deal with before we can start implementing. The previous equations all deal with luminance. Starting with an RGB value, we can compute its luminance, but once we modify the luminance, how do we modify the RGB components to reflect the new luminance without changing the hue (or chromaticity)?</p>
<div class="packt_infobox"><span>The <strong>chromaticity</strong> is the perceived color, independent of the brightness of that color. For example, grey and white are two brightness levels for the same color.</span></div>
<p class="mce-root">The solution involves switching color spaces. If we convert the scene to a color space that separates out the luminance from the chromaticity, then we can change the luminance value independently. The <strong>CIE XYZ</strong> color space has just what we need. The CIE XYZ color space was designed so that the <em>Y</em> component describes the luminance of the color and the chromaticity can be determined by two derived parameters (<em>x</em> and <em>y</em>). The derived color space is called the <strong>CIE xyY</strong> space, and is exactly what we're looking for. The <em>Y</em> component contains the luminance and the <em>x</em> and <em>y</em> components contain the chromaticity. By converting to the <em>CIE xyY</em> space, we've factored out the luminance from the chromaticity allowing us to change the luminance without affecting the perceived color.</p>
<p class="chapter-content">So the process involves converting from RGB to CIE XYZ, then converting to CIE xyY, modifying the luminance, and reversing the process to get back to RGB. Converting from RGB to CIE XYZ (and vice-versa) can be described as a transformation matrix (refer to the code or the <em>See also</em> section for the matrix).</p>
<p class="chapter-content">The conversion from XYZ to xyY involves the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/36de4b90-038e-4504-93da-cae3032f2530.png" style="width:17.42em;height:2.67em;"/></div>
<p class="chapter-content">Finally, converting from xyY back to XYZ is done using the following equations:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5eced92f-b4a5-4b83-8757-5be68a3558d6.png" style="width:15.58em;height:3.00em;"/></div>
<p class="chapter-content">The following images show an example of the results of this tone mapping operator. The left image shows the scene rendered without any tone mapping. The shading was deliberately calculated with a wide dynamic range using three strong light sources. The scene appears <em>blown out</em> because any values that are greater than 1.0 simply get clamped to the maximum intensity. The image on the right uses the same scene and the same shading, but with the previous tone mapping operator applied. Note the recovery of the specular highlights from the <em>blown out</em> areas on the sphere and teapot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5f514e34-78dc-4e56-8b09-e45613529447.png" style="width:39.42em;height:14.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">The steps involved are the following:</p>
<ol>
<li>Render the scene to a high-resolution texture.</li>
<li>Compute the log-average luminance (on the CPU).</li>
<li>Render a screen-filling quad to execute the fragment shader for each screen pixel. In the fragment shader, read from the texture created in step 1, apply the tone mapping operator, and send the results to the screen.</li>
</ol>
<p class="mce-root">To get set up, create a high-res texture (using <kbd>GL_RGB32F</kbd> or a similar format) attached to a framebuffer with a depth attachment. Set up your fragment shader with a uniform to select the pass. The vertex shader can simply pass through the position and normal in eye coordinates.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">To implement HDR tone mapping, we'll perform the following steps:</p>
<ol>
<li>In the first pass, we want to just render the scene to the high-resolution texture. Bind to the framebuffer that has the texture attached and render the scene normally. </li>
<li>Compute the log average luminance of the pixels in the texture. To do so, we'll pull the data from the texture and loop through the pixels on the CPU side. We do this on the CPU for simplicity; a GPU implementation, perhaps with a compute shader, would be faster:</li>
</ol>
<pre style="padding-left: 60px">int size = width * height;<br/>std::vector&lt;GLfloat&gt; texData(size*3);<br/>glActiveTexture(GL_TEXTURE0);<br/>glBindTexture(GL_TEXTURE_2D, hdrTex);<br/>glGetTexImage(GL_TEXTURE_2D, 0, GL_RGB, GL_FLOAT, texData.data());<br/>float sum = 0.0f;<br/>for( int i = 0; i &lt; size; i++ ) {<br/> float lum = computeLum(texData[i*3+0], texData[i*3+1], texData[i*3+2]);<br/> sum += logf( lum + 0.00001f );<br/>}<br/>float logAve = expf( sum / size ); </pre>
<ol start="3">
<li>Set the <kbd>AveLum</kbd> uniform variable using <kbd>logAve</kbd>. Switch back to the default frame buffer, and draw a screen-filling quad. In the fragment shader, apply the tone mapping operator to the values from the texture produced in step 1:</li>
</ol>
<pre style="padding-left: 60px">// Retrieve high-res color from texture 
vec4 color = texture( HdrTex, TexCoord ); 
   
// Convert to XYZ 
vec3 xyzCol = rgb2xyz * vec3(color); 
 
// Convert to xyY 
float xyzSum = xyzCol.x + xyzCol.y + xyzCol.z; 
vec3 xyYCol = vec3(0.0); 
if( xyzSum &gt; 0.0 ) // Avoid divide by zero 
  xyYCol = vec3( xyzCol.x / xyzSum, 
         xyzCol.y / xyzSum, xyzCol.y); 
 
// Apply the tone mapping operation to the luminance 
// (xyYCol.z or xyzCol.y) 
float L = (Exposure * xyYCol.z) / AveLum; 
L = (L * ( 1 + L / (White * White) )) / ( 1 + L ); 
 
// Using the new luminance, convert back to XYZ 
if( xyYCol.y &gt; 0.0 ) { 
 xyzCol.x = (L * xyYCol.x) / (xyYCol.y); 
 xyzCol.y = L; 
 xyzCol.z = (L * (1 - xyYCol.x - xyYCol.y))/xyYCol.y; 
} 
 
// Convert back to RGB and send to output buffer 
FragColor = vec4( xyz2rgb * xyzCol, 1.0); </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the first step, we render the scene to an HDR texture. In step 2, we compute the log-average luminance by retrieving the pixels from the texture and doing the computation on the CPU (OpenGL side).</p>
<p class="chapter-content">In step 3, we render a single screen-filling quad to execute the fragment shader for each screen pixel. In the fragment shader, we retrieve the HDR value from the texture and apply the tone-mapping operator. There are two <em>tunable</em> variables in this calculation. The <kbd>Exposure</kbd> variable corresponds to the <em>a</em> term in the tone mapping operator, and the variable <kbd>White</kbd> corresponds to <em>L<sub>white</sub></em>. For the previous image, we used values of <kbd>0.35</kbd> and <kbd>0.928</kbd>, respectively.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Tone mapping is not an exact science. Often, it is the process of experimenting with the parameters until you find something that works well and looks good.</p>
<p class="chapter-content">We could improve the efficiency of the previous technique by implementing step 2 on the GPU using compute shaders (refer to <a href="d67e01c8-8212-4d49-937f-6b1c62a57744.xhtml"><span class="ChapterrefPACKT">Chapter 11</span></a>, <em>Using Compute Shaders</em>) or some other clever technique. For example, we could write the logarithms to a texture, then iteratively downsample the full frame to a 1 x 1 texture. The final result would be available in that single pixel. However, with the flexibility of the compute shader, we could optimize this process even more.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter06/scenetonemap.cpp</kbd> file in the example code.</li>
<li>Bruce Justin Lindbloom has provided a useful web resource for conversion between color spaces. It includes among other things the transformation matrices needed to convert from RGB to XYZ. Visit: <a href="http://www.brucelindbloom.com/index.html?Eqn_XYZ_to_RGB.html"><span class="URLPACKT">http://www.brucelindbloom.com/index.html?Eqn_XYZ_to_RGB.html</span></a>.</li>
<li>The <em>Rendering to a texture</em> recipe in <a href="a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Using Textures</em>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a bloom effect</h1>
                </header>
            
            <article>
                
<p class="mce-root">A <strong>bloom</strong> is a visual effect where the bright parts of an image seem to have fringes that extend beyond the boundaries into the darker parts of the image. This effect has its basis in the way that cameras and the human visual system perceive areas of high contrast. Sources of bright light <em>bleed</em> into other areas of the image due to the so-called <strong>airy disc</strong>, which is a diffraction pattern produced by light that passes through an aperture.</p>
<p class="chapter-content">The following image shows a bloom effect in the animated film Elephant's Dream (© 2006, Blender Foundation / Netherlands Media Art Institute / <a href="https://orange.blender.org/"><span class="URLPACKT">www.elephantsdream.org</span></a>). The bright white color from the light behind the door <em>bleeds</em> into the darker parts of the image:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c5a4b8cd-0e82-4ad9-bc80-d092d4ca280f.png" style="width:21.58em;height:16.75em;"/></div>
<p class="chapter-content">Producing such an effect within an artificial CG rendering requires determining which parts of the image are bright enough, extracting those parts, blurring, and re-combining with the original image. Typically, the bloom effect is associated with HDR rendering. With HDR rendering, we can represent a larger range of intensities for each pixel (without quantizing artifacts). The bloom effect is more accurate when used in conjunction with HDR rendering due to the fact that a wider range of brightness values can be represented.</p>
<p class="chapter-content">Despite the fact that HDR produces higher quality results, it is still possible to produce a bloom effect when using standard (non-HDR) color values. The result may not be as effective, but the principles involved are similar for either situation.</p>
<p class="chapter-content">In the following example, we'll implement a bloom effect using five passes, consisting of four major steps:</p>
<ol>
<li>In the first pass, we will render the scene to an HDR texture.</li>
<li>The second pass will extract the parts of the image that are brighter than a certain threshold value. We'll refer to this as the <strong>bright-pass filter</strong>. We'll also downsample to a lower resolution buffer when applying this filter. We do so because we will gain additional blurring of the image when we read back from this buffer using a linear sampler.</li>
<li>The third and fourth passes will apply the Gaussian blur to the bright parts (refer to the <em>Applying a Gaussian blur filter</em> recipe in this chapter).</li>
<li>In the fifth pass, we'll apply tone mapping and add the tone-mapped result to the blurred bright-pass filter results.</li>
</ol>
<p class="mce-root">The following diagram summarizes this process. The upper-left image shows the scene rendered to an HDR buffer, with some of the colors out of gamut, causing much of the image to be <em>blown-out</em>. The bright-pass filter produces a smaller (about a quarter or an eighth of the original size) image with only pixels that correspond to a luminance that is above a threshold. The pixels are shown as white because they have values that are greater than one in this example. A two-pass Gaussian blur is applied to the downsampled image, and tone mapping is applied to the original image. The final image is produced by combining the tone-mapped image with the blurred bright-pass filter image. When sampling the latter, we use a linear filter to get additional blurring.The final result is shown at the bottom.</p>
<p class="mce-root">Note the bloom on the bright highlights on the sphere and the back wall:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7265b5a6-cb42-4b43-b879-96d34ee8589b.png" style="width:30.33em;height:39.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">For this recipe, we'll need two framebuffer objects, each associated with a texture. The first will be used for the original HDR render, and the second will be used for the two passes of the Gaussian blur operation. In the fragment shader, we'll access the original render via the variable <kbd>HdrTex</kbd>, and the two stages of the Gaussian blur will be accessed via <kbd>BlurTex</kbd>.</p>
<p class="chapter-content">The uniform variable <kbd>LumThresh</kbd> is the minimum luminance value used in the second pass. Any pixels greater than that value will be extracted and blurred in the following passes.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="chapter-content">Use a vertex shader that passes through the position and normal in eye coordinates.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">To generate a bloom effect, perform the following steps:</p>
<ol>
<li>In the first pass, render the scene to the framebuffer with a high-res backing texture.</li>
<li>In the second pass, switch to a framebuffer containing a high-res texture that is smaller than the size of the full render. In the example code, we use a texture that is one-eighth the size. Draw a fullscreen quad to initiate the fragement shader for each pixel, and in the fragment shader sample from the high-res texture, and write only those values that are larger than <kbd>LumThresh</kbd>. Otherwise, color the pixel black:</li>
</ol>
<pre style="padding-left: 60px">vec4 val = texture(HdrTex, TexCoord); 
if( luminance(val.rgb) &gt; LumThresh ) 
  FragColor = val; 
else 
  FragColor = vec4(0.0); </pre>
<ol start="3">
<li>In the third and fourth passes, apply the Gaussian blur to the results of the second pass. This can be done with a single framebuffer and two textures. Ping-pong between them, reading from one and writing to the other. For details, refer to the <em>Applying a Gaussian blur filter</em> recipe in this chapter.</li>
<li>In the fifth and final pass, switch to linear filtering from the texture that was produced in the fourth pass. Switch to the default frame buffer (the screen). Apply the tone-mapping operator from the <em>Implementing HDR lighting with tone mapping</em> recipe to the original image texture (<kbd>HdrTex</kbd>), and combine the results with the blurred texture from step 3. The linear filtering and magnification should provide an additional blur:</li>
</ol>
<pre style="padding-left: 60px">// Retrieve high-res color from texture 
vec4 color = texture( HdrTex, TexCoord ); 
 
// Apply tone mapping to color, result is toneMapColor 
... 
 
///////// Combine with blurred texture ////////// 
vec4 blurTex = texture(BlurTex1, TexCoord); 
 
FragColor = toneMapColor + blurTex;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Due to space constraints, the entire fragment shader code isn't shown here. The full code is available from the GitHub repository. The fragment shader is implemented with five methods, one for each pass. The first pass renders the scene normally to the HDR texture. During this pass, the active framebuffer object is the one associated with the texture corresponding to <kbd>HdrTex</kbd>, so the output is sent directly to that texture.</p>
<p class="chapter-content">The second pass reads from <kbd>HdrTex</kbd>, and writes out only pixels that have a luminance above the threshold value <kbd>LumThresh</kbd>. The value is (0,0,0,0) for pixels that have a brightness (luma) value below <kbd>LumThresh</kbd>. The output goes to the second framebuffer, which contains a much smaller texture (one-eighth the size of the original).</p>
<p class="chapter-content">The third and fourth passes apply the basic Gaussian blur operation (refer to the <em>Applying a Gaussian blur filter</em> recipe in this chapter). In these passes, we ping-pong between <kbd>BlurTex1</kbd> and <kbd>BlurTex2</kbd>, so we must be careful to swap the appropriate texture into the framebuffer.</p>
<p class="chapter-content">In the fifth pass, we switch back to the default framebuffer, and read from <kbd>HdrTex</kbd> and <kbd>BlurTex1</kbd>. <kbd>BlurTex1</kbd> contains the final blurred result from step four, and <kbd>HdrTex</kbd> contains the original render. We apply tone mapping to the results of <kbd>HdrTex</kbd> and add to <kbd>BlurTex1</kbd>. When pulling from <kbd>BlurTex1</kbd>, we are applying a linear filter, gaining additional blurring.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Note that we applied the tone-mapping operator to the original rendered image, but not to the blurred bright-pass filter image. One could choose to apply the TMO to the blurred image as well, but in practice, it is often not necessary. We should keep in mind that the bloom effect can also be visually distracting if it is overused. A little goes a long way.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter06/scenehdrbloom.cpp</kbd> file in the example code</li>
<li><em>HDR meets Black &amp; White 2</em> by Francesco Caruzzi in <em>Shader X6</em></li>
<li>The <em>Rendering to a texture</em> recipe in <a href="a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Using Textures</em></li>
<li>The <em>Applying an edge detection filter</em> recipe in this chapter</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using gamma correction to improve image quality</h1>
                </header>
            
            <article>
                
<p class="mce-root">It is common for many books about OpenGL and 3D graphics to somewhat neglect the subject of gamma correction. Lighting and shading calculations are performed, and the results are sent directly to the output buffer without modification. However, when we do this, we may produce results that don't quite end up looking the way we might expect. This may be due to the fact that computer monitors (both the old CRT and the newer LCD) have a non-linear response to pixel intensity. For example, without gamma correction, a grayscale value of 0.5 will not appear half as bright as a value of 1.0. Instead, it will appear to be darker than it should.</p>
<p class="chapter-content">The lower curve in the following graph shows the response curves of a typical monitor (gamma of <strong>2.2</strong>). The <em>x</em> axis is the intensity and the <em>y</em> axis is the perceived intensity. The dashed line represents a linear set of intensities. The upper curve represents gamma correction applied to linear values. The lower curve represents the response of a typical monitor. A grayscale value of <strong>0.5</strong> would appear to have a value of <strong>0.218</strong> on a screen that had a similar response curve:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/50cc84d4-57b2-4ad6-8c16-6deeed516115.png" style="width:24.92em;height:18.83em;"/></div>
<p class="chapter-content">The non-linear response of a typical monitor can usually be modeled using a simple power function. The perceived intensity (<em>P</em>) is proportional to the pixel intensity (<em>I</em>) raised to a power that is usually called gamma:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ef0da10a-863e-47ba-b6bb-698d998a9f89.png" style="width:3.42em;height:1.00em;"/></div>
<p class="chapter-content">Depending on the display device, the value of <em>γ</em> is usually somewhere between 2.0 and 2.4. Some kind of monitor calibration is often needed to determine a precise value.</p>
<p class="chapter-content">In order to compensate for this non-linear response, we can apply <strong>gamma correction</strong> before sending our results to the output framebuffer. Gamma correction involves raising the pixel intensities to a power that will compensate for the monitor's non-linear response to achieve a perceived result that appears linear. Raising the linear-space values to the power of <em>1/γ</em> will do the trick:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4221d774-e06e-46f9-ba66-e9f8d89c78ea.png" style="width:5.25em;height:2.25em;"/></div>
<p class="chapter-content">When rendering, we can do all of our lighting and shading computations, ignoring the fact that the monitor's response curve is non-linear. This is sometimes referred to as working in <em>linear space</em>. When the final result is to be written to the output framebuffer, we can apply the gamma correction by raising the pixel to the power of <span>1/</span><span>γ</span> just before writing. This is an important step that will help to improve the look of the rendered result.</p>
<p class="chapter-content">As an example, consider the following images. The image on the left is the mesh rendered without any consideration of gamma at all. The reflection model is computed and the results are directly sent to the framebuffer. On the right is the same mesh with gamma correction applied to the color just prior to output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/3fd4aef2-aafe-476b-bcd5-e22fc760592c.png" style="width:34.17em;height:12.92em;"/></div>
<p class="chapter-content">The obvious difference is that the left image appears much darker than the image on the right. However, the more important distinction is the variations from light to dark across the face. While the transition at the shadow terminator seems stronger than before, the variations within the lighted areas are less extreme.</p>
<p class="chapter-content">Applying gamma correction is an important technique, and can be effective in improving the results of a lighting model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Adding gamma correction to an OpenGL program can be as simple as carrying out the following steps:</p>
<ol>
<li>Set up a uniform variable named <kbd>Gamma</kbd> and set it to an appropriate value for your system.</li>
<li>Use the following code or something similar in a fragment shader:</li>
</ol>
<pre style="padding-left: 60px">vec3 color = lightingModel( ... ); 
FragColor = vec4( pow( color, vec3(1.0/Gamma) ), 1.0 ); </pre>
<p class="mce-root">If your shader involves texture data, care must be taken to make sure that the texture data is not already gamma-corrected so that you don't apply gamma correction twice (refer to the <em>There's more...</em> section of this recipe).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The color determined by the lighting/shading model is computed and stored in the variable <kbd>color</kbd>. We think of this as computing the color in linear space. There is no consideration of the monitor's response during the calculation of the shading model (assuming that we don't access any texture data that might already be gamma-corrected).</p>
<p class="chapter-content">To apply the correction, in the fragment shader, we raise the color of the pixel to the power of <kbd>1.0 / Gamma</kbd>, and apply the result to the output variable <kbd>FragColor</kbd>. Of course, the inverse of <kbd>Gamma</kbd> could be computed outside the fragment shader to avoid the division operation.</p>
<p class="chapter-content">We do not apply the gamma correction to the alpha component because it is typically not desired.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The application of gamma correction is a good idea in general; however, some care must be taken to make sure that computations are done within the correct space. For example, textures could be photographs or images produced by other imaging applications that apply gamma correction before storing the data within the image file.</p>
<p class="mce-root">Therefore, if we use a texture in our application as a part of the lighting model and then apply gamma correction, we will be effectively applying gamma correction twice to the data from the texture. Instead, we need to be careful to "decode" the texture data, by raising to the power of gamma prior to using the texture data in our lighting model.</p>
<p class="chapter-content">There is a very detailed discussion about these and other issues surrounding gamma correction in Chapter 24, <em>The Importance of Being Linear</em> in the book <em>GPU Gems 3</em>, edited by Hubert Nguyen (Addison-Wesley Professional 2007), and this is highly recommended supplemental reading.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter06/scenegamma.cpp</kbd> file in the example code</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using multisample anti-aliasing</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Anti-aliasing</strong> is the technique of removing or reducing the visual impact of <strong>aliasing artifacts</strong> that are present whenever high-resolution or continuous information is presented at a lower resolution. In real-time graphics, aliasing often reveals itself in the jagged appearance of polygon edges, or the visual distortion of textures that have a high degree of variation.</p>
<p class="chapter-content">The following images show an example of aliasing artifacts at the edge of an object. On the left, we can see that the edge appears jagged. This occurs because each pixel is determined to lie either completely inside the polygon or completely outside it. If the pixel is determined to be inside, it is shaded, otherwise it is not. Of course, this is not entirely accurate. Some pixels lie directly on the edge of the polygon. Some of the screen area that the pixel encompasses actually lies within the polygon and some lies outside. Better results could be achieved if we were to modify the shading of a pixel based upon the amount of the pixel's area that lies within the polygon. The result could be a mixture of the shaded surface's color with the color outside the polygon, where the area that is covered by the pixel determines the proportions. You might be thinking that this sounds like it would be prohibitively expensive to do. That may be true; however, we can approximate the results by using multiple <strong>samples</strong> per pixel.</p>
<p class="chapter-content"><strong>Multisample anti-aliasing</strong> involves evaluating multiple samples per pixel and combining the results of those samples to determine the final value for the pixel. The samples are located at various points within the pixel's extent. Most of these samples will fall inside the polygon, but for pixels near a polygon's edge, some will fall outside. The fragment shader will typically execute only once for each pixel as usual. For example, with 4x <strong>multisample anti-aliasing</strong> (<strong>MSAA</strong>), rasterization happens at four times the frequency. For each pixel, the fragment shader is executed once and the result is scaled based on how many of the four samples fall within the polygon.</p>
<p class="chapter-content">The following image on the right shows the results when multisample anti-aliasing is used. The inset image is a zoomed portion of the inside edge of a torus. On the left, the torus is rendered without MSAA. The right-hand image shows the results with MSAA enabled:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/48ab445e-7ee5-4686-b867-0b6bb88f129f.png" style="width:27.58em;height:12.75em;"/></div>
<p class="chapter-content">OpenGL has supported multisampling for some time now, and it is nearly transparent to use. It is simply a matter of turning it on or off. It works by using additional buffers to store the subpixel samples as they are processed. Then, the samples are combined together to produce a final color for the fragment. Nearly all of this is automatic, and there is little that a programmer can do to fine-tune the results. However, at the end of this recipe, we'll discuss the interpolation qualifiers that can affect the results.</p>
<p class="chapter-content">In this recipe, we'll see the code needed to enable multisample anti-aliasing in an OpenGL application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">The technique for enabling multisampling is unfortunately dependent on the window system API. In this example, we'll demonstrate how it is done using GLFW. The steps will be similar in GLUT or other APIs that support OpenGL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">To make sure that the multisample buffers are created and available, use the following steps:</p>
<ol>
<li>When creating your OpenGL window, you need to select an OpenGL context that supports MSAA. The following is how one would do so in GLFW:</li>
</ol>
<pre style="padding-left: 60px">glfwWindowHint(GLFW_SAMPLES, 8); 
... // Other settings 
window = glfwCreateWindow( WIN_WIDTH, WIN_HEIGHT, 
            "Window title", NULL, NULL ); </pre>
<ol start="2">
<li>To determine whether multisample buffers are available and how many samples per-pixel are actually being used, you can use the following code (or something similar):</li>
</ol>
<pre style="padding-left: 60px">GLint bufs, samples; 
glGetIntegerv(GL_SAMPLE_BUFFERS, &amp;bufs); 
glGetIntegerv(GL_SAMPLES, &amp;samples); 
printf("MSAA: buffers = %d samples = %dn", bufs, samples); </pre>
<ol start="3">
<li>To enable multisampling, use the following:</li>
</ol>
<pre style="padding-left: 60px">glEnable(GL_MULTISAMPLE); </pre>
<ol start="4">
<li>To disable multisampling, use the following:</li>
</ol>
<pre style="padding-left: 60px">glDisable(GL_MULTISAMPLE); </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">As we just mentioned, the technique for creating an OpenGL context with multisample buffers is dependent on the API used for interacting with the window system. The preceding example demonstrates how it might be done using GLFW. Once the OpenGL context is created, it is easy to enable multisampling by simply using the <kbd>glEnable</kbd> call shown in the preceding example.</p>
<p class="chapter-content">Stay tuned, because in the next section, we'll discuss a subtle issue surrounding interpolation of shader variables when multisample anti-aliasing is enabled.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">There are two interpolation qualifiers within the GLSL that allow the programmer to fine-tune some aspects of multisampling. They are <kbd>sample</kbd> and <kbd>centroid</kbd>.</p>
<p class="chapter-content">Before we can get into how <kbd>sample</kbd> and <kbd>centroid</kbd> work, we need a bit of background. Let's consider the way that polygon edges are handled without multisampling. A fragment is determined to be inside or outside of a polygon by determining where the center of that pixel lies. If the center is within the polygon, the pixel is shaded, otherwise it is not. The following image represents this behavior. It shows pixels near a polygon edge without MSAA. The line represents the edge of the polygon. Gray pixels are considered to be inside the polygon. White pixels are outside and are not shaded. The dots represent the pixel centers:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/749a3226-d018-4f8f-8944-265097202507.png" style="width:14.75em;height:16.33em;"/></div>
<p class="chapter-content">The values for the interpolated variables (the fragment shader's input variables) are interpolated with respect to the center of each fragment, which will always be inside the polygon.</p>
<p class="chapter-content">When multisample anti-aliasing is enabled, multiple samples are computed per fragment at various locations within the fragment's extent. If any of those samples lie within the polygon, then the shader is executed at least once for that pixel (but not necessarily for each sample).</p>
<p class="chapter-content">As a visual example, the following image represents pixels near a polygon's edge. The dots represent the samples. The dark samples lie within the polygon and the white samples lie outside the polygon. If any sample lies within the polygon, the fragment shader is executed (usually only once) for that pixel. Note that for some pixels, the pixel center lies outside the polygon. So, with MSAA, the fragment shader may execute slightly more often near the edges of polygons:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/bba2698a-8fc3-40ce-8440-9f9a3db25a3d.png" style="width:16.83em;height:16.33em;"/></div>
<p><span>Now, here's the important point. The values of the fragment shader's input variables are normally interpolated to the center of the pixel rather than to the location of any particular sample. In other</span> words<span>, the value that is used by the fragment shader is determined by interpolating to the location of the fragment's center, which may lie outside the polygon! If we are relying on the fact that the fragment shader's input variables are interpolated strictly between their values at the vertices (and not outside that range), then this might lead to unexpected results.</span></p>
<p class="chapter-content">As an example, consider the following portion of a fragment shader:</p>
<pre>in vec2 TexCoord; 
 
layout( location = 0 ) out vec4 FragColor; 
 
void main() { 
 vec3 yellow = vec3(1.0,1.0,0.0); 
 vec3 color = vec3(0.0);  // black 
 if( TexCoord.s &gt; 1.0 ) 
  color = yellow; 
 FragColor = vec4( color , 1.0 ); 
}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">This shader is designed to color the polygon black unless the <kbd>s</kbd> component of the texture coordinate is greater than one. In that case, the fragment gets a yellow color. If we render a square with texture coordinates that range from zero to one in each direction, we may get the results shown in the following image on the left. The following images show the enlarged edge of a polygon where the <kbd>s</kbd> texture coordinate is about <kbd>1.0</kbd>. Both images were rendered using the preceding shader. The right-hand image was created using the <kbd>centroid</kbd> qualifier (more on this later in this chapter):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/434513f5-5d71-4f11-9093-e3de16063b00.png" style="width:25.17em;height:10.17em;"/></div>
<p class="mce-root">The left image shows that some pixels along the edge have a lighter color (yellow, if the image is in full color). This is due to the fact that the texture coordinate is interpolated to the pixel's center, rather than to any particular sample's location. Some of the fragments along the edge have a center that lies outside of the polygon and therefore end up with a texture coordinate that is greater than one!</p>
<p class="mce-root">We can ask OpenGL to instead compute the value for the input variable by interpolating to some location that is not only within the pixel but also within the polygon. We can do so by using the <kbd>centroid</kbd> qualifier, as shown in the following code:</p>
<pre>centroid in vec2 TexCoord; </pre>
<p class="mce-root">(The qualifier needs to also be included with the corresponding output variable in the vertex shader.) When <kbd>centroid</kbd> is used with the preceding shader, we get the preceding image shown on the right.</p>
<div class="packt_infobox"><span>In general, we should use </span><kbd>centroid</kbd><span> or </span><kbd>sample</kbd><span> when we know that the interpolation of the input variables should not extend beyond the values of those variables at the vertices.</span></div>
<p class="mce-root">The <kbd>sample</kbd> qualifier forces OpenGL to interpolate the shader's input variables to the actual location of the sample itself:</p>
<pre>sample in vec2 TexCoord;</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">This, of course, requires that the fragment shader be executed once for each sample. This will produce the most accurate results, but the performance hit may not be worthwhile, especially if the visual results produced by <kbd>centroid</kbd> (or without the default) are good enough.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter06/scenemsaa.cpp</kbd> file in the example code</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using deferred shading</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Deferred shading</strong> is a technique that involves postponing (or <em>deferring</em>) the lighting/shading step to a second pass. We do this (among other reasons) in order to avoid shading a pixel more than once. The basic idea is as follows:</p>
<ol>
<li>In the first pass, we render the scene, but instead of evaluating the reflection model to determine a fragment color, we simply store all of the geometry information (position, normal, texture coordinate, reflectivity, and so on) in an intermediate set of buffers, collectively called the <strong>g-buffer</strong> (g for geometry).</li>
<li>In the second pass, we simply read from the g-buffer, evaluate the reflection model, and produce a final color for each pixel.</li>
</ol>
<p class="mce-root">When deferred shading is used, we avoid evaluating the reflection model for a fragment that will not end up being visible. For example, consider a pixel located in an area where two polygons overlap. The fragment shader may be executed once for each polygon that covers that pixel; however, the resulting color of only one of the two executions will end up being the final color for that pixel (assuming that blending is not enabled). The cycles spent in evaluating the reflection model for one of the two fragments are effectively wasted. With deferred shading, the evaluation of the reflection model is postponed until all the geometry has been processed, and the visible geometry is known at each pixel location. Hence, the reflection model is evaluated only once for each pixel on the screen. This allows us to do lighting in a more efficient fashion. For example, we could use even hundreds of light sources because we are only evaluating the lighting once per screen pixel.</p>
<p class="mce-root">Deferred shading is fairly simple to understand and work with. It can therefore help with the implementation of complex lighting/reflection models.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">In this recipe, we'll go through a simple example of deferred shading. We'll store the following information in our g-buffer: the position, normal, and diffuse color (the diffuse reflectivity). In the second pass, we'll simply evaluate the diffuse lighting model using the data stored in the g-buffer.</p>
<div class="packt_infobox"><span>This recipe is meant to be a starting point for deferred shading. If we were to use deferred shading in a more substantial (real-world) application, we'd probably need more components in our g-buffer. It should be straightforward to extend this example to use more complex lighting/shading models.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">The g-buffer will contain three textures for storing the position, normal, and diffuse color. There are three uniform variables that correspond to these three textures: <kbd>PositionTex</kbd>, <kbd>NormalTex</kbd>, and <kbd>ColorTex</kbd>; these textures should be assigned to texture units <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd>, respectively. Likewise, the vertex shader assumes that position information is provided in vertex attribute <kbd>0</kbd>, the normal is provided in attribute <kbd>1</kbd>, and the texture coordinate in attribute <kbd>2</kbd>.</p>
<p class="mce-root">The fragment shader has several uniform variables related to light and material properties that must be set from the OpenGL program. Specifically, the structures <kbd>Light</kbd> and <kbd>Material</kbd> apply to the shading model used here.</p>
<p class="mce-root">You'll need a variable named <kbd>deferredFBO</kbd> (type <kbd>GLuint</kbd>) to store the handle to the FBO.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">To create the framebuffer object that contains our g-buffer(s) use the following code:</p>
<pre>void createGBufTex(GLenum texUnit, GLenum format, 
          GLuint &amp;texid ) { 
  glActiveTexture(texUnit); 
  glGenTextures(1, &amp;texid); 
  glBindTexture(GL_TEXTURE_2D, texid); 
  glTexStorage2D(GL_TEXTURE_2D,1,format,width,height); 
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, 
          GL_NEAREST); 
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, 
          GL_NEAREST); 
} 
... 
GLuint depthBuf, posTex, normTex, colorTex; 
 
// Create and bind the FBO 
glGenFramebuffers(1, &amp;deferredFBO); 
glBindFramebuffer(GL_FRAMEBUFFER, deferredFBO); 
 
// The depth buffer 
glGenRenderbuffers(1, &amp;depthBuf); 
glBindRenderbuffer(GL_RENDERBUFFER, depthBuf); 
glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH_COMPONENT, 
           width, height); 
 
// The position, normal and color buffers 
createGBufTex(GL_TEXTURE0, GL_RGB32F, posTex); // Position 
createGBufTex(GL_TEXTURE1, GL_RGB32F, normTex); // Normal 
createGBufTex(GL_TEXTURE2, GL_RGB8, colorTex); // Color 
 
// Attach the images to the framebuffer 
glFramebufferRenderbuffer(GL_FRAMEBUFFER, 
     GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, depthBuf); 
glFramebufferTexture2D(GL_FRAMEBUFFER, 
     GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, posTex, 0); 
glFramebufferTexture2D(GL_FRAMEBUFFER, 
     GL_COLOR_ATTACHMENT1, GL_TEXTURE_2D, normTex, 0); 
glFramebufferTexture2D(GL_FRAMEBUFFER, 
     GL_COLOR_ATTACHMENT2, GL_TEXTURE_2D, colorTex, 0); 
 
GLenumdrawBuffers[] = {GL_NONE, GL_COLOR_ATTACHMENT0, 
     GL_COLOR_ATTACHMENT1,GL_COLOR_ATTACHMENT2}; 
glDrawBuffers(4, drawBuffers); :</pre>
<p>During the first pass, the fragment shader writes to the G-buffers.  In the second pass, it reads from them and applies the shading model.</p>
<pre>in vec3 Position;<br/>in vec3 Normal;<br/>in vec2 TexCoord;<br/><br/>layout (location = 0) out vec4 FragColor;<br/>layout (location = 1) out vec3 PositionData;<br/>layout (location = 2) out vec3 NormalData;<br/>layout (location = 3) out vec3 ColorData;<br/><br/>// The g-buffer textures 
layout(binding = 0) uniform sampler2D PositionTex; 
layout(binding = 1) uniform sampler2D NormalTex; 
layout(binding = 2) uniform sampler2D ColorTex; <br/><br/>uniform int Pass; // Pass number<br/><br/>// Material/light uniforms...
 
vec3 diffuseModel( vec3 pos, vec3 norm, vec3 diff ) { 
 vec3 s = normalize( vec3(Light.Position) - pos);<br/> float sDotN = max( dot(s,norm), 0.0 );<br/> return Light.L * diff * sDotN;
}

void pass1() { 
  // Store position, norm, and diffuse color in g-buffer 
  PositionData = Position; 
  NormalData = Normal; 
  ColorData = Material.Kd; 
} 
 
void pass2() { 
  // Retrieve position, normal and color information from 
  // the g-buffer textures 
  vec3 pos = vec3( texture( PositionTex, TexCoord ) ); 
  vec3 norm = vec3( texture( NormalTex, TexCoord ) ); 
  vec3 diffColor = vec3( texture(ColorTex, TexCoord) ); 
 
  FragColor=vec4(diffuseModel(pos,norm,diffColor), 1.0); 
} 
 
void main() { <br/> if( Pass == 1 ) pass1();<br/> else if( Pass==2 ) pass2();
} </pre>
<p class="mce-root">In the <kbd>render</kbd> function of the OpenGL application, use the following steps for pass #1:</p>
<ol>
<li>Bind to the framebuffer object <kbd>deferredFBO</kbd></li>
<li>Clear the color/depth buffers, set <kbd>Pass</kbd> to <kbd>1</kbd>, and enable the depth test (if necessary)</li>
<li>Render the scene normally</li>
</ol>
<p class="mce-root">Use the following steps for pass #2:</p>
<ol>
<li>Revert to the default FBO (bind to framebuffer 0)</li>
<li>Clear the color buffer, set <kbd>Pass</kbd> to <kbd>2</kbd>, and disable the depth test (if desired)</li>
<li>Render a screen-filling quad (or two triangles) with texture coordinates that range from zero to one in each direction</li>
</ol>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">When setting up the FBO for the g-buffer, we use textures with the internal format <kbd>GL_RGB32F</kbd> for the position and normal components. As we are storing geometry information, rather than simply color information, there is a need to use a higher resolution (that is more bits per pixel). The buffer for the diffuse reflectivity just uses <kbd>GL_RGB8</kbd> since we don't need the extra resolution for these values.</p>
<p class="mce-root">The three textures are then attached to the framebuffer at color attachments <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd> using <kbd>glFramebufferTexture2D</kbd>. They are then connected to the fragment shader's output variables with the call to <kbd>glDrawBuffers</kbd>:</p>
<pre>glDrawBuffers(4, drawBuffers); </pre>
<p class="mce-root">The array <kbd>drawBuffers</kbd> indicates the relationship between the framebuffer's components and the fragment shader's output variable locations. The <em>i</em><sup>th</sup> item in the array corresponds to the <em>i</em><sup>th</sup> output variable location. This call sets color attachments <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd> to output variable locations <kbd>1</kbd>, <kbd>2</kbd>, and <kbd>3</kbd>, respectively. (Note that the fragment shader's corresponding variables are <kbd>PositionData</kbd>, <kbd>NormalData</kbd>, and <kbd>ColorData</kbd>.)</p>
<div class="packt_infobox"><span>During pass 2, it is not strictly necessary to convert and pass through the normal and position, as they will not be used in the fragment shader at all. However, to keep things simple, this optimization is not included. It would be a simple matter to add a subroutine to the vertex shader in order to <em>switch off</em> the conversion during pass 2. (Of course, we need to set </span><kbd>gl_Position</kbd><span> regardless.)</span></div>
<p class="mce-root">In the fragment shader, the functionality depends on the value of the variable <kbd>Pass</kbd>. It will either call <kbd>pass1</kbd> or <kbd>pass2</kbd>, depending on its value. In the <kbd>pass1</kbd> function, we store the values of <kbd>Position</kbd>, <kbd>Normal</kbd>, and <kbd>Material.Kd</kbd> in the appropriate output variables, effectively storing them in the textures that we just talked about.</p>
<p class="mce-root">In the <kbd>pass2</kbd> function, the values of the position, normal, and color are retrieved from the textures, and used to evaluate the diffuse lighting model. The result is then stored in the output variable <kbd>FragColor</kbd>. In this pass, <kbd>FragColor</kbd> should be bound to the default framebuffer, so the results of this pass will appear on the screen.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the graphics community, the relative advantages and disadvantages of deferred shading are a source of some debate. Deferred shading is not ideal for all situations. It depends greatly on the specific requirements of your application, and one needs to carefully evaluate the benefits and drawbacks before deciding whether or not to use deferred shading.</p>
<p class="mce-root">Multi-sample anti-aliasing with deferred shading is possible in recent versions of OpenGL by making use of <kbd>GL_TEXTURE_2D_MULTISAMPLE</kbd>.</p>
<p class="mce-root">Another consideration is that deferred shading can't do blending/transparency very well. In fact, blending is impossible with the basic implementation we saw some time ago. Additional buffers with depth-peeling can help by storing additional layered geometry information in the g-buffer.</p>
<p class="mce-root">One notable advantage of deferred shading is that one can retain the depth information from the first pass and access it as a texture during the shading pass. Having access to the entire depth buffer as a texture can enable algorithms such as depth of field (depth blur), screen space ambient occlusion, volumetric particles, and other similar techniques.</p>
<p class="mce-root">For much more information about deferred shading, refer to Chapter 9 in <em>GPU Gems 2</em> edited by Matt Pharr and Randima Fernando (Addison-Wesley Professional 2005) and Chapter 19 of <em>GPU Gems 3</em> edited by Hubert Nguyen (Addison-Wesley Professional 2007). Both combined, provide an excellent discussion of the benefits and drawbacks of deferred shading, and how to make the decision whether or not to use it in your application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter06/scenedeferred.cpp</kbd> file in the example code</li>
<li>The <em>Rendering to a texture</em> recipe in <a href="a5d0db76-6dbb-470d-8685-42ab8ae077b1.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Using Textures</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Screen space ambient occlusion</h1>
                </header>
            
            <article>
                
<p><strong>Ambient occlusion</strong> is a rendering technique that is based on the assumption that a surface receives uniform illumination from all directions. Some surface positions will receive less light than others due to objects nearby that occlude some of the light. If a surface point has a lot of local geometry nearby, some of this ambient illumination will be blocked causing the point to be darker.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>An example of this is shown in the following image (generated using Blender):  </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/56affbb3-efb5-4155-acfc-31954f85d841.png" style="width:17.75em;height:15.33em;"/></div>
<p>This image is rendered using ambient occlusion only, without light sources. Note how the result looks like shadows in areas that have local geometry occluding the ambient illumination. The result is quite pleasing to the eye and adds a significant amount of realism to an image.</p>
<p>Ambient occlusion is calculated by testing the visibility of a surface point from the upper hemisphere centered at the surface point. Consider the two points <strong>A</strong> and <strong>B</strong> in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/192adb51-45a0-4f2c-9f50-ea51deedf14d.png" style="width:23.00em;height:9.00em;"/></div>
<p>Point <strong>A</strong> is near a corner of the surface and point <strong>B</strong> is located on a flat area. The arrows represent directions for visibility testing. All directions in the hemisphere above point <strong>B</strong> are unoccluded, meaning that the rays do not intersect with any geometry. However, in the hemisphere above point <strong>A</strong>, roughly half of the directions are occluded (arrows with dashed lines). Therefore, <strong>A</strong> should receive less illumination and appear darker than point <strong>B</strong>. </p>
<p>Essentially, ambient occlusion boils down to the following process. Sample as many directions as possible in the upper hemisphere around the surface point. Test each direction for visibility (occlusion). The fraction of rays that are unoccluded gives the ambient occlusion factor at that point.  </p>
<p class="mce-root"/>
<p>This process generally requires a large number of samples to produce acceptable results. To do this for every vertex of a mesh would be impractical in real time for complex scenes. However, the results can be precomputed and stored in a texture for static scenes. If the geometry can move, we need some approximation that is independent of the complexity of the scene.</p>
<p><strong>Screen space ambient occlusion</strong> (or <strong>SSAO</strong>) is the name for a class of algorithms that attempt to approximate ambient occlusion in real time using screen space information. In other words, with SSAO, we compute the ambient occlusion in a post process after the scene has been rendered using the data stored in the depth buffer and/or geometry buffers. SSAO works naturally in conjunction with deferred shading (see the recipe <em>Using deferred shading</em>), but has been implemented with forward (non-deferred) renderers as well.</p>
<p>In this recipe, we'll implement SSAO as part of a deferred rendering process. We'll compute the ambient occlusion factor at each screen-space pixel, rather than on the surface of each object in the scene, ignoring any geometry that is occluded from the camera. After the first pass of a deferred shading renderer, we have position, normal, and color information for the visible surface locations at each screen pixel in our g-buffers (see the <em>Using deferred shading</em> recipe). For each pixel, we'll use the position and normal vector to define the hemisphere above the surface point. Then, we will randomly choose locations (samples) within that hemisphere and test each location for visibility:  </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2f5d01ef-48b5-4df2-944a-7bdaf107dc60.png" style="width:23.17em;height:10.17em;"/></div>
<p>The preceding image represents occlusion testing for the surface point <strong>P</strong>. The filled and hollow circles are random sample points chosen within the hemisphere above <strong>P</strong>, centered along the normal vector. Hollow circles fail the visibility test and the filled circles pass.  </p>
<p><span>To accurately test for visibility, we would need to trace a ray from the surface point toward all sample points and check each ray for intersection with a surface. However, we can avoid that expensive process. Rather than tracing rays, we'll estimate the visibility by defining a point as visible from the surface point in the following way. If the point is visible from the camera, we'll assume that it is also visible from the surface point. This can be inaccurate for some cases, but is a good approximation for a wide variety of typical scenes.  </span></p>
<p><span>In fact, we'll use one additional approximation. We won't trace a ray from the camera to the surface point; instead, we'll just compare the <em>z</em> coordinates of the point being tested and the surface point at the same (x,y) position in camera space. This introduces another small amount of error, but not enough to be objectionable in practice. The following diagram illustrates this concept:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/61ce5241-0d1c-464f-be2c-4a4418e977dd.png" style="width:31.00em;height:33.25em;"/></div>
<p>For each sample that we test within the hemisphere, we find the corresponding location on the camera-visible surface at the same (x,y) position in camera coordinates. This position is simply the value in the position g-buffer at the (x,y) location of the sample. We then compare the <em>z</em> coordinates of the sample and the surface point. If the <em>z</em> coordinate of the surface point is larger than the sample's <em>z</em> coordinate (remember, we're in camera coordinates so all <em>z</em> coordinates will be negative), then we consider the sample to be occluded by the surface.</p>
<p>As the preceding diagram shows, this is an approximation of what a trace of the eye-ray might find. What precedes is a basic overview of the process and there's not much more to it than that. This algorithm boils down to testing a number of random samples in the hemisphere above each point.</p>
<p>The proportion of visible samples is the ambient occlusion factor. Of course, there are a bunch of details to be worked out. Let's start with an overview of the process. We'll implement this algorithm with four passes.</p>
<ol>
<li>The first pass renders the data to the g-buffers: camera space position, normal, and base color.</li>
<li>The second pass computes the ambient occlusion factor for each screen pixel.</li>
<li>The third pass is a simple blur of the ambient occlusion data to remove high frequency artifacts.</li>
<li>The final pass is a lighting pass. The reflection model is evaluated, integrating the ambient occlusion.</li>
</ol>
<p>The last three of these passes employs a screen-space technique, meaning that we invoke the fragment shader once for each pixel on the screen by rendering just a single screen filling quad. The actual geometry for the scene is only rendered during the first pass. The bulk of the interesting stuff here happens in pass 2. There we need to generate a number of random points in the hemisphere above the surface at each point. Random number generation within a shader is challenging for a number of reasons that we won't go into here. So, instead of trying to generate random numbers, we'll pre-generate a set of random points in a hemisphere centered around the <em>z</em> axis. We'll refer to this as our random kernel. We'll re-use this kernel at each point by transforming the points to camera space, aligning the kernel's <em>z</em> axis with the normal vector at the surface point. To squeeze out a bit more randomness, we'll also rotate the kernel around the normal vector by a random amount.  </p>
<p>We'll cover the details in the steps presented in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>First, let's build our random kernel. We need a set of points in the positive-z hemisphere centered at the origin. We'll use a hemisphere with a radius of 1.0 so that we can scale it to any size as needed:  </p>
<pre>int kernSize = 64;<br/>std::vector&lt;float&gt; kern(3 * kernSize);<br/>for (int i = 0; i &lt; kernSize; i++) {<br/> glm::vec3 randDir = rand.uniformHemisphere();<br/> float scale = ((float)(i * i)) / (kernSize * kernSize);<br/> randDir *= glm::mix(0.1f, 1.0f, scale);<br/><br/> kern[i * 3 + 0] = randDir.x;<br/> kern[i * 3 + 1] = randDir.y;<br/> kern[i * 3 + 2] = randDir.z;<br/>}</pre>
<p>The <kbd>uniformHemisphere</kbd> function chooses a random point on the surface of the hemisphere in a uniform fashion. The details of how to do this were covered in an earlier recipe (see <em>Diffuse image based lighting</em>). To get a point within the hemisphere, we scale the point by the variable <kbd>scale</kbd>. This value will vary from 0 to 1 and is non-linear. It will produce more points close to the origin and fewer as we move away from the origin. We do this because we want to give slightly more weight to things that are close to the surface point.</p>
<p>We assign the values of the kernel points to a uniform variable (array) in our shader named <kbd>SampleKernel</kbd>.</p>
<p>As mentioned earlier, we want to re-use this kernel for each surface point, but with a random rotation. To do so, we'll build a small texture containing random rotation vectors. Each vector will be a unit vector in the x-y plane:</p>
<pre>int size = 4;<br/>std::vector&lt;GLfloat&gt; randDirections(3 * size * size);<br/>for (int i = 0; i &lt; size * size; i++) {<br/> glm::vec3 v = rand.uniformCircle();<br/> randDirections[i * 3 + 0] = v.x;<br/> randDirections[i * 3 + 1] = v.y;<br/> randDirections[i * 3 + 2] = v.z;<br/>}<br/>glGenTextures(1, &amp;tex);<br/>glBindTexture(GL_TEXTURE_2D, tex);<br/>glTexStorage2D(GL_TEXTURE_2D, 1, GL_RGB16F, size, size);<br/>glTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, size, size, GL_RGB, GL_FLOAT, randDirections.data());<br/>// ...</pre>
<p>The <kbd>uniformCircle</kbd> function gives a random point on the unit circle in the x-y plane. We're using a 4 x 4 texture here, but you could use a larger size. We'll tile this texture across the screen, and we'll make it available to the shader (uniform variable <kbd>RandTex</kbd>).</p>
<p>You might be thinking that a 4 x 4 texture is too small to give us enough randomness. Yes, it will produce high-frequency patterns, but the blur pass will help to smooth that noise out.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In this example, we'll use a single shader and a single framebuffer. You could, of course, use several if desired. We'll need framebuffer textures for the camera space position, camera space normal, base color, and ambient occlusion. The AO buffer can be a single channel texture (for example, format <kbd>R_16F</kbd>). We'll also need one additional AO texture for the blur pass. We will swap each one into the framebuffer as necessary.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>In the first pass, render the scene to the geometry buffers (see <em>Using deferred shading</em> for details).</li>
<li>In the second pass, we'll use this fragment shader code to compute the AO factor. To do so, we first compute a matrix for converting the kernel points into camera space. When doing so, we use a vector from <kbd>RandTex</kbd> to rotate the kernel. This process is similar to computing the tangent space matrix in normal mapping. For more on this, see <em>Using normal maps:</em></li>
</ol>
<pre style="padding-left: 60px">// Create the random tangent space matrix<br/>vec3 randDir = normalize( texture(RandTex, TexCoord.xy * randScale).xyz );<br/>vec3 n = normalize( texture(NormalTex, TexCoord).xyz );<br/>vec3 biTang = cross( n, randDir );<br/>// If n and randDir are parallel, n is in x-y plane<br/>if( length(biTang) &lt; 0.0001 ) <br/>  biTang = cross( n, vec3(0,0,1));<br/>biTang = normalize(biTang);<br/>vec3 tang = cross(biTang, n);<br/>mat3 toCamSpace = mat3(tang, biTang, n);</pre>
<ol start="3">
<li>Then, we compute the ambient occlusion factor by looping over all of the kernel points, transforming them into camera coordinates, and then finding the surface point at the same (x,y) position and comparing the <em>z</em> values. We write the result to the AO buffer:</li>
</ol>
<pre style="padding-left: 60px">float occlusionSum = 0.0;<br/>vec3 camPos = texture(PositionTex, TexCoord).xyz;<br/>for( int i = 0; i &lt; kernelSize; i++ ) {<br/> vec3 samplePos = camPos + Radius * (toCamSpace * SampleKernel[i]);<br/><br/> // Project point to texture space<br/> vec4 p = ProjectionMatrix * vec4(samplePos,1);<br/> p *= 1.0 / p.w;<br/> p.xyz = p.xyz * 0.5 + 0.5;<br/><br/> // Camera space z-coordinate of surface at the x,y position<br/> float surfaceZ = texture(PositionTex, p.xy).z;<br/> float dz = surfaceZ - camPos.z;<br/>    <br/> // Count points that ARE occluded within the hemisphere<br/> if( dz &gt;= 0.0 &amp;&amp; dz &lt;= Radius &amp;&amp; surfaceZ &gt; samplePos.z ) <br/>  occlusionSum += 1.0;<br/>}<br/><br/>AoData = 1.0 - occlusionSum / kernelSize;</pre>
<ol start="4">
<li>In the third pass, we do a simple blur, using a unweighted average of the nine nearest pixels. We read from the texture that was written in the previous pass, and write the results to our second AO buffer texture:</li>
</ol>
<pre style="padding-left: 60px">ivec2 pix = ivec2( gl_FragCoord.xy );<br/>float sum = 0.0;<br/>for( int x = -1; x &lt;= 1; ++x ) {<br/> for( int y = -1; y &lt;= 1; y++ ) {<br/>  sum += texelFetchOffset( AoTex, pix, 0, ivec2(x,y) ).r;<br/> }<br/>}<br/>AoData = sum / 9.0;</pre>
<ol start="5">
<li>The fourth pass applies the reflection model using the ambient occlusion value from the previous pass. We scale the ambient portion by the value in the AO buffer raised to the fourth power (to slightly exaggerate the effect):</li>
</ol>
<pre style="padding-left: 60px">vec3 pos = texture( PositionTex, TexCoord ).xyz;<br/>vec3 norm = texture( NormalTex, TexCoord ).xyz;<br/>vec3 diffColor = texture(ColorTex, TexCoord).rgb;<br/>float aoVal = texture( AoTex, TexCoord).r;<br/><br/>aoVal = pow(aoVal, 4);<br/>vec3 ambient = Light.La * diff * aoVal;<br/>vec3 s = normalize( vec3(Light.Position) - pos);<br/>float sDotN = max( dot(s,norm), 0.0 );<br/>vec3 col = ambient + Light.L * diff * sDotN;<br/><br/>col = pow(col, vec3(1.0/2.2)); // Gamma<br/><br/>FragColor = vec4(col, 1.0);</pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In the second pass, we compute the ambient occlusion factor. To do so, the first step is to find the matrix that converts our kernel into camera space. We want a matrix that converts the kernel's <em>z</em> axis to the normal vector at the surface point, and applies a random rotation using a vector from <kbd>RandTex</kbd>. The columns of the matrix are the three ortho-normal vectors that define the tangent coordinate system in screen space. Since we want the kernel's <em>z</em> axis to be transformed to the normal vector, the third of these three vectors is the normal vector itself. The other two (<kbd>tang</kbd> and <kbd>biTang</kbd>) are determined by using cross products. To find <kbd>biTang</kbd>, we take the cross product of the normal vector (<kbd>n</kbd>) and the random rotation vector retrieved from the texture (<kbd>randDir</kbd>). As long as the two are not parallel, this will give us a vector that is perpendicular to both <kbd>n</kbd> and <kbd>randDir</kbd>. However, there is a small possibility that the two might be parallel. If so, the normal vector is in the x-y plane of camera space (because all of the rotation vectors in the texture are in the x-y plane). So in this case, we compute <kbd>biTang</kbd> by taking the cross product of <kbd>n</kbd> and the <em>z</em> axis. Next, we normalize <kbd>biTang</kbd>.</p>
<div class="packt_infobox">Note that we scale the texture coordinates when accessing the random texture to get the random rotation vector. We do this because the texture is smaller than the size of the screen and we want to tile it to fill the screen so that a texel matches the size of a screen pixel.</div>
<p>Now that we have two ortho-normal vectors, we can compute the third with the cross product of the two. The three vectors <kbd>tang</kbd>, <kbd>biTang</kbd>, and <kbd>n</kbd> make up the axes of the tangent space coordinate system. The matrix that converts from the tangent system to camera space (<kbd>toCamSpace</kbd>) has these three vectors as its columns.</p>
<p>Now that we have the <kbd>toCamSpace</kbd> matrix, we can loop over the 64 kernel points and test each of them. Note that we're not actually treating these as points. Instead, they are treated as vectors that define an offset from the surface point. So a sample point is determined by the following line:</p>
<pre>vec3 samplePos = camPos + Radius * (toCamSpace * SampleKernel[i]);</pre>
<p>Here, we take a vector from the sample kernel, convert it to camera space, scale it by <kbd>Radius</kbd>, and add it to the position of the surface point. The scale factor (<kbd>Radius</kbd>) is an important term that defines the size of the hemisphere around the point. It is a camera space value and may need to be adjusted for different scenes. In the example code, a value of <kbd>0.55</kbd> is used.  </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The next step is to find the visible surface at the sample point's (x,y) position. To do so, we need to look up the value of the position in the g-buffer at the sample position. We need the texture coordinates that correspond to that position. To find that, we first project the point to clip space, divide by the homogeneous <em>w</em> coordinate, and scale/translate to the texture space. Using that value, we then access the position g-buffer and retrieve the <em>z</em> coordinate of the surface position at that location (<kbd>surfaceZ</kbd> ). We don't need the <em>x</em> and <em>y</em> coordinates here as they are the same as the sample's.</p>
<p>Now that we have the <em>z</em> coordinate of the projected point on the surface near the sample (<kbd>sampleZ</kbd>), we compute the difference (<kbd>dz</kbd>) between it and the <em>z</em> coordinate of the <em>original</em> surface point (the point being shaded, <kbd>pos</kbd>). If this value is less than zero or greater than <kbd>Radius</kbd>, then we know that the projected point on the surface at the sample location is outside of the hemisphere. In that case, we assume the sample is unoccluded. If that is not the case, we assume the projected point is within the hemisphere and we compare the <em>z</em> values. If <kbd>surfaceZ</kbd> is greater than <kbd>samplePos.z</kbd>, we know that the sample point is behind the surface. </p>
<div class="packt_infobox">This may seem strange, but remember, we're working in camera coordinates here. All of the <em>z</em> coordinates will be negative. These are not depth values—they are the camera space <em>z</em> coordinates.</div>
<p>We add <kbd>1.0</kbd> to <kbd>occlusionSum</kbd> if we determine that the point is occluded. The final result in <kbd>occlusionSum</kbd> after the loop will be the total number of points that were occluded. Since we want the opposite—the fraction of points that <em>are not</em> occluded—we subtract one from the average before writing to the output variable <kbd>AoData</kbd>.</p>
<p>The following image (on the left) shows the results of this pass. Note that if you look closely, you can see some high frequency grid-like artifacts due to the re-use of the random rotation vectors throughout the image. This is smoothed out by the blur pass (right-hand image):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8a235afc-2b5d-45da-b3b2-c18f929d7894.png" style="width:35.42em;height:10.50em;"/></div>
<p>The third pass is just a simple average of the nine texels near each texel.  </p>
<p>The fourth pass applies the reflection model. In this example, we just compute the diffuse and ambient components of the Blinn-Phong model, scaling the ambient term by the blurred ambient occlusion value (<kbd>aoVal</kbd>). In this example, it is raised to a power of <kbd>4.0</kbd> to make it a bit darker and increase the effect.</p>
<p>The following images show the scene rendered without ambient occlusion (on the left) and with ambient occlusion (on the right). The ambient term is increased substantially to demonstrate the effect:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/62902435-c18e-4df6-9939-08daeb89d21b.png" style="width:38.50em;height:11.50em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter06/scenessao.cpp</kbd> file in the example code</li>
<li><em>The Using deferred shading</em> recipe in this chapter</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the depth test</h1>
                </header>
            
            <article>
                
<p class="mce-root">GLSL 4 provides the ability to configure how the depth test is performed. This gives us additional control over how and when fragments are tested against the depth buffer.</p>
<p class="mce-root">Many OpenGL implementations automatically provide an optimization known as the early depth test or early fragment test. With this optimization, the depth test is performed before the fragment shader is executed. Since fragments that fail the depth test will not appear on the screen (or the framebuffer), there is no point in executing the fragment shader at all for those fragments and we can save some time by avoiding the execution.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">The OpenGL specification, however, states that the depth test must appear to be performed<span> </span><em>after</em><span> </span>the fragment shader. This means that if an implementation wishes to use the early depth test optimization, it must be careful. The implementation must make sure that if anything within the fragment shader might change the results of the depth test, then it should avoid using the early depth test.</p>
<p class="mce-root">For example, a fragment shader can change the depth of a fragment by writing to the output variable,<span> </span><kbd>gl_FragDepth</kbd>. If it does so, then the early depth test cannot be performed because, of course, the final depth of the fragment is not known prior to the execution of the fragment shader. However, the GLSL provides ways to notify the pipeline roughly how the depth will be modified, so that the implementation may determine when it might be okay to use the early depth test.</p>
<p class="mce-root">Another possibility is that the fragment shader might conditionally discard the fragment using the<span> </span><kbd>discard</kbd><span> </span>keyword. If there is any possibility that the fragment may be discarded, some implementations may not perform the early depth test.</p>
<p class="mce-root">There are also certain situations where we want to rely on the early depth test. For example, if the fragment shader writes to memory other than the framebuffer (with image load/store, shader storage buffers, or other incoherent memory writing), we might not want the fragment shader to execute for fragments that fail the depth test. This would help us to avoid writing data for fragments that fail. The GLSL provides a technique for forcing the early depth test optimization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">To ask the OpenGL pipeline to always perform the early depth test optimization, use the following layout qualifier in your fragment shader:</p>
<pre>layout(early_fragment_tests) in; </pre>
<p class="mce-root">If your fragment shader will modify the fragment's depth, but you still would like to take advantage of the early depth test when possible, use the following layout qualifier in a declaration of<span> </span><kbd>gl_FragDepth</kbd><span> </span>within your fragment shader:</p>
<pre>layout (depth_*) out float gl_FragDepth; </pre>
<p class="mce-root">In this,<span> </span><kbd>depth_*</kbd><span> </span>is one of the following:<span> </span><kbd>depth_any</kbd>,<span> </span><kbd>depth_greater</kbd>,<span> </span><kbd>depth_less</kbd>, or<span> </span><kbd>depth_unchanged</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The following statement forces the OpenGL implementation to always perform the early depth test:</p>
<pre>layout(early_fragment_tests) in; </pre>
<p class="mce-root">We must keep in mind that if we attempt to modify the depth anywhere within the shader by writing to<span> </span><kbd>gl_FragDepth</kbd>, the value that is written will be ignored.</p>
<p class="mce-root">If your fragment shader needs to modify the depth value, then we can't force early fragment tests. However, we can help the pipeline to determine when it can still apply the early test. We do so by using one of the layout qualifiers for<span> </span><kbd>gl_FragDepth</kbd><span> </span>as shown before. This places some limits on how the value will be modified. The OpenGL implementation can then determine if the fragment shader can be skipped. If it can be determined that the depth will not be changed in such a way that it would cause the result of the test to change, the implementation can still use the optimization.</p>
<p class="mce-root">The layout qualifier for the output variable<span> </span><kbd>gl_FragDepth</kbd><span> </span>tells the OpenGL implementation specifically how the depth might change within the fragment shader. The qualifier<span> </span><kbd>depth_any</kbd><span> </span>indicates that it could change in any way. This is the default.</p>
<p class="mce-root">The other qualifiers describe how the value may change with respect to<span> </span><kbd>gl_FragCoord.z</kbd>:</p>
<ul>
<li><kbd>depth_greater</kbd>: This fragment shader promises to only increase the depth.</li>
<li><kbd>depth_less</kbd>: This fragment shader promises to only decrease the depth.</li>
<li><kbd>depth_unchanged</kbd>: This fragment shader promises not to change the depth. If it writes to<span> </span><kbd>gl_FragDepth</kbd>, the value will be equal to<span> </span><kbd>gl_FragCoord.z</kbd>.</li>
</ul>
<p class="mce-root">If you use one of these qualifiers, but then go on to modify the depth in an incompatible way, the results are undefined. For example, if you declare<span> </span><kbd>gl_FragDepth</kbd><span> </span>with<span> </span><kbd>depth_greater</kbd>, but decrease the depth of the fragment, the code will compile and execute, but you shouldn't expect to see accurate results.</p>
<div class="packt_infobox"><span>If your fragment shader writes to </span><kbd>gl_FragDepth</kbd><span>, then it must be sure to write a value in all circumstances. In other words, it must write a value no matter which branches are taken within the code.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The<span> </span><em>Implementing order-independent transparency</em><span> </span>recipe</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing order-independent transparency</h1>
                </header>
            
            <article>
                
<p class="mce-root">Transparency can be a difficult effect to do accurately in pipeline architectures like OpenGL. The general technique is to draw opaque objects first, with the depth buffer enabled, then to make the depth buffer read-only (using <kbd>glDepthMask</kbd>), disable the depth test, and draw the transparent geometry. However, care must be taken to ensure that the transparent geometry is drawn from <em>back to front</em>. That is, objects farther from the viewer should be drawn before the objects that are closer. This requires some sort of depth-sorting to take place prior to rendering.</p>
<p class="mce-root">The following images show an example of a block of small, semi-transparent spheres with some semi-transparent cubes placed evenly within them. On the right-hand side, the objects are rendered in an arbitrary order, using standard OpenGL blending. The result looks incorrect because objects are blended in an improper order. The cubes, which were drawn last, appear to be on top of the spheres, and the spheres look jumbled, especially in the middle of the block. On the left, the scene is drawn using proper ordering, so objects appear to be oriented correctly with respect to depth, and the overall look is more realistic looking:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e3e8ea27-c9d9-4059-8f83-5a1c7bdadbbb.png" style="width:31.75em;height:15.92em;"/></div>
<p class="mce-root"><strong>Order Independent Transparency</strong> (<strong>OIT</strong>) means that we can draw objects in any order and still get accurate results. Depth sorting is done at some other level, perhaps within the fragment shader, so that the programmer need not sort objects before rendering. There are a variety of techniques for doing this; one of the most common technique is to keep a list of colors for each pixel, sort them by depth, and then blend them together in the fragment shader. In this recipe we'll use this technique to implement OIT, making use of some of the newest features in OpenGL 4.3.</p>
<p class="mce-root"><strong>Shader storage buffer objects</strong> (<strong>SSBO</strong>) and <strong>image load/store</strong> are some of the newest features in OpenGL, introduced in 4.3 and 4.2, respectively. They allow arbitrary read/write access to data from within a shader. Prior to this, shaders were very limited in terms of what data they could access. They could read from a variety of locations (textures, uniforms, and so on), but writing was very limited. Shaders could only write to controlled, isolated locations such as fragment shader outputs and transform feedback buffers. This was for a very good reason. Since shaders can execute in parallel and in a seemingly arbitrary order, it is very difficult to ensure that data is consistent between instantiations of a shader. Data written by one shader instance might not be visible to another shader instance whether or not that instance is executed after the other. Despite this, there are good reasons for wanting to read and write to shared locations. With the advent of SSBOs and image load/store, that capability is now available to us. We can create buffers and textures (called images) with read/write access to any shader instance. This is especially important for compute shaders, the subject of <a href="d67e01c8-8212-4d49-937f-6b1c62a57744.xhtml"><span class="ChapterrefPACKT">Chapter 11</span></a>, <em>Using Compute Shaders</em>. However, this power comes at a price. The programmer must now be very careful to avoid the types of memory consistency errors that come along with writing to memory that is shared among parallel threads. Additionally, the programmer must be aware of the performance issues that come with synchronization between shader invocations.</p>
<div class="packt_infobox"><span>For a more thorough discussion of the issues involved with memory consistency and shaders, refer to </span>Chapter 11<span>, of <em>The OpenGL Programming Guide</em>, 8th Edition. That chapter also includes another similar implementation of OIT.</span></div>
<p class="mce-root">In this recipe, we'll use SSBOs and image load/store to implement order-independent transparency. We'll use two passes. In the first pass, we'll render the scene geometry and store a linked list of fragments for each pixel. After the first pass, each pixel will have a corresponding linked list containing all fragments that were written to that pixel, including their depth and color. In the second pass, we'll draw a fullscreen quad to invoke the fragment shader for each pixel. In the fragment shader, we'll extract the linked list for the pixel, sort the fragments by depth (largest to smallest), and blend the colors in that order. The final color will then be sent to the output device.</p>
<p class="mce-root"/>
<p class="mce-root">That's the basic idea, so let's dig into the details. We'll need three memory objects that are shared among the fragment shader instances:</p>
<ol>
<li><strong>An atomic counter:</strong> This is just an unsigned integer that we'll use to keep track of<br/>
the size of our linked list buffer. Think of this as the index of the first unused slot in the buffer.</li>
<li><strong>A head-pointer texture that corresponds to the size of the screen</strong>: The texture will store a single unsigned integer in each texel. The value is the index of the head of the linked list for the corresponding pixel.</li>
<li><strong>A buffer containing all of our linked lists:</strong> Each item in the buffer will correspond to a fragment, and contains a struct with the color and depth of the fragment as well as an integer, which is the index of the next fragment in the linked list.</li>
</ol>
<p class="mce-root">In order to understand how all of this works together, let's consider a simple example. Suppose that our screen is three pixels wide and three pixels high. We'll have a head pointer texture that is the same dimensions, and we'll initialize all of the texels to a special value that indicates the end of the linked list (an empty list). In the following diagram, that value is shown as an <strong>x</strong>, but in practice, we'll use <kbd>0xffffffff</kbd>. The initial value of the counter is zero, and the linked list buffer is allocated to a certain size but treated as empty initially. The initial state of our memory is shown in the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/f8dd60ca-4750-4864-8662-e9f409b4c153.png" style="width:27.33em;height:14.83em;"/></div>
<p class="mce-root">Now suppose that a fragment is rendered at the position (0,1) with a depth of 0.75. The fragment shader will take the following steps:</p>
<ol>
<li>Increment the atomic counter. The new value will be 1, but we'll use the previous value (<strong>0</strong>) as the index for our new node in the linked list.</li>
<li>Update the head pointer texture at (0,1) with the previous value of the counter (<strong>0</strong>). This is the index of the new head of the linked list at that pixel. Hold on to the previous value that was stored there (<strong>x</strong>), as we'll need that in the next step.</li>
</ol>
<ol start="3">
<li>Add a new value into the linked list buffer at the location corresponding to the previous value of the counter (<strong>0</strong>). Store the color of the fragment and its depth here. Store in the next component the previous value of the head pointer texture at (0,1) that we held on to in step 2. In this case, it is the special value indicating the end of the list.</li>
</ol>
<p class="mce-root">After processing this fragment, the memory layout looks as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b62cb1aa-bced-41f8-b460-6fad6245100e.png" style="width:36.17em;height:19.67em;"/></div>
<p class="mce-root">Now, suppose another fragment is rendered at (0,1), with a depth of 0.5. The fragment shader will execute the same steps as the previous ones, resulting in the following memory layout:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d36be185-a17f-4146-838d-1981ed3f55f4.png" style="width:33.50em;height:18.25em;"/></div>
<p class="mce-root">We now have a two-element linked list starting at index 1 and ending at index 0. Suppose, now that we have three more fragments in the following order: a fragment at (1,1) with a depth of 0.2, a fragment at (0,1) with a depth of 0.3, and a fragment at (1,1) with a depth of 0.4. Following the same steps for each fragment, we get the following result:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/269f6ef8-f08d-4037-9c41-a8ad6989a9e9.png" style="width:35.33em;height:18.83em;"/></div>
<p class="mce-root"/>
<p class="mce-root">The linked list at (0,1) consists of fragments {3, 1, 0} and the linked list at (1,1) contains fragments {4, 2}.</p>
<p class="mce-root">Now, we must keep in mind that due to the highly parallel nature of GPUs, fragments can be rendered in virtually any order. For example, fragments from two different polygons might proceed through the pipeline in the opposite order as to when the draw instructions for polygons were issued. As a programmer, we must not expect any specific ordering of fragments. Indeed, instructions from separate instances of the fragment shader may interleave in arbitrary ways. The only thing that we can be sure of is that the statements within a particular instance of the shader will execute in order. Therefore, we need to convince ourselves that any interleaving of the previous three steps will still result in a consistent state. For example, suppose instance one executes steps 1 and 2, then another instance (another fragment, perhaps at the same fragment coordinates) executes steps 1, 2, and 3, before the first instance executes step 3. Will the result still be consistent? I think you can convince yourself that it will be, even though the linked list will be broken for a short time during the process. Try working through other interleavings and convince yourself that we're OK.</p>
<p class="mce-root"/>
<div class="packt_infobox"><span>Not only can statements within separate instances of a shader interleave with each other, but the sub-instructions that make up the statements can interleave. (For example, the sub-instructions for an increment operation consist of a load, increment, and a store.) What's more, they could actually execute at exactly the same time. Consequently, if we aren't careful, nasty memory consistency issues can crop up. To help avoid this, we need to make careful use of the GLSL support for atomic operations.</span></div>
<p class="mce-root">Recent versions of OpenGL (4.2 and 4.3) have introduced the tools that we need to make this algorithm possible. OpenGL 4.2 introduced atomic counters and the ability to read and write to arbitrary locations within a texture (called image load/store). OpenGL 4.3 introduced shader storage buffer objects. We'll make use of all three of these features in this example, as well as the various atomic operations and memory barriers that go along with them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">There's a bunch of setup needed here, so we'll go into a bit of detail with some code segments. First, we'll set up a buffer for our atomic counter:</p>
<pre>GLuint counterBuffer;  
glGenBuffers(1, &amp;counterBuffer); 
glBindBufferBase(GL_ATOMIC_COUNTER_BUFFER, 0, counterBuffer); 
glBufferData(GL_ATOMIC_COUNTER_BUFFER, sizeof(GLuint), NULL, 
       GL_DYNAMIC_DRAW); </pre>
<p class="mce-root">Next, we will create a buffer for our linked list storage:</p>
<pre>GLuint llBuf; 
glGenBuffers(1, &amp;llBuf); 
glBindBufferBase(GL_SHADER_STORAGE_BUFFER, 0, llBuf); 
glBufferData(GL_SHADER_STORAGE_BUFFER, maxNodes * nodeSize, NULL, 
       GL_DYNAMIC_DRAW); </pre>
<div class="packt_infobox"><kbd>nodeSize</kbd><span> in the previous code is the size of</span> <kbd>struct NodeType</kbd><span> used in the fragment shader (in the latter part of the code). This is computed based on the </span><kbd>std430</kbd><span> layout. For details on the </span><kbd>std430</kbd><span> layout, see the OpenGL specification document. For this example, </span><kbd>nodeSize</kbd><span> is </span><kbd>5 * sizeof(GLfloat) + sizeof(GLuint)</kbd><span>.</span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">We also need to create a texture to hold the list head pointers. We'll use 32-bit unsigned integers, and bind it to image unit 0:</p>
<pre>glGenTextures(1, &amp;headPtrTex); 
glBindTexture(GL_TEXTURE_2D, headPtrTex); 
glTexStorage2D(GL_TEXTURE_2D, 1, GL_R32UI, width, height); 
glBindImageTexture(0, headPtrTex, 0, GL_FALSE, 0, GL_READ_WRITE, 
          GL_R32UI); </pre>
<p class="mce-root">After we render each frame, we need to clear the texture by setting all texels to a value of <kbd>0xffffffff</kbd>. To help with that, we'll create a buffer of the same size as the texture, with each value set to our clear value:</p>
<pre>vector&lt;GLuint&gt; headPtrClear(width * height, 0xffffffff); 
GLuint clearBuf; 
glGenBuffers(1, &amp;clearBuf); 
glBindBuffer(GL_PIXEL_UNPACK_BUFFER, clearBuf); 
glBufferData(GL_PIXEL_UNPACK_BUFFER, 
       headPtrClear.size()*sizeof(GLuint), 
       &amp;headPtrClear[0], GL_STATIC_COPY); </pre>
<p class="mce-root">That's all the buffers we'll need. Note the fact that we've bound the head pointer texture to image unit 0, the atomic counter buffer to index 0 of the <kbd>GL_ATOMIC_COUNTER_BUFFER</kbd> binding point (<kbd>glBindBufferBase</kbd>), and the linked list storage buffer to index 0 of the <kbd>GL_SHADER_STORAGE_BUFFER</kbd> binding point. We'll refer back to that later. Use a pass-through vertex shader that sends the position and normal along in eye coordinates.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">With all of the buffers set up, we need two render passes. Before the first pass, we want to clear our buffers to default values (that is, empty lists), and to reset our atomic counter buffer to zero:</p>
<pre>glBindBuffer(GL_PIXEL_UNPACK_BUFFER, clearBuf); 
glBindTexture(GL_TEXTURE_2D, headPtrTex); 
glTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, width, height, 
        GL_RED_INTEGER, GL_UNSIGNED_INT, NULL); 
GLuint zero = 0; 
glBindBufferBase(GL_ATOMIC_COUNTER_BUFFER, 0, counterBuffer); 
glBufferSubData(GL_ATOMIC_COUNTER_BUFFER, sizeof(GLuint), &amp;zero);</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mce-root">In the first pass, we'll render the full scene geometry. Generally, we should render all the opaque geometry first and store the results in a texture. However, we'll skip that step for this example to keep things simple and focused. Instead, we'll render only transparent geometry. When rendering the transparent geometry, we need to make sure to put the depth buffer in read-only mode (use <kbd>glDepthMask</kbd>). In the fragment shader, we add each fragment to the appropriate linked list:</p>
<pre>layout (early_fragment_tests) in; 
 
#define MAX_FRAGMENTS 75 
 
in vec3 Position; 
in vec3 Normal; 
 
struct NodeType { 
 vec4 color; 
 float depth; 
 uint next; 
}; 
 
layout(binding=0, r32ui) uniform uimage2D headPointers; 
layout(binding=0, offset=0) uniform atomic_uint 
                     nextNodeCounter; 
layout(binding=0, std430) buffer linkedLists { 
 NodeType nodes[]; 
}; 
uniform uint MaxNodes; 
 
subroutine void RenderPassType(); 
subroutine uniform RenderPassType RenderPass; 
 
... 
 
subroutine(RenderPassType) 
void pass1() 
{ 
 // Get the index of the next empty slot in the buffer 
 uint nodeIdx = atomicCounterIncrement(nextNodeCounter); 
 
 // Is there space left in the buffer? 
 if( nodeIdx &lt; MaxNodes ) { 
  // Update the head pointer image 
  uint prevHead = imageAtomicExchange(headPointers, 
              ivec2(gl_FragCoord.xy), nodeIdx); 
 
  // Set the color and depth of this new node to the color 
  // and depth of the fragment. The next pointer points to the 
  // previous head of the list. 
  nodes[nodeIdx].color = vec4(shadeFragment(), Kd.a); 
  nodes[nodeIdx].depth = gl_FragCoord.z; 
  nodes[nodeIdx].next = prevHead; 
 } 
} </pre>
<p class="mce-root">Before rendering the second pass, we need to be sure that all of the data has been written to our buffers. In order to ensure that is indeed the case, we can use a memory barrier:</p>
<pre>glMemoryBarrier( GL_ALL_BARRIER_BITS ); </pre>
<p class="mce-root">In the second pass, we don't render the scene geometry, just a single, screen-filling quad in order to invoke the fragment shader for each screen pixel. In the fragment shader, we start by copying the linked list for the fragment into a temporary array:</p>
<pre>struct NodeType frags[MAX_FRAGMENTS]; 
int count = 0; 
 
// Get the index of the head of the list 
uint n = imageLoad(headPointers, ivec2(gl_FragCoord.xy)).r; 
 
// Copy the linked list for this fragment into an array 
while( n != 0xffffffff &amp;&amp; count &lt; MAX_FRAGMENTS) { 
 frags[count] = nodes[n]; 
 n = frags[count].next; 
 count++; 
} </pre>
<p class="mce-root">Then, we sort the fragments using insertion sort:</p>
<pre>// Sort the array by depth (largest to smallest). 
for( uint i = 1; i &lt; count; i++ ) 
{ 
 struct NodeType toInsert = frags[i]; 
 uint j = i; 
 while( j &gt; 0 &amp;&amp; toInsert.depth &gt; frags[j-1].depth ) { 
  frags[j] = frags[j-1]; 
  j--; 
 } 
 frags[j] = toInsert; 
}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root">Finally, we blend the fragments manually, and send the result to the output variable:</p>
<pre>// Traverse the array, and blend the colors. 
vec4 color = vec4(0.5, 0.5, 0.5, 1.0); // Background color 
for( int i = 0; i &lt; count; i++ ) { 
 color = mix( color, frags[i].color, frags[i].color.a); 
} 
  
// Output the final color 
FragColor = color; </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">To clear our buffers, prior to the first pass, we bind <kbd>clearBuf</kbd> to the <kbd>GL_PIXEL_UNPACK_BUFFER</kbd> binding point, and call <kbd>glTexSubImage2D</kbd> to copy data from <kbd>clearBuf</kbd> to the the head pointer texture. Note that when a non-zero buffer is bound to <kbd>GL_PIXEL_UNPACK_BUFFER</kbd>, <kbd>glTexSubImage2D</kbd> treats the last parameter as an offset into the buffer that is bound there. Therefore, this will initiate a copy from <kbd>clearBuf</kbd> into <kbd>headPtrTex</kbd>. Clearing the atomic counter is straightforward, but the use of <kbd>glBindBufferBase</kbd> may be a bit confusing. If there can be several buffers bound to the binding point (at different indices), how does <kbd>glBufferSubData</kbd> know which buffer to target? It turns out that when we bind a buffer using <kbd>glBindBufferBase</kbd>, it is also bound to the generic binding point as well.</p>
<p class="mce-root">In the fragment shader during the first pass, we start with the layout specification enabling the early fragment test optimization:</p>
<pre>layout (early_fragment_tests) in; </pre>
<p class="mce-root">This is important because if any fragments are obscured by the opaque geometry, we don't want to add them to a linked list. If the early fragment test optimization is not enabled, the fragment shader may be executed for fragments that will fail the depth test and hence will get added to the linked list. The previous statement ensures that the fragment shader will not execute for those fragments.</p>
<p class="mce-root">The definition of <kbd>struct NodeType</kbd> specifies the type of data that is stored in our linked list buffer. We need to store color, depth, and a pointer to the next node in the linked list.</p>
<p class="mce-root">The next three statements declare the objects related to our linked list storage.</p>
<ol>
<li class="mce-root">The first, <kbd>headPointers</kbd>, is the image object that stores the locations of the heads of each linked list. The layout qualifier indicates that it is located at image unit 0 (refer to the <em>Getting ready</em> section of this recipe), and the data type is <kbd>r32ui</kbd> (red, 32-bit unsigned integer).</li>
</ol>
<ol start="2">
<li class="mce-root">The second object is our atomic counter <kbd>nextNodeCounter</kbd>. The layout qualifier indicates the index within the <kbd>GL_ATOMIC_COUTER_BUFFER</kbd> binding point (refer to the <em>Getting ready</em> section of this recipe) and the offset within the buffer at that location.  Since we only have a single value in the buffer, the offset is 0, but in general, you might have several atomic counters located within a single buffer.</li>
<li class="mce-root">Third is our linked-list storage buffer <kbd>linkedLists</kbd><span>. This is a shader storage buffer object. The organization of the data within the object is defined within the curly braces here. In this case, we just have an array of</span> <kbd>NodeType</kbd> <span>structures. The bounds of the array can be left undefined, the size being limited by the underlying buffer object that we created. The layout qualifiers define the binding and memory layout. The first, binding, indicates that the buffer is located at index 0 within the</span> <kbd>GL_SHADER_STORAGE_BUFFER</kbd> <span>binding point. The second,</span> <kbd>std430</kbd><span>, indicates how memory is organized within the buffer. This is mainly important when we want to read the data back from the OpenGL side. As mentioned previously, this is documented in the OpenGL specification document.</span></li>
</ol>
<p class="mce-root">The first step in the fragment shader during the first pass is to increment our atomic counter using <kbd>atomicCounterIncrement</kbd>. This will increment the counter in such a way that there is no possibility of memory consistency issues if another shader instance is attempting to increment the counter at the same time.</p>
<div class="packt_infobox"><span>An atomic operation is one that is isolated from other threads and can be considered to be a single, uninterruptable operation. Other threads cannot interleave with an atomic operation. It is always a good idea to use atomic operations when writing to shared data within a shader.</span></div>
<p class="mce-root">The return value of <kbd>atomicCounterIncrement</kbd> is the previous value of the counter. It is the next unused location in our linked list buffer. We'll use this value as the location where we'll store this fragment, so we store it in a variable named <kbd>nodeIdx</kbd>. It will also become the new head of the linked list, so the next step is to update the value in the <kbd>headPointers</kbd> image at this pixel's location <kbd>gl_FragCoord.xy</kbd>. We do so using another atomic operation: <kbd>imageAtomicExchange</kbd>. This replaces the value within the image at the location specified by the second parameter with the value of the third parameter. The return value is the previous value of the image at that location. This is the previous head of our linked list. We hold on to this value in <kbd>prevHead</kbd>, because we want to link our new head to that node, thereby restoring the consistency of the linked list with our new node at the head.</p>
<p class="mce-root">Finally, we update the node at <kbd>nodeIdx</kbd> with the color and depth of the fragment, and set the <kbd>next</kbd> value to the previous head of the list (<kbd>prevHead</kbd>). This completes the insertion of this fragment into the linked list at the head of the list.</p>
<p class="mce-root"/>
<p class="mce-root">After the first pass is complete, we need to make sure that all changes are written to our shader storage buffer and image object before proceeding. The only way to guarantee this is to use a memory barrier. The call to <kbd>glMemoryBarrier</kbd> will take care of this for us. The parameter to <kbd>glMemoryBarrier</kbd> is the type of barrier. We can fine tune the type of barrier to specifically target the kind of data that we want to read. However, just to be safe, and for simplicity, we'll use <kbd>GL_ALL_BARRIER_BITS</kbd>, which ensures that all possible data has been written.</p>
<p class="mce-root">In the second pass, we start by copying the linked list for the fragment into a temporary array. We start by getting the location of the head of the list from the <kbd>headPointers</kbd> image using <kbd>imageLoad</kbd>. Then we traverse the linked list with the <kbd>while</kbd> loop, copying the data into the <kbd>array</kbd> frags.</p>
<p class="mce-root">Next, we sort the array by depth from largest to smallest, using the insertion sort algorithm. Insertion sort works well on small arrays, so should be a fairly efficient choice here.</p>
<p class="mce-root">Finally, we combine all the fragments in order, using the <kbd>mix</kbd> function to blend them together based on the value of the alpha channel. The final result is stored in the output variable <kbd>FragColor</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">As mentioned previously, we've skipped anything that deals with opaque geometry. In general, one would probably want to render any opaque geometry first, with the depth buffer enabled, and store the rendered fragments in a texture. Then, when rendering the transparent geometry, one would disable writing to the depth buffer, and build the linked list as shown previously. Finally, you could use the value of the opaque texture as the background color when blending the linked lists.</p>
<p class="mce-root">This is the first example in this book that makes use of reading and writing from/to arbitrary (shared) storage from a shader. This capability, has given us much more flexibility, but that comes at a price. As indicated previously, we have to be very careful to avoid memory consistency and coherence issues. The tools to do so include atomic operations and memory barriers, and this example has just scratched the surface. There's much more to come in <a href="d67e01c8-8212-4d49-937f-6b1c62a57744.xhtml"><span class="ChapterrefPACKT">Chapter 11</span></a>, <em>Using Compute Shaders</em> when we look at compute shaders, and I recommend you read through the memory chapter in the <em>OpenGL Programming Guide</em> for much more detail than is provided here.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <kbd>chapter06/sceneoit.cpp</kbd> file in the example code</li>
<li><a href="d67e01c8-8212-4d49-937f-6b1c62a57744.xhtml">Chapter 11</a>, <em>Using Compute Shaders</em></li>
<li><em>OpenGL Development Cookbook</em> by Muhammad Mobeen Movania has several recipes in Chapter 6, <em>GPU-Based Alpha Blending and Global Illumination</em></li>
</ul>


            </article>

            
        </section>
    </body></html>