<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Understanding RTOS Tasks</h1>
                </header>
            
            <article>
                
<p class="mce-root">The super loop programming paradigm is typically one of the first programming <span>methods that </span>an embedded systems engineer will encounter. A program implemented with a super loop has a single top-level loop that cycles through the various functions the system needs to perform. These simple <kbd>while</kbd> loops are easy to create and understand (when they are small). In FreeRTOS, tasks are very similar to super loops – the main difference is that the system can have more than one task, but only one super loop. </p>
<p class="mce-root">In this chapter, we will take a closer look at super loops and different ways of achieving a degree of parallelism with them. After that, a comparison between super loops and tasks will be made and a theoretical way of thinking about task execution will be introduced. Finally, we'll take a look at how tasks are <em>actually</em> executed with an RTOS kernel and compare two basic scheduling algorithms.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Introducing super loop programming</li>
<li>Achieving parallel operations with super loops</li>
<li>Comparing RTOS tasks to super loops </li>
<li>Achieving parallel operations with RTOS tasks</li>
<li><span>RTOS tasks versus super loops – p</span>ros and cons</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>There are no software or hardware requirements for this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing super loop programming</h1>
                </header>
            
            <article>
                
<p>There is one common property that all embedded systems share – they don't have an exit point. Because of its nature, embedded code is generally expected to always be available – silently running in the background, taking care of housekeeping tasks, and ready for user input at any time. Unlike desktop environments that are meant to start and stop programs, there isn't anything for a micro-controller to do if it exits the <kbd>main()</kbd> function. If this happens, it is likely that the entire device has stopped functioning. For this reason the <kbd>main()</kbd> function in an embedded system never returns. Unlike application programs, which are started and stopped by their host OS, most embedded MCU-based applications start at power on and end abruptly when the system is powered off. Because of this abrupt shutdown, embedded applications typically don't have any of the shutdown tasks normally associated with applications, such as freeing memory and resources.</p>
<p>The following code represents the basic idea of a super loop. Take a look at this before moving on to the more detailed explanations:</p>
<pre>void main ( void )<br/>{<br/>    while(1)<br/>    {<br/>        func1();<br/>        func2();<br/>        func3();<br/>        //do useful stuff, but don't return<br/>        //(otherwise, where would we go. . what would we do. . .?!)<br/>    }<br/>}</pre>
<p>While extremely simple, the preceding code has a number of features worth pointing out. The <kbd>while</kbd> loop never returns – it goes on forever executing the same three functions (this is intended). The three innocent-looking function calls can hide some nasty surprises in a real-time system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The basic super loop</h1>
                </header>
            
            <article>
                
<p>This main loop that never returns is generally referred to as a <em>super loop</em>. It's always fun to think <em>super</em> because it has control over most things in the system – nothing gets done in the following diagram unless the super loop makes it happen. This type of setup is perfect for very simple systems that need to perform just a few tasks that don't take a considerable amount of time. Basic super loop structures are extremely easy to write and understand; if the problem you're trying to solve can be done with a simple super loop, then use a simple super loop. Here is the execution flow of the code presented previously – each function is called sequentially and the loop never exits: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1694 image-border" src="assets/cc1de34f-1936-496b-bdf2-84cbb1d49776.png" style="width:50.92em;height:22.50em;"/></p>
<p>Now, let's have a look at what this execution looks like in a real-time system and some of the drawbacks associated with this approach.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Super loops in real-time systems</h1>
                </header>
            
            <article>
                
<p class="mce-root">When simple super loops are operating quickly (usually because they have limited functionality/responsibility), they are quite responsive. However, the simplicity of the super loop can be a blessing and a curse. Since each function always follows the preceding function, they are always called in the same sequence and fully dependent on one another. Any delay introduced by one function propagates to the next function, which causes the total amount of time it takes to execute that iteration of the loop to increase (as seen in the following diagram). If <kbd>func1</kbd> takes 10 us to execute one time through the loop, and then 100 ms the next, <kbd>func2</kbd> isn't going to be called nearly as quickly the second time through the loop as it was the first time through:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1103 image-border" src="assets/50b20b51-d2ed-46c5-9d06-858b44634a4e.png" style="width:50.00em;height:34.83em;"/></p>
<p>Let's take a look at this in a little bit more depth. In the preceding diagram, <kbd>func3</kbd> is responsible for checking the state of a flag representing an external event (this event signals a rising edge of a signal). The frequency of how often <kbd>func3</kbd> checks the flag is dependent on how long <kbd>func1</kbd> and <kbd>func2</kbd> take to execute. A well designed and responsive super loop will typically execute very rapidly, checking for events more often than they occur (callout B). When an external event does occur, the loop doesn't detect the event until the next time <kbd>func3</kbd> executes (callouts A, C, and D). Notice that there is a delay between when the event is generated and when it is detected by <kbd>func3</kbd>. Also note that the delay isn't always consistent: this difference in time is referred to as jitter.</p>
<div class="packt_infobox">In many super loop-based systems, the execution speed of the super loop is extremely high compared to slowly occurring events being polled. We don't have enough room on the page to show a loop executing hundreds (or thousands) of iterations between detecting an event!</div>
<p><span>If a system has a known maximum amount of jitter when responding to an event, it is considered to be deterministic. That is, it will reliably respond to an event within the specified amount of time after that event occurs. A high level of determinism is crucial for time-critical components in a real-time system because, without it, the system could fail to respond to important events in a timely manner.</span></p>
<p><span>Consider the case of a loop checking a hardware flag repeatedly for an event (this is referred to as polling). The tighter the loop, the faster the flag is checked – when the flag is checked often, the code will be more responsive to the event of interest. If we have an event that needs to be acted upon in a timely manner, we could just write a really tight loop and wait for the important event to occur. This approach works – but <em>only</em> if that event is the only thing of interest for the system. If the <em>only</em> responsibility the entire system has is watching for that event (no background I/O, communication, and so on), then this is a valid approach. This type of situation rarely occurs in today's complex real-world systems. Poor responsiveness is the limitation of solely polled-based systems. Next up, we'll take a look at how to get a bit more parallelism in our super loop.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Achieving parallel operations with super loops</h1>
                </header>
            
            <article>
                
<p>Even though a basic super loop can only step through functions sequentially, there are still ways to achieve parallelism. MCUs have a few different types of specialized hardware designed to take some of the burden away from the CPU, while still enabling a highly responsive system. This section will introduce those systems and how they can be used within the context of a super loop style program.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing interrupts </h1>
                </header>
            
            <article>
                
<p>Polling for a single event is not only wasteful in terms of CPU cycles and power – it also results in a system that isn't responsive to anything else, which should generally be avoided. So then, how can we get a single core processor to do things in parallel? Well, we can't – there's only one processor after all. . . but since our processor is likely to be running millions of instructions per second, it is possible to get it to perform things that are close enough to parallel. MCUs also include dedicated hardware for generating interrupts. Interrupts provide signals to the MCU that allow it to <span><span>jump directly </span></span>to an <strong>interrupt service routine</strong> (<strong>ISR</strong>) when the event occurs. This is such a critical piece of functionality that ARM Cortex-M cores provide a standardized peripheral for it, called the <strong>nested vector in</strong><strong>terrupt controller</strong> (<strong>NVIC</strong>). The NVIC provides a common way of dealing with interrupts. The <em>nested</em> portion of this term signifies that even interrupts can be interrupted by other interrupts with a higher priority. This is quite convenient since it allows us to minimize the amount of latency and jitter for the most time-critical pieces of the system.</p>
<p>So, how do interrupts fit into a super loop in a way that better achieves the illusion of parallel activity? The code inside an ISR is generally kept as short as possible, in order to minimize the amount of time spent in the interrupt. This is important for a few reasons. If the interrupt occurs very often and the ISR contains a <em>lot</em> of instructions, there is a chance that the ISR won't return before being called again. For communication peripherals such as UART or SPI, this will mean dropped data (which obviously isn't desirable). Another reason to keep the code short is because other interrupts also need to be serviced, which is why it's a good idea to push off any responsibility to the code that isn't running inside an ISR context.</p>
<p>To quickly get an idea of how ISRs contribute to jitter, let's take a look at a simple example of an external <strong>analog to digital converter</strong> (<strong>ADC</strong>) signaling to an MCU that a reading has been taken and the conversion is ready to be transferred to the MCU (refer to the hardware diagram shown here):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1104 image-border" src="assets/ddf5caac-58f0-435e-9286-9efceea5f8e0.png" style="width:47.33em;height:27.67em;"/></p>
<p>In the ADC hardware, a pin is dedicated to signaling that a reading of an analog value has been converted to a digital representation and is ready for transfer to the MCU. The MCU would then initiate a transfer over the communication medium (COM in the diagram).</p>
<p>Next, let's have a look at how the ISR calls might stack up against one another over time, relative to the rising edge on the conversion ready line. The following diagram shows six different instances of ISR being called in response to a rising edge of a signal. The small amount of time between when the rising edge occurs in the hardware versus when the ISR in firmware is invoked is the minimum latency. The jitter in the response of the ISR is the difference in the latency over many different cycles:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1105 image-border" src="assets/2fe1639a-87e9-4bc4-9a6c-0506b974a644.png" style="width:42.17em;height:43.42em;"/></p>
<p class="CDPAlignLeft CDPAlign">There are different ways to minimize latency and jitter for critical ISRs. In ARM Cortex-M-based MCUs, interrupt priorities are flexible – a single interrupt source can be assigned different priorities at runtime. The ability to reprioritize interrupts is one way of making sure the most important parts of a system get the CPU when they need it.</p>
<p class="CDPAlignLeft CDPAlign">As mentioned before, it is important to keep the amount of code executing in interrupts as short as possible, since code that is inside an ISR will take precedence over any code that is not in an ISR (for example <kbd>main()</kbd>). Additionally, lower priority ISRs won't be executed until all of the code in a higher priority ISR has been executed and the ISR exits – which is why it is important to keep ISRs short. It is always a good idea to try and limit how much <em>responsibility</em> (and therefore code) an ISR has.</p>
<div class="packt_infobox CDPAlignLeft CDPAlign">When multiple interrupts are nested, they don't fully return – there's actually a really useful feature of ARM Cortex M processors called interrupt-tail chaining. If the processor detects that an interrupt is about to exit, but another one is pending, the next ISR will be executed without the processor totally restoring the pre-interrupt state, which further reduces latency.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interrupts and super loops</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">One way of achieving minimal instructions and responsibility in the ISR is to do the smallest amount of work possible inside the ISR and then set a flag that is checked by code running in the super loop. This way, the interrupt can be serviced as soon as possible, without the entire system being dedicated to waiting on the event. In the following diagram, notice how the interrupt is being generated multiple times before finally being dealt with by <kbd>func3</kbd>. </p>
<p class="CDPAlignLeft CDPAlign"><span>Depending on what exactly that interrupt is trying to achieve, it will typically take a value from the associated peripheral and push it into an array (or take a value from an array and feed it to the peripheral registers). In the case of our external ADC, the ISR (triggered each time the ADC performs a conversion) would go out to the ADC, transfer the digitized reading, and store it in RAM, setting a flag indicating that one or more values are ready for processing. This allows for the interrupt to be serviced multiple times without involving the higher-level code:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1106 image-border" src="assets/b0bf6dd0-d0a7-4e8b-9636-245b27bc1f85.png" style="width:35.50em;height:30.58em;"/></p>
<p class="CDPAlignLeft CDPAlign">In the case of a communication peripheral that is transmitting large blocks of data, an array can be used as a queue for storing items to be transmitted. At the end of the entire transmission, a flag can be set to notify the main loop of the completion. There are many examples of situations where queuing values are appropriate. For instance, if some processing is required to be performed on a block of data, it is often advantageous to collect the data first and then process the entire block together outside of the interrupt. An interrupt-driven approach isn't the only way to achieve this blocked-data approach. In the next section, we'll take a look at a piece of hardware that can make moving large blocks of data both easier for the programmer, and more efficient for the processor.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing DMA </h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">Remember the assertion that the processor couldn't <em>really </em>do things truly in parallel? This is still true. <em>However </em>. . . modern MCUs contain more than just a processing core. While our processing core is chugging along dealing with instructions, there are many other hardware subsystems hard at work inside the MCU. One of these hard working subsystems is called a <strong>Direct Memory Access Controller</strong> (<strong>DMA</strong>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1695 image-border" src="assets/8a6eab36-865d-4565-8043-b8eb88bad6bd.png" style="width:36.25em;height:37.08em;"/></p>
<p class="CDPAlignLeft CDPAlign">The preceding diagram presents a very simplified hardware block diagram that shows a view of two different data paths available from RAM to a UART peripheral. </p>
<p class="CDPAlignLeft CDPAlign">In the case of receiving a stream of bytes from a UART without DMA, information from the UART will move into the UART registers, be read by the CPU, and then pushed out to RAM for storage:</p>
<ol>
<li> The CPU must detect when an individual byte (or word) has been received, either by polling the UART register flags, or by setting up an interrupt service routine that will be fired when a byte is ready. </li>
<li>After the byte is transferred from the UART, the CPU can then place it into RAM for further processing.</li>
<li>Steps 1 and 2 are repeated until the entire message is received.</li>
</ol>
<p class="CDPAlignLeft CDPAlign">When DMA is used in the same scenario, the following happens:</p>
<ol>
<li> The CPU configures the DMA controller and peripheral for the transfer.</li>
<li>The DMA controller takes care of ALL transfers between the UART peripheral and RAM. This requires no intervention from the CPU.</li>
<li>The CPU will be notified when the entire transfer is complete and it can go directly to processing the entire byte stream.</li>
</ol>
<p class="mce-root CDPAlignLeft CDPAlign">Most programmers find DMA to be nearly magical if they're accustomed to dealing with super loops and ISRs. The controller is configured to transfer a block of memory to the peripheral, as the peripheral needs it, and then provide a notification (typically via an interrupt) when the transfer is complete – that's it!</p>
<p class="mce-root CDPAlignLeft CDPAlign">This convenience does come at a price, of course. It does take some time to set up the DMA transfer initially, so for small transfers, it might actually take more CPU time to set up the transfer than if an interrupt or polled method was used. </p>
<p class="mce-root CDPAlignLeft CDPAlign">There are also some caveats to be aware of: each MCU has specific limitations, so be sure to read the details of the datasheet, reference manual, and errata before counting on the availability of DMA for a critical design component of the system:</p>
<ul>
<li class="mce-root">The bandwidth of the MCU's internal buses limits the number of bandwidth-hungry peripherals that can be reliably placed on a single bus. </li>
<li class="mce-root">Occasionally, limited availability of mapped DMA channels to peripherals also complicates the design process. </li>
</ul>
<p class="CDPAlignLeft CDPAlign">These types of reasons are why it is important to get all team members involved with the early-stage design of embedded systems, rather than just <em>throwing it over the wall</em>.</p>
<p class="CDPAlignLeft CDPAlign">DMA is great for accessing a large number of peripherals efficiently, giving us the ability to add more and more functionality to the system. However, as we start adding more and more modules of code to the super loop, inter-dependencies between subsystems become more complex as well. In the next section, we'll discuss the challenges of scaling a super loop for complex systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling a super loop</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign"><span>So, we've now got a responsive system that is able to reliably process interrupts. Perhaps we've configured a DMA controller to take care of the heavy lifting for the communication peripherals as well. Why do we even need an RTOS? Well, it is entirely possible you don't! If the system is dealing with a limited number of responsibilities and none of them are especially complicated or time-consuming, then there may be no need for anything more sophisticated than a super loop.</span></p>
<p class="CDPAlignLeft CDPAlign"><span>However, if the system is also responsible for generating a <strong>User Interface</strong> (<strong>UI</strong>), running complex time-consuming algorithms, or dealing with complex communication stacks, it is very likely that these tasks will take a non-trivial amount of time. If a glitzy eye-catching UI with lots of animation starts to stutter a little bit because the MCU is dealing with collecting data from a critical sensor, that is no big deal. Either the animation can be dialed back or eliminated and the important part of the real-time system is left intact. But what happens if that animation still looks perfectly good, even though there was some missed data from the sensor?</span></p>
<p class="CDPAlignLeft CDPAlign"><span>There are all sorts of different ways in which this problem plays out every day in our industry. Sometimes, if the system was designed well enough, the missing data will be detected and flagged (but it can't be recovered: it is gone forever). If the design team is really lucky, it may even have failed in this way during in-house testing. However, in many cases, the missed sensor data will go completely unnoticed until somebody notices one of the readings seems to be a little bit off ... sometimes. If everyone is lucky, the bug report for the sketchy reading might include a hint that it only seems to happen when someone is at the front panel (playing with those fancy animations). This would at least give the poor firmware engineer assigned to debug the issue a hint – but we're often not even that lucky.</span></p>
<p class="CDPAlignLeft CDPAlign">These are the types of systems where an RTOS is needed. Guaranteeing that the most time-critical tasks are always running when necessary and scheduling lower priority tasks to run whenever spare time is available is a strong point of preemptive schedulers. In this type of setup, the critical sensor readings could be pushed into their own task and assigned a high priority – effectively interrupting anything else in the system (except ISRs) when it was time to deal with the sensor. That complex communication stack could be assigned a lower priority than the critical sensor. Finally, the glitzy UI with the fancy animations gets the left-over processor cycles. It is free to perform as many sliding alpha-blending animations as it wants, but only when the processor doesn't have anything else better to do.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing RTOS tasks to super loops</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">So far, we've only mentioned tasks very casually, but what is a task, really? An easy way to think about a task is that it is <em>just another main loop</em>. In a preemptive RTOS, there are two main differences between tasks and super loops:</p>
<ul>
<li>Each task receives its own private stack. Unlike a super loop in main, which was sharing the system stack, tasks receive their own stack that no other task in the system will use. This allows each task to have its own call stack without interfering with other tasks. </li>
<li>Each task has a priority assigned to it. This priority allows the scheduler to make decisions on which task should be running (the goal is to make sure the highest priority task in the system is always doing useful work).</li>
</ul>
<p class="CDPAlignLeft CDPAlign">Given these two features, each task may be programmed as if it is the only thing the processor has to do. Do you have a single flag you'd like to watch AND some calculations for flashy animations to churn through? No problem: simply program the task and assign it a reasonable priority, relative to the rest of the system's functionality. The preemptive scheduler will always ensure that the most important task is executing when it has work to do. When a higher priority task no longer has useful work to perform and it is waiting on something else in the system, a lower priority task will be switched into context and allowed to run.</p>
<div class="packt_infobox CDPAlignLeft CDPAlign">The FreeRTOS scheduler will be discussed in more detail in <a href="2fa909fe-91a6-48c1-8802-8aa767100b8f.xhtml">Chapter 7</a>, <em>The FreeRTOS Scheduler</em>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Achieving parallel operations with RTOS tasks</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">Earlier, we had looked at a super loop that was looping through three functions. Now, for a very simple example, let's move each one of the three functions into its own task. We'll use these three simple tasks to examine the following:</p>
<ul>
<li><strong>Theoretical task programming model</strong>: How the three tasks can be described <span>theoretically</span></li>
<li><strong>Actual round-robin scheduling</strong>: What the tasks look like when executed using a round-robin scheduling algorithm</li>
<li><strong>Actual preemptive scheduling</strong>: What the tasks look like when executed using preemptive scheduling</li>
</ul>
<div class="packt_infobox CDPAlignLeft CDPAlign">In real-world programs, there is almost never a single function per task; we're only using this as an analog to the overly simplistic super loop from earlier.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Theoretical task programming model</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">Here's some pseudo-code that uses a super loop to execute three functions. The same three functions are also included in a task-based system – each RTOS task (on the right) contains the same functionality as the functions from the super loop on the left. This will be used moving forward as we discuss the differences in how the code is executed when using a super loop versus using a task-driven approach with a scheduler:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1108 image-border" src="assets/ad3dfae0-42ea-4a9f-ad2c-2ca7d38fc3f7.png" style="width:30.17em;height:30.25em;"/></p>
<p class="CDPAlignLeft CDPAlign">One of the immediate differences you might notice between the super loop implementation and the RTOS implementation is the number of infinite <kbd>while</kbd> loops. There is only a single infinite <kbd>while</kbd> loop (in <kbd>main()</kbd>) for the super loop implementation, but each task has its own infinite <kbd>while</kbd> loop.</p>
<p class="CDPAlignLeft CDPAlign"><span>In the super loop, the three functions being executed by a super loop are each run to completion before the next function is called, and then the cycle continues onto the next iteration (illustrated by the following diagram):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1109 image-border" src="assets/3bb60f4a-fad2-4597-abae-6ca87409c6d4.png" style="width:50.33em;height:13.25em;"/></p>
<p class="CDPAlignLeft CDPAlign"><span>In the RTOS implementation, e</span>ach task is essentially its own little infinite <kbd>while</kbd> loop. Whereas functions in the super loop were always sequentially called one after the other (orchestrated by the logic in the super loop), tasks can simply be thought of as all executing in parallel after the scheduler has been started. Here's a diagram of an RTOS executing three tasks:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1110 image-border" src="assets/7a6161d5-f95f-4367-999f-6aabbcb9e90a.png" style="width:50.25em;height:34.67em;"/></p>
<p class="CDPAlignLeft CDPAlign">In the diagram, you'll notice that the size of each <kbd>while</kbd> loop is not the same. This is one of the many benefits of using a scheduler that is executing the tasks in <em>parallel</em> versus a super loop – the programmer doesn't need to be immediately concerned with the length of the longest executing loop slowing down the other tighter loops. The diagram depicts <kbd>Task 2</kbd> having a much longer loop than <kbd>Task 1</kbd>. In a super loop system, this would cause the functionality in <kbd>func1</kbd> to execute less frequently (since the super loop would need to execute <kbd>func1</kbd>, then <kbd>func2</kbd>, and then <kbd>func3</kbd>). <span><span>In a task-based programming model, this isn't the case – the loop of each task can be thought of as being isolated from the other tasks in the system – and they all run in parallel.</span></span></p>
<p class="CDPAlignLeft CDPAlign">This isolation and perceived parallel execution are some of the benefits of using an RTOS; it alleviates some of the complexity for the programmer. So – that's the easiest way of conceptualizing tasks – they're simply independent infinite <kbd>while</kbd> loops that all execute in parallel . . . in theory. In reality, things aren't quite this simple. In the next two sections, we'll take a glimpse into what goes on behind the scenes to make it <em>seem</em> like tasks are executing in parallel.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Round-robin scheduling</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">One of the easiest ways to conceptualize actual task execution is with round-robin scheduling. In round-robin scheduling, each task gets a small slice of time to use the processor, which is controlled by the scheduler. As long as the task has work to perform, it will execute. As far as the task is concerned, it has the processor entirely to itself. The scheduler takes care of all of the complexity of switching in the appropriate context for the next task:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1848 image-border" src="assets/bf10b772-0a6c-4bef-b257-43d452369a0a.png" style="width:64.50em;height:19.08em;"/></p>
<p class="CDPAlignLeft CDPAlign">This is the same three tasks that were shown previously, except that instead of a theoretical conceptualization, each iteration through the tasks' loops are enumerated over time. Because the round-robin scheduler assigns equal time slices to each task, the shortest task (<kbd>Task 1</kbd>) has executed nearly six iterations of its loop, whereas the task with the slowest loop (<kbd>Task 2</kbd>) has only made it through the first iteration. <kbd>Task 3</kbd> has executed three iterations of its loop. </p>
<p class="CDPAlignLeft CDPAlign">An extremely important distinction between a super loop executing the same functions versus a round-robin scheduling routine executing them is this: <kbd>Task 3</kbd> completed its moderately tight loop before <kbd>Task 2</kbd>. When the super loop was running functions in a serial fashion, <kbd>Function 3</kbd> wouldn't even have started until <kbd>Function 2</kbd> had run to completion. So, while the scheduler isn't providing us with true parallelism, each task is getting it's <em>fair share</em> of CPU cycles. So, with this scheduling scheme, if a task has a shorter loop, it will execute more often than a task with a longer loop.</p>
<p class="CDPAlignLeft CDPAlign">All of this switching does come at a (slight) cost – the scheduler needs to be invoked any time there is a context switch. In this example, the tasks are not explicitly calling the scheduler to run. In the case of FreeRTOS running on an ARM Cortex-M, the scheduler will be called from the SysTick interrupt (more details can be found in <a href="2fa909fe-91a6-48c1-8802-8aa767100b8f.xhtml">Chapter 7</a><em>, The FreeRTOS Scheduler</em>). A considerable amount of effort is put into making sure the scheduler kernel is extremely efficient and takes as little time to run as possible. However, the fact remains that it will run at some point and consume CPU cycles. On most systems, the small amount of overhead is generally not noticed (or significant), but it can become an issue in some systems. For example, if a design is on the extreme edge of feasibility because it has extremely tight timing requirements and very few spare CPU cycles, the added overhead may not be desirable (or completely necessary) if the super loop/interrupt approach has been carefully characterized and optimized. However, it is best to avoid this type of situation wherever possible, since the likelihood of overlooking a combination of interrupt stack-up (or nested conditionals taking longer <em>every once in a while) </em>and causing the system to miss a deadline is extremely high on even moderately complex systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preemptive-based scheduling</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">Preemptive scheduling provides a mechanism for ensuring that the system is always performing its most important task. A preemptive scheduling algorithm will give priority to the most important task, regardless of what else in the system is happening – except for interrupts, since they occur <em>underneath</em> the scheduler and always have a higher priority. This sounds very straightforward – and it is – except that there are some details that need to be taken into consideration.</p>
<p class="CDPAlignLeft CDPAlign">Let's take a look at the same three tasks. These three tasks all have the same functionality: a simple <kbd>while</kbd> loop that endlessly increments a volatile variable.</p>
<p class="CDPAlignLeft CDPAlign">Now, consider the following three scenarios to figure out which of the three tasks will get context. The following diagram has the same tasks as previously presented with round-robin scheduling. Each of the three tasks has more than enough work to do, which will prevent the task from going out of context:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1849 image-border" src="assets/f9f432c6-63c5-4991-8e1b-b43ebe54be13.png" style="width:64.25em;height:39.58em;"/></p>
<p><span>So, what happens when three different tasks are set up with three different sets of priorities (A, B, and C)? </span></p>
<ul>
<li><span><strong>A (top left)</strong>: <kbd>Task 1</kbd> has the highest priority in the system – it gets <em>all</em> of the processor time! Regardless of how many iterations <kbd>Task 1</kbd> performs, if it is the highest priority task in the system and it has work to do (without waiting on anything else in the system), it will be given context and run.</span></li>
</ul>
<ul>
<li><strong>B (top right)</strong>: <kbd>Task 2</kbd> is <span>the highest priority task in the system. Since it has more than enough work to do, not needing to wait on anything else in the system, <kbd>Task 2</kbd> will be given context. Since <kbd>Task 2</kbd> is configured as the highest priority in the system, it will execute until it needs to wait on something else in the system. </span></li>
<li><strong>C (bottom left)</strong>: <span><kbd>Task 3</kbd> is configured as the highest priority task in the system. No other tasks run because they are lower priority. </span></li>
</ul>
<p class="CDPAlignLeft CDPAlign"><span>Now, obviously, if you were actually designing a system that required multiple tasks to run in parallel, a preemptive scheduler wouldn't be much use if all of the tasks in the system required 100% CPU time and didn't need to wait on anything. This setup also wouldn't be a great design for a real-time system since it was completely overloaded (and ignoring two of the three primary functions the system was meant to perform)! The situation presented is referred to as <strong>task starvation</strong>, since only the highest priority task in the system is getting CPU time and the other tasks are being <em>starved</em> of processor time.</span></p>
<p class="CDPAlignLeft CDPAlign"><span>Another detail worth pointing out is that the scheduler is still running at predetermined intervals. No matter what is going on in the system, the scheduler will diligently run at its predetermined tick rate.</span></p>
<div class="packt_infobox CDPAlignLeft CDPAlign"><span>There is an exception to this. FreeRTOS has a <em>tick-less</em> scheduler mode designed for use in extremely low power devices, which prevents the scheduler from running on the same predetermined intervals.</span></div>
<p><span>A more realistic use case where a preemptive scheduler is used is shown here:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1850 image-border" src="assets/be80720d-75c8-4919-81b4-ffa20e63c7d6.png" style="width:42.83em;height:29.08em;"/></p>
<p><span>In this case, <kbd>Task 1</kbd> is the highest priority task in the system (it also happens to finish executing very quickly) – the only time <kbd>Task 1</kbd> has context taken from it is when the scheduler needs to run; otherwise, it will keep context until it doesn't have any additional work to perform.</span></p>
<p class="CDPAlignLeft CDPAlign"><kbd>Task 2</kbd> is the next highest priority – you'll also notice that this task is set up to execute once per RTOS scheduler tick (indicated by the downward arrows). <kbd>Task 3</kbd> is the lowest priority task in the system: it only gets context when there is nothing else worth doing in the system. There are three main points worth looking at in this diagram:</p>
<ul>
<li><strong>A</strong>: <kbd>Task 2</kbd> has context. Even though it is interrupted by the scheduler, it immediately gets context again after the scheduler has run (because it still has work to perform).</li>
<li><strong>B</strong>: <kbd>Task 2</kbd> has finished its work for iteration 0. The scheduler has run and determined that (since no other tasks in the system are required to run) <kbd>Task 3</kbd> could have processor time.</li>
</ul>
<ul>
<li><strong>C</strong>: <kbd>Task 2</kbd> has started running iteration 4, but <kbd>Task 1</kbd> now has some work to do – even though <kbd>Task 2</kbd> hasn't finished the work for that iteration. <kbd>Task 1</kbd> is immediately switched in by the scheduler to perform its higher priority work. After <kbd>Task 1</kbd> is finished with what it needs to do, <kbd>Task 2</kbd> is switched back in to finish iteration 4. This time, the iteration runs until the next tick and <kbd>Task 2</kbd> runs again (iteration 5). After <kbd>Task 2</kbd> iteration 5 has completed, there is no higher priority work to perform, so the lowest priority task in the system (<kbd>Task 3</kbd>) runs again. It looks as if <kbd>Task 3</kbd> has finally completed iteration 0, so it moves on to iteration 1 and chugs along . . .</li>
</ul>
<p class="CDPAlignLeft CDPAlign">Hopefully you're still with me! If not, that's OK, given that this is a very abstract example. The key takeaway is that the highest priority task in the system takes precedence. </p>
<div class="mce-root packt_infobox CDPAlignLeft CDPAlign">This is only a brief introduction to the relevant scheduling concepts covered in detail in <a href="2fa909fe-91a6-48c1-8802-8aa767100b8f.xhtml">Chapter 7</a>,<em> The FreeRTOS Scheduler</em>, to put the concept of tasks into context, showing the different ways in which they can be run and scheduled. Many more details and strategies for achieving desired system performance are discussed there, along with real-world examples.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RTOS tasks versus super loops – pros and cons</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">Super loops are great for simple systems with limited responsibilities. If a system is simple enough, they can provide very low jitter in response to an event, but only if the loop is tight enough. As a system grows more complex and acquires more responsibility, polling rates decrease. This decreased polling rate causes much larger jitter in response to events. Interrupts can be introduced into the system to combat the increased jitter. As a super loop-based system becomes more complex, it becomes harder to track and guarantee responsiveness to events. </p>
<p class="CDPAlignLeft CDPAlign">An RTOS becomes very valuable with more complex systems that have not only time-consuming tasks, but also require good responsiveness to external events. With an RTOS, an increase in system complexity, ROM, RAM, and initial setup time is the trade-off for a more easily understood system, which can more easily guarantee responsiveness to external events in a timely manner.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign"><span>We've covered quite a few concepts in this chapter in relation to super loops and tasks. At this point, you should have a good understanding of how super loops can be combined with interrupts and DMA to provide parallel processing to keep a system responsive, without the use of an RTOS. We introduced task-based architectures at a theoretical level and the two main types of scheduling you'll encounter when using FreeRTOS (round-robin and preemptive). You also had a very brief glimpse at how a preemptive scheduler schedules tasks of different priorities. All of these concepts are important to grasp, so feel free to refer back to these simplistic examples as we move forward and discuss more advanced topics.</span></p>
<p class="CDPAlignLeft CDPAlign"><span>In the next chapter, you'll be introduced to the various inter-task communication mechanisms that will cause context switches like the ones covered in this chapter. As we progress through the book and move onto interrupt and task communication mechanisms, many real-world examples will be discussed and we'll take a deep dive into the code that you'll need to write in order to create reliable real-time systems.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions </h1>
                </header>
            
            <article>
                
<p>As we conclude, here is a list of questions for you to test your knowledge regarding this chapter's material. You will find the answers in the <em>Assessments</em> section of the Appendix:</p>
<ol>
<li>What is a super loop?
<ul>
<li>An infinite <kbd>while</kbd> loop</li>
<li>A loop that oversees all function calls in an embedded system</li>
<li>Both of the preceding options</li>
</ul>
</li>
<li><span>RTOS tasks should </span><em>always<span> </span></em><span>be preferred over super loops.</span>
<ul>
<li><span>True</span></li>
<li>False</li>
</ul>
</li>
<li>Name a drawback to complex super loops.</li>
<li><span>How can the responsiveness of a super loop-based application be improved?</span></li>
<li>List two ways in which super loops differ from RTOS tasks.</li>
</ol>
<ol start="6">
<li>What features do RTOS tasks possess to help ensure that the most time-critical task gets CPU time before less time-critical tasks?
<ul>
<li>Time slicing</li>
<li>Prioritization</li>
<li>Round-robin scheduling</li>
</ul>
</li>
<li>What type of scheduler attempts to execute the most critical tasks before less critical tasks?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">If interrupts and DMA are new to you, here are two resources that describe their use (relative to MCUs) fairly well:</p>
<ul>
<li class="CDPAlignLeft CDPAlign">For interrupts: <a href="https://www.renesas.com/eu/en/support/technical-resources/engineer-school/mcu-programming-peripherals-04-interrupts.html">https://www.renesas.com/eu/en/support/technical-resources/engineer-school/mcu-programming-peripherals-04-interrupts.html</a></li>
<li>STM application note AN4031 – Using DMA on the STM32F7: <a href="https://www.st.com/content/ccc/resource/technical/document/application_note/27/46/7c/ea/2d/91/40/a9/DM00046011.pdf/files/DM00046011.pdf/jcr:content/translations/en.DM00046011.pdf">https://www.st.com/content/ccc/resource/technical/document/application_note/27/46/7c/ea/2d/91/40/a9/DM00046011.pdf/files/DM00046011.pdf/jcr:content/translations/en.DM00046011.pdf</a></li>
</ul>


            </article>

            
        </section>
    </body></html>