# Chapter 05

# Stack and Heap

In the previous chapter, we ran an investigation of the memory layout of a running process. System programming without knowing enough about the memory structure and its various segments is like doing surgery without knowing the anatomy of the human body. The previous chapter just gave us the basic information regarding the different segments in the process memory layout, but this chapter wants us to just focus on the most frequently used segments: Stack and Heap.

As a programmer, you are mostly busy working with Stack and Heap segments. Other segments such as Data or BSS are less in use, or you have less control over them. That's basically because of the fact that the Data and BSS segments are generated by the compiler, and usually, they take up a small percentage of the whole memory of a process during its lifetime. This doesn't mean that they are not important, and, in fact, there are issues that directly relate to these segments. But as you are spending most of your time with Stack and Heap, most memory issues have roots in these segments.

As part of this chapter, you will learn:

*   How to probe the Stack segment and the tools you need for this purpose
*   How memory management is done automatically for the Stack segment
*   The various characteristics of the stack segment
*   The guidelines and best practices on how to use the Stack segment
*   How to probe the Heap segment
*   How to allocate and deallocate a Heap memory block
*   The guidelines and best practices regarding the usage of the Heap segment
*   Memory-constrained environments and tuning memory in performant environments

Let's begin our quest by discussing the Stack segment in more detail.

# Stack

A process can continue working without the Heap segment but not without the Stack segment. This says a lot. The Stack is the main part of the process metabolism, and it cannot continue execution without it. The reason is hiding behind the mechanism driving the function calls. As briefly explained in the previous chapter, calling a function can only be done by using the Stack segment. Without a Stack segment, no function call can be made, and this means no execution at all.

With that said, the Stack segment and its contents are engineered carefully to result in the healthy execution of the process. Therefore, messing with the Stack content can disrupt the execution and halt the process. Allocation from the Stack segment is fast, and it doesn't need any special function call. More than that, the deallocation and all memory management tasks happen automatically. All these facts are all very tempting and encourage you to overuse the Stack.

You should be careful about this. Using the Stack segment brings its own complications. The stack is not very big, therefore you cannot store large objects in it. In addition, incorrect use of the Stack content can halt the execution and result in a crash. The following piece of code demonstrates this:

```cpp
#include <string.h>
int main(int argc, char** argv) {
  char str[10];
  strcpy(str, "akjsdhkhqiueryo34928739r27yeiwuyfiusdciuti7twe79ye");
  return 0;
}
```

Code Box 5-1: A buffer overflow situation. The strcpy function will overwrite the content of the Stack

When running the preceding code, the program will most likely crash. That's because the `strcpy` is overwriting the content of the Stack, or as it is commonly termed, *smashing* the stack. As you see in *Code Box 5-1*, the `str` array has `10` characters, but the `strcpy` is writing way more than 10 characters to the `str` array. As you will see shortly, this effectively writes on the previously pushed variables and stack frames, and the program jumps to a wrong instruction after returning from the `main` function. And this eventually makes it impossible to continue the execution.

I hope that the preceding example has helped you to appreciate the delicacy of the Stack segment. In the first half of this chapter, we are going to have a deeper look into the Stack and examine it closely. We first start by probing into the Stack.

## Probing the Stack

Before knowing more about the Stack, we need to be able to read and, probably, modify it. As stated in the previous chapter, the Stack segment is a private memory that only the owner process has the right to read and modify. If we are going to read the Stack or change it, we need to become part of the process owning the Stack.

This is where a new set of tools come in: *debuggers*. A debugger is a program that attaches to another process in order to *debug* it. One of the usual tasks while debugging a process is to observe and manipulate the various memory segments. Only when debugging a process are we able to read and modify the private memory blocks. The other thing that can be done as part of debugging is to control the order of the execution of the program instructions. We give examples on how to do these tasks using a debugger shortly, as part of this section.

Let's start with an example. In *example 5.1*, we show how to compile a program and make it ready for debugging. Then, we demonstrate how to use `gdb`, the GNU debugger, to run the program and read the Stack memory. This example declares a character array allocated on top of the Stack and populates its elements with some characters, as can be seen in the following code box:

```cpp
#include <stdio.h>
int main(int argc, char** argv) {
  char arr[4];
  arr[0] = 'A';
  arr[1] = 'B';
  arr[2] = 'C';
  arr[3] = 'D';
  return 0;
}
```

Code Box 5-2 [ExtremeC_examples_chapter5_1.c]: Declaration of an array allocated on top of the Stack

The program is simple and easy to follow, but the things that are happening inside the memory are interesting. First of all, the memory required for the `arr` array is allocated from the Stack simply because it is not allocated from the Heap segment and we didn't use the `malloc` function. Remember, the Stack segment is the default place for allocating variables and arrays.

In order to have some memory allocated from the Heap, one should acquire it by calling `malloc` or other similar functions, such as `calloc`. Otherwise, the memory is allocated from the Stack, and more precisely, on top of the Stack.

In order to be able to debug a program, the binary must be built for debugging purposes. This means that we have to tell the compiler that we want a binary that contains *debug* *symbols*. These symbols will be used to find the code lines that have been executing or those that caused a crash. Let's compile *example 5.1* and create an executable object file that contains debugging symbols.

First, we build the example. We're doing our compilation in a Linux environment:

```cpp
$ gcc -g ExtremeC_examples_chapter5_1.c -o ex5_1_dbg.out
$
```

Shell Box 5-1: Compiling the example 5.1 with debug option -g

The `-g` option tells the compiler that the final executable object file must contain the debugging information. The size of the binary is also different when you compile the source with and without the debug option. Next, you can see the difference between the sizes of the two executable object files, the first one built without the `-g` option and the second one with the `-g` option:

```cpp
$ gcc ExtremeC_examples_chapter2_10.c -o ex5_1.out
$ ls -al ex5_1.out
-rwxrwxr-x 1 kamranamini kamranamini 8640 jul 24 13:55 ex5_1.out
$ gcc -g ExtremeC_examples_chapter2_10.c -o ex5_1_dbg.out
$ ls -al ex5_1.out
-rwxrwxr-x 1 kamranamini kamranamini 9864 jul 24 13:56 ex5_1_dbg.out
$
```

Shell Box 5-2: The size of the output executable object file with and without the -g option

Now that we have an executable file containing the debug symbols, we can use the debugger to run the program. In this example, we are going to use `gdb` for debugging *example 5.1*. Next, you can find the command to start the debugger:

```cpp
$ gdb ex5_1_dbg.out
```

Shell Box 5-3: Starting the debugger for the example 5.1

**Note**:

`gdb` is usually installed as part of the `build-essentials` package on Linux systems. In macOS systems, it can be installed using the `brew` package manager like this: `brew install gdb`.

After running the debugger, the output will be something similar to the following shell box:

```cpp
$ gdb ex5_1_dbg.out
GNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1
Copyright (C) 2016 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later http://gnu.org/licenses/gpl.html
...
Reading symbols from ex5_1_dbg.out...done.
(gdb)
```

Shell Box 5-4: The output of the debugger after getting started

As you may have noticed, I've run the preceding command on a Linux machine. `gdb` has a command-line interface that allows you to issue debugging commands. Enter the `r` (or `run`) command in order to execute the executable object file, specified as an input to the debugger. The following shell box shows how the `run` command executes the program:

```cpp
...
Reading symbols from ex5_1_dbg.out...done.
(gdb) run
Starting program: .../extreme_c/5.1/ex5_1_dbg.out
[Inferior 1 (process 9742) exited normally]
(gdb)
```

Shell Box 5-5: The output of the debugger after issuing the run command

In the preceding shell box, after issuing the `run` command, `gdb` has started the process, attached to it, and let the program execute its instructions and exit. It did not interrupt the program because we have not set a *breakpoint*. A breakpoint is an indicator that tells `gdb` to pause the program execution and wait for further instructions. You can have as many breakpoints as you want.

Next, we set a breakpoint on the `main` function using the `b` (or `break`) command. After setting the breakpoint, `gdb` pauses the execution when the program enters the `main` function. You can see how to set a breakpoint on the `main` function in the following shell box:

```cpp
(gdb) break main
Breakpoint 1 at 0x400555: file ExtremeC_examples_chapter5_1.c, line 4.
(gdb)
```

Shell Box 5-6: Setting a breakpoint on the main function in gdb

Now, we run the program again. This creates a new process, and `gdb` attaches to it. Next, you can find the result:

```cpp
(gdb) r
Starting program: .../extreme_c/5.1/ex5_1_dbg.out
Breakpoint 1, main (argc=1, argv=0x7fffffffcbd8) at ExtremeC_examples_chapter5_1.c:3
3       int main(int argc, char** argv) {
(gdb)
```

Shell Box 5-7: Running the program again after setting the breakpoint

As you can see, the execution has paused at line 3, which is just the line of the `main` function. Then, the debugger waits for the next command. Now, we can ask `gdb` to run the next line of code and pause again. In other words, we run the program step by step and line by line. This way, you have enough time to look around and check the variables and their values inside the memory. In fact, this is the method we are going to use to probe the Stack and the Heap segments.

In the following shell box, you can see how to use the `n` (or `next`) command to run the next line of code:

```cpp
(gdb) n
5         arr[0] = 'A';
(gdb) n
6         arr[1] = 'B';
(gdb) next
7        arr[2] = 'C';
(gdb) next
8        arr[3] = 'D';
(gdb) next
9        return 0;
(gdb)
```

Shell Box 5-8: Using the n (or next) command to execute upcoming lines of code

Now, if you enter the `print arr` command in the debugger, it will show the content of the array as a string:

```cpp
(gdb) print arr
$1 = "ABCD"
(gdb)
```

Shell Box 5-9: Printing the content of the arr array using gdb

To get back to the topic, we introduced `gdb` to be able to see inside the Stack memory. Now, we can do it. We have a process that has a Stack segment, and it is paused, and we have a `gdb` command line to explore its memory. Let's begin and print the memory allocated for the `arr` array:

```cpp
(gdb) x/4b arr
0x7fffffffcae0: 0x41    0x42    0x43    0x44
(gdb) x/8b arr
0x7fffffffcae0: 0x41    0x42    0x43    0x44    0xff    0x7f    0x00    0x00
(gdb)
```

Shell Box 5-10: Printing bytes of memory starting from the arr array

The first command, `x/4b`, shows 4 bytes from the location that `arr` is pointing to. Remember that `arr` is a pointer that actually is pointing to the first element of the array, so it can be used to move along the memory.

The second command, `x/8b`, prints 8 bytes after `arr`. According to the code written for *example 5.1*, and found in *Code Box 5-2*, the values `A`, `B`, `C`, and `D` are stored in the array, `arr`. You should know that ASCII values are stored in the array, not the real characters. The ASCII value for `A` is `65` decimal or `0x41` hexadecimal. For `B`, it is `66` or `0x42`. As you can see, the values printed in the `gdb` output are the values we just stored in the `arr` array.

What about the other 4 bytes in the second command? They are part of the Stack, and they probably contain data from the recent Stack frame put on top of the Stack while calling the `main` function.

Note that the Stack segment is filled in an opposite fashion in comparison to other segments.

Other memory regions are filled starting from the smaller addresses and they move forward to bigger addresses, but this is not the case with the Stack segment.

The Stack segment is filled from the bigger addresses and moves backward to the smaller addresses. Some of the reasons behind this design lie in the development history of modern computers, and some in the functionality of the Stack segment, which behaves like a stack data structure.

With all that said, if you read the Stack segment from an addresses toward the bigger addresses, just like we did in *Shell Box 5-10*, you are effectively reading the already pushed content as part of the Stack segment, and if you try to change those bytes, you are altering the Stack, and this is not good. We will demonstrate why this is dangerous and how this can be done in future paragraphs.

Why are we able to see more than the size of the `arr` array? Because `gdb` goes through the number of bytes in the memory that we have requested. The `x` command doesn't care about the array's boundary. It just needs a starting address and the number of bytes to print the range.

If you want to change the values inside the Stack, you have to use the `set` command. This allows you to modify an existing memory cell. In this case, the memory cell refers to an individual byte in the `arr` array:

```cpp
(gdb) x/4b arr
0x7fffffffcae0: 0x41    0x42    0x43    0x44
(gdb) set arr[1] = 'F'
(gdb) x/4b arr
0x7fffffffcae0: 0x41    0x46    0x43    0x44
(gdb) print arr
$2 = "AFCD"
(gdb)
```

Shell Box 5-11: Changing an individual byte in the array using the set command

As you can see, using the `set` command, we have set the second element of the `arr` array to `F`. If you are going to change an address that is not in the boundaries of your arrays, it is still possible through `gdb`.

Please observe the following modification carefully. Now, we want to modify a byte located in a far bigger address than `arr`, and as we explained before, we will be altering the already pushed content of the Stack. Remember, the Stack memory is filled in an opposite manner compared to other segments:

```cpp
(gdb) x/20x arr
0x7fffffffcae0: 0x41    0x42    0x43    0x44    0xff    0x7f    0x00    0x00
0x7fffffffcae8: 0x00    0x96    0xea    0x5d    0xf0    0x31    0xea    0x73
0x7fffffffcaf0: 0x90    0x05    0x40    0x00
(gdb) set *(0x7fffffffcaed) = 0xff
(gdb) x/20x arr
0x7fffffffcae0: 0x41    0x42    0x43    0x44    0xff    0x7f    0x00    0x00
0x7fffffffcae8: 0x00    0x96    0xea    0x5d    0xf0    0xff    0x00    0x00
0x7fffffffcaf0: 0x00    0x05    0x40    0x00
(gdb)
```

Shell Box 5-12: Changing an individual byte outside of the array's boundary

That is all. We just wrote the value `0xff` in the `0x7fffffffcaed` address, which is out of the boundary of the `arr` array, and probably a byte within the stack frame pushed before entering the `main` function.

What will happen if we continue the execution? If we have modified a critical byte in the Stack, we expect to see a crash or at least have this modification detected by some mechanism and have the execution of the program halted. The command `c` (or `continue`) will continue the execution of the process in `gdb`, as you can see next:

```cpp
(gdb) c
Continuing.
*** stack smashing detected ***: .../extreme_c/5.1/ex5_1_dbg.out terminated
Program received signal SIGABRT, Aborted.
0x00007ffff7a42428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/Unix/sysv/linux/raise.c:54
54      ../sysdeps/Unix/sysv/linux/raise.c: No such file or directory.
(gdb)
```

Shell Box 5-13: Having a critical byte changed in the Stack terminates the process

As you can see in the preceding shell box, we've just smashed the Stack! Modifying the content of the Stack in addresses that are not allocated by you, even by 1 byte, can be very dangerous and it usually leads to a crash or a sudden termination.

As we have said before, most of the vital procedures regarding the execution of a program are done within the Stack memory. So, you should be very careful when writing to Stack variables. You should not write any values outside of the boundaries defined for variables and arrays simply because the addresses grow backward in the Stack memory, which makes it likely to overwrite the already written bytes.

When you're done with debugging, and you're ready to leave the `gdb`, then you can simply use the command `q` (or `quit`). Now, you should be out of the debugger and back in the terminal.

As another note, writing unchecked values into a *buffer* (another name for a byte or character array) allocated on top of the Stack (not from the Heap) is considered a vulnerability. An attacker can carefully design a byte array and feed it to the program in order to take control of it. This is usually called an *exploit* because of a *buffer overflow* attack.

The following program shows this vulnerability:

```cpp
int main(int argc, char** argv) {
  char str[10];
  strcpy(str, argv[1]);
  printf("Hello %s!\n", str);
}
```

Code Box 5-3: A program showing the buffer overflow vulnerability

The preceding code does not check the `argv[1]` input for its content and its size and copies it directly into the `str` array, which is allocated on top of the Stack.

If you're lucky, this can lead to a crash, but in some rare but dangerous cases, this can lead to an exploit attack.

## Points on using the Stack memory

Now that you have a better understanding of the Stack segment and how it works, we can talk about the best practices and the points you should be careful about. You should be familiar with the *scope* concept. Each Stack variable has its own scope, and the scope determines the lifetime of the variable. This means that a Stack variable starts its lifetime in one scope and dies when that scope is gone. In other words, the scope determines the lifetime of a Stack variable.

We also have automatic memory allocation and deallocation for Stack variables, and it is only applicable to the Stack variables. This feature, automatic memory management, comes from the nature of the Stack segment.

Whenever you declare a Stack variable, it will be allocated on top of the Stack segment. Allocation happens automatically, and this can be marked as the start of its lifetime. After this point, many more variables and other stack frames are put on top of it inside the Stack. As long as the variable exists in the Stack and there are other variables on top of it, it survives and continues living.

Eventually, however, this stuff will get popped out of the Stack because at some point in the future the program has to be finished, and the stack should be empty at that moment. So, there should be a point in the future when this variable is popped out of the stack. So, the deallocation, or getting popped out, happens automatically, and that can be marked as the end of the variable's lifetime. This is basically the reason why we say that we have automatic memory management for the Stack variables that is not controlled by the programmer.

Suppose that you have defined a variable in the `main` function, as we see in the following code box:

```cpp
int main(int argc, char** argv) {
  int a;
  ...
  return 0;
}
```

Code Box 5-4: Declaring a variable on top of the Stack

This variable will stay in the Stack until the `main` function returns. In other words, the variable exists until its scope (the `main` function) is valid. Since the `main` function is the function in which all the program runs, the lifetime of the variable is almost like a global variable that is declared throughout the runtime of the program.

It is like a global variable, but not exactly one, because there will be a time that the variable is popped out from the Stack, whereas a global variable always has its memory even when the main function is finished and the program is being finalized. Note that there are two pieces of code that are run before and after the `main` function, bootstrapping and finalizing the program respectively. As another note, global variables are allocated from a different segment, Data or BSS, that does not behave like the Stack segment.

Let's now look at an example of a very common mistake. It usually happens to an amateur programmer while writing their first C programs. It is about returning an address to a local variable inside a function.

The following code box shows *example 5.2*:

```cpp
int* get_integer() {
  int var = 10;
  return &var;
}
int main(int argc, char** argv) {
  int* ptr = get_integer();
  *ptr = 5;
  return 0;
}
```

Code Box 5-5 [ExtremeC_examples_chapter5_2.c]: Declaring a variable on top of the Stack

The `get_integer` function returns an address to the local variable, `var`, which has been declared in the scope of the `get_integer` function. The `get_integer` function returns the address of the local variable. Then, the `main` function tries to dereference the received pointer and access the memory region behind. The following is the output of the `gcc` compiler while compiling the preceding code on a Linux system:

```cpp
$ gcc ExtremeC_examples_chapter5_2.c -o ex5_2.out
ExtremeC_examples_chapter5_2.c: In function 'get_integer':
ExtremeC_examples_chapter5_2.c:3:11: warning: function returns address of local variable [-Wreturn-local-addr]
   return &var;
          ^~~~
$
```

Shell Box 5-14: Compiling the example 5.2 in Linux

As you can see, we have received a warning message. Since returning the address of a local variable is a common mistake, compilers already know about it, and they show a clear warning message like `warning: function returns address of a local variable`.

And this is what happens when we execute the program:

```cpp
$ ./ex5_2.out
Segmentation fault (core dumped)
$
```

Shell Box 5-15: Executing the example 5.2 in Linux

As you can see in *Shell Box 5-15*, a segmentation fault has happened. It can be translated as a crash. It is usually because of invalid access to a region of memory that had been allocated at some point, but now it is deallocated.

**Note**:

Some warnings should be treated as errors. For example, the preceding warning should be an error because it usually leads to a crash. If you want to make all warning to be treated as errors, it is enough to pass the `-Werror` option to `gcc` compiler. If you want to treat only one specific warning as an error, for example, the preceding warning, it is enough to pass the `-Werror=return-local-addr` option.

If you run the program with `gdb`, you will see more details regarding the crash. But remember, you need to compile the program with the `-g` option otherwise `gdb` won't be that helpful.

It is always mandatory to compile the sources with `-g` option if you are about to debug the program using `gdb` or other debugging tools such as `valgrind`. The following shell box demonstrates how to compile and run *example 5.2* in the debugger:

```cpp
$ gcc -g ExtremeC_examples_chapter5_2.c -o ex5_2_dbg.out
ExtremeC_examples_chapter5_2.c: In function 'get_integer':
ExtremeC_examples_chapter5_2.c:3:11: warning: function returns address of local variable [-Wreturn-local-addr]
   return &var;
          ^~~~
$ gdb ex5_2_dbg.out
GNU gdb (Ubuntu 8.1-0ubuntu3) 8.1.0.20180409-git
...
Reading symbols from ex5_2_dbg.out...done.
(gdb) run
Starting program: .../extreme_c/5.2/ex5_2_dbg.out
Program received signal SIGSEGV, Segmentation fault.
0x00005555555546c4 in main (argc=1, argv=0x7fffffffdf88) at ExtremeC_examples_chapter5_2.c:8
8    *ptr = 5;
(gdb) quit
$
```

Shell Box 5-16: Running the example 5.2 in the debugger

As is clear from the `gdb` output, the source of the crash is located at line 8 in the `main` function, exactly where the program tries to write to the returned address by dereferencing the returned pointer. But the `var` variable has been a local variable to the `get_integer` function and it doesn't exist anymore, simply because at line 8 we have already returned from the `get_integer` function and its scope, together with all variables, have vanished. Therefore, the returned pointer is a *dangling pointer*.

It is usually a common practice to pass the pointers addressing the variables in the current scope to other functions but not the other way around, because as long as the current scope is valid, the variables are there. Further function calls only put more stuff on top of the Stack segment, and the current scope won't be finished before them.

Note that the above statement is not a good practice regarding concurrent programs because in the future, if another concurrent task wants to use the received pointer addressing a variable inside the current scope, the current scope might have vanished already.

To end this section and have a conclusion about the Stack segment, the following points can be extracted from what we have explained so far:

*   Stack memory has a limited size; therefore, it is not a good place to store big objects.
*   The addresses in Stack segment grow backward, therefore reading forward in the Stack memory means reading already pushed bytes.
*   Stack has automatic memory management, both for allocation and deallocation.
*   Every Stack variable has a scope and it determines its lifetime. You should design your logic based on this lifetime. You have no control over it.
*   Pointers should only point to those Stack variables that are still in a scope.
*   Memory deallocation of Stack variables is done automatically when the scope is about to finish, and you have no control over it.
*   Pointers to variables that exist in the current scope can be passed to other functions as arguments only when we are sure that the current scope will be still in place when the code in the called functions is about to use that pointer. This condition might break in situations when we have concurrent logic.

In the next section, we will talk about the Heap segment and its various features.

# Heap

Almost any code, written in any programming language, uses Heap memory in some way. That's because the Heap has some unique advantages that cannot be achieved by using the Stack.

On the other hand, it also has some disadvantages; for example, it is slower to allocate a region of Heap memory in comparison to a similar region in Stack memory.

In this section, we are going to talk more about the Heap itself and the guidelines we should keep in mind when using Heap memory.

Heap memory is important because of its unique properties. Not all of them are advantageous and, in fact, some of them can be considered as risks that should be mitigated. A great tool always has good points and some bad points, and if you are going to use it properly, you are required to know both sides very well.

Here, we are going to list these features and see which ones are beneficial and which are risky:

1.  **The Heap doesn't have any memory blocks that are allocated automatically**. Instead, the programmer must use `malloc` or similar functions to obtain Heap memory blocks, one by one. In fact, this could be regarded as a weak point for Stack memory that is resolved by Heap memory. Stack memory can contain stack frames, which are not allocated and pushed by the programmer but as a result of function calls, and in an automatic fashion.
2.  **The Heap has a large memory size**. While the size of the Stack is limited and it is not a good choice for keeping big objects, the Heap allows the storing of very big objects even tens of gigabytes in size. As the Heap size grows, the allocator needs to request more heap pages from the operating system, and the Heap memory blocks are spread among these pages. Note that, unlike the Stack segment, the allocating addresses in the Heap memory move forward to bigger addresses.
3.  **Memory allocation and deallocation inside Heap memory are managed by the programmer**. This means that the programmer is the sole responsible entity for allocating the memory and then freeing it when it is not needed anymore. In many recent programming languages, freeing allocated Heap blocks is done automatically by a parallel component called *garbage collector*. But in C and C++, we don't have such a concept and freeing the Heap blocks should be done manually. This is indeed a risk, and C/C++ programmers should be very careful while using heap memory. Failing to free the allocated Heap blocks usually leads to *memory leaks*, which can be fatal in most cases.
4.  **Variables allocated from the Heap do not have any scope**, unlike variables in the Stack.
5.  This is a risk because it makes memory management much harder. You don't know when you need to deallocate the variable, and you have to come up with some new definitions for the *scope* and the *owner* of the memory block in order to do the memory management effectively. Some of these methods are covered in the upcoming sections.
6.  **We can only use pointers to address a Heap memory block**. In other words, there is no such concept as a Heap variable. The Heap region is addressed via pointers.

1.  **Since the Heap segment is private to its owner process, we need to use a debugger to probe it**. Fortunately, C pointers work with the Heap memory block exactly the same as they work with Stack memory blocks. C does this abstraction very well, and because of this, we can use the same pointers to address both memories. Therefore, we can use the same methods that we used to examine the Stack to probe the Heap memory.

In the next section, we are going to discuss how to allocate and deallocate a heap memory block.

## Heap memory allocation and deallocation

As we said in the previous section, Heap memory should be obtained and released manually. This means that the programmer should use a set of functions or API (the C standard library's memory allocation functions) in order to allocate or free a memory block in the Heap.

These functions do exist, and they are defined in the header, `stdlib.h`. The functions used for obtaining a Heap memory block are `malloc`, `calloc`, and `realloc`, and the sole function used for releasing a Heap memory block is `free`. *Example 5.3* demonstrates how to use some of these functions.

**Note**:

In some texts, dynamic memory is used to refer to Heap memory. *Dynamic memory allocation* is a synonym for Heap memory allocation.

The following code box shows the source code of *example 5.3*. It allocates two Heap memory blocks, and then it prints its own memory mappings:

```cpp
#include <stdio.h>  // For printf function
#include <stdlib.h> // For C library's heap memory functions
void print_mem_maps() {
#ifdef __linux__
  FILE* fd = fopen("/proc/self/maps", "r");
  if (!fd) {
    printf("Could not open maps file.\n");
    exit(1);
  }
  char line[1024];
  while (!feof(fd)) {
    fgets(line, 1024, fd);
    printf("> %s", line);
  }
  fclose(fd);
#endif
}
int main(int argc, char** argv) {
  // Allocate 10 bytes without initialization
  char* ptr1 = (char*)malloc(10 * sizeof(char));
  printf("Address of ptr1: %p\n", (void*)&ptr1);
  printf("Memory allocated by malloc at %p: ", (void*)ptr1);
  for (int i = 0; i < 10; i++) {
    printf("0x%02x ", (unsigned char)ptr1[i]);
  }
  printf("\n");
  // Allocation 10 bytes all initialized to zero
  char* ptr2 = (char*)calloc(10, sizeof(char));
  printf("Address of ptr2: %p\n", (void*)&ptr2);
  printf("Memory allocated by calloc at %p: ", (void*)ptr2);
  for (int i = 0; i < 10; i++) {
    printf("0x%02x ", (unsigned char)ptr2[i]);
  }
  printf("\n");
  print_mem_maps();
  free(ptr1);
  free(ptr2);
  return 0;
}
```

Code Box 5-6 [ExtremeC_examples_chapter5_3.c]: Example 5.3 showing the memory mappings after allocating two Heap memory blocks

The preceding code is cross-platform, and you can compile it on most Unix-like operating systems. But the `print_mem_maps` function only works on Linux since the `__linux__` macro is only defined in Linux environments. Therefore, in macOS, you can compile the code, but the `print_mem_maps` function won't do anything.

The following shell box is the result of running the example in a Linux environment:

```cpp
$ gcc ExtremeC_examples_chapter5_3.c -o ex5_3.out
$ ./ex5_3.out
Address of ptr1: 0x7ffe0ad75c38
Memory allocated by malloc at 0x564c03977260: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 
Address of ptr2: 0x7ffe0ad75c40
Memory allocated by calloc at 0x564c03977690: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 
> 564c01978000-564c01979000 r-xp 00000000 08:01 5898436                    /home/kamranamini/extreme_c/5.3/ex5_3.out
> 564c01b79000-564c01b7a000 r--p 00001000 08:01 5898436                    /home/kamranamini/extreme_c/5.3/ex5_3.out
> 564c01b7a000-564c01b7b000 rw-p 00002000 08:01 5898436                    /home/kamranamini/extreme_c/5.3/ex5_3.out
> 564c03977000-564c03998000 rw-p 00000000 00:00 0           [heap]
> 7f31978ec000-7f3197ad3000 r-xp 00000000 08:01 5247803     /lib/x86_64-linux-gnu/libc-2.27.so
...
> 7f3197eef000-7f3197ef1000 rw-p 00000000 00:00 0 
> 7f3197f04000-7f3197f05000 r--p 00027000 08:01 5247775     /lib/x86_64-linux-gnu/ld-2.27.so
> 7f3197f05000-7f3197f06000 rw-p 00028000 08:01 5247775     /lib/x86_64-linux-gnu/ld-2.27.so
> 7f3197f06000-7f3197f07000 rw-p 00000000 00:00 0 
> 7ffe0ad57000-7ffe0ad78000 rw-p 00000000 00:00 0           [stack]
> 7ffe0adc2000-7ffe0adc5000 r--p 00000000 00:00 0           [vvar]
> 7ffe0adc5000-7ffe0adc7000 r-xp 00000000 00:00 0           [vdso]
> ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0   [vsyscall]
$
```

Shell Box 5-17: Output of example 5.3 in Linux

The preceding output has a lot to say. The program prints the addresses of the pointers `ptr1` and `ptr2`. If you find the memory mapping of the Stack segment, as part of the printed memory mappings, you see that the Stack region starts from `0x7ffe0ad57000` and ends at `0x7ffe0ad78000`. The pointers are within this range.

This means that the pointers are allocated from the Stack, but they are pointing to a memory region outside of the Stack segment, in this case, the Heap segment. It is very common to use a Stack pointer to address a Heap memory block.

Keep in mind that the `ptr1` and `ptr2` pointers have the same scope and they will be freed when the `main` function returns, but there is no scope to the Heap memory blocks obtained from the Heap segment. They will remain allocated until the program frees them manually. You can see that before returning from the `main` function, both memory blocks are freed using the pointers pointing to them and using the `free` function.

As a further note regarding the above example, we can see that the addresses returned by the `malloc` and `calloc` functions are located inside the Heap segment. This can be investigated by comparing the returned addresses and the memory mapping described as `[heap]`. The region marked as heap starts from `0x564c03977000` and ends at `0x564c03998000`. The `ptr1` pointer points to the address `0x564c03977260` and the `ptr2 p`ointer points to the address `0x564c03977690`, which are both inside the heap region.

Regarding the Heap allocation function, as their names imply, `calloc` stands for **clear and allocate** and `malloc` stands for **memory allocate**. So, this means that `calloc` clears the memory block after allocation, but `malloc` leaves it uninitialized until the program does it itself if necessary.

**Note**:

In C++, the `new` and `delete` keywords do the same as `malloc` and `free` respectively. Additionally, new operator infers the size of the allocated memory block from the operand type and also converts the returned pointer to the operand type automatically.

But if you look at the bytes in the two allocated blocks, both of them have zero bytes. So, it seems that `malloc` has also initialized the memory block after the allocation. But based on the description of `malloc` in the C Specification, `malloc` doesn't initialize the allocated memory block. So, why is that? To move this further, let's run the example in a macOS environment:

```cpp
$ clang ExtremeC_examples_chapter5_3.c -o ex5_3.out
$ ./ ex5_3.out
Address of ptr1: 0x7ffee66b2888
Memory allocated by malloc at 0x7fc628c00370: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x80 0x00 0x00
Address of ptr2: 0x7ffee66b2878
Memory allocated by calloc at 0x7fc628c02740: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00
$
```

Shell Box 5-18: Output of example 5.3 on macOS

If you look carefully, you can see that the memory block allocated by `malloc` has some non-zero bytes, but the memory block allocated by `calloc` is all zeros. So, what should we do? Should we assume that the memory block allocated by `malloc` in Linux is always zeros?

If you are going to write a cross-platform program, always be aligned with the C specification. The specification says `malloc` does not initialize the allocated memory block.

Even when you are writing your program only for Linux and not for other operating systems, be aware that future compilers may behave differently. Therefore, according to the C specification, we must always assume that the memory block allocated by the `malloc` is not initialized and it should be initialized manually if necessary.

Note that since `malloc` doesn't initialize the allocated memory, it is usually faster than `calloc`. In some implementations, `malloc` doesn't actually allocate the memory block and defer the allocation until when the memory block is accessed (either read or write). This way, memory allocations happen faster.

If you are going to initialize the memory after `malloc`, you can use the `memset` function. Here is an example:

```cpp
#include <stdlib.h> // For malloc
#include <string.h> // For memset
int main(int argc, char** argv) {
  char* ptr = (char*)malloc(16 * sizeof(char));
  memset(ptr, 0, 16 * sizeof(char));    // Fill with 0
  memset(ptr, 0xff, 16 * sizeof(char)); // Fill with 0xff
  ...
  free(ptr);
  return 0;
}
```

Code Box 5-7: Using the memset function to initialize a memory block

The `realloc` function is another function that is introduced as part of the Heap allocation functions. It was not used as part of *example 5.3*. It actually reallocates the memory by resizing an already allocated memory block. Here is an example:

```cpp
int main(int argc, char** argv) {
  char* ptr = (char*)malloc(16 * sizeof(char));
  ...
  ptr = (char*)realloc(32 * sizeof(char));
  ...
  free(ptr);
  return 0;
}
```

Code Box 5-8: Using the realloc function to change the size of an already allocated block

The `realloc` function does not change the data in the old block and only expands an already allocated block to a new one. If it cannot expand the currently allocated block because of *fragmentation*, it will find another block that's large enough and copy the data from the old block to the new one. In this case, it will also free the old block. As you can see, reallocation is not a cheap operation in some cases because it involves many steps, hence it should be used with care.

The last note about *example 5.3* is on the `free` function. In fact, it deallocates an already allocated Heap memory block by passing the block's address as a pointer. As it is said before, any allocated Heap block should be freed when it is not needed. Failing to do so leads to *memory leakage*. Using a new example, *example 5.4*, we are going to show you how to detect memory leaks using the `valgrind` tool.

Let's first produce some memory leaks as part of *example 5.4*:

```cpp
#include <stdlib.h> // For heap memory functions
int main(int argc, char** argv) {
  char* ptr = (char*)malloc(16 * sizeof(char));
  return 0;
}
```

Code Box 5-9: Producing a memory leak by not freeing the allocated block when returning from the main function

The preceding program has a memory leak because when the program ends, we have `16` bytes of Heap memory allocated and not freed. This example is very simple, but when the source code grows and more components are involved, it would be too hard or even impossible to detect it by sight.

Memory profilers are useful programs that can detect the memory issues in a running process. The famous `valgrind` tool is one of the most well knowns.

In order to use `valgrind` to analyze *example 5.4*, first we need to build the example with the debug option, `-g`. Then, we should run it using `valgrind`. While running the given executable object file, `valgrind` records all of the memory allocations and deallocations. Finally, when the execution is finished or a crash happens, `valgrind` prints out the summary of allocations and deallocations and the amount of memory that has not been freed. This way, it can let you know how much memory leak has been produced as part of the execution of the given program.

The following shell box demonstrates how to compile and use `valgrind` for *example 5.4*:

```cpp
$ gcc -g ExtremeC_examples_chapter5_4.c -o ex5_4.out
$ valgrind ./ex5_4.out
==12022== Memcheck, a memory error detector
==12022== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==12022== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==12022== Command: ./ex5_4.out
==12022== 
==12022== 
==12022== HEAP SUMMARY:
==12022==     in use at exit: 16 bytes in 1 blocks
==12022==   total heap usage: 1 allocs, 0 frees, 16 bytes allocated
==12022== 
==12022== LEAK SUMMARY:
==12022==    definitely lost: 16 bytes in 1 blocks
==12022==    indirectly lost: 0 bytes in 0 blocks
==12022==      possibly lost: 0 bytes in 0 blocks
==12022==    still reachable: 0 bytes in 0 blocks
==12022==         suppressed: 0 bytes in 0 blocks
==12022== Rerun with --leak-chck=full to see details of leaked memory
==12022== 
==12022== For counts of detected and suppressed errors, rerun with: -v
==12022== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
$
```

Shell Box 5-19: Output of valgrind showing the 16-byte memory leak as part of the execution of example 5.4

If you look into the `HEAP SUMMARY` section in *Shell Box 5-19*, you can see that we had `1` allocation and `0` frees, and `16` bytes remained allocated while exiting. If you come down a bit to the `LEAK SUMMARY` section, it states that `16` bytes are definitely lost, and this means a memory leak!

If you want to know exactly at which line the mentioned leaking memory block has been allocated, you can use `valgrind` with a special option designed for this. In the following shell box, you will see how to use `valgrind` to find the lines responsible for the actual allocation:

```cpp
$ gcc -g ExtremeC_examples_chapter5_4.c -o ex5_4.out
$ valgrind --leak-check=full ./ex5_4.out
==12144== Memcheck, a memory error detector
==12144== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==12144== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==12144== Command: ./ex5_4.out
==12144== 
==12144== 
==12144== HEAP SUMMARY:
==12144==     in use at exit: 16 bytes in 1 blocks
==12144==   total heap usage: 1 allocs, 0 frees, 16 bytes allocated
==12144== 
==12144== 16 bytes in 1 blocks are definitely lost in loss record 1 of 1
==12144==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12144==    by 0x108662: main (ExtremeC_examples_chapter5_4.c:4)
==12144== 
==12144== LEAK SUMMARY:
==12144==    definitely lost: 16 bytes in 1 blocks
==12144==    indirectly lost: 0 bytes in 0 blocks
==12144==      possibly lost: 0 bytes in 0 blocks
==12144==    still reachable: 0 bytes in 0 blocks
==12144==         suppressed: 0 bytes in 0 blocks
==12144== 
==12144== For counts of detected and suppressed errors, rerun with : -v
==12144== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)
$
```

Shell Box 5-20: Output of valgrind showing the line that is responsible for the actual allocation

As you can see, we have passed the `--leak-check=full` option to `valgrind`, and now it shows the line of code that is responsible for the leaking Heap memory. It clearly shows that line 4 in *Code Box 5-9*, which is a `malloc` call, is where the leaking Heap block has been allocated. This can help you to trace it further and find the right place that the mentioned leaking block should be freed.

OK, let's change the preceding example so that it frees the allocated memory. We just need to add the `free(ptr)` instruction before the `return` statement, as we can see here:

```cpp
#include <stdlib.h> // For heap memory functions
int main(int argc, char** argv) {
  char* ptr = (char*)malloc(16 * sizeof(char));
  free(ptr);
  return 0;
}
```

Code Box 5-10: Freeing up the allocated memory block as part of example 5.4

Now with this change, the only allocated Heap block is freed. Let's build and run `valgrind` again:

```cpp
$ gcc -g ExtremeC_examples_chapter5_4.c -o ex5_4.out
$ valgrind --leak-check=full ./ex5_4.out
==12175== Memcheck, a memory error detector
==12175== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==12175== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==12175== Command: ./ex5_4.out
==12175== 
==12175== 
==12175== HEAP SUMMARY:
==12175==     in use at exit: 0 bytes in 0 blocks
==12175==   total heap usage: 1 allocs, 1 frees, 16 bytes allocated
==12175== 
==12175== All heap blocks were freed -- no leaks are possible
==12175== 
==12175== For counts of detected and suppressed errors, rerun with  -v
==12175== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
$
```

Shell Box 5-20: Output of valgrind after freeing the allocated memory block

As you can see, `valgrind` says that `All Heap blocks were freed`, and this effectively means that we have no further memory leakage in our program. Running programs with `valgrind` can slow them down noticeably by a factor of 10 to 50, but it can help you to spot the memory issues very easily. It's a good practice to let your written programs run inside a memory profiler and catch memory leaks as soon as possible.

Memory leaks can be considered both as *technical debts*, if you have a bad design that causes the leaks, or as *risks*, where it's known that we have a leak, but we don't know what will happen if the leak continues to grow. But in my opinion, they should be treated as *bugs*; otherwise, it will take a while for you to look back at them. Usually, in teams, memory leaks are treated as bugs that should be fixed as soon as possible.

There are other memory profilers other than `valgrind`. **LLVM Address Sanitizer** (or **ASAN**) and **MemProf** are also other well-known memory profilers. Memory profilers can profile memory usage and allocations using various methods. Next, we discuss some of them:

*   Some profilers can behave like a sandbox, running the target program inside and monitoring all their memory activities. We've used this method to run *example 5.4* inside a `valgrind` sandbox. This method does not require you to recompile your code.
*   Another method is to use the libraries provided by some memory profilers, which wrap memory-related system calls. This way, the final binary will contain all of the logic required for the profiling task.

    `valgrind` and ASAN can be linked to the final executable object file as a memory profiler library. This method requires the recompilation of your target source code and even making some modifications to your source code as well.

*   Programs can also *preload* different libraries instead of the default C standard libraries, which contain memory *function interpositions* of the C library's standard memory allocation functions. This way, you are not required to compile your target source code. You just need to specify the libraries of such profilers in the `LD_PRELOAD` environment variable to be preloaded instead of the default `libc` libraries. `MemProf` uses this method.

**Note**:

A *function interposition* is a wrapper function defined in a dynamic library loaded before the target dynamic library, which propagates calls to the target function. Dynamic libraries can be preloaded using the `LD_PRELOAD` environment variable.

## Heap memory principles

As pointed out before, Heap memory is different from Stack memory in several ways. Therefore, heap memory has its own guidelines regarding memory management. In this section, we are going to focus on these differences and come up with some dos and don'ts that we should consider when working with the Heap space.

Every memory block (or a variable) in the Stack has a scope. So, it is an easy task to define the lifetime of a memory block based on its scope. Whenever we are out of scope, all of the variables in that scope are gone. But this is different and much more complex with Heap memory.

A Heap memory block doesn't have any scope, so its lifetime is unclear and should be redefined. This is the reason behind having manual deallocation or *generational* *garbage collection* in modern languages such as Java. The Heap lifetime cannot be determined by the program itself or the C libraries used, and the programmer is the sole person who defines the lifetime of a Heap memory block.

When the discussion comes to the programmer's decision, especially in this case, it is complicated and hard to propose a universal silver bullet solution. Every opinion is debatable and can lead to a trade-off.

One of the best proposed strategies to overcome the complexity of the Heap lifetime, which of course is not a complete solution, is to define an *owner* for a memory block instead of having a scope that encompasses the memory block.

The owner is the sole entity responsible for managing the lifetime of a Heap memory block and is the one who allocates the block in the first place and frees it when the block is not needed anymore.

There are many classic examples of how to use this strategy. Most of the well-known C libraries use this strategy to handle their Heap memory allocations. *Example 5.5* is a very simple implementation of this method that is used to manage the lifetime of a queue object written in C. The following code box tries to demonstrate the *ownership* strategy:

```cpp
#include <stdio.h> // For printf function
#include <stdlib.h> // For heap memory functions
#define QUEUE_MAX_SIZE 100
typedef struct {
  int front;
  int rear;
  double* arr;
} queue_t;
void init(queue_t* q) {
  q->front = q->rear = 0;
  // The heap memory block allocated here is owned
  // by the queue object.
  q->arr = (double*)malloc(QUEUE_MAX_SIZE * sizeof(double));
}
void destroy(queue_t* q) {
  free(q->arr);
}
int size(queue_t* q) {
  return q->rear - q->front;
}
void enqueue(queue_t* q, double item) {
  q->arr[q->rear] = item;
  q->rear++;
}
double dequeue(queue_t* q) {
  double item = q->arr[q->front];
  q->front++;
  return item;
}
int main(int argc, char** argv) {
  // The heap memory block allocated here is owned
  // by the function main
  queue_t* q = (queue_t*)malloc(sizeof(queue_t));
  // Allocate needed memory for the queue object
  init(q);
  enqueue(q, 6.5);
  enqueue(q, 1.3);
  enqueue(q, 2.4);
  printf("%f\n", dequeue(q));
  printf("%f\n", dequeue(q));
  printf("%f\n", dequeue(q));
  // Release resources acquired by the queue object
  destroy(q);
  // Free the memory allocated for the queue object
  // acquired by the function main
  free(q);
  return 0;
}
```

Code Box 5-11 [ExtremeC_examples_chapter5_5.c]: The example 5.5 demonstrating the ownership strategy for Heap lifetime management

The preceding example contains two different ownerships each of which owning a specific object. The first ownership is about the Heap memory block addressed by the `arr` pointer in the `queue_t` structure that is owned by the queue object. As long as the queue object exists, this memory block must remain in place and allocated.

The second ownership is regarding the Heap memory block acquired by the `main` function as a placeholder for the queue object, `q`, that is owned by the `main` function itself. It is very important to distinguish between the Heap memory blocks owned by the queue object and the Heap memory blocks owned by the `main` function because releasing one of them doesn't release another.

To demonstrate how a memory leak can happen in the preceding code, suppose that you forget to call the `destroy` function on the queue object. It will definitely lead to a memory leak because the Heap memory block acquired inside the `init` function would be still allocated and not freed.

Note that if an entity (an object, function, and so on) owns a Heap memory block, it should be expressed in the comments. Nothing should free a Heap memory block if it does not own the block.

Note that multiple deallocations of the same Heap memory block will lead to a *double free* situation. A double-free situation is a memory corruption issue and like any other memory corruption issue, it should be dealt with and resolved soon after detection. Otherwise, it can have serious consequences like sudden crashes.

Other than the ownership strategy, one could use a garbage collector. The garbage collector is an automatic mechanism that is embedded in a program and tries to collect memory blocks that have no pointer addressing them. One of the old well-known garbage collectors for C is the *Boehm-Demers-Weiser Conservative Garbage Collector*, which provides a set of memory allocation functions that should be called instead of `malloc` and other standard C memory allocation functions.

**Further Reading**:

More informatio[n about the Boehm-Demers-Weise](http://www.hboehm.info/gc/)r garbage collector can be found here: [http://www.hboehm.info/gc/](http://www.hboehm.info/gc/).

Another technique to manage the lifetime of a Heap block is using a RAII object. **RAII** stands for **Resource Acquisition Is Initialization**. It means that we can bind the lifetime of a resource, possibly a Heap allocated memory block, to the lifetime of an object. In other words, we use an object that upon its construction initializes the resource, and upon its destruction frees the resource. Unfortunately, this technique cannot be used in C because we are not notified about the destruction of an object. But in C++, using destructors, this technique can be used effectively. In RAII objects, resource initialization happens in the constructor and the code required to de-initialize the resource is put into the destructor. Note that in C++, the destructor is invoked automatically when an object is going out of scope or being deleted.

As a conclusion, the following guidelines are important when working with Heap memory:

*   Heap memory allocation is not free, and it has its own costs. Not all memory allocation functions have the same cost and, usually, `malloc` is the cheapest one.
*   All memory blocks allocated from the Heap space must be freed either immediately when they are not needed anymore or just before ending the program.
*   Since Heap memory blocks have no scope, the program must be able to manage the memory in order to avoid any possible leakage.
*   Sticking to a chosen memory management strategy for each Heap memory block seems to be necessary.
*   The chosen strategy and its assumptions should be documented throughout the code wherever the block is accessed so that future programmers will know about it.
*   In certain programming languages like C++, we can use RAII objects to manage a resource, possibly a Heap memory block.

So far, we have considered that we have enough memory to store big objects and run any kind of program. But in the following section, we are going to put some constraints on the available memory and discuss the environments where the memory is low, or it is costly (in terms of money, time, performance, and so on) to add further memory storage. In such cases, we need to use the available memory in the most efficient way.

# Memory management in constrained environments

There are environments in which memory is a precious resource, and it is often limited. There are also other environments in which performance is a key factor and programs should be fast, no matter how much memory we have. Regarding memory management, each of these environments requires a specific technique to overcome the memory shortage and performance degradation. First, we need to know what a constrained environment is.

A constrained environment does not necessarily have a low memory capacity. There are usually some *constraints* that limit the memory usage for a program. These constraints can be your customer's hard limits regarding memory usage, or it could be because of a hardware that provides the low memory capacity, or it can be because of an operating system that does not support a bigger memory (for example, MS-DOS).

Even if there are no constraints or hardware limitations, we as programmers try our best to use the least possible amount of memory and use it in an optimal way. Memory consumption is one of the key *non-functional requirements* in a project and should be monitored and tuned carefully.

In this section, we'll first introduce the techniques used in low memory environments for overcoming the shortage issue, and then we will talk about the memory techniques usually used in performant environments in order to boost the performance of the running programs.

## Memory-constrained environments

In these environments, limited memory is always a constraint, and algorithms should be designed in a way in order to cope with memory shortages. Embedded systems with a memory size of tens to hundreds of megabytes are usually in this category. There are a few tips about memory management in such environments, but none of them work as well as having a nicely tuned algorithm. In this case, algorithms with a low memory complexity are usually used. These algorithms usually have a higher *time complexity*, which should be traded off with their low memory usage.

To elaborate more on this, every algorithm has specific *time* and *memory* complexities. Time complexity describes the relationship between the input size and the time that the algorithm takes to complete. Similarly, memory complexity describes the relationship between the input size and the memory that the algorithm consumes to complete its task. These complexities are usually denoted as *Big-O functions*, which we don't want to deal with in this section. Our discussion is qualitative, so we don't need any math to talk about memory-constrained environments.

An algorithm should ideally have a low time complexity and also a low memory complexity. In other words, having a fast algorithm consuming a low amount of memory is highly desirable, but it is unusual to have this "best of both worlds" situation. It is also unexpected to have an algorithm with high memory consumption while not performing well

Most of the time, we have a trade-off between memory and speed, which represents time. As an example, a sorting algorithm that is faster than another algorithm usually consumes more memory than the other, despite the fact that both of them do the same job.

It is a good but conservative practice, especially when writing a program, to assume that we are writing code for a memory-constrained system, even if we know that we will have more than enough memory in the final production environment. We make this assumption because we want to mitigate the risk of having too much memory consumption.

Note that the driving force behind this assumption should be controlled and adjusted based on a fairly accurate guess about the average memory availability, in terms of size, as part of the final setup. Algorithms designed for memory-constrained environments are intrinsically slower, and you should be careful about this trap.

In the upcoming sections, we will cover some techniques that can help us to collect some wasted memory or to use less memory in memory-constrained environments.

### Packed structures

One of the easiest ways to use less memory is to use packed structures. Packed structures discard the memory alignment and they have a more compact memory layout for storing their fields.

Using packed structures is actually a trade-off. You consume less memory because you discard memory alignments and eventually end up with more memory read time while loading a structure variable. This will result in a slower program.

This method is simple but not recommended for all programs. For more information regarding this method, you can read the *Structures* section found in *Chapter 1*, *Essential Features*.

### Compression

This is an effective technique, especially for programs working with a lot of textual data that should be kept inside the memory. Textual data has a high *compression ratio* in comparison to binary data. This technique allows a program to store the compressed form instead of the actual text data with a huge memory return.

However, saving memory is not free; since compression algorithms are *CPU-bound* and computation-intensive, the program would have worse performance in the end. This method is ideal for programs that keep textual data that is not required often; otherwise, a lot of compression/decompression operations are needed, and the program would be almost unusable eventually.

### External data storage

Using external data storage in the forms of a network service, a cloud infrastructure, or simply a hard drive is a very common and useful technique for resolving low memory issues. Since it is usually considered that a program might be run in a limited or low memory environment, there are a lot of examples that use this method to be able to consume less memory even in environments in which enough memory is available.

This technique usually assumes that memory is not the main storage, but it acts as *cache* memory. Another assumption is that we cannot keep the whole data in the memory and at any moment, only a portion of data or a *page* of data can be loaded into the memory.

These algorithms are not directly addressing the low memory problem, but they are trying to solve another issue: slow external data storage. External data storage is always too slow in comparison to the main memory. So, the algorithms should balance the reads from the external data store and their internal memory. All database services, such as PostgreSQL and Oracle, use this technique.

In most projects, it is not very wise to design and write these algorithms from scratch because these algorithms are not that trivial and simple to write. The teams behind famous libraries such as SQLite have been fixing bugs for years.

If you need to access an external data storage such as a file, a database, or a host on the network while having a low memory footprint, there are always options out there for you.

## Performant environments

As we have explained in the previous sections about the time and memory complexities of an algorithm, it is usually expected to consume more memory when you want to have a faster algorithm. In this section, we therefore expect to consume more memory for the sake of increased performance.

An intuitive example of this statement can be using a cache in order to increase the performance. Caching data means consuming more memory, but we could expect to get better performance if the cache is used properly.

But adding extra memory is not always the best way to increase performance. There are other methods that are directly or indirectly related to memory and can have a substantial impact on the performance of an algorithm. Before jumping to these methods, let's talk about caching first.

### Caching

Caching is a general term for all similar techniques utilized in many parts of a computer system when two data storages with different read/write speeds are involved. For example, the CPU has a number of internal registers that perform quickly in terms of reading and writing operations. In addition, the CPU has to fetch data from the main memory, which is many times slower than its registers. A caching mechanism is needed here; otherwise, the lower speed of the main memory becomes dominant, and it hides the high computational speed of the CPU.

Working with database files is another example. Database files are usually stored on an external hard disk, which is far slower than the main memory, by orders of magnitude. Definitely, a caching mechanism is required here; otherwise, the slowest speed becomes dominant, and it determines the speed of the whole system.

Caching and the details around it deserve to have a whole dedicated chapter since there are abstract models and specific terminology that should be explained.

Using these models, one can predict how well a cache would behave and how much *performance gain* could be expected after introducing the cache. Here, we try to explain caching in a simple and intuitive manner.

Suppose that you have slow storage that can contain many items. You also have another fast storage, but it can contain a limited number of items. This is an obvious tradeoff. We can call the faster but smaller storage a *cache*. It would be reasonable if you bring items from the slow storage into the fast one and process them on the fast storage, simply because it is faster.

From time to time, you have to go to slow storage in order to bring over more items. It is obvious that you won't bring only one item over from the slow storage, as this would be very inefficient. Rather, you will bring a *bucket* of items into the faster storage. Usually, it is said that the items are cached into the faster storage.

Suppose that you are processing an item that requires you to load some other item from the slow storage. The first thing that comes to mind is to search for the required item inside the recently brought bucket, which is in the cache storage at the moment.

If you could find the item in the cache, there is no need to retrieve it from the slow storage, and this is called a *hit*. If the item is missing from the cache storage, you have to go to the slow storage and read another bucket of items into the cache memory. This is called a *miss*. It is clear that the more hits you observe, the more performance you get.

The preceding description can be applied to the CPU cache and the main memory. The CPU cache stores recent instructions and data read from the main memory, and the main memory is slow compared to the CPU cache memory.

In the following section, we discuss cache-friendly code, and we observe why cache-friendly code can be executed faster by the CPU.

#### Cache-friendly code

When the CPU is executing an instruction, it has to fetch all required data first. The data is located in the main memory at a specific address that is determined by the instruction.

The data has to be transferred to the CPU registers before any computation. But the CPU usually brings more blocks than are expected to be fetched and puts them inside its cache.

Next time, if a value is needed in the *proximity* of the previous address, it should exist in the cache, and the CPU can use the cache instead of the main memory, which is far faster than reading it from the main memory. As we explained in the previous section, this is a *cache hit*. If the address is not found in the CPU cache, it is a *cache miss*, and the CPU has to access the main memory to read the target address and bring required data which is quite slow. In general, higher hit rates result in faster executions.

But why does the CPU fetch the neighboring addresses (the proximity) around an address? It is because of the *principle of locality*. In computer systems, it is usually observed that the data located in the same neighborhood is more frequently accessed. So, the CPU behaves according to this principle and brings more data from a local reference. If an algorithm can exploit this behavior, it can be executed faster by the CPU. This is why we refer to such algorithm as a *cache-friendly* algorithm.

*Example 5.6* demonstrates the difference between the performances of cache-friendly code and non-cache-friendly code:

```cpp
#include <stdio.h>  // For printf function
#include <stdlib.h> // For heap memory functions
#include <string.h> // For strcmp function
void fill(int* matrix, int rows, int columns) {
  int counter = 1;
  for (int i = 0; i < rows; i++) {
    for (int j = 0; j < columns; j++) {
      *(matrix + i * columns + j) = counter;
    }
    counter++;
  }
}
void print_matrix(int* matrix, int rows, int columns) {
  int counter = 1;
  printf("Matrix:\n");
  for (int i = 0; i < rows; i++) {
    for (int j = 0; j < columns; j++) {
      printf("%d ", *(matrix + i * columns + j));
    }
    printf("\n");
  }
}
void print_flat(int* matrix, int rows, int columns) {
  printf("Flat matrix: ");
  for (int i = 0; i < (rows * columns); i++) {
    printf("%d ", *(matrix + i));
  }
  printf("\n");
}
int friendly_sum(int* matrix, int rows, int columns) {
  int sum = 0;
  for (int i = 0; i < rows; i++) {
    for (int j = 0; j < columns; j++) {
      sum += *(matrix + i * columns + j);
    }
  }
  return sum;
}
int not_friendly_sum(int* matrix, int rows, int columns) {
  int sum = 0;
  for (int j = 0; j < columns; j++) {
    for (int i = 0; i < rows; i++) {
      sum += *(matrix + i * columns + j);
    }
  }
  return sum;
}
int main(int argc, char** argv) {
  if (argc < 4) {
    printf("Usage: %s [print|friendly-sum|not-friendly-sum] ");
    printf("[number-of-rows] [number-of-columns]\n", argv[0]);
    exit(1);
  }
  char* operation = argv[1];
  int rows = atol(argv[2]);
  int columns = atol(argv[3]);
  int* matrix = (int*)malloc(rows * columns * sizeof(int));
  fill(matrix, rows, columns);
  if (strcmp(operation, "print") == 0) {
    print_matrix(matrix, rows, columns);
    print_flat(matrix, rows, columns);
  }
  else if (strcmp(operation, "friendly-sum") == 0) {
    int sum = friendly_sum(matrix, rows, columns);
    printf("Friendly sum: %d\n", sum);
  }
  else if (strcmp(operation, "not-friendly-sum") == 0) {
    int sum = not_friendly_sum(matrix, rows, columns);
    printf("Not friendly sum: %d\n", sum);
  }
  else {
    printf("FATAL: Not supported operation!\n");
    exit(1);
  }
  free(matrix);
  return 0;
}
```

Code Box 5-12 [ExtremeC_examples_chapter5_6.c]: Example 5.6 demonstrates the performance of cache-friendly code and non-cache-friendly code

The preceding program computes and prints the sum of all elements in a matrix, but it also does more than that.

The user can pass options to this program, which alters its behavior. Suppose that we want to print a 2 by 3 matrix that is initialized by an algorithm written in the `fill` function. The user has to pass the `print` option with the desired number of rows and columns. Next, you can see how these options are passed to the final executable binary:

```cpp
$ gcc ExtremeC_examples_chapter5_6.c -o ex5_6.out
$ ./ex5_6.out print 2 3
Matrix:
1 1 1
2 2 2
Flat matrix: 1 1 1 2 2 2
$
```

Shell Box 5-21: Output of example 5.6 showing a 2 by 3 matrix

The output consists of two different prints for the matrix. The first is the 2D representation of the matrix and the second is the *flat* representation of the same matrix. As you can see, the matrix is stored as a *row-major order* in memory. This means that we store it row by row. So, if something from a row is fetched by the CPU, it is probable that all of the elements in that row are fetched too. Hence, it would be better to do our summation in row-major order and not *column-major* order.

If you look at the code again, you can see that the summation done in the `friendly_sum` function is row-major, and the summation performed in the `not_friendly_sum` function is column-major. Next, we can compare the time it takes to perform the summation of a matrix with 20,000 rows and 20,000 columns. As you can see, the difference is very clear:

```cpp
$ time ./ex5_6.out friendly-sum 20000 20000
Friendly sum: 1585447424
real   0m5.192s
user   0m3.142s
sys    0m1.765s
$ time ./ex5_6.out not-friendly-sum 20000 20000
Not friendly sum: 1585447424
real   0m15.372s
user   0m14.031s
sys    0m0.791s
$
```

Shell Box 5-22: Demonstration of the time difference between the column-major and row-major matrix summation algorithms

The difference between the measured times is about 10 seconds! The program is compiled on a macOS machine using the `clang` compiler. The difference means that the same logic, using the same amount of memory, can take much longer – just by selecting a different order of accessing the matrix elements! This example clearly shows the effect of cache-friendly code.

**Note**:

The `time` utility is available in all Unix-like operating systems. It can be used to measure the time a program takes to finish.

Before continuing to the next technique, we should talk a bit more about the allocation and deallocation cost.

### Allocation and deallocation cost

Here, we want to specifically talk about the cost of Heap memory allocation and deallocation. This might be a bit of a surprise if you realize that Heap memory allocation and deallocation operations are time-and memory-consuming and are usually expensive, especially when you need to allocate and deallocate Heap memory blocks many times per second.

Unlike Stack allocation, which is relatively fast and the allocation itself requires no further memory, Heap allocation requires finding a free block of memory with enough size, and this can be costly.

There are many algorithms designed for memory allocation and deallocation, and there is always a tradeoff between the allocation and deallocation operations. If you want to allocate quickly, you have to consume more memory as part of the allocation algorithm and vice versa if you want to consume less memory you can choose to spend more time with a slower allocation.

There are memory allocators for C other than those provided by the default C standard library through the `malloc` and `free` functions. Some of these memory allocator libraries are `ptmalloc`, `tcmalloc`, `Haord`, and `dlmalloc`.

Going through all allocators here is beyond the scope of this chapter, but it would be good practice for you to go through them and give them a try for yourself.

What is the solution to this silent problem? It is simple: allocate and deallocate less frequently. This may seem impossible in some programs that are required to have a high rate of Heap allocations. These programs usually allocate a big block of the Heap memory and try to manage it themselves. It is like having another layer of allocation and deallocation logic (maybe simpler than implementations of `malloc` and `free`) on top of a big block of the Heap memory.

There is also another method, which is using *memory pools*. We'll briefly explain this technique before we come to the end of this chapter.

### Memory pools

As we described in the previous section, memory allocation and deallocation are costly. Using a pool of preallocated fixed-size Heap memory blocks is an effective way to reduce the number of allocations and gain some performance. Each block in the pool usually has an identifier, which can be acquired through an API designed for pool management. Also, the block can be released later when it is not needed. Since the amount of allocated memory remains almost fixed, it is an excellent choice for algorithms willing to have deterministic behavior in memory-constrained environments.

Describing memory pools in further detail is beyond the scope of this book; many useful resources on this topic exist online if you wish to read more about it.

# Summary

As part of this chapter, we mainly covered the Stack and Heap segments and the way they should be used. After that, we briefly discussed memory-constrained environments and we saw how techniques like caching and memory pools can increase the performance.

In this chapter:

*   We discussed the tools and techniques used for probing both Stack and Heap segments.
*   We introduced debuggers and we used `gdb` as our main debugger to troubleshoot memory-related issues.
*   We discussed memory profilers and we used `valgrind` to find memory issues such as leakages or dangling pointers happening at runtime.
*   We compared the lifetime of a Stack variable and a Heap block and we explained how we should judge the lifetime of such memory blocks.
*   We saw that memory management is automatic regarding Stack variables, but it is fully manual with Heap blocks.
*   We went through the common mistakes that happen when dealing with Stack variables.
*   We discussed the constrained environments and we saw how memory tuning can be done in these environments.
*   We discussed the performant environments and what techniques can be used to gain some performance.

The next four chapters together cover object orientation in C. This might at first glance seem to be unrelated to C, but in fact, this is the correct way to write object-oriented code in C. As part of these chapters, you will be introduced to the proper way of designing and solving a problem in an object-oriented fashion, and you will get guidance through writing readable and correct C code.

The next chapter covers encapsulation and the basics of object-oriented programming by providing the required theoretical discussion and examples to explore the topics discussed.