<html><head></head><body>
		<div id="_idContainer048">
			<h1 id="_idParaDest-762" class="chapter-number"><a id="_idTextAnchor762"/>22</h1>
			<h1 id="_idParaDest-763"><a id="_idTextAnchor763"/>Parallel Algorithms with  the STL</h1>
			<p>This chapter covers the topic of C++ parallelism, particularly with the tools and techniques introduced in C++17. Starting with the foundations, the chapter unfolds the power of execution policies that allow developers to harness parallel processing in their C++ <strong class="bold">Standard Template Library</strong> (<span class="No-Break"><strong class="bold">STL</strong></span><span class="No-Break">) algorithms.</span></p>
			<p>We will cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Introduction to <span class="No-Break">execution policies</span></li>
				<li>Incorporating <span class="No-Break">execution policies</span></li>
				<li>The impact of <strong class="source-inline">constexpr</strong> on algorithms <span class="No-Break">and containers</span></li>
				<li><span class="No-Break">Performance considerations</span></li>
			</ul>
			<h1 id="_idParaDest-764"><a id="_idTextAnchor764"/>Technical requirements</h1>
			<p>The code in this chapter can be found <span class="No-Break">on GitHub:</span></p>
			<p><a href="https://github.com/PacktPublishing/Data-Structures-and-Algorithms-with-the-CPP-STL"><span class="No-Break">https://github.com/PacktPublishing/Data-Structures-and-Algorithms-with-the-CPP-STL</span></a></p>
			<h1 id="_idParaDest-765"><a id="_idTextAnchor765"/>Introduction to execution policies</h1>
			<p>Processors<a id="_idIndexMarker1149"/> have transitioned from focusing on increasing the speed of individual cores to incorporating multiple cores for enhanced performance. For developers, this means the potential to execute multiple instructions concurrently across these cores, improving application efficiency <span class="No-Break">and responsiveness.</span></p>
			<p>This move to multi-core configurations highlights the importance of integrating parallel programming techniques. With the advent of C++17, C++ made notable progress in this domain by introducing the <strong class="source-inline">&lt;</strong><span class="No-Break"><strong class="source-inline">execution&gt;</strong></span><span class="No-Break"> header.</span></p>
			<h2 id="_idParaDest-766"><a id="_idTextAnchor766"/>The &lt;execution&gt; header– enabling parallelism in STL algorithms</h2>
			<p>Before C++17, although <a id="_idIndexMarker1150"/>the STL provided a comprehensive suite of algorithms, they were executed sequentially. This sequential operation meant that STL algorithms did not fully utilize the capabilities of <span class="No-Break">multi-core processors.</span></p>
			<p>The <strong class="source-inline">&lt;execution&gt;</strong> header<a id="_idIndexMarker1151"/> addresses this limitation. Instead of adding new algorithms, it enhances existing ones by incorporating parallelism by introducing <span class="No-Break">execution policies.</span></p>
			<p>Execution policies serve as directives, indicating to the STL algorithms the desired mode of operation: sequential, parallel, or vectorized. With the <strong class="source-inline">&lt;execution&gt;</strong> header, developers can specify <span class="No-Break">these preferences.</span></p>
			<p>The primary execution policies include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">std::execution::seq</strong>: Dictates a sequential execution of <span class="No-Break">the algorithm</span></li>
				<li><strong class="source-inline">std::execution::par</strong>: Facilitates parallel execution <span class="No-Break">where feasible</span></li>
				<li><strong class="source-inline">std::execution::par_unseq</strong>: Supports both parallel and <span class="No-Break">vectorized execution</span></li>
			</ul>
			<h2 id="_idParaDest-767"><a id="_idTextAnchor767"/>Implementing parallel execution</h2>
			<p>Integrating parallelism <a id="_idIndexMarker1152"/>into STL algorithms is straightforward. Consider the <strong class="source-inline">std::sort</strong> algorithm. Typically, it’s used in the <span class="No-Break">following manner:</span></p>
			<pre class="source-code">
std::sort(begin(vec), end(vec));</pre>			<p>To employ parallel sorting with the <strong class="source-inline">&lt;execution&gt;</strong> header, the syntax is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
std::sort(std::execution::par, begin(vec), end(vec));</pre>			<p>This modification equips the <strong class="source-inline">sort</strong> algorithm to leverage multiple cores, potentially enhancing the speed of the <span class="No-Break">sorting process.</span></p>
			<h2 id="_idParaDest-768"><a id="_idTextAnchor768"/>Reflecting on the transition to parallel STL</h2>
			<p>While<a id="_idIndexMarker1153"/> introducing the <strong class="source-inline">&lt;execution&gt;</strong> header and its associated execution policies is a notable advancement, it’s essential to approach their usage with discernment. Parallelism does introduce overheads, such as thread context-switching and data coordination. These overheads can sometimes negate the benefits of parallelism, especially for tasks with <span class="No-Break">smaller datasets.</span></p>
			<p>However, when used judiciously, the <strong class="source-inline">&lt;execution&gt;</strong> header can significantly enhance application performance. Subsequent sections will provide a more detailed exploration of execution policies, enabling developers to utilize <span class="No-Break">them effectively.</span></p>
			<p>In summary, C++17’s <strong class="source-inline">&lt;execution&gt;</strong> header is a pivotal enhancement. Offering a mechanism to imbue existing STL algorithms with parallel capabilities equips developers with the tools to develop applications optimized for <span class="No-Break">multi-core generation.</span></p>
			<h2 id="_idParaDest-769"><a id="_idTextAnchor769"/>Incorporating execution policies</h2>
			<p>The <strong class="source-inline">&lt;execution&gt;</strong> header, introduced in the C++17 standard, adds a significant layer of depth <a id="_idIndexMarker1154"/>to C++ programming by furnishing a suite of tools designed for parallel computation. This header, when used in conjunction with the STL algorithms, allows developers to leverage the capabilities of concurrent <span class="No-Break">computing effectively.</span></p>
			<p>Execution policies, a key feature of the <strong class="source-inline">&lt;execution&gt;</strong> header, are instrumental in controlling the manner in which STL algorithms execute. By specifying an execution policy when invoking an STL algorithm, developers can dictate whether the algorithm should run sequentially, in parallel, or in parallel with vectorization. This level of control can lead to substantial performance improvements, particularly in applications that are computationally intensive or that operate on <span class="No-Break">large datasets.</span></p>
			<p>In essence, the <strong class="source-inline">&lt;execution&gt;</strong> header and its associated execution policies represent a powerful toolset for C++ developers. They provide a means to tap into the potential of modern multi-core processors and distributed computing environments, thereby enabling more efficient and faster <span class="No-Break">code execution.</span></p>
			<h2 id="_idParaDest-770"><a id="_idTextAnchor770"/>Integrating policies with standard algorithms</h2>
			<p>Execution policies<a id="_idIndexMarker1155"/> serve as directives for STL algorithms, indicating the preferred mode of operation. For those familiar with STL algorithms, integrating these policies requires minimal modification to <span class="No-Break">existing code.</span></p>
			<p>Consider <strong class="source-inline">std::for_each</strong>, an algorithm that acts on each element in a collection. By default, it <span class="No-Break">operates sequentially:</span></p>
			<pre class="source-code">
std::for_each(std::begin(vec), std::end(vec), [](int val) { /*...*/ });</pre>			<p>For large datasets or computationally demanding operations within the lambda function, parallel execution can be beneficial. This can be achieved by simply introducing an <span class="No-Break">execution policy:</span></p>
			<pre class="source-code">
std::for_each(std::execution::par, std::begin(vec), std::end(vec), [](int val) { /*...*/ });</pre>			<p>With the inclusion of <strong class="source-inline">std::execution::par</strong>, the algorithm is now prepared for <span class="No-Break">parallel execution.</span></p>
			<h2 id="_idParaDest-771"><a id="_idTextAnchor771"/>Understanding parallel execution policies</h2>
			<p>There are <a id="_idIndexMarker1156"/>two primary parallel <span class="No-Break">execution</span><span class="No-Break"><a id="_idIndexMarker1157"/></span><span class="No-Break"> policies:</span></p>
			<ul>
				<li><strong class="source-inline">std::execution::par</strong>: This indicates that an algorithm may be executed in parallel. It allows the implementation to decide on parallelism based on the <span class="No-Break">specific context.</span></li>
				<li><strong class="source-inline">std::execution::par_unseq</strong>: This goes further by suggesting parallelism and allowing for vectorization. This means that multiple loop iterations might execute concurrently on a single processor core when supported by <span class="No-Break">the hardware.</span></li>
			</ul>
			<p>For instance, the <strong class="source-inline">std::transform</strong> algorithm, which applies a function to each collection element, can utilize <span class="No-Break">these policies:</span></p>
			<pre class="source-code">
std::transform(std::execution::par_unseq, std::begin(vec), std::end(vec), std::begin(output), [](int val) { return val * val; });</pre>			<p>Each <strong class="source-inline">vec</strong> element is squared, and the result populates the output. The <strong class="source-inline">std::execution::par_unseq</strong> policy indicates the potential parallelization and vectorization <a id="_idIndexMarker1158"/>of <span class="No-Break">this operation.</span></p>
			<h2 id="_idParaDest-772"><a id="_idTextAnchor772"/>Selecting the appropriate execution policy</h2>
			<p>While<a id="_idIndexMarker1159"/> execution policies enhance parallel computation capabilities, they must be applied thoughtfully. Not every dataset or algorithm will gain from parallel execution, and sometimes, the overhead might negate the advantages for <span class="No-Break">smaller datasets.</span></p>
			<p>The <strong class="source-inline">std::execution::seq</strong> policy explicitly opts for sequential execution, ensuring the algorithm operates in a single-threaded mode. This is beneficial when parallelism introduces undue overhead or in contexts where parallel execution is <span class="No-Break">not recommended.</span></p>
			<p>It’s also vital to be wary of potential issues when utilizing parallel policies with algorithms that possess side effects or <span class="No-Break">necessitate synchronization.</span></p>
			<p>C++17’s execution policies facilitate straightforward access to parallelism. Pairing these with traditional STL algorithms allows developers to use multi-core processors optimally. Whether utilizing <strong class="source-inline">std::transform</strong> on a vast dataset, sorting large collections with <strong class="source-inline">std::sort</strong>, or filtering items using <strong class="source-inline">std::remove_if</strong>, execution policies provide an added <span class="No-Break">performance dimension.</span></p>
			<p>However, always validate that parallel execution genuinely augments your application without ushering in unforeseen challenges or bottlenecks. It’s imperative to evaluate and test your <span class="No-Break">code continually.</span></p>
			<p>With this foundation, we’re poised to consider performance considerations in the upcoming section. Through discerning the application of parallelism, we can develop efficient C++ applications tailored to contemporary <span class="No-Break">computational demands.</span></p>
			<h1 id="_idParaDest-773"><a id="_idTextAnchor773"/>The impact of constexpr on algorithms and containers</h1>
			<p>With the <a id="_idIndexMarker1160"/>introduction of the <strong class="source-inline">constexpr</strong> specifier in C++11 and its enhancements in subsequent versions, compile-time computation in C++ has taken a significant leap. The ability for functions and constructors to operate at compile time via <strong class="source-inline">constexpr</strong> enables optimization and assurance of specific attributes before the code runs. This section examines the integration of <strong class="source-inline">constexpr</strong> within the STL, particularly concerning algorithms <span class="No-Break">and containers.</span></p>
			<h2 id="_idParaDest-774"><a id="_idTextAnchor774"/>Evolution of constexpr</h2>
			<p>In its <a id="_idIndexMarker1161"/>infancy during C++11, <strong class="source-inline">constexpr</strong> was primarily for straightforward computations. The C++14 extension broadened its scope, embracing loops and conditional structures. By C++20, there was further augmentation allowing for <strong class="source-inline">constexpr</strong> allocations via <strong class="source-inline">std::allocator</strong>. This made containers such as <strong class="source-inline">std::vector</strong> and <strong class="source-inline">std::string</strong> usable with <strong class="source-inline">constexpr</strong>, though with <span class="No-Break">certain restrictions.</span></p>
			<h2 id="_idParaDest-775"><a id="_idTextAnchor775"/>Algorithms and the role of constexpr</h2>
			<p>Originally, <strong class="source-inline">constexpr</strong> wasn’t widely applicable to STL algorithms due to their generic design <a id="_idIndexMarker1162"/>and multifaceted requirements. However, with the C++20 standard, more STL algorithms became <strong class="source-inline">constexpr</strong>-compatible. This means that provided all inputs are constant expressions, it is possible to compute algorithmic outcomes at <span class="No-Break">compile time.</span></p>
			<p>Take, for example, the <strong class="source-inline">std::find</strong> or <strong class="source-inline">std::count</strong> functions. When used on static data structures such as arrays or <strong class="source-inline">std::array</strong>, they can operate during the compilation phase. However, as of C++20, dynamic allocation remains mainly outside the domain <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">constexpr</strong></span><span class="No-Break">.</span></p>
			<p>The following code snippet uses <strong class="source-inline">std::array</strong> to highlight the use of <strong class="source-inline">std::find</strong> and <strong class="source-inline">std::count</strong> <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">constexpr</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
#include &lt;algorithm&gt;
#include &lt;array&gt;
#include &lt;iostream&gt;
constexpr std::array&lt;int, 6&gt; data = {1, 2, 3, 4, 3, 5};
constexpr bool contains(int value) {
  return std::find(data.begin(), data.end(), value) !=
         data.end();
}
constexpr size_t countOccurrences(int value) {
  return std::count(data.begin(), data.end(), value);
}
int main() {
  static_assert(contains(3));
  static_assert(countOccurrences(3) == 2);
  std::cout &lt;&lt; "Array contains 3: " &lt;&lt; contains(3) &lt;&lt; "\n";
  std::cout &lt;&lt; "Occurrences of 3: " &lt;&lt; countOccurrences(3)
            &lt;&lt; "\n";
  return 0;
}</pre>			<p>Here is<a id="_idIndexMarker1163"/> the <span class="No-Break">example output:</span></p>
			<pre class="console">
Array contains 3: 1
Occurrences of 3: 2</pre>			<p>The <strong class="source-inline">contains</strong> and <strong class="source-inline">countOccurrences</strong> functions in the preceding code are evaluated <a id="_idIndexMarker1164"/>at compile time because they operate on a <strong class="source-inline">constexpr</strong>-compatible <strong class="source-inline">std::array</strong>, and all their inputs are <span class="No-Break">constant expressions.</span></p>
			<p>It’s worth noting that parallel algorithms using execution policies such as <strong class="source-inline">std::execution::par</strong> are not suitable for <strong class="source-inline">constexpr</strong> contexts due to their inherent reliance on <span class="No-Break">runtime resources.</span></p>
			<h2 id="_idParaDest-776"><a id="_idTextAnchor776"/>Containers and constexpr integration</h2>
			<p>C++20’s capability<a id="_idIndexMarker1165"/> for compile-time allocations enabled specific STL containers to function within a <strong class="source-inline">constexpr</strong> environment. While <strong class="source-inline">std::array</strong> was always compatible, even some operations on <strong class="source-inline">std::vector</strong> and <strong class="source-inline">std::string</strong> became feasible. Nonetheless, any operation requiring dynamic memory or leading to undefined behavior will result in a compile-time error within a <span class="No-Break"><strong class="source-inline">constexpr</strong></span><span class="No-Break"> context.</span></p>
			<p>The trajectory of <strong class="source-inline">constexpr</strong> indicates an evolving C++ environment where the lines between compile-time and runtime evaluation become increasingly indistinct. We might soon see more advanced algorithms and containers being evaluated entirely during compilation, optimizing performance and <span class="No-Break">code safety.</span></p>
			<p>However, the convergence of <strong class="source-inline">constexpr</strong> and parallel algorithms remains an uncertain prospect due to the fundamental runtime nature <span class="No-Break">of parallelism.</span></p>
			<p>In summary, <strong class="source-inline">constexpr</strong> has undeniably reshaped C++ development. As it integrates more deeply into the STL, developers have more avenues to refine and solidify <span class="No-Break">their applications.</span></p>
			<h1 id="_idParaDest-777"><a id="_idTextAnchor777"/>Performance considerations</h1>
			<p>Parallel algorithms <a id="_idIndexMarker1166"/>are a cornerstone in exploiting the capabilities of multi-core processors, aiming to enhance computational efficiency and performance. However, the journey from sequential to parallel programming is not straightforward. It requires a deep understanding of the inherent complexities and trade-offs. In this section, we will explore the various facets of parallel algorithms, including their potential for performance improvement, the challenges of parallel execution, optimal data sizing for parallelism, synchronization issues, and the subtleties of balancing <a id="_idIndexMarker1167"/>workloads across threads. This comprehensive overview will provide a deeper insight into the effective utilization of parallel algorithms, underlining the importance of informed decision-making and profiling in achieving optimal performance in a parallel <span class="No-Break">computing environment.</span></p>
			<p>Parallel algorithms present both opportunities and challenges for performance enhancement. While they offer the potential for faster computations in multi-core processing environments, their practical use requires careful consideration <span class="No-Break">and decision-making.</span></p>
			<h2 id="_idParaDest-778"><a id="_idTextAnchor778"/>Parallelism overhead</h2>
			<p>As<a id="_idIndexMarker1168"/> developers experiment with parallel solutions, it’s essential to understand that parallel execution doesn’t uniformly benefit all algorithms or scenarios. There can be overheads, such as those associated with initiating multiple threads and data synchronization. For example, for small datasets, the overhead of thread management can surpass the computation time, making parallelism <span class="No-Break">less efficient.</span></p>
			<h2 id="_idParaDest-779"><a id="_idTextAnchor779"/>Determining optimal data size</h2>
			<p>Parallel execution <a id="_idIndexMarker1169"/>reveals its benefits beyond a specific data size threshold. This threshold is influenced by factors such as the algorithm employed, the computation’s nature, and the hardware specifications. A resource-intensive task with a large dataset is typically well-suited for parallelism, whereas smaller datasets might be more efficiently <span class="No-Break">processed sequentially.</span></p>
			<p>Understanding the data and computation type is critical to optimize performance. Profiling becomes invaluable, helping developers evaluate their code’s runtime behavior and decide when to <span class="No-Break">employ parallelism.</span></p>
			<h2 id="_idParaDest-780"><a id="_idTextAnchor780"/>Data access and synchronization challenges</h2>
			<p>Concurrency<a id="_idIndexMarker1170"/> leads to scenarios where multiple threads access the same resources concurrently. Data contention can arise, especially with frequent shared data access. Implementing proper synchronization is vital to prevent data inconsistencies. However, synchronization has its <span class="No-Break">associated overheads.</span></p>
			<h2 id="_idParaDest-781"><a id="_idTextAnchor781"/>False sharing – a subtle performance issue</h2>
			<p>Even if <a id="_idIndexMarker1171"/>threads access distinct data, false sharing can still occur. This happens when threads on different cores modify variables on the same cache line, leading to cache invalidations and potential performance degradation. It’s crucial to be mindful of data layout and aim for <span class="No-Break">cache-optimized code.</span></p>
			<h2 id="_idParaDest-782"><a id="_idTextAnchor782"/>Load balancing</h2>
			<p>Different <a id="_idIndexMarker1172"/>computational tasks may require varying processing times. If threads finish their tasks at different rates, it can result in resource underutilization. Practical parallel algorithms ensure that workloads are distributed uniformly across threads. Some advanced parallel techniques, such as work stealing, can dynamically reallocate tasks to maintain consistent <span class="No-Break">thread engagement.</span></p>
			<h2 id="_idParaDest-783"><a id="_idTextAnchor783"/>The importance of profiling</h2>
			<p>A consistent theme<a id="_idIndexMarker1173"/> in performance optimization is the essential role of profiling. Relying solely on assumptions is not advisable. Profiling tools such as <strong class="source-inline">perf</strong> and <strong class="source-inline">gprof</strong> and advanced tools such as Intel® VTune™ can identify performance bottlenecks, thread behaviors, and contention areas. These tools provide concrete data to fine-tune <span class="No-Break">parallel strategies.</span></p>
			<p>Throughout this section, we reviewed the performance considerations when working with parallel algorithms. We learned that while parallel algorithms can significantly enhance computational efficiency, their effective use requires a nuanced understanding of various factors. We discussed the potential overheads associated with parallel execution, such as thread initiation and data synchronization. We also highlighted the importance of determining the optimal data size for parallel execution, emphasizing that parallelism may not be beneficial for all scenarios, particularly those involving small datasets. We further explored the challenges of data access and synchronization in a concurrent environment, including the issue of false sharing. We also touched upon the concept of load balancing, explaining how uneven distribution of computational tasks can lead to resource underutilization. We discussed advanced techniques such as work stealing that can help maintain consistent thread engagement by dynamically <span class="No-Break">reallocating tasks.</span></p>
			<p>The <a id="_idIndexMarker1174"/>insights gained from this section are invaluable as they equip us with the knowledge to make informed decisions when implementing parallel algorithms. Understanding these performance considerations allows us to exploit the full potential of multi-core processors while avoiding common pitfalls. This knowledge is crucial in today’s multi-core processing environment, enabling us to write more efficient and performant code. It also sets the stage for our continued exploration of data structures and algorithms with the C++ STL, as we strive to deepen our understanding and enhance our <span class="No-Break">programming skills.</span></p>
			<h1 id="_idParaDest-784"><a id="_idTextAnchor784"/>Summary</h1>
			<p>This chapter introduced parallel algorithms within the STL. We began by acquainting ourselves with the <strong class="source-inline">&lt;execution&gt;</strong> header introduced in C++17, which has been pivotal in enabling parallelism in STL algorithms. This addition allows us to specify execution policies such as <strong class="source-inline">std::execution::seq</strong>, <strong class="source-inline">std::execution::par</strong>, and <strong class="source-inline">std::execution::par_unseq</strong>, thereby dictating the execution mode of <span class="No-Break">STL algorithms.</span></p>
			<p>We progressed to implementing these execution policies in standard algorithms, demonstrating the simplicity of transitioning from sequential to parallel execution. This was exemplified by adapting algorithms such as <strong class="source-inline">std::sort</strong> and <strong class="source-inline">std::for_each</strong> to run in parallel, thus harnessing the computational power of <span class="No-Break">multiple cores.</span></p>
			<p>The chapter then focused on the <strong class="source-inline">constexpr</strong> specifier and its profound impact on STL algorithms and containers. We explored the evolution of <strong class="source-inline">constexpr</strong> from C++11 through C++20 and its role in enabling compile-time computations for algorithms such as <strong class="source-inline">std::find</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">std::count</strong></span><span class="No-Break">.</span></p>
			<p>Performance considerations formed the crux of our final discussion, underscoring the benefits and potential pitfalls of employing parallel algorithms. We addressed the overheads associated with parallelism, the importance of determining optimal data size, and strategies for effective data access and synchronization to avoid issues such as false sharing and <span class="No-Break">load imbalance.</span></p>
			<p>The information imparted in this chapter is invaluable for leveraging the STL’s capabilities in a multi-core processing environment. We can write more efficient and responsive code by understanding when and how to apply parallel algorithms. This deepened comprehension of parallel execution policies and the ability to optimize code with <strong class="source-inline">constexpr</strong> equips us to maximize performance and <span class="No-Break">resource utilization.</span></p>
		</div>
	</body></html>