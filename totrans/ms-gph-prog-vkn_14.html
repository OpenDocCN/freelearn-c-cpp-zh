<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-208"><a id="_idTextAnchor241"/>14</h1>
<h1 id="_idParaDest-209"><a id="_idTextAnchor242"/>Adding Dynamic Diffuse Global Illumination with Ray Tracing</h1>
<p>So far in this book, illumination has been based on direct lighting coming from point lights. In this chapter, we will enhance lighting by adding indirect lighting, often referred to as global illumination in the context of video games.</p>
<p>This type of illumination comes from emulating the behavior of light. Without going into quantum physics and optics, the information we need to consider is that light bounces off surfaces a few times until its energy becomes zero.</p>
<p>Throughout movies and video games, global illumination has always been an important aspect of lighting, but often impossible to perform in real time.</p>
<p>With movies, it often took minutes (if not hours) to render a single frame, until global illumination was pioneered. Video games were inspired by this and now include it in their lighting.</p>
<p>In this chapter, we will discover how to implement real-time global illumination by covering these topics:</p>
<ul>
<li>Introduction to indirect lighting</li>
<li>Introduction to <strong class="bold">Dynamic Diffuse Global </strong><strong class="bold">Illumination</strong> (<strong class="bold">DDGI</strong>)</li>
<li>Implementing DDGI</li>
</ul>
<p>Each topic will contain subsections so that you can expand upon the knowledge provided.</p>
<p>The following figure shows how the code from this chapter helps contribute to indirect lighting:</p>
<div><div><img alt="Figure 14.1 – Indirect lighting output" height="629" src="img/B18395_14_01.jpg" width="1101"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1 – Indirect lighting output</p>
<p>In <em class="italic">Figure 14</em><em class="italic">.1</em>, the scene has a point light on the left. We can see the green color from the light bouncing off the left curtain onto the floor and the right pillars and curtains.</p>
<p>On the floor in the distance, we can see the color of the sky tinting the walls. The occlusion given by its visibility provides a very low light contribution to the arches.</p>
<h1 id="_idParaDest-210"><a id="_idTextAnchor243"/>Technical requirements</h1>
<p>The code for this chapter can be found at the following URL: <a href="https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter14">https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter14</a>.</p>
<h1 id="_idParaDest-211"><a id="_idTextAnchor244"/>Introduction to indirect lighting</h1>
<p>Going back to direct <a id="_idIndexMarker721"/>and indirect lighting, direct lighting just shows the first interaction between light and matter, but light continues to travel in space, bouncing at times.</p>
<p>From a rendering perspective, we use the G-buffer information to calculate the first light interaction with surfaces that are visible from our point of view, but we have little data on what is outside of our view.</p>
<p>The following diagram shows direct lighting:</p>
<div><div><img alt="Figure 14.2 – Direct lighting" height="1234" src="img/B18395_14_02.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2 – Direct lighting</p>
<p><em class="italic">Figure 14</em><em class="italic">.2</em> describes <a id="_idIndexMarker722"/>the current lighting setup. There are light-emitting rays, and those rays interact with surfaces. Light bounces off these surfaces and is captured by the camera, becoming the pixel color. This is an extremely simplified vision of the phenomena, but it contains all the basics we need.</p>
<p>For indirect lighting, relying only on the camera’s point of view is insufficient as we need to calculate how other lights and geometries can contribute and still affect the visible part of the scene but are outside of the view, as well as the visible surfaces.</p>
<p>For this matter, <strong class="bold">ray tracing</strong> is<a id="_idIndexMarker723"/> the best tool: it’s a way to query the scene spacially as we can use it to calculate how different bounces of light contribute to the final value of a given fragment.</p>
<p>Here is a diagram showing indirect lighting:</p>
<div><div><img alt="Figure 14.3 – Indirect lighting" height="1035" src="img/B18395_14_03.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.3 – Indirect lighting</p>
<p><em class="italic">Figure 14</em><em class="italic">.3</em> shows indirect rays bouncing off surfaces until they hit the camera again.</p>
<p>There are two rays highlighted in this figure:</p>
<ul>
<li><strong class="bold">Indirect Ray 0</strong>, bouncing off a hidden surface onto the blue floor and finally into the camera</li>
<li><strong class="bold">Indirect Ray 0</strong>, bouncing off another surface and bouncing off the red wall, and finally into the camera</li>
</ul>
<p>With indirect <a id="_idIndexMarker724"/>illumination, we want to capture the phenomena of rays of light bouncing off surfaces, both hidden and not.</p>
<p>For example, in this setup, there are some rays between the red and blue surfaces that will bounce within each other, tinting the closer parts of the surfaces of the respective colors.</p>
<p>Adding indirect illumination to lighting enhances the realism and visual quality of the image, but how can we achieve that?</p>
<p>In the next section, we will<a id="_idIndexMarker725"/> talk about the implementation that we chose: <strong class="bold">Dynamic Diffuse Global Illumination</strong>, or <strong class="bold">DDGI</strong>, which was developed mainly by researchers at Nvidia but is rapidly becoming one of the most used solutions in AAA games.</p>
<h1 id="_idParaDest-212"><a id="_idTextAnchor245"/>Introduction to Dynamic Diffuse Global Illumination (DDGI)</h1>
<p>In this section, we<a id="_idIndexMarker726"/> will explain the algorithm behind DDGI. DDGI is <a id="_idIndexMarker727"/>based on two main tools: light probes and irradiance volumes:</p>
<ul>
<li><strong class="bold">Li<a id="_idTextAnchor246"/>ght probes</strong> are points in space, represented as spheres, that encode light information</li>
<li><strong class="bold">Irradiance volumes</strong> are defined as spaces that contain three-dimensional grids of light probes with fixed spacing between them</li>
</ul>
<p>Sampling is easier when the layout is regular, even though we will see some improvements to placements later. Probes are encoded using octahedral mapping, a convenient way to map a square to a sphere. Links to the math behind octahedral mapping have been provided in the <em class="italic">Further </em><em class="italic">reading</em> section.</p>
<p>The core idea behind DDGI is to dynamically update probes using ray tracing: for each probe, we will cast some rays and calculate the radiance at the triangle intersection. Radiance is calculated with the dynamic lights present in the engine, reacting in real time to any light or geometry chang<a id="_idTextAnchor247"/>es.</p>
<p>Given the low resolution of the grid compared to the pixels on the screen, the only lighting phenomenon possible is diffuse lighting. The following diagram provides an overview of the algorithm, showing the relationships and the sequences between shaders (green rectangles) and textures (yellow ellipses):</p>
<div><div><img alt="Figure 14.4 – Algorithm overview" height="1106" src="img/B18395_14_04.jpg" width="1615"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.4 – Algorithm overview</p>
<p>Let’s provide a quick <a id="_idIndexMarker728"/>overview of the algorithm before looking at each step in detail:</p>
<ol>
<li value="1">Perform ray tracing for each probe and calculate the radiance and distance.</li>
<li>Update the irradiance of all probes with the radiance calculated while applying some hysteresis.</li>
<li>Update the visibility data of all probes with the distance calculated in the ray tracing pass, again with some hysteresis.</li>
<li>(Optional) Calculate the per-probe offset position using the ray tracing distance.</li>
<li>Calculate indirect lighting by reading the updated irradiance, visibility, and probe offsets.</li>
</ol>
<p>In the following<a id="_idTextAnchor248"/> subsections, we will cover each step of the algorithm.</p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor249"/>Ray tracing for each probe</h2>
<p>This is the first<a id="_idIndexMarker729"/> step of the <a id="_idIndexMarker730"/>algorithm. For each ray of each probe that needs an update, we must ray trace the scene using dynamic lighting.</p>
<p>In the ray tracing hit shader, we calculate the world position and normal of the hit triangle and perform a simplified diffuse lighting calculation. Optionally, but more expensive, we can read the other irradiance probes to add an infinite number of bounces to the lighting calculation, giving it an even more realistic <a id="_idTextAnchor250"/>look.</p>
<p>Especially important here is the texture layout: each row represents the rays for a single probe. So, if we have 128 rays per probe, we will have a row of 128 texels, while each column represents a probe.</p>
<p>Thus, a configuration with 128 rays and 24 probes will result in a 128x24 texture dimension. We store the lighting calculation as radiance in the RGB channels of the texture, and the hit distance in the Alpha channel.</p>
<p>Hit distance will be used to help with light leaks and calculating probe offsets.</p>
<h2 id="_idParaDest-214"><a id="_idTextAnchor251"/>Probes offsetting</h2>
<p>Probes offsetting<a id="_idIndexMarker731"/> is <a id="_idIndexMarker732"/>a step that’s done when an irradiance volume is loaded into the world, or its properties are changed (such as spacing or position). Using the hit distances from the ray tracing step, we can calculate if a probe is placed straight into a surface and then create an offset for it.</p>
<p>The offsetting<a id="_idTextAnchor252"/> amount cannot be bigger than half the distance to other probes so that the grid still maintains some coherency between the grid indices and their position. This step is only done a few times (normally, around <a id="_idTextAnchor253"/>five is a suitable number) as having it run continuously will indefinitely move the probes, thus causing light flickering.</p>
<p>Once the offsets have been calculated, every probe will have the final world position, drastically increasing the visual quality of indirect lighting.</p>
<p>Here, we can see the improvement after calculating these offsets:</p>
<div><div><img alt=" Figure 14.5 – Global illumination with (left) and without (right) probe offsets" height="544" src="img/B18395_14_05.jpg" width="1105"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 14.5 – Global illumination with (left) and without (right) probe offsets</p>
<p>As you can see, the probes that are inside a geometry not only give no lighting contribution<a id="_idIndexMarker733"/> to the<a id="_idIndexMarker734"/> sampling but can create visual artifacts.</p>
<p>Thanks to probe offsetting, we can place probes in a better position.</p>
<h2 id="_idParaDest-215"><a id="_idTextAnchor254"/>Probes irradiance and visibility updates</h2>
<p>We now have<a id="_idIndexMarker735"/> the result of each ray that’s been traced for each probe with dynamic lighting applied. How can we encode this information? As seen in the <em class="italic">Introduction to Dynamic Diffuse Global Illumination (DDGI)</em> section, one of the ways is to use octahedral <a id="_idTextAnchor255"/>mapping, which unwraps a sphere into a rectangle.</p>
<p>Given that we are storing each probe’s radiance as a 3D volume, we need a texture that contains a rectangle for each probe. We will choose to create a single texture with a row that contains a <em class="italic">layer</em> of probes as MxN, while the height contains the other layers.</p>
<p>For example, if we have a grid of 3x2x4 probes, each row will contain 6 probes (3x2) and the final texture will have 4 rows. We will execute this step two times, one to update the irradiance from the radiance, and the other to update the visibility from the distance of each probe.</p>
<p>Visibility is crucial for minimizing ligh<a id="_idTextAnchor256"/>t leaks, and irradiance and visibility are stored in different textures and can have different sizes.</p>
<p>One thing to be aware of is that to add support for bilinear filtering, we need to store an additional 1-pixel border around each rectangle; this will be updated here <a id="_idTextAnchor257"/>as well.</p>
<p>The shader will read the new radiance and distances calculated and the previous frame’s irradiance and visibility textures to blend the values to avoid flickering, as Volumetric Fog does with temporal reprojection, using a simple hysteresis.</p>
<p>Hysteresis can be changed dynamically if the lighting conditions change drastically to counteract slow updates using hysteresis. The results will normally be slower to react to light movements, but it is a drawback needed to avoid flickering.</p>
<p>The last part of the shader involves updating the borders for bilinear filtering. Bilinear filtering requires<a id="_idIndexMarker736"/> samples to be read in a specific order, as highlighted in the following diagram:</p>
<div><div><img alt="Figure 14.6 – Bilinear filtering samples. The outer grid copies pixels from the written pixel positions inside each rectangle" height="896" src="img/B18395_14_06.jpg" width="896"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.6 – Bilinear filtering samples. The outer grid copies pixels from the written pixel positions inside each rectangle</p>
<p><em class="italic">Figure 14</em><em class="italic">.6</em> shows the coordinate calculations for copying pixels: the center area is the one that did the full irradiance/visibility update, while the borders copy the values from the pixels at the specified coordinates.</p>
<p>We will run two different shaders – one to update probe irradiance and one to update probe visibility.</p>
<p>In the shader code, we will see the actual code to do this. We are now ready to sample the irradiance<a id="_idIndexMarker737"/> of the probes, as seen in the next subsection.</p>
<h2 id="_idParaDest-216">Probes<a id="_idTextAnchor258"/> sampling</h2>
<p>This step involves<a id="_idIndexMarker738"/> reading the irradiance probes and calculating the indirect lighting contribution. We will render from the main camera’s point of view, and we will sample the eight closest probes given a world position and <a id="_idTextAnchor259"/>direction. The visibility texture is used to minimize leakage and soften the lighting results.</p>
<p>Given the soft lighting nature of diffuse indirect components and to obtain better performance, we have opted to sample this at a quarter resolution, so we need to take extra care of where we sample to avoid pixel inaccuracies.</p>
<p>While looking at probe ray tracing, irradiance updates, visibility updates, probe offsetting, and probe sampling, we described all the basic steps necessary to have a working DDGI implementation.</p>
<p>Other steps can be included to make the rendering even faster, such as using the distances to calculate inactive probes. Other extensions can also be included, such as those that contain a cascade of volumes and hand-placed volumes that give DDGI the best flexibility needed to be used in video games, where different hardware configurations can dictate algorithmic choices.</p>
<p>In the next section, we will learn how to implement DDGI.</p>
<h1 id="_idParaDest-217"><a id="_idTextAnchor260"/>Implementing DDGI</h1>
<p>The first shaders <a id="_idIndexMarker739"/>we will read are the ray tracing shaders. These, as we saw in <a href="B18395_12.xhtml#_idTextAnchor205"><em class="italic">Chapter 12</em></a>, <em class="italic">Getting Started with R<a id="_idTextAnchor261"/>ay Tracing</em>, come as a bundle that includes the ray-generation, ray-hit, and ray-miss shaders.</p>
<p>There are a set of different methods that convert from world space into grid <a id="_idTextAnchor262"/>indices and vice versa that will be used here; they are included wit<a id="_idTextAnchor263"/>h the code.</p>
<p>First, we want to define the ray payload – that is, the information that’s cached after the ray tracing query is performed:</p>
<pre class="source-code">
struct RayPayload {
    vec3 radiance;
    float distance;
};</pre>
<h2 id="_idParaDest-218"><a id="_idTextAnchor264"/>Ray-generation shader</h2>
<p>The first shader is <a id="_idIndexMarker740"/>called <a id="_idIndexMarker741"/>ray-generation. It spawns rays from the probe’s position using random directions on a sphere using spherical Fibonacci sequences.</p>
<p>Like dithering for TAA and Volumetric Fog, using random directions and temporal accumulation (which happens in the Probe Update shader) allows us to have more information about the scene, thus enhancing the visuals:</p>
<pre class="source-code">
layout( location = 0 ) rayPayloadEXT RayPayload payload;
void main() {
const ivec2 pixel_coord = ivec2(gl_LaunchIDEXT.xy);
    const int probe_index = pixel_coord.y;
    const int ray_index = pixel_coord.x;
    // Convert from linear probe index to grid probe 
       indices and then position:
    ivec3 probe_grid_indices = probe_index_to_grid_indices( 
      probe_index );
    vec3 ray_origin = grid_indices_to_world( 
      probe_grid_indices probe_index );
    vec3 direction = normalize( mat3(random_rotation) * 
      spherical_fibonacci(ray_index, probe_rays) );
    payload.radiance = vec3(0);
    payload.distance = 0;
    traceRayEXT(as, gl_RayFlagsOpaqueEXT, 0xff, 0, 0, 0, 
      ray_origin, 0.0, direction, 100.0, 0);
  
    // Store the result coming from Hit or Miss shaders
    imageStore(global_images_2d[ radiance_output_index ], 
    pixel_coord, vec4(payload.radiance, payload.distance));
} </pre>
<h2 id="_idParaDest-219"><a id="_idTextAnchor265"/>Ray-hit shader</h2>
<p>This is <a id="_idIndexMarker742"/>where all the heavy<a id="_idIndexMarker743"/> lifting happens.</p>
<p>First, we must declare the payload and the barycentric coordinates to calculate the correct triangle data:</p>
<pre class="source-code">
layout( location = 0 ) rayPayloadInEXT RayPayload payload;
hitAttributeEXT vec2 barycentric_weights;</pre>
<p>Then, check for back-facing triangles, storing only the distance as lighting is not needed:</p>
<pre class="source-code">
void main() {
    vec3 radiance = vec3(0);
    float distance = 0.0f;
    if (gl_HitKindEXT == gl_HitKindBackFacingTriangleEXT) {
        // Track backfacing rays with negative distance
        distance = gl_RayTminEXT + gl_HitTEXT;
        distance *= -0.2;        
    }</pre>
<p>Otherwise, calculate the triangle data and perform lighting:</p>
<pre class="source-code">
    else {</pre>
<p>Next, read the mesh instance data and read the index buffer:</p>
<pre class="source-code">
    uint mesh_index = mesh_instance_draws[ 
      gl_GeometryIndexEXT ].mesh_draw_index;
    MeshDraw mesh = mesh_draws[ mesh_index ];
 
    int_array_type index_buffer = int_array_type( 
      mesh.index_buffer );
    int i0 = index_buffer[ gl_PrimitiveID * 3 ].v;
    int i1 = index_buffer[ gl_PrimitiveID * 3 + 1 ].v;
    int i2 = index_buffer[ gl_PrimitiveID * 3 + 2 ].v;</pre>
<p>Now, we can read the vertices from the mesh buffer and calculate the world space position:</p>
<pre class="source-code">
    float_array_type vertex_buffer = float_array_type( 
      mesh.position_buffer );
    vec4 p0 = vec4(vertex_buffer[ i0 * 3 + 0 ].v, 
      vertex_buffer[ i0 * 3 + 1 ].v,
      vertex_buffer[ i0 * 3 + 2 ].v, 1.0 );
    // Calculate p1 and p2 using i1 and i2 in the same 
       way.   </pre>
<p>Calculate<a id="_idIndexMarker744"/> the <a id="_idIndexMarker745"/>world position:</p>
<pre class="source-code">
    const mat4 transform = mesh_instance_draws[ 
      gl_GeometryIndexEXT ].model;
    vec4 p0_world = transform * p0;
    // calculate as well p1_w<a id="_idTextAnchor266"/>orld and p2_world</pre>
<p>As we did for the vertex positions, read the UV buffer and calculate the final UVs of the triangle:</p>
<pre class="source-code">
    float_array_type uv_buffer = float_array_type( 
      mesh.uv_buffer );
    vec2 uv0 = vec2(uv_buffer[ i0 * 2 ].v, uv_buffer[ 
      i0 * 2 + 1].v);
    // Read uv1 and uv2 using i1 and i2 
    float b = barycentric_weights.x;
    float c = barycentric_weights.y;
    float a = 1 - b - c;
 
    vec2 uv = ( a * uv0 + b * uv1 + c * uv2 );</pre>
<p>Read the diffuse texture. We can also read a lower MIP to improve performance:</p>
<pre class="source-code">
    vec3 diffuse = texture( global_textures[ 
      nonuniformEXT( mesh.textures.x ) ], uv ).rgb;</pre>
<p>Read the triangle normals and calculate the final normal. You don’t need to read the normal texture <a id="_idIndexMarker746"/>as the cached result is<a id="_idIndexMarker747"/> so small that those details are lost:</p>
<pre class="source-code">
    float_array_type normals_buffer = 
      float_array_type( mesh.normals_buffer );
    vec3 n0 = vec3(normals_buffer[ i0 * 3 + 0 ].v,
      normals_buffer[ i0 * 3 + 1 ].v,
      normals_buffer[ i0 * 3 + 2 ].v );
    // Similar calculations for n1 and n2 using i1 and 
       i2
    vec3 normal = a * n0 + b * n1 + c * n2;
    const mat3 normal_transform = mat3(mesh_instance_draws
      [gl_GeometryIndexEXT ].model_inverse);
    normal = normal_transform * normal;</pre>
<p>We can calculate the world position and the normal, and then calculate the direct lighting:</p>
<pre class="source-code">
    const vec3 world_position = a * p0_world.xyz + b * 
      p1_world.xyz + c * p2_world.xyz;
    vec3 diffuse = albedo * direct_lighting(world_position, 
      normal);
    // Optional: infinite bounces by samplying previous 
       frame Irradiance:
    diffuse += albedo * sample_irradiance( world_position, 
      normal, camera_position.xyz ) * 
      infinite_<a id="_idTextAnchor267"/>bounces_multiplier;</pre>
<p>Finally, we can cache the radiance and the distance:</p>
<pre class="source-code">
    radiance = diffuse;
    distance = gl_RayTminEXT + gl_HitTEXT;
    }</pre>
<p>Now, let’s <a id="_idIndexMarker748"/>write the results to <a id="_idIndexMarker749"/>the payload:</p>
<pre class="source-code">
    payload.radiance = radiance;
    payload.distance = distance;
}</pre>
<h2 id="_idParaDest-220"><a id="_idTextAnchor268"/>Ray-miss shader</h2>
<p>In this shader, we <a id="_idIndexMarker750"/>simply return<a id="_idIndexMarker751"/> the sky color. Alternatively, if present, an environment cube map can be added:</p>
<pre class="source-code">
layout( location = 0 ) rayPayloadInEXT RayPayload payload;
void main() {
payload.radiance = vec3( 0.529, 0.807, 0.921 );
payload.distance = 1000.0f;
}</pre>
<h2 id="_idParaDest-221"><a id="_idTextAnchor269"/>Updating probes irradiance and visibility shaders</h2>
<p>This compute <a id="_idIndexMarker752"/>shader <a id="_idIndexMarker753"/>will read the previous frame’s irradiance/visibility and the current frame’s radiance/distance and update the octahedral representation of each probe. This shader will be executed twice – once to update the irradiance and once to update the visibility. It will also update the borders to add support for bilinear filtering.</p>
<p>First, we must check if the current pixel is a border. If so, we must change modes:</p>
<pre class="source-code">
layout (local_size_x = 8, local_size_y = 8, local_size_z = 
        1) in;
void main() {
    ivec3 coords = ivec3(gl_GlobalInvocationID.xyz);
    const uint probe_with_border_side = probe_side_length + 
                                        2;
    const uint probe_last_pixel = probe_side_length + 1;
    int probe_index = get_probe_index_from_pixels
      (coords.xy, int(probe_with_border_side), 
      probe_texture_width);
    // Check if thread is a border pixel
    bool border_pixel = ((gl_GlobalInvocationID.x % 
      probe_with_border_side) == 0) || 
      ((gl_GlobalInvocationID.x % probe_with_border_side ) 
      == probe_last_pixel );
    border_pixel = border_pixel || 
      ((gl_GlobalInvocationID.y % probe_with_border_side) 
      == 0) || ((gl_GlobalInvocationID.y % 
      probe_with_border_side ) == probe_last_pixel );</pre>
<p>For non-border pixels, calculate a weight based on ray direction and the direction of the sphere encoded with octahedral coordinates, and calculate the irradiance as the summed weight of the radiances:</p>
<pre class="source-code">
    if ( !border_pixel ) {
        vec4 result = vec4(0);
        uint backfaces = 0;
        uint max_backfaces = uint(probe_rays * 0.1f); </pre>
<p>Add the contribution from each ray:</p>
<pre class="source-code">
        for ( int ray_index = 0; ray_index &lt; probe_rays; 
              ++ray_index ) {
            ivec2 sample_position = ivec2( ray_index, 
              probe_index );
            vec3 ray_direction = normalize( 
              mat3(random_rotation) * 
              spherical_fibonacci(ray_index, probe_rays) );
            vec3 texel_direction = oct_decode
              (normalized_oct_coord(coords.xy));
            float weight = max(0.0, dot(texel_direction, 
              ray_direction));</pre>
<p>Read the <a id="_idIndexMarker754"/>distance<a id="_idIndexMarker755"/> for this ray and early out if there are too many back faces:</p>
<pre class="source-code">
            float distance = texelFetch(global_textures
              [nonuniformEXT(radiance_output_index)], 
              sample_position, 
              0).w;
            if ( distance &lt; 0.0f &amp;&amp; 
                 use_backfacing_blending() ) {
                ++backfaces;
                // Early out: only blend ray radiance into 
                   the probe if the backface threshold 
                   hasn't been exceeded
                if (backfaces &gt;= max_backfaces) {
                    return;
                }
                continue;
            }</pre>
<p>At this point, depending on if we are updating the irradiance or the visibility, we perform <a id="_idIndexMarker756"/>different<a id="_idIndexMarker757"/> calculations.</p>
<p>For<strong class="bold"> irradiance</strong>, we must do the following:</p>
<pre class="source-code">
            if (weight &gt;= EPSILON) {
                vec3 radiance = texelFetch(global_textures
                  [nonuniformEXT(radiance_output_index)], 
                  sample_position, 0).rgb;
                radiance.rgb *= energy_conservation;
 
                // Storing the sum of the weights in alpha 
                   temporarily
                result += vec4(radiance * weight, weight);
            }
            </pre>
<p>For<strong class="bold"> visibility</strong>, we must read and limit the distance:</p>
<pre class="source-code">
            float probe_max_ray_distance = 1.0f * 1.5f;
            if (weight &gt;= EPSILON) {
                float distance = texelFetch(global_textures
                  [nonuniformEXT(radiance_output_index)], 
                  sample_position, 0).w;
                // Limit distance
                distance = min(abs(distance), 
                  probe_max_ray_distance);
                vec3 value = vec3(distance, distance * 
                  distance, 0);
                // Storing the sum of the weights in alpha 
                   temporarily
                result += vec4(value * weight, weight);
            }
        }</pre>
<p>Finally, apply<a id="_idIndexMarker758"/> the <a id="_idIndexMarker759"/>weight:</p>
<pre class="source-code">
        if (result.w &gt; EPSILON) {
            result.xyz /= result.w;
            result.w = 1.0f;
        }</pre>
<p>Now, we can read the previous frame’s irradiance or visibility and blend it using hysteresis.</p>
<p>For <strong class="bold">irradiance</strong>, we must do the following:</p>
<pre class="source-code">
        vec4 previous_value = imageLoad( irradiance_image, 
          coords.xy );
        result = mix( result, previous_value, hysteresis );
        imageStore(irradiance_image, coords.xy, result);</pre>
<p>For <strong class="bold">visibility</strong>, we must do the following:</p>
<pre class="source-code">
        vec2 previous_value = imageLoad( visibility_image, 
          coords.xy ).rg;
        result.rg = mix( result.rg, previous_value, 
          hysteresis );
        imageStore(visibility_image, coords.xy, 
          vec4(result.rg, 0, 1));</pre>
<p>At this point, we end the shader for non-border pixels. We will wait for the local group to finish and copy the pixels to the borders:</p>
<pre class="source-code">
        // NOTE: returning here.
        return;
    }</pre>
<p>Next, we must operate on the border pixels.</p>
<p>Given that <a id="_idIndexMarker760"/>we are <a id="_idIndexMarker761"/>working on a local thread group that’s as big as each square, when a group is finished, we can copy the border pixels with the currently updated data. This is an optimization process that helps us avoid dispatching two other shaders and adding barriers to wait for the updates to be done.</p>
<p>After implementing the preceding code, we must wait for the group to finish:</p>
<pre class="source-code">
    groupMemoryBarrier();
    barrier();</pre>
<p>Once those barriers are in the shader code, all the groups will be completed.</p>
<p>We have the final irradiance/visibility stored in the texture, so we can copy the border pixels to add bilinear sampling support. As shown in <em class="italic">Figure 14</em><em class="italic">.6</em>, we need to read the pixels in a specific order to ensure bilinear filtering is working properly.</p>
<p>First, we must calculate the source pixel coordinates:</p>
<pre class="source-code">
    const uint probe_pixel_x = gl_GlobalInvocationID.x % 
      probe_with_border_side;
    const uint probe_pixel_y = gl_GlobalInvocationID.y % 
      probe_with_border_side;
    bool corner_pixel = (probe_pixel_x == 0 || 
      probe_pixel_x == probe_last_pixel) &amp;&amp; (probe_pixel_y 
      == 0 || probe_pixel_y == probe_last_pixel);
    bool row_pixel = (probe_pixel_x &gt; 0 &amp;&amp; probe_pixel_x &lt; 
      probe_last_pixel);
    ivec2 source_pixel_coordinate = coords.xy;
    if ( corner_pixel ) {
        source_pixel_coordinate.x += probe_pixel_x == 0 ? 
          probe_side_length : -probe_side_length;
        source_pixel_coordinate.y += probe_pixel_y == 0 ? 
          probe_side_length : -probe_side_length;
     }
    else if ( row_pixel ) {
        source_pixel_coordinate.x += 
          k_read_table[probe_pixel_x - 1];
        source_pixel_coordinate.y += (probe_pixel_y &gt; 0) ? 
          -1 : 1;
     }
    else {
        source_pixel_coordinate.x += (probe_pixel_x &gt; 0) ? 
          -1 : 1;
        source_pixel_coordinate.y += 
          k_read_table[probe_pixel_y - 1];
     }
 </pre>
<p>Next, we must copy the source pixels to the current border.</p>
<p>For <strong class="bold">irradiance</strong>, we must do the following:</p>
<pre class="source-code">
    vec4 copied_data = imageLoad( irradiance_image, 
      source_pixel_coordinate );
    imageStore( irradiance_image, coords.xy, copied_data );</pre>
<p>For <strong class="bold">visibility</strong>, we must do the following:</p>
<pre class="source-code">
    vec4 copied_data = imageLoad( visibility_image, 
      source_pixel_coordinate );
    imageStore( visibility_image, coords.xy, copied_data );
}</pre>
<p>We now <a id="_idIndexMarker762"/>have the<a id="_idIndexMarker763"/> updated irradiance and visibility ready to be sampled by the scene.</p>
<h2 id="_idParaDest-222"><a id="_idTextAnchor270"/>Indirect lighting sampling</h2>
<p>Thi<a id="_idTextAnchor271"/>s compute <a id="_idIndexMarker764"/>shader is<a id="_idIndexMarker765"/> responsible for reading the indirect irradiance so that it’s ready to<a id="_idTextAnchor272"/> be used by the illumination. It uses a utility method called <code>sample_irradiance</code>, which is also used inside the ray-hit shader to simula<a id="_idTextAnchor273"/>te an infinite bounce.</p>
<p>First, though, let’s look at the compute shader. When using the quarter resolution, cycle through a neighborhood of 2x2 pixels and get the closest depth, and save the pixel index:</p>
<pre class="source-code">
layout (local_size_x = 8, local_size_y = 8, local_size_z = 
        1) in;
void main() {
    ivec3 coords = ivec3(gl_GlobalInvocationID.xyz);
    int resolution_divider = output_resolution_half == 1 ? 
      2 : 1;
    vec2 screen_uv = uv_nearest(coords.xy, resolution / 
      resolution_divider);
    
    float raw_depth = 1.0f;
    int chosen_hiresolution_sample_index = 0;
    if (output_resolution_half == 1) {
        float closer_depth = 0.f;
        for ( int i = 0; i &lt; 4; ++i ) {
            float depth = texelFetch(global_textures
             [nonuniformEXT(depth_fullscreen_texture_index)
             ], (coords.xy) * 2 + pixel_offsets[i], 0).r;
            if ( closer_depth &lt; depth ) {
                closer_depth = depth;
                chosen_hiresolution_sample_index = i;
            }
        }
 
        raw_depth = closer_depth;
    }</pre>
<p>With the <a id="_idIndexMarker766"/>cached <a id="_idIndexMarker767"/>index of the closest depth, read the normal as well: </p>
<pre class="source-code">
    vec3 normal = vec3(0);
    if (output_resolution_half == 1) {
        vec2 encoded_normal = texelFetch(global_textures
          [nonuniformEXT(normal_texture_index)],      
          (coords.xy) * 2 + pixel_offsets
          [chosen_hiresolution_sample_index], 0).rg;
       normal = normalize(octahedral_decode(encoded_normal)
       );
    }</pre>
<p>Now that we have calculated the depth and the normal, we can gather the world position and use the normal to sample the irradiance:</p>
<pre class="source-code">
    const vec3 pixel_world_position = 
      world_position_from_depth(screen_uv, raw_depth, 
      inverse_view_projection)
    vec3 irradiance = sample_irradiance( 
      pixel_world_position, normal, camera_position.xyz );
    imageStore(global_images_2d[ indirect_output_index ], 
      coords.xy, vec4(irradiance,1));
}</pre>
<p>The second part of this shader is about the <code>sample_irradiance</code> function, which does the actual heavy lifting.</p>
<p>It starts by <a id="_idIndexMarker768"/>calculating<a id="_idIndexMarker769"/> a bias vector to move the sampling so that it’s a little bit in front of the geometry, to help with leaks:</p>
<pre class="source-code">
vec3 sample_irradiance( vec3 world_position, vec3 normal, 
  vec3 camera_position ) {
    const vec3 V = normalize(camera_position.xyz – 
      world_position);
    // Bias vector to offset probe sampling based on normal 
       and view vector.
    const float minimum_distance_between_probes = 1.0f;
    vec3 bias_vector = (normal * 0.2f + V * 0.8f) * 
      (0.75f  minimum_distance_between_probes) * 
      self_shadow_bias;
    vec3 biased_world_position = world_position + 
      bias_vector;
 
    // Sample at world position + probe offset reduces 
       shadow leaking.
    ivec3 base_grid_indices = 
      world_to_grid_indices(biased_world_position);
    vec3 base_probe_world_position = 
      grid_indices_to_world_no_offsets( base_grid_indices 
      );</pre>
<p>We now have the grid world position and indices at the sampling world position (plus the bias).</p>
<p>Now, we must <a id="_idIndexMarker770"/>calculate <a id="_idIndexMarker771"/>a per-axis value of where the sampling position is within the cell:</p>
<pre class="source-code">
    // alpha is how far from the floor(currentVertex) 
       position. on [0, 1] for each axis.
    vec3 alpha = clamp((biased_world_position – 
      base_probe_world_position) , vec3(0.0f), vec3(1.0f));</pre>
<p>At this point, we can sample the eight adjacent probes to the sampling point:</p>
<pre class="source-code">
    vec3  sum_irradiance = vec3(0.0f);
    float sum_weight = 0.0f;</pre>
<p>For each probe, we must calculate its world space <a id="_idIndexMarker772"/>position from the indices:</p>
<pre class="source-code">
    // Iterate over adjacent probe cage
    for (int i = 0; i &lt; 8; ++i) {
        // Compute the offset grid coord and clamp to the 
           probe grid boundary
        // Offset = 0 or 1 along each axis
        ivec3  offset = ivec3(i, i &gt;&gt; 1, i &gt;&gt; 2) &amp; 
          ivec3(1);
        ivec3  probe_grid_coord = clamp(base_grid_indices + 
          offset, ivec3(0), probe_counts - ivec3(1));
        int probe_index = 
          probe_indices_to_index(probe_grid_coord);
        vec3 probe_pos = 
          grid_indices_to_world(probe_grid_coord, 
          probe_index); </pre>
<p>Compute the trilinear weights based on the grid cell vertex to smoothly transition between probes:</p>
<pre class="source-code">
        vec3 trilinear = mix(1.0 - alpha, alpha, offset);
        float weight = 1.0;</pre>
<p>Now, we can <a id="_idIndexMarker773"/>see how the <a id="_idIndexMarker774"/>visibility texture is used. It stores depth and depth squared values, and helps tremendously with light leaking.</p>
<p>This test is based on variance, such as Variance Shadow Map:</p>
<pre class="source-code">
        vec3 probe_to_biased_point_direction = 
          biased_world_position - probe_pos;
        float distance_to_biased_point = 
          length(probe_to_biased_point_direction);
        probe_to_biased_point_direction *= 1.0 / 
          distance_to_biased_point;
       {
            vec2 uv = get_probe_uv
              (probe_to_biased_point_direction,
              probe_index, probe_texture_width, 
              probe_texture_height, 
              probe_side_length );
            vec2 visibility = textureLod(global_textures
            [nonuniformEXT(grid_visibility_texture_index)],
            uv, 0).rg;
            float mean_distance_to_occluder = visibility.x;
            float chebyshev_weight = 1.0;</pre>
<p>Check if the<a id="_idIndexMarker775"/> sampled<a id="_idIndexMarker776"/> probe is in “shadow” and calculate the Chebyshev weight:</p>
<pre class="source-code">
            if (distance_to_biased_point &gt; 
                mean_distance_to_occluder) {
                float variance = abs((visibility.x * 
                  visibility.x) - visibility.y);
                const float distance_diff = 
                  distance_to_biased_point – 
                  mean_distance_to_occluder;
                chebyshev_weight = variance / (variance + 
                  (distance_diff * distance_diff));
                // Increase contrast in the weight
                chebyshev_weight = max((chebyshev_weight * 
                  chebyshev_weight * chebyshev_weight), 
                    0.0f);
            }
 
            // Avoid visibility weights ever going all of 
               the way to zero
           chebyshev_weight = max(0.05f, chebyshev_weight);
           weight *= chebyshev_weight;
        }</pre>
<p>With the weight <a id="_idIndexMarker777"/>calculated for this probe, we can apply the trilinear offset, read the irradiance, and calculate its contribution:</p>
<pre class="source-code">
         vec2 uv = get_probe_uv(normal, probe_index, 
           probe_texture_width, probe_texture_height, 
           probe_side_length );
        vec3 probe_irradiance = 
          textureLod(global_textures
          [nonuniformEXT(grid_irradiance_output_index)],
          uv, 0).rgb;
         // Trilinear weights
        weight *= trilinear.x * trilinear.y * trilinear.z + 
          0.001f;
        sum_irradiance += weight * probe_irradiance;
        sum_weight += weight;
    }</pre>
<p>With all the probes sampled, the final irradiance is scaled accordingly and returned:</p>
<pre class="source-code">
    vec3 irradiance = 0.5f * PI * sum_irradiance / 
      sum_weight;
    return irradiance;
}</pre>
<p>With that, we’ve finished looking at the irradiance sampling compute shader and utility functions.</p>
<p>More filters can be applied to the sampling to further smooth the image, but this is the most basic version that’s<a id="_idIndexMarker778"/> enhanced by the visibility data.</p>
<p>Now, let’s learn how the <code>calculate_lighting</code> method can be modified to add diffuse indirect.</p>
<h2 id="_idParaDest-223"><a id="_idTextAnchor274"/>Modifications to the calculate_lighting method</h2>
<p>In our <code>lighting.h</code> shader <a id="_idIndexMarker779"/>file, add<a id="_idIndexMarker780"/> the following lines once the direct lighting computations have been done:</p>
<pre class="source-code">
    vec3 F = fresnel_schlick_roughness(max(dot(normal, V), 
      0.0), F0, roughness);
    vec3 kS = F;
    vec3 kD = 1.0 - kS;
    kD *= 1.0 - metallic;
    vec3 indirect_irradiance = textureLod(global_textures
      [nonuniformEXT(indirect_lighting_texture_index)], 
      screen_uv, 0).rgb;
    vec3 indirect_diffuse = indirect_irradiance * 
      base_colour.rgb;
    const float ao = 1.0f;
    final_color.rgb += (kD * indirect_diffuse) * ao;</pre>
<p>Here, <code>base_colour</code> is the albedo coming from the G-buffer and <code>final_color</code> is the pixel color with all the direct lighting contributions calculated.</p>
<p>The basic algorithm is <a id="_idIndexMarker781"/>complete, but there is one last shader to<a id="_idIndexMarker782"/> have a look at: the Probe Offset shader. It calculates a per-probe world-space offset to avoid intersecting probes with geometries.</p>
<h2 id="_idParaDest-224"><a id="_idTextAnchor275"/>Probe offsets shader</h2>
<p>This compute<a id="_idIndexMarker783"/> shader cleverly uses the <a id="_idIndexMarker784"/>per-ray distances coming from the ray tracing pass to calc<a id="_idTextAnchor276"/>ulate the offset based on backface and frontface counts.</p>
<p>First, we must check for an invalid probe index to avoid writing to the wrong memory:</p>
<pre class="source-code">
layout (local_size_x = 32, local_size_y = 1, local_size_z = 
        1) in;
void main() {
    ivec3 coords = ivec3(gl_GlobalInvocationID.xyz);
    // Invoke this shader for each probe
    int probe_index = coords.x;
    const int total_probes = probe_counts.x * 
      probe_counts.y * probe_counts.z;
    // Early out if index is not valid
    if (probe_index &gt;= total_probes) {
        return;
    }</pre>
<p>Now, we must search for front and backface hits based on the ray tracing distance that’s been calculated.</p>
<p>First, declare all the necessary variables:</p>
<pre class="source-code">
    int closest_backface_index = -1;
    float closest_backface_distance = 100000000.f;
    int closest_frontface_index = -1;
    float closest_frontface_distance = 100000000.f;
    int farthest_frontface_index = -1;
    float farthest_frontface_distance = 0;
    int backfaces_count = 0;</pre>
<p>For each ray of this probe, read the distance and calculate if it is a front or backface. We store negative<a id="_idIndexMarker785"/> distances <a id="_idIndexMarker786"/>for backfaces in the hit shader:</p>
<pre class="source-code">
    // For each ray cache front/backfaces index and 
       distances.
    for (int ray_index = 0; ray_index &lt; probe_rays; 
         ++ray_index) {
        ivec2 ray_tex_coord = ivec2(ray_index, 
          probe_index);
        float ray_distance = texelFetch(global_textures
          [nonuniformEXT(radiance_output_index)], 
          ray_tex_coord, 0).w;
        // Negative distance is stored for backface hits in 
           the Ray Tracing Hit shader.
        if ( ray_distance &lt;= 0.0f ) {
            ++backfaces_count;
            // Distance is a positive value, thus negate 
               ray_distance as it is negative already if
            // we are inside this branch.
            if ( (-ray_distance) &lt; 
                  closest_backface_distance ) {
                closest_backface_distance = ray_distance;
                closest_backface_index = ray_index;
            }
        }
        else {
            // Cache either closest or farther distance and 
               indices for this ray.
            if (ray_distance &lt; closest_frontface_distance) 
            {
                closest_frontface_distance = ray_distance;
                closest_frontface_index = ray_index;
            } else if (ray_distance &gt; 
                       farthest_frontface_distance) {
                farthest_frontface_distance = ray_distance;
                farthest_frontface_index = ray_index;
            }
        }
    }</pre>
<p>We know the front and backface indices and distances for this probe. Given that we incrementally <a id="_idIndexMarker787"/>move the probe, read<a id="_idIndexMarker788"/> the previous frame’s offset:</p>
<pre class="source-code">
       vec4 current_offset = vec4(0);
    // Read previous offset after the first frame.
    if ( first_frame == 0 ) {
        const int probe_counts_xy = probe_counts.x * 
          probe_counts.y;
        ivec2 probe_offset_sampling_coordinates = 
          ivec2(probe_index % probe_counts_xy, probe_index 
          / probe_counts_xy);
        current_offset.rgb = texelFetch(global_textures
          [nonuniformEXT(probe_offset_texture_index)], 
          probe_offset_sampling_coordinates, 0).rgb;
    }</pre>
<p>Now, we must check if the probe can be considered inside a geometry and calculate an offset moving away from that direction, but within the probe spacing limit, that we can call a <code>cell</code>:</p>
<pre class="source-code">
    vec3 full_offset = vec3(10000.f);
    vec3 cell_offset_limit = max_probe_offset * 
      probe_spacing;
    // Check if a fourth of the rays was a backface, we can 
       assume the probe is inside a geometry.
    const bool inside_geometry = (float(backfaces_count) / 
      probe_rays) &gt; 0.25f;
    if (inside_geometry &amp;&amp; (closest_backface_index != -1)) 
    {
        // Calculate the backface direction.
        const vec3 closest_backface_direction = 
          closest_backface_distance * normalize( 
          mat3(random_rotation) * 
          spherical_fibonacci(closest_backface_index, 
          probe_rays) );        </pre>
<p>Find the<a id="_idIndexMarker789"/> maximum<a id="_idIndexMarker790"/> offset inside the cell to move the probe:</p>
<pre class="source-code">
        const vec3 positive_offset = (current_offset.xyz + 
          cell_offset_limit) / closest_backface_direction;
        const vec3 negative_offset = (current_offset.xyz – 
          cell_offset_limit) / closest_backface_direction;
        const vec3 maximum_offset = vec3(max
          (positive_offset.x, negative_offset.x), 
          max(positive_offset.y, negative_offset.y), 
          max(positive_offset.z, negative_offset.z));
        // Get the smallest of the offsets to scale the 
           direction
        const float direction_scale_factor = min(min
          (maximum_offset.x, maximum_offset.y),
          maximum_offset.z) - 0.001f;
        // Move the offset in the opposite direction of the 
           backface one.
        full_offset = current_offset.xyz – 
          closest_backface_direction * 
          direction_scale_factor;
    }</pre>
<p>If we have not <a id="_idIndexMarker791"/>hit a backface, we must<a id="_idIndexMarker792"/> move the probe slightly to put it in a resting position:</p>
<pre class="source-code">
    else if (closest_frontface_distance &lt; 0.05f) {
        // In this case we have a very small hit distance.
        // Ensure that we never move through the farthest 
           frontface
        // Move minimum distance to ensure not moving on a 
           future iteration.
        const vec3 farthest_direction = min(0.2f, 
          farthest_frontface_distance) * normalize( 
          mat3(random_rotation) * 
          spherical_fibonacci(farthest_frontface_index, 
          probe_rays) );
        const vec3 closest_direction = normalize(mat3
          (random_rotation) * spherical_fibonacci
          (closest_frontface_index, probe_rays));
        // The farthest frontface may also be the closest 
           if the probe can only 
        // see one surface. If this is the case, don't move 
           the probe.
        if (dot(farthest_direction, closest_direction) &lt; 
            0.5f) {
            full_offset = current_offset.xyz + 
              farthest_direction;
        }
    } </pre>
<p>Update the offset only if it is within the spacing or inside the cell limits. Then, store the value in the<a id="_idIndexMarker793"/> appropriate texture:</p>
<pre class="source-code">
    if (all(lessThan(abs(full_offset), cell_offset_limit)))
    {
        current_offset.xyz = full_offset;
    }
    const int probe_counts_xy = probe_counts.x * 
      probe_counts.y;
    const int probe_texel_x = (probe_index % 
      probe_counts_xy);
    const int probe_texel_y = probe_index / 
      probe_counts_xy;
    imageStore(global_images_2d[ probe_offset_texture_index 
      ], ivec2(probe_texel_x, probe_texel_y), 
      current_offset);
}</pre>
<p>With that, we have <a id="_idIndexMarker794"/>calculated the probe offsets.</p>
<p>Again, this shader demonstrates how to cleverly use information you already have – in this case, the per-ray probe distances – to move probes outside of intersecting geometries.</p>
<p>We presented a fully funcitonal version of DDGI, but there are some improvements that can be made and the technique can be expanded in different directions. Some examples of improvements are a classification system to disable non contributing probes, or adding a moving grid with cascades of different grid spacing centered around the camera. Combined with hand-placed volumes can create a complete diffuse global-illumination system.</p>
<p>While having a GPU with ray-tracing capabilities is necessary for this technique, we could bake irradiance and visibility for static scene parts and use them on older GPUs. Another improvement can be changing hysteresis based on probe luminance changes, or adding a<a id="_idIndexMarker795"/> staggered probe<a id="_idIndexMarker796"/> update based on distance and importance.</p>
<p>All these ideas show how powerful and configurable DDGI is and we encourage the reader to experiment and create other improvements.</p>
<h1 id="_idParaDest-225"><a id="_idTextAnchor277"/>Summary</h1>
<p>In this chapter, we introduced the DDGI technique. We started by talking about global illumination, the lighting phenomena that is implemented by DDGI. Then, we provided an overview of the algorithm, explaining each step in more detail.</p>
<p>Finally, we wrote <a id="_idTextAnchor278"/>and commented on all the shaders in the implementation. DDGI already enhances the lighting of the rendered frame, but it can be improved and optimized.</p>
<p>One of the aspects of DDGI that makes it useful is its configurability: you can change the resolution of irradiance and visibility textures and change the number of rays, number of probes, and spacing of probes to support lower-end ray tracing-enabled GPUs.</p>
<p>In the next chapter we are going to add another element that will help us increase the accuracy of our lighting solution: reflections!</p>
<h1 id="_idParaDest-226"><a id="_idTextAnchor279"/>Further reading</h1>
<p>Global illumination is an incredibly big topic that’s covered extensively in all rendering literature, but we wanted to highlight links that are more connected to the implementation of DDGI.</p>
<p>DDGI itself is an idea that mostly came from a team at Nvidia in 2017, with the central ideas described at <a href="https://morgan3d.github.io/articles/2019-04-01-ddgi/index.xhtml">https://morgan3d.github.io/articles/2019-04-01-ddgi/index.xhtml</a>.</p>
<p>The original articles on DDGI and its evolution are as follows. They also contain supplemental code that was incredibly helpful in implementing the technique:</p>
<ul>
<li><a href="https://casual-effects.com/research/McGuire2017LightField/index.xhtml">https://casual-effects.com/research/McGuire2017LightField/index.xhtml</a></li>
<li><a href="https://www.jcgt.org/published/0008/02/01/">https://www.jcgt.org/published/0008/02/01/</a></li>
<li><a href="https://jcgt.org/published/0010/02/01/">https://jcgt.org/published/0010/02/01/</a></li>
</ul>
<p>The following is a great overview of DDGI with Spherical Harmonics support, and the only diagram to copy the border pixels for bilinear interpolation. It also describes other interesting topics: <a href="https://handmade.network/p/75/monter/blog/p/7288-engine_work__global_illumination_with_irradiance_probes">https://handmade.network/p/75/monter/blog/p/7288-engine_work__global_illumination_with_irradiance_probes</a>.</p>
<p>The DDGI presentation by Nvidia can be found at <a href="https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9900-irradiance-fields-rtx-diffuse-global-illumination-for-local-and-cloud-graphics.pdf">https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9900-irradiance-fields-rtx-diffuse-global-illumination-for-local-and-cloud-graphics.pdf</a>.</p>
<p>The following is an intuitive introduction to global illumination: <a href="https://www.scratchapixel.com/lessons/3d-basic-rendering/global-illumination-path-tracing">https://www.scratchapixel.com/lessons/3d-basic-rendering/global-illumination-path-tracing</a>.</p>
<p><em class="italic">Global Illumination </em><em class="italic">Compendium</em>: <a href="https://people.cs.kuleuven.be/~philip.dutre/GI/">https://people.cs.kuleuven.be/~philip.dutre/GI/</a>.</p>
<p>Finally, here is the greatest website for real-time rendering: <a href="https://www.realtimerendering.com/">https://www.realtimerendering.com/</a>.</p>
</div>
</div></body></html>