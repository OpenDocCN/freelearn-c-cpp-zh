<html><head></head><body>
<div><h1 class="chapterNumber">2</h1>
<h1 class="chapterTitle" id="_idParaDest-33">Challenges in Embedded Systems with Limited Resources</h1>
<p class="normal">If you are reading this book, chances are you have a good grasp of embedded systems. There are many definitions of embedded systems, and while the following may not be the most common, it captures the essence shared by others. <strong class="keyWord">Embedded systems</strong> are specialized computing systems for specific use with a limited set of responsibilities, in contrast to general-purpose <a id="_idIndexMarker062"/>computing systems. Embedded systems can be embedded in a larger electronic or mechanical system, or act as a standalone device.</p>
<p class="normal">The line between embedded systems and general-purpose computing devices is sometimes blurred. We can all agree that the system that controls a toaster or a pump in an airplane is an embedded system. Cellphones and early smartphones were also considered embedded systems. Nowadays, smartphones are closer to the definition of a general-purpose computing device. In this book, we will focus on firmware development using modern C++ on small embedded systems or resource-constrained embedded systems.</p>
<p class="normal">Resource-constrained embedded systems are often employed in safety-critical applications. They have a responsibility to control a process in a timely manner and they cannot fail, as failure can mean the loss of human lives. In this chapter, we will cover limitations imposed by regulations on software development for safety-critical devices and implications for the usage of C++. We will learn how to mitigate these concerns.</p>
<p class="normal">In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li class="bulletList">Safety-critical and hard real-time embedded systems</li>
<li class="bulletList">Dynamic memory management</li>
<li class="bulletList">Disabling unwanted C++ features</li>
</ul>
<h1 class="heading-1" id="_idParaDest-34">Technical requirements</h1>
<p class="normal">To get the most out of this chapter, I strongly recommend using Compiler Explorer (<a href="https://godbolt.org/">https://godbolt.org/</a>) as you read through the examples. Select GCC as your compiler and target x86 architecture. This will allow you to see standard output (stdio) results and better observe the code’s behavior. As we are using a lot of modern C++ features, make sure to select C++23 standard, by adding <code class="inlineCode">-std=c++23</code> in compiler options box.</p>
<p class="normal">The examples from this chapter are available on GitHub (<a href="https://github.com/PacktPublishing/Cpp-in-Embedded-Systems/tree/main/Chapter02">https://github.com/PacktPublishing/Cpp-in-Embedded-Systems/tree/main/Chapter02</a>).</p>
<h1 class="heading-1" id="_idParaDest-35">Safety-critical and hard real-time embedded systems</h1>
<p class="normal"><strong class="keyWord">Safety-critical embedded systems</strong> are systems whose failure may result in damage to property <a id="_idIndexMarker063"/>or environment, injury to people, or even a loss of life. Failure of these systems is not acceptable. Brakes, steering systems, and airbags in cars are good examples of safety-critical systems. The correct functioning of these systems is essential for the safe operation of a vehicle.</p>
<p class="normal">Next, we will analyze the real-time requirements of an airbag control unit in a car.</p>
<h2 class="heading-2" id="_idParaDest-36">Airbag control unit and real-time requirements</h2>
<p class="normal">Safety-critical <a id="_idIndexMarker064"/>embedded systems often impose hard real-time requirements, meaning that any missed deadline results in <a id="_idIndexMarker065"/>system failure. An <strong class="keyWord">A</strong><strong class="keyWord">irbag Control Unit</strong> (<strong class="keyWord">ACU</strong>) collects data from accelerometers and pressure sensors, runs an algorithm that processes the collected data, and detects side, front, and rear-end crashes. Upon the crash detection, the ACU controls the deployment of different restraint systems, including airbags and seat belt tensioners.</p>
<p class="normal">ACU implementations <a id="_idIndexMarker066"/>must be resilient to different scenarios, such as malfunctioning sensors and electronics. These are mitigated by redundant sensors, comparing data from sensors, comparing data against thresholds, and self-tests. Most importantly, ACUs need to meet timing requirements, as they have only a couple of milliseconds to collect data, make decisions, and initiate deployment of restraint systems.</p>
<p class="normal">The ACU fails if it doesn’t detect a crash on time, but it also fails if it deploys restraint systems just a bit too late, as this can do more harm to a driver and passengers than if the ACU hadn’t initiated a deployment at all. This is why an ACU must meet hard real-time requirements, and when it comes to firmware, this means all the worst-case execution times must be predictable.</p>
<p class="normal">The effect of delayed airbag deployment is the subject of many studies concerned with injuries <a id="_idIndexMarker067"/>caused to occupants. The following extract is part of the conclusion from the paper <em class="italic">Study regarding the influence of airbag deployment time on the occupant injury level during a frontal vehicle collision</em>, published at MATEC Web of Conferences 184(1):01007, by authors Alexandru Ionut Radu, Corneliu Cofaru, Bogdan Tolea, and Dragoș Sorin Dima, outlining results of simulations of delayed airbag deployment:</p>
<blockquote class="packt_quote">
<p class="quote"><em class="italic">“It has been found that by increasing the delay of the airbag deployment time in the event of a frontal impact, the probability of injury to the occupant’s head increases by up to 46%. Reducing the distance between the occupant’s head and the dashboard / steering wheel when the airbag ignites would result in a force expansion of the gas that is transmitted to the occupant’s head generating an extra acceleration and also throws back the occupant increasing the injury potential due to the impact between the head and headrest. Thus, an increase in injury probability of 8% was observed in the 0 ms delay of the airbag deployment, while a 100 ms delay resulted in a 54% increase in the head acceleration value. So, the role of the airbag is reversed, it no longer has the role of cushioning the collision, but to generate injuries.”</em></p>
</blockquote>
<p class="normal">A graphic illustration of collision and delayed airbag deployment is shown in the following figure (source: <a href="https://www.researchgate.net/publication/326715516_Study_regarding_the_influence_of_airbag_deployment_time_on_the_occupant_injury_level_during_a_frontal_vehicle_collision">https://www.researchgate.net/publication/326715516_Study_regarding_the_influence_of_airbag_deployment_time_on_the_occupant_injury_level_during_a_frontal_vehicle_collision</a>):</p>
<figure class="mediaobject"><img alt="Figure 2.1 – Crash simulation with delayed restraint system deployment" src="img/B22402_2_1.png"/></figure>
<p class="packt_figref">Figure 2.1 – Crash simulation with delayed restraint system deployment</p>
<p class="normal"><em class="italic">Figure 2</em><em class="italic">.1</em> effectively illustrates what happens if an ACU doesn’t meet hard real-time requirements <a id="_idIndexMarker068"/>and produces delayed results. The figure is taken from the paper <em class="italic">Study regarding the influence of airbag deployment time on the occupant injury level during a frontal vehicle collision</em>.</p>
<p class="normal"> There are multiple reasons why an ACU may fail and cause no or a delayed deployment:</p>
<ul>
<li class="bulletList">Sensor malfunctioning</li>
<li class="bulletList">Electronics malfunctioning</li>
<li class="bulletList">Crash detection algorithm failure</li>
<li class="bulletList">Firmware failure to meet a deadline</li>
</ul>
<p class="normal">Sensors and electronics malfunctioning are mitigated by redundancy, data sanity checks, cross-comparison, and startup and runtime self-tests. This puts additional stress on firmware and its correct functioning. A crash detection algorithm may fail due to a bad model that was built upon, or other factors that are out of firmware responsibilities. The firmware’s job is to feed the algorithm with sensors’ data on time, execute it in a timely manner within a set time window, and act based on the output of the algorithm.</p>
<h2 class="heading-2" id="_idParaDest-37">Measuring firmware performance and non-determinism</h2>
<p class="normal">How do <a id="_idIndexMarker069"/>we ensure <a id="_idIndexMarker070"/>that the firmware will run all functions within imposed real-time requirements? We measure it. We can measure different metrics, such as performance profiling, response to external events, and A-B timing. Performance profiling will tell us in which functions the program spends the most time. Response to external events will indicate how much time it takes for a system to respond to an external event, such as an interrupt or a message on a communication bus.</p>
<h3 class="heading-3" id="_idParaDest-38">A-B timing and real-time execution</h3>
<p class="normal">The most important metric when dealing with real-time requirements is <strong class="keyWord">A-B timing</strong>. We measure how <a id="_idIndexMarker071"/>long it takes for firmware to execute a program from point A to point B. A-B timing can measure a function’s duration, but not necessarily. We can use it to measure different things. Going from A to B can take different times, based on the state of the system and inputs.</p>
<p class="normal">A simple way <a id="_idIndexMarker072"/>to make an A-B measurement is toggling a <strong class="keyWord">General Purpose Input Output</strong> (<strong class="keyWord">GPIO</strong>) and using an oscilloscope to measure the time between changes of a GPIO. It’s a simple solution that works well but doesn’t scale, as we would need a GPIO for every function we want to measure or we’d need to measure one function at <a id="_idIndexMarker073"/>a time. We could also use the internal timer of a <strong class="keyWord">Microcontroller Unit (MCU)</strong>to make precise measurements and output that information over a UART port. This would require us to utilize a general-purpose timer just for the sake of measuring. Most microcontrollers have specialized units for instrumentation and profiling.</p>
<p class="normal">Some ARM-based <a id="_idIndexMarker074"/>microcontrollers have a <strong class="keyWord">Data Watchpoint and Trace</strong> (<strong class="keyWord">DWT</strong>) unit. DWT is used for data tracing and system profiling, including <a id="_idIndexMarker075"/>the following:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Program Counter</strong> (<strong class="keyWord">PC</strong>) sampling</li>
<li class="bulletList">Cycle counting</li>
</ul>
<p class="normal">DWT generates <a id="_idIndexMarker076"/>events and outputs them using an <strong class="keyWord">Instrumentation Trace Macrocell</strong> (<strong class="keyWord">ITM</strong>) unit. The ITM unit can also be used to output data generated from the firmware itself, in the <code class="inlineCode">printf</code> style. ITM buffers data and sends it <a id="_idIndexMarker077"/>over to an ITM sink. <strong class="keyWord">Single Wire Output</strong> (<strong class="keyWord">SWO</strong>) can be used as an ITM sink.</p>
<p class="normal">We can utilize DWT and ITM for profiling as follows:</p>
<ol>
<li class="numberedList" value="1">DWT can generate periodic sampling of the PC and use ITM to send them over SWO.</li>
<li class="numberedList">On a host machine, we capture and analyze the received data.</li>
<li class="numberedList">By using a linker map file for our firmware, we can generate the distribution of time spent in each of the functions in our program.</li>
</ol>
<p class="normal">This can help us to see which function takes the most time. It’s not particularly useful for A-B timing measurements, but it allows us to see where the program spends most of the time without any <a id="_idIndexMarker078"/>direct software instrumentation except setting up DWT and ITM units.</p>
<h3 class="heading-3" id="_idParaDest-39">Sotware instrumentation with GCC</h3>
<p class="normal"><strong class="keyWord">GNU Compiler Collection </strong>(<strong class="keyWord">GCC</strong>) supports software instrumentation by using the <code class="inlineCode">-finstrument-functions</code> flag to instrument functions’ entries and exists. This inserts <code class="inlineCode">entry</code> and <code class="inlineCode">exit</code> calls to each function with the following signature:</p>
<pre class="programlisting code"><code class="hljs-code">__attribute__((no_instrument_function))
void __cyg_profile_func_enter(void *this_fn, void *call_site)
{
}
__attribute__((no_instrument_function))
void __cyg_profile_func_exit(void *this_fn, void *call_site)
{
}
</code></pre>
<p class="normal">We can utilize DWT and ITM in the <code class="inlineCode">__cyg_profile_func_enter</code> and <code class="inlineCode">__cyg_profile_func_exit</code> functions to send the clock cycle count and analyze it on the host machine to make A-B timing measurements. The following is an example of a simplified implementation of <code class="inlineCode">entry</code> and <code class="inlineCode">exit</code> functions:</p>
<pre class="programlisting code"><code class="hljs-code">extern "C" {
__attribute__((no_instrument_function))
void __cyg_profile_func_enter(void *this_fn, void *call_site)
{
    printf("entry, %p, %d", this_fn, DWT_CYCCNT);
}
__attribute__((no_instrument_function))
void __cyg_profile_func_exit(void *this_fn, void *call_site)
{
    printf("exit, %p, %d", this_fn, DWT_CYCCNT);
}
}
</code></pre>
<p class="normal">The preceding implementation uses <code class="inlineCode">extern</code> <code class="inlineCode">"C"</code> as a linkage language specifier for <code class="inlineCode">entry</code> and <code class="inlineCode">exit</code> instrumentation functions as they are linked with C libraries by the compiler. The example also assumes that <code class="inlineCode">printf</code> is redirected to use ITM as output and that the cycle counter register in DWT is started.</p>
<p class="normal">Another option is to use ITM’s timestamping and send both timestamps and function addresses from <code class="inlineCode">entry</code> and <code class="inlineCode">exit</code> instrumentation functions. With the help of a linker map file, we can then <a id="_idIndexMarker079"/>reconstruct the sequence of function calls and returns. There are specialized <a id="_idIndexMarker080"/>formats for sending traces, such as <strong class="keyWord">Common Trace Format</strong> (<strong class="keyWord">CTF</strong>), and desktop <a id="_idIndexMarker081"/>tools called <strong class="keyWord">trace viewers</strong> that can allow us to streamline software instrumentation. CTF is an open format used to serialize an event <a id="_idIndexMarker082"/>in a packet with one or more fields. Specialized tools, such as <strong class="keyWord">barectf</strong> (<a href="https://barectf.org/docs/barectf/3.1/index.html">https://barectf.org/docs/barectf/3.1/index.html</a>) are used to facilitate CTF packet generation.</p>
<p class="normal">Events are <a id="_idIndexMarker083"/>described using a <strong class="keyWord">YAML Ain’t Markup Language</strong> (<strong class="keyWord">YAML</strong>) configuration file. A simple C library containing trace functions is generated by <code class="inlineCode">barectf</code> using the configuration file. These functions are used in source code in places where we want to emit traces.</p>
<p class="normal">CTF traces can be sent over different transport layers such as ITM or serial. Traces can be analyzed using tools such as Babeltrace (<a href="https://babeltrace.org">https://babeltrace.org</a>) and TraceCompass (<a href="https://eclipse.dev/tracecompass">https://eclipse.dev/tracecompass</a>). There are other tools that facilitate trace generation, transfer, and viewing such as SEGGER SystemView. On the target side, a small software module provided by SEGGER is included to make calls to tracing functions. Traces are sent <a id="_idIndexMarker084"/>over SEGGER’s <strong class="keyWord">Real Time Transfer</strong> (<strong class="keyWord">RTT</strong>) protocol using SWD and analyzed in SystemView.</p>
<p class="normal">We covered some basic approaches to A-B timing. There are more advanced techniques, and they often depend on the target capabilities, as there are some more advanced tracing units that can be utilized for A-B measurements.</p>
<h3 class="heading-3" id="_idParaDest-40">Determinism vs. Non-Determinism in Firmware</h3>
<p class="normal">If we measure the duration of a function using the A-B timing approach and have the same duration and <a id="_idIndexMarker085"/>function output for the same inputs, we say <a id="_idIndexMarker086"/>that the function is <strong class="keyWord">deterministic</strong>. If a function <a id="_idIndexMarker087"/>depends on a global state and the measured duration <a id="_idIndexMarker088"/>is different for the same inputs, we say it is <strong class="keyWord">non-deterministic</strong>.</p>
<p class="normal">Default dynamic memory allocators in C++ tend to be non-deterministic. The duration of allocation depends on the current global state of the allocator and the complexity of the allocating algorithm. We can measure duration for the same inputs with different global states, but it is hard to evaluate all possible global states and to guarantee the <strong class="keyWord">Worst-Case Execution Time</strong> (<strong class="keyWord">WCET</strong>) with default allocators.</p>
<p class="normal">The non-deterministic <a id="_idIndexMarker089"/>behavior of dynamic memory allocation is just one problem for safety-critical systems. The other problem is that it can fail. If there is no more available memory or if the memory is fragmented, then the allocation <a id="_idIndexMarker090"/>can fail. This is why many <a id="_idIndexMarker091"/>safety coding standards such as <strong class="keyWord">Motor Industry Software Reliability Association</strong> (<strong class="keyWord">MISRA</strong>) and <strong class="keyWord">Automotive Open System Architecture</strong> (<strong class="keyWord">AUTOSAR</strong>) discourage dynamic memory.</p>
<p class="normal">We will explore dynamic memory management implications and safety-critical concerns next.</p>
<h1 class="heading-1" id="_idParaDest-41">Dynamic memory management</h1>
<p class="normal">The C++ standard <a id="_idIndexMarker092"/>defines the following storage durations for objects:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Automatic storage duration</strong>: Objects with automatic storage duration are automatically <a id="_idIndexMarker093"/>created and <a id="_idIndexMarker094"/>destroyed as the program enters and exits the block in which they are defined. These are typically local variables within functions, except those declared <code class="inlineCode">static</code>, <code class="inlineCode">extern</code>, or <code class="inlineCode">thread_local</code>.</li>
<li class="bulletList"><strong class="keyWord">Static storage duration</strong>: Objects with static storage duration are allocated when the program <a id="_idIndexMarker095"/>starts and deallocated <a id="_idIndexMarker096"/>when the program ends. All objects declared at the namespace scope (including the global namespace) have this static duration, plus those declared with <code class="inlineCode">static</code> or <code class="inlineCode">extern</code>.</li>
<li class="bulletList"><strong class="keyWord">Thread storage duration</strong>: Introduced in C++11, objects with thread storage duration <a id="_idIndexMarker097"/>are created and <a id="_idIndexMarker098"/>destroyed with the thread in which they are defined, allowing each thread to have its own instance of a variable. They are declared with the <code class="inlineCode">thread_local</code> specifier.</li>
<li class="bulletList"><strong class="keyWord">Dynamic storage duration</strong>: Objects <a id="_idIndexMarker099"/>with dynamic <a id="_idIndexMarker100"/>storage duration are explicitly created and destroyed using dynamic memory allocation functions (<code class="inlineCode">new</code> and <code class="inlineCode">delete</code> in C++), giving the software developer control over the lifetime of these objects.</li>
</ul>
<p class="normal">Dynamic storage gives great flexibility to a software developer, providing full control over an object’s lifetime. With great power comes great responsibility. Objects are dynamically allocated using the <code class="inlineCode">new</code> operator and freed using <code class="inlineCode">delete</code>. Every object that is allocated dynamically <a id="_idIndexMarker101"/>must be freed exactly once and should never be accessed after it has been freed. This is a straightforward rule but failing to follow it causes a range of problems, such as the following:</p>
<ul>
<li class="bulletList">Memory leaks occur when dynamically allocated memory is not freed properly. Over time, this unused memory accumulates potentially exhausting system resources.</li>
<li class="bulletList">Dangling pointers happen when a pointer still references a memory location that has been freed. Accessing such a pointer leads to undefined behavior.</li>
<li class="bulletList">Double free errors occur when memory that has already been freed is deleted again, leading to undefined behavior.</li>
</ul>
<p class="normal">Another problem with dynamic memory management is memory fragmentation.</p>
<h2 class="heading-2" id="_idParaDest-42">Memory fragmentation</h2>
<p class="normal"><strong class="keyWord">Memory fragmentation</strong> occurs when free memory is divided into small, non-contiguous blocks <a id="_idIndexMarker102"/>over time, making it difficult or impossible to allocate large blocks of <a id="_idIndexMarker103"/>memory even when there is enough free memory available in total. There are two main types:</p>
<ul>
<li class="bulletList"><strong class="keyWord">External fragmentation</strong>: This happens when there is enough total memory available <a id="_idIndexMarker104"/>to satisfy an allocation <a id="_idIndexMarker105"/>request but no single continuous block is large enough due to fragmentation. It’s common in systems where memory allocation and deallocation occur frequently, and sizes vary significantly.</li>
<li class="bulletList"><strong class="keyWord">Internal fragmentation</strong>: This occurs when allocated memory blocks are larger than the <a id="_idIndexMarker106"/>requested memory, leading <a id="_idIndexMarker107"/>to wasted space within allocated blocks. It happens when using allocators that have fixed-size memory blocks or memory pools and with allocators designed to give WCET.</li>
</ul>
<p class="normal">Memory fragmentation leads to inefficient memory use, reducing the performance or preventing further allocations resulting in out-of-memory scenarios, even when it appears that sufficient memory is available. Let’s visualize the memory region reserved for dynamic memory allocation in the following figure:</p>
<figure class="mediaobject"><img alt="Figure 2.2 – Memory region used for dynamic allocation" src="img/B22402_2_2.png"/></figure>
<p class="packt_figref">Figure 2.2 – Memory region used for dynamic allocation</p>
<p class="normal">In <em class="italic">Figure 2</em><em class="italic">.2</em>, each block represents a memory unit allocated during the allocation process. Empty regions were not allocated, or they were freed using the <code class="inlineCode">delete</code> operator. Even though there <a id="_idIndexMarker108"/>is plenty of memory available, if there were a request for the allocation of four memory units, the allocation would fail, as there are not four continuous <a id="_idIndexMarker109"/>memory blocks available due to memory fragmentation.</p>
<p class="normal">Non-deterministic behavior of default memory allocators and out-of-memory scenarios are major concerns for safety-critical systems. MISRA and AUTOSAR provide coding guidelines for the use of C++ in safety-critical systems.</p>
<p class="normal">MISRA is an organization that provides guidelines for the software developed for electronic components used in the automotive industry. It is a collaboration between vehicle manufacturers, component suppliers, and engineering consultancies. Standards produced by MISRA are also used in aerospace, defense, space, medical, and other industries.</p>
<p class="normal">AUTOSAR <a id="_idIndexMarker110"/>is a global development partnership by automotive manufacturers, suppliers, and other companies from the <a id="_idIndexMarker111"/>electronics, semiconductor, and software industries. AUTOSAR also produces guidelines for the use of C++ in critical and safety-related systems.</p>
<h2 class="heading-2" id="_idParaDest-43">Safety-critical guidelines for dynamic memory management in C++</h2>
<p class="normal">MISRA <a id="_idIndexMarker112"/>C++ 2008, which covers the C++03 standard, prohibits the usage of dynamic memory allocation, while AUTOSAR’s <em class="italic">Guidelines for the use of the C++14 language in critical and safety-related systems</em> specifies, among others, the following rules:</p>
<ul>
<li class="bulletList">Rule A18-5-5 (required, toolchain, partially automated)</li>
</ul>
<blockquote class="packt_quote">
<p class="quote">“Memory management functions shall ensure the following: (a) deterministic behavior resulting with the existence of worst-case execution time, (b) avoiding memory fragmentation, (c) avoid running out of memory, (d) avoiding mismatched allocations or deallocations, (e) no dependence on non-deterministic calls to kernel.”</p>
</blockquote>
<ul>
<li class="bulletList">Rule A18-5-6 (required, verification / toolchain, non-automated)</li>
</ul>
<blockquote class="packt_quote">
<p class="quote"> “An analysis shall be performed to analyze the failure modes of dynamic memory management. In particular, the following failure modes shall be analyzed: (a) non-deterministic behavior resulting with nonexistence of worst-case execution time, (b) memory fragmentation, (c) running out of memory, (d) mismatched allocations and deallocations, (e) dependence on non-deterministic calls to kernel.”</p>
</blockquote>
<p class="normal">Now, following these two rules to the letter is an extremely hard task. We can write a custom allocator that has deterministic WCET and minimizes fragmentation, but how do we write an allocator that avoids running out of memory? Or, in case it happens, how do we ensure the non-failure of the system? Every call to the allocator would need to verify the success of the operation and, in case of failure, somehow mitigate it. Or we would need to be able to estimate the amount of memory needed for an allocator accurately, so it doesn’t run out of memory in runtime under any circumstances. This adds a whole new layer of complexity to our software design and adds more complexity than we would add value by allowing dynamic memory allocation.</p>
<p class="normal">An in-between approach to dynamic memory allocation policy is to allow it on startup, but not when the system is running. This is the policy used by <strong class="keyWord">Joint Strike Fighter Air Vehicle C++ Coding Standards</strong>. MISRA C++ 2023 also advises against the usage of dynamic memory allocation when the system is running, and as a mitigation policy, recommends using it at startup.</p>
<p class="normal">The C++ standard library uses dynamic memory allocation heavily. Exception handling mechanism implementations also often use dynamic allocation. Before dismissing the idea of using the standard library in embedded projects, let’s discover the internal workings of the <code class="inlineCode">std::vector</code> container and see what C++ offers to mitigate our concerns.</p>
<h2 class="heading-2" id="_idParaDest-44">Dynamic memory management in the C++ standard library</h2>
<p class="normal">We introduced <code class="inlineCode">std::vector</code> as a container from the standard library that uses dynamic memory <a id="_idIndexMarker113"/>allocation. <code class="inlineCode">vector</code> is a template class, and we can specify the underlying type. It stores the elements contiguously, and we can get direct access to the underlying contiguous storage using the <code class="inlineCode">data</code> method. </p>
<p class="normal">The following code example demonstrates the usage of a vector:</p>
<pre class="programlisting code"><code class="hljs-code">  std::vector&lt;std::uint8_t&gt; vec;
  constexpr std::size_t n_elem = 8;
  for (std::uint8_t i = 0; i &lt; n_elem; i++) {
    vec.push_back(i);
  }
  const auto print_array = [](uint8_t *arr, std::size_t n) {
    for (std::size_t i = 0; i &lt; n; i++) {
      printf("%d ", arr[i]);
    }
    printf("\r\n");
  };
  print_array(vec.data(), n_elem);
</code></pre>
<p class="normal">We created a vector with the underlying <code class="inlineCode">uint8_t</code> type and added values from <code class="inlineCode">0</code> to <code class="inlineCode">8</code> using the <code class="inlineCode">push_back</code> method. The example also demonstrates access to a pointer to the underlying contiguous storage, which we provided as an argument to the <code class="inlineCode">print_array</code> lambda.</p>
<p class="normal">The usual allocation strategy of <code class="inlineCode">vector</code> is to allocate one element on the first insertion, then double it each time it reaches its capacity. Storing values for <code class="inlineCode">0</code> to <code class="inlineCode">8</code> would result in 4 allocation requests, as shown in the following figure:</p>
<figure class="mediaobject"><img alt="Figure 2.3 – Vector allocation requests" src="img/B22402_2_3.png"/></figure>
<p class="packt_figref">Figure 2.3 – Vector allocation requests</p>
<p class="normal"><em class="italic">Figure 2</em><em class="italic">.3</em> depicts the vector’s allocation requests. In order to inspect vector implementation on any <a id="_idIndexMarker114"/>platform, we can overload the <code class="inlineCode">new</code> and <code class="inlineCode">delete</code> operators and monitor the allocation requests:</p>
<pre class="programlisting code"><code class="hljs-code">void *operator new(std::size_t count) {
  printf("%s, size = %ld\r\n", __PRETTY_FUNCTION__, count);
  return std::malloc(count);
}
void operator delete(void *ptr) noexcept {
  printf("%s\r\n", __PRETTY_FUNCTION__);
  std::free(ptr);
}
</code></pre>
<p class="normal">The <code class="inlineCode">new</code> overloaded operator passes allocation calls to <code class="inlineCode">malloc</code>, and it prints out the size requested by the caller. The <code class="inlineCode">delete</code> overloaded operator just prints out the function signature so we can see when it is called. Some standard library implementations using GCC implement the <code class="inlineCode">new</code> operator using <code class="inlineCode">malloc</code>. Our vector allocation calls will result in the following output:</p>
<pre class="programlisting con"><code class="hljs-con">void* operator new(std::size_t), size = 1
void* operator new(std::size_t), size = 2
void operator delete(void*)
void* operator new(std::size_t), size = 4
void operator delete(void*)
void* operator new(std::size_t), size = 8
void operator delete(void*)
</code></pre>
<p class="normal">The preceding results are obtained using the GCC compiler, and they are the same both for x86_64 and Arm Cortex-M4 platforms. When the vector fills the available memory, it requests allocation of the doubled amount of currently used memory. It then copies data from the original storage to newly acquired memory. Afterward, it deletes previously used storage, as we <a id="_idIndexMarker115"/>can see from the preceding generated output.</p>
<p class="normal">Overloading the <code class="inlineCode">new</code> and <code class="inlineCode">delete</code> operators would allow us to change the allocation mechanism globally, in order to meet the safety-critical guidelines requesting for deterministic WTEC and avoiding out-of-memory scenarios, which is quite challenging.</p>
<p class="normal">The allocation requests from the vector can be optimized by using the <code class="inlineCode">reserve</code> method if the number of elements is known beforehand:</p>
<pre class="programlisting code"><code class="hljs-code">  vec.reserve(8);
</code></pre>
<p class="normal">Using the <code class="inlineCode">reserve</code> method will make the vector request eight elements, and it will ask for more memory only if we go beyond eight elements. This makes it useful for projects that allow dynamic allocation at startup if we can guarantee that the number of elements at any point will stay within reserved memory. If we add a ninth element to the vector, it will make another allocation request, requesting the memory to fit 16 elements.</p>
<p class="normal">The C++ standard library also makes possible usage of local allocators for containers. Let’s take a look at the vector’s declaration:</p>
<pre class="programlisting code"><code class="hljs-code">template&lt;
    class T,
    class Allocator = std::allocator&lt;T&gt;
&gt; class vector;
</code></pre>
<p class="normal">We can see that the second template parameter is <code class="inlineCode">Allocator</code>, and the default argument is <code class="inlineCode">std::allocator</code>, which uses the <code class="inlineCode">new</code> and <code class="inlineCode">delete</code> operators. C++17 introduced <code class="inlineCode">std::pmr::polymorphic_allocator</code>, an allocator that exhibits different allocation behavior depending upon the <code class="inlineCode">std::pmr::memory_resource</code> type from which it is constructed.</p>
<p class="normal">There is a memory resource that can be constructed by providing it with an initial, statically allocated buffer, and it’s called <code class="inlineCode">std::pmr::monotonic_buffer_resource</code>. The monotonic buffer is built for performance, and it releases memory only when it is destroyed. Initializing it with a statically allocated buffer makes it suitable for embedded applications. Let’s see how we can use it for a vector:</p>
<pre class="programlisting code"><code class="hljs-code"> using namespace std;
  using namespace std::pmr;
  array&lt;uint8_t, sizeof(uint8_t) * 8&gt; buffer{0};
  monotonic_buffer_resource mbr{buffer.data(), buffer.size()};
  polymorphic_allocator&lt;uint8_t&gt; pa{&amp;mbr};
  std::pmr::vector&lt;uint8_t&gt; vec{pa};
</code></pre>
<p class="normal">In the preceding example, we do the following:</p>
<ol>
<li class="numberedList" value="1">Create a <code class="inlineCode">std::array</code> container, with an underlying type of <code class="inlineCode">uint8_t</code>.</li>
<li class="numberedList">Construct a monotonic buffer and provide it with the array we just created as the initial buffer.</li>
<li class="numberedList">Use the monotonic buffer to create a polymorphic allocator, which we use to create a vector.</li>
</ol>
<p class="normal">Please note <a id="_idIndexMarker116"/>that the vector is from the <code class="inlineCode">std::pmr</code> namespace, and it’s just a partial specialization of <code class="inlineCode">std::vector</code>, as shown here:</p>
<pre class="programlisting code"><code class="hljs-code">namespace pmr {
    template&lt; class T &gt;
    using vector = std::vector&lt;T, std::pmr::polymorphic_allocator&lt;T&gt;&gt;;
}
</code></pre>
<p class="normal">A vector created by utilizing a monotonic buffer will allocate memory in the space provided by the buffer. Let’s examine the behavior of such a vector in the following example built from the previously explained code:</p>
<pre class="programlisting code"><code class="hljs-code">#include &lt;cstdio&gt;
#include &lt;cstdlib&gt;
#include &lt;array&gt;
#include &lt;memory_resource&gt;
#include &lt;vector&gt;
#include &lt;new&gt;
void *operator new(std::size_t count, std::align_val_t al) {
  printf("%s, size = %ld\r\n", __PRETTY_FUNCTION__, count);
  return std::malloc(count);
}
int main() {
  using namespace std;
  using namespace std::pmr;
  constexpr size_t n_elem = 8;
  array&lt;uint8_t, sizeof(uint8_t) * 8&gt; buffer{0};
  monotonic_buffer_resource mbr{buffer.data(), buffer.size()};
  polymorphic_allocator&lt;uint8_t&gt; pa{&amp;mbr};
  std::pmr::vector&lt;uint8_t&gt; vec{pa};
  //vec.reserve(n_elem);
for (uint8_t i = 0; i &lt; n_elem; i++) {
    vec.push_back(i);
  }
  for (uint8_t data : buffer) {
    printf("%d ", data);
  }
  printf("\r\n");
  return 0;
}
</code></pre>
<p class="normal">The preceding <a id="_idIndexMarker117"/>program will provide the following output:</p>
<pre class="programlisting con"><code class="hljs-con">void* operator new(std::size_t, std::align_val_t), size = 64
0 0 1 0 1 2 3 0
</code></pre>
<p class="normal">We see that even though we used the monotonic buffer, the program called the <code class="inlineCode">new</code> operator. You can notice that the call to the <code class="inlineCode">reserve</code> method is commented. This will result in a vector-expanding strategy, as described previously. When the monotonic buffer initial memory is used, it will fall to the upstream memory resource pointer. The default upstream memory resource will use the <code class="inlineCode">new</code> and <code class="inlineCode">delete</code> operators.</p>
<p class="normal">If we print the buffer used as initial storage for <code class="inlineCode">monotonic_buffer_resource</code>, we can see that the vector is allocating the first element and storing <code class="inlineCode">0</code> to it, then it doubles it and stores <code class="inlineCode">0</code> and <code class="inlineCode">1</code>, and then doubles it again, storing <code class="inlineCode">0</code>, <code class="inlineCode">1</code>, <code class="inlineCode">2</code>, and <code class="inlineCode">3</code>. When it tries to double it again, the monotonic buffer will not be able to meet the allocation request and will fall to using the default allocator, which relies on the <code class="inlineCode">new</code> and <code class="inlineCode">delete</code> operators. We can visualize this in the following figure:</p>
<figure class="mediaobject"><img alt="Figure 2.4 – State of the buffer used by the monotonic buffer resource" src="img/B22402_2_4.png"/></figure>
<p class="packt_figref">Figure 2.4 – State of the buffer used by the monotonic buffer resource</p>
<p class="normal"><em class="italic">Figure 2</em><em class="italic">.4</em> depicts the internal state of the used by the monotonic buffer resource. We can see that the monotonic buffer resource is not deallocating memory in any way. On an allocation buffer request, it returns a pointer to the last available element in the initial buffer if there is enough space in the buffer to fit the requested number of elements.</p>
<p class="normal">You will notice <a id="_idIndexMarker118"/>that the <code class="inlineCode">new</code> operator used in this example has a different signature from the one previously used. Actually, the standard library defines different versions of <code class="inlineCode">new</code> and matching <code class="inlineCode">delete</code> operators, and it’s hard to tell which version is used by a container from the standard library without inspection. This makes overloading them globally and replacing implementation with a custom one even more challenging, making a local allocator usually a better choice.</p>
<p class="normal">The polymorphic allocator utilizing a monotonic buffer initialized with a buffer on the stack may be a good option to mitigate some of the issues imposed by the dynamic memory management when working with containers from the standard C++ library. The approach we demonstrated on the vector can be used on other containers from standard libraries, such as <code class="inlineCode">list</code> and <code class="inlineCode">map</code>, but also other types from the library, such as <code class="inlineCode">basic_string</code>.</p>
<p class="normal">Mitigating concerns of dynamic memory allocation is possible but it still poses some challenges. If you want to be absolutely sure that your C++ program is not calling a <code class="inlineCode">new</code> operator, there are means to ensure it. Let us explore how we can disable unwanted C++ features.</p>
<h1 class="heading-1" id="_idParaDest-45">Disabling unwanted C++ features</h1>
<p class="normal">You may have noticed that we used <code class="inlineCode">printf</code> from the C standard library for printing debug information on <a id="_idIndexMarker119"/>standard output instead of <code class="inlineCode">std::cout</code> from the C++ standard library. The reason is twofold – the implementation of the <code class="inlineCode">std::cout</code> global object from <code class="inlineCode">ostream</code> has a large memory footprint and it uses dynamic memory allocation. C++ works well with the C standard library, and using <code class="inlineCode">printf</code> is a good alternative for resource-constrained systems.</p>
<p class="normal">We already discussed the exception handling mechanism, which often relies on dynamic memory allocation. Disabling exceptions in C++ is as easy as passing the appropriate flag to the compiler. In the <a id="_idIndexMarker120"/>case of GCC, that flag is <code class="inlineCode">–fno-exceptions</code>. The same goes for <strong class="keyWord">Run-Time Type Information</strong> (<strong class="keyWord">RTTI</strong>). We can disable it with the <code class="inlineCode">–fno-rtti</code> flag.</p>
<p class="normal">Disabling exceptions will result in calling <code class="inlineCode">std::terminate</code> when an exception is thrown. We can replace <a id="_idIndexMarker121"/>the default terminate handler with our own implementation and handle it appropriately, as shown in the following example:</p>
<pre class="programlisting code"><code class="hljs-code">#include &lt;cstdio&gt;
#include &lt;cstdlib&gt;
#include &lt;exception&gt;
#include &lt;array&gt;
int main() {
  constexpr auto my_terminate_handler = []() {
    printf("This is my_terminate_handler\r\n");
    std::abort();
  };
  std::set_terminate(my_terminate_handler);
  std::array&lt;int, 4&gt; arr;
  for (int i = 0; i &lt; 5; i++) {
   arr.at(i) = i;
  }
  return 0;
}
</code></pre>
<p class="normal">The preceding example demonstrates setting the terminate handler using <code class="inlineCode">std::set_terminate</code> by our own implementation. This allows us to handle cases that shouldn’t happen in runtime and try to recover from them or gracefully terminate them. Some features or behaviors in C++ can’t be disabled by compiler flags, but there are other means to handle them,</p>
<p class="normal">As we saw previously, we can redefine global <code class="inlineCode">new</code> and <code class="inlineCode">delete</code> operators. We can also delete them, which will make the compilation fail if we use a software component that calls <code class="inlineCode">new</code>, effectively allowing us to prevent any attempts of dynamic memory allocation if needed:</p>
<pre class="programlisting code"><code class="hljs-code">#include &lt;cstdio&gt;
#include &lt;vector&gt;
#include &lt;new&gt;
void *operator new(std::size_t count) = delete;
void *operator new[](std::size_t count) = delete;
void *operator new(std::size_t count, std::align_val_t al) = delete;
void *operator new[](std::size_t count, std::align_val_t al) = delete;
void *operator new(std::size_t count, const std::nothrow_t &amp;tag) = delete;
void *operator new[](std::size_t count, const std::nothrow_t &amp;tag) = delete;
void *operator new(std::size_t count, std::align_val_t al, const std::nothrow_t &amp;) = delete;
void *operator new[](std::size_t count, std::align_val_t al,const std::nothrow_t &amp;) = delete;
int main() {
  std::vector&lt;int&gt; vec;
  vec.push_back(123);
  printf("vec[0] = %d\r\n", vec[0]);
  return 0;
}
</code></pre>
<p class="normal">The preceding <a id="_idIndexMarker122"/>example will fail with the following compiler message (among others):</p>
<pre class="programlisting con"><code class="hljs-con">/usr/include/c++/13/bits/new_allocator.h:143:59: error: use of deleted function 'void* operator new(std::size_t, std::align_val_t)'
  143 |             return static_cast&lt;_Tp*&gt;(_GLIBCXX_OPERATOR_NEW (__n * sizeof(_Tp),
</code></pre>
<p class="normal">By deleting <code class="inlineCode">new</code> operators, we can make the compilation of a C++ program that is trying to use dynamic memory management fail. This is useful if we want to be sure our program is not using dynamic memory management.</p>
<h1 class="heading-1" id="_idParaDest-46">Summary</h1>
<p class="normal">C++ allows a great degree of flexibility. Resource-constrained embedded systems and safety-critical guidelines can impose some limitations on the usage of certain C++ features, such as exception handling, RTTI, and the usage of dynamic memory allocation by containers and other modules from the standard C++ library. C++ acknowledges those concerns and provides mechanisms for disabling unwanted features. In this chapter, we learned about different strategies for mitigating concerns of dynamic memory allocation by means of local allocators and overloading global <code class="inlineCode">new</code> and <code class="inlineCode">delete</code> operators.</p>
<p class="normal">The learning curve is steep but worth the effort, so let’s continue our journey of discovering C++ in embedded systems.</p>
<p class="normal">In the next chapter, we will explore the C++ ecosystem for embedded development.</p>
</div>
</body></html>