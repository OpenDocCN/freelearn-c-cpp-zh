- en: '22'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel Algorithms with the STL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers the topic of C++ parallelism, particularly with the tools
    and techniques introduced in C++17\. Starting with the foundations, the chapter
    unfolds the power of execution policies that allow developers to harness parallel
    processing in their C++ **Standard Template Library** (**STL**) algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to execution policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating execution policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The impact of `constexpr` on algorithms and containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code in this chapter can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Data-Structures-and-Algorithms-with-the-CPP-STL](https://github.com/PacktPublishing/Data-Structures-and-Algorithms-with-the-CPP-STL)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to execution policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processors have transitioned from focusing on increasing the speed of individual
    cores to incorporating multiple cores for enhanced performance. For developers,
    this means the potential to execute multiple instructions concurrently across
    these cores, improving application efficiency and responsiveness.
  prefs: []
  type: TYPE_NORMAL
- en: This move to multi-core configurations highlights the importance of integrating
    parallel programming techniques. With the advent of C++17, C++ made notable progress
    in this domain by introducing the `<``execution>` header.
  prefs: []
  type: TYPE_NORMAL
- en: The <execution> header– enabling parallelism in STL algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before C++17, although the STL provided a comprehensive suite of algorithms,
    they were executed sequentially. This sequential operation meant that STL algorithms
    did not fully utilize the capabilities of multi-core processors.
  prefs: []
  type: TYPE_NORMAL
- en: The `<execution>` header addresses this limitation. Instead of adding new algorithms,
    it enhances existing ones by incorporating parallelism by introducing execution
    policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execution policies serve as directives, indicating to the STL algorithms the
    desired mode of operation: sequential, parallel, or vectorized. With the `<execution>`
    header, developers can specify these preferences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary execution policies include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::execution::seq`: Dictates a sequential execution of the algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par`: Facilitates parallel execution where feasible'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par_unseq`: Supports both parallel and vectorized execution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing parallel execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Integrating parallelism into STL algorithms is straightforward. Consider the
    `std::sort` algorithm. Typically, it’s used in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To employ parallel sorting with the `<execution>` header, the syntax is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This modification equips the `sort` algorithm to leverage multiple cores, potentially
    enhancing the speed of the sorting process.
  prefs: []
  type: TYPE_NORMAL
- en: Reflecting on the transition to parallel STL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While introducing the `<execution>` header and its associated execution policies
    is a notable advancement, it’s essential to approach their usage with discernment.
    Parallelism does introduce overheads, such as thread context-switching and data
    coordination. These overheads can sometimes negate the benefits of parallelism,
    especially for tasks with smaller datasets.
  prefs: []
  type: TYPE_NORMAL
- en: However, when used judiciously, the `<execution>` header can significantly enhance
    application performance. Subsequent sections will provide a more detailed exploration
    of execution policies, enabling developers to utilize them effectively.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, C++17’s `<execution>` header is a pivotal enhancement. Offering
    a mechanism to imbue existing STL algorithms with parallel capabilities equips
    developers with the tools to develop applications optimized for multi-core generation.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating execution policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `<execution>` header, introduced in the C++17 standard, adds a significant
    layer of depth to C++ programming by furnishing a suite of tools designed for
    parallel computation. This header, when used in conjunction with the STL algorithms,
    allows developers to leverage the capabilities of concurrent computing effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Execution policies, a key feature of the `<execution>` header, are instrumental
    in controlling the manner in which STL algorithms execute. By specifying an execution
    policy when invoking an STL algorithm, developers can dictate whether the algorithm
    should run sequentially, in parallel, or in parallel with vectorization. This
    level of control can lead to substantial performance improvements, particularly
    in applications that are computationally intensive or that operate on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, the `<execution>` header and its associated execution policies represent
    a powerful toolset for C++ developers. They provide a means to tap into the potential
    of modern multi-core processors and distributed computing environments, thereby
    enabling more efficient and faster code execution.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating policies with standard algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Execution policies serve as directives for STL algorithms, indicating the preferred
    mode of operation. For those familiar with STL algorithms, integrating these policies
    requires minimal modification to existing code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider `std::for_each`, an algorithm that acts on each element in a collection.
    By default, it operates sequentially:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For large datasets or computationally demanding operations within the lambda
    function, parallel execution can be beneficial. This can be achieved by simply
    introducing an execution policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With the inclusion of `std::execution::par`, the algorithm is now prepared for
    parallel execution.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding parallel execution policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two primary parallel execution policies:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::execution::par`: This indicates that an algorithm may be executed in
    parallel. It allows the implementation to decide on parallelism based on the specific
    context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par_unseq`: This goes further by suggesting parallelism and
    allowing for vectorization. This means that multiple loop iterations might execute
    concurrently on a single processor core when supported by the hardware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, the `std::transform` algorithm, which applies a function to each
    collection element, can utilize these policies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Each `vec` element is squared, and the result populates the output. The `std::execution::par_unseq`
    policy indicates the potential parallelization and vectorization of this operation.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the appropriate execution policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While execution policies enhance parallel computation capabilities, they must
    be applied thoughtfully. Not every dataset or algorithm will gain from parallel
    execution, and sometimes, the overhead might negate the advantages for smaller
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The `std::execution::seq` policy explicitly opts for sequential execution, ensuring
    the algorithm operates in a single-threaded mode. This is beneficial when parallelism
    introduces undue overhead or in contexts where parallel execution is not recommended.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also vital to be wary of potential issues when utilizing parallel policies
    with algorithms that possess side effects or necessitate synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: C++17’s execution policies facilitate straightforward access to parallelism.
    Pairing these with traditional STL algorithms allows developers to use multi-core
    processors optimally. Whether utilizing `std::transform` on a vast dataset, sorting
    large collections with `std::sort`, or filtering items using `std::remove_if`,
    execution policies provide an added performance dimension.
  prefs: []
  type: TYPE_NORMAL
- en: However, always validate that parallel execution genuinely augments your application
    without ushering in unforeseen challenges or bottlenecks. It’s imperative to evaluate
    and test your code continually.
  prefs: []
  type: TYPE_NORMAL
- en: With this foundation, we’re poised to consider performance considerations in
    the upcoming section. Through discerning the application of parallelism, we can
    develop efficient C++ applications tailored to contemporary computational demands.
  prefs: []
  type: TYPE_NORMAL
- en: The impact of constexpr on algorithms and containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the introduction of the `constexpr` specifier in C++11 and its enhancements
    in subsequent versions, compile-time computation in C++ has taken a significant
    leap. The ability for functions and constructors to operate at compile time via
    `constexpr` enables optimization and assurance of specific attributes before the
    code runs. This section examines the integration of `constexpr` within the STL,
    particularly concerning algorithms and containers.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of constexpr
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In its infancy during C++11, `constexpr` was primarily for straightforward computations.
    The C++14 extension broadened its scope, embracing loops and conditional structures.
    By C++20, there was further augmentation allowing for `constexpr` allocations
    via `std::allocator`. This made containers such as `std::vector` and `std::string`
    usable with `constexpr`, though with certain restrictions.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms and the role of constexpr
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Originally, `constexpr` wasn’t widely applicable to STL algorithms due to their
    generic design and multifaceted requirements. However, with the C++20 standard,
    more STL algorithms became `constexpr`-compatible. This means that provided all
    inputs are constant expressions, it is possible to compute algorithmic outcomes
    at compile time.
  prefs: []
  type: TYPE_NORMAL
- en: Take, for example, the `std::find` or `std::count` functions. When used on static
    data structures such as arrays or `std::array`, they can operate during the compilation
    phase. However, as of C++20, dynamic allocation remains mainly outside the domain
    of `constexpr`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet uses `std::array` to highlight the use of `std::find`
    and `std::count` with `constexpr`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the example output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `contains` and `countOccurrences` functions in the preceding code are evaluated
    at compile time because they operate on a `constexpr`-compatible `std::array`,
    and all their inputs are constant expressions.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that parallel algorithms using execution policies such as
    `std::execution::par` are not suitable for `constexpr` contexts due to their inherent
    reliance on runtime resources.
  prefs: []
  type: TYPE_NORMAL
- en: Containers and constexpr integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C++20’s capability for compile-time allocations enabled specific STL containers
    to function within a `constexpr` environment. While `std::array` was always compatible,
    even some operations on `std::vector` and `std::string` became feasible. Nonetheless,
    any operation requiring dynamic memory or leading to undefined behavior will result
    in a compile-time error within a `constexpr` context.
  prefs: []
  type: TYPE_NORMAL
- en: The trajectory of `constexpr` indicates an evolving C++ environment where the
    lines between compile-time and runtime evaluation become increasingly indistinct.
    We might soon see more advanced algorithms and containers being evaluated entirely
    during compilation, optimizing performance and code safety.
  prefs: []
  type: TYPE_NORMAL
- en: However, the convergence of `constexpr` and parallel algorithms remains an uncertain
    prospect due to the fundamental runtime nature of parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, `constexpr` has undeniably reshaped C++ development. As it integrates
    more deeply into the STL, developers have more avenues to refine and solidify
    their applications.
  prefs: []
  type: TYPE_NORMAL
- en: Performance considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel algorithms are a cornerstone in exploiting the capabilities of multi-core
    processors, aiming to enhance computational efficiency and performance. However,
    the journey from sequential to parallel programming is not straightforward. It
    requires a deep understanding of the inherent complexities and trade-offs. In
    this section, we will explore the various facets of parallel algorithms, including
    their potential for performance improvement, the challenges of parallel execution,
    optimal data sizing for parallelism, synchronization issues, and the subtleties
    of balancing workloads across threads. This comprehensive overview will provide
    a deeper insight into the effective utilization of parallel algorithms, underlining
    the importance of informed decision-making and profiling in achieving optimal
    performance in a parallel computing environment.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel algorithms present both opportunities and challenges for performance
    enhancement. While they offer the potential for faster computations in multi-core
    processing environments, their practical use requires careful consideration and
    decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism overhead
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As developers experiment with parallel solutions, it’s essential to understand
    that parallel execution doesn’t uniformly benefit all algorithms or scenarios.
    There can be overheads, such as those associated with initiating multiple threads
    and data synchronization. For example, for small datasets, the overhead of thread
    management can surpass the computation time, making parallelism less efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Determining optimal data size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parallel execution reveals its benefits beyond a specific data size threshold.
    This threshold is influenced by factors such as the algorithm employed, the computation’s
    nature, and the hardware specifications. A resource-intensive task with a large
    dataset is typically well-suited for parallelism, whereas smaller datasets might
    be more efficiently processed sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the data and computation type is critical to optimize performance.
    Profiling becomes invaluable, helping developers evaluate their code’s runtime
    behavior and decide when to employ parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Data access and synchronization challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Concurrency leads to scenarios where multiple threads access the same resources
    concurrently. Data contention can arise, especially with frequent shared data
    access. Implementing proper synchronization is vital to prevent data inconsistencies.
    However, synchronization has its associated overheads.
  prefs: []
  type: TYPE_NORMAL
- en: False sharing – a subtle performance issue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even if threads access distinct data, false sharing can still occur. This happens
    when threads on different cores modify variables on the same cache line, leading
    to cache invalidations and potential performance degradation. It’s crucial to
    be mindful of data layout and aim for cache-optimized code.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Different computational tasks may require varying processing times. If threads
    finish their tasks at different rates, it can result in resource underutilization.
    Practical parallel algorithms ensure that workloads are distributed uniformly
    across threads. Some advanced parallel techniques, such as work stealing, can
    dynamically reallocate tasks to maintain consistent thread engagement.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A consistent theme in performance optimization is the essential role of profiling.
    Relying solely on assumptions is not advisable. Profiling tools such as `perf`
    and `gprof` and advanced tools such as Intel® VTune™ can identify performance
    bottlenecks, thread behaviors, and contention areas. These tools provide concrete
    data to fine-tune parallel strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this section, we reviewed the performance considerations when working
    with parallel algorithms. We learned that while parallel algorithms can significantly
    enhance computational efficiency, their effective use requires a nuanced understanding
    of various factors. We discussed the potential overheads associated with parallel
    execution, such as thread initiation and data synchronization. We also highlighted
    the importance of determining the optimal data size for parallel execution, emphasizing
    that parallelism may not be beneficial for all scenarios, particularly those involving
    small datasets. We further explored the challenges of data access and synchronization
    in a concurrent environment, including the issue of false sharing. We also touched
    upon the concept of load balancing, explaining how uneven distribution of computational
    tasks can lead to resource underutilization. We discussed advanced techniques
    such as work stealing that can help maintain consistent thread engagement by dynamically
    reallocating tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The insights gained from this section are invaluable as they equip us with the
    knowledge to make informed decisions when implementing parallel algorithms. Understanding
    these performance considerations allows us to exploit the full potential of multi-core
    processors while avoiding common pitfalls. This knowledge is crucial in today’s
    multi-core processing environment, enabling us to write more efficient and performant
    code. It also sets the stage for our continued exploration of data structures
    and algorithms with the C++ STL, as we strive to deepen our understanding and
    enhance our programming skills.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced parallel algorithms within the STL. We began by acquainting
    ourselves with the `<execution>` header introduced in C++17, which has been pivotal
    in enabling parallelism in STL algorithms. This addition allows us to specify
    execution policies such as `std::execution::seq`, `std::execution::par`, and `std::execution::par_unseq`,
    thereby dictating the execution mode of STL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We progressed to implementing these execution policies in standard algorithms,
    demonstrating the simplicity of transitioning from sequential to parallel execution.
    This was exemplified by adapting algorithms such as `std::sort` and `std::for_each`
    to run in parallel, thus harnessing the computational power of multiple cores.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter then focused on the `constexpr` specifier and its profound impact
    on STL algorithms and containers. We explored the evolution of `constexpr` from
    C++11 through C++20 and its role in enabling compile-time computations for algorithms
    such as `std::find` and `std::count`.
  prefs: []
  type: TYPE_NORMAL
- en: Performance considerations formed the crux of our final discussion, underscoring
    the benefits and potential pitfalls of employing parallel algorithms. We addressed
    the overheads associated with parallelism, the importance of determining optimal
    data size, and strategies for effective data access and synchronization to avoid
    issues such as false sharing and load imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: The information imparted in this chapter is invaluable for leveraging the STL’s
    capabilities in a multi-core processing environment. We can write more efficient
    and responsive code by understanding when and how to apply parallel algorithms.
    This deepened comprehension of parallel execution policies and the ability to
    optimize code with `constexpr` equips us to maximize performance and resource
    utilization.
  prefs: []
  type: TYPE_NORMAL
