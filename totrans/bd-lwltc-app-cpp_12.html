<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-297"><a id="_idTextAnchor310"/>12</h1>
<h1 id="_idParaDest-298"><a id="_idTextAnchor311"/>Analyzing and Optimizing the Performance of Our C++ System</h1>
<p>In this chapter, we will analyze the performance of our electronic trading ecosystem based on the measurements we added in the previous chapter, <em class="italic">Adding instrumentation and measuring performance</em>. Using the insights we develop about the performance of our trading systems based on this analysis, we will learn what areas to focus on in terms of potential performance bottlenecks and what areas we can improve. We will discuss tips and techniques for optimizing our C++ trading ecosystem. Finally, we will think about the future of our electronic trading ecosystem and what enhancements can be made in the future.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Analyzing the performance of our trading ecosystem</li>
<li>Discussing tips and techniques for optimizing our C++ trading system</li>
<li>Thinking about the future of our trading ecosystem</li>
</ul>
<h1 id="_idParaDest-299"><a id="_idTextAnchor312"/>Technical requirements</h1>
<p>All the book’s code can be found in the GitHub repository for this book at <a href="https://github.com/PacktPublishing/Building-Low-Latency-Applications-with-CPP">https://github.com/PacktPublishing/Building-Low-Latency-Applications-with-CPP</a>. The source code for this chapter is in the <code>Chapter12</code> directory in the repository.</p>
<p>Since this is the concluding chapter of this book and we will discuss tips for improving the performance of the full electronic trading ecosystem as well as future enhancements, we expect you to have gone through all the preceding chapters.</p>
<p>The specifications of the environment in which the source code for this book was developed are listed as follows. We have presented the details of this environment since all the C++ code presented in this book is not necessarily portable and might require some minor changes to work in your environment:</p>
<ul>
<li>OS: <code>Linux 5.19.0-41-generic #42~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Apr 18 17:40:00 UTC 2 x86_64 x86_64 </code><code>x86_64 GNU/Linux</code></li>
<li>GCC: <code>g++ (Ubuntu </code><code>11.3.0-1ubuntu1~22.04.1) 11.3.0</code></li>
<li>CMake: <code>cmake </code><code>version 3.23.2</code></li>
<li>Ninja: <code>1.10.2</code></li>
</ul>
<p>Additionally, for those who are interested in running the <em class="italic">optional</em> Python Jupyter notebook included with this chapter, the following environment was used. We will not discuss the installation process for Python, Jupyter, and these libraries and assume that you will figure it out on your own:</p>
<pre class="source-code">
-----
Python 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]
Linux-5.19.0-43-generic-x86_64-with-glibc2.35
-----
IPython             8.13.2
jupyter_client      8.2.0
jupyter_core        5.3.0
notebook            6.5.4
-----
hvplot              0.8.3
numpy               1.24.3
pandas              2.0.1
plotly              5.14.1
-----</pre>
<h1 id="_idParaDest-300"><a id="_idTextAnchor313"/>Analyzing the performance of our trading ecosystem</h1>
<p>Before we analyze the performance of our<a id="_idIndexMarker1536"/> electronic trading ecosystem, let us quickly recap the measurements we added in the previous chapter.</p>
<h2 id="_idParaDest-301"><a id="_idTextAnchor314"/>Revisiting the latencies we measure</h2>
<p>We added two <a id="_idIndexMarker1537"/>forms of measurement. The first one measures the performance of internal components and the second one generates timestamps at key points in our entire system.</p>
<p>The first form, which measures the latencies of internal components, generates differences in <code>RDTSC</code> values before and after calls to different functions, and generates log entries such as the following:</p>
<pre class="source-code">
exchange_order_server.log:18:48:29.452140238 RDTSC Exchange_FIFOSequencer_addClientRequest 26
trading_engine_1.log:18:48:29.480664387 RDTSC Trading_FeatureEngine_onOrderBookUpdate 39272
trading_engine_1.log:18:48:29.480584410 RDTSC Trading_MarketOrderBook_addOrder 176
trading_engine_1.log:18:48:29.480712854 RDTSC Trading_OrderManager_moveOrder 32
trading_engine_1.log:18:48:29.254832602 RDTSC Trading_PositionKeeper_addFill 94350
trading_engine_1.log:18:48:29.480492650 RDTSC Trading_RiskManager_checkPreTradeRisk 1036
...</pre>
<p>The second form, which measures the latencies at key points in the trading ecosystem, generates absolute timestamp values and generates log entries such as the following:</p>
<pre class="source-code">
trading_engine_1.log:18:48:29.440526826 TTT T10_TradeEngine_LFQueue_write 1686008909440526763
exchange_order_server.log:18:48:29.452087295 TTT T1_OrderServer_TCP_read 1686008909452087219
exchange_market_data_publisher.log:18:48:29.467680305 TTT T5_MarketDataPublisher_LFQueue_read 1686008909467680251
trading_market_data_consumer_1.log:18:48:29.478030090 TTT T8_MarketDataConsumer_LFQueue_write 1686008909478029956
trading_engine_1.log:18:48:29.480552551 TTT T9_TradeEngine_LFQueue_read 1686008909480552495
...</pre>
<p>Now, let us move <a id="_idIndexMarker1538"/>forward and analyze these latency measurements.</p>
<h2 id="_idParaDest-302"><a id="_idTextAnchor315"/>Analyzing the performance</h2>
<p>To analyze these<a id="_idIndexMarker1539"/> performance metrics, we have built a Python Jupyter notebook, which is available at <code>Chapter12/notebooks/perf_analysis.ipynb</code>. Note that since this is a book about C++ and low-latency applications, we will not discuss the source code in this notebook, but instead describe the analysis. Running the notebook is optional, so we also included an HTML file with the results of this analysis, which is available at <code>Chapter12/notebooks/perf_analysis.html</code>. To run this notebook, you will first have to launch the <code>jupyter notebook</code> server from the <code>Chapter12</code> root directory (where the log files exist) using the following command:</p>
<pre class="source-code">
sghosh@sghosh-ThinkPad-X1-Carbon-3rd:~/Building-Low-Latency-Applications-with-CPP/Chapter12$ jupyter notebook
...
    To access the notebook, open this file in a browser:
        file:///home/sghosh/.local/share/jupyter/runtime/nbserver-182382-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=d28e3bd3b1f8109b12afe1210ae8c494c7 7a4128e23bdae7
     or http://127.0.0.1:8888/?token=d28e3bd3b1f8109b12afe1210ae8c494c7 7a4128e23bdae7</pre>
<p>If your browser does not already launch the web page for this notebook, you can copy and paste the URL you receive and navigate to and open the <code>notebooks/perf_analysis.ipynb</code> notebook. Note that the preceding addresses are just examples for this specific run; you will receive a different address, which you should <a id="_idIndexMarker1540"/>use. Once you open the notebook, you can run it using <strong class="bold">Cell</strong> | <strong class="bold">Run All</strong>, or the closest equivalent in your notebook instance, as shown in the following screenshot.</p>
<div><div><img alt="Figure 12.1 – Screenshot of the perf_analysis.ipynb notebook" src="img/B19434_12_001.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Screenshot of the perf_analysis.ipynb notebook</p>
<p>Since we will not discuss the details of this notebook, we will briefly describe the analysis performed in it. This notebook performs the following steps in the order presented here:</p>
<ol>
<li>First, it looks for the log files generated by running the electronic trading ecosystem in the current working directory. Specifically, it looks for log files from the trading exchange; in the case of this notebook, we look for log files from the trading client with <code>ClientId=1</code>.</li>
<li>It opens<a id="_idIndexMarker1541"/> each log file and looks for log entries that contain the <code>RDTSC</code> and <code>TTT</code> tokens in them to find the log entries corresponding to the measurements we discussed in the previous chapter and revisited in the preceding sub-section.</li>
<li>It then creates two <code>pandas</code> <code>DataFrame</code> instances containing each of the measurements it extracts from the log files.</li>
<li>For the measurement entries corresponding to the measurement of internal functions, which are tagged with the <code>RDTSC</code> token, we generate a scatter plot of those measurements as well as a rolling mean of those plots (to smooth the overall latency measurements). One crucial point here is that the measurement values in the log files represent the difference in <code>RDTSC</code> values, that is, the number of CPU cycles elapsed for a function call. In this notebook, we convert the CPU cycles into nanoseconds using a constant factor of 2.6 GHz, which is specific to our system and will differ based on your hardware; it will need to be adjusted. We will look at a few examples of these plots in the next sub-section.</li>
<li>For the measurement entries corresponding to the timestamps at key spots in our electronic trading ecosystem, which are tagged with the <code>TTT</code> token, we also generate a scatter plot and a plot of the rolling mean values. The difference here is that we display the transit times from one hop to the other. For instance, we will plot the time it takes from the hop at <code>T1_OrderServer_TCP_read</code> to the hop at <code>T2_OrderServer_LFQueue_write</code>, from <code>T2_OrderServer_LFQueue_write</code> to <code>T3_MatchingEngine_LFQueue_read</code>, from <code>T3_MatchingEngine_LFQueue_read</code> to <code>T4_MatchingEngine_LFQueue_write</code>, and so forth.</li>
</ol>
<p>Each of these inter-hop transits on the side of the exchange is shown in the following diagram.</p>
<div><div><img alt="Figure 12.2 – Flow of data between different hops at the electronic exchange" src="img/B19434_12_002.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Flow of data between different hops at the electronic exchange</p>
<p>Each of these inter-hop<a id="_idIndexMarker1542"/> transits on the side of the trading client is shown in the following diagram.</p>
<div><div><img alt="Figure 12.3 – Flow of data between different hops on the electronic trading client" src="img/B19434_12_003.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Flow of data between different hops on the electronic trading client</p>
<p>In the next sub-section, we will observe the distribution of a few of these different latency metrics from both groups (<code>RDTSC</code> and <code>TTT</code>) and see what we can learn from them.</p>
<h2 id="_idParaDest-303"><a id="_idTextAnchor316"/>Understanding the output of our analysis</h2>
<p>In this section, we will present the distribution of the latencies for a subset of the measurements we added in the previous chapter and analyzed using the notebook presented in the previous sub-section. Our objective here is to gain some insight into the performance of different<a id="_idIndexMarker1543"/> components and sub-components in our ecosystem. First, we will start with a few examples of latencies for internal function calls in the next sub-section. One thing to note is that for the sake of brevity, we will present and discuss a subset of all the performance plots available in the Python notebook in this chapter. Also, note that these are not arranged in any particular order; we simply picked some of the more interesting ones and left all possible plots in the notebook for you to inspect further.</p>
<h3>Observing the latencies for internal function calls</h3>
<p>The first performance plot we present in this chapter is the distribution of the latency of calling the <code>Exchange::MEOrderBook::removeOrder()</code> method in the matching engine inside the trading exchange. That is presented as follows, but our key takeaway here<a id="_idIndexMarker1544"/> is that this is a very well-behaved function; that is, the minimum and maximum latencies are within a tight range between 0.4 and 3.5 microseconds and the mean is relatively stable around the 1-to-1.5-microsecond range. There might be the possibility to make this faster, of course, but this seems quite well behaved for now and has low-performance latencies; we should evaluate whether this method is a bottleneck before trying to optimize it any further.</p>
<div><div><img alt="Figure 12.4 – Latency distribution for the removeOrder() method in MEOrderBook for the matching engine" src="img/B19434_12_004.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Latency distribution for the removeOrder() method in MEOrderBook for the matching engine</p>
<p>The next plot presents the distribution of latencies for the <code>Exchange::FIFOSequencer::</code><code> sequenceAndPublish()</code> method. This instance is more interesting because here we see that while this method has low average latencies in<a id="_idIndexMarker1545"/> the 90 microseconds range, it experiences many spikes in latencies spiking up to values in the 500 to 1,200 microseconds range. This behavior will result in jitter in the <code>OrderServer</code> component’s performance when it comes to processing client order requests and is something we might need to investigate.</p>
<div><div><img alt="Figure 12.5 – Latency distribution of the sequenceAndPublish() method in FIFOSequencer for the matching engine" src="img/B19434_12_005.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Latency distribution of the sequenceAndPublish() method in FIFOSequencer for the matching engine</p>
<p>The next plot shows another interesting distribution of latency values for the <code>Trading::PositionKeeper::addFill()</code> method. In this case, the average performance latency <a id="_idIndexMarker1546"/>remains stable around the 50 microseconds range. However, between <strong class="bold">15:28:00</strong> and <strong class="bold">15:29:00</strong>, there are a few spikes in latency that warrant a closer look. The difference here compared to <em class="italic">Figure 12</em><em class="italic">.4</em> is that there the spikes were distributed evenly, but in this case, there appears to be a small patch of spikes.</p>
<div><div><img alt="Figure 12.6 – Latency distribution of the addFill() method in PositionKeeper for the trade engine" src="img/B19434_12_006.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Latency distribution of the addFill() method in PositionKeeper for the trade engine</p>
<p>We conclude this sub-section by presenting one more plot, this time of the <code>Trading::</code><code> PositionKeeper::updateBBO()</code> method, which updates the PnL for open positions. This is<a id="_idIndexMarker1547"/> another well-behaved method with an average performance latency of 10 microseconds, and there seem to be many measurements close to 0 microseconds, which is slightly different from <em class="italic">Figure 12</em><em class="italic">.3</em>, where the minimum latency value was never remarkably close to 0.</p>
<div><div><img alt="Figure 12.7 – Latency distribution of the updateBBO() method in PositionKeeper for the trade engine" src="img/B19434_12_007.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – Latency distribution of the updateBBO() method in PositionKeeper for the trade engine</p>
<p>In the next<a id="_idIndexMarker1548"/> sub-section, we will look at a few similar examples, but this time pertaining to the latencies between the different hops in our ecosystem.</p>
<h3>Observing the latencies between hops in the ecosystem</h3>
<p>The first plot we will look at is the<a id="_idIndexMarker1549"/> time difference between when a trading client’s <code>OrderGateway</code> component writes a client request to the TCP socket (<code>T12</code>) up to the point when the exchange’s <code>OrderServer</code> component reads that client request from the TCP socket (<code>T1</code>). This represents the network transit time from the trading client to the trading exchange on the TCP connection. The average latency in this case is around 15 to 20 microseconds and the distribution is evenly distributed.</p>
<div><div><img alt="Figure 12.8 – Latency distribution between the T12_OrderGateway_TCP_write and T1_OrderServer_TCP_read hops" src="img/B19434_12_008.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – Latency distribution between the T12_OrderGateway_TCP_write and T1_OrderServer_TCP_read hops</p>
<p>The next plot displays the distribution of the network transit time for the market data updates, from when the market data<a id="_idIndexMarker1550"/> updates are written to the UDP socket by <code>MarketDataPublisher</code> (<code>T6</code>) to when they are read from the UDP socket by <code>MarketDataConsumer</code> (<code>T7</code>). There seems to be a great amount of variance in the latencies for this measurement, as the plot shows; however, this has lower overall latencies than the TCP path.</p>
<div><div><img alt="Figure 12.9 – Latency distribution between the T6_MarketDataPublisher_UDP_write and T7_MarketDataConsumer_UDP_read hops" src="img/B19434_12_009.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – Latency distribution between the T6_MarketDataPublisher_UDP_write and T7_MarketDataConsumer_UDP_read hops</p>
<p>The next diagram shows the<a id="_idIndexMarker1551"/> distribution of latencies measured from <code>MarketDataConsumer</code> reading a market update from the UDP socket (<code>T7</code>) to the time when the market update is written to <code>LFQueue</code> connected to <code>TradeEngine</code> (<code>T8</code>). This path experiences huge spikes in latencies (up to 2,000 microseconds) compared to its average performance of around 100 microseconds, so this is something we need to investigate.</p>
<div><div><img alt="Figure 12.10 – Latency distribution between the T7_MarketDataConsumer_UDP_read and T8_MarketDataConsumer_LFQueue_write hops" src="img/B19434_12_010.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.10 – Latency distribution between the T7_MarketDataConsumer_UDP_read and T8_MarketDataConsumer_LFQueue_write hops</p>
<p>The next plot displays<a id="_idIndexMarker1552"/> the distribution of the latencies between <code>MatchingEngine</code> reading a client request from <code>LFQueue</code> attached to <code>OrderServer</code> (<code>T3</code>) and the time <code>MatchingEngine</code> processes it and writes the client response to <code>LFQueue</code> back to <code>OrderServer</code> (<code>T4t</code>). This path also appears to be experiencing large latency spikes and should be investigated.</p>
<div><div><img alt="Figure 12.11 – Latency distribution between the T3_MatchingEngine_LFQueue_read and T4t_MatchingEngine_LFQueue_write hops" src="img/B19434_12_011.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.11 – Latency distribution between the T3_MatchingEngine_LFQueue_read and T4t_MatchingEngine_LFQueue_write hops</p>
<p>This section was <a id="_idIndexMarker1553"/>dedicated to the analysis of the different latency measurements in our ecosystem. In the next section, we will discuss some tips and techniques that we can use to optimize the design and implementation of the different components in our electronic trading ecosystem.</p>
<h1 id="_idParaDest-304"><a id="_idTextAnchor317"/>Discussing tips and techniques for optimizing our C++ trading system</h1>
<p>In this section, we will present a few <a id="_idIndexMarker1554"/>possible areas where we can optimize our C++ trading ecosystem. Note that these are only some examples and a lot more is possible, but we will leave you to measure and discover those inefficiencies, as well as improve on them. To reiterate what we have mentioned a few times before, you should measure the performance of various parts of your system with everything we learned in the previous chapter, <em class="italic">Adding instrumentation and measuring performance</em>. You should analyze them using the approach we discussed in this chapter and use the C++ discussions we had in the chapter <em class="italic">Exploring C++ Concepts from a Low-Latency Application’s Perspective</em> to improve on them further. Now, let us discuss some areas of improvement next. We have tried to arrange these<a id="_idIndexMarker1555"/> loosely in order from least to most effort.</p>
<h2 id="_idParaDest-305"><a id="_idTextAnchor318"/>Optimizing the release build</h2>
<p>The first suggestion would be to try and optimize the release build we run for our system. Some simple things we<a id="_idIndexMarker1556"/> can do in the code itself is remove the calls to <code>ASSERT()</code> from the release binaries. The motivation behind this is to remove the extra <code>if</code> condition this macro introduces in our code base wherever it gets used. However, this can be dangerous since we might allow exceptional conditions through. The optimal middle ground is to remove the use of this macro only from the critical code path wherever it is safe to do so.</p>
<p>Another suggestion would be to reduce logging in the release build. We have made a decent amount of effort to make logging efficient and low-latency. Additionally, it is not wise to eliminate all logging since it makes troubleshooting difficult, if not impossible. However, logging is not free, so we should try to reduce logging on the critical path for release builds as much as possible.</p>
<p>The most common method to perform optimizations, as we suggested here, that only apply to release builds is to define the NDEBUG (No Debug) preprocessor flag and check for its existence in our code base. If the flag is defined, we build a release build and skip non-essential code such as asserts and logging.</p>
<p>An example of this for the <code>MemoryPool::deallocate()</code> method is shown here:</p>
<pre class="source-code">
    auto deallocate(const T *elem) noexcept {
      const auto elem_index = (reinterpret_cast&lt;const
         ObjectBlock *&gt;(elem) - &amp;store_[0]);
#if !defined(NDEBUG)
      ASSERT(elem_index &gt;= 0 &amp;&amp; static_cast&lt;size_t&gt;
       (elem_index) &lt; store_.size(), "Element being
      deallocated does not belong to this Memory pool.");
      ASSERT(!store_[elem_index].is_free_, "Expected in-use
         ObjectBlock at index:" + std::
            to_string(elem_index));
#endif
      store_[elem_index].is_free_ = true;
    }</pre>
<p>Another example <a id="_idIndexMarker1557"/>for the <code>FIFOSequencer::sequenceAndPublish()</code> method is shown here:</p>
<pre class="source-code">
    auto sequenceAndPublish() {
      ...
#if !defined(NDEBUG)
      logger_-&gt;log("%:% %() % Processing % requests.\n",
          __FILE__, __LINE__, __FUNCTION__, Common::
            getCurrentTimeStr(&amp;time_str_), pending_size_);
#endif
      ...
      for (size_t i = 0; i &lt; pending_size_; ++i) {
        const auto &amp;client_request =
           pending_client_requests_.at(i);
#if !defined(NDEBUG)
        logger_-&gt;log("%:% %() % Writing RX:% Req:% to
         FIFO.\n", __FILE__, __LINE__, __FUNCTION__,
          Common::getCurrentTimeStr(&amp;time_str_),
                     client_request.recv_time_,
                        client_request.request_.toString());
#endif
    ...
    }</pre>
<p>Another thing to think about is whether the actual entries being logged can be output in a more optimal method. For instance, <code>Common:: getCurrentTimeStr()</code>, which gets called in each of our log lines in the current code base state itself, is quite expensive. This is because it performs string formatting operations using <code>sprintf()</code>, which is quite expensive, like most string formatting operations. Here, we have another optimization where in<a id="_idIndexMarker1558"/> release builds, we can output a simple integer value representing time, instead of a formatted string, which, while more readable, is less efficient.</p>
<p>Let us move on to the next possible optimization area – managing thread affinity.</p>
<h2 id="_idParaDest-306"><a id="_idTextAnchor319"/>Setting thread affinity correctly</h2>
<p>So far, in all the instances of creating and launching threads, we have passed the <code>core_id</code> parameter to be <code>-1</code> in the call to the <code>Common::createAndStartThread()</code> method; that is, the threads were not pinned to any specific core. This was<a id="_idIndexMarker1559"/> done intentionally since, as we mentioned before, the <code>exchange_main</code> application instance creates and runs 10 threads and each <code>trading_main</code> application instance creates and runs 8 threads. Unless you are executing the source code for this book on a production-grade trading server, it is unlikely to have too many CPU cores. Our system, for example, has only four cores. In practice, however, each of the following performance-critical threads would be assigned a CPU core all to themselves. We present a sample core assignment next; however, this will change from server to server and might also depend on the NUMA architecture – but that is beyond the scope of this book. Note that these names refer to the names we passed to the method in the <code>name</code> string parameter:</p>
<ul>
<li><code>core_id</code>=0 : <code>Exchange/MarketDataPublisher</code></li>
<li><code>core_id</code>=1 : <code>Exchange/MatchingEngine</code></li>
<li><code>core_id</code>=2 : <code>Exchange/OrderServer</code></li>
<li><code>core_id</code>=3 : <code>Trading/MarketDataConsumer</code></li>
<li><code>core_id</code>=4 : <code>Trading/OrderGateway</code></li>
<li><code>core_id</code>=5 : <code>Trading/TradeEngine</code></li>
<li>Any additional performance critical threads get assigned the remaining core ids in a similar fashion</li>
</ul>
<p>The remaining non-critical threads, as well as any Linux processes running on the server, would be given a block of CPU<a id="_idIndexMarker1560"/> cores to be run on without any affinity settings. Specifically, in our system, they would be the following non-critical threads:</p>
<ul>
<li><code>core_id</code>=-1 : <code>Exchange/SnapshotSynthesizer</code></li>
<li><code>core_id</code>=-1 : <code>Common/Logger exchange_main.log</code></li>
<li><code>core_id</code>=-1 : <code>Common/Logger exchange_matching_engine.log</code></li>
<li><code>core_id</code>=-1 : <code>Common/Logger exchange_market_data_publisher.log</code></li>
<li><code>core_id</code>=-1 : <code>Common/Logger exchange_snapshot_synthesizer.log</code></li>
<li><code>core_id</code>=-1 : <code>Common/Logger exchange_order_server.log</code></li>
<li><code>core_id</code>=-1 : <code>Common/Logger trading_main_1.log</code></li>
<li><code>core_id</code>=-1 : <code>Common/Logger trading_engine_1.log</code></li>
<li><code>core_id</code>=-1 : <code>Common/Logger trading_order_gateway_1.log</code></li>
<li><code>core_id</code>=-1 : <code>Common/Logger trading_market_data_con</code><code>sumer_1.log</code></li>
<li>Any other non-critical threads would also be assigned core id -1 i.e. these threads will not be pinned to any specific CPU code</li>
</ul>
<p>Note one additional detail: for this setup to be as optimized as possible, we need to make sure that the Linux process scheduler does not assign any OS processes to the CPU cores being used by the critical threads. This is achieved on Linux using the <code>isolcpus</code> kernel parameter, which we will not discuss in detail here. The <code>isolcpus</code> parameter tells the process scheduler which cores to ignore when deciding where to schedule a process.</p>
<h2 id="_idParaDest-307"><a id="_idTextAnchor320"/>Optimizing Logger for strings</h2>
<p>There is an opportunity to <a id="_idIndexMarker1561"/>optimize our <code>Logger</code> class to handle parameters of the <code>char*</code> type better. Remember that our implementation for logging <code>char*</code> parameters consists of calling the <code>Logger::pushValue(const char value)</code> method on each of the characters iteratively, as shown:</p>
<pre class="source-code">
    auto pushValue(const char *value) noexcept {
      while (*value) {
        pushValue(*value);
        ++value;
      }
    }</pre>
<p>One option here is to introduce a new enumeration value to the <code>LogType</code> enumeration. Let’s call it <code>STRING</code>, like so:</p>
<pre class="source-code">
  enum class LogType : int8_t {
    ...
    DOUBLE = 8,
    STRING = 9
  };</pre>
<p>We’ll update the <code>LogElement</code> type to have a fixed-size <code>char*</code> array of <em class="italic">some</em> size, as shown. We are vague on the size of this array on purpose since this is pseudo-code and we want to focus more on the design and the idea and less on the implementation details:</p>
<pre class="source-code">
  struct LogElement {
    LogType type_ = LogType::CHAR;
    union {
      ...
      double d;
      char str[SOME_SIZE];
    } u_;
  };</pre>
<p>Then, finally, update <code>Logger::pushValue(const char *value)</code> and <code>Logger::flushQueue()</code> to copy <a id="_idIndexMarker1562"/>and write the strings in blocks of characters rather than a single character at a time.</p>
<h2 id="_idParaDest-308"><a id="_idTextAnchor321"/>Eliminating the use of std::function instances</h2>
<p>In our <a id="_idIndexMarker1563"/>code base, we used the <code>std::function&lt;&gt;</code> function wrapper in a couple of places, as listed here:</p>
<ul>
<li><code>Common::McastSocket</code>:<pre class="source-code">
std::function&lt;void(McastSocket *s)&gt; recv_callback_;</pre></li>
<li><code>Common::TCPServer</code>:<pre class="source-code">
std::function&lt;void(TCPSocket *s, Nanos rx_time)&gt; recv_callback_;</pre><pre class="source-code">
std::function&lt;void()&gt; recv_finished_callback_;</pre></li>
<li><code>Common::TCPSocket</code>:<pre class="source-code">
std::function&lt;void(TCPSocket *s, Nanos rx_time)&gt; recv_callback_;</pre></li>
<li><code>Trading::TradeEngine</code>:<pre class="source-code">
std::function&lt;void(TickerId ticker_id, Price price, Side side, MarketOrderBook *book)&gt; algoOnOrderBookUpdate_;</pre><pre class="source-code">
std::function&lt;void(const Exchange::MEMarketUpdate *market_update, MarketOrderBook *book)&gt; algoOnTradeUpdate_;</pre><pre class="source-code">
std::function&lt;void(const Exchange::MEClientResponse *client_response)&gt; algoOnOrderUpdate_;</pre></li>
</ul>
<p>Calling functions through these objects is slower than directly calling functions, and these incur similar costs as <code>virtual</code> functions. This mechanism of calling methods using the <code>std::function&lt;&gt;</code> objects can be replaced with templates. To refresh your memory on the drawbacks of calling functions indirectly, please revisit the chapter <em class="italic">Exploring C++ Concepts from a Low-Latency Application’s Perspective</em>, specifically the <em class="italic">Avoiding function pointers</em> sub-section of the <em class="italic">Calling functions efficiently</em> section. Additionally, revisit the <em class="italic">Using compile-time polymorphism</em> section in the same chapter, reviewing the discussion on the <code>std::function&lt;&gt;</code> instances in our code base, but we encourage those who are interested to attempt that improvement.</p>
<h2 id="_idParaDest-309"><a id="_idTextAnchor322"/>Inspecting the impact of these optimizations</h2>
<p>We will not be able to investigate every optimization opportunity in detail, but before we finish this section, we will discuss the details of two optimizations that we discussed in this section. First, let us discuss the implementation and impact of the optimization on our <code>Logger</code> class for logging strings.</p>
<h3>Benchmarking Logger string optimization</h3>
<p>To implement the Logger<a id="_idIndexMarker1565"/> string optimization, we will change the <code>pushValue()</code> method for <code>char*</code> arguments as discussed before. For the sake of brevity, we will not look at the full class, which we implement in an alternate <code>OptLogger</code> class available in the <code>Chapter12/common/opt_logging.h</code> source file. The most important change is shown here, but please refer to the full source file to see the other minor changes:</p>
<pre class="source-code">
    auto pushValue(const char *value) noexcept {
      LogElement l{LogType::STRING, {.s = {}}};
      strncpy(l.u_.s, value, sizeof(l.u_.s) - 1);
      pushValue(l);
    }</pre>
<p>To benchmark this and compare it against the original <code>Logger</code> implementation, we will create a simple standalone binary called <code>logger_benchmark</code>. We do this so that we can check the performance impact in a controlled environment. Remember that running the full trading ecosystem introduces a lot of variance due to the number of processes and threads, the network activity, the trading activity, and so on, and it can be difficult to properly assess the impact of the <code>Logger</code> optimization. The source code for this benchmark application can be found in the <code>Chapter12/benchmarks/logger_benchmark.cpp</code> source file. Let us look at the implementation of this source file quickly before looking at the results.</p>
<p>First, we will include the<a id="_idIndexMarker1566"/> header files corresponding to the original <code>Logger</code> and the new <code>OptLogger</code> classes:</p>
<pre class="source-code">
#include "common/logging.h"
#include "common/opt_logging.h"</pre>
<p>Next, we will define a <code>random_string()</code> method, which simply generates random strings of a specified length. We will use this to generate random strings for the two loggers to log to compare the performance difference when it comes to strings. This uses a <code>charset()</code> lambda method, which returns a random alphanumeric (0-9, a-z, or A-z) character. It then uses the <code>std::generate_n()</code> method to generate a <code>std::string</code> with a length specified in the length argument by calling the <code>charset()</code> lambda method repeatedly:</p>
<pre class="source-code">
std::string random_string(size_t length) {
  auto randchar = []() -&gt; char {
    const char charset[] =
        "0123456789"
        "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
        "abcdefghijklmnopqrstuvwxyz";
    const size_t max_index = (sizeof(charset) - 1);
    return charset[rand() % max_index];
  };
  std::string str(length, 0);
  std::generate_n(str.begin(), length, randchar);
  return str;
}</pre>
<p>Next, we will define a <code>benchmarkLogging()</code> method, which accepts a template parameter, <code>T</code>, which it expects<a id="_idIndexMarker1567"/> to be an instance of one of the two loggers we are comparing here. It runs a loop 100,000 times and uses the <code>random_string()</code> method we built previously and the logger’s <code>log()</code> method to log 100,000 random strings. For each call to the <code>log()</code> method, it records and sums up the difference in clock cycles, using the <code>Common::rdtsc()</code> method we built in the previous chapter. Finally, it returns the average clock cycles by dividing the sum of each RDTSC difference by the loop count:</p>
<pre class="source-code">
template&lt;typename T&gt;
size_t benchmarkLogging(T *logger) {
  constexpr size_t loop_count = 100000;
  size_t total_rdtsc = 0;
  for (size_t i = 0; i &lt; loop_count; ++i) {
    const auto s = random_string(128);
    const auto start = Common::rdtsc();
    logger-&gt;log("%\n", s);
    total_rdtsc += (Common::rdtsc() - start);
  }
  return (total_rdtsc / loop_count);
}</pre>
<p>Now, we can finally build the <code>main()</code> method, which is quite simple. It creates an instance of the old logger – <code>Common::Logger()</code> – calls the <code>benchmarkLogging()</code> method on it, and <a id="_idIndexMarker1568"/>outputs the average clock cycle count to the screen. Then, it does exactly the same thing again, except this time it uses the new logger – <code>OptCommon::OptLogger()</code>:</p>
<pre class="source-code">
int main(int, char **) {
  using namespace std::literals::chrono_literals;
  {
    Common::Logger logger("logger_benchmark_original.log");
    const auto cycles = benchmarkLogging(&amp;logger);
    std::cout &lt;&lt; "ORIGINAL LOGGER " &lt;&lt; cycles &lt;&lt; " CLOCK
      CYCLES PER OPERATION." &lt;&lt; std::endl;
    std::this_thread::sleep_for(10s);
  }
  {
    OptCommon::OptLogger opt_logger
      ("logger_benchmark_optimized.log");
    const auto cycles = benchmarkLogging(&amp;opt_logger);
    std::cout &lt;&lt; "OPTIMIZED LOGGER " &lt;&lt; cycles &lt;&lt; " CLOCK
      CYCLES PER OPERATION." &lt;&lt; std::endl;
    std::this_thread::sleep_for(10s);
  }
  exit(EXIT_SUCCESS);
}</pre>
<p>This binary can be built using the same script as before, that is, by running <code>scripts/build.sh</code> from the <code>Chapter12</code> root directory. To run the binary, you can call it directly from the command line, as shown here, and, among other output, you will see the following two lines <a id="_idIndexMarker1569"/>displaying the results of the benchmarking:</p>
<pre class="source-code">
sghosh@sghosh-ThinkPad-X1-Carbon-3rd:~/Building-Low-Latency-Applications-with-CPP/Chapter12$ ./cmake-build-release/logger_benchmark
ORIGINAL LOGGER 25757 CLOCK CYCLES PER OPERATION.
OPTIMIZED LOGGER 466 CLOCK CYCLES PER OPERATION.</pre>
<p>Note that there will be some variance in the output for each run, and the results you get will likely be different due to system-dependent reasons, but amount the optimization has sped things up, should be somewhat similar to what we have shown. In this case, it seems like our optimization efforts have sped up the <code>log()</code> method for strings to be roughly 50 times faster. Next, let us look at another example of the optimization tips we discussed before, which is optimizing the binary for release builds.</p>
<h3>Benchmarking release build optimization</h3>
<p>To benchmark an example of leaving out non-essential code from the release build, we picked the <code>MemPool</code> class. Note that this <a id="_idIndexMarker1570"/>principle applies to all the components we built, but we arbitrarily picked a single one to limit the scope of our discussion. Similar to what we did for the <code>Logger</code> class, we create a new class called <code>OptMemPool</code>, which you will find in the <code>Chapter12/common/opt_mem_pool.h</code> source file. The primary change in this file compared to the <code>MemPool</code> class is that the calls to <code>ASSERT()</code> are only built for non-release builds. This is achieved by checking for the <code>NDEBUG</code> preprocessor flag, as shown in the following two examples. You can check out the full source code in the file we mentioned previously:</p>
<pre class="source-code">
    template&lt;typename... Args&gt;
    T *allocate(Args... args) noexcept {
      auto obj_block = &amp;(store_[next_free_index_]);
#if !defined(NDEBUG)
      ASSERT(obj_block-&gt;is_free_, "Expected free
        ObjectBlock at index:" + std::to_string
          (next_free_index_));
#endif
      ...
    }
    auto deallocate(const T *elem) noexcept {
      const auto elem_index = (reinterpret_cast&lt;const
        ObjectBlock *&gt;(elem) - &amp;store_[0]);
#if !defined(NDEBUG)
      ASSERT(elem_index &gt;= 0 &amp;&amp; static_cast
        &lt;size_t&gt;(elem_index) &lt; store_.size(), "Element
          being deallocated does not belong to this Memory
            pool.");
      ASSERT(!store_[elem_index].is_free_, "Expected in-use
        ObjectBlock at index:" + std::to_string
          (elem_index));
#endif
      ...
    }</pre>
<p>To benchmark this optimization, we will build a <code>release_benchmark</code> binary, and the code for that is available in the <code>Chapter12/benchmarks/release_benchmark.cpp</code> source file. First, let us look at the header files we need to include, most importantly the <code>mem_pool.h</code> and <code>opt_mem_pool.h</code> files. Since memory<a id="_idIndexMarker1571"/> pools store structures, we will use <code>Exchange::MDPMarketUpdate</code> as an example, so we include the <code>market_update.h</code> header file for that as well:</p>
<pre class="source-code">
#include "common/mem_pool.h"
#include "common/opt_mem_pool.h"
#include "common/perf_utils.h"
#include "exchange/market_data/market_update.h"</pre>
<p>Similar to what we did with the <code>logger_benchmark.cpp</code> file, we will create a <code>benchmarkMemPool()</code> method, which accepts a template parameter, <code>T</code>, and expects it to be one of the two memory pools we are comparing. In this method, we will first allocate and save 256 <code>MDPMarketUpdate</code> objects from the memory pool, using the <code>allocate()</code> method. Then, we will deallocate each of these objects and return them to the memory pool, using the <code>deallocate()</code> method. We will run this loop 100,000 times to find a reliable average over many iterations. We will measure and sum up the clock cycles elapsed<a id="_idIndexMarker1572"/> for each call to <code>allocate()</code> and <code>deallocate()</code> as we did before with the logger benchmark. Finally, we return the average clock cycles by dividing the sum of elapsed clock cycles by the loop count:</p>
<pre class="source-code">
template&lt;typename T&gt;
size_t benchmarkMemPool(T *mem_pool) {
  constexpr size_t loop_count = 100000;
  size_t total_rdtsc = 0;
  std::array&lt;Exchange::MDPMarketUpdate*, 256&gt;
    allocated_objs;
  for (size_t i = 0; i &lt; loop_count; ++i) {
    for(size_t j = 0; j &lt; allocated_objs.size(); ++j) {
      const auto start = Common::rdtsc();
      allocated_objs[j] = mem_pool-&gt;allocate();
      total_rdtsc += (Common::rdtsc() - start);
    }
    for(size_t j = 0; j &lt; allocated_objs.size(); ++j) {
      const auto start = Common::rdtsc();
      mem_pool-&gt;deallocate(allocated_objs[j]);
      total_rdtsc += (Common::rdtsc() - start);
    }
  }
  return (total_rdtsc / (loop_count *
    allocated_objs.size()));
}</pre>
<p>Finally, we build the <code>main()</code> method, which again is quite simple. It calls the <code>benchmarkMemPool()</code> method twice, once with an object of the <code>Common::MemPool</code> type and next with <a id="_idIndexMarker1573"/>an object of the <code>OptCommon::OptMemPool</code> type, and outputs the average clock cycles elapsed for the <code>allocate()</code> and <code>deallocate()</code> methods:</p>
<pre class="source-code">
int main(int, char **) {
  {
    Common::MemPool&lt;Exchange::MDPMarketUpdate&gt;
      mem_pool(512);
    const auto cycles = benchmarkMemPool(&amp;mem_pool);
    std::cout &lt;&lt; "ORIGINAL MEMPOOL " &lt;&lt; cycles &lt;&lt; " CLOCK
      CYCLES PER OPERATION." &lt;&lt; std::endl;
  }
  {
    OptCommon::OptMemPool&lt;Exchange::MDPMarketUpdate&gt;
      opt_mem_pool(512);
    const auto cycles = benchmarkMemPool(&amp;opt_mem_pool);
    std::cout &lt;&lt; "OPTIMIZED MEMPOOL " &lt;&lt; cycles &lt;&lt; " CLOCK
      CYCLES PER OPERATION." &lt;&lt; std::endl;
  }
  exit(EXIT_SUCCESS);
}</pre>
<p>The process to build this benchmark binary remains the same, so we will not repeat it. Running the binary will yield something that resembles the following:</p>
<pre class="source-code">
sghosh@sghosh-ThinkPad-X1-Carbon-3rd:~/Building-Low-Latency-Applications-with-CPP/Chapter12$ ./cmake-build-release/release_benchmark
ORIGINAL MEMPOOL 343 CLOCK CYCLES PER OPERATION.
OPTIMIZED MEMPOOL 44 CLOCK CYCLES PER OPERATION.</pre>
<p>In this case, our optimization efforts yielded a speed up of around 7 to 8 times for the <code>allocate()</code> and <code>deallocate()</code> methods.</p>
<p>In this section, we presented and<a id="_idIndexMarker1574"/> explained a subset of optimization areas/ideas in our electronic trading ecosystem. The goal here is to get you to understand what these optimization areas can look like and how to approach them with the goal of optimizing performance. In the next section, we’ll discuss some more future improvements and enhancements that can be made to our electronic trading ecosystem.</p>
<h1 id="_idParaDest-310"><a id="_idTextAnchor323"/>Thinking about the future of our trading ecosystem</h1>
<p>Before we conclude this chapter and this book, we will discuss a few possible enhancements to our electronic<a id="_idIndexMarker1575"/> trading ecosystem. In the previous section, we discussed some examples of things that can be optimized for those interested in maximizing the performance of the electronic trading system we built in this book. In this section, we will discuss some examples of how this ecosystem can be enhanced, not necessarily to reduce latency but to make the system more feature-rich and add functionality.</p>
<h2 id="_idParaDest-311"><a id="_idTextAnchor324"/>Growing containers dynamically</h2>
<p>We built and used a<a id="_idIndexMarker1576"/> few containers in this book, as listed here:</p>
<ul>
<li>The lock-free queue – <code>LFQueue</code> – which is used in multiple components for various object types, such as <code>MEMarketUpdate</code>, <code>MDPMarketUpdate</code>, <code>MEClientRequest</code>, and <code>MEClientResponse</code></li>
<li>The memory pool – <code>MemPool</code> – which was used for multiple object types, such as instances of <code>MEMarketUpdate</code>, <code>MEOrder</code>, <code>MEOrdersAtPrice</code>, <code>MarketOrder</code>, and <code>MarketOrdersAtPrice</code></li>
</ul>
<p>In all these cases, we assumed a safe maximum size value. In practice, that still leaves us open to the possibility that <a id="_idIndexMarker1577"/>under some circumstances, we might exceed these limits and get in trouble. One enhancement we can make to this system is to improve our handling of this unlikely edge case.</p>
<p>One option would be to fail/exit if we encounter a scenario where <code>LFQueue</code> is full or <code>MemPool</code> is out of memory. Another option would be to fall back to dynamic memory allocation or a secondary inefficient container for this unlikely event; that is, we will be inefficient and slow in this extremely rare case that we run out of memory or space in our containers, but we will continue to function until it is resolved. Yet another option is to make these containers flexible where they can be grown if needed even though the task of growing these containers when needed will be extremely slow, since in practice we do not expect to encounter that condition.</p>
<h2 id="_idParaDest-312"><a id="_idTextAnchor325"/>Growing and enhancing the hash maps</h2>
<p>In this book, we used <code>std::array</code> in many contexts as a hash map by assuming a safe upper bound. For instance, by <a id="_idIndexMarker1578"/>assuming that valid <code>TickerId</code> values fall in the range of 0 and <code>ME_MAX_TICKERS</code>, we used <code>std::array</code> instances of size <code>ME_MAX_TICKERS</code> as hash maps with <code>TickerId</code> keys. A similar<a id="_idIndexMarker1579"/> design was used for containers such as <code>TradeEngineCfgHashMap</code>, <code>OrderHashMap</code>, <code>ClientOrderHashMap</code>, <code>OrdersAtPriceHashMap</code>, <code>OrderBookHashMap</code>, <code>MarketOrderBookHashMap</code>, and <code>OMOrderTickerSideHashMap</code>. While in practice, some of these can continue to exist, that is, valid and reasonable upper bounds can be decided and used, for some of these, this design will not scale up elegantly.</p>
<p>There are several different hash map implementations available – <code>std::unordered_map</code>, <code>absl::flat_hash_map</code>, <code>boost::</code> hash maps, <code>emhash7::HashMap</code>, <code>folly::AtomicHashmap</code>, <code>robin_hood::unordered_map</code>, <code>tsl::hopscotch_map</code>, and many more. Additionally, it is common to optimize and tweak these containers so that they perform best under our specific use cases. We’ll leave those of you who are interested with the task of exploring these and deciding which ones can replace the <code>std::array</code>-based hash maps in our system.</p>
<p>For the sake of demonstrating an example, we will replace the <code>std::array</code>-based hash maps in the limit order book that the matching engine builds and maintains (<code>MEOrderBook</code>) with <code>std::unordered_map</code> hash maps. We will then benchmark the two implementations to see how much of a difference it makes. Following the same pattern as we used in the<a id="_idIndexMarker1580"/> benchmarking we performed earlier in this chapter, we will introduce a new <code>MEOrderBook</code> class, <code>UnorderedMapMEOrderBook</code>, where the only difference is the <a id="_idIndexMarker1581"/>use of the <code>std::unordered_map</code> containers instead of the <code>std::array</code> containers. All the source code for this new class is available in the <code>Chapter12/exchange/matcher/unordered_map_me_order_book.h</code> and <code>Chapter12/exchange/matcher/unordered_map_me_order_book.cpp</code> source files. For the sake of brevity, we will not repeat the entire class implementation here, but we will discuss the important changes. The first important and obvious change is the inclusion of the <code>unordered_map</code> header file in the <code>unordered_map_me_order_book.h</code> header file:</p>
<pre class="source-code">
#include &lt;unordered_map&gt;</pre>
<p>We change the <code>cid_oid_to_order_</code> data member to be <code>std::unordered_map&lt;ClientId, std::unordered_map&lt;OrderId, MEOrder *&gt;&gt;</code> instead of <code>ClientOrderHashMap</code>, which is a <code>typedef</code> for <code>std::array&lt;OrderHashMap, ME_MAX_NUM_CLIENTS&gt;</code>. This data member is a hash map from <code>ClientId</code> to <code>OrderId</code> to <code>MEOrder</code> objects. Remember that <code>ClientOrderHashMap</code> is actually a hash map of hash maps, that is, a <code>std::array</code> whose elements are also <code>std::array</code> objects. The other data member we change is the <code>price_orders_at_price_</code> member, which we change to <code>std::unordered_map&lt;Price, MEOrdersAtPrice *&gt;</code> instead of the <code>OrdersAtPriceHashMap</code> type. This data member is a hash map from <code>Price</code> to <code>MEOrdersAtPrice</code> objects. If you have forgotten what <code>MEOrder</code> and <code>MEOrdersAtPrice</code> are, please revisit the <em class="italic">Designing the exchange order book</em> sub-section in the <em class="italic">Defining the operations and interactions in our matching engine</em> section of the chapter <em class="italic">Building the C++ Matching Engine</em>. These changes are shown here:</p>
<pre class="source-code">
namespace Exchange {
  class UnorderedMapMEOrderBook final {
  private:
    ...
    std::unordered_map&lt;ClientId, std::
      unordered_map&lt;OrderId, MEOrder *&gt;&gt; cid_oid_to_order_;
    std::unordered_map&lt;Price, MEOrdersAtPrice *&gt;
      price_orders_at_price_;
    ...
  };
}</pre>
<p>We will need to<a id="_idIndexMarker1582"/> remove the following lines from the<a id="_idIndexMarker1583"/> destructor since the <code>fill()</code> method does not apply to <code>std::unordered_map</code> objects:</p>
<pre class="source-code">
  MEOrderBook::~MEOrderBook() {
    …
    for (auto &amp;itr: cid_oid_to_order_) {
      itr.fill(nullptr);
    }
  }</pre>
<p>In terms of accessing these modified containers, we replace the calls to the <code>std::array::at()</code> method for <code>cid_oid_to_order_</code> and <code>price_orders_at_price_</code> with the <code>std::unordered_map::operator[]</code> method. These changes for <code>cid_oid_to_order_</code> are shown here:</p>
<pre class="source-code">
    auto removeOrder(MEOrder *order) noexcept {
      ...
      cid_oid_to_order_[order-&gt;client_id_][order-&gt;
        client_order_id_] = nullptr;
      order_pool_.deallocate(order);
    }
    auto addOrder(MEOrder *order) noexcept {
      ...
      cid_oid_to_order_[order-&gt;client_id_][order-&gt;
        client_order_id_] = order;
    }
  auto UnorderedMapMEOrderBook::cancel(ClientId client_id,
    OrderId order_id, TickerId ticker_id) noexcept -&gt; void {
    auto is_cancelable = (client_id &lt;
      cid_oid_to_order_.size());
    MEOrder *exchange_order = nullptr;
    if (LIKELY(is_cancelable)) {
      auto &amp;co_itr = cid_oid_to_order_[client_id];
      exchange_order = co_itr[order_id];
      is_cancelable = (exchange_order != nullptr);
    }
    ...
  }</pre>
<p>We need to make similar changes in <a id="_idIndexMarker1584"/>spots where we access the <code>price_orders_at_price_</code> container, which is shown here:</p>
<pre class="source-code">
    auto getOrdersAtPrice(Price price) const noexcept -&gt;
      MEOrdersAtPrice * {
      if(price_orders_at_price_.find(priceToIndex(price))
        == price_orders_at_price_.end())
        return nullptr;
      return price_orders_at_price_
        .at(priceToIndex(price));
    }
    auto addOrdersAtPrice(MEOrdersAtPrice
      *new_orders_at_price) noexcept {
      price_orders_at_price_
        [priceToIndex(new_orders_at_price-&gt;price_)] =
          new_orders_at_price;
      ...
    }
    auto removeOrdersAtPrice(Side side, Price price)
      noexcept {
      ...
      price_orders_at_price_[priceToIndex(price)] =
        nullptr;
      orders_at_price_pool_.deallocate(orders_at_price);
    }</pre>
<p>Finally, we present the <code>hash_benchmark</code> binary to measure the performance differences because of these<a id="_idIndexMarker1585"/> changes. The source code for this binary can be found in the <code>Chapter12/benchmarks/hash_benchmark.cpp</code> source file. First, we include the header files shown as follows and also define a global <code>loop_count</code> variable as we have done in our previous benchmarks:</p>
<pre class="source-code">
#include "matcher/matching_engine.h"
#include "matcher/unordered_map_me_order_book.h"
static constexpr size_t loop_count = 100000;</pre>
<p>As we have done before, we will define a <code>benchmarkHashMap()</code> method, which accepts a template parameter, <code>T</code>, to represent either <code>MEOrderBook</code> or <code>UnorderedMapMEOrderBook</code>. It also accepts a vector of <code>Exchange::MEClientRequest</code> messages, which will be processed in the benchmark. The actual processing is <a id="_idIndexMarker1586"/>quite simple. It checks the type of <code>MEClientRequest</code> and then calls the <code>add()</code> method for <code>ClientRequestType::NEW</code> and the <code>cancel()</code> method for <code>ClientRequestType::CANCEL</code>. We use <code>Common::rdtsc()</code> to measure and sum up the clock cycles elapsed for each of these calls and then return the average at the end of this method:</p>
<pre class="source-code">
template&lt;typename T&gt;
size_t benchmarkHashMap(T *order_book, const
  std::vector&lt;Exchange::MEClientRequest&gt;&amp; client_requests) {
  size_t total_rdtsc = 0;
  for (size_t i = 0; i &lt; loop_count; ++i) {
    const auto&amp; client_request = client_requests[i];
    switch (client_request.type_) {
      case Exchange::ClientRequestType::NEW: {
        const auto start = Common::rdtsc();
        order_book-&gt;add(client_request.client_id_,
          client_request.order_id_,
            client_request.ticker_id_,
                        client_request.side_,
                          client_request.price_,
                            client_request.qty_);
        total_rdtsc += (Common::rdtsc() - start);
      }
        break;
      case Exchange::ClientRequestType::CANCEL: {
        const auto start = Common::rdtsc();
        order_book-&gt;cancel(client_request.client_id_,
          client_request.order_id_,
            client_request.ticker_id_);
        total_rdtsc += (Common::rdtsc() - start);
      }
        break;
      default:
        break;
    }
  }
  return (total_rdtsc / (loop_count * 2));
}</pre>
<p>Now we can look at the <code>main()</code> method. We need <code>Logger</code> and a <code>MatchingEngine</code> object to create the <code>MEOrderBook</code> or <code>UnorderedMapMEOrderBook</code> object, but to create the <code>MatchingEngine</code> object, we need<a id="_idIndexMarker1587"/> three lock-free queues as we have seen in the implementation of the <code>exchange_main</code> binary. So, we create these objects as shown here, even though we are not measuring the performance of any of these components:</p>
<pre class="source-code">
int main(int, char **) {
  srand(0);
  Common::Logger logger("hash_benchmark.log");
  Exchange::ClientRequestLFQueue
    client_requests(ME_MAX_CLIENT_UPDATES);
  Exchange::ClientResponseLFQueue
    client_responses(ME_MAX_CLIENT_UPDATES);
  Exchange::MEMarketUpdateLFQueue
    market_updates(ME_MAX_MARKET_UPDATES);
  auto matching_engine = new Exchange::
    MatchingEngine(&amp;client_requests, &amp;client_responses,
      &amp;market_updates);</pre>
<p>Next, we will create a vector of 100,000 (<code>loop_count</code>) <code>MEClientRequest</code> objects, which will be composed of new order requests as well as requests to cancel these orders. We have seen<a id="_idIndexMarker1588"/> similar code in the <code>trading_main</code> application for the random trading algorithm:</p>
<pre class="source-code">
  Common::OrderId order_id = 1000;
  std::vector&lt;Exchange::MEClientRequest&gt;
    client_requests_vec;
  Price base_price = (rand() % 100) + 100;
  while (client_requests_vec.size() &lt; loop_count) {
    const Price price = base_price + (rand() % 10) + 1;
    const Qty qty = 1 + (rand() % 100) + 1;
    const Side side = (rand() % 2 ? Common::Side::BUY :
      Common::Side::SELL);
    Exchange::MEClientRequest new_request
      {Exchange::ClientRequestType::NEW, 0, 0, order_id++,
        side, price, qty};
    client_requests_vec.push_back(new_request);
    const auto cxl_index = rand() %
      client_requests_vec.size();
    auto cxl_request = client_requests_vec[cxl_index];
    cxl_request.type_ =
      Exchange::ClientRequestType::CANCEL;
    client_requests_vec.push_back(cxl_request);
  }</pre>
<p>Finally, we<a id="_idIndexMarker1589"/> end the <code>main()</code> method by calling the <code>benchmarkHashMap()</code> method twice – once with an instance of <code>MEOrderBook</code> and once with an instance of <code>UnorderedMapMEOrderBook</code>, as shown:</p>
<pre class="source-code">
  {
    auto me_order_book = new Exchange::MEOrderBook(0,
      &amp;logger, matching_engine);
    const auto cycles = benchmarkHashMap(me_order_book,
      client_requests_vec);
    std::cout &lt;&lt; "ARRAY HASHMAP " &lt;&lt; cycles &lt;&lt; " CLOCK
      CYCLES PER OPERATION." &lt;&lt; std::endl;
  }
  {
    auto me_order_book = new Exchange::
      UnorderedMapMEOrderBook(0, &amp;logger, matching_engine);
    const auto cycles = benchmarkHashMap(me_order_book,
      client_requests_vec);
    std::cout &lt;&lt; "UNORDERED-MAP HASHMAP " &lt;&lt; cycles &lt;&lt; "
      CLOCK CYCLES PER OPERATION." &lt;&lt; std::endl;
  }
  exit(EXIT_SUCCESS);
}</pre>
<p>The process to build this application remains the same, which is by calling the <code>scripts/build.sh</code> script from the <code>Chapter12</code> root directory. Running the application by calling the <code>hash_benchmark</code> binary will yield output like what is shown here, with some variance<a id="_idIndexMarker1590"/> between independent runs and depending on the system:</p>
<pre class="source-code">
sghosh@sghosh-ThinkPad-X1-Carbon-3rd:~/Building-Low-Latency-Applications-with-CPP/Chapter12$ ./cmake-build-release/hash_benchmark
Set core affinity for Common/Logger hash_benchmark.log 140327631447616 to -1
Set core affinity for Common/Logger exchange_matching_engine.log 140327461033536 to -1
ARRAY HASHMAP 142650 CLOCK CYCLES PER OPERATION.
UNORDERED-MAP HASHMAP 152457 CLOCK CYCLES PER OPERATION.</pre>
<p>Based on the output of this run, it appears that switching from a <code>std::array</code> hash map implementation to a <code>std::unordered_map</code> hash map implementation adds an approximate 6 to 7% extra overhead to the <code>MEOrderBook</code> <code>add()</code> and <code>cancel()</code> performance.</p>
<h2 id="_idParaDest-313"><a id="_idTextAnchor326"/>Optimizing snapshot messages</h2>
<p>In our design of the<a id="_idIndexMarker1591"/> snapshot messages in the <code>MarketDataPublisher</code> component at the trading exchange, a full cycle of snapshot messages between the <code>START_SNAPSHOT</code> and <code>END_SNAPSHOT</code> messages contains the snapshot for all trading instruments, as shown in the following diagram (which we have seen before). In our <code>SnapshotSynthesizer</code>, this full snapshot for all trading instruments is published once every 60 seconds. What this means is that if the order books for each of these trading instruments have a lot of orders, then every 60 seconds, there is a huge spike in network traffic on the snapshot multicast channels followed by silence in the remaining 60 seconds.</p>
<div><div><img alt="Figure 12.12 – Current composition of snapshot messages" src="img/B19434_12_012.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.12 – Current composition of snapshot messages</p>
<p>It would be an<a id="_idIndexMarker1592"/> enhancement to this design if we changed this such that these snapshots are spaced out more evenly and each snapshot cycle contained the snapshot messages corresponding to only one <code>TickerId</code>. As a simple example, instead of sending a snapshot message cycle for 6 instruments every 60 seconds, we can send 6 snapshots each containing information for a single instrument, and each of these snapshots is spaced out with 10 seconds in between them. This hypothetical proposal is represented in the following diagram.</p>
<div><div><img alt="Figure 12.13 – A proposal for an optimized snapshot messaging format" src="img/B19434_12_013.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.13 – A proposal for an optimized snapshot messaging format</p>
<p>In this new proposal, as we mentioned, there are fewer spikes in network traffic since the full snapshot is <a id="_idIndexMarker1593"/>distributed over time. This leads to a lower chance of dropping packets on the snapshot multicast stream for the <code>MarketDataConsumer</code> components in the trading client’s systems. This also leads to the client’s system synchronizing or catching up with the snapshot stream for each trading instrument faster, since it does not need to wait for the full snapshot across all trading instruments before it can mark some of the instruments as <em class="italic">recovered</em>.</p>
<h2 id="_idParaDest-314"><a id="_idTextAnchor327"/>Adding authentication and rejection messages to the Order protocol</h2>
<p>Our electronic trading <a id="_idIndexMarker1594"/>exchange right now<a id="_idIndexMarker1595"/> has no concept of user authentication and is missing a lot of error checking and handling. By this, we mean that it does not check whether clients log in with the correct credentials and are authorized to trade the instruments they try to trade. Additionally, if the <code>ClientId</code> and <code>TCPSocket</code> instances do not match or there is a sequence number gap in the <code>ClientRequest</code> messages that a client sends, we quietly ignore it in <code>Exchange::OrderServer</code>. This is <a id="_idIndexMarker1596"/>shown in the following code block from the <code>exchange/order_server/order_server.h</code> source file, which we have already discussed in detail:</p>
<pre class="source-code">
    auto recvCallback(TCPSocket *socket, Nanos rx_time)       noexcept {
      ...
      if (socket-&gt;next_rcv_valid_index_ &gt;=         sizeof(OMClientRequest)) {
          ...
          if (cid_tcp_socket_[request-&gt;            me_client_request_.client_id_] != socket) {
            ...
            continue;
          }
          auto &amp;next_exp_seq_num =             cid_next_exp_seq_num_[request-&gt;              me_client_request_.client_id_];
          if (request-&gt;seq_num_ != next_exp_seq_num) {
            ...
            continue;
          }
          ...
        }
        ...
      }
    }</pre>
<p>Silently ignoring errors like these is not ideal since the clients are not notified about these errors. An enhancement to this workflow would be to add a rejection message to the <code>ClientResponse</code> message <a id="_idIndexMarker1597"/>protocol, which the <code>OrderServer</code> component can use to notify the clients about these errors. This enhancement is in addition to the enhancements we <a id="_idIndexMarker1598"/>suggested to the order protocol to facilitate the authentication of the trading clients.</p>
<h2 id="_idParaDest-315"><a id="_idTextAnchor328"/>Supporting modify messages in the Order protocol</h2>
<p>Our current order protocol for <code>ClientRequest</code> messages only supports <code>ClientRequestType::NEW</code> and <code>ClientRequestType::CANCEL</code> requests. An enhancement to this protocol would be to add a <code>ClientRequestType::MODIFY</code> message type so that <a id="_idIndexMarker1599"/>client trading systems can modify their order’s price or quantity attributes. We would need to update the <code>OrderServer</code>, <code>MatchingEngine</code>, <code>MEOrderBook</code>, and other components on the exchange’s side and update the <code>OrderGateway</code>, <code>OrderManager</code>, <code>MarketMaker</code>, <code>TradeEngine</code>, and other components on the trading client’s side.</p>
<h2 id="_idParaDest-316"><a id="_idTextAnchor329"/>Enhancing trade engine components</h2>
<p>The trade engine has <a id="_idIndexMarker1600"/>several components that can be improved and/or enhanced. In this section, we provide brief descriptions of these improvements for each of the components with potential future enhancements.</p>
<h3>Adding risk metrics to RiskManager</h3>
<p>In the<a id="_idIndexMarker1601"/> chapter <em class="italic">Designing Our Trading Ecosystem</em>, in the <em class="italic">Understanding the risk management systems</em> section, we described a couple of different risk metrics. <code>RiskManager</code> was built only with a small subset of those risk metrics and can be enhanced by adding additional risk <a id="_idIndexMarker1602"/>measures, as described in that section.</p>
<h3>Enhancing OrderManager</h3>
<p><code>OrderManager</code> was built extremely simply – it supports a maximum of one active order on each side, that is, at most one buy order and one sell order. Obviously, this is an extremely simplified <a id="_idIndexMarker1603"/>version and <code>OrderManager</code> can be enhanced to support much more complex order management.</p>
<h3>Enriching FeatureEngine</h3>
<p><code>FeatureEngine</code> was set up with two<a id="_idIndexMarker1604"/> hardcoded features built into it. It can be enriched a lot to support complex configurations of features, a library of diverse types of features, complex interactions between these features, and so on.</p>
<h3>Enhancing the trading algorithms</h3>
<p><code>LiquidityTaker</code> and <code>MarketMaker</code> in this book were also extremely simple representations of realistic<a id="_idIndexMarker1605"/> trading strategies. These can be enhanced/improved in many ways – improvements in terms of feature compositions, order management, efficient execution, and so on.</p>
<p>This concludes our discussion of future enhancement possibilities for our electronic trading ecosystem.</p>
<h1 id="_idParaDest-317"><a id="_idTextAnchor330"/>Summary</h1>
<p>The first section of this chapter focused on analyzing the latency metrics we added to our electronic trading systems in the previous chapter. We discussed a few examples of latency measurements for internal functions, as well as a few examples of latency measurements between critical hops in our system. The goal was to understand the distribution of latencies in different cases so that you understand how to identify and investigate areas of potential problems or optimization opportunities.</p>
<p>In the second section of this chapter, we discussed a few tips and techniques regarding how to approach potential performance optimization possibilities. We presented a few examples of what could be improved and discussed the performance problems that exist in the current design and solutions to those problems.</p>
<p>In the concluding section, we described a roadmap for the future of the electronic trading ecosystem we built in this book. We discussed several different components, sub-components, and workflows that can be enriched to build a more mature electronic trading ecosystem.</p>
<p>The approach and principles we discussed in this book pertaining to latency-sensitive applications developed in C++ should guide you on your journey. The full end-to-end electronic trading ecosystem we built is a prime example of a low-latency application and hopefully provided a good practical example of how to build a low-latency application from scratch. Hopefully, this chapter added to the experience by providing you with tools to analyze the performance and iteratively improve the system. We wish you all the best as you continue your low-latency application development journey!</p>
</div>
</body></html>