<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-138"><a id="_idTextAnchor141"/>9</h1>
<h1 id="_idParaDest-139"><a id="_idTextAnchor142"/>Atypical Allocation Mechanisms</h1>
<p>We are progressing in our exploration of memory management with C++. In <a href="B21071_07.xhtml#_idTextAnchor116"><em class="italic">Chapter 7</em></a>, we explored the various syntactic ways in which one can overload <code>operator new()</code> and <code>operator delete()</code> (as well as their array counterparts), and in <a href="B21071_08.xhtml#_idTextAnchor128"><em class="italic">Chapter 8</em></a>, we wrote an actual, real-life example (a memory leak detector) relying on the capacity to write such overloads. It’s a nice start, showing concretely that this knowledge has practical uses, but you might (rightfully) wonder what else we can do when controlling memory management facilities.</p>
<p>This chapter will be slightly different from the others. What we will do here is present a non-exhaustive set of ways in which one can benefit from taking control of the memory allocation functions of C++. More precisely, we will show the following:</p>
<ul>
<li>How placement <code>new</code> can let us drive memory-mapped hardware efficiently</li>
<li>How one can simplify usage of error management with the <code>nothrow</code> version of <code>operator new()</code></li>
<li>How one can install and use <code>std::new_handler</code> to make it easier to react to out-of-memory situations</li>
<li>How one can handle “exotic” memories such as shared memory or persistent memory through the mediation of standard C++</li>
</ul>
<p>At the end of this chapter, we will have a broader view of what opportunities the basic memory allocation facilities of C++ provide us with. Later chapters will return to more focused topics such as arena-based allocation (<a href="B21071_10.xhtml#_idTextAnchor153"><em class="italic">Chapter 10</em></a>), deferred reclamation (<a href="B21071_11.xhtml#_idTextAnchor163"><em class="italic">Chapter 11</em></a>), and, in later chapters, how to control memory allocation with containers and allocators.</p>
<h1 id="_idParaDest-140"><a id="_idTextAnchor143"/>Technical requirements</h1>
<p><a id="_idTextAnchor144"/>You can find the code files for this chapter in the book’s GitHub repository here: <a href="https://github.com/PacktPublishing/C-Plus-Plus-Memory-Management/tree/main/chapter9">https://github.com/PacktPublishing/C-Plus-Plus-Memory-Management/tree/main/chapter9</a>.</p>
<h1 id="_idParaDest-141"><a id="_idTextAnchor145"/>Placement new and memory-mapped hardware</h1>
<p>There are many uses for placement <code>new</code> (an important<a id="_idIndexMarker491"/> feature<a id="_idIndexMarker492"/> discussed in <a href="B21071_07.xhtml#_idTextAnchor116"><em class="italic">Chapter 7</em></a>, as you might remember) but one use that is particularly interesting is that it allows us to map software objects to memory-mapped hardware, effectively allowing us to drive hardware as if it was software.</p>
<p>A working example of this feature would be tricky to write as we would find ourselves in “non-portable code land,” using operating-system-specific features to get the address of a particular device and discussing ways to get read and write privileges to memory locations normally accessed by software drivers. For that reason, we will craft an artificial yet illustrative example and ask you, esteemed reader, to imagine that the missing parts of this example exist.</p>
<p>First, suppose that we are developing a driver for a new video card, one that is so wonderful that its codename is <code>super_video_card</code>. For the sake of this illustration, we will model this through the following class:</p>
<pre class="source-code">
#include &lt;cstdint&gt;
class super_video_card {
  // ...
public:
  // super duper registers
  volatile std::uint32_t r0{}, r1{}, r2{}, r3{};
  static_assert(sizeof(float) == 4); // sanity check
  volatile float f0{}, f1{}, f2{}, f3{};
  // etc.
  // initialize the video card's state
  super_video_card() = default;
  super_video_card(const super_video_card&amp;) = delete;
  super_video_card&amp;
    operator=(const super_video_card&amp;) = delete;
  // could be used to reset the video card's state
  ~super_video_card() = default;
  // various services (omitted for brevity)
};
// ...</pre> <p>The important aspects of this class for our purpose are the following:</p>
<ul>
<li>It is an uncopiable type, as it is meant to map to a specific zone of memory. Copying<a id="_idIndexMarker493"/> an object <a id="_idIndexMarker494"/>of this type would be counterproductive, to say the least.</li>
<li>It has been designed in such a way that its state can conceptually be superimposed on its hardware equivalent. For example, given the preceding class declaration, starting at the beginning of the hardware’s memory layout, we expect four 32-bit integral registers followed by four 32-bit floating point registers. We used <code>&lt;cstdint&gt;</code> to get the aliases for fixed-width integral types on our compiler.</li>
<li>As should be the case under such circumstances, we express our expectations through <code>static_assert</code> whenever possible. Also, since the state of the hardware registers can change through other actions than that of our program, we qualified the register-equivalents as <code>volatile</code> such that accesses to these member variables will be equivalent to I/O operations for the purpose of C++’s abstract machine.</li>
</ul>
<p class="callout-heading">Why do we use volatile variables in this example?</p>
<p class="callout">If you are not used<a id="_idIndexMarker495"/> to <code>volatile</code> variables, you might be wondering why we used this qualification on the data members of our memory-mapped hardware-representing class. The reason why this is important is that we want to avoid our compiler optimizing code based on the (wrong, in this case) assumption that if our code does not touch these variables, then they do not change state or that if our writes to these variables are not followed by reads in our code, then that can be assumed to have no effect. Through <code>volatile</code>-qualified variables, we are effectively telling the compiler “<em class="italic">There are things you do not know happening on these objects, so please do not assume </em><em class="italic">too much</em>.”</p>
<p>For simplicity, we used a constructor that zeros out the data members and a trivial destructor, but in practice, we could have used constructors (default or otherwise) to initialize the state of the memory-mapped device to match our needs and the destructor to reset the state of that device to some acceptable state.</p>
<p>Normally, for a program to access the memory-mapped hardware, we would probably communicate with the operating system with services that accept as argument the required information to identify the device whose address we seek. In our case, we will simply make it look like we can access a zone of memory of the right size and alignment to which we can read and write. The memory address is exposed as raw memory (of type <code>void*</code>), which<a id="_idIndexMarker496"/> is<a id="_idIndexMarker497"/> what we can realistically expect from an operating system function under similar circumstances:</p>
<pre class="source-code">
// somewhere in memory where we have read / write
// access privileges is a memory-mapped hardware
// that corresponds to the actual device
alignas(super_video_card) char
  mem_mapped_device[sizeof(super_video_card)];
void* get_super_card_address() {
  return mem_mapped_device;
}
// ...</pre> <p>We then arrive at how one can use placement <code>new</code> to map an object to some memory-mapped hardware location. Note that we need to include the <code>&lt;new&gt;</code> header as this is where placement <code>new</code> is defined. The steps to meet our objective are as follows:</p>
<ol>
<li>First, obtain the address where we want to map our carefully crafted <code>super_video_card</code> object.</li>
<li>Then, through placement <code>new</code> at that address, construct a <code>super_video_card</code> object such that the data members of that object correspond to the address of the registers they represent.</li>
<li>For the duration of that object’s lifetime, use the object through the corresponding pointer (the <code>the_card</code> variable in the following code excerpt).</li>
<li>When we are done, the one thing we do not want to do is apply <code>operator delete()</code> on <code>the_card</code> as we never allocated the associated memory in the first place. We do want to finalize the object through <code>~super_video_card()</code>, however, to make sure the cleanup or reset code (if any) for that object is run.</li>
</ol>
<p>We thus <a id="_idIndexMarker498"/>end <a id="_idIndexMarker499"/>up with the following:</p>
<pre class="source-code">
// ...
<strong class="bold">#include &lt;new&gt;</strong>
int main() {
  // map our object to the hardware
<strong class="bold">  void* p = get_super_card_address();</strong>
<strong class="bold">  auto the_card =</strong>
<strong class="bold">      new(p) super_video_card{ /* args */ };</strong>
  // through pointer the_card, use the actual memory-
  // mapped hardware
  // ...
<strong class="bold">  the_card-&gt;~super_video_card();</strong>
}</pre> <p>If the explicit destructor call is a problem, such as in code where exceptions could be thrown along the way, we can use a <code>std::unique_ptr</code> object with a custom deleter (see <a href="B21071_05.xhtml#_idTextAnchor079"><em class="italic">Chapter 5</em></a>) to finalize the <code>super_video_card</code> object:</p>
<pre class="source-code">
// ...
#include &lt;new&gt;
<strong class="bold">#include &lt;memory&gt;</strong>
int main() {
  // map our object to the hardware
  void* p = get_super_card_address();
<strong class="bold">  std::unique_ptr&lt;</strong>
<strong class="bold">      super_video_card,</strong>
<strong class="bold">      decltype([](super_video_card *p) {</strong>
<strong class="bold">        p-&gt;~super_video_card(); // do not call delete p!</strong>
<strong class="bold">      })</strong>
<strong class="bold">  &gt; the_card {</strong>
<strong class="bold">      new(p) super_video_card{ /* args */ }</strong>
<strong class="bold">  </strong><strong class="bold">};</strong>
  // through pointer the_card, use the actual memory-
  // mapped hardware
   // ...
<strong class="bold">   // implicit call to the_card-&gt;~super_video_card()</strong>
}</pre> <p>In<a id="_idIndexMarker500"/> this<a id="_idIndexMarker501"/> case, the <code>std::unique_ptr</code>  object finalizes the pointee (the <code>super_video_card</code> object) but does not free its memory storage, leading to more robust code in the presence of exceptions during the lifetime of the <code>the_card</code> variable.</p>
<h1 id="_idParaDest-142"><a id="_idTextAnchor146"/>Simplifying nothrow new usage</h1>
<p>As mentioned in <a href="B21071_07.xhtml#_idTextAnchor116"><em class="italic">Chapter 7</em></a>, the default behavior of <code>operator new()</code> when unable to perform an<a id="_idIndexMarker502"/> allocation request is to throw an exception. This can result from such situations as running out of memory or otherwise being unable to service the allocation request, in which case, one usually throws <code>std::bad_alloc</code>; from an incorrect array length (for example, a negative length of one exceeding implementation-defined limits), usually leading to <code>std::bad_array_new_length</code> being thrown; or from failure to complete the subsequent construction of the object following the completion of <code>operator new()</code>, in which case, the exception that will be thrown will be whatever was thrown from the failing constructor.</p>
<p>Exceptions are the “normal” way for a C++ function to signal failure to meet the function’s postconditions. In some cases, such as a constructor or an overloaded operator, it’s the only real, workable way to do so: a constructor has no return value, and the signature of functions that overload operators generally does not leave room for additional arguments or error-reporting return values, although one could make a case for some types such as <code>std::optional</code> or <code>std::expected</code> as allowing an alternative for some overloaded operator use cases.</p>
<p>Of course, some domains typically do not use exceptions: a significant number of video games are compiled without exception support, for example, and the same goes for a lot of programs written for embedded systems. Reasons invoked go from the technical (fear of overhead considered undesirable in terms of memory space consumption, execution speed, or both) to the more philosophical (dislike for what is seen as hidden control paths), but no matter what the reasons are, the fact is that C++ code compiled without exception support exists and the <code>nothrow</code> version of <code>operator new()</code> is a reality.</p>
<p>This does mean, of course, that even seemingly simple code such as the following can lead<a id="_idIndexMarker503"/> to <strong class="bold">undefined </strong><strong class="bold">behavior</strong> (<strong class="bold">UB</strong>):</p>
<pre class="source-code">
#include &lt;new&gt;
#include &lt;iostream&gt;
struct X {
  int n;
  X(int n) : n { n } { }
};
int main() {
  auto p = <strong class="bold">new (std::nothrow) X{ 3 }</strong>;
<strong class="bold">  std::cout &lt;&lt; p-&gt;n; // &lt;-- HERE</strong>
  delete p;
}</pre> <p>The reason for this potential UB is that if the <code>nothrow</code> version of <code>operator new()</code> fails (unlikely but not impossible, especially in memory-constrained situations), then <code>p</code> will be null, and accessing the <code>n</code> data member through <code>p</code> will be… a very bad idea.</p>
<p>Of course, the solution is simple, and being the astute reader that you are, you have probably noticed it <a id="_idIndexMarker504"/>already: just test the pointer before using it! This works, of course, as shown here:</p>
<pre class="source-code">
#include &lt;new&gt;
#include &lt;iostream&gt;
struct X {
  int n;
  X(int n) : n { n } { }
};
int main() {
  auto p = <strong class="bold">new (std::nothrow) X{ 3 }</strong>;
<strong class="bold">  if(p) {</strong>
<strong class="bold">      std::cout &lt;&lt; p-&gt;n; // ...use *p as needed...</strong>
<strong class="bold">  }</strong>
  delete p; // fine even in p is null
}</pre> <p>The problem with this approach is that code quickly becomes littered with tests, as there is rarely only one pointer in a program, reminding us that the beauty of code using exceptions is that one does not need to worry about those tests. With exceptions, either <code>operator new()</code> and the subsequent construction both succeeded and one can use the resulting pointer confidently, or one of these steps failed and code execution did not reach the point where one could get into trouble:</p>
<pre class="source-code">
#include &lt;new&gt;
#include &lt;iostream&gt;
struct X {
  int n;
  X(int n) : n { n } { }
};
int main() {
  auto p = <strong class="bold">new X{ 3 }</strong>; // throws if operator new() or
                        // X::X(int) fails
<strong class="bold">  std::cout &lt;&lt; p-&gt;n; // ...use *p as needed...</strong>
  delete p;
}</pre> <p>Of course, one can<a id="_idIndexMarker505"/> get in trouble even with exceptions, for example, if there is an execution path that lets <code>p</code> remain null or uninitialized and others where that cannot happen (you can usually avoid this by initializing your objects on declaration, but that is not always possible); let us leave these code hygiene considerations aside for now as they would deviate from our topic of interest.</p>
<p>An important consideration when facing a failure-to-allocate situation is what to do when it happens. Whether our code base uses exceptions or not, we most probably do not want to let the execution of our program continue and therefore incur UB through such things as the improper use of a null pointer.</p>
<p>A common way to stop execution at the point of failure-to-allocate is to wrap the tentative allocation and construction operation, the subsequent test on the resulting pointer, and the action to take if the pointer is null in some code construct. The code we want to wrap will be something like the following, supposing we want to allocate-then-construct an <code>int</code> object:</p>
<pre class="source-code">
// ...
int *p = new int{ 3 };
if(!p) std::abort(); // for example
return p;
// ...</pre> <p>This code used <code>std::abort()</code> as a mechanism to end program execution; exceptions would provide us with potentially recoverable errors, but without exceptions, most standard mechanisms at our disposal will lead to program termination, and <code>std::abort()</code> is a<a id="_idIndexMarker506"/> reasonable choice in this case.</p>
<p class="callout-heading">Ways to conclude program execution</p>
<p class="callout">A C++ program can conclude in many different ways: reaching the end of the <code>main()</code> function is the most obvious one, but other examples exist. For example, <code>std::exit()</code> is used for normal program termination accompanied by cleanup steps; <code>std::quick_exit()</code> is used for program termination without cleanup steps. One can use <code>std::atexit()</code> and <code>std::at_quick_exit()</code> to register some functions to be called before exiting, and <code>std::abort()</code> is used to signal abnormal program termination without cleanup steps. The <code>std::terminate()</code> function is used when some unpleasantness in a documented list of situations occurs (this list includes such things as an exception being thrown from the constructor of a <code>static</code> variable or from the body of a <code>noexcept</code> function). In our case, the only mechanism that really fit was <code>std::abort()</code>.</p>
<p>One possible approach<a id="_idIndexMarker507"/> to solve this problem is to use a macro and an <strong class="bold">immediately-invoked function expression</strong> (<strong class="bold">IIFE</strong>), which is the name given to an expression made from an anonymous lambda that is at once created, executed, and discarded. To make our solution general, we need to be able to do the following:</p>
<ul>
<li>Specify the type of object to create</li>
<li>Make the macro variadic, as we need to be able to pass any number of arguments of any type to the object’s constructor</li>
</ul>
<p>A possible implementation of such a macro would be <code>TRY_NEW</code> as follows:</p>
<pre class="source-code">
#include &lt;new&gt;
#include &lt;cstdlib&gt;
<strong class="bold">#define TRY_NEW(T,...) [&amp;] { \</strong>
<strong class="bold">  auto p = new (std::nothrow) T(__VA_ARGS__); \</strong>
<strong class="bold">  if(!p) std::abort(); \</strong>
<strong class="bold">  return p; \</strong>
<strong class="bold">}()</strong>
struct dies_when_newed {
  void* operator new(std::size_t, std::nothrow_t) {
      return {};
  }
};
int main() {
<strong class="bold">  // p0 is int*, points to an int{ 0 }</strong>
  auto p0 = <strong class="bold">TRY_NEW(int)</strong>;
<strong class="bold">  // p1 is int*, points to an int{ 3 }</strong>
  auto p1 = <strong class="bold">TRY_NEW(int, 3)</strong>;
  auto q = <strong class="bold">TRY_NEW(dies_when_newed)</strong>; <strong class="bold">// calls abort()</strong>
}</pre> <p>Not everyone is<a id="_idIndexMarker508"/> familiar with variadic macros, so let’s take it step by step:</p>
<ul>
<li>The “signature” of our macro is <code>TRY_NEW(T,...)</code>, meaning <code>T</code> is mandatory and <code>...</code> could be any number of tokens (including none at all) separated by commas. Unsurprisingly, we will use <code>T</code> for the type to construct and <code>...</code> for the arguments to pass to the constructor that will be invoked.</li>
<li>Since we wrote the macro on more than one line (for readability), each line but the last terminates with a space followed by a backslash to inform the preprocessor that it should continue parsing on the next line.</li>
<li>The symbols on <code>...</code> are relayed through the special macro named <code>__VA_ARGS__</code>, which expands to what <code>...</code> contained and can be empty if <code>...</code> itself is empty. This works in both C and C++. Note that we use parentheses, not braces, in the constructor call as we want to avoid unwittingly building an initializer list if all elements of <code>__VA_ARGS__</code> are of the same type.</li>
<li>We test the <code>p</code> pointer resulting from the call to a <code>std::nothrow</code> version of <code>operator new()</code> and call <code>std::abort()</code> if <code>p</code> is null.</li>
<li>This entire sequence of operations is, as announced, wrapped in an IIFE and the newly allocated pointer is returned. Note that we could also have returned a <code>std::unique_ptr&lt;T&gt;</code> object from that lambda if we had wanted to do so. Also, note that this lambda expression uses a <code>[&amp;]</code> capture block to ensure the availability <a id="_idIndexMarker509"/>of tokens in <code>__VA_ARGS__</code> within the scope of the lambda.</li>
</ul>
<p class="callout-heading">A small but interesting side effect</p>
<p class="callout">Note that since we used parentheses (the same would hold for braces), an empty <code>__VAR_ARGS__</code> will lead this macro to zero-initialize fundamental types such as <code>int</code> instead of leaving them uninitialized. You can compare: as of C++23, <code>new int;</code> yields a pointer to an uninitialized <code>int</code> object, but <code>new int();</code> and <code>new int{};</code> both initialize the allocated block with a value of zero. There is an upside to this, as with this macro, we will not end up with a pointer to an uninitialized object, even for trivial types. However, there is also a downside as we will be paying for an initialization even in cases where it might not have been necessary.</p>
<p>Another approach would be to use a variadic function template, which might lead to a better debugging experience in practice. It has slightly different-looking client code but is otherwise similar in usage and effect:</p>
<pre class="source-code">
#include &lt;new&gt;
#include &lt;cstdlib&gt;
#include &lt;utility&gt;
<strong class="bold">template &lt;class T, class ... Args&gt;</strong>
<strong class="bold">  auto try_new(Args &amp;&amp;... args) {</strong>
<strong class="bold">      auto p =</strong>
<strong class="bold">        new (std::nothrow) T(std::forward&lt;Args&gt;(args)...);</strong>
<strong class="bold">      if(!p) std::abort();</strong>
<strong class="bold">      return p;</strong>
<strong class="bold">  }</strong>
struct dies_when_newed {
  void* operator new(std::size_t, std::nothrow_t) {
      return {};
  }
};
int main() {
<strong class="bold">  // p0 is int*, points to an int{ 0 }</strong>
  auto p0 = <strong class="bold">try_new&lt;int&gt;()</strong>;
<strong class="bold">  // p1 is int*, points to an int{ 3 }</strong>
  auto p1 = <strong class="bold">try_new&lt;int&gt;(3)</strong>;
  auto q = <strong class="bold">try_new&lt;dies_when_newed&gt;()</strong>; <strong class="bold">// calls abort()</strong>
}</pre> <p>The call syntax for the<a id="_idIndexMarker510"/> variadic function version looks like a cast, and arguments passed to <code>try_new()</code> are perfectly forwarded to the constructor of <code>T</code> to ensure that the expected constructor is called in the end. As was the case with the macro, we could have chosen to return a <code>std::unique_ptr&lt;T&gt;</code> object instead of a <code>T*</code> object with this function.</p>
<h1 id="_idParaDest-143"><a id="_idTextAnchor147"/>Out-of-memory situations and new_handler</h1>
<p>So far in this <a id="_idIndexMarker511"/>book, including this<a id="_idIndexMarker512"/> chapter, we have stated that <code>operator new()</code> and <code>operator new[]()</code> typically throw <code>std::bad_alloc</code> when failing to allocate memory. It’s true to a wide extent, but there is a subtlety we have avoided so far and to which we will now give some time and attention.</p>
<p>Imagine a situation where user code has specialized the memory allocation functions to fetch memory blocks from a pre-allocated data structure with interesting performance characteristics. Suppose that this data structure initially allocates space for a small number of blocks and then goes on to allocate more space once the user code exhausts the blocks from the initial allocation. Expressed otherwise: in this situation, we have an initial, fast setting (let’s call that the “optimistic” state) and a secondary setting (let’s call that the “second chance” state) that lets user code continue allocating once the “optimistic” state’s resources have been consumed.</p>
<p>For a scenario such as this to be seamless, with a transparent change of allocation strategy achievable without the explicit intervention of user code, explicitly throwing <code>std::bad_alloc</code> would be insufficient. Throwing would complete the execution of <code>operator new()</code> and client code could catch the exception and take action, of course, but in this (reasonable) scenario, we would like failure to allocate to lead to some action being taken and <code>operator new()</code> to try again with the updated state of things, if any.</p>
<p>In C++, scenarios such as this are handled through a <code>std::new_handler</code>, which is an alias for a function pointer of type <code>void(*)()</code>. What one needs to know is the following:</p>
<ul>
<li>There is a global <code>std::new_handler</code> in a program, and by default, its value is <code>nullptr</code>.</li>
<li>One can set the active <code>std::new_handler</code> through the <code>std::set_new_handler()</code> function, and one can get the active <code>std::new_handler</code> through the <code>std::get_new_handler()</code> function. Note that as a convenience, <code>std::set_new_handler()</code> returns the <code>std::new_handler</code> that is being replaced.</li>
<li>When an allocation function such as <code>operator new()</code> fails, what it should do is first get the active <code>std::new_handler</code>. If that pointer is null, then the allocation function should throw <code>std::bad_alloc</code> as we have done so far; otherwise, it should call that <code>std::new_handler</code> and try again under the new conditions that this call installed.</li>
</ul>
<p>As could be expected, your standard library should already implement this algorithm, but our own overloads of <code>operator new()</code> and <code>operator new[]()</code> have not done so, at least so far. To show how to benefit from a <code>std::new_handler</code>, we will now implement an artificial version of the aforementioned two-step scenario.</p>
<p>This toy implementation will use the member version of the allocation operators for some <code>X</code> type and behave as if we initially had enough memory for <code>limit</code> objects of that type (normally, we would actually manage that memory, and you can see an example of such management in <a href="B21071_10.xhtml#_idTextAnchor153"><em class="italic">Chapter 10</em></a> where we will provide a more realistic example). We will install a <code>std::new_handler</code> that, when called, changes <code>limit</code> to a higher number, and then resets the active handler to <code>nullptr</code> such that subsequent failures to <a id="_idIndexMarker513"/>allocate <code>X</code> objects <a id="_idIndexMarker514"/>will lead to throwing <code>std::bad_alloc</code>:</p>
<pre class="source-code">
#include &lt;new&gt;
#include &lt;vector&gt;
#include &lt;iostream&gt;
<strong class="bold">struct X {</strong>
<strong class="bold">  // toy example, not thread-safe</strong>
<strong class="bold">  static inline int limit = 5;</strong>
<strong class="bold">  void* operator new(std::size_t n) {</strong>
<strong class="bold">      std::cout &lt;&lt; "X::operator new() called with "</strong>
<strong class="bold">                &lt;&lt; limit &lt;&lt; " blocks left\n";</strong>
<strong class="bold">      while (limit &lt;= 0) {</strong>
<strong class="bold">        if (auto hdl = std::get_new_handler(); hdl)</strong>
<strong class="bold">            hdl();</strong>
<strong class="bold">        else</strong>
<strong class="bold">            throw std::bad_alloc{};</strong>
<strong class="bold">      }</strong>
<strong class="bold">      --limit;</strong>
<strong class="bold">      return ::operator new(n);</strong>
<strong class="bold">  }</strong>
<strong class="bold">  void operator delete(void* p) {</strong>
<strong class="bold">      std::cout &lt;&lt; "X::operator delete()\n";</strong>
<strong class="bold">      ::operator delete(p);</strong>
<strong class="bold">  }</strong>
<strong class="bold">  // same for the array versions</strong>
<strong class="bold">};</strong>
<strong class="bold">int main() {</strong>
<strong class="bold">  std::set_new_handler([]() noexcept {</strong>
<strong class="bold">      std::cout &lt;&lt; "allocation failure, "</strong>
<strong class="bold">                   "fetching more memory\n";</strong>
<strong class="bold">      X::limit = 10;</strong>
<strong class="bold">      std::set_new_handler(nullptr); // as per default</strong>
<strong class="bold">  });</strong>
  std::vector&lt;X*&gt; v;
  v.reserve(100);
  try {
      for (int i = 0; i != 10; ++i)
         v.emplace_back(new X);
  } catch(...) {
<strong class="bold">      // this will never be reached with this program</strong>
      std::cerr &lt;&lt; "out of memory\n";
  }
  for (auto p : v) delete p;
}</pre> <p>Note the way that <code>X::operator new()</code> handles failure: if it notices that it will not be able to meet its postconditions, it gets the active <code>std::new_handler</code>, and if it’s non-null, calls it before trying again. This means that the <code>std::new_handler</code>, when called, has to either change the situation in such a way that a subsequent tentative allocation could succeed or change the <code>std::new_handler</code> to <code>nullptr</code> such that failure will lead to an exception being thrown. Failure to respect these rules could lead to an infinite loop and much sadness would ensue.</p>
<p>The handler installed in <code>main()</code> for this toy example does this: when called, it changes the conditions under which the allocations will be performed (it raises the value of <code>X::limit</code>). It then calls <code>std::set_new_handler()</code> with <code>nullptr</code> as we have not planned for another approach after the “optimistic” and “second chance” situations, so if we exhaust <a id="_idIndexMarker515"/>the<a id="_idIndexMarker516"/> second chance resources, we (as they say) are toast.</p>
<p class="callout-heading">A lambda as new_handler?</p>
<p class="callout">You might have<a id="_idIndexMarker517"/> noticed that we described the <code>std::new_handler</code> type as being an alias for a function pointer of the <code>void(*)()</code> type, yet in our toy example, we installed a lambda. Why does that work? Well, it happens that a stateless lambda—a lambda expression with an empty capture block—is implicitly convertible to a function pointer with the same calling signature. It’s a useful thing to know under many circumstances, such as when writing C++ code that interfaces with C code or operating system APIs.</p>
<p>We are now about to enter a strange and quite technical part of this chapter, where we will see how to leverage C++ to handle atypical memory.</p>
<h1 id="_idParaDest-144"><a id="_idTextAnchor148"/>Standard C++ and exotic memory</h1>
<p>Our last example in <a id="_idIndexMarker518"/>this slightly strange chapter with examples of unusual memory management usage is concerned with the ways in which we can write standard C++ programs that deal with “exotic” memory. By “exotic,” we mean memory that requires explicit actions to “touch” (allocate, read from, write to, deallocate, and so on) and that differs from a “normal” memory block under the control of our program, such as the one used in the illustrative example of memory-mapped usage with placement <code>new</code> earlier in this chapter. Examples of such memory include persistent (non-volatile) memory or shared memory, but anything <em class="italic">out of the ordinary</em> will do, really.</p>
<p>Since we have to pick an example, we will write an example using a (fictional) shared memory block.</p>
<p class="callout-heading">A little white lie…</p>
<p class="callout">It’s important to understand that we are describing a mechanism for memory that would normally be shared between <em class="italic">processes</em>, but inter-process communication is the domain of the operating system. Standard C++ only describes the rules for sharing data between <em class="italic">threads</em> in a process; for that reason, we will tell a little white lie and write a multithreaded system, not a multiple-process one, using that memory to share data. Our focus is on memory management facilities, not inter-process communication, so that should not pose a problem.</p>
<p>Following the same approach as we did in previous sections of this chapter, we will craft a portable illustration of how to proceed in code that seeks to manage atypical memory, and let you map the details to the services of your chosen platform. Our example code will take the following shape:</p>
<ul>
<li>A shared memory block will be allocated. We will make it look like this memory is special in the sense that one needs special operating system functions to create it, allocate it, or deallocate it, but we will deliberately avoid using actual operating system functions. This means that if you want to use the code in this section for a real application, you will need to adapt it to your chosen platform’s API.</li>
<li>We will craft a “handmade” version of a toy program that uses this fictional API for shared memory in order to illustrate what user code would look like under these circumstances.</li>
<li>Then, we will show how understanding the memory management facilities of C++ can help us write more pleasant and “normal looking” user code that does the same thing as the “handmade” one… or even better.</li>
</ul>
<p class="callout-heading">Fictional realism?</p>
<p class="callout">This entire section on C++ and exotic memory, which we cover next, will hopefully be interesting, and the code we will write will strive to be realistic with respect to memory management. As mentioned previously, since the C++ standard is mostly silent on the idea of multi-process systems, we will try to make multithreaded code look kind of like multi-process code. I hope you, astute reader, will accept this proposition.</p>
<p class="callout">Please note that there will be a small amount of low-level synchronization in user code for this section, including some through atomic variables. I tried to keep it minimal yet reasonably realistic and hope you will be able to accept it even though I will not explain it all in detail, with this book’s focus being on memory management rather than on concurrent computing (another fine topic, of course). Feel free to use your favorite concurrent programming resource if you want to know more about such things as waiting on atomics or using thread fences.</p>
<p>Ready? Let’s<a id="_idIndexMarker519"/> do this!</p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor149"/>A fictional shared memory API</h2>
<p>We will write an<a id="_idIndexMarker520"/> API that is <a id="_idIndexMarker521"/>fictional but inspired by what one finds in most operating systems, except that we will report errors through exceptions to simplify user code. Operating systems mostly report errors through error codes expressed from return values, but this leads to user code that is more involved. I hope this seems like an acceptable compromise to you, dear reader.</p>
<p>As most operating systems do, we will abstract the actual resource through a form of handle, or key; creating a “shared memory” segment of some size will yield a key (an integral identifier), after which, accessing that memory will require that key, and so will destroying that memory. As can be expected with a facility meant to be used to share data between processes, destroying the memory will not finalize the objects therein, so user code will need to ensure that objects in the shared memory are destroyed before releasing the shared memory segment.</p>
<p>The signatures <a id="_idIndexMarker522"/>and types<a id="_idIndexMarker523"/> for our API will be as follows:</p>
<pre class="source-code">
// ...
#include &lt;cstddef&gt; // std::size_t
#include &lt;new&gt; // std::bad_alloc
#include &lt;utility&gt; // std::pair
class invalid_shared_mem_key {};
enum shared_mem_id : std::size_t;
shared_mem_id create_shared_mem(std::size_t size);
std::pair&lt;void*, std::size_t&gt;
  get_shared_mem(shared_mem_id);
void destroy_shared_mem(shared_mem_id);
// ...</pre> <p>You might notice that we are using an <code>enum</code> type for <code>shared_mem_id</code>. The reason for this is that <code>enum</code> types are distinct types in C++, not just aliases as one would get from <code>using</code> or <code>typedef</code>. Having distinct types can be useful when overloading functions based on the types of their arguments. It’s a useful trick to know: if we write two functions with the same name (one that takes an argument of the <code>shared_mem_id</code> type and another that takes an argument of the <code>std::size_t</code> type), these will be distinct functions, even though the underlying type of <code>shared_mem_id</code> is <code>std::size_t</code>.</p>
<p>Since we are building an artificial implementation of “shared memory” to show how memory allocation functions can simplify user code, the implementation for the functions of our API will be written to be simple, but let us write client code that behaves as if it were using shared memory. We will define a shared memory segment as a <code>shared_mem_block</code> modeled by a pair made from an array of bytes and a size in bytes. We will keep a <code>std::vector</code> object of that type, using the indices in that array as <code>shared_mem_id</code>. This means that when a <code>shared_mem_block</code> object is destroyed, we will not reuse its index in the <code>std::vector</code> (the container will eventually have “holes,” so to speak).</p>
<p>Our implementation is as follows. Note that it is not thread-safe, but that does not impact our <a id="_idIndexMarker524"/>memory <a id="_idIndexMarker525"/>management-related discourse:</p>
<pre class="source-code">
// ...
#include &lt;vector&gt;
#include &lt;memory&gt;
#include &lt;utility&gt;
struct shared_mem_block {
  std::unique_ptr&lt;char[]&gt; mem;
  std::size_t size;
};
std::vector&lt;shared_mem_block&gt; shared_mems;
std::pair&lt;void*, std::size_t&gt;
  get_shared_mem(shared_mem_id id) {
  if (id &lt; std::size(shared_mems))
      return { shared_mems[id].mem.get(),
               shared_mems[id].size };
  return { nullptr, 0 };
}
shared_mem_id create_shared_mem(std::size_t size) {
  auto p = std::make_unique&lt;char[]&gt;(size);
  shared_mems.emplace_back(std::move(p), size);
  // note the parentheses
  return shared_mem_id(std::size(shared_mems) - 1);
}
// function for internal purposes only
bool is_valid_shared_mem_key(shared_mem_id id) {
  return id &lt; std::size(shared_mems) &amp;&amp;
         shared_mems[id].mem;
}
void destroy_shared_mem(shared_mem_id id) {
  if (!is_valid_shared_mem_key(id))
      throw invalid_shared_mem_key{};
  shared_mems[id].mem.reset();
}</pre> <p>If you want to experiment, you can replace the implementation of these functions with equivalent implementations that call the functions of your chosen operating system, adjusting the API if needed.</p>
<p>Equipped with this implementation, we can now compare a “handmade” example of shared memory-using code with one that benefits from the facilities of C++. We will do this comparison with code where one allocates some chunk of data from a shared memory segment and then launches two threads (a writer and a reader). The writer will write to that shared data, and then (with minimal synchronization) the reader will read from it. As <a id="_idIndexMarker526"/>mentioned <a id="_idIndexMarker527"/>previously, our code will use <em class="italic">intra</em>-process synchronization (C++ atomic variables), but in real code, you should use <em class="italic">inter</em>-process synchronization mechanisms provided by the operating system.</p>
<p class="callout-heading">A note on lifetime</p>
<p class="callout">You might remember from <a href="B21071_01.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a> that each object has an associated lifetime, and that the compiler keeps track of this fact in your programs. Our fictional multiple-process example is really a single-process, multithreaded example, so the usual C++ lifetime rules apply.</p>
<p class="callout">If you want to take the code in this section and write a real multi-process system to run some tests, you might want to consider using <code>std::start_lifetime_as()</code> from C++23 in those processes that did not explicitly create the <code>data</code> object, and avoid detrimental optimizations from happening based on the compiler’s reasoning that, in these processes, the objects have never been constructed. In earlier compilers, one trick that generally works is calling <code>std::memcpy()</code> of the not-officially-constructed object onto itself, effectively starting its lifetime.</p>
<p>In both our “handmade” and our standard-looking implementations, we will be using a <code>data</code> object made of an <code>int</code> value and a Boolean <code>ready</code> flag:</p>
<pre class="source-code">
struct data {
  bool ready;
  int value;
};</pre> <p>In a single-process <a id="_idIndexMarker528"/>implementation, a <a id="_idIndexMarker529"/>better choice for the completion flag would be an <code>atomic&lt;bool&gt;</code> object as we want to make sure the write to the <code>ready</code> flag happens before the write to the value, but since we want this example to look like we are using inter-process shared memory, we will limit ourselves to a simple <code>bool</code> and ensure this synchronization through other means.</p>
<p class="callout-heading">A word on synchronization</p>
<p class="callout">In a contemporary<a id="_idIndexMarker530"/> program, optimizing compilers will often reorder operations that seem independent to generate better code, and processors will do the same once the code has been generated in order to maximize usage of the processor’s internal pipeline. Concurrent code sometimes contains dependencies that are neither visible to the compiler nor to the processor. In our examples, we will want the <code>ready</code> completion flag to become <code>true</code> only after the write to <code>value</code> has been performed; this order is only important because the writes are performed in one thread but <em class="italic">another</em> thread will look at <code>ready</code> to know whether <code>value</code> can be read.</p>
<p class="callout">Not enforcing the <code>value</code>-then-<code>ready</code> sequence of writes through some form of synchronization would let either the compiler or the processor reorder these (seemingly independent) writes <a id="_idIndexMarker531"/>and break our assumptions on the meaning of <code>ready</code>.</p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor150"/>A handmade user code example</h2>
<p>We can, of course, write <a id="_idIndexMarker532"/>user code that uses our fictional API without resorting to specialized memory management facilities of C++, simply relying on placement <code>new</code> usage as seen in <a href="B21071_07.xhtml#_idTextAnchor116"><em class="italic">Chapter 7</em></a>. It might be tempting to think of placement <code>new</code> as a specialized facility since you might have learned of it from this book, but if that is your perspective, you are invited to reconsider: the placement <code>new</code> mechanism is a fundamental memory management tool used in almost every program, whether user code is aware of it or not.</p>
<p>As a reminder, our example program will do the following:</p>
<ul>
<li>Create a shared memory segment of some size (we will allocate much more than we need in this case).</li>
<li>Construct a <code>data</code> object at the beginning of that segment, obviously through placement <code>new</code>.</li>
<li>Start a thread that will wait for a signal on the <code>go</code> variable of type <code>atomic&lt;bool&gt;</code>, then obtain access to the shared memory segment, write to the <code>value</code> data member and then only signal that the write has occurred through the <code>ready</code> data member.</li>
<li>Start another thread that will obtain access to the shared memory segment, get a pointer to the shared <code>data</code> object therein, and then do some (very inefficient) busy waiting on the <code>ready</code> flag to change state, after which <code>value</code> will be read and used. Once this has been done, completion will be signaled through the <code>done</code> flag of type <code>atomic&lt;bool&gt;</code>.</li>
<li>Our program will then read a key from the keyboard, signal the threads (the writer thread, really) that it’s time to start working, and wait until they are done before freeing the shared memory segment and concluding its work.</li>
</ul>
<p>We thus end up with the following:</p>
<pre class="source-code">
// ...
#include &lt;thread&gt;
#include &lt;atomic&gt;
#include &lt;iostream&gt;
int main() {
  // we need a N-bytes shared memory block
  constexpr std::size_t N = 1'000'000;
<strong class="bold">  auto key = create_shared_mem(N);</strong>
  // map a data object in the shared memory block
<strong class="bold">  auto [p, sz] = get_shared_mem(key);</strong>
  if (!p) return -1;
  // start the lifetime of a non-ready data object
<strong class="bold">  auto p_data = new (p) data{ false };</strong>
  std::atomic&lt;bool&gt; go{ false };
  std::atomic&lt;bool&gt; done{ false };
  std::jthread writer{ [key, &amp;go] {
<strong class="bold">      go.wait(false);</strong>
<strong class="bold">      auto [p, sz] = get_shared_mem(key);</strong>
<strong class="bold">      if (p) {</strong>
<strong class="bold">        auto p_data = static_cast&lt;data*&gt;(p);</strong>
<strong class="bold">        p_data-&gt;value = 3;</strong>
<strong class="bold">        std::atomic_thread_fence(</strong>
<strong class="bold">            std::memory_order_release</strong>
<strong class="bold">        );</strong>
<strong class="bold">        p_data-&gt;ready = true;</strong>
<strong class="bold">      }</strong>
  } };
  std::jthread reader{ [key, &amp;done] {
<strong class="bold">      </strong><strong class="bold">auto [p, sz] = get_shared_mem(key);</strong>
<strong class="bold">      if (p) {</strong>
<strong class="bold">        auto p_data = static_cast&lt;data*&gt;(p);</strong>
<strong class="bold">        while (!p_data-&gt;ready)</strong>
<strong class="bold">            ; // busy waiting, not cool</strong>
        std::cout &lt;&lt; "read value "
                  &lt;&lt; p_data-&gt;value &lt;&lt; '\n';
      }
<strong class="bold">      done = true;</strong>
<strong class="bold">      done.notify_all();</strong>
  } };
  if (char c; !std::cin.get(c)) exit(-1);
<strong class="bold">  go = true;</strong>
<strong class="bold">  go.notify_all();</strong>
  // writer and reader run to completion, then complete
<strong class="bold">  done.wait(false);</strong>
<strong class="bold">  p_data-&gt;~data();</strong>
<strong class="bold">  destroy_shared_mem(key);</strong>
}</pre> <p>We made this <a id="_idIndexMarker533"/>work: we have an infrastructure of sorts to manage shared memory segments, we can use these memory blocks to share data, and we can write code that reads from that shared data as well as writes to it. Note that we captured the key in each thread in a <code>key</code> variable and then obtained the memory block within each lambda through that key, but it would also be reasonable to simply capture the <code>p_data</code> pointer and use it.</p>
<p>Notice, however, that we did not really manage that block: we created it and used a small chunk of size <code>sizeof(data)</code> at the beginning. Now, what if we had wanted to create multiple objects in that zone? And what if we had wanted to write code that both creates and destroys objects, introducing the need to manage what parts of that block are in use at a given time? With what we just wrote, that would mean doing it all in user code, a<a id="_idIndexMarker534"/> somewhat burdensome endeavor.</p>
<p>Keeping that in mind, we will now solve the same problem but with a different approach.</p>
<h2 id="_idParaDest-147"><a id="_idTextAnchor151"/>A standard-looking user code equivalent</h2>
<p>So, what<a id="_idIndexMarker535"/> mechanism does C++ offer us if we want to use “exotic” memory in a more idiomatic manner? Well, one way to do so is as follows:</p>
<ul>
<li>To write a manager class for the “exotic” memory, encapsulating the non-portable interface to the operating system and exposing services that are closer to what C++ user code would expect</li>
<li>To write overloads of the memory allocation operators (<code>operator new()</code>, <code>operator delete()</code>, and so on) that take a reference to such a manager object as an additional argument</li>
<li>To make these overloaded memory allocation operators bridge the gap between portable and non-portable code through delegation on the memory manager object</li>
</ul>
<p>This way, user code can be written essentially as “normal looking” code that calls <code>new</code> and <code>delete</code> operators, except that these calls will use the same kind of extended notation seen in <a href="B21071_07.xhtml#_idTextAnchor116"><em class="italic">Chapter 7</em></a> for such things as the <code>nothrow</code> or placement versions of <code>operator new()</code>.</p>
<p>Our <code>shared_mem_mgr</code> class will use the fictional operating system API described earlier in this section but, normally, one would write a class that encapsulates whatever operating system services are required to access the atypical memory one aims to use in a program.</p>
<p>Being an example made for simplicity, mostly to show how the feature works and can be used, the astute reader that you are will hopefully see much room for improvement and optimization… Indeed, this manager is really slow and memory consuming, keeping a <code>std::vector&lt;bool&gt;</code> object where each <code>bool</code> value indicates whether a byte in the memory block is taken or not and performing a naïve linear search through that container whenever an allocation request is made (also, it’s not thread-safe, which is bad!). We will examine some quality of implementation considerations in <a href="B21071_10.xhtml#_idTextAnchor153"><em class="italic">Chapter 10</em></a>, but nothing stops you from taking <code>shared_mem_mgr</code> and making it significantly better in the meantime.</p>
<p>You will notice that <code>shared_mem_mgr</code> has been expressed as an RAII type: its constructor creates a shared memory segment, its destructor frees that memory segment, and the <code>shared_mem_mgr</code> type has been made uncopiable as is often the case for RAII types. The key member functions to look at in the following code excerpt are <code>allocate()</code> and <code>deallocate()</code>; the former tries to allocate a block from the shared memory<a id="_idIndexMarker536"/> segment and notes that this has been done, whereas the latter frees the memory associated with an address within the block:</p>
<pre class="source-code">
#include &lt;algorithm&gt;
#include &lt;iterator&gt;
#include &lt;new&gt;
class shared_mem_mgr {
  shared_mem_id key;
  std::vector&lt;bool&gt; taken;
  void *mem;
  auto find_first_free(std::size_t from = 0) {
      using namespace std;
      auto p = find(begin(taken) + from, end(taken),
                    false);
      return distance(begin(taken), p);
  }
  bool at_least_free_from(std::size_t from, int n) {
      using namespace std;
      return from + n &lt; size(taken) &amp;&amp;
             count(begin(taken) + from,
                   begin(taken) + from + n,
                   false) == n;
  }
  void take(std::size_t from, std::size_t to) {
      using namespace std;
      fill(begin(taken) + from, begin(taken) + to,
          begin(taken) + from, true);
  }
  void free(std::size_t from, std::size_t to) {
      using namespace std;
      fill(begin(taken) + from, begin(taken) + to,
          begin(taken) + from, false);
  }
public:
  // create shared memory block
<strong class="bold">  shared_mem_mgr(std::size_t size)</strong>
      : key{ <strong class="bold">create_shared_mem(size)</strong> }, taken(size) {
<strong class="bold">      auto [p, sz] = get_shared_mem(key);</strong>
      if (!p) throw invalid_shared_mem_key{};
<strong class="bold">      </strong><strong class="bold">mem = p;</strong>
  }
  shared_mem_mgr(const shared_mem_mgr&amp;) = delete;
  shared_mem_mgr&amp;
      operator=(const shared_mem_mgr&amp;) = delete;
<strong class="bold">  void* allocate(std::size_t n) {</strong>
      using namespace std;
      std::size_t i = find_first_free();
      // insanely inefficient
      while (!at_least_free_from(i, n) &amp;&amp; i != size(taken))
        i = find_first_free(i + 1);
      if (i == size(taken)) throw bad_alloc{};
      take(i, i + n);
      return static_cast&lt;char*&gt;(mem) + i;
  }
<strong class="bold">  void deallocate(void *p, std::size_t n) {</strong>
      using namespace std;
      auto i = distance(
         static_cast&lt;char*&gt;(mem), static_cast&lt;char*&gt;(p)
      );
      take(i, i + n);
  }
<strong class="bold">  ~shared_mem_mgr() {</strong>
<strong class="bold">      destroy_shared_mem(key);</strong>
<strong class="bold">  }</strong>
};</pre> <p>As you can see, <code>shared_mem_mgr</code> really is a class that manages a chunk of memory, and there is<a id="_idIndexMarker537"/> no magic involved. Should someone want to improve the memory management algorithms, one could do so without touching the interface of this class, benefiting from the low coupling that stems from encapsulation.</p>
<p class="callout-heading">If you want to play…</p>
<p class="callout">One interesting way to refine <code>shared_mem_mgr</code> would be to first make this class responsible for allocating and freeing the shared memory, as it already does, then write a different class to manage the memory within that shared memory block, and finally, make them work together. This way, one could use <code>shared_mem_mgr</code> with different memory management algorithms and pick management strategies based on the needs of individual programs, or sections thereof. Something to try if you want to have fun!</p>
<p>The next step is to implement the allocation operator overloads that take an argument of type <code>shared_mem_mgr&amp;</code>. This is essentially trivial since all these overloads need to do is delegate <a id="_idIndexMarker538"/>the work to the manager:</p>
<pre class="source-code">
void* operator new(std::size_t n, shared_mem_mgr&amp; mgr) {
  return mgr.allocate(n);
}
void* operator new[](std::size_t n, shared_mem_mgr&amp; mgr) {
  return mgr.allocate(n);
}
void operator delete(void *p, std::size_t n,
                    shared_mem_mgr&amp; mgr) {
  mgr.deallocate(p, n);
}
void operator delete[](void *p, std::size_t n,
                      shared_mem_mgr&amp; mgr) {
  mgr.deallocate(p, n);
}</pre> <p>Equipped with our manager and these overloads, we can write our test program that performs the same task as the “handmade” one from the previous section. In this case, however, there are some differences:</p>
<ul>
<li>We do not need to manage the shared memory segment’s creation and destruction. These tasks are handled by the <code>shared_mem_mgr</code> object as part of its implementation of the RAII idiom.</li>
<li>We do not need to manage the shared memory block at all, as this task is assigned to the <code>shared_mem_mgr</code> object. Finding a location in the block to put an object, tracking how the block is being used for objects, ensuring that it’s possible to distinguish used areas from unused ones, and so on are all part of that class’s responsibilities.</li>
<li>As a corollary, in the “handmade” version, we constructed an object at the beginning of the shared memory block and stated that it would be a burden on user code to construct more objects or manage the shared memory segment to take into account numerous calls to the <code>new</code> and <code>delete</code> operators, but in this implementation, we can freely call <code>new</code> and <code>delete</code> as much as we want since this memory management becomes transparent to client code.</li>
</ul>
<p>The construction<a id="_idIndexMarker539"/> aspect of objects in atypical memory is rather easy: just pass the additional argument in the call to the <code>new</code> and <code>new[]</code>operators. The finalization part of objects managed through a manager such as this is slightly more complex though: we cannot write the equivalent of <code>delete p</code> on our pointers as this would try to finalize the object <em class="italic">and</em> deallocate the memory through “normal” means. Instead, we need to manually finalize the objects, and then manually call the appropriate version of the <code>operator delete()</code> function in order to do the exotic memory cleanup tasks. Of course, given what we have written in <a href="B21071_06.xhtml#_idTextAnchor096"><em class="italic">Chapter 6</em></a>, you could encapsulate these tasks in a smart pointer of your own to get simpler and safer user code.</p>
<p>We end up with the following example program:</p>
<pre class="source-code">
int main() {
  // we need a N-bytes shared memory block
  constexpr std::size_t N = 1'000'000;
  // HERE
<strong class="bold">  shared_mem_mgr mgr{ N };</strong>
  // start the lifetime of a non-ready data object
<strong class="bold">  </strong><strong class="bold">auto p_data = new (mgr) data{ false };</strong>
  std::atomic&lt;bool&gt; go{ false };
  std::atomic&lt;bool&gt; done{ false };
  std::jthread writer{ [p_data, &amp;go] {
      go.wait(false);
      p_data-&gt;value = 3;
      std::atomic_thread_fence(std::memory_order_release);
      p_data-&gt;ready = true;
  } };
  std::jthread reader{ [p_data, &amp;done] {
      while (!p_data-&gt;ready)
        ; // busy waiting, not cool
      std::cout &lt;&lt; "read value " &lt;&lt; p_data-&gt;value &lt;&lt; '\n';
      done = true;
      done.notify_all();
  } };
  if (char c; !std::cin.get(c)) exit(-1);
  go = true;
  go.notify_all();
  // writer and reader run to completion, then complete
  done.wait(false);
<strong class="bold">  p_data-&gt;~data();</strong>
<strong class="bold">  operator delete(p_data, sizeof(data), mgr);</strong>
}</pre> <p>This is still not a <a id="_idIndexMarker540"/>trivial example, but the memory management aspect is clearly simpler than in the “handmade” version, and the compartmentalization of tasks makes it easier to <a id="_idIndexMarker541"/>optimize the way in which memory is managed.</p>
<p>And… we’re done. Whew! That was quite the ride, once more!</p>
<h1 id="_idParaDest-148"><a id="_idTextAnchor152"/>Summary</h1>
<p>This chapter explored various ways in which one can use the C++ memory management facilities in unusual ways: mapping objects onto memory-mapped hardware, integrating basic forms of error handling with the <code>nothrow</code> version of <code>operator new()</code>, reacting to out-of-memory situations with a <code>std::exception_handler</code>, and accessing atypical memory with non-portable services through a specialization of the “normal” allocation operator and a manager object. This gives us a broader overview of memory management facilities in C++ and how one can use them to one’s advantage.</p>
<p>One thing we have mentioned but not yet discussed is optimization: how to make memory allocation and memory allocation fast, blazingly fast even, and deterministic in terms of execution speed when some conditions are met. This is what we will do in <a href="B21071_10.xhtml#_idTextAnchor153"><em class="italic">Chapter 10</em></a> when explaining how to write arena-based allocation code.</p>
<p>Oh, and as a bonus, we will kill Orcs.</p>
<p class="callout-heading">Orcs? What are you talking about?</p>
<p class="callout">Orcs are fictional creatures found in numerous works of fictional fantasy, usually mean beasts used as foes and that have an unhealthy relation to Elves, another kind of fictional creature that often has a better reputation. As your friendly author has worked a lot with game programmers over the last few decades, Orcs tend to appear in his examples and will be central to the code we write in <a href="B21071_10.xhtml#_idTextAnchor153"><em class="italic">Chapter 10</em></a>.</p>
<p>Sounds good? Then, on to the next chapter!</p>
</div>
</body></html>