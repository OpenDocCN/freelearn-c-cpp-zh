<html><head></head><body><div><div><div><h1 class="chapterNumber">2</h1>
    <h1 id="_idParaDest-61" class="chapterTitle">Moving Animation Calculations from CPU to GPU</h1>
    <p class="normal">Welcome to <a href=""><em class="italic">Chapter 2</em></a>! In the previous chapter, we explored the steps to load and animate a 3D model by using Open Assimp Import Library, or Assimp for short. The resulting application can render a large number of model instances. But, depending on your processor type and speed, the computational part for the model matrices becomes dominant quite fast. As a consequence, we are no longer able to reach 60 frames per second in the application.</p>
    <p class="normal">In this chapter, we move the matrix calculations to compute shaders, running entirely on the GPU. We start with a short history of methods to do computations that are independent of the main code of the application, and the growth of parallelism in CPUs and GPUs. Next, we examine the current state of the matrix calculations. Then, we create a plan for what we should move to a compute shader, and how this relocation could be accomplished. As the last step, we check the results of the relocation and take a short look at which other parts of the application could possibly take advantage of offloading compute-intense work.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">What are compute shaders and why should we love them?</li>
      <li class="bulletList">Profiling animation performance</li>
      <li class="bulletList">Moving the node computations to the GPU</li>
      <li class="bulletList">Testing the implementation by scaling up</li>
      <li class="bulletList">How to debug a compute shader</li>
    </ul>
    <h1 id="_idParaDest-62" class="heading-1">Technical requirements</h1>
    <p class="normal">To use compute shaders, a GPU supporting at least OpenGL 4.3 and/or Vulkan 1.0 is required. Since the source code for the book is written for OpenGL 4.6 and Vulkan 1.1, we are safe here.</p>
    <p class="normal">You can find the example code in the folder <code class="inlineCode">chapter02</code>, subfolders <code class="inlineCode">01_opengl_computeshader</code> for OpenGL, and <code class="inlineCode">02_vulkan_computeshader</code> for Vulkan.</p>
    <h1 id="_idParaDest-63" class="heading-1">What are compute shaders and why should we love them?</h1>
    <p class="normal">Let’s take a short look at<a id="_idIndexMarker062"/> the history of home computers to see how concurrency was handled. On servers, concurrent programs have been the norm since the mid-1960s but for home computers and game consoles, the evolution is a bit different.</p>
    <h2 id="_idParaDest-64" class="heading-2">The famous raster interrupt</h2>
    <p class="normal">While the general idea of interrupts has <a id="_idIndexMarker063"/>existed in computer systems since the beginning of computers, interrupts in home computers were normally used by the operating system to react to external events (though the first machines with interrupts were introduced in the 1950s). One of these interrupts signaled the beginning of a new picture to output to old “cathode-ray tube” TV sets: the raster interrupt.</p>
    <p class="normal">The raster interrupt fired after the cathode ray was reset to the top left of the TV set. This steady event, occurring 50 times per second (in the EU; 60 times per second in the US), became a point of interest for programmers really quickly. By redirecting the interrupt handler to their own code, the machine could do work that needed to be done to a fixed time schedule, like playing music or graphic changes that should happen at a specific location on the screen. These programs embraced the capabilities of home computers even more than the architects of the machines could have imagined, like adding more sprites to the screen than the machine had available, drawing sprites inside of screen borders, raster bars, or even a simple form of multitasking on 8-bit CPUs.</p>
    <p class="normal">Up to this day, retro coders do even more magic with old home computers. See the <em class="italic">Additional resources</em> section for links to demos plus tutorials on how the limitations of hardware were embraced over time.</p>
    <p class="normal">Then, for a long time, nothing special happened. The era of 8- and 16-bit home computers ended, and x86 machines took over. However, the general system layout stayed the same – one processor core using time-sharing via interrupts to present the illusion of having multiple programs running at the same time.</p>
    <h2 id="_idParaDest-65" class="heading-2">The rise of multi-core machines</h2>
    <p class="normal">By the start of the year 2000, common desktop machines became capable of working with multiple CPU cores: Windows 2000 <a id="_idIndexMarker064"/>was introduced (Linux was able to utilize more than one CPU for a long time, but it was a niche system on desktops in 2000).</p>
    <p class="normal">Five years later, the first processors with more than one computational core were available for desktop users: Pentium D and AMD 64 X2. These new CPUs were seen as the start of a new era in programming, since more than one process could run at the same time. That was also the start of an era of headaches for programmers – two threads could really run in parallel, requiring new thinking about synchronization.</p>
    <p class="normal">Right now, the average CPU core count of a desktop machine is between 4 and 8. Taking into account the simultaneous multithreading of modern CPUs, many desktop machines can even handle between 28 and 32 threads in parallel. Sadly, the headaches for programmers are the same as 20 years ago – utilizing a large number of cores is still a complex and error-prone process.</p>
    <p class="normal">Behind the scenes, another technology<a id="_idIndexMarker065"/> with an even more massive number of cores evolved: graphics processors.</p>
    <h2 id="_idParaDest-66" class="heading-2">Hidden multi-core champions</h2>
    <p class="normal">In the shadows of processor core upgrades, graphics cards also raised the number of parallel cores. They did this on even larger scales. Starting <a id="_idIndexMarker066"/>with only a couple of shader cores in 2009 and 2010, the growth in numbers is insane:</p>
    <p class="normal">A NVIDIA GeForce RTX 4090 has a whopping 16,384 shader cores, and an AMD Radeon RX 7900 XTX has 6,144 shader cores.</p>
    <p class="normal">These two numbers can’t be compared directly due to internal differences between these two GPUs, but the raw numbers show one thing: If we were able to use some of the shader cores to calculate our model matrices for the animation frames, the computation would be a lot faster. At the same time, our CPU would have less work to do, enabling us to do other tasks while the GPU calculates the model matrices.</p>
    <p class="normal">Thanks to graphics API designers and GPU vendors, using these shader cores is as easy as writing a small, C-like program: a compute shader.</p>
    <h2 id="_idParaDest-67" class="heading-2">Welcome to the wonderful world of compute shaders</h2>
    <p class="normal">Up to OpenGL 4.2, doing computations on the GPU was already possible by utilizing the other shader types, like vertex and fragment shaders. Similar to uploading arbitrary data to the GPU via texture buffer objects, shaders could be used to do massive parallel computations, saving the results into a texture buffer. The<a id="_idIndexMarker067"/> final texture could be read back to the CPU-accessible memory – et voila: the GPU helped us do some expensive calculations.</p>
    <p class="normal">With the introduction of OpenGL 4.3, this <a id="_idIndexMarker068"/>process was simplified by officially adding compute shaders and <strong class="keyWord">shader storage buffer objects</strong> (<strong class="keyWord">SSBOs</strong>). In Vulkan 1.0, the support for compute shaders and SSBOs was already mandatory, bringing the new graphics API to par with OpenGL 4.3+.</p>
    <p class="normal">The advantages of SSBOs are great: shaders can read and write to an SSBO, unlike read-only uniform buffers. The general access to an SSBO is simplified too, as it has no hard-limited maximum size. Combined with slightly different padding for <code class="inlineCode">float</code> and <code class="inlineCode">vec2</code> data types, getting or setting a value in an SSBO is simple, like using a C-style array:</p>
    <pre class="programlisting code"><code class="hljs-code">layout (std430, binding = 0) readonly buffer Matrices {
  mat4 matrix[];
};
...
void main() {
  ...
  mat4 boneMat = matrix[index];
  ...
}
</code></pre>
    <p class="normal">On the other hand, with compute shaders, you get full control of the number of shader instances you want to start. The overall number of shader invocations depends on the setting in the compute shader and the dispatch call.</p>
    <p class="normal">Suppose we use the following<a id="_idIndexMarker069"/> compute shader settings:</p>
    <pre class="programlisting code"><code class="hljs-code">layout(local_size_x = 16, local_size_y = 32,
  local_size_z = 1) in;
</code></pre>
    <p class="normal">Then, run this OpenGL dispatch call:</p>
    <pre class="programlisting code"><code class="hljs-code">glDispatchCompute(10, 10, 1);
</code></pre>
    <p class="normal">It means we will send a request to start 51,200 instances of the shader to the GPU driver:</p>
    <pre class="programlisting code"><code class="hljs-code">16*32*1*10*10*1 = 51200
</code></pre>
    <p class="normal">For more details about compute shaders, links to tutorials for OpenGL and Vulkan are available in the <em class="italic">Additional resources</em> section.</p>
    <p class="normal">While there are some additional limitations, like the number of shader cores used together for the sake of simplified internal management (called a wave on AMD GPUs, and a warp on NVIDIA GPUs), the number of invocations shows the user-friendly usage of compute shaders.</p>
    <p class="normal">You, the programmer, don’t need to care about spawning a massive number of threads in the code or joining them at the end of the program. Also, there is no need to create mutexes, or atomic variables, to control access to the data. All these steps are firmly hidden from your eyes in the depths of the graphics driver.</p>
    <p class="normal">Though you are not free of obligations – you still have to make sure only a single shader invocation reads or writes a single buffer address. But, with the help of the control variables set by the GPU, like the global and local invocation IDs, this part is also easy – a lot easier compared to the efforts needed for manual multi-threading on the CPU.</p>
    <p class="normal">So, how do we use the magic of the compute shaders in our program? The first step here is to analyze the hotspots in the code and to create a plan for how the same data could be computed on the GPU.</p>
    <h1 id="_idParaDest-68" class="heading-1">Profiling animation performance</h1>
    <p class="normal">To test the performance of the application on your system, you can import the test model named <code class="inlineCode">Woman.gltf</code> in the <code class="inlineCode">woman</code> subfolder<a id="_idIndexMarker070"/> of the <code class="inlineCode">assets</code> folder, move the slider next to the <strong class="screenText">Create Multiple Instances</strong> button to 100, and click the button <strong class="screenText">Create Multiple Instances</strong> several times. Every click will add another 100 instances of the model, distributed randomly across the virtual world.</p>
    <p class="normal">Or, you can change the code for the instance slider in the <code class="inlineCode">createFrame()</code> method of the <code class="inlineCode">UserInterface</code> class in the <code class="inlineCode">opengl</code> folder. Adjust the fourth parameter of the call, controlling the maximum value of the slider:</p>
    <pre class="programlisting code"><code class="hljs-code">ImGui::SliderInt("##MassInstanceCreation",
  &amp;manyInstanceCreateNum, 1, <strong class="hljs-number-slc">100</strong>, "%d", flags);
</code></pre>
    <p class="normal">After you add a couple of hundreds of instances, you should see a picture similar to <em class="italic">Figure 2.1</em>. The <strong class="screenText">Timers</strong> section of the user interface has been zoomed into to show the values for the time it takes to generate the model matrices:</p>
    <figure class="mediaobject"><img src="img/figure_2_01.png" alt="" width="1280" height="749"/></figure>
    <p class="packt_figref">Figure 2.1: Model matrix generation time with 1,601 instances on the screen</p>
    <p class="normal">Here, the 1,601 instances require more than 20 milliseconds to create the model matrices – which is still a small value, if we <a id="_idIndexMarker071"/>calculate the raw numbers.</p>
    <p class="normal">Each model has 41 animated bones. For each of the bones, two values for each of the <strong class="keyWord">translation, rotation, and scale</strong> (<strong class="keyWord">TRS</strong>) are<a id="_idIndexMarker072"/> read in every frame. These values are mixed together by<a id="_idIndexMarker073"/> linear interpolation for translation and scale, and <strong class="keyWord">spherical linear interpolation</strong> (<strong class="keyWord">SLERP</strong>) for rotation:</p>
    <pre class="programlisting code"><code class="hljs-code">1601*41*3*2 = 393846
</code></pre>
    <p class="normal">On top of these nearly 400,000 vector multiplications, every bone needs the resulting TRS matrix created, multiplied by the parent matrix. Every matrix multiplication consists of 16 float multiplications, so we have another ~100,000 multiplications:</p>
    <pre class="programlisting code"><code class="hljs-code">1601*4*16 = 102464
</code></pre>
    <p class="normal">That’s quite a large amount of work to be done for the CPU in every single frame. These numbers are also reflected in the profiling outputs for Windows and Linux.</p>
    <p class="normal">Let’s verify the assumption about<a id="_idIndexMarker074"/> the workload of the CPU.</p>
    <h2 id="_idParaDest-69" class="heading-2">Locating the hotspots in the code</h2>
    <p class="normal">By using the built-in profiler<a id="_idIndexMarker075"/> of Visual Studio 2022, we see the function calls for the animations and the matrix multiplications among the functions with the most execution time spent inside a single function:</p>
    <figure class="mediaobject"><img src="img/figure_2_02.png" alt="" width="1580" height="389"/></figure>
    <p class="packt_figref">Figure 2.2: Animation calls in Visual Studio 2022 profiling</p>
    <p class="normal">After compiling the executable on Linux with the extra flag <code class="inlineCode">-pg</code>, running the application, and starting <code class="inlineCode">gprof</code>, the result is similar:</p>
    <figure class="mediaobject"><img src="img/figure_2_03.png" alt="" width="1613" height="358"/></figure>
    <p class="packt_figref">Figure 2.3: Animation calls in Linux profiling</p>
    <p class="normal">The vast amount of CPU time is needed to calculate the new translation, rotation, scaling, and model matrices for every node. So, let’s check how to change the data representation to allow a simple upload to a compute shader.</p>
    <h2 id="_idParaDest-70" class="heading-2">Analyzing the current data representation</h2>
    <p class="normal">In the current implementation, the<a id="_idIndexMarker076"/> matrix work is done in the <code class="inlineCode">updateAnimation()</code> method of the <code class="inlineCode">AssimpInstance</code> class. For every frame the renderer draws to the screen, the following steps must be done:</p>
    <ol>
      <li class="numberedList" value="1">First, we loop over all animation channels, getting the corresponding node of the model and updating the translation, scaling, and rotation of every node with the bone-local transforms from the animation data:
        <pre class="programlisting code-one"><code class="hljs-code">  for (const auto&amp; channel : animChannels) {
    std::string nodeNameToAnimate =
      channel-&gt;getTargetNodeName();
    std::shared_ptr&lt;AssimpNode&gt; node =
      mAssimpModel-&gt;getNodeMap().at(nodeNameToAnimate);
    node-&gt;setRotation(
      channel-&gt;getRotation(
      mInstanceSettings.isAnimPlayTimePos));
    node-&gt;setScaling(
      channel-&gt;getScaling(
      mInstanceSettings.isAnimPlayTimePos));
    node-&gt;setTranslation(
      channel-&gt;getTranslation(
      mInstanceSettings.isAnimPlayTimePos));
  }
</code></pre>
      </li>
      <li class="numberedList">Then, we iterate over all bones, and update the TRS matrix of every node, calculating the node-local transforms:
        <pre class="programlisting code-one"><code class="hljs-code">  mBoneMatrices.clear();
  for (auto&amp; bone : mAssimpModel-&gt;getBoneList()) {
    std::string nodeName = bone-&gt;getBoneName();
    std::shared_ptr&lt;AssimpNode&gt; node =
       mAssimpModel-&gt;getNodeMap().at(nodeName);
    node-&gt;updateTRSMatrix();
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The TRS matrix update of a node includes the multiplication by the parent node TRS matrix.</p>
    <ol>
      <li class="numberedList" value="3">At this point, we can collect the<a id="_idIndexMarker077"/> final TRS matrix for the nodes, and multiply it by the corresponding bone offset node, generating the <code class="inlineCode">mBoneMatrices</code> vector containing the world position for every node:
        <pre class="programlisting code-one"><code class="hljs-code">    if (mAssimpModel-&gt;getBoneOffsetMatrices().count(
      nodeName) &gt; 0) {
      mBoneMatrices.emplace_back(
        mAssimpModel-&gt;getNodeMap().at(
        nodeName)-&gt;getTRSMatrix() *
      mAssimpModel-&gt;getBoneOffsetMatrices().at(nodeName));
    }
  }
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The extra <code class="inlineCode">.count()</code> check for the bone offset matrices is done to avoid accessing an invalid matrix. The bone offset matrix should be valid for every node that is part of the animation, but it’s better to be safe than sorry.</p>
    <ol>
      <li class="numberedList" value="4">Then, in the <code class="inlineCode">draw()</code> call of our renderer, i.e., in the <code class="inlineCode">OGLRenderer</code> class, the animation is updated for every instance. After the animation update, the <code class="inlineCode">mBoneMatrices</code> vector is<a id="_idIndexMarker078"/> retrieved and added to a local <code class="inlineCode">mBoneMatrices</code> vector:
        <pre class="programlisting code-one"><code class="hljs-code">      for (unsigned int i = 0; i &lt; numberOfInstances; ++i) {
        modelType.second.at(i)-&gt;updateAnimation(
          deltaTime);
        std::vector&lt;glm::mat4&gt; instanceBoneMatrices =
          modelType.second.at(i)-&gt;getBoneMatrices();
        mModelBoneMatrices.insert(
          mModelBoneMatrices.end(),
          instanceBoneMatrices.begin(),
          instanceBoneMatrices.end());
      }
</code></pre>
      </li>
      <li class="numberedList">As the next step, the local <code class="inlineCode">mBoneMatrices</code> vector will be uploaded into the SSBO buffer:
        <pre class="programlisting code-one"><code class="hljs-code">        mShaderBoneMatrixBuffer.uploadSsboData(
          mModelBoneMatrices, 1);
</code></pre>
      </li>
    </ol>
    <p class="normal-one">In the <code class="inlineCode">assimp_skinning.vert</code> vertex shader in the <code class="inlineCode">shader</code> folder, the bone matrices are visible as the <code class="inlineCode">readonly</code> buffer:</p>
    <pre class="programlisting code-one"><code class="hljs-code">layout (std430, binding = 1) readonly buffer BoneMatrices {
  mat4 boneMat[];
};
</code></pre>
    <ol>
      <li class="numberedList" value="6">We use the values from the bone number of every vertex as an index into the bone matrices SSBO to calculate the final vertex skinning matrix named <code class="inlineCode">skinMat</code>:
        <pre class="programlisting code-one"><code class="hljs-code">  mat4 skinMat =
    aBoneWeight.x * boneMat[int(aBoneNum.x) +
      gl_InstanceID * aModelStride] +
    aBoneWeight.y * boneMat[int(aBoneNum.y) +
      gl_InstanceID * aModelStride] +
    aBoneWeight.z * boneMat[int(aBoneNum.z) +
      gl_InstanceID * aModelStride] +
    aBoneWeight.w * boneMat[int(aBoneNum.w) +
      gl_InstanceID * aModelStride];
</code></pre>
      </li>
      <li class="numberedList">As the last step, we use the <code class="inlineCode">skinMat</code> matrix to move the vertex to the correct position for the specific animation frame:
        <pre class="programlisting code-one"><code class="hljs-code">  gl_Position = projection * view * skinMat *
    vec4(aPos, 1.0);
</code></pre>
      </li>
    </ol>
    <p class="normal">As you can see, there are a lot of calculations needed for every single frame of the animation we render. Let’s transfer the<a id="_idIndexMarker079"/> computational load to the graphics card.</p>
    <h2 id="_idParaDest-71" class="heading-2">Adjusting the data model</h2>
    <p class="normal">To move the calculations to the <a id="_idIndexMarker080"/>GPU, we create a new struct called <code class="inlineCode">NodeTransformData</code> in the file <code class="inlineCode">OGLRenderData.h</code> in the <code class="inlineCode">opengl</code> folder:</p>
    <pre class="programlisting code"><code class="hljs-code">struct NodeTransformData {
  glm::vec4 translation = glm::vec4(0.0f);
  glm::vec4 scale = glm::vec4(1.0f);
  glm::vec4 rotation = glm::vec4(1.0f, 0.0f, 0.0f, 0.0f);
}
</code></pre>
    <p class="normal">For the Vulkan renderer, the struct needs to be created in the file <code class="inlineCode">VkRenderData.h</code> in the <code class="inlineCode">vulkan</code> folder.</p>
    <p class="normal">In this new <code class="inlineCode">struct</code>, we will save the transformation values on a per-node basis. We are using a <code class="inlineCode">glm::vec4</code>, that’s a vector type with four <code class="inlineCode">float</code> elements for translation and scaling to avoid additional padding values for proper alignment and simply ignoring the last element in the shader.</p>
    <div><p class="normal">GPU/CPU memory alignment may differ</p>
      <p class="normal">Since GPUs are optimized for fast memory access, data in the buffers must be aligned in memory, in most cases to multiples of 16 bytes. This alignment will be automatically created when uploading data to the GPU. On the CPU side, a different alignment may be used, for instance for 3-element vector types like a <code class="inlineCode">glm::vec3</code>, which is 12 bytes long. To use a <code class="inlineCode">glm::vec3</code> vector, an additional <code class="inlineCode">float</code> is needed as padding to match the 16-byte alignment because uploading misaligned data will end up in distorted images and incorrect results.</p>
    </div>
    <p class="normal">We also use a <code class="inlineCode">glm::vec4</code> vector for the rotation, which is a <code class="inlineCode">glm::quat</code> quaternion in the <code class="inlineCode">AssimpChannel</code> class. The reason for this decision is simple: <strong class="keyWord">GLSL</strong>, the <strong class="keyWord">OpenGL Shading Language</strong>, does not know what a quaternion is, or<a id="_idIndexMarker081"/> how to handle a quaternion. We will have to implement the quaternion functions by ourselves in the compute shader. So, we utilize the normal 4-element vector to transport the four elements of the rotation quaternion to the shader.</p>
    <p class="normal">Now, we can simplify the animation update. First, we add a local <code class="inlineCode">std::vector</code> of our new type <code class="inlineCode">NodeTransformData</code> to the class:</p>
    <pre class="programlisting code"><code class="hljs-code">    std::vector&lt;NodeTransformData&gt; mNodeTransformData{};
</code></pre>
    <p class="normal">We iterate again over all channels, but instead of modifying the nodes of the model, we fill a local <code class="inlineCode">NodeTransformData</code> variable with the transformation data:</p>
    <pre class="programlisting code"><code class="hljs-code">  for (const auto&amp; channel : animChannels) {
    NodeTransformData nodeTransform;
    nodeTransform.translation =
      channel-&gt;getTranslation(
      mInstanceSettings.isAnimPlayTimePos);
    nodeTransform.rotation =
      channel-&gt;getRotation(
      mInstanceSettings.isAnimPlayTimePos);
    nodeTransform.scale =
      channel-&gt;getScaling(
      mInstanceSettings.isAnimPlayTimePos);
</code></pre>
    <p class="normal">And, after a check to avoid <a id="_idIndexMarker082"/>accessing an invalid bone, we set the node transform of the corresponding bone with the collected transformation data:</p>
    <pre class="programlisting code"><code class="hljs-code">    int boneId = channel-&gt;getBoneId();
    if (boneId &gt;= 0) {
      mNodeTransformData.at(boneId) = nodeTransform;
    }
  }
</code></pre>
    <p class="normal">During the <code class="inlineCode">draw()</code> call of our renderer, we still need to update the animations in the same way:</p>
    <pre class="programlisting code"><code class="hljs-code">  for (unsigned int i = 0; i &lt; numberOfInstances; ++i) {
    modelType.second.at(i)-&gt;updateAnimation(deltaTime);
</code></pre>
    <p class="normal">Then, we get the node transformation from the instance, and collect them in a local array:</p>
    <pre class="programlisting code"><code class="hljs-code">    std::vector&lt;NodeTransformData&gt; instanceNodeTransform =
      modelType.second.at(i)-&gt;getNodeTransformData();
    std::copy(instanceNodeTransform.begin(),
      instanceNodeTransform.end(),
      mNodeTransFormData.begin() + i * numberOfBones);
  }
</code></pre>
    <p class="normal">As the last step, we must upload the node transforms to an SSBO:</p>
    <pre class="programlisting code"><code class="hljs-code">mNodeTransformBuffer.uploadSsboData(mNodeTransFormData, 0);
</code></pre>
    <p class="normal">The elements of the <code class="inlineCode">NodeTransformData</code> struct are not 4x4 matrices, but only the three <code class="inlineCode">glm::vec4</code> elements per node. So, we need to upload 25% less data to the SSBO in this step.</p>
    <p class="normal">Having the node transformations available on the GPU is a cool first step. But, if we further analyze the data flow, we will find out we need much more data in our compute shaders to calculate the final model <a id="_idIndexMarker083"/>matrices. Let’s see what else is required to calculate the world space positions from the bone-local transform data.</p>
    <h2 id="_idParaDest-72" class="heading-2">Adding missing data for the compute shader</h2>
    <p class="normal">The first, and most obvious missing data part is the array of bone offset matrices. In the CPU implementation, we <a id="_idIndexMarker084"/>multiply the final TRS matrix per node with the bone offset matrix for the same node:</p>
    <pre class="programlisting code"><code class="hljs-code">mBoneMatrices.emplace_back(
    mAssimpModel-&gt;getNodeMap().at(
    nodeName)-&gt;getTRSMatrix() *
    <strong class="hljs-slc">mAssimpModel-&gt;</strong><strong class="hljs-built_in-slc">getBoneOffsetMatrices</strong><strong class="hljs-slc">().</strong><strong class="hljs-built_in-slc">at</strong><strong class="hljs-slc">(nodeName)</strong>);
</code></pre>
    <p class="normal">Since the bone offset matrices are on a per-model base, we can add an SSBO to our <code class="inlineCode">AssimpModel</code> class and upload the data during the model loading. We can simply add an SSBO to the <code class="inlineCode">AssimpModel.h</code> header file in the <code class="inlineCode">model</code> folder:</p>
    <pre class="programlisting code"><code class="hljs-code">    ShaderStorageBuffer mShaderBoneMatrixOffsetBuffer{};
</code></pre>
    <p class="normal">Then, in the <code class="inlineCode">loadModel()</code> method, we fill a local vector with the offset matrices and upload the data to the SSBO:</p>
    <pre class="programlisting code"><code class="hljs-code">  std::vector&lt;glm::mat4&gt; boneOffsetMatricesList{};
  for (const auto&amp; bone : mBoneList) {
    boneOffsetMatricesList.emplace_back(
      bone-&gt;getOffsetMatrix());
  }
  mShaderBoneMatrixOffsetBuffer.uploadSsboData(
    boneOffsetMatricesList);
</code></pre>
    <p class="normal">After we prepare the data for our compute shader, we bind the SSBO containing the bone offset matrices to the same binding point we configured in the matrix multiplication compute shader (<code class="inlineCode">binding = 2</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">    modelType.second.at(0)-&gt;getModel()
      -&gt;bindBoneMatrixOffsetBuffer(2);
</code></pre>
    <p class="normal">A bit more hidden at first glance is the need for parent matrices. In the method <code class="inlineCode">updateTRSMatrix()</code> of <code class="inlineCode">AssimpNode</code>, we retrieve the TRS matrix from the parent node (if we have a parent node). Then, we use the parent node to calculate the TRS matrix of the node itself:</p>
    <pre class="programlisting code"><code class="hljs-code">  if (std::shared_ptr&lt;AssimpNode&gt; parentNode =
      mParentNode.lock()) {
    mParentNodeMatrix = parentNode-&gt;getTRSMatrix();
  }
  mLocalTRSMatrix = mRootTransformMatrix *
    mParentNodeMatrix * mTranslationMatrix *
    mRotationMatrix * mScalingMatrix;
</code></pre>
    <p class="normal">In the <code class="inlineCode">updateAnimation()</code> method of the <code class="inlineCode">AssimpInstance</code> class, we start with the update of the TRS matrix of the root node and descend into the child nodes, collecting the parent matrix node, which contains all transformation matrices up to the model root node.</p>
    <p class="normal">For the compute shader, we need a different approach. Since all shader invocations run in parallel, we would need to cut down<a id="_idIndexMarker085"/> the number of invocations to one per model, allowing the known linear progression on the model matrices. To use a larger amount of shader invocations, we will create an <code class="inlineCode">int</code> vector that contains the number of the parent node at each position. This “parent node vector” enables us to “walk” backward on the model skeleton in the shader, collecting all parent node matrices on the way.</p>
    <p class="normal">We create the parent node vector in the loop with the bone offset matrices. First, we get the parent node of our current bone, then use a small lambda to get the position of the parent bone in the same bone list:</p>
    <pre class="programlisting code"><code class="hljs-code">    std::string parentNodeName = mNodeMap.at(
      bone-&gt;getBoneName())-&gt;getParentNodeName();
    const auto boneIter = std::find_if(mBoneList.begin(),
      mBoneList.end(),
      [parentNodeName](std::shared_ptr&lt;AssimpBone&gt;&amp; bone)
       { return bone-&gt;getBoneName() == parentNodeName; });
</code></pre>
    <p class="normal">If we don’t find a parent node in the bone list, we have found the root node of the model. In this case, we add a <code class="inlineCode">-1</code> to identify the root node. In all other cases, we add the index number of the parent bone:</p>
    <pre class="programlisting code"><code class="hljs-code">    if (boneIter == mBoneList.end()) {
      boneParentIndexList.emplace_back(-1);
    } else {
      boneParentIndexList.emplace_back(
        std::distance(mBoneList.begin(), boneIter));
    }
</code></pre>
    <p class="normal">The <code class="inlineCode">boneParentIndexList</code> now contains a flat list of the parent nodes for all the nodes in the model, with the special parent <code class="inlineCode">-1</code> for the root node. By a repeated lookup of the parent node, we can ascend the skeleton tree from every node, until we reach the root node with the special number <code class="inlineCode">-1</code> as parent.</p>
    <p class="normal">To make the parent bone list available in the compute shader, we create another SSBO in the <code class="inlineCode">AssimpModel</code> class, and upload the <code class="inlineCode">boneParentIndexList</code> to the GPU:</p>
    <pre class="programlisting code"><code class="hljs-code">mShaderBoneParentBuffer.uploadSsboData(boneParentIndexList);
</code></pre>
    <p class="normal">Back in the renderer, the parent bone buffer will be bound to a binding point of our compute shader:</p>
    <pre class="programlisting code"><code class="hljs-code">      modelType.second.at(0)-&gt;getModel()
        -&gt;bindBoneParentBuffer(1);
</code></pre>
    <p class="normal">We haven’t finished the workload<a id="_idIndexMarker086"/> transformation to the GPU yet. Some data needs to be handled in a different way when using a compute shader.</p>
    <h2 id="_idParaDest-73" class="heading-2">Relocating data to another shader</h2>
    <p class="normal">Also missing from the <a id="_idIndexMarker087"/>calculations now is the instance world position. The <code class="inlineCode">updateAnimation()</code> method contains the following line to set the transformation matrix for the root node of the model:</p>
    <pre class="programlisting code"><code class="hljs-code">  mAssimpModel-&gt;getNodeMap().at(
    mAssimpModel-&gt;getBoneList().at(0)-&gt;getBoneName())
    -&gt;setRootTransformMatrix(mLocalTransformMatrix *
    mAssimpModel-&gt;getRootTranformationMatrix());
</code></pre>
    <p class="normal">The root transformation matrix of the model contains general transformations that will be applied to the entire model, like a global scaling of the model. The other matrix, <code class="inlineCode">mLocalTransformMatrix</code>, is used to set the user-controlled parameters of the model instance. The local transformation matrix allows us to rotate and move the model instance in the virtual world.</p>
    <p class="normal">In contrast to the bone offset matrices, the root node transformation will be moved to the <code class="inlineCode">assimp_skinning.vert</code> vertex shader, not to a compute shader. It does not matter which of the two shaders does the matrix multiplication, but moving the root node transformation to the vertex shader may lower the load of the computer shaders a bit. Also, the vertex shader only runs for objects that are drawn to the screen, not for instances that are culled before the rendering itself, or invisible instances, potentially lowering the overall computational load of the GPU.</p>
    <h2 id="_idParaDest-74" class="heading-2">Doing the last preparations</h2>
    <p class="normal">And, at last, we can also decide how many distinct computer shaders we need:</p>
    <p class="normal"><em class="italic">We need – at least – two compute shaders</em>.</p>
    <p class="normal">To calculate the final TRS matrix <a id="_idIndexMarker088"/>for a node, we need to have all parent TRS matrices completed, with all matrices multiplied from the current node up to the model root. Since we can only control the amount of shader invocations we start, but not when or how long such a shader invocation runs, we need to set some sort of barrier between the calculation of the node TRS matrices, and the process of collecting the matrices along the skeleton.</p>
    <p class="normal">The only way to create such a barrier is on the CPU side. A barrier will be added while submitting the compute shader to the graphics API, telling the GPU to wait for the first shaders to finish, before it starts the second batch.</p>
    <p class="normal">So, we will have to start with the node transforms, wait until all node transform matrices are finished, and then start the<a id="_idIndexMarker089"/> calculation of the final node matrices.</p>
    <p class="normal">After the theoretical part is done, we can start the shader-related implementation.</p>
    <h1 id="_idParaDest-75" class="heading-1">Moving the node computations to the GPU</h1>
    <p class="normal">The process of loading a<a id="_idIndexMarker090"/> compute shader differs only slightly from a vertex or fragment shader. For OpenGL, we have to set the shader type in the <code class="inlineCode">glCreateShader()</code> call:</p>
    <pre class="programlisting code"><code class="hljs-code">glCreateShader(<strong class="hljs-slc">GL_COMPUTE_SHADER</strong>);
</code></pre>
    <p class="normal">For Vulkan, we must set the correct shader stage during the creation of the <code class="inlineCode">VkShaderModule</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">VkPipelineShaderStageCreateInfo computeShaderStageInfo{};
computeShaderStageInfo.sType =
  VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO; computeShaderStageInfo.stage =
  <strong class="hljs-slc">VK_SHADER_STAGE_COMPUTE_BIT</strong>;
</code></pre>
    <p class="normal">All the other steps of loading the shader code, linking, or creating the shader module, stay the same. Because we have only a single shader file, additional methods have been added to the <code class="inlineCode">Shader</code> class. Loading a compute shader in OpenGL can now be achieved by calling the <code class="inlineCode">loadComputeShader()</code> method of the <code class="inlineCode">Shader</code> class with the relative file name of the shader source:</p>
    <pre class="programlisting code"><code class="hljs-code">  if (!mAssimpTransformComputeShader.loadComputeShader(
      "shader/assimp_instance_transform.comp")) {
    return false;
  }
</code></pre>
    <p class="normal">Vulkan uses the <strong class="keyWord">Standard Portable Intermediate Representation</strong> (<strong class="keyWord">SPIR-V</strong>) format for shaders. Instead of the shader source, the precompiled<a id="_idIndexMarker091"/> shader code must be loaded into the <code class="inlineCode">Shader</code> class for the Vulkan renderer.</p>
    <p class="normal">As we compute new matrices in the compute shaders, and we have to move these matrices between different shaders, two additional SSBOs are required.</p>
    <h2 id="_idParaDest-76" class="heading-2">Adding more shader storage buffers</h2>
    <p class="normal">The first SSBO will store the<a id="_idIndexMarker092"/> TRS matrices that we create from the node transforms. This SSBO is a simple buffer, defined in the header file for the renderer:</p>
    <pre class="programlisting code"><code class="hljs-code">    ShaderStorageBuffer mShaderTRSMatrixBuffer{};
</code></pre>
    <p class="normal">The second SSBO will contain the final bone matrices that will be used in the skinning vertex shader. The bone matrix buffer is also added as a normal SSBO declaration in the header file of the renderer:</p>
    <pre class="programlisting code"><code class="hljs-code">    ShaderStorageBuffer mShaderBoneMatrixBuffer{};
</code></pre>
    <p class="normal">One important step to use the SSBO in the shaders is the have a correct size set. If the SSBO is too small, not all data will be stored in the compute shader, and instances or body parts of instances may be missing. A wrong buffer size may be hard to debug – you may not even get a warning that the shader writes <a id="_idIndexMarker093"/>beyond the end of the buffer. We must calculate the buffer size according to the number of bones, the number of instances, and the size of the 4x4 matrix, as shown here:</p>
    <pre class="programlisting code"><code class="hljs-code">  size_t trsMatrixSize = numberOfBones *
 numberOfInstances * sizeof(glm::mat4);
</code></pre>
    <p class="normal">Then, we resize the two SSBOs to the final matrix size:</p>
    <pre class="programlisting code"><code class="hljs-code">  mShaderBoneMatrixBuffer.checkForResize(trsMatrixSize);
  mShaderTRSMatrixBuffer.checkForResize(trsMatrixSize);
</code></pre>
    <p class="normal">When drawing multiple models, both buffers will end up with the maximum size of all models. But this does not do any harm, as the buffers will be reused for the next model and filled only up to the real amount of data used in the new model.</p>
    <h2 id="_idParaDest-77" class="heading-2">Calculating the node transforms in a shader</h2>
    <p class="normal">For the first compute shader, we must upload the node transform data to the first compute shader. We bind the SSBO storing<a id="_idIndexMarker094"/> the new TRS matrices created from the node transform to the proper binding point of the compute shader:</p>
    <pre class="programlisting code"><code class="hljs-code">    mAssimpTransformComputeShader.use();
    mNodeTransformBuffer.uploadSsboData(
      mNodeTransFormData, 0);
    mShaderTRSMatrixBuffer.bind(1)
</code></pre>
    <p class="normal">The compute shader itself is named <code class="inlineCode">assimp_instance_transform.comp</code>, located in the <code class="inlineCode">shader</code> folder. The first line of the compute shader is the usual version definition; the second line defines the local invocation sizes:</p>
    <pre class="programlisting code"><code class="hljs-code">#version 460 core
layout(local_size_x = 1, local_size_y = 32,
  local_size_z = 1) in;
</code></pre>
    <p class="normal">Here, we create 32 invocations of the shader by default. You may need to experiment with the local sizes to achieve maximum performance. Shaders are started in groups of fixed sizes to simplify the GPU-internal management. Common values are 32 (called “warps,” for NVIDIA GPUs) or 64 (called “waves,” for AMD GPUs). It’s kind of useless to set all the local sizes to 1 for NVIDIA or AMD GPUs, since the remaining<a id="_idIndexMarker095"/> 31 warps or respective 63 waves will be unused.</p>
    <p class="normal">Next, we must add the <code class="inlineCode">NodeTransformData</code> with the same data types as we used while declaring the type in the <code class="inlineCode">OGLRenderData.h</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">struct NodeTransformData {
  vec4 translation;
  vec4 scale;
  vec4 rotation;
};
</code></pre>
    <p class="normal">As a reminder: The <code class="inlineCode">rotation</code> element is a quaternion, disguised as <code class="inlineCode">vec4</code>.</p>
    <p class="normal">Now, we define the two SSBOs, using the same binding points as in the renderer code:</p>
    <pre class="programlisting code"><code class="hljs-code">layout (std430, binding = 0) readonly restrict
    buffer TransformData {
  NodeTransformData data[];
};
layout (std430, binding = 1) writeonly restrict
    buffer TRSMatrix {
  mat4 trsMat[];
};
</code></pre>
    <p class="normal">We mark the node transform data as <code class="inlineCode">readonly</code>, and the TRS matrices as <code class="inlineCode">writeonly</code>. The two modifiers could help the shader compiler to optimize the access of the buffers, since some operations could be left out. The other modifier, <code class="inlineCode">restrict</code>, also helps the shader compiler to optimize the shader code. By adding <code class="inlineCode">restrict</code>, we tell the shader compiler that we will never read a value with a variable that we wrote before from another variable. Eliminating read-after-write dependencies will make the life of the shader compiler much easier.</p>
    <p class="normal">To read the data from the <code class="inlineCode">TransformData</code> buffer, three methods have been added. Within these three methods, called <code class="inlineCode">getTranslationMatrix()</code>, <code class="inlineCode">getScaleMatrix()</code>, and <code class="inlineCode">getRotationMatrix()</code>, we read the data elements of the buffer and create 4x4 matrices for the corresponding transformation.</p>
    <p class="normal">As an example, see the implementation of the <code class="inlineCode">getTranslationMatrix()</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code">mat4 getTranslationMatrix(uint index) {
  return mat4(1.0, 0.0, 0.0, 0.0,
              0.0, 1.0, 0.0, 0.0,
              0.0, 0.0, 1.0, 0.0,
              data[index].translation[0],
                   data[index].translation[1],
                        data[index].translation[2],
                            1.0);
}
</code></pre>
    <p class="normal">The resulting 4x4 matrix is an identity matrix, enriched by the translation data for the specific <code class="inlineCode">index</code> in the <code class="inlineCode">TransformData</code> buffer. The <code class="inlineCode">getScaleMatrix()</code> method creates a scaling matrix, having the first three elements of the main diagonal set to the scaling values. Finally, the <code class="inlineCode">getRotationMatrix()</code> method resembles the spirit of the <code class="inlineCode">mat3_cast</code> algorithm from GLM, converting a quaternion into a 4x4 rotation matrix.</p>
    <p class="normal">In the <code class="inlineCode">main()</code> method of the first<a id="_idIndexMarker096"/> compute shader, we get the <code class="inlineCode">x</code> and <code class="inlineCode">y</code> dimensions of the shader invocations:</p>
    <pre class="programlisting code"><code class="hljs-code">void main() {
  uint node = gl_GlobalInvocationID.x;
  uint instance = gl_GlobalInvocationID.y;
</code></pre>
    <p class="normal">We will use the number of bones in the model as <code class="inlineCode">x</code> dimension, simplifying the remaining part of the shader code:</p>
    <pre class="programlisting code"><code class="hljs-code">  uint numberOfBones = gl_NumWorkGroups.x;
</code></pre>
    <p class="normal">Locating the correct index in the buffer is done by combining the number of bones, the shader instance (invocation), and the node we will work on:</p>
    <pre class="programlisting code"><code class="hljs-code">  uint index = node + numberOfBones * instance;
</code></pre>
    <p class="normal">The main logic for the compute shader multiplies the translation, rotation, and scaling matrix in the TRS order, and saves the result in the buffer for the TRS matrices, at the same <code class="inlineCode">index</code> of the node transforms:</p>
    <pre class="programlisting code"><code class="hljs-code">  trsMat[index] = getTranslationMatrix(index) *
    getRotationMatrix(index) * getScaleMatrix(index);
}
</code></pre>
    <p class="normal">In GLM, matrices are multiplied from right to left, a fact that may be confusing at first. So, despite the name of the matrix being “TRS,” the multiplications are performed in reverse order of the name: The model scaling is applied first, then the rotation, and then the translation comes last. Other math libraries or different matrix packings may use a different order of multiplication. Two extensive matrix tutorials are listed in the <em class="italic">Additional resources</em> section.</p>
    <p class="normal">Saving the TRS matrix on the same spot as the node transforms retains the order of nodes in a model and the order of nodes in all model instances.</p>
    <p class="normal">To trigger the shader execution, we call <code class="inlineCode">glDispatchCompute()</code> for the OpenGL renderer, adding a memory barrier that waits for the SSBO:</p>
    <pre class="programlisting code"><code class="hljs-code">    glDispatchCompute(numberOfBones,
      std::ceil(numberOfInstances / 32.0f), 1);
    glMemoryBarrier(GL_SHADER_STORAGE_BARRIER_BIT);
</code></pre>
    <p class="normal">The memory barrier makes sure the CPU waits for a specific state of the GPU. In this case, we must wait until all SSBO writes have finished, so we set the bit for the shader storage buffers. The call to <code class="inlineCode">glMemoryBarrier()</code> simply blocks execution, returning only after the GPU has reached the desired state.</p>
    <p class="normal">Before we go on, let’s take a<a id="_idIndexMarker097"/> look at what happens inside the compute shader when <code class="inlineCode">glDispatchCompute()</code> or <code class="inlineCode">vkCmdDispatch()</code> is called. <em class="italic">Figure 2.4</em> shows the internal elements of the compute shader invocations:</p>
    <figure class="mediaobject"><img src="img/figure_2_4.png" alt="" width="1386" height="763"/></figure>
    <p class="packt_figref">Figure 2.4: Global work groups and local invocation structure of a compute shader</p>
    <p class="normal">When we call the dispatch command with the parameters <code class="inlineCode">4,4,2</code>, a total of number of <code class="inlineCode">4*4*2 = 32</code> workgroups will be started, as shown on the left side of <em class="italic">Figure 2.4</em>. The total number of workgroups is simply the product of the three dimensions <code class="inlineCode">X</code>, <code class="inlineCode">Y</code>, and <code class="inlineCode">Z</code> of the global compute space.</p>
    <p class="normal">In each of the 32 workgroups, a total of four shader invocations are running, as seen for the workgroup <code class="inlineCode">[3,0,0]</code> in the middle of <em class="italic">Figure 2.4</em>. The so-called local size is defined by the three shader layout values <code class="inlineCode">local_size_x</code>, <code class="inlineCode">local_size_y</code>, and <code class="inlineCode">local_size_z</code>. The local size of a workgroup is calculated like the number of workgroups, by multiplying the three values for <code class="inlineCode">X</code>, <code class="inlineCode">Y</code>, and <code class="inlineCode">Z</code> dimensions: <code class="inlineCode">2*2*1 = 4</code>.</p>
    <p class="normal">A separation into workgroups is important if the shader instances need to communicate between each other since communication is only possible inside the same workgroup. Shader invocations from different workgroups are effectively isolated and unable to communicate.</p>
    <p class="normal">As you can see, the total<a id="_idIndexMarker098"/> number of shader invocations can become huge quite quickly, since the local size of a single workgroup and the total number of workgroups are multiplied. This massive parallelism is the secret behind the raw power of a GPU.</p>
    <p class="normal">So, for the <code class="inlineCode">x</code> dimension, we use the <code class="inlineCode">numberOfBones</code>, as stated before. By calculating the <code class="inlineCode">std::ceil</code> of the <code class="inlineCode">numberOfInstances</code> divided by 32 as the <code class="inlineCode">y</code> dimension, we make sure to start groups of 32 shader invocations to calculate the matrices for up to 32 instances at once, as configured as the local <code class="inlineCode">y</code> dimension in the shader code. If we have an instance count of less than a multiple of 32, the additional waves or warps are still running, but the results are ignored. Technically, we are reading and<a id="_idIndexMarker099"/> writing outside the buffer bounds, but the GPU driver should handle the situation, i.e., by discarding the writes.</p>
    <p class="normal">For Vulkan, we must call <code class="inlineCode">VkCmdDispatch()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    vkCmdDispatch(commandBuffer, numberOfBones,
      std::ceil(numberOfInstances / 32.0f), 1);
</code></pre>
    <p class="normal">The size of the Shader Storage Buffer Object for the compute shaders in Vulkan should also be rounded to hold a multiple of 32 times the number of bones to avoid accidental overwrites of buffer data:</p>
    <pre class="programlisting code"><code class="hljs-code">boneMatrixBufferSize +=
  numberOfBones * ((numberOfInstances - 1) / 32 + 1) * 32;
</code></pre>
    <p class="normal">Barriers to synchronizing the shaders in Vulkan must be set to wait for the results of the queues. For synchronization between the compute shader and the vertex shader, we need to set the barrier between the writes of the compute shader and the first read operation of the vertex shader like this:</p>
    <pre class="programlisting code"><code class="hljs-code">VkMemoryBarrier memoryBarrier {}
  ...
 memoryBarrier.srcAccessMask = VK_ACCESS_SHADER_WRITE_BIT:
 memoryBarrier.dstAccessMask = VK_ACCESS_SHADER_READ_BIT;
vkCmdPipelineBarrier(...
  VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
  VK_PIPELINE_STAGE_VERTEX_INPUT_BIT,
  1, &amp;memoryBarrier, ...);
</code></pre>
    <p class="normal">Now, Vulkan waits for the compute shader to finish all calculations before starting the draw calls in the vertex shader.</p>
    <p class="normal">The TRS matrix buffer now contains the matrices for every node, but without the parent nodes, the root node transform matrix, or any offset matrices.</p>
    <h2 id="_idParaDest-78" class="heading-2">Creating the final node matrices</h2>
    <p class="normal">Before we can start the next <a id="_idIndexMarker100"/>compute shader, we must bind all buffers that will be used during the shader run. We have a total of four SSBOs:</p>
    <pre class="programlisting code"><code class="hljs-code">        mAssimpMatrixComputeShader.use();
        mShaderTRSMatrixBuffer.bind(0);
        modelType.second.at(0)-&gt;getModel()
          -&gt;bindBoneParentBuffer(1);
        modelType.second.at(0)-&gt;getModel()
          -&gt;bindBoneMatrixOffsetBuffer(2);
        mShaderBoneMatrixBuffer.bind(3);
</code></pre>
    <p class="normal">Since all data already resides on the GPU, we don’t need any kind of upload here.</p>
    <p class="normal">The second compute shader itself is called <code class="inlineCode">assimp_instance_matrix_mult.comp</code> and can be found in the <code class="inlineCode">shader</code> folder. The shader code starts – again – with a version and the local size definitions:</p>
    <pre class="programlisting code"><code class="hljs-code">#version 460 core
layout(local_size_x = 1, local_size_y = 32,
  local_size_z = 1) in;
</code></pre>
    <p class="normal">A local size of 32 is used since the code was developed on a machine with an NVIDIA GPU. For an AMD GPU, you should use a<a id="_idIndexMarker101"/> local size of 64, as explained in the section <em class="italic">Calculating the node transforms in a shader</em>.</p>
    <p class="normal">Similar to the first compute shader, the SSBOs follow:</p>
    <pre class="programlisting code"><code class="hljs-code">layout (std430, binding = 0) readonly restrict
    buffer TRSMatrix {
  mat4 trsMat[];
};
layout (std430, binding = 1) readonly restrict
    buffer ParentMatrixIndices {
  int parentIndex[];
};
layout (std430, binding = 2) readonly restrict
    buffer BoneOffsets {
  mat4 boneOffset[]
};
layout (std430, binding = 3) writeonly restrict
    buffer BoneMatrices {
  mat4 boneMat[];
};
</code></pre>
    <p class="normal">The first buffer, <code class="inlineCode">TRSMatrix</code>, contains the TRS matrices from the first compute shader. In the <code class="inlineCode">ParentMatrixIndices</code> buffer, the shader can find the list containing the parent node for each of the nodes. The bone matrix offsets for every node are made available in the third buffer, <code class="inlineCode">BoneOffsets</code>, and the final node matrices will be stored in the last buffer, <code class="inlineCode">BoneMatrices</code>. The <code class="inlineCode">readonly</code> and <code class="inlineCode">writeonly</code> modifiers are set according to the usage of the buffers.</p>
    <p class="normal">Since we use the same settings as the first compute shader, having virtually the same first lines in the <code class="inlineCode">main()</code> method of the second compute shader should be no surprise:</p>
    <pre class="programlisting code"><code class="hljs-code">void main() {
  uint node = gl_GlobalInvocationID.x;
  uint instance = gl_GlobalInvocationID.y;
  uint numberOfBones = gl_NumWorkGroups.x;
  uint index = node + numberOfBones * instance;
</code></pre>
    <p class="normal">Now, we get the TRS matrix for the bone we will be working on:</p>
    <pre class="programlisting code"><code class="hljs-code">  mat4 nodeMatrix = trsMat[index];
</code></pre>
    <p class="normal">Next, we introduce a variable called <code class="inlineCode">parent</code>, storing the <code class="inlineCode">index</code> of the parent node:</p>
    <pre class="programlisting code"><code class="hljs-code">  uint parent = 0;
</code></pre>
    <p class="normal">We will need the <code class="inlineCode">parent</code> node index <a id="_idIndexMarker102"/>to get the correct parent matrix while we walk the node skeleton up to the root node.</p>
    <p class="normal">As the first step of the skeleton walk, we get the parent node of the node that we are working on:</p>
    <pre class="programlisting code"><code class="hljs-code">  int parentNode = parentIndex[node];
</code></pre>
    <p class="normal">In the following <code class="inlineCode">while</code> loop, we get the parent matrix of the node and multiply both matrices. Then we look up the parent of the parent node, and so on:</p>
    <pre class="programlisting code"><code class="hljs-code">  while (parentNode &gt;= 0) {
    parent = parentNode + numberOfBones * instance;
    nodeMatrix = trsMat[parent] * nodeMatrix;
    parentNode = parentIndex[parentNode];
  }
</code></pre>
    <p class="normal">The preceding lines of code may make you raise your eyebrows, since we apparently break one of the basic rules of GLSL shader code: the size of a loop must be known at compile time. Luckily, this rule does not apply to a <code class="inlineCode">while</code> loop. We are free to alter the loop control variable inside the body of the loop, creating loops of various lengths.</p>
    <p class="normal">However, this code could impact shader performance as GPUs are optimized to execute the same instructions on every thread. You may have to check the shader code on different GPUs to make sure you see the expected speedup.</p>
    <p class="normal">Also be aware that the accidental creation of an infinite loop may end in a locked-up system since the shader code never returns the wave or warp to the pool. It’s a good idea to ensure a valid exit condition for a while loop on the CPU side since a GPU lockup may only be resolved by a forced restart of your computer.</p>
    <p class="normal">As long as we don’t have errors or cycles in the parent node list, we will end at the last block for every node:</p>
    <pre class="programlisting code"><code class="hljs-code">  if (parentNode == -1) {
    nodeMat[index] = nodeMatrix * boneOff[node];
  }
}
</code></pre>
    <p class="normal">Here, we multiply the resulting node matrix, containing all matrices up to the root node, by the bone offset matrix for the node, and store the result in the writable <code class="inlineCode">NodeMatrices</code> buffer.</p>
    <p class="normal">Starting the computation is done in exactly the same way as for the first shader. Run <code class="inlineCode">glDispatchCompute()</code> for OpenGL, followed by a <code class="inlineCode">glMemoryBarrier()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    glDispatchCompute(numberOfBones,
      std::ceil(numberOfInstances / 32.0f), 1);
    glMemoryBarrier(GL_SHADER_STORAGE_BARRIER_BIT);
</code></pre>
    <p class="normal">And, for Vulkan, use <code class="inlineCode">VkCmdDispatch()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    vkCmdDispatch(commandBuffer, numberOfBones,
      std::ceil(numberOfInstances / 32.0f), 1);
</code></pre>
    <p class="normal">At this point, the <code class="inlineCode">NodeMatrices</code> buffer contains the TRS matrices for all nodes, close to the result we had after the <code class="inlineCode">updateAnimation()</code> call in the CPU-based version of the code in <a href="Chapter_1.xhtml"><em class="italic">Chapter 1</em></a> – with the exception of the <a id="_idIndexMarker103"/>model root matrix for the instance.</p>
    <h2 id="_idParaDest-79" class="heading-2">Finalizing the compute relocation</h2>
    <p class="normal">So, let’s add the missing matrix calculation to the vertex skinning shader. First, we collect the matrices containing the world <a id="_idIndexMarker104"/>positions during the loop over all instances of the model:</p>
    <pre class="programlisting code"><code class="hljs-code">    mWorldPosMatrices.resize(numberOfInstances);
    for (unsigned int i = 0; i &lt; numberOfInstances; ++i) {
      ...
      <strong class="hljs-slc">mWorldPosMatrices.</strong><strong class="hljs-built_in-slc">at</strong><strong class="hljs-slc">(i) =</strong>
      <strong class="hljs-slc">  modelType.second.</strong><strong class="hljs-built_in-slc">at</strong><strong class="hljs-slc">(i)-&gt;</strong><strong class="hljs-built_in-slc">getWorldTransformMatrix</strong><strong class="hljs-slc">();</strong>
    }
</code></pre>
    <p class="normal">Then, the world position matrices are uploaded into an SSBO, and bound to the vertex skinning shader:</p>
    <pre class="programlisting code"><code class="hljs-code">        mAssimpSkinningShader.use();
        mAssimpSkinningShader.setUniformValue(
          numberOfBones);
        mShaderBoneMatrixBuffer.bind(1);
        <strong class="hljs-slc">mShaderModelRootMatrixBuffer.</strong><strong class="hljs-built_in-slc">uploadSsboData</strong><strong class="hljs-slc">(</strong>
          <strong class="hljs-slc">mWorldPosMatrices, </strong><strong class="hljs-number-slc">2</strong><strong class="hljs-slc">);</strong>
</code></pre>
    <p class="normal">In the vertex skinning shader itself, the new buffer is introduced:</p>
    <pre class="programlisting code"><code class="hljs-code">layout (std430, binding = 2) readonly restrict
    buffer WorldPosMatrices {
  mat4 worldPos[];
};
</code></pre>
    <p class="normal">Finally, we create a combined matrix from the world position and the vertex skin matrix, and use the new matrix to calculate the position of the vertex and the normal:</p>
    <pre class="programlisting code"><code class="hljs-code">  <strong class="hljs-type-slc">mat4</strong><strong class="hljs-slc"> worldPosSkinMat = worldPos[</strong><strong class="hljs-built_in-slc">gl_InstanceID</strong><strong class="hljs-slc">] * skinMat;</strong>
  gl_Position = projection * view * <strong class="hljs-slc">worldPosSkinMat</strong> *
    vec4(aPos.x, aPos.y, aPos.z, 1.0);
  ...
  normal = transpose(inverse(<strong class="hljs-slc">worldPosSkinMat</strong>)) *
    vec4(aNormal.x, aNormal.y, aNormal.z, 1.0);
</code></pre>
    <p class="normal">Compiling and running the example from <a href=""><em class="italic">Chapter 2</em></a> should result in the same functionality as the example from <a href="Chapter_1.xhtml"><em class="italic">Chapter 1</em></a>. We can load models and create a large number of instances, but we are still able to control the parameters of every single instance of every model. The main difference should be the amount of time it takes to create the transform matrices – we should see a large drop, compared to the CPU-based version, and end up most probably below 10 milliseconds. Depending on your CPU<a id="_idIndexMarker105"/> and GPU types, the speed gain will differ. But in all cases, the GPU shader should be notably faster than pure CPU calculations.</p>
    <p class="normal">Let’s see the speedup we achieved by using compute shaders.</p>
    <h1 id="_idParaDest-80" class="heading-1">Testing the implementation by scaling up</h1>
    <p class="normal">All features and the user interface are<a id="_idIndexMarker106"/> identical to <a href="Chapter_1.xhtml"><em class="italic">Chapter 1</em></a>. But our changes can be made visible by adding more and more instances. If you add the same 1,600 instances as in <em class="italic">Figure 2.1</em>, you will see much smaller matrix generation times. The values may be similar to <em class="italic">Figure 2.5</em>:</p>
    <figure class="mediaobject"><img src="img/figure_2_5.png" alt="" width="1292" height="758"/></figure>
    <p class="packt_figref">Figure 2.5: The compute shader version with 1,600 instances</p>
    <p class="normal">The time for virtually the same matrix operations went down from ~24 milliseconds on the CPU to less than 6 milliseconds by using compute shaders. We won around 18 milliseconds of CPU time in every single frame!</p>
    <p class="normal">Now let us add more models – many models. Let’s say we add a total of 4,000 instances of the example model. The resulting matrix generation times on your machine may be similar to the number in <em class="italic">Figure 2.6</em>:</p>
    <figure class="mediaobject"><img src="img/figure_2_6.png" alt="" width="1292" height="760"/></figure>
    <p class="packt_figref">Figure 2.6: The compute shader version with 4,000 instances</p>
    <p class="normal">Even with 2.5 times the number of instances, the average matrix generation time of the compute shader code is still at about half the time of the CPU version. You may even see a much larger, non-linear performance gain with more powerful GPUs. Recent GPUs not only have several thousands of cores that are working in parallel on the matrix multiplications, but the next biggest model also<a id="_idIndexMarker107"/> nearly doubles the number of cores, leading to more parallelization.</p>
    <p class="normal">We can scale up the number of instances a lot more or process more complex models while still having a lower matrix generation time. At some arbitrary number of instances, the frame rate of the application will still drop below 60 FPS. Depending on your system, this may happen before reaching the 4,000 instances of <em class="italic">Figure 2.6</em>, or much later.</p>
    <p class="normal">If you attach a profiler to the application, you will spot the new bottleneck of our calculation: The quaternion SLERP at the end of the method <code class="inlineCode">getRotation()</code> in the <code class="inlineCode">AssimpAnimChannel</code> class:</p>
    <pre class="programlisting code"><code class="hljs-code">  glm::quat rotation =
    glm::normalize(glm::slerp(mRotations.at(timeIndex),
    mRotations.at(timeIndex + 1), interpolatedTime));
</code></pre>
    <p class="normal">Also, the two <code class="inlineCode">mix()</code> calls of <code class="inlineCode">getTranslation()</code> and <code class="inlineCode">getScale()</code> in the <code class="inlineCode">AssimpAnimChannel</code> class will be among the top findings of the profiler.</p>
    <p class="normal">At this point, you could try to move even more operations to the compute shaders. But be aware that your mileage may vary. Some changes could raise the computational load of the GPU more than the CPU load will be lowered. That’s the moment when you should grab a good book about shader programming, or watch some conference talks, if you want to continue your journey into the world of compute shaders. The best way to get into GPU computation is still “learning by doing” and not giving up if the shader does not give the expected results. But be warned: Here will be dragons around, eating your time...</p>
    <p class="normal">Before we close this chapter, let’s <a id="_idIndexMarker108"/>talk briefly about compute shader debugging.</p>
    <h1 id="_idParaDest-81" class="heading-1">How to debug a compute shader</h1>
    <p class="normal">Compute shaders are cool – at least, until you run into some kind of trouble.</p>
    <p class="normal">While you can easily attach a debugger to the CPU code to see what’s going on, the GPU side is harder to check. A mistake in a fragment shader may cause distorted graphics, providing some hint for where the bug lies, but in other cases, you might see just nothing. In addition to undoing the latest changes, you can always attach a<a id="_idIndexMarker109"/> debugging tool like <strong class="keyWord">RenderDoc</strong> and check out what’s going wrong with<a id="_idIndexMarker110"/> the usual shader types.</p>
    <p class="normal">But, while RenderDoc has experimental support for compute shader debugging, this support is still limited. So, in contrast to other shader types, a compute shader is mostly a “black box” for us with RenderDoc – a program receiving and outputting opaque data.</p>
    <p class="normal">Depending on your GPU, you might want to try out NVIDIA Nsight (for NVIDIA GPUs) or the AMD Radeon GPU Profiler (for AMD GPUs). Links to all three tools are available in the <em class="italic">Additional resources</em> section.</p>
    <p class="normal">In many cases though, the problems in a compute shader come from simple mistakes. Uploading wrong or incomplete data to an SSBO, stride or padding problems, getting the order of elements wrong, swapping the order of a (non-commutative) matrix multiplication by accident... simple, but annoying errors that can take ages to find.</p>
    <p class="normal">A quite easy way to see what a compute shader stage does is by reading back the contents of the SSBOs. As an example for OpenGL, these lines read the data inside the SSBO <code class="inlineCode">buffer</code> into the <code class="inlineCode">std::vector</code> named <code class="inlineCode">bufferVector</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">  glBindBuffer(GL_SHADER_STORAGE_BUFFER, buffer);
  glGetBufferSubData(GL_SHADER_STORAGE_BUFFER, 0,
    buffer, bufferVector.data());
  glBindBuffer(GL_SHADER_STORAGE_BUFFER, 0)
</code></pre>
    <p class="normal">The contents of the SSBO could be compared to the results of the same calculations done on the CPU. Step by step and buffer by buffer, the problem may be narrowed down until the error has been found.</p>
    <p class="normal">Reading back an SSBO may not be an obvious solution to do compute shader debugging, but every little bit of help is welcome here. But, depending on the complexity of the shader, you may be thrown back to a manual walk-through of the code. Also, try to use a simple dataset to simplify debugging.</p>
    <h1 id="_idParaDest-82" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we moved a large part of the computations from the CPU to compute shaders on the GPU. After a brief history of concurrent code execution, we created a plan on how to move the node transform calculation to the GPU, and we finally executed that plan. At the end of the chapter, we checked the resulting application for the speedup we achieved.</p>
    <p class="normal">In the next chapter, we will take a look at solutions to add a visual selection to the model view application. Being able to create thousands of model instances is nice, but locating one special instance among all the others is nearly impossible right now. We will discuss two different approaches and implement one of them.</p>
    <h1 id="_idParaDest-83" class="heading-1">Practical sessions</h1>
    <p class="normal">There are some additions you could make to the code:</p>
    <ul>
      <li class="bulletList">Add “Programmable Vertex Pulling” to the code.</li>
    </ul>
    <p class="normal-one">With Programmable Vertex Pulling, the vertex data will no longer be pushed by using a vertex buffer. Instead, the vertex data will be uploaded to a UBO or SSBO to the GPU, and the vertex shader is used to extract all the data for every vertex from that buffer.</p>
    <ul>
      <li class="bulletList">Move <code class="inlineCode">mix()</code> and <code class="inlineCode">slerp()</code> from <code class="inlineCode">AssimpAnimChannel</code> to the GPU.</li>
    </ul>
    <p class="normal-one">When the two data values for the timings of translation, rotation, and scaling have been extracted from the channel vector, a linear interpolation for translation and scaling and a SLERP for rotation are required. Both interpolation types are called thousands of items per frame – maybe the GPU is faster.</p>
    <ul>
      <li class="bulletList">Blend between two animations in a compute shader.</li>
    </ul>
    <p class="normal-one">This task is similar to the previous practical session. But, instead of doing the interpolation between the animation keys of a single animation clip on the GPU, do the interpolation between the transformations at the same time for two different animation clips.</p>
    <p class="normal-one">For extra difficulty: Combine both tasks and do interpolations between the 4 values for the node transformations of two animation clips in a compute shader.</p>
    <ul>
      <li class="bulletList">Use RenderDoc to view the buffer contents.</li>
    </ul>
    <p class="normal-one">Since the buffer data type is shown as RGB values in RenderDoc, you may see some interesting and recurring patterns in the buffers.</p>
    <h1 id="_idParaDest-84" class="heading-1">Additional resources</h1>
    <ul>
      <li class="bulletList">C64 demo coding: <a href="https://codebase64.org/doku.php?id=base:start">https://codebase64.org/doku.php?id=base:start</a></li>
      <li class="bulletList">Atari ST demo scene: <a href="https://democyclopedia.wordpress.com">https://democyclopedia.wordpress.com</a></li>
      <li class="bulletList">pouët.net demo scene archive: <a href="https://www.pouet.net">https://www.pouet.net</a></li>
      <li class="bulletList">LearnOpenGL on compute shaders: <a href="https://learnopengl.com/Guest-Articles/2022/Compute-Shaders/Introduction">https://learnopengl.com/Guest-Articles/2022/Compute-Shaders/Introduction</a></li>
      <li class="bulletList">Vulkan Tutorial on compute shaders: <a href="https://vulkan-tutorial.com/Compute_Shader">https://vulkan-tutorial.com/Compute_Shader</a></li>
      <li class="bulletList"><em class="italic">Vulkan Compute: High-Performance Compute Programming with Vulkan and Compute Shaders</em> by <em class="italic">Kenwright</em>, published by the author himself, ISBN: 979-8345148280</li>
      <li class="bulletList">GLSL Interface block restrictions: <a href="https://www.khronos.org/opengl/wiki/Interface_Block_(GLSL)">https://www.khronos.org/opengl/wiki/Interface_Block_(GLSL)</a></li>
      <li class="bulletList">Matrix multiplication guide: <a href="https://blog.mecheye.net/2024/10/the-ultimate-guide-to-matrix-multiplication-and-ordering/%0D%0A">https://blog.mecheye.net/2024/10/the-ultimate-guide-to-matrix-multiplication-and-ordering/</a></li>
      <li class="bulletList">Tutorial on different matrix multiplications: <a href="https://tomhultonharrop.com/mathematics/matrix/2022/12/26/column-row-major.html%0D%0A">https://tomhultonharrop.com/mathematics/matrix/2022/12/26/column-row-major.html</a></li>
      <li class="bulletList">OpenGL memory barriers: <a href="https://registry.khronos.org/OpenGL-Refpages/gl4/html/glMemoryBarrier.xhtml%0D%0A">https://registry.khronos.org/OpenGL-Refpages/gl4/html/glMemoryBarrier.xhtml</a></li>
      <li class="bulletList">RenderDoc homepage: <a href="https://renderdoc.org">https://renderdoc.org</a></li>
      <li class="bulletList">NVIDIA Nsight: <a href="https://developer.nvidia.com/tools-overview">https://developer.nvidia.com/tools-overview</a></li>
      <li class="bulletList">AMD Radeon GPU Profiler: <a href="https://gpuopen.com/rgp/">https://gpuopen.com/rgp/</a></li>
    </ul>
  </div>
</div></div></body></html>