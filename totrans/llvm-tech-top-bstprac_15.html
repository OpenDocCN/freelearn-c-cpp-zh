<html><head></head><body>
		<div><h1 id="_idParaDest-166"><em class="italic"><a id="_idTextAnchor174"/>Chapter 12</em>: Learning LLVM IR Instrumentation</h1>
			<p>In the previous chapter, we learned how to leverage various utilities to improve our productivity while developing with LLVM. Those skills can give us a smoother experience when diagnosing problems that are raised by LLVM. Some of these utilities can even reduce the number of potential mistakes that are made by compiler engineers. In this chapter, we are going to learn how instrumentation works in LLVM IR.</p>
			<p>The <strong class="bold">instrumentation</strong> we are referring to here is a kind of technique that inserts some <em class="italic">probes</em> into the code we are compiling in order to collect runtime information. For example, we can collect information about how many times a certain function was called – which is only available once the target program has been executed. The advantage of this technique is that it provides extremely accurate information about the target program's behavior. This information can be used in several different ways. For instance, we can use the collected values to compile and optimize the same code <em class="italic">again</em> – but this time, since we have accurate data, we can perform more aggressive optimizations that couldn't be done previously. This technique is also called <strong class="bold">Profile-Guided Optimization</strong> (<strong class="bold">PGO</strong>). In another example, will be using the inserted probes to catch undesirable incidents that happened at runtime – buffer overflows, race conditions, and double-free memory, to name a few. The probe that's used for this purpose is also called a <strong class="bold">sanitizer</strong>.</p>
			<p>To implement instrumentation in LLVM, we not only need the help of LLVM pass, but also the synergy between <em class="italic">multiple</em> subprojects in LLVM – <strong class="bold">Clang</strong>, <strong class="bold">LLVM IR Transformation</strong>, and <strong class="bold">Compiler-RT</strong>. We already know about the first two from earlier chapters. In this chapter, we are going to introduce Compiler-RT and, more importantly, how can we <em class="italic">combine</em> these subsystems for the purpose of instrumentation.</p>
			<p>Here is the list of topics we are going to cover:</p>
			<ul>
				<li>Developing a sanitizer</li>
				<li>Working with PGO</li>
			</ul>
			<p>In the first part of this chapter, we are going to see how a sanitizer is implemented in Clang and LLVM, before creating a simple one by ourselves. The second half of this chapter is going to show you how to use the PGO framework in LLVM and how we can <em class="italic">extend</em> it.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor175"/>Technical requirements</h1>
			<p>In this chapter, we are going to work with multiple subprojects. One of them – Compiler-RT – needs to be included in your build by us modifying the CMake configuration. Please open the <code>CMakeCache.txt</code> file in your build folder and add the <code>compiler-rt</code> string to the value of the <code>LLVM_ENABLE_PROJECTS</code> variable. Here is an example:</p>
			<pre>//Semicolon-separated list of projects to build…
LLVM_ENABLE_PROJECTS:STRING="<strong class="bold">clang;compiler-rt</strong>"</pre>
			<p>After editing the file, launch a build with any build target. CMake will try to reconfigure itself.</p>
			<p>Once everything has been set up, we can build the components we need for this chapter. Here is an example command:</p>
			<pre>$ ninja clang compiler-rt opt llvm-profdata</pre>
			<p>This will build the <code>clang</code> tool we're all familiar with and a collection of Compiler-RT libraries, which we are going to introduce shortly.</p>
			<p>You can find the sample code for this chapter in the same GitHub repository: <a href="https://github.com/PacktPublishing/LLVM-Techniques-Tips-and-Best-Practices-Clang-and-Middle-End-Libraries/tree/main/Chapter12">https://github.com/PacktPublishing/LLVM-Techniques-Tips-and-Best-Practices-Clang-and-Middle-End-Libraries/tree/main/Chapter12</a>.</p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor176"/>Developing a sanitizer</h1>
			<p>A sanitizer is a <a id="_idIndexMarker661"/>kind of technique that checks certain runtime properties of the code (<code>probe</code>) that's inserted by the compiler. People usually use a sanitizer to ensure program correctness or enforce security policies. To give you <a id="_idIndexMarker662"/>an idea of how a sanitizer works, let's use one of the most popular sanitizers in Clang as an example – the <strong class="bold">address sanitizer</strong>.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor177"/>An example of using an address sanitizer</h2>
			<p>Let's assume <a id="_idIndexMarker663"/>we have some simple C code, such <a id="_idIndexMarker664"/>as the following:</p>
			<pre>int main(int argc, char **argv) {
  int <strong class="bold">buffer[3]</strong>;
  for (int i = 1; i &lt; argc; ++i)
    <strong class="bold">buffer[i-1]</strong> = atoi(argv[i]);
  for (int i = 1; i &lt; argc; ++i)
    printf("%d ", buffer[i-1]);
  printf("\n");
  return 0;
}</pre>
			<p>The preceding code converted the command-line arguments into integers and stored them in a buffer of size 3. Then, we printed them out.</p>
			<p>You should be able to easily spot an outstanding problem: the value of <code>argc</code> can be arbitrarily big when it's larger than 3 – the size of <code>buffer</code>. Here, we are storing the value in an <em class="italic">invalid</em> memory location. However, when we compile this code, the compiler will say nothing. Here is an example:</p>
			<pre>$ clang -Wall buffer_overflow.c -o buffer_overflow
$ # No error or warning</pre>
			<p>In the preceding command, even if we enable all the compiler warnings via the <code>-Wall</code> flag, <code>clang</code> won't complain about the potential bug.</p>
			<p>If we try to execute the <code>buffer_overflow</code> program, the program will crash at some time point after we pass more than three command-line arguments to it; for example:</p>
			<pre>$ ./buffer_overflow 1 2 3
1 2 3
$ ./buffer_overflow 1 2 3 4
<strong class="bold">Segmentation fault (core dumped)</strong>
$</pre>
			<p>What's worse, the number of command-line arguments to crash <code>buffer_overflow</code> actually <em class="italic">varies</em> from machine to machine. This makes it even more difficult to debug if the example shown here were a real-world bug. To summarize, the problem we're encountering here is caused by the fact that <code>buffer_overflow</code> only goes rogue on <em class="italic">some</em> inputs and the compiler failed to catch the problem.</p>
			<p>Now, let's <a id="_idIndexMarker665"/>try to use an address sanitizer <a id="_idIndexMarker666"/>to catch this bug. The following command asks <code>clang</code> to compile the same code with an address sanitizer:</p>
			<pre>$ clang <strong class="bold">-fsanitize=address</strong> buffer_overflow.c -o san_buffer_overflow</pre>
			<p>Let's execute the program again. Here is the output:</p>
			<pre>$ ./san_buffer_overflow 1 2 3
1 2 3
$ ./san_buffer_overflow 1 2 3 4
=================================================================
==137791==<strong class="bold">ERROR: AddressSanitizer: stack-buffer-overflow</strong> on address 0x7ffea06bccac at pc 0x0000004f96df bp 0x7ffea06bcc70…
WRITE of size 4 at 0x7ffea06bccac thread T0
…
  This frame has 1 object(s):
    [32, 44) <strong class="bold">'buffer' &lt;== Memory access at offset 44 overflows this variable</strong>
…
==137791==ABORTING
$</pre>
			<p>Instead of just crashing, the address sanitizer gave us many details about the issue that was raised at runtime: the sanitizer told us that it detected a <em class="italic">buffer overflow</em> on the stack, which might be the <code>buffer</code> variable.</p>
			<p>These messages were extremely useful. Imagine that you are working on a much more complicated software project. When a strange memory bug occurs, rather than just crash or silently change the program's logic, the address sanitizer can point out the problematic area – with high accuracy – right away.</p>
			<p>To go a <a id="_idIndexMarker667"/>little deeper into its mechanisms, the <a id="_idIndexMarker668"/>following diagram illustrates how the address sanitizer detects the buffer overflow:</p>
			<div><div><img src="img/B14590_12.1.jpg" alt="Figure 12.1 – Instrumentation code inserted by the address sanitizer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – Instrumentation code inserted by the address sanitizer</p>
			<p>Here, we can see that the address sanitizer is effectively inserting a boundary check into the array index that's used for accessing <code>buffer</code>. With this extra check – which will be executed at runtime – the target program can bail out with error details before violating the memory access. More generally speaking, during the compilation, a sanitizer inserts some instrumentation code (into the target program) that will eventually be executed at runtime to check or <em class="italic">guard</em> certain properties.</p>
			<p class="callout-heading">Detecting overflow using an address sanitizer</p>
			<p class="callout">The preceding diagram shows a simplified version of how an address sanitizer works. In reality, the address sanitizer will leverage multiple strategies to monitor memory access in a program. For example, an address sanitizer can use a special memory allocator that allocates memory with <code>traps</code> put at the invalid memory region.</p>
			<p>While an <a id="_idIndexMarker669"/>address sanitizer is specialized in catching illegal memory access, a <strong class="bold">ThreadSanitizer</strong> can be used to catch data race conditions; that is, invalid access from multiple threads on the same chunk of data. Some other examples of sanitizers in Clang are the <strong class="bold">LeakSanitizer</strong>, which is used for detecting sensitive data such as <a id="_idIndexMarker670"/>passwords being leaked, and <strong class="bold">MemorySanitizer</strong>, which is used for <a id="_idIndexMarker671"/>detecting reads to uninitialized memory.</p>
			<p>Of course, there are some downsides to using sanitizers. The most prominent problem is the performance impact: using a thread sanitizer (in Clang) as an example, programs that are compiled with one are <em class="italic">5~15 times slower</em> than the original version. Also, since sanitizers insert extra code into the program, it might hinder some optimization opportunities, or even affect the original program's logic! In other words, it is a trade-off between the <em class="italic">robustness</em> and <em class="italic">performance</em> of the target program.</p>
			<p>With that, you've learned about the high-level idea of a sanitizer. Let's try to create a real one <a id="_idIndexMarker672"/>by ourselves to understand how Clang <a id="_idIndexMarker673"/>and LLVM implement a sanitizer. The following section contains more code than any of the examples in previous chapters, not to mention the changes are spread across different subprojects in LLVM. To focus on the most important knowledge, we won't go into the details of some <em class="italic">supporting</em> code – for example, changes that are made to CMake build scripts. Instead, we will go through them by providing a brief introduction and pointing out where you can find it in this book's GitHub repository.</p>
			<p>Let's start by providing an overview of the project we are going to create.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor178"/>Creating a loop counter sanitizer</h2>
			<p>To (slightly) simplify <a id="_idIndexMarker674"/>our task, the sanitizer <a id="_idIndexMarker675"/>we are going to create – a loop <a id="_idIndexMarker676"/>counter sanitizer, or <strong class="bold">LPCSan</strong> for short – looks just like a sanitizer except that it is not checking any serious program properties. Instead, we want to use it to print out the real, concrete <strong class="bold">trip count</strong> – the number of iterations – of a loop, which is only available during runtime.</p>
			<p>For example, let's assume we have the following input code:</p>
			<pre>void foo(int S, int E, int ST, int *a) {
  for (int i = S; i &lt; E; i += ST) {
    a[i] = a[i + 1];
  }
}
int main(int argc, char **argv) {
  int start = atoi(argv[1]),
      end = atoi(argv[2]),
      step = atoi(argv[3]);
  int a[100];
  foo(start, end, step, a);
  return 0;
}</pre>
			<p>We can compile it with a LPCSan using the following command:</p>
			<pre>$ clang <strong class="bold">-O1 -fsanitize=loop-counter</strong> test_lpcsan.c -o test_lpcsan</pre>
			<p>Note that compiling with optimization greater than <code>-O0</code> is necessary; we will explain why later.</p>
			<p>When we <a id="_idIndexMarker677"/>execute <code>test_lpcsan</code> (with some command-line argument), we can print out the precise trip count <a id="_idIndexMarker678"/>of the loop in the <code>foo</code> function. For example, look at the following code:</p>
			<pre>$ ./test_lpcsan 0 100 1
==143813==<strong class="bold">INFO: Found a loop with trip count 100</strong>
$ ./test_lpcsan 0 50 2
==143814==INFO: Found a loop with trip count 25
$</pre>
			<p>The message highlighted in the preceding code was printed by our sanitizer code.</p>
			<p>Now, let's dive into the steps for creating the LPCSan. We will divide this tutorial into three parts: </p>
			<ul>
				<li>Developing an IR transformation</li>
				<li>Adding Compiler-RT components</li>
				<li>Adding the LPCSan to Clang</li>
			</ul>
			<p>We will start with the IR transformation part of this sanitizer.</p>
			<h3>Developing an IR transformation</h3>
			<p>Previously, we learned that an address sanitizer – or just a sanitizer in general – usually inserts <a id="_idIndexMarker679"/>code into the target <a id="_idIndexMarker680"/>program to check certain runtime properties or collect data. In <a href="B14590_09_Final_JC_ePub.xhtml#_idTextAnchor127"><em class="italic">Chapter 9</em></a>, <em class="italic">Working with PassManager and AnalysisManager</em>, and <a href="B14590_10_Final_JC_ePub.xhtml#_idTextAnchor141"><em class="italic">Chapter 10</em></a>, <em class="italic">Processing LLVM IR</em>, we learned how to modify/transform LLVM IR, including inserting new code into it, so this seems to be a good starting point for crafting our LPCSan.</p>
			<p>In this section, we are going to develop an LLVM pass called <code>LoopCounterSanitizer</code> that inserts special function calls to collect the exact trip count of every loop in <code>Module</code>. Here are the detailed steps:</p>
			<ol>
				<li>First, let's create two files: <code>LoopCounterSanitizer.cpp</code> under the <code>llvm/lib/Transforms/Instrumentation</code> folder and its corresponding header file inside the <code>llvm/include/llvm/Transforms/Instrumentation</code> folder. Inside the header file, we will place the declaration of this pass, as shown here:<pre>struct LoopCounterSanitizer
  : public PassInfoMixin&lt;LoopCounterSanitizer&gt; {
  PreservedAnalyses run(Loop&amp;, LoopAnalysisManager&amp;,
                        LoopStandardAnalysisResults&amp;, 
                        LPMUpdater&amp;);
private:
  // Sanitizer functions
  <code>LPCSetStartFn</code> and <code>LPCAtEndFn</code> memory variables – they will store the <code>Function</code> instances that collect loop trip counts (<code>FunctionCallee</code> is a thin wrapper around <code>Function</code> that provides additional function signature information).</p></li>
				<li>Finally, in <code>LoopCounterSanitizer.cpp</code>, we are placing the skeleton code for our pass, as shown here:<pre>PreservedAnalyses
LoopCounterSanitizer::run(Loop &amp;LP, LoopAnalysisManager &amp;LAM, LoopStandardAnalysisResults &amp;LSR, LPMUpdater &amp;U) {
  <code>initializeSanitizerFuncs</code> method <a id="_idIndexMarker681"/>in the preceding code will populate <code>LPCSetStartFn</code> and <code>LPCAtEndFn</code>. Before we go into the details of <code>initializeSanitizerFuncs</code>, let's talk more about <code>LPCSetStartFn</code> and <code>LPCAtEndFn</code>.</p></li>
				<li>To <a id="_idIndexMarker682"/>figure out the exact trip count, the <code>Function</code> instance stored in <code>LPCSetStartFn</code> will be used to collect the <em class="italic">initial</em> induction variable value of a loop. On the other hand, the <code>Function</code> instance stored in <code>LPCAtEndFn</code> will be used to collect the <em class="italic">final</em> induction variable value and the step value of the loop. To give you a concrete idea of how these two <code>Function</code> instances work together, let's assume we have the following pseudocode as our input program:<pre>void foo(int S, int E, int ST) {
  for (int i = S; i &lt; E; i += ST) {
    …
  }
}</pre><p>In the preceding code, the <code>S</code>, <code>E</code>, and <code>ST</code> variables represent the initial, final, and step <a id="_idIndexMarker683"/>values of a loop, respectively. The goal of the <code>LoopCounterSanitizer</code> pass is to insert <code>LPCSetStartFn</code> and <code>LPCAtEndFn</code> in the following way:</p><pre>void foo(int S, int E, int ST) {
  for (int i = S; i &lt; E; i += ST) {
    <code>lpc_set_start</code> and <code>lpc_at_end</code> in the preceding code are <code>Function</code> instances that <a id="_idIndexMarker684"/>are stored in <code>LPCSetStartFn</code> and <code>LPCAtEndFn</code>, respectively. Here is one of the possible (pseudo) implementations of these two functions:</p><pre>static int CurrentStartVal = 0;
void lpc_set_start(int start) {
  <code>LPCSetStartFn</code> and <code>LPCAtEndFn</code>, it's time to take a look at how <code>initializeSanitizerFuncs</code> initializes them.</p></li>
				<li>Here is the code inside <code>initializeSanitizerFuncs</code>:<pre>void LoopCounterSanitizer::initializeSanitizerFuncs(Loop &amp;LP) {
  Module &amp;M = *LP.getHeader()-&gt;getModule();
  auto &amp;Ctx = M.getContext();
  Type *VoidTy = Type::<code>__lpcsan_set_loop_start</code> and <code>__lpcsan_at_loop_end</code>, from the module <a id="_idIndexMarker686"/>and storing their <code>Function</code> instances in <code>LPCSetStartFn</code> and <code>LPCAtEndFn</code>, respectively.</p><p>The <code>Module::getOrInsertFunction</code> method either grabs the <code>Function</code> instance of the given function name from the module or creates one if it doesn't exist. If it's a newly created instance, it has an empty function body; in other words, it only has a function <em class="italic">declaration</em>.</p><p>It is also worth noting that the second argument of <code>Module::getOrInsertFunction</code> is the return type of the <code>Function</code> inquiry. The rest (the arguments for <code>getOrInsertFunction</code>) represent the argument types of that <code>Function</code>.</p><p>With <code>LPCSetStartFn</code> and <code>LPCAtEndFn</code> set up, let's see how we can insert them into the right place in IR.</p></li>
				<li>Recall that in <a href="B14590_10_Final_JC_ePub.xhtml#_idTextAnchor141"><em class="italic">Chapter 10</em></a>, <em class="italic">Processing LLVM IR</em>, we learned about several utility classes for working with <code>Loop</code>. One of them – <code>LoopBounds</code> – can give us the boundary of a <code>Loop</code>. We can do this by including the start, end, and step values <a id="_idIndexMarker687"/>of an induction variable, which is exactly <a id="_idIndexMarker688"/>the information we are looking for. Here is the code that tries to retrieve a <code>LoopBounds</code> instance:<pre>PreservedAnalyses
LoopCounterSanitizer::run(Loop &amp;LP, LoopAnalysisManager &amp;LAM, LoopStandardAnalysisResults &amp;LSR, LPMUpdater &amp;U) {
  initializeSanitizerFuncs(LP);
  <code>Loop::getBounds</code> from the preceding code returned an <code>Optional&lt;LoopBounds&gt;</code> instance. The <code>Optional&lt;T&gt;</code> class is a useful container that <a id="_idIndexMarker689"/>either stores an instance of the <code>T</code> type or is <em class="italic">empty</em>. You can <a id="_idIndexMarker690"/>think of it as a replacement for the <code>T*</code> to represent a computation result where a null pointer <a id="_idIndexMarker691"/>means an empty value. However, this has the risk of dereferencing a null pointer if the programmer forgets to check the pointer first. The <code>Optional&lt;T&gt;</code> class doesn't have this problem.</p><p>With a <code>LoopBounds</code> instance, we can retrieve the induction variable's range and store it in the <code>StartVal</code>, <code>EndVal</code>, and <code>StepVal</code> variables.</p></li>
				<li><code>StartVal</code> is the <code>Value</code> instance to be collected by <code>__lpcsan_set_loop_start</code>, whereas <code>__lpcsan_at_loop_end</code> is going to collect <code>EndVal</code> and <code>StepVal</code> at runtime. Now, the question is, <em class="italic">where</em> should we insert function calls to <code>__lpcsan_set_loop_start</code> and <code>__lpcsan_at_loop_end</code> to correctly collect those values?<p>The rule of thumb is that we need to insert those function calls after the <em class="italic">definition</em> of those values. While we can find the exact locations where those values were defined, let's try to simplify the problem by inserting instrumentation function calls at some fixed locations – locations where our target values are <em class="italic">always</em> available.</p><p>For <code>__lpcsan_set_loop_start</code>, we are inserting it at the end of the <code>getTerminator</code> to get the last <code>Instruction</code> from the header block. Then, we used <code>IRBuilder&lt;&gt;</code> – with the last instruction as the insertion point – to insert new <code>Instruction</code> instances.</p><p>Before <a id="_idIndexMarker693"/>we can pass <code>StartVal</code> as an argument to the new <code>__lpcsan_set_loop_start</code> function call, we need to convert its IR type (represented by the <code>Type</code> class) into a compatible one. <code>IRBuilder::CreateInstCast</code> is a handy utility that automatically generates either an instruction to <em class="italic">extend</em> the integer bit width or an instruction to <em class="italic">truncate</em> the bit width, depending on the given <code>Value</code> and <code>Type</code> instances.</p><p>Finally, we can create a function call to <code>__lpcsan_set_loop_start</code> via <code>IRBuilder::CreateCall</code>, with <code>StartVal</code> as the function call argument.</p></li>
				<li>For <code>__lpcsan_at_loop_end</code>, we are using the same trick to collect the runtime values of <code>EndVal</code> and <code>StepVal</code>. Here is the code:<pre>BasicBlock *ExitBlock = LP.<code>__lpcsan_at_loop_end</code> at the beginning of the <em class="italic">exit block</em>. This is because we can always expect the end value and the step value of the induction variable being <a id="_idIndexMarker694"/>defined before we leave the loop.</p><p>These <a id="_idIndexMarker695"/>are all the implementation details for the <code>LoopCounterSanitizer</code> pass.</p></li>
				<li>Before we wrap up this section, we need to edit a few more files to make sure everything works. Please look at the <code>Changes-LLVM.diff</code> file in the sample code folder for this chapter. Here is the summary of the changes that were made in other supporting files:<p>i. Changes in <code>llvm/lib/Transforms/Instrumentation/CMakeLists.txt</code>: Add our new pass source file to the build.</p><p>ii. Changes in <code>llvm/lib/Passes/PassRegistry.def</code>: Add our pass to the list of available passes so that we can test it using our old friend <code>opt</code>.</p></li>
			</ol>
			<p>With that, we've finally finished making all the necessary modifications to the LLVM part.</p>
			<p>Before we move on to the next section, let's test our newly created <code>LoopCounterSanitizer</code> pass. We are going to be using the same C code we saw earlier in this section. Here is the function that contains the loop we want to instrument:</p>
			<pre>void foo(int S, int E, int ST, int *a) {
  for (int i = S; i &lt; E; i += ST) {
    a[i] = a[i + 1];
  }
}</pre>
			<p>Note that <a id="_idIndexMarker696"/>although we didn't explicitly check the loop <a id="_idIndexMarker697"/>form in our pass, some of the APIs that were used in the pass actually required the loop to be <em class="italic">rotated</em>, so please generate the LLVM IR code with an O1 optimization level to make sure the loop rotation's Pass has kicked in:</p>
			<p>Here is the simplified LLVM IR for the <code>foo</code> function:</p>
			<pre>define void @foo(i32 %S, i32 %E, i32 %ST, i32* %a) {
  %cmp9 = icmp slt i32 %S, %E
  br i1 %cmp9, label %for.body.preheader, label %for.cond.   cleanup
<strong class="bold">for.body.preheader</strong>:  
  %0 = sext i32 %S to i64
  %1 = sext i32 %ST to i64
  %2 = sext i32 %E to i64
  br label %for.body
…
<strong class="bold">for.body</strong>:                                         
  %indvars.iv = phi i64 [ %0, %for.body.preheader ], [   %indvars.iv.next, %for.body ]
  …
  %indvars.iv.next = add i64 %indvars.iv, %1
  %cmp = icmp slt i64 %indvars.iv.next, %2
  br i1 %cmp, label %for.body, label %for.cond.cleanup
}</pre>
			<p>The highlighted labels are the preheader and loop body blocks for this loop. Since this loop <a id="_idIndexMarker698"/>has been rotated, the <code>for.body</code> block is both the header, latch, and exiting block for this loop.</p>
			<p>Now, let's transform this IR with <code>opt</code> using the following command:</p>
			<pre>$ opt -S –passes="loop(<strong class="bold">lpcsan</strong>)" input.ll -o -</pre>
			<p>In the <code>–passes</code> command-line option, we asked <code>opt</code> to run our <code>LoopCounterSanitizer</code> pass (with the name <code>lpcsan</code>, which is registered in the <code>PassRegistry.def</code> file). The enclosing <code>loop(…)</code> string is simply telling <code>opt</code> that <code>lpcsan</code> is a loop pass (you can actually omit <a id="_idIndexMarker699"/>this decoration since <code>opt</code> can find the right pass most of the time).</p>
			<p>Here is the simplified result:</p>
			<pre><strong class="bold">declare void @__lpcsan_set_loop_start(i32)</strong>
<strong class="bold">declare void @__lpcsan_at_loop_end(i32, i32)</strong>
define void @foo(i32 %S, i32 %E, i32* %a) {
  %cmp8 = icmp slt i32 %S, %E
  br i1 %cmp8, label %for.body.preheader, label %for.cond.cleanup
for.body.preheader: 
  %0 = sext i32 %S to i64
  %wide.trip.count = sext i32 %E to i64
  br label %for.body
<strong class="bold">for.cond.cleanup.loopexit</strong>:                        
  %1 = trunc i64 %wide.trip.count to i32
  <strong class="bold">call void @__lpcsan_at_loop_end(i32 %1, i32 1)</strong>
  br label %for.cond.cleanup
<strong class="bold">for.body</strong>:
  …
  %3 = trunc i64 %0 to i32
  <strong class="bold">call void @__lpcsan_set_loop_start(i32 %3)</strong>
  br i1 %exitcond.not, label %for.cond.cleanup.loopexit, label    %for.body
}</pre>
			<p>As you can see, <code>__lpcsan_set_loop_start</code> and <code>__lpcsan_at_loop_end</code> have been correctly <a id="_idIndexMarker700"/>inserted into the header block and exit block, respectively. They are also collecting the desired values related to the loop trip count.</p>
			<p>Now, the <a id="_idIndexMarker701"/>biggest question is: where are the <em class="italic">function bodies</em> for <code>__lpcsan_set_loop_start</code> and <code>__lpcsan_at_loop_end</code>? Both only have declarations in the preceding IR code.</p>
			<p>In the next section, we will use Compiler-RT to answer this question.</p>
			<h3>Adding the Compiler-RT component</h3>
			<p>The <a id="_idIndexMarker702"/>name <strong class="bold">Compiler-RT</strong> stands <a id="_idIndexMarker703"/>for <strong class="bold">Compiler RunTime</strong>. The usage of <em class="italic">runtime</em> is a <a id="_idIndexMarker704"/>little ambiguous here because too many things can be called a runtime in a normal compilation pipeline. But the truth is that Compiler-RT <em class="italic">does</em> contain a wide range of libraries for completely different tasks. What these libraries have in common is that they provide <em class="italic">supplement</em> code for the target program to implement enhancement features or functionalities that were otherwise absent. It is important to remember that Compiler-RT libraries are NOT used for building a compiler or related tool – they should be linked with the program we are compiling.</p>
			<p>One of the most used <a id="_idIndexMarker705"/>features in Compiler-RT is the <strong class="bold">builtin function</strong>. As you might have heard, more and more computer architectures nowadays support <em class="italic">vector operation</em> natively. That is, you can process multiple data elements at the <a id="_idIndexMarker706"/>same time with the support <a id="_idIndexMarker707"/>from hardware. Here is some example code, written in C, that uses vector operations:</p>
			<pre>typedef int v4si __attribute__((__vector_size__(16)));
v4si v1 = (v4si){1, 2, 3, 4};
v4si v2 = (v4si){5, 6, 7, 8};
v4si v3 = <strong class="bold">v1 + v2</strong>; // = {6, 8, 10, 12}</pre>
			<p>The preceding code used a non-standardized (currently, you can only use this syntax in Clang and GCC) C/C++ vector extension to declare two vectors, <code>v1</code> and <code>v2</code>, before adding them to yield a third one.</p>
			<p>On X86-64 <a id="_idIndexMarker708"/>platforms, this code will be compiled to use one of <a id="_idIndexMarker709"/>the vector instruction sets, such as <code>for-loop</code> to replace vector summation in this case. More specifically, whenever we see a vector summation at compilation time, we replace it with a call to a function that contains the synthesis implementation using <code>for-loop</code>. The function body can be put anywhere, as long as it is eventually linked with the program. The following diagram illustrates this process:</p>
			<div><div><img src="img/B14590_12.2.jpg" alt="Figure 12.2 – Workflow of the Compiler-RT builtin&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 – Workflow of the Compiler-RT builtin</p>
			<p>As you <a id="_idIndexMarker711"/>may have <a id="_idIndexMarker712"/>noticed, the workflow shown here is similar to our requirement in the LPCSan: in the previous section, we developed an LLVM pass that inserted extra function calls to collect the loop trip count, but we still need to implement those collector functions. If we leverage the workflow shown in the preceding diagram, we can come up with a design, as shown in the following diagram:</p>
			<div><div><img src="img/B14590_12.3.jpg" alt="Figure 12.3 – Workflow of the Compiler-RT LPCSan component&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3 – Workflow of the Compiler-RT LPCSan component</p>
			<p>The previous diagram shows that the function bodies of <code>__lpcsan_set_loop_start</code> and <code>__lpcsan_at_loop_end</code> are put inside a Compiler-RT library that will eventually be linked with the final binary. Inside these two functions, we calculate the trip count using the <a id="_idIndexMarker713"/>input arguments and print the result. In the rest of this section, we'll show you how to create such <a id="_idIndexMarker714"/>a Compiler-RT library for the LPCSan. Let's get started:</p>
			<ol>
				<li value="1">First, switch the folder to <code>llvm-project/compiler-rt</code>, the root of Compiler-RT. Inside this subproject, we must create a new folder called <code>lib/lpcsan</code> before we put a new <code>lpcsan.cpp</code> file inside it. Within this file, let's create the skeleton for our instrumentation functions. Here is the code:<pre>#include "sanitizer_common/sanitizer_common.h"
#include "sanitizer_common/sanitizer_internal_defs.h"
using namespace __sanitizer;
extern "C" SANITIZER_INTERFACE_ATTRIBUTE
void <code>s32</code> – available under the <code>__sanitizer</code> namespace – for a signed 32-bit integer rather than the normal <code>int</code>. The rationale behind this is that we might need to build Compiler-RT libraries for different hardware architectures or platforms, and the width of <code>int</code> might not be 32 bits on some of them.</p><p>Second, although we are using C++ to implement our instrumentation functions, we <a id="_idIndexMarker715"/>need to expose them as C functions because C functions have a more stable <code>extern "C"</code> to functions you want to export. The <code>SANITIZER_INTERFACE_ATTRIBUTE</code> macro also ensures that the function <a id="_idIndexMarker716"/>will be exposed at the library interface correctly, so please add this as well.</p></li>
				<li>Next, we <a id="_idIndexMarker717"/>will add the necessary code to these two functions. Here is how we do this:<pre>static <code>CurLoopStart</code> is a global variable that memorizes the <em class="italic">initial</em> induction variable value of the current loop. This is updated by <code>__lpcsan_set_loop_start</code>.</p><p>Recall that when a loop is complete, <code>__lpcsan_at_loop_end</code> will be invoked. When that happens, we use the value stored in <code>CurLoopStart</code> and the <code>end</code> and <code>step</code> arguments to calculate the exact trip count of the current loop, before printing the result.</p></li>
				<li>Now <a id="_idIndexMarker718"/>that we have <a id="_idIndexMarker719"/>implemented the core logic, it's time to build this library. Inside the <code>lib/lpcsan</code> folder, create a new <code>CMakeLists.txt</code> file and insert the following code:<pre>…
set(LPCSAN_RTL_SOURCES
    lpcsan.cpp)
<code>CMakeLists.txt</code>. Here are some highlights:</p><p>i. Compiler-RT creates its own set of CMake macros/functions. Here, we are using two of them, <code>add_compiler_rt_component</code> and <code>add_compiler_rt_runtime</code>, to create a pseudo build target for the entire LPCSan and the real library build target, respectively.</p><p>ii. Different from a conventional build target, if a sanitizer wants to use supporting/utility libraries in Compiler-RT – for example, <code>RTSanitizerCommon</code> in the preceding code – we usually link against their <em class="italic">object files</em> rather than their library files. More specifically, we can use the <code>$&lt;TARGET_OBJECTS:…&gt;</code> directive to import supporting/utility components as one of the input sources.</p><p>iii. A sanitizer library can support multiple architectures and platforms. In Compiler-RT, we are enumerating all the supported architectures and creating a sanitizer library for each of them.</p><p>Again, the <a id="_idIndexMarker720"/>preceding snippet is just a small part of our build script. Please refer to our sample code folder for the complete <code>CMakeLists.txt</code> file.</p></li>
				<li>To <a id="_idIndexMarker721"/>successfully build the LPCSan, we still need to make some changes in Compiler-RT. The <code>Base-CompilerRT.diff</code> patch in the same code folder provides the rest of the changes that are necessary to build our sanitizer. Apply it to Compiler-RT's source tree. Here is the summary of this patch:<p>i. Changes in <code>compiler-rt/cmake/config-ix.cmake</code> basically specify the supported architectures and operating systems of the LPCSan. The <code>LPCSAN_SUPPORTED_ARCH</code> CMake variable we saw in the previous snippet comes from here.</p><p>ii. The entire <code>compiler-rt/test/lpcsan</code> folder is actually a placeholder. For some reason, having tests is a <em class="italic">requirement</em> for every sanitizer in Compiler-RT – which is different from LLVM. Therefore, we are putting an empty test folder here to pass this requirement that's being imposed by the build infrastructure.</p></li>
			</ol>
			<p>These are <a id="_idIndexMarker722"/>all the steps for producing a Compiler-RT component for our LPCSan.</p>
			<p>To just <a id="_idIndexMarker723"/>build our LPCSan library, invoke the following command:</p>
			<pre>$ ninja lpcsan</pre>
			<p>Unfortunately, we can't test this LPCSan library until we've modified the compilation pipeline in Clang. In the last part of this section, we are going to learn how to achieve this task.</p>
			<h3>Adding the LPCSan to Clang</h3>
			<p>In the previous section, we learned how Compiler-RT libraries provide supplement functionalities <a id="_idIndexMarker724"/>to the target program or assist with special instrumentation, such as the sanitizer we just created. In this section, we are going to put everything together so that we can use our LPCSan simply by passing the <code>-fsanitize=loop-counter</code> flag to <code>clang</code>.</p>
			<p>Recall that in <em class="italic">Figure 12.3</em>, Compiler-RT libraries need to be linked with the program we are compiling. Also, recall that in order to insert the instrumentation code into the target program, we must run our <code>LoopCounterSanitizer</code> pass. In this section, we are going to modify the compilation pipeline in Clang so that it runs our LLVM pass at a certain time and sets up the correct configuration for our Compiler-RT library. More specifically, the following diagram shows the tasks that each component needs to complete to run our LPCSan:</p>
			<div><div><img src="img/B14590_12.4.jpg" alt="Figure 12.4 – Tasks for each component in the pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.4 – Tasks for each component in the pipeline</p>
			<p>Here are the descriptions for each of the numbers (enclosed in circles) in the preceding diagram:</p>
			<ol>
				<li value="1">The driver needs to recognize the <code>-fsanitize=loop-counter</code> flag.</li>
				<li>When the frontend is about to generate LLVM IR from an <code>LoopCounterSanitizer</code> pass.</li>
				<li>The LLVM pass pipeline needs to run our <code>LoopCounterSanitizer</code> (we don't need to worry about this task if the previous task is done correctly). </li>
				<li>The linker needs to link our Compiler-RT library to the target program.</li>
			</ol>
			<p>Although this workflow looks a little scary, don't be overwhelmed by the prospective workload – Clang can actually do most of these tasks for you, as long as you provide sufficient information. In the rest of this section, we'll show you how to implement the tasks <a id="_idIndexMarker725"/>shown in the preceding diagram to fully integrate our LPCSan into the Clang compilation pipeline (the following tutorial works inside the <code>llvm-project/clang</code> folder). Let's get started:</p>
			<ol>
				<li value="1">First, we must modify <code>include/clang/Basic/Sanitizers.def</code> to add our sanitizer:<pre>…
// Shadow Call Stack
SANITIZER("shadow-call-stack", ShadowCallStack)
// Loop Counter Sanitizer
<code>LoopCounter</code>, to the <code>SanitizerKind</code> class.</p><p>It turns out that the driver will parse the <code>-fsanitize</code> command-line option and <em class="italic">automatically</em> translate <code>loop-counter</code> into <code>SanitizerKind::LoopCounter</code> based on the information we provided in <code>Sanitizers.def</code>.</p></li>
				<li>Next, let's work on the driver part. Open <code>include/clang/Driver/SanitizerArgs.h</code> and add a new utility method, <code>needsLpcsanRt</code>, to the <code>SanitizerArgs</code> class. Here is the code:<pre>bool needsLsanRt() const {…}
bool <strong class="bold">needsLpcsanRt</strong>() const {
  return <strong class="bold">Sanitizers.has(SanitizerKind::LoopCounter)</strong>;
}</pre><p>The utility <a id="_idIndexMarker726"/>method we created here can be used by other places in the driver to check if our sanitizer needs a Compiler-RT component.</p></li>
				<li>Now, let's navigate to the <code>lib/Driver/ToolChains/CommonArgs.cpp</code> file. Here, we're adding a few lines to the <code>collectSanitizerRuntimes</code> function. Here is the code:<pre>…
if (SanArgs.needsLsanRt() &amp;&amp; SanArgs.linkRuntimes())
  StaticRuntimes.push_back("lsan");
if (<strong class="bold">SanArgs.needsLpcsanRt()</strong> &amp;&amp; SanArgs.linkRuntimes())
  <strong class="bold">StaticRuntimes.push_back("lpcsan");</strong>
…</pre><p>The preceding snippet effectively makes the linker link the correct Compiler-RT library to the target binary.</p></li>
				<li>The last change we will make to the driver is in <code>lib/Driver/ToolChains/Linux.cpp</code>. Here, we add the following lines to the <code>Linux::getSupportedSanitizers</code> method:<pre>SanitizerMask Res = ToolChain::getSupportedSanitizers();
…
<strong class="bold">Res |= SanitizerKind::LoopCounter;</strong>
…</pre><p>The previous code is essentially telling the driver that we support the LPCSan in the current toolchain – the toolchain for Linux. Note that to simplify our example, we are <a id="_idIndexMarker727"/>only supporting the LPCSan in Linux. If you want to support this custom sanitizer in other platforms and architectures, modify the other toolchain implementations. Please refer to <a href="B14590_08_Final_JC_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 8</em></a>, <em class="italic">Working with Compiler Flags and Toolchains</em>, for more details if needed.</p></li>
				<li>Finally, we are going to insert our <code>LoopCounterSanitizer</code> pass into the LLVM pass pipeline. Open <code>lib/CodeGen/BackendUtil.cpp</code> and add the following lines to the <code>addS<a id="_idTextAnchor179"/>anitizers</code> function:<pre>…
// `PB` has the type of `<code>CodeGen</code>, is a place where the Clang and LLVM libraries meet. Therefore, we will see several LLVM APIs appear in this place. There are primarily two tasks for this <code>CodeGen</code> component:</p><p>a. Converting the Clang AST into its equivalent LLVM IR <code>module</code></p><p>b. Constructing an LLVM pass pipeline to optimize the IR and generate machine code</p><p>The previous snippet was trying to customize the second task – that is, customizing the LLVM Pass pipeline. The specific function – <code>addSanitizers</code> – we are <a id="_idIndexMarker728"/>modifying here is responsible for putting sanitizer passes into the pass pipeline. To have a better understanding of this code, let's focus on two of its components:</p><p>i. <code>PassBuilder</code>: This class provides predefined pass pipeline configurations for each optimization level – that is, the O0 ~ O3 notations (as well as Os and Oz for size optimization) we are familiar with. In addition to these predefined layouts, developers <a id="_idIndexMarker729"/>are free to customize the pipeline by leveraging the <code>PassBuilder</code> supports several EPs, such as at the <em class="italic">beginning</em> of the pipeline, at the <em class="italic">end</em> of the pipeline, or at the end of the vectorization process, to name a few. An example of using EP can be found in the preceding code, where we used the <code>PassBuilder::registerOptimizerLastEPCallback</code> method and a lambda function to customize the EP located at the <em class="italic">end</em> of the Pass pipeline. The lambda function has two arguments: <code>ModulePassManager</code> – which represents the pass pipeline – and the current optimization level. Developers can use <code>ModulePassManager::addPass</code> to insert arbitrary LLVM passes into this EP.</p><p>ii. <code>ModulePassManager</code>: This class represents a Pass pipeline – or, more specifically, the pipeline for <code>Module</code>. There are, of course, other PassManager classes for different IR units, such as <code>FunctionPassManager</code> for <code>Function</code>. </p><p>In the <a id="_idIndexMarker730"/>preceding code, we were trying to use the <code>ModulePassManager</code> instance to insert our <code>LoopCounterSanitizer</code> pass whenever <code>SanitizerKind::LoopCounter</code> was one of the sanitizers that had been designated by the user. Since <code>LoopCounterSanitizer</code> is a loop pass rather than a module pass, we need to add some <em class="italic">adaptors</em> between the pass and PassManager. The <code>createFunctionToLoopPassAdaptor</code> and <code>createModuleToFunctionPassAdaptor</code> functions we were using here created a special instance that adapts a pass to a PassManager of a different IR unit.</p><p>This is all the program logic that supports our LPCSan in the Clang compilation pipeline.</p></li>
				<li>Last but not least, we must make a small modification to the build system. Open the <code>runtime/CMakeLists.txt</code> file and change the following CMake variable:<pre>…
set(<code>COMPILER_RT_RUNTIMES</code> effectively imports our LPCSan Compiler-RT libraries into the build.</p></li>
			</ol>
			<p>These are all the steps necessary to support the LPCSan in Clang. Now, we can finally use the LPCSan in the same way we showed you at the beginning of this section:</p>
			<pre>$ clang -O1 <strong class="bold">-fsanitize=loop-counter</strong> input.c -o input</pre>
			<p>In this section, we learned how to create a sanitizer. A sanitizer is a useful tool for capturing runtime behaviors without modifying the original program code. The ability to create a <a id="_idIndexMarker731"/>sanitizer increases the flexibility for compiler developers to create custom diagnosing tools tailored for their own use cases. Developing a sanitizer requires comprehensive knowledge of Clang, LLVM, and Compiler-RT: creating a new LLVM pass, making a new Compiler-RT component, and customizing the compilation pipeline in Clang. You can use the content in this section, to reinforce what you've learned in previous chapters of this book. </p>
			<p>In the last <a id="_idIndexMarker732"/>section of this chapter, we are going to look at one more instrumentation technique: <strong class="bold">PGO</strong>.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor180"/>Working with PGO</h1>
			<p>In the previous section, we learned how a sanitizer assists developers in performing sanity checks <a id="_idIndexMarker733"/>with higher precision using data that is only available at runtime. We also learned how to create a custom sanitizer. In this section, we will follow up on the idea of leveraging runtime data. We are going to learn an alternative use for such information – using it for compiler optimization.</p>
			<p>PGO is a technique that uses statistics that have been collected during runtime to enable more aggressive compiler optimizations. The <em class="italic">profile</em> in its name refers to the runtime data that's been collected. To give you an idea of how such data enhances an optimization, let's assume we have the following C code:</p>
			<pre>void foo(int N) {
  if (<strong class="bold">N &gt; 100</strong>)
    bar();
  else
    zoo();
}</pre>
			<p>In this code, we have three functions: <code>foo</code>, <code>bar</code>, and <code>zoo</code>. The first function conditionally calls the latter two.</p>
			<p>When we try to optimize this code, the optimizer usually tries to <em class="italic">inline</em> callee functions into the caller. In this case, <code>bar</code> or <code>zoo</code> might be inlined into <code>foo</code>. However, if either <code>bar</code> or <code>zoo</code> has a large function body, inlining <em class="italic">both</em> might bloat the size of the final binary. Ideally, it will be great if we could inline only the one that executes the most frequently. Sadly, from a statistics point of view, we have no clue about which function has the highest execution frequency, because the <code>foo</code> function conditionally calls either of them based on a (non-constant) variable.</p>
			<p>With PGO, we <a id="_idIndexMarker734"/>can collect the execution frequencies of both <code>bar</code> and <code>zoo</code> at runtime and use the data to compile (and optimize) the same code <em class="italic">again</em>. The following diagram shows the high-level overview of this idea:</p>
			<div><div><img src="img/B14590_12.5.jpg" alt="Figure 12.5 – PGO workflow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.5 – PGO workflow</p>
			<p>Here, the first compilation phase compiled and optimized the code normally. After we executed the compiled program (an arbitrary number of times), we were able to collect the profile data files. In the second compilation phase, we not only optimized the code, as we did previously, but also integrated the profile data into the optimizations to make them act more aggressively.</p>
			<p>There are <a id="_idIndexMarker735"/>primarily two ways for PGO to collect runtime profiling data: inserting <strong class="bold">instrumentation</strong> code or leveraging <strong class="bold">sampling</strong> data. Let's introduce both.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor181"/>Introduction to instrumentation-based PGO</h2>
			<p>Instrumentation-based PGO inserts instrumentation code into the target program during the <a id="_idIndexMarker736"/>first compilation phase. This code measures the execution frequency of the program constructions we're interested in – for example, basic blocks and functions – and writes the result in a file. This is similar to how a sanitizer works.</p>
			<p>Instrumentation-based PGO usually generates profiling data with higher precision. This is because the compiler can insert instrumentation code in a way that provides the greatest benefit for other optimizations. However, just like the sanitizer, instrumentation-based PGO changes the execution flow of the target program, which increases the risk of performance regression (for the binary that was generated from the first compilation phase).</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor182"/>Introduction to sampling-based PGO</h2>
			<p>Sampling-based PGO uses <em class="italic">external</em> tools to collect profiling data. Developers use profilers such as <code>perf</code> or <code>valgrind</code> to diagnose performance issues. These tools usually leverage <a id="_idIndexMarker737"/>advanced system features or even hardware features to collect the runtime behavior of a program. For example, <code>perf</code> can give you insights into branch prediction and cache line misses.</p>
			<p>Since we are leveraging data from other tools, there is no need to modify the original code to collect profiles. Therefore, sampling-based PGO usually has an extremely low runtime overhead (usually, this is less than 1%). Also, we don't need to recompile the code for profiling purposes. However, profiling data that's generated in this way is usually less precise. It's also more difficult to map the profiling data back to the original code during the second compilation phase.</p>
			<p>In the rest of this section, we are going to focus on instrumentation-based PGO. We are going to learn how to leverage it with LLVM IR. Nevertheless, as we will see shortly, these two PGO strategies in LLVM share lots of common infrastructures, so the code is portable. Here is the list of topics we are going to cover:</p>
			<ul>
				<li>Working with profiling data</li>
				<li>Learning about the APIs for accessing profiling data</li>
			</ul>
			<p>The first topic <a id="_idIndexMarker738"/>will show us how to create and use instrumentation-based PGO profiles with Clang, as well as some of the tools that can help us inspect and modify profiling data. The second topic will give you more details on how to access profiling data using LLVM APIs. This is useful if you want to create your own PGO pass.</p>
			<h3>Working with profiling data</h3>
			<p>In this section, we are <a id="_idIndexMarker739"/>going to learn how to use generate, inspect, and even modify instrumentation-based profiling data. Let's start with the following example:</p>
			<pre>__attribute__((noinline))
void foo(int x) {
  if (<strong class="bold">get_random() &gt; 5</strong>)
    printf("Hello %d\n", x * 3);
}
int main(int argc, char **argv) {
  <strong class="bold">for (int i = 0; i &lt; argc + 10; ++i)</strong> {
    foo(i);
  }
  return 0;
}</pre>
			<p>In the preceding code, <code>get_random</code> is a function that generates a random number from 1 to 10 with uniform distribution. In other words, the highlighted <code>if</code> statement in the <code>foo</code> function should have a 50% chance of being taken. In addition to the <code>foo</code> function, the trip count of the <code>for</code> loop within <code>main</code> depends on the number of command-line arguments there are.</p>
			<p>Now, let's try to build this code with instrumentation-based PGO. Here are the steps:</p>
			<ol>
				<li value="1">The first thing we are going to do is generate an executable for PGO profiling. Here is the command:<pre>$ clang -O1 <code>-fprofile-generate</code> option enables instrumentation-based PGO. The path that we added after this flag is the directory where profiling data will be stored.</p></li>
				<li>Next, we must <a id="_idIndexMarker740"/>run the <code>pgo</code> program with three command-line arguments:<pre>$ ./pgo `seq 1 3`
Hello 0
Hello 6
…
Hello 36
Hello 39
$</pre><p>You might get a totally different output since there is only a 50% of chance of the string being printed.</p><p>After this, the <code>pgo_prof.dir</code> folder should contain the <code>default_&lt;hash&gt;_&lt;n&gt;.profraw</code> file, as shown here:</p><pre>$ ls pgo_prof.dir
default_10799426541722168222_0.profraw</pre><p>The <em class="italic">hash</em> in the filename is a hash that's calculated based on your code.</p></li>
				<li>We cannot directly use the <code>*.profraw</code> file for our second compilation phase. Instead, we must convert it into another kind of binary form using the <code>llvm-profdata</code> tool. Here is the command:<pre>$ llvm-profdata <code>llvm-profdata</code> is a powerful tool for inspecting, converting, and merging profiling data files. We will look at it in more detail later. In the preceding command, we are merging and converting all the data files under <code>pgo_prof.dir</code> into a <em class="italic">single</em> <code>*.profdata</code> file.</p></li>
				<li>Finally, we can <a id="_idIndexMarker741"/>use the file we just merged for the second stage of compilation. Here is the command:<pre>$ clang -O1 <strong class="bold">-fprofile-use=pgo_prof.profdata</strong> pgo.cpp \
        -emit-llvm -S -o pgo.after.ll</pre></li>
			</ol>
			<p>Here, the <code>-fprofile-use</code> option told <code>clang</code> to use the profiling data stored in <code>pgo_prof.profdata</code> to optimize the code. We are going to look at the LLVM IR code after we've done this optimization.</p>
			<p>Open <code>pgo.after.ll</code> and navigate to the <code>foo</code> function. Here is a simplified version of <code>foo</code>:</p>
			<pre>define void @foo(i32 %x) <strong class="bold">!prof !71</strong> {
entry:
  %call = call i32 @get_random()
  %cmp = icmp sgt i32 %call, 5
  br i1 %cmp, label %if.then, label %if.end, <strong class="bold">!prof !72</strong>
if.then:                                          
  %mul = mul nsw i32 %x, 3
  …
}</pre>
			<p>In the preceding LLVM IR code, two places were different from the original IR; that is, the <code>!prof</code> tags that followed after both the function header and the branch instruction, which correspond to the <code>if(get_random() &gt; 5)</code> code we saw earlier.</p>
			<p>In LLVM IR, we can attach <code>'!'</code>) in the textual LLVM IR.<code>!prof</code>, <code>!71</code>, and <code>!72</code> in the preceding code are metadata tags that represent the profiling data we collected. More specifically, if we have profiling data associated with an IR unit, it always starts with <code>!prof</code>, followed by another metadata tag that contains the required values. These metadata values are put at the very bottom of the IR file. If we navigate there, we will see the content of <code>!71</code> and <code>!72</code>. Here is the code:</p>
			<pre>!71 = !{!"<strong class="bold">function_entry_count</strong>", i64 110}
!72 = !{!"<strong class="bold">branch_weights</strong>", i32 57, i32 54}</pre>
			<p>These two <a id="_idIndexMarker742"/>metadata are tuples with two and three elements. <code>!71</code>, as suggested by its first element, represents the number of times the <code>foo</code> function was called (in this case, it was called 110 times).</p>
			<p>On the other hand,<code>!72</code> marks the number of times each branch in the <code>if(get_random() &gt; 5)</code> statement was taken. In this case, the true branch was taken 57 times and the false branch was taken 54 times. We got these numbers because we were using uniform distribution for random number generation (namely, a ~50% chance for each branch).</p>
			<p>In the second part of this section, we will learn how to access these values for the sake of developing a more aggressive compiler optimization. Before we do that, though, let's take a deeper look at the profiling data file we just collected.</p>
			<p>The <code>llvm-profdata</code> tool we just used can not only help us convert the format of the profiling data, but also gives us a quick preview of its content. The following command prints out the summary for <code>pgo_prof.profdata</code>, including the profiling values that were collected from every function:</p>
			<pre>$ llvm-profdata <strong class="bold">show –-all-functions –-counts</strong> pgo_prof.profdata
…
  <strong class="bold">foo:</strong>
    Hash: 0x0ae15a44542b0f02
    Counters: 2
    <strong class="bold">Block counts: [54, 57]</strong>
  main:
    Hash: 0x0209aa3e1d398548
    Counters: 2
    Block counts: [110, 1]
…
Instrumentation level: IR  entry_first = 0
Functions shown: 9
Total functions: 9
Maximum function count: …
Maximum internal block count: …</pre>
			<p>Here, we can <a id="_idIndexMarker743"/>see the profiling data entries for each function. Each entry has a list of numbers representing the <em class="italic">execution frequency</em> of all the enclosing basic blocks.</p>
			<p>Alternatively, you can inspect the same profiling data file by converting it into a textual file first. Here is the command:</p>
			<pre>$ llvm-profdata merge <strong class="bold">–-text</strong> pgo_prof.profdata -o pgo_prof.proftext
$ cat pgo_prof.proftext
# IR level Instrumentation Flag
:ir
…
<strong class="bold">foo</strong>
# Func Hash:
784007059655560962
# Num Counters:
2
<strong class="bold"># Counter Values:</strong>
54
57
…</pre>
			<p>The <code>*.proftext</code> file is in a human-readable textual format where all the profiling data is simply put in its own line.</p>
			<p>This textual <a id="_idIndexMarker744"/>representation can actually be converted <em class="italic">back</em> into the <code>*.profdata</code> format using a similar command. Here is an example:</p>
			<pre>$ llvm-profdata merge <strong class="bold">–-binary</strong> pgo_prof.proftext -o pgo_prof.profdata</pre>
			<p>Therefore, <code>*.proftext</code> is especially useful when you want to <em class="italic">edit</em> the profiling data manually.</p>
			<p>Before we dive into the APIs for PGO, there is one more concept we need to learn about: the instrumentation level.</p>
			<h3>Understanding the instrumentation level</h3>
			<p>So far, we've <a id="_idIndexMarker745"/>learned that instrumentation-based PGO can insert instrumentation code for collecting runtime profiling data. On top of this fact, the places where we insert this instrumentation code and its <a id="_idIndexMarker746"/>granularity also matter. This property is called the <strong class="bold">instrumentation level</strong> in instrumentation-based PGO. LLVM currently supports three different instrumentation levels. Here are descriptions of each:</p>
			<ul>
				<li><code>-fprofile-generate</code> command-line option we introduced earlier will generate profiling data with this instrumentation level. For example, let's say we have the following C code:<pre>void foo(int x) {
  if (x &gt; 10)
    puts("hello");
  else
    puts("world");
}</pre><p>The <a id="_idIndexMarker747"/>corresponding IR – without enabling instrumentation-based PGO – is shown here:</p><pre>define void @foo(i32 %0) {
  …
  %4 = icmp sgt i32 %3, 10
  <code>%5</code> or <code>%7</code>. Now, let's generate the IR with instrumentation-based PGO enabled with the following command:</p><pre>$ clang <code>@__profc_foo.0</code> or <code>@__profc_foo.1</code> – by one. The values in these two variables will eventually be exported as the profiling data for branches, representing the number of times each branch was taken. </p><p>This instrumentation level provides decent precision but suffers from compiler changes. More specifically, if Clang changes the way it emits LLVM IR, the places where the instrumentation code will be inserted will also be different. This effectively means that for the same input code, the profiling data that's generated with an older version of LLVM might be incompatible with the profiling data that's generated with a newer LLVM.</p></li>
				<li><code>Stmt</code> AST node inside an <code>IfStmt</code> (an AST node). With this method, the instrumentation code is barely affected by compiler changes and we can have a more stable profiling data format across different compiler versions. The downside of this instrumentation level is that it has less precision than the IR instrumentation level.<p>You can <a id="_idIndexMarker749"/>adopt this instrumentation level by simply using the <code>-fprofile-instr-generate</code> command-line option in place of <code>-fprofile-generate</code> when invoking <code>clang</code> for the first compilation. You don't need to change the command for the second compilation, though. </p></li>
				<li><code>clang</code> with two PGO command-line options, <code>-fprofile-use</code> and <code>-fcs-profile-generate</code>, with the path to the profiling file from the previous step and the prospective <a id="_idIndexMarker750"/>output path, respectively. When we use <code>llvm-profdata</code> to do the post-processing, we are merging all the profiling data files we have:</p><pre>$ clang <strong class="bold">-fprofile-use=combined_prof.profdata</strong> \
        foo.c -o optimized_foo</pre><p>Finally, feed the combined profiling file into Clang so that it can use this context-sensitive profiling data to get a more accurate portrait of the program's runtime behavior.</p></li>
			</ul>
			<p>Note that different instrumentation levels only affect the accuracy of the profiling data; they don't affect how we <em class="italic">retrieve</em> this data, which we are going to talk about in the next section.</p>
			<p>In the last <a id="_idIndexMarker751"/>part of this section, we are going to learn how to access this profiling data inside an LLVM pass via the APIs provided by LLVM.</p>
			<h3>Learning about the APIs for accessing profiling data</h3>
			<p>In the previous section, we learned how to run the instrumentation-based PGO using Clang <a id="_idIndexMarker752"/>and view the profiling data file using <code>llvm-profdata</code>. In this section, we are going to learn how to access that data within an LLVM pass to help us develop our own PGO.</p>
			<p>Before we go into the development details, let's learn how to <em class="italic">consume</em> those profiling data files into <code>opt</code>, since it's easier to test individual LLVM pass using it. Here is a sample command:</p>
			<pre>$ opt <strong class="bold">-pgo-test-profile-file=pgo_prof.profdata</strong> \
      --passes="<strong class="bold">pgo-instr-use</strong>,my-pass…" pgo.ll …</pre>
			<p>There are two keys in the preceding command:</p>
			<ul>
				<li>Use <code>-pgo-test-profile-file</code> to designate the profiling data file you want to put in.</li>
				<li>The "<code>pgo-instr-use</code>" string represents the <code>PGOInstrumentaitonUse</code> pass, which reads (instrumentation-based) profiling files and <em class="italic">annotates</em> the data on an LLVM IR. However, it is not run by default, even in the predefined optimization levels (that is, O0 ~ O3, Os, and Oz). Without this pass being ahead in the Pass pipeline, we are unable to access any profiling data. Therefore, we need to explicitly add it to the optimization pipeline. The preceding sample command demonstrated how to run it before a custom LLVM pass, <code>my-pass</code>, in the pipeline. If you want to run it before any of the predefined optimization pipelines – for instance, O1 – you must specify the <code>--passes="pgo-instr-use,default&lt;O1&gt;"</code> command-line option.</li>
			</ul>
			<p>Now, you might be wondering, what happens after the profiling data is read into <code>opt</code>? It turns out that the LLVM IR file that was generated by the <em class="italic">second</em> compilation phase – <code>pgo.after.ll</code> – has provided us with some answers to this question.</p>
			<p>In <code>pgo.after.ll</code>, we saw that some branches were <em class="italic">decorated</em> with <strong class="bold">metadata</strong> specifying the number of times they were taken. Similar metadata appeared in functions, which represented the total number of times those functions were called. </p>
			<p>More generally speaking, LLVM directly <em class="italic">combines</em> the profiling data – read from the file – with its associated IR constructions via <strong class="bold">metadata</strong>. The biggest advantage of this strategy is that we don't need to carry the raw profiling data throughout the entire optimization pipeline – the IR itself contains this profiling information.</p>
			<p>Now, the question becomes, how can we access metadata that's been attached to IR? LLVM's metadata can be attached to many kinds of IR units. Let's take a look at the most common <a id="_idIndexMarker753"/>one first: accessing metadata attached to an <code>Instruction</code>. The following code shows us how to read the profiling metadata –<code>!prof !71</code>, which we saw previously – that's attached to a branch instruction:</p>
			<pre>// `BB` has the type of `BasicBlock&amp;`
Instruction *BranchInst = BB.getTerminator();
<strong class="bold">MDNode</strong> *<strong class="bold">BrWeightMD</strong> = BranchInst-&gt;getMetadata(<strong class="bold">LLVMContext::MD_prof</strong>);</pre>
			<p>In the preceding snippet, we are using <code>BasicBlock::getTerminator</code> to get the last instruction in a basic block, which is a branch instruction most of the time. Then, we tried to retrieve the profiling metadata with the <code>MD_prof</code> metadata. <code>BrWeightMD</code> is the result we are looking for.</p>
			<p>The type of <code>BrWeightMD</code>, <code>MDNode</code>, represents a single metadata node. Different <code>MDNode</code> instances can be <em class="italic">composed</em> together. More specifically, a <code>MDNode</code> instance can use other <code>MDNode</code> instances as its operands – similar to the <code>Value</code> and <code>User</code> instances we saw in <a href="B14590_10_Final_JC_ePub.xhtml#_idTextAnchor141"><em class="italic">Chapter 10</em></a>, <em class="italic">Processing LLVM IR</em>. The compound <code>MDNode</code> can express more complex concepts.</p>
			<p>For example, in this case, each operand in <code>BrWeightMD</code> represents the number of times each branch was taken. Here is the code to access them:</p>
			<pre>if (BrWeightMD-&gt;getNumOperands() &gt; 2) {
  // Taken counts for true branch
  <strong class="bold">MDNode</strong> *TrueBranchMD = BrWeightMD-&gt;<strong class="bold">getOperand</strong>(1);
  // Taken counts for false branch
  <strong class="bold">MDNode</strong> *FalseBranchMD = BrWeightMD-&gt;getOperand(2);
}</pre>
			<p>As you can see, the taken counts are also expressed as <code>MDNode</code>. </p>
			<p class="callout-heading">Operand indices for both branches</p>
			<p class="callout">Note that the data for both branches is placed at the operands starting from index 1 rather than index 0.</p>
			<p>If we <a id="_idIndexMarker754"/>want to convert these branch <code>MDNode</code> instances into constants, we can leverage a small utility provided by the <code>mdconst</code> namespace. Here is an example:</p>
			<pre>if (BrWeightMD-&gt;getNumOperands() &gt; 2) {
  // Taken counts for true branch
  MDNode *TrueBranchMD = BrWeightMD-&gt;getOperand(1);
  <strong class="bold">ConstantInt</strong> *NumTrueBrTaken
    = <strong class="bold">mdconst::dyn_extract&lt;ConstantInt&gt;</strong>(TrueBranchMD);
  …
}</pre>
			<p>The previous code <em class="italic">unwrapped</em> an <code>MDNode</code> instance and extracted the underlying <code>ConstantInt</code> instance. </p>
			<p>For <code>Function</code>, we can get the number of times it was called in an even easier way. Here is the code:</p>
			<pre>// `F` has the type of `Function&amp;`
<strong class="bold">Function::ProfileCount</strong> EntryCount = F.getEntryCount();
uint64_t EntryCountVal = EntryCount.<strong class="bold">getCount</strong>();</pre>
			<p><code>Function</code> is using a slightly different way to present its called frequency. But retrieving the numerical profiling value is still pretty easy, as shown in the preceding snippet.</p>
			<p>It is worth noting that although we only focused on instrumentation-based PGO here, for <em class="italic">sampling</em>-based PGO, LLVM also uses the same programming interface to expose its data. In other words, even if you're using profiling data that's been collected from sampling tools with a different <code>opt</code> command, the profiling data will also be annotated on IR units and you can still access it using the aforementioned method. In fact, the tools and APIs we are going to introduce in the rest of this section are mostly profiling-data-source agnostic.</p>
			<p>So far, we have been dealing with real values that have been retrieved from the profiling data. However, these low-level values cannot help us go far in terms of developing a compiler <a id="_idIndexMarker755"/>optimization or program analysis algorithm – usually, we are more interested in high-level concepts such as "functions that are executed <em class="italic">most frequently</em>" or "branches that are <em class="italic">least taken</em>". To address these demands, LLVM builds several analyses on top of profiling data to deliver such high-level, structural information.</p>
			<p>In the next section, we are going to introduce some of these analyses and their usages in LLVM Pass.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor183"/>Using profiling data analyses</h2>
			<p>In this section, we are <a id="_idIndexMarker756"/>going to learn three analysis classes that can help us reason about the execution frequency of basic blocks and functions at runtime. They are as follows:</p>
			<ul>
				<li><code>BranchProbabilityInfo</code></li>
				<li><code>BlockFrequencyInfo</code></li>
				<li><code>ProfileSummaryInfo</code></li>
			</ul>
			<p>This list is ordered by their analyzing scope in the IR – from local to global. Let's start with <a id="_idIndexMarker757"/>the first two.</p>
			<h3>Using BranchProbabilityInfo and BlockFrequencyInfo</h3>
			<p>In the <a id="_idIndexMarker758"/>previous <a id="_idIndexMarker759"/>section, we <a id="_idIndexMarker760"/>learned how to access the profiling metadata that's attached to each branch instruction – the <code>BranchProbabilityInfo</code> class. Here is some example code showing how to use it in a (function) Pass:</p>
			<pre>#include "llvm/Analysis/BranchProbabilityInfo.h"
PreservedAnalyses run(Function &amp;F, FunctionAnalysisManager &amp;FAM) {
  <strong class="bold">BranchProbabilityInfo</strong> &amp;BPI
    = FAM.getResult&lt;<strong class="bold">BranchProbabilityAnalysis</strong>&gt;(F);
  BasicBlock *Entry = F.getEntryBlock();
  BranchProbability BP = BPI.<strong class="bold">getEdgeProbability</strong>(Entry, 0);
  …
}</pre>
			<p>The previous code retrieved a <code>BranchProbabilityInfo</code> instance, which is the result of <code>BranchProbabilityAnalysis</code>, and tried to get the weight from the entry block to its first successor block.</p>
			<p>The returned <a id="_idIndexMarker762"/>value, a <code>BranchProbability</code> instance, gives <a id="_idIndexMarker763"/>you the branch's probability <a id="_idIndexMarker764"/>in the form of a percentage. You <a id="_idIndexMarker765"/>can use <code>BranchProbability::getNumerator</code> to retrieve the value (the "denominator" is 100 by default). The <code>BranchProbability</code> class also provides some handy utility methods for performing arithmetic between two branch probabilities or scaling the probability by a specific factor. Although we can easily tell which branch is more likely to be taken using <code>BranchProbabilityInfo</code>, without additional data, we can't tell the branch's probability (to be taken) in the <em class="italic">whole function</em>. For example, let's assume we have the following CFG:</p>
			<div><div><img src="img/B14590_12.6.jpg" alt="Figure 12.6 – CFG with nested branches&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.6 – CFG with nested branches</p>
			<p>For the preceding diagram, we have profiling counter values for the following basic blocks:</p>
			<ul>
				<li><strong class="bold">if.then4</strong>: 2</li>
				<li><strong class="bold">if.else</strong>: 10</li>
				<li><strong class="bold">if.else7</strong>: 20</li>
			</ul>
			<p>If we <em class="italic">only</em> look at the branch weight metadata toward blocks <code>if.then4</code> and <code>if.else</code> – that is, the true and false branches for <code>if.then</code>, respectively – we might come under the <em class="italic">illusion</em> that the <code>if.else</code> block has a ~83% chance of being taken. But the truth is, it only has a ~31% chance because the control flow has a higher probability to go into <code>if.else7</code> before even <a id="_idIndexMarker766"/>entering the <code>if.then</code> region. Of <a id="_idIndexMarker767"/>course, in this case, we can do simple math to <a id="_idIndexMarker768"/>figure out the correct answer, but when the CFG is <a id="_idIndexMarker769"/>getting bigger and more complex, we might have a hard time doing this by ourselves.</p>
			<p>The <code>BlockFrequencyInfo</code> class provides a shortcut to this problem. It can tell us the frequency of each basic block to be taken under the context of its enclosing function. Here is an example of its usage in a Pass:</p>
			<pre>#include "llvm/Analysis/BlockFrequencyInfo.h"
PreservedAnalyses run(Function &amp;F, FunctionAnalysisManager &amp;FAM) {
  <strong class="bold">BlockFrequencyInfo</strong> &amp;BFI
    = FAM.getResult&lt;<strong class="bold">BlockFrequencyAnalysis</strong>&gt;(F);
  for (BasicBlock *BB : F) {
    <strong class="bold">BlockFrequency</strong> BF = BFI.<strong class="bold">getBlockFreq</strong>(BB);
  }
  …
}</pre>
			<p>The previous code retrieved a <code>BlockFrequencyInfo</code> instance, which is the result of <code>BlockFrequencyAnalysis</code>, and tried to evaluate the block frequency of each basic block in the function.</p>
			<p>Similar to the <code>BranchProbability</code> class, <code>BlockFrequency</code> also provides nice utility methods to calculate with other <code>BlockFrequency</code> instances. But different from <code>BranchProbability</code>, the numeric value that's retrieved from <code>BlockFrequency</code> is not presented <a id="_idIndexMarker770"/>as a percentage. More <a id="_idIndexMarker771"/>specifically, <code>BlockFrequency::getFrequency</code> returns an integer that is the frequency <em class="italic">relative</em> to the entry <a id="_idIndexMarker772"/>block of the current function. In other words, to get <a id="_idIndexMarker773"/>a percentage-based frequency, we can use the following snippet:</p>
			<pre>// `BB` has the type of `BasicBlock*`
// `Entry` has the type of `BasicBlock*` and represents entry // block
BlockFrequency BBFreq = BFI.getBlockFreq(BB),
               EntryFreq = BFI.getBlockFreq(Entry);
auto <strong class="bold">FreqInPercent</strong>
  = (BBFreq.<strong class="bold">getFrequency</strong>() / EntryFreq.<strong class="bold">getFrequency</strong>()) * 100;</pre>
			<p>The highlighted <code>FreqInPercent</code> is the block frequency of <code>BB</code>, expressed as a percentage.</p>
			<p><code>BlockFrequencyInfo</code> calculates the frequency of a specific basic block under the context of a function – but what about the entire <em class="italic">module</em>? More specifically, if we bring a <code>ProfileSummaryInfo</code>.</p>
			<h3>Using ProfileSummaryInfo</h3>
			<p>The <code>ProfileSummaryInfo</code> class gives you a global view of all the profiling data in a <code>Module</code>. Here <a id="_idIndexMarker775"/>is an example of retrieving <a id="_idIndexMarker776"/>an instance of it inside a module Pass:</p>
			<pre>#include "llvm/Analysis/ProfileSummaryInfo.h"
PreservedAnalyses run(Module &amp;M, ModuleAnalysisManager &amp;MAM) {
  <strong class="bold">ProfileSummaryInfo</strong> &amp;PSI = MAM.  getResult&lt;<strong class="bold">ProfileSummaryAnalysis</strong>&gt;(M);
  …
}</pre>
			<p><code>ProfileSummaryInfo</code> provides <a id="_idIndexMarker777"/>a wide variety of functionalities. Let's take a look at three of its most interesting methods:</p>
			<ul>
				<li><code>isFunctionEntryCold/Hot(Function*)</code>: These two methods compare the entry count of a <code>Function</code> – which effectively reflects the number of times a function was called –against that of other functions in the same module and tell us if the inquiry function is ranking high or low in this metric.</li>
				<li><code>isHot/ColdBlock(BasicBlock*, BlockFrequencyInfo&amp;)</code>: These two methods work similarly to the previous bullet one but compare the execution frequency of a <code>BasicBlock</code> against <em class="italic">all</em> the other blocks in the module.</li>
				<li><code>isFunctionCold/HotInCallGraph(Function*, BlockFrequencyInfo&amp;)</code>: These two methods combine the methods from the previous two bullet points they can tell you whether a function is considered hot or cold based on its entry count or the execution frequency of its enclosing basic blocks. This is useful when a function has a low entry count – that is, it was not called often – but contains a <em class="italic">loop</em> that has extremely a high basic block execution frequency. In this case, the <code>isFunctionHotInCallGraph</code> method can give us a more accurate assessment.</li>
			</ul>
			<p>These APIs also have variants where you can designate the cutoff point as being "hot" or "cold." Please refer to the API documentation for more information.</p>
			<p>For a long time, the compiler was only able to analyze and optimize the source code with a static view. For dynamic factors inside a program – for instance, the branch taken count – compilers could only make an approximation. PGO opened an alternative path to provide <a id="_idIndexMarker778"/>extra information for compilers <a id="_idIndexMarker779"/>to peek into the target program's runtime behavior, for the sake of making less ambiguous and more aggressive decisions. In this section, we learned how to collect and use runtime profiling information – the key to PGO – with LLVM. We learned how to use the related infrastructure in LLVM to collect and generate such profiling data. We also learned about the programming interface we can use to access that data – as well as some high-level analyses built on top of it – to assist our development inside an LLVM Pass. With these abilities, LLVM developers can plug in this runtime information to further improve the quality and precision of their existing optimization Passes.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor184"/>Summary</h1>
			<p>In this chapter, we <em class="italic">augmented</em> the workspace of the compiler by processing the static source code and capturing the program's runtime behaviors. In the first part of this chapter, we learned how to use the infrastructure provided by LLVM to create a sanitizer – a technique that inserts instrumentation code into the target program for the sake of checking certain runtime properties. By using a sanitizer, software engineers can improve their development quality with ease and with high precision. In the second part of this chapter, we extended the usages of such runtime data to the domain of compiler optimization; PGO is a technique that uses dynamic information, such as the execution frequency of basic blocks or functions, to make more aggressive decisions for optimizing the code. Finally, we learned how to access such data with an LLVM Pass, which enables us to add PGO enhancement to existing optimizations.</p>
			<p>Congratulations, you've just finished the last chapter! Thank you so much for reading this book. Compiler development has never been an easy subject – if not an obscure one – in computer science. In the past decade, LLVM has significantly lowered the difficulties of this subject by providing robust yet flexible modules that fundamentally change how people think about compilers. A compiler is not just a single executable such as <code>gcc</code> or <code>clang</code> anymore – it is a collection of building blocks that provide developers with <em class="italic">countless</em> ways to create tools to deal with hard problems in the programming language field.</p>
			<p>However, with so many choices, I often became lost and confused when I was still a newbie to LLVM. There was documentation for every single API in this project, but I had no idea how to put them together. I wished there was a book that pointed in the general direction of each important component in LLVM, telling me <em class="italic">what</em> it is and <em class="italic">how</em> I can take advantage of it. And here it is, the book I wished I could have had at the beginning of my LLVM career – the book you just finished – come to life. I hope you won't stop your expedition of LLVM after finishing this book. To improve your skills even further and reinforce what you've learned from this book, I recommend you to check out the official document pages (<a href="https://llvm.org/docs">https://llvm.org/docs</a>) for content that complements this book. More importantly, I encourage you to participate in the LLVM community via either their mailing list (<a href="https://lists.llvm.org/cgi-bin/mailman/listinfo/llvm-dev">https://lists.llvm.org/cgi-bin/mailman/listinfo/llvm-dev</a>) or Discourse forum (https://llvm.discourse.group/), especially the first one – although a mailing list might sound old-school, there are many talented people there willing to answer your questions and provide useful learning resources. Last but not least, annual LLVM dev meetings (<a href="https://llvm.org/devmtg/">https://llvm.org/devmtg/</a>), in both the United States and Europe, are some of the best events where you can learn new LLVM skills and chat face-to-face with people who literally built LLVM.</p>
			<p>I hope this book enlightened you on your path to mastering LLVM and helped you find joy in crafting compilers.</p>
		</div>
	</body></html>