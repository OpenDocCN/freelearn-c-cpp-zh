- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrent System Programming with C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will look at what concurrency means and how it is different
    from parallelism. We will go through the fundamentals and the theory behind processes
    and threads. We will look at the changes in the C++ memory model, which enforce
    native concurrency support in the language. We will also familiarize ourselves
    with what a race condition is, how it can lead to a data race, and how to prevent
    data races. Next, we will get acquainted with the C++20 `std::jthread` primitive,
    which enables multithreading support. We will learn about the specifics of the
    `std::jthread` class and how we can stop already running `std::jthread` instances
    by using the `std::stop_source` primitive. Finally, we will learn how to synchronize
    the execution of concurrent code and how to report calculation results from executed
    tasks. We will learn how to use C++ synchronization primitives such as *barriers*
    and *latches* to synchronize the execution of concurrent tasks, and how to properly
    report the result of these tasks using *promises* and *futures*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, we will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is concurrency?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread versus process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency with C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demystifying race conditions and data races
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical multithreading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing data during parallel execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All examples in this chapter have been tested in an environment with the following
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: Linux Mint 21 Cinnamon Edition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCC 12.2 with compiler flags – `-``std=c++20 -pthread`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A stable internet connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please make sure your environment is at least this recent. For all the examples,
    you can alternatively use [https://godbolt.org/](https://godbolt.org/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All code examples in this chapter are available to download from [https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%206](https://github.com/PacktPublishing/C-Programming-for-Linux-Systems/tree/main/Chapter%206).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is concurrency?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern cars have become highly intricate machines that provide not only transportation
    but also various other functionalities. These functionalities include infotainment
    systems, which allow users to play music and videos, and heating and air conditioning
    systems, which regulate the temperature for passengers. Consider a scenario in
    which these features did not work simultaneously. In such a case, the driver would
    have to choose between driving the car, listening to music, or staying in a comfortable
    climate. This is not what we expect from a car, right? We expect all of these
    features to be available at the same time, enhancing our driving experience and
    providing a comfortable trip. To achieve this, these features must operate in
    parallel.
  prefs: []
  type: TYPE_NORMAL
- en: But do they really run in parallel, or do they just run concurrently? Is there
    any difference?
  prefs: []
  type: TYPE_NORMAL
- en: In computer systems, **concurrency** and **parallelism** are similar in certain
    ways, but they are not the same. Imagine you have some work to do, but this work
    can be done in separate smaller chunks. Concurrency refers to the situation where
    multiple chunks of the work begin, execute, and finish during overlapping time
    intervals, without a guaranteed specific order of execution. On the other hand,
    parallelism is an execution policy where these chunks execute simultaneously on
    hardware with multiple computing resources, such as a multi-core processor.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency happens when multiple chunks of work, which we call **tasks**, are
    executed in an unspecified order for a certain period of time. The operating system
    could run some of the tasks and force the rest to wait. In concurrent execution,
    the task continuously strives for an execution slot because the operating system
    does not guarantee that it will execute all of them at once. Furthermore, it is
    highly possible that while a task is being executed, it is suddenly suspended,
    and another task starts executing. This is called **preemption**. It is clear
    that in concurrent task execution, the order of how the tasks will be executed
    is not guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get back to our car example. In modern cars, the infotainment system
    is responsible for performing many activities simultaneously. For example, it
    can run the navigation part while allowing you to listen to music. This is possible
    because the system runs these tasks concurrently. It runs the tasks related to
    route calculation while processing the music content. If the hardware system has
    a single core, then these tasks should run concurrently:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Concurrent task execution](img/Figure_6.01_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Concurrent task execution
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding figure, you can see that each task gets a non-deterministic
    execution time in an unpredictable order. In addition, there is no guarantee that
    your task will be finished before the next one is started. This is where the preemption
    happens. While your task is running, it is suddenly suspended, and another task
    is scheduled for execution. Keep in mind that task switching is not a cheap process.
    The system consumes the processor’s computation resource to perform this action
    – to make the context switch. The conclusion should be the following: we have
    to design our systems to respect these limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, parallelism is a form of concurrency that involves executing
    multiple operations simultaneously on *separate processing units*. For example,
    a computer with multiple CPUs can execute multiple tasks in parallel, which can
    lead to significant performance improvements. You don’t have to worry about the
    context switching and the preemption. It has its drawbacks, though, and we will
    discuss them thoroughly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Parallel task execution](img/Figure_6.02_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Parallel task execution
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our car example, if the CPU of the infotainment system is multi-core,
    then the tasks related to the navigation system could be executed on one core,
    and the tasks for the music processing on some of the other cores. Therefore,
    you don’t have to take any action to design your code to support preemption. Of
    course, this is only true if you are sure that your code will be executed in such
    an environment.
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental connection between concurrency and parallelism lies in the fact
    that parallelism can be applied to concurrent computations without affecting the
    accuracy of the outcome, but the presence of concurrency alone does not guarantee
    parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, concurrency is an important concept in computing that allows multiple
    tasks to be executed simultaneously, even though that is not guaranteed. This
    could lead to improved performance and efficient resource utilization but at the
    cost of more complicated code respecting the pitfalls that concurrency brings.
    On the other hand, truly parallel execution of code is easier to handle from a
    software perspective but must be supported by the underlying system.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will get familiar with the difference between execution
    threads and processes in Linux.
  prefs: []
  type: TYPE_NORMAL
- en: Threads versus processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Linux, a **process** is an instance of a program in execution. A process
    can have one or more threads of execution. A **thread** is a sequence of instructions
    that can proceed independently of other threads within the same process.
  prefs: []
  type: TYPE_NORMAL
- en: Each process has its own memory space, system resources, and execution context.
    Processes are isolated from each other and do not share memory by default. They
    can only communicate through files and **inter-process communication** (**IPC**)
    mechanisms, such as pipes, queues, sockets, shared memory, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: A thread, on the other hand, is a lightweight unit of execution within a process.
    The overhead of loading the instructions from non-volatile memory to RAM or even
    the cache is already paid for by the process creating the thread – the parent
    process. Each thread has its own stack and register values but shares the memory
    space and system resources of the parent process. Because threads share memory
    within the process, they can easily communicate with each other and synchronize
    their own execution. In general, this makes them more efficient than processes
    for concurrent execution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – IPC](img/Figure_6.03_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – IPC
  prefs: []
  type: TYPE_NORMAL
- en: 'The main differences between processes and threads are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource allocation**: Processes are independent entities that have their
    own memory space, system resources, and scheduling priority. On the other hand,
    threads share the same memory space and system resources as the process they belong
    to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creation and destruction**: Processes are created and destroyed by the operating
    system, while threads are created and managed by the process that they belong
    to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context switching**: When a context switch occurs, the operating system switches
    the entire process context, including all its threads. In contrast, a thread context
    switch only requires switching the state of the current thread, which, in general,
    is faster and less resource-intensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communication and synchronization**: IPC mechanisms such as pipes, queues,
    sockets, and shared memory are used to enable communication between processes.
    Threads, on the other hand, can communicate directly by sharing memory within
    the same process. This also enables efficient synchronization between threads,
    as they can use locks and other synchronization primitives to coordinate their
    access to shared resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Linux schedules tasks in the kernel, which are either threads or single-threaded
    processes. Each task is represented through a kernel thread; thus, the scheduler
    does not differentiate between a thread and a process.
  prefs: []
  type: TYPE_NORMAL
- en: Processes and threads have their analogy in real life. Let’s say you are working
    on a project with a group of people, and the project is divided into different
    tasks. Each task represents a unit of work that needs to be completed. You can
    think of the project as a process, and each task as a thread.
  prefs: []
  type: TYPE_NORMAL
- en: In this analogy, the process (project) is a collection of related tasks that
    need to be completed to achieve a common goal. Each task (thread) is a separate
    unit of work that can be assigned to a specific person to complete.
  prefs: []
  type: TYPE_NORMAL
- en: When you assign a task to someone, you are creating a new thread within the
    project (process). The person who is assigned the task (thread) can work on it
    independently, without interfering with the work of others. They may also communicate
    with other team members (threads) to coordinate their work, just as threads within
    a process can communicate with each other. They also need to use the common project
    resource to finish their tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if you divide the project into different projects, you create multiple
    processes. Each process has its own resources, team members, and goals. It is
    harder to ensure that both processes share a resource needed for the project to
    finish.
  prefs: []
  type: TYPE_NORMAL
- en: So, processes and threads in computing are like real-life projects and tasks,
    respectively. A process represents a collection of related tasks that need to
    be completed to achieve a common goal, while a thread is a separate unit of work
    that can be assigned to a specific person to complete.
  prefs: []
  type: TYPE_NORMAL
- en: In Linux, processes are separate instances of a program with their own memory
    and resources, while threads are lightweight execution units within a process
    that share the same memory and resources. Threads can communicate more efficiently
    and are more suitable for tasks that require parallel execution, while processes
    provide better isolation and fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Having all this in mind, let’s see how to write concurrent code in C++.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency with C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The C++ language has had built-in support for managing and executing concurrent
    threads since C++11\. But it doesn’t have any native support for managing concurrent
    processes. The C++ Standard Library provides various classes for thread management,
    synchronization and communication between threads, protection of shared data,
    atomic operations, and parallel algorithms. The **C++ memory model** is also designed
    with thread awareness in mind. This makes it a great choice for developing concurrent
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading with C++ is the ability to have multiple threads of execution
    running concurrently within a single program. This allows a program to take advantage
    of multiple CPU cores and perform tasks in parallel, leading to faster completion
    of tasks and improved overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: The C++ Standard Library introduced the `std::thread` thread management class.
    Once it is instantiated, it is the responsibility of the user to take care of
    the thread’s objective. The users have to choose to either join the thread or
    detach it from its parent thread. If they don’t take care of it, the program terminates.
  prefs: []
  type: TYPE_NORMAL
- en: With the release of C++20, a brand-new thread management class, `std::jthread`,
    was introduced. It makes it relatively easy to create and manage threads. To create
    a new thread, you can create an instance of the `std::jthread` class, passing
    the function or callable object that you want to run as a separate thread. A key
    advantage of `std::jthread` compared to `std::thread` is that you don’t have to
    explicitly worry about joining it. It will be done automatically during the `std::jthread`
    destruction. Later in the chapter, we will have a deeper look into `std::jthread`
    and how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that multithreading will also make a program more complex, as it
    requires careful management of shared resources and synchronization of threads.
    If not properly managed, multithreading can lead to issues such as deadlocks and
    race conditions, which can cause a program to hang or produce unexpected results.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, multithreading requires the developers to ensure that the code
    is thread-safe, which can be a challenging task. Not all tasks are suitable for
    multithreading; some tasks may actually run slower if attempted to be parallelized.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, multithreading with C++ can provide significant benefits in terms of
    performance and resource utilization, but it also requires careful consideration
    of the potential challenges and pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get familiar with the most common pitfalls of writing concurrent
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying race conditions and data races
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In C++, multithreading support was first introduced with the C++11 version
    of the language. One of the key elements provided by the C++11 standard to facilitate
    multithreading is the memory model. The memory model tackles two problems: the
    layout of objects in memory and the concurrent access to these objects. In C++,
    all data is represented by objects, which are blocks of memory that have various
    properties such as type, size, alignment, lifetime, value, and an optional name.
    Each object remains in memory for a specific period of time and is stored in one
    or more memory locations, depending on whether it is a simple scalar object or
    a more complex type.'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of multithreaded programming in C++, it is crucial to consider
    how to tackle concurrent access by multiple threads to shared objects. If two
    or more threads try to access different memory locations, there is usually no
    problem. However, when threads attempt to write in the same memory location simultaneously,
    it can lead to data races, which can cause unexpected behaviors and errors in
    the program.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Data races occur when multiple threads try to access data and at least one of
    them attempts to modify it, and no precautions are taken to synchronize the memory
    access. Data races can cause undefined behavior in your program and are a source
    of trouble.
  prefs: []
  type: TYPE_NORMAL
- en: 'But how does your program come to a *data race*? This happens when there is
    a *race condition* that hasn’t been properly handled. Let’s have a look into the
    difference between data races and race conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Race condition**: A situation where the correctness of a code depends on
    specific timing or a strict sequence of operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data race**: When two or more threads access one object and at least one
    of these threads modifies it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on these definitions, we can deduce that every data race that occurs
    in your program comes as a result of not correctly handling race conditions. But
    the opposite is not always true: not every race condition leads to a data race.'
  prefs: []
  type: TYPE_NORMAL
- en: There is no better way to understand race conditions and data races than by
    looking at an example. Let’s imagine a primitive banking system, really primitive,
    which we hope doesn’t exist anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: Bill and John have accounts in a bank. Bill has $100 in his account and John
    has $50\. Bill owes John a total of $30\. To pay off his debt, Bill decides to
    make two transfers to John’s account. The first is worth $10 and the second is
    $20\. So de facto, Bill will repay John. After both transfers are complete, Bill
    will have $70 left in his account, while John will have accumulated a total of
    $80.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define an `Account` structure that contains the name of the owner of
    the account together with their account balance at a certain moment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `Account` structure, we will also add the overloaded operator methods
    for `+=` and `-=`. These are responsible for depositing or withdrawing a specific
    amount of money to the corresponding account, respectively. Before and after each
    of the operations, the current balance of the account is printed. Here is the
    definition of these operators, which are part of the `Account` structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Looking into the implementation of the operator functions shows that they first
    read the current balance of the account, then store it in a local object (marker
    `{1}`), and finally, using the value of the local object, they increment or decrement
    with the specified amount.
  prefs: []
  type: TYPE_NORMAL
- en: As simple as it gets!
  prefs: []
  type: TYPE_NORMAL
- en: The resulting value of the new balance of the account is written back into the
    `balance` member of the `Account` structure (marker `{2}`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to define a method that will be responsible for the actual money
    transfer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The only thing it does is withdraw the desired amount from one account (marker
    `{1}`) and deposit it to the other account (marker `{2}`), which is exactly what
    we need to successfully transfer money between accounts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s have a look at our `main` program method, which will execute our
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we need to create accounts for Bill and John and deposit $100 and $70
    into them, respectively (markers `{1}` and `{2}`). Then, we have to do the actual
    money transfers: one transfer for $10 and one for $20 (markers `{3}` and `{4}`).
    I know that this code may look unfamiliar to you but don’t worry, we will deep-dive
    into `std::jthread` shortly in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: The only important detail you have to know so far is that we try to make both
    transfers *concurrently* with the help of the C++ multithreading library. At the
    end of the process, we set some time for both execution threads to finish the
    money transfers (marker `{5}`) and print the result (markers `{6}` and `{7}`).
    As we already discussed, after the transfers are finished, Bill should have $70
    in his account while John should have $80.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the program output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Wait, what? Bill has $80 while John has $60! How is that possible?
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s possible because we created a *race condition* that led to a *data race*!
    Let’s explain. Having a deeper look into the implementation of the `operator+=`
    method reveals the problem. By the way, the situation is absolutely the same with
    the other operator method as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: At marker `{1}`, we cache the current balance of the account into a local object
    living on the stack.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The C++ memory model guarantees that each thread has its own copy of all objects
    with automatic storage duration – the stack objects.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we give the current execution thread some rest time of at least `1ms`
    (marker `{2}`). With this statement, we put our thread to sleep, allowing other
    threads (if any) to take processor time and start executing. Nothing to worry
    about so far, right? Once the thread is back on executing, it uses its cached
    value of the account’s balance and increments it with the new amount. Finally,
    it stores the newly calculated value back to the `balance` member of the `Account`
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having a closer look into the output of the program, we observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first transfer starts executing. It is running as part of a thread with
    the `140278035490560` identifier. We see that before the withdrawal is finished,
    the second transfer is started too. Its identifier is `140278027097856`. The second
    transfer finishes the withdrawal first, leaving Bill’s bank account with a balance
    of $80\. Then, the first withdrawal is back in action. But what happens then?
    Instead of taking $10 more from Bill’s account, it actually returns $10! This
    happens because the first thread was suspended when it had already cached the
    initial account balance of $100\. A *race condition* was created. Meanwhile, the
    second transfer has changed the account balance, and now, when the first transfer
    is back to execution, it already works with outdated cached values. This results
    in blindly overriding the newer account balance with the outdated value. A *data*
    *race* happened.
  prefs: []
  type: TYPE_NORMAL
- en: How do we avoid them?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Luckily, the C++ programming language provides various concurrency control mechanisms
    to address these challenges, such as atomic operations, locks, semaphores, condition
    variables, barriers, and others. These mechanisms help ensure that shared resources
    are accessed in a predictable and safe manner and that threads are coordinated
    effectively to avoid a data race. In the next sections, we will get deeper into
    some of these synchronization primitives.
  prefs: []
  type: TYPE_NORMAL
- en: Practical multithreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In computer science, a thread of execution is a sequence of code instructions
    that can be managed independently by a scheduler of the operating system. On a
    Linux system, the thread is always part of a process. The C++ threads could be
    executed concurrently with each other via the multithreading capabilities provided
    by the standard. During execution, threads share common memory space, unlike processes,
    where each has its own. Specifically, the threads of a process share its executable
    code, the dynamically and globally allocated objects, which are not defined as
    `thread_local`.
  prefs: []
  type: TYPE_NORMAL
- en: Hello C++ jthread
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Every C++ program contains at least one thread, and this is the thread that
    runs the `int main()` method. Multithreaded programs have additional threads started
    at some point in the execution of the main thread. Let’s have a look at a simple
    C++ program that uses multiple threads to print to the standard output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: When the program starts, the `int main()` method is entered. Nothing surprising
    so far. At the beginning of the execution, we create a variable on the method
    stack, called `my_threads`. It is a type of `std::array`, which contains five
    elements in it. The `std::array` type represents a container from the Standard
    Library, encapsulating C-style, fixed-sized arrays. It has the advantages of a
    standard container, such as being aware of its own size, supporting assignment,
    random access iterators, and so on. As with any other array type in C++, we need
    to specify what kind of elements it contains. In our example, `my_threads` contains
    five `std::jthread` objects. The `std::jthread` class was introduced in the C++
    Standard Library with the C++20 standard release. It represents a single thread
    of execution, just like `std::thread`, which was introduced with the release of
    C++11\. Some advantages of `std::jthread` compared to `std::thread` are that it
    automatically rejoins on destruction and it can be canceled or stopped in some
    specific cases. It is defined in the `<thread>` header; therefore, we must include
    it in order to compile successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Yes, you are asking the right question! If we already defined an array of `jthread`
    objects, what job do they really perform? The expectation is that every thread
    is associated with some job that needs to be done. But here, the simple answer
    is *nothing*. Our array contains five `jthread` objects, which don’t actually
    represent an execution thread. They are used more like placeholders because, when
    `std::array` is instantiated, it also creates the objects it contains using their
    default constructors if no other arguments are passed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now define some workers that our threads can be associated with. The
    `std::jthread` class accepts, as a worker, any *callable* type. Such types provide
    a single operation that can be invoked. Widely known examples of such types are
    function objects and lambda expressions, which we already covered in detail in
    [*Chapter 4*](B20833_04.xhtml#_idTextAnchor060). In our example, we will use lambda
    expressions because they provide a way of creating anonymous function objects
    (functors) that can be utilized in-line or passed as an argument. The introduction
    of lambda expressions in C++11 simplifies the process of creating anonymous functors,
    making it more efficient and straightforward. The following code shows our worker
    method defined as a lambda expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The defined lambda expression, `const auto worker{…};`, is pretty simple. It
    is instantiated on the function stack. It has no input parameters, and it doesn’t
    capture any state from outside. The only work it does is to print to the standard
    output the `jthread` object’s ID. Every thread in C++ provided by the standard
    concurrency support library has a unique identifier associated with it. The `std::this_thread::get_id()`
    method returns the ID of the specific thread in which it has been invoked. This
    means that if this lambda expression is passed to several different threads, it
    should print a different thread ID.
  prefs: []
  type: TYPE_NORMAL
- en: Printing to `std::cout` by many concurrent threads could bring surprising results.
    The `std::cout` object is defined as a global, thread-safe object, which ensures
    that each character written to it is done so atomically. However, no guarantees
    are made about a sequence of characters such as strings, and it is likely that
    the output when multiple threads are concurrently writing strings to `std::cout`
    will be a mixture of these strings. Well, this is not what we really want here.
    We expect that each thread will be able to fully print its messages. Therefore,
    we need a synchronization mechanism, which ensures that writing a string to `std::cout`
    is fully atomic. Luckily, C++20 introduces a whole new family of class templates
    defined in the `<syncstream>` standard library header, which provides mechanisms
    to synchronize threads writing to one and the same stream. One of them is `std::osyncstream`.
    You can use it as a regular stream. Just create an instance of it by passing `std::cout`
    as a parameter. Then, with the help of its `std::basic_ostream& operator<<(...)`
    class method, you can insert data, just like a regular stream. It is guaranteed
    that all of the inserted data will be flushed atomically to the output once the
    `std::osyncstream` object goes out of scope and is destroyed. In our example,
    the `sync_cout` object will be destroyed when the lambda is about to finish its
    execution and leave its scope. This is exactly the behavior we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we are ready to give some work to our threads to do. This means that
    we need to associate worker lambdas with the five threads we have in the `my_threads`
    array. But the `std::jthread` type supports adding a worker method only as part
    of its construction. That’s why we need to create other `jthread` objects and
    replace them with the placeholders in the `my_threads` array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Being a standard container, `std::array` natively supports range-based for loops.
    Therefore, we can easily iterate through all elements in `my_threads` and replace
    them with new `jthread` objects that already have associated workers with them.
    Firstly, we create new `jthread` objects with automatic storage duration and assign
    them a worker object. In our case, for every newly created thread, we assign one
    and the same worker object. This is possible because, in the current case, the
    `jthread` class makes a copy of the worker instance in the `jthread` objects and,
    therefore, each `jthread` object gets its own copy of the worker lambda. When
    constructing these objects, the process is carried out within the context of the
    caller. This means that any exceptions that occur during the evaluation and copying
    or movement of the arguments are thrown in the current `main` thread.
  prefs: []
  type: TYPE_NORMAL
- en: An important detail is that the newly created `jthread` objects are not copied
    to the existing elements of the array, but they are moved. Therefore, the `std::jthread`
    class has implicitly deleted its copy constructor and assignment operator because
    it doesn’t make much sense to copy a thread to an already existing thread. In
    our case, the newly created `jthread` objects will be created in the storage of
    the existing array elements.
  prefs: []
  type: TYPE_NORMAL
- en: When a `jthread` object is constructed, the associated thread starts immediately,
    although there may be some delays due to Linux scheduling specifics. The thread
    begins executing at the function specified as an argument to the constructor.
    In our example, this is the worker lambda associated with each thread. If the
    worker returns a result, it will be ignored, and if it ends by throwing an exception,
    the `std::terminate` function is executed. Therefore, we need to make sure that
    either our worker code doesn’t throw or we catch everything throwable.
  prefs: []
  type: TYPE_NORMAL
- en: When a thread is started, it begins executing its dedicated worker. Each thread
    has its own function stack space, which guarantees that any local variable defined
    in the worker will have a separate instance per thread. Therefore, `const auto
    thread_id` in the worker is initialized with a different ID depending on the thread
    it is run by. We do not need to take any precautions to ensure that the data stored
    in `thread_id` is consistent. It is guaranteed by the Standard that data with
    automatic storage duration is not shared between the threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the `jthread` objects have been created, the `main` thread concurrently
    prints its ID along with the rest of the threads. There is no guaranteed order
    of execution for each thread, and it is possible for one thread to be interrupted
    by another. As a result, it is important to ensure that the code is written in
    a manner that can handle potential preemption and remains robust in all scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: All threads are now running concurrently with the `main` thread. We need to
    make sure that the `main` thread is also printing to the standard output in a
    thread-safe manner. We again use an instance of `std::osyncstream`, but this time,
    we don’t create a named variable – instead, we create a temporary one. This approach
    is favored due to its ease of use, similar to using the `std::cout` object. The
    standard guarantees that the output will be flushed at the end of each statement,
    as the temporary ones persist until the end of the statement and their destructor
    is invoked, resulting in the flushing of the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample output from the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `std::jthread` name refers to a *joining* thread. Unlike `std::thread`,
    `std::jthread` also has the ability to *automatically* join the thread that it
    has been started by. The behavior of `std::thread` can be confusing at times.
    If `std::thread` has not been joined or detached, and it is still considered *joinable*,
    the `std::terminate` function will be called upon its destruction. A thread is
    considered joinable if neither the `join()` nor the `detach()` method has been
    called. In our example, all the `jthread` objects automatically join during their
    destruction and do not result in the termination of the program.
  prefs: []
  type: TYPE_NORMAL
- en: Canceling threads – is this really possible?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before C++ 20 was released, this wasn’t quite possible. It was not guaranteed
    that `std::thread` was stoppable in the sense that there wasn’t a standard utility
    to halt the thread’s execution. Different mechanisms were used instead. Stopping
    `std::thread` required cooperation between the `main` and worker threads, typically
    using a flag or atomic variable or some kind of messaging system.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the release of C++20, there is now a standardized utility for requesting
    `std::jthread` objects to stop their execution. The stop tokens come to help.
    Looking at the C++ standard reference page about the definition of `std::jthread`
    ([https://en.cppreference.com/w/cpp/thread/jthread](https://en.cppreference.com/w/cpp/thread/jthread)),
    we find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: “The class jthread represents a single thread of execution. It has the same
    general behavior as std::thread, except that jthread automatically rejoins on
    destruction, and can be canceled/stopped in certain situations.”
  prefs: []
  type: TYPE_NORMAL
- en: We already saw that `jthread` objects automatically join on destruction, but
    what about canceling/stopping and what does “certain situations” mean? Let’s dig
    deeper into this.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, don’t expect that `std::jthread` exposes some magical mechanism,
    some red button that stops the running thread when it is pressed. It is always
    a matter of implementation, how exactly your worker function is implemented. If
    you want your thread to be cancelable, you have to make sure that you have implemented
    it in the right way in order to allow cancellation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the definition of our worker lambda function, we observe that it
    is now slightly reworked (marker `{1}`). It accepts two new parameters – `std::stop_token
    token` and `int num`. The stop token reflects the shared stop state that a `jthread`
    object has. If the worker method accepts many parameters, then the stop token
    must always be the first parameter passed.
  prefs: []
  type: TYPE_NORMAL
- en: It is imperative to ensure that the worker method is designed to be able to
    handle cancellation. This is what the stop token is used for. Our logic should
    be implemented in such a way that it regularly checks whether a stop request has
    been received. This is done with a call to the `stop_requested()` method of the
    `std::stop_token` object. Every specific implementation decides where and when
    these checks are to be done. If the code doesn’t respect the stop token state,
    then the thread can’t be canceled gracefully. So, it’s up to you to correctly
    design your code.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, our worker lambda respects the state of the thread’s stop token. It
    continuously checks whether a stop is requested (marker `{2}`). If not, it prints
    the thread’s ID and goes to sleep for `200ms`. This loop continues until the parent
    thread decides to send stop requests to its worker threads (marker `{3}`). This
    is done by invoking the `request_stop()` method of the `std::jthread` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output of the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know how we can stop the execution of a specific `std::jthread`
    using `std::stop_token`, let’s see how we can stop the execution of multiple `std::jthread`
    objects using a single stop source.
  prefs: []
  type: TYPE_NORMAL
- en: std::stop_source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `std::stop_source` class enables you to signal a cancellation request for
    `std::jthread`. When a stop request is issued through a `stop_source` object,
    it becomes visible to all other `stop_source` and `std::stop_token` objects associated
    with the same stop state. You just need to signal it, and any thread worker that
    consumes it will be notified.
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing `std::stop_token` and `std::stop_source`, threads can signal or
    check for a request to stop their execution asynchronously. The request to stop
    is made through `std::stop_source`, which affects all related `std::stop_token`
    objects. These tokens can be passed to the worker functions and used to monitor
    stop requests. Both `std::stop_source` and `std::stop_token` share ownership of
    the stop state. The method of the `std::stop_source` class – `request_stop()`
    – and the methods in `std::stop_token` – `stop_requested()` and `stop_possible()`
    – are all atomic operations to ensure that no data race will occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at how our previous example could be reworked with the help
    of the stop tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `main` method starts with the declaration of the `std::stop_source` source,
    which will be used by the `main` thread to signal all child worker threads and
    request them to stop. The worker lambda is slightly reworked in order to accept
    `std::stop_source sr` as an input. This is in fact the communication channel through
    which the worker is notified for a stop request. The `std::stop_source` object
    is copied in all workers associated with the started threads.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than iterating through all the threads and invoking on each of them a
    stop request, the only operation that we need to invoke is to directly call `request_stop()`
    on the source instance in the `main` thread (marker `{1}`). This will broadcast
    stop requests to all workers that consume it.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, the call to the `request_stop()` method on the stop source
    object is just a request rather than a blocking call. So, don’t expect your threads
    to stop immediately once the call is finished.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the sample output from the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We are now familiar with two mechanisms for halting thread execution in C++.
    Now, it’s time to see how we can share data between multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing data during parallel execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Think in terms of tasks rather than* *threads* ([https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#cp4-think-in-terms-of-tasks-rather-than-threads](https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#cp4-think-in-terms-of-tasks-rather-than-threads)).'
  prefs: []
  type: TYPE_NORMAL
- en: Referring back to the *C++ Core Guidelines*, they advise us that it is better
    to stick to tasks rather than threads. A thread is a technical implementation
    idea, a perspective on how the machine works. On the other hand, a task is a practical
    concept for work that you want to do, ideally alongside other tasks. In general,
    practical concepts are simpler to understand and provide better abstraction, and
    we prefer them.
  prefs: []
  type: TYPE_NORMAL
- en: But what is a task in C++? Is it another standard library primitive or what?
    Let’s have a look!
  prefs: []
  type: TYPE_NORMAL
- en: 'In C++, besides threads, tasks are also available to perform work asynchronously.
    A task consists of a worker and two associated components: a **promise** and a
    **future**. These components are connected through a shared state, which is a
    kind of data channel. The promise does the work and places the result in the shared
    state, while the future retrieves the result. Both the promise and the future
    can run in separate threads. One unique aspect of the future is that it can retrieve
    the result at a later time, making the calculation of the result by the promise
    independent from the retrieval of the result by the associated future.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Inter-thread communication](img/Figure_6.04_B20833.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Inter-thread communication
  prefs: []
  type: TYPE_NORMAL
- en: The `<future>` header, defined in the Standard Library, is necessary for utilizing
    tasks. It provides the capability to obtain the results of functions executed
    in separate threads, also referred to as `std::promise` class, these results are
    communicated through a shared state, where the asynchronous task can store its
    return value or an exception. This shared state can then be accessed using `std::future`
    to retrieve the return value or the stored exception.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at a simple example where a thread reports a string as a
    result to its parent thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As we already discussed, threads communicate with each other using a shared
    state. In the `int main()` method, we declare `std::promise<std::string> promise`,
    which is our de facto data source (marker `{1}`). The `std::promise` class is
    a template class that needs to be parameterized once it is instantiated. In our
    example, we want our worker thread, `std::thread th1`, to return a string as a
    result. Therefore, we instantiate `std::promise` with the `std::string` type.
    We also need a way for the `main` thread to be able to get the result that will
    be set by the worker thread. In order to do so, we need to get a `std::future`
    object from the promise we already instantiated. This is possible because the
    `std::promise` type has a method that returns its associated future – `std::future<...>
    get_future()`. In our example, we instantiate a future object, `future`, which
    is initialized by the `get_future()` method of the promise (marker `{2}`).
  prefs: []
  type: TYPE_NORMAL
- en: Since we already have a promise and its associated future, we are now ready
    to move the promise as part of the worker thread. We are moving it in order to
    be sure that it won’t be used by the `main` thread anymore (marker `{3}`). Our
    worker thread is quite simple, and it just sleeps for `20ms` and sets the result
    in the promise (marker `{4}`). The `std::promise` type provides several ways to
    set a result. The result could be either a value of type by which the promise
    is parameterized or it could be an exception thrown during worker execution. The
    value is set by the `set_value()` and `set_value_at_thread_exit()`methods. The
    main difference between both methods is that `set_value()` immediately notifies
    the shared state that the value is ready, whereas `set_value_at_thread_exit()`
    does it when the thread execution is finished.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the `main` thread execution has been blocked waiting for the result
    of the worker thread. This is done on the call to the `future.get()` method. This
    is a blocking call on which the waiting thread is blocked until the shared state
    is notified that the result of the future is set. In our example, this happens
    after the completion of the worker thread because the shared state is only notified
    when the worker is finished (marker `{5}`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The expected output from the program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Barriers and latches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: New thread synchronization primitives were introduced with the C++20 standard.
    Barriers and latches are straightforward thread synchronization primitives that
    block threads to wait until a counter reaches zero. These primitives are offered
    by the standard library in the form of the `std::latch` and `std::barrier` classes.
  prefs: []
  type: TYPE_NORMAL
- en: What distinguishes these two synchronization mechanisms? The key difference
    is that `std::latch` can only be used once, while `std::barrier` can be used multiple
    times by multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: What advantages do barriers and latches offer over other synchronization primitives
    that the C++ standard provides, such as condition variables and locks? Barriers
    and latches are easier to use, more intuitive, and, in some circumstances, may
    provide better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We have an array of two threads that are synchronized on a latch. This means
    that each thread starts its execution and does its work until it reaches the latch.
  prefs: []
  type: TYPE_NORMAL
- en: The `std::latch` class is a synchronization mechanism that utilizes a downward-counting
    counter to coordinate threads. The counter is set at initialization and passed
    as an argument to the constructor. The threads can then wait until the counter
    reaches zero. It is not possible to increase or reset the counter once it is initialized.
    Access to the member functions of `std::latch` from multiple threads concurrently
    is guaranteed to be thread-safe and free from data races.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example (marker `{1}`), we have initialized the latch with a value of
    `2` because we have two worker threads that need to be synchronized with the main
    one. Once the worker reaches the latch, it has three options:'
  prefs: []
  type: TYPE_NORMAL
- en: Decrement it and continue (marker `{2}`). This is done using the member of the
    `std::latch` class – `void count_down(n = 1)`. This call is non-blocking and automatically
    decrements the latch’s internal counter value by `n`. It is undefined behavior
    if you try to decrement with a negative value or with a value greater than the
    value that the internal counter currently has. In our example, this is a worker
    thread with an ID of `0`, which, once it is ready, decrements the latch counter
    and finishes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decrement it and wait until the latch becomes zero (marker `{3}`). In order
    to do so, you have to use another method of the `std::latch` class – `void arrive_and_wait(n
    = 1)`. This method, once it is invoked, decrements the latch by `n` and blocks
    it until the latch’s internal counter hits `0`. In our example, this is a worker
    thread with an ID of `1`, which, once it is ready, starts waiting until the other
    worker is finished.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just block and wait until the internal counter of the latch becomes zero (marker
    `{4}`). This is possible because `std::latch` provides a method – `void wait()
    const`. This is a blocking call on which the invoking thread is blocked until
    the internal counter of the latch hits zero. In our example, the `main` thread
    blocks and starts waiting for the worker threads to finish their execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result of our program is that the `main` thread execution is suspended
    until the worker threads finish their jobs. The `std::latch` class provides a
    convenient way to synchronize the execution of several threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Another very similar synchronization primitive to `std::latch` is `std::barrier`.
    Barriers are thread synchronization primitives that permit a group of threads
    to wait until all of them reach a specific synchronization point. Unlike a latch,
    a barrier can be used multiple times. Once the threads have been released from
    the synchronization point, they can reuse the barrier. A synchronization point
    is a specific moment where a thread can pause its execution until a specific condition
    has been met. This makes barriers ideal for synchronizing repeated tasks or executing
    different phases from the same bigger task by many threads.
  prefs: []
  type: TYPE_NORMAL
- en: In order to get a better understanding of what barriers are, let’s use an example.
    Imagine that you have a network of temperature sensors installed in your home.
    In each room, there is a sensor installed. Each sensor takes a temperature measurement
    at a specific time period and the result is buffered in its memory. When the sensor
    does 10 measurements, it sends them as a chunk to a server. This server is responsible
    for collecting all measurements from all sensors in your home and calculating
    temperature mean values – the mean temperature for each room and the mean temperature
    for your entire home.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss the algorithm now. In order to calculate the mean temperature
    of your entire home, we first need to process the temperature measurements done
    by the sensors that are sent to the server at some specific time period. This
    means that we need to process all the temperature samples received for a specific
    room to calculate the mean temperature for that room, and we need to do this for
    all the rooms in your home. Finally, with the calculated mean temperatures for
    each room, we can calculate the mean temperature for the entire home.
  prefs: []
  type: TYPE_NORMAL
- en: 'It sounds like we need to process a lot of data. It makes sense to try to parallelize
    the data processing wherever possible. Yes, you are right: not all of the data
    processing can be parallelized! There is a strict sequence of actions we need
    to respect. Firstly, we need to calculate the mean temperature in each room. There
    are no dependencies between the rooms, so we can execute these calculations in
    parallel. Once we have all the room temperatures calculated, we can continue to
    the calculation of the mean temperature of the entire home. This is exactly where
    `std::barrier` will come to the rescue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `std::barrier` synchronization primitive blocks the threads at a specific
    synchronization point (the barrier) until all of them arrive. Then, it allows
    a callback to be invoked and a specific action to be performed. In our example,
    we need to wait for all room calculations to be finished – to wait on the barrier.
    Then, a callback will be executed where we will calculate the mean temperature
    for the entire home:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let’s start with the definition of our data container where we will store the
    temperature measurements done for each room, together with their calculated mean
    values by our worker threads. We will use a vector of rooms, `room_temperature`,
    in which we will store the room name, a vector of measurements, and the mean value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to define the workers that will, in parallel, calculate the mean
    values for each room:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We create the same count of `jthread` instances as the count of the rooms. Each
    `jthread` instance is created and a worker lambda is assigned to it. As you can
    see, the worker lambda captures a `std::stop_source` object, which will be used
    to notify it that no other work is pending and the thread execution should be
    finished. The lambda also captures `std::barrier measurementBarrier`, which will
    be used to block each thread that is ready with its computation until all other
    threads are also ready (marker `{1}`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `std::barrier` instance needs to be initialized with the count of the synchronization
    points (marker `{2}`). This means that the barrier will be raised when the count
    of threads reaching the barrier is equal to the initialized value. In our example,
    we initialize the barrier with the count of the worker threads that will concurrently
    compute the mean temperatures for each room. An optional initialization parameter
    that the barrier accepts is a callback function (marker `{3}`). This function
    must not throw and, therefore, we mark it as `noexcept`. It will be invoked when
    all threads in a certain cycle arrive at the barrier and before the barrier is
    raised. Keep in mind that the standard doesn’t specify which thread this callback
    will be executed on. We will use this callback to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Iterate through all already computed mean temperatures for the rooms and compute
    the mean temperature of the entire home. This is the result we expect our program
    to deliver.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feed the worker threads with new temperature data for the next computation cycle.
    In contrast to `std::latch`, `std::barrier` allows us to use the same barrier
    as many times as we need.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check whether we have already calculated five times the mean temperature of
    the entire home and, if so, notify the workers that they need to gracefully stop
    and exit the program.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When a thread starts working and it is ready with its computation, it hits
    the barrier (marker `{4}`). This is possible because `std::barrier` exposes a
    method: `void arrive_and_wait()`. This call effectively decrements the internal
    counter of the barrier, which notifies it that the thread has arrived and blocks
    the thread until the counter hits zero and the barrier’s callback is triggered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, you can find the methods responsible for generating
    example temperature values and calculating the mean temperature value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have all the code pieces available, let’s see the `main` method implementation
    of our program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For the input temperature data of our example, we use a random number generator,
    which produces data with normal distribution. As a result, we get the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: With the preceding example, we have demonstrated how you can use synchronization
    primitives with `std::jthread` to provide inter-thread synchronization for your
    program.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored several topics related to concurrency and parallelism
    in C++. We began by discussing the terminology and differences between concurrency
    and parallelism, including preemption. We then delved into how programs execute
    on single and multiple processing units, distinguishing between processes and
    execution threads and briefly exploring communication mechanisms such as pipes,
    sockets, and shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of C++, we examined how the language supports concurrency, specifically
    through the `std::thread` class and the new `std::jthread` primitive introduced
    in C++20\. We also discussed the risks associated with race conditions and data
    races, including an example of a money transfer operation. To avoid these issues,
    we examined mechanisms such as locks, atomic operations, and memory barriers.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, we looked closely at the `std::jthread` class, exploring its functionality
    and proper usage. Additionally, we learned about a new synchronized stream wrapper
    delivered in C++20 for printing in concurrent environments. We also covered how
    to cancel running threads using `std::stop_token` and how to request a stop to
    several threads using `std::stop_source`.
  prefs: []
  type: TYPE_NORMAL
- en: We then shifted our focus to returning results from threads using `std::future`
    and `std::promise`. Additionally, we discussed the use of `std::latch` and `std::barrier`,
    using an example of a temperature station to demonstrate how the latter can be
    used to synchronize threads.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we explored a range of topics related to concurrency and parallelism
    in C++, from basic terminology and concepts to more advanced techniques and mechanisms
    for avoiding data races and synchronizing threads. But please stay tuned because,
    in the next chapter, you will get familiar with some mechanisms for IPC that are
    widely used in software programming.
  prefs: []
  type: TYPE_NORMAL
