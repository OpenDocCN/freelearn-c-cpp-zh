- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Atomic Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B22219_04.xhtml#_idTextAnchor074) , we learned about lock-based
    thread synchronization. We learned about mutexes, condition variables, and other
    thread synchronization primitives, which are all based on acquiring and releasing
    locks. Those synchronization mechanisms are built on top of *atomic types and
    operations* , this chapter’s topic.
  prefs: []
  type: TYPE_NORMAL
- en: We will study what atomic operations are and how they differ from lock-based
    synchronization primitives. After reading this chapter, you will have a basic
    knowledge of atomic operations and some of their applications. Lock-free (not
    using locks) synchronization based on atomic operations is a very complex subject
    requiring years to master, but we will give you what we hope will be a good introduction
    to the subject.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What are atomic operations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to the C++ memory model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What atomic types and operations are provided by the C++ Standard Library?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some examples of atomic operations, from a simple counter to be used to gather
    statistics and a basic mutex-like lock to a full **single-producer-single-consumer**
    ( **SPSC** ) lock-free bounded queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need a recent C++ compiler with C++20 support. Some short code examples
    will be provided as links to the very useful godbolt website ( [https://godbolt.org](https://godbolt.org)
    ). For full code examples, we will use the book repo, which is available at [https://github.com/PacktPublishing/Asynchronous-Programming-with-CPP](https://github.com/PacktPublishing/Asynchronous-Programming-with-CPP)
    .
  prefs: []
  type: TYPE_NORMAL
- en: The examples can be compiled and run locally. We have tested the code on an
    Intel CPU computer running Linux (Ubuntu 24.04 LTS). For atomic operations and
    especially for memory ordering (more on this later in this chapter), Intel CPUs
    are different from Arm CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Please note here that code performance and profiling will be the subject of
    [*Chapter 13*](B22219_13.xhtml#_idTextAnchor267) . We will just make some remarks
    on performance in this chapter to avoid making it unnecessarily long.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to atomic operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Atomic operations are indivisible (hence the word atomic, from the Greek *ἄτομος*
    , *atomos* , indivisible).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will introduce atomic operations, what they are, and some
    reasons to use (and not to use!) them.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations versus non-atomic operations – an example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you remember the simple counter example from [*Chapter 4*](B22219_04.xhtml#_idTextAnchor074)
    , we needed to use a synchronization mechanism (we used a mutex) for modifying
    the counter variable from different threads to avoid race conditions. The cause
    of the race condition was that incrementing the counter required three operations:
    reading the counter value, incrementing it, and writing the modified counter value
    back to memory. If only we could do that in one go, there would be no race condition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is exactly what could be achieved with an atomic operation: if we had
    some kind of **atomic_increment** operation, each thread would read, increment,
    and write the counter in a single instruction, avoiding the race condition because
    at any time, incrementing the counter would be fully done. By fully done we mean
    that each thread would either increment the counter or do nothing at all, making
    interruptions in the middle of a counter increment operation impossible.'
  prefs: []
  type: TYPE_NORMAL
- en: The following two examples are for illustration purposes only and are not multithreaded.
    We focus here on just the operations, whether atomic or non-atomic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see this in the code. For the C++ code and the generated assembly language
    shown in the following example, refer to [https://godbolt.org/z/f4dTacsKW](https://godbolt.org/z/f4dTacsKW)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The code increments a global counter. Now let’s see the assembly code generated
    by the compiler and what instructions the CPU executes (the full assembly can
    be found in the previous link):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**[1]** copies the value stored in **counter** to the **eax** register, **[2]**
    adds **1** to the value stored in **eax** , and finally, **[3]** copies back the
    content of the **eax** register to the **counter** variable. So, a thread could
    execute **[1]** and then be scheduled out, and another thread execute all three
    instructions after that. When the first thread finishes incrementing the result,
    the counter will be incremented just once and thus the result will be incorrect.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code does the same: it increments a global counter. This time,
    though, it uses atomic types and operations. To get the code and the generated
    assembly in the following example, refer to [https://godbolt.org/z/9hrbo31vx](https://godbolt.org/z/9hrbo31vx)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will explain the **std::atomic<int>** type and the atomic increment operation
    later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generated assembly code is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Just one instruction has been generated to add **1** to the value stored in
    the **counter** variable. The **lock** prefix here means that the following instruction
    (in this case **add** ) is going to be executed atomically. Hence, in this second
    example, a thread cannot be interrupted in the middle of incrementing the counter.
    As a side note, some Intel x64 instructions execute atomically and don’t use the
    **lock** prefix.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations allow threads to read, modify (for example, increase a value),
    and write indivisibly, and can also be used as synchronization primitives (similar
    to the mutexes we saw in [*Chapter 4*](B22219_04.xhtml#_idTextAnchor074) ). In
    fact, all the lock-based synchronization primitives we have seen so far in this
    book are implemented using atomic operations. Atomic operations must be provided
    by the CPU (as in the **lock** **add** instruction).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have introduced atomic operations, defined what they are,
    and studied a very simple example of how they are implemented by looking at the
    assembly instructions that the compiler generates. In the next section, we will
    look at some of the advantages and disadvantages of atomic operations.
  prefs: []
  type: TYPE_NORMAL
- en: When to use (and when not to use) atomic operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using atomic operations is a complex subject and it can be very difficult (or
    at least quite tricky) to master. It requires a lot of experience, and we have
    attended some courses on this very subject where we were advised not to do it!
    Anyway, you can always learn the basics and experiment as you do so. We hope this
    book will help you progress in your learning journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'Atomic operations can be used in the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**If multiple threads share a mutable state** : The need to synchronize threads
    is the most common case. Of course, it is possible to use locks such as mutexes,
    but atomic operations, in some cases, will provide better performance. Please
    note, however, that the use of atomic operations *does not* guarantee better performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**If synchronized access to shared state is fine-grained** : If the data we
    must synchronize is an integer or a pointer or any other variable of a C++ intrinsic
    type, then using atomic operations may be better than using locks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**To improve performance** : If you want to achieve maximum performance, then
    atomic operations can help reduce thread context switches (see [*Chapter 2*](B22219_02.xhtml#_idTextAnchor035)
    ) and reduce the overhead introduced by locks, thus lowering latency. Remember
    to always profile your code to be sure that performance is improved (we will see
    this in depth in [*Chapter 13*](B22219_13.xhtml#_idTextAnchor267) ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Locks can be used in the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**If the protected data is not fine-grained** : For example, we are synchronizing
    access to a data structure or an object bigger than 8 bytes (in modern CPUs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**If performance is not an issue** : Locks are much simpler to use and reason
    about (in some cases using locks gives better performance than using atomic operations).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**To avoid the need to acquire low-level knowledge** : To get the maximum performance
    out of atomic operations, a lot of low-level knowledge is required. We will introduce
    some of it in the section, *The C++* *memory model* .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have just learned when to use and when not to use atomic operations. Some
    applications such as low-latency/high-frequency trading systems require maximum
    performance and use atomic operations to achieve the lowest latency possible.
    Most applications will work just fine synchronizing with locks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will study the differences between blocking and non-blocking
    data structures and some related concept definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Non-blocking data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B22219_04.xhtml#_idTextAnchor074) we studied the implementation
    of a synchronized queue. We used mutexes and condition variables as synchronization
    primitives. Data structures synchronized with locks are called **blocking data
    structures** because threads are *blocked* (by the operating system), waiting
    until the locks become available.
  prefs: []
  type: TYPE_NORMAL
- en: Data structures that don’t use locks are called **non-blocking data structures**
    . Most (but not all) of them are lock-free.
  prefs: []
  type: TYPE_NORMAL
- en: A data structure or algorithm is considered lock-free if each synchronized action
    completes in a finite number of steps, not allowing indefinite waiting for a condition
    to become true or false.
  prefs: []
  type: TYPE_NORMAL
- en: 'The types of lock-free data structures are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Obstruction-free** : A thread will complete its operation in a bounded number
    of steps if all other threads are suspended'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lock-free** : A thread will complete its operation in a bounded number of
    steps while multiple threads are working on the data structure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wait-free** : All the threads will complete their operations in a bounded
    number of steps while multiple threads are working on the data structure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implementing lock-free data structures is very complicated and before doing
    it, we need to be sure it’s necessary. The reasons to use lock-free data structures
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Achieving maximum concurrency** : As we saw earlier, atomic operations are
    a good choice when the data access synchronization involves fine-grained data
    (such as native-type variables). From the preceding definitions, a lock-free data
    structure will allow at least one of the threads accessing the data structure
    to make some progress in a bounded number of steps. A wait-free structure will
    allow all the threads accessing the data structure to make some progress.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we use locks, however, a thread owns the lock while the rest of the threads
    are just waiting for the lock to be available, so the concurrency achievable with
    lock-free data structures can be much better.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**No deadlocks** : Because there are no locks involved, it is impossible to
    have any deadlocks in our code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance** : Some applications must achieve the lowest latency possible
    and so waiting for a lock can be unacceptable. When a thread tries to acquire
    the lock, and it is not available, then the operating system blocks the thread.
    While the thread is blocked, there is a context switch for the scheduler to be
    able to schedule another thread for execution. These context switches take time,
    and that time may be too much in a low-latency application such as a high-performance
    network packet receiver/processor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have now looked at what blocking and non-blocking data structures are and
    what lock-free code is. We will introduce the C++ memory model in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The C++ memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section explains the C++ memory model and how it deals with concurrency.
    The C++ memory model comes with C++11, and defines the two main features of memory
    in C++:'
  prefs: []
  type: TYPE_NORMAL
- en: How objects are laid out in memory (that is, structural aspects). This subject
    won’t be covered in this book, which is about asynchronous programming.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory modification order (that is, concurrency aspects). We will see the different
    memory modification orders specified in the memory model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory access order
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we explain the C++ memory model and the different memory orderings it
    supports, let’s clarify what we mean by memory order. Memory order refers to the
    order in which memory (that is, the variables in a program) is accessed. Memory
    access can be either read or write (load and store). But what is the actual order
    in which the variables of a program are accessed? For the following code, there
    are three points of view: the written code order, the compiler-generated instructions
    order, and finally, the order in which the instructions are executed by the CPU.
    These three orderings can all be the same or (more likely) different.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first and obvious ordering is the one in the code. An example of this is
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The **func_a** function first adds 1 to variable **a** , then adds 10 to variable
    **b** , and finally, adds 2 to variable **a** . This is our intention and the
    order in which we define the statements to be executed.
  prefs: []
  type: TYPE_NORMAL
- en: The compiler will transform the preceding code into assembly instructions. The
    compiler can change the order of our statements to make the generated code more
    efficient if the outcome of the code execution is unchanged. For example, with
    the preceding code, the compiler could either do the two additions with variable
    **a** first and then the addition with variable **b** , or it could simply add
    3 to **a** and then 10 to **b** . As we mentioned previously, the compiler can
    do whatever it wants to optimize the code if the result is the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the operation on **b** depends on the previous operation on **a**
    , so the compiler cannot reorder the statements, and the generated code will be
    like the code we write (same order of operations).
  prefs: []
  type: TYPE_NORMAL
- en: The CPU (which used in this book is a modern Intel x64 CPU) will run the generated
    code. It can execute the compiler-generated instructions in a different order.
    This is called out-of-order execution. The CPU can do this, again, if the result
    is correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'See this link for the generated code shown in the preceding example: [https://godbolt.org/z/Mhrcnsr9e](https://godbolt.org/z/Mhrcnsr9e)'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the generated instructions for **func_1** show an optimization: the
    compiler combined both additions into one by adding 3 to variable **a** in one
    instruction. Second, the generated instructions for **func_2** are in the same
    order as the C++ statements we wrote. In this case, the CPU could execute the
    instructions out of order, as there is no dependency among the operations.'
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, we can say that the code that the CPU will run can be different
    from the code we wrote (again, given that the result of the execution is the same
    as we intended in the program we wrote).
  prefs: []
  type: TYPE_NORMAL
- en: All the examples we have shown are fine for code that runs in a single thread.
    The code instructions may be executed in different order depending on the compiler
    optimizations and the CPU out-of-order execution and the result will still be
    correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following code for an example of out-of-order execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The CPU could execute the instructions in the order shown in the preceding
    code, that is, **load var1 [1]** . Then, while the variable is being read, it
    could issue some of the later instructions, such as **[4]** and **[5]** , and
    then, once **var1** has been read, execute **[2]** , then **[3]** , and, finally,
    **[6]** . The instructions were executed in a different order, but the result
    is still the same. This is a typical example of out-of-order execution: the CPU
    issues a load instruction and instead of waiting for the data to become available,
    it executes some other instructions, if possible, to avoid being idle and to maximize
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: All the optimizations we have mentioned (both compiler and CPU) are always done
    without considering the interactions between threads. Neither the compiler nor
    the CPU knows about different threads. In these cases, we need to tell the compiler
    what it can and cannot do. Atomic operations and locks are the way to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'When, for example, we use atomic variables, we may not only require the operations
    to be atomic but also to follow a certain order for the code to work properly
    when running multiple threads. This cannot be done by just the compiler or the
    CPU because neither has any information involving multiple threads. To specify
    what order we want to use, the C++ memory model offers different options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Relaxed** **ordering** : **std::memory_order_relaxed**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Acquire and release ordering** : **std::memory_order_acquire** , **std::memory_order_release**
    , **std::memory_order_acq_rel** , and **std::memory_order_consume**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential consistency** **ordering** : **std::memory_order_seq_cst**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The C++ memory model defines an abstract machine to achieve independence from
    any specific CPU. However, the CPU is still there and the features available in
    the memory model may not be available to a specific CPU. For example, the Intel
    x64 architecture is quite restrictive and enforces quite a strong memory order.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Intel x64 architecture uses a processor-ordered memory-ordering model that
    can be defined as being *write-ordered with store-buffer forwarding* . In a single-processor
    system, the memory-ordering model respects the following principles:'
  prefs: []
  type: TYPE_NORMAL
- en: Reads are not reordered with any reads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writes are not reordered with any writes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writes are not reordered with older reads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reads may be reordered with older writes (if the read and write to be reordered
    refer to different memory locations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reads and writes are not reordered with locked ( atomic) instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are more details in the Intel manuals (see the references at the end of
    the chapter), but the preceding principles are the most relevant.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a multi-processor system, the following principles apply:'
  prefs: []
  type: TYPE_NORMAL
- en: Each of the individual processors uses the same ordering principles as in a
    single-processor system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writes by a single processor are observed in the same order by all processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writes from an individual processor are not ordered with respect to the writes
    from other processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory ordering obeys causality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any two stores are seen in a consistent order by processors other than those
    performing the store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locked (atomic) instructions have total order
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Intel architecture is strongly ordered; the store operations (write instructions)
    for each processor are observed by other processors in the same order they were
    performed, and each processor executes the stores in the same order as they appear
    in the program. This is called **Total Store** **Ordering** ( **TSO** ).
  prefs: []
  type: TYPE_NORMAL
- en: 'The ARM architecture supports **Weak Ordering** ( **WO** ). These are the main
    principles:'
  prefs: []
  type: TYPE_NORMAL
- en: Reads and writes can be performed out of order. In contrast to TSO where, as
    we have seen, there is no local reordering except of reads after writes to different
    addresses, the ARM architecture allows local reordering (unless otherwise specified
    using special instructions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A write is not guaranteed to be visible to all threads at the same time as it
    was in the Intel architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, this relatively non-restrictive memory ordering allows the cores
    to reorder instructions more freely, potentially increasing multicore performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must say here that the more relaxed the memory order is, the more difficult
    it is to reason about the executed code, and the more challenging it becomes to
    correctly synchronize multiple threads with atomic operations. Also, you should
    bear in mind that the atomicity is always guaranteed irrespective of the memory
    order.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have seen what is meant by order when accessing memory and
    how the ordering we specify in the code may not be the same order in which the
    CPU executes the code. In the next section, we will see how to enforce some ordering
    using atomic types and operations.
  prefs: []
  type: TYPE_NORMAL
- en: Enforcing ordering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen already in [*Chapter 4*](B22219_04.xhtml#_idTextAnchor074) and
    earlier in this chapter that non-atomic operations on the same memory addresses
    executed from different threads may cause data races and undefined behavior. To
    enforce the ordering of the operations between threads, we will use atomic types
    and their operations. This section will explore what the use of atomics achieves
    in multithreaded code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following simple example will help us to see what can be done with atomic
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this example, **reader()** waits until the **ready** variable is **true**
    and then prints a message set by **writer()** . The **writer()** function sets
    the message and then sets the **store** variable to **true** .
  prefs: []
  type: TYPE_NORMAL
- en: 'Atomic operations provide us with two features for enforcing a certain order
    of execution in multithreaded code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Happens before** : In the preceding code, **[1]** (setting the **message**
    variable) happens before **[2]** (setting the atomic **ready** variable to **true**
    ). Also, **[3]** , reading the **ready** variable in a loop until it is **true**
    , happens before **[4]** , printing the message. In this case, we are using sequential
    consistency memory order (the default memory order).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synchronizes with** : This only happens between atomic operations. In the
    preceding example, this means that when **ready** is set by **[1]** the value
    will be visible for subsequent reads (or writes) in different threads (of course,
    it is visible in the current thread), and when **ready** is read by **[3]** ,
    the changed value will be visible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have seen how atomic operations enforce memory access order from
    different threads, let’s see in detail each of the memory order options provided
    by the C++ memory model.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start, let’s remember here that the Intel x64 architecture (Intel
    and AMD desktop processors) is quite restrictive in relation to memory order,
    that there is no need for any additional instructions for acquire/release, and
    sequential consistency is cheap in terms of performance cost.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential consistency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sequential consistency guarantees the execution of the program in the way you
    wrote it. In 1979 Leslie Lamport defined sequential consistency as being “ *the
    result of an execution is the same as if the reads and writes occurred in some
    order, and the operations of each individual processor appear in this sequence
    in the order specified by* *its program.* ”
  prefs: []
  type: TYPE_NORMAL
- en: In C++, sequential consistency is specified with the **std::memory_order_seq_cst**
    option. This is the most stringent memory order and it’s also the default one.
    If no ordering option is specified, then sequential consistency will be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The C++ memory model by default ensures sequential consistency in the absence
    of race conditions within your code. Consider it a pact: if we properly synchronize
    our program to prevent race conditions, C++ will maintain the appearance that
    the program executes in the sequence it was written.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this model, all threads must see the same order of operations. Operations
    can still be reordered as far as the visible result of the computation has the
    same result as the result of the unordered code. The instructions and operations
    can be reordered if the reads and writes are performed in the same order as in
    the compiled code. The CPU is free to reorder any other instructions between the
    reads and writes if the dependencies are satisfied. Because of the consistent
    ordering it defines, sequential consistency is the most intuitive form of ordering.
    To illustrate sequential consistency, let’s consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we are using **std::memory_order_seq_cst** when running the code, we
    should note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Operations in each thread are executed in the given order (no reordering of
    atomic operations).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t1** and **t2** update **x** and **y** in order, and **t3** and **t4** see
    the same order. Without this property, **t3** could see **x** and **y** change
    in order, but **t4** could see the opposite.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any other ordering may print **This will never happen** because **t3** and **t4**
    could see the changes to **x** and **y** in the opposite order. We will see an
    example of this in the next section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The sequential consistency in this example means that the following two things
    will happen:'
  prefs: []
  type: TYPE_NORMAL
- en: Each store is seen by all the threads; that is, each store operation synchronizes
    with all the load operations for each variable, and all the threads see these
    changes in the same order they are made
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The operations happen in the same order for each thread (operations run in the
    same order as in the code)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that the order between operations in different threads is not guaranteed
    and instructions from different threads may be executed in any order because the
    threads may be scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: Acquire-release ordering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Acquire-release ordering** is less stringent than sequential consistency
    ordering. We don’t get the total ordering of operations we had with sequential
    consistency ordering, but some synchronization is still possible. In general,
    as we add more freedom to the memory ordering we may see a performance gain, but
    it will get more difficult to reason about the execution order of our code.'
  prefs: []
  type: TYPE_NORMAL
- en: In this ordering model, the atomic load operations are the **std::memory_order_acquire**
    operations, the atomic store operations are the **std::memory_order_release**
    operations, and the atomic read-modify-write operations may be the **std::memory_order_acquire**
    , **std::memory_order_release** or **std::memory_order_acq_rel** operations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Acquire semantics** (used with **std::memory_order_acquire** ) ensure that
    all of the read or write operations in one thread that appear *after* the acquire
    operation in the source code happen after the acquire operation. This prevents
    the memory from reordering the reads and writes that follow the acquire operation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Release semantics** (used with **std::memory_order_release** ) ensure that
    the read or write operations in one thread that appear *before* the release operation
    in the source code are completed before the release operation. This prevents the
    memory reordering of the reads and writes that follow the release operation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows the same code as that shown in the previous section
    about sequential consistency, but in this case, we use the acquire-release memory
    order for the atomic operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this case, it is possible for the value of **z** to be 0. Because we don’t
    have sequential consistency anymore after **t1** sets **x** to **true** and **t2**
    sets **y** to **true** , **t3** and **t4** may have different views of how memory
    access is being performed. Because of the use of the acquire-release memory ordering,
    **t3** may see **x** as **true** and **y** as **false** (remember, there is no
    enforce ordering) and **t4** may see **x** as **false** and **y** as **true**
    . When this happens, the value of **z** will be 0.
  prefs: []
  type: TYPE_NORMAL
- en: Besides **std::memory_order_acquire** , **std::memory_order_release** , and
    **std::memory_order_acq_rel** , the acquire-release memory ordering also includes
    the **std::memory_order_consume** option. We won’t be describing it because according
    to the online C++ reference, “ *the specification of release-consume ordering
    is being revised, and the use of std::memory_order_consume is* *temporarily discouraged*
    .”
  prefs: []
  type: TYPE_NORMAL
- en: Relaxed memory ordering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To perform the atomic operation with **relaxed memory ordering** , we specify
    **std::memory_order_relaxed** as the memory order option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Relaxed memory ordering is the weakest form of synchronization. It offers two
    guarantees:'
  prefs: []
  type: TYPE_NORMAL
- en: Atomicity of the operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atomic operations on the same atomic variable in a single thread are not reordered.
    This is called **modification order consistency** . There is no guarantee, however,
    that the other threads will see these operations in the same order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s consider the following scenario: one thread ( **th1** ) stores values
    into an atomic variable. After a certain random interval of time, the variable
    will be overwritten with a new random value. We should assume for the sake of
    this example, that the sequence written is 2, 12, 23, 4, 6. Another thread, **th2**
    , reads the same variable periodically. The first time the variable is read, **th2**
    gets the value 23. Remember that the variable is atomic and that both load and
    store operations are done using the relaxed memory order.'
  prefs: []
  type: TYPE_NORMAL
- en: If **th2** reads the variable again, it can get the same value or any value
    written *after* the previously read value. It cannot read any value written before
    because the modification order consistency property would be violated. In the
    current example, the second read may get 23, 4, or 6 but not 2 or 12. If we get
    4, th1 will go on to write 8, 19, and 7. Now th2 may get 4, 6, 8, 19, or 7 but
    not any number before 4 and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Between two or more threads, there is no guarantee of any order, but once a
    value is read, a previously written value cannot be read.
  prefs: []
  type: TYPE_NORMAL
- en: The relaxed model cannot be used to synchronize threads, because there is no
    visibility order guarantee, but it is useful in scenarios where operations do
    not need to be coordinated tightly between threads, which can lead to performance
    improvements.
  prefs: []
  type: TYPE_NORMAL
- en: It is generally safe to use when the order of execution does not affect the
    correctness of the program, such as incrementing counters used for statistics
    or reference counters where the exact order of increment is not important.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about the C++ memory model and how it allows the
    order and synchronization of atomic operations with different memory order constraints.
    In the next section, we will see the atomic types and operations provided by the
    C++ Standard Library.
  prefs: []
  type: TYPE_NORMAL
- en: C++ Standard Library atomic types and operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now introduce the data types and functions provided by the C++ Standard
    Library to support atomic types and operations. As we have already seen, an atomic
    operation is an indivisible operation. To be able to perform atomic operations
    in C++, we need to use the atomic types provided by the C++ Standard Library.
  prefs: []
  type: TYPE_NORMAL
- en: C++ Standard Library atomic types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The atomic types provided by the C++ Standard Library are defined in the **<atomic>**
    header file.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the documentation for all the atomic types defined in the **<atomic>**
    header in the online C++ reference, which you can access at [https://en.cppreference.com/w/cpp/atomic/atomic](https://en.cppreference.com/w/cpp/atomic/atomic)
    . We won’t include all the content in this reference here (that’s what the reference
    is for!), but we will introduce the main concepts and use examples to further
    elaborate our explanations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The atomic types provided by the C++ Standard Library are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**std::atomic_flag** : Atomic Boolean type (but different from **std::atomic<bool>**
    ). It is the only atomic type that is guaranteed to be lock-free. It does not
    provide load or store operations. It is the most basic atomic type of all. We
    will use it to implement a very simple mutex-like lock.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std::atomic<T>** : This is a template for defining atomic types. All the
    intrinsic types have their own corresponding atomic type defined using this template.
    The following are some examples of these types:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std::atomic<bool>** (and its alias **atomic_bool** ): We will use this atomic
    type to implement the lazy one-time initialization of a variable from several
    threads.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std::atomic<int>** (and its alias **atomic_int** ): We have seen this atomic
    type already in the simple counter example. We will use it again in an example
    to gather statistics (very similar to the counter example).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std::atomic<intptr_t>** (and its alias **atomic_intptr_t** ).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'C++20 introduced atomic smart pointers: **std::atomic<std::shared_ptr<U>>**
    and **std::atomic<std::weak_ptr<U>>** .'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the release of C++20, there is a new atomic type, **std::atomic_ref<T>**
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will focus on **std::atomic_flag** and some of the **std::atomic**
    types. For the other atomic types we have mentioned here, you can access the online
    C++ reference using the previous link.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before any further explanation of some of these types, there is a very important
    clarification to be made: just because a type is *atomic* , that doesn’t guarantee
    it is *lock-free* . By atomic here, we mean indivisible operation, and by lock-free,
    we mean with special CPU atomic instructions support. If there is no hardware
    support for certain atomic operations, they will be implemented using locks by
    the C++ Standard Library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To check whether an atomic type is lock-free we can use the following member
    function of any of the **std::atomic<T>** types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**bool is_lock_free() const noexcept** : This returns **true** if all the atomic
    operations of this type are lock-free, and **false** otherwise (except for **std::atomic_flag**
    , which is guaranteed to always be lock-free). The rest of the atomic types can
    be implemented using locks such as mutexes to guarantee the atomicity of the operations.
    Also, some atomic types may be lock-free only sometimes. If only aligned memory
    access can be lock-free in a certain CPU, then the misaligned objects of that
    same atomic type will be implemented using locks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is also a constant used to indicate whether an atomic type is always
    lock-free:'
  prefs: []
  type: TYPE_NORMAL
- en: '**static constexpr bool is_always_lock_free = /* implementation defined */**
    : The value of this constant will be **true** if the atomic type is always lock-free
    (even for misaligned objects, for example)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is important to be aware of this: an atomic type is not guaranteed to be
    lock-free. The **std::atomic<T>** template is not a magic mechanism that can turn
    all atomic types into lock-free atomic types.'
  prefs: []
  type: TYPE_NORMAL
- en: C++ Standard Library atomic operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two main types of atomic operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Member functions of atomic types** : For example, **std::atomic<int>** has
    the **load()** member function to atomically read its value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Free functions** : The **const std::atomic_load(const std::atomic<T>* obj)**
    function does exactly the same as the previous one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can access the following code (and the generated assembly code, if you
    are interested) at [https://godbolt.org/z/Yhdr3Y1Y8](https://godbolt.org/z/Yhdr3Y1Y8)
    . This code shows the use of both member functions and free functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Most of the atomic operation functions have a parameter to indicate the memory
    order. We have already explained what the memory order is, and what memory ordering
    types are provided by C++ in the section about the C++ memory model.
  prefs: []
  type: TYPE_NORMAL
- en: Example – simple spin-lock implemented using the C++ atomic flag
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **std::atomic_flag** atomic type is the most basic standard atomic type.
    It only has two states: set and not set (which we can also call true and false).
    It is always lock-free, in contrast to any other standard atomic type. Because
    it is so simple, it is mainly used as a building block.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the code for the atomic flag example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to initialize **std::atomic_flag** before using it. The following code
    shows how to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is the only way to initialize **std::atomic_flag** to a definite value.
    The value of **ATOMIC_FLAG_INIT** is implementation defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the flag is initialized, we can perform two atomic operations on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**clear** : This atomically sets the flag to **false**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**test_and_set** : This atomically sets the flag to **true** and obtains its
    previous value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **clear** function can only be called with a relaxed, release, or sequential
    consistency memory order. The **test_and_set** function can only be called with
    relaxed, acquire, or sequential consistency. Using any other memory order will
    result in undefined behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s see how we can implement a simple spinlock using **std::atomic_flag**
    . First, we know that the operations are atomic, so the thread either clears the
    flag or it doesn’t, and if a thread clears the flag, it is fully cleared. It is
    not possible for the thread to *half-clear* the flag (remember this would be possible
    for some non-atomic flags). The **test_and_set** function is atomic too, so the
    flag is set to **true** , and we get the previous state in just one go.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement the basic spinlock, we need an atomic flag to atomically handle
    the lock status and two functions: **lock()** to acquire the lock (as we have
    for a mutex) and **unlock()** to release the lock.'
  prefs: []
  type: TYPE_NORMAL
- en: Simple spin lock unlock() function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will begin with **unlock()** , the simplest function. It will only reset
    the flag (by making it false) and nothing more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The code is straightforward. If we leave out the **std::memory_order_seq_cst**
    parameter, the strictest memory order option, sequential consistency, will be
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: Simple spin lock lock() function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The lock function has more steps. First, let’s explain what it does: **lock()**
    must see whether the atomic flag is on. If it is off, then turn it on and finish.
    If the flag is on, then keep on looking until another thread turns it off. We
    will use **test_and_set()** to make this function work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code works in the following way: inside a **while** loop, **test_and_set**
    sets the flag to **true** and returns the previous value. If the flag is already
    set, setting it again doesn’t change anything and the function returns **true**
    , so the loop keeps on setting the flag. When, eventually, **test_and_set** returns
    **false** , this means that the flag was cleared and we can exit the loop.'
  prefs: []
  type: TYPE_NORMAL
- en: Simple spin lock issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The simple spin lock implementation has been included in this chapter to introduce
    the use of atomic types ( **std::atomic_flag** , the simplest standard atomic
    type) and operations ( **clear** and **test_and_set** ), but it has some serious
    issues:'
  prefs: []
  type: TYPE_NORMAL
- en: The first of these is its bad performance. The code in the repo will let you
    experiment. Expect the spinlock performance to be much worse than that of the
    mutex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The thread is spinning all the time waiting for the flag to be cleared. This
    busy wait is something to avoid, especially if there is thread contention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can try out the preceding code for this example. We got these results, shown
    in *Table 5.1* , when we ran it. The code adds 1 to a counter 200 million times
    in each thread.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **std::mutex** | **spinlock** | **atomic counter** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| One thread | 1.03 s | 1.33 s | 0.82 s |'
  prefs: []
  type: TYPE_TB
- en: '| Two threads | 10.15 s | 39.14 s | 4.52 s |'
  prefs: []
  type: TYPE_TB
- en: '| Four threads | 24.61 s | 128.84 s | 9.13 s |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5.1: Synchronization primitives profiling results'
  prefs: []
  type: TYPE_NORMAL
- en: We can see from the preceding table how poorly the simple spinlock works and
    how it worsens with the addition of threads. Note that this simple example is
    only for learning and that both the simple **std::mutex** spinlock and the atomic
    counter can be improved so that the atomic type performs better.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have looked at **std::atomic_flag** , the most basic atomic
    type provided by the C++ Standard Library. For further information about this
    type and about the new functionality added in C++20 please refer to the online
    C++ reference, which is available at [https://en.cppreference.com/w/cpp/atomic/atomic_flag](https://en.cppreference.com/w/cpp/atomic/atomic_flag)
    .
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will look at how to create a simple way for a thread
    to tell the main thread how many items it has processed.
  prefs: []
  type: TYPE_NORMAL
- en: Example – thread progress reporting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes we want to check the progress of a thread or be notified when it
    finishes. This can be done in different ways, for example, using a mutex and a
    condition variable, or a shared variable synchronized by a mutex, as we have seen
    in [*Chapter 4*](B22219_04.xhtml#_idTextAnchor074) . We also saw how to use atomic
    operations to synchronize a counter in this chapter. We will use a similar counter
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code implements a thread ( **worker** ) that handles a certain
    number of items (here the handling is simulated just by making the thread sleep).
    Every time the thread handles an item, it increments the variable progress. The
    main thread executes a **while** loop and, in each iteration, it accesses the
    **progress** variable and writes a report of the progress (number of items handled).
    Once all the items are handled, the loop is finished.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we use the **std::atomic<int>** atomic type (an atomic integer)
    and two atomic operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**load()** : This atomically retrieves the value of the **progress** variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**store()** : This atomically modifies the value of the **progress** variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **worker** thread processing **progress** is read and written atomically,
    so no race conditions occur when two threads access the **progress** variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **load()** and **store()** atomic operations have an extra parameter to
    indicate the memory order. In this example, we have used **std::memory_order_relaxed**
    . This is a typical example of the use of the relaxed memory order: one thread
    increases a counter, and another reads it. The only ordering we need is reading
    increasing values and for that, the relaxed memory order is enough.'
  prefs: []
  type: TYPE_NORMAL
- en: Having introduced the **load()** and **store()** atomic operations to atomically
    read and write a variable, let’s see another example of a simple statistic-gathering
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Example – simple statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This example builds on the same idea as the previous one: a thread can use
    atomic operations to communicate progress (for example, the number of items processed)
    to another thread. In this new example, one thread will produce some data that
    another thread will read. We need to synchronize memory access because we have
    two threads sharing the same memory and at least one of them is changing the memory.
    As in the previous example, we will use atomic operations for this purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code declares the atomic variables we are going to use to gather
    statistics – one for the number of items processed and two more (for the total
    processing time and average processing time for each item, respectively):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We use atomic float and double for total time and average time. In the full
    example code, we make sure both types are lock-free, which means they use atomic
    instructions from the CPU (all modern CPUs should have that).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s see how the worker thread uses the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The first line increments the processed items by 1 in an atomic way. The **fetch_add**
    function adds **1** to the variable value and gives back the old value (we are
    not using it in this case).
  prefs: []
  type: TYPE_NORMAL
- en: The second line adds **elapsed_s** (the time it took to process one item in
    seconds) to the **total_time** variable, which we use to keep track of the time
    it takes to process all the items.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the third line computes the mean time for each item by atomically reading
    **total_time** and **processed_items** and atomically writing the result in **average_time**
    . Alternatively, we could use the values from the **fetch_add()** calls to calculate
    the mean time, but they don’t include the last item that was processed. We could
    also do the calculation of **average_time** in the main thread, but we do it in
    the worker thread here, just as an example and to practice using atomic operations.
    Keep in mind that our aim (at least in this chapter) is not so much speed but
    learning how to use atomic operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the full code for the statistics example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s summarize what we have seen up to this point in the current section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'C++ standard atomic types: we used **std::atomic_flag** to implement a simple
    spinlock and we have used some of the **std::atomic<T>** types to implement communication
    of simple data between threads. All the atomic types that we have seen are lock-free.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **load()** atomic operation to atomically read the value of an atomic variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **store()** atomic operation to atomically write a new value to an atomic
    variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**clear()** and **test_and_set()** , the special atomic operations provided
    by **std::atomic_flag** .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**fetch_add()** , to atomically add some value to an atomic variable and get
    its previous value. Integral and floating-point types also implement **fetch_sub()**
    , to subtract a certain value from an atomic variable and return its previous
    value. Some functions for performing bitwise logic operations have been implemented
    just for integral types: **fetch_and()** , **fetch_or()** , and **fetch_xor()**
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table summarizes atomic types and operations. For an exhaustive
    description, please refer to the online C++ reference: [https://en.cppreference.com/w/cpp/atomic/atomic](https://en.cppreference.com/w/cpp/atomic/atomic)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The table shows three new operations: **exchange** , **compare_exchange_weak**
    , and **compare_exchange_strong** . We will explain them using an example later.
    Most of the operations (that is, the functions, not the operators) have another
    parameter for the memory order.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operation** | **atomic_****flag** | **atomic <bool>** | **atomic <integral>**
    | **atomic <floating-point>** | **atomic <other>** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **test_and_set** | YES |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **Clear** | YES |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **Load** |  | YES | YES | YES | YES |'
  prefs: []
  type: TYPE_TB
- en: '| **Store** |  | YES | YES | YES | YES |'
  prefs: []
  type: TYPE_TB
- en: '| **fetch_add, +=** |  |  | YES | YES |  |'
  prefs: []
  type: TYPE_TB
- en: '| **fetch_sub, -=** |  |  | YES | YES |  |'
  prefs: []
  type: TYPE_TB
- en: '| **fetch_and, &=** |  |  | YES |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **fetch_or, &#124;=** |  |  | YES |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **fetch_xor, ^=** |  |  | YES |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **++, --** |  |  | YES |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **Exchange** |  | YES | YES | YES | YES |'
  prefs: []
  type: TYPE_TB
- en: '| **compare_exchange_weak,****compare_exchange_strong** |  | YES | YES | YES
    | YES |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5.2: Atomic types and operations'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review the **is_lock_free()** function and the **is_always_lock_free**
    constant. We saw that if **is_lock_free()** is true, then the atomic type has
    lock-free operations with special CPU instructions. An atomic type can be lock-free
    only sometimes, so the **is_always_lock_free** constant tells us if the type is
    always lock-free. So far, all the types we have seen are lock-free. Let’s see
    what happens when an atomic type is non-lock-free.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows the code for the non-lock-free atomic type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: When you execute the code, you will notice that the **std::atomic<no_lock_free>**
    type is not lock-free. Its size, 512 bytes, is the cause of this. When we assign
    a value to the atomic variable, that value is written *atomically* , but this
    operation does not use CPU atomic instructions, that is, it is not lock-free.
    The implementation of this operation depends on the compiler but, in general,
    it uses either a mutex or a special spinlock (such as Microsoft Visual C++).
  prefs: []
  type: TYPE_NORMAL
- en: The lesson here is that all atomic types have atomic operations, but they are
    not all magically lock-free. If an atomic type is not lock-free, it is always
    better to implement it using locks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that some atomic types are not lock-free. Now we will look at another
    example that shows the atomic operations we have not covered yet: the **exchange**
    and **compare_exchange** operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Example – lazy one-time initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes initializing an object can be costly. For example, a given object
    may need to connect to a database or a server, and establishing this connection
    can take a long time. In these cases, we should initialize the object just before
    its use, and not when we define it in our program. This is called **lazy initialization**
    . Now let’s assume that more than one thread needs to use the object for the first
    time. If more than one thread initializes the object, then different connections
    would be created, and that would be wrong because the object opens and closes
    only one connection. For this reason, multiple initializations must be avoided.
    To ensure the object is initialized only once, we will utilize a method known
    as lazy one-time initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows the code for lazy one-time initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: There are some operations in the atomic type operations table that we saw earlier
    in this chapter that we have not yet discussed. We will now explain **compare_exchange_strong**
    using an example. In the example, we have a variable that starts with a value
    of 0. Several threads are running, each with a unique integer ID (1, 2, 3, and
    so on). We want to set the variable’s value to the ID of the thread that sets
    it first and initialize the variable only once. In [*Chapter 4*](B22219_04.xhtml#_idTextAnchor074)
    , we learned about **std::once_flag** and **std::call_once** , which we could
    use to implement this one-time initialization, but this chapter is about atomic
    types and operations, so we will use those to achieve our goal.
  prefs: []
  type: TYPE_NORMAL
- en: To be sure that the initialization of the **init_thread** variable is done only
    once and to avoid race conditions due to write access from more than one thread,
    we use an atomic **int** . Line **[1]** atomically reads the content of **init_thread**
    . If the value is not 0, then that means it has been already initialized and the
    worker thread does nothing else.
  prefs: []
  type: TYPE_NORMAL
- en: 'The current value of **init_thread** is stored in the **expected** variable,
    which represents the value we expect **init_thread** will have when we try to
    initialize it. Now line **[2]** performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Compare the **init_thread** current value to the **expected** value (which,
    again, is equal to 0).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the comparison is not successful, copy the **init_thread** current value
    into **expected** and return **false** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the comparison is successful, copy the **init_thread** current value into
    **expected** , then set the **init_thread** current value to **i** and return
    **true** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The current thread will have initialized **init_thread** only if **compare_exchange_strong**
    returns **true** . Also, note that we need to perform a comparison again (even
    if line **[1]** returned 0 as the current value of **init_thread** ) because it
    is possible that another thread has already initialized the variable.
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to note that if **compare_exchange_strong** returns **false**
    , then the comparison has failed, and if it returns **true** , then the comparison
    was successful. This is always true of **compare_exchange_strong** . On the other
    hand, **compare_exchange_weak** can fail (i.e., return **false** ) even if the
    comparison is successful. The reason for using it is that in some platforms it
    gives better performance when it is called inside a loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on these two functions, please refer to the online C++
    reference: [https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange](https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section about the C++ Standard Library atomic types and operations,
    we have seen the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used standard atomic types, such as **std::atomic_flag** and
    **std::atomic<int>**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The most-used atomic operations: **load()** , **store()** , and **exchange_compare_strong()**
    / **exchange_compare_weak()**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic examples incorporating these atomic types and operations, including lazy
    one-time initialization and thread progress communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have mentioned several times that most of the atomic operations (functions)
    let us pick the memory order we want to use. In the next section, we will implement
    a lock-free programming example: an SPSC lock-free queue.'
  prefs: []
  type: TYPE_NORMAL
- en: SPSC lock-free queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already looked at the C++ Standard Library’s features for atomics, such
    as atomic types and operations and the memory model and orderings. Now we will
    see a complete example of using atomics to implement an SPSC lock-free queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main features of this queue are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SPSC** : This queue is designed to work with two threads, one pushing elements
    to the queue and another getting elements from the queue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bounded** : This queue has a fixed size. We need a method for checking when
    the queue reaches its capacity and when it has no elements).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lock-free** : This queue uses atomic types that are always lock-free on modern
    Intel x64 CPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before you begin to develop the queue, keep in mind that lock-free is not the
    same as wait-free (also keep in mind that wait-free does not eliminate waiting
    entirely; it just ensures that there is a limit to the number of steps required
    for each queue push/pop). Some aspects that mostly affect performance will be
    discussed in [*Chapter 13*](B22219_13.xhtml#_idTextAnchor267) . In that chapter,
    we will also optimize the queue’s performance. For now, in this chapter, we will
    build an SPSC lock-free queue that is correct and performs adequately – we will
    show how its performance can be improved later.
  prefs: []
  type: TYPE_NORMAL
- en: We used mutex and condition variables to make an SPSC queue in [*Chapter 4*](B22219_04.xhtml#_idTextAnchor074)
    that consumer and producer threads could access safely. This chapter will use
    atomic operations to achieve the same goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will store the items in the queue using the same data structure: **std::vector<T>**
    with a fixed size, that is, a power of 2. This way, we can improve performance
    and find the next head and tail indices quickly without using the modulo operator
    that needs a division instruction. When using lock-free atomic types for better
    performance, we need to pay attention to everything that affects performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we use a power of 2 buffer size?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use a vector to hold the queue items. The vector will have a fixed
    size, say **N** . We will make the vector act similarly to a ring buffer, meaning
    that the index for accessing an element in the vector will loop back to the start
    after the end. The first element will follow the last one. As we learned in [*Chapter
    4*](B22219_04.xhtml#_idTextAnchor074) , we can do this with the modulo operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If the size is, for example, four elements, the index to the next element will
    be calculated as in the preceding code. For the last index, we have the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, as we said, the vector will be a ring buffer because, after the last
    element, we will go back to the first one, then the second one, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this method to get the next index for any buffer size **N** . But
    why do we only use sizes that are powers of 2? The answer is easy: performance.
    The modulo ( **%** ) operator requires a division instruction, which is expensive.
    When the size **N** is a power of 2, we can just do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This is much faster than using the modulo operator.
  prefs: []
  type: TYPE_NORMAL
- en: Buffer access synchronization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To access the queue buffer, we need two indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**head** : The index of the current element to be read'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tail** : The index of the next element to be written'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The consumer thread will use the head index to read and write. The producer
    thread will use the tail index to read and write. We need to synchronize access
    to these variables because of this:'
  prefs: []
  type: TYPE_NORMAL
- en: Only one thread (the consumer) writes **head** , meaning that it can read it
    with relaxed memory ordering because it always sees its own changes. Reading **tail**
    is done by the reader thread and it needs to synchronize with the producer’s writing
    of **tail** , so it needs acquire memory ordering. We could use sequential consistency
    for everything, but we want the best performance. When the consumer thread writes
    **head** , it needs to synchronize with the producer’s read of it, so it needs
    release memory ordering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For **tail** , only the producer thread writes it, so we can use relaxed memory
    ordering to read it, but we need release memory ordering to write it and synchronize
    it with the consumer thread’s reading. To synchronize with the consumer thread’s
    writing, we need acquire memory ordering to read **head** .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The queue class member variables are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we have seen how to synchronize access to queue buffer.
  prefs: []
  type: TYPE_NORMAL
- en: Pushing elements into the queue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we have decided on the data representation of the queue and how to synchronize
    access to its elements, let’s implement the function for pushing elements into
    the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The current tail index, which is the buffer slot where the data item is to be
    pushed (if possible) into the queue, is atomically read in line **[1]** . As we
    mentioned earlier, this read can use **std::memory_order_relaxed** because only
    the producer thread changes this variable, and it is the only thread that calls
    push.
  prefs: []
  type: TYPE_NORMAL
- en: Line **[2]** calculates the next index modulo capacity (remember that the buffer
    is a ring). We need to do this to check whether the queue is full.
  prefs: []
  type: TYPE_NORMAL
- en: We perform the check in line **[3]** . We first atomically read the current
    value of the head using **std::memory_order_acquire** because we want the producer
    thread to observe the modifications that the consumer thread has made to this
    variable. Then we compare its value with the next head index.
  prefs: []
  type: TYPE_NORMAL
- en: If the next tail value is equal to the current head value, then (as per our
    convention) the queue is full, and we return **false** .
  prefs: []
  type: TYPE_NORMAL
- en: If the queue is not full, line **[4]** copies the data item to the queue buffer.
    It is worth commenting here that the data copy is not atomic.
  prefs: []
  type: TYPE_NORMAL
- en: Line **[5]** atomically writes the new tail index value into **tail_** . Then,
    **std::memory_order_release** is used to make the changes visible to the consumer
    thread that atomically reads this variable with **std::memory_order_acquire**
    .
  prefs: []
  type: TYPE_NORMAL
- en: Popping elements from the queue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now see how the **pop** function is implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Line **[1]** atomically reads the current value of **head_** (index for the
    next item to be read). We use **std::memory_order_relaxed** because no order enforcement
    is required due to the **head_** variable being modified only by the consumer
    thread, which is the only thread calling **pop** .
  prefs: []
  type: TYPE_NORMAL
- en: Line **[2]** checks whether the queue is empty. If the current value of **head_**
    is the same as the current value of **tail_** , then the queue is empty, and the
    function just returns **false** . We atomically read the value of **tail_** with
    **std::memory_order_acquire** to see the latest change done to **tail_** by the
    producer thread.
  prefs: []
  type: TYPE_NORMAL
- en: Line **[3]** copies the data from the queue to the item reference passed as
    an argument to **pop** . Again, this copy is not atomic.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, line **[4]** updates the value of **head_** . Again, we atomically
    write the value using **std::memory order_release** for the consumer thread to
    see the changes made to **head_** by the consumer thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the SPSC lock-free queue implementation is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for the full example can be found in the following book repo: [https://github.com/PacktPublishing/Asynchronous-Programming-in-CPP/blob/main/Chapter_05/5x09-SPSC_lock_free_queue.cpp](https://github.com/PacktPublishing/Asynchronous-Programming-in-CPP/blob/main/Chapter_05/5x09-SPSC_lock_free_queue.cpp)'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have implemented an SPSC lock-free queue as an application
    of atomic types and operations. In [*Chapter 13*](B22219_13.xhtml#_idTextAnchor267)
    , we will revisit this implementation and improve its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has introduced atomic types and operations, the C++ memory model,
    and a basic implementation of an SPSC lock-free queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a summary of what we have looked at:'
  prefs: []
  type: TYPE_NORMAL
- en: The C++ Standard Library atomic types and operations, what they are, and how
    to use them with some examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The C++ memory model, and especially the different memory orderings it defines.
    Bear in mind that this is a very complex subject and that this section was just
    a basic introduction to it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement a basic SPSC lock-free queue. As we mentioned previously, we
    will demonstrate how to improve its performance in [*Chapter 13*](B22219_13.xhtml#_idTextAnchor267)
    . Examples of performance-improving actions include eliminating false sharing
    (what happens when two variables are in the same cache line and each variable
    is just modified by one thread) and reducing true sharing. Don’t worry if you
    don’t understand any of this now. We will cover it later and demonstrate how to
    run performance tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a basic introduction to atomic operations to synchronize memory access
    from different threads. In some cases, the use of atomic operations is quite easy,
    similar to gathering statistics and simple counters. More involved applications,
    such as the implementation of an SPSC lock-free queue, require a deeper knowledge
    of atomic operations. The material we have seen in this chapter helps build an
    understanding of the basics and builds a foundation for further study of this
    complex subject.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at promises and futures, two fundamental building
    blocks of asynchronous programming in C++.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Butenhof, 1997] David R. Butenhof, Programming with POSIX Threads, Addison
    Wesley, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Williams, 2019] Anthony Williams, C++ Concurrency in Action, Second Edition,
    Manning, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory Model: Get Your Shared Data Under Control, Jana Machutová, [https://www.youtube.com/watch?v=L5RCGDAan2Y](https://www.youtube.com/watch?v=L5RCGDAan2Y)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*C++ Atomics: From Basic To Advanced* , Fedor Pikus, [https://www.youtube.com/watch?v=ZQFzMfHIxng](https://www.youtube.com/watch?v=ZQFzMfHIxng)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Intel 64 and IA-32 Architectures Software Developer’s Manual, Volume 3A: System
    Programming Guide, Part 1* , Intel Corporation, [https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.pdf](https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.pdf)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 3: Asynchronous Programming with Promises, Futures, and Coroutines'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we shift our focus to the core subject of this book, asynchronous
    programming, a critical aspect of building responsive, high-performance applications.
    We will learn how to execute tasks concurrently without blocking the main execution
    flow by utilizing tools such as promises, futures, packaged tasks, the **std::async**
    function, and coroutines, a revolutionary feature enabling asynchronous programming
    without the overhead of creating threads. We will also cover advanced techniques
    for sharing futures and examine real-world scenarios where these concepts are
    essential. These powerful mechanisms allow us to develop efficient, scalable,
    and maintainable asynchronous software needed for modern software systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B22219_06.xhtml#_idTextAnchor125) , *Promises and Futures*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B22219_07.xhtml#_idTextAnchor143) , *The Async Function*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B22219_08.xhtml#_idTextAnchor164) , *Asynchronous Programming
    Using Coroutines*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
