<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-319"><a id="_idTextAnchor832"/>18</h1>
<h1 id="_idParaDest-320"><a id="_idTextAnchor833"/>Patterns for Concurrency</h1>
<p>The last chapter is dedicated to a set of patterns for use in concurrent programs. Concurrency and C++ have a somewhat complex relationship. On the one hand, C++ is a performance-oriented language, and concurrency is almost always employed to improve performance, so the two are a natural fit. Certainly, C++ was used to develop concurrent programs since the earliest days of the language. On the other hand, for a language so often used for writing concurrent programs, C++ has a surprising dearth of constructs and features that directly address the needs of concurrent programming. These needs are mostly addressed by a wide range of community-developed libraries and, often, application-specific solutions. In this chapter, we will review common problems encountered in the development of concurrent programs and solutions that emerged from years of experience; together, these are the two sides of a design pattern.</p>
<p>The following topics are covered in this chapter:</p>
<ul>
<li>What is the state of concurrency support in C++?</li>
<li><a id="_idTextAnchor834"/>What are the main challenges of concurrency?</li>
<li>The challenges of data synchronization and the C++ tools to meet them</li>
<li>What is the design for concurrency?</li>
<li>What are common patterns for managing concurrent workloads in C++?</li>
</ul>
<h1 id="_idParaDest-321"><a id="_idTextAnchor835"/>Technical requirements</h1>
<p>The example code for this chapter can be found on GitHub at the following link: <a href="https://github.com/PacktPublishing/Hands-On-Design-Patterns-with-CPP-Second-Edition/tree/master/Chapter18">https://github.com/PacktPublishing/Hands-On-Design-Patterns-with-CPP-Second-Edition/tree/master/Chapter18</a>.<a id="_idTextAnchor836"/> Also, basic knowledge of concurrency in general as well as concurrency support in C++ are a pre-requisite.</p>
<h1 id="_idParaDest-322"><a id="_idTextAnchor837"/>C++ and concurrency</h1>
<p>The concept of<a id="_idIndexMarker1127"/> concurrency<a id="_idIndexMarker1128"/> was introduced into the language in C++11, but concurrent programs were written in C++ long before that. This chapter is not meant to be an introduction to concurrency or even an introduction to concurrency in C++. This subject is well-covered in the literature (at the time of publication of this book, one of the works that are both general and up-to-date is the book <em class="italic">C++ Concurrency in Action</em> by Anthony Williams). Also, while concurrency is almost always used to improve performance, we will not directly address performance and optimization issues here; for that, you can refer to my book <em class="italic">The Art of Writing Efficient Programs</em>. We are going to focus on the problems that arise in the design of concurrent software.</p>
<p>There are, broadly speaking, three types of challenges we encounter when developing concurrent programs. First, how to make sure the program is correct even when multiple threads operate on the same data concurrency? Second, how to execute the work of a program on multiple threads to improve the overall performance? Finally, how to design software in a way that allows us to reason about it, understand its functions, and maintain it, all with the added complexity of concurrency.</p>
<p>The first set of challenges broadly relates to data sharing and synchronization. We will examine the related patterns first: the program must be first and foremost correct, and great performance in a program that crashes or produces results that cannot be trusted is useless.</p>
<h1 id="_idParaDest-323"><a id="_idTextAnchor838"/>Synchronization patterns</h1>
<p>Synchronization <a id="_idIndexMarker1129"/>patterns have one overarching purpose: to ensure correct operations on data shared by multiple threads. These patterns are critically important for the absolute majority of concurrent programs. The only programs that do not have any need for synchronization are the ones that execute several entirely independent tasks that do not involve any common data (except for, possibly, reading shared and immutable inputs) and produce separate results. For every other program, there is a need to manage some shared state, which exposes us to the danger of the dreaded data races. Formally, the C++ standard says that concurrent access to the same object (same memory location) without the appropriate synchronization that guarantees exclusive access for each thread results in undefined behavior. To be precise, the behavior is undefined if at least one thread can modify the shared data: if the data is never changed by any thread, then there is no possibility of a data race. There are design patterns that take advantage of that loophole, but let us start with the most widely known synchronization pattern. What comes to mind first when you hear about<a id="_idIndexMarker1130"/> avoiding data races?</p>
<h2 id="_idParaDest-324"><a id="_idTextAnchor839"/>Mutex and locking patterns</h2>
<p>If there is one <a id="_idIndexMarker1131"/>tool for<a id="_idIndexMarker1132"/> writing concurrent programs, it is a mutex. A mutex is used to guarantee exclusive access to the shared data accessed by multiple threads:</p>
<pre class="source-code">
std::mutex m;
MyData data;
...
// On several threads:
m.lock();
transmogrify(data);
m.unlock();</pre>
<p>The data-modifying operation <code>transmogrify()</code> must be guaranteed exclusive access to the shared data: only one thread may do this operation at any given time. The programmer <a id="_idIndexMarker1133"/>uses a <code>lock()</code> and <code>unlock()</code>).</p>
<p>The use of a mutex is sufficient to ensure correct access to the shared data, but this is hardly a good design. The first issue is that it is error-prone: if <code>transmogrify()</code> throws an exception, or if the programmer adds a check for the return value and exits the critical section early, the final <code>unlock()</code> is never executed and the mutex remains locked forever, thus blocking every other thread from ever accessing the data.</p>
<p>This challenge is easily addressed by a particular application of the very general C++ pattern we have already seen in <a href="B19262_05.xhtml#_idTextAnchor199"><em class="italic">Chapter 5</em></a>, <em class="italic">A Comprehensive Look at RAII</em>. All we need is an object to lock and unlock the mutex, and the C++ standard library already provides one, <code>std::lock_guard</code>:</p>
<pre class="source-code">
// Example 01
std::mutex m;
int i = 0;
void add() {
  std::lock_guard&lt;std::mutex&gt; l(m);
  ++i;
}
...
std::thread t1(add);
std::thread t2(add);
t1.join();
t2.join();
std::cout &lt;&lt; i &lt;&lt; std::endl;</pre>
<p>The function <code>add()</code> modifies the shared variable <code>i</code> and, therefore, needs exclusive access; this is provided by the use of the mutex <code>m</code>. Note that if you run this example without the mutex, chances <a id="_idIndexMarker1134"/>are you will get the correct result<a id="_idIndexMarker1135"/> nonetheless because one of the threads will execute before the other. Sometimes the program will fail, and more often it won’t. This doesn’t make it correct, it just makes it hard to debug. You can see the race condition with the help <a id="_idIndexMarker1136"/>of the <code>--sanitize=address</code> to enable it. Remove the mutex from <code>add()</code> (<em class="italic">Example 02</em>), compile with TSAN, run the program, and you will see this:</p>
<pre class="source-code">
WARNING: ThreadSanitizer: data race
...
Location is global 'i' of size 4 at &lt;address&gt;</pre>
<p>There is a lot more information shown to help you figure out which threads have a data race and for which variable. This is a far more reliable way to test for data races than waiting for your program to fail.</p>
<p>In C++17, the use of <code>std::lock_guard</code> is slightly simpler since the compiler figures out the template argument from the constructor:</p>
<pre class="source-code">
// Example 03
std::lock_guard l(m);</pre>
<p>In C++20, we can use <code>std::jthread</code> instead of calling <code>join()</code> explicitly:</p>
<pre class="source-code">
// Example 03
{
  std::jthread t1(add);
  std::jthread t2(add);
}
std::cout &lt;&lt; i &lt;&lt; std::endl;</pre>
<p>Note that care must be taken to destroy the threads before using the result of the computation since the destructor now joins the thread and waits for the calculation to complete. Otherwise, there is another data race: the main thread is reading the value of <code>i</code> while it is being incremented (TSAN finds this race as well).</p>
<p>The use of RAII ensures that every time a mutex is locked it is also unlocked, but this does not avoid other errors that can happen when using mutexes. The most common one is forgetting to use the mutex in the first place. The synchronization guarantees apply only if every thread uses the same mechanism to ensure exclusive access to the data. If even one thread does not use the mutex, even if it’s only to read the data, then the entire program is incorrect.</p>
<p>A pattern was developed to prevent unsynchronized access to the shared. This pattern usually goes by the name “mutex-guarded” or “mutex-protected” and it has two key elements: first, the <a id="_idIndexMarker1137"/>data <a id="_idIndexMarker1138"/>that needs to be protected and the mutex that is used to do so are combined in the same object. Second, the design ensures that every access to the data is protected by the mutex. Here is the basic mutex-guarded class template:</p>
<pre class="source-code">
// Example 04
template &lt;typename T&gt; class MutexGuarded {
  std::mutex m_;
  T data_ {};
  public:
  MutexGuarded() = default;
  template &lt;typename... Args&gt;
  explicit MutexGuarded(Args&amp;&amp;... args) :
    data_(std::forward&lt;Args&gt;(args)...) {}
  template &lt;typename F&gt; decltype(auto) operator()(F f) {
    std::lock_guard&lt;std::mutex&gt; l(m_);
    return f(data_);
  }
};</pre>
<p>As you can see, this template combines the mutex and the data guarded by it, and offers only one way to access the data: by invoking the <code>MutexGuarded</code> object with an arbitrary callable. This<a id="_idIndexMarker1139"/> ensures<a id="_idIndexMarker1140"/> that all data accesses are synchronized:</p>
<pre class="source-code">
// Example 04
MutexGuarded&lt;int&gt; i_guarded(0);
void add() {
  i_guarded([](int&amp; i) { ++i; });
}
...
// On many threads:
std::thread t1(add);
std::thread t2(add);
t1.join();
t2.join();
i_guarded([](int i) { std::cout &lt;&lt; i &lt;&lt; std::endl; });</pre>
<p>These are the most basic versions of the patterns for the correct and reliable use of mutexes. In practice, the needs are often more complex, and so are the solutions: there are much more efficient locks than <code>std::mutex</code> (for example, spinlocks for guarding short computations, which you can find in my book <em class="italic">The Art of Writing Efficient Programs</em>), and there are also more complex locks such as shared and exclusive locks for efficient read-write access. Also, often we have to operate on several shared objects at the same time, which leads to the problem of safely locking multiple mutexes. Many of these problems are solved by more complex variations of the patterns we have just seen. Some call for an entirely different approach to the synchronization of data accesses, and we will see several of those later in this section. Finally, some data access challenges are <a id="_idIndexMarker1141"/>better<a id="_idIndexMarker1142"/> solved at a much higher level of the overall system design; this will also be illustrated in this chapter.</p>
<p>Let us next review different approaches to data sharing that go beyond the commonly used mutex.</p>
<h2 id="_idParaDest-325"><a id="_idTextAnchor840"/>No sharing is the best sharing</h2>
<p>While protecting <a id="_idIndexMarker1143"/>shared data with a mutex does not seem that complicated, in reality, data races are the most common bugs in any concurrent program. While it may seem a useless truism to state that you cannot have data races accessing data you do not share, not sharing is a frequently overlooked alternative to sharing. To put it another way, it is often possible to redesign a program to avoid sharing some of the variables or to restrict access to shared data to a smaller part of the code.</p>
<p>This idea is the basis of a design pattern that is simple to explain but often hard to apply because it requires thinking outside of the box – the thread-specific data pattern. It is also known as “thread-local data,” but the name invites confusion with the C++ <code>thread_local</code> keyword. To illustrate the idea, we consider this example: we need to count certain events that may be happening in multiple threads simultaneously (for this demonstration, it does not matter what is counted). We need the total count of these events in the entire program, so the straightforward approach is to have a shared count and increment it when a thread detects an event (in the demonstration, we count random numbers divisible by 10):</p>
<pre class="source-code">
// Example 05
MutexGuarded&lt;size_t&gt; count;
void events(unsigned int s) {
  for (size_t i = 1; i != 100; ++i) {
    if ((rand_r(&amp;s) % 10) == 0) { // Event!
      count([](size_t&amp; i) { ++i; });
    }
  }
}</pre>
<p>This is a straightforward design; it is not the best one. Notice that while each thread is counting events, it does not need to know how many events were counted by other threads. This is not to be confused with the fact that, in our implementation, each thread needs to know what the current value of the count is so it can correctly increment it. The distinction is subtle but important and suggests an alternative: each thread can count its own events using a thread-specific count, one for each thread. None of these counts are correct, but it doesn’t matter as long as we can add all counts together when we need the correct total event count. There are several possible designs here. We can use a <a id="_idIndexMarker1144"/>local count for the events and update the shared count once before the thread exits:</p>
<pre class="source-code">
// Example 06
MutexGuarded&lt;size_t&gt; count;
void events(unsigned int s) {
  size_t n = 0;
  for (size_t i = 1; i != 100; ++i) {
    if ((rand_r(&amp;s) % 10) == 0) { // Event!
      ++n;
    }
  }
  if (n &gt; 0) count([n](size_t&amp; i) { i += n; });
}</pre>
<p>Any local (stack-allocated) variable declared in a function that is being executed by one or more threads is specific to each thread: there is a unique copy of this variable on the stack of each thread, and each thread accesses its own variable when they all refer to the same name <code>n</code>.</p>
<p>We could also give each thread a unique count variable to increment and add them together in the main thread after all the counting threads are finished:</p>
<pre class="source-code">
// Example 07
void events(unsigned int s, size_t&amp; n) {
  for (size_t i = 1; i != 100; ++i) {
    if ((rand_r(&amp;s) % 10) == 0) ++n;
  }
}</pre>
<p>When calling this counting function on multiple threads, we have to take some precautions. Obviously, we should give each thread its own variable for the count <code>n</code>. This is not enough: due to the hardware-related effect known as “false sharing,” we must also ensure<a id="_idIndexMarker1145"/> that the thread-specific counts are not adjacent in memory (a detailed description of false sharing can be found in my book <em class="italic">The Art of Writing </em><em class="italic">Efficient Programs</em>):</p>
<pre class="source-code">
// Example 07
alignas(64) size_t n1 = 0;
alignas(64) size_t n2 = 0;
std::thread t1(events, 1, std::ref(n1));
std::thread t2(events, 2, std::ref(n2));
t1.join();
t2.join();
size_t count = n1 + n2;</pre>
<p>The <code>alignas</code> attribute ensures 64-byte alignment for each count variable, thus ensuring that there is at least 64 bytes difference between the addresses of <code>n1</code> and <code>n2</code> (64 is the size of the cache line on most modern CPUs, including X86 and ARM). Note the <code>std::ref</code> wrapper that is needed for <code>std::thread</code> to invoke functions that use reference arguments.</p>
<p>The previous example reduces the need for shared data access to once per thread, while the last one does not have any shared data at all; the preferred solution depends on exactly when is the value of the total count needed.</p>
<p>The last example can be examined from a slightly different point of view; it would help to slightly rewrite it:</p>
<pre class="source-code">
// Example 08
struct {
  alignas(64) size_t n1 = 0;
  alignas(64) size_t n2 = 0;
} counts;
std::thread t1(events, 1, std::ref(counts.n1));
std::thread t2(events, 2, std::ref(counts.n2));
t1.join();
t2.join();
size_t count = counts.n1 + counts.n2;</pre>
<p>This does not change<a id="_idIndexMarker1146"/> anything of substance, but we can view the thread-specific counts as parts of the same data structure rather than independent variables created for each thread. This way of thinking leads us to another variant of the thread-specific data pattern: sometimes, multiple threads must operate on the same data, but it may be possible to partition the data and give each thread its own subset to work on.</p>
<p>In the next example, we need to clamp each element in the vector (if an element exceeds the maximum value, it is replaced by this value, so the result is always in the range between zero and the maximum). The computation is implemented by this templated algorithm:</p>
<pre class="source-code">
// Example 09
template &lt;typename IT, typename T&gt;
void clamp(IT from, IT to, T value) {
  for (IT it = from; it != to; ++it) {
    if (*it &gt; value) *it = value;
  }
}</pre>
<p>A production-quality implementation would ensure that the iterator arguments satisfy the iterator requirements and the maximum value is comparable with the iterator value type, but we omit all that for brevity (we had an entire chapter on concepts and other ways to restrict templates).</p>
<p>The <code>clamp()</code> function can be called on any sequence, and sometimes we will be lucky to have separate unrelated data structures we can process independently on multiple threads. But to continue this example, let us say that we have only one vector we need to clamp. All is not lost, however, as we can process non-overlapping parts of it on multiple threads<a id="_idIndexMarker1147"/> with no risk of data races:</p>
<pre class="source-code">
// Example 09
std::vector&lt;int&gt; data = ... data ...;
std::thread t1([&amp;](){
  clamp(data.begin(), data.begin() + data.size()/2, 42);
});
std::thread t2([&amp;](){
  clamp(data.begin() + data.size()/2, data.end(), 42);
});
...
t1.join();
t2.join();</pre>
<p>Even though the data structure in our program is shared between two threads and both threads modify it, this program is correct: for each vector element, there is only one thread that can modify it. But what about the vector object itself? Isn’t it shared between all threads?</p>
<p>We have already highlighted that there is one case when data sharing is allowed without any synchronization: any number of threads can read the same variable as long as no other thread is modifying it. Our example takes advantage of this: all threads read the size of the vector and other data members of the vector object, but no threads change them.</p>
<p>The application of the thread-specific data pattern must be carefully thought through and often requires a good understanding of the data structures. We must be absolutely certain that none of the threads attempt to modify the variables they do share, such as the size and the pointer to the data that are members of the vector object itself. For example, if one of the threads could resize the vector, that would be a data race even if no two threads access the same element: the size of the vector is a variable that is modified by one or more threads without a lock.</p>
<p>The last pattern we want to describe in this subsection applies when several threads need to modify the entire data set (so it cannot be partitioned) but the threads do not need to see the modifications done by other threads. Usually, this happens when the modifications are done as a part of the computation of some result, but the modified data itself is not the final result. In this case, sometimes the best approach is to create a thread-specific copy of the data for each thread. This pattern works best when such copy is a “throw-away” object: each thread needs to modify its copy but the result of the modifications does not need to be committed back to the original data structure.</p>
<p>In the following <a id="_idIndexMarker1148"/>example, we use an algorithm to count unique elements in a vector that sorts the vector in place:</p>
<pre class="source-code">
// Example 10
void count_unique(std::vector&lt;int&gt; data, size_t&amp; count) {
  std::sort(data.begin(), data.end());
  count = std::unique(data.begin(),
                      data.end()) - data.begin();
}</pre>
<p>Also, when we need to count only elements that satisfy a predicate, we erase all other elements first (<code>std::erase_if</code> is a C++20 addition, but is easy to implement in a prior version of C++):</p>
<pre class="source-code">
// Example 10
void count_unique_even(std::vector&lt;int&gt; data, size_t&amp; count) {
  std::erase_if(data, [](int i) { return i &amp; 1; });
  std::sort(data.begin(), data.end());
  count = std::unique(data.begin(),
                      data.end()) - data.begin();
}</pre>
<p>Both are destructive operations on a vector, but they are only means to an end: the altered vector can be discarded once we have our count. The simplest, and often the most efficient, way to compute our counts on several threads simultaneously is to make thread-specific copies of the vector. Actually, we did this already: both counting functions take the vector argument by value and, therefore, make a copy. Usually, this would be a mistake, but in our case, it is intentional and allows both functions to operate on the same vector concurrently:</p>
<pre class="source-code">
// Example 10
std::vector&lt;int&gt; data = ...;
size_t unique_count = 0;
size_t unique_even_count = 0;
{
  std::jthread t1(count_unique, data,
                  std::ref(unique_count));
  std::jthread t2(count_unique_even, data,
                  std::ref(unique_even_count));
}</pre>
<p>Of course, there is <a id="_idIndexMarker1149"/>still concurrent access to the original data, and it is done without a lock: both threads need to make their thread-specific copies. However, this falls under the exception of read-only concurrent access and is safe.</p>
<p>In principle, avoiding data sharing when possible and using mutexes otherwise is sufficient to arrange race-free access to data in any program. However, this may not be an efficient way to accomplish this goal, and good performance is almost always the goal of concurrency. We will now consider several other patterns for concurrent access to shared data that, when applicable, can offer superior performance. We are going to start with synchronization primitives that go beyond a mutex and are specifically designed to allow<a id="_idIndexMarker1150"/> threads to efficiently wait for some event.</p>
<h2 id="_idParaDest-326"><a id="_idTextAnchor841"/>Waiting patterns</h2>
<p>Waiting is a problem <a id="_idIndexMarker1151"/>that is frequently encountered in concurrent programs and takes many forms. We have already seen one: the mutex. Indeed, if two threads are trying to enter the critical section simultaneously, one of them will have to wait. But waiting is not the goal here, just an unfortunate side effect of exclusive access to the critical section. There are other situations where waiting is the primary objective. For example, we may have threads that are waiting for some event to happen. This could be a user interface thread waiting for input (little to no performance requirements) or a thread waiting on a network socket (moderate performance requirements) or even a high-performance thread such as a computing thread in a thread pool waiting for a task to execute (extremely high performance requirements). Not surprisingly, there are different implementations for these scenarios, but fundamentally there are two approaches: polling and notifications. We are going to look at notifications first.</p>
<p>The basic pattern for waiting for a notification<a id="_idIndexMarker1152"/> is the <strong class="bold">condition pattern</strong>. It usually consists of a condition variable and a mutex. One or more threads are blocked waiting on the condition variable. During this time, there is one more thread that locks the mutex (thus guaranteeing exclusive access) and does the work whose completion the other threads are waiting for. Once the work is done, the thread that completed it must release the mutex (so other threads can access the shared data containing the results of this work) and notify the waiting threads that they can proceed.</p>
<p>For example, in a thread pool, the waiting threads are the pool worker threads that are waiting for tasks to be added to the pool. Since the pool task queue is a shared resource, a thread needs exclusive access to push or pop tasks. A thread that adds one or more tasks to the queue must hold the mutex while doing it and then notify the worker threads that there are tasks for them to execute.</p>
<p>Let us now see a very basic example of the notification pattern with just two threads. First, we have the main thread that starts a worker thread and then waits for it to produce some results:</p>
<pre class="source-code">
// Example 11
std::mutex m;
std::condition_variable cv;
size_t n = 0;               // Zero until work is done
// Main thread
void main_thread() {
  std::unique_lock l(m);
  std::thread t(produce);     // Start the worker
  cv.wait(l, []{ return n != 0; });
  ... producer thread is done, we have the lock ...
}</pre>
<p>The locking in this case is provided by <code>std::unique_lock</code>, an object that wraps around a mutex and has a mutex-like interface with <code>lock()</code> and <code>unlock()</code> member functions. The mutex is locked in the constructor and almost immediately unlocked by the <code>wait()</code> function when we start waiting on the condition. When the notification is received, <code>wait()</code> locks the mutex again before returning control to the caller.</p>
<p>Many implementations of waiting and conditions suffer from what is known as spurious wake-up: wait can be interrupted even without notification. This is why we also check whether the results are ready, in our case, by checking the result count <code>n</code>: if it is still zero, there are no<a id="_idIndexMarker1153"/> results, the main thread has been awakened in error and we can go back to waiting (note that the waiting thread must still acquire the mutex before <code>wait()</code> returns, so it must wait for the worker thread to release this mutex).</p>
<p>The worker thread must lock the same mutex before it can access the shared data, then unlock it before notifying the main thread that the work is done:</p>
<pre class="source-code">
// Example 11
// Worker thread
void produce() {
  {
    std::lock_guard l(m);
    ... compute results ...
    n = ... result count ...
  } // Mutex unlocked
  cv.notify_one();          // Waiting thread notified
}</pre>
<p>It is not necessary to hold the mutex the entire time the worker thread is active: its only purpose is to protect the shared data such as the result count <code>n</code> in our example.</p>
<p>The two synchronization primitives <code>std::conditional_variable</code> and <code>std::unique_lock</code> are standard C++ tools for implementing the waiting pattern with a condition. Just as with a mutex, there are many variations.</p>
<p>The alternative to notifications is polling. In this pattern, the waiting thread repeatedly checks whether some condition is met. In C++20, we can implement a simple example of polling wait using <code>std::atomic_flag</code> which is essentially an atomic boolean variable (prior to C++20 we<a id="_idIndexMarker1154"/> could do the same with <code>std::atomic&lt;bool&gt;</code>):</p>
<pre class="source-code">
// Example 12
std::atomic_flag flag;
// Worker thread:
void produce() {
  ... produce the results ...
  flag.test_and_set(std::memory_order_release);
}
// Waiting thread:
void main_thread() {
  flag.clear();
  std::thread t(produce);
  while (!flag.test(std::memory_order_acquire)) {} // Wait
  ... results are ready ...
}</pre>
<p>Atomic operations such as <code>test_and_set()</code> make <a id="_idIndexMarker1155"/>use of <strong class="bold">memory barriers</strong>: a kind of global synchronization flag that ensures that all changes made to the memory before the flag is set (release) are visible to any operation on any other thread that is executed after the flag is tested (acquire). There is a lot more to these barriers, but it is outside of the scope of this book and can be found in many books dedicated to concurrency and efficiency.</p>
<p>The most important difference between this and the previous example is the explicit polling loop for the waiting thread in <em class="italic">Example 12</em>. If the wait is long, this is highly inefficient since the waiting thread is busy computing (reading from memory) the entire time it waits. Any practical implementation would introduce some sleep into the wait loop, but doing so also comes at a cost: the waiting thread will not wake up immediately after the worker thread sets the flag but must finish the sleep first. These efficiency concerns are outside of the scope of this book; here we want to show the overall structure and the components of these patterns.</p>
<p>The boundary between polling and waiting is not always clear. For example, for all we know, <code>wait()</code> could be<a id="_idIndexMarker1156"/> implemented by polling some internal state of the condition variable periodically. In fact, the same atomic flag we just saw can be used to wait for a notification:</p>
<pre class="source-code">
// Example 13
std::atomic_flag flag;
// Worker thread:
void produce() {
  ... produce the results ...
  flag.test_and_set(std::memory_order_release);
  flag.notify_one();
}
// Waiting thread:
void main_thread() {
  flag.clear();
  std::thread t(produce);
  flag.wait(true, std::memory_order_acquire); // Wait
  while (!flag.test(std::memory_order_acquire)) {}
  ... results are ready ...
}</pre>
<p>The call to <code>wait()</code> requires a corresponding call to <code>notify_one()</code> (or <code>notify_all()</code> if we have more than one thread waiting on the flag). Its implementation is almost certainly more efficient than our simple polling loop. After the notification is received and the wait is over, we check the flag to make sure it was really set. The standard says that this is not necessary and <code>std::atomic_flag::wait()</code> does not suffer from spurious wakeups, but TSAN in both GCC and Clang disagree (this could be a false positive in TSAN or a bug in the standard library implementation).</p>
<p>There are many other situations where waiting is needed, and the conditions we need to wait for vary widely. Another common need is to wait for a certain number of events to occur. For example, we may have several threads producing results and we may need all of them to complete their share of the work before the main thread can proceed. This is accomplished<a id="_idIndexMarker1157"/> by waiting on a barrier or a latch. Prior to C++20, we would need to implement these synchronization primitives ourselves or use a third-party library, but in C++20 they became standard:</p>
<pre class="source-code">
// Example 14
// Worker threads
void produce(std::latch&amp; latch) {
  ... do the work ...
  latch.count_down();     // One more thread is done
}
void main_thread() {
  constexpr size_t nthread = 4;
  std::jthread t[nthread];
  std::latch latch(nthread); // Wait for 4 count_down()
  for (size_t i = 0; i != nthread; ++i) {
    t[i] = std::jthread(std::ref(latch));
  }
  latch.wait();   // Wait for producers to finish
  ... results are ready ...
}</pre>
<p>The latch is initialized with the count of events to wait for. It will unlock when that many <code>count_down()</code> calls have been done.</p>
<p>There are many other applications of waiting, but almost all waiting patterns fall broadly into one of the categories we have seen in this section (a specific implementation can have a dramatic effect on performance in a particular case, which is you are far more likely to see custom application-specific versions of these synchronization constructs than you are to find non-standard containers or other basic data structures).</p>
<p>We are now going <a id="_idIndexMarker1158"/>to see several examples of very specialized and very efficient synchronization patterns. They are not for all situations, but when they fit the need, they often offer the best performance.</p>
<h2 id="_idParaDest-327"><a id="_idTextAnchor842"/>Lock-free synchronization patterns</h2>
<p>Most of the time, safely<a id="_idIndexMarker1159"/> accessing <a id="_idIndexMarker1160"/>shared data relies on mutexes. C++ also supports another type of synchronizing concurrent threads: atomic operations. Again, a detailed explanation is outside of the scope of this book, and this section requires some prior knowledge of the atomics.</p>
<p>The basic idea is this: some data types (usually integers) have special hardware instructions that allow a few simple operations such as reading or writing or incrementing the values to be done atomically, in a single event. During this atomic operation, other threads cannot access the atomic variable at all, so if one thread performs an atomic operation, all other threads can see the same variable as it was before the operation or after the operation but not in the middle of the operation. For example, an increment is a read-modify-write operation, but an atomic increment is a special hardware transaction such that once the read began, no other thread can access the variable until the write completes. These atomic operations are often accompanied by memory barriers; we have used them already to ensure that not just atomic but all other operations on all variables in the program are synchronized and free from data races.</p>
<p>The simplest but useful application of atomic operations is for counting. We often need to count something in programs, and in concurrent programs, we may need to count some events that can occur on multiple threads. If we are only interested in the total count after all threads are done, this is best handled by the “non-sharing” or thread-specific counter we saw earlier. But what if all threads need to know the current count as well? We can always use a mutex, but using a mutex to protect a simple increment of an integer is highly inefficient. C++ gives us a better way, the atomic counter:</p>
<pre class="source-code">
// Example 15
std::atomic&lt;size_t&gt; count;
void thread_work() {
  size_t current_count = 0;
  if (... counted even ...) {
    current_count =
      count.fetch_add(1, std::memory_order_relaxed);
  }
}</pre>
<p>There is only one <a id="_idIndexMarker1161"/>shared variable<a id="_idIndexMarker1162"/> in this example, <code>count</code> itself. Since we do not have any other shared data, we have no need for memory barriers (“relaxed” memory order means there are no requirements on the order of accesses to other data). The <code>fetch_add()</code> operation is an atomic increment, it increments <code>count</code> by one and returns the old value of <code>count</code>.</p>
<p>The atomic count can also be used to let multiple threads work on the same data structure without any need for locking: to do this, we need to make sure there is only one thread working on each element of the data structure. When used in this manner, the pattern is often referred to as the atomic index. In the next example, we have an array of data that is shared between all threads:</p>
<pre class="source-code">
// Example 16
static constexpr size_t N = 1024;
struct Data { ... };
Data data[N] {};</pre>
<p>We also have an atomic index that is used by all threads that need to store the results of their work in the array. To do so safely, each thread increments an atomic index and used the pre-increment value as the index into the array. No two atomic increment operations can produce the same value, therefore, each thread gets its own array elements to work on:</p>
<pre class="source-code">
// Example 16
std::atomic&lt;size_t&gt; index(0);
// Many producer threads
void produce(size_t&amp; n) {
  while (... more work … ) {
    const size_t s =
      index.fetch_add(1, std::memory_order_relaxed);
    if (s &gt;= N) return;     // No more space
    data[s] = ... results ...
  }
}</pre>
<p>Each thread <a id="_idIndexMarker1163"/>gets to <a id="_idIndexMarker1164"/>initialize however many array elements it can and stops when it (and all other threads) fill the entire array. The main thread has to wait until all the work is done before accessing any of the results. This cannot be done with just the atomic index since it is incremented when a thread starts working on a particular array element, not when that thread is done with the work. We have to use some other synchronization mechanism to make the main thread wait until all the work is done, such as a latch or, in a simple case, joining the producer threads:</p>
<pre class="source-code">
// Example 16
void main_thread() {
  constexpr size_t nthread = 5;
  std::thread t[nthread];
  for (size_t i = 0; i != nthread; ++i) {
    t[i] = std::thread(produce);
  }
  // Wait for producers. to finish.
  for (size_t i = 0; i != nthread; ++i) {
    t[i].join();
  }
  ... all work is done, data is ready ...
}</pre>
<p>The atomic count is good when we don’t rely on the value of the count to access the results that are already produced. In the last example, the producer threads did not need access to the array elements computed by other threads, and the main thread waits for all threads to complete before accessing the results. Often, this is not the case and we need to access data as it is being produced. This is where memory barriers come in.</p>
<p>The simplest but surprisingly powerful lock-free pattern that relies on memory barriers is known as<a id="_idIndexMarker1165"/> the <a id="_idIndexMarker1166"/>publishing protocol. The pattern is applicable when one thread is producing some data that is going to be made accessible to one or more other threads when it is ready. The pattern looks like this:</p>
<pre class="source-code">
// Example 17
std::atomic&lt;Data*&gt; data;
void produce() {
  Data* p = new Data;
  ... complete *p object ...
  data.store(p, std::memory_order_release);
}
void consume() {
  Data* p = nullptr;
  while (!(p = data.load(std::memory_order_acquire))) {}
  ... safe to use *p ...
}</pre>
<p>The shared variable is an atomic pointer to the data. It is often called the “root” pointer because the data itself may be a complex data structure with multiple pointers connecting its parts. The key requirement of this pattern is that there is only one way to access the entire data structure and this is through the root pointer.</p>
<p>The producer thread builds all the data it needs to produce. It uses a thread-specific pointer, usually a local variable, to access the data. No other thread can see the data yet because the root pointer does not point to it and the local pointer of the producer thread is not shared with other threads.</p>
<p>Finally, when the data is complete, the producer atomically stores the pointer to the data in the shared root pointer. It is often said that the producer atomically publishes the data, hence the name of the pattern, the publishing protocol.</p>
<p>The consumers must wait for the data to be published: as long as the root pointer is null, there is nothing for them to do. They wait for the root pointer to become non-null (the wait does not have to use polling, a notification mechanism is also possible). Once the data is published, the consumer threads can access it through the root pointer. Because there is no other synchronization, no thread can modify the data once it’s published (the<a id="_idIndexMarker1167"/> data<a id="_idIndexMarker1168"/> may contain a mutex or some other mechanism to allow parts of it to be modified safely).</p>
<p>The atomic variable itself is insufficient for this pattern to guarantee no data races: all threads access not just the atomic pointer but the memory it points to. This is why we needed the specific memory barriers: when publishing the data, the producer uses the release barrier to not only initialize the pointer atomically but also ensure that all memory modifications that were done before the atomic write operations on the pointer become visible to anyone who reads the new value of the pointer. The consumer uses the acquire barrier to ensure that any operation on the shared data that is done after the new value of the pointer is read observes the latest state of the shared data as it existed at the moment the data was published. In other words, if you read the value of the pointer and then dereference it, you generally do not know if you will get the latest value of the data the pointer points to. But if you read the pointer with the acquire barrier (and the pointer was written with the release barrier), then you can be sure that you will read (acquire) the data as it was last written (released). Together, the release and acquire barriers guarantee that the consumer sees the shared data exactly as it was seen by the producer at the moment it published the address of the data in the root pointer.</p>
<p>The same pattern can be used to publish completed elements of a larger data structure shared between threads. For example, we can have a producer thread publish how many array<a id="_idIndexMarker1169"/> elements it initialized with <a id="_idIndexMarker1170"/>the results:</p>
<pre class="source-code">
// Example 18
constexpr size_t N = ...;
Data data[N];     // Shared, not locked
std::atomic&lt;size_t&gt; size;
void produce() {
  for (size_t n = 0; n != N; ++n) {
    data[n] = ... results ...
    size.store(n, std::memory_order_release);
  }
}
void consume() {
  size_t n = 0;
  do {
    n = size.load(std::memory_order_acquire);
    ... n elements are safe to access ...
  } while (n &lt; N - 1);
}</pre>
<p>The idea is exactly the same as in the previous example, only instead of the pointer we use the index into an array. In both cases, we have one producer thread that computes and publishes data, and one or more consumer threads that wait for the data to be published. If we need multiple producers, we must use some other synchronization mechanism to ensure that they don’t work on the same data, such as the atomic index we just saw.</p>
<p>In a program with multiple producer and consumer threads, we often have to combine several synchronization patterns. In the next example, we have a large shared data structure organized as an array of pointers to the individual elements. Several producer threads fill this data structure with results; we are going to use the atomic index to ensure that each element is handled by only one producer:</p>
<pre class="source-code">
// Example 19
static constexpr size_t N = 1024;
struct Data { ... };
std::atomic&lt;Data*&gt; data[N] {};
std::atomic&lt;size_t&gt; size(0);     // Atomic index
void produce() {
  Data* p = new Data;
  ... compute *p ...
  const size_t s =
    size.fetch_add(1, std::memory_order_relaxed);
  data[s].store(p, std::memory_order_release);
}</pre>
<p>Our<a id="_idIndexMarker1171"/> producer<a id="_idIndexMarker1172"/> computes the result, then fetches the current index value and, at the same time, increments the index so the next producer cannot get the same index value. The array slot <code>data[s]</code> is, therefore, uniquely reserved for this producer thread. This is enough to avoid sharing conflicts between producers, but the consumers cannot use the same index to know how many elements are already in the array: the index is incremented before the corresponding array element is initialized. For the consumers, we use the publishing protocol: each array element is an atomic pointer that remains null until the data is published. The consumers must wait for a pointer to become non-null before they can access the data:</p>
<pre class="source-code">
// Example 19
void consumer() {
  for (size_t i = 0; i != N; ++i) {
    const Data* p =
      data[i].load(std::memory_order_acquire);
    if (!p) break; // No more data
    ... *p is safe to access ...
  }
}</pre>
<p>In this example, the consumer stops as soon as it finds a data element that is not ready. We could continue scanning the array: some of the subsequent elements may be ready because they were filled by another producer thread. If we do, we have to somehow remember to come back to handle the elements we missed. The right approach depends on the problem we need to solve, of course.</p>
<p>The literature on lock-free programming is extensive and full of (usually) very complex examples. The concurrency patterns we have demonstrated are only the basic building blocks for more <a id="_idIndexMarker1173"/>complex data<a id="_idIndexMarker1174"/> structures and data synchronization protocols.</p>
<p>In the next section, we will see some of the much higher-level patterns that are applicable to the design of such data structures or even entire programs and their major components.</p>
<h1 id="_idParaDest-328"><a id="_idTextAnchor843"/>Concurrent design patterns and guidelines</h1>
<p>Designing and <a id="_idIndexMarker1175"/>implementing concurrent software is hard. Even the basic patterns for controlling access to shared data, such as the ones we saw in the last section, are complex and full of subtle details. Failing to notice one of these details usually results in hard-to-debug data races. To simplify the task of writing concurrent programs, the programming community came up with several guidelines. All of them arise out of earlier disastrous experiences, so take these guidelines seriously. Central to these guidelines is the concept of thread safety guarantees.</p>
<h2 id="_idParaDest-329"><a id="_idTextAnchor844"/>Thread safety guarantees</h2>
<p>While this is not a <a id="_idIndexMarker1176"/>pattern, it is <a id="_idIndexMarker1177"/>a concept that is much broader in scope and one of the key design principles for any concurrent software. Every class, function, module, or component of a concurrent program should specify the thread safety guarantees it provides, as well as the guarantees it requires from the components it uses.</p>
<p>In general, a software component can offer three levels of thread-safety guarantees:</p>
<ul>
<li><strong class="bold">Strong thread safety guarantee</strong>: Any<a id="_idIndexMarker1178"/> number of threads can access this component without restrictions and without encountering undefined behavior. For a function, it means that any number of threads can call this function at the same time (possibly, with some restrictions on parameters). For a class, it means that any number of threads can call member functions of this class concurrently. For a larger component, any number of threads can operate its interfaces (again, possibly with some restrictions). Such components, classes, and data structures are sometimes called thread-safe.</li>
<li><code>const</code> member functions). Only one thread can modify the state of the component at any time, and the locking or another way of ensuring such exclusive access is the responsibility of the caller. Such components, classes, and data structures are sometimes called thread-compatible because you can build a concurrent program from them using the appropriate synchronization mechanisms. All STL containers offer this level of guarantee.</li>
<li><strong class="bold">No thread-safety guarantee</strong>: Such components cannot be used in a concurrent program at all and are sometimes called thread-hostile. These classes and functions often have hidden global states that cannot be accessed in a thread-safe manner.</li>
</ul>
<p>By designing each component to provide certain thread safety guarantees, we can divide the intractable problem of making the entire program thread-safe into a hierarchy of design challenges where the more complex components take advantage of the guarantees <a id="_idIndexMarker1179"/>provided by the simpler ones. Central to this process is the notion of the transactional interface.</p>
<h2 id="_idParaDest-330"><a id="_idTextAnchor845"/>Transactional interface design</h2>
<p>The idea of the <a id="_idIndexMarker1180"/>transactional <a id="_idIndexMarker1181"/>interface design is very simple: every component should have an interface such that every operation is an atomic transaction. From the point of view of the rest of the program, the operation either has not happened yet or is done. No other thread can observe the state of the component during the operation. This can be accomplished using mutexes or any other synchronization scheme that fits the need – the particular implementation can influence performance but is not essential for correctness as long as the interface guarantees transaction processing.</p>
<p>This guideline is most useful for designing data structures for concurrent programs. Here, it is so important that it is generally accepted that one cannot design a thread-safe data structure that does not offer a transactional interface (at least not a useful data structure). For example, we can consider a queue. The C++ standard library offers a <code>std::queue</code> template. As with any other STL container, it offers the weak guarantee: any number of threads can call <code>const</code> methods of the queue as long as no thread calls any non-<code>const</code> methods. Alternatively, any one thread can call a non-<code>const</code> method. To ensure the latter, we have to lock all accesses to the queue with an external mutex. If we want to pursue this approach, we should combine the queue and the mutex in a new class:</p>
<pre class="source-code">
template &lt;typename T&gt; class ts_queue {
  std::mutex m_;
  std::queue&lt;T&gt; q_;
  public:
  ts_queue() = default;
  ...
};</pre>
<p>To push another element onto a queue, we need to lock the mutex, since the <code>push()</code> member function modifies the queue:</p>
<pre class="source-code">
template &lt;typename T&gt; class ts_queue {
  public:
  void push(const T&amp; t) {
    std::lock_guard l(m_);
    q_.push(t);
  }
};</pre>
<p>This works exactly as we want it to: any number of threads can call <code>push()</code> and every element will be added to the queue exactly once (the order is going to be arbitrary if multiple calls happen simultaneously, but this is the nature of concurrency). We have successfully provided the strong thread safety guarantee!</p>
<p>The triumph is going to be short-lived, unfortunately. Let us see what it takes to pop an element from the<a id="_idIndexMarker1182"/> queue. There<a id="_idIndexMarker1183"/> is a member function <code>pop()</code> that removes the element from the queue, so we can protect it with the same mutex:</p>
<pre class="source-code">
template &lt;typename T&gt; class ts_queue {
  public:
  void pop() {
    std::lock_guard l(m_);
    q_.pop();
  }
};</pre>
<p>Notice that this function does not return anything: it removes the oldest element in the queue and destroys it, but that’s not what we need to find out what that element is (or was). For that, we need to use the function <code>front()</code> which returns a reference to the oldest element but does not modify the queue. It is a <code>const</code> member function, so we need to lock it only if we call any non-<code>const</code> functions at the same time; we are going to ignore this optimization possibility for now and always lock this call as well:</p>
<pre class="source-code">
template &lt;typename T&gt; class ts_queue {
  public:
  T&amp; front() const {
    std::lock_guard l(m_);
    return q_.front();
  }
};</pre>
<p>If we call <code>front()</code> from multiple threads and don’t call any other functions, this implementation is sub-optimal, but it is not wrong.</p>
<p>There is one special case we have neglected to mention: if the queue is empty, you should not call <code>pop()</code> or <code>front()</code> – doing so leads to undefined behavior, according to the standard. How do you know if it is safe to pop an element from the queue? You can check if the queue is empty. This is another <code>const</code> member function, and again we are going to over-protect it and lock every call to it:</p>
<pre class="source-code">
template &lt;typename T&gt; class ts_queue {
  public:
  bool empty() const {
    std::lock_guard l(m_);
    return q_.empty();
  }
};</pre>
<p>Now every<a id="_idIndexMarker1184"/> member<a id="_idIndexMarker1185"/> function of the underlying <code>std::queue</code> is protected by a mutex. We can call any of them from any number of threads and be guaranteed that only one thread can access the queue at any time. Technically, we have achieved the strong guarantee. Unfortunately, it is not very useful.</p>
<p>To see why, let us consider the process of removing an element from the queue:</p>
<pre class="source-code">
ts_queue&lt;int&gt; q;
int i = 0;
if (!q.empty()) {
  i = q.front();
  q.pop();
}</pre>
<p>This works fine on one thread, but we didn’t need a mutex for that. It still (mostly) works when we have two threads, one of which is pushing new elements onto the queue and the other one is taking them from the queue. Let us consider what happens when two threads try to pop one element each. First, they both call <code>empty()</code>. Let us assume that the queue is not empty and both calls return <code>true</code>. Then, they both call <code>front()</code>. Since neither thread did a <code>pop()</code> yet, both threads get the same front element. This is not what was supposed to happen if we want each thread to pop an element from the queue. Finally, both threads call <code>pop()</code>, and two elements are removed from the queue. One of these elements we have never seen and will never see again, so we lost some of the data that was enqueued.</p>
<p>But this isn’t the only way it can go wrong. What happens if there is only one element on the queue? Both calls to <code>empty()</code> still return true – a queue with one element is not empty. Both calls to <code>front()</code> still return the (same) front element. The first call to <code>pop()</code> succeeds<a id="_idIndexMarker1186"/> but the <a id="_idIndexMarker1187"/>second one is undefined behavior because the queue is now empty. It is also possible that one thread calls <code>pop()</code> before the other thread calls <code>front()</code> but after it calls <code>empty()</code>. In this case, the second call to <code>front()</code> is also undefined.</p>
<p>We have a perfectly safe and a perfectly useless data structure. Clearly, a thread safety guarantee is not enough. We also need an interface that does not expose us to undefined behavior, and the only way to do this is to perform all three steps of the pop operation (<code>empty()</code>, <code>front()</code>, and <code>pop()</code>) in a single critical section, i.e., without releasing the mutex between the calls. Unless we want the caller to supply their own mutex, the only way to do this is to change the interface of our <code>ts_queue</code> class:</p>
<pre class="source-code">
// Example 20
template &lt;typename T&gt; class ts_queue {
  std::queue&lt;T&gt; q_;
  std::mutex m_;
  public:
  ts_queue() = default;
  template &lt;typename U&gt; void push(U&amp;&amp; u) {
    std::lock_guard l(m_);
    q_.push(std::forward&lt;U&gt;(u));
  }
  std::optional&lt;T&gt; pop() {
    std::lock_guard l(m_);
    if (q_.empty()) return {};
    std::optional&lt;T&gt; res(std::move(q_.front()));
    q_.pop();
    return res;
 }
};</pre>
<p>The <code>push()</code> function is the same as it was before (we made the argument type more flexible, but this is <a id="_idIndexMarker1188"/>not related <a id="_idIndexMarker1189"/>to thread safety). The reason we did not need to change the push operation is that it is already transactional: at the end, the queue has one more element than it had at the beginning of the operation, and the state of the queue is otherwise identical. We just made it atomic by protecting it with a mutex (no other thread that also uses the same mutex correctly can observe our queue in its transitional non-invariant state).</p>
<p>The <code>pop()</code> operation is where the transactional interface looks very different. In order to provide a meaningful thread safety guarantee, we have to provide an operation that returns the front element to the caller and removes it from the queue atomically: no other thread should be able to see the same front element, therefore, we have to lock both <code>front()</code> and <code>pop()</code> on the original queue with the same mutex. We also have to consider the possibility that the queue is empty and we have no front element to return to the caller. What do we return in this case? If we decided to return the front element by value, we would have to default-construct this value (or return some other agreed-upon value that means “no element”). In C++17, a better way is to return a <code>std::optional</code> that holds the front element if there is one.</p>
<p>Now both <code>pop()</code> and <code>push()</code> are atomic and transactional: we can call both methods from as many threads as we want, and the results are always well-defined.</p>
<p>You may wonder why didn’t <code>std::queue</code> offer this transactional interface, to begin with. First, STL was designed long before threads made it into the standard. But the other, very important, reason is that the queue interface was influenced by the need to provide exception safety. Exception safety is the guarantee that the object remains in a well-defined state if an exception is thrown. Here, the original queue interface does very well: <code>empty()</code> just returns the size and cannot throw an exception, <code>front()</code> returns the reference to the front element and also cannot throw, and finally <code>pop()</code> calls the destructor of the front element, which normally does not throw either. Of course, when accessing the front element, the caller’s code may throw (for example, if the caller needs to copy the front element to another object) but the caller is expected to handle that. In any case, the queue itself remains in a well-defined state.</p>
<p>Our thread-safe queue, however, has an exception safety problem: the code that copies the front element of the queue to return it to the caller is now inside <code>pop()</code>. If the copy constructor throws during the construction of the local <code>std::optional</code> variable <code>res</code>, we are probably OK. However, if an exception is thrown when the result is returned to the<a id="_idIndexMarker1190"/> caller (which<a id="_idIndexMarker1191"/> can happen by move or copy), then <code>pop()</code> was already done, so we are going to lose the element we just popped from the queue.</p>
<p>This tension between thread safety and exception safety is often unavoidable and has to be considered when designing thread-safe data structures for concurrent programs. Regardless, it must be reiterated that the only way to design thread-safe data structures or larger modules is to ensure that every interface call is a complete transaction: any steps that are conditionally defined must be packaged into a single transactional call together with the operations that are needed to ensure that such conditions are met. Then, the entire call should be guarded by a mutex or some other way to ensure race-free exclusive access.</p>
<p>Designing thread-safe data structures is generally very hard, especially if we want good performance (and what is the point of concurrency if we don’t?). That is why it is very important to take advantage of any use restrictions or special requirements that allow us to impose restrictions on how these data structures are used. In the next section, we will see one<a id="_idIndexMarker1192"/> common case of such restrictions.</p>
<h2 id="_idParaDest-331"><a id="_idTextAnchor846"/>Data structures with access limitations</h2>
<p>Designing thread-safe<a id="_idIndexMarker1193"/> data structures is so hard that one should look for any opportunity to simplify the requirements and the implementation. If there is any scenario you don’t need right now, think if you can make your code simpler if you do not support that scenario. One obvious case is a data structure that is built by a single thread (no thread safety guarantees needed) then becomes immutable and is accessed by many threads that act as readers (a weak guarantee is sufficient). Any STL container, for example, can operate in this mode as-is. We still need to ensure that no reader can access the container while it’s still being filled with data, but that can be easily done with a barrier or a condition. This is a very useful but rather trivial case. Are there any other restrictions we can make use of?</p>
<p>In this section, we consider a particular use case that occurs quite frequently and allows for much simpler data structures. Specifically, we examine the situation when a particular data structure is accessed by only two threads. One thread is the producer, it adds data to the data structure. The other thread is the consumer, it removes the data. Both threads do modify the data structure but in different ways. This situation occurs rather frequently and often allows for very specialized and very efficient data structure implementations. It probably deserves recognition as a design pattern for concurrent designs, and it already has a commonly recognized name: “single-producer single-consumer data structure.”</p>
<p>In this section, we are going to see an example of a single-producer single-consumer queue. It is a data structure that is frequently used with one producer and one consumer thread, but the ideas we explore here can be used to design other data structures as well. The main distinguishing feature of this queue is going to be that it is lock-free: there are no mutexes in it at all, so we can expect much higher performance from it.</p>
<p>The queue is built on an array of a fixed size, so, unlike a regular queue, it cannot grow indefinitely (this is another common restriction used to simplify lock-free data structures):</p>
<pre class="source-code">
// Example 21
template &lt;typename T, size_t N&gt; class ts_queue {
  T buffer_[N];
  std::atomic&lt;size_t&gt; back_{0};
  std::atomic&lt;size_t&gt; front_{N - 1};
  ...
};</pre>
<p>In our example, we default-construct elements in the array. If this is undesirable, we can also use a properly aligned uninitialized buffer. All accesses to the queue are determined by two atomic variables, <code>back_</code> and <code>front_</code>. The former is the index of the array element that we will write into when we push a new element onto the queue. The latter is the index of the array element we will read from when we need to pop an element from the queue. All array elements in the range [<code>front_</code>, <code>back_</code>) are filled with elements currently on the queue. Note that this range can wrap over the end of the buffer: after using the element <code>buffer_[N-1]</code> the queue does not run out of space but starts again from <code>buffer_[0]</code>. This is known <a id="_idIndexMarker1194"/>as a <strong class="bold">circular buffer</strong>.</p>
<p>How do we use<a id="_idIndexMarker1195"/> these indices to manage the queue? Let us start with the push operation:</p>
<pre class="source-code">
// Example 21
template &lt;typename T, size_t N&gt; class ts_queue {
  public:
  template &lt;typename U&gt; bool push(U&amp;&amp; u) {
    const size_t front =
      front_.load(std::memory_order_acquire);
    size_t back = back_.load(std::memory_order_relaxed);
    if (back == front) return false;
    buffer_[back] = std::forward&lt;U&gt;(u);
    back_.store((back + 1) % N, std::memory_order_release);
    return true;
  }
};</pre>
<p>We need to read the current value of <code>back_</code>, of course: this is the index of the array element we are about to write. We support only one producer, and only the producer thread can increment <code>back_</code>, so we do not need any particular precautions here. We do, however, need to be careful to avoid overwriting any elements already in the queue. To do this we must check the current value of <code>front_</code> (we can read it before or after reading <code>back_</code>, it makes no difference). If the element <code>buffer_[back]</code> that we are about to overwrite is also the front element, then the queue is full and the <code>push()</code> operation fails (note that there is another solution to this problem that is often used in real-time systems: if the queue is full, the oldest element is silently overwritten and lost). After the new element is stored, we atomically increment the <code>back_</code> value to signal to the consumer that this slot is now available for reading. Because we are publishing this memory location, we must use the release barrier. Also note the modular arithmetic: after reaching the array element <code>N-1</code>, we’re looping back to element 0.</p>
<p>Next, let us <a id="_idIndexMarker1196"/>see the <code>pop()</code> operation:</p>
<pre class="source-code">
// Example 21
template &lt;typename T, size_t N&gt; class ts_queue {
  public:
  std::optional&lt;T&gt; pop() {
    const size_t back =
      back_.load(std::memory_order_acquire);
    const size_t front =
     (front_.load(std::memory_order_relaxed) + 1) % N;
    if (front == back) return {};
    std::optional&lt;T&gt; res(std::move(buffer_[front]));
    front_.store(front, std::memory_order_release);
    return res;
  }
};</pre>
<p>Again, we need to read both <code>front_</code> and <code>back_</code>: <code>front_</code> is the index of the element we are about to read, and only the consumer can advance this index. On the other hand, <code>back_</code> is needed to make sure we actually have an element to read: if the front and back are the same, the queue is empty; again, we use <code>std::optional</code> to return a value that might not exist. We must use acquire barrier when reading <code>back_</code> to make sure we see the element values that were written into the array by the producer thread. Finally, we advance <code>front_</code> to ensure that we don’t read the same element again and to make this array slot available to the producer thread.</p>
<p>There are several subtle details here that must be pointed out. Reading <code>back_</code> and <code>front_</code> is not done in a single transaction (it is not atomic). In particular, if the producer reads <code>front_</code> first, it is possible that, by the time it reads <code>back_</code> and compares the two, the consumer has already advanced <code>front_</code>. That does not make our data structure incorrect, though. At worst, the producer can report that the queue is full when in fact, it is no longer full. We could read both values atomically, but this will only degrade the performance, and the caller still has to handle the case when the queue is full. Similarly, when <code>pop()</code> reports that the queue is empty, it may no longer be so by the time the call completes. Again, these are the inevitable complexities of concurrency: each operation reflects the state of the data at some point in time. By the time the caller<a id="_idIndexMarker1197"/> gets the return value and can analyze it, the data may have changed already.</p>
<p>Another note-worthy detail is the careful management of the queue elements’ lifetime. We default-construct all elements in the array, so the proper way to transfer the data from the caller into the queue during <code>push()</code> is by copy or move assignment (<code>std::forward</code> does both). On the other hand, once a value is returned to the caller by <code>pop()</code>, we never need that value again, so the right operation here is move, first into the optional and then into the caller’s return value object. Note that moving an object is not the same as destroying it; indeed, the moved-from array elements are not destroyed until the queue itself is. If an array element is reused, it is copy- or move-assigned a new value, and assignments are two of the three operations that are safe to do on a moved-from object (the third one is the destructor, which we will also call eventually).</p>
<p>The single-producer single-consumer pattern is a common pattern that allows a programmer to greatly simplify their concurrent data structures. There are others, you can find them in books and papers dedicated to concurrent data structures. All these patterns are ultimately designed to help you write data structures that perform correctly and efficiently when accessed by multiple threads. We, however, must move on and finally <a id="_idIndexMarker1198"/>tackle the problem of using these threads to get some useful work done.</p>
<h1 id="_idParaDest-332"><a id="_idTextAnchor847"/>Concurrent execution patterns</h1>
<p>The next group of <a id="_idIndexMarker1199"/>patterns for concurrency we must learn are execution patterns. These patterns are used to organize the computations done on multiple threads. You will find out that, just as with the synchronization patterns we saw earlier, all of these are low-level patterns: most solutions for practical problems must combine these patterns into larger, more complex, designs. This is not because C++ is ill-suited for such larger designs; if anything, it is the opposite: there are so many ways to implement, for example, a thread pool, in C++, that for every concrete application, there is a version that is ideally suited in terms of performance and features. This is why it is hard to describe these more complete solutions as patterns: while the problems they address are common, the solutions vary a great deal. But all of these designs have a number of challenges to resolve, and the solutions to those challenges usually use the same tools over and over, so we can at least describe these more basic challenges and their common solutions as design patterns.</p>
<h2 id="_idParaDest-333"><a id="_idTextAnchor848"/>Active object</h2>
<p>The first concurrent <a id="_idIndexMarker1200"/>execution pattern we are<a id="_idIndexMarker1201"/> going to see is the Active Object. An active object usually encapsulates the code to be executed, the data needed for the execution, and the flow of control needed to execute the code asynchronously. This flow of control could be as simple as a separate thread that the object starts and joins. In most cases, we do not start a new thread for every task, so an active object would have some way to run its code on a multi-threaded executor such as a thread pool. From the caller’s point of view, an active object is an object that the caller constructs, initializes with the data, then tells the object to execute itself, and the execution happens asynchronously.</p>
<p>The basic <a id="_idIndexMarker1202"/>active<a id="_idIndexMarker1203"/> object looks like this:</p>
<pre class="source-code">
// Example 22
class Job {
  ... data ...
  std::thread t_;
  bool done_ {};
  public:
  Job(... args ...) { ... initialize data ... }
  void operator()() {
    t_ = std::thread([this](){ ... computations ... }
  );
  }
  void wait() {
    if (done_) return;
    t_.join();
    done_ = true;
  }
  ~Job() { wait(); }
  auto get() { this-&gt;wait(); return ... results ...; }
};
Job j(... args ...);
j();     // Execute code on a thread
... do other work ...
std::cout &lt;&lt; j.get();  // Wait for results and print them</pre>
<p>In the simplest case shown here, the active object contains a thread that is used to execute the code asynchronously. In most practical cases you would use an executor that schedules the work on one of the threads it manages, but this gets us into implementation-specific details. The execution starts when <code>operator()</code> is called; we can also make the object execute as soon as it is constructed by calling <code>operator()</code> from the constructor. At some point, we have to wait for the results. If we use a separate thread, we can join the thread at that time (and take care to not attempt to join it twice if the caller calls <code>wait()</code> again). If the object represents not a thread but a task in a thread pool or some other executor, we would do the cleanup necessary for that case.</p>
<p>As you can see, once we settle on a particular way to execute code asynchronously, writing active objects with different data and code is a rather repetitive task. Nobody writes active objects the way we just did, we always use some generic reusable framework. There are <a id="_idIndexMarker1204"/>two general approaches to<a id="_idIndexMarker1205"/> implementing such a framework. The first one uses inheritance: the base class does the boilerplate work, and the derived class contains the unique task-specific data and code. Staying with our simple approach to active objects, we could write the base class as follows:</p>
<pre class="source-code">
// Example 23
class Job {
  std::thread t_;
  bool done_ {};
  virtual void operator()() = 0;
  public:
  void wait() {
    if (done_) return;
    t_.join();
    done_ = true;
  }
  void run() {
    t_ = std::thread([this](){ (*this)(); });
  }
  virtual ~Job() { wait(); }
};</pre>
<p>The base object <code>Job</code> contains everything needed to implement the asynchronous control flow: the thread and the state flag needed to join the thread only once. It also defines the way to execute the code by calling the non-virtual function <code>run()</code>. The code that is executed on the thread must be provided by the derived object by overriding <code>operator()</code>. Note that only <code>run()</code> is public, and <code>operator()</code> is not: this is the non-virtual idiom in action (we saw it in <a href="B19262_14.xhtml#_idTextAnchor640"><em class="italic">Chapter 14</em></a>, <em class="italic">The Template Method Pattern and the </em><em class="italic">Non-Virtual Idiom</em>).</p>
<p>The derived<a id="_idIndexMarker1206"/> object is problem-specific, of course, but <a id="_idIndexMarker1207"/>generally looks like this:</p>
<pre class="source-code">
// Example 23
class TheJob final : public Job {
  ... data ...
  void operator()() override { ... work ... }
  public:
  TheJob(... args ...) {
    ... initialize data ...
    this-&gt;run();
  }
  auto get() { this-&gt;wait(); return ... results ...; }
};</pre>
<p>The only subtlety here is the call to <code>run()</code> done at the end of the constructor of the derived object. It is not necessary (we can execute the active object later ourselves) but if we do want the constructor to run it, it has to be done in the derived class. If we start the thread and the asynchronous execution in the base class constructor, then we will have a race between the execution on the thread – <code>operator()</code> – and the rest of the initialization which continues in the derived class constructor. For the same reason, an active object that starts executing from the constructor should not be derived from again; we ensure that by making the object final.</p>
<p>The use of our active object is very simple: we create it, the object starts executing the code in the background (on a separate thread), and when we need the result we ask for it (this may involve waiting):</p>
<pre class="source-code">
// Example 23
TheJob j1(... args ...);
TheJob j2(... args ...);
... do other stuff ...
std::cout &lt;&lt; "Results: " &lt;&lt; j1.get() &lt;&lt; " " &lt;&lt; j2.get();</pre>
<p>If you’ve written <a id="_idIndexMarker1208"/>any concurrent code in C++ at all, you <a id="_idIndexMarker1209"/>will have definitely used an active object already: <code>std::thread</code> is an active object, it lets us execute arbitrary code on a separate thread. There are concurrency libraries for C++ where a thread is a base object and all concrete threads are derived from it. But this is not the approach chosen for the C++ standard thread. It follows the second way to implement a reusable active object: type erasure. If you need to familiarize yourself with it, reread <a href="B19262_06.xhtml#_idTextAnchor266"><em class="italic">Chapter 6</em></a>, <em class="italic">Understanding Type Erasure</em>. Even though <code>std::thread</code> itself is a type-erased active object, we’re going to implement our own just to demonstrate the design (the standard library code is rather hard to read). This time, there is no base class. The framework is provided by a single class:</p>
<pre class="source-code">
// Example 24
class Job {
  bool done_ {};
  std::function&lt;void()&gt; f_;
  std::thread t_;
  public:
  template &lt;typename F&gt; explicit Job(F&amp;&amp; f) :
    f_(f), t_(f_) {}
  void wait() {
     if (done_) return;
     t_.join();
     done_ = true;
  }
  ~Job() { wait(); }
};</pre>
<p>To implement the type-erased callable, we use <code>std::function</code> (we could also use one of the more efficient implementations from <a href="B19262_06.xhtml#_idTextAnchor266"><em class="italic">Chapter 6</em></a>, <em class="italic">Understanding Type Erasure</em>, or implement type erasure ourselves following the same approach). The code supplied by the caller to be executed on a thread comes from the callable <code>f</code> in the constructor argument. Note that the order of the class members is very important: the asynchronous execution starts as soon as the thread <code>t_</code> is initialized, so other data members, in particular, the callable <code>f_</code>, must be initialized before that happens.</p>
<p>To use the active <a id="_idIndexMarker1210"/>object of this style, we need to supply a<a id="_idIndexMarker1211"/> callable. It could be a lambda expression or a named object, for example:</p>
<pre class="source-code">
// Example 24
class TheJob {
  ... data ...
  public:
  TheJob(... args ...) { ... initialize data ... }
  void operator()() { // Callable!
    ... do the work ...
  }
};
Job j(TheJob(... args ...));
j.wait();</pre>
<p>Note that, in this design, there is no easy way to access the data members of the callable <code>TheJob</code>, unless it was created as a named object. For this reason, the results are usually returned through arguments passed to the constructor by reference (the same as we do with <code>std::thread</code>):</p>
<pre class="source-code">
// Example 24
class TheJob {
  ... data ...
  double&amp; res_; // Result
  public:
  TheJob(double&amp; res, ... args ...) : res_(res) {
    ... initialize data ...
  }
  void operator()() { // Callable!
    ... do the work ...
    res_ = ... result ...
  }
};
double res = 0;
Job j(TheJob(res, ... args ...));
j.wait();
std::cout &lt;&lt; res;</pre>
<p>Active objects can <a id="_idIndexMarker1212"/>be found in every concurrent C++ program, but <a id="_idIndexMarker1213"/>some uses of them are common and specialized, and so are recognized as concurrent design patterns in their own right. We will now see several of these patterns.</p>
<h2 id="_idParaDest-334"><a id="_idTextAnchor849"/>Reactor Object pattern</h2>
<p>The Reactor<a id="_idIndexMarker1214"/> pattern often uses for event <a id="_idIndexMarker1215"/>handling or responding to service requests. It solves a specific problem where we have multiple requests for certain actions that are issued by multiple threads; however, the nature of these actions is such that at least part of them must be executed on one thread or otherwise synchronized. The reactor object is the object that services these requests: it accepts requests from multiple threads and executes them.</p>
<p>Here is an example of a reactor that can accept requests to perform a specific computation with caller-supplied inputs and store the results. The requests can come from any number of threads. Each request is allocated a slot in the array of results – that is the part that must be synchronized across all threads. After the slot is allocated, we can do the computations concurrently. To implement this reactor, we are going to use the atomic index<a id="_idIndexMarker1216"/> to allocate unique array <a id="_idIndexMarker1217"/>slots to each request:</p>
<pre class="source-code">
// Example 25
class Reactor {
  static constexpr size_t N = 1024;
  Data data_[N] {};
  std::atomic&lt;size_t&gt; size_{0};
  public:
  bool operator()(... args ...) {
    const size_t s =
      size_.fetch_add(1, std::memory_order_acq_rel);
    if (s &gt;= N) return false;  // Array is full
    data_[s] = ... result ...;
    return true;
  }
  void print_results() { ... }
};</pre>
<p>The calls to <code>operator()</code> are thread-safe: any number of threads can call this operator simultaneously, and each call will add the result of the computation to the next array slot without overwriting any data produced by other calls. To retrieve the results from the object, we can either wait until all requests are done or implement another synchronization mechanism such as the publishing protocol to make calls to <code>operator()</code> and <code>print_results()</code> thread-safe with respect to each other.</p>
<p>Note that usually, a reactor object processes requests asynchronously: it has a separate thread to execute the computations and a queue to channel all the requests to a single thread. We can build such a reactor by combining several patterns we saw earlier, for example, we can add a thread-safe queue to our basic reactor to get an asynchronous reactor (we are about to see an example of such a design).</p>
<p>So far, we focused<a id="_idIndexMarker1218"/> on starting and executing<a id="_idIndexMarker1219"/> jobs, and then we wait for the work to complete. The next pattern focuses on handling the completion of asynchronous tasks.</p>
<h2 id="_idParaDest-335"><a id="_idTextAnchor850"/>Proactor Object pattern</h2>
<p>The Proactor<a id="_idIndexMarker1220"/> pattern is used to execute <a id="_idIndexMarker1221"/>asynchronous tasks, usually long-running, by requests from one or more threads. This sounds a lot like the Reactor, but the difference is what happens when a task is done: in the case of the Reactor, we just have to wait for the work to get done (the wait can be blocking or non-blocking, but in all cases, the caller initiates the check for completion). The Proactor object associates each task with a callback, and the callback is executed asynchronously when the task is done. The Reactor and the Proactor are the synchronous and asynchronous solutions to the same problem: handling the completion of concurrent tasks.</p>
<p>A proactor object typically has a queue of tasks to be executed asynchronously or uses another executor to schedule these tasks. Each task is submitted with a callback, usually a callable. The callback is executed when the task is done; often, the same thread that executed the task will also invoke the callback. Since the callback is always asynchronous, care must be taken if it needs to modify any shared data (for example, any data that is accessed by the thread that submitted the task to the proactor). If there is any data shared between the callback and other threads, it must be accessed in a thread-safe way.</p>
<p>Here is an example of a proactor object that uses the thread-safe queue from the previous section. In this example, each task takes one integer as input and computes a <code>double</code> result:</p>
<pre class="source-code">
// Example 26
class Proactor {
  using callback_t = std::function&lt;void(size_t, double)&gt;;
  struct op_task {
    size_t n;
    callback_t f;
  };
  std::atomic&lt;bool&gt; done_{false}; // Must come before t_
  ts_queue&lt;op_task&gt; q_;           // Must come before t_
  std::thread t_;
  public:
  Proactor() : t_([this]() {
    while (true) {
      auto task = q_.pop();
      if (!task) {                // Queue is empty
        if (done_.load(std::memory_order_relaxed)) {
          return;                 // Work is done
        }
        continue;                 // Wait for more work
      }
      ... do the work ...
      double x = ... result ...
      task-&gt;f(n, x);
    } // while (true)
  }) {}
  template &lt;typename F&gt;
  void operator()(size_t n, F&amp;&amp; f) {
    q_.push(op_task{n, std::forward&lt;F&gt;(f)});
  }
  ~Proactor() {
    done_.store(true, std::memory_order_relaxed);
    t_.join();
  }
};</pre>
<p>The queue stores the work requests which consist of the input and the callable; any number of threads can call <code>operator()</code> to add requests to the queue. A more generic proactor might take a callable for the work request instead of having the computation coded into the concrete proactor object. The proactor executes all the requests in order on a single thread. When the requested computation is done, the thread invokes the callback and passes the result to it. This is how we may use such a proactor object:</p>
<pre class="source-code">
// Example 26
Proactor p;
for (size_t n : ... all inputs ...) {
  p(n, [](double x) { std::cout &lt;&lt; x &lt;&lt; std::endl; });
}</pre>
<p>Note that our proactor executes all callbacks on one thread, and the main thread does not do any output. Otherwise, we would have to protect <code>std::cout</code> with a mutex.</p>
<p>The Proactor <a id="_idIndexMarker1222"/>pattern<a id="_idIndexMarker1223"/> is used to both execute asynchronous events and perform additional actions (callbacks) when these events happen. The last pattern we explore in this section does not execute anything but is used to react to external events.</p>
<h2 id="_idParaDest-336"><a id="_idTextAnchor851"/>Monitor pattern</h2>
<p>The Monitor<a id="_idIndexMarker1224"/> pattern<a id="_idIndexMarker1225"/> is used when we need to observe, or monitor, some conditions and respond to certain events. Usually, a monitor runs on its own thread that is sleeping or waiting most of the time. The thread is awakened either by a notification or simply by the passage of time. Once awakened, the monitor object examines the state of the system it is tasked to observe. It may take certain actions if the specified conditions are met, then the thread goes back to waiting.</p>
<p>We are going to see a monitor implementation that uses a timeout; a monitor with a condition variable can be implemented using the same approach but with the waiting on notification pattern as seen earlier in this chapter.</p>
<p>First, we need something to monitor. Let us say that we have several producer threads that do some computations and store results in an array using an atomic index:</p>
<pre class="source-code">
// Example 27
static constexpr size_t N = 1UL &lt;&lt; 16;
struct Data {... data ... };
Data data[N] {};
std::atomic&lt;size_t&gt; index(0);
void produce(std::atomic&lt;size_t&gt;&amp; count) {
  for (size_t n = 0; ; ++n) {
    const size_t s =
      index.fetch_add(1, std::memory_order_acq_rel);
    if (s &gt;= N) return;
    const int niter = 1 &lt;&lt; (8 + data[s].n);
    data[s] = ... result ...
    count.store(n + 1, std::memory_order_relaxed);
  }
}</pre>
<p>Our producer<a id="_idIndexMarker1226"/> also<a id="_idIndexMarker1227"/> stores the count of results computed by the thread in the <code>count</code> variable passed to it. Here is how we launch the producer threads:</p>
<pre class="source-code">
// Example 27
std::thread t[nthread];
std::atomic&lt;size_t&gt; work_count[nthread] = {};
for (size_t i = 0; i != nthread; ++i) {
  t[i] = std::thread(produce, std::ref(work_count[i]));
}</pre>
<p>We have one result count per thread, so each producer has its own count to increment. Why, then, did we make the counts atomic? Because the counts are also what we are going to monitor: our monitor thread will periodically report on how much work is done. Thus, each work count is accessed by two threads, the producer and the monitor, and we need to either use atomic operations or a mutex to avoid data races.</p>
<p>The monitor is going to be a separate thread that wakes up every now and then, reads the values of the result counts, and reports the progress of the work:</p>
<pre class="source-code">
// Example 27
std::atomic&lt;bool&gt; done {false};
std::thread monitor([&amp;]() {
  auto print = [&amp;]() { ... print work_count[] ... };
  std::cout &lt;&lt; "work counts:" &lt;&lt; std::endl;
  while (!done.load(std::memory_order_relaxed)) {
    std::this_threa<a id="_idTextAnchor852"/><a id="_idTextAnchor853"/>d::sleep_for(
      std::chrono::duration&lt;double, std::milli&gt;(500));
    print();
  }
  print();
});</pre>
<p>The monitor can <a id="_idIndexMarker1228"/>be <a id="_idIndexMarker1229"/>started before the producer threads, or at any time we need to monitor the progress of the work, and it will report how many results are computed by each producer thread, for example:</p>
<pre class="source-code">
work counts:
1096 1083 957 1046 1116 -&gt; 5298/65536
2286 2332 2135 2242 2335 -&gt; 11330/65536
...
13153 13061 13154 12979 13189 -&gt; 65536/65536
13153 13061 13154 12979 13189 -&gt; 65536/65536</pre>
<p>Here we used five threads to compute the total of 64K results, and the monitor reports the counts for each thread and the total result count. To shut down the monitor, we need to set the <code>done</code> flag and join the monitor thread:</p>
<pre class="source-code">
// Example 27
done.store(true, std::memory_order_relaxed);
monitor.join();</pre>
<p>The other common variant of the Monitor pattern is the one where, instead of waiting on a timer, we wait on a condition. This monitor is a combination of the basic monitor and the pattern for waiting on a notification that we saw earlier in this chapter.</p>
<p>The concurrent programming community has come up with many other patterns for solving common problems related to concurrency; most of these can be used in C++ programs but they are not specific to C++. There are C++-specific features such as atomic variables that influence the way we implement and use these patterns. The examples from this chapter should give you enough guidance to be able to adapt any other concurrent pattern to C++.</p>
<p>The description <a id="_idIndexMarker1230"/>of execution patterns mostly <a id="_idIndexMarker1231"/>concludes the brief study of C++ patterns for concurrency. Before you turn the last page, I want to show you a totally different type of concurrent pattern that is just coming to C++.</p>
<h1 id="_idParaDest-337"><a id="_idTextAnchor854"/>Coroutine patterns in C++</h1>
<p>Coroutines are<a id="_idIndexMarker1232"/> a very recent addition to C++: they were introduced in C++20, and<a id="_idIndexMarker1233"/> their present state is a foundation for building libraries and frameworks as opposed to features you should use in the application code directly. It is a complex feature with many subtle details, and it would take an entire chapter to explain what it does (there is a chapter like that in my book <em class="italic">The Art of Writing Efficient Programs</em>). Briefly, coroutines are functions that can suspend and resume themselves. They cannot be forced to suspend, a coroutine continues to execute until it suspends itself. They are used to implement what is known as cooperative multitasking, where multiple streams of execution voluntarily yield control to each other rather than being forcibly preempted by the OS.</p>
<p>Every execution pattern we saw in this chapter, and many more, can be implemented using coroutines. It is, however, too early to say whether this is going to become a common use of coroutines in C++, so we cannot say whether a “proactor coroutine” will ever become a pattern. One application of coroutines, however, is well on the way to becoming a new pattern in C++: the coroutine generator.</p>
<p>This pattern comes into play when we want to take some computation that is normally done with a complex loop and rewrite it as an iterator. For example, let us say that we have a 3D array and we want to iterate over all its elements and do some computation on them. This is easy enough to do with a loop:</p>
<pre class="source-code">
size_t*** a; // 3D array
for (size_t i = 0; i &lt; N1; ++i) {
  for (size_t j = 0; j &lt; N2; ++j) {
    for (size_t k = 0; k &lt; N3; ++k) {
      ... do work with a[i][j][k] ...
    }
  }
}</pre>
<p>But it’s hard to <a id="_idIndexMarker1234"/>write <a id="_idIndexMarker1235"/>reusable code this way: if we need to customize the work we do on each array element, we have to modify the inner loop. It would be much easier if we had an iterator that runs over the entire 3D array. Unfortunately, to implement this iterator we have to turn the loops inside out: first, we increment <code>k</code> until it reaches <code>N3</code>; then, we increment <code>j</code> by one and go back to incrementing <code>k</code>, and so on. The result is a very convoluted code that reduced many a programmer to counting on their fingers to avoid one-off errors:</p>
<pre class="source-code">
// Example 28
class Iterator {
  const size_t N1, N2, N3;
  size_t*** const a;
  size_t i = 0, j = 0, k = 0;
  bool done = false;
  public:
  Iterator(size_t*** a, size_t N1, size_t N2, size_t N3) :
    N1(N1), N2(N2), N3(N3), a(a) {}
  bool next(size_t&amp; x) {
    if (done) return false;
    x = a[i][j][k];
    if (++k == N3) {
      k = 0;
      if (++j == N2) {
        j = 0;
        if (++i == N1) return (done = true);
      }
    }
    return true;
  }
};</pre>
<p>We even took <a id="_idIndexMarker1236"/>a shortcut<a id="_idIndexMarker1237"/> and gave our iterator a non-standard interface:</p>
<pre class="source-code">
// Example 28
Iterator it(a, N1, N2, N3);
size_t val;
while (it.next(val)) {
  ... val is the current array element ...
}</pre>
<p>The implementation is even more convoluted if we want to conform to the STL iterator interface.</p>
<p>Problems such as this, where a complex function such as our nested loop must be suspended in the middle of the execution so the caller can execute some arbitrary code and resume the suspended function, are an ideal fit for coroutines. Indeed, a coroutine that produces the same sequence as our iterator looks very simple and natural:</p>
<pre class="source-code">
// Example 28
generator&lt;size_t&gt;
coro(size_t*** a, size_t N1, size_t N2, size_t N3) {
  for (size_t i = 0; i &lt; N1; ++i) {
    for (size_t j = 0; j &lt; N2; ++j) {
      for (size_t k = 0; k &lt; N3; ++k) {
        co_yield a[i][j][k];
      }
    }
  }
}</pre>
<p>That’s it; we have a<a id="_idIndexMarker1238"/> function <a id="_idIndexMarker1239"/>that takes the parameters necessary to loop over a 3D array, a regular nested loop, and we do something to each element. The secret is in that innermost line where “something” happens: the C++20 keyword <code>co_yield</code> suspends the coroutine and returns the value <code>a[i][j][k]</code> to the caller. It is very similar to the <code>return</code> operator, except <code>co_yield</code> does not exit the coroutine permanently: the caller can resume the coroutine, and the execution continues from the next line after <code>co_yield</code>.</p>
<p>The use of this coroutine is also straightforward:</p>
<pre class="source-code">
// Example 28
auto gen = coro(a, N1, N2, N3);
while (true) {
  const size_t val = gen();
  if (!gen) break;
  ... val is the current array element ...
}</pre>
<p>The coroutine magic happens inside the generator object that is returned by the coroutine. Its implementation is anything but simple, and, if you want to write one yourself, you have to become an expert on C++ coroutines (and do so by reading another book or article). You can find a very minimal implementation in <em class="italic">Example 28</em>, and, with the help of a good reference for coroutines, you can understand its inner working line by line. Fortunately, if all you want is to write code like that shown previously, you don’t really have to learn the details of the coroutines: there are several open-source libraries that provide utility types such as the generator (with slightly different interfaces), and in C++23 <code>std::generator</code> will be added to the standard library.</p>
<p>While it is certainly easier to write the coroutine with a loop and <code>co_yield</code> than the convoluted inverted loop of the iterator, what is the price of this convenience? Obviously, you have to either write a generator or find one in a library, but once that is done, are there any more disadvantages to the coroutines? In general, a coroutine involves more work than<a id="_idIndexMarker1240"/> a<a id="_idIndexMarker1241"/> regular function, but the performance of the resulting code depends greatly on the compiler and can vary with seemingly insignificant changes to the code (as is the case for any compiler optimizations). The coroutines are still quite new and the compilers do not have comprehensive optimizations for them. That being said, the performance of coroutines can be comparable to that of the hand-crafted iterator. For our <em class="italic">Example 28</em>, the current (at the moment of this writing) release of Clang 17 gives the following results:</p>
<pre class="source-code">
Iterator time: 9.20286e-10 s/iteration
Generator time: 6.39555e-10 s/iteration</pre>
<p>On the other hand, GCC 13 gives an advantage to the iterator:</p>
<pre class="source-code">
Iterator time: 6.46543e-10 s/iteration
Generator time: 1.99748e-09 s/iteration</pre>
<p>We can expect the compilers to get better at optimizing coroutines in the future.</p>
<p>Another variant of the coroutine generator is useful when the sequence of values that we want to produce is not limited in advance and we want to generate new elements only when they are needed (lazy generator). Again, the advantage of coroutines is the simplicity of returning results to the caller from inside of the loop.</p>
<p>Here is a simple random number generator implemented as a coroutine:</p>
<pre class="source-code">
// Example 29
generator&lt;size_t&gt; coro(size_t i) {
  while (true) {
    constexpr size_t m = 1234567890, k = 987654321;
    for (size_t j = 0; j != 11; ++j) {
      if (1) i = (i + k) % m; else ++i;
    }
    co_yield i;
  }
}</pre>
<p>This coroutine never<a id="_idIndexMarker1242"/> ends: it <a id="_idIndexMarker1243"/>suspends itself to return the next pseudo-random number <code>i</code>, and every time it is resumed, the execution jumps back into the infinite loop. Again, the generator is a rather complex object with a lot of boilerplate code that you would be better off getting from a library (or waiting until C++23). But once that’s done, the use of the generator is very simple:</p>
<pre class="source-code">
// Example 29
auto gen = coro(42);
size_t random_number = gen();</pre>
<p>Every time you call <code>gen()</code>, you get a new random number (of rather poor quality since we have implemented one of the oldest and simplest pseudo-random number generators, so consider this example useful for illustration only). The generator can be called as many times as you need; when it is finally destroyed, so is the coroutine.</p>
<p>We will likely see more design patterns that take advantage of coroutines develop in the coming years. For now, the generator is the only established one, and just recently at that, so it is fitting to conclude the last chapter of the book on C++ design patterns with the newest addition<a id="_idIndexMarker1244"/> to<a id="_idIndexMarker1245"/> our pattern toolbox.</p>
<h1 id="_idParaDest-338"><a id="_idTextAnchor855"/>Summary</h1>
<p>In this chapter, we explored common C++ solutions<a id="_idTextAnchor856"/> to the problems of developing concurrent software. This is a very different type of problem compared to everything we studied before. Our main concerns here are correctness, specifically, by avoiding data races, and performance. Synchronization patterns are standard ways to control access to shared data to avoid undefined behavior. Execution patterns are the basic building blocks of thread schedulers and asynchronous executors. Finally, the high-level patterns and guidelines for the concurrent design are the ways we, the programmers, keep our <a id="_idTextAnchor857"/>sanity while trying to think about all the things that could happen before, after, or at the same time as one another.</p>
<h1 id="_idParaDest-339"><a id="_idTextAnchor858"/>Questions</h1>
<ol>
<li>What is concurrency?</li>
<li>How does C++ support concurrency?</li>
<li>What are synchronization design patterns?</li>
<li>What are execution design patterns?</li>
<li>What overall guidelines for the design and the architecture of concurrent programs should be followed?</li>
<li>What is a transactional interface?</li>
</ol>
</div>


<div><h1 id="_idParaDest-340"><a id="_idTextAnchor859"/>Assessments</h1>
<h1 id="_idParaDest-341"><a id="_idTextAnchor860"/>Chapter 1, An Introduction to Inheritance and Polymorphism</h1>
<ol>
<li value="1">Objects and classes are the building blocks of a C++ program. By combining data and algorithms (<em class="italic">code</em>) into a single unit, the C++ program represents the components of the system that it models, as well as their interactions.</li>
<li>Public inheritance represents an <em class="italic">is-a</em> relationship between objects—an object of the derived class can be used as if it was an object of the base class. This relation implies that the interface of the base class, with its invariants and restrictions, is also a valid interface for the derived class.Unlike public inheritance, private inheritance says nothing about the interfaces. It expresses a <em class="italic">has-a</em> or <em class="italic">is implemented in terms of </em>relationship. The derived class reuses the implementation provided by the base class. For the most part, the same can be accomplished by composition. Composition should be preferred when possible; however, empty base optimization and (less often) virtual method overrides are valid reasons to use private inheritance.</li>
<li>A polymorphic object in C++ is an object whose behavior depends on its type, and the type is not known at compile time (at least at the point where the behavior in question is requested). An object that is referred to as a base class object can demonstrate the behavior of the derived class if that is its true type. In C++, polymorphic behavior is implemented using virtual functions.</li>
<li>Dynamic cast verifies at run time that the destination type of the cast is valid: it must be either the actual type of the object (the type the object was created with) or one if its base types. It is the latter part, checking all possible base types of an object, that makes dynamic casts expensive.</li>
</ol>
<h1 id="_idParaDest-342"><a id="_idTextAnchor861"/>Chapter 2, Class and Function Templates</h1>
<ol>
<li value="1">A template is not a type; it is a <em class="italic">Factory</em> for many different types with similar structures. A template is written in terms of generic types; substituting concrete types for these generic types results in a type generated from the template.</li>
<li>There are class, function, and variable templates. Each kind of template generates the corresponding entities—functions in the case of function templates, classes (types) from class templates, and variables from variable templates.</li>
<li>Templates can have type and non-type parameters. Type parameters are types. Non-type parameters can be integral or enumerated values or templates (in the case of variadic templates, the placeholders are also non-type parameters).</li>
<li>A template instantiation is the code generated by a template. Usually, the instantiations are implicit; the use of a template forces its instantiation. An explicit instantiation, without use, is also possible; it generates a type or a function that can be used later. An explicit specialization of a template is a specialization where all generic types are specified; it is not an instantiation, and no code is generated until the template is used. It is only an alternative recipe for generating code for these specific types.</li>
<li>Usually, the parameter pack is iterated over using recursion. The compiler will typically inline the code generated by this recursion, so the recursion exists only during compilation (as well as in the head of the programmer reading the code). In C++17 (and, rarely, in C++14), it is possible to operate on the entire pack without recursion.</li>
<li>Lambda expressions are essentially a compact way to declare local classes that can be called like functions. They are used to effectively store a fragment of code in a variable (or, rather, associate the code with a variable) so that this code can be called later.</li>
<li>Concepts impose restrictions on the template parameters. This can be used to avoid substituting types and instantiating a template that would lead to an error in the body of the template. In the more complex cases, concepts can be used to disambiguate the choice between multiple template overloads.</li>
</ol>
<h1 id="_idParaDest-343"><a id="_idTextAnchor862"/>Chapter 3, Memory and Ownership</h1>
<ol>
<li value="1">Clear memory ownership, and by extension, resource ownership, is one of the key attributes of a good design. With clear ownership, resources are certain to be created and made available in time for when they are needed, maintained while they are in use, and released/cleaned up when no longer needed.</li>
<li>Resource leaks, including memory leaks; dangling handles (resource handles, such as pointers, references, or iterators, pointing to resources that do not exist); multiple attempts to release the same resource; multiple attempts to construct the same resource.</li>
<li>Non-ownership, exclusive ownership, shared ownership, as well as conversion between different types of ownership and transfer of ownership.</li>
<li>Ownership-agnostic functions and classes should refer to objects by raw pointers and references if the corresponding ownership is handled through owning pointers. If the objects are owned by rich pointers or containers, the problem becomes more difficult. If the additional data contained in the rich pointer is not needed or a single element of a container is accessed, raw pointers and references are perfectly adequate. Otherwise, ideally, we would use a corresponding non-owning reference object like <code>std::string_view</code> or one of the views from the ranges library. If none is available, we may have to pass the owning object itself by reference.</li>
<li>Exclusive memory ownership is easier to understand and follow the control flow of the program. It is also more efficient.</li>
<li>Preferably, by allocating the object on the stack or as a data member of the owning class (including container classes). If reference semantics or certain move semantics are needed, a unique pointer should be used. For conditionally constructed objects, <code>std::optional</code> is a great solution.</li>
<li>Shared ownership should be expressed through a shared pointer such as <code>std::shared_ptr</code>.</li>
<li>Shared ownership in a large system is difficult to manage and may delay the deallocation of resources unnecessarily. It also has a nontrivial performance overhead, compared to exclusive ownership. Maintaining shared ownership in a thread-safe concurrent program requires very careful implementation.</li>
<li>Views such as <code>std::string_view</code>, <code>std::span</code>, and views from <code>std::ranges</code> are essentially non-owning rich pointers. A string view to a string is what a raw pointer is to a unique pointer: a non-owning object containing the same information as the corresponding owning object.</li>
</ol>
<h1 id="_idParaDest-344"><a id="_idTextAnchor863"/>Chapter 4, Swap - From Simple to Subtle</h1>
<ol>
<li value="1">The swap function exchanges the state of the two objects. After the swap call, the objects should remain unchanged, except for the names they are accessed by.</li>
<li>Swap is usually employed in programs that provide commit-or-rollback semantics; a temporary copy of the result is created first, then swapped into its final destination only if no errors were detected.</li>
<li>The use of swap to provide commit-or-rollback semantics assumes that the swap operation itself cannot throw an exception or otherwise fail and leave the swapped objects in an undefined state.</li>
<li>A non-member swap function should always be provided, to ensure that the calls to non-member swap are executed correctly. A member swap function can also be provided, for two reasons—first, it is the only way to swap an object with a temporary, and second, the swap implementation usually needs access to the private data members of the class. If both are provided, the non-member function should call the member swap function on one of the two parameters.</li>
<li>All STL containers and some other standard library classes provide a member function <code>swap()</code>. In addition, the non-member <code>std::swap() </code>function template has standard overloads for all STL types.</li>
<li>The <code>std:: qualifier</code> disables the argument-dependent lookup and forces the default <code>std::swap</code> template instantiation to be called, even if a custom swap function was implemented with the class. To avoid this problem, it is recommended to also provide an explicit instantiation of the <code>std::swap</code> template.</li>
</ol>
<h1 id="_idParaDest-345"><a id="_idTextAnchor864"/>Chapter 5, Comprehensive Look at RAII</h1>
<ol>
<li value="1">Memory is the most common resource, but any object can be a resource. Any virtual or physical quantity that the program operates on is a resource.</li>
<li>Resources should not be lost (leaked). If a resource is accessed through a handle, such as a pointer or an ID, that handle should not be dangling (referring to a resource that does not exist). Resources should be released when they are no longer needed, in the manner that corresponds to the way they were acquired.</li>
<li>Resource Acquisition Is Initialization is an idiom; it is the dominant C++ approach to resource management, where each resource is owned by an object, acquired in the constructor, and released in the destructor of that object.</li>
<li>An RAII object should always be created on the stack or as a data member of another object. When the flow of the program leaves the scope containing the RAII object or the larger object containing the RAII object is deleted, the destructor of the RAII object is executed. This happens regardless of how the control flow leaves the scope.</li>
<li>If each resource is owned by an RAII object and the RAII object does not give out raw handles (or the user is careful to not clone the raw handle), the handle can only be obtained from the RAII object and the resource is not released as long as that object remains.</li>
<li>The most frequently used is <code>std::unique_ptr</code> for memory management; <code>std::lock_guard</code> is used to manage mutexes.</li>
<li>As a rule, RAII objects must be non-copyable. Moving an RAII object transfers the ownership of the resource; the classic RAII pattern does not support this, so most RAII objects should be non-movable (differentiate between <code>std::unique_ptr</code> and <code>const std::unique_ptr</code>).</li>
<li>RAII has difficulty handing release failures, because exceptions cannot propagate from the destructors, and hence there is no good way to report the failure to the caller. For that reason, failing to release a resource often results in undefined behavior (this approach is sometimes taken by the C++ standard as well).</li>
</ol>
<h1 id="_idParaDest-346"><a id="_idTextAnchor865"/>Chapter 6, Understanding Type Erasure</h1>
<ol>
<li value="1">Type erasure is a programming technique where the program, as written, does not show an explicit dependence on some of the types it uses. It is a powerful design tool when used for separating abstract behavior from a particular implementation.</li>
<li>The implementation involves either a polymorphic object and a virtual function call, or a function that is implemented specifically for the erased type and is invoked through a function pointer. Usually, this is combined with generic programming to construct such polymorphic objects or generate functions from a template automatically and ensure that the reified type is always the same as the one provided during construction.</li>
<li>A program may be written in a way that avoids explicit mention of most types. The types are deduced by template functions and declared as <code>auto</code> or as template-deduced typedef types. However, the actual types of objects that are hidden by <code>auto</code> still depend on all types the object operates on (such as the deleter type for a pointer). The erased type is not captured by the object type at all. In other words, if you could get the compiler to tell you what this particular auto stands for, all types would be explicitly there. But if the type was erased, even the most detailed declaration of the containing object will not reveal it (such as <code>std::shared_ptr&lt;int&gt;</code> —this is the entire type, the deleter type is not there).</li>
<li>The type is reified by the function that is generated for that type: while its signature (arguments) does not depend on the erased type, the body of the function does. Usually, the first step is casting one of the arguments from a generic pointer such as <code>void*</code> to the pointer to the erased type.</li>
<li>The performance of type erasure always incurs some overhead compared to invoking the same callable directly: there is always an extra indirection and the pointer associated with it. Almost all implementations use runtime polymorphism (virtual functions or dynamic casts) or the equivalent virtual table of function pointers, which increases both the time (indirect function calls) and memory (virtual pointers). The greatest overhead usually comes from additional memory allocations, necessary for storing objects whose size is not known at compile time. If such allocations can be minimized and the additional memory made local to the object, the total overhead at runtime may be quite small (the overhead in memory remains and is often increased by such optimizations).</li>
</ol>
<h1 id="_idParaDest-347"><a id="_idTextAnchor866"/>Chapter 7, SFINAE, Concepts, and Overload Resolution Management</h1>
<ol>
<li value="1">For each function call, it is the set of all functions with the specified name that are accessible from the call location (the accessibility may be affected by the namespaces, nested scopes, and so on).</li>
<li>It is the process of selecting which function in the overload set is going to be called, given the arguments and their types.</li>
<li>For template functions and member functions (and class constructors in C++17), type deduction determines the types of template parameters from the types of the function arguments. For each parameter, it may be possible to deduce the type from several arguments. In this case, the results of this deduction must be the same, otherwise, the type deduction fails. Once the template parameter types are deduced, the concrete types are substituted for the template parameters in all arguments, the return type, and the default arguments. This is a type substitution.</li>
<li>Type substitution, described previously, can result in invalid types, such as a member function pointer for a type that has no member functions. Such substitution failures do not generate compilation errors; instead, the failing overload is removed from the overload set.</li>
<li>This is only in the function declaration (return type, parameter types, and default values). Substitution failures in the body of the function chosen by the overload resolution are hard errors.</li>
<li>If each overload returns a different type, these types can be examined at compile time. The types must have some way to distinguish them, for example, different sizes or different values of embedded constants. For <code>constexpr</code> functions, we can also examine the return values (the function needs the body in this case).</li>
<li>It is used with great care and caution. By deliberately causing substitution failures, we can direct the overload resolution toward a particular overload. Generally, the desired overload is preferred unless it fails; otherwise, the variadic overload remains and is chosen, indicating that the expression we wanted to test was invalid. By differentiating between the overloads using their return types, we can generate a compile-time (<code>constexpr</code>) constant that can be used in conditional compilation.</li>
<li>C++20 constraints offer a more natural, easier-to-understand syntax. They also result in much clearer error messages when a called function does not meet the requirements. Also, unlike SFINAE, constraints are not limited to the substituted parameters of function templates.</li>
<li>The standard did not just define concepts and constraints in the language. It also offers a way of thinking about template restrictions. While a wide range of SFINAE-based techniques exists, the code is easier to read and maintain if the use of SFINAE is restricted to a few powerful approaches that resemble the use of concepts.</li>
</ol>
<h1 id="_idParaDest-348"><a id="_idTextAnchor867"/>Chapter 8, The Curiously Recurring Template Pattern</h1>
<ol>
<li value="1">While not very expensive in absolute numbers (a few nanoseconds at most), a virtual function call is several times more expensive than a non-virtual one, and could easily be an order of magnitude or more slower than an inlined function call. The overhead comes from the indirection: a virtual function is always invoked by a function pointer, and the actual function is unknown at compile time and cannot be inlined.</li>
<li>If the compiler knows the exact function that is going to be called, it can optimize away the indirection and may be able to inline the function.</li>
<li>Just like the runtime polymorphic calls are made through the pointer to the base class, the static polymorphic calls must be also made through a pointer or reference to the base class. In the case of CRTP and static polymorphism, the base type is actually a whole collection of types generated by the base class template, one for each derived class. To make a polymorphic call we have to use a function template that can be instantiated on any of these base types.</li>
<li>When the derived class is called directly, the use of CRTP is quite different from the compile-time equivalent of the virtual functions. It becomes an implementation technique, where common functionality is provided to multiple derived classes, and each one expands and customizes the interface of the base class template.</li>
<li>Strictly speaking, nothing new is needed to use multiple CRTP bases: the derived class can inherit from several such base types, each an instantiation of a CRTP base class template. However, listing these based together with the correct template parameter (derived class itself) for each one becomes cumbersome. It is easier and less error-prone to declare the derived class as a variadic template with a template template parameter and inherit from the entire parameter pack.</li>
</ol>
<h1 id="_idParaDest-349"><a id="_idTextAnchor868"/>Chapter 9, Named Arguments, Method Chaining, and the Builder Pattern</h1>
<ol>
<li value="1">It is easy to miscount arguments, change the wrong argument, or use an argument of the wrong type that happens to convert to the parameter type. Also, adding a new parameter requires changing all function signatures that must pass these parameters along.</li>
<li>The argument values within the aggregate have explicit names. Adding a new value does not require changing the function signatures. Classes made for different groups of arguments have different types and cannot be accidentally mixed.</li>
<li>The named argument idiom permits the use of temporary aggregate objects. Instead of changing each data member by name, we write a method to set the value of each argument. All such methods return a reference to the object itself and can be chained together in one statement.</li>
<li>Method cascading applies multiple methods to the same object. In a method chain, in general, each method returns a new object and the next method applies to it. Often, method chaining is used to cascade methods. In this case, all chained methods return the reference to the original object.</li>
<li>The Builder pattern is a design pattern that uses a separate builder object to construct complex objects. It is used when a constructor is not sufficient or not easy to use to construct an object in its desired fully built state. The need for a builder can arise when the constructor of the object being built cannot be modified (a general object used for a particular purpose), when a desired constructor would have many similar arguments and be hard to use when the construction process is complex, or when the construction process is computationally expensive but some of the results can be reused.</li>
<li>The fluent interface is an interface that uses method chaining to present multiple instructions, commands, or operations that can be executed on an object. In particular, fluent builders are used in C++ to split complex object construction into multiple smaller steps. Some of these steps can be conditional or depend on other data.</li>
</ol>
<h1 id="_idParaDest-350"><a id="_idTextAnchor869"/>Chapter 10, Local Buffer Optimization</h1>
<ol>
<li value="1">Micro-benchmarks can measure the performance of small fragments of code in isolation. To measure the performance of the same fragment in the context of a program, we have to use a profiler.</li>
<li>Processing small amounts of data usually involve a correspondingly small amount of computing and are therefore very fast. Memory allocation adds a constant overhead, not proportional to the data size. The relative impact is larger when the processing time is short. In addition, memory allocation may use a global lock or otherwise serialize multiple threads.</li>
<li>Local buffer optimization replaces external memory allocation with a buffer that is a part of the object itself. This avoids the cost, and the overhead, of an additional memory allocation.</li>
<li>The object has to be constructed and the memory for it must be allocated, regardless of whether any secondary allocations happen. This allocation has some cost – more if the object is allocated on the heap and less if it’s a stack variable – but that cost must be paid before the object can be used. Local buffer optimization increases the size of the object and therefore of the original allocation, but that usually does not significantly affect the cost of that allocation.</li>
<li>Short string optimization involves storing string characters in a local buffer contained inside the string object, up to a certain length of the string.</li>
<li>Small vector optimization involves storing a few elements of the vector’s content in a local buffer contained in the vector object.</li>
</ol>
<h1 id="_idParaDest-351"><a id="_idTextAnchor870"/>Chapter 11, ScopeGuard</h1>
<ol>
<li value="1">An error-safe program maintains a well-defined state (a set of invariants) even if it encounters an error. Exception safety is a particular kind of error safety; it assumes that errors are signaled by throwing expressions. The program must not enter an undefined state when an (allowed) expression is thrown. An exception-safe program may require that certain operations do not throw exceptions.</li>
<li>If a consistent state must be maintained across several actions, each of which may fail, then the prior actions must be undone if a subsequent action fails. This often requires that the actions do not commit fully until the end of the transaction is reached successfully. The final commit operation must not fail (for example, throw an exception), otherwise error safety cannot be guaranteed. The rollback operation also must not fail.</li>
<li>RAII classes ensure that a certain action is always taken when the program leaves a scope, such as a function. With RAII, the closing action cannot be skipped or bypassed, even if the function exits the scope prematurely with an early return or by throwing an exception.</li>
<li>The classic RAII needs a special class for every action. ScopeGuard automatically generates an RAII class from an arbitrary code fragment (at least, if lambda expressions are supported).</li>
<li>If the status is returned through error codes, it cannot. If all errors in the program are signaled by exceptions and any return from a function is a success, we can detect at runtime whether an exception was thrown. The complication is that the guarded operation may itself take place during stack unwinding caused by another exception. That exception is propagating when the guard class has to decide whether the operation succeeded or failed, but its presence does not indicate the failure of the guarded operation (it may indicate that something else failed elsewhere). Robust exception detection must keep track of how many exceptions are propagating at the beginning and at the end of the guarded scope, which is possible only in C++17 (or using compiler extensions).</li>
<li>The ScopeGuard classes are usually template instantiations. This means that the concrete type of the ScopeGuard is unknown to the programmer, or at least difficult to specify explicitly. The ScopeGuard relies on lifetime extension and template argument deduction to manage this complexity. A type-erased ScopeGuard is a concrete type; it does not depend on the code it holds. The downside is that type erasure requires runtime polymorphism and, most of the time, a memory allocation.</li>
</ol>
<h1 id="_idParaDest-352"><a id="_idTextAnchor871"/>Chapter 12, Friend Factory</h1>
<ol>
<li value="1">A non-member friend function has the same access to the members of the class as a member function.</li>
<li>Granting friendship to a template makes every instantiation of this template a friend; this includes instantiations of the same template but with different, unrelated, types.</li>
<li>Binary operators implemented as member functions are always called on the left-hand-side operand of the operator, with no conversions allowed for that object. Conversions are allowed for the right-hand-side operand, according to the type of the argument of the member operator. This creates an asymmetry between expressions such as <code>x + 2</code> and <code>2 + x</code>, where the latter cannot be handled by a member function since the type of <code>2</code> (<code>int</code>) does not have any.</li>
<li>The first operand of the inserter is always the stream, not the object that is printed. Therefore, a member function would have to be on that stream, which is a part of the standard library; it cannot be extended by the user to include user-defined types.</li>
<li>While the details are complex, the main difference is that user-defined conversions (implicit constructors and conversion operators) are considered when calling non-template functions but, for template functions, the argument types must match the parameter types (almost) exactly, and no user-defined conversions are permitted.</li>
<li>Defining an in situ friend function (with the definition immediately following the declaration) in a class template causes every instantiation of that template to generate one non-template, non-member function with the given name and parameter types in the containing scope.</li>
</ol>
<h1 id="_idParaDest-353"><a id="_idTextAnchor872"/>Chapter 13, Virtual Constructors and Factories</h1>
<ol>
<li value="1">There are several reasons, but the simplest is that the memory must be allocated in the amount <code>sizeof(T)</code>, where <code>T</code> is the actual object type, and the <code>sizeof()</code> operator is <code>constexpr</code> (a compile-time constant).</li>
<li>The Factory pattern is a creational pattern that solves the problem of creating objects without having to explicitly specify the type of the object.</li>
<li>While in C++ the actual type has to be specified at the construction point, the Factory pattern allows us to separate the point of construction from the place where the program has to decide what object to construct and identify the type using some alternative identifier, a number, a value, or another type.</li>
<li>The virtual copy constructor is a particular kind of factory where the object to construct is identified by the type of another object we already have. A typical implementation involves a virtual <code>clone()</code> method that is overridden in every derived class.</li>
<li>The Template pattern describes the design where the overall control flow is dictated by the base class, with derived classes providing customizations at certain predefined points. In our case, the overall control flow is that of factory construction, and the customization point is the act of construction of an object (memory allocation and constructor invocation).</li>
<li>The Builder pattern is used when it is necessary (or just more convenient) to delegate the work of constructing an object to another class instead of doing the complete initialization in the constructor. An object that constructs other objects of different types depending on some run-time information using the factory method is also a builder. In addition to the factory itself, such builders, or factory classes, usually have other run-time data that is used for constructing objects and must be stored in another object – in our case, the factory object, which is also the builder object.</li>
</ol>
<h1 id="_idParaDest-354"><a id="_idTextAnchor873"/>Chapter 14, The Template Method Pattern and the  Non-Virtual Idiom</h1>
<ol>
<li value="1">A behavioral pattern describes a way to solve a common problem by using a specific method to communicate between different objects.</li>
<li>The template method pattern is a standard way to implement an algorithm that has a rigid <em class="italic">skeleton,</em> or the overall flow of control, but allows for one or more customization points for specific kinds of problems.</li>
<li>The Template Method lets the sub-classes (derived types) implement specific behaviors of the otherwise generic algorithm. The key to this pattern is the way the base and the derived types interact.</li>
<li>The more common hierarchical approach to design sees the low-level code provide <em class="italic">building blocks</em> from which the high-level code builds the specific algorithm, by combining them in a particular flow of control. In the template pattern, the high-level code does not determine the overall algorithm and is not in control of the overall flow. The lower-level code controls the algorithm and determines when the high-level code is called to adjust specific aspects of the execution.</li>
<li>It is a pattern where the public interface of a class hierarchy is implemented by non-virtual public methods of the base class and the derived classes contain only virtual private methods (as well as any necessary data and non-virtual methods needed to implement them).</li>
<li>A public virtual function performs two separate tasks – it provides the interface (since it is public) and also modifies the implementation. A better separation of concerns is to use virtual functions only to customize the implementation and to specify the common interface using the non-virtual functions of the base class.</li>
<li>Once the NVI is employed, virtual functions can usually be made private. One exception is when the derived class needs to invoke a virtual function of the base class to delegate part of the implementation. In this case, the function should be made protected.</li>
<li>Destructors are called in <em class="italic">nested</em> order, starting from the most derived class. When the destructor for the derived class is done, it calls the destructor of the base class. By that time, the <em class="italic">extra</em> information that the derived class contained is already destroyed, and only the base portion is left. If the base class destructor were to call a virtual function, it would have to be dispatched to the base class (since the derived class is gone by then). There is no way for the base class destructor to call the virtual functions of the derived class.</li>
<li>The fragile base class problem manifests itself when a change to the base class unintentionally breaks the derived class. While not specific to the template method, it affects, potentially, all object-oriented designs, including ones based on the template pattern. In the simplest example, changing the non-virtual public function in the base class, in a way that changes the names of the virtual functions called to customize the behavior of the algorithm, will break all existing derived classes because their current customizations, implemented by the virtual functions with the old names, would suddenly stop working. To avoid this problem, the existing customization points should not be changed.</li>
</ol>
<h1 id="_idParaDest-355"><a id="_idTextAnchor874"/>Chapter 15, Policy-Based Design</h1>
<ol>
<li value="1">The Strategy pattern is a behavioral pattern that allows the user to customize a certain aspect of the behavior of the class by selecting an algorithm that implements this behavior from a set of provided alternatives, or by providing a new implementation.</li>
<li>While the traditional OOP Strategy applies at runtime, C++ combines generic programming with the Strategy pattern in a technique known as policy-based design. In this approach, the primary class template delegates certain aspects of its behavior to the user-specified policy types.</li>
<li>In general, there are almost no restrictions on the policy type, although the particular way in which the type is declared and used imposes certain restrictions by convention. For example, if a policy is invoked as a function, then any callable type can be used. On the other hand, if a specific member function of the policy is called, the policy must necessarily be a class and provide the required member function. Template policies can be used as well but must match the specified number of template parameters exactly.</li>
<li>The two primary ways are composition and inheritance. The composition should generally be preferred; however, many policies in practice are empty classes with no data members and can benefit from empty base class optimization. Private inheritance should be preferred unless the policy must also modify the public interface of the primary class. Policies that need to operate on the primary policy-based class itself often have to employ CRTP. In other cases, when the policy object itself does not depend on the types used in the construction of the primary template, the policy behavior can be exposed through a static member function.</li>
<li>As a general rule, policies that contain only constants and are used to constrain the public interface are easier to write and maintain. However, there are several cases when injecting public member functions through the base class policies is preferred: when we also need to add member variables to the class or when the complete set of public functions would be difficult to maintain or lead to conflicts.</li>
<li>The primary drawback is complexity, in various manifestations. Policy-based types with different policies are, generally, different types (the only alternative, type erasure, usually carries a prohibitive runtime overhead). This may force large parts of the code to be templated as well. Long lists of policies are difficult to maintain and use correctly. For this reason, care should be taken to avoid creating unnecessary or hard-to-justify policies. Sometimes a type with two sufficiently unrelated sets of policies is better to be split into two separate types.</li>
</ol>
<h1 id="_idParaDest-356"><a id="_idTextAnchor875"/>Chapter 16, Adapters and Decorators</h1>
<ol>
<li value="1">The Adapter is a very general pattern that modifies an interface of a class or a function (or a template, in C++) so it can be used in a context that requires a different interface but similar underlying behavior.</li>
<li>The Decorator pattern is a more narrow pattern; it modifies the existing interface by adding or removing behavior but does not convert an interface into a completely different one.</li>
<li>In the classic OOP implementation, both the decorated class and the Decorator class inherit from a common base class. This has two limitations; the most important one is that the decorated object preserves the polymorphic behavior of the decorated class but cannot preserve the interface that is added in a concrete (derived) decorated class and was not present in the base class. The second limitation is that the Decorator is specific to a particular hierarchy. We can remove both limitations using the generic programming tools of C++.</li>
<li>In general, a Decorator preserves as much of the interface of the decorated class as possible. Any functions the behavior of which is not modified are left unchanged. For that reason, public inheritance is commonly used. If a Decorator has to forward most calls to the decorated class explicitly, then the inheritance aspect is less important, and composition or private inheritance can be used.</li>
<li>Unlike Decorators, adapters usually present a very different interface from that of the original class. Composition is often preferred in this case. The exception is compile-time adapters that modify the template parameters but otherwise are essentially the same class template (similar to template aliases). These adapters must use public inheritance.</li>
<li>The main limitation is that it cannot be applied to template functions. It also cannot be used to replace function arguments with expressions containing those arguments.</li>
<li>Template aliases are never considered by the argument type deduction when function templates are instantiated. Both adapter and policy patterns can be used to add or modify the public interface of a class.</li>
<li>Adapters are easy to stack (compose) to build a complex interface one function at a time. The features that are not enabled do not need any special treatment at all; if the corresponding adapter is not used, then that feature is not enabled. The traditional policy pattern requires predetermined slots for every pattern. With the exception of the default arguments after the last explicitly specified one, all policies, even the default ones, must be explicitly specified. On the other hand, the adapters in the middle of the stack do not have access to the final type of the object, which complicates the implementation. The policy-based class is always the final type, and using CRTP, this type can be propagated into the policies that need it.</li>
</ol>
<h1 id="_idParaDest-357"><a id="_idTextAnchor876"/>Chapter 17, The Visitor Pattern and Multiple Dispatch</h1>
<ol>
<li value="1">The Visitor pattern provides a way to separate the implementation of algorithms from the objects they operate on; in other words, it is a way to add operations to classes without modifying them by writing new member functions.</li>
<li>The Visitor pattern allows us to extend the functionality of class hierarchies. It can be used when the source code of the class is not available for modification or when such modifications would be difficult to maintain.</li>
<li>Double dispatch is the process of dispatching a function call (selecting the algorithm to run) based on two factors. Double dispatch can be implemented at runtime using the Visitor pattern (virtual functions provide the single dispatch) or at compile time using templates or compile-time visitors.</li>
<li>The classic visitor has a circular dependency between the visitor class hierarchy and the visitable class hierarchy. While the visitable classes do not need to be edited when a new visitor is added, they do need to be recompiled when the visitor hierarchy changes. The latter must happen every time a new visitable class is added, hence a dependency circle. The Acyclic Visitor breaks this circle by using cross-casting and multiple inheritance.</li>
<li>A natural way to accept a visitor into an object composed of smaller objects is to visit each of these objects one by one. This pattern, implemented recursively, ends up visiting every built-in data member contained in an object and does so in a fixed, predetermined order. Hence, the pattern maps naturally onto the requirement for serialization and deserialization, where we must deconstruct an object into a collection of built-in types, then restore it.</li>
</ol>
<h1 id="_idParaDest-358"><a id="_idTextAnchor877"/>Chapter 18, Patterns for Concurrency</h1>
<ol>
<li value="1">Concurrency is the property of a program that allows multiple tasks to execute at the same time or partially overlapping in time. Usually, concurrency is achieved through the use of multiple threads.</li>
<li>C++11 has the basic support for writing concurrent programs: threads, mutexes, and condition variables. C++14 and C++17 added several convenience classes and utilities, but the next major addition to concurrency features of C++ is C++20: here, we have several new synchronization primitives as well as the introduction of the coroutines.</li>
<li>Synchronization patterns are the common solutions to the basic problems of accessing shared data. Usually, they provide ways to arrange exclusive access to the data that is modified by multiple threads or is accessed by some threads while being modified by others.</li>
<li>Execution patterns are the standard ways to arrange asynchronous execution of some computations using one or more threads. These patterns offer ways to initiate the execution of some code and receive the results of this execution without the caller being responsible for the execution itself (some other entity in the program has that duty).</li>
<li>The most important guideline for design for concurrency is modularity; when applies specifically to concurrency, it means building concurrent software from components that satisfy certain restrictions on their behavior in concurrent programs. The most important of these restrictions is the thread safety guarantee: generally, it is much easier to build concurrent software from components that allow a wide range of thread-safe operations.</li>
<li>In order to be useful in concurrent programs, any data structure or component must provide a transactional interface. An operation is a transaction if it performs a well-defined complete computation and leaves the system in a well-defined state. A simplified way to identify which operations are and are not transactions is this: if a concurrent program executes each operation under lock but there are no order guarantees between the operations, is the state of the system guaranteed to be well-defined? If it isn’t, then some of the operations should be executed as a sequence without releasing the lock. That sequence is a transaction; the operations that are the steps of the sequence are not transactions by themselves. It should not be the responsibility of the caller to arrange these operations in a sequence. Instead, the data structure or component should offer the interface for performing the entire transaction as a single operation.</li>
</ol>
</div>
</body></html>