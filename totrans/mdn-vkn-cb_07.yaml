- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ray Tracing and Hybrid Rendering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we venture into the fascinating world of **ray tracing** and
    **hybrid rendering**. Ray tracing, to put it simply, is a special technique used
    in computer graphics that simulates how light interacts with objects. This results
    in images that are so lifelike, they can be mistaken for reality. However, pure
    ray tracing is computationally intensive and requires significant hardware resources,
    which makes it unfeasible for real-time applications with the current generation
    of hardware. On the other hand, there’s hybrid rendering, which is a mix of conventional
    rasterization techniques and the realism of ray tracing. This blend offers both
    good performance and stunning visuals. This chapter will take you through how
    these techniques can be implemented using Vulkan. We’ll show you how to set up
    a ray tracing pipeline and guide you on how to integrate hybrid rendering into
    your work. By the end of this chapter, you will have a deeper understanding of
    how these advanced techniques work. More importantly, you’ll learn how to use
    them in your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: The first part of the chapter focuses on developing a GPU-based ray tracer.
    We’ll elaborate on how to effectively develop this GPU-based ray tracer, detailing
    the steps involved, and how each function contributes to the final lifelike image.
    The second part of our chapter will revolve around the integration of shadows
    from a ray tracer along with rasterized deferred rendering. We will delve into
    how the shadows generated from a ray tracer can be combined with the rasterization
    technique of deferred rendering, a technique commonly referred to as hybrid rendering.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a GPU ray tracer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a hybrid renderer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need to make sure you have Visual Studio 2022 installed
    along with the Vulkan SDK. Basic familiarity with the C++ programming language
    and an understanding of ray tracing concepts would be useful. Please revisit [*Chapter
    1*](B18491_01.xhtml#_idTextAnchor019)*, Vulkan Core Concepts,* for details about
    setting up and building the code in the repository. We also assume that by now
    you are familiar with the Vulkan API and various concepts that were introduced
    in previous chapters. This chapter has multiple recipes, which can be launched
    using the following executables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Chapter07_RayTracer.exe`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Chapter07_HybridRenderer.exe`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code files for this chapter can be found here: [https://github.com/PacktPublishing/The-Modern-Vulkan-Cookbook](https://github.com/PacktPublishing/The-Modern-Vulkan-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a GPU ray tracer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ray tracing is a rendering technique that simulates the physical behavior of
    light to generate highly realistic graphics. Ray tracing works by tracing the
    path of light from a pixel in the image sensor back to its source. Each ray of
    light can interact with the objects in the scene, causing a variety of effects
    such as reflection, refraction, or absorption. This allows for the creation of
    realistic shadows, reflections, and light dispersion effects in complex 3D scenes.
    In previous chapters, specifically [*Chapter 4*](B18491_04.xhtml#_idTextAnchor241)*,
    Exploring Techniques for Lighting, Shading, and Shadows*, we explored **rasterization**.
    It takes a more direct approach, converting 3D polygons that make up a scene directly
    into a 2D image. It essentially fills in the pixels of each polygon based on its
    color and texture. On the other hand, ray tracing simulates the path of light
    rays from the camera to the scene, accounting for how these rays interact with
    the scene’s objects.
  prefs: []
  type: TYPE_NORMAL
- en: Before we delve into the specifics of how ray tracing is implemented in Vulkan,
    it is beneficial to gain an understanding of how ray tracing operates, along with
    several fundamental concepts such as the **bidirectional reflectance distribution
    function** (**BRDF**), radiance, and irradiance. These concepts play a crucial
    role in determining how light interacts with surfaces in a scene and subsequently
    influences the final rendered image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simplify understanding, let’s break down the flow of the ray tracing algorithm,
    as depicted in *Figure 7**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Ray tracing algorithm](img/B18491_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Ray tracing algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following section, we will outline the fundamental principles of the
    ray tracing algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: For each pixel on the screen, a ray is projected from the viewpoint or “eye”
    into the scene. This is the initial step in ray tracing and sets the stage for
    further calculations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm then calculates the point of intersection between the ray and
    the objects within the scene. It identifies the closest object that is hit by
    the ray, along with the exact hit point on the object’s geometry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the intersection point is determined, shading is performed at the hit point.
    The color and lighting information calculated at this point is added to the radiance
    value for the pixel, which contributes to the final color of the pixel in the
    rendered image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ray doesn’t stop at the first hit. It can continue to propagate due to phenomena
    such as reflection or refraction. The rays resulting from reflection or refraction
    are assigned a throughput value, which represents the remaining energy of the
    light.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The recursive process can potentially go on indefinitely, which is computationally
    expensive. To handle this, techniques such as **Russian Roulette** are used. In
    Russian Roulette, the recursion is probabilistically terminated based on the remaining
    energy in the ray. If the ray’s energy falls below a certain threshold, it has
    a certain chance of being terminated early, which helps to control the computational
    cost of the algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we understand how the ray tracing algorithm functions, it’s beneficial
    to delve into the principles of **radiometry**. Radiometry is a branch of physics
    that quantifies light’s behavior, providing a series of methods and units to describe
    and measure different aspects of light in a scene. Several key concepts that are
    fundamental to understanding radiometry include radiant intensity, irradiance,
    and radiance. The following diagram (*Figure 7**.2*) can help you remember these
    concepts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Radiometry basics](img/B18491_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Radiometry basics
  prefs: []
  type: TYPE_NORMAL
- en: '**Radiant intensity**: This is a measure of the power of light emitted, or
    radiant flux, per unit solid angle, typically measured in **watts per steradian**
    (**W/sr**). The steradian, analogous to the radian in angular measure, quantifies
    solid angles in 3D space. In the context of ray tracing, it serves as a pivotal
    unit when calculating radiant intensity, capturing how light spreads across surfaces
    within the simulated environment. It is directional in nature, meaning it varies
    depending on the direction from which the light is observed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Irradiance**: Irradiance measures the power of radiant flux incident upon
    a surface per unit area, typically measured in **watts per square meter** (**W/m²**).
    In the context of ray tracing, irradiance is used to calculate the amount of light
    energy striking a surface, which is then used for shading calculations. It plays
    a key role in determining how bright an object appears in the scene.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Radiance**: Radiance refers to how much light is either coming from a specific
    area or passing through it, considering the particular direction or viewpoint
    from which the light is observed. It is used to describe the amount of light that
    reaches the camera from a specific point in the scene, through a specific direction.
    It’s measured in **watts per square meter per steradian** (**W/m²/sr**). Radiance
    is a critical concept in ray tracing as it integrates both directional and positional
    information, helping to generate accurate shading and lighting effects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As the next step, we will learn a bit about the **rendering equation** used
    in ray tracing. The equation essentially depicts that the light leaving a point
    in a certain direction is equal to the light emitted by the point in that direction
    plus the light reflected by the point in that direction. The reflected light is
    integral over all directions of incoming light, where each incoming direction
    is weighted by the BRDF and the cosine of the angle between the incoming light
    and the surface normal. The link provided below offers a simplified explanation
    of the rendering equation: [https://twitter.com/KostasAAA/status/1379918353553371139/photo/1](https://twitter.com/KostasAAA/status/1379918353553371139/photo/1).'
  prefs: []
  type: TYPE_NORMAL
- en: L o(x, ω o) is the total amount of radiance (light), leaving point xs(x, ω o)
    is the emitted light from the point x in the direction ω o. This term is usually
    only non-zero for light sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term ∫ Ω  represents an integral over the entire hemisphere Ω above point
    x.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f r(x, ω i → ω o) is the BRDF at point x, which defines how much light is reflected
    off x in the direction ω o when light comes in from direction ω i.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L i(x, ω i) is the incoming light at point *p* from direction ω i.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ω i ∙ n is the cosine of the angle between ω i and the normal at point x. This
    accounts for the fact that light arriving at a shallow angle spread over a larger
    area. d ω i is a small amount of solid angle around direction ω i.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monte Carlo method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we will discuss the **Monte Carlo method**, which is a statistical technique
    that allows for numerical solutions to complex problems by performing repeated
    random sampling. Suppose you want to calculate the area under a curve described
    by the function f(x) = x 2 between x = 0 and x = 1\. Mathematically, you’d solve
    this using calculus with an integral. However, imagine now that the function is
    extremely complex or has many variables, such that you can’t easily integrate
    it using standard calculus techniques. This is where the Monte Carlo method comes
    into play. Instead of trying to compute the integral exactly, we can estimate
    it using random sampling. In the case of ray tracing, the rendering equation,
    which models how light interacts with surfaces, is quite complex, especially because
    it involves an integral over all possible directions of incoming light. This is
    the reason Monte Carlo is used. Instead of trying to calculate the exact value
    of the integral, we can approximate it by randomly sampling directions of incoming
    light, evaluating the integrand for each of these samples, and then averaging
    the results. This process is repeated many times to get a more accurate estimate.
  prefs: []
  type: TYPE_NORMAL
- en: 'We briefly talked about the BRDF during the rendering equation; it tells us
    how light bounces off a surface. When light hits a surface, it doesn’t just bounce
    back in one direction but scatters in many directions. The BRDF gives us a way
    to predict this behavior. It considers two directions: the direction from which
    the light is coming, and the direction in which it’s going after it hits the surface.'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine the sun shining on the surface. The BRDF helps us figure out how much
    light from the sun is reflected off that surface and in what direction it goes.
    This is important for calculating the color and brightness that we see in a rendered
    image. Here’s where the concept of throughput or contribution comes in. It’s like
    a measure of how much light energy is retained or lost when the light bounces
    off the surface. Think of it as the efficiency of light reflection. We need to
    include this in our calculations to get accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: The **probability density function** (**PDF**) is a statistical tool that helps
    us handle the randomness involved in these calculations. When light hits the surface,
    it can bounce off in many different directions, and the PDF helps us figure out
    the likelihood of each possible direction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Importance sampling** is a technique used in ray tracing where we choose
    to send more rays in directions where the BRDF is high and fewer rays in directions
    where it is low. This helps us get a more accurate result with fewer rays, which
    can be computationally cheaper. However, since we’re sending more rays in certain
    directions and fewer in others, we’re biasing our sampling toward those directions.
    We divide our BRDF result by the PDF to get our result. The reason we divide the
    BRDF by a PDF is essentially to correct for bias that was introduced when we used
    importance sampling to choose the next direction in which to trace the ray.'
  prefs: []
  type: TYPE_NORMAL
- en: In ray tracing, each light ray carries its own energy. Each time it bounces,
    we add the energy it carries times the BRDF to the overall brightness of the image.
  prefs: []
  type: TYPE_NORMAL
- en: In Vulkan, ray tracing is implemented through a series of distinct shader stages.
    In this recipe, we will guide you through the process of implementing a GPU ray
    tracer with Vulkan, providing a step-by-step walkthrough on how to set up each
    shader stage involved in the ray tracing process. By the end of this recipe, you’ll
    be able to create your own ray tracer that can produce highly realistic graphics
    by accurately simulating the behavior of light.
  prefs: []
  type: TYPE_NORMAL
- en: 'The shader stages include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ray generation shader**: This is the starting point of the ray tracing process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intersection shader**: This shader calculates how rays intersect with the
    scene’s geometry'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Miss and hit shaders**: These define how rays behave when they hit or miss
    an object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By understanding and implementing each of these stages, you’ll be well on your
    way to creating visually stunning and realistic graphics.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ray tracing pipeline in Vulkan is made up of six stages: ray generation,
    intersection, any-hit, closest hit, miss, and callable. *Figure 7**.3* shows the
    stages and their general layout in the pipeline. Another key component of Vulkan
    ray tracing is **acceleration structure**. This structure is pivotal in efficiently
    handling the large amount of geometric data involved in ray tracing. The role
    of the acceleration structure is to organize data in a way that allows for rapid
    ray tracing calculations. **Bounding volume hierarchy**(**BVH**) is an algorithmic
    tree structure on a set of geometric objects. All geometric objects are wrapped
    in bounding volumes that form the leaf nodes of the tree. These nodes are then
    paired, bounded, and connected to form a parent node. This process continues up
    the tree until there is only one bounding volume remaining: the root of the tree.
    This structure allows the ray tracing algorithm to efficiently discard many objects
    that the ray cannot intersect, thereby speeding up the process significantly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The acceleration structure is divided into two levels: the **bottom level acceleration
    structures** (**BLASs**) and the **top level acceleration** **structures** (**TLASs**):'
  prefs: []
  type: TYPE_NORMAL
- en: '**BLAS**: BLASs are responsible for storing the geometric data for individual
    objects in the scene. Each object can have one or more BLAS associated with it,
    and each BLAS can contain one or more geometric primitives, such as triangles
    or instances of other BLASs. The BLAS is responsible for determining how rays
    intersect with the geometry they contains, making it a fundamental part of the
    ray tracing process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TLAS**: The TLAS, on the other hand, does not contain geometric data. Instead,
    it contains instances of BLASs. Each instance defines a transformation (such as
    translation, rotation, or scaling) and a BLAS to apply it to. When ray tracing,
    the system starts from the TLAS and works its way down to the appropriate BLAS.
    The TLAS essentially acts as a directory that guides the system to the correct
    BLAS based on the ray’s path.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Ray tracing pipeline and its stages](img/B18491_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Ray tracing pipeline and its stages
  prefs: []
  type: TYPE_NORMAL
- en: 'The shader stages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`traceRayExt` function. These rays are what will eventually interact with the
    objects in the scene to create the final rendered image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Acceleration structure traversal**: The acceleration structure is a key component
    in optimizing the process of ray tracing. It functions as a scene management tree,
    akin to a BVH. Its primary use is to speed up collision detection between rays
    and objects within the scene. This part of the pipeline is fixed, meaning Vulkan
    has already implemented the logic behind it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Intersection stage**: As the rays traverse the BVH, they may call upon an
    intersection shader. This shader is particularly useful when dealing with custom
    types but isn’t necessary when using the default triangle mesh primitive. This
    is because Vulkan has already incorporated the logic required for these default
    primitives, thereby bypassing the need for the intersection shader.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Any-hit stage**: This stage processes intersection events found in the intersection
    stage; in case of an intersection, the any-hit shader is invoked. The any-hit
    shader determines the subsequent steps after the intersection of light and material
    occurs, such as whether to abandon the intersection and so on. Depending on the
    specific requirements, the intersection point can be discarded, at which point
    it is considered that no intersection took place. This process is then returned
    to the BLAS traversal.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Closest hit stage**: This shader is responsible for processing the intersection
    that is currently the closest to the ray origin and has not yet been discarded
    by an any-hit stage. It typically involves applying light and material calculations
    to render the final color of the pixel.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Miss stage**: The miss stage determines how to handle the light in the event
    the ray doesn’t hit anything. This could involve assigning a default color, environment
    color, and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Callable stage**: This stage may be called from any other stage (from their
    shader code).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the repository, the ray tracing code is encapsulated in the `RayTracer` class.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a first step, we will look at the code that needs to be executed on the
    host side; most of the implementation is inside the `RayTracer` class:'
  prefs: []
  type: TYPE_NORMAL
- en: The first critical step in setting up ray tracing with Vulkan is to verify if
    our physical device (GPU) supports ray tracing features. This verification is
    achieved by adding `VkPhysicalDeviceRayTracingPipelineFeaturesKHR` and `VkPhysicalDeviceAccelerationStructureFeaturesKHR`
    to the list of physical features that are checked for support. This feature-checking
    operation is implemented in the `PhysicalDevice` class. Here, these specific features
    are added to the `VkPhysicalDeviceFeatures2` structure’s chain, serving as a mechanism
    to query the support for a set of features. This class also supplies the `isRayTracingSupported`
    function, which is utilized to activate the necessary features for ray tracing
    during the Vulkan device’s creation process. In the `Context` class, we introduce
    specific features for `VkPhysicalDeviceAccelerationStructureFeaturesKHR` and `VkPhysicalDeviceRayTracingPipelineFeaturesKHR`.
    However, these features are only activated during the construction of the Vulkan
    device if the demo application has ray tracing enabled and the physical device
    confirms its support for these features. Please also note that the demo application
    will only run if your GPU supports Vulkan ray tracing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we create shader modules for each of the shaders that will be used by
    the ray tracing pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a ray tracing pipeline by calling `Pipeline::createRayTracingPipeline()`.
    To facilitate creating a ray tracing pipeline, we added a helper structure, called
    `Pipeline::RayTracingPipelineDescriptor`, which stores the descriptor sets and
    their bindings (just like in the graphics and compute pipelines descriptors),
    and all shaders required to create a ray tracing pipeline. An instance of this
    structure must be passed to the constructor of the `VulkanCore::Pipeline` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A ray tracing pipeline in Vulkan necessitates an array of shader group structures.
    Rather than holding a list of shaders, each `VkRayTracingShaderGroupCreateInfoKHR`)
    structure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The structure contains fields to specify shaders for only four different stages
    of the pipeline (`generalShader`, `closestHitShader`, `anyHitShader`, and `intersectionShader`).
    That’s because the indices of shaders for the miss and callable stages are provided
    in the `generalShader` field. It’s important to note that the function of these
    fields depends on the value of the `type` member in the structure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the sake of brevity, we show here only the creation of one shader stage
    and one shader group. The other shader modules passed along with the pipeline
    descriptor are grouped into their own shader groups. In the provided code snippet,
    we demonstrate the construction of a shader group specifically for a ray generation
    shader. It’s crucial to understand that each type of shader utilized in Vulkan
    ray tracing requires its own individual shader group. It’s necessary because the
    design of the Vulkan ray tracing pipeline is such that it allows for different
    types of shaders to operate independently, each performing a unique task in the
    ray tracing process. By structuring each type of shader in its own shader group,
    we ensure that the corresponding tasks are executed independently and efficiently,
    aiding in the parallel computation capabilities of the GPU. Please refer to the
    code in `Pipeline::createrayTracingPipeline()` for more details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, here’s how you create a ray tracing pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to create the `VkStridedDeviceAddressRegionKHR` structure
    that describes the location and structure of the SBT in memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `createShaderBindingTable()` function is where the SBT is created. This
    function begins by defining several variables to store the sizes and counts of
    various shader types in the application. In the code, `handleSize` and `handleSizeAligned`
    represent the size of a single shader group handle in the SBT, with the latter
    ensuring correct memory alignment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`numRayGenShaders`, `numRayMissShaders`, and `numRayClosestHitShaders` represent
    the number of each type of shader used in the pipeline. Next, we calculate the
    total size of the SBT (`sbtSize`) and create a `shaderHandleStorage` vector to
    store the shader handles:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `vkGetRayTracingShaderGroupHandlesKHR` Vulkan function is then called to
    retrieve the shader group handles. These handles are unique identifiers for the
    shader groups in the pipeline. Afterward, we create separate buffers for each
    shader type (`copyDataToBuffer` method is called to copy the relevant shader handles
    from `shaderHandleStorage` into the buffer. We recommend looking at the `createShaderBindingTable`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each buffer and its respective `VkStridedDeviceAddressRegionKHR` need to be
    filled. Here, we only show how ray generation is populated. The other groups follow
    a similar pattern:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we need to load the environment map along with its acceleration structure.
    The `RayTracer::loadEnvMap()` method performs the loading of the environment map
    and the creation of the acceleration structure. It loads a `context_->createTexture()`.
    It then calls `createEnvironmentAccel()`, which is responsible for creating an
    acceleration data structure for importance sampling of the environment map. This
    function computes a vector of `EnvAccel` structures, one for each texel of the
    map. This data is uploaded to a device-only buffer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we create TLASs and BLASs with the `RayTracer::initBottomLevelAccelStruct()`
    and `RayTracer::initTopLevelAccelStruct()` methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following steps, you will learn how to set up BLAS using Vulkan:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`VK_GEOMETRY_TYPE_TRIANGLES_KHR`) with vertices and indices from the model’s
    buffers:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`vkGetAccelerationStructureBuildSizesKHR` function call returns the size information
    needed to allocate the acceleration structure and the build scratch buffer. The
    `bLAS_[meshIdx].buffer` buffer is populated as a direct result of the `vkGetAccelerationStructureBuildSizesKHR`
    function call:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`bLAS_[meshIdx].buffer`, which is used to store the BLAS structure. We then
    create a structure of the `VkAccelerationStructureCreateInfoKHR` type, to which
    we provide the buffer just created, its size, and specify that it’s a BLAS. Next,
    we call `vkCreateAccelerationStructureKHR` to create the actual acceleration structure
    and store the handle to it in `bLAS_[meshIdx].handle`. We created a temporary
    buffer named `tempBuffer` to store the temporary data needed when building the
    acceleration structure. When you are building an acceleration structure in Vulkan,
    the build procedure often needs some temporary space to perform its calculations.
    This temporary space is also referred to as a scratch buffer. We then fill a `VkAccelerationStructureBuildGeometryInfoKHR`
    structure with the details of the acceleration structure build, including the
    handle of the acceleration structure, the geometry, and the device address of
    `tempBuffer`. Next, we create a `VkAccelerationStructureBuildRangeInfoKHR` structure
    to specify the range of geometries to be used in the build. The `vkCmdBuildAccelerationStructuresKHR`
    function records the command to build the acceleration structure into the command
    buffer:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following steps, you will learn how to set up a TLAS in Vulkan:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Creating the acceleration structure instances**: The following loop creates
    the instances, each of which references a BLAS. Instances contain information
    about the transformation matrix, mask, flags, and the device address of the BLAS
    it references:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`VK_GEOMETRY_TYPE_INSTANCES_KHR`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`vkGetAccelerationStructureBuildSizesKHR` function call returns the size information
    needed to allocate the acceleration structure and the build scratch buffer:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`VK_ACCELERATION_STRUCTURE_TYPE_TOP_LEVEL_KHR` in the type field since we are
    building a TLAS. The final part is to record the `vkCmdBuildAccelerationStructuresKHR`
    command on the command buffer which is executed when the command buffer is submitted:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We also create a `RayTraced` storage image (encapsulated in `initRayTracedStorageImages`)
    and bind resources by calling `bindResource` on the pipeline during the initialization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To execute a ray tracer, we need to call `RayTracer::execute()`, which is responsible
    for copying the camera data, binding the pipeline, and calling `vkCmdTraceRaysKHR`.
    This Vulkan function launches the `RayGen` shader.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we have understood the steps on the host side, it’s time to understand
    the device-side code. The device-side code for ray tracing is implemented using
    several shaders including `raytrace_raygen.rgen`, `raytrace_miss.rmiss`, `raytrace_closesthit.rchit`,
    and `raytrace_shadow.rmiss`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The process begins with the invocation of the `raytrace_raygen.rgen` shader
    via `vkCmdTraceRaysKHR`. In the following shader code block, the ray tracing process
    initiates by generating rays for each pixel sample through a unique seed for randomness.
    These rays, defined by their origin and direction, are traced into the scene within
    a loop until either the maximum number of bounces is reached, or an exit condition
    is met in the payload. The payload carries essential information such as origin,
    direction, and bounce index. Once the final color for the pixel is calculated
    using an average from all samples, it is stored in the output image. If temporal
    accumulation is applied, the shader retrieves and adds colors from previous frames.
    Temporal accumulation is beneficial in ray tracing as it helps to reduce noise
    and improve image quality. Accumulating or averaging the color samples over multiple
    frames effectively increases the number of rays traced per pixel without the cost
    of tracing extra rays in a single frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `raytrace_miss.rmiss` shader is pretty simple: it is invoked if the ray
    doesn’t intersect any object. In such scenarios, the shader takes a sample from
    the environment map, determining the color based on the point of interaction between
    the ray and the environment. The `envMapColor` function takes a 3D direction vector
    as an input, normalizes it, and converts it into spherical coordinates (theta
    and phi). It then maps these coordinates onto a 2D plane (UV) and retrieves the
    corresponding color from the environment map texture. The following code block
    simply calls the `envMapColor` function to get the radiance for the current ray
    payload:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `raytrace_closesthit.rchit` shader is where most of the magic, including
    a calculation for shading and determining the subsequent direction of the ray,
    happens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The initial stage of the process involves extracting the vertex and material
    data for the mesh that has been struck by the ray. This is achieved by utilizing
    the `gl_InstanceID` and `gl_PrimitiveID` variables, which are populated with the
    relevant data by the intersection shader. The hit shader also provides access
    to `hitAttributeEXT vec2 attribs`. In the context of triangles, these attributes
    represent the barycentric coordinates of the intersection point. Barycentric coordinates
    are a form of coordinate system used to specify the position of a point within
    a triangle. They are particularly useful in computer graphics because they allow
    for easy interpolation across a triangle. By using these coordinates, we can interpolate
    the positions of the vertices to determine the precise point of intersection within
    the triangle where the ray has made contact. Please refer to the code in `raytrace_closesthit.rchit`
    to understand how we use barycentric coordinates to get world space position.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next step is to call the `envSample()` function. This function is a crucial
    part of the ray tracing process, responsible for sampling the HDR environment
    map using importance sampling. The environment map is represented as a 2D texture
    (latitude-longitude format) and contains the illumination data of the surrounding
    environment. The function starts by uniformly picking a texel index in the environment
    map. It fetches the sampling data for that texel, which includes the ratio between
    the texel’s emitted radiance and the environment map’s average, the texel alias,
    and the distribution function values for that texel and its alias. The function
    then decides to either pick the texel directly or pick its alias based on a random
    variable and the intensity ratio. It computes the 2D integer coordinates of the
    chosen texel and uniformly samples the solid angle subtended by the pixel. The
    function converts the sampled UV coordinates to a direction in spherical coordinates,
    which is then converted to a light direction vector in Cartesian coordinates.
    This light direction vector is then returned along with the texel’s PDF:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Shadow rays play a crucial role in the ray tracing process. They help in creating
    realistic lighting effects by determining which parts of the scene are in shadow,
    thus adding depth and realism to the rendered image. The next step is to trace
    a shadow ray from the intersection point towards the light source to check for
    any occluding objects. This is a critical step in determining whether a point
    is in shadow or not. The `inshadow` variable is declared using `layout(location
    = 1) rayPayloadEXT bool inshadow`. When a ray is traced in a ray tracing shader,
    it carries with it a payload. This payload can be used to store information that
    needs to be passed between different stages of the ray tracing pipeline, such
    as from the closest hit shader to the ray generation shader. The `inshadow` variable
    is a Boolean that is used to store the information of whether a particular point
    is in shadow. When the shadow ray (a ray traced from the intersection point towards
    the light) is occluded by another object, this variable will be set to true, indicating
    that the point is in shadow. Please be aware that in the `traceRayEXT` function,
    the 6th parameter is set to `1`. This value serves as an index to specify which
    miss shader should be invoked. In this context, it refers to the miss shader found
    in `raytrace_shadow.miss`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is responsible for lighting calculation using the `PbrEval` function
    evaluates the PBR model for a given set of inputs. It uses the material properties
    (such as base color, specular color, roughness, and metallic factors), `rayPayload.radiance`
    is the accumulated color or light contribution that the ray has gathered from
    all the light sources it has encountered up until now. `rayPayload.throughput`
    is a measure of how much light makes it through a certain path without being absorbed
    or scattered. Essentially, it’s a measure of the energy left of a light path.
    For details on PBR theory, please visit [https://learnopengl.com/PBR/Theory](https://learnopengl.com/PBR/Theory):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The final part is to figure out the next ray direction as well as the throughput
    (energy left) for the next ray. It starts by sampling a direction for the next
    ray (`bsdfDirNextRay`) using the `PbrSample` function, which uses the material
    properties and the current ray direction to generate this direction. We calculate
    `cosTheta`, which is the cosine of the angle between the surface normal and the
    direction of the next ray. This is used in the calculation of the new throughput
    because the amount of light reflected is proportional to the cosine of this angle
    (`bsdfDirNextRay`, and the origin is slightly offset from the current position
    to avoid self-intersection. Please note `PBREval` is used to evaluate the BRDF
    in a specific direction while `PBRSample` is used to generate a new direction
    and evaluate the BRDF in that direction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This concludes various parts of how to implement a simple GPU-based ray tracer
    in Vulkan.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We recommend reading the *Ray Tracing in One Weekend* book series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/RayTracing/raytracing.github.io](https://github.com/RayTracing/raytracing.github.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adam Celarek and Bernhard Kerbl’s YouTube channel contains a trove of information
    about lighting and ray-tracing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/playlist?list=PLmIqTlJ6KsE2yXzeq02hqCDpOdtj6n6A9](https://www.youtube.com/playlist?list=PLmIqTlJ6KsE2yXzeq02hqCDpOdtj6n6A9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing hybrid rendering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore the integration of rasterization, specifically
    deferred rendering, with ray-traced shadows.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B18491_04.xhtml#_idTextAnchor241)*, Exploring Techniques for
    Lighting, Shading, and Shadows*, we implemented deferred rendering, incorporating
    techniques such as shadow mapping, screen space AO, and screen space reflections.
    These techniques allowed us to generate multiple textures, which were then composited
    during the lighting pass. Within this recipe, you will gain insights into generating
    shadow textures using ray tracing, which will help in overcoming challenges associated
    with techniques such as screen space shadow mapping. Screen space shadow mapping
    relies on the information available in the rendered image. It doesn’t have complete
    access to the entire 3D scene geometry. This limitation can result in inaccuracies
    and artifacts. Screen space shadow mapping is susceptible to aliasing issues,
    particularly along edges and boundaries due to the resolution of the screen space
    texture. The ray tracing approach doesn’t have these problems as it works on a
    full scene.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Within the code repository, the hybrid rendering functionality is realized through
    the `RayTracedShadowPass` and `LightingPassHybridRenderer` classes.
  prefs: []
  type: TYPE_NORMAL
- en: The process begins with the execution of the `Gbuffer` pass, generating G-buffer
    textures based on the concepts discussed in [*Chapter 4*](B18491_04.xhtml#_idTextAnchor241)*,
    Exploring Techniques for Lighting, Shading, and Shadows*. Following this, `RayTracedShadowPass`
    is initiated, employing the ray tracing stages outlined in the preceding section.
    However, in this pass, ray tracing is specifically employed to generate the shadow
    texture. The final step involves employing `LightingPassHybridRenderer` to compose
    information from the G-buffer and the ray-traced shadow texture, culminating in
    the production of a final image for display.
  prefs: []
  type: TYPE_NORMAL
- en: 'The device side code for the ray tracing shader is in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The device side code for compositing is in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have understood the code structure, we will investigate how to implement
    it in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The host side part of the code is in `RayTracedShadowPass` and the setup for
    it is very similar to what we described during the previous recipe. We will focus
    on the device side code to look at how we generate shadows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we start the shader with a declaration for the input and uniform
    variables that the shader will use. The `layout(location = 0) rayPayloadEXT float
    visibilityRayPayload;` line defines the payload that will be returned by the ray
    tracing operation. The other uniform variables declared are for the acceleration
    structure, output image, and textures for the normal and position G-buffers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `main` function is where the actual computation happens. It starts by calculating
    the pixel center and the UV coordinates for the current pixel (or launch). Then,
    it fetches the normal and world position from the G-buffers using the UV coordinates.
    `rayOrigin` is calculated by offsetting the world position slightly along the
    normal direction. This is to prevent **self-intersection**, where the ray might
    incorrectly intersect with the surface it was launched from:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The shader launches multiple shadow rays towards random points on the light
    source. The loop runs for several samples, generating a random point on the light
    source for each sample, and then calculates the direction to that point. The `traceRayEXT`
    function is called to trace a ray from `rayOrigin` toward the light source. If
    the ray hits something before it reaches the light, the payload will be `0`, indicating
    that the light source is occluded. If the ray reaches the light source without
    hitting anything, the payload will be `1`, indicating that the light source is
    visible. The visibility for each sample is accumulated in the `visible` variable.
    The accumulated visibility for each sample, represented by the `visible` variable,
    is then stored in the corresponding location of the final image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`raytrace_miss_shadow_hybrid.rmiss` and `raytrace_closesthit_shadow_hybrid.rchit`
    are pretty straightforward; they simply set `visibilityRayPayload` to `1.0` if
    it’s a miss and `0.0` in case we hit something.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last step is the compositing step. This is the same as the lighting pass
    we discussed in [*Chapter 4*](B18491_04.xhtml#_idTextAnchor241)*, Exploring Techniques
    for Lighting, Shading, and Shadows*, the only difference being that now we are
    using a shadow texture that has been created using ray tracing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this chapter, we explored the world of ray tracing and hybrid rendering in
    Vulkan. We delved into these advanced graphical techniques, understanding how
    they can provide unprecedented levels of realism in rendered images. We learned
    how ray tracing algorithms work, tracing the path of rays of light to create highly
    detailed and physically accurate reflections and shadows in a 3D scene. Through
    hybrid rendering, we uncovered the process of combining traditional rasterization
    with ray tracing to achieve a balance between performance and visual fidelity.
    This blend allows for the high speed of rasterization where the utmost precision
    isn’t required while using ray tracing to handle complex light interactions that
    rasterization struggles with. Vulkan’s robust support for both techniques was
    explored, leveraging its efficient capabilities and explicit control over hardware
    resources.
  prefs: []
  type: TYPE_NORMAL
