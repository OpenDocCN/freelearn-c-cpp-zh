<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-149"><a id="_idTextAnchor153"/>10</h1>
<h1 id="_idParaDest-150"><a id="_idTextAnchor154"/>Arena-Based Memory Management and Other Optimizations</h1>
<p>Our memory-management toolbox is growing with every chapter. We now know how to overload memory allocation operators (<a href="B21071_07.xhtml#_idTextAnchor116"><em class="italic">Chapter 7</em></a>) and how to put this skill to work in ways that solve a variety of concrete problems (<a href="B21071_08.xhtml#_idTextAnchor128"><em class="italic">Chapter 8</em></a> and <a href="B21071_09.xhtml#_idTextAnchor141"><em class="italic">Chapter 9</em></a> both give a few illustrative, real-world examples).</p>
<p>One important reason why one would want to take control of memory allocation mechanisms is <em class="italic">performance</em>. Now, it would be presumptuous (and plain wrong!) to state that it’s trivial to beat the implementation of these functions as provided by your library vendor, as these are good, often <em class="italic">very</em> good, for the average case. The key element of the previous phrase, of course, is “for the average case.” When one’s use case has specificities that are known of beforehand, it is sometimes possible to benefit from that information and carve an implementation that outperforms, maybe by a wide margin, anything that could have been designed for excellent <em class="italic">average</em> performance.</p>
<p>This chapter is about using knowledge of the memory management problem we want to solve and building a solution that excels for us. This can mean a solution that’s faster on average, that’s fast enough even in the worst case, that shows deterministic execution times, that reduces memory fragmentation, and so on. There are many different needs and constraints in real-world programs after all, and we often have to make choices.</p>
<p>Once this chapter is over, our toolbox will be expanded to let us do the following:</p>
<ul>
<li>Write arena-based allocation strategy algorithms optimized to face a priori known constraints</li>
<li>Write per-memory block-size allocation strategies</li>
<li>Understand the benefits as well as the risks associated with such techniques</li>
</ul>
<p>The techniques covered in this chapter will lead us to explore use cases very close to those for which memory allocation operators are overloaded in some specialized application domains. Thus, we will initially apply them to a “real life” problem: the fight between Orcs and Elves in a medieval fantasy game.</p>
<p class="callout-heading">On the (sometimes diminishing) returns of optimization</p>
<p class="callout">Since we will be discussing optimization techniques (among other things) in this chapter, some words of warning are in order: <em class="italic">optimization is a tricky thing</em>, a moving target, and what makes code better one day could pessimize it another day. Similarly, what can seem like a good idea in theory can lead to slowdowns in practice once implemented and tested, and one can sometimes spend a lot of time optimizing a piece of code that is rarely taken, effectively wasting time and money.</p>
<p class="callout">Before trying to optimize parts of your program, it’s generally wise to measure, ideally with a profiling tool, and identify the parts that might benefit from your efforts. Then, keep a simple (but correct) version of your code close by and use it as a baseline. Whenever you try an optimization, compare the results with the baseline code and run these tests regularly, particularly when changing hardware, library, compiler, or version thereof. Sometimes, something such as a compiler upgrade might induce a new optimization that “sees through” the simple baseline code and makes it faster than your finely crafted alternative. Be humble, be reasonable, measure early, and measure often.</p>
<h1 id="_idParaDest-151"><a id="_idTextAnchor155"/>Technical requirements</h1>
<p><a id="_idTextAnchor156"/>You can find the code files for this chapter in the book’s GitHub repository here: <a href="https://github.com/PacktPublishing/C-Plus-Plus-Memory-Management/tree/main/chapter10">https://github.com/PacktPublishing/C-Plus-Plus-Memory-Management/tree/main/chapter10</a>.</p>
<h1 id="_idParaDest-152"><a id="_idTextAnchor157"/>Arena-based memory management</h1>
<p>The idea behind<a id="_idIndexMarker542"/> arena-based memory management is to allocate a chunk of memory at a known moment in the program and manage it as a “small, personalized heap” based on a strategy that benefits from knowledge of the situation or of the problem domain.</p>
<p>There are many variants on this general theme, including the following:</p>
<ul>
<li>In a game, allocate and manage the memory by scene or by level, deallocating it as a single chunk at the end of said scene or level. This can help reduce memory fragmentation in the program.</li>
<li>When the conditions in which allocations and deallocations are known to follow a given pattern or have bounded memory requirements, specialize allocation functions to benefit from this information.</li>
<li>Express a form of ownership for a group of similar objects in such as way as to destroy them all at a later point in the program instead of doing so one object at a time.</li>
</ul>
<p>The best way to explain how arena-based allocation works is probably to write an example program that uses it and shows both what it does and what benefits this provides. We will write code in such a way as to use the same test code with either the standard library-provided<a id="_idIndexMarker543"/> allocation functions or our own specialized implementation, depending on the presence of a macro, and, of course, we will measure the allocation and deallocation code to see whether there is a benefit to our efforts.</p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor158"/>Specific example – size-based implementation</h2>
<p>Suppose we are<a id="_idIndexMarker544"/> working on a video game where the action converges toward a stupendous finale where Orcs and Elves meet in a grandiose battle. No one really remembers why these two groups hate each other, but there is a suspicion that one day, one of the Elves said to one of the Orcs “You know, you don’t smell all that bad today!” and this Orc was so insulted that it started a feud that still goes on today. It’s a rumor, anyway.</p>
<p>It so happens that, in this game, some things are known about the behavior of Orc-using code, specifically, the following:</p>
<ul>
<li>There will never be more than a certain number of dynamically allocated <code>Orc</code> objects overall, so we have an upper bound to the space required to store these beasties.</li>
<li>The Orcs that die will not come back to life in that game, as there are no shamans to resurrect them. Expressed otherwise, there is no need to implement a strategy that reuses the storage of an <code>Orc</code> object once it has been destroyed.</li>
</ul>
<p>These two properties open algorithmic options for us:</p>
<ul>
<li>If we have enough memory available, we could allocate upfront a single memory block large enough to put all the <code>Orc</code> objects in the game as we know what the worst-case scenario is</li>
<li>Since we know that we will not need to reuse the memory associated with individual <code>Orc</code> objects, we can implement a simple (and very fast) strategy for allocation that does almost no bookkeeping and, as we will see, lets us achieve deterministic, constant-time allocation <em class="italic">for </em><em class="italic">this type</em></li>
</ul>
<p>For the sake of this example, the <code>Orc</code> class will be represented by three data members, <code>name</code> (a <code>char[4]</code> as these beasties have a limited vocabulary), <code>strength</code> (of type <code>int</code>), and <code>smell</code> (of<a id="_idIndexMarker545"/> the <code>double</code> type as these things have… a reputation), as follows:</p>
<pre class="source-code">
class Orc {
  char name[4]{ 'U', 'R', 'G' };
  int strength = 100;
  double smell = 1000.0;
public:
  static constexpr int NB_MAX = 1'000'000;
  // ...
};</pre> <p>We will use arbitrary default values for our <code>Orc</code> objects as we are only concerned about allocation and deallocation for this example. You can write more elaborate test code that uses non-default values if you feel like it, of course, but that would not impact our discussion so we will target simplicity.</p>
<p>Since we are discussing the memory allocation of a large block upfront through our size-based arena, we need to look at memory size consumption for <code>Orc</code> objects. Supposing <code>sizeof(int)==4</code> and <code>sizeof(double)==8</code> and supposing that, being fundamental types, their alignment requirements match their respective sizes, we can assume that <code>sizeof(Orc)==16</code> in this case. If we aim to allocate enough space for all <code>Orc</code> objects at once, ensuring <code>sizeof(Orc)</code> remains reasonable for the resources at our disposal is important. For example, defining the maximum number of <code>Orc</code> objects in a program as <code>Orc::NB_MAX</code> and the maximal amount of memory we can allocate at once for <code>Orc</code> objects as some hypothetical constant named <code>THRESHOLD</code>, we could leave a <code>static_assert</code> such as the following in our source code as a form of <em class="italic">constraints-respected check</em>:</p>
<pre class="source-code">
static_assert(Orc::NB_MAX*sizeof(Orc) &lt;= THRESHOLD);</pre> <p>This way, if we end up evolving the <code>Orc</code> class to the point where resources become an issue, the code will stop compiling and we will be able to reevaluate the situation. In our case, with a memory consumption of approximately 16 MB, we will suppose we are within budget and that we can proceed with our arena.</p>
<p>We will want to compare our arena-based implementation with a baseline implementation, which, in this case, will be the standard library-provided implementation <a id="_idIndexMarker546"/>of the memory allocation functions. It’s important to note upfront that each standard library implementation provides its own version of these functions, so you might want to run the code we will be writing here on more than one implementation to get a better perspective on the impact of our techniques.</p>
<p>To write code that allows us to do a proper comparison, we will need two distinct executables as we will be in an either/or situation (we either get the standard version or the “homemade” one we are writing), so this is a good use case for macro-based conditional compilation. We will thus write a single set of source files that will conditionally replace the standard library-provided versions of the allocation operators with ours but will otherwise be essentially identical.</p>
<p>We will work from three files: <code>Orc.h</code>, which declares the <code>Orc</code> class and the conditionally defined allocation operator overloads; <code>Orc.cpp</code>, which provides the implementation for these overloads as well as the arena implementation itself; and a test program that allocates <code>Orc::NB_MAX</code> objects of type <code>Orc</code> then later destroys them and measures the time it takes to do these two operations. Of course, as with most microbenchmarks, take these measurements with a grain of salt: the numbers will not be the same in a real program where allocations are interspersed with other code, but at least we will apply the same tests to both implementations of the allocation operators so the comparison should be reasonably fair.</p>
<h3>Declaring the Orc class</h3>
<p>First, let us examine <code>Orc.h</code>, which we have already seen in part when showing the data member<a id="_idIndexMarker547"/> layout <a id="_idIndexMarker548"/>of the <code>Orc</code> class earlier:</p>
<pre class="source-code">
#ifndef ORC_H
#define ORC_H
<strong class="bold">// #define HOMEMADE_VERSION</strong>
#include &lt;cstddef&gt;
#include &lt;new&gt;
class Orc {
  char name[4]{ 'U', 'R', 'G' };
  int strength = 100;
  double smell = 1000.0;
public:
  static constexpr int NB_MAX = 1'000'000;
<strong class="bold">#ifdef HOMEMADE_VERSION</strong>
<strong class="bold">   void * operator new(std::size_t);</strong>
<strong class="bold">   void * operator new[](std::size_t);</strong>
<strong class="bold">   void operator delete(void *) noexcept;</strong>
<strong class="bold">   void operator delete[](void *) noexcept;</strong>
<strong class="bold">#endif</strong>
};
#endif</pre> <p>The <code>HOMEMADE_VERSION</code> macro can be uncommented to use our version of the allocation functions. As can be expected, since we are applying a special strategy for the <code>Orc</code> class and its expected usage patterns, we are using member-function overloads for the allocation operators. (We would not want to treat <code>int</code> objects or – imagine! – Elves the same way we<a id="_idIndexMarker549"/> will treat Orcs, would we? I thought not.)</p>
<h3>Defining the Orc class and implementing an arena</h3>
<p>The essence<a id="_idIndexMarker550"/> of the memory management-related code will be in <code>Orc.cpp</code>. We will go through it in two steps, the arena implementation and the allocation operator overloads, and analyze the different important parts separately. The whole implementation found in this file will be conditionally compiled based on the <code>HOMEMADE_VERSION</code> macro.</p>
<p>We will name our arena class <code>Tribe</code>, and it will be a singleton. Yes, that reviled design pattern we used in <a href="B21071_08.xhtml#_idTextAnchor128"><em class="italic">Chapter 8</em></a> again, but we really do want a single <code>Tribe</code> object in our program so that <a id="_idIndexMarker551"/>conveys the intent well. The important parts of our implementation are as follows:</p>
<ul>
<li>The default (and only) constructor of the <code>Tribe</code> class allocates a single block of <code>Orc::NB_MAX*sizeof(Orc)</code> bytes. It is important to note right away that there are no <code>Orc</code> objects in that chunk: this memory block is just the right size and shape to put all the <code>Orc</code> objects we will need. A key idea for arena-based allocation is that, at least for this implementation, <em class="italic">the arena manages raw memory, not objects</em>: object construction and destruction are the province of user code, and any object not properly destroyed at the end of the program is user code’s fault, not the fault of the arena.</li>
<li>We validate at once that the allocation succeeded. I used an <code>assert()</code> in this case, as the rest of the code depends on this success, but throwing <code>std::bad_alloc</code> or calling <code>std::abort()</code> would also have been reasonable options. A <code>Tribe</code> object keeps two pointers, <code>p</code> and <code>cur</code>, both initially pointing at the beginning of the block. We will use <code>p</code> as the <em class="italic">beginning of block</em> marker, and <code>cur</code> as the <em class="italic">pointer to the next block to return</em>; as such, <code>p</code> will remain stable throughout program execution and <code>cur</code> will move forward by <code>sizeof(Orc)</code> bytes with each allocation.</li>
</ul>
<p class="callout-heading">Using char* or Orc*</p>
<p class="callout">This <code>Tribe</code> implementation uses <code>char*</code> for the <code>p</code> and <code>cur</code> pointers but <code>Orc*</code> would have been a correct choice also. One simply needs to remember that, as far as the <code>Tribe</code> object is concerned, there are no <code>Orc</code> objects in the arena and the use of type <code>Orc*</code> is simply a convenient lie to simplify pointer arithmetic. The changes this would entail would be replacing <code>static_cast&lt;char*&gt;</code> with <code>static_cast&lt;Orc*&gt;</code> in the constructor, and replacing <code>cur+=sizeof(Orc)</code> with <code>++cur</code> in the implementation of the <code>allocate()</code> member function. It’s mostly a matter of style and personal preference.</p>
<ul>
<li>The destructor frees the entire block of memory managed by the <code>Tribe</code> object. This is a very efficient procedure: it’s quicker than separately freeing smaller blocks, and it leads to very little memory fragmentation.</li>
<li>This first implementation uses the Meyers singleton technique seen in <a href="B21071_08.xhtml#_idTextAnchor128"><em class="italic">Chapter 8</em></a>, but we will use a different approach later in this chapter to compare the performance impacts of two implementation strategies for the same design pattern… because there are such impacts, as we will see.</li>
</ul>
<p>The way our size-based <a id="_idIndexMarker552"/>arena <a id="_idIndexMarker553"/>implementation will benefit from our a priori knowledge of the expected usage pattern is as follows:</p>
<ul>
<li>Each allocation will return a sequentially “allocated” <code>Orc</code>-sized block, meaning that there is no need to search for an appropriately sized block – we always know where it is.</li>
<li>There is no work to do when deallocating as we are not reusing the blocks once they have been used. Note that, per standard rules, the allocation and deallocation functions have to be thread-safe, which explains our use of <code>std::mutex</code> in this implementation.</li>
</ul>
<p>The code follows:</p>
<pre class="source-code">
#include "Orc.h"
<strong class="bold">#ifdef HOMEMADE_VERSION</strong>
#include &lt;cassert&gt;
#include &lt;cstdlib&gt;
#include &lt;mutex&gt;
class Tribe {
  std::mutex m;
  char *p, *cur;
  Tribe() : p{ <strong class="bold">static_cast&lt;char*&gt;(</strong>
<strong class="bold">      std::malloc(Orc::NB_MAX * sizeof(Orc))</strong>
<strong class="bold">  )</strong> } {
      assert(p);
      cur = p;
  }
  Tribe(const Tribe&amp;) = delete;
  Tribe&amp; operator=(const Tribe&amp;) = delete;
public:
  ~Tribe() {
      <strong class="bold">std::free(p);</strong>
  }
  static auto &amp;get() {
<strong class="bold">      static Tribe singleton;</strong>
<strong class="bold">      return singleton;</strong>
  }
<strong class="bold">  void * allocate() {</strong>
<strong class="bold">      std::lock_guard _ { m };</strong>
<strong class="bold">      auto q = cur;</strong>
<strong class="bold">      cur += sizeof(Orc);</strong>
<strong class="bold">      return q;</strong>
<strong class="bold">  }</strong>
<strong class="bold">  void deallocate(void *) noexcept {</strong>
<strong class="bold">  }</strong>
};
// ...</pre> <p>As you might have guessed already, these allocation conditions are close to optimal, but they happen more often than we would think in practice. A similarly efficient usage pattern would model a stack (the last block allocated is the next block freed), and we write code that uses local variables every day without necessarily realizing that we are using what is often an optimal usage pattern for the underlying memory.</p>
<p>We then come to the overloaded allocation operators. To keep this implementation simple, we will suppose there will be no array of <code>Orc</code> objects to allocate, but you can refine the implementation to take arrays into account (it’s not a difficult task; it’s just more complicated<a id="_idIndexMarker554"/> to write<a id="_idIndexMarker555"/> relevant test code). The role played by these functions is to delegate the work to the underlying arena, and they will only be used for the <code>Orc</code> class (there is a caveat to this, which will be discussed in the <em class="italic">When parameters change</em> section later in this chapter). As such, they are almost trivial:</p>
<pre class="source-code">
// ...
void * Orc::operator new(std::size_t) {
  return Tribe::get().allocate();
}
void * Orc::operator new[](std::size_t) {
  assert(false);
}
void Orc::operator delete(void *p) noexcept {
  Tribe::get().deallocate(p);
}
void Orc::operator delete[](void *) noexcept {
  assert(false);
}
<strong class="bold">#endif // HOMEMADE_VERSION</strong></pre> <h3>Testing our implementation</h3>
<p>We then come to the test<a id="_idIndexMarker556"/> code implementation we will be using. This program will be made of a <a id="_idIndexMarker557"/>microbenchmark function named <code>test()</code> and of a <code>main()</code> function. We will examine both separately.</p>
<p>The <code>test()</code> function will take a non-<code>void</code> function, <code>f()</code>, a variadic pack of arguments, <code>args</code>, and call <code>f(args...)</code> making sure to use perfect forwarding for the arguments in that call to make sure the arguments are passed with the semantic intended in the original call. It reads a clock before and after the call to <code>f()</code> and returns a <code>pair</code> made of the result of executing <code>f(args...)</code> and the time elapsed during this call. I used <code>high_resolution_clock</code> in my code but there are valid reasons to use either <code>system_clock</code> or <code>steady_clock</code> in this situation:</p>
<pre class="source-code">
#include &lt;chrono&gt;
#include &lt;utility&gt;
template &lt;class F, class ... Args&gt;
  auto test(F f, Args &amp;&amp;... args) {
      using namespace std;
      using namespace std::chrono;
<strong class="bold">      auto pre = high_resolution_clock::now();</strong>
<strong class="bold">      auto res = f(std::forward&lt;Args&gt;(args)...);</strong>
<strong class="bold">      auto post = high_resolution_clock::now();</strong>
<strong class="bold">      return pair{ res, post - pre };</strong>
  }
// ...</pre> <p>You might wonder why we are requiring non-<code>void</code> functions and returning the result of calling <code>f(args...)</code> even if, in some cases, the return value might be a little artificial. The idea here is to ensure that the compiler thinks the result of <code>f(args...)</code> is useful and does not optimize it away. Compilers are clever beasts indeed and can remove code that seems useless under what is colloquially known as the “as-if rule” (simply put, if there is no visible effect to calling a function, just get rid of it!).</p>
<p>For the test program itself, pay attention to the following aspects:</p>
<ul>
<li>First, we will use <code>std::vector&lt;Orc*&gt;</code>, not <code>std::vector&lt;Orc&gt;</code>. This might seem strange at first, but since we are testing the speed of <code>Orc::operator new()</code> and <code>Orc::operator delete()</code>, we will want to actually call these operators! If we were using a container of <code>Orc</code> objects, there would be no call to our operators whatsoever.</li>
<li>We call <code>reserve()</code> on that <code>std::vector</code> object before running our tests, to allocate the<a id="_idIndexMarker558"/> space<a id="_idIndexMarker559"/> to put the pointers to the <code>Orc</code> objects we will be constructing. That is an important aspect of our measurements: calls to <code>push_back()</code> and similar insertion functions in a <code>std::vector</code> object will need to reallocate if we try to add an element to a full container, and this reallocation will add noise to our benchmarks, so ensuring the container will not need to reallocate during the tests helps us focus on what we want to measure.</li>
<li>What we measure with our <code>test()</code> function (used many times already in this book) is a sequence of <code>Orc::NB_MAX</code> calls to <code>Orc::operator new()</code>, eventually followed by the same number of calls to <code>Orc::operator delete()</code>. We suppose a carnage of sorts in the time between the constructions and the destructions, but we are not showing this violence out of respect for you, dear reader.</li>
<li>Once we reach the end, we print out the results of our measurements, using microseconds as the measurement unit – our computers today are fast enough that milliseconds would probably not be granular enough.</li>
</ul>
<p>The code<a id="_idIndexMarker560"/> follows:</p>
<pre class="source-code">
// ...
#include "Orc.h"
#include &lt;print&gt;
#include &lt;vector&gt;
int main() {
  using namespace std;
  using namespace std::chrono;
<strong class="bold">#ifdef HOMEMADE_VERSION</strong>
<strong class="bold">  print("HOMEMADE VERSION\n");</strong>
<strong class="bold">#else</strong>
<strong class="bold">  print("STANDARD LIBRARY VERSION\n");</strong>
<strong class="bold">#endif</strong>
  vector&lt;<strong class="bold">Orc*</strong>&gt; orcs;
  auto [r0, dt0] = test([&amp;orcs] {
<strong class="bold">      for(int i = 0; i != Orc::NB_MAX; ++i)</strong>
<strong class="bold">        orcs.push_back(new Orc);</strong>
      return size(orcs);
  });
  // ...
  // CARNAGE (CENSORED)
  // ...
  auto [r1, dt1] = test([&amp;orcs] {
<strong class="bold">      for(auto p : orcs)</strong>
<strong class="bold">        delete p;</strong>
      return size(orcs);
  });
  print("Construction: {} orcs in {}\n",
        size(orcs), duration_cast&lt;microseconds&gt;(dt0));
  print("Destruction:  {} orcs in {}\n",
        size(orcs), duration_cast&lt;microseconds&gt;(dt1));
}</pre> <p>At this point, you might wonder whether this is all worth the effort. After all, our standard libraries are probably very efficient (and indeed, they are, on average, excellent!). The only way to know whether the results will make us happy is to run the test code and <a id="_idIndexMarker561"/>see for <a id="_idIndexMarker562"/>ourselves.</p>
<h3>Looking at the numbers</h3>
<p>Using an online gcc 15 compiler with the -O2 optimization level and running this code twice (once with the standard library version and once with the homemade version using a Meyers singleton), I get the following numbers for calls to the <code>new</code> and <code>delete</code> operators on <code>Orc::NB_MAX</code> (here, 106) objects:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p>Homemade</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>N=106</p>
</td>
<td class="No-Table-Style">
<p>Standard library</p>
</td>
<td class="No-Table-Style">
<p>Meyers singleton</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>operator new()</code></p>
</td>
<td class="No-Table-Style">
<p>23433μs</p>
</td>
<td class="No-Table-Style">
<p>17906μs</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>operator delete()</code></p>
</td>
<td class="No-Table-Style">
<p>7943μs</p>
</td>
<td class="No-Table-Style">
<p>638μs</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.1 – Speed comparison with Meyers singleton implementation</p>
<p>Actual numbers will vary depending on a variety of factors, of course, but the interesting aspect of the comparison is the ratio: our homemade <code>operator new()</code> only took 76.4% of the time consumed by the standard library-provided version and our homemade <code>operator delete()</code> took… 8.03% of the time required by our baseline.</p>
<p>Those are quite pleasant results, but they should not really surprise us: we perform constant-time allocation and essentially “no time” deallocation. We do take the time to lock and unlock a <code>std::mutex</code> object on every allocation, but most standard libraries implement mutexes that expect low contention and are very fast under those circumstances, and it so happens that our program does single-threaded allocations and deallocations that lead to code that is clearly devoid of contention.</p>
<p>Now, your acute reasoning skills might lead you to be surprised that deallocation is not actually faster than what we just measured. It’s an empty function we are calling, after all, so what’s consuming this CPU time?</p>
<p>The answer is… our singleton, or more precisely, access to the <code>static</code> local variable used for the Meyers implementation. Remember from <a href="B21071_08.xhtml#_idTextAnchor128"><em class="italic">Chapter 8</em></a> that this technique aims to ensure that a singleton is created when needed, and <code>static</code> local variables are constructed the first time their enclosing function is called.</p>
<p>C++ implements “magic statics” where the call to the <code>static</code> local object’s constructor is guarded by synchronization mechanisms that ensure the object is constructed only once. As we can see, this synchronization, efficient as it is, is not free. In our case, if we can guarantee that no other global object will need to call <code>Tribe::get()</code> before <code>main()</code> is called, we<a id="_idIndexMarker563"/> can replace the Meyers approach with a more classical approach where the singleton is simply a <code>static</code> data member of the <code>Tribe</code> class, declared within the scope of that class and defined at global scope:</p>
<pre class="source-code">
// ...
<strong class="bold">// "global" singleton implementation (the rest of</strong>
<strong class="bold">// the code remains unchanged)</strong>
class Tribe {
  std::mutex m;
  char *p, *cur;
  Tribe() : p{ static_cast&lt;char*&gt;(
      std::malloc(Orc::NB_MAX * sizeof(Orc))
  ) } {
      assert(p);
      cur = p;
  }
  Tribe(const Tribe&amp;) = delete;
  Tribe&amp; operator=(const Tribe&amp;) = delete;
<strong class="bold">  static Tribe singleton;</strong>
public:
  ~Tribe() {
      std::free(p);
  }
<strong class="bold">  </strong><strong class="bold">static auto &amp;get() {</strong>
<strong class="bold">      return singleton;</strong>
<strong class="bold">  }</strong>
  void * allocate() {
      std::lock_guard _ { m };
      auto q = cur;
      cur += sizeof(Orc);
      return q;
  }
  void deallocate(void *) noexcept {
  }
};
<strong class="bold">// in a .cpp file somewhere, within a block surrounded</strong>
<strong class="bold">// with #ifdef HOMEMADE_VERSION and #endif</strong>
<strong class="bold">Tribe Tribe::singleton;</strong>
// ...</pre> <p>Moving the <a id="_idIndexMarker564"/>definition of the singleton object away from within the function – placing it at global scope – removes the need for synchronization around the call to its constructor. We can now compare this implementation with our previous results to evaluate the costs involved, and the gains to be made (if any).</p>
<p>With the same test setup as used previously, adding the “global” singleton to the set of implementations under comparison, we get the following:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style" rowspan="2">
<p>N=106</p>
</td>
<td class="No-Table-Style"/>
<td class="No-Table-Style" colspan="2">
<p>Homemade</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Standard library</p>
</td>
<td class="No-Table-Style">
<p>Meyers singleton</p>
</td>
<td class="No-Table-Style">
<p>Global singleton</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>Operator new()</code></p>
</td>
<td class="No-Table-Style">
<p>23433μs</p>
</td>
<td class="No-Table-Style">
<p>17906μs</p>
</td>
<td class="No-Table-Style">
<p>17573μs</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>Operator delete()</code></p>
</td>
<td class="No-Table-Style">
<p>7943μs</p>
</td>
<td class="No-Table-Style">
<p>638μs</p>
</td>
<td class="No-Table-Style">
<p>0μs</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption">Table 10.2 – Speed comparison with Meyers and “global” singleton implementations</p>
<p>Now, this is more like it! The calls to <code>operator new()</code> are slightly faster than they were 74.99% (of the time it took with the standard library version, and 98.14% of the time it took with the Meyers singleton), but the calls to <code>operator delete()</code> have become no-ops. It’s hard to do better than this!</p>
<p>So, is it worth the effort? It depends on your needs, of course. Speed is a factor; in some programs, the speed gain can be a necessity, but in others, it can be a non-factor or almost so. The <a id="_idIndexMarker565"/>reduction in memory fragmentation can make a big difference in some programs too, and some will use arenas precisely for that reason. The point is this: if you need to do this, now you know how.</p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor159"/>Generalizing to SizeBasedArena&lt;T,N&gt;</h2>
<p>The <code>Tribe</code> class as written <a id="_idIndexMarker566"/>seems specific to the <code>Orc</code> class but, in practice, it really is specific to <code>Orc</code>-<em class="italic">sized</em> objects as it never calls any function of the <code>Orc</code> class; it never constructs an <code>Orc</code> object, nor does it ever destroy one. This means that we could turn that class into a generic class and reuse it for other types that are expected to be used under similar constraints.</p>
<p>To achieve this, we would decouple the arena code from the <code>Orc</code> class and put it in a separate file, maybe called <code>SizeBasedArena.h</code>, for example:</p>
<pre class="source-code">
#ifndef SIZE_BASED_ARENA_H
#define SIZE_BASED_ARENA_H
#include &lt;cassert&gt;
#include &lt;cstdlib&gt;
#include &lt;mutex&gt;
<strong class="bold">template &lt;class T, std::size_t N&gt;</strong>
<strong class="bold">class SizeBasedArena {</strong>
  std::mutex m;
  char *p, *cur;
  SizeBasedArena() : p{ static_cast&lt;char*&gt;(
      std::malloc(N * sizeof(T))
  ) } {
      assert(p);
      cur = p;
  }
  SizeBasedArena(const SizeBasedArena&amp;) = delete;
  SizeBasedArena&amp;
      operator=(const SizeBasedArena&amp;) = delete;
public:
  ~SizeBasedArena() {
      std::free(p);
  }
  static auto &amp;get() {
      static SizeBasedArena singleton;
      return singleton;
  }
<strong class="bold">  void * allocate_one() {</strong>
      std::lock_guard _ { m };
      auto q = cur;
      <strong class="bold">cur += sizeof(T);</strong>
      return q;
  }
<strong class="bold">  void * allocate_n(std::size_t n) {</strong>
      std::lock_guard _ { m };
      auto q = cur;
      <strong class="bold">cur += n * sizeof(T);</strong>
      return q;
  }
<strong class="bold">  void deallocate_one(void *) noexcept {</strong>
  }
<strong class="bold">  void deallocate_n(void *) noexcept {</strong>
  }
};
#endif</pre> <p>It might be surprising that we used <code>T</code> and <code>N</code> as template parameters. Why type <code>T</code> instead of an integer initialized with <code>sizeof(T)</code> if we do not use <code>T</code> in the arena? Well, if the <code>Elf</code> class (for example) used a size-based arena too, and if we were unlucky enough that <code>sizeof(Orc)==sizeof(Elf)</code>, then basing ourselves on the sizes of the types rather than on the types themselves might, if the values for their respective <code>N</code> parameters are the same, lead <code>Orc</code> and <code>Elf</code> to use the same arena… and we do not want that (nor do they!).</p>
<p>To simplify the<a id="_idIndexMarker567"/> initialization <a id="_idIndexMarker568"/>of the singleton in this generic example, we went back to the Meyers technique. It’s more difficult to guarantee the absence of interdependence at construction time for global objects when writing generic code than it was writing the <code>Orc</code>-specific equivalent, as the move to generic code just enlarged the potential user base significantly.</p>
<p>The implementation in <code>Orc.cpp</code> would now be as follows:</p>
<pre class="source-code">
#include "Orc.h"
#ifdef HOMEMADE_VERSION
<strong class="bold">#include "SizeBasedArena.h"</strong>
<strong class="bold">using Tribe = SizeBasedArena&lt;Orc, Orc::NB_MAX&gt;;</strong>
void * Orc::operator new(std::size_t) {
  <strong class="bold">return Tribe::get().allocate_one();</strong>
}
void * Orc::operator new[](<strong class="bold">std::size_t n</strong>) {
  <strong class="bold">return Tribe::get().allocate_n(n / sizeof(Orc));</strong>
}
void Orc::operator delete(void *p) noexcept {
  <strong class="bold">Tribe::get().deallocate_one(p);</strong>
}
void Orc::operator delete[](void *p) noexcept {
  <strong class="bold">Tribe::get().deallocate_n(p);</strong>
}
#endif</pre> <p>You might have noted that since <code>SizeBasedArena&lt;T,N&gt;</code> implements allocation functions for <a id="_idIndexMarker569"/>a single<a id="_idIndexMarker570"/> object or an array of <code>n</code> objects, we have extended the <code>Orc</code> class’s member function allocation operator overloads to cover <code>operator new[]()</code> and <code>operator delete[]()</code>. There’s really no reason not to do so at this point.</p>
<h1 id="_idParaDest-155"><a id="_idTextAnchor160"/>When parameters change</h1>
<p>Our size-based<a id="_idIndexMarker571"/> arena implementation is very specific: it supposes the possibility of sequential allocations and the ability to dismiss the (generally important) question of reusing memory after it has been freed.</p>
<p>An important caveat to any size-based implementation is, obviously, that we are counting on a specific size. Know, thus, that with this constraint, our current implementation is slightly dangerous. Indeed, consider the following evolution of our program, where we envision tougher, meaner <code>Orc</code> subclasses such as the following:</p>
<pre class="source-code">
class MeanOrc <strong class="bold">: public Orc</strong> {
  <strong class="bold">float attackBonus; // oops!</strong>
  // ...
};</pre> <p>It might not be apparent at first, but we just might have broken something important with this new class, as <em class="italic">the member function allocation operators are inherited by derived classes</em>. This means that the <code>Tribe</code> class, also known under the somewhat noisier name of <code>SizeBasedArena&lt;Orc,Orc::NB_MAX&gt;</code>, would implement a strategy meant for blocks of <code>sizeof(Orc)</code> bytes but be used (accidentally) also for objects of size <code>MeanOrc</code>. This can only lead to pain.</p>
<p>We can protect ourselves from this disastrous situation in two ways. For the <code>Orc</code> class, we could disallow<a id="_idIndexMarker572"/> derived classes altogether by marking the class as <code>final</code>:</p>
<pre class="source-code">
class Orc <strong class="bold">final</strong> {
  // ...
};</pre> <p>This removes the possibility of writing <code>MeanOrc</code> as a derived class of <code>Orc</code>; we can still write <code>MeanOrc</code>, but through composition or other techniques, which would sidestep the inherited operators problem.</p>
<p>From the perspective of <code>SizeBasedArena&lt;T,N&gt;</code> itself, we can also decide to restrict our implementation to <code>final</code> types, as in this example:</p>
<pre class="source-code">
// ...
<strong class="bold">#include &lt;type_traits&gt;</strong>
template &lt;class T, std::size_t N&gt;
class SizeBasedArena {
<strong class="bold">  static_assert(std::is_final_v&lt;T&gt;);</strong>
   // ...
};</pre> <p>This last part might not be for everyone, however. There are lots of types (fundamental types, for example) that are not <code>final</code> and that could reasonably be used in a size-based arena, so it’s up to you to see whether this is a good idea or not for the kind of code you write. If it’s not good for you, then these constraints could be expressed as prose rather than as code.</p>
<p>Size-based arenas are far from the only use case for memory arenas. We could envision many variations on both the size-based theme and the allocation strategy.</p>
<p>For example, suppose we introduce shamans in our game and the need to reuse memory becomes a reality. We could have a situation where there are, at most, <code>Orc::NB_MAX</code> objects of the <code>Orc</code> type in the program <em class="italic">at once</em>, but there might be more than that number <em class="italic">overall</em> during the entire program’s execution. In such a situation, we need to consider the following things:</p>
<ul>
<li>If we allow arrays, we will have to deal with <em class="italic">internal</em> fragmentation within the arena, so we might want to consider an implementation that allocates more than <code>N*sizeof(T)</code> bytes per arena, but how much more?</li>
<li>We will need a strategy to reuse memory. There are many approaches at our disposal, including maintaining an ordered list of <code>begin,end</code> pairs to delimit the free blocks (and fuse them more easily to reduce fragmentation) or keeping a stack (maybe a set of stacks based on block size) of recently freed blocks to make it easier to reuse freed blocks quickly.</li>
</ul>
<p>Answers to such<a id="_idIndexMarker573"/> questions as “<em class="italic">What is the best approach for our code base?</em>” are in part technical and in part political: what makes allocation fast may slow down deallocation, what makes allocation speed deterministic may cost more in memory space overhead, and so on. The question is to determine what trade-offs work best in our situation and measure to ensure we reap the desired benefits. If we cannot manage to do better than the standard library already does, then by all means, use the standard library!</p>
<h1 id="_idParaDest-156"><a id="_idTextAnchor161"/>Chunked pools</h1>
<p>Our size-based<a id="_idIndexMarker574"/> arena example was optimized for a single block size and specific usage patterns, but there are many other reasons to want to apply a specialized allocation strategy. In this section, we will explore the idea of a “chunked pool,” or a pool of pre-allocated raw memory of selected block sizes. This is meant as an academic example to build upon more than as something to use in production; the code that follows will be reasonably fast and can be made to become very fast, but in this book, we will focus on the general approach and leave you, dear reader, to enjoy optimizing it to your liking.</p>
<p>The idea in this example is that user code plans to allocate objects of similar (but not necessarily identical) sizes and of various types and supposes an upper bound on the maximal number of objects. This gives us additional knowledge; using that knowledge, we will write a <code>ChunkSizedAllocator&lt;N,Sz...&gt;</code> type where <code>N</code> will be the number of objects of each “size category” and each integral value in <code>Sz...</code> will be a distinct size category.</p>
<p>To give a clarifying example, a <code>ChunkSizedAllocator&lt;10,20,40,80,160&gt;</code> object would pre-allocate sufficient raw memory to hold 10 objects of size 20 bytes, 40 bytes, 80 bytes, and 160 bytes each for a total of at least 3,000 bytes (the sum of the minimal size required for each size category being <em class="italic">200 + 400 + 800 + 1600</em>). We say “at least” in this case because to be useful, our class will need to consider alignment and will generally need more than the minimal amount of memory if we are to avoid allocating misaligned objects.</p>
<p>To understand what we are going to do, here are some pointers (pun intended):</p>
<ul>
<li>In the variadic sequence of integral values <code>Sz...</code> we will require the values to be sorted in ascending order, as this will make further lookup faster (linear complexity rather than quadratic complexity). Since these values are known at compile time, being part of the template parameters of our type, this has no runtime costs and is more of a constraint imposed on the user. We will, of course, validate this at compile time to avoid unpleasant mishaps.</li>
<li>In C++, variadic packs can be empty, but in our case, an empty set of size categories would make no sense so we will ensure that does not happen (at compile time, of course). Obviously, <code>N</code> has to be more than zero for this class to be useful so we will validate this also.</li>
<li>What might not be self-evident is that values in <code>Sz...</code> have to be at least <code>sizeof(std::max_align_t)</code> (we could have tested for <code>alignof</code> too but, for fundamental types, this is redundant) and that, in practice, we will need to make the effective size categories powers of two to make sure arbitrary types can be allocated. This latter part will be handled internally, as it’s trickier to impose on user code.</li>
</ul>
<p>Looking at the code, we can see these constraints expressed explicitly. Note that to make the “code narrative” easier to follow, the code that follows is presented step by step, so make sure to<a id="_idIndexMarker575"/> look at the complete example if you want to experiment with it:</p>
<pre class="source-code">
#include &lt;algorithm&gt;
#include &lt;vector&gt;
#include &lt;utility&gt;
#include &lt;memory&gt;
#include &lt;cassert&gt;
#include &lt;concepts&gt;
#include &lt;limits&gt;
#include &lt;array&gt;
#include &lt;iterator&gt;
#include &lt;mutex&gt;
<strong class="bold">// ... helper functions (shown below)...</strong>
template &lt;int N, auto ... Sz&gt;
  class ChunkSizedAllocator {
      static_assert(<strong class="bold">is_sorted(make_array(Sz...))</strong>);
      static_assert(sizeof...(Sz) &gt; 0);
      static_assert(
        <strong class="bold">((Sz &gt;= sizeof(std::max_align_t)) &amp;&amp; ...)</strong>
      );
      static_assert(N &gt; 0);
      static constexpr unsigned long long sizes[] {
<strong class="bold">        next_power_of_two(Sz)...</strong>
      };
      using raw_ptr = void*;
<strong class="bold">      raw_ptr blocks[sizeof...(Sz)];</strong>
<strong class="bold">      int cur[sizeof...(Sz)] {}; // initialized to zero</strong>
      // ...</pre> <p>Note that we have two data members – namely, <code>blocks</code>, which will contain a pointer to a block of raw<a id="_idIndexMarker576"/> memory for each size category, and <code>cur</code>, which will contain the index of the next allocation within a block for each size category (initialized to zero by default, as we will start from the beginning in each case).</p>
<p>The code for this class continues shortly. For now, you might notice some unexplained helper functions:</p>
<ul>
<li>We use <code>make_array(Sz...)</code>, a <code>constexpr</code> function that constructs an object of type <code>std::array&lt;T,N&gt;</code> from the values of <code>Sz...</code>, expecting all values to be of the same type (the type of the first value of <code>Sz...</code>). We know <code>N</code> for the resulting <code>std::array&lt;T,N&gt;</code> to be a compile-time constant as it is computed from the number of values in <code>Sz...</code>.</li>
<li>We use the <code>is_sorted()</code> predicate on that <code>std::array&lt;T,N&gt;</code> object to ensure, at compile time, that the values are sorted in ascending order, as we expect them to be. Unsurprisingly, this will simply call the <code>std::is_sorted()</code> algorithm, which is <code>constexpr</code> and thus usable in this context.</li>
<li>The non-<code>static</code> member array named <code>sizes</code> will contain the next power of two for each value in <code>Sz...</code>, including that value, of course: if the value is already a power of two, wonderful! Thus, if <code>Sz...</code> is <code>10,20,32</code>, then <code>sizes</code> will contain <code>16,32,32</code>.</li>
</ul>
<p class="callout-heading">Why powers of two?</p>
<p class="callout">In practice, blocks that are not powers of two will lead to misaligned objects after the first allocation if we allocate them contiguously, and managing padding to avoid this is possible but would complicate our implementation significantly. To make allocations quicker, we compute the next power to two for each element of <code>Sz...</code> at compile time and store them in the <code>sizes</code> array. This means we could have two size categories that end up being of the same size (for example, <code>40</code> and <code>60</code> would both lead to 64 bytes blocks) but that’s a minor issue (as code would still work) considering that this is a specialized facility designed for knowledgeable users.</p>
<p>The code for these <a id="_idIndexMarker577"/>helper functions, in practice, defined before the declaration of the <code>ChunkSizedAllocator&lt;N,Sz...&gt;</code> class is as follows:</p>
<pre class="source-code">
// ...
template &lt;class T, std::same_as&lt;T&gt; ... Ts&gt;
  constexpr std::array&lt;T, sizeof...(Ts)+1&gt;
      make_array(T n, Ts ... ns) {
        return { n, ns... };
      }
constexpr bool is_power_of_two(std::integral auto n) {
  return n &amp;&amp; ((n &amp; (n - 1)) == 0);
}
class integral_value_too_big {};
constexpr auto next_power_of_two(std::integral auto n) {
  constexpr auto upper_limit =
      std::numeric_limits&lt;decltype(n)&gt;::max();
  for(; n != upper_limit &amp;&amp; !is_power_of_two(n); ++n)
       ;
  if(!is_power_of_two(n)) throw integral_value_too_big{};
  return n;
}
template &lt;class T&gt;
  constexpr bool is_sorted(const T &amp;c) {
      return std::is_sorted(std::begin(c), std::end(c));
  }
// ...</pre> <p>Note that <code>make_array()</code> uses concepts to constrain that all values are of the same type, and that <code>is_power_of_two(n)</code> ensures that the proper bits of <code>n</code> are tested to make this test quick (it also tests <code>n</code> to ensure we do not report <code>0</code> as being a power of two). The <code>next_power_of_two()</code> function could probably be made much faster but that’s of little consequence here as it is only used at compile time (we could enforce this by making it <code>consteval</code> instead of <code>constexpr</code>, but there might be users that want to choose between run time and compile time usage so we’ll give them that choice).</p>
<p>Returning to<a id="_idIndexMarker578"/> our <code>ChunkSizedAllocator&lt;N,Sz...&gt;</code> implementation after this short digression on helper functions, we have a member function named <code>within_block(p,i)</code> that returns <code>true</code> only if pointer <code>p</code> is within <code>blocks[i]</code>, which is the <code>i</code>-th pre-allocated block of memory of our object. The logic for that function seems deceptively simple: one might simply want to test something that looks like <code>blocks[i]&lt;=p&amp;&amp;p&lt;blocks[i]+N</code> but with the proper casts applied, as the <code>blocks[i]</code> variable is of type <code>void*,</code> which precludes pointer arithmetic, but that happens to be incorrect in C++ (remember our discussion of the intricacies of pointer arithmetic in <a href="B21071_02.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>). It probably works in practice for compatibility with C code, but it’s not something you want to rely on.</p>
<p>As of this writing, there are ongoing discussions to add a standard library function to test whether a pointer is between two others, but until this happens, we can at least use the standard library-provided <code>std::less</code> functor to make the comparisons somewhat legal. This is unsatisfactory, I know, but it will probably work on all compilers today… and by making this test local to a specialized function, we will simplify source code updates once we have a real standard solution to this problem:</p>
<pre class="source-code">
      // ...
      bool within_block(void *p, int i) {
        void* b = blocks[i];
        void* e = static_cast&lt;char*&gt;(b) + N * sizes[i];
        return p == b ||
                (<strong class="bold">std::less{}(b, p) &amp;&amp; std::less{}(p, e)</strong>);
      }
      // ...</pre> <p>There’s no reason to make objects of <code>ChunkSizedAllocator&lt;N,Sz...&gt;</code> globally available: this is a tool that could be instantiated many times in a program and used to solve various problems. We do not want that type to be copyable, however (we could, but that would really complicate the design for limited returns).</p>
<p>Through <code>std::malloc()</code>, our constructor allocates the raw memory blocks for the various sizes in <code>Sz...</code>, or at least the next power of two for each of these sizes, as explained earlier in this section, ensuring afterward that all of the allocations succeeded. We used <code>assert()</code> for this, but one could also throw <code>std::bad_alloc</code> on failure as long as one carefully called <code>std::free()</code> on the memory blocks that were successfully allocated before doing so.</p>
<p>Our destructor, unsurprisingly, calls <code>std::free()</code> on each memory block: as with the arena implementation <a id="_idIndexMarker579"/>earlier in this chapter, a <code>ChunkSizedAllocator&lt;N,Sz...&gt;</code> object is responsible for memory, not the objects put there by client code, so we have to suppose that client code destroyed all objects stored within the memory blocks of a <code>ChunkSizedAllocator</code> object before that object’s destructor is called.</p>
<p>Note the presence of a <code>std::mutex</code> data member, as we will need this (or some other synchronization tool) to ensure allocations and deallocations are thread-safe later:</p>
<pre class="source-code">
      // ...
      <strong class="bold">std::mutex m</strong>;
  public:
      ChunkSizedAllocator(const ChunkSizedAllocator&amp;)
         = delete;
      ChunkSizedAllocator&amp;
        operator=(const ChunkSizedAllocator&amp;) = delete;
      ChunkSizedAllocator() {
        int i = 0;
<strong class="bold">        for(auto sz : sizes)</strong>
<strong class="bold">            blocks[i++] = std::malloc(N * sz);</strong>
<strong class="bold">        assert(std::none_of(</strong>
<strong class="bold">            </strong><strong class="bold">std::begin(blocks), std::end(blocks),</strong>
<strong class="bold">            [](auto p) { return !p; }</strong>
<strong class="bold">        ));</strong>
      }
      ~ChunkSizedAllocator() {
<strong class="bold">        for(auto p : blocks)</strong>
<strong class="bold">            std::free(p);</strong>
      }
      // ...</pre> <p>Finally, we reach <a id="_idIndexMarker580"/>the crux of our effort with the <code>allocate()</code> and <code>deallocate()</code> member functions. In <code>allocate(n)</code>, we search for the smallest element, <code>sizes[i]</code>, for which the allocated block size is sufficiently big to hold <code>n</code> bytes. Once one such block is found, we lock our <code>std::mutex</code> object to avoid race conditions and then look to see whether there is still at least one available block in <code>blocks[i]</code>; this implementation takes them sequentially and does not reuse them, to keep the discussion simple. If there is one, we take it, update <code>cur[i]</code>, and return the appropriate address to the user code.</p>
<p>Note that when we do not find a free block in our pre-allocated blocks, or when <code>n</code> is too large for the blocks we allocated upfront, we delegate the allocation responsibility to <code>::operator new()</code> such that the allocation request might still succeed. We could also have thrown <code>std::bad_alloc</code> in this case, depending on what the intent is: if it’s important to us that the allocation is made within our blocks and nowhere else, throwing or otherwise failing is a better choice.</p>
<p class="callout-heading">How could failing be a good thing?</p>
<p class="callout">Some applications, particularly in embedded systems of low-latency or real-time system domains, are such that software that delivers the right answer or produces the right computation but not in due time is as bad as software that produces a wrong answer. Think, for example, of a system that controls the brakes of a car: a car that stops after colliding is of limited usefulness indeed. Such systems are rigorously tested to catch failures before being released and will count on specific runtime behavior; for that reason, when under development, they might prefer failing (in a way that will be caught during their testing phase) rather than defaulting to a strategy that might sometimes not meet their timing requirements. Of course, please do not ship critical systems that stop working when used in real life: test them well and make sure users are kept safe! But maybe you are developing a system where, if something bad happens, you will prefer to print “Sorry, we messed up” somewhere and just restart the program, and that’s perfectly fine too sometimes.</p>
<p>The <code>deallocate(p)</code> deallocation<a id="_idIndexMarker581"/> function goes through each memory block to see whether <code>p</code> is within that block. Remember that our <code>within_block()</code> function would benefit from a pointer comparison test that the standard does not yet provide as of this writing, so if you use this code in practice, make sure you leave yourself a note to apply this new function as soon as it becomes available. If <code>p</code> is in none of our blocks, then it was probably allocated through <code>::operator new()</code> so we make sure to free it through <code>::operator delete()</code> as we should.</p>
<p>As stated previously, our implementation does not reuse memory once it has been freed, but the location where that reuse should happen has been left in comments (along with code that locks the mutex for that section) so feel free to implement memory block reuse logic there if you want to:</p>
<pre class="source-code">
      // ...
      auto allocate(std::size_t n) {
        using std::size;
<strong class="bold">        // use smallest block available</strong>
        for(std::size_t i = 0; i != size(sizes); ++i) {
            if(n &lt; sizes[i]) {
              std::lock_guard _ { m };
              if(cur[i] &lt; N) {
                  void *p = static_cast&lt;char*&gt;(blocks[i]) +
                            cur[i] * sizes[i];
                  ++cur[i];
                  return p;
              }
            }
        }
<strong class="bold">        // either no block fits or no block left</strong>
        return ::operator new(n);
      }
      void deallocate (void *p) {
        using std::size;
        for(std::size_t i = 0; i != size(sizes); ++i) {
            if(within_block(p, i)) {
<strong class="bold">              //std::lock_guard _ { m };</strong>
<strong class="bold">              // if you want to reuse the memory,</strong>
<strong class="bold">              // it's in blocks[i]</strong>
              return;
            }
        }
<strong class="bold">        // p is not in our blocks</strong>
        ::operator delete(p);
      }
  };
  // ...</pre> <p>Since this is a specialized form of allocation to be used by client code as needed, we will use specialized overloads of the allocation operators. As can be expected, these overloads will be <a id="_idIndexMarker582"/>templates based on the parameters of the <code>ChunkSizedAllocator</code> object to be used:</p>
<pre class="source-code">
<strong class="bold">template &lt;int N, auto ... Sz&gt;</strong>
  void *operator new(std::size_t n, <strong class="bold">ChunkSizedAllocator&lt;</strong>
<strong class="bold">      N, Sz...</strong>
<strong class="bold">  &gt; &amp;chunks</strong>) {
      return <strong class="bold">chunks.allocate(n);</strong>
  }
<strong class="bold">template &lt;int N, auto ... Sz&gt;</strong>
  void operator delete (void *p, <strong class="bold">ChunkSizedAllocator&lt;</strong>
<strong class="bold">      N, Sz...</strong>
<strong class="bold">  &gt; &amp;chunks</strong>) {
      return <strong class="bold">chunks.deallocate(p);</strong>
  }
// new[] and delete[] left as an exercise ;)</pre> <p>Now, we wrote these allocation facilities, but we need to test them, as we need to see whether there are <a id="_idIndexMarker583"/>benefits to this approach.</p>
<h3>Testing ChunkSizedAllocator</h3>
<p>We will now write <a id="_idIndexMarker584"/>a simple test program that <a id="_idIndexMarker585"/>uses a <code>ChunkSizedAllocator</code> object with an appropriate set of size categories, then allocate and deallocate objects with sizes that fit within these categories in ways that should benefit our class. In so doing, we are supposing that users of this class do so seeking to benefit from a priori known size categories. Other tests could be conducted to verify the code’s behavior with inappropriate size requests or in the presence of throwing constructors, for example, so feel free to write a more elaborate test harness than the one we will be providing for the sake of our execution speed-related discussion.</p>
<p>The <code>test()</code> function used to test our size-based arena earlier in this chapter will be used here again. See that section for an explanation of its workings.</p>
<p>It’s not trivial to write a good test program to validate the behavior of a program that allocates and deallocates objects of various sizes. What we will do is use a <code>dummy&lt;N&gt;</code> type whose objects will each occupy a space of <code>N</code> bytes in memory (as we will use <code>char[N]</code> data members to get this result, we know that <code>alignof(dummy&lt;N&gt;)==1</code> for all valid values of <code>N</code>).</p>
<p>We will also write two distinct <code>test_dummy&lt;N&gt;()</code> functions. Each of these functions will allocate and then construct the <code>dummy&lt;N&gt;</code> object and set up the associated destroy-then-deallocate code, but one will use the standard library implementation of the allocation operators and the other will use our overloads.</p>
<p>You will note that both of our <code>test_dummy&lt;N&gt;()</code> functions return a pair of values: one will be a pointer to the allocated object and the other will be the code to destroy and deallocate that object. Since we will store this information in client code, we need these pairs to be<a id="_idIndexMarker586"/> abstractions that share a common <a id="_idIndexMarker587"/>type, which explains our use of <code>void*</code> for the address and <code>std::function&lt;void(void*)&gt;</code> for the destruction code. We need <code>std::function</code> or something similar here: a function pointer would not suffice as the destruction code can be stateful (we sometimes need to remember what object was used to manage the allocation).</p>
<p>The code for these tools follows:</p>
<pre class="source-code">
#include &lt;chrono&gt;
#include &lt;utility&gt;
#include &lt;functional&gt;
template &lt;class F, class ... Args&gt;
  auto test(F f, Args &amp;&amp;... args) {
      using namespace std;
      using namespace std::chrono;
      auto pre = high_resolution_clock::now();
      auto res = f(std::forward&lt;Args&gt;(args)...);
      auto post = high_resolution_clock::now();
      return pair{ res, post - pre };
  }
<strong class="bold">template &lt;int N&gt; struct dummy { char _[N] {}; };</strong>
template &lt;int N&gt; auto <strong class="bold">test_dummy()</strong> {
<strong class="bold">  return std::pair&lt;void *, std::function&lt;void(void*)&gt;&gt; {</strong>
<strong class="bold">      new dummy&lt;N&gt;{},</strong>
<strong class="bold">      [](void *p) { delete static_cast&lt;dummy&lt;N&gt;*&gt;(p); }</strong>
<strong class="bold">  };</strong>
}
template &lt;int N, <strong class="bold">class T</strong>&gt; auto test_dummy(<strong class="bold">T &amp;alloc</strong>) {
  return <strong class="bold">std::pair&lt;void *, std::function&lt;void(void*)&gt;&gt; {</strong>
<strong class="bold">      new (alloc) dummy&lt;N&gt;{},</strong>
<strong class="bold">    </strong><strong class="bold">[&amp;alloc](void *p) { ::operator delete(p, alloc); }</strong>
<strong class="bold">  };</strong>
}
// ...</pre> <p>Finally, we have to <a id="_idIndexMarker588"/>write the test program. We <a id="_idIndexMarker589"/>will discuss this program step by step to make sure we grasp all the subtleties involved in the process.</p>
<p>Our program first decides on a value of <code>N</code> for the <code>ChunkSizedAllocator</code> object as well as on size categories <code>Sz...</code> for that memory manager to use (the value I picked for <code>N</code> is arbitrary). I deliberately used one <em class="italic">non-power-of-two</em> size category to show that the values are “rounded up” to the next power of two appropriately: the size request of <code>62</code> is translated into <code>64</code> when constructing the <code>sizes</code> data member of our type. We then construct that object and name it <code>chunks</code> because… well, why not?</p>
<pre class="source-code">
// ...
#include &lt;print&gt;
#include &lt;vector&gt;
int main() {
  using namespace std;
  using namespace std::chrono;
  constexpr int N = 100'000;
<strong class="bold">  using Alloc = ChunkSizedAllocator&lt;</strong>
<strong class="bold">      N, 32, 62 /* 64 */, 128</strong>
<strong class="bold">  &gt;;</strong>
<strong class="bold">  Alloc chunks; // construct the ChunkSizedAllocator</strong>
  // ...</pre> <p>The tests that <a id="_idIndexMarker590"/>follow take the same form for the<a id="_idIndexMarker591"/> standard library and for our specialized facility. Let’s look at them in detail:</p>
<ol>
<li>We create a <code>std::vector</code> object of pairs named <code>ptrs</code> filled with default values (null pointers and non-callable functions) for <code>N</code> objects in three size categories (because <code>sizeof...(Sz)==3</code> in our example). This ensures that the allocation for the space used by the <code>std::vector</code> object is performed prior to our measurements (prior to the execution of the lambda expression passed to <code>test()</code>) and does not interfere with them later. Note that each tested lambda is mutable as it needs to modify the captured <code>ptrs</code> object.</li>
<li>For each of the three size categories, we then allocate <code>N</code> objects of sizes that fit in that category and remember through the returned <code>pair</code> both that object’s address and the code that will correctly finalize it later.</li>
<li>Then, to end each test, we use the finalization code on each object and destroy and then deallocate it.</li>
</ol>
<p>It sounds worse than it is, happily for us. Once the tests have run to completion, we print out the execution <a id="_idIndexMarker592"/>time of each test expressed as microseconds:</p>
<pre class="source-code">
  // ...
  auto [r0, dt0] = test([ptrs = std::vector&lt;
      std::pair&lt;
         void*, std::function&lt;void(void*)&gt;
      &gt;&gt;(N * 3)]() mutable {
      <strong class="bold">// allocation</strong>
      for(int i = 0; i != N * 3; i += 3) {
        ptrs[i] = <strong class="bold">test_dummy&lt;30&gt;()</strong>;
        ptrs[i + 1] = <strong class="bold">test_dummy&lt;60&gt;()</strong>;
        ptrs[i + 2] = <strong class="bold">test_dummy&lt;100&gt;()</strong>;
      }
      <strong class="bold">// cleanup</strong>
      for(auto &amp; p : ptrs)
        <strong class="bold">p.second(p.first)</strong>;
      return std::size(ptrs);
  });
  auto [r1, dt1] = test([<strong class="bold">&amp;chunks</strong>, ptrs = std::vector&lt;
      std::pair&lt;
        void*, std::function&lt;void(void*)&gt;
      &gt;&gt;(N * 3)]() mutable {
      <strong class="bold">// allocation</strong>
      for(int i = 0; i != N * 3; i += 3) {
        ptrs[i] = <strong class="bold">test_dummy&lt;30&gt;(chunks)</strong>;
        ptrs[i + 1] = <strong class="bold">test_dummy&lt;60&gt;(chunks)</strong>;
        ptrs[i + 2] = <strong class="bold">test_dummy&lt;100&gt;(chunks)</strong>;
      }
      <strong class="bold">// cleanup</strong>
      for(auto &amp; p : ptrs)
         <strong class="bold">p.second(p.first)</strong>;
      return std::size(ptrs);
  });
   std::print("Standard version : {}\n",
              duration_cast&lt;microseconds&gt;(dt0));
  std::print("Chunked version  : {}\n",
              duration_cast&lt;microseconds&gt;(dt1));
}</pre> <p>Okay, so that was slightly intricate but hopefully instructive. Is it worth the trouble? Well, it depends on your needs.</p>
<p>When I ran this code on the same online gcc 15 compiler with the -O2 optimization level as with the size-based<a id="_idIndexMarker593"/> arena, the standard library <a id="_idIndexMarker594"/>version reported an execution time of 13,360, whereas the time reported for the “chunked” version was 12,032, effectively 90.05% of the standard version’s execution time. This kind of speedup can be lovely as long as we remember that the initial allocation done in the constructor of our <code>chunks</code> object was not measured: the idea here is to show we can save time when it’s important and choose to pay for it when we are not in a hurry.</p>
<p>It’s important to remember that this implementation does not reuse memory, but the standard version does so, which means our speedup might be counterbalanced by a loss of functionality (if it’s a functionality you need, of course). In the tests I ran, locking the <code>std::mutex</code> object or not doing so had a significant impact on speedup, so (a) depending on your platform, there might be a better choice of synchronization mechanism at your disposal, and (b) this implementation is probably too naïve to bring benefits as is if the <code>deallocate()</code> member function also needs to lock the <code>std::mutex</code> object.</p>
<p>Of course, one could optimize this (quite academic) version quite a bit, and I invite you dear readers to do so (and test the results every step of the way!). The point of this section was more to show (a) that chunk size-based allocation can be done, (b) how it can be done from<a id="_idIndexMarker595"/> an architectural standpoint, and (c) point <a id="_idIndexMarker596"/>out some risks and potential pitfalls along the way.</p>
<p>That was fun, wasn’t it?</p>
<h1 id="_idParaDest-157"><a id="_idTextAnchor162"/>Summary</h1>
<p>As a reminder, in this chapter, we examined arena-based allocation with a concrete example (a size-based arena with a particular usage pattern) and saw we could get significant results from it, and then saw another use case with pre-allocated memory blocks from which we picked chunks where we placed objects, again seeing some benefits. These techniques showed new ways to control memory management, but in no way are they meant to represent an exhaustive discussion on the subject. To be honest, this entire book cannot be an exhaustive treatise on the subject, but it can hopefully give us ideas!</p>
<p>The next step in our journey will be to expand the techniques seen in this chapter and write something that is not really a garbage collector but is in some ways weaker and in some ways better: deferred reclamation memory zones. This will be our last step before we start discussing memory management in containers.</p>
</div>
</body></html>