<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-17"><a id="_idTextAnchor016"/>1</h1>
<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Introducing the Raptor Engine and Hydra</h1>
<p>When we set out to write this book, we decided our goal was to start where a traditional Vulkan tutorial might end. There are plenty of great resources, both in print and on the web, that help beginners discover and understand the Vulkan API.</p>
<p>We decided to write this book as we felt there was a gap between these introductory tutorials and some of the more advanced material. Some of these topics might be covered in articles and blog posts, but we couldn’t find a resource that organized them in a single and cohesive format.</p>
<p>While we assume some familiarity with Vulkan, in this chapter, we take the opportunity to go over some of the basic concepts that we will build upon throughout the remainder of the book. We will present the code organization and the classes and libraries that we use throughout the book.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>How to read this book</li>
<li>Understanding the code structure</li>
<li>Understanding the glTF scene format</li>
<li>Physically based rendering in a nutshell</li>
<li>A word on GPU debugging</li>
</ul>
<p>By the end of this chapter, you will be familiar with the Raptor Engine and the rendering framework we developed for this book. You will have also learned the structure of the glTF model format and the base concepts behind physically based rendering.</p>
<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>Technical requirements</h1>
<p>You will need a GPU that supports at least Vulkan 1.1. At the time of writing, Vulkan 1.3 had just been announced and many vendors, such as AMD and Nvidia, have provided day-one support. We have kept the lower requirements to allow as many people as possible to follow along.</p>
<p>Some of the later chapters will make use of hardware features that might not be available on some of the older graphics cards. Wherever possible, we will provide an alternative software solution. If it’s not feasible, we try to focus more on the generic aspects of the implementation and less on the API details.</p>
<p>The complete code for this chapter is available on GitHub at <a href="https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter1">https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter1</a>.</p>
<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Windows</h2>
<p>The code has<a id="_idIndexMarker000"/> been tested on Windows with Visual Studio 2019 16.11 and <a id="_idIndexMarker001"/>the Vulkan SDK version 1.2.198.1 (this might change as we write the book).</p>
<p>To install the Vulkan SDK on Windows, you will need to download and run the following executable:</p>
<p><a href="https://sdk.lunarg.com/sdk/download/1.2.198.1/windows/VulkanSDK-1.2.198.1-Installer.exe%0D">https://sdk.lunarg.com/sdk/download/1.2.198.1/windows/VulkanSDK-1.2.198.1-Installer.exe</a></p>
<p>After installing the Vulkan SDK, make sure you can run the <code>vulkaninfoSDK.exe</code> program in the <code>Bin</code> folder to confirm that the SDK has been installed correctly and that your graphics drivers support Vulkan.</p>
<p>Please check the official documentation (<a href="https://vulkan.lunarg.com/doc/sdk/latest/windows/getting_started.xhtml">https://vulkan.lunarg.com/doc/sdk/latest/windows/getting_started.xhtml</a>) should you need further details on the installation process.</p>
<p>We have provided a Visual Studio solution that contains the full code for the book and that allows you to easily build the executable for each chapter.</p>
<p>Once the solution has been built, set the <code>Chapter1</code> project as the run target and run the program. Here’s what you should be seeing:</p>
<div><div><img alt="Figure 1.1 – The rendering result" height="1235" src="img/B18395_01_01.jpg" width="1656"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – The rendering result</p>
<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Linux</h2>
<p>For Linux, we <a id="_idIndexMarker002"/>have used Visual Studio Code, GCC 9 or above, and<a id="_idIndexMarker003"/> CMake 3.22.1. The version of the Vulkan SDK matches the one on Windows. We tested both on Debian 11 and Ubuntu 20.04.</p>
<p>We have used CMake to support different build systems, but we have only tested with Makefile.</p>
<p>To install the Vulkan SDK, you will need to download this file: <a href="https://sdk.lunarg.com/sdk/download/1.2.198.1/linux/vulkansdk-linux-x86_64-1.2.198.1.tar.gz">https://sdk.lunarg.com/sdk/download/1.2.198.1/linux/vulkansdk-linux-x86_64-1.2.198.1.tar.gz</a>.</p>
<p>Assuming you have downloaded it in the <code>~/Downloads</code> folder, extract the package by running the following command:</p>
<pre class="console">
$ tar -xvf vulkansdk-linux-x86_64-1.2.198.1.tar.gz</pre>
<p>This will create the <code>1.2.198.1</code> top-level folder.</p>
<p>There are two options to make the SDK available to build the code:</p>
<ul>
<li>You can add the following environment variables to your <code>~/.bashrc</code> file (or the main configuration file of your shell if you are not using Bash). Please note that you might have to create this file:<pre class="source-code">
export VULKAN_SDK=~/vulkan/1.2.198.1/x86_64</pre><pre class="source-code">
export PATH=$VULKAN_SDK/bin:$PATH</pre><pre class="source-code">
export LD_LIBRARY_PATH=$VULKAN_SDK/lib:</pre><pre class="source-code">
$LD_LIBRARY_PATH</pre><pre class="source-code">
export VK_LAYER_PATH=$VULKAN_SDK/etc/vulkan/</pre><pre class="source-code">
explicit_layer.d</pre></li>
<li>The other option is to add the following to your <code>~/.</code><code>bashrc</code> file:<pre class="source-code">
source ~/Downloads/1.2.198.1/setup-env.sh</pre></li>
</ul>
<p>After you have edited the <code>~/.bashrc</code> file, restart your Terminal. You should now be able to run <code>vulkaninfo</code>. If that’s not the case, try to follow the previous steps again. Please <a id="_idIndexMarker004"/>refer <a id="_idIndexMarker005"/>to the official LunarG guide (<a href="https://vulkan.lunarg.com/doc/sdk/latest/linux/getting_started.xhtml">https://vulkan.lunarg.com/doc/sdk/latest/linux/getting_started.xhtml</a>) should you need more details on the installation process.</p>
<p>To generate the build files, you need to run the following command:</p>
<pre class="console">
$ cmake -B build -DCMAKE_BUILD_TYPE=Debug</pre>
<p>If you’d like to create a release build, run the following command:</p>
<pre class="console">
$ cmake -B build -DCMAKE_BUILD_TYPE=Release</pre>
<p>This will create the build files in the <code>build</code> folder. You can, of course, use a different name for the folder.</p>
<p>To build the code for this chapter, run the following command:</p>
<pre class="console">
$ cmake --build build --target chapter1 -- -j 4</pre>
<p>The number after <code>-j</code> tells the compiler how many threads to use to compile the code in parallel. The recommended value is to use the number of cores your processor has.</p>
<p>After the build has completed, the <code>Chapter1</code> executable has been created and is ready to run!</p>
<p class="callout-heading">Note</p>
<p class="callout">Both Windows and Linux builds have been tested throughout the writing of the book by our technical reviewers and beta readers, but some issues might have gone unnoticed. If you have questions or if you would like to report an issue, please open a GitHub issue or reach out <a id="_idIndexMarker006"/>to <a id="_idIndexMarker007"/>us on Twitter: <code>@marco_castorina</code> and <code>@GabrielSassone</code>.</p>
<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>macOS</h2>
<p>Vulkan is<a id="_idIndexMarker008"/> not natively available on macOS but is provided through a<a id="_idIndexMarker009"/> translation layer into Metal, the graphics API developed by Apple. This translation layer is provided by the Vulkan SDK with the MoltenVK library.</p>
<p>Because of this indirection, not all features and extensions are available on macOS. Given that we are going to make use of some advanced features such as ray tracing in this book, we didn’t want to provide a partially working version of our code for macOS. For the time being, this platform is not supported.</p>
<h1 id="_idParaDest-23"><a id="_idTextAnchor022"/>How to read this book</h1>
<p>We have organized the content of this book to gradually build on more advanced features. Advanced chapters in this book will rely on topics exposed earlier in the book. For this reason, we suggest that you read the book in order.</p>
<p>However, some of the later chapters on ray tracing could be read in any order as they cover topics that can be developed independently. If you are already familiar with the topic in one of the chapters, we still recommend that you skim through it as you might still find some valuable information.</p>
<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>Understanding the code structure</h1>
<p>In this section, we will <a id="_idIndexMarker010"/>deep dive into the foundational code used throughout the book and we will explain the rationale behind some of the decisions we made.</p>
<p>When we started thinking about the code to be used, the objective was clear: there was the need for something lightweight, simple, and basic enough to give us the possibility to build upon it. A fully fledged library would have been too much.</p>
<p>Also, we needed something that we were familiar with to make the development process smoother and give us confidence.</p>
<p>There are different great libraries out there, such as Sokol (<a href="https://github.com/floooh/sokol">https://github.com/floooh/sokol</a>) or BGFX (<a href="https://github.com/bkaradzic/bgfx">https://github.com/bkaradzic/bgfx</a>), and a few more, but they all have some drawbacks that seemed problematic.</p>
<p>Sokol, for example, even though it is a great library, does not support the Vulkan API, and has an interface still based on older graphics APIs (such as OpenGL and D3D11).</p>
<p>BGFX is a more complete library, but it is a little too generic and feature-fledged to give us the possibility to build upon it.</p>
<p>After some research, we leaned toward the Hydra Engine – a library that Gabriel developed in the last couple of years as code to experiment with and write articles on rendering.</p>
<p>Here are some advantages of starting from the Hydra Engine (<a href="https://github.com/JorenJoestar/DataDrivenRendering">https://github.com/JorenJoestar/DataDrivenRendering</a>) and evolving it into the Raptor Engine:</p>
<ul>
<li>Code familiarity</li>
<li>Small and simple code base</li>
<li>Vulkan-based API</li>
<li>No advanced features, but strong building blocks</li>
</ul>
<p>The Hydra Engine seemed perfect, being small but usable and familiar. From an API design perspective, it was a clear advantage compared to other libraries that both authors had used in the past.</p>
<p>Being designed from scratch by Gabriel, evolving the code through this book is done with the full knowledge of the underlying architecture.</p>
<p>Starting from the Hydra Engine, we changed some code to be more Vulkan-focused, and thus the Raptor Engine was born. In the following sections, we will have a brief look at the code architecture to familiarize you with the building blocks that will be used throughout all the chapters.</p>
<p>We will also <a id="_idIndexMarker011"/>look at the glTF data format used to import meshes, textures, and materials into the Raptor Engine.</p>
<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>Layers of code</h2>
<p>The Raptor<a id="_idIndexMarker012"/> Engine is created with a layer-based mentality for code, in which a layer can interact only with lower ones.</p>
<p>This choice was made to simplify communication between layers and simplify the API design and the expected behavior for the final user.</p>
<p>There are three layers in Raptor:</p>
<ul>
<li>Foundation</li>
<li>Graphics</li>
<li>Application</li>
</ul>
<p>The <code>source/raptor</code>.</p>
<p>Each chapter has its own implementation of the <code>source/chapter1/graphics</code>.</p>
<p>While developing the Raptor Engine, we enforced the communication direction based on the layer we were on, so that a layer could interact with the code within the same layer and the bottom layer only.</p>
<p>In this case, the foundation layer can interact only with the other code inside the layer, the graphics layer can interact with the foundation layer, and the application layer interacts with all the layers.</p>
<p>There will be possible situations where we need to have some communication from a bottom layer to an upper layer, and the solution to that is to create code in the upper layer to drive the communication between the lower layers.</p>
<p>For example, the <code>Camera</code> class is defined in the foundation layer, and it is a class that contains all the mathematical code to drive a rendering camera.</p>
<p>What if we need user input to move the camera, say with a mouse or gamepad?</p>
<p>Based on this decision, we created <code>GameCamera</code> in the application layer, which contains the input code, takes the user input, and modifies the camera as needed.</p>
<p>This upper layer bridging will be used in other areas of the code and will be explained when needed.</p>
<p>The following<a id="_idIndexMarker013"/> sections will give you an overview of the main layers and some of their fundamental code so that you will be familiar with all the available building blocks that will be used throughout the book.</p>
<h3>Foundation layer</h3>
<p>The foundation<a id="_idIndexMarker014"/> layer is a set of different classes that behave as fundamental bricks for everything needed in the framework.</p>
<p>The classes are very specialized and cover different types of needs, but they are required to build the rendering code written in this book. They range from data structures to file operations, logging, and string processing.</p>
<p>While similar data structures are provided by the C++ standard library, we have decided to write our own as we only need a subset of functionality in most cases. It also allows us to carefully control and track memory allocations.</p>
<p>We traded some comfort (that is, automatic release of memory on destruction) for more fine-tuned control over memory lifetime and better compile times. These all-important data structures are used for separate needs and will be used heavily in the graphics layer.</p>
<p>We will briefly go over each foundational block to help you get accustomed to them.</p>
<h4>Memory management</h4>
<p>Let’s start<a id="_idIndexMarker015"/> with <code>source/raptor/foundation/memory.hpp</code>).</p>
<p>One key API <a id="_idIndexMarker016"/>decision made here is to have an explicit allocation model, so for any dynamically allocated memory, an allocator will be needed. This is reflected in all classes through the code base.</p>
<p>This foundational brick defines the main allocator API used by the different allocators that can be used throughout the code.</p>
<p>There is <code>HeapAllocator</code>, based on the <code>tlsf</code> allocator, a fixed-size linear allocator, a malloc-based allocator, a fixed-size stack allocator, and a fixed-size double stack allocator.</p>
<p>While we will not cover memory management techniques here, as it is less relevant to the purpose of this book, you can glimpse a more professional memory management mindset in the code base.</p>
<h4>Arrays</h4>
<p>Next, we <a id="_idIndexMarker017"/>will <a id="_idIndexMarker018"/>look at <code>source/raptor/foundation/array.hpp</code>).</p>
<p>Probably the most fundamental data structure of all software engineering, arrays are used to represent contiguous and dynamically allocated data, with an interface similar to the better-known <code>std::vector</code> (<a href="https://en.cppreference.com/w/cpp/container/vector">https://en.cppreference.com/w/cpp/container/vector</a>).</p>
<p>The code is simpler<a id="_idIndexMarker019"/> compared to the <code>std</code>) implementation and requires an explicit allocator to be initialized.</p>
<p>The only notable difference from <code>std::vector</code> can be seen in the methods, such as <code>push_use()</code>, which grows the array and returns the newly allocated element to be filled, and the <code>delete_swap()</code> method, which removes an element and swaps it with the last element.</p>
<h4>Hash maps</h4>
<p><code>source/raptor/foundation/hash_map.hpp</code>) are another fundamental<a id="_idIndexMarker020"/> data structure, as they boost search operation <a id="_idIndexMarker021"/>performance, and they are used extensively in the code base: every time there is the need to quickly find an object based on some simple search criteria (<em class="italic">search the texture by name</em>), then a hash map is the de facto standard data structure.</p>
<p>The sheer volume of information about hash maps is huge and out of the scope of this book, but recently a good all-round implementation of hash maps was documented and shared by Google inside their Abseil library (code available here: <a href="https://github.com/abseil/abseil-cpp">https://github.com/abseil/abseil-cpp</a>).</p>
<p>The Abseil hash map is an evolution of the SwissTable hash map, storing some extra metadata per entry to quickly reject elements, using linear probing to insert elements, and finally, using Single Instruction Multiple Data (SIMD) instructions to quickly test more entries.</p>
<p class="callout-heading">Important note</p>
<p class="callout">For a good overview of the ideas behind the Abseil hash map implementation, there are a couple of nice articles that can be read. They can be found here:</p>
<p class="callout"><strong class="bold">Article </strong><strong class="bold">1</strong>: <a href="https://gankra.github.io/blah/hashbrown-tldr/%0D">https://gankra.github.io/blah/hashbrown-tldr/</a></p>
<p class="callout"><strong class="bold">Article </strong><strong class="bold">2</strong>: <a href="https://blog.waffles.space/2018/12/07/deep-dive-into-hashbrown/">https://blog.waffles.space/2018/12/07/deep-dive-into-hashbrown/</a></p>
<p class="callout"><em class="italic">Article 1</em> is a good overview of the topic and <em class="italic">Article 2</em> goes a little more in-depth about the implementation.</p>
<h4>File operations</h4>
<p>Next, we <a id="_idIndexMarker022"/>will<a id="_idIndexMarker023"/> look at <code>source/raptor/foundation/file.hpp</code>).</p>
<p>Another common set of operations performed in an engine is file handling, for example, to read a texture, a shader, or a text file from the hard drive.</p>
<p>These operations follow a similar pattern to the C file APIs, such as <code>file_open</code> being similar to the <code>fopen</code> function (<a href="https://www.cplusplus.com/reference/cstdio/fopen/">https://www.cplusplus.com/reference/cstdio/fopen/</a>).</p>
<p>In this set of functions, there are also the ones needed to create and delete a folder, or some utilities such as extrapolating the filename or the extension of a path.</p>
<p>For example, to create a texture, you need to first open the texture file in memory, then send it to the <a id="_idIndexMarker024"/>graphics layer to create a Vulkan<a id="_idIndexMarker025"/> representation of it to be properly usable by the GPU.</p>
<h4>Serialization</h4>
<p><code>source/raptor/foundation/blob_serialization.hpp</code>), the<a id="_idIndexMarker026"/> process <a id="_idIndexMarker027"/>of converting human-readable files to a binary counterpart, is also present here.</p>
<p>The topic is vast, and there is not as much information as it deserves, but a good starting point is the article <a href="https://yave.handmade.network/blog/p/2723-how_media_molecule_does_serialization">https://yave.handmade.network/blog/p/2723-how_media_molecule_does_serialization</a>, or <a href="https://jorenjoestar.github.io/post/serialization_for_games">https://jorenjoestar.github.io/post/serialization_for_games</a>.</p>
<p>We will use serialization to process some human-readable files (mostly JSON files) into more custom files as they are needed.</p>
<p>The process is done to speed up loading files, as human-readable formats are great for expressing things and can be modified, but binary files can be created to suit the application’s needs.</p>
<p>This is a fundamental step in any game-related technology, also <a id="_idIndexMarker028"/>called <strong class="bold">asset baking</strong>.</p>
<p>For the purpose of this code, we will use a minimal amount of serialization, but as with memory management, it is a topic to have in mind when designing any performant code.</p>
<h4>Logging</h4>
<p><code>source/raptor/foundation/log.hpp</code>) is the process of writing some<a id="_idIndexMarker029"/> user-defined text to both help understand the flow of <a id="_idIndexMarker030"/>the code and debug the application.</p>
<p>It can be used to write the initialization steps of a system or to report some error with additional information so it can be used by the user.</p>
<p>Provided with the code is a simple logging service, providing the option of adding user-defined callbacks and intercepting any message.</p>
<p>An example of logging usage is the Vulkan debug layer, which will output any warning or error to the logging service when needed, giving the user instantaneous feedback on the application’s behavior.</p>
<h4>String processing</h4>
<p>Next, we<a id="_idIndexMarker031"/> will<a id="_idIndexMarker032"/> look at <code>source/raptor/foundation/string.hpp</code>).</p>
<p>Strings are arrays of characters used to store text. Within the Raptor Engine, the need to have clean control of memory and a simple interface added the need for custom-written string code.</p>
<p>The main class provided is the <code>StringBuffer</code> class, which lets the user allocate a maximum fixed amount of memory, and within that memory, perform typical string operations: concatenation, formatting, and substrings.</p>
<p>A second class provided is the <code>StringArray</code> class, which allows the user to efficiently store and track different strings inside a contiguous chunk of memory.</p>
<p>This is used, for example, when retrieving a list of files and folders. A final utility string class is the <code>StringView</code> class, used for read-only access to a string.</p>
<h4>Time management</h4>
<p>Next <a id="_idIndexMarker033"/>is <code>source/raptor/foundation/time.hpp</code>).</p>
<p>When<a id="_idIndexMarker034"/> developing a custom engine, timing is very important, and having some functions to help calculate different timings is what the time management functions do.</p>
<p>For example, any application needs to calculate a time difference, used to advance time and calculations in various aspects, often <a id="_idIndexMarker035"/>known as <strong class="bold">delta time</strong>.</p>
<p>This will be manually calculated in the application layer, but it uses the time functions to do it. It can be also used to measure CPU performance, for example, to pinpoint slow code or gather statistics when performing some operations.</p>
<p>Timing methods conveniently allow the user to calculate time durations in different units, from<a id="_idIndexMarker036"/> seconds<a id="_idIndexMarker037"/> down to milliseconds.</p>
<h4>Process execution</h4>
<p>One last utility<a id="_idIndexMarker038"/> area is <code>source/raptor/foundation/process.hpp</code>) – defined<a id="_idIndexMarker039"/> as running any external program from within our own code.</p>
<p>In the Raptor Engine, one of the most important usages of external processes is the execution of Vulkan’s shader compiler to convert GLSL shaders to SPIR-V format, as seen at <a href="https://www.khronos.org/registry/SPIR-V/specs/1.0/SPIRV.xhtml">https://www.khronos.org/registry/SPIR-V/specs/1.0/SPIRV.xhtml</a>. The Khronos specification is needed for shaders to be used by Vulkan.</p>
<p>We have been through all the different utilities building blocks (many seemingly unrelated) that cover the basics of a modern rendering engine.</p>
<p>These basics are not graphics related by themselves, but they are required to build a graphical application that gives the final user full control of what is happening and represents a watered-down mindset of what modern game engines do behind the scenes.</p>
<p>Next, we will introduce the graphics layer, where some of the foundational bricks can be seen in action and represent the most important part of the code base developed for this book.</p>
<h3>Graphics layer</h3>
<p>The most<a id="_idIndexMarker040"/> important <a id="_idIndexMarker041"/>architectural layer is the graphics layer, which will be the main focus of this book. Graphics will include all the Vulkan-related code and abstractions needed to draw anything on the screen using the GPU.</p>
<p>There is a caveat in the organization of the source code: having the book divided into different chapters and having one GitHub repository, there was the need to have a snapshot of the graphics code for each chapter; thus, graphics code will be duplicated and evolved in each chapter’s code throughout the game.</p>
<p>We expect the code to grow in this folder as this book progresses after each chapter, and not only here, as we will develop shaders and use other data resources as well, but it is fundamental to know where we are starting from or where we were at a specific time in the book.</p>
<p>Once again, the API design comes from Hydra as follows:</p>
<ul>
<li>Graphics resources are created using a <code>creation</code> struct containing all the necessary parameters</li>
<li>Resources are externally passed as handles, so they are easily copiable and safe to pass around</li>
</ul>
<p>The main class in this layer is the <code>GpuDevice</code> class, which is responsible for the following:</p>
<ul>
<li>Vulkan API abstractions and usage</li>
<li>Creation, destruction, and update of graphics resources</li>
<li>Swapchain creation, destruction, resize, and update</li>
<li>Command buffer requests and submission to the GPU</li>
<li>GPU timestamps management</li>
<li>GPU-CPU synchronization</li>
</ul>
<p>We define graphics resources as anything residing on the GPU, such as the following:</p>
<ul>
<li><strong class="bold">Textures</strong>: Images to read and write from</li>
<li><strong class="bold">Buffers</strong>: Arrays of homogeneous or heterogeneous data</li>
<li><strong class="bold">Samplers</strong>: Converters from raw GPU memory to anything needed from the shaders</li>
<li><strong class="bold">Shaders</strong>: SPIR-V compiled GPU executable code</li>
<li><strong class="bold">Pipeline</strong>: An almost complete snapshot of GPU state</li>
</ul>
<p>The usage of graphics resources is the core of any type of rendering algorithm.</p>
<p>Therefore, <code>GpuDevice</code> (<code>source/chapter1/graphics/gpu_device.hpp</code>) is the gateway to creating rendering algorithms.</p>
<p>Here is a snippet of the <code>GpuDevice</code> interface for resources:</p>
<pre class="source-code">
struct GpuDevice {
  BufferHandle create_buffer( const BufferCreation&amp; bc );
  TextureHandle create_texture( const TextureCreation&amp; tc
  );
  ...
  void destroy_buffer( BufferHandle&amp; handle );
  void destroy_texture( TextureHandle&amp; handle );</pre>
<p>Here is an <a id="_idIndexMarker042"/>example <a id="_idIndexMarker043"/>of the creation and destruction to create <code>VertexBuffer</code>, taken from the Raptor <code>ImGUI</code> (<code>source/chapter1/graphics/raptor_imgui.hpp</code>) backend:</p>
<pre class="source-code">
GpuDevice gpu;
// Create the main ImGUI vertex buffer
BufferCreation bc;
bc.set( VK_BUFFER_USAGE_VERTEX_BUFFER_BIT,
  ResourceUsageType::Dynamic, 65536 )
  .set_name( "VB_ImGui" );
BufferHandle imgui_vb = gpu.create(bc);
…
// Destroy the main ImGUI vertex buffer
gpu.destroy(imgui_vb);</pre>
<p>In the Raptor Engine, graphics resources (<code>source/chapter1/graphics/gpu_resources.hpp</code>) have the same granularity as Vulkan but are enhanced to help the user write simpler and safer code.</p>
<p>Let’s have <a id="_idIndexMarker044"/>a look at the <code>Buffer</code> class:</p>
<pre class="source-code">
struct Buffer {
    VkBuffer                        vk_buffer;
    VmaAllocation                   vma_allocation;
    VkDeviceMemory                  vk_device_memory;
    VkDeviceSize                    vk_device_size;
    VkBufferUsageFlags              type_flags      = 0;
    u32                             size            = 0;
    u32                             global_offset   = 0;
    BufferHandle                    handle;
    BufferHandle                    parent_buffer;
    const char* name                = nullptr;
}; // struct Buffer</pre>
<p>As we can see, the <code>Buffer</code> struct <a id="_idIndexMarker045"/>contains quite a few extra pieces of information.</p>
<p>First of all, <code>VkBuffer</code> is the main Vulkan struct used by the API. Then there are some members related to memory allocations on the GPU, such as device memory and size.</p>
<p>Note that there is a utility class used in the Raptor Engine called <strong class="bold">Virtual Memory Allocator</strong> (<strong class="bold">VMA</strong>) (<a href="https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator">https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator</a>), which<a id="_idIndexMarker046"/> is the de facto standard utility library to write Vulkan code.</p>
<p>Here, it is reflected in the <code>vma_allocation</code> member variable.</p>
<p>Furthermore, there are usage flags – size and offset, as well as global offsets – current buffer handle and parent handle (we will see their usage later in the book), as well as a human-readable string for easier debugging. This <code>Buffer</code> can be seen as the blueprint of how other abstractions are created in the Raptor Engine, and how they help the user to write simpler and safer code.</p>
<p>They still respect Vulkan’s design and philosophy but can hide some implementation details that can be less important once the focus of the user is exploring rendering algorithms.</p>
<p>We had a brief overview of the graphics layer, the most important part of the code in this book. We will evolve its code after each chapter, and we will dwell deeper on design choices and <a id="_idIndexMarker047"/>implementation <a id="_idIndexMarker048"/>details throughout the book.</p>
<p>Next, there is the application layer, which works as the final step between the user and the application.</p>
<h3>The application layer</h3>
<p>The application<a id="_idIndexMarker049"/> layer is responsible for handling the actual application <a id="_idIndexMarker050"/>side of the engine – from window creation and update based on the operating system to gathering user input from the mouse and keyboard.</p>
<p>In the layer is also included a <a id="_idIndexMarker051"/>very handy backend for ImGui (<a href="https://github.com/ocornut/imgui">https://github.com/ocornut/imgui</a>), an amazing library to design the UI to enhance user interaction with the application so that it is much easier to control its behavior.</p>
<p>There is an application class that will be the blueprint for any demo application that will be created in the book so that the user can focus more on the graphics side of the application.</p>
<p>The foundation and application layers’ code is in the <code>source/raptor</code> folder. This code will be almost constant throughout the book, but as we are writing mainly a graphics system, this is put in a shared folder between all the chapters.</p>
<p>In this section, we have explained the structure of the code and presented the three main layers of the Raptor Engine: foundation, graphics, and application. For each of these layers, we highlighted some of the main classes, how to use them, and the reasoning and inspiration behind the choices we have made.</p>
<p>In the next <a id="_idIndexMarker052"/>section, we<a id="_idIndexMarker053"/> are going to present the file format we selected to load 3D data from and how we have integrated it into the engine.</p>
<h1 id="_idParaDest-26"><a id="_idTextAnchor025"/>Understanding the glTF scene format</h1>
<p>Many 3D file formats <a id="_idIndexMarker054"/>have been developed over the years, and for this book, we chose to use glTF. It has become increasingly popular in recent years; it has an open specification, and it<a id="_idIndexMarker055"/> supports a <strong class="bold">physically based rendering</strong> (<strong class="bold">PBR</strong>) model by default.</p>
<p>We chose this format because of its open specification and easy-to-understand structure. We can use several models provided by Khronos on GitHub to test our implementation and compare our results with other frameworks.</p>
<p>It is a JSON-based format and we built a custom parser for this book. The JSON data will be deserialized into a C++ class, which we are going to use to drive the rendering.</p>
<p>We now provide an overview of the main sections of the glTF format. At its root, we have a list of scenes, and each scene can have multiple nodes. You can see this in the following code:</p>
<pre class="source-code">
"scene": 0,
"scenes": [
    {
        "nodes": [
            0,
            1,
            2,
            3,
            4,
            5
        ]
    }
],</pre>
<p>Each node contains an index that is present in the <code>mesh</code> array:</p>
<pre class="source-code">
"nodes": [
    {
        "mesh": 0,
        "name": "Hose_low"
    },
]</pre>
<p>The data for the<a id="_idIndexMarker056"/> scene is stored in one or more buffers, and each section of the buffer is described by a buffer view:</p>
<pre class="source-code">
"buffers": [
    {
        "uri": "FlightHelmet.bin",
        "byteLength": 3227148
    }
],
"bufferViews": [
    {
        "buffer": 0,
        "byteLength": 568332,
        "name": "bufferViewScalar"
    },
]</pre>
<p>Each buffer view references the buffer that contains the actual data and its size. An accessor points into a buffer view by defining the type, offset, and size of the data:</p>
<pre class="source-code">
"accessors": [
    {
        "bufferView": 1,
        "byteOffset": 125664,
        "componentType": 5126,
        "count": 10472,
        "type": "VEC3",
        "name": "accessorNormals"
    }
]</pre>
<p>The <code>mesh</code> array <a id="_idIndexMarker057"/>contains a list of entries, and each entry is composed of one or more mesh primitives. A mesh primitive contains a list of attributes that point into the accessors array, the index of the indices accessor, and the index of the material:</p>
<pre class="source-code">
"meshes": [
    {
        "primitives": [
            {
                "attributes": {
                    "POSITION": 1,
                    "TANGENT": 2,
                    "NORMAL": 3,
                    "TEXCOORD_0": 4
                },
                "indices": 0,
                "material": 0
            }
        ],
        "name": "Hose_low"
    }
]</pre>
<p>The <code>materials</code> object defines which textures are used (diffuse color, normal map, roughness, and so on) and <a id="_idIndexMarker058"/>other parameters that control the rendering of the material:</p>
<pre class="source-code">
"materials": [
    {
        "pbrMetallicRoughness": {
            "baseColorTexture": {
                "index": 2
            },
            "metallicRoughnessTexture": {
                "index": 1
            }
        },
        "normalTexture": {
            "index": 0
        },
        "occlusionTexture": {
            "index": 1
        },
        "doubleSided": true,
        "name": "HoseMat"
    }
]</pre>
<p>Each texture is <a id="_idIndexMarker059"/>specified as a combination of an image and a sampler:</p>
<pre class="source-code">
"textures": [
    {
        "sampler": 0,
        "source": 0,
        "name": "FlightHelmet_Materials_RubberWoodMat_Nor
                 mal.png"
    },
],
"images": [
    {
        "uri": "FlightHelmet_Materials_RubberWoodMat_Nor
                mal.png"
    },
],
"samplers": [
    {
        "magFilter": 9729,
        "minFilter": 9987
    }
]</pre>
<p>The glTF format can specify many other details, including animation data and cameras. Most of the models that we are using in this book don’t make use of these features, but we will highlight them when that’s the case.</p>
<p>The JSON data is deserialized into a C++ class, which is then used for rendering. We omitted glTF extensions in the resulting object as they are not used in this book. We are now going through a code example that shows how to read a glTF file using our parser. The first step is to<a id="_idIndexMarker060"/> load the file into a <code>glTF</code> object:</p>
<pre class="source-code">
char gltf_file[512]{ };
memcpy( gltf_file, argv[ 1 ], strlen( argv[ 1] ) );
file_name_from_path( gltf_file );
glTF::glTF scene = gltf_load_file( gltf_file );</pre>
<p>We now have a glTF model loaded into the <code>scene</code> variable.</p>
<p>The next step is to upload the buffers, textures, and samplers that are part of our model to the GPU for rendering. We start by processing textures and samplers:</p>
<pre class="source-code">
Array&lt;TextureResource&gt; images;
images.init( allocator, scene.images_count );
for ( u32 image_index = 0; image_index
  &lt; scene.images_count; ++image_index ) {
    glTF::Image&amp; image = scene.images[ image_index ];
    TextureResource* tr = renderer.create_texture(
        image.uri.data, image.uri.data );
    images.push( *tr );
}
Array&lt;SamplerResource&gt; samplers;
samplers.init( allocator, scene.samplers_count );
for ( u32 sampler_index = 0; sampler_index
  &lt; scene.samplers_count; ++sampler_index ) {
  glTF::Sampler&amp; sampler = scene.samplers[ sampler_index ];
  SamplerCreation creation;
  creation.min_filter = sampler.min_filter == glTF::
      Sampler::Filter::LINEAR ? VK_FILTER_LINEAR :
          VK_FILTER_NEAREST;
  creation.mag_filter = sampler.mag_filter == glTF::
      Sampler::Filter::LINEAR ? VK_FILTER_LINEAR :
          VK_FILTER_NEAREST;
  SamplerResource* sr = renderer.create_sampler( creation
  );
  samplers.push( *sr );
}</pre>
<p>Each resource is stored in an array. We go through each entry in the array and create the corresponding GPU resource. We then store the resources we just created in a separate array that will be used in the rendering loop.</p>
<p>Now let’s see<a id="_idIndexMarker061"/> how we process the buffers and buffer views, as follows:</p>
<pre class="source-code">
Array&lt;void*&gt; buffers_data;
buffers_data.init( allocator, scene.buffers_count );
for ( u32 buffer_index = 0; buffer_index
  &lt; scene.buffers_count; ++buffer_index ) {
    glTF::Buffer&amp; buffer = scene.buffers[ buffer_index ];
    FileReadResult buffer_data = file_read_binary(
        buffer.uri.data, allocator );
    buffers_data.push( buffer_data.data );
}
Array&lt;BufferResource&gt; buffers;
buffers.init( allocator, scene.buffer_views_count );
for ( u32 buffer_index = 0; buffer_index
  &lt; scene.buffer_views_count; ++buffer_index ) {
    glTF::BufferView&amp; buffer = scene.buffer_views[
        buffer_index ];
    u8* data = ( u8* )buffers_data[ buffer.buffer ] +
        buffer.byte_offset;
    VkBufferUsageFlags flags =
        VK_BUFFER_USAGE_VERTEX_BUFFER_BIT |
            VK_BUFFER_USAGE_INDEX_BUFFER_BIT;
    BufferResource* br = renderer.create_buffer( flags,
        ResourceUsageType::Immutable, buffer.byte_length,
            data, buffer.name.data );
    buffers.push( *br );
}</pre>
<p>First, we read the<a id="_idIndexMarker062"/> full buffer data into CPU memory. Then, we iterate through each buffer view and create its corresponding GPU resource. We store the newly created resource in an array that will be used in the rendering loop.</p>
<p>Finally, we read the mesh definition to create its corresponding draw data. The following code provides a sample for reading the position buffer. Please refer to the code in <code>chapter1/main.cpp</code> for the full implementation:</p>
<pre class="source-code">
for ( u32 mesh_index = 0; mesh_index &lt; scene.meshes_count;
  ++mesh_index ) {
    glTF::Mesh&amp; mesh = scene.meshes[ mesh_index ];
    glTF::MeshPrimitive&amp; mesh_primitive = mesh.primitives[
        0 ];
    glTF::Accessor&amp; position_accessor = scene.accessors[
        gltf_get_attribute_accessor_index(
        mesh_primitive.attributes, mesh_primitive.
        attribute_count, "POSITION" ) ];
    glTF::BufferView&amp; position_buffer_view =
        scene.buffer_views[ position_accessor.buffer_view
        ];
    BufferResource&amp; position_buffer_gpu = buffers[
        position_accessor.buffer_view ];
    MeshDraw mesh_draw{ };
    mesh_draw.position_buffer = position_buffer_gpu.handle;
    mesh_draw.position_offset = position_accessor.
                                byte_offset;
}</pre>
<p>We have grouped all the GPU resources needed to render a mesh into a <code>MeshDraw</code> data structure. We <a id="_idIndexMarker063"/>retrieve the buffers and textures as defined by the <code>Accessor</code> object and store them in a <code>MeshDraw</code> object to be used in the rendering loop.</p>
<p>In this chapter, we load a model at the beginning of the application, and it’s not going to change. Thanks to this constraint, we can create all of our descriptor sets only once before we start rendering:</p>
<pre class="source-code">
DescriptorSetCreation rl_creation{};
rl_creation.set_layout( cube_rll ).buffer( cube_cb, 0 );
rl_creation.texture_sampler( diffuse_texture_gpu.handle,
    diffuse_sampler_gpu.handle, 1 );
rl_creation.texture_sampler( roughness_texture_gpu.handle,
    roughness_sampler_gpu.handle, 2 );
rl_creation.texture_sampler( normal_texture_gpu.handle,
    normal_sampler_gpu.handle, 3 );
rl_creation.texture_sampler( occlusion_texture_gpu.handle,
    occlusion_sampler_gpu.handle, 4 );
 mesh_draw.descriptor_set = gpu.create_descriptor_set(
     rl_creation );</pre>
<p>For each resource type, we call the relative method on the <code>DescriptorSetCreation</code> object. This object stores the data that is going to be used to create the descriptor set through the Vulkan API.</p>
<p>We have now <a id="_idIndexMarker064"/>defined all the objects we need for rendering. In our render loop, we simply have to iterate over all meshes, bind each mesh buffer and descriptor set, and call <code>draw</code>:</p>
<pre class="source-code">
for ( u32 mesh_index = 0; mesh_index &lt; mesh_draws.size;
  ++mesh_index ) {
    MeshDraw mesh_draw = mesh_draws[ mesh_index ];
    gpu_commands-&gt;bind_vertex_buffer( sort_key++,
        mesh_draw.position_buffer, 0,
            mesh_draw.position_offset );
    gpu_commands-&gt;bind_vertex_buffer( sort_key++,
        mesh_draw.tangent_buffer, 1,
            mesh_draw.tangent_offset );
    gpu_commands-&gt;bind_vertex_buffer( sort_key++,
        mesh_draw.normal_buffer, 2,
            mesh_draw.normal_offset );
    gpu_commands-&gt;bind_vertex_buffer( sort_key++,
        mesh_draw.texcoord_buffer, 3,
            mesh_draw.texcoord_offset );
    gpu_commands-&gt;bind_index_buffer( sort_key++,
        mesh_draw.index_buffer, mesh_draw.index_offset );
    gpu_commands-&gt;bind_descriptor_set( sort_key++,
        &amp;mesh_draw.descriptor_set, 1, nullptr, 0 );
    gpu_commands-&gt;draw_indexed( sort_key++,
        TopologyType::Triangle, mesh_draw.count, 1, 0, 0,
            0 );
}</pre>
<p>We are going to evolve this code over the course of the book, but it’s already a great starting point for you to try and load a different model or to experiment with the shader code (more on this<a id="_idIndexMarker065"/> in the next section).</p>
<p>There are several tutorials online about the glTF format, some of which are linked in the <em class="italic">Further reading</em> section. The glTF spec is also a great source of details and is easy to follow. We recommend you refer to it if something about the format is not immediately clear from reading the book or the code.</p>
<p>In this section, we have analyzed the glTF format and we have presented examples of the JSON objects most relevant to our renderer. We then demonstrated how to use the glTF parser, which we added to our framework, and showed you how to upload geometry and texture data to the GPU. Finally, we have shown how to use this data to draw the meshes that make up a model.</p>
<p>In the next section, we explain how the data we just parsed and uploaded to the GPU is used to render our model using a physically-based rendering implementation.</p>
<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>PBR in a nutshell</h1>
<p>PBR is at the heart <a id="_idIndexMarker066"/>of many rendering engines. It was originally developed for offline rendering, but thanks to the advances in hardware capabilities and research efforts by the graphics community, it can now be used for real-time rendering as well.</p>
<p>As the name implies, this technique aims at modeling the physical interactions of light and matter and, in some implementations, ensuring that the amount of energy in the system is preserved.</p>
<p>There are plenty of in-depth resources available that describe PBR in great detail. Nonetheless, we want to give a brief overview of our implementation for reference. We have followed the implementation presented in the glTF spec.</p>
<p>To compute the final color of our surface, we have to determine the diffuse and specular components. The amount of specular reflection in the real world is determined by the roughness of the surface. The smoother the surface, the greater the amount of light that is reflected. A mirror reflects (almost) all the light it receives.</p>
<p>The roughness of the surface is modeled through a texture. In the glTF format, this value is packed with the metalness and the occlusion values in a single texture to optimize resource use. We distinguish materials between conductors (or metallic) and dielectric (non-metallic) surfaces.</p>
<p>A metallic material has only a specular term, while a non-metallic material has both diffuse and specular terms. To model materials that have both metallic and non-metallic components, we use the metalness term to interpolate between the two.</p>
<p>An object made of wood will likely have a metalness of 0, plastic will have a mix of both metalness and roughness, and the body of a car will be dominated by the metallic component.</p>
<p>As we are modeling the real-world response of a material, we need a function that takes the view and light direction and returns the amount of light that is reflected. This function is <a id="_idIndexMarker067"/>called the <strong class="bold">bi-directional distribution </strong><strong class="bold">function</strong> (<strong class="bold">BRDF</strong>).</p>
<p>We use the Trowbridge-Reitz/GGX distribution for the specular BRDF, and it is implemented as follows:</p>
<pre class="source-code">
float NdotH = dot(N, H);
float alpha_squared = alpha * alpha;
float d_denom = ( NdotH * NdotH ) * ( alpha_squared - 1.0 )
    + 1.0;
float distribution = ( alpha_squared * heaviside( NdotH ) )
    / ( PI * d_denom * d_denom );
float NdotL = dot(N, L);
float NdotV = dot(N, V);
float HdotL = dot(H, L);
float HdotV = dot(H, V);
float visibility = ( heaviside( HdotL ) / ( abs( NdotL ) +
  sqrt( alpha_squared + ( 1.0 - alpha_squared ) *
  ( NdotL * NdotL ) ) ) ) * ( heaviside( HdotV ) /
  ( abs( NdotV ) + sqrt( alpha_squared +
  ( 1.0 - alpha_squared ) *
  ( NdotV * NdotV ) ) ) );
float specular_brdf = visibility * distribution;</pre>
<p>First, we compute the <a id="_idIndexMarker068"/>distribution and visibility terms according to the formula presented in the glTF specification. Then, we multiply them to obtain the specular BRDF term.</p>
<p>There are other approximations that can be used, and we encourage you to experiment and replace ours with a different one!</p>
<p>We then compute the diffuse BDRF, as follows:</p>
<pre class="source-code">
vec3 diffuse_brdf = (1 / PI) * base_colour.rgb;</pre>
<p>We now introduce the Fresnel term. This determines the color of the reflection based on the viewing angle and the index of refraction of the material. Here is the implementation of the Schlick approximation, both for the metallic and dielectric components:</p>
<pre class="source-code">
// f0 in the formula notation refers to the base colour
   here
vec3 conductor_fresnel = specular_brdf * ( base_colour.rgb
  + ( 1.0 - base_colour.rgb ) * pow( 1.0 - abs( HdotV ),
      5 ) );
// f0 in the formula notation refers to the value derived
   from ior = 1.5
float f0 = 0.04; // pow( ( 1 - ior ) / ( 1 + ior ), 2 )
float fr = f0 + ( 1 - f0 ) * pow(1 - abs( HdotV ), 5 );
vec3 fresnel_mix = mix( diffuse_brdf, vec3(
                        specular_brdf ), fr );</pre>
<p>Here we compute<a id="_idIndexMarker069"/> the Fresnel term for both the conductor and the dielectric components according to the formula in the glTF specification.</p>
<p>Now that we have computed all the components of the model, we interpolate between them, based on the metalness of the material, as follows:</p>
<pre class="source-code">
vec3 material_colour = mix( resnel_mix,
                            conductor_fresnel, metalness );</pre>
<p>The occlusion term is not used as it only affects indirect light, which we haven’t implemented yet.</p>
<p>We realize this is a very quick introduction, and we skipped over a lot of the theory that makes these approximations work. However, it should provide a good starting point for further study.</p>
<p>We have added links to some excellent resources in the <em class="italic">Further reading</em> section if you’d like to experiment and modify our base implementation.</p>
<p>In the next section, we are going to introduce a debugging tool that we rely on whenever we have a <a id="_idIndexMarker070"/>non-trivial rendering issue. It has helped us many times while writing this book!</p>
<h1 id="_idParaDest-28"><a id="_idTextAnchor027"/>A word on GPU debugging</h1>
<p>No matter how<a id="_idIndexMarker071"/> much experience you have in graphics programming, there will come a time when you need to debug an issue. Understanding exactly what the GPU is doing when it executes your program is not as immediate as on the CPU. Thankfully, GPU debugging tools have come a long way to help us when our program doesn’t behave as expected.</p>
<p>GPU vendors provide great tools to debug and profile your shaders: Nvidia has developed Nsight graphics, and AMD has a suite of tools that includes the Radeon GPU analyzer and Radeon GPU profiler.</p>
<p>For this book, we have primarily used RenderDoc (available at <a href="https://renderdoc.org/">https://renderdoc.org/</a>). It is a staple tool of the graphics programming community as it allows you to capture a frame and record all the Vulkan API calls that have been issued during that frame.</p>
<p>Using RenderDoc is really simple. You start by providing the path to your application, as follows:</p>
<div><div><img alt="Figure 1.2 – Setting the application path in RenderDoc" height="224" src="img/B18395_01_02.jpg" width="888"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Setting the application path in RenderDoc</p>
<p>You then start the application by clicking <strong class="bold">Launch</strong>, and you will notice an overlay reporting the frame time and the number of frames rendered. If you press <em class="italic">F12</em>, RenderDoc will record the current frame. You can now close your application, and the recorded frame will automatically load.</p>
<p>On the left, you have the list of API calls grouped in render passes. This view also lists the <strong class="bold">event ID</strong> (<strong class="bold">EID</strong>), which<a id="_idIndexMarker072"/> is a progressive number defined by RenderDoc. This is useful for comparing events across multiple frames:</p>
<div><div><img alt="Figure 1.3 – The list of Vulkan API calls for the captured frame" height="551" src="img/B18395_01_03.jpg" width="795"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – The list of Vulkan API calls for the captured frame</p>
<p>On the right side of the <a id="_idIndexMarker073"/>application window, you have multiple tabs that allow you to inspect which textures are bound when a draw call is made, the buffer content, and the state of the pipeline.</p>
<p>The following figure shows the <strong class="bold">Texture Viewer</strong> tab. It shows the rendering output after a given draw and which input textures were bound:</p>
<div><div><img alt="Figure 1.4 – RenderDoc texture viewer" height="1044" src="img/B18395_01_04.jpg" width="1303"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – RenderDoc texture viewer</p>
<p>If you right-click on a<a id="_idIndexMarker074"/> pixel in the <strong class="bold">Texture Viewer</strong> tab, you can inspect the history of that pixel to understand which draws affected it.</p>
<p>There is also a debug feature that allows you to step through the shader code and analyze intermediate values. Be careful when using this feature, as we have noticed that the values are not always accurate.</p>
<p>This was a quick overview of RenderDoc and its functionality. You have learned how to capture a frame in RenderDoc when running a graphics application. We presented a breakdown of the main panels, their functionality, and how to use them to understand how the final image is rendered.</p>
<p>We encourage you<a id="_idIndexMarker075"/> to run the code from this chapter under RenderDoc to better understand how the frame is built.</p>
<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Summary</h1>
<p>In this chapter, we laid the foundations for the rest of the book. By now, you should be familiar with how the code is structured and how to use it. We introduced the Raptor Engine, and we have provided an overview of the main classes and libraries that are going to be used throughout the book.</p>
<p>We have presented the glTF format of the 3D models and how we parse this format into objects that will be used for rendering. We gave a brief introduction to PBR modeling and our implementation of it. Finally, we introduced RenderDoc and how it can be used to debug rendering issues or to understand how a frame is built.</p>
<p>In the next chapter, we are going to look at how to improve our resource management!</p>
<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Further reading</h1>
<p>We have only skimmed the surface of the topics we have presented. Here, we provide links to resources you can use to get more information on the concepts exposed in this chapter, which will be useful throughout the book.</p>
<p>While we have written our own standard library replacement, there are other options if you are starting your own project. We highly recommend looking into <a href="https://github.com/electronicarts/EASTL">https://github.com/electronicarts/EASTL</a>, developed by EA.</p>
<ul>
<li><strong class="bold">The Vulkan </strong><strong class="bold">specification</strong>: <a href="https://www.khronos.org/registry/vulkan/specs/1.3-extensions/html/vkspec.xhtml%0D">https://www.khronos.org/registry/vulkan/specs/1.3-extensions/html/vkspec.xhtml</a></li>
<li><strong class="bold">The </strong><strong class="bold">glTF format</strong>:<ul><li><a href="https://www.khronos.org/registry/glTF/specs/2.0/glTF-2.0.xhtml">https://www.khronos.org/registry/glTF/specs/2.0/glTF-2.0.xhtml</a></li><li><a href="https://github.com/KhronosGroup/glTF-Sample-Viewer%0D">https://github.com/KhronosGroup/glTF-Sample-Viewer</a></li></ul></li>
<li><strong class="bold">glTF libraries</strong>: We have written our own parser for educational purposes. If you are starting your own project, we suggest evaluating these libraries:<ul><li><a href="https://github.com/jkuhlmann/cgltf">https://github.com/jkuhlmann/cgltf</a></li><li><a href="https://github.com/code4game/libgltf">https://github.com/code4game/libgltf</a></li><li><a href="https://github.com/syoyo/tinygltfloader">https://github.com/syoyo/tinygltfloader</a></li></ul></li>
<li><strong class="bold">Resources </strong><strong class="bold">on PBR</strong>:<ul><li><a href="https://google.github.io/filament/Filament.xhtml">https://google.github.io/filament/Filament.xhtml</a></li><li><a href="https://blog.selfshadow.com/publications/s2012-shading-course/">https://blog.selfshadow.com/publications/s2012-shading-course/</a></li><li><a href="https://pbr-book.org/">https://pbr-book.org/</a></li></ul></li>
</ul>
</div>
</div></body></html>