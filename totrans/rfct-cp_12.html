<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-226"><a id="_idTextAnchor225"/>12</h1>
<h1 id="_idParaDest-227"><a id="_idTextAnchor226"/>Testing</h1>
<p>Software testing stands as a cornerstone in the edifice of software development, holding paramount importance in the assurance of software quality, reliability, and maintainability. It is through the meticulous process of testing that developers can ensure their creations meet the highest standards of functionality and user satisfaction. The inception of any software project is invariably intertwined with the potential for bugs and unforeseen issues; it is testing that illuminates these hidden pitfalls, allowing developers to address them proactively, thereby enhancing the overall integrity and performance of the software.</p>
<p>At the heart of software testing lies a diverse array of methodologies, each tailored to examine distinct facets of the software. Among these, unit testing serves as the foundational layer, focusing on the smallest testable parts of the software to ensure their correct behavior. This granular approach facilitates the early detection of errors, streamlining the development process by enabling immediate corrections. Ascending from the micro to the macro perspective, integration testing takes precedence, wherein the interaction between integrated units is scrutinized. This method is pivotal in identifying issues in the interfacing of components, ensuring seamless communication and functionality within the software.</p>
<p>Progressing further, system testing emerges as a comprehensive examination of the complete and integrated software system. This methodology delves into the software’s adherence to specified requirements, offering an overarching assessment of its behavior and performance. It is a crucial phase that validates the software’s readiness for deployment, ensuring that it functions correctly in its intended environment. Lastly, acceptance testing marks the culmination of the testing process, where the software is evaluated to determine whether it fulfills the criteria for delivery to end users. This final stage is instrumental in affirming the software’s alignment with user needs and expectations, serving as the ultimate testament to its quality and effectiveness.</p>
<p>Embarking on this chapter, you will be guided through the intricate landscape of software testing, gaining insights into the pivotal role it plays in the development life cycle. The exploration will encompass the nuanced distinctions between testing methodologies, shedding light on their unique objectives and the scope of their application. Through this journey, you will acquire a comprehensive understanding of how testing underpins the creation of robust, reliable, and user-centric software, setting the stage for the subsequent chapters that delve deeper into the specifics of unit testing and beyond in the realm of C++.</p>
<h1 id="_idParaDest-228"><a id="_idTextAnchor227"/>Test-driven development</h1>
<p><strong class="bold">Test-driven development</strong>, commonly abbreviated as <strong class="bold">TDD</strong>, is a modern software development approach that <a id="_idIndexMarker658"/>has revolutionized the way code is written and tested. At its core, TDD inverts traditional development methodologies by advocating for the creation of tests before the development of the actual functional code. This paradigm shift is encapsulated in a cyclic process known as “Red-Green-Refactor.” Initially, a developer writes a test that defines a desired improvement or a new function, which inevitably fails on the first run – this is the “Red” phase, indicating the absence of the corresponding functionality. Subsequently, in the “Green” phase, the developer crafts the minimum amount of code necessary to pass the test, thereby ensuring that the functionality meets the specified requirements. The cycle culminates in the “Refactor” phase, where the new code is refined and optimized without altering its behavior, thus maintaining the test’s successful outcome.</p>
<p>The adoption of TDD brings with it a plethora of advantages that contribute to a more robust and reliable code base. One of the most significant benefits is the marked improvement in code quality. Since TDD necessitates the definition of tests upfront, it inherently encourages a more thoughtful and deliberate design process, reducing the likelihood of bugs and errors. Moreover, tests crafted in the TDD process serve a dual purpose as detailed documentation of the code base. These tests provide clear insights into the code’s intended functionality and usage, offering valuable guidance for current and future developers. Additionally, TDD facilitates the design and refactoring of code by ensuring that changes do not inadvertently break existing functionalities, thereby fostering a code base that is both flexible and maintainable.</p>
<p>Despite its numerous benefits, TDD is not without its challenges and potential drawbacks. One of the initial hurdles encountered when adopting TDD is the perceived slowdown in the development process. Writing tests before functionality can feel counterintuitive and may extend the time to deliver features, particularly in the early stages of adoption. Furthermore, TDD demands a steep learning curve, requiring developers to acquire new skills and adapt to a different mindset, which can be a significant investment in time and resources. It’s also worth noting that TDD may not be universally applicable or ideal for all scenarios. Certain types of projects, such as those involving complex user interfaces or requiring extensive interaction with external systems, may pose challenges to the TDD methodology, necessitating a more nuanced or hybrid approach to testing.</p>
<p>In conclusion, while TDD presents a transformative approach to software development with its emphasis on test-first methodology, it is essential to weigh its benefits against the potential challenges. The effectiveness of TDD is contingent upon the context of its application, the proficiency of the development team, and the nature of the project at hand. As we delve deeper into the subsequent sections, the nuances of unit testing, integration with testing frameworks, and practical considerations will further illuminate the role of TDD in shaping high-quality, maintainable C++ code bases.</p>
<h1 id="_idParaDest-229"><a id="_idTextAnchor228"/>Unit testing in C++</h1>
<p>Unit tests are <a id="_idIndexMarker659"/>a foundational aspect of TDD in software engineering, playing a pivotal role in the C++ development process. They focus on validating the smallest sections of code, known as units, which are typically individual functions, methods, or classes. By testing these components in isolation, unit tests ensure that each part of the software behaves as intended, which is crucial for the system’s overall functionality.</p>
<p>In the TDD framework, unit tests take on an even more significant role. They are often written before the actual code, guiding the development process and ensuring that the software is designed with testability and correctness in mind from the outset. This approach to writing unit tests before the implementation helps in identifying bugs early in the development cycle, allowing for timely corrections that prevent the bugs from becoming more complex or affecting other parts of the system. This proactive bug detection not only saves time and resources but also contributes to the software’s stability.</p>
<p>Moreover, unit tests act as a safety net for developers, enabling them to refactor code confidently without fear of breaking existing functionality. This is particularly valuable in TDD, where refactoring is a key step in the cycle of writing a test, making it pass, and then improving the code. Beyond their role in bug detection and facilitating refactoring, unit tests also serve as effective documentation, providing clear insights into the expected behavior of the system. This makes them an invaluable resource for developers, especially those new to the code base. Additionally, the process of writing unit tests in the TDD approach often highlights design improvements, leading to more robust and maintainable code.</p>
<h1 id="_idParaDest-230"><a id="_idTextAnchor229"/>C++ unit testing frameworks</h1>
<p>The C++ ecosystem is rich with unit testing frameworks designed to facilitate the creation, execution, and maintenance of tests. Among these, Google Test and Google Mock stand out for their comprehensive feature set, ease of use, and integration capabilities with C++ projects. In this section, we’ll delve into Google Test and Google Mock, highlighting their key features and syntax, and demonstrate how they can be integrated into a CMake project.</p>
<h1 id="_idParaDest-231"><a id="_idTextAnchor230"/>Google Test and Google Mock</h1>
<p><code>EXPECT_EQ</code> and <code>ASSERT_NE</code> to compare expected outcomes with actual results, ensuring precise validation of test conditions. Furthermore, Google Test simplifies <a id="_idIndexMarker662"/>the management of common test configurations through test fixtures, which define setup and teardown operations, providing a consistent environment for each test.</p>
<p>Another significant feature is the support for parameterized tests, allowing developers to write a single test and run it with multiple inputs. This approach greatly enhances test coverage without the need for duplicative code. Complementing this, Google Test also supports type-parameterized tests, which permit the execution of the same test logic across different data types, broadening the scope of test coverage even further.</p>
<p>One of the most user-friendly features of Google Test is its automatic test discovery mechanism. This feature eliminates the need for manual test registration, as Google Test automatically identifies and executes tests within the project, streamlining the testing process and saving valuable development time.</p>
<p><strong class="bold">Google Mock</strong>, also known as <strong class="bold">gMock</strong>, complements Google Test by providing a robust mocking framework, which<a id="_idIndexMarker663"/> integrates seamlessly to simulate complex object <a id="_idIndexMarker664"/>behaviors. This capability is invaluable in creating conditions that mimic real-world scenarios, allowing for more thorough testing of code interactions. With Google Mock, developers gain the flexibility to set expectations on mocked objects, tailoring them to specific needs such as the number of times a function is called, the arguments it receives, and the sequence of calls. This level of control ensures that tests can verify not just the outcomes but also the interactions between different parts of the code.</p>
<p>Furthermore, Google Mock is specifically designed to work in harmony with Google Test, facilitating the creation of comprehensive tests that can leverage both actual objects and their mocked counterparts. This integration simplifies the process of writing tests that are both extensive and reflective of real application behavior, thereby enhancing the reliability and maintainability of the codebase.</p>
<h1 id="_idParaDest-232"><a id="_idTextAnchor231"/>Integrating Google Test into a C++ project</h1>
<p>We’re going<a id="_idIndexMarker665"/> to demonstrate how to incorporate Google Test into a CMake project, providing a step-by-step guide to configuring CMake to work with Google Test for unit testing in C++ projects.</p>
<p>To start, ensure that Google Test is included in your project. This can be done by adding Google Test as a submodule in your project’s repository or downloading it via CMake. Once Google Test is part of your project, the next step is to configure your <code>CMakeLists.txt</code> file to include Google Test in the build process.</p>
<p>Here’s an example of how you might configure your <code>CMakeLists.txt</code> file to integrate Google Test via a submodule:</p>
<pre class="source-code">
git submodule add https://github.com/google/googletest.git external/googletest</pre>
<p>Update <code>CMakeLists.txt</code> to include<a id="_idIndexMarker666"/> Google Test and Google Mock in the build:</p>
<pre class="source-code">
# Minimum version of CMake
cmake_minimum_required(VERSION 3.14)
project(MyProject)
# GoogleTest requires at least C++14
set(CMAKE_CXX_STANDARD 14)
set(CMAKE_CXX_STANDARD_REQUIRED ON)
# Enable testing capabilities
enable_testing()
# Add GoogleTest to the project
add_subdirectory(external/googletest)
# Include GoogleTest and GoogleMock headers
include_directories(${gtest_SOURCE_DIR}/include ${gmock_SOURCE_DIR}/include)
# Define your test executable
add_executable(my_tests test1.cpp test2.cpp)
# Link GoogleTest and GoogleMock to your test executable
target_link_libraries(my_tests gtest gtest_main gmock gmock_main)</pre>
<p>In this<a id="_idIndexMarker667"/> configuration, <code>add_subdirectory(external/googletest)</code> tells CMake to include Google Test in the build. <code>include_directories</code> ensures that the Google Test headers are accessible to your test files. <code>add_executable</code> defines a new executable for your tests, and <code>target_link_libraries</code> links the Google Test libraries to your test executable.</p>
<p>After configuring <code>CMakeLists.txt</code>, you can build and run your tests using CMake and make commands. This setup not only integrates Google Test into your project but also leverages CMake’s testing capabilities to automate running the tests.</p>
<p>The following code snippet demonstrates another way to configure CMake to use Google Test, which is by downloading Google Test via CMake’s <code>FetchContent</code> module. This approach allows CMake to download Google Test during the build process, ensuring that the project’s dependencies are automatically managed:</p>
<pre class="source-code">
cmake_minimum_required(VERSION 3.14)
project(MyProject)
# GoogleTest requires at least C++14
set(CMAKE_CXX_STANDARD 14)
set(CMAKE_CXX_STANDARD_REQUIRED ON)
include(FetchContent)
FetchContent_Declare(
  googletest
  URL https://github.com/google/googletest/archive/03597a01ee50ed33e9dfd640b249b4be3799d395.zip
)
# For Windows: Prevent overriding the parent project’s compiler/linker settings
set(gtest_force_shared_crt ON CACHE BOOL “” FORCE)
FetchContent_MakeAvailable(googletest)</pre>
<p>While this example focuses<a id="_idIndexMarker668"/> on integrating Google Test with CMake, it’s worth noting that Google Test is versatile and can be integrated into other build systems as well, such as Google’s own Bazel. For projects using different build systems or for more complex configurations, refer to the official Google Test documentation for comprehensive guidance and best practices. This documentation provides valuable insights into leveraging Google Test across various environments and build systems, ensuring that you can effectively implement unit testing in your C++ projects regardless of the development setup.</p>
<h1 id="_idParaDest-233"><a id="_idTextAnchor232"/>Usage of Google Test in C++ projects</h1>
<p>Google Test<a id="_idIndexMarker669"/> provides a comprehensive suite of functionalities to support various testing needs in C++ development. Understanding how to effectively leverage these features can significantly enhance your testing practices. Let’s explore the usage of Google Test through simple examples and explanations.</p>
<h1 id="_idParaDest-234"><a id="_idTextAnchor233"/>Writing a simple test</h1>
<p>A <a id="_idIndexMarker670"/>simple test in Google Test can be written using the <code>TEST</code> macro, which defines a test function. Within this function, you can use various assertions to verify the behavior of your code. Here’s a basic example:</p>
<pre class="source-code">
#include &lt;gtest/gtest.h&gt;
int add(int a, int b) {
    return a + b;
}
TEST(AdditionTest, HandlesPositiveNumbers) {
    EXPECT_EQ(5, add(2, 3));
}</pre>
<p>In this example, <code>EXPECT_EQ</code> is used to assert that the <code>add</code> function returns the expected sum of two positive numbers. Google Test provides a variety of assertions such as <code>EXPECT_GT</code> (greater than), <code>EXPECT_TRUE</code> (Boolean <code>true</code>), and many others for different testing scenarios.</p>
<p>The key<a id="_idIndexMarker671"/> difference between <code>EXPECT_*</code> and <code>ASSERT_*</code> assertions lies in their behavior upon failure. While <code>EXPECT_*</code> assertions allow the test to continue running after a failure, <code>ASSERT_*</code> assertions will halt the current test function immediately upon failure. Use <code>EXPECT_*</code> when subsequent lines of the test do not depend on the success of the current assertion, and <code>ASSERT_*</code> when the failure of an assertion would make the continuation of the test meaningless or potentially cause errors.</p>
<h1 id="_idParaDest-235"><a id="_idTextAnchor234"/>Using a test fixture</h1>
<p>For tests <a id="_idIndexMarker672"/>that require a common setup and teardown for multiple test cases, Google Test offers the concept of a test fixture. This is achieved by defining a class derived from <code>::testing::Test</code> and then using the <code>TEST_F</code> macro to write tests that use this fixture:</p>
<pre class="source-code">
class CalculatorTest : public ::testing::Test {
protected:
    void SetUp() override {
        // Code here will be called immediately before each test
        calculator.reset(new Calculator());
    }
    void TearDown() override {
        // Code here will be called immediately after each test
        calculator.reset();
    }
    std::unique_ptr&lt;Calculator&gt; calculator;
};
TEST_F(CalculatorTest, CanAddPositiveNumbers) {
    EXPECT_EQ(5, calculator-&gt;add(2, 3));
}
TEST_F(CalculatorTest, CanAddNegativeNumbers) {
    EXPECT_EQ(-5, calculator-&gt;add(-2, -3));
}</pre>
<p>In this <a id="_idIndexMarker673"/>example, <code>SetUp</code> and <code>TearDown</code> are overridden to provide a common setup (initializing a <code>Calculator</code> object) and teardown (cleaning up the <code>Calculator</code> object) for each test case. <code>TEST_F</code> is used to define test functions that automatically use this setup and teardown, ensuring that each test starts with a fresh <code>Calculator</code> instance.</p>
<h1 id="_idParaDest-236"><a id="_idTextAnchor235"/>The main function</h1>
<p>To run the <a id="_idIndexMarker674"/>tests, Google Test requires a main function that initializes the Google Test framework and runs all the tests. Here’s an example:</p>
<pre class="source-code">
#include &lt;gtest/gtest.h&gt;
int main(int argc, char **argv) {
    ::testing::InitGoogleTest(&amp;argc, argv);
    return RUN_ALL_TESTS();
}</pre>
<p>This main function initializes Google Test, passing the command-line arguments to it, which allows for controlling test execution from the command line. <code>RUN_ALL_TESTS()</code> runs all the tests that have been defined and returns <code>0</code> if all tests pass or <code>1</code> otherwise.</p>
<p>By following these examples and explanations, you can start using Google Test to write comprehensive tests for your C++ projects, ensuring that your code behaves as expected across a wide range of scenarios.</p>
<h1 id="_idParaDest-237"><a id="_idTextAnchor236"/>Running Google Test tests</h1>
<p>After setting up <a id="_idIndexMarker675"/>Google Test with your CMake project and compiling your tests, running them is straightforward. You execute the tests using the <code>ctest</code> command in your build directory, which CMake uses to run tests defined in your <code>CMakeLists.txt</code> file.</p>
<p>When you run the tests for a <code>Calculator</code> class, the standard output to your terminal might look like this if you execute the test binary directly:</p>
<pre class="source-code">
$ cd path/to/build
[==========] Running 4 tests from 2 test suites.
[----------] Global test environment set-up.
[----------] 2 tests from AdditionTests
[ RUN      ] AdditionTests.HandlesZeroInput
[       OK ] AdditionTests.HandlesZeroInput (0 ms)
[ RUN      ] AdditionTests.HandlesPositiveInput
[       OK ] AdditionTests.HandlesPositiveInput (0 ms)
[----------] 2 tests from AdditionTests (0 ms total)
[----------] 2 tests from SubtractionTests
[ RUN      ] SubtractionTests.HandlesZeroInput
[       OK ] SubtractionTests.HandlesZeroInput (0 ms)
[ RUN      ] SubtractionTests.HandlesPositiveInput
[       OK ] SubtractionTests.HandlesPositiveInput (0 ms)
[----------] 2 tests from SubtractionTests (0 ms total)
[----------] Global test environment tear-down
[==========] 4 tests from 2 test suites ran. (1 ms total)
[  PASSED  ] 4 tests.</pre>
<p>This output <a id="_idIndexMarker676"/>details each test suite and test case, showing which tests were run (<code>[ RUN      ]</code>) and their results (<code>[       OK ]</code> for passed tests). It provides a clear breakdown of the testing process, including setup and teardown phases, and aggregates the results at the end.</p>
<p>If you run the tests using <code>ctest</code>, the output is more concise by default:</p>
<pre class="source-code">
$ ctest
Test project /path/to/build
    Start 1: AdditionTests.HandlesZeroInput
1/4 Test #1: AdditionTests.HandlesZeroInput ......   Passed    0.01 sec
    Start 2: AdditionTests.HandlesPositiveInput
2/4 Test #2: AdditionTests.HandlesPositiveInput ...   Passed    0.01 sec
    Start 3: SubtractionTests.HandlesZeroInput
3/4 Test #3: SubtractionTests.HandlesZeroInput .....   Passed    0.01 sec
    Start 4: SubtractionTests.HandlesPositiveInput
4/4 Test #4: SubtractionTests.HandlesPositiveInput ..   Passed    0.01 sec
100% tests passed, 0 tests failed out of 4</pre>
<p>In this <code>ctest</code> output, each line corresponds to a test case, showing its start order, name, and result. The summary at the end gives a quick overview of the total number of tests, how many passed, and how many failed. This format is useful for getting a quick assessment of your test suite’s health without the detailed breakdown provided by the Google Test output.</p>
<h1 id="_idParaDest-238"><a id="_idTextAnchor237"/>Advanced features of Google Test</h1>
<p>Google Test <a id="_idIndexMarker677"/>offers a range of advanced features designed to handle complex testing scenarios, providing developers with powerful tools to ensure their code’s robustness. Among these features, one notable capability is the support for <em class="italic">death tests</em>. Death tests are particularly useful for verifying that your code exhibits the expected behavior when it encounters fatal conditions, such as failed assertions or explicit calls to <code>abort()</code>. This is crucial in scenarios where you want to ensure that your application responds appropriately to unrecoverable errors, enhancing its reliability and safety.</p>
<p>Here’s a brief example of a death test:</p>
<pre class="source-code">
void risky_function(bool trigger) {
    if (trigger) {
        assert(false &amp;&amp; “Triggered a fatal error”);
    }
}
TEST(RiskyFunctionTest, TriggersAssertOnCondition) {
    EXPECT_DEATH_IF_SUPPORTED(risky_function(true), “Triggered a fatal error”);
}</pre>
<p>In this example, <code>EXPECT_DEATH_IF_SUPPORTED</code> checks that <code>risky_function(true)</code> indeed causes the program to exit (due to the failed assertion), and it matches the specified error message. This ensures that the function behaves as expected under fatal conditions.</p>
<p>Other advanced features of Google Test include <em class="italic">mocking</em> for simulating complex object interactions, <em class="italic">parameterized tests</em> for running the same test logic with various inputs, and <em class="italic">type-parameterized tests</em> for applying the same test logic across different data types. These features enable comprehensive testing strategies that can cover a wide range of scenarios and inputs, ensuring thorough validation of your code.</p>
<p>For developers seeking to leverage the full potential of Google Test, including its advanced features such as death tests and more, the official Google Test documentation serves as an invaluable resource. It offers detailed explanations, examples, and best practices, guiding you through the nuances of effective test writing and execution in C++ projects. By referring to this documentation, you can <a id="_idIndexMarker678"/>deepen your understanding of Google Test’s capabilities and integrate them effectively into your testing workflow.</p>
<h1 id="_idParaDest-239"><a id="_idTextAnchor238"/>Using gMock in C++ projects</h1>
<p>In the<a id="_idIndexMarker679"/> world of software testing, particularly within the methodology of TDD, a mock object plays a crucial role. It’s designed to mimic the behavior of real objects by implementing the same interface, allowing it to stand in for the actual object in tests. However, the power of a mock object lies in its flexibility; developers can specify its behavior at runtime, including which methods are called, their call order, frequency, argument specifications, and the return values. This level of control turns mock objects into powerful tools for testing interactions and integrations within the code.</p>
<p>Mocks address several challenges in testing complex or interconnected systems. When developing prototypes or tests, relying solely on real objects might not be feasible or practical due to constraints such as external dependencies, execution time, or costs associated with real operations. In such cases, mocks provide a lightweight, controllable substitute that replicates the necessary interactions without the overhead or side effects of the real implementations. They enable developers to focus on the behavior and integration of components rather than their underlying implementations, facilitating more focused and efficient testing.</p>
<p>The distinction between fake objects and mock objects is crucial to understanding their appropriate use cases. While both serve as substitutes for real objects in testing, they have different characteristics and purposes:</p>
<ul>
<li><strong class="bold">Fake objects</strong>: These <a id="_idIndexMarker680"/>are simplified implementations that mimic real objects but typically take shortcuts for the sake of testing efficiency. An example would be an in-memory database that replicates the functionality of a real database system without persistent storage. Fakes are practical for tests where the exact workings of the real object are not under scrutiny.</li>
<li><strong class="bold">Mock objects</strong>: Unlike <a id="_idIndexMarker681"/>fakes, mocks are pre-programmed with specific expectations that form a contract of how they should be used. They are ideal for testing the interactions between the system under test and its dependencies. For instance, when testing a class that relies on a service, a mock of the service can be used to ensure that the class interacts with the service as expected without actually invoking the service’s real implementation.</li>
</ul>
<p>gMock, Google’s framework for creating mock classes in C++, provides a comprehensive solution akin to what jMock and EasyMock offer for Java. With gMock, developers first describe the interface of the object to be mocked using macros, which then generate the mock class implementation. Developers <a id="_idIndexMarker682"/>can then instantiate mock objects, setting up their expected behaviors and interactions using gMock’s intuitive syntax. During test execution, gMock monitors these mock objects, ensuring that all specified interactions adhere to the defined expectations, and flagging any deviations as errors. This immediate feedback is invaluable for identifying issues in how components interact with their dependencies.</p>
<h1 id="_idParaDest-240"><a id="_idTextAnchor239"/>Example of using gMock</h1>
<p>In<a id="_idIndexMarker683"/> unit testing, particularly when interfacing with network operations, mocking is an invaluable technique. This is exemplified in the case of a <code>Socket</code> class, which serves as a foundational element for network communication. The <code>Socket</code> class abstracts the functionality of sending and receiving raw byte arrays over a network, providing methods such as <code>send</code> and <code>recv</code>. Concrete classes such as <code>TcpSocket</code>, <code>UdpSocket</code>, and <code>WebSocket</code> extend this base class to implement specific network protocols. The following code shows the definition of the <code>Socket</code> class:</p>
<pre class="source-code">
class Socket {
public:
    // sends raw byte array of given size, returns number of bytes sent
    // or -1 in case of error
    virtual ssize_t send(void* data, size_t size) = 0;
    // receives raw byte array of given size, returns number of bytes received
    // or -1 in case of error
    virtual ssize_t recv(void* data, size_t size) = 0;
};</pre>
<p>For instance, the <code>DataSender</code> class relies on a <code>Socket</code> instance to send data. This class is meticulously designed to manage data transmission, attempting retries as necessary and handling various scenarios such as partial data sends, peer-initiated connection closures, and connection errors. The objective in unit testing <code>DataSender</code> is to validate its behavior across these different scenarios without engaging in actual network communication. The <code>DataSender</code> class<a id="_idIndexMarker684"/> is defined as follows:</p>
<pre class="source-code">
struct DataSentParitally {};
struct ConnectionClosedByPeer {};
struct ConnectionError {};
// Class under test
class DataSender {
    static constexpr size_t RETRY_NUM = 2;
public:
    DataSender(Socket* socket) : _socket{socket} {}
    void send() {
        auto data = std::array&lt;int, 32&gt;{};
        auto bytesSent = 0;
        for (size_t i = 0; i &lt; RETRY_NUM &amp;&amp; bytesSent != sizeof(data); ++i) {
            bytesSent = _socket-&gt;send(&amp;data, sizeof(data));
            if (bytesSent &lt; 0) {
                throw ConnectionError{};
            }
            if (bytesSent == 0) {
                throw ConnectionClosedByPeer{};
            }
        }
        if (bytesSent != sizeof(data)) {
            throw DataSentParitally{};
        }
    }
private:
    Socket* _socket;
};</pre>
<p>This <a id="_idIndexMarker685"/>requirement leads us to the use of a <code>MockSocket</code> class, derived from <code>Socket</code>, to simulate network interactions. Here’s how <code>MockSocket</code> is defined:</p>
<pre class="source-code">
class MockSocket : public Socket {
public:
    MOCK_METHOD(ssize_t, send, (void* data, size_t size), (override));
    MOCK_METHOD(ssize_t, recv, (void* data, size_t size), (override));
};</pre>
<p>The <code>MockSocket</code> class utilizes the <code>MOCK_METHOD</code> macro from gMock to mock the <code>send</code> and <code>recv</code> methods of the <code>Socket</code> class, allowing for the specification of expected behavior during tests. The <code>override</code> keyword ensures that these mock methods correctly override their counterparts in the <code>Socket</code> class.</p>
<p>Setting expectations in gMock is done using constructs such as <code>WillOnce</code> and <code>WillRepeatedly</code>, which define how mock methods behave when invoked:</p>
<pre class="source-code">
TEST(DataSender, HappyPath) {
    auto socket = MockSocket{};
    EXPECT_CALL(socket, send(_, _)).Times(1).WillOnce(Return(32 * sizeof(int)));
    auto sender = DataSender(&amp;socket);
    sender.send();
}</pre>
<p>In this <code>HappyPath</code> test, <code>EXPECT_CALL</code> sets an expectation that <code>send</code> will be called exactly <a id="_idIndexMarker686"/>once, successfully transmitting all the data in a single attempt.</p>
<pre class="source-code">
TEST(DataSender, SendSuccessfullyOnSecondAttempt) {
    auto socket = MockSocket{};
    EXPECT_CALL(socket, send(_, _)).Times(2)
                                   .WillOnce(Return(2 * sizeof(int)))
                                   .WillOnce(Return(32 * sizeof(int)));
    auto sender = DataSender(&amp;socket);
    sender.send();
}</pre>
<p>This test expects two calls to <code>send</code>: the first transmits only a portion of the data, while the second completes the transmission, simulating a successful <code>send</code> on the second attempt.</p>
<p>The rest of the tests check various error scenarios, such as partial data transmission, connection closure by the peer, and connection errors. Here’s an example of a test for the scenario where data is sent partially:</p>
<pre class="source-code">
 TEST(DataSender, DataSentParitally) {
     auto socket = MockSocket{};
     EXPECT_CALL(socket, send(_, _)).Times(2)
                                    .WillRepeatedly(Return(2 * sizeof(int)));
     auto sender = DataSender(&amp;socket);
     EXPECT_THROW(sender.send(), DataSentParitally);
 }
 TEST(DataSender, ConnectionClosedByPeer) {
     auto socket = MockSocket{};
     EXPECT_CALL(socket, send(_, _)).Times(1)
                                    .WillRepeatedly(Return(0 * sizeof(int)));
     auto sender = DataSender(&amp;socket);
     EXPECT_THROW(sender.send(), ConnectionClosedByPeer);
 }
 TEST(DataSender, ConnectionError) {
     auto socket = MockSocket{};
     EXPECT_CALL(socket, send(_, _)).Times(1)
                                    .WillRepeatedly(Return(-1 * sizeof(int)));
     auto sender = DataSender(&amp;socket);
     EXPECT_THROW(sender.send(), ConnectionError);
 }</pre>
<p>Running these<a id="_idIndexMarker687"/> tests with gMock and observing the output allows us to confirm the <code>DataSender</code> class’s behavior under various conditions:</p>
<pre class="source-code">
[==========] Running 5 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 5 tests from DataSender
[ RUN      ] DataSender.HappyPath
[       OK ] DataSender.HappyPath (0 ms)
[ RUN      ] DataSender.SendSuccessfullyOnSecondAttempt
[       OK ] DataSender.SendSuccessfullyOnSecondAttempt (0 ms)
[ RUN      ] DataSender.DataSentPartially
[       OK ] DataSender.DataSentPartially (1 ms)
[ RUN      ] DataSender.ConnectionClosedByPeer
[       OK ] DataSender.ConnectionClosedByPeer (0 ms)
[ RUN      ] DataSender.ConnectionError
[       OK ] DataSender.ConnectionError (0 ms)
[----------] 5 tests from DataSender (1 ms total)
[----------] Global test environment tear-down
[==========] 5 tests from 1 test suite ran. (1 ms total)
[  PASSED  ] 5 tests.</pre>
<p>The <a id="_idIndexMarker688"/>output succinctly reports the execution and outcomes of each test, indicating the successful validation of the <code>DataSender</code> class’s handling of different network communication scenarios. For more comprehensive details on utilizing gMock, including its full suite of features, the official gMock documentation serves as an essential resource, guiding developers through effective mocking strategies in C++ unit testing.</p>
<h2 id="_idParaDest-241"><a id="_idTextAnchor240"/>Mocking non-virtual methods via dependency injection</h2>
<p>In <a id="_idIndexMarker689"/>certain scenarios, you might encounter the need to mock non-virtual methods for unit testing. This can be challenging, as traditional mocking frameworks such as gMock primarily target virtual methods due to C++’s polymorphism requirements. However, one effective strategy to overcome this limitation is through dependency injection, coupled with the use of templates. This approach enhances testability and flexibility by decoupling the class dependencies.</p>
<h3>Refactoring for testability</h3>
<p>To <a id="_idIndexMarker690"/>illustrate this, let’s refactor the <code>Socket</code> class interface and the <code>DataSender</code> class to accommodate the mocking of non-virtual methods. We’ll introduce templates to <code>DataSender</code> to allow injecting either the real <code>Socket</code> class or its mock version.</p>
<p>First, consider a simplified version of the <code>Socket</code> class without virtual methods:</p>
<pre class="source-code">
class Socket {
public:
    // sends raw byte array of given size, returns number of bytes sent
    // or -1 in case of error
    ssize_t send(void* data, size_t size);
    // receives raw byte array of given size, returns number of bytes received
    // or -1 in case of error
    ssize_t recv(void* data, size_t size);
};</pre>
<p>Next, we modify the <code>DataSender</code> class to accept a <code>template</code> parameter for the socket type, enabling the injection of either a real socket or a mock socket at compile-time:</p>
<pre class="source-code">
template&lt;typename SocketType&gt;
class DataSender {
    static constexpr size_t RETRY_NUM = 2;
public:
    DataSender(SocketType* socket) : _socket{socket} {}
    void send() {
        auto data = std::array&lt;int, 32&gt;{};
        auto bytesSent = 0;
        for (size_t i = 0; i &lt; RETRY_NUM &amp;&amp; bytesSent != sizeof(data); ++i) {
            bytesSent = _socket-&gt;send(&amp;data, sizeof(data));
            if (bytesSent &lt; 0) {
                throw ConnectionError{};
            }
            if (bytesSent == 0) {
                throw ConnectionClosedByPeer{};
            }
        }
        if (bytesSent != sizeof(data)) {
            throw DataSentPartially{};
        }
    }
private:
    SocketType* _socket;
};</pre>
<p>With<a id="_idIndexMarker691"/> this template-based design, <code>DataSender</code> can now be instantiated with any type that conforms to the <code>Socket</code> interface, including mock types.</p>
<h2 id="_idParaDest-242"><a id="_idTextAnchor241"/>Mocking with templates</h2>
<p>For the<a id="_idIndexMarker692"/> mock version of <code>Socket</code>, we can define a <code>MockSocket</code> class as follows:</p>
<pre class="source-code">
class MockSocket {
public:
    MOCK_METHOD(ssize_t, send, (void* data, size_t size), ());
    MOCK_METHOD(ssize_t, recv, (void* data, size_t size), ());
};</pre>
<p>This <code>MockSocket</code> class mimics the <code>Socket</code> interface but uses gMock’s <code>MOCK_METHOD</code> to define mock methods.</p>
<h3>Unit testing with dependency injection</h3>
<p>When<a id="_idIndexMarker693"/> writing tests for <code>DataSender</code>, we can now inject <code>MockSocket</code> using templates:</p>
<pre class="source-code">
TEST(DataSender, HappyPath) {
    MockSocket socket;
    EXPECT_CALL(socket, send(_, _)).Times(1).WillOnce(Return(32 * sizeof(int)));
    DataSender&lt;MockSocket&gt; sender(&amp;socket);
    sender.send();
}</pre>
<p>In this test, <code>DataSender</code> is instantiated with <code>MockSocket</code>, allowing the <code>send</code> method to be mocked as desired. This demonstrates how templates and dependency injection enable the mocking of non-virtual methods, providing a flexible and powerful approach to unit testing in C++.</p>
<p>This technique, while powerful, requires careful design consideration to ensure that the code remains clean <a id="_idIndexMarker694"/>and maintainable. For complex scenarios or further exploration of mocking strategies, the official gMock documentation remains an invaluable resource, offering a wealth of information on advanced mocking techniques and best practices.</p>
<h4>Mocking singletons</h4>
<p>Despite being <a id="_idIndexMarker695"/>considered an anti-pattern due to its potential to introduce a global state and tight coupling in software designs, the Singleton pattern is nevertheless prevalent in many code bases. Its convenience for ensuring a single instance of a class often leads to its use in scenarios such as database connections, where a single, shared resource is logically appropriate.</p>
<p>The Singleton pattern’s characteristic of restricting class instantiation and providing a global access point presents a challenge for unit testing, particularly when the need arises to mock the singleton’s behavior.</p>
<p>Consider the example of a <code>Database</code> class implemented as a singleton:</p>
<pre class="source-code">
class Database {
public:
    std::vector&lt;std::string&gt; query(uint32_t id) const {
        return {};
    }
    static Database&amp; getInstance() {
        static Database db;
        return db;
    }
private:
    Database() = default;
};</pre>
<p>In this scenario, the <code>DataHandler</code> class interacts with the <code>Database</code> singleton to perform operations, such as querying data:</p>
<pre class="source-code">
class DataHandler {
public:
    DataHandler() {}
    void doSomething() {
        auto&amp; db = Database::getInstance();
        auto result = db.query(42);
    }
};</pre>
<p>To<a id="_idIndexMarker696"/> facilitate testing of the <code>DataHandler</code> class without relying on the real <code>Database</code> instance, we can introduce a templated variation, <code>DataHandler1</code>, that allows injecting a mock database instance:</p>
<pre class="source-code">
template&lt;typename Db&gt;
class DataHandler1 {
public:
    DataHandler1() {}
    std::vector&lt;std::string&gt; doSomething() {
        auto&amp; db = Db::getInstance();
        auto result = db.query(42);
        return result;
    }
};</pre>
<p>This approach leverages templates to decouple <code>DataHandler1</code> from the concrete <code>Database</code> singleton, enabling the substitution of a <code>MockDatabase</code> during tests:</p>
<pre class="source-code">
class MockDatabase {
public:
    std::vector&lt;std::string&gt; query(uint32_t id) const {
        return {“AAA”};
    }
    static MockDatabase&amp; getInstance() {
        static MockDatabase db;
        return db;
    }
};</pre>
<p>With <code>MockDatabase</code> in<a id="_idIndexMarker697"/> place, unit tests can now simulate database interactions without hitting the actual database, as demonstrated in the following test case:</p>
<pre class="source-code">
TEST(DataHandler, check) {
    auto dh = DataHandler1&lt;MockDatabase&gt;{};
    EXPECT_EQ(dh.doSomething(), std::vector&lt;std::string&gt;{“AAA”});
}</pre>
<p>This test instantiates <code>DataHandler1</code> with <code>MockDatabase</code>, ensuring that the <code>doSomething</code> method interacts with the mock rather than the real database. The expected result is a predefined mock response, making the test predictable and isolated from external dependencies.</p>
<p>This templated solution, a variation of the dependency injection technique discussed earlier, showcases the flexibility and power of templates in C++. It elegantly addresses the challenge of mocking singletons, thereby enhancing the testability of components that depend on singleton instances. For more complex scenarios or further exploration of mocking strategies, referring to the official gMock documentation is advisable, as it offers comprehensive insights into advanced mocking techniques and best practices.</p>
<h2 id="_idParaDest-243"><a id="_idTextAnchor242"/>The Nice, the Strict, and the Naggy</h2>
<p>In the world of unit testing with gMock, managing the behavior of mock objects and their interactions with the system under test is crucial. gMock introduces three modes to control this behavior: Naggy, Nice, and Strict. These modes determine how gMock handles uninteresting calls – those not matched by any <code>EXPECT_CALL</code>.</p>
<h3>Naggy mocks</h3>
<p>By default, mock<a id="_idIndexMarker698"/> objects in gMock are “naggy.” This means that while they warn about uninteresting calls, these calls do not cause the test to fail. The warning serves as a<a id="_idIndexMarker699"/> reminder that there might be unexpected interactions with the mock, but it’s not critical enough to warrant a test failure. This behavior ensures that tests focus on the intended expectations without being too lenient or too strict about incidental interactions.</p>
<p>Consider the following test scenario:</p>
<pre class="source-code">
TEST(DataSender, Naggy) {
    auto socket = MockSocket{};
    EXPECT_CALL(socket, send(_, _)).Times(1).WillOnce(Return(32 * sizeof(int)));
    auto sender = DataSender(&amp;socket);
    sender.send();
}</pre>
<p>In this case, if there’s an uninteresting call to <code>recv</code>, gMock issues a warning but the test will pass, marking unanticipated interactions without failing the test.</p>
<h3>Nice mocks</h3>
<p><code>NiceMock</code> objects<a id="_idIndexMarker700"/> go a step further by suppressing warnings for <a id="_idIndexMarker701"/>uninteresting calls. This mode is useful when the test’s focus is strictly on specific interactions, and other incidental calls to the mock should be ignored without cluttering the test output with warnings.</p>
<p>Using <code>NiceMock</code> in a test looks like this:</p>
<pre class="source-code">
TEST(DataSender, Nice) {
    auto socket = NiceMock&lt;MockSocket&gt;{};
    EXPECT_CALL(socket, send(_, _)).Times(1).WillOnce(Return(32 * sizeof(int)));
    auto sender = DataSender(&amp;socket);
    sender.send();
}</pre>
<p>In this <code>Nice</code> mode, even <a id="_idIndexMarker702"/>if there are uninteresting calls to <code>recv</code>, gMock<a id="_idIndexMarker703"/> quietly ignores them, keeping the test output clean and focused on the defined expectations.</p>
<h3>Strict mocks</h3>
<p>On<a id="_idIndexMarker704"/> the other<a id="_idIndexMarker705"/> end of the spectrum, <code>StrictMock</code> objects treat uninteresting calls as errors. This strictness ensures that every interaction with the mock is accounted for by an <code>EXPECT_CALL</code>. This mode is particularly useful in tests where precise control over mock interactions is necessary, and any deviation from the expected calls should lead to test failure.</p>
<p>A test using <code>StrictMock</code> might look like this:</p>
<pre class="source-code">
TEST(DataSender, Strict) {
    auto socket = StrictMock&lt;MockSocket&gt;{};
    EXPECT_CALL(socket, send(_, _)).Times(1).WillOnce(Return(32 * sizeof(int)));
    auto sender = DataSender(&amp;socket);
    sender.send();
}</pre>
<p>In <code>Strict</code> mode, any uninteresting call, such as to <code>recv</code>, results in a test failure, enforcing strict adherence to the defined expectations.</p>
<h3>Test output and recommended settings</h3>
<p>The<a id="_idIndexMarker706"/> behavior of these mocking modes is reflected in the test output:</p>
<pre class="source-code">
Program returned: 1
Program stdout
[==========] Running 3 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 3 tests from DataSender
[ RUN      ] DataSender.Naggy
GMOCK WARNING:
Uninteresting mock function call - returning default value.
    Function call: recv(0x7ffd4aae23f0, 128)
          Returns: 0
NOTE: You can safely ignore the above warning unless this call should not happen.  Do not suppress it by blindly adding an EXPECT_CALL() if you don’t mean to enforce the call.  See https://github.com/google/googletest/blob/master/googlemock/docs/cook_book.md#knowing-when-to-expect for details.
[       OK ] DataSender.Naggy (0 ms)
[ RUN      ] DataSender.Nice
[       OK ] DataSender.Nice (0 ms)
[ RUN      ] DataSender.Strict
unknown file: Failure
Uninteresting mock function call - returning default value.
    Function call: recv(0x7ffd4aae23f0, 128)
          Returns: 0
[  FAILED  ] DataSender.Strict (0 ms)
[----------] 3 tests from DataSender (0 ms total)
[----------] Global test environment tear-down
[==========] 3 tests from 1 test suite ran. (0 ms total)
[  PASSED  ] 2 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] DataSender.Strict
 1 FAILED TEST</pre>
<p>In <code>Naggy</code> mode, the <a id="_idIndexMarker707"/>test passes with a warning for uninteresting calls. <code>Nice</code> mode also passes but without any warnings. <code>Strict</code> mode, however, fails the test if there are uninteresting calls.</p>
<p>It is recommended to start with <code>StrickMock</code> and then relax the mode as needed. This approach ensures that tests are initially strict about interactions with mock objects, providing a safety net for unexpected calls. As the test suite matures and the expected interactions become clearer, the mode can be relaxed to <code>Naggy</code> or <code>Nice</code> to reduce noise in the test output.</p>
<p>For further exploration of these modes and advanced mocking techniques, the official gMock documentation provides comprehensive insights and examples, guiding developers through effective mock object management in unit testing.</p>
<p>Throughout this section, we delved into the functionalities and practical applications of Google Test (GTest) and Google Mock (GMock), essential tools for enhancing the testing framework and development workflow of C++ projects. GTest offers a robust environment for creating, managing, and executing unit tests, featuring test fixtures for shared setup and teardown routines, parameterized tests for varied input testing, and type-parameterized tests for applying the same tests across different data types. Its comprehensive assertion library ensures thorough validation of code behavior, contributing to the stability and durability of the software.</p>
<p>Complementing GTest, GMock allows for the seamless creation and utilization of mock objects, enabling isolated component testing by mimicking the behavior of dependencies. This is invaluable in complex systems where direct testing with real dependencies is either impractical or counterproductive. With GMock, developers gain access to a suite of features including automatic mock generation, versatile expectation settings, and detailed behavior verification, enabling in-depth testing of component interactions.</p>
<p>By integrating GTest and GMock into the C++ development life cycle, developers can adopt a robust test-driven approach, ensuring code quality and facilitating continuous testing and integration practices, ultimately leading to more reliable and maintainable software projects.</p>
<h1 id="_idParaDest-244"><a id="_idTextAnchor243"/>Other notable C++ unit testing frameworks</h1>
<p>Beyond Google Test and Google Mock, the C++ ecosystem is rich with unit testing frameworks, each offering unique features and philosophies. These frameworks cater to various testing needs and preferences, providing developers with multiple options for integrating unit testing into their projects.</p>
<h2 id="_idParaDest-245"><a id="_idTextAnchor244"/>Catch2</h2>
<p>Catch2 <a id="_idIndexMarker708"/>stands out for its simplicity and ease of use, requiring<a id="_idIndexMarker709"/> minimal boilerplate code to get started. It adopts a header-only distribution, making it straightforward to integrate into projects. Catch2 supports a variety of testing paradigms, including BDD-style test cases, and offers expressive assertion macros that enhance test readability and intent. Its standout feature is the “Sections” mechanism, which provides a natural way to share setup and teardown code among tests in a flexible and hierarchical manner.</p>
<h2 id="_idParaDest-246"><a id="_idTextAnchor245"/>Boost.Test</h2>
<p>Part of the <a id="_idIndexMarker710"/>extensive Boost libraries, Boost.Test offers robust support <a id="_idIndexMarker711"/>for unit testing in C++. It provides a comprehensive assertion framework, test organization facilities, and integration with the Boost build system. Boost.Test can be used in a header-only mode or compiled mode, offering flexibility in its deployment. It’s known for its detailed test result reports and wide range of built-in tools for test case management, making it suitable for both small and large-scale projects.</p>
<h2 id="_idParaDest-247"><a id="_idTextAnchor246"/>Doctest</h2>
<p>Doctest is <a id="_idIndexMarker712"/>designed with a focus on simplicity and speed, positioning <a id="_idIndexMarker713"/>itself as the lightest feature-rich C++ testing framework. It’s particularly appealing for TDD due to its fast compile times. Inspired by Catch2, Doctest offers a similar syntax but aims to be more lightweight and faster to compile, making it ideal for including tests in everyday development without impacting build times significantly.</p>
<h2 id="_idParaDest-248"><a id="_idTextAnchor247"/>Google Test versus Catch2 versus Boost.Test versus Doctest</h2>
<ul>
<li><strong class="bold">Simplicity</strong>: Catch2 <a id="_idIndexMarker714"/>and Doctest excel in simplicity and ease of use, with Catch2 offering BDD-style syntax and Doctest being extremely lightweight</li>
<li><strong class="bold">Integration</strong>: Google Test and Boost.Test will provide more extensive integration capabilities, particularly suited for larger projects with complex testing needs</li>
<li><strong class="bold">Performance</strong>: Doctest stands out for its compile-time and runtime performance, making it ideal for rapid development cycles</li>
<li><strong class="bold">Features</strong>: Boost.Test and Google Test come with a more comprehensive set of features out of the box, including<a id="_idIndexMarker715"/> advanced test case management and detailed reporting</li>
</ul>
<p>Choosing the right framework often comes down to project-specific requirements, developer preferences, and the desired balance between simplicity, performance, and feature richness. Developers are encouraged to explore these frameworks further to determine which best fits their unit testing needs, contributing to more reliable, maintainable, and high-quality C++ software.</p>
<h1 id="_idParaDest-249"><a id="_idTextAnchor248"/>Good candidates for unit tests</h1>
<p>Identifying the<a id="_idIndexMarker716"/> optimal candidates for unit testing is pivotal in establishing a robust testing strategy. Unit tests excel when applied to parts of the code base that are well-suited to isolation and fine-grained verification. Here are some key examples and recommendations:</p>
<p>Classes and functions with clear boundaries and well-defined responsibilities are prime candidates for unit testing. These components should ideally embody the Single Responsibility Principle, handling a specific aspect of the application’s functionality. Testing these isolated units allows for precise verification of their behavior, ensuring that they perform their intended tasks correctly under various conditions.</p>
<p>Pure functions, which depend solely on their input parameters and produce no side effects, are excellent targets for unit tests. Their deterministic nature – where a given input always results in the same output – makes them straightforward to test and verify. Pure functions are often found in utility libraries, mathematical computations, and data transformation operations.</p>
<p>Components that interact with dependencies through well-defined interfaces are easier to test, especially when those dependencies can be easily mocked or stubbed. This facilitates testing the component in isolation, focusing on its logic rather than the implementation details of its dependencies.</p>
<p>The business logic layer, which encapsulates the core functionality and rules of the application, is typically well-suited for unit testing. This layer often involves calculations, data processing, and decision-making that can be tested in isolation from the user interface and external systems.</p>
<p>While many aspects of an application are suitable for unit testing, it’s prudent to recognize scenarios that pose challenges. Components that require complex interactions with external resources, such as databases, filesystems, and network services, might be difficult to effectively mock or might<a id="_idIndexMarker717"/> lead to flaky tests due to their reliance on external state or behavior. While mocking can simulate some of these interactions, the complexity and overhead might not always justify the effort in the context of unit testing.</p>
<p>Although unit tests are invaluable for verifying individual components, they have their limitations, especially concerning integrations and end-to-end interactions. For code that is inherently difficult to isolate or requires complex external interactions, <strong class="bold">end-to-end</strong> (<strong class="bold">E2E</strong>) tests become crucial. E2E tests simulate real-world usage scenarios, covering the flow from the user interface through to the backend systems and external integrations. In the next section, we will delve into E2E testing, exploring its role in complementing unit tests and providing comprehensive coverage of the application’s functionality.</p>
<h1 id="_idParaDest-250"><a id="_idTextAnchor249"/>E2E testing in software development</h1>
<p>E2E testing is a<a id="_idIndexMarker718"/> comprehensive testing approach that evaluates the application’s functionality and performance from start to finish. Unlike unit testing, which isolates and tests individual components or units of code, E2E testing examines the application as an integrated whole, simulating real-world user scenarios. This method ensures that all the various components of the application, including its interfaces, databases, networks, and other services, work harmoniously to deliver the desired user experience.</p>
<h2 id="_idParaDest-251"><a id="_idTextAnchor250"/>E2E testing frameworks</h2>
<p>Given that <a id="_idIndexMarker719"/>E2E testing often involves interacting with the application from the outside, it’s not confined to the language in which the application is written. For C++ applications, which might be part of a larger ecosystem or serve as backend systems, E2E testing can be conducted using a variety of frameworks across different languages. Some popular E2E testing frameworks include the following:</p>
<ul>
<li><strong class="bold">Selenium</strong>: Predominantly <a id="_idIndexMarker720"/>used for web applications, Selenium<a id="_idIndexMarker721"/> can automate browsers to simulate user interactions with web interfaces, making it a versatile tool for E2E testing</li>
<li><strong class="bold">Cypress</strong>: Another <a id="_idIndexMarker722"/>powerful tool for web applications, Cypress<a id="_idIndexMarker723"/> offers a more modern and developer-friendly approach to E2E testing with rich debugging capabilities and a robust API</li>
<li><strong class="bold">Postman</strong>: For <a id="_idIndexMarker724"/>applications exposing RESTful APIs, Postman <a id="_idIndexMarker725"/>allows comprehensive API testing, ensuring that the application’s endpoints perform as expected under various conditions</li>
</ul>
<h2 id="_idParaDest-252"><a id="_idTextAnchor251"/>When to use E2E testing</h2>
<p>E2E testing is<a id="_idIndexMarker726"/> particularly valuable in scenarios where the application’s components must interact in complex workflows, often involving multiple systems and external dependencies. It’s crucial for the following:</p>
<ul>
<li><strong class="bold">Testing complex user workflows</strong>: E2E testing shines in validating user journeys that span multiple application components, ensuring a seamless experience from the user’s perspective</li>
<li><strong class="bold">Integration scenarios</strong>: When the application interacts with external systems or services, E2E testing verifies that these integrations work as intended, catching issues that might not be evident in isolation</li>
<li><strong class="bold">Critical path testing</strong>: For features and pathways that are critical to the application’s core functionality, E2E testing ensures reliability and performance under realistic usage conditions</li>
</ul>
<h1 id="_idParaDest-253"><a id="_idTextAnchor252"/>Situations favoring E2E testing</h1>
<h2 id="_idParaDest-254"><a id="_idTextAnchor253"/>Complex interactions</h2>
<p>In<a id="_idIndexMarker727"/> situations where the application’s components engage in intricate interactions, possibly spanning different technologies and platforms, unit tests might fall short. E2E testing is indispensable for ensuring that the collective behavior of these components aligns with the expected outcomes, especially in:</p>
<p>The architecture outlined in the diagram represents a typical web application with several interconnected services, each serving a distinct role in the system.</p>
<div><div><img alt="Figure 12.1 – E2E testing" src="img/B19606_12_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – E2E testing</p>
<p>At the frontend, there’s a <strong class="bold">user UI</strong>, which<a id="_idIndexMarker728"/> is the graphical interface where users interact with the application. It’s designed to send and receive data to and from the backend services through <a id="_idIndexMarker729"/>an <strong class="bold">API gateway</strong>. The API gateway acts as an intermediary that routes requests from the user UI to the appropriate backend services and aggregates responses to send back to the UI.</p>
<p>Several backend services are illustrated:</p>
<ul>
<li><strong class="bold">Account management</strong>: This service handles user accounts, including authentication, profile management, and other user-related data</li>
<li><strong class="bold">Billing</strong>: Responsible for managing billing information, subscriptions, and invoicing</li>
<li><strong class="bold">Payments</strong>: Processes financial transactions, such as credit card processing or interfacing with payment gateways</li>
<li><strong class="bold">Notifications</strong>: Sends out alerts or messages to users, likely triggered by certain events in the account management or billing services</li>
</ul>
<p>External services, possibly third-party applications or data providers, can also interact with the API gateway, providing additional functionality or data that supports the main application.</p>
<p>For E2E testing of this system, tests would simulate user actions on the user UI, such as signing up for an account or making a payment. The tests would then verify that the UI correctly sends the appropriate requests through the API gateway to the backend services. Subsequently, the tests would confirm that the user UI responds correctly to the data received from the backend, ensuring that the entire workflow, from the user UI down to notifications, operates as expected. This comprehensive testing approach ensures that each component functions individually and in concert with the rest of the system, delivering a seamless experience for the user.</p>
<p>To summarize, it is <a id="_idIndexMarker730"/>essential to consider E2E testing in scenarios where the application’s components engage in complex interactions, especially when these interactions span different technologies and platforms. E2E testing ensures that the collective behavior of these components aligns with the expected outcomes, providing a comprehensive assessment of the application’s functionality and performance. Here are some of the most common cases when E2E is beneficial:</p>
<ul>
<li><strong class="bold">Multi-layered applications</strong>: Applications with multiple layers or tiers, such as client-server architectures, benefit from E2E testing to ensure the layers communicate effectively</li>
<li><strong class="bold">Distributed systems</strong>: For applications spread across different environments or services, E2E testing can validate the data flow and functionality across these distributed components</li>
</ul>
<h2 id="_idParaDest-255"><a id="_idTextAnchor254"/>Real-world environment testing</h2>
<p>One of <a id="_idIndexMarker731"/>the primary advantages of E2E testing is its ability to replicate the conditions close to the production environment. This includes testing the application on actual hardware, interacting with real databases, and navigating through the genuine network infrastructure. This level of testing is crucial for the following:</p>
<ul>
<li><strong class="bold">Performance validation</strong>: Ensuring that the application performs optimally under expected load conditions and user traffic</li>
<li><strong class="bold">Security assurance</strong>: Verifying that the application’s security measures are effective in a realistic environment, protecting against potential vulnerabilities</li>
</ul>
<p>E2E testing serves as the final checkpoint before software release, offering a comprehensive assessment of the application’s readiness for deployment. By simulating real-world scenarios, E2E testing ensures that the application not only meets its technical specifications but also delivers a reliable and user-friendly experience, making it an essential component of the software development life cycle.</p>
<h1 id="_idParaDest-256"><a id="_idTextAnchor255"/>Automatic test coverage tracking tools</h1>
<p>In the <a id="_idIndexMarker732"/>quest to ensure comprehensive testing of software projects, automatic test coverage tracking tools play a pivotal role. These tools provide invaluable insights into the extent to which the source code of an application is executed during testing, highlighting areas that are well-tested and those that may need additional attention.</p>
<h2 id="_idParaDest-257"><a id="_idTextAnchor256"/>Automatic test coverage tracking tools with examples</h2>
<p>Ensuring<a id="_idIndexMarker733"/> comprehensive test coverage is a cornerstone of reliable<a id="_idIndexMarker734"/> software development. Tools such as <code>gcov</code> for the <code>llvm-cov</code> for LLVM projects automate the tracking of test coverage, providing crucial insights into how thoroughly the tests exercise the code.</p>
<h3>Tool overview with examples</h3>
<p>There are two major tools used for automatic test coverage tracking in C++ projects:</p>
<ul>
<li><code>gcov</code> analyzes <a id="_idIndexMarker736"/>the execution paths taken in your code during test runs. For instance, after compiling a C++ <code>example.cpp</code> file with <code>g++ -fprofile-arcs -ftest-coverage example.cpp</code>, running the corresponding test suite generates coverage data. Running <code>gcov example.cpp</code> afterward produces a report detailing the number of times each line of code was executed.</li>
<li><code>llvm-cov</code> works with Clang to offer detailed coverage reports. Compiling with <code>clang++ -fprofile-instr-generate -fcoverage-mapping example.cpp</code> and then executing the test binary with <code>LLVM_PROFILE_FILE=”example.profraw” ./example</code> prepares the coverage data. <code>llvm-profdata merge -sparse example.profraw -o example.profdata</code> followed by <code>llvm-cov show ./example -instr-profile=example.profdata</code> generates a coverage report for <code>example.cpp</code>.</li>
</ul>
<h3>Integration with C++ projects</h3>
<p>Integrating these <a id="_idIndexMarker739"/>tools into C++ projects involves compiling the source with coverage flags, executing the tests to generate coverage data, and then analyzing this data to produce reports.</p>
<p>For a project with multiple files, you might compile with the following:</p>
<pre class="source-code">
g++ -fprofile-arcs -ftest-coverage file1.cpp file2.cpp -o testExecutable</pre>
<p>After running <code>./testExecutable</code> to execute your tests, use <code>gcov file1.cpp file2.cpp</code> to generate coverage reports for each source file.</p>
<p>With <code>llvm-cov</code>, the<a id="_idIndexMarker740"/> process is similar but tailored for Clang. After compilation and test execution, merging profile data with <code>llvm-profdata</code> and generating the report with <code>llvm-cov</code> provides a comprehensive view of test coverage.</p>
<h3>Interpreting coverage reports</h3>
<p>The<a id="_idIndexMarker741"/> coverage reports generated by these tools offer several metrics:</p>
<ul>
<li><code>gcov</code> report might state <code>Lines executed:90.00% of 100</code>, meaning 90 out of 100 lines were run during tests.</li>
<li><code>gcov</code> report<a id="_idIndexMarker743"/> such as <code>Branches executed:85.00% of 40</code> shows that 85% of all branches were tested.</li>
<li><code>Functions executed:95.00% of 20</code> in a <code>gcov</code> report indicates that 95% of functions were invoked during testing.</li>
</ul>
<p>For example, a simplified <code>gcov</code> report might look like this:</p>
<pre class="source-code">
File ‘example.cpp’
Lines executed:90.00% of 100
Branches executed:85.00% of 40
Functions executed:95.00% of 20</pre>
<p>Similarly, an <code>llvm-cov</code> report provides detailed coverage metrics, along with the specific lines and branches covered, enhancing the ability to pinpoint areas needing additional tests.</p>
<p>These reports guide developers in improving test coverage by highlighting untested code paths and functions, but they should not be the sole metric for test quality. High coverage with poorly designed tests can give a false sense of security. Effective use of these tools involves not just aiming for high coverage percentages but also ensuring that tests are meaningful and reflective of real-world usage scenarios.</p>
<h2 id="_idParaDest-258"><a id="_idTextAnchor257"/>Utilizing hit maps for enhanced test coverage analysis</h2>
<p>Hit maps, produced <a id="_idIndexMarker745"/>by test coverage tracking tools such as <code>gcov</code> and <code>llvm-cov</code>, offer a granular view of how tests exercise the code, serving as a detailed guide for developers aiming to improve test coverage. These hit maps go beyond simple percentage metrics, showing precisely which lines of code were executed during tests and how many times, thus enabling a more informed approach to enhance test suites.</p>
<h3>Understanding hit maps</h3>
<p>A hit map is<a id="_idIndexMarker746"/> essentially a detailed annotation of the source code, with each line accompanied by execution counts indicating how many times tests have run that particular line. This level of detail helps identify not only untested parts of the code but also areas that might be over-tested or need more varied testing scenarios.</p>
<p>The <code>.gcov</code> files generated by <code>gcov</code> and the annotated source code produced by <code>llvm-cov</code> provide these hit maps, offering a clear picture of test coverage at the line level.</p>
<pre class="source-code">
-:    0:Source:example.cpp
-:    0:Graph:example.gcno
-:    0:Data:example.gcda
-:    0:Runs:3
-:    0:Programs:1
3:    1:int main() {
-:    2:  // Some comment
2:    3:  bool condition = checkCondition();
1:    4:  if (condition) {
1:    5:    performAction();
    ...</pre>
<p>In this example, line 3 (<code>bool condition = checkCondition();</code>) was executed twice, while the <code>performAction();</code> line within the <code>if</code> statement was executed once, indicating that the condition was <code>true</code> in one of the test runs.</p>
<p>Similar to <code>gcov</code>, after compiling with <code>clang++</code> using the <code>-fprofile-instr-generate -fcoverage-mapping</code> flags and executing the tests, <code>llvm-cov</code> can produce a hit map using the <code>llvm-cov show</code> command with the <code>-instr-profile</code> flag pointing to the generated profile data. For example, <code>llvm-cov show ./example -instr-profile=example.profdata example.cpp</code> outputs the annotated source code with execution counts.</p>
<p>The output <a id="_idIndexMarker747"/>would resemble the following:</p>
<pre class="source-code">
example.cpp:
int main() {
    |  3|  // Some comment
    |  2|  bool condition = checkCondition();
    |  1|  if (condition) {
    |  1|    performAction();
    ...</pre>
<p>Here, the execution count is prefixed to each line, providing a clear picture of test coverage at a glance.</p>
<h3>Leveraging hit maps for test improvement</h3>
<p>By <a id="_idIndexMarker748"/>examining hit maps, developers can identify code sections that are not covered by any test case, indicated by execution counts of zero. These areas represent potential risks for undetected bugs and should be prioritized for additional testing. Conversely, lines with exceptionally high execution counts might indicate areas where tests are redundant or overly focused, suggesting an opportunity to diversify test scenarios or refocus testing efforts on less-covered parts of the code base.</p>
<p>Incorporating hit map analysis into regular development workflows encourages a proactive approach to maintaining and enhancing test coverage, ensuring that tests remain effective and aligned with the evolving code base. As with all testing strategies, the goal is not merely to achieve high coverage numbers but to ensure that the test suite comprehensively validates the software’s functionality and reliability in a variety of scenarios.</p>
<p>Incorporating hit maps into the development workflow has been made even more accessible with the advent<a id="_idIndexMarker749"/> of <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>) plugins that integrate coverage visualization directly into the coding environment. A notable example is the “Code Coverage” plugin by Markis Taylor for <strong class="bold">Visual Studio Code</strong> (<strong class="bold">VSCode</strong>). This <a id="_idIndexMarker750"/>plugin overlays hit maps onto the source code within the VSCode editor, providing immediate, visual feedback on test coverage.</p>
<p>The “Code Coverage” plugin <a id="_idIndexMarker751"/>processes coverage reports generated by tools such as <code>gcov</code> or <code>llvm-cov</code> and visually annotates the source code in VSCode. Lines of code covered by tests are highlighted, typically in green, while uncovered lines are marked in red. This immediate visual representation allows developers to quickly identify untested code regions without leaving the editor or navigating through external coverage reports.</p>
<h2 id="_idParaDest-259"><a id="_idTextAnchor258"/>Recommendations for code coverage</h2>
<p>Code coverage<a id="_idIndexMarker752"/> is a vital metric in<a id="_idIndexMarker753"/> the realm of software testing, providing insights into the extent to which the code base is exercised by the test suite. For C++ projects, leveraging tools such as <code>gcov</code> for GCC and <code>llvm-cov</code> for LLVM projects can offer detailed coverage analysis. These tools are adept at not only tracking coverage from unit tests but also from E2E tests, allowing for a comprehensive assessment of test coverage across different testing levels.</p>
<p>A robust<a id="_idIndexMarker754"/> testing strategy involves a combination of focused unit tests, which validate individual components in isolation, and broader E2E tests, which assess the system’s functionality as a whole. By employing <code>gcov</code> or <code>llvm-cov</code>, teams can aggregate coverage data from both testing types, providing a holistic view of the project’s test coverage. This combined approach helps identify areas of the code that are either under-tested or not tested at all, guiding efforts to enhance the test suite’s effectiveness.</p>
<p>It is recommended to keep a vigilant eye on code coverage metrics and strive to prevent any decrease in coverage percentages. A decline in coverage might indicate new code being added without adequate testing, potentially introducing undetected bugs into the system. To mitigate this risk, teams should integrate coverage checks into their <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) pipelines, ensuring that any changes that reduce coverage are promptly identified and addressed.</p>
<p>Periodically, it’s beneficial to allocate time specifically for increasing test coverage, especially in areas identified as critical or risky. This might involve writing additional tests for complex logic, edge cases, or error-handling paths that were previously overlooked. Investing in coverage improvement initiatives not only enhances the software’s reliability but also contributes to a more maintainable and robust code base in the long term.</p>
<h1 id="_idParaDest-260"><a id="_idTextAnchor259"/>Summary</h1>
<p>This chapter provided a thorough overview of testing in C++, covering essential topics from unit testing basics to advanced E2E testing. You learned about unit testing’s role in ensuring individual components work correctly and how tools such as Google Test and Google Mock help write and manage these tests effectively. The chapter also touched on mocking techniques for simulating complex behaviors in tests.</p>
<p>Additionally, the importance of tracking test coverage using tools such as <code>gcov</code> and <code>llvm-cov</code> was discussed, emphasizing the need to maintain and improve coverage over time. E2E testing was highlighted as crucial for checking the entire application’s functionality, complementing the more focused unit tests.</p>
<p>By exploring different C++ testing frameworks, the chapter offered insights into the various tools available for developers, helping them choose the right ones for their projects. In essence, this chapter equipped you with the knowledge to implement comprehensive and effective testing strategies in your C++ development endeavors, contributing to the creation of reliable and robust software.</p>
<p>In the next chapter, we will explore modern approaches to third-party management in C++, including Docker-based solutions and available package managers.</p>
</div>
</body></html>