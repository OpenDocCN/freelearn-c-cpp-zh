- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Turning the Source File into an Abstract Syntax Tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we learned in the previous chapter, a compiler is typically divided into
    two parts – the frontend and the backend. In this chapter, we will implement the
    frontend of a programming language – that is, the part that mainly deals with
    the source language. We will learn about the techniques that real-world compilers
    use and apply them to our programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Our journey will begin with us defining our programming language’s grammar and
    end with an **abstract syntax tree** (**AST**), which will become the base for
    code generation. You can use this approach for every programming language for
    which you would like to implement a compiler.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a real programming language, where you will learn about the `tinylang`
    language, which is a subset of a real programming language, and for which you
    will implement a compiler frontend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing the directory structure of a compiler project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowing how to handle multiple input files for the compiler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The skill of handling user messages and informing them of issues in a pleasant
    manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the lexer using modular pieces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing a recursive descent parser from the rules derived from a grammar
    to perform syntax analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing semantic analysis by creating an AST and analyzing its characteristics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the skills you’ll acquire in this chapter, you’ll be able to build a compiler
    frontend for any programming language.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a real programming language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real programming brings up more challenges than the simple calc language from
    the previous chapter. To have a look at the details, we will be using a tiny subset
    of *Modula-2* in this and the following chapters. Modula-2 is well-designed and
    optionally supports `tinylang`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin with an example of what a program in `tinylang` looks like. The
    following function computes the greatest common divisor using the *Euclidean algorithm*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a feeling for how a program in the language looks, let’s take
    a quick tour of the `tinylang` subset’s grammar as used in this chapter. In the
    next few sections, we’ll use this grammar to derive the lexer and the parser from
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A compilation unit in Modula-2 begins with the `MODULE` keyword, followed by
    the name of the module. The content of a module can have a list of imported modules,
    declarations, and a block containing statements that run at initialization time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'A declaration introduces constants, variables, and procedures. The declaration
    of constants is prefixed with the `CONST` keyword. Similarly, variable declarations
    begin with the `VAR` keyword. The declaration of a constant is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The identifier is the name of the constant. The value is derived from an expression,
    which must be computable at compile time. The declaration of variables is a bit
    more complex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To be able to declare more than one variable in one go, a list of identifiers
    is used. The type name can potentially come from another module and is prefixed
    by the module name in this case. This is called a **qualified identifier**. A
    procedure requires the most details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows how constants, variables, and procedures are declared.
    Procedures can have parameters and a return type. Normal parameters are passed
    as values, and `VAR` parameters are passed by reference. The other part missing
    from the `block` rule is `statementSequence`, which is a list of single statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'A statement is delimited by a semicolon if it is followed by another statement.
    Again, only a subset of the *Modula-2* statements is supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The first part of this rule describes an assignment or a procedure call. A
    qualified identifier followed by `:=` is an assignment. If it is followed by `(`,
    then it is a procedure call. The other statements are the usual control statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `IF` statement also has a simplified syntax as it can only have a single
    `ELSE` block. With that statement, we can conditionally guard a statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `WHILE` statement describes a loop that’s guarded by a condition. Together
    with the `IF` statement, this enables us to write simple algorithms in `tinylang`.
    Finally, the definition of an expression is missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The expression syntax is very similar to that of calc in the previous chapter.
    Only the `INTEGER` and `BOOLEAN` data types are supported.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the `identifier` and `integer_literal` tokens are used. An `H`.
  prefs: []
  type: TYPE_NORMAL
- en: These are already a lot of rules, and we’re only covering a part of Modula-2!
    Nevertheless, it is possible to write small applications in this subset. Let’s
    implement a compiler for `tinylang`!
  prefs: []
  type: TYPE_NORMAL
- en: Creating the project layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The project layout for `tinylang` follows the approach we laid out in [*Chapter
    1*](B19561_01.xhtml#_idTextAnchor017), *Installing LLVM*. The source code for
    each component is in a subdirectory of the `lib` directory, and the header files
    are in a subdirectory of `include/tinylang`. The subdirectory is named after the
    component. In [*Chapter 1*](B19561_01.xhtml#_idTextAnchor017), *Installing LLVM*,
    we only created the `Basic` component.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the previous chapter, we know that we need to implement a lexer, a parser,
    an AST, and a semantic analyzer. Each is a component of its own, called `Lexer`,
    `Parser`, `AST`, and `Sema`, respectively. The directory layout that will be used
    in this chapter looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – The directory layout of the tinylang project](img/B19561_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – The directory layout of the tinylang project
  prefs: []
  type: TYPE_NORMAL
- en: The components have clearly defined dependencies. `Lexer` depends only on `Basic`.
    `Parser` depends on `Basic`, `Lexer`, `AST`, and `Sema`. `Sema` only depends on
    `Basic` and `AST`. The well-defined dependencies help us reuse the components.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a closer look at the implementation!
  prefs: []
  type: TYPE_NORMAL
- en: Managing the input files for the compiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A real compiler has to deal with many files. Usually, the developer calls the
    compiler with the name of the main compilation unit. This compilation unit can
    refer to other files – for example, via `#include` directives in C or `import`
    statements in Python or Modula-2\. An imported module can import other modules,
    and so on. All these files must be loaded into memory and run through the analysis
    stages of the compiler. During development, a developer may make syntactical or
    semantical errors. When detected, an error message, including the source line
    and a marker, should be printed. This essential component is not trivial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, LLVM comes with a solution: the `llvm::SourceMgr` class. A new source
    file is added to `SourceMgr` with a call to the `AddNewSourceBuffer()` method.
    Alternatively, a file can be loaded with a call to the `AddIncludeFile()` method.
    Both methods return an ID to identify the buffer. You can use this ID to retrieve
    a pointer to the memory buffer of the associated file. To define a location in
    the file, you can use the `llvm::SMLoc` class. This class encapsulates a pointer
    to the buffer. Various `PrintMessage()` methods allow you to emit errors and other
    informational messages to the user.'
  prefs: []
  type: TYPE_NORMAL
- en: Handling messages for the user
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Only a centralized definition of messages is missing. In a large piece of software
    (such as a compiler), you do not want to sprinkle message strings all over the
    place. If there is a request to change messages or translate them into another
    language, then you better have them in a central place!
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple approach is that each message has an ID (an `enum` member), a severity
    level such as `Error` or `Warning`, and a string containing the messages. In your
    code, you only refer to the message ID. The severity level and message string
    are only used when the message is printed. These three items (the ID, the security
    level, and the message) must be managed consistently. The LLVM libraries use the
    preprocessor to solve this. The data is stored in a file with the `.def` suffix
    and is wrapped in a macro name. That file is usually included several times, with
    different definitions for the macro. The definition is in the `include/tinylang/Basic/Diagnostic.def`
    file path and looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The first macro parameter, `ID`, is the enumeration label, the second parameter,
    `Level`, is the severity, and the third parameter, `Msg`, is the message text.
    With this definition at hand, we can define a `DiagnosticsEngine` class to emit
    error messages. The interface is in the `include/tinylang/Basic/Diagnostic.h`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After including the necessary header files, `Diagnostic.def` can be used to
    define the enumeration. To not pollute the global namespace, a nested namespace
    called `diag` is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `DiagnosticsEngine` class uses a `SourceMgr` instance to emit the messages
    via the `report()` method. Messages can have parameters. To implement this facility,
    the variadic-format support provided by LLVM is used. The message text and the
    severity level are retrieved with the help of the `static` method. As a bonus,
    the number of emitted error messages is also counted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The message string is returned by `getDiagnosticText()`, while the level is
    returned by `getDiagnosticKind()`. Both methods are later implemented in the `.``cpp`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As messages can have a variable number of parameters, the solution in C++ is
    to use a variadic template. Of course, this is also used by the `formatv()` function
    provided by LLVM. To get the formatted message, we just need to forward the template
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we have implemented most of the class. Only `getDiagnosticText()`
    and `getDiagnosticKind()`are missing. They are defined in the `lib/Basic/Diagnostic.cpp`
    file and also make use of the `Diagnostic.def` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the header file, the `DIAG` macro is defined to retrieve the desired
    part. Here, we define an array that holds the text messages. Therefore, the `DIAG`
    macro only returns the `Msg` part. We use the same approach for the level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Not surprisingly, both functions simply index the array to return the desired
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The combination of the `SourceMgr` and `DiagnosticsEngine` classes provides
    a good basis for the other components. We’ll use them in the lexer first!
  prefs: []
  type: TYPE_NORMAL
- en: Structuring the lexer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know from the previous chapter, we need a `Token` class and a `Lexer`
    class. Additionally, a `TokenKind` enumeration is required to give each token
    class a unique number. Having an all-in-one header and implementation file does
    not scale, so let’s move the items. `TokenKind` can be used universally and is
    placed in the `Basic` component. The `Token` and `Lexer` classes belong to the
    `Lexer` component but are placed in different headers and implementation files.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three different classes of tokens: `CONST` keyword, the`;` delimiter,
    and the `ident` token, respectively, each of which represents identifiers in the
    source. Each token needs a member name for the enumeration. Keywords and punctuators
    have natural display names that can be used for messages.'
  prefs: []
  type: TYPE_NORMAL
- en: Like in many programming languages, the keywords are a subset of the identifiers.
    To classify a token as a keyword, we need a keyword filter, which checks if the
    found identifier is indeed a keyword. This is the same behavior as in C or C++,
    where keywords are also a subset of identifiers. Programming languages evolve
    and new keywords may be introduced. As an example, the original K&R C language
    had no enumerations defined with the `enum` keyword. Due to this, a flag indicating
    the language level of a keyword should be present.
  prefs: []
  type: TYPE_NORMAL
- en: 'We collected several pieces of information, all of which belong to a member
    of the `TokenKind` enumeration: the label for the enumeration member, the spelling
    of punctuators, and a flag for keywords. For the diagnostic messages, we centrally
    store the information in a `.def` file called `include/tinylang/Basic/TokenKinds.def`,
    which looks like this. One thing to note is that keywords are prefixed with `kw_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With these centralized definitions, it’s easy to create the `TokenKind` enumeration
    in the `include/tinylang/Basic/TokenKinds.h` file. Again, the enumeration is put
    into its own namespace, `tok`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The pattern to fill the array should be familiar by now. The `TOK` macro is
    defined to only return `ID`. As a useful addition, we also define `NUM_TOKENS`
    as the last member of the enumeration, which denotes the number of defined tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation file, `lib/Basic/TokenKinds.cpp`, also uses the `.def` file
    to retrieve the names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The textual name of a token is derived from its enumeration label, `ID`. There
    are two particularities:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to define the `TOK` and `KEYWORD` macros since the default definition
    of `KEYWORD` does not use the `TOK` macro
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second, a `nullptr` value is added at the end of the array, accounting for
    the added `NUM_TOKENS` enumeration member:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We take a slightly different approach in the `getPunctuatorSpelling()` and
    `getKeywordSpelling()` functions. These functions only return a meaningful value
    for a subset of the enumeration. This can be realized with a `switch` statement,
    returning a `nullptr` value by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Note how the macros are defined to retrieve the necessary piece of information
    from the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, the `Token` class was declared in the same header
    file as the `Lexer` class. To make this more versatile, we will put the `Token`
    class into its own header file in `include/Lexer/Token.h`. As before, `Token`
    stores a pointer to the start of the token, its length, and the token kind, as
    defined previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `SMLoc` instance, which denotes the source position in messages, is created
    from the pointer to the token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `getIdentifier()` and `getLiteralData()` methods allow access to the text
    of the token for identifiers and literal data. It is not necessary to access the
    text for any other token type as this is implied by the token type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We declare the `Lexer` class in the `include/Lexer/Lexer.h` header file and
    put the implementation in the `lib/Lexer/lexer.cpp` file. The structure is the
    same as for the calc language from the previous chapter. We need to take a closer
    look at two details here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, some operators share the same prefix – for example, `<` and `<=`. When
    the current character we look at is `<`, then we must check the next character
    before deciding which token we found. Remember that the input needs to end with
    a null byte. Therefore, the next character can always be used if the current character
    is valid:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The other detail is that there are far more keywords now. How can we handle
    this? A simple and fast solution is to populate a hash table with the keywords,
    which are all stored in the `TokenKinds.def` file. This can be done during the
    instantiation of the `Lexer` class. With this approach, it is also possible to
    support different levels of the language as the keywords can be filtered with
    the attached flag. Here, this flexibility is not needed yet. In the header file,
    the keyword filter is defined as follows, using an instance of `llvm::StringMap`
    for the hash table:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `getKeyword()` method returns the token kind of the given string, or a
    default value if the string does not represent a keyword:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the implementation file, the keyword table is filled:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the techniques you’ve just learned about, it’s not difficult to write an
    efficient lexer class. As compilation speed matters, many compilers use a handwritten
    lexer, with one example being clang.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing a recursive descent parser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As shown in the previous chapter, the parser is derived from the grammar. Let’s
    recall all the *construction rules*. For each rule of the grammar, you create
    a method named after the non-terminal on the left-hand side of the rule to parse
    the right-hand side of the rule. Following the definition of the right-hand side,
    you do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: For each non-terminal, the corresponding method is called
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each token is consumed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For alternatives and optional or repeating groups, the look-ahead token (the
    next unconsumed token) is examined to decide where to continue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s apply these construction rules to the following rule of grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily translate this into the following C++ method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The whole grammar of `tinylang` can be turned into C++ in this way. In general,
    you have to be careful to avoid some pitfalls since most grammars that you will
    find on the internet are not suitable for this kind of construction.
  prefs: []
  type: TYPE_NORMAL
- en: Grammars and parsers
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two different types of parsers: top-down parsers and bottom-up parsers.
    Their names are derived from the order in which a rule is handled during parsing.
    The input for a parser is the sequence of tokens generated by the lexer.'
  prefs: []
  type: TYPE_NORMAL
- en: A top-down parser expands the leftmost symbol in a rule until a token is matched.
    Parsing is successful if all tokens are consumed and all symbols are expanded.
    This is exactly how the parser for tinylang works.
  prefs: []
  type: TYPE_NORMAL
- en: 'A bottom-up parser does the opposite: it looks at the sequence of tokens and
    tries to replace the tokens with a symbol of the grammar. For example, if the
    next tokens are `IF`, `3`, `+`, and `4`, then a bottom-up parser replaces the
    `3 + 4` token with the `expression` symbol, resulting in the `IF` `expression`
    sequence. When all tokens that belong to the `IF` statement are seen, then this
    sequence of tokens and symbols is replaced by the `ifStatement` symbol.'
  prefs: []
  type: TYPE_NORMAL
- en: The parsing is successful if all tokens are consumed and the only symbol left
    is the start symbol. While top-down parsers can easily be constructed by hand,
    this is not the case for bottom-up parsers.
  prefs: []
  type: TYPE_NORMAL
- en: A different way to characterize both types of parsers is by which symbols are
    expanded first. Both read the input from left to right, but a top-down parser
    expands the leftmost symbol first while a bottom-up parser expands the rightmost
    symbol. Because of this, a top-down parser is also called an LL parser, while
    a bottom-up parser is called an LR parser.
  prefs: []
  type: TYPE_NORMAL
- en: 'A grammar must have certain properties so that either an LL or an LR parser
    can be derived from it. The grammars are named accordingly: you need an LL grammar
    to construct an LL parser.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more details in university textbooks about compiler construction,
    such as Wilhelm, Seidl, and Hack: *Compiler Design. Syntactic and Semantic Analysis*,
    Springer 2013, and Grune and Jacobs: *Parsing Techniques, A practical guide*,
    Springer 2008.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One issue to look for is left-recursive rules. A rule is called **left-recursive**
    if the right-hand side begins with the same terminal that’s on the left-hand side.
    A typical example can be found in grammars for expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'If it’s not already clear from the grammar, then the translation to C++ makes
    it obvious that this results in infinite recursion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Left recursion can also occur indirectly and involve more rules, which is much
    more difficult to spot. That’s why an algorithm exists that can detect and eliminate
    left recursion.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Left-recursive rules are only a problem for LL parsers, such as the recursive-descent
    parser for `tinylang`. The reason is that these parsers expand the leftmost symbol
    first. In contrast, if you use a parser generator to generate an LR parser, which
    expands the rightmost symbol first, then you should avoid right-recursive rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'At each step, the parser decides how to continue by just using the look-ahead
    token. The grammar is said to have conflicts if this decision cannot be made deterministically.
    To illustrate this, have a look at the `using` statement in C#. Like in C++, the
    `using` statement can be used to make a symbol visible in a namespace, such as
    in `using Math;`. It is also possible to define an alias name for the imported
    symbol with `using M = Math;`. In a grammar, this can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'There’s a problem here: after the parser consumes the `using` keyword, the
    look-ahead token is `ident`. However, this information is not enough for us to
    decide if the optional group must be skipped or parsed. This situation always
    arises if the set of tokens, with which the optional group can begin, overlaps
    with the set of tokens that follow the optional group.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s rewrite the rule with an alternative instead of an optional group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, there is a different conflict: both alternatives begin with the same token.
    Looking only at the look-ahead token, the parser can’t decide which of the alternatives
    is the right one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These conflicts are very common. Therefore, it’s good to know how to handle
    them. One approach is to rewrite the grammar in such a way that the conflict disappears.
    In the previous example, both alternatives begin with the same token. This can
    be factored out, resulting in the following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This formulation has no conflict, but it should also be noted that it is less
    expressive. In the two other formulations, it is obvious which `ident` is the
    alias name and which `ident` is the namespace name. In the conflict-free rule,
    the leftmost `ident` changes its role. First, it is the namespace name, but if
    an equals sign follows, then it turns into the alias name.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second approach is to add a predicate to distinguish between both cases.
    This predicate, often called a `Token &peek(int n)` that returns the *n*th token
    after the current look-ahead token. Here, the existence of an equals sign can
    be used as an additional predicate in the decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: A third approach is to use backtracking. For this, you need to save the current
    state. Then, you must try to parse the conflicting group. If this does not succeed,
    then you need to go back to the saved state and try the other path. Here, you
    are searching for the correct rule to apply, which is not as efficient as the
    other methods. Therefore, you should only use this approach as a last resort.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s incorporate error recovery. In the previous chapter, I introduced
    the so-called *panic mode* as a technique for error recovery. The basic idea is
    to skip tokens until one is found that is suitable for continuing parsing. For
    example, in `tinylang`, a statement is followed by a semicolon (`:`).
  prefs: []
  type: TYPE_NORMAL
- en: If there is a syntax problem in an `IF` statement, then you skip all tokens
    until you find a semicolon. Then, you continue with the next statement. Instead
    of using an ad hoc definition for the token set, it’s better to use a systematic
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: For each non-terminal, you compute the set of tokens that can follow the non-terminal
    anywhere (called the `;`, `ELSE`, and `END` tokens can follow. So, you must use
    this set in the error recovery part of `parseStatement()`. This method assumes
    that a syntax error can be handled locally. In general, this is not possible.
    Because the parser skips tokens, so many could be skipped that the end of input
    is reached. At this point, local recovery is not possible.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent meaningless error messages, the calling method needs to be informed
    that an error recovery is still not finished. This can be done with `bool`. If
    it returns `true`, this means that error recovery hasn’t finished yet, while `false`
    means that parsing (including a possible error recovery) was successful.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous ways to extend this error recovery scheme. Using the `FOLLOW`
    sets of active callers is a popular approach. As a simple example, assume that
    `parseStatement()` was called by `parseStatementSequence()`, which was itself
    called by `parseBlock()` and that from `parseModule()`.
  prefs: []
  type: TYPE_NORMAL
- en: Here, each of the corresponding non-terminals has a `FOLLOW` set. If the parser
    detects a syntax error in `parseStatement()`, then tokens are skipped until the
    token is in at least one of the `FOLLOW` sets of the active callers. If the token
    is in the `FOLLOW` set of the statement, then the error was recovered locally
    and a `false` value is returned to the caller. Otherwise, a `true` value is returned,
    meaning that error recovery must continue. A possible implementation strategy
    for this extension is passing `std::bitset` or `std::tuple` to represent the union
    of the current `FOLLOW` sets to the callee.
  prefs: []
  type: TYPE_NORMAL
- en: 'One last question is still open: how can we call the error recovery? In the
    previous chapter, `goto` was used to jump to the error recovery block. This works
    but is not a pleasing solution. Given what we discussed earlier, we can skip tokens
    in a separate method. Clang has a method has `skipUntil()` for this purpose; we
    also use this for `tinylang`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the next step is to add semantic actions to the parser, it would also
    be nice to have a central place to put cleanup code if necessary. A nested function
    would be ideal for this. C++ does not have a nested function. Instead, a Lambda
    function can serve a similar purpose. The `parseIfStatement()` method, which we
    looked at initially, looks as follows when the complete error recovery code is
    added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Parser and lexer generators
  prefs: []
  type: TYPE_NORMAL
- en: Manually constructing a parser and a lexer can be a tedious task, especially
    if you try to invent a new programming language and change the grammar very often.
    Luckily, some tools automate this task.
  prefs: []
  type: TYPE_NORMAL
- en: The classic Linux tools are **flex** ([https://github.com/westes/flex](https://github.com/westes/flex))
    and **bison** ([https://www.gnu.org/software/bison/](https://www.gnu.org/software/bison/)).
    flex generates a lexer from a set of regular expressions, while bison generates
    an **LALR(1)** parser from a grammar description. Both tools generate C/C+ source
    code and can be used together.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular tool is **AntLR** (https://www.antlr.org/). AntLR can generate
    a lexer, a parser, and an AST from a grammar description. The generated parser
    belongs to the **LL(*)** class, which means it is a top-down parser that uses
    a variable number of lookaheads to solve conflicts. The tool is written in Java
    but can generate source code for many popular languages, including C/C++.
  prefs: []
  type: TYPE_NORMAL
- en: All these tools require some library support. If you are looking for a tool
    that generates a self-contained lexer and parser, then **Coco/R** ([https://ssw.jku.at/Research/Projects/Coco/](https://ssw.jku.at/Research/Projects/Coco/))
    may be the tool for you. Coco/R generates a lexer and a recursive-descent parser
    from an **LL(1)** grammar description, similar to the one used in this book. The
    generated files are based on a template file that you can change if needed. The
    tool is written in C# but ports to C++, Java, and other languages.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other tools available, and they vary a lot in terms of the features
    and output languages they support. Of course, when choosing a tool, there are
    also trade-offs to consider. An LALR(1) parser generator such as bison can consume
    a wide range of grammars, and free grammars you can find on the internet are often
    LALR(1) grammars.
  prefs: []
  type: TYPE_NORMAL
- en: As a downside, these generators generate a state machine that needs to be interpreted
    at runtime, which can be slower than a recursive descent parser. Error handling
    is also more complicated. bison has basic support for handling syntax errors,
    but the correct use requires a deep understanding of how the parser works. Compared
    to this, AntLR consumes a slightly smaller grammar class but automatically generates
    error handling, and can also generate an AST. So, rewriting grammar so that it
    can be used with AntLR may speed up development later.
  prefs: []
  type: TYPE_NORMAL
- en: Performing semantic analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The parser we constructed in the previous section only checks the syntax of
    the input. The next step is to add the ability to perform semantic analysis. In
    the calc example in the previous chapter, the parser constructed an AST. In a
    separate phase, the semantic analyzer worked on this tree. This approach can always
    be used. In this section, we will use a slightly different approach and intertwine
    the parser and the semantic analyzer more.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does the semantic analyzer need to do? Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: For each declaration, the names of variables, objects, and more must be checked
    to ensure they have not been declared elsewhere.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each occurrence of a name in an expression or statement, it must be checked
    that the name is declared and that the desired use fits the declaration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each expression, the resulting type must be computed. It is also necessary
    to compute if the expression is constant and if so, which value it has.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For assignment and parameter passing, we must check that the types are compatible.
    Further, we must check that the conditions in `IF` and `WHILE` statements are
    of the `BOOLEAN` type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s already a lot to check for such a small subset of a programming language!
  prefs: []
  type: TYPE_NORMAL
- en: Handling the scope of names
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s have a look at the scope of names first. The scope of a name is the range
    where the name is visible. Like C, `tinylang` uses a declare-before-use model.
    For example, the `B` and `X` variables are declared at the module level to be
    of the `INTEGER` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Before the declaration, the variables are not known and cannot be used. That’s
    only possible after the declaration. Inside a procedure, more variables can be
    declared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Inside the procedure, at the point where the comment is, a use of `B` refers
    to the `B` local variable while a use of `X` refers to the `X` global variable.
    The scope of the local variable, `B`, is `Proc`. If a name cannot be found in
    the current scope, then the search continues in the enclosing scope. Therefore,
    the `X` variable can be used inside the procedure. In `tinylang`, only modules
    and procedures open a new scope. Other language constructs, such as structs and
    classes, usually also open a scope. Predefined entities such as the `INTEGER`
    type and the `TRUE` literal are declared in a global scope, enclosing the scope
    of the module.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `tinylang`, only the name is crucial. Therefore, a scope can be implemented
    as a mapping from a name to its declaration. A new name can only be inserted if
    it is not already present. For the lookup, the enclosing or parent scope must
    also be known. The interface (in the `include/tinylang/Sema/Scope.h` file) looks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation in the `lib/Sema/Scope.cpp` file looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the `StringMap::insert()` method does not override an existing
    entry. The `second` member of the resulting `std::pair` indicates if the table
    was updated. This information is returned to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement the search for the declaration of a symbol, the `lookup()` method
    searches in the current scope and, if nothing is found, searches the scopes linked
    by the `parent` member:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The variable declaration is then processed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The current scope is the module scope.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `INTEGER` type declaration is looked up. It’s an error if no declaration
    is found or if it is not a type declaration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new AST node called `VariableDeclaration` is instantiated, with the important
    attributes being the name, `B`, and the type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name, `B`, is inserted into the current scope, mapping to the declaration
    instance. If the name is already present in the scope, then this is an error.
    The content of the current scope is not changed in this case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same is done for the `X` variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two tasks are performed here. As in the calc example, AST nodes are constructed.
    At the same time, attributes of the node, such as the type, are computed. Why
    is this possible?
  prefs: []
  type: TYPE_NORMAL
- en: The semantic analyzer can fall back on two different sets of attributes. The
    scope is inherited from the caller. The type declaration can be computed (or synthesized)
    by evaluating the name of the type declaration. The language is designed in such
    a way that these two sets of attributes are sufficient to compute all attributes
    of the AST node.
  prefs: []
  type: TYPE_NORMAL
- en: An important aspect is the *declare-before-use* model. If a language allows
    the use of names before declaration, such as members inside a class in C++, then
    it is not possible to compute all attributes of an AST node at once. In such a
    case, the AST node must be constructed with only partially computed attributes
    or just with plain information (such as in the calc example).
  prefs: []
  type: TYPE_NORMAL
- en: The AST must then be visited one or more times to determine the missing information.
    In the case of `tinylang` (and Modula-2), it would be possible to dispense with
    the AST construction – the AST is indirectly represented through the call hierarchy
    of the `parseXXX()` methods. Code generation from an AST is much more common,
    so we construct an AST here, too.
  prefs: []
  type: TYPE_NORMAL
- en: Before we put the pieces together, we need to understand the LLVM style of using
    **runtime type** **information** (**RTTI**).
  prefs: []
  type: TYPE_NORMAL
- en: Using an LLVM-style RTTI for the AST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Naturally, the AST nodes are a part of a class hierarchy. A declaration always
    has a name. Other attributes depend on what is being declared. If a variable is
    declared, then a type is required. A constant declaration needs a type, a value,
    and so on. Of course, at runtime, you need to find out which kind of declaration
    you are working with. The `dynamic_cast<>` C++ operator could be used for this.
    The problem is that the required RTTI is only available if the C++ class has a
    virtual table attached – that is, it uses virtual functions. Another disadvantage
    is that C++ RTTI is bloated. To avoid these disadvantages, the LLVM developers
    introduced a self-made RTTI style, which is used throughout the LLVM libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The (abstract) base class of our hierarchy is `Decl`. To implement the LLVM-style
    RTTI, a public enumeration containing a label for each subclass must be added.
    Also, a private member of this type and a public getter are required. The private
    member is usually called `Kind`. In our case, this looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Each subclass now needs a special function member called `classof`. The purpose
    of this function is to determine if a given instance is of the requested type.
    For `VariableDeclaration`, it is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can use the special templates, `llvm::isa<>`, to check if an object
    is of the requested type and `llvm::dyn_cast<>` to dynamically cast the object.
    More templates exist, but these two are the most commonly used ones. For the other
    templates, see [https://llvm.org/docs/ProgrammersManual.html#the-isa-cast-and-dyn-cast-templates](https://llvm.org/docs/ProgrammersManual.html#the-isa-cast-and-dyn-cast-templates)
    and for more information about the LLVM style, including more advanced uses, see
    [https://llvm.org/docs/HowToSetUpLLVMStyleRTTI.html](https://llvm.org/docs/HowToSetUpLLVMStyleRTTI.html).
  prefs: []
  type: TYPE_NORMAL
- en: Creating the semantic analyzer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Equipped with this knowledge, we can now implement all the parts. First, we
    must create the definition of the AST node for a variable that’s stored in the
    `include/llvm/tinylang/AST/AST.h` file. Besides support for the LLVM-style RTTI,
    the base class stores the name of the declaration, the location of the name, and
    a pointer to the enclosing declaration. The latter is required during code generation
    of nested procedures. The `Decl` base class is declared as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The declaration for a variable only adds a pointer to the type declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The method in the parser needs to be extended with a semantic action and variables
    for collected information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '`DeclList` is a list of declarations, `std::vector<Decl*>`, and `IdentList`
    is a list of locations and identifiers, `std::vector<std::pair<SMLoc, StringRef>>`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `parseQualident()` method returns a declaration, which in this case is expected
    to be a type declaration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parser class knows an instance of the semantic analyzer class, `Sema`,
    that’s stored in the `Actions` member. A call to `actOnVariableDeclaration()`
    runs the semantic analyzer and the AST construction. The implementation is in
    the `lib/Sema/Sema.cpp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The type declaration is checked with `llvm::dyn_cast<TypeDeclaration>`. If it
    is not a type declaration, then an error message is printed. Otherwise, for each
    name in the `Ids` list, `VariableDeclaration` is instantiated and added to the
    list of declarations. If adding the variable to the current scope fails because
    the name is already declared, then an error message is printed as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the other entities are constructed in the same way – the complexity
    of the semantic analysis is the only difference. More work is required for modules
    and procedures because they open a new scope. Opening a new scope is easy: only
    a new `Scope` object must be instantiated. As soon as the module or procedure
    has been parsed, the scope must be removed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This must be done reliably because we do not want to add names to the wrong
    scope in case of a syntax error. This is a classic use of the **Resource Acquisition
    Is Initialization** (**RAII**) idiom in C++. Another complication comes from the
    fact that a procedure can recursively call itself. Therefore, the name of the
    procedure must be added to the current scope before it can be used. The semantic
    analyzer has two methods to enter and leave a scope. The scope is associated with
    a declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple helper class is used to implement the RAII idiom:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'When parsing a module or procedure, two interactions occur with the semantic
    analyzer. The first is after the name is parsed. Here, the (almost empty) AST
    node is constructed and a new scope is established:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The semantic analyzer checks the name in the current scope and returns the
    AST node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The real work is done after all the declarations and the procedure body have
    been parsed. You only need to check if the name at the end of the procedure declaration
    is equal to the name of the procedure and if the declaration used for the return
    type is a type declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Some declarations are inherently present and cannot be defined by the developer.
    This includes the `BOOLEAN` and `INTEGER` types and the `TRUE` and `FALSE` literals.
    These declarations exist in the global scope and must be added programmatically.
    Modula-2 also predefines some procedures, such as `INC` or `DEC`, that can be
    added to the global scope. Given our classes, initializing the global scope is
    simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'With this scheme, all required calculations for `tinylang` can be done. For
    example, let’s look at how to compute if an expression results in a constant value:'
  prefs: []
  type: TYPE_NORMAL
- en: We must ensure literal or a reference to a constant declaration is a constant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If both sides of an expression are constant, then applying the operator also
    yields a constant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These rules are embedded into the semantic analyzer while creating the AST nodes
    for an expression. Likewise, the type and the constant value can be computed.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that not all kinds are computation can be done in this way.
    For example, to detect the use of uninitialized variables, a method called *symbolic
    interpretation* can be used. In its general form, the method requires a special
    walk order through the AST, which is not possible during construction time. The
    good news is that the presented approach creates a fully decorated AST that is
    ready for code generation. This AST can be used for further analysis, given that
    costly analysis can be turned on or off on demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'To play around with the frontend, you also need to update the driver. Since
    the code generation is missing, a correct `tinylang` program produces no output.
    Still, it can be used to explore error recovery and provoke semantic errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Congratulations! You’ve finished implementing the frontend for `tinylang`!
    You can use the example program, `Gcd.mod`, provided in the *Defining a real programming
    language* section to run the frontend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Of course, this is a valid program, and it looks like nothing happens. Be sure
    to modify the file and provoke some error messages. We’ll continue with the fun
    in the next chapter by adding code generation.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the techniques that a real-world compiler
    uses in the frontend. Starting with the project layout, you created separate libraries
    for the lexer, the parser, and the semantic analyzer. To output messages to the
    user, you extended an existing LLVM class, allowing the messages to be stored
    centrally. The lexer is now separated into several interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you learned how to construct a recursive descent parser from a grammar
    description, looked at what pitfalls to avoid, and learned how to use generators
    to do the job. The semantic analyzer you constructed performs all the semantic
    checks required by the language while being intertwined with the parser and AST
    construction.
  prefs: []
  type: TYPE_NORMAL
- en: The result of your coding effort is a fully decorated AST. You’ll use this in
    the next chapter to generate IR code and, finally, object code.s
  prefs: []
  type: TYPE_NORMAL
