<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-262"><a id="_idTextAnchor299"/>7</h1>
<h1 id="_idParaDest-263"><a id="_idTextAnchor300"/>Ray Tracing and Hybrid Rendering</h1>
<p>In this chapter, we venture <a id="_idIndexMarker500"/>into the fascinating world of <strong class="bold">ray tracing</strong> and <strong class="bold">hybrid rendering</strong>. Ray tracing, to put it simply, is a special technique used in computer graphics <a id="_idIndexMarker501"/>that simulates how light interacts with objects. This results in images that are so lifelike, they can be mistaken for reality. However, pure ray tracing is computationally intensive and requires significant hardware resources, which makes it unfeasible for real-time applications with the current generation of hardware. On the other hand, there’s hybrid rendering, which is a mix of conventional rasterization techniques and the realism of ray tracing. This blend offers both good performance and stunning visuals. This chapter will take you through how these techniques can be implemented using Vulkan. We’ll show you how to set up a ray tracing pipeline and guide you on how to integrate hybrid rendering into your work. By the end of this chapter, you will have a deeper understanding of how these advanced techniques work. More importantly, you’ll learn how to use them in your own projects.</p>
<p>The first part of the chapter focuses on developing a GPU-based ray tracer. We’ll elaborate on how to effectively develop this GPU-based ray tracer, detailing the steps involved, and how each function contributes to the final lifelike image. The second part of our chapter will revolve around the integration of shadows from a ray tracer along with rasterized deferred rendering. We will delve into how the shadows generated from a ray tracer can be combined with the rasterization technique of deferred rendering, a technique commonly referred to as hybrid rendering.</p>
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Implementing a GPU ray tracer</li>
<li>Implementing a hybrid renderer</li>
</ul>
<h1 id="_idParaDest-264"><a id="_idTextAnchor301"/>Technical requirements</h1>
<p>For this chapter, you will need to make sure you have Visual Studio 2022 installed along with the Vulkan SDK. Basic familiarity with the C++ programming language and an understanding of ray tracing concepts would be useful. Please revisit <a href="B18491_01.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a><em class="italic">, Vulkan Core Concepts,</em> for details about setting up and building the code in the repository. We also assume that by now you are familiar with the Vulkan API and various concepts that were introduced in previous chapters. This chapter has multiple recipes, which can be launched using the following executables:</p>
<ol>
<li><code>Chapter07_RayTracer.exe</code></li>
<li><code>Chapter07_HybridRenderer.exe</code></li>
</ol>
<p>The code files for this chapter can be found here: <a href="https://github.com/PacktPublishing/The-Modern-Vulkan-Cookbook">https://github.com/PacktPublishing/The-Modern-Vulkan-Cookbook</a>.</p>
<h1 id="_idParaDest-265"><a id="_idTextAnchor302"/>Implementing a GPU ray tracer</h1>
<p>Ray tracing is a rendering technique that simulates the physical behavior of light to generate highly <a id="_idIndexMarker502"/>realistic graphics. Ray tracing works by tracing the path of light from a pixel in the image sensor back to its source. Each ray of light can interact with the objects in the scene, causing a variety of effects such as reflection, refraction, or absorption. This allows for the creation of realistic shadows, reflections, and light dispersion effects in complex 3D scenes. In previous chapters, specifically <a href="B18491_04.xhtml#_idTextAnchor241"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Techniques for Lighting, Shading, and Shadows</em>, we explored <strong class="bold">rasterization</strong>. It takes a more direct approach, converting <a id="_idIndexMarker503"/>3D polygons that make up a scene directly into a 2D image. It essentially fills in the pixels of each polygon based on its color and texture. On the other hand, ray tracing simulates the path of light rays from the camera to the scene, accounting for how these rays interact with the scene’s objects.</p>
<p>Before we delve into the specifics of how ray tracing is implemented in Vulkan, it is beneficial to gain an understanding of how ray tracing operates, along with several fundamental <a id="_idIndexMarker504"/>concepts such as the <strong class="bold">bidirectional reflectance distribution function</strong> (<strong class="bold">BRDF</strong>), radiance, and irradiance. These concepts play a crucial role in determining how light interacts with surfaces in a scene and subsequently influences the final rendered image.</p>
<p>To simplify understanding, let’s break down the flow of the ray tracing algorithm, as depicted in <em class="italic">Figure 7</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 7.1 – Ray tracing algorithm" src="img/B18491_07_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Ray tracing algorithm</p>
<p>In the <a id="_idIndexMarker505"/>following section, we will outline the fundamental principles of the ray tracing algorithm:</p>
<ol>
<li>For each pixel on the screen, a ray is projected from the viewpoint or “eye” into the scene. This is the initial step in ray tracing and sets the stage for further calculations.</li>
<li>The algorithm then calculates the point of intersection between the ray and the objects within the scene. It identifies the closest object that is hit by the ray, along with the exact hit point on the object’s geometry.</li>
<li>Once the intersection point is determined, shading is performed at the hit point. The color and lighting information calculated at this point is added to the radiance value for the pixel, which contributes to the final color of the pixel in the rendered image.</li>
<li>The ray doesn’t stop at the first hit. It can continue to propagate due to phenomena such as reflection or refraction. The rays resulting from reflection or refraction are assigned a throughput value, which represents the remaining energy of the light.</li>
<li>The recursive process can potentially go on indefinitely, which is computationally <a id="_idIndexMarker506"/>expensive. To handle this, techniques such as <strong class="bold">Russian Roulette</strong> are used. In Russian Roulette, the recursion is probabilistically terminated based on the remaining energy in the ray. If the ray’s energy falls below a certain threshold, it has a certain chance of being terminated early, which helps to control the computational cost of the algorithm.</li>
<li>Now that <a id="_idIndexMarker507"/>we understand how the ray tracing algorithm functions, it’s beneficial to delve into the principles of <strong class="bold">radiometry</strong>. Radiometry is <a id="_idIndexMarker508"/>a branch of physics that quantifies light’s behavior, providing a series of methods and units to describe and measure different aspects of light in a scene. Several key concepts that are fundamental to understanding radiometry include radiant intensity, irradiance, and radiance. The following diagram (<em class="italic">Figure 7</em><em class="italic">.2</em>) can help you remember these concepts:</li>
</ol>
<div><div><img alt="Figure 7.2 – Radiometry basics" src="img/B18491_07_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Radiometry basics</p>
<ul>
<li><strong class="bold">Radiant intensity</strong>: This is <a id="_idIndexMarker509"/>a measure of the power <a id="_idIndexMarker510"/>of light emitted, or radiant flux, per unit solid angle, typically <a id="_idIndexMarker511"/>measured in <strong class="bold">watts per steradian</strong> (<strong class="bold">W/sr</strong>). The steradian, analogous to the radian in angular measure, quantifies solid angles in 3D space. In the context of ray tracing, it serves as a pivotal unit when calculating radiant intensity, capturing how light spreads across surfaces within the simulated environment. It is directional in nature, meaning it varies depending on the direction from which the light is observed.</li>
<li><strong class="bold">Irradiance</strong>: Irradiance <a id="_idIndexMarker512"/>measures the power of radiant flux incident upon a surface <a id="_idIndexMarker513"/>per unit area, typically measured in <strong class="bold">watts per square meter</strong> (<strong class="bold">W/m²</strong>). In the context of ray tracing, irradiance <a id="_idIndexMarker514"/>is used to calculate the amount of light energy striking a surface, which is then used for shading calculations. It plays a key role in determining how bright an object appears in the scene.</li>
<li><strong class="bold">Radiance</strong>: Radiance <a id="_idIndexMarker515"/>refers to how much light is either coming from a <a id="_idIndexMarker516"/>specific area or passing through it, considering the particular direction or viewpoint from which the light is observed. It is used to describe the amount of light that reaches the camera from a <a id="_idIndexMarker517"/>specific point in the scene, through a specific direction. It’s measured in <strong class="bold">watts per square meter per steradian</strong> (<strong class="bold">W/m²/sr</strong>). Radiance is a critical concept in ray tracing as it integrates both directional and positional information, helping to generate accurate shading and lighting effects.</li>
</ul>
<p>As the next step, we will <a id="_idIndexMarker518"/>learn a bit about the <strong class="bold">rendering equation</strong> used in ray tracing. The equation essentially depicts that the light leaving a point in a certain direction is equal to the light emitted by the point in that direction plus the light reflected by the point in that direction. The reflected light is integral over all directions of incoming light, where each incoming direction is weighted by the BRDF and the cosine of the angle between the incoming light and the surface normal. The link provided below <a id="_idIndexMarker519"/>offers a simplified explanation of the rendering equation: <a href="https://twitter.com/KostasAAA/status/1379918353553371139/photo/1">https://twitter.com/KostasAAA/status/1379918353553371139/photo/1</a>.</p>
<ul>
<li>L o(x, ω o) is the total amount of radiance (light), leaving point xs(x, ω o) is the emitted light from the point x in the direction ω o. This term is usually only non-zero for light sources.</li>
<li>The term ∫ Ω  represents an integral over the entire hemisphere Ω above point x.</li>
<li>f r(x, ω i → ω o) is the BRDF at point x, which defines how much light is reflected off x in the direction ω o when light comes in from direction ω i.</li>
<li>L i(x, ω i) is the incoming light at point <em class="italic">p</em> from direction ω i.</li>
<li>ω i ∙ n is the <a id="_idIndexMarker520"/>cosine of the angle between ω i and the normal at point x. This accounts for the fact that light arriving at a shallow angle spread over a larger area. d ω i is a small amount of solid angle around direction ω i.</li>
</ul>
<h2 id="_idParaDest-266"><a id="_idTextAnchor303"/>Monte Carlo method</h2>
<p>Next, we will <a id="_idIndexMarker521"/>discuss the <strong class="bold">Monte Carlo method</strong>, which is a statistical technique that allows for numerical solutions to complex problems by <a id="_idIndexMarker522"/>performing repeated random sampling. Suppose you want to calculate the area under a curve described by the function f(x) = x 2 between x = 0 and x = 1. Mathematically, you’d solve this using calculus with an integral. However, imagine now that the function is extremely complex or has many variables, such that you can’t easily integrate it using standard calculus techniques. This is where the Monte Carlo method comes into play. Instead of trying to compute the integral exactly, we can estimate it using random sampling. In the case of ray tracing, the rendering equation, which models how light interacts with surfaces, is quite complex, especially because it involves an integral over all possible directions of incoming light. This is the reason Monte Carlo is used. Instead of trying to calculate the exact value of the integral, we can approximate it by randomly sampling directions of incoming light, evaluating the integrand for each of these samples, and then averaging the results. This process is repeated many times to get a more accurate estimate.</p>
<p>We briefly talked about the BRDF during the rendering equation; it tells us how light bounces off a surface. When light hits a surface, it doesn’t just bounce back in one direction but scatters in many directions. The BRDF gives us a way to predict this behavior. It considers two directions: the direction from which the light is coming, and the direction in which it’s going after it hits the surface.</p>
<p>Imagine the sun shining on the surface. The BRDF helps us figure out how much light from the sun is reflected off that surface and in what direction it goes. This is important for calculating the color and brightness that we see in a rendered image. Here’s where the concept of throughput or contribution comes in. It’s like a measure of how much light energy is retained or lost when the light bounces off the surface. Think of it as the efficiency of light reflection. We need to include this in our calculations to get accurate results.</p>
<p>The <strong class="bold">probability density function</strong> (<strong class="bold">PDF</strong>) is a statistical tool that helps us handle the randomness <a id="_idIndexMarker523"/>involved in these calculations. When light hits the surface, it can bounce off in many different directions, and the PDF helps us figure out the likelihood of each possible direction.</p>
<p><strong class="bold">Importance sampling</strong> is a technique used in ray tracing where we choose to send more rays in <a id="_idIndexMarker524"/>directions where the BRDF is high and fewer rays in directions where it is low. This helps us get a more accurate result with fewer rays, which can be computationally cheaper. However, since we’re sending more rays in certain directions and fewer in others, we’re biasing our sampling toward those directions. We divide our BRDF result by the PDF to get our result. The reason we divide the BRDF <a id="_idIndexMarker525"/>by a PDF is essentially to correct for bias <a id="_idIndexMarker526"/>that was introduced when we used importance sampling to choose the next direction in which to trace the ray.</p>
<p>In ray tracing, each light ray carries its own energy. Each time it bounces, we add the energy it carries times the BRDF to the overall brightness of the image.</p>
<p>In Vulkan, ray tracing is implemented through a series of distinct shader stages. In this recipe, we will guide you through the process of implementing a GPU ray tracer with Vulkan, providing a step-by-step walkthrough on how to set up each shader stage involved in the ray tracing process. By the end of this recipe, you’ll be able to create your own ray tracer that can produce highly realistic graphics by accurately simulating the behavior of light.</p>
<p>The shader stages include the following:</p>
<ul>
<li><strong class="bold">Ray generation shader</strong>: This <a id="_idIndexMarker527"/>is the starting point of the ray tracing process</li>
<li><strong class="bold">Intersection shader</strong>: This <a id="_idIndexMarker528"/>shader calculates how rays intersect with the scene’s geometry</li>
<li><strong class="bold">Miss and hit shaders</strong>: These <a id="_idIndexMarker529"/>define how rays behave when they hit or miss an object</li>
</ul>
<p>By understanding and implementing each of these stages, you’ll be well on your way to creating visually stunning and realistic graphics.</p>
<h2 id="_idParaDest-267"><a id="_idTextAnchor304"/>Getting ready</h2>
<p>The ray tracing <a id="_idIndexMarker530"/>pipeline in Vulkan is made up of six stages: ray generation, intersection, any-hit, closest hit, miss, and callable. <em class="italic">Figure 7</em><em class="italic">.3</em> shows the stages and <a id="_idIndexMarker531"/>their general layout in the pipeline. Another key component of Vulkan ray tracing is <strong class="bold">acceleration structure</strong>. This structure is pivotal in efficiently handling the large amount of geometric data involved in ray tracing. The role of the acceleration structure is to organize data in a way that allows for rapid ray tracing <a id="_idIndexMarker532"/>calculations. <strong class="bold">Bounding volume hierarchy</strong><strong class="bold"> </strong>(<strong class="bold">BVH</strong>) is an algorithmic tree structure on a set of geometric objects. All geometric objects are wrapped in bounding volumes that form the leaf nodes of the tree. These nodes are then paired, bounded, and connected to form a parent node. This process continues up the tree until there is only one bounding volume remaining: the root of the tree. This structure allows the ray tracing algorithm to efficiently discard many objects that the ray cannot intersect, thereby speeding up the process significantly.</p>
<p>The acceleration structure is divided into two levels: the <strong class="bold">bottom level acceleration structures</strong> (<strong class="bold">BLASs</strong>) and the <strong class="bold">top level acceleration </strong><strong class="bold">structures</strong> (<strong class="bold">TLASs</strong>):</p>
<ul>
<li><strong class="bold">BLAS</strong>: BLASs <a id="_idIndexMarker533"/>are responsible for storing the geometric data for individual objects in the scene. Each object can have one or more BLAS associated with it, and each BLAS can contain one or more geometric primitives, such as triangles or instances of other BLASs. The BLAS is responsible for determining how rays intersect with the geometry they contains, making it a fundamental part of the ray tracing process.</li>
<li><strong class="bold">TLAS</strong>: The <a id="_idIndexMarker534"/>TLAS, on the other hand, does not contain geometric data. Instead, it contains instances of BLASs. Each instance defines a transformation (such as translation, rotation, or scaling) and a BLAS to apply it to. When ray tracing, the system starts from the TLAS and works its way down to the appropriate BLAS. The TLAS essentially acts as a directory that guides the system to the correct BLAS based on the ray’s path.</li>
</ul>
<div><div><img alt="Figure 7.3 – Ray tracing pipeline and its stages" src="img/B18491_07_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Ray tracing pipeline and its stages</p>
<p>The shader <a id="_idIndexMarker535"/>stages are as follows:</p>
<ol>
<li><code>traceRayExt</code> function. These rays are what will eventually interact with the objects in the scene to create the final rendered image.</li>
<li><strong class="bold">Acceleration structure traversal</strong>: The acceleration structure is a key component in optimizing the process of ray tracing. It functions as a scene management tree, akin to a BVH. Its primary use is to speed up collision detection between rays and objects within the scene. This part of the pipeline is fixed, meaning Vulkan has already implemented the logic behind it.</li>
<li><strong class="bold">Intersection stage</strong>: As the rays traverse the BVH, they may call upon an intersection shader. This shader is particularly useful when dealing with custom types but isn’t necessary when using the default triangle mesh primitive. This is because Vulkan has already incorporated the logic required for these default primitives, thereby bypassing the need for the intersection shader.</li>
<li><strong class="bold">Any-hit stage</strong>: This stage processes intersection events found in the intersection stage; in case of an intersection, the any-hit shader is invoked. The any-hit shader <a id="_idIndexMarker536"/>determines the subsequent steps after the intersection of light and material occurs, such as whether to abandon the intersection and so on. Depending on the specific requirements, the intersection point can be discarded, at which point it is considered that no intersection took place. This process is then returned to the BLAS traversal.</li>
<li><strong class="bold">Closest hit stage</strong>: This shader is responsible for processing the intersection that is currently the closest to the ray origin and has not yet been discarded by an any-hit stage. It typically involves applying light and material calculations to render the final color of the pixel.</li>
<li><strong class="bold">Miss stage</strong>: The miss stage determines how to handle the light in the event the ray doesn’t hit anything. This could involve assigning a default color, environment color, and so on.</li>
<li><strong class="bold">Callable stage</strong>: This stage may be called from any other stage (from their shader code).</li>
</ol>
<p>In the repository, the ray tracing code is encapsulated in the <code>RayTracer</code> class.</p>
<h2 id="_idParaDest-268"><a id="_idTextAnchor305"/>How to do it...</h2>
<p>As a first step, we will <a id="_idIndexMarker537"/>look at the code that needs to be executed on the host side; most of the implementation is inside the <code>RayTracer</code> class:</p>
<ol>
<li>The first critical step in setting up ray tracing with Vulkan is to verify if our physical device (GPU) supports ray tracing features. This verification is achieved by adding <code>VkPhysicalDeviceRayTracingPipelineFeaturesKHR</code> and <code>VkPhysicalDeviceAccelerationStructureFeaturesKHR</code> to the list of physical features that are checked for support. This feature-checking operation is implemented in the <code>PhysicalDevice</code> class. Here, these specific features are added to the <code>VkPhysicalDeviceFeatures2</code> structure’s chain, serving as a mechanism to query the support for a set of features. This class also supplies the <code>isRayTracingSupported</code> function, which is utilized to activate the necessary features for ray tracing during the Vulkan device’s creation process. In the <code>Context</code> class, we introduce specific features for <code>VkPhysicalDeviceAccelerationStructureFeaturesKHR</code> and <code>VkPhysicalDeviceRayTracingPipelineFeaturesKHR</code>. However, these features are only activated during the construction of the Vulkan device if the demo application has ray tracing enabled and the physical device confirms its support for these features. Please also note that the demo application will only run if your GPU supports Vulkan ray tracing.</li>
<li>Next, we create <a id="_idIndexMarker538"/>shader modules for each of the shaders that will be used by the ray tracing pipeline:<pre class="source-code">
auto rayGenShader = context_-&gt;createShaderModule(
  (resourcesFolder / "raytrace_raygen.rgen")
    .string(),
  VK_SHADER_STAGE_RAYGEN_BIT_KHR,
  "RayTracer RayGen Shader");
auto rayMissShader = context_-&gt;createShaderModule(
  (resourcesFolder / "raytrace_miss.rmiss")
    .string(),
  VK_SHADER_STAGE_MISS_BIT_KHR,
  "RayTracer Miss Shader");
auto rayMissShadowShader =
  context_-&gt;createShaderModule(
    (resourcesFolder / "raytrace_shadow.rmiss")
      .string(),
    VK_SHADER_STAGE_MISS_BIT_KHR,
    "RayTracer Miss Shadow Shader");
auto rayClosestHitShader =
  context_-&gt;createShaderModule(
    (resourcesFolder /
     "raytrace_closesthit.rchit")
      .string(),
    VK_SHADER_STAGE_CLOSEST_HIT_BIT_KHR,
    "RayTracer Closest hit Shader");</pre></li> <li>Next, we <a id="_idIndexMarker539"/>create a ray tracing pipeline by calling <code>Pipeline::createRayTracingPipeline()</code>. To facilitate creating a ray tracing pipeline, we added a helper structure, called <code>Pipeline::RayTracingPipelineDescriptor</code>, which stores the descriptor sets and their bindings (just like in the graphics and compute pipelines descriptors), and all shaders required to create a ray tracing pipeline. An instance of this structure must be passed to the constructor of the <code>VulkanCore::Pipeline</code> class:<pre class="source-code">
struct RayTracingPipelineDescriptor {
  std::vector&lt;SetDescriptor&gt; sets_;
  std::weak_ptr&lt;ShaderModule&gt; rayGenShader_;
  std::vector&lt;std::weak_ptr&lt;ShaderModule&gt;&gt;
    rayMissShaders_;
  std::vector&lt;std::weak_ptr&lt;ShaderModule&gt;&gt;
    rayClosestHitShaders_;
  std::vector&lt;VkPushConstantRange&gt; pushConstants_;
};</pre><p class="list-inset">A ray tracing pipeline in Vulkan necessitates an array of shader group structures. Rather than holding a list of shaders, each <code>VkRayTracingShaderGroupCreateInfoKHR</code>) structure:</p><pre class="source-code">typedef struct <strong class="bold">VkRayTracingShaderGroupCreateInfoKHR</strong> {
  VkStructureType sType;
  const void *pNext;
  VkRayTracingShaderGroupTypeKHR type;
  uint32_t generalShader;
  uint32_t closestHitShader;
  uint32_t anyHitShader;
  uint32_t intersectionShader;
  const void *pShaderGroupCaptureReplayHandle;
} VkRayTracingShaderGroupCreateInfoKHR;</pre><p class="list-inset">The structure contains fields to specify shaders for only four different stages of the pipeline (<code>generalShader</code>, <code>closestHitShader</code>, <code>anyHitShader</code>, and <code>intersectionShader</code>). That’s because the indices of shaders for the miss and callable stages are provided in the <code>generalShader</code> field. It’s important to note that the function of these fields depends on the value of the <code>type</code> member in the structure.</p></li> <li>For the sake of brevity, we show here only the creation of one shader stage and one shader group. The other shader modules passed along with the pipeline descriptor are grouped into their own shader groups. In the provided code snippet, we demonstrate the construction of a shader group specifically for a ray generation shader. It’s crucial to understand that each type of shader utilized in Vulkan ray tracing requires its own individual shader group. It’s necessary <a id="_idIndexMarker541"/>because the design of the Vulkan ray tracing pipeline is such that it allows for different types of shaders to operate independently, each performing a unique task in the ray tracing process. By structuring each type of shader in its own shader group, we ensure that the corresponding tasks are executed independently and efficiently, aiding in the parallel computation capabilities of the GPU. Please refer to the code in <code>Pipeline::createrayTracingPipeline()</code> for more details:<pre class="source-code">
std::vector&lt;VkPipelineShaderStageCreateInfo&gt;
  shaderStages;
std::vector&lt;VkRayTracingShaderGroupCreateInfoKHR&gt;
  shaderGroups;
const VkPipelineShaderStageCreateInfo
  rayGenShaderInfo{
    .sType =
      VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO,
    .stage = rayGenShader-&gt;vkShaderStageFlags(),
    .module = rayGenShader-&gt;vkShaderModule(),
    .pName = rayGenShader-&gt;entryPoint().c_str(),
  };
shaderStages.push_back(rayGenShaderInfo);
const VkRayTracingShaderGroupCreateInfoKHR
  shaderGroup{
    .sType =
      VK_STRUCTURE_TYPE_RAY_TRACING_SHADER_GROUP_CREATE_INFO_KHR,
    .type =
      VK_RAY_TRACING_SHADER_GROUP_TYPE_GENERAL_KHR,
    .generalShader =
      static_cast&lt;uint32_t&gt;(shaderStages.size()) -
      1,
    .closestHitShader = VK_SHADER_UNUSED_KHR,
    .anyHitShader = VK_SHADER_UNUSED_KHR,
    .intersectionShader = VK_SHADER_UNUSED_KHR,
  };
shaderGroups.push_back(shaderGroup);</pre></li> <li>Finally, here’s <a id="_idIndexMarker542"/>how you create a ray tracing pipeline:<pre class="source-code">
VkRayTracingPipelineCreateInfoKHR rayTracingPipelineInfo{
      .sType = VK_STRUCTURE_TYPE_RAY_TRACING_PIPELINE_CREATE_INFO_KHR,
      .stageCount = static_cast&lt;uint32_t&gt;(shaderStages.size()),
      .pStages = shaderStages.data(),
      .groupCount = static_cast&lt;uint32_t&gt;(shaderGroups.size()),
      .pGroups = shaderGroups.data(),
      .maxPipelineRayRecursionDepth = 10,
      .layout = vkPipelineLayout_,
  };
  VK_CHECK(vkCreateRayTracingPipelinesKHR(context_-&gt;device(), VK_NULL_HANDLE,
                                          VK_NULL_HANDLE, 1, &amp;rayTracingPipelineInfo,
                                          nullptr, &amp;vkPipeline_));</pre></li> <li>The next <a id="_idIndexMarker543"/>step is to create the <code>VkStridedDeviceAddressRegionKHR</code> structure that describes the location and structure of the SBT in memory:<pre class="source-code">
struct SBT {
  std::shared_ptr&lt;VulkanCore::Buffer&gt; buffer;
  VkStridedDeviceAddressRegionKHR sbtAddress;
};</pre></li> <li>The <code>createShaderBindingTable()</code> function is where the SBT is created. This function begins by defining several variables to store the sizes and counts of various shader types in the application. In the code, <code>handleSize</code> and <code>handleSizeAligned</code> represent the size of a single shader group handle in the SBT, with the latter ensuring correct memory alignment:<pre class="source-code">
void EngineCore::RayTracer::
  createShaderBindingTable() {
  const uint32_t handleSize =
    context_-&gt;physicalDevice()
      .rayTracingProperties()
      .shaderGroupHandleSize;
  const uint32_t handleSizeAligned =
    alignedSize(context_-&gt;physicalDevice()
                  .rayTracingProperties()
                  .shaderGroupHandleSize,
                context_-&gt;physicalDevice()
                  .rayTracingProperties()
                  .shaderGroupHandleAlignment);</pre><p class="list-inset"><code>numRayGenShaders</code>, <code>numRayMissShaders</code>, and <code>numRayClosestHitShaders</code> represent the number of each type of shader used in <a id="_idIndexMarker545"/>the pipeline. Next, we calculate the total size of the SBT (<code>sbtSize</code>) and create a <code>shaderHandleStorage</code> vector to store the shader handles:</p><pre class="source-code">const uint32_t numRayGenShaders = 1;
const uint32_t numRayMissShaders =
  2; // 1 for miss and 1 for shadow
const uint32_t numRayClosestHitShaders = 1;
const uint32_t numShaderGroups =
  numRayGenShaders + numRayMissShaders +
  numRayClosestHitShaders;
const uint32_t groupCount =
  static_cast&lt;uint32_t&gt;(numShaderGroups);
const uint32_t sbtSize =
  groupCount * handleSizeAligned;</pre></li> <li>The <code>vkGetRayTracingShaderGroupHandlesKHR</code> Vulkan function is then called to retrieve the shader group handles. These handles are unique identifiers <a id="_idIndexMarker546"/>for the shader groups in the pipeline. Afterward, we create separate buffers for each shader type (<code>copyDataToBuffer</code> method is called to copy the relevant shader handles from <code>shaderHandleStorage</code> into the buffer. We recommend looking at the <code>createShaderBindingTable</code> function:<pre class="source-code">
std::vector&lt;uint8_t&gt; shaderHandleStorage(sbtSize);
VK_CHECK(vkGetRayTracingShaderGroupHandlesKHR(
  context_-&gt;device(), pipeline_-&gt;vkPipeline(), 0,
  groupCount, sbtSize,
  shaderHandleStorage.data()));</pre></li> <li>Each buffer and its respective <code>VkStridedDeviceAddressRegionKHR</code> need to be filled. Here, we only show how ray generation is populated. The other groups follow a similar pattern:<pre class="source-code">
raygenSBT_.buffer = context_-&gt;createBuffer(
  context_-&gt;physicalDevice()
      .rayTracingProperties()
      .shaderGroupHandleSize *
    numRayGenShaders,
  VK_BUFFER_USAGE_SHADER_BINDING_TABLE_BIT_KHR |
    VK_BUFFER_USAGE_SHADER_DEVICE_ADDRESS_BIT,
  VMA_MEMORY_USAGE_CPU_ONLY, "RayGen SBT Buffer");
raygenSBT_.sbtAddress.deviceAddress =
  raygenSBT_.buffer-&gt;vkDeviceAddress();
raygenSBT_.sbtAddress.size =
  handleSizeAligned * numRayGenShaders;
raygenSBT_.sbtAddress.stride = handleSizeAligned;
raygenSBT_.buffer-&gt;copyDataToBuffer(
  shaderHandleStorage.data(),
  handleSize *numRayGenShaders);</pre></li> <li>Next, we need to load the environment map along with its acceleration structure. The <code>RayTracer::loadEnvMap()</code> method performs the loading of the <a id="_idIndexMarker547"/>environment map and the creation of the <a id="_idIndexMarker548"/>acceleration structure. It loads a <code>context_-&gt;createTexture()</code>. It then calls <code>createEnvironmentAccel()</code>, which is responsible for creating an acceleration data structure for importance sampling of the environment map. This function computes a vector of <code>EnvAccel</code> structures, one for each texel of the map. This data is uploaded to a device-only buffer.</li>
<li>Next, we create TLASs and BLASs with the <code>RayTracer::initBottomLevelAccelStruct()</code> and <code>RayTracer::initTopLevelAccelStruct()</code> methods.<p class="list-inset">In the following steps, you will learn how to set up BLAS using Vulkan:</p><ol><li class="Alphabets"><code>VK_GEOMETRY_TYPE_TRIANGLES_KHR</code>) with vertices and indices from the model’s buffers:</li></ol><pre class="source-code">
VkAccelerationStructureGeometryKHR
  accelerationStructureGeometry{
    .sType =
      VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_GEOMETRY_KHR,
    .geometryType = VK_GEOMETRY_TYPE_TRIANGLES_KHR,
    .geometry = {
        .triangles = {
            .sType =
              VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_GEOMETRY_TRIANGLES_DATA_KHR,
            .vertexFormat =
              VK_FORMAT_R32G32B32_SFLOAT,
            .vertexData = vertexBufferDeviceAddress,
            .vertexStride =
              sizeof(EngineCore::Vertex),
            .maxVertex = numVertices,
            .indexType = VK_INDEX_TYPE_UINT32,
            .indexData = indexBufferDeviceAddress,
          },
      },
  };</pre><ol><li class="Alphabets" value="2"><code>vkGetAccelerationStructureBuildSizesKHR</code> function call returns the size information needed to allocate the acceleration structure and the <a id="_idIndexMarker550"/>build scratch buffer. The <code>bLAS_[meshIdx].buffer</code> buffer is populated as a direct result of the <code>vkGetAccelerationStructureBuildSizesKHR</code> function call:</li></ol><pre class="source-code">VkAccelerationStructureBuildSizesInfoKHR
  accelerationStructureBuildSizesInfo{
    .sType =
      VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_BUILD_SIZES_INFO_KHR,
  };
vkGetAccelerationStructureBuildSizesKHR(
  context_-&gt;device(),
  VK_ACCELERATION_STRUCTURE_BUILD_TYPE_DEVICE_KHR,
  &amp;accelerationStructureBuildGeometryInfo,
  &amp;numTriangles,
  &amp;accelerationStructureBuildSizesInfo);</pre><ol><li class="Alphabets" value="3"><code>bLAS_[meshIdx].buffer</code>, which is used to store the BLAS structure. We then create a structure of the <code>VkAccelerationStructureCreateInfoKHR</code> type, to which we provide the buffer just created, its size, and specify that it’s a BLAS. Next, we call <code>vkCreateAccelerationStructureKHR</code> to create the actual acceleration structure and store the handle to it in <code>bLAS_[meshIdx].handle</code>. We created a temporary buffer named <code>tempBuffer</code> to store the temporary data needed when building the acceleration structure. When you are building an acceleration structure in Vulkan, the build procedure often needs some temporary space to perform its calculations. This temporary space is also referred to as a scratch buffer. We then fill a <code>VkAccelerationStructureBuildGeometryInfoKHR</code> structure with the details of the acceleration structure build, including the handle of the acceleration structure, the geometry, and the device address of <code>tempBuffer</code>. Next, we create a <code>VkAccelerationStructureBuildRangeInfoKHR</code> structure to specify the <a id="_idIndexMarker551"/>range of geometries to be used in the build. The <code>vkCmdBuildAccelerationStructuresKHR</code> function records the command to build the acceleration structure into the command buffer:</li></ol><pre class="source-code">// Creating buffer to hold the acceleration structure
bLAS_[meshIdx].buffer = context_-&gt;createBuffer(...);
// Creating acceleration structure
VkAccelerationStructureCreateInfoKHR accelerationStructureCreateInfo{
    .sType = VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_CREATE_INFO_KHR,
    .buffer = bLAS_[meshIdx].buffer-&gt;vkBuffer(),
    .size = accelerationStructureBuildSizesInfo.accelerationStructureSize,
    .type = VK_ACCELERATION_STRUCTURE_TYPE_BOTTOM_LEVEL_KHR};
VK_CHECK(vkCreateAccelerationStructureKHR(context_-&gt;device(),
                                          &amp;accelerationStructureCreateInfo, nullptr,
                                          &amp;bLAS_[meshIdx].handle));
// Creating temporary buffer
auto tempBuffer = context_-&gt;createBuffer(...);
// Setting up geometry and build range info for acceleration structure
VkAccelerationStructureBuildGeometryInfoKHR accelerationBuildGeometryInfo{
    .sType = VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_BUILD_GEOMETRY_INFO_KHR,
    .dstAccelerationStructure = bLAS_[meshIdx].handle,
    .scratchData = {.deviceAddress = tempBuffer-&gt;vkDeviceAddress()}};
VkAccelerationStructureBuildRangeInfoKHR accelerationStructureBuildRangeInfo{
    .primitiveCount = numTriangles};
// Building acceleration structure
const auto commandBuffer = commandQueueMgr.getCmdBufferToBegin();
vkCmdBuildAccelerationStructuresKHR(
    commandBuffer, 1, &amp;accelerationBuildGeometryInfo,
    &amp;accelerationStructureBuildRangeInfo);</pre></li> <li>In the following steps, you will learn how to set up a TLAS in Vulkan:<ol><li class="Alphabets"><strong class="bold">Creating the acceleration structure instances</strong>: The following loop creates the instances, each of which references a BLAS. Instances contain information <a id="_idIndexMarker552"/>about the transformation matrix, mask, flags, and the device address of the BLAS it references:</li></ol><pre class="source-code">
for (int meshIdx = 0; meshIdx &lt; model-&gt;meshes.size();
     ++meshIdx) {
  VkAccelerationStructureInstanceKHR instance{};
  ...
  instance.accelerationStructureReference =
    bLAS_[meshIdx].buffer-&gt;vkDeviceAddress();
  accelarationInstances_.push_back(instance);
}</pre><ol><li class="Alphabets" value="2"><code>VK_GEOMETRY_TYPE_INSTANCES_KHR</code>):</li></ol><pre class="source-code">VkAccelerationStructureGeometryKHR
  accelerationStructureGeometry{
  .sType = VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_GEOMETRY_KHR,
  .geometryType = VK_GEOMETRY_TYPE_INSTANCES_KHR,
  .geometry = {
    .instances = {
      .sType = VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_GEOMETRY_INSTANCES_DATA_KHR,
      .data = instanceDataDeviceAddress,
    },
  },
  .flags = VK_GEOMETRY_OPAQUE_BIT_KHR,
};</pre><ol><li class="Alphabets" value="3"><code>vkGetAccelerationStructureBuildSizesKHR</code> function call returns <a id="_idIndexMarker553"/>the size information needed to allocate the acceleration structure and the build scratch buffer:</li></ol><pre class="source-code">VkAccelerationStructureBuildSizesInfoKHR
  accelerationStructureBuildSizesInfo{
    .sType =
      VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_BUILD_SIZES_INFO_KHR,
  };
vkGetAccelerationStructureBuildSizesKHR(
  context_-&gt;device(),
  VK_ACCELERATION_STRUCTURE_BUILD_TYPE_DEVICE_KHR,
  &amp;accelerationStructureBuildGeometryInfo,
  &amp;primitiveCount,
  &amp;accelerationStructureBuildSizesInfo);</pre><ol><li class="Alphabets" value="4"><code>VK_ACCELERATION_STRUCTURE_TYPE_TOP_LEVEL_KHR</code> in the type field since we are building a TLAS. The final part is to record the <code>vkCmdBuildAccelerationStructuresKHR</code> command on the command buffer which is executed when the command buffer is submitted:</li></ol><pre class="source-code">VkAccelerationStructureCreateInfoKHR
  accelerationStructureCreateInfo{
    .sType =
      VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_CREATE_INFO_KHR,
    .buffer = tLAS_.buffer-&gt;vkBuffer(),
    .size = accelerationStructureBuildSizesInfo
              .accelerationStructureSize,
    .type =
      VK_ACCELERATION_STRUCTURE_TYPE_TOP_LEVEL_KHR,
  };
VK_CHECK(vkCreateAccelerationStructureKHR(
  context_-&gt;device(),
  &amp;accelerationStructureCreateInfo, nullptr,
  &amp;tLAS_.handle));
vkCmdBuildAccelerationStructuresKHR(
  commandBuffer, 1, &amp;accelerationBuildGeometryInfo,
  accelerationBuildStructureRangeInfos.data());</pre></li> <li>We also <a id="_idIndexMarker554"/>create a <code>RayTraced</code> storage image (encapsulated in <code>initRayTracedStorageImages</code>) and bind resources by calling <code>bindResource</code> on the pipeline during the initialization.</li>
<li>To execute a ray tracer, we need to call <code>RayTracer::execute()</code>, which is responsible for copying the camera data, binding the pipeline, and calling <code>vkCmdTraceRaysKHR</code>. This Vulkan function launches the <code>RayGen</code> shader.</li>
</ol>
<p>Now that we have understood the steps on the host side, it’s time to understand the device-side code. The device-side code for ray tracing is implemented using several shaders including <code>raytrace_raygen.rgen</code>, <code>raytrace_miss.rmiss</code>, <code>raytrace_closesthit.rchit</code>, and <code>raytrace_shadow.rmiss</code>:</p>
<ol>
<li>The process begins with the invocation of the <code>raytrace_raygen.rgen</code> shader via <code>vkCmdTraceRaysKHR</code>. In the following shader code block, the ray tracing process initiates by generating rays for each pixel sample through a unique <a id="_idIndexMarker555"/>seed for randomness. These rays, defined by their origin and direction, are traced into the scene within a loop until either the maximum number of bounces is reached, or an exit condition is met in the payload. The payload carries essential information such as origin, direction, and bounce index. Once the final color for the pixel is calculated using an average from all samples, it is stored in the output image. If temporal accumulation is applied, the shader retrieves and adds colors from previous frames. Temporal accumulation is beneficial in ray tracing as it helps to reduce noise and improve image quality. Accumulating or averaging the color samples over multiple frames effectively increases the number of rays traced per pixel without the cost of tracing extra rays in a single frame:<pre class="source-code">
void main() {
  // Ray Generation
  // tea refers to Tiny Encryption Algorithm, used to generate a unique and reproducible seed for each task and frame.
  uint seed =
    tea(gl_LaunchIDEXT.y * gl_LaunchIDEXT.x +
          gl_LaunchIDEXT.x,
        camProps.frameId);
  vec3 finalOutColor = vec3(0);
  vec2 pixelCenter = vec2(gl_LaunchIDEXT.xy) +
                     vec2(0.5) +
                     vec2(rand(seed), rand(seed));
  vec4 target =
    camProps.projInverse *
    vec4(pixelCenter / vec2(gl_LaunchSizeEXT.xy) *
             2.0 -
           1.0,
         1, 1);
  vec4 direction =
    camProps.viewInverse *
    vec4(normalize(target.xyz / target.w), 0);
  // Initial Payload Setup
  rayPayload.currentBounceIndex = 0;
  rayPayload.exit = false;
  rayPayload.origin =
    (camProps.viewInverse * vec4(0, 0, 0, 1)).xyz;
  rayPayload.direction = direction.xyz;
  // Ray Tracing Loop
  for (int j = 0; j &lt; MAX_BOUNCES; ++j) {
    rayPayload.currentBounceIndex = j;
  // Traces a ray using a culling mask of 0xff to include all potential intersections.
    traceRayEXT(topLevelAccelStruct,
                gl_RayFlagsOpaqueEXT, 0xff, 0, 0, 0,
                rayPayload.origin.xyz, 0.001,
                rayPayload.direction.xyz, 10000.0, 0);
    if (rayPayload.exit)
      break;
  }
  // Final Color Calculation and Image Store
  finalOutColor += rayPayload.radiance / MAX_SAMPLES;
  imageStore(outputImage, ivec2(gl_LaunchIDEXT.xy),
             vec4(linear2sRGB(finalOutColor), 0.0));
}</pre></li> <li>The <code>raytrace_miss.rmiss</code> shader is pretty simple: it is invoked if the ray doesn’t intersect any object. In such scenarios, the shader takes a sample from the environment map, determining the color based on the point of interaction between <a id="_idIndexMarker556"/>the ray and the environment. The <code>envMapColor</code> function takes a 3D direction vector as an input, normalizes it, and converts it into spherical coordinates (theta and phi). It then maps these coordinates onto a 2D plane (UV) and retrieves the corresponding color from the environment map texture. The following code block simply calls the <code>envMapColor</code> function to get the radiance for the current ray payload:<pre class="source-code">
void main() {
  rayPayload.radiance =
    envMapColor(gl_WorldRayDirectionEXT);
  rayPayload.exit = true;
}</pre></li> <li>The <code>raytrace_closesthit.rchit</code> shader is where most of the magic, including a calculation for shading and determining the subsequent direction of the ray, happens:<ol><li class="Alphabets">The initial stage of the process involves extracting the vertex and material data for the mesh that has been struck by the ray. This is achieved by utilizing the <code>gl_InstanceID</code> and <code>gl_PrimitiveID</code> variables, which are populated with the relevant data by the intersection shader. The hit shader also <a id="_idIndexMarker557"/>provides access to <code>hitAttributeEXT vec2 attribs</code>. In the context of triangles, these attributes represent the barycentric coordinates of the intersection point. Barycentric coordinates are a form of coordinate system used to specify the position of a point within a triangle. They are particularly useful in computer graphics because they allow for easy interpolation across a triangle. By using these coordinates, we can interpolate the positions of the vertices to determine the precise point of intersection within the triangle where the ray has made contact. Please refer to the code in <code>raytrace_closesthit.rchit</code> to understand how we use barycentric coordinates to get world space position.</li><li class="Alphabets">The next step is to call the <code>envSample()</code> function. This function is a crucial part of the ray tracing process, responsible for sampling the HDR environment map using importance sampling. The environment map is represented as a 2D texture (latitude-longitude format) and contains the illumination data of the surrounding environment. The function starts by uniformly picking a texel index in the environment map. It fetches the sampling data for that texel, which includes the ratio between the texel’s emitted radiance and the environment map’s average, the texel alias, and the distribution function values for that texel and its alias. The function then decides to either pick the texel directly or pick its alias based on a random variable and the intensity ratio. It computes the 2D integer coordinates of the chosen texel and uniformly samples the solid angle subtended by the pixel. The function converts the sampled UV coordinates to a direction in spherical coordinates, which is then converted to a light direction vector in Cartesian coordinates. This light direction vector is then returned along with the texel’s PDF:</li></ol><pre class="source-code">
vec3 envLightColor = vec3(0);
vec4 dirPdf =
  envSample(envLightColor, rayPayload.seed);
vec3 lightDir = dirPdf.xyz;
float lightPdf = dirPdf.w</pre><ol><li class="Alphabets" value="3">Shadow rays play a crucial role in the ray tracing process. They help in creating realistic lighting effects by determining which parts of the scene are in shadow, thus adding depth and realism to the rendered image. The next step is to <a id="_idIndexMarker558"/>trace a shadow ray from the intersection point towards the light source to check for any occluding objects. This is a critical step in determining whether a point is in shadow or not. The <code>inshadow</code> variable is declared using <code>layout(location = 1) rayPayloadEXT bool inshadow</code>. When a ray is traced in a ray tracing shader, it carries with it a payload. This payload can be used to store information that needs to be passed between different stages of the ray tracing pipeline, such as from the closest hit shader to the ray generation shader. The <code>inshadow</code> variable is a Boolean that is used to store the information of whether a particular point is in shadow. When the shadow ray (a ray traced from the intersection point towards the light) is occluded by another object, this variable will be set to true, indicating that the point is in shadow. Please be aware that in the <code>traceRayEXT</code> function, the 6th parameter is set to <code>1</code>. This value serves as an index to specify which miss shader should be invoked. In this context, it refers to the miss shader found in <code>raytrace_shadow.miss</code>:</li></ol><pre class="source-code">inshadow = true;
const int layoutLocation = 1;
// Trace the shadow ray
traceRayEXT(topLevelAccelStruct, rayFlags, cullMask,
            0, 0, 1, worldPosition, rayMinDist,
            lightDir, rayMaxDist, layoutLocation);</pre></li> </ol>
<p>The next step is responsible for lighting calculation using the <code>PbrEval</code> function evaluates the PBR model for a given set of inputs. It uses the material properties (such as base color, specular color, roughness, and metallic factors), <code>rayPayload.radiance</code> is the accumulated color or light contribution that the ray has gathered from all the light sources it has encountered up until now. <code>rayPayload.throughput</code> is a measure of how much light makes it through a certain path without being absorbed or scattered. Essentially, it’s a measure of the energy left of a light path. For details on PBR theory, please visit <a href="https://learnopengl.com/PBR/Theory">https://learnopengl.com/PBR/Theory</a>:</p>
<pre class="source-code">
if (!inshadow) {
  float pdf;
  // returns diffuse &amp; specular both
  vec3 F =
    PbrEval(eta, metallic, roughness, baseColor.rgb,
            specularColor, -rayPayload.direction, N,
            lightDir, pdf);
  float cosTheta = abs(dot(lightDir, N));
  float misWeight =
    max(0.0, powerHeuristic(lightPdf, pdf));
  if (misWeight &gt; 0.0) {
    directLightColor += misWeight * F * cosTheta *
                        envLightColor /
                        (lightPdf + EPS);
  }
}
rayPayload.radiance +=
  directLightColor * rayPayload.throughput;</pre> <ol>
<li class="Alphabets">The final <a id="_idIndexMarker562"/>part is to figure out the next ray direction as well as the throughput (energy left) for the next ray. It starts by sampling a direction for the next ray (<code>bsdfDirNextRay</code>) using the <code>PbrSample</code> function, which uses the material properties and the current ray direction to generate this direction. We calculate <code>cosTheta</code>, which is the cosine of the angle between the surface normal and the direction of the next ray. This is used in the calculation of the new throughput because the amount of <a id="_idIndexMarker563"/>light reflected is proportional to the cosine of this angle (<code>bsdfDirNextRay</code>, and the origin is slightly offset from the current position to avoid self-intersection. Please note <code>PBREval</code> is used to evaluate the BRDF in a specific direction while <code>PBRSample</code> is used to generate a new direction and evaluate the BRDF in that direction:</li>
</ol>
<pre class="source-code">
Vec3 F = PbrSample(baseColor.rgb, specularColor, eta,
                   materialIOR, metallic, roughness,
                   T, B, -rayPayload.direction,
                   ffnormal, bsdfDirNextRay,
                   bsdfpdfNextRay, rayPayload.seed);
float cosTheta = abs(dot(N, bsdfDirNextRay));
rayPayload.throughput *=
  F * cosTheta / (bsdfpdfNextRay);
// Russian roulette
float rrPcont =
  min(max3(rayPayload.throughput) * eta * eta + 0.001,
      0.95);
rayPayload.throughput /= rrPcont;
// update new ray direction &amp; position
rayPayload.direction = bsdfDirNextRay;
rayPayload.origin = offsetRay(
  worldPosition, dot(bsdfDirNextRay, worldNormal) &gt; 0
                   ? worldNormal
                   : -worldNormal);</pre> <p>This concludes <a id="_idIndexMarker564"/>various parts of how to implement a simple GPU-based ray tracer in Vulkan.</p>
<h2 id="_idParaDest-269"><a id="_idTextAnchor306"/>See also</h2>
<p>We recommend reading the <em class="italic">Ray Tracing in One Weekend</em> book series:</p>
<ul>
<li><a href="https://github.com/RayTracing/raytracing.github.io">https://github.com/RayTracing/raytracing.github.io</a></li>
</ul>
<p>Adam Celarek and Bernhard Kerbl’s YouTube channel contains a trove of information about lighting and ray-tracing:</p>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLmIqTlJ6KsE2yXzeq02hqCDpOdtj6n6A9">https://www.youtube.com/playlist?list=PLmIqTlJ6KsE2yXzeq02hqCDpOdtj6n6A9</a></li>
</ul>
<h1 id="_idParaDest-270"><a id="_idTextAnchor307"/>Implementing hybrid rendering</h1>
<p>In this recipe, we will explore the integration of rasterization, specifically deferred rendering, with ray-traced shadows.</p>
<p>In <a href="B18491_04.xhtml#_idTextAnchor241"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Techniques for Lighting, Shading, and Shadows</em>, we implemented deferred rendering, incorporating techniques such as shadow mapping, screen space AO, and screen space reflections. These techniques allowed us to <a id="_idIndexMarker565"/>generate multiple textures, which were then composited during the lighting pass. Within this recipe, you will gain insights into generating shadow textures using ray tracing, which will help in overcoming challenges associated with techniques such as screen space shadow mapping. Screen space shadow mapping relies on the information available in the rendered image. It doesn’t have complete access to the entire 3D scene geometry. This limitation can result in inaccuracies and artifacts. Screen space shadow mapping is susceptible to aliasing issues, particularly along edges and boundaries due to the resolution of the screen space texture. The ray tracing approach doesn’t have these problems as it works on a full scene.</p>
<h2 id="_idParaDest-271"><a id="_idTextAnchor308"/>Getting ready</h2>
<p>Within the code repository, the hybrid rendering functionality is realized through the <code>RayTracedShadowPass</code> and <code>LightingPassHybridRenderer</code> classes.</p>
<p>The process begins with the execution of the <code>Gbuffer</code> pass, generating G-buffer textures based on the concepts discussed in <a href="B18491_04.xhtml#_idTextAnchor241"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Techniques for Lighting, Shading, and Shadows</em>. Following this, <code>RayTracedShadowPass</code> is initiated, employing the ray tracing stages outlined in the preceding section. However, in this pass, ray tracing is specifically employed to generate the shadow texture. The final step involves employing <code>LightingPassHybridRenderer</code> to compose information from the G-buffer and the ray-traced shadow texture, culminating in the production of a final image for display.</p>
<p>The device side code for the ray tracing shader is in the following:</p>
<pre class="source-code">
 raytrace_raygen_shadow_hybrid.rgen, raytrace_miss_shadow_hybrid.rmiss, raytrace_closesthit_shadow_hybrid.rchit</pre> <p>The device <a id="_idIndexMarker566"/>side code for compositing is in the following:</p>
<pre class="source-code">
 hybridRenderer_lighting_composite.frag.</pre> <p>Now that we have understood the code structure, we will investigate how to implement it in the following section.</p>
<h2 id="_idParaDest-272"><a id="_idTextAnchor309"/>How to do it...</h2>
<p>The host side part of the code is in <code>RayTracedShadowPass</code> and the setup for it is very similar to what we described during the previous recipe. We will focus on the device side code to look at how we generate shadows:</p>
<ol>
<li>As usual, we start the shader with a declaration for the input and uniform variables that the shader will use. The <code>layout(location = 0) rayPayloadEXT float visibilityRayPayload;</code> line defines the payload that will be returned by the ray tracing operation. The other uniform variables declared are for the acceleration structure, output image, and textures for the normal and position G-buffers:<pre class="source-code">
layout(location = 0) rayPayloadEXT
  float visibilityRayPayload;
layout(set = 0, binding = 0) uniform
  accelerationStructureEXT topLevelAccelStruct;
layout(set = 0, binding = 1,
       rgba8) uniform image2D outputImage;
layout(set = 1,
       binding = 0) uniform sampler2D gbufferNormal;
layout(set = 1,
       binding = 1) uniform sampler2D gbufferPosition;</pre></li> <li>The <code>main</code> function is where the actual computation happens. It starts by calculating the pixel center and the UV coordinates for the current pixel (or launch). Then, it fetches the normal and world position from the G-buffers using the UV coordinates. <code>rayOrigin</code> is calculated by offsetting the world position slightly along <a id="_idIndexMarker567"/>the normal direction. This is to prevent <strong class="bold">self-intersection</strong>, where the ray might incorrectly intersect with the surface it was launched from:<pre class="source-code">
const vec2 pixelCenter =
  vec2(gl_LaunchIDEXT.xy) + vec2(0.5);
const vec2 inUV =
  pixelCenter / vec2(gl_LaunchSizeEXT.xy);
vec3 normal =
  normalize(texture(gbufferNormal, inUV).xyz);
vec3 worldPosition =
  texture(gbufferPosition, inUV).xyz;
vec3 rayOrigin = worldPosition + normal * 0.1f;</pre></li> <li>The shader launches multiple shadow rays towards random points on the light source. The loop runs for several samples, generating a random point on the light source for each sample, and then calculates the direction to that point. The <code>traceRayEXT</code> function is called to trace a ray from <code>rayOrigin</code> toward the light source. If the ray hits something before it reaches the light, the payload will be <code>0</code>, indicating that the light source is occluded. If the ray reaches the light source without hitting anything, the payload will be <code>1</code>, indicating that the light source is visible. The visibility for each sample is accumulated in the <code>visible</code> variable. The accumulated visibility for each sample, represented by the <code>visible</code> variable, is then stored in the corresponding location of the final image:<pre class="source-code">
for (int i = 0; i &lt; numSamples; i++) {
  vec3 randomPointOnLight =
    lightData.lightPos.xyz +
    (rand3(seed) - 0.5) * lightSize;
  vec3 directionToLight =
    normalize(randomPointOnLight - worldPosition);
  // Start the raytrace
  traceRayEXT(topLevelAccelStruct, rayFlags, 0xFF, 0,
              0, 0, rayOrigin.xyz, tMin,
              directionToLight.xyz, tMax, 0);
  visible += visibilityRayPayload;
}
visible /= float(numSamples);</pre></li> <li><code>raytrace_miss_shadow_hybrid.rmiss</code> and <code>raytrace_closesthit_shadow_hybrid.rchit</code> are pretty straightforward; they simply set <code>visibilityRayPayload</code> to <code>1.0</code> if it’s a miss and <code>0.0</code> in case we hit something.</li>
<li>The last <a id="_idIndexMarker568"/>step is the compositing step. This is the same as the lighting pass we discussed in <a href="B18491_04.xhtml#_idTextAnchor241"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Techniques for Lighting, Shading, and Shadows</em>, the only difference being that now we are using a shadow texture that has been created using ray tracing.</li>
</ol>
<p>In this chapter, we explored the world of ray tracing and hybrid rendering in Vulkan. We delved into these advanced graphical techniques, understanding how they can provide unprecedented levels of realism in rendered images. We learned how ray tracing algorithms work, tracing the path of rays of light to create highly detailed and physically accurate reflections and shadows in a 3D scene. Through hybrid rendering, we uncovered the process of combining traditional rasterization with ray tracing to achieve a balance between performance and visual fidelity. This blend allows for the high speed of rasterization <a id="_idIndexMarker569"/>where the utmost precision isn’t required while using ray tracing to handle complex light interactions that rasterization struggles with. Vulkan’s robust support for both techniques was explored, leveraging its efficient capabilities and explicit control over hardware resources.</p>
</div>
</body></html>