- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extended Reality with OpenXR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to what Vulkan is for graphics, OpenXR, an integral part of the world
    of **Extended Reality** (**XR**), is an API that serves as a powerful tool for
    implementing XR applications. This chapter provides an overview of OpenXR and
    how to use it in conjunction with Vulkan. We start with a basic introduction to
    OpenXR, explaining its role and significance in XR applications, and follow with
    recipes that may be used to improve your XR applications, such as **single pass
    multiview** rendering, a technique that optimizes the rendering of stereo scenes.
    The chapter further expands into the realm of foveated rendering, a method that
    significantly bolsters **Frames Per Second** (**FPS**) by rendering different
    sections of the screen at diverse resolutions. We delve into the implementation
    of this technique using the **fragment shading rate** feature of the Vulkan extension,
    providing you with a practical understanding of its application. Lastly, we delve
    into the use of **half floats**, a practical aid in conserving memory space on
    **Head-Mounted Displays** (**HMDs**). By the end of this chapter, you will have
    gained an understanding of these concepts and will be equipped with the skills
    to apply them effectively in your XR projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with OpenXR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement single pass multiview rendering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing static foveated rendering with a fragment density map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving eye gaze information from OpenXR in your app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing dynamic foveated rendering using Qualcomm’s fragment density map
    Offset extension
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using half floats to reduce memory load
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need to install Android Studio and will also need
    a Meta Quest 2 or Meta Quest Pro to run the **Virtual Reality** (**VR**) sample
    application provided in the repository. Please follow these steps to install the
    tools needed to build, install, and run an application on the device:'
  prefs: []
  type: TYPE_NORMAL
- en: Download and install the Android Studio Hedgehog version from [https://developer.android.com/studio/releases](https://developer.android.com/studio/releases).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also recommend installing the Meta Quest developer hub from [https://developer.oculus.com/downloads/package/oculus-developer-hub-win](https://developer.oculus.com/downloads/package/oculus-developer-hub-win).
    This tool provides several features that help the development of XR applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please follow the steps outlined in the following link to make sure that your
    device is developer-ready—that is, you can debug, deploy, and test VR apps: [https://developer.oculus.com/documentation/native/android/mobile-device-setup/](https://developer.oculus.com/documentation/native/android/mobile-device-setup/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To launch the project, simply launch Android Studio and open this chapter's
    `project` folder located in `source/chapter8` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with OpenXR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we dive into how our application code is structured, let’s talk about
    some important OpenXR concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`XrInstance`: This is the starting point for an OpenXR application. It represents
    the application’s connection to an OpenXR runtime. It is the first object you
    create and the last thing you destroy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XrSystemId`: After creating an instance, the application queries for a system
    ID, which represents a specific device or group of devices, such as a VR headset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XrViewConfigurationType`: This is used to select a view configuration that
    the application will use to display images. Different configurations can represent
    different display setups, such as monoscopic, stereoscopic, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XrSession`: Once the instance is set up and the system ID and view configuration
    are determined, a session is created. A session represents the application’s interaction
    with a device. The session manages the life cycle, rendering parameters, and input
    data for the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XrSpace`: Spaces represent coordinate systems within the XR environment. They
    are used to position objects in 3D space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XrSwapchain`: A swapchain is a collection of textures used to buffer images
    for display. After the session has been established, the swapchain is created
    to handle the rendering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xrBeginFrame` and `xrEndFrame`: These are functions used to start and end
    the rendering of a frame. The `xrBeginFrame` function signals the start of a rendering
    frame, and `xrEndFrame` signals the end of a frame. They are called for each frame
    in the render loop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 8**.1* depicts the basic idea about how to use OpenXR:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.1 – OpenXR object interaction diagram](img/B18491_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – OpenXR object interaction diagram
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn about the main OpenXR initialization events, and
    which functions we need to use to render a frame and display them on a device.
    The recipe will also cover how the OpenXR code is handled in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in creating an OpenXR application involves setting up an `XrInstance`.
    This instance is the primary connection between your application and the OpenXR
    runtime. To create an `XrInstance`, you’ll need to call the `xrCreateInstance`
    function. Before you do this, you will need to decide which extensions your application
    requires. At the very least, your application will need to enable a graphics binding
    extension, which specifies the graphics API that will be used. You can also use
    `xrEnumerateInstanceExtensionProperties` to enumerate all extensions supported
    by the platform. Additionally, before calling `xrCreateInstance`, you will need
    to populate the `XrApplicationInfo` structure. This structure holds essential
    details about your application, such as the application’s name, engine name, and
    version information.
  prefs: []
  type: TYPE_NORMAL
- en: After these details are set, you can call `xrCreateInstance`, which will return
    an instance handle upon successful creation. Following the creation of the `XrInstance`,
    the next step involves querying for a `SystemId` and selecting an `XrViewConfigurationView`.
    The `SystemId` represents a specific XR device or a group of devices, such as
    a VR headset, and it can be retrieved using the `xrGetSystem` function. `XrViewConfigurationView`,
    on the other hand, allows you to choose the view configuration that your application
    will use for displaying images. This could range from monoscopic to stereoscopic
    configurations, depending on your device type. In the recipes in this chapter,
    we will be using the stereo view by specifying `XR_VIEW_CONFIGURATION_TYPE_PRIMARY_STEREO`.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to create a instance of `XrSession`. A `XrSession` represents
    your application’s active interaction with the XR device. It handles the rendering
    parameters, input data, and overall life cycle of the application’s interaction
    with the device. To create an `XrSession`, we will need to fill the graphics binding
    information in `XrSessionCreateInfo`. Since we are using Vulkan, we will specify
    graphics binding using the `XrGraphicsBindingVulkanKHR` structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tracking spatial relationships is very important in XR platforms. An instance
    of `XrSpace` class represent something that is being tracked by the XR system.
    To interact with tracked objects, we will use `XrSpace` handles. Several spaces
    are known as reference spaces, which can be accessed using sessions and enumerations.
    There are three types of reference spaces in OpenXR:'
  prefs: []
  type: TYPE_NORMAL
- en: '`XR_REFERENCE_SPACE_TYPE_LOCAL`: Seated or static space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XR_REFERENCE_SPACE_TYPE_VIEW`: Head locked space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XR_REFERENCE_SPACE_TYPE_STAGE`: An area bounded by an environment in which
    the user can move around'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get an `XrSpace` from these enumerations, you will use `xrCreateReferenceSpace`.
    Another kind of space you can create is `xrCreateActionSpace`, which is used when
    you need to create a space from a pose action. For instance, we use it to create
    an `XrSpace` for gaze location and orientation. `xrLocateSpace` is an API which
    is used to determine transform relative to other spaces.
  prefs: []
  type: TYPE_NORMAL
- en: To render graphics, we will need to create a swapchain, just like in Vulkan.
    To create one, you need to call `xrCreateSwapchain`. Next, we will use `xrEnumerateSwapchainImages`
    to acquire multiple `XrSwapchainImageVulkanKHR` instances that hold a reference
    to `vkImage`.
  prefs: []
  type: TYPE_NORMAL
- en: In OpenXR, a key concept is that of layers. Imagine layers as distinct sections
    or elements of the final rendered scene in a virtual or augmented reality experience.
    Rather than presenting a flat, single-image view, OpenXR creates a multi-dimensional
    perspective by independently rendering each layer and then compositing them to
    form the final image. The most frequently used layer is `XrCompositionLayerProjection`.
    This layer is responsible for rendering the main scene. To create a sense of depth
    and immersion typical of VR experiences, this layer incorporates multiple views—one
    for each eye in a VR headset. This arrangement produces a stereoscopic 3D effect.
    But `XrCompositionLayerProjection` isn’t the only layer at work. OpenXR also employs
    layers such as `XrCompositionLayerQuad`, `XrCompositionLayerCubeKHR`, and `XrCompositionLayerEquirectKHR`.
    Each of these plays a unique role in enhancing the rendering of the final image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will move to the render loop; the application render loop consists of
    three main functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`xrWaitFrame` blocks until the OpenXR runtime determines that it’s the right
    time to start the next frame. This includes computations and rendering based on
    the user’s head pose.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`xrBeginFrame` is called by the application to mark the start of rendering
    for the given frame.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`xrEndFrame` submits the frame for display.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next part is acquiring and releasing swapchain images: `xrAcquireSwapchainImage`
    gives the index of the current swapchain image but it doesn’t give you permission
    to write to the image. To write to the swapchain image, you will need to call
    `xrWaitSwapchainImage`. `xrReleaseSwapchainImage` is called just before `xrEndFrame`,
    before the rendering is done. `xrEndFrame` will use the most recently released
    swapchain image for displaying to the device.'
  prefs: []
  type: TYPE_NORMAL
- en: The last important call is `xrPollEvents`, which is used to retrieve events
    from the event queue. Events in OpenXR represent various types of occurrences,
    such as changes in session state, input from the user, or changes in the environment.
    For instance, an event might be generated when the user puts on or takes off their
    headset, when they press a button on a controller, or when the tracking system
    loses or regains sight of a tracked object. It’s usually called once a frame.
  prefs: []
  type: TYPE_NORMAL
- en: In the repository, the code for OpenXR is encapsulated in the `OXR::Context`
    and `OXR::OXRSwapchain` classes.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `OXR::Context` class in the repository manages most of OpenXR calls and
    states. In this recipe, we will show you the details of these functions and how
    to use them to initialize the OpenXR sample app in the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `OXR::Context::initializeExtensions` method finds the extensions available
    in the OpenXR runtime and filters out the requested extensions that aren’t supported.
    Once the available extensions are fetched, it iterates through the requested extensions,
    eliminating any that aren’t available. That results in a list of extensions that
    are both requested and supported:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `Context::createInstance()` method is responsible for creating an OpenXR
    instance with basic application information and the extension details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `Context::systemInfo` method retrieves and stores the properties of the
    OpenXR system for a head-mounted display. It fetches the system ID and its properties,
    including system name, vendor ID, graphics properties, tracking properties, and
    eye gaze support:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `Context::enumerateViewConfigurations` function enumerates all the view
    configurations supported by the system and then selects and stores properties
    of the one that matches the predefined supported configuration. If the selected
    configuration supports the required number of viewports, it stores the configuration
    properties and the view configuration views.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `Context::initGraphics` function is designed to initialize the graphics
    requirements for Vulkan. It achieves this by obtaining key components such as
    the Vulkan instance and device extensions. `xrGetVulkanInstanceExtensionsKHR`
    and `xrGetVulkanDeviceExtensionsKHR` are functions used in the OpenXR API to retrieve
    the names of Vulkan instance and device extensions, respectively, that are needed
    by a particular OpenXR runtime:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `Context::initializeSession` function creates a new OpenXR session. It
    begins by creating an `XrGraphicsBindingVulkanKHR` object, which is used to bind
    Vulkan to the XR session. This object is populated with the Vulkan instance, physical
    device, and device, as well as the queue family index. This information allows
    the OpenXR runtime to interface with the Vulkan API. Then, an `XrSessionCreateInfo`
    object is created, which is used to specify the parameters for creating a new
    session. This object attributes are populated with the nature of the session to
    be created, the graphics binding, and the system ID. Finally, the `xrCreateSession`
    function is called to create the session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `Context::enumerateReferenceSpaces` function retrieves the types of reference
    spaces available for the current OpenXR session. It calls `xrEnumerateReferenceSpaces`
    to fill a vector of `XrReferenceSpaceType` structures with the available reference
    space types. Finally, it checks whether the `XR_REFERENCE_SPACE_TYPE_STAGE` type
    is available and stores this information in the `stageSpaceSupported_` variable.
    The `XR_REFERENCE_SPACE_TYPE_STAGE` type represents a standing-scale experience
    where the user has a small amount of room to move around.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `Context::createSwapchains` function is responsible for creating the swapchains
    needed for rendering. Based on the value of `useSinglePassStereo_`, it either
    creates a single swapchain that will be used for both views (in case of single-pass
    stereo rendering), or separate swapchains for each view. For each swapchain, it
    creates a new `OXRSwapchain` instance. The `OXRSwapchain` constructor is called
    with the Vulkan context, the OpenXR session, the viewport for the swapchain, and
    the number of views per swapchain. We call the `initialize` function to initialize
    the `OXRSwapchain` instance. The `initialize` function in the `OXRSwapchain` class
    sets up the color and depth swapchains for an OpenXR session by calling the `xrCreateSwapchain`
    function. Once `XrSwapchain` is created, we call `enumerateSwapchainImages` in
    `OXRSwapchain`, which is responsible for creating a vector of `XrSwapchainImageVulkanKHR`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`OXRSwapchain` also provides functions such as `getSurfaceTexture` and `releaseSwapchainImages`.
    `getSurfaceTexture` is responsible for acquiring a swapchain by calling `xrAcquireSwapchainImage`
    and `xrWaitSwapchainImage`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before starting to render, `OXR::Context::beginFrame` first synchronizes frame
    submission with the display by calling `xrWaitFrame`, which returns an `XrFrameState`
    structure. The frame state specifies a predicted display time when the runtime
    predicts a frame will be displayed. The function also calls `xrBeginFrame`, which
    must be called before rendering starts, and retrieves some other important information,
    such as the head and view poses, and calculates view and camera transformations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once rendering has been completed, the application must call the `OXR::endFrame`
    method, which in turn calls `xrEndFrame`. The `XrFrameEndInfo` structure specifies
    the type of layer being presented (and its flags) and its associated spaces (with
    its poses and field of view angles and maybe depth information) and how the image(s)
    should be blended with underlying layers. Note that, for the sake of conciseness,
    only the critical sections of the code are displayed here. For a comprehensive
    understanding, please refer to the full code in the original source:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `android_main` function, outside of the `OXR::Context` class, serves as
    the main entry point for a native Android activity. It initializes both OpenXR
    (`oxrContext`) and Vulkan (`vkContext`) contexts and sets up their required extensions
    and features. After creating an instance, it establishes a session and creates
    swapchains for rendering. Shader modules for vertex and fragment shaders are also
    created. The function then enters a loop where it handles OpenXR events, begins
    a frame, carries out rendering actions, and ends the frame. This loop continues
    until the app is requested to be destroyed. Please note that, for brevity, a significant
    amount of detail has been omitted from this summary. You are encouraged to review
    the actual code in the repository for a comprehensive understanding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This recipe involves a sequence of steps starting from initializing the OpenXR
    and Vulkan contexts to entering a game event loop for handling OpenXR events and
    rendering. The process is intricate and involves enabling specific features, handling
    graphics commands, and managing frames. This guide has provided a simplified overview,
    and we strongly recommend reviewing the full code in the repository for a complete
    understanding.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more details, please refer to the OpenXR guide by Khronos:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.khronos.org/files/openxr-10-reference-guide.pdf](https://www.khronos.org/files/openxr-10-reference-guide.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement single pass multiview rendering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XR devices must render scenes at least twice for each frame, generating one
    image for each eye. Single pass multiview rendering is a technique used to enhance
    the performance of XR applications by allowing the rendering of multiple views
    in a single pass. This effectively enables the rendering of the scene from both
    eye’s perspectives with one draw call.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will navigate how to enable the Multiview rendering feature
    in Vulkan and how to use it to render the scene for both eyes in one render pass.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of Vulkan, the `VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_MULTIVIEW_FEATURES`
    extension specifies whether multiple views are supported in a single rendering
    pass. Once the feature is enabled, you can specify multiple viewports and scissor
    rectangles for your rendering pass. The graphics pipeline will then render the
    scene from different perspectives in a single pass, reducing the need for duplicate
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Besides enabling a Vulkan extension, you will also need to enable the `GL_EXT_multiview`
    extension in your shader code. `GL_EXT_multiview` is a GLSL extension that allows
    multiple views to be rendered in a single pass. `GL_EXT_multiview` introduces
    a new built-in variable, `gl_ViewIndex`, that can be used in your shaders to determine
    which view is being rendered. It contains the index of the current view being
    processed and can be used to adjust your drawing based on the view index (for
    instance, index `0` may represent the left eye, while index `1` can represent
    the right eye).
  prefs: []
  type: TYPE_NORMAL
- en: We also need the ability to query whether multiview is supported by hardware
    or not using `VkPhysicalDeviceMultiviewFeatures`. Additionally, we need to specify
    that we will be using multiple views when creating the render pass. This is done
    by adding an instance of the `VkRenderPassMultiviewCreateInfo` structure to the
    `pNext` chain of the `VkRenderPassCreateInfo` structure. One other important part
    is that swapchain images need to have multiple layers (in our case, two—one for
    each eye), and the results of the rendering go to different layers of the attachments.
    You may think that we could have rendered the same scene twice (one for left and
    one for right), but that would mean we build a command buffer that sends all the
    geometry and textures twice. This extension helps us send data only once and only
    shaders are fired twice (for each view ID). The only difference between these
    two executions is uniform data for the camera.
  prefs: []
  type: TYPE_NORMAL
- en: To support multiview, code changes are required in various areas of the code
    base. In this case, we needed to change `Texture`, `RenderPass`, `Context` classes,
    and shader files.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following steps, we will go through details on how to implement this
    recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Extend `VulkanCore::Texture` to support `vkImageView` created using `VK_IMAGE_VIEW_TYPE_2D_ARRAY`;
    this is necessary if we have multiple layers in the same texture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add support for multiview in `VulkanCore::RenderPass`; this is achieved by connecting
    `VkRenderPassMultiviewCreateInfo` to `VkRenderPassCreateInfo`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add support to enable multiview extension in `VulkanCore::Context`; this is
    abstracted in a function named `enableMultiView`, which simply enables `VkPhysicalDeviceMultiviewFeatures`
    if it is supported by a physical device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The vertex shader is now passed two `Context::mvp(index)` was introduced, so
    that we can query MVP for the left and right eye.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also introduced a constant named `kUseSinglePassStereo` that can be used
    to control whether we want to use a single pass or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given that the code is distributed across various files, we strongly suggest
    delving into the repository for a comprehensive review of the implementation.
    Specifically, the file located at `source/chapter8/app/src/main/cpp/main.cpp`
    should warrant your particular attention.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing static foveated rendering with a fragment density map
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Foveated rendering** is a cutting-edge graphics rendering technique that
    leverages the human eye’s natural tendency to focus on specific regions of a scene,
    optimizing computational resources by allocating higher detail and resolution
    to the central, foveal vision, and progressively reducing it toward the peripheral
    vision. This mimics the way the human eye perceives detail, offering a substantial
    performance boost in graphics rendering without sacrificing visual quality.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to implement fixed foveated rendering by using
    the **fragment density map** (**FDM**) extension.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The FDM device extension in Vulkan (`VK_EXT_fragment_density`) enables an application
    to specify different levels of detail to use in different areas of the render
    target by means of a texture that encodes how many times a fragment shader will
    be invoked for that area. The FDM may be modified on each frame to accommodate
    the user’s eye gaze direction. This recipe only works with HMDs that provide eye
    gaze detection, such as Meta’s Quest Pro. The recipe presented here works for
    a single-pass stereo rendering approach.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before creating and using an FDM and the FDM Offset extension, we need to enable
    the extensions:'
  prefs: []
  type: TYPE_NORMAL
- en: Before enabling the feature, it is necessary to check whether the physical device
    supports it. Doing so requires appending an instance of the `VkPhysicalDeviceFragmentDensityMapFeaturesEXT`
    structure to the `pNext` chain of `VkPhysicalDeviceFeatures2` passed to the `vkGetPhysicalDeviceFeatures2`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`VkPhysicalDeviceFragmentDensityMapFeaturesEXT:: fragmentDensityMap` specifies
    whether the device supports the FDM extension.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The extension has properties that need to be queried to be used properly. To
    do that, also include an instance of the `VkPhysicalDeviceFragmentDensityMapPropertiesEXT`
    structure to the `pNext` chain of `VkPhysicalDeviceProperties2` and query those
    properties with `vkGetPhysicalDeviceProperties2`. We will use these properties
    in *step 4*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The FDM extension is a device extension and its name needs to be passed in
    during the creation of the `VkDevice` object: `"VK_EXT_fragment_density_map"`
    (or the definition, `VK_EXT_FRAGMENT_DENSITY_MAP_EXTENSION_NAME`).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An FDM’s size doesn’t map to the framebuffers on a one-to-one ratio. One texel
    of the map affects an *area* of the render target. This area’s size can be queried
    from `VkPhysicalDeviceFragmentDensityMapPropertiesEXT`, from the `minFragmentDensityTexelSize`
    and `maxFragmentDensityTexelSize` properties.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In our recipe, we will create an FDM with texels that map to an area that is
    at least 32 x 32 of the render target, bounded by `minFragmentDensityTexelSize`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'An FDM is a regular texture with some special usage flags:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The format of the texture is `VK_FORMAT_R8G8_UNORM`. Each pixel stored in the
    map specifies the density of fragments to be used for that area of the render
    target, where `255` means the density should be the highest (or the default: one
    fragment per render target’s pixel; `128` for half the density, and so on). In
    our recipe, our map is initialized to `128` (half density) and then manipulated
    to have an area in the center with a radius equal to `2` texels with full density:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the image has two layers, one for each eye. The data is uploaded to
    the device twice, once for each layer of the image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the data has been uploaded for each layer of the map, the texture’s layout
    needs to be transitioned to the special layout, `VK_IMAGE_LAYOUT_FRAGMENT_DENSITY_MAP_OPTIMAL_EXT`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The FDM needs to be specified and referenced by the render pass in a `VkAttachmentDescription`
    structure, just like any other attachment used in the render pass:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The FDM must not appear as a color or depth stencil attachment in the `VkSubpassDescription::pColorAttachments`
    or `VkSubpassDescription::pDepthStencilAttachment` arrays. Instead, it must be
    referenced in an instance of the special `VkRenderPassFragmentDensityMapCreateInfoEXT`
    structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Bug prevention notice
  prefs: []
  type: TYPE_NORMAL
- en: 'The order in which this structure appears in the `VkRenderPassCreateInfo::
    pAttachments` array must match the index of the `VkImage` array passed to `VkFramebufferCreateInfo::pAttachments`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The instance of the `VkRenderPassFragmentDensityMapCreateInfoEXT` structure
    needs to be added to the `pNext` chain property of the `VkRenderPassCreateInfo`
    structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The image view of the FDM needs to be part of the framebuffer as well. Its image
    view must be added to the `VkFramebufferCreateInfo::pAttachments` array and its
    index into this array must match that of the `VkAttachmentDescription` structure
    passed to the creation of the render pass.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This marks the end of our guide on static foveated rendering. In the upcoming
    sections, we’ll expand our exploration into the realm of dynamic foveated rendering.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information, check out the extension information at the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: https://registry.khronos.org/vulkan/specs/1.3-extensions/man/html/VK_EXT_fragment_density_map.html
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: https://registry.khronos.org/vulkan/specs/1.3-extensions/man/html/VK_QCOM_fragment_density_map_offset.html.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving eye gaze information from OpenXR in your app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The realm of VR has evolved to the extent that some HMDs are now equipped with
    the capability to track the user’s eye gaze. This feature, which identifies the
    direction in which the user is looking, can be harnessed for a variety of tasks,
    enhancing the interactivity and immersion of VR experiences. In this recipe, we
    will guide you through the process of enabling and retrieving eye gaze data from
    OpenXR in your application. Additionally, we will illustrate how to calculate
    the focal region—the specific area the user is looking at—in pixel coordinates
    on the render target used for display.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, you will need an HMD that supports eye-tracking features, such
    as Meta’s Quest Pro. You will also need to provide permission to the app to track
    the user’s eye, which can be achieved through the **Settings** menu on most devices.
  prefs: []
  type: TYPE_NORMAL
- en: Also, get acquainted with how spaces and actions are supported and used in OpenXR
    (see the *Getting started with* *OpenXR* recipe).
  prefs: []
  type: TYPE_NORMAL
- en: This recipe was authored and tested with Meta’s Quest Pro device, so some of
    the code shown here is specific to that platform. Your implementation might require
    small tweaks to work on your device.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Adding eye gaze support requires allowing the device to track the user’s eyes.
    This requires executing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before using the eye-tracking feature in your app, you need to request permission
    by adding the following lines to the `AndroidManifest.xml` file of your app:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Grant permission to your app to track the user’s eye with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Enable the OpenXR extension when creating your OpenXR instance by adding `XR_EXT_EYE_GAZE_INTERACTION_EXTENSION_NAME`
    to the `XrInstanceCreateInfo::enableExtensionNames` array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We begin by adding a few member variables to the `OXR:Context` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Eye tracking is considered an input action in OpenXR, so we create an action
    set to store the eye-tracking action (`OXR::Context::eyegazeActionSet_`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then create an action that represents the eye gaze input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We’ll need paths that identify the input action and its pose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The action and its pose need to be bound together using an instance of the
    `XrActionSuggestedBinding` structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Actions need to be attached to a session to work, which can be done by calling
    `xrAttachSessionActionSets` with the action set that stores the eye gaze action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to create an action space for the eye gaze action to define a
    position and orientation of the new space’s origin within a natural reference
    frame of the pose action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last initialization step is to create a local reference space, which we’ll
    use to base the eye gaze position and orientation. The type of the reference space
    is `XR_REFERENCE_SPACE_TYPE_VIEW` as the eye gaze is locked to the eye or headset
    location and orientation. The `eyePoseIdentity` variable is initialized with the
    identity orientation, at a height of `1.8` meters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the `OXR::Context::beginFrame` method, we update the current state of the
    eye gaze action, but only if the current state of the app is `focused`. We can
    then get the action’s state pose with `xrGetActionStatePose`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If `actionStatePose` is `active`, that means we can go ahead and locate the
    action in `localReferenceSpace` at the predicted time from the frame state queried
    before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If both the gaze’s orientation and position are valid, we can use them to calculate
    where, in pixel coordinates, the user is looking at the image being presented
    on the device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculating the screen coordinates of the user’s gaze is simple. The following
    function performs all the math to convert from an `XrPosef` structure (the eye
    gaze location) to the coordinates on the screen. It uses the swapchain dimensions
    to convert the canonical view direction in OpenXR, which points to the *-Z* direction,
    to screen space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The function uses the helper types and functions defined in `xr_linear.h`. The
    projection matrix is calculated in the function, and not cached at the class level,
    to allow it to be modified while the app is running.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The sample app in the repository displays a washed-out round cursor, about 10
    pixels in radius, for each eye if eye-tracking is supported by the device to help
    you see how the eye gaze behaves in the final output.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing dynamic foveated rendering using Qualcomm’s fragment density map
    Offset extension
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Implementing static foveated rendering with a fragment density map*
    recipe, we discussed how to render fragments at a lower density than one fragment
    per pixel using a map that dictates the fragment density for regions of the render
    target. Although useful, the application of a static map is limited because the
    user’s gaze changes as they look around to inspect the scene displayed on the
    device. Recomputing and modifying the map for each frame, based on the user’s
    input, may be computationally expensive and tax the CPU with extra work, making
    the performance gained with the FDM moot.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to apply an offset to the static FDM and let the GPU perform
    the heavy lifting of translating the densities from the map to the rendered scene.
    Thanks to Qualcomm’s FDM Offset device extension, this is possible.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will show you how to use this extension to dynamically translate
    the FDM based on the user’s gaze direction.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, you will need an HMD that supports eye-tracking features, such
    as Meta’s Quest Pro. This recipe was authored and tested with Meta’s Quest Pro
    device, so some of the code shown here is specific to that platform. This recipe
    assumes you have already implemented static foveated rendering using a fragment
    density map. If not, you might want to refer to our previous guide on that topic
    to understand the foundational concepts.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This extension simplifies the application code by applying an offset to the
    FDM at render time, inside the render loop:'
  prefs: []
  type: TYPE_NORMAL
- en: 'All attachments used in the render pass where the offset is applied to the
    FDM must be created with the `VK_IMAGE_CREATE_FRAGMENT_DENSITY_MAP_OFFSET_BIT_QCOM`
    flag. Since we are rendering directly to swapchain images, the swapchain images
    need to be created with that flag. Swapchain images are created by OpenXR. Thankfully,
    Meta devices provide the ability to provide additional Vulkan flags to be used
    during the creation of swapchain images. For that, create an instance of the `XrVulkanSwapchainCreateInfoMETA`
    structure and add the flag mentioned before to its `addditionalCreateFlags` property:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The instance of the `XrVulkanSwapchainCreateInfoMETA` structure must be added
    to the `pNext` chain of the `XrSwapchainCreateInfo` structure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Before enabling the FDM Offset feature, it is necessary to check whether the
    physical device supports it. Doing so requires appending an instance of the `VkPhysicalDeviceFragmentDensityMapOffsetFeaturesQCOM`
    structure to the `pNext` chain of `VkPhysicalDeviceFeatures2` passed to the `vkGetPhysicalDeviceFeatures2`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`VkPhysicalDeviceFragmentDensityMapOffsetFeaturesQCOM:: fragmentDensityMapOffset`
    specifies whether the FDM Offset extension is supported.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The extension has properties that need to be queried to be used properly. To
    do that, also include an instance of the `VkPhysicalDeviceFragmentDensityMapOffsetPropertiesQCOM`
    structure to the `pNext` chain of `VkPhysicalDeviceProperties2` and query those
    properties with `vkGetPhysicalDeviceProperties2`. We will use them later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The FDM Offset extension is a device extension and its name needs to be passed
    in during the creation of the `VkDevice` object: `"VK_QCOM_fragment_density_map_offset"`
    (or `VK_QCOM_FRAGMENT_DENSITY_MAP_OFFSET_EXTENSION_NAME`).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The FDM texture needs to be created with the `VK_IMAGE_CREATE_FRAGMENT_DENSITY_MAP_OFFSET_BIT_QCOM`
    creation flag.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The offsets are applied to the FDM by creating an instance of the `VkSubpassFragmentDensityMapOffsetEndInfoQCOM`
    structure and adding it to the `pNext` chain of the `VkSubpassEndInfo` structure.
    Note that, in this case, you need to call `vkCmdEndRenderPass2`. `vkCmdEndRenderPass`
    isn’t extensible (we’ll see how to calculate the offsets in the next step):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `eyeGazeScreenPosLeft` and `eyeGazeScreenPosRight` offsets can be calculated
    using the previous recipe, *Retrieving eye gaze information from OpenXR in your
    app*. In the sample app provided in the repository, they can be retrieved from
    the context with the `OXR::Context::eyeGazeScreenPos(int` `eye)` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This extension is powerful because it allows static FDM to be used to achieve
    dynamic foveation without the need to impose an extra CPU load of recalculating
    the map every frame. *Figure 8**.2* shows the result of rendering the bistro scene
    on a Quest Pro with an FDM plus Qualcomm’s FDM Offset extension. The white circle
    is the cursor used to help visualize the eye gaze direction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.2 – The bistro scene rendered on a Quest Pro with an FDM applied
    to the eye gaze direction](img/B18491_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – The bistro scene rendered on a Quest Pro with an FDM applied to
    the eye gaze direction
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our recipe on dynamic foveated rendering. In the next recipe,
    we will learn how we can reduce memory load since the VR devices have limited
    GPU memory.
  prefs: []
  type: TYPE_NORMAL
- en: Using half floats to reduce memory load
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **half float**, also known as a **half-precision floating point**, is a binary
    floating-point format that occupies 16 bits. It plays a crucial role specifically
    for its application in VR devices and other low-performance hardware. A half-precision
    floating point has a smaller memory footprint and requires less bandwidth, which
    can significantly improve the performance and efficiency of such devices. They
    are ideal for scenarios where the precision of full single-precision floating-point
    numbers is not necessary, such as storing pixel values in graphics, performing
    large but simple computations in machine learning models, and certain calculations
    in 3D graphics. Employing 16 bits not only bolsters throughput but also diminishes
    register usage, a key determinant of GPU performance. The quantity of shaders
    that can run concurrently is directly contingent upon the available registers,
    thus making their efficient usage crucial. In this recipe, we demonstrate how
    to use half floats in Vulkan and how we can reduce memory consumption by storing
    vertex data in 16-bit floats instead of 32-bit.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To implement a half float in your application, there are several Vulkan and
    GLSL features that you need to be aware of. Vulkan supports half floats by enabling
    the `storageBuffer16BitAccess` and `shaderFloat16` features. The `storageBuffer16BitAccess`
    feature allows you to use a 16-bit format for storage buffers, which can save
    memory and bandwidth. The `shaderFloat16` feature enables the use of 16-bit floating-point
    types in your shaders, which can improve performance by reducing the amount of
    data that needs to be processed.
  prefs: []
  type: TYPE_NORMAL
- en: On the GLSL side, you would need to enable the `GL_EXT_shader_explicit_arithmetic_types_float16`
    and `GL_EXT_shader_16bit_storage` extensions. The `GL_EXT_shader_explicit_arithmetic_types_float16`
    extension allows you to perform arithmetic operations with half-precision floating-point
    numbers directly in your shaders. Meanwhile, the `GL_EXT_shader_16bit_storage`
    extension enables you to store half-precision floating-point numbers in your shader
    storage blocks and interface blocks.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging these Vulkan and GLSL features, you can effectively incorporate
    a half float in your application, optimizing performance, especially for low-performance
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to effectively implement the 16-bit float, starting with
    the activation of specific features and then modifying the shader code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, we must activate two specific features: `storageBuffer16BitAccess`
    (found in `VkPhysicalDeviceVulkan11Features`) and `shaderFloat16` (located in
    `VkPhysicalDeviceVulkan12Features`). To facilitate this, we have incorporated
    a function within the `VulkanCore::Context` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we change our shader code and add GLSL extensions to it. This is done
    inside the `app/src/main/assets/shaders/Common.glsl` file. We also change the
    vertex structure inside this file to use `float16_t` instead of `float`. We also
    use `glm::packHalf1x16` to convert a 32-bit float to 16-bit when loading GLB assets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In conclusion, implementing a 16-bit float offers significant improvement in
    GPU performance, especially in the context of VR and other low-performance devices.
    By activating the necessary features in Vulkan and making the appropriate adjustments
    in our GLSL shaders, we can take advantage of the benefits that a 16-bit float
    has to offer. It’s a relatively straightforward process that involves enabling
    specific features, adjusting shader code, and modifying data structures to accommodate
    the half-precision format.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you embarked on a journey through the world of OpenXR. You
    started by grasping the fundamentals and swiftly moved on to mastering advanced
    techniques. You learned how to implement single pass multiview rendering and how
    to utilize the fragment density map for static foveated rendering. You also gained
    the skills to retrieve eye gaze information for your app. Further, you unlocked
    the secrets of implementing dynamic foveated rendering using Qualcomm’s fragment
    density map Offset extension. Lastly, you discovered the power of using half floats
    to significantly reduce memory load in your applications.
  prefs: []
  type: TYPE_NORMAL
