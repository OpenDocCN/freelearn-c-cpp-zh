- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced Animation Blending
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to *Chapter 10*! In the previous chapter, we added a bit more *real-life
    behavior* to the instances. After a brief overview of behavior trees, we added
    a visual node editor to visually draw the behavior of the instances by using a
    simple finite state machine. At the end of the chapter, we extended the code and
    implemented interaction as an additional form of behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, the instances will come even more to life. We start with a
    short exploration of the world of face animations made by morph target animations.
    Then we will add extra functionality to load morph meshes into the application
    and enable control over the face animations of the instances. Next, we add a graph
    node to be able to use face animations in the node trees as well. At the end of
    the chapter, additive blending will be implemented, allowing us to move the head
    of the instances independently of the skeletal and face animations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to animate facial expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding face animations to code and GPU shaders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using face animations in node trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing additive blending
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The example code is in the `chapter10` folder, in the `01_opengl_morphanim`
    subfolder for OpenGL and `02_vulkan_morphanim` subfolder for Vulkan.
  prefs: []
  type: TYPE_NORMAL
- en: How to animate facial expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After implementing behavior in [*Chapter 9*](Chapter_9.xhtml), life in our virtual
    world has become a lot more vivid. The instances can walk or run around by themselves,
    do simple tasks at random times, react to a collision with another instance, and
    won’t leave the virtual world beyond the invisible borders.
  prefs: []
  type: TYPE_NORMAL
- en: But the instances still appear a bit sterile and lifeless. They roam around
    like robots, always looking forward, keeping a straight face. No emotions are
    visible, and there are no reactions to interactions other than playing a waving
    animation.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s give the instances the ability to show emotions by adding facial expressions.
    The most common way to add face animations to any kind of *living* virtual object
    is **morph target animations**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the idea of morph target animations, here’s a simple example. In *Figure
    10.1*, three different weights of a morph target animation are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_10.01_B22428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Three different weights during an angry morph target animation'
  prefs: []
  type: TYPE_NORMAL
- en: The left face has an *angry* morph target animation applied to 0%, showing only
    the original face. The middle face has the morph target applied to 50%, blending
    halfway between the original face and the full morph, and for the right face,
    the full morph target animation has been applied.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the eyebrows have been rotated, the mouth vertex has been moved
    a bit upward in the final state, and the vertex positions are only interpolated
    between the original and the final mesh. But these small vertex position changes
    create an entirely different facial expression for the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Morph target animations may have a different name in your tool, such as **per-vertex
    animation**, **shape interpolation**, **shape keys**, or **blend shapes**. All
    these names describe the same technique: multiple deformed versions of a mesh
    are stored in keyframes, and the animation of the mesh is done by interpolating
    the vertex positions between the positions in the keyframes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s an important difference between skeletal animals and morph target animations
    when it comes to cost: a skeletal animation only affects the properties of the
    model’s nodes, while morph animations replace an entire mesh of the model’s virtual
    skin, and the mesh needs to be duplicated for every morph animation the model
    should play, raising the overall size of the model as a file on disk and in memory.'
  prefs: []
  type: TYPE_NORMAL
- en: Models have quite a small number of bones but many vertices for the skin, so
    recalculating the positions for a large number of vertices in every frame adds
    extra computation costs to morph animations. Luckily, the morph animations happen
    entirely in the vertex shader and are only linear interpolations between two positions
    saved as vectors. Thus, the additional computational burden of a morph animation
    remains negligible in our example code.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Blender, morph target animations can be controlled by the **Shape Keys**
    options on the **Data** tab. *Figure 10.2* shows the setting used for the right
    face of *Figure 10.1*, with the **Value** set to `1.000` to apply 100% of the
    **Angry** morph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_10.02_B22428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Shape keys in Blender controlling morph target animations'
  prefs: []
  type: TYPE_NORMAL
- en: Creating and modifying shape key-based morph target animations in Blender is
    beyond the scope of this book. Blender has some basic documentation about shape
    keys, a link is included in the *Additional resources* section, and there are
    plenty of videos around showing how to work with shape keys in Blender.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have a model file containing morph target animations available, like
    the two models in the `assets` folder of *Chapter 10*, or if you have created
    your own set of morph animations in any other existing model, you are ready to
    go for the next step: using the Open Asset Importer Library to import these extra
    animations.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now learn how to add morph target animations to our application.
  prefs: []
  type: TYPE_NORMAL
- en: Adding face animations to code and GPU shaders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Morph target animation data is stored in two places in an Assimp model file.
  prefs: []
  type: TYPE_NORMAL
- en: The first part of the morph animation data, the meshes, reside in the `mAnimMeshes`
    array of an `aiMesh` node, and the number of meshes is stored in the `mNumAnimMeshes`
    variable of the `aiMesh`. Every element of the `mAnimMeshes` array contains the
    exact same number of vertices as the original mesh, allowing us to interpolate
    the vertex positions between different versions of the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: This interpolation is not limited to blending between the original mesh and
    one of the morph target meshes. Also, blending between two morph target meshes
    is possible, or mixing positions of more than two meshes. Be aware that the outcome
    of mixing meshes may not always be as expected as the effect of the morph target
    animations heavily depends on the intention of the animator.
  prefs: []
  type: TYPE_NORMAL
- en: The second part of the morph animation data, the keyframe data, is in the `mMorphMeshChannels`
    array of an `aiAnimation` node, which has the number of keyframes stored in a
    variable called `mNumMorphMeshChannels`. The keys in every keyframe contain the
    points in time for the specific key, plus the numbers of the morph mesh to use
    and the weight of the morph mesh in a linear interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: We will only use the mesh data to interpolate between different facial expressions,
    so we ignore the animation data for the morph meshes. But it is easy to add support
    for the morph target animations on top of the code of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: As the first step on the way to morph animations, we will learn how to load
    the additional mesh data and extract the vertices.
  prefs: []
  type: TYPE_NORMAL
- en: Loading morph meshes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since every vertex in a morph mesh replaces the position and the normal of
    the same vertex in the original mesh, only a subset of vertex data is needed for
    the replacement vertices. We will create a *lightweight* version of a vertex named
    `OGLMorphVertex` in the `OGLRenderData.h` file in the `opengl` folder, containing
    only the position and normal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To collect the replacement morph vertices into a mesh, we also create a new
    `struct` called `OGLMorphMesh` that contains all the vertices in a `std::vector`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As all the morph meshes depend on the original meshes, we add a `OGLMorphMesh`
    vector to the default mesh, `struct` `OGLMesh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For Vulkan, the two new structs are named `VkMorphVertex` and `VkMorphMesh`,
    residing in the `VkRenderData.h` file in the `vulkan` folder. The `VkMorphMesh`
    vector is added to the `VkMesh` `struct`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right before the end of the `processMesh()` method in the `AssimpMesh` class,
    we add a new code block to extract the morph mesh data from the model file. First,
    we check if we have any morph meshes attached to the current mesh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If we find morph meshes, we iterate over all morph meshes, extracting the mesh
    data and the number of vertices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Per definition, the number of vertices in the morph mesh must match the number
    of vertices in the original mesh. It does not hurt to do an addition check here,
    skipping the entire morph mesh and printing an error when a vertex count mismatch
    is detected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we check whether the morph mesh contains position data and create a temporary
    `OGLMorphMesh` if the check succeeds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: It may sound silly to check whether the morph mesh has vertex positions, but
    morph meshes can also override other data, such as normals, colors, or texture
    positions. It is possible to encounter a morph mesh without position data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we loop over all vertices and extract the vertex positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If normal data is stored in the morph mesh too, we extract the normals. Without
    normal data, we set the vertex normal to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we place the vertex into the temporary `OGLMorphMesh`, and after all
    the vertices are processed, the `OGLMorphMesh` is added to the `morphMeshes` vector
    of the `OGLMesh` for this `AssimpMesh` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Accessing any alternative morph meshes is now as simple as checking the `morphMeshes`
    vector for a size greater than zero, and if we have any morph meshes, extracting
    the vertex data and interpolating between the positions and normals of the original
    vertices and the vertices of the selected morph mesh.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, all valid morph meshes found in the model file are available
    as part of the `AssimpMesh` mesh data. To use these morph meshes for face animations,
    we must add some code and logic to the application.
  prefs: []
  type: TYPE_NORMAL
- en: Storing all morph meshes in a single buffer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use the vertices of the morph meshes in a shader, we store the vertex data
    of all morph meshes into a single SSBO. Using a single SSBO is needed due to the
    instanced rendering of the model instances on the screen – we need the vertex
    data of all meshes available at all times during the rendering since we cannot
    tell when a specific model instance will be drawn to the screen. Splitting the
    rendering depending on the selected morph mesh would also be possible, but that
    would be quite a costly alternative as we must filter the instances on every draw
    call.
  prefs: []
  type: TYPE_NORMAL
- en: 'The morph mesh SSBO plus some related variables are added to the `AssimpModel`
    class. First, three new `private` variables are added to the `AssimpModel.h` header
    file: `mNumAnimatedMeshes`, `mAnimatedMeshVertexSize`, and `mAnimMeshVerticesBuffer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In `mNumAnimatedMeshes`, we store the number of morph meshes for this model.
    Right now, the code supports only a single mesh containing morph meshes, so the
    number in `mNumAnimatedMeshes` is equal to the number of morph meshes in this
    specific mesh.
  prefs: []
  type: TYPE_NORMAL
- en: But since we are doing only face animations, the limit of a single mesh with
    morph meshes is no problem. Also, a task in the *Practical sessions* section is
    available to extend the code accordingly and add support for multiple meshes containing
    morph meshes.
  prefs: []
  type: TYPE_NORMAL
- en: The value in `mAnimatedMeshVertexSize` is used to find the start of vertex data
    for the selected morph clip in the SSBO. By multiplying the mesh vertex size and
    the index of the morph clip, we can jump directly to the first vertex of the morph
    clip.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, all vertex data is stored in the `mAnimMeshVerticesBuffer` SSBO.
  prefs: []
  type: TYPE_NORMAL
- en: We also add two `public` methods called `hasAnimMeshes()` and `getAnimMeshVertexSize()`
    to the `AssimpModel.cpp` class implementation file. Thanks to the *descriptive*
    method names, no further explanation should be required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Filling the SSBO is done in the `loadModel()` method of the `AssimpModel` class.
    When all meshes are collected into the `mModelMeshes` vector, we can iterate over
    all meshes to add the vertex data to the new buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As the first step of collecting the vertices in the `mAnimMeshVerticesBuffer`
    SSBO, we check if we have any morph meshes in this mesh. If we have a mesh without
    additional morph meshes, we simply continue with the next mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we create a temporary `OGLMorphMesh` called `animMesh` to collect all
    vertices and resize the `morphVertices` vector in the `animMesh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can copy all vertices of the morph mesh into `animMesh`, using the number
    of vertices to calculate the correct position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we upload the collected vertices to the SSBO:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Whenever we need the morph animations of this model now, we can bind the buffer
    to a specified shader binding point. This binding can be achieved by using the
    new `public` method, `bindMorphAnimBuffer()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To use face animations on a per-instance basis, we must add a couple of variables
    and methods and extend some data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Adding face morph settings to the code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most important change to enable face animations is a new `enum` `class`
    called `faceAnimation`, which resides in the `Enums.h` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: All supported morph animations are stored in the `faceAnimation` `enum` the
    same way as actions or node types. Instead of using the morph animation from the
    model files, we will use only a fixed set of face animations in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to other data types, we add a mapping from the `enum` values to strings.
    It is a lot easier to use the `enum` value in code and show the user-friendly
    string in the UI. The new map named `micFaceAnimationNameMap` will be added to
    the `ModelInstanceCamData` `struct` in the `ModelInstanceCamData.h` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Filling the map with strings happens in the `init()` method of the renderer
    class files, `OGLRenderer.cpp` or `VkRenderer.cpp`, the best place is next to
    the existing mapping code.
  prefs: []
  type: TYPE_NORMAL
- en: Fixed morph mapping vs. dynamic loading from the model
  prefs: []
  type: TYPE_NORMAL
- en: The reason to hard code all morph target animation clips in the `faceAnimation`
    `enum` and `micFaceAnimationNameMap` is to keep the code simple.
  prefs: []
  type: TYPE_NORMAL
- en: While populating the list of morph target clips from the model file is easy,
    maintaining a dynamic list in the UI becomes quite complex. For instance, adding
    code to choose a morph clip in a node tree would create a hard dependency between
    the tree and a single model – using the same tree for other models will become
    impossible.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid a model-tree dependency, only a predefined set of morph animations
    will be used. Any model can support all morph animations, none of them, or a partial
    number of clips with matching indices, filling any gaps with empty morph animations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the instance part, two new variables named `isFaceAnimType` and `isFaceAnimWeight`
    are added to the `InstanceSettings` `struct`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In `isFaceAnimType`, we store the current face animation clip. Due to the extra
    `none` value in the `faceAnimation` `enum`, we do not need another Boolean to
    switch the face animations on or off. The interpolation weight between the default
    face mesh and the face animation can be controlled by `isFaceAnimWeight`, with
    `0.0f` showing only the default mesh and `1.0f` the morph mesh.
  prefs: []
  type: TYPE_NORMAL
- en: '`is` stands for “InstanceSettings”, not for “it is”'
  prefs: []
  type: TYPE_NORMAL
- en: 'To bring up the sidenote again, and to avoid confusion: The `is` in the `InstanceSettings`
    variable names is just the abbreviation of the `struct` name, not something to
    define a state. So, `isFaceAnimType` stands for “Instance Settings Face Animation
    Type”, not a Boolean controlling whether the face animation is enabled.'
  prefs: []
  type: TYPE_NORMAL
- en: We also need to give the renderer the information about the face animations
    for every instance. Let’s extend the renderer as the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Filling the per-instance buffer data in the renderer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As for all other shader-related data, we need an SSBO to hand the data over
    to the GPU. To fill the SSBO, a `std::vector` of some data type is needed. So,
    the renderer class header file `OGLRenderer.h` will get two new `private` variables
    called `mFaceAnimPerInstanceData` and `mFaceAnimPerInstanceDataBuffer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For the Vulkan renderer, the data type of the buffer differs. We need to add
    the following lines to the `VkRenderer.h` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Even though we need only three values, we will use a `glm::vec4` here to tackle
    possible data-alignment problems. You should try to avoid the three-element vector
    (`glm::vec3`) to transport data via an SSBO to the GPU since you may get a mismatch
    between the vector or struct on the CPU side and the buffer on the GPU side.
  prefs: []
  type: TYPE_NORMAL
- en: 'The per-instance face animation SSBO will be added to the `draw()` call of
    the renderer class file, `OGLRenderer.cpp` or `VkRenderer.cpp`, more specifically
    in the loop over the instances of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We also add the resizing of the vector containing the morphing data before the
    loop; see the highlighted part in the previous code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: And since we have already extracted the `InstanceSettings` `struct` of the current
    instance in the loop, adding the face animation is done in just a couple of lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we add an empty `glm::vec4` named `morphData`, plus we check whether
    the instance has a face animation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If we should animate the face of the instance, we fill three elements of `morphData`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now we set the weight of the face animation, the clip number decreased by one
    to honor the `none` element of the `faceAnimation` `enum`, and the number of vertices
    to skip between two morph meshes.
  prefs: []
  type: TYPE_NORMAL
- en: The current shader code uses the number of vertices and the clip number to calculate
    the first vertex of the desired morph animation, but it is possible to an absolute
    value here. An absolute value could become handy if we plan to extend the code
    to support multiple meshes with morph target animations (see the *Practical sessions*
    section).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we store the `morphData` in the vector used to fill the SSBO:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As a last step for the face animations, the vertex shaders must be made aware
    of the new buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Extending the shader to draw face animations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As morph target animations only change vertex data, we need to add the new SSBOs
    and a bit of logic to the vertex shader. There is no need to touch a compute or
    fragment shader, a fact that simplifies the face animation implementation a lot.
  prefs: []
  type: TYPE_NORMAL
- en: 'To prevent distortions for models without face animations, we will use a separate
    set of shaders to draw the meshes containing morph animations. First, we add two
    new `private` shader variables to the renderer header file `OGLRenderer.h`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'For the Vulkan renderer, more work is needed here since the shaders are part
    of the pipelines. We need to add two `VkPipelineLayout` handles, two `VkPipeline`
    handles, two `VkDescriptorSetLayout` handles, and two `VkDescriptorSet` handles
    in the `VkRenderData.h` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: With these handles, two new Vulkan pipelines are created to draw the models
    with and without the special selection handling. For further Vulkan implementation
    details, check out the `VkRenderer.cpp` file.
  prefs: []
  type: TYPE_NORMAL
- en: We need to adjust the selection vertex shader to draw the morphed face meshes
    in the selection buffer. Without the selection shader, the head of the instances
    would no longer be selectable by clicking the mouse button.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the shader code itself, we can reuse and extend the existing files. Copy
    the following four files to the new file names:'
  prefs: []
  type: TYPE_NORMAL
- en: '`assimp_skinning.vert` to `assimp_skinning_morph.vert`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assimp_skinning.frag` to `assimp_skinning_morph.frag`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assimp_skinning_selection.vert` to `assimp_skinning_morph_selection.vert`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assimp_skinning_selection.frag` to `assimp_skinning_morph_selection.frag`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fragment shaders with the `.frag` extension will not be changed, but for
    further changes or debugging it is always better to use separate files for the
    new shaders.
  prefs: []
  type: TYPE_NORMAL
- en: 'In both vertex shader files ending with `.vert`, add the following lines to
    define the new `MorphVertex` `struct`, matching the `OGLMorphVertex` struct defined
    in the `OGLRenderData.h` file in the `opengl` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: For Vulkan, the name of the original `struct` is `VkMorphVertex`, defined in
    `VkRenderData.h` in the `vulkan` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, add two new SSBOs named `AnimMorphBuffer` and `AnimMorphData` on binding
    points four and five:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The first buffer, `AnimMorphBuffer`, contains the vertices of all morph available
    animations. In the second buffer, `AnimMorphData`, all instance settings are handed
    over from the CPU to the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the `main()` method of the vertex shader, we calculate the offset to
    the vertices for the desired face animation by multiplying the vertex count and
    the index of the face animation clip for every instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: It is no problem to cast the `float` values to `int` here to avoid another struct
    with separate integer and floating-point values and to use `glm::vec4` as a “transportation
    object.” The first inaccurate integer represented by a float would be 2^(24)+1,
    and that value is large enough even for bigger meshes with many face animations
    (2^(24)+1 would be ~16 MiB of vertex data).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we extract the vertices from the morph mesh by using the `gl_VertexID`
    internal shader variable (`gl_VertexIndex` for Vulkan):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can mix the positions of the original vertex and the replacement vertex
    from the morph mesh according to the weight factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We do the same extraction and calculation work for the vertex normals since
    a position change of a vertex may also affect the normal of the triangle the vertex
    is part of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: For a better selection of meshes with and without morph animations, we also
    need some small changes in the `AssimpModel` class and the renderer.
  prefs: []
  type: TYPE_NORMAL
- en: Finalizing the face animation code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the `AssimpModel` class, two new drawing methods are added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The names say what the methods are doing: `drawInstancedNoMorphAnims()` draws
    all meshes without morph animations, and `drawInstancedMorphAnims()` draws only
    the meshes with morph animations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To filter the meshes in both methods, a quite simple check is used – we loop
    over all the meshes and look at the size of the `morphMeshes` vector inside the
    mesh. For the `drawInstancedNoMorphAnims()` method that draws only non-morph meshes,
    we simply skip the meshes without extra morph meshes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'And for the morph-mesh-only version, we reverse the check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason for using two separate methods lies in the `draw()` method of the
    renderer class files, `OGLRenderer.cpp` or `VkRenderer.cpp`. There, we replace
    the normal instanced draw call for the animated models with the no-morph version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Drawing the meshes with morph animations is done after checking whether the
    model contains any morph meshes, as shown in the following code of the OpenGL
    renderer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we use the new morphing shader, bind the two SSBOs containing
    the vertex data and the per-instance settings, and use the draw call that only
    draws meshes with morph animations.
  prefs: []
  type: TYPE_NORMAL
- en: For a simple test, you can set the `morphData` values in the renderer to some
    fixed values and check whether the instances run around angry or smiling. But
    to have full control of the new settings, we will also add a combo box and a slider
    to the UI.
  prefs: []
  type: TYPE_NORMAL
- en: Adding UI elements to control face animations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the UI, the amount of code is small. We just need a combo box, mapping the
    `isFaceAnimType` value of `InstanceSettings` to the string from `micFaceAnimationNameMap`,
    and a float slider linked to `isFaceAnimWeight`. With a model without a face animation,
    we simply disable the combo box and the slider.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10.3* shows the expanded combo box with the four face animations plus
    the `None` setting to disable face animations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_10.03_B22428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: UI settings to control the face animations of an instance'
  prefs: []
  type: TYPE_NORMAL
- en: We can choose the face animation clip now, and by using the weight slider, we
    can control how much of the face animation morph will be used.
  prefs: []
  type: TYPE_NORMAL
- en: As the last step for the face animations implementation, we will cover how to
    add the new settings to the YAML configuration file.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and loading the new instance settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Luckily, the current state of the YAML parser and emitter code allows us to
    add face animations with truly little effort. Since the face animation settings
    are set per instance, we need to extend the YAML emitter output operator for `InstanceSettings`
    in the `YamlParser.cpp` file in the `tools` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right after the output of the optional node tree setting, we check whether
    a face animation is configured and output the instance settings if a clip was
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'To output the `faceAnimation` `enum`, we also need a definition of the output
    operator for the new data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `YamlParserTypes.h` file, we also need a simple `decode()` method for
    the new `faceAnimation` data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `encode()` method is not shown here, it essentially does the same as all
    other `enum` `encode()` methods do: it casts the node data to an `int`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we just must extend the `decode()` method for the `ExtendedInstanceSettings`,
    adding the two new values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The `encode()` method extension is also super simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to bump the version of the configuration file since we added new data
    to it. In this case, changing the file version is less crucial since parser versions
    from previous chapters simply do not know the new settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'And that’s it! When you select an instance now, you can change the type and
    strength of the facial expression, as shown in *Figure 10.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_10.04_B22428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: A “worried” instance next to the default face mesh'
  prefs: []
  type: TYPE_NORMAL
- en: The face animations can be controlled for every instance, and changes to one
    instance do not affect other instances. It is up to you if you want the instances
    to be angry, worried, happy, or even surprised, and how much of that expression
    will be shown.
  prefs: []
  type: TYPE_NORMAL
- en: Adding more morph clips to the application is also easy. The most complex thing
    for new clips will be most probably the vertex animation in a tool such as Blender.
    In the code, it is just adding a new value to the `faceAnimation` `enum` `class`
    and a new string to the name mapping in the `micFaceAnimationNameMap` variable
    of the `ModelInstanceCamData` `struct`.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to use the new face animations in a node tree too, we need to create
    a new node type, allowing us to control both the animation clip and the weight
    of the desired morph target animation. So, let’s add the all-new **FaceAnim**
    node.
  prefs: []
  type: TYPE_NORMAL
- en: Using face animations in node trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating a new node type is easy. First, we add a new class, `FaceAnimNode`,
    consisting of the `FaceAnimNode.h` header and the implementation file, `FaceAnimNode.cpp`,
    both placed in the `graphnodes` folder. We can borrow most of the implementation
    from the WaitNode, adding ImGui elements and a bit of logic to control the face
    animation during the execution time. *Figure 10.5* shows the final layout of the
    FaceAnim node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_10.05_B22428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: The new FaceAnim node'
  prefs: []
  type: TYPE_NORMAL
- en: The node allows us to choose one of the face animation clips, including the
    `None` setting to disable face animations on an instance, starting and ending
    weights for the animation in both play directions, and a timer to control how
    long the animation replay will take.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can add the new `FaceAnim` node, we must extend the `enum` containing
    the node types and the graph node factory class.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the code for the new FaceAnim node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to WaitNode, the `FaceAnim` node delays the control flow until the timer
    reaches zero and informs the parent node about it once the animation replay has
    ended.
  prefs: []
  type: TYPE_NORMAL
- en: Next to adding the two new files, creating the new node type needs two extra
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must extend the `graphNodeType` `enum` in the `Enums.h` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the constructor and the `makeNode()` method of the `GraphNodeFactory`
    class must be made aware of the new node. In the constructor, we add the node
    title string to `mGraphNodeTypeMap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In `makeNode()`, we add a case block for the new node type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Now we can adjust the implementation of the new `FaceAnimNode` class.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the FaceAnim node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we will blend manually between the two weight values, we will map the blend
    time to a range between zero and one in the `update()` method once the node is
    active:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: By doing a simple division, surrounded by a check to avoid a division by zero,
    the time difference in `morphTimeDiff` will go from one to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we interpolate the final weight by using the product of the time difference
    and weight difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'On every run of the `update()` method, we continuously sent the new weight
    via the `fireNodeOutputCallback` to the renderer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Before doing the weight update, we send the desired animation clip index in
    the `activate()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'To signal the face animation values to the renderer, the `instanceUpdateType`
    `enum` needs to be extended by two new values, `faceAnimIndex` and `faceAnimWeight`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need the `faceAnimation` type in the `nodeCallbackVariant` variant
    to use the new data type in the callbacks between the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Since we use the `fireNodeOutputCallback` in the node, both the `GraphEditor`
    and the `SingleInstanceBehavior` classes need to be extended.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `GraphEditor.cpp` file in the `graphnodes` folder, the `faceAnim` node
    type must be added to the `createNodeEditorWindow()` method to bind the node action
    callback to the newly created `faceAnim` nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'A similar check exists in the `SingleInstanceBehavior` copy constructor; we
    also have to add the `faceAnim` node type here to bind the node action callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: In addition to a manipulation of the `InstanceSettings` variables when changing
    the face animation settings, we add two new setters to the `AssimpInstance` class
    for simplified access to the new `InstanceSettings` variables.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling instance and renderer to react to face animation changes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Updating the `InstanceSettings` by reading all data first plus writing all
    data back at the end is good if we need to adjust more than multiple values. For
    a single change, separate setters are easier to use. We add the two new `public`
    methods `setFaceAnim()` and `setFaceAnimWeight()` to the `AssimpInstance` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Both methods update the two values in the `InstanceSettings` data of the instance,
    plus a bit of extra logic to handle the `none` value of the `faceAnimation` `enum`.
  prefs: []
  type: TYPE_NORMAL
- en: As the last step for the new node, the `updateInstanceSettings()` method of
    the renderer class, `OGLRenderer.cpp` or `VkRenderer.cpp`, needs to know what
    to do when an instance wants to change the face animation settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, in the `switch` block for the node type, a new `case` block for
    the new `faceAnim` node type must be added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we check for the type of face animation update we have received. Since
    we need to react to a change to the face animation clip index and the clip weight,
    a new `switch`/`case` statement with the two update types is added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to close the case block for the `faceAnim` node type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: For the two new update types, `faceAnimIndex` and `faceAnimWeight`, the newly
    added methods in the `AssimpInstance` class will be called, using the data from
    the `nodeCallbackVariant` variant as parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'This last step completes the chain from the new node to the renderer, allowing
    us to use the face animations in the node editor. The FaceAnim node to the node
    tree of the man’s model can be used to change the wave animations of all instances
    to let the instance wave with a smiling face, as shown in *Figure 10.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_10.06_B22428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Combined “Wave” action and “Happy” face animation in a node tree'
  prefs: []
  type: TYPE_NORMAL
- en: More additions to the node tree are possible. You can make the models angry
    before punching or kicking, or surprised before playing the picking animation
    to simulate that the instance has seen something on the ground. And with more
    skeletal and face animation clips, even more funny and crazy combinations can
    be created.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to see someone’s mood in their face helps us as humans to evaluate
    possible next steps, and bringing facial expressions into the application enables
    a much broader way to interact with the instances. By using morph target animations,
    even our basic low-poly models take on much more personality.
  prefs: []
  type: TYPE_NORMAL
- en: But morph target animations have three severe limitations, which we’ll discuss
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of morph target animations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When using morph target animations, we must take care of these three limits:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of vertices in the mesh must be identical in all animations and frames.
    It is not possible to add or remove vertices within a frame or during an animation,
    or to change the vertex assignment to triangles. You can only move the vertices
    around.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The entire mesh of the morphing part of the model must be replicated for every
    morph key in an animation. For smaller parts of the model’s body, this may be
    okay, but having a high-detail part of the body several times in memory may create
    a noticeable overhead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only vertex position changes are supported, the morphing is usually done by
    a simple linear interpolation. Small rotations can be simulated by position changes
    but moving vertices by a large rotation or scaling, like for turning the head
    or moving the hands around, will result in visual distortions during the interpolation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can test the third limit by yourself in Blender. To do that, add a morph
    target-based rotation of the head, and you will see the rotation also affects
    the volume of the head. The larger the rotation angle gets, the bigger the distortion
    during the animation becomes.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10.7* shows the result of around 50% of a head rotation by 180 degrees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_10.07_B22428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Distorted volume while rotating the head with a morph target animation'
  prefs: []
  type: TYPE_NORMAL
- en: Morph target animations must be created and tested carefully while animating
    a model. They are a valuable addition to animations, but they still have some
    drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: How do we create advanced animations that may need rotations without using morph
    target animations, such as a fancy head movement animation to let the model look
    around in the virtual world?
  prefs: []
  type: TYPE_NORMAL
- en: That’s what we’ll learn next as we dive into additive blending.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing additive blending
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Additive blending is an alternative method of animation blending. While *normal*
    animation blending is used to interpolate between two skeletal animation clips
    and morph target animations are changing vertex positions of a mesh, additive
    blending *stacks* two different skeletal animations on top of each other.
  prefs: []
  type: TYPE_NORMAL
- en: The technical part of additive blending is astonishingly simple, but the effect
    achieved by the combination of two different skeletal animations leads to a much
    more natural appearance of a 3D model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore the similarities and differences between additive blending and
    the animation blending methods we already know.
  prefs: []
  type: TYPE_NORMAL
- en: How additive blending works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The basic idea of additive blending comes from the desire to split up model
    animations into multiple and especially independent parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'A skeletal animation usually delivers an animation for the entire model, allowing
    the model to either run, walk, punch, or jump. Blending between skeletal animations
    will smooth the transition between these two clips but won’t add new movements.
    So, there are two different approaches: splitting the skeleton or stacking animations.'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the skeleton into two or even more animation domains and playing a
    different clip for each part of the skeleton is called **layered blending**. Layered
    blending is a simple and cost-effective way to mix animations since every node
    of the skeleton is affected only by the transforms of a single animation clip,
    just the animation clips are different for the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: But splitting the model skeleton into multiple parts with each part playing
    a different skeletal animation may lead to extra effort to synchronize clips across
    the body. Failures in a synchronous replay on a split-skeleton animation may lead
    to visual artifacts, just think of different replay speeds for the clips.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t handle layered blending in the book, but a task in the *Practical sessions*
    section is available to implement layered animation blending for the models. In
    contrast, additive blending allows *adding up* skeletal animations on top of other
    skeletal animations. While the basic movement created by the basic skeletal animation
    is applied normally, property changes of one or even more other skeletal animations
    are added to the nodes of the model, creating a concatenated motion with what’s
    provided by the basic animation. Additive blending is more expensive to calculate
    than layered blending because we need to calculate multiple animations, and we
    also have to mix all the animations together. As an example, this simple addition
    of property changes allows us to add a head movement to the normal skeletal animations.
    The model will be able to run, walk, punch, or jump from a skeletal animation
    *and* move the head around at the same time. As a bonus, facial animations are
    not affected by additive blending, so a model can walk, look to the right, and
    smile, and all three animations are running in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: For the technical implementation part, an additive animation is done by adding
    the differences in the node transformations between the current pose and a reference
    pose to another skeletal animation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use another example to explain the technical side. *Figure 10.8* shows
    the first and the last keyframe for an animation that only rotates the head node
    of the model to the right (from the model’s perspective), while all other nodes
    remain in the T-pose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_10.08_B22428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Start and end pose for the additive animation “head look right”'
  prefs: []
  type: TYPE_NORMAL
- en: As the reference pose, we use the first keyframe, with the entire model in the
    T-pose. To calculate the values for an additive animation, we take the desired
    keyframe and simply subtract the translation, rotation, and scale values of the
    reference pose from the destination pose.
  prefs: []
  type: TYPE_NORMAL
- en: If the model remains in the T-pose, all values for the additive animation will
    be zero. And nothing is added to the running skeletal animation, for instance,
    the walking cycle.
  prefs: []
  type: TYPE_NORMAL
- en: When we advance further in the animation clip in *Figure 10.8*, the rotation
    of the head will lead to a bigger difference in the rotation value of the head
    node between the current pose and the reference pose. But we will get only a difference
    for the head node, all other node transformations are still identical to the reference
    pose.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the difference of the head node rotation to the currently running skeletal
    animation clip is easy. Since we collect the transformation properties for all
    nodes, a combination of two skeletal animations is just a simple per-node addition
    of the values for translation and scale, and a quaternion multiplication for the
    rotation value.
  prefs: []
  type: TYPE_NORMAL
- en: This addition only changes the values for nodes that have changed in the animation
    clip of the additive animation clip compared to the reference pose. All nodes
    without changes in the additive animation will remain unaltered in the skeletal
    animation.
  prefs: []
  type: TYPE_NORMAL
- en: How to create suitable animations
  prefs: []
  type: TYPE_NORMAL
- en: Creating animations to use in additive blending is out of the scope of this
    book, similar to creating face animations. You can use a tool like Blender, or
    use the man’s model from *Chapter 10*, which already contains four extra animations
    altering only the head.
  prefs: []
  type: TYPE_NORMAL
- en: If you create extra animations by yourself, make sure to prefix the clip names
    with something common, like an underscore, or the letters *ZZZ_* to keep them
    grouped together. At least Blender tends to sort the clips by name during the
    export and since we store several clip mappings based on the index number of the
    clip in the YAML configuration file, adding the new clips at the start or somewhere
    in-between the existing clips would lead to broken config files.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing additive animations in our application is also surprisingly simple.
  prefs: []
  type: TYPE_NORMAL
- en: Extending the code to support additive animations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To bring the additional data to the GPU, we add four new variables to the `struct`
    `PerInstanceAnimData` in the `OGLRenderData.h` file in the `opengl` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: As always for Vulkan, the file is called `VkRenderData.h` and resides in the
    `vulkan` folder.
  prefs: []
  type: TYPE_NORMAL
- en: We split the head animation into two parts and use separate variables to control
    both the left/right animation and the up/down animation of the head. It’s impossible
    to move the head to the left and right at the same time, so we can combine those
    two directions into a single control variable. The same holds true for moving
    the head up and down.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we create a new `private` compute shader called `mAssimpTransformHeadMoveComputeShader`
    in the renderer class file, `OGLRanderer.cpp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'For Vulkan, we add a new `VkPipeline` handle named `rdAssimpComputeHeadMoveTransformPipeline`
    to the `VkRenderData.h` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Since not all models may have additive head animations, we could skip the extra
    calculations if no head animations were set. We also add the new variables in
    the `PerInstanceAnimData` `struct` to the compute shader file, `assimp_instance_transform.comp.`
    The new head animation variables will be ignored, but we need to expand the struct
    to the same size in both shaders.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we copy the `assimp_instance_transform.comp` file to `assimp_instance_headmove_transform.comp`
    and load the new file into the new `mAssimpTransformHeadMoveComputeShader` compute
    shader during the `init()` method of the `OGLRenderer.cpp`. For Vulkan, we create
    the new rendering pipeline loading the head transform compute shader in the `createPipelines()`
    method of `VkRenderer.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the new shader file, most of the additions are just copy and paste work.
    We must do the following steps in the extended compute shader, using the code
    for the rotational part of the left/right head movement as examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the animation clip numbers for both head animations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the inverse scale factors for both head animations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the index values for accessing the lookup data of the head animations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get translation, rotation, and scale values for both the reference pose at
    the first lookup positions and the desired head animation clip timestamps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the difference between the transform values of the current pose and
    the references pose, using a quaternion multiplication for the rotation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add up the differences for translation, rotation, and scale for both head animations
    to a single value for each transform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the summed-up differences to the first and second clip transforms, again
    using a quaternion multiplication for the rotation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The instances also need to carry information about the head movements, so we
    add the two new variables, `isHeadLeftRightMove` and `isHeadUpDownMove`, to the
    `InstanceSettings` `struct` in the `InstanceSettings.h` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: We map the positive ranges between `0.0f` and `1.0f` to a head movement to the
    left and upwards, and the negative range from `0.0f` to `-1.0f` to move the head
    to the right or down. A movement value of zero will use the values of the reference
    pose for both animations, resulting in no head movement at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Filling the new data in the `PerInstanceAnimData` `struct` is done in the `draw()`
    call of the `OGLRenderer` or `VKRenderer` class, in the same part of the code
    as the facial animations. Following the mapping explained before, selecting the
    clip number is done as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'For head movement, we use the absolute value of the timestamp:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Hard coding the clip numbers in the code is a bad idea, different models have
    these new animations on other clip indices, or not at all. Let’s add another mapping,
    this time between the additive blending clip numbers and the four possible directions
    of the head movement.
  prefs: []
  type: TYPE_NORMAL
- en: Creating mappings for the new head animations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a mapping between clips and head animations, a new `enum` `class` called
    `headMoveDirection` is created in the `Enums.h` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding string map `micHeadMoveAnimationNameMap` to show the names
    in the UI is added to the `ModelInstanceCamData` `struct`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'And since the mapping is model-related, the new `msHeadMoveClipMappings` mapping
    is added to the `ModelSettings` `struct` in the `ModelSettings.h` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The `AssimpModel` class also gets a new `public` method to check whether all
    the mappings in `msHeadMoveClipMappings` are active:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Failing to find at least one of the head animations leads to a disabled additive
    head animation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `draw()` call of the `OGLRenderer.cpp` files, we switch the compute
    shader based on the availability of all head animation mappings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'For Vulkan, we use the availability of head move animations to choose the pipeline
    to bind for the compute shader in the `runComputeShaders()` method of the `VkRenderr.cpp`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The UI part for the head animations can be copied mostly from other parts of
    the code in the `UserInterface` class. A combo box to select the clip, a loop
    over all four values in the `headMoveDirection` `enum`, two buttons, and two sliders
    to test the animations are all we need to create a new UI section, as shown in
    *Figure 10.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_10.09_B22428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: UI control for the head movement/animation clip mapping'
  prefs: []
  type: TYPE_NORMAL
- en: Clip mapping and clips are taken from the model of the currently selected instance,
    making it easy to configure the additive head animations for all models.
  prefs: []
  type: TYPE_NORMAL
- en: To use the head animation in node trees, another new node type is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a head animation node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thanks to the previous `FaceAnimNode` node, adding a new `HeadAnimNode` is done
    in minutes. You can follow the steps in the *Using face animations in node trees*
    section to create the new node as you have to do the same actions as with the
    `FaceAnimNode`. Only a couple of minor changes are needed, like the names of the
    `enum` `class` entries.
  prefs: []
  type: TYPE_NORMAL
- en: For the UI part of the new node, you can reuse the `FaceAnimNode` class code
    for the controls and copy the code to switch the two sections on or off from the
    `InstanceNode` class code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final **HeadAnim** node to be used in a node tree looks like in *Figure
    10.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_10.10_B22428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: The HeadAnim node'
  prefs: []
  type: TYPE_NORMAL
- en: Like the Instance node, we can control which head animation values we want to
    change, and for each animation, we can adjust the starting and ending weight plus
    the time it takes to blend between the two weight values. And like the FaceAnim
    node, the HeadAnim node delays the control flow until both timers are expired
    and signals the end of the execution to the parent node.
  prefs: []
  type: TYPE_NORMAL
- en: We close the chapter with the changes needed to save and load the settings for
    the additive head animations.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and loading the head animation settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the new tree node, implementing the YAML configuration file changes
    to save and load head animation settings is a matter of minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the `ModelSettings` YAML emitter output in the `YamlParser.cpp` file, we
    add the clip mappings directly from the map if all four clips are configured.
    We also need a new emitter output for the `headMoveDirection` `enum`, casting
    the value to an `int`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'To load the mapping back, we add a section to the `decode()` method for the
    `ModelSettings` in the `YamlParserTypes.h` file, reading back the map values one
    by one. A new `decode()` method for the `headMoveDirection` `enum` is needed here
    too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'And for the instance settings, the two `float` values stored in the `isHeadLeftRightMove`
    and `isHeadUpDownMove` of the `InstanceSettings` are added to the emitter in `YamlParser.cpp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'And the two values are also to the `decode()` method for the `ExtendedInstanceSettings`
    data type in `YamlParserTypes.h`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'After all these additions, you can add some HeadAnim nodes to the node tree
    of the man’s model, creating animations like the one shown in *Figure 10.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_10.11_B22428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: The instance looks up while waving and smiling'
  prefs: []
  type: TYPE_NORMAL
- en: The instance can now turn the head at any time in a natural manner, we just
    need to add the new HeadAnim node to the control flow. If you go back to *Figure
    10.6*, you will see that a small addition like the head movement makes an enormous
    difference in the appearance of the instance.
  prefs: []
  type: TYPE_NORMAL
- en: You can let your imagination flow about other possibilities for the new head
    movement. Do you want the head to follow the camera or any other nearby instance?
    Do you want to nod the head to signal *yes* and shake the head slightly if the
    answer to a question is *no*? Do you want to make the player look up or down to
    move the player’s point of interest toward the sky or the floor? Some ideas are
    listed in the *Practical sessions* section if you want to extend the code.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we added both facial expressions and separate head animations
    to the instances. We started with a brief exploration of face animations. Then
    we implemented face animations in the form of morph target animations to code
    and shaders, enabling the instances to smile or be angry. Next, we added a tree
    node for the face animations, enabling us to use the new facial expressions in
    the node trees. Finally, we looked at additive animation blending and added head
    movement by using additive blending, including a new tree node and UI controls.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will leave the animation controls for a while and give
    the instances literally a room to live in by adding level data to the virtual
    world, like a game level. We start by checking the formats supported by the Open
    Assimp Importer Library and search for available level files. Then, we explore
    reasons why we should separate level data from models and instances. Finally,
    we load the level data from a file and add the level-related data to the application
    and renderer.
  prefs: []
  type: TYPE_NORMAL
- en: Practical sessions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some additions you could make to the code:'
  prefs: []
  type: TYPE_NORMAL
- en: Add support for multiple meshes containing morph target animations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Currently, only one mesh of a model can have morph target animations. For a
    simple head animation, this limit is fine. But if you want to control more than
    one part of the face, using multiple morph targets may be helpful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add blending between two morph targets. For even better and more natural facial
    expressions, a direct blending between two morph targets would be nice. A detour
    via the neutral position is no longer needed, then, but a direct path between
    anger and worry is available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add more morph targets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may try to let the models *speak*. You could add expressions for different
    vowels and consonants, including the direct blending from the previous task. With
    such animations, you could mimic instances speaking to you during interactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add layered/masked animations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In contrast to additive animation blending, layered blending uses different
    skeletal animation clips for distinct parts of the model’s virtual body. For instance,
    all but the right arm uses the running animation and only the right arm plays
    the waving animation. As noted in the *How additive blending works* section, layered
    animations may need additional effort to synchronize the two animation clips.
    You need to add some logic to mask out parts of the model’s skeleton.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let the instance turn their head toward you on interaction. This is a feature
    of many games: if you start interacting with an instance, they turn their head
    around to look directly toward you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let nearby instances *judge* you. This is like the previous task: you could
    also add the additive head-turning animation to instances walking near you. Add
    a random facial expression too, enabling the instances to show some sort of emotion
    when they pass near you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let the instances smile and wave at the nearest instance passing by.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is a combination and extension of the two previous tasks: use the interaction
    logic to find the nearest instance for every instance in the virtual world, then
    move the head towards that instance. Play the smiling morph animation and a new
    additive animation with only the right arm waving. You might want to use a layered
    animation for the arm here.'
  prefs: []
  type: TYPE_NORMAL
- en: Add more additive blending animations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turning the head around is a good start, but what about doing an entire animation
    of someone looking around? Try to add more layers of additive blending animations
    to make instances make gestures in interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Optimize C++ and shader code for better performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current C++ and GLSL shader code and the data structures on the CPU and
    GPU were created to explain and explore the features we added here, like morph
    target animations and facial expressions. There is a lot of room left for optimization,
    both on the CPU and the GPU. You could try to squeeze more frames per second out
    of the application, for instance, by optimizing the data types sent to the GPU,
    by moving more work to compute shaders, or by removing the busy-waits for the
    shader results on Vulkan. You could also check if data compression has a positive
    or negative outcome on the frame times. For an easy comparison, add a checkbox
    to the UI to switch between the default code and the optimized version.
  prefs: []
  type: TYPE_NORMAL
- en: Additional resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Layered animations in Unreal Engine: [https://dev.epicgames.com/documentation/en-us/unreal-engine/using-layered-animations-in-unreal-engine](https://dev.epicgames.com/documentation/en-us/unreal-engine/using-layered-animations-in-unreal-engine)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additive animations in Unreal Engine: [https://dev.epicgames.com/documentation/en-us/unreal-engine/additive-vs.-full-body?application_version=4.27](https://dev.epicgames.com/documentation/en-us/unreal-engine/additive-vs.-full-body?application_version=4.27)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unity Animation Layers: [https://docs.unity3d.com/Manual/AnimationLayers.html](https://docs.unity3d.com/Manual/AnimationLayers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Godot Animation Trees: [https://docs.godotengine.org/en/latest/tutorials/animation/animation_tree.html](https://docs.godotengine.org/en/latest/tutorials/animation/animation_tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blender: [https://www.blender.org](https://www.blender.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blender Shape Keys: [https://docs.blender.org/manual/en/latest/animation/shape_keys/introduction.html](https://docs.blender.org/manual/en/latest/animation/shape_keys/introduction.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
