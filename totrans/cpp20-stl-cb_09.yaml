- en: '*Chapter 9*: Concurrency and Parallelism'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency and parallelism refer to the ability to run code in separate *threads
    of execution*.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, *concurrency* is the ability to run threads in the background,
    and *parallelism* is the ability to run threads simultaneously in separate cores
    of a processor. The run-time library, along with the host operating system, will
    choose between concurrent and parallel execution models for a given thread on
    a given hardware environment.
  prefs: []
  type: TYPE_NORMAL
- en: In a modern multi-tasking operating system, the `main()` function already represents
    a thread of execution. When a new thread is started, it's said to be *spawned*
    by an existing thread. A group of threads may be called a *swarm*.
  prefs: []
  type: TYPE_NORMAL
- en: In the C++ standard library, the `std::thread` class provides the basic unit
    of threaded execution. Other classes build upon `thread` to provide *locks*, *mutexes*,
    and other concurrency patterns. Depending on system architecture, execution threads
    may run concurrently on one processor, or in parallel on separate cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover these tools and more in the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Sleep for a specific amount of time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `std::thread` for concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `std::async` for concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run STL algorithms in parallel with execution policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share data safely with mutex and locks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share flags and values with `std::atomic`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize threads with `std::call_once`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `std::condition_variable` to resolve the producer-consumer problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement multiple producers and consumers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/CPP-20-STL-Cookbook/tree/main/chap09](https://github.com/PacktPublishing/CPP-20-STL-Cookbook/tree/main/chap09).
  prefs: []
  type: TYPE_NORMAL
- en: Sleep for a specific amount of time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `<thread>` header provides two functions for putting a thread to sleep,
    `sleep_for()` and `sleep_until()`. Both functions are in the `std::this_thread`
    namespace.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe explores the use of these functions, as we will be using them later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look at how to use the `sleep_for()` and `sleep_until()` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sleep-related functions are in the `std::this_thread` namespace. Because
    it has just a few symbols, we''ll go ahead and issue `using` directives for `std::this_thread`
    and `std::chrono_literals`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `chrono_literals` namespace has symbols for representing durations, such
    as `1s` for one second, or `100ms` for 100 milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `main()`, we''ll mark a point in time with `steady_clock::now()`, so we
    can time our test:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `sleep_for()` function takes a `duration` object to specify the amount of
    time to sleep. The argument `(1s + 300ms)` uses `chrono_literal` operators to
    return a `duration` object representing 1.3 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: The `sleep_until()` function takes a `time_point` object to specify a specific
    time to resume from sleep. In this case, the `chrono_literal` operators are used
    to modify the `time_point` object returned from `steady_clock::now()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `sleep_for(duration)` and `sleep_until(time_point)` functions suspend execution
    of the current thread for the specified `duration`, or until the `time_point`
    is reached.
  prefs: []
  type: TYPE_NORMAL
- en: The `sleep_for()` function will use the `steady_clock` implementation, if supported.
    Otherwise, the duration may be subject to time adjustments. Both functions may
    block for longer due to scheduling or resource delays.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some systems support a POSIX function, `sleep()`, which suspends execution
    for the number of seconds specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `sleep()` function is part of the POSIX standard and is not part of the
    C++ standard.
  prefs: []
  type: TYPE_NORMAL
- en: Use std::thread for concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A *thread* is a unit of concurrency. The `main()` function may be thought of
    as the *main thread of execution*. Within the context of the operating system,
    the main thread runs concurrently with other threads owned by other processes.
  prefs: []
  type: TYPE_NORMAL
- en: The `std::thread` class is the root of concurrency in the STL. All other concurrency
    features are built on the foundation of the `thread` class.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will examine the basics of `std::thread` and how `join()`
    and `detach()` determine its execution context.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we create some `std::thread` objects and experiment with their
    execution options.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with a convenience function for sleeping a thread, in milliseconds:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `sleep_for()` function takes a `duration` object and blocks execution of
    the current thread for the specified duration. This `sleepms()` function serves
    as a convenience wrapper that takes an `unsigned` value for the number of milliseconds
    to sleep.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need a function for our thread. This function sleeps for a variable
    number of milliseconds, based on an integer parameter:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`fthread()` calls `sleepms()` five times, sleeping each time for `100 * n`
    milliseconds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run this in a separate thread with `std::thread` from `main()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It compiles but we get this error when we run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: (Your error message will vary. This is the error message on Debian with GCC.)
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that the operating system doesn't know what to do with the thread
    object when it goes out of scope. We must specify if the caller waits for the
    thread, or if it's detached and runs independently.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `join()` method to indicate that the caller will wait for the thread
    to finish:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, `main()` waits for the thread to finish.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we call `detach()` instead of `join()`, then `main()` doesn''t wait, and
    the program ends before the thread can run:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'When the thread is detached, we need to give it time to run:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start and detach a second thread and see what happens:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Because our `fthread()` function uses its parameter as a multiplier for `sleepms()`,
    the second thread runs a bit slower than the first. We can see the timers interlaced
    in the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we do this with `join()` instead of `detatch()`, we get a similar result:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Because `join()` waits for the thread to finish, we no longer need the 2-second
    `sleepms()` in `main()` to wait for the threads to finish.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `std::thread` object represents a thread of execution. There is a one-to-one
    relationship between object and thread. One `thread` object represents one thread,
    and one thread is represented by one `thread` object. A `thread` object cannot
    be copied or assigned, but it can be moved.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `thread` constructor looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'A thread is constructed with a function pointer and zero or more arguments.
    The function is called immediately with the arguments provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This creates the object `t1` and immediately calls the function `fthread(int)`
    with the literal value `1` as the argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the thread, we must use either `join()` or `detach()` on the
    thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `join()` method blocks execution of the calling thread until the `t1` thread
    has completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `detach()` method allows the calling thread to continue independently of
    the `t1` thread.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'C++20 provides `std::jthread`, which automatically joins the caller at the
    end of its scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This allows the `t1` thread to execute independently and then automatically
    join the `main()` thread at the end of its scope.
  prefs: []
  type: TYPE_NORMAL
- en: Use std::async for concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`std::async()` runs a target function asynchronously and returns a `std::future`
    object to carry the target function''s return value. In this way, `async()` operates
    much like `std::thread` but allows return values.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider the use of `std::async()` with a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In its simplest forms, the `std::async()` function performs much the same task
    as `std::thread`, without the need to call `join()` or `detach()` and while also
    allowing return values via a `std::future` object.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll use a function that counts the number of primes in a range.
    We'll use `chrono::steady_clock` to time the execution of each thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start with a couple of convenience aliases:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`std::launch` has launch policy constants, for use with the `async()` call.
    The `secs` alias is a `duration` class, for timing our prime number calculations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our target function counts prime numbers in a range. This is essentially a
    way to understand the execution policies by eating some clock cycles:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `prime_time` structure is for the return value, with elements for duration
    and count. This allows us to time the loop itself. The `isprime` lambda returns
    `true` if a value is prime. We use `steady_clock` to calculate the duration of
    the loop that counts primes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `main()`, we call our function and report its timing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can run `count_primes()` asynchronously with `std::async()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we call `async()` with our `count_primes` function and the `MAX_PRIME`
    parameter. This runs `count_primes()` in the background.
  prefs: []
  type: TYPE_NORMAL
- en: '`async()` returns a `std::future` object, which carries the return value of
    an asynchronous operation. The `future` object''s `get()` method blocks until
    the asynchronous function has completed and then returns the return object from
    the function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This runs with almost the same timing as we got without `async()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `async()` function optionally takes execution policy flags as its first
    parameter:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The choices are `async` or `deferred`. These flags are in the `std::launch`
    namespace.
  prefs: []
  type: TYPE_NORMAL
- en: The `async` flag enables asynchronous operation, and the `deferred` flag enables
    lazy evaluation. These flags are bitmapped and may be combined with the bitwise
    or `|` operator.
  prefs: []
  type: TYPE_NORMAL
- en: The default is for both bits to be set, as if `async | deferred` was specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run several instances of our function simultaneously with `async()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We know that `async` returns a `future` object. So, we can run 15 threads by
    storing the `future` objects in a container. Here''s our output on a 6-core i7
    running Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Even though the 6-core i7 is not able to run all the processes in separate cores,
    it still completes 15 instances in under 6 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: It looks like it finishes the first 13 threads in about 4 seconds, and then
    takes another 2 seconds to finish the last 2 threads. It appears to take advantage
    of Intel's Hyper-Threading technology that allows 2 threads to run in one core
    under some circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run the same code on a 12-core Xeon, we get this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The 12-core Xeon gets through all 15 processes in under a second.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key to understanding `std::async` is in its use of `std::promise` and `std::future`.
  prefs: []
  type: TYPE_NORMAL
- en: The `promise` class allows a `thread` to store an object that may later be retrieved
    asynchronously by a `future` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say we have a function like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can run it with `std::thread`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: That works fine for a simple function with no return value. When we want to
    return a value from `f()`, we can use `promise` and `future`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We set up the promise and future objects in the `main()` thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'And we pass the `promise` object to our function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note that a `promise` object cannot be copied, so we need to use `std::move`
    to pass it to the function.
  prefs: []
  type: TYPE_NORMAL
- en: The `promise` object serves as a bridge to a `future` object, which allows us
    to retrieve the value when it becomes available.
  prefs: []
  type: TYPE_NORMAL
- en: '`std::async()` is just a helper function to simplify the creation of the `promise`
    and `future` objects. With `async()`, we can do all of that like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: That's the value of `async()`. For many purposes, it makes the use of `promise`
    and `future` much easier.
  prefs: []
  type: TYPE_NORMAL
- en: Run STL algorithms in parallel with execution policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beginning with C++17, many of the standard STL algorithms can run with *parallel
    execution*. This feature allows an algorithm to split its work into sub-tasks
    to run simultaneously on multiple cores. These algorithms accept an execution
    policy object that specifies the kind of parallelism applied to the algorithm.
    This feature requires hardware support.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Execution policies are defined in the `<execution>` header and in the `std::execution`
    namespace. In this recipe, we will test the available policies using the `std::transform()`
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For timing purposes, we''ll use the `duration` object with the `std::milli`
    ratio so that we can measure in milliseconds:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For demonstration purposes, we''ll start with a `vector` of `int` with 10 million
    random values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we apply a simple transformation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `mul2` lambda simply multiplies a value by 2\. The `transform()` algorithm
    applies `mul2` to every member of the vector.
  prefs: []
  type: TYPE_NORMAL
- en: This transformation does not specify an execution policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We can specify an execution policy in the first argument of the algorithm:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `seq` policy means that the algorithm shall not be parallelized. This is
    the same as no execution policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the duration is roughly the same as without a policy. It will never
    be exact because it varies each time it's run.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `execution::par` policy allows the algorithm to parallelize its workload:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the algorithm runs somewhat faster with the parallel execution policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `execution::par_unseq` policy allows unsequenced parallel execution of
    the workload:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Here, we notice another increase in performance with this policy.
  prefs: []
  type: TYPE_NORMAL
- en: The `execution::par_unseq` policy has tighter requirements of the algorithm.
    The algorithm must not perform operations that require concurrent or sequential
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The execution policies interface doesn't specify how the algorithm workloads
    are parallelized. It's designed to work with a diverse set of hardware and processors
    under varying loads and circumstances. It may be implemented entirely in the library
    or rely on compiler or hardware support.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallelization will show the most improvement on algorithms that do more than
    *O(n)* work. For example, `sort()` shows a dramatic improvement. Here''s a `sort()`
    with no parallelization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'With `execution::par`, we see significant performance gains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The improvement with `execution::par_unseq` is better still:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: It's a good idea to do a lot of testing when using the parallelized algorithms.
    If your algorithm or predicates do not lend themselves well to parallelization,
    you may end up with minimal performance gains or unintended side effects.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, execution policies are poorly supported in GCC and not
    yet supported by LLVM/Clang. This recipe was tested on a 6-core i7 running Windows
    10 and a preview release of Visual C++.
  prefs: []
  type: TYPE_NORMAL
- en: Share data safely with mutex and locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term *mutex* refers to *mutually exclusive* access to shared resources.
    A mutex is commonly used to avoid data corruption and race conditions, due to
    multiple threads of execution attempting to access the same data. A mutex will
    typically use *locks* to restrict access to one thread at a time.
  prefs: []
  type: TYPE_NORMAL
- en: The STL provides *mutex* and *lock* classes in the `<mutex>` header.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will use a simple `Animal` class to experiment with *locking*
    and *unlocking* a `mutex`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating a `mutex` object:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `mutex` is declared in the global scope, so it's accessible to all the relevant
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `Animal` class has a name and a list of friends:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Adding and deleting friends will be a useful test case for our `mutex`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equality operator is the only operator we''ll need:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `s_name` member is a `string_view` object, so we can test the address of
    its data store for equality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `is_friend()` method tests if another `Animal` is in the `l_friends` list:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `find_friend()` method returns an `optional`, with an iterator to the `Animal`
    if found:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `print()` method prints `s_name` along with names of each of the `Animal`
    objects in the `l_friends` list:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `add_friend()` method adds an `Animal` object to the `l_friends` list:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `delete_friend()` method removes an `Animal` object from the `l_friends`
    list:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the `main()` function, we create some `Animal` objects:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We call `add_friends()` on our objects with `async()`, to run them in separate
    threads:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We call `wait()` to allow our threads to complete before continuing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We call `print()` to see our `Animals` and their relationships:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And finally, we call `delete_friend()` to remove one of our relationships:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, our output looks like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This output is somewhat scrambled. It will be different each time you run it.
    It may be fine sometimes, but don't let that fool you. We need to add some mutex
    locks to control access to the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to use `mutex` is with its `lock()` and `unlock()` methods. Let''s
    add them to the `add_friend()` function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `lock()` method attempts to acquire a lock on the `mutex`. If the mutex
    is already locked, it will wait (block execution) until the `mutex` is unlocked.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to add a lock to `delete_friend()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to add a lock to `print()` so that data is not changed while printing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, our output is sensible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Your output may have the lines in a different order due to asynchronous operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `lock()` and `unlock()` methods are rarely called directly. The `std::lock_guard`
    class manages locks with a proper `add_friend()` method with `lock_guard`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `lock_guard` object is created and holds a lock until it is destroyed. Like
    the `lock()` method, `lock_guard` also blocks until a lock is available.
  prefs: []
  type: TYPE_NORMAL
- en: Let's apply `lock_guard` to the `delete_friend()` and `print()` methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is `delete_friend()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is `print()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Our output remains coherent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: As before, your output may have the lines in a different order due to asynchronous
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's important to understand that a `mutex` does not lock data; it blocks execution.
    As shown in this recipe, when a `mutex` is applied in object methods, it can be
    used to enforce mutually exclusive access to data.
  prefs: []
  type: TYPE_NORMAL
- en: When one thread locks a `mutex`, with either `lock()` or `lock_guard`, that
    thread is said to *own* the `mutex`. Any other thread that tries to lock the same
    `mutex` will be blocked until it's unlocked by the owner.
  prefs: []
  type: TYPE_NORMAL
- en: The `mutex` object must not be destroyed while it's owned by any thread. Likewise,
    a thread must not be destroyed while it owns a `mutex`. An RAII-compliant wrapper,
    such as `lock_guard`, will help ensure this doesn't happen.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While `std::mutex` provides an exclusive mutex suitable for many purposes,
    the STL does provide a few other choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '`shared_mutex` allows more than one thread to simultaneously own a mutex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recursive_mutex` allows one thread to stack multiple locks on a single mutex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timed_mutex` provides a timeout for mutex blocks. Both `shared_mutex` and
    `recursive_mutex` also have timed versions available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share flags and values with std::atomic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `std::atomic` class encapsulates a single object and guarantees it to be
    *atomic*. Writing to the *atomic object* is controlled by memory-order policies
    and reads may occur simultaneously. It's typically used to synchronize access
    among different threads.
  prefs: []
  type: TYPE_NORMAL
- en: '`std::atomic` defines an *atomic type* from its template type. The type must
    be *trivial*. A type is trivial if it occupies contiguous memory, has no user-defined
    constructor, and has no virtual member functions. All primitive types are trivial.'
  prefs: []
  type: TYPE_NORMAL
- en: While it is possible to construct a trivial type, `std::atomic` is most often
    used with simple primitive types, such as `bool`, `int`, `long`, `float`, and
    `double`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe uses a simple function that loops over a counter to demonstrate
    sharing atomic objects. We will spawn a swarm of these loops as threads that share
    atomic values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Atomic objects are often placed in a global namespace. They must be accessible
    to all the threads that need to share its value:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `ready` object is a `bool` type that gets set to `true` when all the threads
    are ready to start counting.
  prefs: []
  type: TYPE_NORMAL
- en: The `g_count` object is a global counter. It is incremented by each of the threads.
  prefs: []
  type: TYPE_NORMAL
- en: The `winner` object is a special `atomic_flag` type. It is used to indicate
    which thread finishes first.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a couple of constants to control the number of threads and the number
    of loops for each thread:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: I've set it to run 100 threads and count 1,000,000 iterations in each thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `countem()` function is spawned for each thread. It loops `max_count` times
    and increments `g_count` for each iteration of the loop. This is where we use
    our atomic values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `ready` atomic value is used to synchronize the threads. Each thread will
    call `yield()` until the `ready` value is set `true`. The `yield()` function yields
    execution to other threads.
  prefs: []
  type: TYPE_NORMAL
- en: Each iteration of the `for` loop increments the `g_count` atomic value. The
    final value should be equal to `max_count * max_threads`.
  prefs: []
  type: TYPE_NORMAL
- en: After the loop is complete, the `test_and_set()` method of the `winner` object
    is used to report the winning thread. `test_and_set()` is a method of the `atomic_flag`
    class. It sets the flag and returns the `bool` value from before it is set.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve used the `make_commas()` function before. It displays a number with
    thousands of separators:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `main()` function spawns the threads and reports the results:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we create a `vector<std::thread>` object to hold the threads.
  prefs: []
  type: TYPE_NORMAL
- en: In the `for` loop, we use `emplace_back()` to create each `thread` in the `vector`.
  prefs: []
  type: TYPE_NORMAL
- en: Once the threads have been spawned, we set the `ready` flag so that the threads
    may start their loops.
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Every time you run it, a different thread will win.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `std::atomic` class encapsulates an object to synchronize access among multiple
    threads.
  prefs: []
  type: TYPE_NORMAL
- en: The encapsulated object must be a *trivial type*, which means it occupies contiguous
    memory, has no user-defined constructor, and has no virtual member functions.
    All primitive types are trivial.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to use a simple struct with `atomic`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: While this usage is possible, it's not practical. Anything beyond setting and
    retrieving compound values loses the benefits of the atomicity and ends up requiring
    a *mutex*. The atomic class is best suited for *scalar* values.
  prefs: []
  type: TYPE_NORMAL
- en: Specializations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are specializations of the `atomic` class for a few different purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::atomic<U*>` specialization includes support for atomic pointer arithmetic
    operations, including `fetch_add()` for addition and `fetch_sub()` for subtraction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float`, `double`, and `long double`, `std::atomic` includes support for atomic
    floating-point arithmetic operations, including `fetch_add()` for addition and
    `fetch_sub()` for subtraction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::atomic` provides support for additional atomic operations, including
    `fetch_add()`, `fetch_sub()`, `fetch_and()`, `fetch_or()`, and `fetch_xor()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard aliases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The STL provides type aliases for all the standard scalar integral types. This
    means that instead of these declarations in our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'We could use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 46 standard aliases, one for each of the standard integral types:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18267_table_9.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Lock-free variations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most modern architectures provide *atomic CPU instructions* for performing atomic
    operations. `std::atomic` should use hardware support for atomic instructions
    where supported by your hardware. Some atomic types may not be supported on some
    hardware. `std::atomic` may use a *mutex* to ensure thread-safe operations for
    those specializations, causing threads to block while waiting for other threads
    to complete operations. Specializations that use hardware support are said to
    be *lock-free* because they don't require a mutex.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `is_lock_free()` method checks whether a specialization is lock-free:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: This result will be `true` for most modern architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few guaranteed lock-free variations of `std::atomic` available.
    These specializations guarantee the use of the most efficient hardware atomic
    operations for each purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::atomic_signed_lock_free` is an alias for the most efficient lock-free
    specialization of a signed integral type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::atomic_unsigned_lock_free` is an alias for the most efficient lock-free
    specialization of an unsigned integral type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `std::atomic_flag` class provides a lock-free atomic Boolean type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Current Windows systems don't support 64-bit hardware integers, even on 64-bit
    systems. When testing this code on one of these systems in my lab, replacing `std::atomic<uint64_t>`
    with `std::atomic_unsigned_lock_free` resulted in a *3x* performance improvement.
    Performance was unchanged on 64-bit Linux and Mac systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When multiple threads read and write variables simultaneously, one thread may
    observe the changes in a different order than they were written. `std::memory_order`
    specifies how memory accesses are ordered around an atomic operation.
  prefs: []
  type: TYPE_NORMAL
- en: '`std::atomic` provides methods for accessing and changing its managed value.
    Unlike the associated operators, these access methods provide arguments for `memory_order`
    to be specified. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: In this case, `memory_order_seq_cst` specifies *sequentially consistent* ordering.
    So, this call to `fetch_add()` will add 1 to the value of `g_count` with sequentially
    consistent ordering.
  prefs: []
  type: TYPE_NORMAL
- en: 'The possible `memory_order` constants are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`memory_order_relaxed`: This is a *relaxed operation*. No synchronization or
    ordering constraints are imposed; only the operation''s atomicity is guaranteed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory_order_consume`: This is a *consume operation*. Access in the current
    thread that is dependent on the value cannot be reordered before this load. This
    only affects compiler optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory_order_acquire`: This is an *acquire operation*. Access cannot be reordered
    before this load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory_order_release`: This is a *store operation*. Access in the current
    thread cannot be reordered after this store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory_order_acq_rel`: This is both *acquire* and *release*. Access in the
    current thread cannot be reordered before or after this store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory_order_seq_cst`: This is *sequentially consistent* ordering, either
    *acquire* or *release*, depending on the context. A load performs acquire, a store
    performs release, and a read/write/modify performs both. All threads observe all
    modifications in the same order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no `memory_order` is specified, `memory_order_seq_cst` is the default.
  prefs: []
  type: TYPE_NORMAL
- en: Initialize threads with std::call_once
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may need to run the same code in many threads but must initialize that code
    only once.
  prefs: []
  type: TYPE_NORMAL
- en: One solution would be to call the initialization code before running the threads.
    This approach can work but has some drawbacks. By separating the initialization,
    it may be called when unnecessary, or it may be missed when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: The `std::call_once` function provides a more robust solution. `call_once` is
    in the `<mutex>` header.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we use a print function for the initialization, so we can clearly
    see when it''s called:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use a constant for the number of threads to spawn:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need a `std::once_flag` to synchronize the `std::call_once` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Our initialization function simply prints a string to let us know it''s been
    called:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our worker function, `do_print()`, uses `std::call_once` to call the initialization
    function then prints its own `id`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In `main()`, we use a `list` container to manage the `thread` objects:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our output shows the initialization happens first, and only once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Notice that it's not always the first spawned thread (`0`) that ends up calling
    the initialization function, but it is always called first. If you run this repeatedly,
    you'll see thread `0` gets the initialization often, but not every time. You'll
    see thread `0` in the initialization more often on a system with fewer cores.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`std::call_once` is a template function that takes a flag, a *callable* (function
    or functor), and a parameter pack of arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: The callable `f` is called exactly one time. Even if `call_once` is called concurrently
    from several threads, `f` is still called once and only once.
  prefs: []
  type: TYPE_NORMAL
- en: This requires a `std::once_flag` object for coordination. The `once_flag` constructor
    sets its state to indicate that the callable has not yet been called.
  prefs: []
  type: TYPE_NORMAL
- en: When `call_once` invokes the callable, any other calls on the same `once_flag`
    are blocked until the callable returns. After the callable returns, the `once_flag`
    is set, and any subsequent calls to `call_once` return without invoking `f`.
  prefs: []
  type: TYPE_NORMAL
- en: Use std::condition_variable to resolve the producer-consumer problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest version of the *producer-consumer problem* is where you have one
    process that *produces* data and another that *consumes* data, using one *buffer*
    or container to hold the data. This requires coordination between the producer
    and consumer to manage the buffer and prevent unwanted side effects.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we consider a simple solution to the producer-consumer problem
    using `std::condition_variable` to coordinate the processes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with some namespace and alias declarations for convenience:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `lock_guard` and `unique_lock` aliases make it easier to use these types
    without error.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a couple of constants:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Keeping these in one place makes it safer and easier to experiment with different
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re using these global variables for coordinating the data store:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We're using `deque` to hold the data as a **First-In-First-Out** (**FIFO**)
    queue.
  prefs: []
  type: TYPE_NORMAL
- en: '`mutex` is used with the `condition_variable` to coordinate the movement of
    data from producer to consumer.'
  prefs: []
  type: TYPE_NORMAL
- en: The `finished` flag indicates that there is no more data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The producer thread will use this function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `producer()` function loops `num_items` iterations and pushes a number onto
    the `deque` each time through the loop.
  prefs: []
  type: TYPE_NORMAL
- en: We include a `sleep_for()` call to simulate a delay in producing each value.
  prefs: []
  type: TYPE_NORMAL
- en: The `conditional_variable` requires a `mutex` lock to operate. We use `lock_guard`
    (via the `guard_t` alias) to obtain the lock, then push the value onto the `deque`,
    and then call `notify_all()` on the `conditional_variable`. This tells the consumer
    thread that there is a new value available.
  prefs: []
  type: TYPE_NORMAL
- en: When the loop completes, we set the `finished` flag and notify the consumer
    thread that the producer is completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The consumer thread waits for each value from the producer, displays it on
    the console, and waits for the `finished` flag:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `wait()` method waits to be notified by the producer. It uses the lambda
    as a predicate to continue waiting until the `deque` is *not empty* or the `finished`
    flag is set.
  prefs: []
  type: TYPE_NORMAL
- en: When we get a value, we display it and then pop it from the `deque`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We run this in `main()` with simple `thread` objects:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Notice that there's a 200 ms delay between each line. This tells us that the
    producer-consumer coordination is working as expected.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The producer-consumer problem requires coordination between writing and reading
    a buffer or container. In this example, our container is a `deque<size_t>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: The `condition_variable` class can block a thread, or multiple threads, while
    a shared variable is modified. It may then notify other threads that the value
    is available.
  prefs: []
  type: TYPE_NORMAL
- en: '`condition_variable` requires a `mutex` to perform the lock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: The `std::lock_guard` acquires a lock, so we can push a value onto our `deque`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `wait()` method on `condition_variable` is used to block the current thread
    until it receives a notification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'The predicate form of `wait()` is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'The predicate form is used to prevent spurious waking while waiting for a specific
    condition. We use it with a lambda in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: This prevents our consumer from waking until the `deque` has data or the `finished`
    flag is set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `condition_variable` class has two notification methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`notify_one()` unblocks one waiting thread'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notify_all()` unblocks all waiting threads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used `notify_all()` in our example. Because there is only one consumer thread,
    either notification method would work the same.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that `unique_lock` is the *only* form of lock that supports the `wait()`
    method on a `condition_variable` object.
  prefs: []
  type: TYPE_NORMAL
- en: Implement multiple producers and consumers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *producer-consumer problem* is really a set of problems. Solutions will
    differ if the buffer is bounded or unbounded, or if there are multiple producers,
    multiple consumers, or both.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a case with multiple producers, multiple consumers, and a bounded
    (limited capacity) buffer. This is a common condition.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll look at a case with multiple producers and consumers
    and a *bounded buffer*, using a variety of techniques we''ve covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start with some constants for convenience and reliability:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`consumer_wait` is a `duration` object, used with the `consumer` condition
    variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`queue_limt` is the buffer limit – the maximum number of items in the `deque`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_items` is the maximum number of items produced per `producer`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_producers` is the number of spawned producers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_producers` is the number of spawned consumers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we need some objects to control the process:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`q_mutex` controls access to `deque`.*   `cv_producer` is a condition variable
    that coordinates producers.*   `cv_consumer` is a condition variable that coordinates
    consumers.*   `production_complete` is set `true` when all producer threads have
    finished.*   The `producer()` threads run this function:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The passed value `id` is a sequential number used to identify the producer.
  prefs: []
  type: TYPE_NORMAL
- en: The main `for` loop repeats `num_item` times. The `sleep_for()` function is
    used to simulate some work required to produce an item.
  prefs: []
  type: TYPE_NORMAL
- en: Then we obtain a `unique_lock` from `q_mutex` and invoke `wait()` on `cv_producer`,
    using a lambda that checks the size of the `deque` against the `queue_limit` constant.
    If the `deque` has reached maximum size, the `producer` waits for `consumer` threads
    to reduce the size of the `deque`. This represents the *bounded buffer* limit
    on the producer.
  prefs: []
  type: TYPE_NORMAL
- en: Once the condition is satisfied, we push an *item* onto the `deque`. The item
    is a formatted string with the producer's `id`, the size of `qs`, and an item
    number (`i + 1`) from the loop control variable.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we notify the consumers that new data is available, with `notify_all()`
    on the `cv_consumer` condition variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `consumer()` threads run this function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The passed `id` value is a sequential number used to identify the consumer.
  prefs: []
  type: TYPE_NORMAL
- en: The main `while()` loop continues until `production_complete` is set.
  prefs: []
  type: TYPE_NORMAL
- en: We obtain `unique_lock` from `q_mutex` and invoke `wait_for()` on `cv_consumer`,
    with a timeout and a lambda that tests if the `deque` is empty. We need the timeout
    because it's possible for the `producer` threads to finish while some of the `consumer`
    threads are still running, leaving the `deque` empty.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a non-empty `deque`, we can print (*consume*) an *item* and pop
    it off the `deque`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `main()`, we use `async()` to spawn the `producer` and `consumer` threads.
    `async()` conforms to the RAII pattern, so I''ll usually prefer it over `thread`,
    where possible. `async()` returns a `future` object, so we''ll keep a list of
    `future<void>` objects for process management:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use `for` loops to create `producer` and `consumer` threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we use `list` of `future` objects to determine when our `producer`
    and `consumer` threads are complete:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We loop through our `producers` container, calling `wait()` to allow the `producer`
    threads to complete. Then, we can set the `production_complete` flag. We likewise
    loop through the `consumers` container, calling `wait()` to allow the `consumer`
    threads to complete. We could perform any final analysis or completion processes
    here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is a bit long to show in its entirety:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The heart of this recipe is in the use of two `condition_variable` objects
    to control the `producer` and `consumer` threads asynchronously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `producer()` function, the `cv_producer` object obtains a `unique_lock`,
    waits for the `deque` to be available, and notifies the `cv_consumer` object when
    an item has been produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Conversely, in the `consumer()` function, the `cv_consumer` object obtains
    a `unique_lock`, waits for the `deque` to have items, and notifies the `cv_producer`
    object when an item has been consumed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: These complementary locks, waits, and notifications constitute the balance of
    coordination between multiple producers and consumers.
  prefs: []
  type: TYPE_NORMAL
