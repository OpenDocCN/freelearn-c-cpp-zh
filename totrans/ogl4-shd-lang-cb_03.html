<html><head></head><body>
        

                            
                    <h1 class="header-title">The Basics of GLSL Shaders</h1>
                
            
            
                
<p class="mce-root">In this chapter, we will cover the following recipes:</p>
<ul>
<li>Diffuse and per-vertex shading with a single point light source</li>
<li>Implementing the Phong reflection model</li>
<li>Using functions in shaders</li>
<li>Implementing two-sided shading</li>
<li> Implementing flat shading</li>
<li> Using subroutines to select shader functionality</li>
<li>Discarding fragments to create a perforated look</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Introduction</h1>
                
            
            
                
<p>Shaders were first added into OpenGL in version 2.0, introducing programmability into the formerly fixed-function OpenGL pipeline. Shaders give us the power to implement custom rendering algorithms and provide us with a greater degree of flexibility in the implementation of those techniques. With shaders, we can run custom code directly on the GPU, providing us with the opportunity to leverage the high degree of parallelism available with modern GPUs.</p>
<p>Shaders are implemented using the <strong>OpenGL Shading Language</strong> (<strong>GLSL</strong>). GLSL is syntactically similar to C, which should make it easier for experienced OpenGL programmers to learn. Due to the nature of this text, I won't present a thorough introduction to GLSL here. Instead, if you're new to GLSL, reading through these recipes should help you to learn the language through example. If you are already comfortable with GLSL, but don't have experience with version 4.x, you'll see how to implement these techniques by utilizing the newer API. However, before we jump into GLSL programming, let's take a quick look at how vertex and fragment shaders fit within the OpenGL pipeline.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Vertex and fragment shaders</h1>
                
            
            
                
<p>In OpenGL Version 4.3 and above, there are six shader stages/types: vertex, geometry, tessellation control, tessellation evaluation, fragment, and compute. In this chapter, we'll focus only on the vertex and fragment stages. In <a href="fab663d4-e210-417c-aa3b-2c4c307ec913.xhtml">Chapter 7</a>, <em>Using Geometry and Tessellation Shaders</em>, I'll provide some recipes for working with the geometry and tessellation shaders, and in <a href="d67e01c8-8212-4d49-937f-6b1c62a57744.xhtml">Chapter 11</a>, <em>Using Compute Shaders</em>, I'll focus specifically on compute shaders.</p>
<p>Shaders are fundamental parts of the modern OpenGL pipeline. The following block diagram shows a simplified view of the OpenGL pipeline with only the vertex and fragment shaders installed:</p>
<div><img src="img/9b21bf23-b73d-4288-a641-55bfea8b9842.png" style="width:34.00em;height:8.83em;"/></div>
<p>Vertex data is sent down the pipeline and arrives at the vertex shader via shader input variables. The vertex shader's input variables correspond to the vertex attributes (refer to the <em>Sending data to a shader using vertex attributes and vertex buffer objects</em> recipe in <a href="15752c1f-eee7-4117-9632-f08f84a9405d.xhtml">Chapter 2</a>, <em>Working with GLSL Programs</em>). In general, a shader receives its input via programmer-defined input variables, and the data for those variables comes either from the main OpenGL application or previous pipeline stages (other shaders). For example, a fragment shader's input variables might be fed from the output variables of the vertex shader. Data can also be provided to any shader stage using uniform variables (refer to the <em>Sending data to a shader using uniform variables</em> recipe in <a href="15752c1f-eee7-4117-9632-f08f84a9405d.xhtml">Chapter 2</a>, <em>Working with GLSL Programs</em>). These are used for information that changes less often than vertex attributes (for example, matrices, light position, and other settings). The following diagram shows a simplified view of the relationships between input and output variables when there are two shaders active (vertex and fragment):</p>
<div><div><img src="img/6ae9550c-eb4e-4582-877f-c31256676587.png" style="width:37.75em;height:10.33em;"/></div>
</div>
<p>The vertex shader is executed once for each vertex, in parallel. The data corresponding to the position of the vertex must be transformed into clip space coordinates and assigned to the output variable <kbd>gl_Position</kbd> before the vertex shader finishes execution. The vertex shader can send other information down the pipeline using shader output variables. For example, the vertex shader might also compute the color associated with the vertex. That color would be passed to later stages via an appropriate output variable.</p>
<p>Between the vertex and fragment shader, vertices are assembled into primitives, clipping takes place, and the viewport transformation is applied (among other operations). The rasterization process then takes place and the polygon is filled (if necessary). The fragment shader is executed once for each fragment of the polygon being rendered (typically in parallel). Data provided from the vertex shader is (by default) interpolated in a perspective correct manner, and provided to the fragment shader via shader input variables. The fragment shader determines the appropriate color for the pixel and sends it to the frame buffer using output variables. The depth information is handled automatically, but can be modified by the fragment shader if desired.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Learning the basics first</h1>
                
            
            
                
<p>Programmable shaders give us tremendous power and flexibility. A good place to start is to learn how to implement a simple, common reflection model known as the <strong>Phong reflection model</strong>. It is a good basis for building upon.</p>
<p>In this chapter, we'll look at the basic techniques for implementing the Phong model. We'll modify it in a few simple ways, including two-sided rendering and flat shading. Along the way, we'll also see some examples of other GLSL features such as functions, subroutines, and the <kbd>discard</kbd> keyword.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Diffuse and per-vertex shading with a single point light source</h1>
                
            
            
                
<p>Before learning the full Phong reflection model, we'll start with just one part: diffuse reflection. It is a simple reflection model that makes the assumption that the surface exhibits purely diffuse reflection. That is to say that the surface appears to scatter light in all directions equally, regardless of direction.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Incoming light strikes the surface and penetrates slightly before being reradiated in all directions. Of course, the incoming light interacts with the surface before it is scattered, causing some wavelengths to be fully or partially absorbed and others to be scattered. A typical example of a diffuse surface is a surface that has been painted with a matte paint. The surface has a dull look with no shine at all.</p>
<p>The following image shows a torus rendered with diffuse shading:</p>
<div><img src="img/477790cd-0263-4bb5-9123-a9770f48abb9.png" style="width:9.92em;height:10.25em;"/></div>
<p>The mathematical model for diffuse reflection involves two vectors: the direction from the surface point to the light source (<strong>s</strong>), and the normal vector at the surface point (<strong>n</strong>). The vectors are represented in the following diagram:</p>
<div><img src="img/7b05ab82-3db9-46b0-a1a9-f0aacd3c81ea.png" style="width:14.83em;height:11.42em;"/></div>
<p>The amount of incoming light (or radiance) per unit area that strikes a surface is dependent on the orientation of the surface with respect to the light source. The physics of the situation tells us that the amount of radiation per unit area is maximal when the light arrives along the direction of the normal vector, and zero when the light is perpendicular to the normal vector. In between, it is proportional to the cosine of the angle between the direction towards the light source and the normal vector. So, since the dot product is proportional to the cosine of the angle between two vectors, we can express the amount of radiation striking the surface as the product of the light intensity and the dot product of <em>s</em> and <em>n</em>, as follows:</p>
<div><img class="fm-editor-equation" src="img/b6b510f6-32fc-4f53-a48e-2fb4c995e299.png" style="width:5.00em;height:1.58em;"/></div>
<p><em>L<sub>d</sub></em> is the intensity of the light source, and the vectors <strong>s</strong> and <strong>n</strong> are assumed to be normalized.</p>
<p>The dot product of two unit vectors is equal to the cosine of the angle between them.</p>
<p>As stated previously, some of the incoming light is absorbed before it is reemitted. We can model this interaction by using a reflection coefficient (<em>K<sub>d</sub></em>), which represents the fraction of the incoming light that is scattered. This is sometimes called the <strong>diffuse reflectivity</strong>, or the diffuse reflection coefficient. The diffuse reflectivity becomes a scaling factor, so the intensity of the outgoing light can be expressed as follows:</p>
<div><img class="fm-editor-equation" src="img/42dcb457-527b-4407-b8ec-d1f7538bd691.png" style="width:9.00em;height:1.50em;"/></div>
<p>Because this model depends only on the direction towards the light source and the normal to the surface, not on the direction towards the viewer, we have a model that represents uniform (omnidirectional) scattering.</p>
<p class="mce-root">In this recipe, we'll evaluate this equation at each vertex in the vertex shader and interpolate the resulting color across the face. We'll use uniform variables for the <em>K<sub>d</sub></em> and <em>L<sub>d</sub></em> terms as well as the light position.</p>
<p>In this and the following recipes, light intensities and material reflectivity coefficients are represented by three-component (RGB) vectors. Therefore, the equations should be treated as component-wise operations, applied to each of the three components separately. Luckily, the GLSL will make this nearly transparent because the necessary operators operate component-wise on vector variables.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>Start with an OpenGL application that provides the vertex position in attribute location 0, and the vertex normal in attribute location 1 (refer to the <em>Sending data to a shader using vertex attributes and vertex buffer objects</em> recipe in <a href="15752c1f-eee7-4117-9632-f08f84a9405d.xhtml">Chapter 2</a>, <em>Working with GLSL Programs</em>). The OpenGL application should also provide the standard transformation matrices (projection, modelview, and normal) via uniform variables.</p>
<p>The light position (in camera coordinates), <kbd>Kd</kbd>, and <kbd>Ld</kbd> should also be provided by the OpenGL application via uniform variables. Note that <kbd>Kd</kbd> and <kbd>Ld</kbd> are of type <kbd>vec3</kbd>. We can use <kbd>vec3</kbd> to store an RGB color as well as a vector or point.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>To create a shader pair that implements diffuse shading, take the following steps:</p>
<ol>
<li>The vertex shader computes the diffuse reflection equation and sends the result to the fragment shader via the output variable <kbd>LightIntensity</kbd>:</li>
</ol>
<pre style="padding-left: 60px">layout (location = 0) in vec3 VertexPosition; 
layout (location = 1) in vec3 VertexNormal; 
 
out vec3 LightIntensity; 
 
uniform vec4 LightPosition;// Light position in camera coords. 
uniform vec3 Kd;           // Diffuse reflectivity 
uniform vec3 Ld;           // Light source intensity 
 
uniform mat4 ModelViewMatrix; 
uniform mat3 NormalMatrix; 
uniform mat4 ProjectionMatrix; 
uniform mat4 MVP;             // Projection * ModelView 
 
void main() 
{ 
    // Convert normal and position to eye coords 
    vec3 tnorm = normalize( NormalMatrix * VertexNormal); 
    vec4 camCoords = ModelViewMatrix * 
                     vec4(VertexPosition,1.0)); 
    vec3 s = normalize(vec3(LightPosition - camCoords)); 
 
    // The diffuse shading equation 
    LightIntensity = Ld * Kd * max( dot( s, tnorm ), 0.0 ); 
 
    // Convert position to clip coordinates and pass along 
    gl_Position = MVP * vec4(VertexPosition,1.0); 
} </pre>
<ol start="2">
<li>The fragment shader simply applies the color to the fragment:</li>
</ol>
<pre style="padding-left: 60px">in vec3 LightIntensity; 
layout( location = 0 ) out vec4 FragColor; 
 
void main() { 
    FragColor = vec4(LightIntensity, 1.0); 
}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<ol start="3">
<li>Compile and link both shaders within the OpenGL application and install the shader program prior to rendering. See <a href="3b817a9a-28a1-4be7-936c-b982b4dfacdf.xhtml">Chapter 1</a>, <em>Getting Started with GLSL</em>, for details about compiling, linking, and installing shaders.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>The vertex shader does all of the work in this example. The diffuse reflection is computed in camera coordinates by first transforming the normal vector using the normal matrix, normalizing, and storing the result in <kbd>tnorm</kbd>. Note that the normalization here may not be necessary if your normal vectors are already normalized and the normal matrix does not do any scaling.</p>
<p>The normal matrix is the inverse transpose of the upper-left 3x3 portion of the model-view matrix. We use the inverse transpose because normal vectors transform differently than the vertex position. For a more thorough discussion of the normal matrix, and the reasons why, see any introductory computer graphics textbook (a good choice would be <em>Computer Graphics with OpenGL</em> by Hearn and Baker). If your model-view matrix does not include any nonuniform scaling, then one can use the upper-left 3 x 3 of the model-view matrix in place of the normal matrix to transform your normal vectors. However, if your model-view matrix does include (uniform) scaling, you'll still need to (re)normalize your normal vectors after transforming them.</p>
<p>The next step converts the vertex position to camera coordinates by transforming it with the model-view matrix. Then, we compute the direction toward the light source by subtracting the vertex position from the light position and storing the result in <kbd>s</kbd>.</p>
<p>Next, we compute the scattered light intensity using the equation described previously and store the result in the output variable <kbd>LightIntensity</kbd>. Note the use of the <kbd>max</kbd> function here. If the dot product is less than zero, then the angle between the normal vector and the light direction is greater than 90 degrees. This means that the incoming light is coming from inside the surface. Since such a situation would mean that no radiation reaches the surface, and the dot product would produce negative values, we use a value of <kbd>0.0</kbd>. However, you may decide that you want to properly light both sides of your surface, in which case the normal vector needs to be reversed for those situations where the light is striking the back side of the surface (refer to the <em>Implementing two-sided shading</em> recipe in this chapter).</p>
<p>Finally, we convert the vertex position to clip space coordinates by multiplying it with the model-view projection matrix, (which is <em>projection * view * model</em>) and store the result in the built-in output variable <kbd>gl_Position</kbd>.</p>
<p class="mce-root"/>
<p>The subsequent stage of the OpenGL pipeline expects that the vertex position will be provided in clip space coordinates in the output variable <kbd>gl_Position</kbd>. This variable does not directly correspond to any input variable in the fragment shader, but is used by the OpenGL pipeline in the primitive assembly, clipping, and rasterization stages that follow the vertex shader. It is important that we always provide a valid value for this variable.</p>
<p>Since <kbd>LightIntensity</kbd> is an output variable from the vertex shader, its value is interpolated across the face and passed into the fragment shader. The fragment shader then simply assigns the value to the output fragment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">There's more...</h1>
                
            
            
                
<p>Diffuse shading is a technique that models only a very limited range of surfaces. It is best used for surfaces that have a <em>matte</em> appearance. Additionally, with the technique used previously, the dark areas may look a bit too dark. In fact, those areas that are not directly illuminated are completely black. In real scenes, there is typically some light that has been reflected about the room that brightens these surfaces. In the following recipes, we'll look at ways to model more surface types, as well as providing some light for those dark parts of the surface.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">See also</h1>
                
            
            
                
<ul>
<li>The <kbd>chapter03/scenediffuse.cpp</kbd> file in the example code</li>
<li>The <em>Sending data to a shader using uniform variables</em> recipe in <a href="15752c1f-eee7-4117-9632-f08f84a9405d.xhtml">Chapter 2</a>, <em>Working with GLSL Programs</em></li>
<li>The <em>Compiling a shader</em> recipe in <a href="3b817a9a-28a1-4be7-936c-b982b4dfacdf.xhtml">Chapter 1</a>, <em>Getting Started with GLSL</em></li>
<li>The <em>Linking a shader program</em> recipe in <a href="3b817a9a-28a1-4be7-936c-b982b4dfacdf.xhtml">Chapter 1</a>, <em>Getting Started with GLSL</em></li>
<li>The <em>Sending data to a shader using vertex attributes and vertex buffer objects</em> recipe in <a href="15752c1f-eee7-4117-9632-f08f84a9405d.xhtml">Chapter 2</a>, <em>Working with GLSL Programs</em></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing the Phong reflection model</h1>
                
            
            
                
<p>In this recipe, we'll implement the well-known Phong reflection model. The OpenGL fixed-function pipeline's default shading technique was very similar to the one presented here.  It models the light-surface interaction as a combination of three components: ambient, diffuse, and specular. The <strong>ambient</strong> component is intended to model light that has been reflected so many times that it appears to be emanating uniformly from all directions. The <strong>diffuse</strong> component was discussed in the previous recipe, and represents omnidirectional reflection. The <strong>specular</strong> component models the shininess of the surface and represents glossy reflection around a preferred direction. Combining these three components together can model a nice (but limited) variety of surface types. This shading model is called the <strong>Phong reflection model</strong> (or <strong>Phong shading model</strong>), after graphics researcher Bui Tuong Phong.</p>
<p>An example of a torus rendered with the Phong shading model is shown in the following image:</p>
<div><img src="img/3dd29349-b614-4337-845e-045460c4dbad.png" style="width:13.75em;height:12.75em;"/></div>
<p>The Phong model is implemented as the sum of three components: ambient, diffuse, and specular. The ambient component represents light that illuminates all surfaces equally and reflects equally in all directions. It is used to help brighten some of the darker areas within a scene. Since it does not depend on the incoming or outgoing directions of the light, it can be modeled simply by multiplying the light source intensity (<em>L<sub>a</sub></em>) by the surface reflectivity (<em>K<sub>a</sub></em>):</p>
<div><img class="fm-editor-equation" src="img/1a820503-2e84-49ca-a4cc-4808a8eaa91a.png" style="width:7.42em;height:1.50em;"/></div>
<p>The diffuse component models a rough surface that scatters light in all directions (see<em> Diffuse and per-vertex shading with a single point light source</em> recipe in this chapter). The diffuse contribution is given by the following equation:</p>
<div><img class="fm-editor-equation" src="img/c9d6c8dd-395b-403e-8d6f-f9cf54ab9556.png" style="width:10.33em;height:1.67em;"/></div>
<p>The specular component is used for modeling the shininess of a surface. When a surface has a glossy shine to it, the light is reflected off of the surface, scattered around some preferred direction. We model this so that the reflected light is strongest in the direction of perfect (mirror-like) reflection. The physics of the situation tells us that for perfect reflection, the angle of incidence is the same as the angle of reflection and that the vectors are coplanar with the surface normal, as shown in the following diagram:</p>
<div><img src="img/7a401c95-2047-452b-828a-d3693931d9ec.png" style="width:12.42em;height:9.58em;"/></div>
<p>In the preceding diagram, <strong>r</strong> represents the direction of pure reflection corresponding to the incoming light vector (<strong>-s</strong>), and <strong>n</strong> is the surface normal. We can compute <strong>r</strong> by using the following equation:</p>
<div><img class="fm-editor-equation" src="img/034eb641-c42d-438b-beb0-8b4d18f2a215.png" style="width:11.25em;height:1.67em;"/></div>
<p>To model specular reflection, we need to compute the following (normalized) vectors: the direction toward the light source (<strong>s</strong>), the vector of perfect reflection (<strong>r</strong>), the vector toward the viewer (<strong>v</strong>), and the surface normal (<strong>n</strong>). These vectors are represented in the following diagram:</p>
<div><img src="img/81a67689-22a4-41fc-9ecd-5451330197cd.png" style="width:18.83em;height:10.58em;"/></div>
<p>We would like the reflection to be maximal when the viewer is aligned with the vector <strong>r</strong>, and to fall off quickly as the viewer moves farther away from alignment with <strong>r</strong>. This can be modeled using the cosine of the angle between <strong>v</strong> and <strong>r</strong> raised to some power (<strong>f</strong>):</p>
<div><img class="fm-editor-equation" src="img/4cb2ff7c-8609-4f5c-87c8-2f9c94d4d970.png" style="width:10.08em;height:1.75em;"/></div>
<p>(Recall that the dot product is proportional to the cosine of the angle between the vectors involved.) The larger the power, the faster the value drops toward zero as the angle between <strong>v</strong> and <strong>r</strong> increases. Again, similar to the other components, we also introduce a specular light intensity term (<em>L<sub>s</sub></em>) and reflectivity term (<em>K<sub>s</sub></em>). It is common to set the <em>K<sub>s</sub></em> term to some grayscale value (for example, (0.8, 0.8, 0.8)), since glossy reflection is not (generally) wavelength dependent. </p>
<p>The specular component creates <strong>specular highlights</strong> (bright spots) that are typical of glossy surfaces. The larger the power of <em>f</em> in the equation, the smaller the specular highlight and the shinier the surface. The value for <em>f</em> is typically chosen to be somewhere between 1 and 200.</p>
<p>Putting all of this together by simply summing the three terms, we have the following shading equation:</p>
<div><img class="fm-editor-equation" src="img/fb91f8d1-8b54-4c2c-93fe-df8b866a44d1.png" style="width:28.00em;height:1.58em;"/></div>
<p>In the following code, we'll evaluate this equation in the vertex shader, and interpolate the color across the polygon.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>In the OpenGL application, provide the vertex position in location 0 and the vertex normal in location 1. The light position and the other configurable terms for our lighting equation are uniform variables in the vertex shader and their values must be set from the OpenGL application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>To create a shader pair that implements the Phong reflection model, take the following steps:</p>
<ol>
<li>The vertex shader computes the Phong reflection model at the vertex position and sends the result to the fragment shader:</li>
</ol>
<pre style="padding-left: 60px">layout (location = 0) in vec3 VertexPosition; 
layout (location = 1) in vec3 VertexNormal; 
 
out vec3 LightIntensity; 
 
uniform struct LightInfo {<br/>  vec4 Position; // Light position in eye coords.<br/>  vec3 La;       // Ambient light intensity<br/>  vec3 Ld;       // Diffuse light intensity<br/>  vec3 Ls;       // Specular light intensity<br/>} Light;<br/><br/>uniform struct MaterialInfo {<br/>  vec3 Ka;      // Ambient reflectivity<br/>  vec3 Kd;      // Diffuse reflectivity<br/>  vec3 Ks;      // Specular reflectivity<br/>  float Shininess; // Specular shininess factor<br/>} Material;
 
uniform mat4 ModelViewMatrix; 
uniform mat3 NormalMatrix; 
uniform mat4 ProjectionMatrix; 
uniform mat4 MVP; 
 
void main() { <br/>  vec3 n = normalize( NormalMatrix * VertexNormal);<br/>  vec4 camCoords = ModelViewMatrix * vec4(VertexPosition,1.0);<br/>    <br/>  vec3 ambient = Light.La * Material.Ka;<br/>  vec3 s = normalize(vec3(Light.Position - camCoords));<br/>  float sDotN = max( dot(s,n), 0.0 );<br/>  vec3 diffuse = Light.Ld * Material.Kd * sDotN;<br/>  vec3 spec = vec3(0.0);<br/>  if( sDotN &gt; 0.0 ) {<br/>    vec3 v = normalize(-camCoords.xyz);<br/>    vec3 r = reflect( -s, n );<br/>    spec = Light.Ls * Material.Ks *<br/>            pow( max( dot(r,v), 0.0 ), Material.Shininess );<br/>  }<br/><br/>  LightIntensity = ambient + diffuse + spec;
  gl_Position = MVP * vec4(VertexPosition,1.0); 
} </pre>
<ol start="2">
<li>The fragment shader simply applies the color to the fragment:</li>
</ol>
<pre style="padding-left: 60px">in vec3 LightIntensity; 
layout( location = 0 ) out vec4 FragColor; 
 
void main() { 
    FragColor = vec4(LightIntensity, 1.0); 
} </pre>
<ol start="3">
<li>Compile and link both shaders within the OpenGL application, and install the shader program prior to rendering.</li>
</ol>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>The vertex shader computes the shading equation in eye coordinates. It begins by transforming the vertex normal into camera coordinates and normalizing, then storing the result in <kbd>n</kbd>. The vertex position is also transformed into camera coordinates and stored in <kbd>camCoords</kbd>.</p>
<p>The ambient component is computed and stored in the variable <kbd>ambient</kbd>. </p>
<p>Next, we compute the normalized direction towards the light source (<kbd>s</kbd>). This is done by subtracting the vertex position in camera coordinates from the light position and normalizing the result.</p>
<p>The dot product of <kbd>s</kbd> and <kbd>n</kbd> is computed next. As in the preceding recipe, we use the built-in function <kbd>max</kbd> to limit the range of values to between zero and one. The result is stored in the variable named <kbd>sDotN</kbd>, and is used to compute the diffuse component. The resulting value for the diffuse component is stored in the variable <kbd>diffuse</kbd>. </p>
<p>Before computing the specular component, we check the value of <kbd>sDotN</kbd>. If <kbd>sDotN</kbd> is zero, then there is no light reaching the surface, so there is no point in computing the specular component, as its value must be zero. Otherwise, if <kbd>sDotN</kbd> is greater than zero, we compute the specular component using the equation presented earlier.  </p>
<p>If we did not check <kbd>sDotN</kbd> before computing the specular component, it is possible that some specular highlights could appear on faces that are facing away from the light source. This is clearly an unrealistic and undesirable result. Another way to solve this problem is to multiply both the specular and diffuse components by <kbd>sDotN</kbd> (instead of only the diffuse component as we are doing now). This is actually somewhat more physically accurate, but is not part of the traditional Phong model.</p>
<p>The direction toward the viewer (<kbd>v</kbd>) is the negation of the position (normalized) because in camera coordinates the viewer is at the origin.</p>
<p>We compute the direction of pure reflection by calling the GLSL built-in function <kbd>reflect</kbd>, which reflects the first argument about the second. We don't need to normalize the result because the two vectors involved are already normalized.</p>
<p>When computing the specular component, we use the built-in function <kbd>max</kbd> to limit the range of values of the dot product to between zero and one, and the function <kbd>pow</kbd> raises the dot product to the power of the <kbd>Shininess</kbd> exponent (corresponding to <em>f</em> in our lighting equation).</p>
<p class="mce-root"/>
<p>The sum of the three components is then stored in the output variable <kbd>LightIntensity</kbd>. This value will be associated with the vertex and passed down the pipeline. Before reaching the fragment shader, its value will be interpolated in a perspective correct manner across the face of the polygon.</p>
<p>Finally, the vertex shader transforms the position into clip coordinates, and assigns the result to the built-in output variable <kbd>gl_Position</kbd> (refer to the D<em>iffuse and per-vertex shading with a single point light source</em> recipe in this chapter).</p>
<p>The fragment shader simply applies the interpolated value of <kbd>LightIntensity</kbd> to the output fragment by storing it in the shader output variable <kbd>FragColor</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">There's more...</h1>
                
            
            
                
<p>The Phong reflection model works quite well, but has some drawbacks. A slight change to the model, introduced by James Blinn, is more commonly used in practice. The Blinn-Phong model replaces the vector of pure reflection with the so-called <em>halfway vector</em>, and produces specular highlights that have been shown to be more realistic. This model is discussed in <em>The Blinn-Phong reflection model</em> recipe in <a href="343fbd70-0012-4449-afe6-a724b330b441.xhtml">Chapter 4</a>, <em>Lighting and Shading</em>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using a nonlocal viewer</h1>
                
            
            
                
<p>We can avoid the extra normalization needed to compute the vector towards the viewer (<kbd>v</kbd>) by using a so-called <strong>nonlocal viewer</strong>. Instead of computing the direction toward the origin, we simply use the constant vector (0, 0, 1) for all vertices. Of course, it is not accurate, but in practice the visual results are very similar, often visually indistinguishable, saving us one normalization.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Per-vertex versus per-fragment</h1>
                
            
            
                
<p>Since the shading equation is computed within the vertex shader, we refer to this as <strong>per-vertex shading</strong>. Per-vertex shading is also called <strong>Gouraud shading</strong>. One of the disadvantages of this is that specular highlights can be warped or lost, due to the fact that the shading equation is not evaluated at each point across the face.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For example, a specular highlight that should appear in the middle of a polygon might not appear at all when per-vertex shading is used, because of the fact that the shading equation is only computed at the vertices where the specular component is near zero. In the <em>Using per-fragment shading for improved realism</em> recipe of <a href="343fbd70-0012-4449-afe6-a724b330b441.xhtml">Chapter 4</a>, <em>Lighting and Shading</em>, we'll look at the changes needed to move the shading computation into the fragment shader, producing more realistic results.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Directional lights</h1>
                
            
            
                
<p>We can also avoid the need to compute a light direction (<kbd>s</kbd>) for each vertex if we assume a directional light. A <strong>directional light source</strong> is one that has no position, only a direction. Instead of computing the direction towards the source for each vertex, a constant vector is used, which represents the direction towards the remote light source. This is a good way to model lighting from distant sources such as sunlight. We'll look at an example of this in the <em>Shading with a directional light source</em> recipe of <a href="343fbd70-0012-4449-afe6-a724b330b441.xhtml">Chapter 4</a>, <em>Lighting and Shading</em>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Light attenuation with distance</h1>
                
            
            
                
<p>You might think that this shading model is missing one important component. It doesn't take into account the effect of the distance to the light source. In fact, it is known that the intensity of radiation from a source falls off in proportion to the inverse square of the distance from the source. So why not include this in our model?</p>
<p>It would be fairly simple to do so, however, the visual results are often less than appealing. It tends to exaggerate the distance effects and create unrealistic-looking images. Remember, our equation is just an approximation of the physics involved and is not a truly realistic model, so it is not surprising that adding a term based on a strict physical law produces unrealistic results.</p>
<p>In the OpenGL fixed-function pipeline, it was possible to turn on distance attenuation using the <kbd>glLight</kbd> function. If desired, it would be straightforward to add a few uniform variables to our shader to produce the same effect.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">See also</h1>
                
            
            
                
<ul>
<li>The <kbd>chapter03/scenephong.cpp</kbd> file in the example code</li>
<li>The <em>Shading with a directional light source</em> recipe in <a href="343fbd70-0012-4449-afe6-a724b330b441.xhtml">Chapter 4</a>, <em><em>Lighting and Shading</em></em></li>
<li>The <em>Using per-fragment shading for improved realism</em> recipe in <a href="343fbd70-0012-4449-afe6-a724b330b441.xhtml">Chapter 4</a>, <em>Lighting and Shading</em></li>
<li>The <em>Using the Blinn-Phong model</em> recipe in <a href="343fbd70-0012-4449-afe6-a724b330b441.xhtml">Chapter 4</a>, <em>Lighting and Shading</em></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Using functions in shaders</h1>
                
            
            
                
<p>The GLSL supports functions that are syntactically similar to C functions. However, the calling conventions are somewhat different. In the following example, we'll revisit the Phong shader using functions to help provide abstractions for the major steps.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>As with previous recipes, provide the vertex position at attribute location 0 and the vertex normal at attribute location 1. Uniform variables for all of the Phong coefficients should be set from the OpenGL side, as well as the light position and the standard matrices.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>The vertex shader is nearly identical to the one from the previous recipe, except that the Phong model is evaluated within a function, and we add another function to convert the position and the normal to camera coordinates:</p>
<pre>// Uniform variables and attributes omitted...<br/> void getCamSpace( out vec3 norm, out vec3 position ) {<br/>    norm = normalize( NormalMatrix * VertexNormal);<br/>    position = (ModelViewMatrix * vec4(VertexPosition,1.0)).xyz;<br/>}<br/><br/>vec3 phongModel( vec3 position, vec3 n ) { <br/>  vec3 ambient = Light.La * Material.Ka;<br/>  vec3 s = normalize( Light.Position.xyz - position );<br/>  float sDotN = max( dot(s,n), 0.0 );<br/>  vec3 diffuse = Light.Ld * Material.Kd * sDotN;<br/>  vec3 spec = vec3(0.0);<br/>  if( sDotN &gt; 0.0 ) {<br/>    vec3 v = normalize(-position.xyz);<br/>    vec3 r = reflect( -s, n );<br/>    spec = Light.Ls * Material.Ks *<br/>            pow( max( dot(r,v), 0.0 ), Material.Shininess );<br/>  }<br/><br/>  return ambient + diffuse + spec;<br/>}<br/><br/>void main() {<br/>    // Get the position and normal in camera space<br/>    vec3 camNorm, camPosition;<br/>    getCamSpace(camNorm, camPosition);<br/><br/>    // Evaluate the reflection model<br/>    LightIntensity = phongModel( camPosition, camNorm );<br/><br/>    gl_Position = MVP * vec4(VertexPosition,1.0);<br/>} </pre>
<p>The fragment shader has no changes from the previous recipe.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In GLSL functions, the parameter evaluation strategy is <strong>call by value-return</strong> (also called <strong>call by copy-restore</strong> or <strong>call by value-result</strong>). Parameter variables can be qualified with <kbd>in</kbd>, <kbd>out</kbd>, or <kbd>inout</kbd>. Arguments corresponding to input parameters (those qualified with <kbd>in</kbd> or <kbd>inout</kbd>) are copied into the parameter variable at call time, and output parameters (those qualified with <kbd>out</kbd> or <kbd>inout</kbd>) are copied back to the corresponding argument before the function returns. If a parameter variable does not have any of the three qualifiers, the default qualifier is <kbd>in</kbd>.</p>
<p>We've created two functions in the vertex shader. The first, named <kbd>getCamSpace</kbd>, transforms the vertex position and vertex normal into camera coordinates, and returns them via output parameters. In the <kbd>main</kbd> function, we create two uninitialized variables (<kbd>camNorm</kbd> and <kbd>camPosition</kbd>) to store the results, and then call the function with the variables as the function's arguments. The function stores the results into the parameter variables (<kbd>n</kbd> and <kbd>position</kbd>) which are copied into the arguments before the function returns.</p>
<p>The second function, <kbd>phongModel</kbd>, uses only input parameters. The function receives the eye-space position and normal, and computes the result of the Phong reflection model. The result is returned by the function and stored in the shader output variable <kbd>LightIntensity</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">There's more...</h1>
                
            
            
                
<p>Since it makes no sense to read from an output parameter variable, output parameters should only be written to within the function. Their value is undefined.</p>
<p>Within a function, writing to an input-only parameter (qualified with <kbd>in</kbd>) is allowed. The function's copy of the argument is modified, and changes are not reflected in the argument.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The const qualifier</h1>
                
            
            
                
<p>The additional qualifier <kbd>const</kbd> can be used with input-only parameters (not with <kbd>out</kbd> or <kbd>inout</kbd>). This qualifier makes the input parameter read-only, so it cannot be written to within the function.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Function overloading</h1>
                
            
            
                
<p>Functions can be overloaded by creating multiple functions with the same name, but with a different number and/or type of parameters. As with many languages, two overloaded functions may not differ in return type only.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Passing arrays or structures to a function</h1>
                
            
            
                
<p>It should be noted that when passing arrays or structures to functions, they are passed by value. If a large array or structure is passed, it can incur a large copy operation, which may not be desired. It would be a better choice to declare these variables in the global scope.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">See also</h1>
                
            
            
                
<ul>
<li>The <kbd>chapter03/shader/function.vert.glsl</kbd> and <kbd>chapter03/shader/function.frag.glsl</kbd> files in the example code</li>
<li>The <em>Implementing the Phong reflection model</em> recipe</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing two-sided shading</h1>
                
            
            
                
<p>When rendering a mesh that is completely closed, the back faces of polygons are hidden. However, if a mesh contains holes, it might be the case that the back faces would become visible. In this case, the polygons may be shaded incorrectly due to the fact that the normal vector is pointing in the wrong direction. To properly shade those back faces, one needs to invert the normal vector and compute the lighting equations based on the inverted normal.</p>
<p>The following image shows a teapot with the lid removed. On the left, the Phong model is used. On the right, the Phong model is augmented with the two-sided rendering technique discussed in this recipe:</p>
<div><img src="img/f09a8b43-dc22-49e7-8435-999064553601.png" style="width:33.17em;height:16.33em;"/></div>
<p>In this recipe, we'll look at an example that uses the Phong model discussed in the previous recipes, augmented with the ability to correctly shade back faces.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>The vertex position should be provided in attribute location 0 and the vertex normal in attribute location 1. As in the previous examples, the lighting parameters must be provided to the shader via uniform variables.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>To implement a shader pair that uses the Phong reflection model with two-sided lighting, take the following steps:</p>
<ol>
<li>The vertex shader is similar to the one in the previous recipe, except that it computes the Phong equation twice. First, without any change to the normal vector, and again with the normal inverted. The results are stored in output variables <kbd>FrontColor</kbd> and <kbd>BackColor</kbd>, respectively:</li>
</ol>
<pre style="padding-left: 60px">// Uniforms and attributes...<br/>out vec3 FrontColor;<br/>out vec3 BackColor;<br/><br/>vec3 phongModel( vec3 position, vec3 n ) { 
    // The Phong model calculations go here...
} 
 
void main() {<br/>    vec3 tnorm = normalize( NormalMatrix * VertexNormal);<br/>    vec3 camCoords = (ModelViewMatrix * <br/>    vec4(VertexPosition,1.0)).xyz;<br/><br/>    FrontColor = phongModel( camCoords, tnorm );<br/>    BackColor = phongModel( camCoords, -tnorm );<br/><br/>    gl_Position = MVP * vec4(VertexPosition,1.0);<br/>}</pre>
<ol start="2">
<li>The fragment shader chooses which color to use based on the value of the built-in <kbd>gl_FrontFacing</kbd> variable:</li>
</ol>
<pre style="padding-left: 60px">in vec3 FrontColor; 
in vec3 BackColor; 
 
layout( location = 0 ) out vec4 FragColor; 
 
void main() { 
    if( gl_FrontFacing ) { 
        FragColor = vec4(FrontColor, 1.0); 
    } else { 
        FragColor = vec4(BackColor, 1.0); 
    } 
} </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In the vertex shader, we compute the lighting equation using both the vertex normal and the inverted version, and pass each color to the fragment shader. The fragment shader chooses and applies the appropriate color depending on the orientation of the face.</p>
<p>The evaluation of the reflection model is placed within a function named <kbd>phongModel</kbd>. The function is called twice, first using the normal vector (transformed into camera coordinates), and second using the inverted normal vector. The combined results are stored in <kbd>FrontColor</kbd> and <kbd>BackColor</kbd>, respectively.</p>
<p>There are a few aspects of the shading model that are independent of the orientation of the normal vector (such as the ambient component). One could optimize this code by rewriting it so that the redundant calculations are only done once. However, in this recipe, we compute the entire shading model twice in the interest of making things clear and readable.</p>
<p>In the fragment shader, we determine which color to apply based on the value of the built-in variable <kbd>gl_FrontFacing</kbd>.  This is a Boolean value that indicates whether the fragment is part of a front- or back-facing polygon. Note that this determination is based on the <strong>winding</strong> of the polygon, and not the normal vector. (A polygon is said to have counterclockwise winding if the vertices are specified in counterclockwise order, as viewed from the front side of the polygon.) By default, when rendering, if the order of the vertices appear on the screen in a counterclockwise order, it indicates a front-facing polygon, however, we can change this by calling <kbd>glFrontFace</kbd> from the OpenGL program.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">There's more...</h1>
                
            
            
                
<p>In the vertex shader, we determine the front side of the polygon by the direction of the normal vector, and in the fragment shader, the determination is based on the polygon's winding. For this to work properly, the normal vector must be defined appropriately for the face determined by the setting of <kbd>glFrontFace</kbd>.</p>
<p>An alternative choice for this recipe would be to determine whether the face being shaded is a front or back face first in the vertex shader, and send only a single result to the fragment shader. One way to do this would be to compute the dot product between a vector pointing towards the camera (the origin in camera coordinates), and the normal. If the dot product is negative, then the normal must be pointing away from the viewer, meaning that the viewer is seeing the back side of the face.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In which case, we invert the normal. Specifically, we could change the main function in the vertex shader as follows:</p>
<pre>void main() {<br/>  vec3 tnorm = normalize( NormalMatrix * VertexNormal);<br/>  vec3 camCoords = (ModelViewMatrix * vec4(VertexPosition,1.0)).xyz;<br/>  vec3 v = normalize(-camCoords.xyz);<br/><br/>  float vDotN = dot(v, tnorm);<br/><br/>  if( vDotN &gt;= 0 ) {<br/>    Color = phongModel(camCoords, tnorm);<br/>  } else {<br/>    Color = phongModel(camCoords, -tnorm);<br/>  }<br/>  gl_Position = MVP * vec4(VertexPosition,1.0);<br/>}</pre>
<p>In this case, we only need a single output variable to send to the fragment shader (<kbd>Color</kbd>, in the preceding code), and the fragment shader simply applies the color to the fragment. In this version, there's no need to check the value of <kbd>gl_FrontFacing</kbd> in the fragment shader.</p>
<p>In this version, the only thing that is used to determine whether or not it is a front face is the normal vector. The polygon winding is not used. If the normals at the vertices of a polygon are not parallel (which is often the case for curved shapes), then it may be the case that some vertices are treated as <em>front</em> and others are treated as <em>back</em>. This has the potential of producing unwanted artifacts as the color is blended across the face. It would be better to compute all of the reflection model in the fragment shader, as is common practice these days. See the <em>Using per-fragment shading for improved realism</em> recipe in <a href="343fbd70-0012-4449-afe6-a724b330b441.xhtml">Chapter 4</a>, <em>Lighting and Shading</em>, for details about per-fragment shading.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using two-sided rendering for debugging</h1>
                
            
            
                
<p>It can sometimes be useful to visually determine which faces are front-facing and which are back-facing (based on winding). For example, when working with arbitrary meshes, polygons may not be specified using the appropriate winding. As another example, when developing a mesh procedurally, it can sometimes be helpful to determine which faces have proper winding in order to help with debugging. We can easily tweak our fragment shader to help us solve these kinds of problems by mixing a solid color with all back (or front) faces. For example, we could change the <kbd>else</kbd> clause within our fragment shader to the following:</p>
<pre>FragColor = mix( vec4(BackColor,1.0), 
                vec4(1.0,0.0,0.0,1.0), 0.7 );</pre>
<p class="mce-root"/>
<p>This would mix a solid red color with all back faces, helping them stand out, as shown in the following image. In the image, back faces are mixed with 70 percent red, as shown in the preceding code:</p>
<div><img src="img/b57cf931-dad2-40a2-97fb-df90eeb9a5e6.png" style="width:16.92em;height:14.42em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">See also</h1>
                
            
            
                
<ul>
<li>The <kbd>chapter03/scenetwoside.cpp</kbd> file in the example code</li>
<li>The <em>Implementing the Phong reflection model</em> recipe</li>
<li><em>Using per-fragment shading for improved realism</em> in <a href="343fbd70-0012-4449-afe6-a724b330b441.xhtml">Chapter 4</a>, <em>Lighting and Shading</em></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing flat shading</h1>
                
            
            
                
<p>Per-vertex shading involves computation of the shading model at each vertex and associating the result (a color) with that vertex. The colors are then interpolated across the face of the polygon to produce a smooth shading effect. This is also referred to as <strong>Gouraud shading</strong>. In earlier versions of OpenGL, this per-vertex shading with color interpolation was the default shading technique.</p>
<p>It is sometimes desirable to use a single color for each polygon so that there is no variation of color across the face of the polygon, causing each polygon to have a flat appearance. This can be useful in situations where the shape of the object warrants such a technique, perhaps because the faces really are intended to look flat, or to help visualize the locations of the polygons in a complex mesh. Using a single color for each polygon is commonly called <strong>flat shading</strong>.</p>
<p>The following image shows a mesh rendered with the Phong reflection model. On the left, Gouraud shading is used. On the right, flat shading is used:</p>
<div><img src="img/0316c524-3878-496c-9a90-008e3f9963f3.png" style="width:31.42em;height:11.83em;"/></div>
<p>In earlier versions of OpenGL, flat shading was enabled by calling the function <kbd>glShadeModel</kbd> with the argument <kbd>GL_FLAT</kbd>, in which case the computed color of the last vertex of each polygon was used across the entire face.</p>
<p>In OpenGL 4, flat shading is facilitated by the interpolation qualifiers available for shader input/output variables.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>To modify the Phong reflection model to use flat shading, take the following steps:</p>
<ol>
<li>Use the same vertex shader as in the Phong example provided earlier. Change the output variable <kbd>LightIntensity</kbd> as follows:</li>
</ol>
<pre style="padding-left: 60px">flat out vec3 LightIntensity; </pre>
<ol start="2">
<li>Change the corresponding variable in the fragment shader to use the <kbd>flat</kbd> qualifier:</li>
</ol>
<pre style="padding-left: 60px">flat in vec3 LightIntensity; </pre>
<ol start="3">
<li>Compile and link both shaders within the OpenGL application, and install the shader program prior to rendering.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>Flat shading is enabled by qualifying the vertex output variable (and its corresponding fragment input variable) with the <kbd>flat</kbd> qualifier. This qualifier indicates that no interpolation of the value is to be done before it reaches the fragment shader. The value presented to the fragment shader will be the one corresponding to the result of the invocation of the vertex shader for either the first or last vertex of the polygon. This vertex is called the <strong>provoking vertex</strong>, and can be configured using the OpenGL function <kbd>glProvokingVertex</kbd>. For example, the following call:</p>
<pre>glProvokingVertex(GL_FIRST_VERTEX_CONVENTION); </pre>
<p>Indicates that the first vertex should be used as the value for the flat-shaded variable. The <kbd>GL_LAST_VERTEX_CONVENTION</kbd> argument indicates that the last vertex should be used. The default value is <kbd>GL_LAST_VERTEX_CONVENTION</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">See also</h1>
                
            
            
                
<ul>
<li>The <kbd>chapter03/sceneflat.cpp</kbd> file in the example code</li>
<li>The <em>Implementing the Phong reflection model</em> recipe</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Using subroutines to select shader functionality</h1>
                
            
            
                
<p>In GLSL, a subroutine is a mechanism for binding a function call to one of a set of possible function definitions based on the value of a variable. In many ways, it is similar to function pointers in C. A uniform variable serves as the pointer and is used to invoke the function. The value of this variable can be set from the OpenGL side, thereby binding it to one of a few possible definitions. The subroutine's function definitions need not have the same name, but must have the same number and type of parameters and the same return type.</p>
<p>Subroutines therefore provide a way to select alternative implementations at runtime without swapping shader programs and/or recompiling, or using the <kbd>if</kbd> statements along with a uniform variable. For example, a single shader could be written to provide several shading algorithms intended for use on different objects within the scene. When rendering the scene, rather than swapping shader programs or using a conditional statement, we can simply change the subroutine's uniform variable to choose the appropriate shading algorithm as each object is rendered.</p>
<p>Since performance is crucial in shader programs, avoiding a conditional statement or a shader swap may be valuable. With subroutines, we can implement the functionality of a conditional statement or shader swap without the computational overhead. However, modern drivers do a good job of handling conditionals, so the benefits of subroutines over conditionals is not always clear-cut. Depending on the condition, conditional statements based on uniform variables can be as efficient as subroutines. </p>
<p>In this example, we'll demonstrate the use of subroutines by rendering a teapot twice. The first teapot will be rendered with the full Phong reflection model described earlier. The second teapot will be rendered with diffuse shading only. A subroutine uniform will be used to choose between the two shading techniques.</p>
<div><strong>Subroutines are not supported in SPIR-V</strong>. Therefore, their use should probably be avoided. Since SPIR-V is evidently the future of shaders in OpenGL, subroutines should be considered deprecated.</div>
<p>In the following image, we can see an example of a rendering that was created using subroutines. The teapot on the left is rendered with the full Phong reflection model, and the teapot on the right is rendered with diffuse shading only. A subroutine is used to switch between shader functionality:</p>
<div><img src="img/c0c7d649-c7a9-4d04-9e24-6581420ee598.png" style="width:28.75em;height:9.33em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>As with the previous recipes, provide the vertex position at attribute location 0 and the vertex normal at attribute location 1. Uniform variables for all of the Phong coefficients should be set from the OpenGL side, as well as the light position and the standard matrices.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We'll assume that, in the OpenGL application, the <kbd>programHandle</kbd> variable contains the handle to the shader program object.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>To create a shader program that uses a subroutine to switch between pure-diffuse and Phong, take the following steps:</p>
<ol>
<li>Set up the vertex shader with a subroutine uniform variable, and two functions of the subroutine type:</li>
</ol>
<pre style="padding-left: 60px">subroutine vec3 shadeModelType( vec3 position, vec3 normal); 
subroutine uniform shadeModelType shadeModel; <br/><br/>out vec3 LightIntensity;
 
// Uniform variables and attributes here...<br/>  
subroutine( shadeModelType ) 
vec3 phongModel( vec3 position, vec3 norm ) { 
    // The Phong reflection model calculations go here...
} 
 
subroutine( shadeModelType ) 
vec3 diffuseOnly( vec3 position, vec3 norm ) { 
   // Compute diffuse shading only..
} 
 
void main() {<br/>   // Compute camPosition and camNorm ...<br/> 
    // Evaluate the shading equation, calling one of 
    // the functions: diffuseOnly or phongModel. 
    LightIntensity = shadeModel(camPosition, camNorm); 
 
    gl_Position = MVP * vec4(VertexPosition,1.0); 
} </pre>
<ol start="2">
<li>The fragment shader is the same as the one in<em> The Phong reflection model</em> recipe.</li>
<li>In the OpenGL application, compile and link the shaders into a shader program, and install the program into the OpenGL pipeline.</li>
<li>Within the render function of the OpenGL application, use the following code:</li>
</ol>
<pre style="padding-left: 60px">GLuint phongIndex = 
   glGetSubroutineIndex(programHandle, 
                       GL_VERTEX_SHADER,"phongModel"); 
GLuint diffuseIndex = 
    glGetSubroutineIndex(programHandle, 
                       GL_VERTEX_SHADER, "diffuseOnly"); 
 
glUniformSubroutinesuiv( GL_VERTEX_SHADER, 1, &amp;phongIndex); 
... // Render the left teapot 
 
glUniformSubroutinesuiv( GL_VERTEX_SHADER, 1, &amp;diffuseIndex); 
... // Render the right teapot </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this example, the subroutine is defined within the vertex shader. The first step involves declaring the subroutine type, as follows:</p>
<pre>subroutine vec3 shadeModelType( vec3 position, vec3 normal); </pre>
<p>This defines a new subroutine type with the name <kbd>shadeModelType</kbd>. The syntax is very similar to a function prototype, in that it defines a name, a parameter list, and a return type. As with function prototypes, the parameter names are optional.</p>
<p>After creating the new subroutine type, we declare a uniform variable of that type named <kbd>shadeModel</kbd>:</p>
<pre>subroutine uniform shadeModelType shadeModel; </pre>
<p>This variable serves as our function pointer and will be assigned to one of the two possible functions in the OpenGL application.</p>
<p>We declare two functions to be part of the subroutine by prefixing their definition with the subroutine qualifier:</p>
<pre>subroutine ( shadeModelType ) </pre>
<p>This indicates that the function matches the subroutine type, and therefore its header must match the one in the subroutine type definition. We use this prefix for the definition of the functions <kbd>phongModel</kbd> and <kbd>diffuseOnly</kbd>. The <kbd>diffuseOnly</kbd> function computes the diffuse shading equation, and the <kbd>phongModel</kbd> function computes the complete Phong reflection equation.</p>
<p>We call one of the two subroutine functions by utilizing the subroutine uniform <kbd>shadeModel</kbd> within the main function:</p>
<pre>LightIntensity = shadeModel( eyePosition, eyeNorm );</pre>
<p class="mce-root"/>
<p>Again, this call will be bound to one of the two functions depending on the value of the subroutine uniform <kbd>shadeModel</kbd>, which we will set within the OpenGL application.</p>
<p>Within the render function of the OpenGL application, we assign a value to the subroutine uniform with the following two steps:</p>
<ol>
<li>First, we query for the index of each subroutine function using <kbd>glGetSubroutineIndex</kbd>. The first argument is the program handle. The second is the shader stage. In this case, the subroutine is defined within the vertex shader, so we use <kbd>GL_VERTEX_SHADER</kbd> here. The third argument is the name of the subroutine. We query for each function individually and store the indexes in the variables <kbd>phongIndex</kbd> and <kbd>diffuseIndex</kbd>.</li>
<li>Second, we select the appropriate subroutine function. To do so, we need to set the value of the subroutine uniform <kbd>shadeModel</kbd> by calling <kbd>glUniformSubroutinesuiv</kbd>. This function is designed for setting multiple subroutine uniforms at once. In our case, of course, we are setting only a single uniform. The first argument is the shader stage (<kbd>GL_VERTEX_SHADER</kbd>), the second is the number of uniforms being set, and the third is a pointer to an array of subroutine function indexes. Since we are setting a single uniform, we simply provide the address of the <kbd>GLuint</kbd> variable containing the index, rather than a true array of values. Of course, we would use an array if multiple uniforms were being set. In general, the array of values provided as the third argument is assigned to subroutine uniform variables in the following way. The i<sup>th</sup> element of the array is assigned to the subroutine uniform variable with index i. Since we have provided only a single value, we are setting the subroutine uniform at index zero.</li>
</ol>
<p>You may be wondering, "How do we know that our subroutine uniform is located at index zero? We didn't query for the index before calling <kbd>glUniformSubroutinesuiv</kbd>!" The reason that this code works is that we are relying on the fact that OpenGL will always number the indexes of the subroutines consecutively starting at zero. If we had multiple subroutine uniforms, we could (and should) query for their indexes using <kbd>glGetSubroutineUniformLocation</kbd>, and then order our array appropriately.</p>
<div><kbd>glUniformSubroutinesuiv</kbd> requires us to set <kbd>all</kbd> subroutine uniform variables at once, in a single call. This is so that they can be validated by OpenGL in a single burst.</div>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">There's more...</h1>
                
            
            
                
<p>Unfortunately, subroutine bindings get reset when a shader program is unbound (switched out) from the pipeline by calling <kbd>glUseProgram</kbd> or another technique. This requires us to call <kbd>glUniformSubroutinsuiv</kbd> each time that we activate a shader program.</p>
<p>A subroutine function defined in a shader can match more than one subroutine type. The subroutine qualifier can contain a comma-separated list of subroutine types. For example, if a subroutine matched the types <kbd>type1</kbd> and <kbd>type2</kbd>, we could use the following qualifier:</p>
<pre>subroutine( type1, type2 ) </pre>
<p>This would allow us to use subroutine uniforms of differing types to refer to the same subroutine function.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">See also</h1>
                
            
            
                
<ul>
<li>The <kbd>chapter03/scenesubroutine.cpp</kbd> file in the example code</li>
<li> <em>T</em><em>he Phong reflection model</em> recipe</li>
<li>The <em>Diffuse and per-vertex shading with a single point light source</em> recipe</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Discarding fragments to create a perforated look</h1>
                
            
            
                
<p>Fragment shaders can make use of the <kbd>discard</kbd> keyword to <em>throw away</em> fragments. Use of this keyword causes the fragment shader to stop execution, without writing anything (including depth) to the output buffer. This provides a way to create holes in polygons without using blending. In fact, since fragments are completely discarded, there is no dependence on the order in which objects are drawn, saving us the trouble of doing any depth sorting that might have been necessary if blending was used.</p>
<p>In this recipe, we'll draw a teapot, and use the <kbd>discard</kbd> keyword to remove fragments selectively, based on texture coordinates. The result will look like the following image:</p>
<div><img src="img/600677f4-55bc-481a-9449-b37c1b2bb767.png" style="width:18.92em;height:11.17em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>The vertex position, normal, and texture coordinates must be provided to the vertex shader from the OpenGL application. The position should be provided at location 0, the normal at location 1, and the texture coordinates at location 2. As in the previous examples, the lighting parameters must be set from the OpenGL application via the appropriate uniform variables.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>To create a shader program that discards fragments based on a square lattice (as in the preceding image):</p>
<ol>
<li>In the vertex shader, we use two-sided lighting, and include the texture coordinate:</li>
</ol>
<pre style="padding-left: 60px">layout (location = 0) in vec3 VertexPosition; 
layout (location = 1) in vec3 VertexNormal; 
layout (location = 2) in vec2 VertexTexCoord; 
 
out vec3 FrontColor; 
out vec3 BackColor; 
out vec2 TexCoord; 
<br/>// Other uniform variables here...
// getCamSpace and phongModel functions...<br/> 
void main() { 
    TexCoord = VertexTexCoord;
 
    // Get the position and normal in camera space <br/>    vec3 camNorm, camPosition;
    getCamSpace(camNorm, camPosition); 
 
    FrontColor = phongModel( camPosition, eyeNorm ); 
    BackColor = phongModel( camPosition, -eyeNorm ); 
 
    gl_Position = MVP * vec4(VertexPosition,1.0); 
} </pre>
<ol start="2">
<li>In the fragment shader, discard the fragment based on a certain condition by using the <kbd>discard</kbd> keyword:</li>
</ol>
<pre style="padding-left: 60px">in vec3 FrontColor; 
in vec3 BackColor; 
in vec2 TexCoord; 
 
layout( location = 0 ) out vec4 FragColor; 
 
void main() { 
    const float scale = 15.0; 
    bvec2 toDiscard = greaterThan( fract(TexCoord * scale), 
                                   vec2(0.2,0.2) );
    if( all(toDiscard) ) 
        discard; 
     
    if( gl_FrontFacing ) 
        FragColor = vec4(FrontColor, 1.0); 
    else 
        FragColor = vec4(BackColor, 1.0); 
} </pre>
<ol start="3">
<li>Compile and link both shaders within the OpenGL application, and install the shader program prior to rendering.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>Since we will be discarding some parts of the teapot, we will be able to see through the teapot to the other side. This will cause the back sides of some polygons to become visible. Therefore, we need to compute the lighting equation appropriately for both sides of each face. We'll use the same technique presented earlier in the two-sided shading recipe.</p>
<p>The vertex shader is essentially the same as in the two-sided shading recipe, with the main difference being the addition of the texture coordinate. To manage the texture coordinate, we have an additional input variable, <kbd>VertexTexCoord</kbd>, that corresponds to attribute location 2. The value of this input variable is passed directly on to the fragment shader unchanged via the output variable <kbd>TexCoord</kbd>. The Phong reflection model is calculated twice, once using the given normal vector, storing the result in <kbd>FrontColor</kbd>, and again using the reversed normal, storing that result in <kbd>BackColor</kbd>.</p>
<p class="mce-root"/>
<p>In the fragment shader, we calculate whether or not the fragment should be discarded based on a simple technique designed to produce the lattice-like pattern shown in the preceding image. We first scale the texture coordinate by the arbitrary scaling factor <kbd>scale</kbd>. This corresponds to the number of lattice rectangles per unit (scaled) texture coordinate. We then compute the fractional part of each component of the scaled texture coordinate using the built-in function <kbd>fract</kbd>. Each component is compared to 0.2 using the built-in the <kbd>greaterThan</kbd> function, and the result is stored in the Boolean vector <kbd>toDiscard</kbd>. The <kbd>greaterThan</kbd> function compares the two vectors component-wise, and stores the Boolean results in the corresponding components of the return value.</p>
<p>If both components of the vector <kbd>toDiscard</kbd> are true, then the fragment lies within the inside of each lattice frame, and therefore we wish to discard this fragment. We can use the built-in function <kbd>all</kbd> to help with this check. The function <kbd>all</kbd> will return true if all of the components of the parameter vector are true. If the function returns true, we execute the <kbd>discard</kbd> statement to reject the fragment.</p>
<p>In the <kbd>else</kbd> branch, we color the fragment based on the orientation of the polygon, as in the <em>Implementing two-sided shading</em> recipe presented earlier.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">See also</h1>
                
            
            
                
<ul>
<li>The <kbd>chapter03/scenediscard.cpp</kbd> recipe in the example code</li>
<li>The <em>Implementing two-sided shading</em> recipe in this chapter</li>
</ul>


            

            
        
    </body></html>