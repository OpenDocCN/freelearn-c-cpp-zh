- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adding Tests to a Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’re going to add a major new ability to the test library.
    The new ability will let you check conditions within a test to make sure everything
    is going as planned. Sometimes, these checks are called an *assert*, and sometimes,
    they are called an *expect*. Whatever they are called, they let you confirm that
    the values you get back from the code being tested match expectations.
  prefs: []
  type: TYPE_NORMAL
- en: For this book and the test library that we’re creating, I’m going to call these
    checks confirmations. Each confirmation will be called a *confirm*. The reason
    for this is that assert is already being used in C++, and it can be confusing
    to use the same name. Additionally, expect is a common term within other test
    libraries, which is not by itself a reason to avoid using the same term. I actually
    like the term expect. But expect has another common behavior that we don’t want.
    Many other testing libraries will let a test continue even if an expect fails.
    I don’t really like this behavior. Once something has gone wrong, I think it’s
    time to end that test. Other tests can still run. But we shouldn’t continue running
    a test once something doesn’t match what we expect.
  prefs: []
  type: TYPE_NORMAL
- en: So far, you can use the test library to write multiple tests, run them, and
    see the results. The result of each test is to either pass or fail. You can even
    expect certain failures and treat them as passing. And there’s a third result
    that will likely not be needed outside of the test library itself and that is
    a missed failure. You can read all about these abilities in the first three chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to detect whether a test passes or fails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing the testing library to support confirmations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should error cases be tested, too?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s a reason we’ve waited until this chapter to add confirms. We’re following
    a TDD approach to the design of the test library itself. That means we let the
    tests drive the design. This is an agile approach to software design. We think
    about what is the most valuable or necessary feature or capability to add next,
    what the end use of that feature will be, write the minimum amount of code needed
    to get it working, and then enhance the design by adding more.
  prefs: []
  type: TYPE_NORMAL
- en: Until now, there was no point in adding confirms. We needed to get the essential
    functionality working first, which would let tests be created and run before we
    could think about what to do inside the tests. Maybe we could have added confirms
    before the exception handling. But I choose to work on exception handling before
    confirms. Exceptions seem more closely related to the essential declaration and
    running of the tests than confirms and, therefore, are more valuable than confirms.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you’ll also see that we’ll be using exceptions to enable confirmations.
    This is another reason why the basic ability to handle exceptions came before
    confirms.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can turn our attention to the tests with confirms. Again, we’re going
    to do the minimum amount of work needed to get the confirms functional and useful.
    We’ll continue adding more abilities to confirms in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the code in this chapter uses standard C++, which builds on any modern
    C++ 17, or later, compiler and standard library. The code is based on and continues
    from the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find all of the code for this chapter at the following GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Test-Driven-Development-with-CPP](https://github.com/PacktPublishing/Test-Driven-Development-with-CPP)'
  prefs: []
  type: TYPE_NORMAL
- en: How to detect whether a test passes or fails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the tests we’ll be creating are different enough from the creation
    tests that they should have their own file. When writing your own tests, you’ll
    want to organize them into multiple files, too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a new file called `Confirm.cpp` and place it inside the `tests`
    folder. With the new file, the project structure will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, add a single test to the new file so that it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We already have an empty test in `Creation.cpp`, which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The only real difference between these two tests is the name. Do we really need
    another test that does the exact same thing but with a different name? I could
    argue on either side of a debate about adding code that does the same thing but
    with a different name. Some people might see this and think it is pure code duplication.
  prefs: []
  type: TYPE_NORMAL
- en: To me, the difference comes down to *intent*. Yes, both tests happen to be the
    same right now. But who knows whether one or both will be modified later? And
    if that ever happens, will we be able to remember that a test was serving multiple
    purposes?
  prefs: []
  type: TYPE_NORMAL
- en: I strongly urge you to write each test as if it is the only thing standing between
    your code and the bugs that the test is designed to prevent. Or maybe the test
    is exercising a specific usage to make sure that nothing breaks later during a
    design change. Having two identical tests is okay as long as they are testing
    different things. It’s the goal that should be unique.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the original test just ensures that a test can be created in its
    most basic form. The new test is specifically making sure that an empty test will
    pass. These are two different tests that just happen to require the same test
    method body to accomplish their goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a new file in the project with a new test, let’s build and
    make sure everything works as expected. And it fails. The reason for the build
    failure is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything compiles okay, but the project fails to link into the final runnable
    executable. We have five linker errors. One of the linker errors says the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: I have only listed one of the linker errors because they are all similar. The
    problem is that we now have two declarations of `Test3`. One declaration comes
    from each file, `Creation.cpp` and `Confirm.cpp`; that is because the `TEST` macro
    declares the `Test` class with a unique number based on the line number where
    the `TEST` macro appears in the source file. Both files happen to use the `TEST`
    macro on line 3, so they each declare a class called `Test3`.
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this is to use an *unnamed namespace* in the macros when declaring
    the class. This will still create two classes such as `Test3`, but each will be
    in a namespace that does not extend outside of the `.cpp` file in which it is
    declared. This means that the test classes can continue to be based on the line
    number, which is guaranteed to be unique within each `.cpp` file and will now
    no longer conflict with any other tests that happen to be declared on the same
    line number in a different `.cpp` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'All we need to do is modify the `TEST` and `TEST_EX` macros to add an unnamed
    namespace around just the class declaration inside of each macro. We don’t need
    to extend the namespace to the end of the macro because the macros go on to declare
    the beginning of the `run` method. Luckily, the `run` method declaration does
    not need to be inside the namespace. Otherwise, we would have to figure out how
    to end the namespace with the closing curly brace after the `run` method has been
    fully defined. As it is, we can end the namespace after the class declaration.
    The `TEST` macro looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And the `TEST_EX` macro needs a similar unnamed namespace as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the project builds again, running it will show the new test. Depending
    on what order your linker built the final executable, you might find the new test
    runs before or after the previous tests. Here is a portion of the results when
    I ran the test project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The other five tests and the summary are not shown. The previous chapter ended
    with six tests, and we just added one more bringing the total to seven tests.
    The important part is that the new test runs and passes. Now we can think about
    what a confirm will look like. And what does it mean to confirm something?
  prefs: []
  type: TYPE_NORMAL
- en: When running a test, you want to not just verify that the test completes but
    that it completes correctly. And it also helps to check along the way to make
    sure everything is running as expected. You can do this by comparing the values
    you get from the code being tested to make sure they match the expected values.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that you have a function that adds two numbers and returns a result.
    You can call this function with known values and compare the returned sum with
    an expected sum that you calculate yourself. You confirm that the calculated sum
    matches the expected sum. If the values match, then the confirm passes. But if
    the values don’t match, then the confirm fails, which should cause the test to
    fail, too.
  prefs: []
  type: TYPE_NORMAL
- en: A test can have multiple confirms and each will be checked to make sure they
    pass. The moment one confirm fails, there’s no point in continuing the test because
    it has already failed. Some TDD purists will claim that a test should only have
    a single confirmation. I think there’s a good compromise between only having a
    single confirm versus writing epic tests that try to verify everything.
  prefs: []
  type: TYPE_NORMAL
- en: 'A popular style of writing tests with multiple confirmations is to keep track
    of how many confirms pass by letting a test continue even if a confirm fails.
    There is a benefit to this style because the developer can sometimes fix multiple
    problems with a single run of the tests. We’re not taking this approach because
    I think the benefit is rarely achieved in practice. Some people might argue this,
    but hear me out. Once something is proven to not meet your expectations, the most
    likely result is a chain reaction of further failures. I have rarely seen a well-designed
    test fail one confirmation and then somehow recover to pass unrelated confirmations.
    If the test behaves like this, then it normally is testing unrelated issues and
    should be broken into multiple tests. The practice we’re going to be following
    is this: when a confirm fails, then the test itself has failed. Other tests might
    proceed just fine. But the test with a failed confirm has already failed, and
    there is no point in continuing to see whether maybe some part of the test might
    still be okay.'
  prefs: []
  type: TYPE_NORMAL
- en: When writing tests, just like when writing regular code, it’s good to avoid
    duplication. In other words, if you find yourself testing the same things by checking
    values that have already been checked in other tests, then it’s time to think
    about the goal of each test. Write one test that covers some basic functionality
    that will be used many times. Then, in other tests that make use of that same
    functionality, you can assume it has already been tested and works, so there is
    no need to verify it again with extra confirms.
  prefs: []
  type: TYPE_NORMAL
- en: Some code will probably make all of this clearer. First, let’s think about how
    to verify an expected result without a confirm. This is a time when we can’t just
    write the code for what a confirm will look like because we don’t know yet what
    we want it to do. A little exploration is in order. The next section will turn
    the exploration we’ll do here into actual confirms.
  prefs: []
  type: TYPE_NORMAL
- en: For a moment, let’s pretend that we have a real TDD project that we’re working
    on. We’ll keep things simple and say that we need some way to determine whether
    a school grade is passing or not. Even this simple example could become complicated
    if there were different guidelines for passing homework versus quizzes or tests.
    If that were the case, there might be a whole class hierarchy involved. We just
    have a simple need to determine whether a score from 0 to 100 is a passing grade
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our scenario, what would a simple test look like? We don’t
    have any code to support the grading requirement. It’s just a general idea of
    what we want. So, we expect the build to fail if we try running right after creating
    a test. This is how you can use TDD to come up with a design.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we’ll put this code inside `Confirm.cpp`. If we were really building
    a test project for a school grading application, then there might be a test file
    called `Grades.cpp`. Because we’re just exploring, we’ll use the test file we
    already have, called `Confirm.cpp`, and create a test like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The first thing is to think about the usage. If you had a function called `isPassingGrade`
    that accepted a score and returned a bool result, would that meet your requirements
    and be easy to use? It seems easy enough. It will do whatever it needs inside
    to tell us whether the score is passing or not and return true if the grade is
    passing and false if it’s not.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you can think about how to test this function. It’s always good to test
    boundary conditions, so we can start by asking whether a score of 0 is passing
    or not. We assign the passing result to a variable that can be tested against
    an expected value. We expect 0 to be a failing grade, which is why the code throws
    something if the result is true instead. This will cause the test case to fail
    because of an unexpected exception.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re on the right track. This is what I want you to understand about checking
    along the way to make sure everything is running okay. We could add another check
    in the same test like this to make sure that 100 is a passing grade:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can see a single test that checks two things. First, it makes sure
    that a score of 0 is a failing grade and then that a score of 100 is a passing
    grade. Because these checks are so related, I would put them in the same test
    as this and confirm that the first case should be a failing grade and the second
    should be a passing grade.
  prefs: []
  type: TYPE_NORMAL
- en: A test confirmation is nothing more than a simple check against an expected
    value that throws an exception if the expectation is not met.
  prefs: []
  type: TYPE_NORMAL
- en: Some TDD purists will recommend that you split the test into two separate tests.
    My advice is to use your best judgment. I tend to avoid absolute guidance that
    says you should *always* do something one way or another. I think there’s room
    to be flexible.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get this building so that we can run it and see the results. All we need
    to do is add the `isPassingGrade` function. We’ll add the function to the top
    of `Confirm.cpp`. If this was a real project you were working on, then you would
    have a better place to put this function. It would not be in the test project;
    instead, it would be included in the project being tested.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside `Confirm.cpp`, create a function called `isPassingGrade`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can build and run the project to see the results. The test result we’re
    interested in fails like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The function should obviously fail because it always returns true for a passing
    grade regardless of the score given. But that’s not the part we’re going to focus
    on next. It would be if you really were building and testing a grading application.
    You would enhance the design, get the test to pass, and then enhance the test,
    and continue until all the tests pass.
  prefs: []
  type: TYPE_NORMAL
- en: This is enough to demonstrate what I mean by checking on the progress of a running
    test to make sure it’s proceeding as expected. Now we have a test that, first,
    checks to make sure 0 is a failing grade and then checks to make sure 100 is a
    passing grade. Each of these is a confirm. At each point, we are checking whether
    the actual result matches what we expect. And we confirm in different ways to
    fit each condition.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’re going to enhance the test library to fix problems
    with the current solution and make it easier to write confirms. Right now, the
    code throws an int when it detects a problem, and while the throw definitely causes
    the test to fail, it leads to a test result that says the failure was caused by
    an unexpected exception.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will wrap the `if` statement with its criteria and the exception
    throwing into an easy macro that will handle everything and lead to a better description
    of what actually failed and where it failed.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing the testing library to support assertions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The passing grades test from the previous section has two confirms that we’re
    going to improve in this section. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the first confirm, we want to make sure that `result` is false because we
    know that a score of 0 should not be a passing grade. And for the second confirm,
    we want to make sure that, this time, `result` is true because we know that a
    score of 100 should lead to a passing grade.
  prefs: []
  type: TYPE_NORMAL
- en: Can you see how the `if` condition needs to be the *opposite* of what we’re
    trying to confirm? This is because the `if` block runs when the confirm does *not*
    meet the expected value. We’ll need to make this easier to use because it will
    lead to bugs if we always have to write confirms like this. But there are still
    bigger problems with the test code.
  prefs: []
  type: TYPE_NORMAL
- en: Why does it throw an int if the check fails? That’s because we’re still exploring
    what a real confirm should look like. The code we have now just shows you the
    need for making checks along the way inside of a test to ensure things are proceeding
    as expected. This section will change how we’re going to be writing confirms in
    our tests.
  prefs: []
  type: TYPE_NORMAL
- en: Throwing an int when a value does not match what was expected also leads to
    the wrong test result description. We don’t want the test results to say that
    an unexpected exception was thrown.
  prefs: []
  type: TYPE_NORMAL
- en: However, we do want to throw something. Because once a test deviates from the
    expected path, we don’t want the test to continue. It’s already shown that it
    has failed. Throwing whenever an expected condition is not met is a great way
    to fail the test at that point. We need to figure out a way to change the test
    result description to better inform us of what went wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s fix the test result by throwing something more meaningful. Note
    that the following code uses hardcoded numeric values, such as 17 and 23\. Numbers
    such as these are often called *magic numbers* and should be avoided. We’ll be
    fixing the problem soon, and the use of direct numbers whose meaning is unclear
    is included to show you that there is a better way. In `Confirm.cpp`, change the
    passing grades test to throw `BoolConfirmException` from both confirms like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Later, we’ll need to create this class. For now, we want to code it like we
    intend to use it. It’s called `BoolConfirmException` because it will let us confirm
    that a bool value matches what we expect. The constructor parameters will be the
    expected bool value and the line number. I used line numbers 17 and 23 because
    they are the line numbers in my editor for the two `throw` statements. Later in
    this section, we’ll use a macro so that we can let the macro provide the line
    number automatically. Normally, you would want to avoid hardcoding any numeric
    value in the code except for simple values such as 0, 1, and maybe -1\. Any other
    values are called magic numbers because the meaning is confusing.
  prefs: []
  type: TYPE_NORMAL
- en: The exception thrown in confirms will be based on the information needed to
    make a meaningful test result description. For bool values, the expected value
    and line number are enough. Other exceptions will need more information and will
    be explained in the next chapter. We’ll have more than one exception type, but
    they will be related. Inheritance is a good way to represent the different exception
    types that we’ll be throwing. The base class for all the types will be called
    `ConfirmException`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `Test.h`, create a new class called `ConfirmException` inside the `MereTDD`
    namespace like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, right after the base exception class, we can declare the derived `BoolConfirmException`
    class like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The purpose of `BoolConfirmException` is to format a meaningful description
    that can be read through the `reason` method in the base class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing we need to do is catch the base class when running the tests
    and display the confirm reason instead of a message saying that there was an unexpected
    exception. Modify the `runTests` method in `Test.h` so that it will catch the
    new exception base class and set the appropriate failed message like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The confirm exception is ready. Building and running shows the following test
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This is a lot better than saying there was an unexpected exception. Now, we
    understand there was a confirm failure on line 17 and that the test expected the
    value to be false. Line 17 is for grade 0, which we expected to be a failing grade.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add a macro for the confirm so that we no longer have to provide the
    line number manually. And the macro can include the backward logic in the `if`
    condition and the throwing of the proper confirm exception. Here’s what the test
    should look like with the macro. We’ll add the macro but only after we write the
    code that intends to use the macro. Change the passing grades test in `Confirm.cpp`
    to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now the test really looks like it’s using confirms. Additionally, the macros
    make it very clear that the first confirm is *expecting* `result` to be false,
    while the second confirm is *expecting* `result` to be true. The value that gets
    passed to the macro is called the *actual* value. As long as the actual value
    matches the expected value, then the confirm passes and lets the test continue.
  prefs: []
  type: TYPE_NORMAL
- en: 'To define these macros, we’ll put them at the end of `Test.h`. Note that each
    one is almost identical to what the test used to code manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You can see that when confirming a false expected value, the `if` condition
    looks for a true actual value. Additionally, when confirming a true expected value,
    the `if` condition looks for a false actual value. Both macros throw `BoolConfirmException`
    and use `__LINE__` to get the line number automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, running the tests shows almost the exact same results. The only difference
    is the line number that the passing grades test fails at. This is because the
    confirm macros now use a single line each. The results look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The confirms are much easier to use now, and they make the test clearer to read
    and understand. Our goal is not to build a school grading application, so we’ll
    be removing the exploratory code. However, before removing it, the next section
    will use the passing grades test to explain another important aspect of TDD. And
    that is the question of what to do about error cases.
  prefs: []
  type: TYPE_NORMAL
- en: Should error cases be tested, too?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is it possible to get to 100% testing code coverage? And what does that mean?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me explain by continuing to use the passing grades code we were exploring
    in the previous section. Here is the test again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Right now, this test does cover 100% of the function under test. That means
    that all of the code inside the `isPassingGrade` function is being run by at least
    one test. I know, the `isPassingGrade` function is a simple function with a single
    line of code that always returns true. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'With a function this simple, just calling it from within a test will make sure
    that all of the code is covered or run. As it is, the function doesn’t work and
    needs to be enhanced to pass both confirms. We can enhance it to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Building and running the project now passes the test. The results of the passing
    grades test look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: And we still have 100% code coverage for this function because the passing grades
    test calls the function twice with the values of 0 and 100\. The first call causes
    the `if` condition to be true, which executes the code inside the `if` block.
    And the second call causes the `return` statement after the `if` block to run.
    By calling `isPassingGrade` with both the 0 and 100 values, we cause all of the
    code inside to be run at least once. That is what it means to achieve 100% code
    coverage.
  prefs: []
  type: TYPE_NORMAL
- en: Both values of 0 and 100 are valid grades, and it makes sense to test with them.
    We don’t need to test what will happen if we call `isPassingGrade` with the values
    of 1 or 99\. That’s because they are not interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Edge values are almost always interesting. So, it would make sense to add a
    couple more calls inside the test for values 59 and 60\. While these represent
    good call values and confirms to add to the test, they won’t do anything for the
    code coverage.
  prefs: []
  type: TYPE_NORMAL
- en: That leads to the first point I want you to understand. Simply achieving 100%
    code coverage is not enough. You want to ensure that you are testing everything
    that needs to be tested. Look for edge cases that should be tested even if they
    don’t do anything to improve your code coverage.
  prefs: []
  type: TYPE_NORMAL
- en: And then look for error cases.
  prefs: []
  type: TYPE_NORMAL
- en: Error cases will likely drive your code to add extra checking to make sure the
    error cases are properly handled. TDD is a great way to drive these conditions.
    Alternatively, you might decide to change your design as a way to make an error
    case no longer applicable.
  prefs: []
  type: TYPE_NORMAL
- en: For example, does it make sense to check whether a negative grade is passing?
    If so, definitely add a test and then add the code to make the test pass. This
    is something that I would put into a new test. Remember the balance between having
    a single confirm per test versus allowing multiple confirms?
  prefs: []
  type: TYPE_NORMAL
- en: It makes sense to include all confirms for calling `isPassingGrade` for the
    values of 0, 59, 60, and 100 in a single test. At least to me.
  prefs: []
  type: TYPE_NORMAL
- en: However, calling `isPassingGrade` with a value of -1 is different enough that
    it should have its own test. Or maybe thinking of this test is enough to cause
    you to change the design so that `isPassingGrade` no longer accepts an int parameter,
    and you decide to use an unsigned int parameter instead. For this particular example,
    I would probably use an unsigned int. That would mean we no longer need a test
    for -1 or any negative number grade.
  prefs: []
  type: TYPE_NORMAL
- en: But what about grades above 100? Maybe they should be allowed for extra credit
    grades. If so, then add a new test for grades above 100 and make sure to confirm
    they pass. You might find the values of 101, 110, and 1,000,000 to be interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Why the values of 101, 110, and 1,000,000? Well, 101 is an edge value and should
    be included. The value of 110 seems like a reasonable extra credit value. And
    the value of 1,000,000 is a good example of a ridiculous value that should be
    included just to make sure the code doesn’t fail with some unexpected exception.
    You might even consider putting the 1,000,000 value in its own test.
  prefs: []
  type: TYPE_NORMAL
- en: Error cases should be tested. Ideally, you will think of the error cases while
    writing the tests, and you can write the test first before adding code to handle
    the error condition. For example, if you decide that any grade over 1,000 should
    result in an exception being thrown, then write a test that expects the exception
    and call `isPassingGrade` with the value of 1,000 to make sure that it does throw.
  prefs: []
  type: TYPE_NORMAL
- en: 'One final thought about testing error cases is this: I’ve seen a lot of code
    that was not designed using TDD, and one thing that stands out to me regarding
    a lot of this code is that error cases are much harder to test. Sometimes, it’s
    no longer feasible to add tests for certain error cases because they are too difficult
    to isolate and get them to run so that the test can verify how the code responds.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you start following TDD, you’ll find that you have much better test coverage.
    That’s because you designed tests first, including the tests for error cases.
    This forces you to make your designs *testable* from the very beginning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned how to write tests that can detect a failure even
    before reaching the end of the test. You learned how to use confirms to make sure
    that the actual values match what you expect them to be. However, this chapter
    only explained how to check bool values. There are many other types of values
    you will need to check, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You might have a number such as a count that needs to be confirmed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might need to check a string value to make sure it contains the text you
    expect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next chapter will add these additional types and explain a common problem
    when comparing fractional or floating-point numeric values.
  prefs: []
  type: TYPE_NORMAL
