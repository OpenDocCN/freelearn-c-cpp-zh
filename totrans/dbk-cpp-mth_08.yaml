- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Fastest C++ Code is Inline Assembly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Lower than this you should* *not get*'
  prefs: []
  type: TYPE_NORMAL
- en: In the fast-paced world of C++ developers, where efficiency is paramount, optimizing
    code to squeeze out every last drop of performance has always been a fascinating
    challenge. This journey often takes developers down to the very roots of computing,
    where C++ meets assembly language, and every CPU cycle counts.
  prefs: []
  type: TYPE_NORMAL
- en: Circa three decades ago, during the wild 90s, programmers frequently had to
    manually craft every byte of executable code, often diving into the murky waters
    of assembly language (and even lower) to achieve the desired performance. These
    early pioneers of optimization developed techniques that, while rudimentary by
    today’s standards, laid the groundwork for understanding the power and limitations
    of both C++ and assembly.
  prefs: []
  type: TYPE_NORMAL
- en: This exploration delves into the specifics of optimizing a seemingly simple
    task, lighting up a pixel on the screen, by comparing handcrafted and optimized
    assembly routines from three decades ago with the modern-day output of advanced
    compilers such as Clang, GCC, and MSVC. As we navigate through the evolution of
    compilers, we’ll see how the balance between human intuition and machine-generated
    optimization has shifted, offering new insights into the ever-evolving relationship
    between the code we write and the machine that ultimately runs our programs. As
    a side note, in this chapter, we’ll focus on Intel’s x86 family of processors
    and delve into specific features, while leaving coverage of the ARM architecture
    for another book, potentially by a different author.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to use assembly code to speed up your routines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How not to use assembly code and trust your compiler’s optimizer to come up
    with the fastest solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Light me a pixel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Circa 30 years ago, at the nearer end of the wild 90s, the author of these lines
    spent quite a significant time optimizing code that was supposed to run as fast
    as possible, consuming the least amount of resources while showing incredible
    spinning graphics on a screen (there was also scrolling involved, too, and other
    not relevant calculations).
  prefs: []
  type: TYPE_NORMAL
- en: These applications were called demos (intros, etc.) and showcased some spectacular
    graphical effects, backed by a strong mathematical background, and had an in-house
    developed graphical engine; in those days, there was no DirectX to take all those
    nasty low-level details off your plate, so all had to be done by hand. Methods
    for pixel color calculation, color palette setting, vertical retrace of the CRT
    screen, and flipping of back and front buffers were all coded by hand, using C++
    of the 90s and some assembly language routines for the time-critical bits.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of these methods was putting a pixel on the screen, which, in its simplest
    incarnation of the method, looked like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'I’ll spare you the very low-level details such as how segment/offset memory
    worked 30 years ago. Instead, imagine that the following apply:'
  prefs: []
  type: TYPE_NORMAL
- en: You are using DOS (in 1994, in the wild-far-eastern part of Europe, almost everyone
    who had a PC used DOS – kudos to the 0.1 percent of early Linux adopters)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are also using a special graphic mode, 0x13 (almost all the games used this
    mode because it allowed 256 colors to be drawn on the screen using the mysterious
    320 by 200 resolution, whose origins only IBM engineers from 40 years ago know)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, if you put a byte at the **0xA000** segment and a specific offset,
    the graphic card will light up a pixel at the specific coordinates, which can
    be obtained from the preceding formula.
  prefs: []
  type: TYPE_NORMAL
- en: Now, after several iterations of the code, the aforementioned programmer observed
    that the routine was not that optimal, and it could have benefited from some optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please bear with me; the code that was generated by the affordable compiler
    (the one you just copied from the disk that we mentioned in [*Chapter 2*](B22235_02.xhtml#_idTextAnchor026)
    of the book) is in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Everyone’s favorite Turbo Debugger from 30 years ago](img/B22235_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Everyone’s favorite Turbo Debugger from 30 years ago
  prefs: []
  type: TYPE_NORMAL
- en: Now, this looks pretty wild, considering the age of it, but again, we need just
    a bit of patience, and all the mystery surrounding why it’s here will be revealed.
    You see, we were discussing how the code generated by the compiler is far from
    being optimal.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a moment to consider this piece of code. After giving it some thought,
    especially from the perspective of someone familiar with assembly language, which
    is becoming increasingly rare these days, it might be clear to them that the compiler
    didn’t struggle as much as we might expect.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the assembly code that the compiler generated for the **putpixel**
    routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For those not familiar with the notation, **[]** represents the data at the
    address given in the square parentheses, so the parameters are being passed in
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: The **x** coordinate of the pixel ( from **[bp+4]** )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **y** coordinate of the pixel ( from **[bp+6]** )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The color value to set ( from **[bp+8]** )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indeed, the code as is contains a lot of unnecessary memory access to move data
    around, when those operations could have been kept in registers, and there is
    quite a lot of unnecessary access to various memory areas, which can be skipped.
    The code compiled by the compiler of the day generated code that was easy to debug,
    but which could have been written much neater. Compilers today generate the same
    kind of code, having a very similar performance, when compiling in Debug mode
    but once you switch them to optimized Release mode, they will do magic.
  prefs: []
  type: TYPE_NORMAL
- en: Modern CPUs are highly complex beasts; when running in protected mode, they
    employ various techniques, such as out-of-order execution, instruction pipelining,
    and other techniques that make really low-level performance analysis nowadays
    quite difficult to nail down properly... but old machines were much simpler! Or
    just use DOS on a modern computer and you will get the same feeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Not considering that protected mode was introduced in the early 80286 processors,
    DOS simply could not handle it (and still can’t), so it stuck to what it knew
    best: running programs in real mode. While running in real mode, the processor
    just executed one instruction after the other, and there even was an instruction
    table explaining how many cycles each instruction would take [1](B22235_08.xhtml#footnote-024)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: '[1](B22235_08.xhtml#footnote-024-backlink) [https://zs3.me/intel.php](https://zs3.me/intel.php)'
  prefs: []
  type: TYPE_NORMAL
- en: After spending a significant amount of time consulting those tables, we came
    to the conclusion that one **imul** can take longer than two shifts and an add
    on a processor of those days (the same conclusion was drawn by several other thousands
    of programmers all around the world after consulting those tables, but we felt
    that we must be some kind of local heroes for discovering this feature).
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering that 320 is a very nice number, as it is the sum of 256 and 64,
    after several rounds of optimizations, we came up with the following slightly
    more optimized version for the routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It is not the most optimal routine that one can come up with for this purpose,
    but for our specific requirements, it was more than enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'A significantly reduced amount of direct memory access (which was considered
    slow even in the old days), the lengthy multiplication by 320 using **imul** changed
    to multiplication by 256 (this is the shift to the left by 8 operations: **shl
    dx,8** ), and 64 (the same by 6), then a sum, which still adds up to fewer cycles
    than the power-consuming multiplication.'
  prefs: []
  type: TYPE_NORMAL
- en: And thus, the foundation was laid for the myth that if you really want fast
    code, you have to write it yourself at the lowest possible level.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an interesting mental exercise, let’s jump forward in time 30 years, skipping
    several generations of compilers. If we feed the C++ routine as it is to a modern
    compiler (for our purpose, we have used Clang – the latest at the time of writing
    was version 18.1 – but using GCC will get also a very similar result, just using
    a different set of registers), we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is way shorter than the one we concocted for our purpose and considered
    optimal targeting the processors from 3 decades ago, but processors have evolved
    a lot in the last 30 years, and a lot more advanced features have come in, with
    new commands (some more words about new commands a bit late in this chapter, so
    stay tuned) and we find it extremely satisfying how compilers’ optimization routines
    have resolved the multiplication with that nice number, 320.
  prefs: []
  type: TYPE_NORMAL
- en: C++ compilers have evolved significantly over the past few decades, from their
    humble beginnings as Turbo C++ or Watcom C++, becoming incredibly sophisticated
    and capable of performing a wide range of optimizations that were previously unimaginable
    due to mostly hardware constraints because, well... 640 KB should be enough for
    everyone.
  prefs: []
  type: TYPE_NORMAL
- en: Modern compilers are no longer just simple translators from human-readable code
    to machine code; they have become complex systems that analyze and transform code
    in ways that can drastically improve performance and memory usage, taking into
    consideration some aspects that are all meant to help developers bring out the
    best of their source.
  prefs: []
  type: TYPE_NORMAL
- en: GCC, Clang, and MSVC all employ advanced optimization techniques such as inlining
    functions, loop unrolling, constant folding, dead code elimination, and aggressive
    optimizations that span across entire modules or programs, since, at their stage,
    they have an overview of the entire application, allowing these high-level optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: On a side note, these compilers also leverage modern hardware features, such
    as vectorization and parallelism, to generate highly efficient machine code that
    can target a specific processor. We will soon see how these optimizations fall
    into place when we present the example in the next section, where we take a mundane
    task and let our compilers churn through it.
  prefs: []
  type: TYPE_NORMAL
- en: 'But till we reach that stage, just one more use case from 30 years ago. The
    subtitle of this chapter is *Lower than this you should not get* . Certainly,
    we meant coding at a lower level, not something else, and right now, we will proudly
    contradict ourselves. Again. Here is the contradiction: *in certain situations,
    you really should get to a level lower* *than assembly* .'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are familiar with graphic programming, then I suppose you are familiar
    with the concept of double-buffering and back-buffering. The back buffer is an
    off-screen buffer (memory area, with the same size as the screen) where all the
    rendering (drawing of graphics) happens first. When the rendering is done, the
    back buffer is copied onto the screen in order to show the graphics, the back
    buffer is cleared, and the rendering restarts. At some time in history, Tom Duff,
    a Canadian programmer, invented a wonderful piece of ingenious code that was meant
    to accomplish exactly this task; the name of it is Duff’s device and it has been
    discussed several times in several forums, and we are not going to discuss it
    now. Instead, we will show you the “highly optimized” code that we used to copy
    data from the back buffer to the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The preceding trick consists of the **rep movsb** instruction, which will do
    the actual copying of bytes ( **movsb** ), repeated ( **rep** ) 64,000 times,
    as indicated by the CX register (we all know that 64,000 = 320 x 200; that’s why
    they are magic numbers).
  prefs: []
  type: TYPE_NORMAL
- en: 'This code works perfectly given the circumstances. However, there is an opportunity
    for a tiny bit of optimization; you see, we are using a decent processor – at
    least, an 80386. Unlike its predecessor, the 80286, which was a pure 16-bit processor,
    the 80386 is a huge step forward, since it is the first 32-bit x86 processor coming
    from Intel. So, what we can do is the following: instead of copying 64,000 bytes
    using **rep movsd** , we can harvest the opportunities given by our high-end processor
    and put to use the new 32-bit framework, keywords, and registers. What we do is
    move 16,000 double words (we all know that a byte is measured as 8 bits, two bytes
    are called a word, measuring 16 bits, and two words are called a double word,
    totaling 32 bits) because that is exactly what the new processor has support for:
    operation on 32-bit values. The newly introduced **movsd** command does exactly
    this: copies 4 bytes in one step, so that could be a speed-up of 4 times compared
    to our older code.'
  prefs: []
  type: TYPE_NORMAL
- en: Our anecdotical C++ compiler, introduced at the beginning of this book, is Turbo
    C++ Lite. Unfortunately for us, Turbo C++ cannot compile code for anything other
    than processors below 80286, so we are stuck with 16-bit registers and some really
    inefficient register handling.
  prefs: []
  type: TYPE_NORMAL
- en: 'And here is where the lowest level of hack anyone can see in C++ code comes
    in – we simply add the bytes of the **rep movsd** command as hexadecimal values
    in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Nothing is simpler and more eye-watering than seeing this in production code,
    right? Now, regardless that our compiler cannot compile code for 80386 because
    it’s stuck in the Stone Age (pretty much like half of the chapter you are reading
    right now), we can still produce code that runs optimally on your processor. Please
    don’t do this.
  prefs: []
  type: TYPE_NORMAL
- en: A note on the past
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, you might ask why we even bother mentioning assembly language in 2024,
    when the major trends exhaust themselves concerning the widespread adoption of
    AI-driven development tools, the growth of low-code/no-code platforms, and the
    continued rise of the Nth iteration of various JavaScript modules that have exactly
    the same output as the previous one, except that the syntax is different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless that these are the loudest happenings in the IT world nowadays,
    assembly language is still not obsolete. It might not get as much focus as everyone’s
    favorite Rust language (Alex will debate Rust in a later chapter if all goes according
    to plan), but there are still major business branches where the assembly is a
    must, and still essential in several hardware environments that require precise
    control, performance optimization, or direct hardware access, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Embedded systems** : Microcontrollers and IoT devices often use assembly
    for efficient, low-level programming. There isn’t too much power on these small
    devices; every bit counts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operating system** ( **OS** ) **development** : Bootloaders and critical
    parts of OS kernels require assembly for hardware initialization and management.
    To achieve this feat, either you work for a large corporation or start your own
    project. Linux is pretty much accounted for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-performance computing** ( **HPC** ): Assembly is used for optimizing
    performance-critical code, particularly in scientific computing or custom hardware
    (e.g., FPGAs). To pursue this, you must find someone wanting to pay you to pursue
    this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and reverse engineering** : Analyzing and exploiting binaries often
    involves understanding and writing assembly. This is the most lucrative of all,
    and the most realistic way of getting into assembly programming, unfortunately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Firmware development** : BIOS/UEFI and low-level device drivers are commonly
    written in assembly for direct hardware interaction. Here, again, you must be
    on the payroll of a large corporation, although there are a few open source projects
    too (coreboot, libreboot, or just google free bios to get a decent list).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legacy systems** : Maintaining older systems or working with retro computing
    often requires assembly. This is the ideal chance to blend both fun and suffering
    into one experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specialized hardware** : DSPs and custom CPU architectures may need assembly
    for specialized, efficient processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please don’t dismiss assembly language just yet. It remains relevant and will
    continue to be as long as computers exist. For those who are interested in the
    topic, it has its place. Otherwise, you can stick to standard C++.
  prefs: []
  type: TYPE_NORMAL
- en: The sum of all numbers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dearest esteemed reader. It is a truth universally acknowledged that all developers
    at some stage in their lives must go through a technical interview. There are
    various levels of interrogations: some just on the level of “Please tell me something
    about yourself” (these are the hardest), while some go deeper and might even ask
    you to write some code on a blackboard or even a computer.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the programs that very frequently comes up in interview questions is
    to write some code that will calculate the sum of a series of numbers sharing
    a certain peculiarity, for example, the sum of all even numbers, the sum of all
    numbers divisible by, let’s say, five, or the sum of odd numbers in a specific
    interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity’s sake, let’s stick to something simple: the sum of all odd
    numbers up to 100. The following quick program delivers exactly this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Not an overly complicated program: just iterate through the numbers; check
    whether they are odd; if yes, add their value to the final sum; and, in the end,
    print out the sum (for everyone interested, the sum of odd numbers from 1 to 100
    is exactly 2,500).'
  prefs: []
  type: TYPE_NORMAL
- en: 'But our clear thinking was clouded by the well-known fact (at least, for C++
    programmers) that the fastest C++ code is inline assembly, so we decided to sacrifice
    the portability and understandability of our program on the altar of speed and
    rewrite the main part of it using assembly language. Because, well, that is the
    fastest. Here is our attempt at this, using AT&T assembly syntax, just to demonstrate
    the widely available assembly dialects we can embed in a non-standard compliant
    C++ program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Just a quick presentation of what the assembly code does, because I hope the
    other lines of code are self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the assembly code breakdown:'
  prefs: []
  type: TYPE_NORMAL
- en: '**"movl $1, %[i]\n"** : This instruction sets **i** to **1** . Although **i**
    was already initialized to **1** in the C++ code, this explicitly sets it again
    in assembly for clarity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**"movl $0, %[sum]\n"** : This sets the sum to **0** , ensuring that the sum
    starts from **0** in the assembly code. We have to admit that these two initializations
    are not required, but we wanted them to be a gentle introduction to the assembly
    code so as not to scare you away.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**loop_start** : This is just a label, and needs no further clarification.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**"cmpl $100, %[i]\n"** : Compares **i** with **100** . The comparison is used
    to check whether **i** has reached or exceeded **100** .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**"jg loop_end\n"** : If **i** is greater than **100** , the program jumps
    to **loop_end** , exiting the loop.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**"addl %[i], %[sum]\n"** : Adds the current value of **i** to **sum** . This
    accumulates the sum of all odd numbers up to **99** .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**"addl $2, %[i]\n"** : Increments **i** by 2 to move to the next odd number
    (e.g., 1 → 3 → 5, etc.).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**"jmp loop_start\n"** : Jumps back to the start of the loop to repeat the
    process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**loop_end** : This is the label where the program jumps when **i** exceeds
    **100** , effectively ending the loop.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The weirdly looking **"+r" (sum)** and **"+r" (i)** parts are constraints that
    tell the compiler to treat **sum** and **i** as read-write variables, meaning
    their values can be both read from and written to during the assembly operations.
  prefs: []
  type: TYPE_NORMAL
- en: As a first drawback, the readability and understandability of the code have
    suffered exponentially. We intentionally use the AT&T syntax for assembly because
    it is much more cumbersome and harder to comprehend, and we want you to suffer
    with it and remember never to use assembly in your code unless you know what you’re
    doing, and then you’re excused.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, this code is not portable anymore because there is no such thing as
    **__asm__** under Visual C++; they used **__asm** back in the day (or more recently,
    at the beginning of this chapter, Turbo C demonstrated the introduction of the
    **asm** keyword). And since we are here, the C++ standard does not include a common
    assembly block identifier because assembly language syntax is compiler- and platform-specific,
    and inline assembly is an extension rather than a core part of the language. You
    have been warned. I really hope that the preceding statement managed to entirely
    discourage you from ever considering writing assembly code in the body of your
    C++ function, regardless of the presence of the non-standard keyword to enable
    you to do this.
  prefs: []
  type: TYPE_NORMAL
- en: But now that we are here, courtesy of [gcc.godbolt.org](http://gcc.godbolt.org)
    , we have asked the major compilers to churn through the original little C++ program
    (with no assembly incursion at all) at various optimization levels because we
    feel the urge to demonstrate to you that, indeed, entirely skipping the assembly
    language at this stage is the wisest decision you can take.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one to demonstrate how efficient the compiler is in generating optimal
    C++ code is Microsoft Visual C++. Microsoft’s own tiny, squishy C++ compiler has
    several options to generate and optimize the generated code [2](B22235_08.xhtml#footnote-023)
    , but we have a saying here: the shorter the code, the faster it runs. So, we
    have explicitly told the compiler to generate the shortest code ( **/O1** ), which
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[2](B22235_08.xhtml#footnote-023-backlink) [https://learn.microsoft.com/en-us/cpp/build/reference/o-options-optimize-code?view=msvc-170](https://learn.microsoft.com/en-us/cpp/build/reference/o-options-optimize-code?view=msvc-170)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, the assembly output from MSVC is very much in line with the one
    we concocted by hand; it has a loop, a bit differently dealing with the various
    registers based on whether we are currently dealing with an odd or even number,
    but besides this, it’s similar to the one we wrote.
  prefs: []
  type: TYPE_NORMAL
- en: Using the other combinations for optimization flags ( **/Ox** , **/O2** , and
    **/Ot** ) for MSVC did not produce a very different code, just a slightly different
    assignment of the registers, but nothing that would make us say VOW!
  prefs: []
  type: TYPE_NORMAL
- en: 'After switching to GCC (14.1) in order for it to churn through our simple code,
    we noticed that for the optimization levels of **–O1** and **–O2** , the code
    generated was very similar to the one generated by MSVC: it had a variable, churned
    through the numbers, and made some test for oddness and sum. That’s it, not black
    magic... unlike the code that was generated for **–O3** .'
  prefs: []
  type: TYPE_NORMAL
- en: Using this flag, we were surprised to see how the **single instruction, multiple
    data** ( **SIMD** ) instructions were being pulled in by the compiler, in order
    to increase the speed, and the unexpected feature this compiler pulled in was
    that it calculated the sum of elements in an evolving 4-element array, starting
    with the values {1, 2, 3, 4} and incrementing each element by 4 over 25 iterations
    using SIMD instructions. The accumulated sum was stored in a SIMD register, and
    after the loop, it was reduced to a single integer, supplying the correct result.
  prefs: []
  type: TYPE_NORMAL
- en: The assembly code produced for this was simply too long (more than three pages),
    and we decided not to publish it here because it would have been useless, but
    as a fact of curiosity, we mentioned it.
  prefs: []
  type: TYPE_NORMAL
- en: The next compiler that we checked for how it deals with our simple C++ program
    is Clang. At this stage (meaning after the long SIMD instruction dump from GCC
    with **–O3** ), we did not expect anything spectacular, but we had a surprise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even at **–O1** , Clang greeted us with the following, may I say quite short
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: What a surprise! It seems that Clang did all the calculations behind the scenes,
    and just simply put the result in the compiled binary. More optimized than this
    it cannot be. We were really thrilled that compilers have matured and grown into
    these clever beasts, so this tantalized us to check whether other compilers can
    be clever like this, too.
  prefs: []
  type: TYPE_NORMAL
- en: GCC exposes the same behavior at **–O3** , but surprisingly, only if we want
    to summarize odd numbers up to 71. At 72, something breaks inside and churns out
    again the long list of SIMD assembly sources.
  prefs: []
  type: TYPE_NORMAL
- en: We could not convince MSVC under any circumstance, combinations of numbers and
    parameters to go the Clang way, and precalculate the number required to print
    out the sum of odd numbers, so we just concluded that it cannot. Maybe it will
    be implemented in the next version, what do you say Microsoft Visual C++ developers?
  prefs: []
  type: TYPE_NORMAL
- en: A glimpse into the future
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a phrase circulating amongst C++ developers that goes along the lines
    of *today’s compiler optimizations are the best we’ve ever managed to cobble together
    and a not-so-gentle reminder of just how much better they* *could be* .
  prefs: []
  type: TYPE_NORMAL
- en: Taking into consideration that this book was written in 2024 (hopefully, it
    will be published in 2025, and if all goes according to plan, in 2027, it will
    be obsolete, and we will get a commission to come up with a more up-to-date version
    of it), we have a pretty clear overview of what is happening in the world today.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you are reading this book while someone tries to grow potatoes on
    a different planet and the walls of your building are covered by graffitied monkeys,
    then you might have had some insights on how far the compilers have come in the
    last 10 years. Actually, it might even happen that Microsoft’s own (yes, we know,
    tiny, squishy) C++ compiler managed to grow up to the stage where it can calculate
    the sum of a few numbers before compilation and GCC is not throwing a tantrum
    at 72. Even for a short, simple program like the one we have.
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the future.
  prefs: []
  type: TYPE_NORMAL
- en: One instruction to rule them all
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dear reader. In our previous section of this chapter, unfortunately, we exhausted
    the only pompous introduction we could borrow from various cultural sources concerning
    technical interviews, career and life choices, and whether should we take the
    red pill or the blue one, so let’s focus our attention on more technical questions
    that our candidates might face at a technical interview (the word technical appears
    four times in this short introductory paragraph).
  prefs: []
  type: TYPE_NORMAL
- en: 'One of these questions, served to the author of these lines a few years ago,
    was to write a short code snippet that will count the number of 1 bits (the on
    bits) in a 32-bit integer. Let’s draft up a quick application to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here’s what happens. Firstly, we initialize a counter, starting with **0** .
    The next step is to loop through the bits. While **n** is non-zero, we add the
    least significant bit of **n** to the counter ( **n&1** gives us this value).
    Following this, we shift **n** right by one bit (discarding the least significant
    bit).
  prefs: []
  type: TYPE_NORMAL
- en: Once all bits are processed (when **n** becomes **0** ), return the total count
    of 1 bits. Not a very complicated process, just raw work.
  prefs: []
  type: TYPE_NORMAL
- en: It seems that this procedure of counting bits in numbers must be of a very peculiar
    interest in computing circles, such as for the purpose of error detection and
    correction, data compression, cryptography, algorithmic efficiency, digital signal
    processing, hardware design, and performance metrics, so no wonder it managed
    to creep itself into the STL (C++ STL, which is the standard template library)
    too in the form of **std::popcount** from C++ 20.
  prefs: []
  type: TYPE_NORMAL
- en: The interesting part of the story is that not only in the STL do we find this
    handy operation, but it was deemed so useful that it even exists at the level
    of the processors, under the infamous **POPCNT** mnemonic. Infamous it is, due
    to the fact that in 2024, it was effectively used in hindering the installation
    of Windows 11 on older machines that were not officially supported [3](B22235_08.xhtml#footnote-022)
    .
  prefs: []
  type: TYPE_NORMAL
- en: '[3](B22235_08.xhtml#footnote-022-backlink) [https://www.theregister.com/2024/04/23/windows_11_cpu_requirements/](https://www.theregister.com/2024/04/23/windows_11_cpu_requirements/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'But what that means for our candidate, who has to write code to impress the
    interviewers, is that they can simply replace the complicated code from before
    with the following very handy snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Not forgetting to include the **<bit>** header, after feeding the preceding
    program into [gcc.godbolt.org](http://gcc.godbolt.org) ’s compilers, we get a
    strange mishmash of results. The code compiled by GCC, regardless of the optimization
    level, always generates a variation of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: So, the code at some level disappears from our eyes into a strange call deep
    inside the libraries offered by GCC, called **__popcountdi2** [4](B22235_08.xhtml#footnote-021)
    . In order to convince GCC to fully utilize the power of the processor that we
    are running the code on, we need to utilize some of the not-so-well-known command-line
    options, such as **-march** (or **-mpopcnt** for this specific purpose).
  prefs: []
  type: TYPE_NORMAL
- en: '[4](B22235_08.xhtml#footnote-021-backlink) [https://gcc.gnu.org/onlinedocs/gccint/Integer-library-routines.html](https://gcc.gnu.org/onlinedocs/gccint/Integer-library-routines.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the official documentation, [5](B22235_08.xhtml#footnote-020)
    this command will select the appropriate processor instruction set in order to
    use the available extensions of the specific processor. Since, at this stage,
    we know that the **POPCNT** instruction was introduced in the early Core i5 and
    i7 processors, in the Nehalem family, we should simply specify the following to
    GCC: **-march=nehalem** . And now, not surprisingly, the compiler generates the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[5](B22235_08.xhtml#footnote-020-backlink) [https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html](https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Interestingly, if we provide the compiler with just the **-mpopcnt** flag,
    then it generates an extra **xor eax, eax** (meaning it nulls the EAX register)
    so maybe we have witnessed some processor-specific extra optimizations by choosing
    the Nehalem architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We cannot squeeze more than this out of GCC; there is simply no lower level
    for this functionality, so we focus our attention on the next compiler on our
    list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without explicitly asking to optimize the code, Clang also generates a generic
    call to a **std::popcount** function, found somewhere in its libraries; however,
    explicitly asking to optimize the generated code, Clang at various levels of optimization
    yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Surprising as it seems, there is a perfectly logical explanation for this code,
    found at the bit-twiddling site [6](B22235_08.xhtml#footnote-019) of Sean Eron
    Anderson at Stanford. Not considering this extra detour, Clang behaves identically
    to GCC when it comes to handling architecture and specifying the subset of CPU
    extensions to use while generating code.
  prefs: []
  type: TYPE_NORMAL
- en: '[6](B22235_08.xhtml#footnote-019-backlink) [https://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetParallel](https://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetParallel)'
  prefs: []
  type: TYPE_NORMAL
- en: The last of the big three, Microsoft’s own (we know, tiny, squishy) C++ compiler
    handles the situation very similarly to Clang. When asking to optimize the code
    while we specify an architecture that does not support the **POPCNT** instruction,
    it generates code like the one generated by Clang with low-level bit hacks, while
    if the architecture has support for the **POPCNT** instruction, it will adjust
    to the correct type and will call **POPCNT** for the proper parameters (/std:c++latest
    / arch:SSE4.2 /O1).
  prefs: []
  type: TYPE_NORMAL
- en: Good work, tiny, squishy compiler.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Myths related to C++ programming are shaped by the language’s evolving history
    through time, the differences and various levels of mastery between the users
    of the language, and psychological needs within the developer community. Early
    C++ compilers, which often generated less optimal code compared to modern compilers,
    contributed to myths about the language’s inefficiency and the necessity of manual
    optimization, such as rewriting entire routines using platform-specific assembly
    languages.
  prefs: []
  type: TYPE_NORMAL
- en: As compilers and language features have advanced, these myths persist, sometimes
    overshadowing modern best practices. This, combined with a culture of elitism
    and a sense of mastery among C++ programmers, reinforces outdated perceptions,
    even as C++ continues to be seen as a powerful and versatile language for serious,
    performance-critical applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapter, we will host a beauty pageant of programming languages,
    quickly eliminating all but our favorite, and the process will culminate in the
    crowning of the undisputed queen, C++. Admittedly, our admiration for this language
    is so profound that one might suspect the contest was rigged from the very start.
  prefs: []
  type: TYPE_NORMAL
