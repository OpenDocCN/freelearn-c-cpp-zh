- en: '20'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thread Safety and Concurrency with the STL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explores concurrency within the C++ **Standard Template Library**
    (**STL**). The chapter begins by building a solid foundational understanding of
    thread safety, race conditions, and their inherent risks. We then shift to the
    STL, decoding its thread safety guarantees and spotlighting its potential pitfalls.
    As we proceed, readers will gain insights into the array of synchronization tools
    available in C++, mastering their application to safeguard STL containers in multi-threaded
    environments. Upon concluding this chapter, readers can ensure data consistency
    and stability in concurrent C++ applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency versus thread safety
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding thread safety
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Race conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mutexes and locks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: STL containers and thread safety
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specific container concerns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency support within the STL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `std::thread`, `std::async`, `std::future`, and thread-local storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrent data structures in the STL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code in this chapter can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Data-Structures-and-Algorithms-with-the-CPP-STL](https://github.com/PacktPublishing/Data-Structures-and-Algorithms-with-the-CPP-STL)'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency versus thread safety
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Concurrency** is the concept of multiple tasks executing in overlapping periods.
    These tasks can either run at the same time on different processing units or might
    interleave on a single processing unit. The main goal of concurrency is to increase
    the system’s responsiveness and throughput. Concurrency is beneficial in various
    scenarios, such as when designing servers that handle multiple simultaneous client
    requests or in user interfaces that must remain responsive while processing tasks
    in the background.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In C++, concurrency can manifest in multiple forms: multi-threading, where
    separate threads of execution run (potentially) in parallel, or asynchronous programming,
    in which specific tasks are offloaded to be executed later.'
  prefs: []
  type: TYPE_NORMAL
- en: In C++, it’s crucial to understand that concurrency and **thread safety** are
    related but distinct concepts. Concurrency refers to the program’s ability to
    execute multiple sequences of operations simultaneously, which can be achieved
    through multi-threading or other parallel execution techniques. However, being
    concurrent does not inherently guarantee thread safety. Thread safety is the property
    that ensures code functions correctly when accessed by multiple threads concurrently.
    This involves carefully managing shared resources, synchronizing data access,
    and avoiding race conditions. Achieving thread safety in a concurrent environment
    is a challenging aspect of C++ programming. It requires deliberate design choices
    and the use of specific mechanisms, such as mutexes, locks, and atomic operations,
    to prevent data corruption and ensure consistent behavior across all threads.
  prefs: []
  type: TYPE_NORMAL
- en: Thread safety – a pillar for stable concurrency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Thread safety** refers to the capability of a piece of code to function correctly
    when accessed by multiple threads concurrently. It ensures that shared data maintain
    their integrity and that the results remain consistent. Thread safety doesn’t
    inherently mean a function or method is lock-free or lacks performance bottlenecks;
    instead, it signifies that concurrent access won’t lead to unpredictable results
    or compromise data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider an analogy: If **concurrency** were akin to a busy city intersection,
    then thread safety would be the traffic signals ensuring that cars (threads) don’t
    crash into each other.'
  prefs: []
  type: TYPE_NORMAL
- en: The interplay of concurrency and thread safety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While both concepts are intertwined, they serve different purposes. Concurrency
    focuses on designing systems to perform multiple tasks in overlapping time frames,
    aiming for improved performance and responsiveness. Thread safety, on the other
    hand, is all about correctness. It’s about ensuring they don’t step on each other’s
    toes when these concurrent tasks interact with shared resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider a simple example: a counter class in C++. Concurrency might
    involve incrementing the counter’s value from multiple threads. However, if the
    counter’s increment operation isn’t thread-safe, two threads might read the same
    value simultaneously, increment it, and then write back the same incremented value.
    In such a case, despite trying to be faster using concurrency, the counter would
    end up missing counts, leading to incorrect results.'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and rewards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introducing concurrency can undoubtedly make applications faster and more responsive.
    However, it also introduces complexities. Managing multiple threads with issues
    such as deadlocks and race conditions can be challenging.
  prefs: []
  type: TYPE_NORMAL
- en: But, when done right, the rewards are substantial. Programs become more efficient,
    potentially utilizing all available processing units fully. Applications can be
    more responsive, leading to improved user experiences. Concurrent programming
    is no longer a choice but is necessary for many modern high-performance applications.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency without thread safety – a recipe for chaos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine a world where every task tries to execute itself as fast as possible
    without coordination. In such a world, tasks might collide, disrupt each other,
    and produce nonsensical outcomes. That’s what concurrent programming without thread
    safety looks like. It’s a realm where speed is prioritized over correctness, often
    leading to chaos.
  prefs: []
  type: TYPE_NORMAL
- en: As a C++ developer, the key is to find the right balance. While striving for
    high concurrency to make applications fast, investing in thread safety mechanisms
    is equally crucial to ensure correctness.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the difference between concurrency and thread safety sets the
    stage for the following sections. We’ll be looking at the tools and constructs
    provided by the STL to achieve high concurrency and ensure thread safety.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding thread safety
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Executing multiple simultaneous tasks can lead to boosted performance and responsiveness.
    Ensuring thread safety, especially when using the STL, becomes paramount. If overlooked,
    the dream of seamless concurrency can quickly morph into the nightmare of data
    inconsistency and unpredictable behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Thread safety in STL containers – laying the groundwork
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The allure of the STL lies in its rich ensemble of containers, which offer a
    smooth experience for storing and managing data. But the moment we introduce multiple
    threads, potential dangers loom.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thread safety is primarily about ensuring that your code behaves predictably
    and correctly when accessed by multiple threads, even when those threads overlap.
    For STL containers, the basic guarantee is simple: simultaneous read-only access
    to containers is safe. However, once you introduce writes (modifications), things
    get intricate.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s critical to understand that while STL containers have thread-safe read
    operations, write operations don’t. If one thread is updating a container, no
    other thread should be reading or writing to it. Otherwise, we’re courting disaster
    or, in technical jargon, **undefined behavior**.
  prefs: []
  type: TYPE_NORMAL
- en: Grasping the thread-safe nature of STL algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If STL containers are the soul of the library, algorithms are undoubtedly its
    beating heart. They’re responsible for the STL’s rich functionality, from searching
    and sorting to transforming data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the catch: STL algorithms are functions, and their thread safety isn’t
    determined by the algorithm itself but by the data they operate on. If an algorithm
    operates on shared data across threads without adequate synchronization, you’re
    setting the stage for race conditions, even if that algorithm only reads data.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the scenario where you’re using `std::find` across multiple threads.
    While the algorithm is inherently safe for concurrent read operations, the results
    could be skewed if another thread modifies the data during the search.
  prefs: []
  type: TYPE_NORMAL
- en: Race conditions – the ghosts in the machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Race conditions keep concurrent programmers up at night. A **race condition**
    occurs when the behavior of your software depends on the relative timing of events,
    such as the order in which threads are scheduled. The consequences range from
    benign (slightly incorrect data) to catastrophic (complete data corruption or
    application crashes).
  prefs: []
  type: TYPE_NORMAL
- en: Using the STL in a multi-threaded environment without the proper precautions
    can introduce race conditions. For instance, imagine two threads simultaneously
    pushing elements onto a `std::vector`. Without synchronization, the internal memory
    of the vector could become corrupted, leading to a host of problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a simple race condition. In this example, we will use two threads
    to increment a counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a possible example output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A race condition occurs because both threads access and modify the shared variable
    counter simultaneously without any synchronization mechanism (such as **mutexes**
    or **locks**). Due to the lack of synchronization, the two threads may read, increment,
    and write back the value of the counter in an unpredictable order. This leads
    to the final value of the counter being unpredictable and usually less than the
    expected 200,000, as some increments are lost. Running this program multiple times
    will likely yield different results for the final value of the counter due to
    the race condition. To resolve this issue, proper synchronization mechanisms,
    such as mutexes, should be used to ensure that only one thread modifies the shared
    variable at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Safeguarding concurrency – the way forward
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s evident that merely understanding thread safety is half the battle. As
    we progress through this chapter, we’ll arm you with the tools and techniques
    to tackle race conditions head-on, master the synchronization mechanisms at your
    disposal, and ensure that your STL-powered multi-threaded applications stand as
    bastions of stability and consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Race conditions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A race condition in programming occurs when the behavior of a system depends
    on the relative timing of multiple threads or processes. In such scenarios, the
    system’s outcome becomes unpredictable because different threads may access and
    modify shared data concurrently without proper synchronization. This can lead
    to inconsistent or erroneous results, as the final state of the data depends on
    the order in which the threads execute, which cannot be determined in advance.
    Race conditions are a common issue in concurrent programming. They can be particularly
    challenging to detect and resolve, requiring careful design and synchronization
    mechanisms to ensure correct and predictable program behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Steering clear of a silent peril – race conditions in the STL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you journey into concurrent programming, race conditions represent one of
    the most subtle yet treacherous pitfalls. Though silent in their manifestation,
    they can cause unexpected and, at times, bewildering results. Recognizing and
    sidestepping these race conditions, especially within the realm of the STL, is
    crucial to crafting robust multi-threaded applications.
  prefs: []
  type: TYPE_NORMAL
- en: The anatomy of a race condition in the STL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At its core, a race condition materializes when the behavior of your application
    hinges on the sequence or timing of uncontrollable events. In the STL context,
    this typically arises when multiple threads access shared data in an uncoordinated
    fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a scenario where two threads, in an unfortunate coincidence, try to
    insert elements into the same position of `std::vector` concurrently or consider
    another instance where one thread reads from `std::unordered_map` while another
    erases an element. What is the outcome? Undefined behavior, which in the world
    of C++, is the equivalent of opening Pandora’s box.
  prefs: []
  type: TYPE_NORMAL
- en: More than meets the eye
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Race conditions are especially treacherous due to their unpredictable nature.
    While a concurrent application may seem to work flawlessly in one run, slight
    changes in thread execution timings can lead to entirely different results in
    the next.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond erratic behavior, race conditions with STL containers and algorithms
    can lead to more sinister problems. Data corruption, memory leaks, and crashes
    are just the tip of the iceberg. Given their elusive and intermittent appearance,
    these issues can be challenging to debug.
  prefs: []
  type: TYPE_NORMAL
- en: Anticipating race conditions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Forewarned is forearmed. By familiarizing yourself with common scenarios where
    race conditions manifest in the STL, you position yourself to tackle them preemptively:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::vector` and `std::string`, automatically resize when their capacity is
    exceeded. If two threads simultaneously trigger a resize, the internal state could
    be left in turmoil.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterator invalidation**: Modifying containers often invalidates existing
    iterators. If one thread traverses using an iterator while another modifies the
    container, the first thread’s iterator can end up in no-man’s-land.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithm assumptions**: STL algorithms make certain assumptions about the
    data they operate upon. Concurrent modifications can violate these assumptions,
    leading to incorrect results or infinite loops.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Safeguarding your code – a proactive stance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having acquainted ourselves with the potential hotspots, the natural progression
    is to fortify our code against these hazards. The essence lies in synchronization.
    We can effectively thwart race conditions by ensuring that only one thread can
    access shared data or perform certain operations simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: However, indiscriminate synchronization can lead to performance bottlenecks,
    rendering the benefits of concurrency moot. The key is to strike a balance, applying
    synchronization judiciously.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll introduce a robust arsenal of tools and techniques as we move further
    into this chapter. From mutexes to locks, you’ll acquire the means to detect and
    effectively neutralize race conditions, ensuring your STL-driven applications
    are swift and steadfast.
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready to conquer the challenges of concurrent programming with the STL?
    Let’s navigate this landscape together, ensuring your software remains consistent,
    reliable, and race condition-free.
  prefs: []
  type: TYPE_NORMAL
- en: Mutexes and locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **mutex**, short for **mutual exclusion**, is akin to a digital gatekeeper.
    It regulates access, ensuring that at any given moment, only a single thread can
    enter its protected domain, eliminating the chaos of concurrent access. Imagine
    a high-stakes auction room where only one person can place a bid at any instant,
    thereby preventing overlap and conflict. That’s the function of a mutex in the
    world of multi-threaded applications.
  prefs: []
  type: TYPE_NORMAL
- en: Within the C++ Standard Library, the header `<mutex>` bestows several types
    of mutexes upon us. The most commonly used among them is `std::mutex`. This basic
    mutex is a versatile tool suitable for many synchronization needs. A pair of operations—`lock()`
    and `unlock()`—provides a straightforward means to guard shared resources.
  prefs: []
  type: TYPE_NORMAL
- en: From manual to automatic – lock guards and unique locks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Manually locking and unlocking mutexes can be error-prone. There’s always the
    lurking danger of forgetting to unlock a mutex, leading to a deadlock. Enter lock
    guards and unique locks; these simplify mutex management by embracing the **resource
    acquisition is initialization** (**RAII**) principle.
  prefs: []
  type: TYPE_NORMAL
- en: '`std::lock_guard` is a lightweight wrapper that automatically manages the mutex’s
    state. Once a lock guard acquires a mutex, it guarantees its release when the
    lock guard’s scope ends. This eliminates the risk of forgetting to release the
    mutex.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, `std::unique_lock` is a bit more flexible. Besides the automatic
    lock management that `lock_guard` offers, `unique_lock` provides manual control,
    deferred locking, and even the ability to transfer ownership of a mutex. This
    makes it suitable for more complex synchronization scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding the stalemate – deadlock prevention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine a scenario where two threads are in a standoff, each expecting the other
    to relinquish a resource. As a result, both are stuck in a perpetual waiting,
    leading to a classic deadlock. This situation isn’t merely hypothetical, especially
    when mutexes are involved, as they can inadvertently create such a deadlock if
    not managed carefully. When multiple mutexes are involved, it is essential to
    adopt strategies to avoid deadlocks. One common approach is always to acquire
    the mutexes in the same order, regardless of which thread you are in. But when
    this isn’t feasible, `std::lock` comes to the rescue. It’s designed to lock multiple
    mutexes simultaneously without the risk of causing a deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating mutexes with STL containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the knowledge of mutexes, lock guards, unique locks, and deadlock prevention
    techniques, integrating these synchronization tools with STL containers becomes
    an intuitive exercise.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, protecting `std::vector` from concurrent access might involve
    placing `std::lock_guard` at every function that modifies or accesses the vector.
    Similarly, if multiple operations on `std::unordered_map` must be executed atomically,
    `std::unique_lock` can offer protection and the flexibility to manually control
    the lock’s state when needed.
  prefs: []
  type: TYPE_NORMAL
- en: With the tools of mutexes and locks in hand, threading in the STL no longer
    feels like treading on thin ice. By ensuring the reasonable and consistent application
    of these synchronization primitives, you can harness the full power of concurrency
    while keeping the pitfalls of race conditions and deadlocks at bay.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we’ll continue our exploration, specifically focusing
    on the unique challenges and considerations when threading with specific STL containers.
  prefs: []
  type: TYPE_NORMAL
- en: STL containers and thread safety
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When discussing STL containers, assuming a blanket level of thread safety across
    all of them is tempting. However, such assumptions can be misleading. By default,
    STL containers are not thread-safe for modifications, meaning if one thread modifies
    a container, other threads simultaneously accessing it might lead to undefined
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: However, some inherent guarantees exist. For instance, it is safe for multiple
    threads to simultaneously read from an STL container, as long as no thread is
    modifying it. This is often referred to as **read concurrency**. Yet, the moment
    even a single thread tries to change the container while others read, we’re back
    in the dangerous territory of race conditions.
  prefs: []
  type: TYPE_NORMAL
- en: When safety needs reinforcements – concurrent modifications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While reading concurrently is safe, modifications bring a different set of challenges.
    Suppose two or more threads attempt to modify an STL container simultaneously.
    In that case, the behavior becomes undefined unless synchronization mechanisms
    (such as those we explored with mutexes and locks) are used.
  prefs: []
  type: TYPE_NORMAL
- en: Take the case of `std::vector`. A race condition emerges if one thread appends
    an element using `push_back` while another tries to remove one with `pop_back`
    without a mutex guarding these operations. The vector’s size could change mid-operation,
    or memory could be reallocated, leading to crashes or data inconsistencies.
  prefs: []
  type: TYPE_NORMAL
- en: Container iterators – the fragile bridge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Iterators are fundamental to STL containers, providing a means to traverse and
    manipulate container elements. However, iterators are fragile when it comes to
    concurrency. If a thread modifies the container in a way that causes reallocation
    or restructuring, other threads’ iterators might become invalidated. Using invalidated
    iterators is, yet again, undefined behavior.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in containers such as `std::list` or `std::map`, adding an element
    won’t invalidate the existing iterators. However, with `std::vector`, a reallocation
    triggered when the vector exceeds its current capacity can invalidate all existing
    iterators. Being aware of these nuances is crucial when orchestrating multi-threaded
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Containers with a built-in shield – concurrent containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recognizing the challenges developers face when synchronizing standard STL
    containers, the library introduced concurrent containers. These containers, such
    as `std::atomic` and those in the `concurrency` namespace (for some compilers),
    come with built-in synchronization, offering thread-safe operations at the potential
    cost of performance.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that these containers might not provide the same interface
    or performance characteristics as their standard STL counterparts. They are specialized
    tools that are ideal for scenarios where the overhead of manual synchronization
    might be too significant.
  prefs: []
  type: TYPE_NORMAL
- en: While STL containers bring a world of convenience and efficiency to C++ programming,
    they come with the responsibility of understanding their threading characteristics.
    By discerning when and where explicit synchronization is required and leveraging
    the tools and techniques at our disposal, we can ensure that our multi-threaded
    applications remain robust, efficient, and free of concurrency-induced bugs.
  prefs: []
  type: TYPE_NORMAL
- en: Specific container concerns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Different STL container types present unique challenges and considerations in
    a multi-threaded environment. The thread safety of operations on these containers
    is not inherently guaranteed, making their use in concurrent scenarios a matter
    of careful planning. For instance, containers such as `std::vector` or `std::map`
    might behave unpredictably when simultaneously accessed or modified from multiple
    threads, leading to data corruption or race conditions. In contrast, containers
    such as `std::atomic` are designed for safe concurrent operations on individual
    elements, but they don’t safeguard the container’s structure as a whole. Therefore,
    understanding the specific threading implications of each STL container type is
    essential. Developers must implement appropriate locking mechanisms or use thread-safe
    variants where necessary to ensure data integrity and correct program behavior
    in a multi-threaded environment.
  prefs: []
  type: TYPE_NORMAL
- en: Behaviors of std::vector in multi-threading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`std::vector` is a widely-used STL container that acts as a dynamic array,
    adjusting its size as needed. Its contiguous memory allocation provides advantages
    such as cache locality. However, in multi-threaded scenarios, challenges arise.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, when a vector’s capacity is surpassed and reallocates memory, all
    associated iterators, pointers, and references can be invalidated. If one thread
    iterates the vector while another prompts a reallocation (adding elements beyond
    its limit), this can lead to issues. To prevent such scenarios, synchronization
    mechanisms should be implemented during operations that trigger reallocations
    when multiple threads access the vector.
  prefs: []
  type: TYPE_NORMAL
- en: Characteristics of std::list in concurrency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`std::list`, which is a doubly-linked list, has behaviors that are beneficial
    in multi-threaded situations but also require caution. A key advantage is that
    insertions or deletions do not invalidate iterators unless they target the specific
    removed element, making some operations naturally thread-safe.'
  prefs: []
  type: TYPE_NORMAL
- en: However, there’s a need for caution. While iterators may remain intact, concurrent
    modifications can alter the sequence of elements, resulting in inconsistent outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations with associative containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Containers such as `std::set`, `std::map`, `std::multiset`, and `std::multimap`
    order elements based on their keys. This ensures organized data retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: In multi-threaded situations, this trait presents challenges. Concurrent element
    insertions might result in an unpredictable final sequence. Additionally, concurrent
    removals can give rise to race conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency aspects of unordered containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The unordered versions of associative containers, such as `std::unordered_set`
    and `std::unordered_map`, do not keep elements in a defined order. However, they
    are not exempt from multi-threading issues. These containers leverage hashing,
    and element additions might trigger rehashing to optimize performance.
  prefs: []
  type: TYPE_NORMAL
- en: Rehashing can lead to iterator invalidation. Hence, despite their unordered
    nature, careful handling is necessary during concurrent operations.
  prefs: []
  type: TYPE_NORMAL
- en: Insights into container adaptors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The STL provides container adaptors such as `std::stack`, `std::queue`, and
    `std::priority_queue`. These don’t possess their storage and instead encapsulate
    other containers. Their thread safety properties depend on the containers they
    are based on. For example, an instance of `std::stack` that utilizes `std::vector`
    would have the same reallocation and iterator invalidation issues.
  prefs: []
  type: TYPE_NORMAL
- en: Being informed about the specific behaviors of each STL container is vital for
    developing thread-safe C++ programs. While the STL delivers numerous tools with
    distinct advantages, they also have challenges in multi-threaded contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency support within the STL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The STL has evolved significantly, transforming from a collection of data structures
    and algorithms into a comprehensive library incorporating advanced constructs
    for concurrent programming. This expansion responds to the increasing demand for
    efficient and robust multi-threaded applications, especially in the era of multi-core
    processors. Modern software development frequently requires leveraging the power
    of concurrency to enhance performance and responsiveness. As such, a deep understanding
    of the STL’s concurrency support is beneficial and essential for developers looking
    to optimize their applications in this multi-threaded landscape.
  prefs: []
  type: TYPE_NORMAL
- en: This section will examine the concurrency features integrated within the STL.
    This includes a detailed examination of thread management, asynchronous tasks,
    atomic operations, and challenges with utilizing concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: The STL’s offerings in the area of concurrency are not just about facilitating
    multi-threading but are also about doing it in an effective and manageable way.
    This section is designed to provide a comprehensive understanding of these tools,
    enabling you to write high-performance, scalable, and reliable C++ applications
    in today’s computationally demanding world.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to threads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the heart of concurrent programming lies the concept of threads. Within
    the STL, this is represented by `std::thread`. This class offers a straightforward
    interface for creating and overseeing threads. Initiating a new thread is essentially
    about defining a function or a callable entity and passing it to the thread constructor.
    After executing your task, you can join (await its conclusion) or detach (permit
    its independent execution) the thread. However, here’s a word of caution: manually
    handling threads requires careful attention. It’s imperative to ensure all threads
    are correctly joined or detached to avoid potential issues, including lingering
    threads.'
  prefs: []
  type: TYPE_NORMAL
- en: The advent of asynchronous tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Direct thread management provides considerable control, but the STL introduces
    `std::async` and `std::future` for tasks that don’t require such meticulous oversight.
    These constructs enable developers to delegate tasks for potential parallel execution
    without the intricacies of direct thread oversight. The function `std::async`
    initiates a task, and its resultant `std::future` offers a method to fetch the
    result when it’s ready. This fosters more organized code, mainly when the focus
    is on task-centric parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The STL provides a robust solution through atomic operations for inefficient,
    low-overhead operations, where the locking mechanisms may appear disproportionate.
    The atomic operations, encapsulated within the `std::atomic` class template, play
    a pivotal role in concurrent programming by guaranteeing the atomicity of operations
    in fundamental data types.
  prefs: []
  type: TYPE_NORMAL
- en: '`std::atomic` is designed to ensure that operations on basic types, such as
    integers and pointers, are executed as indivisible units. This atomicity is crucial
    in multi-threaded environments, as it prevents the potential hazards of interrupted
    operations, which can lead to inconsistent or corrupt data states. By ensuring
    that these operations are completed without interruption, `std::atomic` obviates
    the need for traditional locking mechanisms, such as mutexes, thereby enhancing
    performance by reducing the overhead associated with lock contention and context
    switching.'
  prefs: []
  type: TYPE_NORMAL
- en: However, it is essential to note that using atomic operations requires careful
    consideration and an understanding of their characteristics and limitations. While
    they provide a mechanism for lock-free programming, atomic operations are not
    a panacea for all concurrency problems. Developers must know the memory order
    constraints and the potential performance implications on different hardware architectures.
    In particular, the choice between memory orderings (such as `memory_order_relaxed`,
    `memory_order_acquire`, `memory_order_release`, etc.) demands a thorough understanding
    of the synchronization requirements and the trade-offs involved.
  prefs: []
  type: TYPE_NORMAL
- en: Memory orderings, such as `memory_order_relaxed`, `memory_order_acquire`, and
    `memory_order_release`, dictate how operations on atomic variables are ordered
    with respect to other memory operations.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the correct memory ordering is crucial for ensuring the desired level
    of synchronization while balancing performance. For instance, `memory_order_relaxed`
    offers minimal synchronization and imposes no ordering constraints on memory operations,
    leading to higher performance but at the risk of allowing other threads to see
    operations in a different order. On the other hand, `memory_order_acquire` and
    `memory_order_release` provide stronger guarantees about the ordering of reads
    and writes, which is essential for correctly implementing lock-free data structures
    and algorithms but can come with a performance cost, especially in systems with
    weak memory models.
  prefs: []
  type: TYPE_NORMAL
- en: The trade-offs involved in these decisions are significant. A more relaxed memory
    ordering can lead to performance gains but also introduce subtle bugs if the program’s
    correctness relies on certain memory ordering guarantees. Conversely, opting for
    stronger memory orderings can simplify the reasoning about the correctness of
    concurrent code but may lead to decreased performance due to additional memory
    synchronization barriers.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, developers must be aware of the synchronization requirements of their
    specific application and understand how their choice of memory ordering will interact
    with the underlying hardware architecture. This knowledge is critical for writing
    efficient and correct concurrent programs in C++.
  prefs: []
  type: TYPE_NORMAL
- en: Potential concurrent challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Concurrency, though powerful, isn’t devoid of challenges. Developers might confront
    deadlocks, race conditions, and resource contention. Deadlocks transpire when
    multiple threads indefinitely wait for each other to release resources. Race conditions
    can give rise to erratic bugs stemming from unforeseen overlaps in thread operations.
  prefs: []
  type: TYPE_NORMAL
- en: '**False sharing** is another notable challenge. It happens when different threads
    modify data situated in the same cache line. This can hamper performance because
    even if threads modify distinct data, their memory closeness can trigger redundant
    cache invalidations. Awareness and prudence can aid in sidestepping these challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the STL’s concurrency features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The STL provides a range of tools for concurrent programming, spanning from
    the initiation of threads to the assurance of atomic tasks. These tools cater
    to a variety of requirements. Nevertheless, it’s vital to employ them judiciously.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency promises enhanced performance and nimble applications but comes
    with complexities and potential bugs. In concurrency, knowing what tools are available
    is a necessary starting point, but effectively using them requires ongoing trial
    and learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following C++ code example illustrates the STL’s various concurrency features.
    This example encompasses thread creation, asynchronous task execution, and atomic
    operations while highlighting the importance of proper thread management and the
    potential pitfalls of concurrency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the example output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Created a thread using `std::thread` that sleeps for a given number of seconds
    and then prints a message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used `std::async` to perform a computation in a potentially parallel manner,
    and we used `std::future` to obtain the result once it was ready.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demonstrated using `std::atomic` to perform an atomic increment operation within
    multiple threads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensured that all threads are correctly joined to avoid dangling threads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This code is a simple demonstration and serves as a starting point for understanding
    concurrency in C++. Developers must further explore and handle more complex scenarios,
    including synchronization, preventing deadlocks, and avoiding race conditions
    and false sharing for robust concurrent applications.
  prefs: []
  type: TYPE_NORMAL
- en: Using std::thread, std::async, std::future, and thread -local storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s look at four core components of C++’s concurrency toolkit: `std::thread`,
    `std::async`, `std::future`, and thread-local storage. Each of these elements
    is vital for facilitating multi-threaded programming in C++. `std::thread` is
    the foundation, allowing for the creation and management of threads. `std::async`
    and `std::future` work in tandem to asynchronously execute tasks and retrieve
    their results in a controlled manner, offering a higher level of abstraction over
    raw threads. Thread-local storage, on the other hand, provides a unique data instance
    for each thread. This is crucial for avoiding data conflicts in a concurrent environment.
    This section aims to comprehensively understand these tools, demonstrating how
    they can be used effectively to write robust, efficient, and thread-safe C++ applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Initiating threads using std::thread
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A primary tool in the realm of concurrency within C++ is `std::thread`. This
    class allows developers to concurrently run procedures by starting distinct threads
    for execution. To launch a new thread, pass a callable entity (such as a function
    or a lambda) to the `std::thread` constructor. For instance, to print “Hello,
    Concurrent World!” from an independent thread, see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Utilizing the `join()` function ensures that the main thread waits until `my_thread`
    completes. There’s also `detach()`, which lets the primary thread progress without
    delay. However, the careful management of detached threads is crucial to avoid
    unexpected behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Managing asynchronous operations with std::async and std::future
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Though `std::thread` offers significant capabilities, direct thread management
    can be intricate. The STL presents an elevated abstraction for administering potential
    parallel operations through `std::async` and `std::future`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach is clear-cut: assign a task to `std::async` and retrieve a `std::future`
    object that will eventually contain that task’s result. This division allows the
    primary thread to either continue or optionally await the outcome using the `get()`
    method of `std::future`, as shown in the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `std::async` and `std::future` are designed to work well together
    to help manage asynchronous operations.
  prefs: []
  type: TYPE_NORMAL
- en: Preserving data consistency using thread-local storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensuring distinct data storage for each thread to avoid overlap and maintain
    data consistency in concurrent programming can be challenging. This is addressed
    by **thread-local** **storage** (**TLS**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `thread_local` keyword when declaring a variable ensures a unique
    instance of that variable for each thread. This is instrumental in sustaining
    data consistency and circumventing the issues associated with shared data access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, `thread_counter` is instantiated for each thread, shielding it from inter-thread
    interference.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating tools for proficient concurrency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With `std::thread`, `std::async`, `std::future`, and TLS, you are prepared to
    navigate various concurrent programming situations in C++. The STL offers the
    requisite tools for delegating tasks for parallel execution or adeptly managing
    thread-specific data.
  prefs: []
  type: TYPE_NORMAL
- en: It’s pivotal to note that while initiating threads or tasks is straightforward,
    ensuring synchronized operations devoid of contention, deadlocks, or data races
    demands attentiveness and continual refinement.
  prefs: []
  type: TYPE_NORMAL
- en: Retaining the foundational insights from this segment is paramount as we transition
    to the subsequent sections that review the STL’s concurrent data structures. Concurrent
    programming is an evolving landscape, and mastering each tool and concept augments
    your capacity to develop efficient and stable concurrent applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk through a code example that illustrates the use of `std::thread`,
    `std::async`, `std::future`, and TLS to concurrently execute tasks and manage
    per-thread data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the example output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Created a thread to print a message to the console using `std::thread`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used `std::async` to perform an asynchronous operation that returns a string.
    The result is accessed via a `std::future` object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demonstrated the use of TLS with the `thread_local` keyword to maintain a separate
    counter for each thread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Started multiple threads, each incrementing its local counter, to show how TLS
    variables are instantiated for each thread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This example encapsulates the essentials of concurrent programming with the
    STL, from thread creation and synchronization to data isolation with TLS. While
    these mechanisms simplify parallel execution, we must exercise careful judgment
    to prevent concurrency-related issues, such as deadlocks and race conditions.
    The upcoming sections will explore STL’s concurrent data structures, which build
    upon these foundational concepts to enable the creation of robust concurrent programs.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent data structures in the STL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The STL provides a variety of data structures, but not all are inherently suited
    for concurrent access. Understanding how to effectively utilize and adapt these
    data structures for safe and efficient use in a multi-threaded context is crucial.
    We will examine the thread safety aspects of common STL data structures, discuss
    the appropriate use cases for each in a concurrent environment, and explore the
    strategies to ensure safe and effective concurrent access. This section is designed
    to equip developers with the knowledge to leverage STL data structures to maximize
    performance while maintaining data integrity in a multi-threaded landscape.
  prefs: []
  type: TYPE_NORMAL
- en: The STL’s concurrency-optimized containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the STL provides many containers, not all are optimized for concurrent
    access. However, with the increasing demand for concurrent programming, specific
    concurrency-friendly containers have made their way into the repertoire of many
    C++ programmers.
  prefs: []
  type: TYPE_NORMAL
- en: One notable example is `std::shared_timed_mutex` and its sibling `std::shared_mutex`
    (from C++17 onwards). These synchronization primitives allow multiple threads
    to read shared data simultaneously while ensuring exclusive access for writing.
    This is particularly handy when read operations are more frequent than writes,
    such as in caching scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a situation where you have `std::map` storing configuration data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To read from this map, multiple threads can acquire a shared lock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'However, for writing, a unique lock ensures exclusive access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: While not a container, `std::shared_timed_mutex` can protect any STL container,
    ensuring concurrent read access while serializing writes.
  prefs: []
  type: TYPE_NORMAL
- en: Striving for maximum efficiency in concurrent environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Concurrency isn’t just about making operations thread-safe but is also about
    achieving better performance. As you’ve seen, atomic types and concurrency-optimized
    containers help ensure safety, but there’s more to it than that. Fine-tuning performance
    may involve considering lock contention, avoiding false sharing, and minimizing
    synchronization overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few tips for maximizing efficiency include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Limit the scope of locks**: While locks are essential for ensuring data consistency,
    holding them for extended durations can impede performance. Ensure you’re only
    holding locks for the necessary duration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choose the right data structure**: Containers optimized for concurrency might
    offer better performance for multi-threaded applications, even if they might be
    slower in single-threaded scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consider granularity**: Think about the granularity of your locks. Sometimes,
    a finer-grained lock (protecting just a part of your data) can perform better
    than a coarser-grained one (protecting the entire data structure).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices in action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at a code example demonstrating best practices in using STL containers
    in a concurrent environment, focusing on performance optimization techniques such
    as minimizing lock scope, selecting appropriate data structures, and considering
    lock granularity.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will write a concurrency-optimized container, specifically `ConcurrentVector`,
    designed to handle multi-threaded environments effectively. This custom container
    class, which is templated to hold elements of any type (`T`), encapsulates a standard
    `std::vector` for data storage while employing `std::shared_mutex` to manage concurrent
    access (we will break this example up into a few sections. For the complete code,
    please refer to the book''s GitHub repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will write the function `performConcurrentOperations`, which will
    demonstrate the practical application of our `ConcurrentVector` class in a multi-threaded
    context. This function accepts a reference to `ConcurrentVector<int>` and initiates
    two parallel operations using C++ standard threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we write `main()` to drive the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the example output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In total, in the preceding code example, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We have defined a `ConcurrentVector` template class that mimics a concurrency-optimized
    container, which internally uses `std::shared_mutex` to enable fine-grained control
    over read and write operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `insert` method uses a unique lock to ensure exclusive access during write
    operations, but the lock is held only for the insert duration, minimizing the
    lock scope.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `find` and `size` methods use shared locks, allowing for concurrent reads,
    demonstrating the use of shared locking to enable higher read throughput.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A writer thread and a reader thread were created to perform concurrent insertions
    and searches on the `ConcurrentVector` instance, showcasing the container’s ability
    to handle concurrent operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This example illustrates critical considerations for optimizing concurrent performance,
    such as limiting the duration of locks, choosing appropriate concurrency-friendly
    data structures, and using fine-grained locking to protect smaller sections of
    the data. These practices are crucial for intermediate-level C++ developers looking
    to enhance the performance of multi-threaded applications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter discussed the intricacies of thread safety and concurrency within
    the STL. We started by distinguishing between concurrency and thread safety, underscoring
    that while related, each serves a distinct purpose. Our journey began with a foundational
    understanding of thread safety as a pillar for stable concurrency and how the
    lack thereof can lead to unpredictable software behavior. We examined the interplay
    between these concepts, addressing the challenges and highlighting the rewards
    of concurrent programming when thread safety is maintained.
  prefs: []
  type: TYPE_NORMAL
- en: We looked into the thread-safe nature of STL containers and algorithms, dissecting
    race conditions and the techniques to anticipate and guard against them. The chapter
    provided detailed insights into the behaviors of various STL containers under
    multi-threaded scenarios, from `std::vector` to `std::list` and associative to
    unordered containers. We also uncovered the concurrency aspects of container adaptors,
    asserting that knowledge is power when writing concurrent applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve been equipped with the core tools: `std::thread`, `std::async`, `std::future`,
    and TLS. With these, we initiated threads, managed asynchronous operations, and
    preserved data consistency across threads. These capabilities have prepared us
    for proficient concurrency about safety and performance.'
  prefs: []
  type: TYPE_NORMAL
- en: The chapter examined the STL’s atomic types and concurrency-optimized containers,
    providing tips for maximizing efficiency in concurrent environments. These insights
    are pivotal for developing high-performance, thread-safe applications using the
    STL.
  prefs: []
  type: TYPE_NORMAL
- en: The knowledge imparted in this chapter is essential because thread safety and
    efficient concurrency are critical for modern C++ developers. As multi-core and
    multi-threaded applications become the norm, it is crucial to understand these
    principles to be able to leverage the full power of the STL.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dig further into advanced STL usage. We will introduce
    concepts and robust template features, allowing for more precise type checks at
    compile-time. We will learn how to refine the constraints in STL algorithms and
    effectively use these constraints to enhance data structures with explicit requirements.
    Moreover, we will explore the integration of the STL with coroutines, assessing
    the potential synergies with ranges and views and preparing for the paradigm shift
    that awaits in contemporary C++ programming.
  prefs: []
  type: TYPE_NORMAL
