<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-64"><a id="_idTextAnchor079"/>3</h1>
<h1 id="_idParaDest-65"><a id="_idTextAnchor080"/>Architectural Patterns</h1>
<p>Starting an embedded project from scratch means progressively stepping towards the final solution by going through all the research and development phases and considering the synergy among all the parts involved.</p>
<p>Software development needs to evolve accordingly throughout these phases. In order to get the best results without excessive overhead, there are a few best practices to follow and tools to discover.</p>
<p>This chapter describes a possible approach toward configuration-management tools and design patterns, based on real-life experiences. Describing this approach may help you to understand the dynamics of working in a team focused on producing an embedded device or solution.</p>
<p>We will discuss the following topics in this chapter:</p>
<ul>
<li>Configuration management</li>
<li>Source code organization</li>
<li>The life cycle of an embedded project</li>
<li>Security considerations</li>
</ul>
<p>By the end of the chapter, you will gain an overview of the architectural patterns useful for designing the system based on the specifications and the platform limits.</p>
<h1 id="_idParaDest-66"><a id="_idTextAnchor081"/>Configuration management</h1>
<p>When working as a team, coordination and synchronization can be optimized to improve efficiency. Tracking and <a id="_idIndexMarker180"/>controlling the development life cycle smoothens the development flow, cutting downtime and costs.</p>
<p>The most important tools known to help manage the software life cycle are the following:</p>
<ul>
<li>Revision control</li>
<li>Issue tracking</li>
<li>Code reviews</li>
<li>Continuous integration</li>
</ul>
<p>Different options exist for the four categories. The source code is synchronized among developers through <a id="_idIndexMarker181"/>a revision control system. <strong class="bold">Issue tracking systems</strong> (<strong class="bold">ITSs</strong>) usually consist of web <a id="_idIndexMarker182"/>platforms that keep track of the activities and known bugs of the system. Code reviews can be encouraged with specific web-based tools and enforced through rules on the revision control systems.</p>
<p>Continuous integration tools ensure that build and test execution tasks are scheduled to automatically execute, periodically or upon changes in the code, collecting test results and notifying the developers about regression<a id="_idTextAnchor082"/>s.</p>
<h2 id="_idParaDest-67"><a id="_idTextAnchor083"/>Revision control</h2>
<p>No matter whether <a id="_idIndexMarker183"/>you are working alone or in a large development team, properly keeping track of the development progress is extremely important. <strong class="bold">Revision control</strong> tools allow developers to roll back failed experiments at any time with the press of a button, and visiting its history gives a clear view of how the project is evolving at any time.</p>
<p>A <strong class="bold">revision control system</strong>, also <a id="_idIndexMarker184"/>known as a <strong class="bold">version control system</strong>, or <strong class="bold">VCS</strong>, encourages <a id="_idIndexMarker185"/>cooperation by making merge operations easier. The most updated official version is referred to as a <strong class="bold">trunk</strong>, <strong class="bold">master</strong>, or <strong class="bold">main</strong> branch, depending on the VCS in use. VCSs offer, among other things, fine-grained access control and authorship attribution down to a single commit.</p>
<p>One of the most modern and widely used open source VCSs is Git. Originally created as the VCS for the Linux kernel, Git offers a range of features but, most importantly, provides a flexible mechanism to allow switching among different versions and feature branches quickly and reliably and facilitates the integration of conflicting modifications in the code.</p>
<p class="callout-heading">Note</p>
<p class="callout">Git terminology is used in this <a id="_idIndexMarker186"/>book when describing specific activities related to the VCS.</p>
<p>A <strong class="bold">commit</strong> is a VCS action that <a id="_idIndexMarker187"/>results in a new version of the repository. The repository keeps track of the sequence of commits and the changes introduced in each version in a hierarchical structure:</p>
<ul>
<li><strong class="bold">Branch</strong>: A linear sequence of commits is a branch.</li>
<li><strong class="bold">HEAD</strong>: The latest version in a branch is called HEAD.</li>
<li><strong class="bold">master</strong>: Git refers to the main development branch as a master. The master branch is the main focus of the development. Bug fixes and minor changes may be committed directly to the master.</li>
<li><strong class="bold">Feature branches</strong>: These are created for self-contained tasks in progressing and ongoing experiments, which will eventually be merged into the master. When not abused, feature <a id="_idIndexMarker188"/>branches are a perfect fit when working in a smaller sub-team on a task and can simplify the code review process, allowing developers to work simultaneously on separate branches, and concentrate the validation of completed tasks as single <strong class="bold">merge</strong> requests.</li>
</ul>
<p>A <strong class="bold">merge operation</strong> consists of joining together two versions from two different branches that may have diverged <a id="_idIndexMarker189"/>and present conflict in the code throughout the development. Some merges are trivial and are automatically resolved by the VCS, while others may require manual fixing.</p>
<p>Using meaningful and verbose commit messages improves the readability of the history of the repository and can <a id="_idIndexMarker190"/>help to track regressions later on. <strong class="bold">Tags</strong> can be used to track intermediate versions that are released and di<a id="_idTextAnchor084"/>stributed.</p>
<h2 id="_idParaDest-68"><a id="_idTextAnchor085"/>Tracking activities</h2>
<p>Keeping track of <a id="_idIndexMarker191"/>activities and tasks can be simplified by using ITS. Some tools can be directly linked to the revision control system so that tasks can be linked to specific commits in the repository and vice versa. This is, in general, a good idea, as it is possible to have a good overview of what has been changed to accomplish a specific task.</p>
<p>At first, tasking out the specifications into short activities facilitates the approach to development. Ideally, tasks are as small as possible and may be grouped by category. Later on, priorities can be set based on intermediate goals and taking into account the availability of the final hardware. Tasks created should be grouped into intermediate milestones, which some tools refer to as blueprints, so that the overall progress towards an intermediate deliverable can be measured based on the progress made on single tasks.</p>
<p>ITS can be used to <a id="_idIndexMarker192"/>track actual issues in the project. A <strong class="bold">bug report</strong> should be extensive enough for other developers to understand the symptoms and <a id="_idIndexMarker193"/>reproduce the behavior that proves that there is a defect in the code. Ideally, final users and early adopters should be able to add new issues to the tracking system, so the tracking system can be used to track all communication with the development team. Community-based open source projects should provide a publicly accessible ITS interface to users.</p>
<p>Bug-fixing activities generally get a higher priority than development tasks, except in a few cases, for instance, when the bug is the effect of a temporary approximation done by an intermediate <a id="_idIndexMarker194"/>prototype, which is expected to be fixed in the next iteration. When a bug affects the behavior of the system, which was proven to work beforehand, it must be marked as a regression. This is important because regressions can usually be handled differently than normal bugs, as it is possible to track them down to a single commit using revision control tools.</p>
<p><strong class="bold">Repository control</strong> platforms provide several tools, including source code history browsing and the issue-tracking <a id="_idIndexMarker195"/>features described earlier. GitLab is a free and open source implementation of such repository control platforms, which can be installed to run as a self-hosted solution. Community projects are often hosted on social coding platforms, such as GitHub, which aim to facilitate contributions to open source and free sof<a id="_idTextAnchor086"/>tware projects.</p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor087"/>Code reviews</h2>
<p>Often integrated <a id="_idIndexMarker196"/>into ITS tools, <strong class="bold">code reviews</strong> facilitate team cooperation by encouraging the critical analysis of the changes proposed to the code base, which can be useful to detect potential issues before the proposed changes make it to the master branch. Depending on the project requirements, code reviews may be recommended, or even enforced by the team, to increase the quality of the code and early detection of defects by human inspection.</p>
<p>When properly integrated with the VCS, it is possible to set a threshold of mandatory positive reviews from other members of the team before the commit is considered for merging. It is possible to mandate the review of every single commit in the master branch, using tools such as <strong class="bold">Gerrit</strong>, integrated with the VCS. Depending on the size of the contribution, this <a id="_idIndexMarker197"/>mechanism can introduce some unnecessary overhead, so, in most cases, it may be more appropriate to group the changes introduced by a branch altogether to facilitate the <a id="_idIndexMarker198"/>review when the branch is proposed for merging into the master. Mechanisms based on merge requests give the reviewers an overview of the changes introduced during the entire development of the modification proposed. In the case of open source projects that accept external contributions, code reviews are a necessary step to validate the changes coming from less trusted contributors, or in general, from outside the team of maintainers. Code reviews are the most powerful tool to prevent malicious code that may be disguised and could not be detected by automatic test and code a<a id="_idTextAnchor088"/>nalysis utilities.</p>
<h2 id="_idParaDest-70"><a id="_idTextAnchor089"/>Continuous integration</h2>
<p>As previously mentioned, the test-driven approach is crucial in an embedded environment. Automating tests is the <a id="_idIndexMarker199"/>best way to promptly detect regressions, and defects in general, while the development is ongoing. Using an <a id="_idIndexMarker200"/>automation server, such as <strong class="bold">Jenkins</strong>, it is possible to plan several actions, or <em class="italic">jobs</em>, to run responsively (such as at every commit), periodically (such as every Tuesday at 1 a.m.), or manually, upon user requests. Here are a few examples of jobs that can be automated to improve the efficiency of an embedded project:</p>
<ul>
<li>Unit tests on the development machine</li>
<li>System validation tests</li>
<li>Functional tests on a simulated environment</li>
<li>Functional tests on a physical target platform</li>
<li>Stability tests</li>
<li>Static code analysis</li>
<li>Generating documentation</li>
<li>Tagging, versioning, and packaging</li>
</ul>
<p>The desired level of quality must be decided during design, and test cases must be coded accordingly. Unit test code coverage can be measured using <code>gcov</code> upon each test execution. Some projects intended for life-critical applications may require a very high percentage of coverage for <a id="_idIndexMarker201"/>unit tests, but writing a complete set of tests for a complex system has a great impact on the total programming effort and may increase the cost of the development significantly, so researching the right balance between efficiency and quality is advisable in most cases.</p>
<p>A different approach has to be taken with functional tests. All the functionalities implemented on the target should be tested, and tests prepared in advance should be used to define performance indicators and acceptance thresholds. Functional tests should be run in an <a id="_idIndexMarker202"/>environment that is as close as possible to the real use case scenario in all those cases where it is impossible to recreate the full use case on the target system<a id="_idTextAnchor090"/> and its surroundings.</p>
<h1 id="_idParaDest-71"><a id="_idTextAnchor091"/>Source code organization</h1>
<p>The code base should contain all the source code, third-party libraries, data, scripts, and automation needed to <a id="_idIndexMarker203"/>build the final image. It is a good idea to keep self-contained libraries in separate directories so that they can be easily updated to newer versions by replacing the subdirectory. Makefiles and other scripts can be placed in the project’s root directory.</p>
<p>Application code should be short and synthetic and access the modules abstracting the macro functionalities. Functional modules should describe a process while hiding the details of the underlying implementation, such as reading data from a sensor after it has been properly sampled and processed. Aiming for small, self-contained, and adequately abstracted modules also makes the components of the architecture easier to test. Keeping the majority of the logic for the application components separated from their hardware-specific implementation improves portability across different platforms and allows us to change the peripherals and the interfaces used on the target even during the development <a id="_idIndexMarker204"/>phase. Abstracting too much, though, impacts costs, in terms of development effort and resources needed, so the right balan<a id="_idTextAnchor092"/>ce should be researched.</p>
<h2 id="_idParaDest-72"><a id="_idTextAnchor093"/>Hardware abstraction</h2>
<p>General-purpose prototyping platforms are built and distributed by silicon manufacturers to <a id="_idIndexMarker205"/>evaluate microcontrollers and peripherals, so part of the software development may often be performed on these devices even before the design of the final product begins.</p>
<p>The software that can be run on the evaluation board is usually distributed as a reference implementation, in the form of source code or proprietary precompiled libraries. These libraries can be configured and adapted for the final target, to be used as reference hardware abstraction from the beginning, and their settings updated to match changes in the hardware configuration.</p>
<p>On our reference target, support for the hardware components of a generic Cortex-M microcontroller is <a id="_idIndexMarker206"/>provided in the form of a library called <strong class="bold">Cortex Microcontroller Software Interface Standard</strong> (<strong class="bold">CMSIS</strong>), distributed by ARM as a reference implementation. Silicon manufacturers derive their specific hardware abstractions by extending CMSIS. An application linked to a target-specific hardware abstraction can access peripherals through its specific API calls and core MCU functionalities through CMSIS.</p>
<p>For code to be portable across different MCUs in the same family, drivers may require an additional level of abstraction on top of the vendor-specific API calls. If the HAL implements multiple targets, it can provide the same API to access generic features across multiple platforms, hiding the hardware-specific implementation under the hood.</p>
<p>The goal of CMSIS and <a id="_idIndexMarker207"/>other free software alternatives, such as <strong class="bold">libopencm3</strong> and <strong class="bold">unicore-mx</strong>, is to group all the generic Cortex-M abstractions and the vendor-specific <a id="_idIndexMarker208"/>code for the most common Cortex-M silicon manufacturers while masking the difference among platform-specific calls when controlling the system and the peripherals.</p>
<p>Regardless of the hardware abstraction, some of the code required at the earliest stage of the boot is very specific to each target the software is intended to run on. Each platform has its own specific address space segmentation, interrupt vector, and configuration register displacement. This means that while working on code that is supposed to be portable among different platforms, makefiles and scripts automating the build must be configurable to link using the correct startup code and linker configurations.</p>
<p>The examples contained in this book do not depend on any specific hardware abstraction, as they <a id="_idIndexMarker209"/>aim to introduce the control of the system components by directly interacting with the system registers and implementing platform-specific device drivers while focusing on the interaction<a id="_idTextAnchor094"/> with the hardware component.</p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor095"/>Middleware</h2>
<p>Some of the features may already have a well-known solution that has been previously implemented by <a id="_idIndexMarker210"/>a single developer, a community, or an enterprise. Solutions may be generic, perhaps designed for a different platform, or even come from outside the embedded world.</p>
<p>In any case, it is always worth looking for libraries for any data transformation, protocol implementation, or subsystem model that might already have been coded and is waiting to be integrated into our project.</p>
<p>Several open source libraries and software components are ready to be included in embedded projects, allowing us to implement a broader set of functionalities. Integrating components from open source projects is particularly useful for delivering standard functionalities. There is a vast choice of well-established open source implementations designed for embedded devices that can be easily integrated into embedded projects, including the following examples:</p>
<ul>
<li>Real-time operating systems</li>
<li>Cryptography libraries</li>
<li>TCP/IP, 6LoWPAN, <a id="_idIndexMarker211"/>and other network protocols</li>
<li><strong class="bold">Transport Layer Security</strong> (<strong class="bold">TLS</strong>) libraries</li>
<li>Filesystems</li>
<li>IoT message queue protocols</li>
<li>Parsers</li>
</ul>
<p>Some components from these categories are described in more detail later in this book.</p>
<p>Basing the software upon an operating system allows us to manage memory areas and thread execution. In this case, threads execute independently from each other, and it is even possible to implement memory separation among threads and between running threads and the kernel. This approach is advisable when the complexity of the design increases or when there are well-known blocking points in the modules that cannot be redesigned. Other libraries usually require multithreading support if an operating system is used, which can be enabled at compile time.</p>
<p>The decision of integrating <a id="_idIndexMarker212"/>third-party libraries must be evaluated by measuring the resources needed, in terms of code size and memory used, to perform specific tasks on the target platform. As the whole firmware is distributed as a single executable file, all the licenses of the components must be compatible, and the integration must not violate the license terms <a id="_idTextAnchor096"/>of any of its single components.</p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor097"/>Application code</h2>
<p>The role of the application code is to coordinate, from the highest layer in the project design, all the <a id="_idIndexMarker213"/>modules involved, and orchestrate the heuristics of the system. A clean main module that is well-designed allows us to keep a clear view of all the macroscopic blocks of the system, how they are related to each other, and the execution timing of the various components.</p>
<p>Bare-metal applications are built around a main endless loop function, which is in charge of distributing the CPU time among the entry points of the underlying libraries and drivers. The execution happens sequentially, so the code cannot be suspended except by interrupt handlers. For this reason, all the functions and library calls invoked from the main loop are supposed to return as fast as possible because stall points hidden inside other modules may compromise the system’s reactivity, or even block them forever, with the risk of never returning to the main loop. Ideally, in a bare-metal system, every component is designed to interact with the main loop using the event-driven paradigm, with the main loop constantly waiting for events and mechanisms to register callbacks to wake up the application on specific events.</p>
<p>The advantage of the bare-metal, single-thread approach is that synchronization among threads is not needed, all the memory is accessible by any function in the code, and it is not necessary to implement complex mechanisms, such as context and execution model switches. Some basic synchronization mechanisms, however, could still be required when interrupts are on and the flow of execution can be interrupted by external events at any point to execute a specific handler.</p>
<p>If multiple tasks are meant to run on top of an operating system, each task should be confined as much as possible within its own module and explicitly export its start function and public variables as global symbols. In this case, tasks can sleep and call blocking functions, which should implement the OS-specific blocking mechanisms.</p>
<p>Thanks to the flexibility of the Cortex-M CPU, there are different degrees of threads and process separation that can be activated on the system.</p>
<p>The CPU offers multiple tools to facilitate the development of multithreading systems with separation among tasks, multiple execution modes, kernel-specific registers, privilege separation, and memory-segmentation techniques. These options allow architects to define complex systems, more oriented to general-purpose applications, which offer privilege <a id="_idIndexMarker214"/>separation and memory segmentation among processes, but also smaller, simpler, more straightforward systems, which do not need these as they are generally designed for a single purpose.</p>
<p>Selecting an executing model that is based on non-privileged threads results in a much more complex implementation of the context changes in the system, and may impact the latency of the real-time operations, which is why bare-metal, single-threaded solutions are still preferred for most real-time applications.</p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor098"/>Security considerations</h1>
<p>One of the most important aspects to consider when designing a new system is security. Depending on the characteristics of the system, the requirements, and the evaluation of the risks, different countermeasures may be appropriate. Security-enhancing features are often a mix of hardware and software efforts to provide specific protections against known attacks.</p>
<h2 id="_idParaDest-76"><a id="_idTextAnchor099"/>Vulnerability management</h2>
<p>Software components <a id="_idIndexMarker215"/>keep evolving as new features are introduced, and defects are fixed along the way. Some of the defects that are discovered and fixed in a later version may impact the security of the system running outdated software if proper action is not taken promptly. Once vulnerabilities in third-party components are fully disclosed to the public, it is no longer a good choice to keep running outdated code.</p>
<p>Older versions with known defects running on public networks have an increased possibility of becoming the attack surface for attempts to damage the system, take control of the software execution, or steal important data. The best response is prepared very early during the design of the system and consists of planning remote updates using procedures that fit the specific use case, security requirements, and safety levels.</p>
<p>When using third-party <a id="_idIndexMarker216"/>libraries, it is appropriate to follow the development of their latest versions and fully understand the impact of the defects that are fixed, especially when those are marked as security issues.</p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor100"/>Software cryptography</h2>
<p><strong class="bold">Cryptography</strong> algorithms should be used when appropriate, for example, to encrypt data stored locally or in <a id="_idIndexMarker217"/>transit between two <a id="_idIndexMarker218"/>systems, authenticate a remote actor on the network, or verify that data has not been altered and comes from a trusted source.</p>
<p>Good cryptography is always based on open, transparent standards so that the security of the system depends solely on the security of the keys, according to the Kerckhoff principle formulated by the Dutch cryptographer Auguste Kerckhoff in the 19th century, rather than on secret mechanisms, with the (false) hope that its implementation will never be disclosed or reverse engineered. Although this last statement should be obvious to whoever has some confidence with the concept of information security, in the past many embedded systems have adopted <em class="italic">security by obscurity</em>, as a bad practice of taking shortcuts in the attempt to circumvent the restrictions posed by the lack of appropriate resources to run well-established cryptography primitives on older hardware architectures.</p>
<p>Nowadays, embedded cryptography libraries exist, capable of running the same latest standard algorithms used in PCs and servers in microcontroller-based systems, which meanwhile are becoming more powerful and fit for running the (often CPU-hungry) cryptography math primitives. A complete cryptography library offers a ready-to-use implementation of, typically, three families of algorithms:</p>
<ul>
<li><strong class="bold">Asymmetric cryptography</strong> (RSA, ECC) is based on a pair of keys, private and public, associated <a id="_idIndexMarker219"/>with each other. Besides one-way encryption, these algorithms provide other mechanisms, such as authenticating a signature and deriving secondary keys starting from two key pairs, for example, to use as a shared secret by both endpoints communicating over an untrusted medium.</li>
<li><strong class="bold">Symmetric cryptography</strong> (AES, ChaCha20) is mostly adopted for bidirectional encryption <a id="_idIndexMarker220"/>using the same pre-shared secret key in both directions.</li>
<li><strong class="bold">Hash algorithms</strong> (SHA) provide an <a id="_idIndexMarker221"/>injective digest calculation and are often used to verify that data has not been altered.</li>
</ul>
<p>A complete set of algorithms, optimized for embedded systems, is provided by wolfCrypt, the <a id="_idIndexMarker222"/>cryptography engine distributed as part of wolfSSL, a professionally maintained open source library that also includes transport layer security protocols, which will be further explained in <a href="B18730_09.xhtml#_idTextAnchor311"><em class="italic">Chapter 9</em></a>, <em class="italic">Distributed Systems and </em><em class="italic">IoT Architecture</em>.</p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor101"/>Hardware cryptography</h2>
<p>Taking security aspects under consideration from the very beginning of the design process is important <a id="_idIndexMarker223"/>to prematurely identify software and hardware components needed to implement the correct mechanisms. Simply adding a cryptography library does not guarantee an increased level of security in the system unless all the requirements are fulfilled, which often implies some participation from specific hardware components.</p>
<p>Some algorithms require random values with high entropy, which is often hard to obtain on microcontrollers <a id="_idIndexMarker224"/>without the help of specific hardware, such as <strong class="bold">True Random Number </strong><strong class="bold">Generators</strong> (<strong class="bold">TRNGs</strong>).</p>
<p>Other public-key-based cryptography requires trust anchor storage, which means a memory location that cannot be modified at runtime by an attacker, and usually relies on some non-volatile memory features that may be present on the flash memory controller. Finally, to store secret keys, hardware assistance can be needed to provide a secure vault that <a id="_idIndexMarker225"/>can only be accessed by privileged code or, in some cases, is never accessible from software and is only allowed to be used in combination with hardware cryptography engines that are coupled with secure storage.</p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor102"/>Running untrusted code</h2>
<p>As embedded systems become more complex and the code memory increases, it is not unusual to see <a id="_idIndexMarker226"/>software components from multiple sources integrated into a single firmware image. Some systems may even provide a software development kit that runs custom code provided by the user.</p>
<p>Others may have an interface that allows you to execute code from a remote location. In all these cases, it would be appropriate to consider separation mechanisms to prevent accidental (or intentional) access to memory areas or peripherals that should not be reachable by actors with lower capabilities.</p>
<p>Most microcontrollers provide two levels of execution privileges, and on some platforms, it is possible to divide the addressable memory space according to those privileges through context switching in the OS. Newer generations of microcontrollers provide TEEs to strictly enforce memory boundaries based<a id="_idTextAnchor103"/> on the execution level of the current stage.</p>
<h1 id="_idParaDest-80"><a id="_idTextAnchor104"/>The life cycle of an embedded project</h1>
<p>Modern development frameworks suggest splitting the work into smaller action points and marking <a id="_idIndexMarker227"/>milestones through the project development while producing intermediate working deliverables. Each deliverable focuses on giving a prototype of the entire system, with the missing features temporarily replaced using dummy code.</p>
<p>These recommendations seem particularly effective for embedded projects. In an environment where every error could be fatal to the entire system, working on small action points, one <a id="_idIndexMarker228"/>at a time, is an efficient way to promptly identify defects and regressions while working on the code base, provided that a <strong class="bold">Continuous Integration</strong> (<strong class="bold">CI</strong>) mechanism is in place from the early stages of the development. Intermediate milestones should be as frequent as possible, and for this reason, it is advisable to create a prototype of the final system as soon as possible in the development phase. This has to be taken into account when actions are identified, prioritized, and distributed to the team.</p>
<p>Once the steps to reach the goal are defined, we need to find the optimal sequence to produce working prototypes for the intermediate milestones. The dependencies among the development actions are taken into account to sort the priorities for the assignments of the work.</p>
<p>A progressive understanding of the system behavior and hardware constraints may change the view on the system’s architecture while it is under development, as unexpected issues are faced. Changing specifications as a reaction to measurements and evaluations performed on <a id="_idIndexMarker229"/>the intermediate prototype may require a major code rework. Throwing away consistent parts of the project to replace them with a new, improved design is often beneficial for the quality of the project and may result <a id="_idIndexMarker230"/>in improved productivity in the later stages. This process, known as <strong class="bold">refactoring</strong>, must not be seen as a development overhead whenever it is aimed at improving the design and behavior of the system.</p>
<p>Finally, the process of creating system software includes defining a clear API for the applications to interact with the system in the desired way. Embedded systems provide specific APIs to access system resources most of the time; however, some operating systems and libraries may provide POSIX-like interfaces to access functionalities. In any case, the API is the entry point for the system interfaces and m<a id="_idTextAnchor105"/>ust be designed for usability and well documented.</p>
<h2 id="_idParaDest-81"><a id="_idTextAnchor106"/>Defining project steps</h2>
<p>When analyzing specifications, defining the required steps, and assigning priorities, several factors may <a id="_idIndexMarker231"/>have to be taken into account. Consider designing an air quality monitor device with a PM10 air quality serial sensor, which collects the hourly measurements into the internal flash, then transmits all the statistics daily to a gateway using a wireless transceiver. The target system is a custom board based on Cortex-M MCU, which is adequately sized to run the final software. The final hardware design will not be available until some real-life measurements are done on the transceiver transmitting data to the gateway.</p>
<p>The list of steps to be performed to reach the final goal resulting from these specifications may look as follows:</p>
<ol>
<li>Boot a minimal system on the target (empty main loop).</li>
<li>Set up serial port <code>0</code> for logging.</li>
<li>Set up serial port <code>1</code> for the communication to the sensor.</li>
<li>Set up a timer.</li>
<li>Write the PM10 sensor driver.</li>
<li>Create an application that wakes up every hour and reads from the sensor.</li>
<li>Write a flash submodule to store/restore measurements.</li>
<li>Set up an SPI port to communicate to the radio chip.</li>
<li>Write the radio driver.</li>
<li>Implement a <a id="_idIndexMarker232"/>protocol to communicate with the gateway.</li>
<li>Every 24 measurements, the application sends daily measurements to the gateway.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Some of the steps may depend on others, so there are constraints on the order of execution. Some of these dependencies can be removed by using simulators or emulators.</p>
<p>For example, we might want to implement the communication protocol without having a working radio only if there is a way to test the protocol against the agent running on the gateway by using a simulated radio channel on the gateway itself. Keeping the modules self-contained and with a minimal set of API calls exposed to the outside makes it easier to detach the single modules to run and test them on different architectures, and under a controlled environm<a id="_idTextAnchor107"/>ent, before integrating them into the target system.</p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor108"/>Prototyping</h2>
<p>As it is part of the <a id="_idIndexMarker233"/>specifications, we know that we should prioritize the activities related to the radio communication to allow the hardware team to progress on the design, so in this case, the first prototype must do the following:</p>
<ol>
<li value="1">Boot a minimal system on the target (empty main loop).</li>
<li>Set up serial port <code>0</code> for logging.</li>
<li>Set up an SPI port to communicate to the radio chip.</li>
<li>Write the radio driver.</li>
<li>Set up the timer.</li>
<li>Write the main application to test the radio channel (sending raw packets at regular intervals).</li>
</ol>
<p>This first prototype will already start to look like the final device, even if it does not yet know how to communicate with the sensor. Some test cases can already be implemented to run on a mock gateway, checking that messages are received and valid.</p>
<p>Moving ahead to the next prototype definition, we can start to add a few additional features. Real sensor readings are not necessary to progress on the protocol with the gateway, as it is possible to use made-up, <em class="italic">synthetic</em> test values that reproduce a specific behavior instead. This allows us to progress on other tasks when the real hardware is not available.</p>
<p>Whether the development team is adopting pure agile software development or is working with a different methodology, fast prototyping in an embedded development environment allows responding faster to the uncertainties on the path, which often depend on the behavior of the hardware and the actions that need to be taken in the software.</p>
<p>Providing workable intermediate deliverables is a common practice in embedded development teams, which directly derives from agile methodologies. Agile software development foresees the delivery of working software regularly and within short intervals of time. Like in the preceding example, an intermediate prototype does not have to implement all the logic of the final software image but instead must be used to prove concepts, make <a id="_idIndexMarker234"/>measurements, or pr<a id="_idTextAnchor109"/>ovide examples on top of a reduced part of the system.</p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor110"/>Refactoring</h2>
<p>Too often considered a drastic remedy for a failure, refactoring is actually a healthy practice that improves <a id="_idIndexMarker235"/>the software while the system takes its final shape, and the support for software components and peripherals evolves over time.</p>
<p>Refactoring works better if all the tests are up and running on the old code. Unit tests should be adapted to the new function signatures while redesigning the module internals. On the other hand, existing functional tests for the module being refactored should not change if the API of the module stays unchanged and will provide continuous feedback about the status and the accuracy of the process as long as the interface toward other modules remains the same.</p>
<p>Smaller portions of the code base are exponentially easier to refactor than larger ones, which gives us yet another reason to keep each module small and dedicated to a specific function on the system. Progressing through intermediate deliverable prototypes implies constant alterations in the application code, which should require less effort when the subsystems are designed to be independ<a id="_idTextAnchor111"/>ent of each other and from the application code itself.</p>
<h2 id="_idParaDest-84"><a id="_idTextAnchor112"/>API and documentation</h2>
<p>We all know that a book should not be judged by its cover. However, a system can often be judged by its API, which may reveal many aspects of the internal implementation and the design choices of the <a id="_idIndexMarker236"/>system architects. A clear, readable, and easy-to-understand API is one of the most important features of an embedded system. Application developers expect to understand how to access functionalities quickly and to use the system in the most efficient way possible. The API represents the <em class="italic">contract</em> between the system and the applications, and for this reason, it must be designed beforehand and modified as little as possible, if at all, while the development moves towards the final delivery.</p>
<p>Some interfaces in the API may describe complex subsystems and abstract more elaborate characteristics, so it is always a good idea to provide adequate documentation to help application developers move around and exploit all the system capabilities. There are different ways to provide documentation along with the code, either distributing user <a id="_idIndexMarker237"/>manuals in the repository as separate files or including the explanation of the different interfaces directly in the code.</p>
<p>The amount of comments in the code is not an indicator of quality. Comments tend to <em class="italic">age</em> whenever the code they refer to gets modified because of the possibility that the developer forgets to update the comment to match the new behavior in the code. Moreover, not all code needs to be commented; good habits, such as keeping functions short and low in complexity or using expressive symbol names, would make code comments redundant in most cases, as the code can explain itself.</p>
<p>There are exceptions for lines of code that contain complex calculations, bit shifting, elaborate conditions, or side effects that are not easy to spot when reading the code for the first time. Some portions of code may also require a description at the beginning, for example, those functions with multiple return values and specific error handling. Switch/case statements not containing the break instruction between two cases must always have a comment to indicate that the fall-through is intended and not a mistake.</p>
<p>They should also possibly explain why some actions are grouped between two or more cases. Adding superfluous <a id="_idIndexMarker238"/>comments that do <a id="_idIndexMarker239"/>not provide any valuable explanation of the code only contributes to making the code harder to read.</p>
<p>On the other hand, describing the behavior of a module with a separate editor and tools requires dedication, as all the documentation must be updated every time there are significant changes in the code, and the developers are asked to switch the focus away from the actual code.</p>
<p>Usually, the important part to document is the description of the contract mentioned previously, enumerating and explaining the functions and the variables that the applications and the other components involved can access at runtime. Since these declarations can be grouped within header files, it is possible to describe the entire contract by adding extended comments on top of the declaration of each exported symbol.</p>
<p>Software tools exist that convert <a id="_idIndexMarker240"/>these comments into formatted documentation. A popular example is <strong class="bold">Doxygen</strong>, a free and open source document-generation tool that parses comments matching a specific syntax in the whole code base to produce hypertexts, structured PDF manuals, and many other formats. If the documentation is in the code base, updating and keeping track of its results is easier and less invasive for <a id="_idIndexMarker241"/>the developers’ workflow. Integrating the <a id="_idIndexMarker242"/>generation of the documentation on the automation server can provide a freshly generated copy of the <a id="_idTextAnchor113"/>manuals for all the APIs at every commit on the master branch.</p>
<h1 id="_idParaDest-85"><a id="_idTextAnchor114"/>Summary</h1>
<p>The methodologies that have been proposed are meant as an example of reference patterns used to design and manage the development of embedded projects. While it is possible that some of the patterns described may not apply to all projects, the goal of this chapter is to encourage embedded architects to look for improvements in the process that may result in a more efficient and less expensive software life cycle. Finally, we analyzed the possibility of increasing security by adding appropriate processes and components when required by the use case.</p>
<p>In the next chapter, we shall analyze what happens at boot time inside the embedded system, and how to prepare a bootable application using a simple, bare-metal, main-loop approach.</p>
</div>
</body></html>