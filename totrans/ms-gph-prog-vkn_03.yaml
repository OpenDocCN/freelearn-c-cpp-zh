- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlocking Multi-Threading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will talk about adding multi-threading to the Raptor Engine.
  prefs: []
  type: TYPE_NORMAL
- en: This requires both a big change in the underlying architecture and some Vulkan-specific
    changes and synchronization work so that the different cores of the CPU and the
    GPU can cooperate in the most correct and the fastest way.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-threading** rendering is a topic covered many times over the years
    and a feature that most game engines have needed since the era of multi-core architectures
    exploded. Consoles such as the PlayStation 2 and the Sega Saturn already offered
    multi-threading support, and later generations continued the trend by providing
    an increasing number of cores that developers could take advantage of.'
  prefs: []
  type: TYPE_NORMAL
- en: The first trace of multi-threading rendering in a game engine is as far back
    as 2008 when Christer Ericson wrote a blog post ([https://realtimecollisiondetection.net/blog/?p=86](https://realtimecollisiondetection.net/blog/?p=86))
    and showed that it was possible to parallelize and optimize the generation of
    commands used to render objects on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Older APIs such as OpenGL and DirectX (up until version 11) did not have proper
    multi-threading support, especially because they were big state machines with
    a global context tracking down each change after each command. Still, the command
    generation across different objects could take a few milliseconds, so multi-threading
    was already a big save in performance.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily for us, Vulkan fully supports multi-threading command buffers natively,
    especially with the creation of the `VkCommandBuffer` class, from an architectural
    perspective of the Vulkan API.
  prefs: []
  type: TYPE_NORMAL
- en: The Raptor Engine, up until now, was a single-threaded application and thus
    required some architectural changes to fully support multi-threading. In this
    chapter, we will see those changes, learn how to use a task-based multi-threading
    library called enkiTS, and then unlock both asynchronous resource loading and
    multi-threading command recording.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to use a task-based multi-threading library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to asynchronously load resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to draw in parallel threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the chapter, we will know how to run concurrent tasks both for
    loading resources and drawing objects on the screen. By learning how to reason
    with a task-based multi-threading system, we will be able to perform other parallel
    tasks in future chapters as well.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found at the following URL: [https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter3](https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan/tree/main/source/chapter3).'
  prefs: []
  type: TYPE_NORMAL
- en: Task-based multi-threading using enkiTS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To achieve parallelism, we need to understand some basic concepts and choices
    that led to the architecture developed in this chapter. First, we should note
    that when we talk about parallelism in software engineering, we mean the act of
    executing chunks of code at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: This is possible because modern hardware has different units that can be operated
    independently, and operating systems have dedicated execution units called **threads**.
  prefs: []
  type: TYPE_NORMAL
- en: A common way to achieve parallelism is to reason with tasks – small independent
    execution units that can run on any thread.
  prefs: []
  type: TYPE_NORMAL
- en: Why task-based parallelism?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-threading is not a new subject, and since the early years of it being
    added to various game engines, there have been different ways of implementing
    it. Game engines are pieces of software that use all of the hardware available
    in the most efficient way, thus paving the way for more optimized software architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we’ll take some ideas from game engines and gaming-related presentations.
    The initial implementations started by adding a thread with a single job to do
    – something specific, such as rendering a single thread, an asynchronous **input/output**
    (**I/O**) thread, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This helped add more granularity to what could be done in parallel, and it was
    perfect for the older CPUs (having two cores only), but it soon became limiting.
  prefs: []
  type: TYPE_NORMAL
- en: 'There was the need to use cores in a more agnostic way so that any type of
    job could be done by almost any core and to improve performance. This gave way
    to the emergence of two new architectures: **task-based** and **fiber-based**
    architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: Task-based parallelism is achieved by feeding multiple threads with different
    tasks and orchestrating them through dependencies. Tasks are inherently platform
    agnostic and cannot be interrupted, leading to a more straightforward capability
    to schedule and organize code to be executed with them.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, fibers are software constructs similar to tasks, but they
    rely heavily on the scheduler to interrupt their flow and resume when needed.
    This main difference makes it hard to write a proper fiber system and normally
    leads to a lot of subtle errors.
  prefs: []
  type: TYPE_NORMAL
- en: For the simplicity of using tasks over fibers and the bigger availability of
    libraries implementing task-based parallelism, the enkiTS library was chosen to
    handle everything. For those curious about more in-depth explanations, there are
    a couple of great presentations about these architectures.
  prefs: []
  type: TYPE_NORMAL
- en: A great example of a task-based engine is the one behind the Destiny franchise
    (with an in-depth architecture you can view at [https://www.gdcvault.com/play/1021926/Destiny-s-Multithreaded-Rendering](https://www.gdcvault.com/play/1021926/Destiny-s-Multithreaded-Rendering)),
    while a fiber-based one is used by the game studio Naughty Dog for their games
    (there is a presentation about it at [https://www.gdcvault.com/play/1022186/Parallelizing-the-Naughty-Dog-Engine](https://www.gdcvault.com/play/1022186/Parallelizing-the-Naughty-Dog-Engine)).
  prefs: []
  type: TYPE_NORMAL
- en: Using the enkiTS (Task-Scheduler) library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Task-based multi-threading is based on the concept of a task, defined as a *unit
    of independent work that can be executed on any core of* *a CPU*.
  prefs: []
  type: TYPE_NORMAL
- en: To do that, there is a need for a scheduler to coordinate different tasks and
    take care of the possible dependencies between them. Another interesting aspect
    of a task is that it could have one or more dependencies so that it could be scheduled
    to run only after certain tasks finish their execution.
  prefs: []
  type: TYPE_NORMAL
- en: This means that tasks can be submitted to the scheduler at any time, and with
    proper dependencies, we create a graph-based execution of the engine. If done
    properly, each core can be utilized fully and results in optimal performance to
    the engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scheduler is the brain behind all the tasks: it checks dependencies and
    priorities, and schedules or removes tasks based on need, and it is a new system
    added to the Raptor Engine.'
  prefs: []
  type: TYPE_NORMAL
- en: When initializing the scheduler, the library spawns a number of threads, each
    waiting to execute a task. When adding tasks to the scheduler, they are inserted
    into a queue. When the scheduler is told to execute pending tasks, each thread
    gets the next available task from the queue – according to dependency and priority
    – and executes it.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that running tasks can spawn other tasks. These tasks
    will be added to the thread’s local queue, but they are up for grabs if another
    thread is idle. This implementation is called a **work-stealing queue**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initializing the scheduler is as simple as creating a configuration and calling
    the `Initialize` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With this code, we are telling the task scheduler to spawn four threads that
    it will use to perform its duties. enkiTS uses the `TaskSet` class as a unit of
    work, and it uses both inheritance and lambda functions to drive the execution
    of tasks in the scheduler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this simple snippet, we see how to create an empty `TaskSet` (as the name
    implies, a set of tasks) that defines how a task will execute the code, leaving
    the scheduler with the job of deciding how many of the tasks will be needed and
    which thread will be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more streamlined version of the previous code uses lambda functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This version can be easier when reading the code as it does break the flow less,
    but it is functionally equivalent to the previous one.
  prefs: []
  type: TYPE_NORMAL
- en: Another feature of the enkiTS scheduler is the possibility to add pinned tasks
    – special tasks that will be bound to a thread and will always be executed there.
    We will see the use of pinned tasks in the next section to perform asynchronous
    I/O operations.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we talked briefly about the different types of multi-threading
    so that we could express the reason for choosing to use task-based multi-threading.
    We then showed some simple examples of the enkiTS library and its usage, adding
    multi-threading capabilities to the Raptor Engine.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will finally see a real use case in the engine, which
    is the asynchronous loading of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous loading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The loading of resources is one of the (if not *the*) slowest operations that
    can be done in any framework. This is because the files to be loaded are big,
    and they can come from different sources, such as optical units (DVD and Blu-ray),
    hard drives, and even the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is another great topic, but the most important concept to understand is
    the inherent speed necessary to read the memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – A memory hierarchy](img/B18395_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – A memory hierarchy
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding diagram, the fastest memory is the registers memory.
    After registers follows the cache, with different levels and access speeds: both
    registers and caches are directly in the processing unit (both the CPU and GPU
    have registers and caches, even with different underlying architectures).'
  prefs: []
  type: TYPE_NORMAL
- en: Main memory refers to the RAM, which is the area that is normally populated
    with the data used by the application. It is slower than the cache, but it is
    the target of the loading operations as the only one directly accessible from
    the code. Then there are magnetic disks (hard drives) and optical drives – much
    slower but with greater capacity. They normally contain the asset data that will
    be loaded into the main memory.
  prefs: []
  type: TYPE_NORMAL
- en: The final memory is in remote storage, such as from some servers, and it is
    the slowest. We will not deal with that here, but it can be used when working
    on applications that have some form of online service, such as multiplayer games.
  prefs: []
  type: TYPE_NORMAL
- en: With the objective of optimizing the read access in an application, we want
    to transfer all the needed data into the main memory, as we can’t interact with
    caches and registers. To hide the slow speed of magnetic and optical disks, one
    of the most important things that can be done is to parallelize the loading of
    any resource coming from any medium so that the fluidity of the application is
    not slowed down.
  prefs: []
  type: TYPE_NORMAL
- en: The most common way of doing it, and one example of the thread-specialization
    architecture we talked briefly about before, is to have a separate thread that
    handles just the loading of resources and interacts with other systems to update
    the used resources in the engine.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will talk about how to set up enkiTS and create
    tasks for parallelizing the Raptor Engine, as well as talk about Vulkan queues,
    which are necessary for parallel command submission. Finally, we will dwell on
    the actual code used for asynchronous loading.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the I/O thread and tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the enkiTS library, there is a feature called **pinned-task** that associates
    a task to a specific thread so that it is continuously running there unless stopped
    by the user or a higher priority task is scheduled on that thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simplify things, we will add a new thread and avoid it being used by the
    application. This thread will be mostly idle, so the context switch should be
    low:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create a pinned task and associate it with a thread ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can create the actual task responsible for asynchronous loading,
    associating it with the same thread as the pinned task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The final piece of the puzzle is the actual code for these two tasks. First,
    let us have a look at the first pinned task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This task will wait for any other pinned task and run them when possible. We
    have added an `execute` flag to stop the execution when needed, for example, when
    exiting the application, but it could be used in general to suspend it in other
    situations (such as when the application is minimized).
  prefs: []
  type: TYPE_NORMAL
- en: 'The other task is the one executing the asynchronous loading using the `AsynchronousLoader`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The idea behind this task is to always be active and wait for requests for resource
    loading. The `while` loop ensures that the root pinned task never schedules other
    tasks on this thread, locking it to I/O as intended.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to look at the `AsynchronousLoader` class, we need to look
    at an important concept in Vulkan, namely queues, and why they are a great addition
    for asynchronous loading.
  prefs: []
  type: TYPE_NORMAL
- en: Vulkan queues and the first parallel command generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of a *queue* – which can be defined as the entry point to submit
    commands recorded in `VkCommandBuffers` to the GPU – is an addition to Vulkan
    compared to OpenGL and needs to be taken care of.
  prefs: []
  type: TYPE_NORMAL
- en: Submission using a queue is a single-threaded operation, and a costly operation
    that becomes a synchronization point between CPU and GPU to be aware of. Normally,
    there is the main queue to which the engine submits command buffers before presenting
    the frame. This will send the work to the GPU and create the rendered image intended.
  prefs: []
  type: TYPE_NORMAL
- en: But where there is one queue, there can be more. To enhance parallel execution,
    we can instead create different *queues* – and use them in different threads instead
    of the main one.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more in-depth look at queues can be found at [https://github.com/KhronosGroup/Vulkan-Guide/blob/master/chapters/queues.adoc](https://github.com/KhronosGroup/Vulkan-Guide/blob/master/chapters/queues.adoc),
    but what we need to know is that each queue can submit certain types of commands,
    visible through a queue’s flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '`VK_QUEUE_GRAPHICS_BIT` can submit all `vkCmdDraw` commands'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VK_QUEUE_COMPUTE` can submit all `vkCmdDispatch` and `vkCmdTraceRays` (used
    for ray tracing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VK_QUEUE_TRANSFER` can submit copy commands, such as `vkCmdCopyBuffer`, `vkCmdCopyBufferToImage`,
    and `vkCmdCopyImageToBuffer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each available queue is exposed through a queue family. Each queue family can
    have multiple capabilities and can expose multiple queues. Here is an example
    to clarify:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The first queue exposes all capabilities, and we only have one of them. The
    next queue can be used for compute and transfer, and the third one for transfer
    (we’ll ignore the sparse feature for now). We have two queues for each of these
    families.
  prefs: []
  type: TYPE_NORMAL
- en: It is guaranteed that on a GPU there will always be at least one queue that
    can submit all types of commands, and that will be our main queue.
  prefs: []
  type: TYPE_NORMAL
- en: In some GPUs, though, there can be specialized queues that have only the `VK_QUEUE_TRANSFE`R
    flag activated, which means that they can use **direct memory access** (**DMA**)
    to speed up the transfer of data between the CPU and the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'One last thing: the Vulkan logical device is responsible for creating and destroying
    queues – an operation normally done at the startup/shutdown of the application.
    Let us briefly see the code to query the support for different queues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen in the preceding code, we get the list of all queues for the
    selected GPU, and we check the different bits that identify the types of commands
    that can be executed there.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we will save the *main queue* and the *transfer queue*, if it is
    present on the GPU, and we will save the indices of the *queues* to retrieve the
    `VkQueue` after the device creation. Some devices don’t expose a separate transfer
    queue. In this case, we will use the main queue to perform transfer operations,
    and we need to make sure that access to the queue is correctly synchronized for
    upload and graphics submissions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how to create the *queues*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As already mentioned, `vkCreateDevice` is the command that creates *queues*
    by adding `pQueueCreateInfos` in the `VkDeviceCreateInfo` struct.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the device is created, we can query for all the queues as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we have both the main and the transfer queues ready to be used
    to submit work in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: We had a look at how to submit parallel work to copy memory over the GPU without
    blocking either the GPU or the CPU, and we created a specific class to do that,
    `AsynchronousLoader`, which we will cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The AsynchronousLoader class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we’ll finally see the code for the class that implements asynchronous
    loading.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `AsynchronousLoader` class has the following responsibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Process load from file requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process GPU upload transfers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage a staging buffer to handle a copy of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enqueue the command buffers with copy commands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signal to the renderer that a texture has finished a transfer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before focusing on the code that uploads data to the GPU, there is some Vulkan-specific
    code that is important to understand, relative to command pools, transfer queues,
    and using a staging buffer.
  prefs: []
  type: TYPE_NORMAL
- en: Creating command pools for the transfer queue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to submit commands to the transfer queue, we need to create command
    pools that are linked to that queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The important part is `queueFamilyIndex`, to link `CommandPool` to the transfer
    queue so that every command buffer allocated from this pool can be properly submitted
    to the transfer queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will simply allocate the command buffers linked to the newly created
    pools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With this setup, we are now ready to submit commands to the transfer queue using
    the command buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will have a look at the staging buffer – an addition to ensure that
    the transfer to the GPU is the fastest possible from the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the staging buffer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To optimally transfer data between the CPU and the GPU, there is the need to
    create an area of memory that can be used as a source to issue commands related
    to copying data to the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we will create a staging buffer, a persistent buffer that will
    serve this purpose. We will see both the Raptor wrapper and the Vulkan-specific
    code to create a persistent staging buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will allocate a persistently mapped buffer of 64
    MB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This translates to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This buffer will be the source of the memory transfers, and the `VMA_ALLOCATION_CREATE_MAPPED_BIT`
    flag ensures that it will always be mapped.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can retrieve and use the pointer to the allocated data from the `allocation_info`
    structure, filled by `vmaCreateBuffer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We can now use the staging buffer for any operation to send data to the GPU,
    and if ever there is the need for a bigger allocation, we could recreate a new
    staging buffer with a bigger size.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to see the code to create a semaphore and a fence used to submit
    and synchronize the CPU and GPU execution of commands.
  prefs: []
  type: TYPE_NORMAL
- en: Creating semaphores and fences for GPU synchronization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The code here is straightforward; the only important part is the creation of
    a signaled fence because it will let the code start to process uploads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we have now arrived at processing the requests.
  prefs: []
  type: TYPE_NORMAL
- en: Processing a file request
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: File requests are not specifically Vulkan-related, but it is useful to see how
    they are done.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the STB image library ([https://github.com/nothings/stb](https://github.com/nothings/stb))
    to load the texture into memory and then simply add the loaded memory and the
    associated texture to create an upload request. This will be responsible for copying
    the data from the memory to the GPU using the transfer queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will see how to process an upload request.
  prefs: []
  type: TYPE_NORMAL
- en: Processing an upload request
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the part that finally uploads the data to the GPU. First, we need to
    ensure that the fence is signaled to proceed, which is why we created it already
    signaled.
  prefs: []
  type: TYPE_NORMAL
- en: 'If it is signaled, we can reset it so we can let the API signal it when the
    submission is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We then proceed to take a request, allocate memory from the staging buffer,
    and use a command buffer to upload the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `upload_texture_data` method is the one that takes care of uploading data
    and adding the needed barriers. This can be tricky, so we’ve included the code
    to show how it can be done.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to copy the data to the staging buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can prepare a copy, in this case, from the staging buffer to an image.
    Here, it is important to specify the offset into the staging buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We then proceed with adding a precopy memory barrier to perform a layout transition
    and specify that the data is using the transfer queue.
  prefs: []
  type: TYPE_NORMAL
- en: This uses the code suggested in the synchronization examples provided by the
    Khronos Group ([https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples](https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, we show the raw Vulkan code that is simplified with some utility
    functions, highlighting the important lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The texture is now ready to be copied to the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The texture is now on the GPU, but it is still not usable from the main queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is why we need another memory barrier that will also transfer ownership:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Once the ownership is transferred, a final barrier is needed to ensure that
    the transfer is complete and the texture can be read from the shaders, but this
    will be done by the renderer because it needs to use the main queue.
  prefs: []
  type: TYPE_NORMAL
- en: Signaling the renderer of the finished transfer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The signaling is implemented by simply adding the texture to a mutexed list
    of textures to update so that it is thread safe.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we need to perform a final barrier for each transferred texture.
    We opted to add these barriers after all the rendering is done and before the
    present step, but it could also be done at the beginning of the frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated before, one last barrier is needed to signal that the newly updated
    image is ready to be read by shaders and that all the writing operations are done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to use the texture on the GPU in our shaders, and the asynchronous
    loading is working. A very similar path is created for uploading buffers and thus
    will be omitted from the book but present in the code.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw how to unlock the asynchronous loading of resources
    to the GPU by using a transfer queue and different command buffers. We also showed
    how to manage ownership transfer between queues. Then, we finally saw the first
    steps in setting up tasks with the task scheduler, which is used to add multi-threading
    capabilities to the Raptor Engine.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will use the acquired knowledge to add the parallel
    recording of commands to draw objects on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Recording commands on multiple threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To record commands using multiple threads, it is necessary to use different
    command buffers, at least one on each thread, to record the commands and then
    submit them to the main queue. To be more precise, in Vulkan, any kind of pool
    needs to be externally synchronized by the user; thus, the best option is to have
    an association between a thread and a pool.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of command buffers, they are allocated from the associated pool
    and commands registered in it. Pools can be `CommandPools`, `DescriptorSetPools`,
    and `QueryPools` (for time and occlusion queries), and once associated with a
    thread, they can be used freely inside that thread of execution.
  prefs: []
  type: TYPE_NORMAL
- en: The execution order of the command buffers is based on the order of the array
    submitted to the main queue – thus, from a Vulkan perspective, sorting can be
    performed on a command buffer level.
  prefs: []
  type: TYPE_NORMAL
- en: We will see how important the allocation strategy for command buffers is and
    how easy it is to draw in parallel once the allocation is in place. We will also
    talk about the different types of command buffers, a unique feature of Vulkan.
  prefs: []
  type: TYPE_NORMAL
- en: The allocation strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The success in recording commands in parallel is achieved by taking into consideration
    both thread access and frame access. When creating command pools, not only does
    each thread need a unique pool to allocate command buffers and commands from,
    but it also needs to not be in flight in the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: A simple allocation strategy is to decide the maximum number of threads (we
    will call them `T`) that will record commands and the max number of frames (we
    will call them `F`) that can be in flight, then allocate command pools that are
    `F *` `T`.
  prefs: []
  type: TYPE_NORMAL
- en: For each task that wants to render, using the pair frame-thread ID, we will
    guarantee that no pool will be either in flight or used by another thread.
  prefs: []
  type: TYPE_NORMAL
- en: This is a very conservative approach and can lead to unbalanced command generations,
    but it can be a great starting point and, in our case, enough to provide support
    for parallel rendering to the Raptor Engine.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we will allocate a maximum of five empty command buffers, two primary
    and three secondary, so that more tasks can execute chunks of rendering in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The class responsible for this is the `CommandBufferManager` class, accessible
    from the device, and it gives the user the possibility to request a command buffer
    through the `get_command_buffer` method.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see the difference between primary and secondary
    command buffers, which are necessary to decide the granularity of the tasks to
    draw the frame in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Command buffer recycling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linked to the allocation strategy is the recycling of the buffers. When a buffer
    has been executed, it can be reused to record new commands instead of always allocating
    new ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to the allocation strategy we chose, we associate a fixed amount of
    `CommandPools` to each frame, and thus to reuse the command buffers, we will reset
    its corresponding `CommandPool` instead of manually freeing buffers: this has
    been proven to be much more efficient on CPU time.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are not freeing the memory associated with the buffer, but we give
    `CommandPool` the freedom to reuse the total memory allocated between the command
    buffers that will be recorded, and it will reset all the states of all its command
    buffers to their initial state.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the beginning of each frame, we call a simple method to reset pools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: There is a utility method to calculate the pool index, based on the thread and
    frame.
  prefs: []
  type: TYPE_NORMAL
- en: After the reset of the pools, we can reuse the command buffers to record commands
    without needing to explicitly do so for each command.
  prefs: []
  type: TYPE_NORMAL
- en: We can finally have a look at the different types of command buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Primary versus secondary command buffers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Vulkan API has a unique difference in what command buffers can do: a command
    buffer can either be primary or secondary.'
  prefs: []
  type: TYPE_NORMAL
- en: Primary command buffers are the most used ones and can perform any of the commands
    – drawing, compute, or copy commands, but their granularity is pretty coarse –
    at least one render pass must be used, and no pass can be further parallelized.
  prefs: []
  type: TYPE_NORMAL
- en: Secondary command buffers are much more limited – they can actually only execute
    draw commands within a render pass – but they can be used to parallelize the rendering
    of render passes that contain many draw calls (such as a G-Buffer render pass).
  prefs: []
  type: TYPE_NORMAL
- en: It is paramount then to make an informed decision about the granularity of the
    tasks, and especially important is to understand when to record using a primary
    or secondary buffer.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B18395_04.xhtml#_idTextAnchor064), *Implementing a Frame Graph*,
    we will see how a graph of the frame can give enough information to decide which
    command buffer type to use and how many objects and render passes should be used
    in a task.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how to use both primary and secondary command
    buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing using primary command buffers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Drawing using primary command buffers is the most common way of using Vulkan
    and also the simplest. A primary command buffer, as already stated before, can
    execute any kind of command with no limitation, and it is the only one that can
    be submitted to a queue to be executed on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a primary command buffer is simply a matter of using `VK_COMMAND_BUFFER_LEVEL_PRIMARY`
    in the `VkCommandBufferAllocateInfo` structure passed to the `vkAllocateCommandBuffers`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Once created, at any time, we can begin the commands recording (with the `vkBeginCommandBuffer`
    function), bind passes and pipelines, and issue draw commands, copy commands,
    and compute ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the recording is finished, the `vkEndCommandBuffer` function must be used
    to signal the end of recording and prepare the buffer to be ready to be submitted
    to a queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'To record commands in parallel, there are only two conditions that must be
    respected by the recording threads:'
  prefs: []
  type: TYPE_NORMAL
- en: Simultaneous recording on the same `CommandPool` is forbidden
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commands relative to `RenderPass` can only be executed in one thread
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens if a pass (such as a Forward or G-Buffer typical pass) contains
    a lot of draw-calls, thus requiring parallel rendering? This is where secondary
    command buffers can be useful.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing using secondary command buffers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Secondary command buffers have a very specific set of conditions to be used
    – they can record commands relative to only one render pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is why it is important to allow the user to record more than one secondary
    command buffer: it could be possible that more than one pass needs per-pass parallelism,
    and thus more than one secondary command buffer is needed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondary buffers always need a primary buffer and can’t be submitted directly
    to any queue: they must be copied into the primary buffer and inherit only `RenderPass`
    and `FrameBuffers` set when beginning to record commands.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at the different steps involving the usage of secondary command
    buffers. First, we need to have a primary command buffer that needs to set up
    a render pass and frame buffer to be rendered into, as this is absolutely necessary
    because no secondary command buffer can be submitted to a queue or set `RenderPass`
    or `FrameBuffer`.
  prefs: []
  type: TYPE_NORMAL
- en: Those will be the only states inherited from the primary command buffer, thus,
    even when beginning to record commands, viewport and stencil states must be set
    again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by showing a primary command buffer setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'When beginning a render pass that will be split among one or more secondary
    command buffers, we need to add the `VK_SUBPASS_CONTENTS_SECONDARY_COMMAND_BUFFERS`
    flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then pass the `inheritanceInfo` struct to the secondary buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we can begin the secondary command buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The secondary command buffer is now ready to start issuing drawing commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Note that the scissor and viewport must always be set at the beginning, as no
    state is inherited outside of the bound render pass and frame buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have finished recording the commands, we can call the `VkEndCommandBuffer`
    function and put the buffer into a copiable state in the primary command buffer.
    To copy the secondary command buffers into the primary one, there is a specific
    function, `vkCmdExecuteCommands`, that needs to be called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This function accepts an array of secondary command buffers that will be sequentially
    copied into the primary one.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure a correct ordering of the commands recorded, not guaranteed by multi-threading
    (as threads can finish in any order), we can give each command buffer an execution
    index, put them all into an array, sort them, and then use this sorted array in
    the `vkCmdExecuteCommands` function.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the primary command buffer can record other commands or be submitted
    to the queue, as it contains all the commands copied from the secondary command
    buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Spawning multiple tasks to record command buffers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last step is to create multiple tasks to record command buffers in parallel.
    We have decided to group multiple meshes per command buffer as an example, but
    usually, you would record separate command buffers per render pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We add a task to the scheduler for each mesh group. Each task will record a
    command buffer for a range of meshes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have added all the tasks, we have to wait until they complete before
    adding the secondary command buffers for execution on the main command buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We suggest reading the code for this chapter for more details on the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have described how to record multiple command buffers in
    parallel to optimize this operation on the CPU. We have detailed our allocation
    strategy for command buffers and how they can be reused across frames.
  prefs: []
  type: TYPE_NORMAL
- en: We have highlighted the differences between primary and secondary buffers and
    how they are used in our renderer. Finally, we have demonstrated how to record
    multiple command buffers in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to introduce the frame graph, a system that
    allows us to define multiple render passes and that can take advantage of the
    task system we have described to record the command buffer for each render pass
    in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the concept of task-based parallelism and
    saw how using a library such as enkiTS can quickly add multi-threading capabilities
    to the Raptor Engine.
  prefs: []
  type: TYPE_NORMAL
- en: We then learned how to add support for loading data from files to the GPU using
    an asynchronous loader. We also focused on Vulkan-related code to have a second
    queue of execution that can run in parallel to the one responsible for drawing.
    We saw the difference between primary and secondary command buffers.
  prefs: []
  type: TYPE_NORMAL
- en: We talked about the importance of the buffer’s allocation strategy to ensure
    safety when recording commands in parallel, especially taking into consideration
    command reuse between frames.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we showed step by step how to use both types of command buffers, and
    this should be enough to add the desired level of parallelism to any application
    that decides to use Vulkan as its graphics API.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will work on a data structure called **Frame Graph**,
    which will give us enough information to automate some of the recording processes,
    including barriers, and will ease the decision making about the granularity of
    the tasks that will perform parallel rendering.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Task-based systems have been in use for many years. [https://www.gdcvault.com/play/1012321/Task-based-Multithreading-How-to](https://www.gdcvault.com/play/1012321/Task-based-Multithreading-How-to)
    provides a good overview.
  prefs: []
  type: TYPE_NORMAL
- en: Many articles can be found that cover work-stealing queues at [https://blog.molecular-matters.com/2015/09/08/job-system-2-0-lock-free-work-stealing-part-2-a-specialized-allocator/](https://blog.molecular-matters.com/2015/09/08/job-system-2-0-lock-free-work-stealing-part-2-a-specialized-allocator/)
    and are a good starting point on the subject.
  prefs: []
  type: TYPE_NORMAL
- en: The PlayStation 3 and Xbox 360 use the Cell processor from IBM to provide more
    performance to developers through multiple cores. In particular, the PlayStation
    3 has several **synergistic processor units** (**SPUs**) that developers can use
    to offload work from the main processor.
  prefs: []
  type: TYPE_NORMAL
- en: There are many presentations and articles that detail many clever ways developers
    have used these processors, for example, [https://www.gdcvault.com/play/1331/The-PlayStation-3-s-SPU](https://www.gdcvault.com/play/1331/The-PlayStation-3-s-SPU)
    and [https://gdcvault.com/play/1014356/Practical-Occlusion-Culling-on](https://gdcvault.com/play/1014356/Practical-Occlusion-Culling-on).
  prefs: []
  type: TYPE_NORMAL
