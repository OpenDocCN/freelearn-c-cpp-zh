<html><head></head><body>
		<div id="_idContainer106">
			<h1 class="chapterNumber">Chapter 13</h1>
			<h1 id="_idParaDest-181" class="chapterTitle" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor345"/>Concurrency</h1>
			<p class="normal">Over the course of the next two chapters we are going to talk about <em class="italics">concurrency</em> and the theoretical background that is required for developing concurrent programs, not only in C, but necessarily in other languages as well. As such, these two chapters won't contain any C code and instead use pseudo-code to represent concurrent systems and their intrinsic properties.</p>
			<p class="normal">The topic of concurrency, due to its length, has been split into two chapters. In this chapter we will be looking at the basic concepts regarding concurrency itself, before moving to <em class="italics">Chapter 14</em>, <em class="italics">Synchronization</em>, where we will discuss concurrency-related issues and the <em class="italics">synchronization</em> mechanisms used in concurrent programs to resolve said issues. The collective end goal of these two chapters is to provide you with enough theoretical knowledge to proceed with the multithreading and multi-processing topics discussed in upcoming chapters.</p>
			<p class="normal">The background knowledge we build in this chapter will also be useful when working with the <em class="italics">POSIX threading library</em>, which we use throughout this book.</p>
			<p class="normal">In this first chapter on concurrency, we will be working on understanding:</p>
			<ul>
				<li class="list">How parallel systems differ from concurrent systems</li>
				<li class="list">When we need concurrency</li>
				<li class="list">What a <em class="italics">task scheduler</em> is, and what the widely used scheduling algorithms are</li>
				<li class="list">How a concurrent program is run and what the interleavings are</li>
				<li class="list">What a shared state is and how various tasks can access it</li>
			</ul>
			<p class="normal">Let's start our look into concurrency by giving an introduction to the concept, and understanding broadly what it means for us.</p>
			<h1 id="_idParaDest-182" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor346"/>Introducing concurrency</h1>
			<p class="normal">Concurrency simply means having multiple pieces of logic within a program being executed simultaneously. Modern software systems are often concurrent, as programs need to run various pieces of logic at the same time. As such, concurrency is something that every program today is using to a certain extent.</p>
			<p class="normal">We can say that concurrency is a powerful tool that lets you write programs that can manage different tasks at the same time, and the support for it usually lies in the kernel, which is at the heart of the operating system.</p>
			<p class="normal">There are <a id="_idIndexMarker883"/>numerous examples in which an ordinary program manages multiple jobs simultaneously. For example, you can surf the web while downloading files. In this case, tasks are being executed in the context of the browser process concurrently. Another notable example is in a <em class="italics">video streaming</em> scenario, such as when you are watching a video on YouTube. The video player might be in the middle of downloading future chunks of the video while you are still watching previously downloaded chunks.</p>
			<p class="normal">Even simple word-processing software has several concurrent tasks running in the background. As I write this chapter on Microsoft Word, a spell checker and a formatter are running in the background. If you were to be reading this on the Kindle application on an iPad, what programs do you think might be running concurrently as part of the Kindle program?</p>
			<p class="normal">Having multiple programs being run at the same time sounds amazing, but as with most technology, concurrency brings along with it several headaches in addition to its benefits. Indeed, concurrency brings some of the most painful headaches in the history of computer science! These "headaches," which we will address later on, can remain hidden for a long time, even for months after a release, and they are usually hard to find, reproduce, and resolve.</p>
			<p class="normal">We started this section describing concurrency as having tasks being executed at the same time, or concurrently. This description implies that the tasks are being run in parallel, but that's not strictly true. Such a description is too simple, as well as inaccurate, because <em class="italics">being concurrent is different from being parallel</em>, and we have not yet explained the differences between the two. Two concurrent programs are different from two parallel programs, and one of our goals in this chapter is to shine a light on these differences and give some definitions used by the official literature in this field.</p>
			<p class="normal">In the following sections, we are going to explain some basic concurrency-related concepts such as <em class="italics">tasks</em>, <em class="italics">scheduling</em>, <em class="italics">interleavings</em>, <em class="italics">state</em>, and <em class="italics">shared state</em>, which are some of the terms you will come across frequently in this book. It's worth pointing out that most of these concepts are abstract and can be applied to any concurrent system, not just in C.<a id="_idTextAnchor347"/></p>
			<p class="normal">To understand the difference between parallel and concurrent, we are going to briefly touch upon parallel systems.</p>
			<p class="normal">Note that in <a id="_idIndexMarker884"/>this chapter we stick to simple definitions. Our sole purpose is to give you a basic idea of how concurrent systems work, as going beyond this would be outside of the scope of this book on C.</p>
			<h1 id="_idParaDest-183" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor348"/>Parallelism</h1>
			<p class="normal">Parallelism simply means having two tasks run at the same time, or <em class="italics">in parallel</em>. The phrase "in parallel" is the key element that differentiates parallelism from concurrency. Why is this? Because parallel implies that two things are happening simultaneously. This is not the case <a id="_idIndexMarker885"/>in a concurrent system; in concurrent systems, you need to pause one task in order to let another continue execution. Note that this definition can be too simple and incomplete regarding the modern concurrent systems, but it is sufficient for us to give you a basic idea.</p>
			<p class="normal">We meet parallelism regularly in our daily lives. When you and your friend are doing two separate tasks simultaneously, those tasks are being done in parallel. To have a number of tasks in parallel, we need separate and isolated <em class="italics">processing units</em>, each of which is assigned to a certain task. For instance, in a computer system, each <em class="italics">CPU core</em> is a processor unit that can handle one task at a time.</p>
			<p class="normal">For a minute, look at yourself as the sole reader of this book. You cannot read two books in parallel; you would have to pause in reading one of them in order to read the other. Yet, if you added your friend into the mix, then it's possible for two books to be read in parallel.</p>
			<p class="normal">What would happen if you had a third book that needed to be read? Since neither of you can read two books in parallel, then one of you would need to pause in reading your book to continue with the third one. This simply means that either you or your friend need to divide your time properly in order to read all three books.</p>
			<p class="normal">In a computer system, there must be at least two separate and independent processing units in order to have two parallel tasks being executed on that system. Modern CPUs have a number of <em class="italics">cores</em> inside, and those cores are the actual processing units. For example, a 4-core CPU has 4 processing units, and therefore can support 4 parallel tasks being run simultaneously. For simplicity, in this chapter we will suppose that our imaginary CPU has only one core inside and therefore cannot perform parallel tasks. There will be some discussion regarding multi-core CPUs later, within relevant sections.</p>
			<p class="normal">Suppose that you get two laptops with our imaginary CPU inside, with one playing a piece of music, and the other one finding the solution to a differential equation. Both of them are functioning in parallel, but if you want them to do both on the same laptop using only one CPU, and with one core, then it <em class="italics">cannot</em> be parallel and it is in fact concurrent.</p>
			<p class="normal">Parallelism is about tasks that can be parallelized. This means that the actual algorithm can be divided and run on multiple processor units. But most of the algorithms we write, as of today, are <em class="italics">sequential</em> and not parallel in nature. Even in multithreading, each thread has a number of sequential instructions that cannot be broken into some parallel <em class="italics">execution flows</em>.</p>
			<p class="normal">In other words, a sequential algorithm cannot be easily broken into some parallel flows of execution <a id="_idIndexMarker886"/>automatically by the operating system, and this should be done by a programmer. Therefore, with having a multi-core CPU, you still need to assign each of the execution flows to a certain CPU core, and in that core, if you have more than one flow assigned, you cannot have both of them running in parallel, and you immediately observe a concurrent behavior.</p>
			<p class="normal">In short, of course having two flows, each assigned to a different core, can end up in two parallel flows but assigning them to just one core, would result in two concurrent flows. In multi-core CPUs we effectively observe a mixed behavior, both parallelism between the cores, and concurrency on the same core.</p>
			<p class="normal">Despite its simple meaning and numerous everyday examples, parallelism is a complex and tough topic in computer architecture. In fact, it is a separate academic subject from concurrency, with its own theories, books, and literature. Being able to have an operating system that can break a sequential algorithm into some parallel execution flows is an open field of research and the current operating systems cannot do that.</p>
			<p class="normal">As stated, the purpose of this chapter is not to go into any depth in parallelism, but only to provide an initial definition for the concept. Since further depth of discussion about parallelism is beyond the scope of this book, let's begin with the concept of concurren<a id="_idTextAnchor349"/>cy.</p>
			<p class="normal">Firstly, we'll talk about concurrent systems and what it really means in comparison to parallelism.</p>
			<h1 id="_idParaDest-184" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor350"/>Concurrency</h1>
			<p class="normal">You may <a id="_idIndexMarker887"/>have heard about <em class="italics">multitasking</em> – well, concurrency has the same idea. If your system is managing multiple tasks at the same time, you need to understand that it does not necessarily mean that the tasks are being run in parallel. Instead, there can be a <em class="italics">task scheduler</em> in the middle; this simply switches very quickly between the different tasks and performs a tiny bit of each of them in a fairly small amount of time.</p>
			<p class="normal">This certainly happens when you have just one processor unit. For the rest of our discussion in this section, we assume that we are operating on just one processor unit.</p>
			<p class="normal">If a task scheduler is sufficiently <em class="italics">fast</em> and <em class="italics">fair</em>, you won't notice the <em class="italics">switching</em> between the tasks, and they'll appear to be running in parallel from your perspective. That's the magic of concurrency, and the very reason why it is being used in most of the widely known operating systems, including Linux, macOS, and Microsoft Windows.</p>
			<p class="normal">Concurrency could <a id="_idIndexMarker888"/>be seen as a simulation of performing tasks in parallel, using a single processor unit. In fact, the whole idea can be referred to as a form of artificial parallelism. For old systems that only had a single CPU, with only one core, it was a huge advance when people were able to use that single core in a multitasking fashion.</p>
			<p class="normal">As a side note, <em class="italics">Multics</em> was one of the first operating systems designed to multitask and manage simultaneous processes. You'll remember that in <em class="italics">Chapter 10</em>, <em class="italics">Unix – History and Architecture</em>, Unix was built based on the ideas gained from the Multics project.</p>
			<p class="normal">As we've explained previously, almost all operating systems can perform concurrent tasks through multitasking, especially POSIX-compliant operating systems, since the ability is clearly exposed in the POSIX standard.</p>
			<h1 id="_idParaDest-185" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor351"/>Task scheduler unit</h1>
			<p class="normal">As we've said before, all multitasking operating systems are required to have a <em class="italics">task scheduler</em> unit, or simply a <em class="italics">scheduler unit</em>, in their kernel. In this section, we're going to see how this <a id="_idIndexMarker889"/>unit works and how it contributes to the seamless execution of some concurrent tasks.</p>
			<p class="normal">Some facts <a id="_idIndexMarker890"/>regarding the task scheduler unit are listed as follows:</p>
			<ul>
				<li class="list">The scheduler has a <em class="italics">queue</em> for tasks waiting to be executed. <em class="italics">Tasks</em> or <em class="italics">jobs</em> are simply the pieces of work that should be performed in separate flows of execution.</li>
				<li class="list">This queue is usually <em class="italics">prioritized</em>, with the high-priority tasks being chosen to start first.</li>
				<li class="list">The processor unit is managed and shared among all the tasks by the task scheduler. When the processor unit is free (no task is using it), the task scheduler must select another task from its queue before letting it use the processor unit. When the task is finished, it releases the processor unit and make it <a id="_idIndexMarker891"/>available again, then the task scheduler selects <a id="_idIndexMarker892"/>another task. This goes on in a continuous loop. This is called <em class="italics">task scheduling</em>, and it is the sole responsibility of the task scheduler to do this.</li>
				<li class="list">There are many <em class="italics">scheduling algorithms</em> that the task scheduler can operate, but all of them should address specific requirements. For example, all of them should be <em class="italics">fair</em>, and no task should be <em class="italics">starved</em> in the queue as a result of not being chosen for a prolonged period of time.</li>
				<li class="list">Based on a chosen <em class="italics">scheduling strategy</em>, the scheduler should either dedicate a specific <em class="italics">time slice</em> or <em class="italics">time quantum</em> to the task in order to use the processor unit, or alternatively, the scheduler must wait for the task to release the processor unit.</li>
				<li class="list">If the scheduling strategy is <em class="italics">preemptive</em>, the scheduler should be able to forcefully take back the CPU core from the running task in order to give it to the next task. This is called <em class="italics">preemptive scheduling</em>. There is also another scheme in which the task releases the CPU voluntarily, which is called <em class="italics">cooperative scheduling</em>.</li>
				<li class="list">Preemptive scheduling algorithms try to share <em class="italics">time slices</em> evenly and fairly between different tasks. Prioritized tasks may get chosen more frequently, or they may even get longer time slices depending upon the implementation of the scheduler.</li>
			</ul>
			<p class="normal">A task is a general abstract concept, used to refer to any piece of work that should be done in a concurrent system, not necessarily a computer system. We'll look at what exactly these non-computer systems are shortly. Likewise, CPUs are not the only type of resource that can be shared between tasks. Humans have been scheduling and prioritizing tasks for as long as we have existed, when we are faced with tasks that we cannot complete simultaneously. In the next few paragraphs, we will consider such a situation as a good example for understanding scheduling.</p>
			<p class="normal">Let's suppose that we are at the beginning of the twentieth century and there is only one telephone booth in the street, and 10 people are waiting to use the telephone. In this case, these 10 people should follow a scheduling algorithm in order to share the booth fairly between themselves.</p>
			<p class="normal">First of all, they need to stand in a queue. This is the most basic decision that enters the civilized mind in such a situation – to stand in the queue and wait for your turn. However, this alone is not enough; we also need some regulations to support this method. The first person, who is currently using the phone, can't talk as much as they might like to when there are nine other people waiting for the booth. The first person must leave the booth after a certain amount of time in order to allow the next person in the queue their turn.</p>
			<p class="normal">In the rare <a id="_idIndexMarker893"/>case that they have not finished their conversation yet, the first person should stop using the phone after a certain amount of time, leave the booth, and go back to the end of the queue. They must then wait for their next turn so that they can continue their talk. This way, each of the 10 people will need to continue entering the booth, until they have completed their conversation.</p>
			<p class="normal">This is just an example. We encounter examples of sharing resources between a number of consumers every day, and humans have invented many ways to share these resources fairly between themselves – to the extent that human nature allows! In the next section, we return to considering scheduling within the context of a computer system.</p>
			<h1 id="_idParaDest-186" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor352"/>Processes and threads</h1>
			<p class="normal">Throughout this book, we are mainly interested in task scheduling within computer systems. In an operating system, tasks are either <em class="italics">processes</em> or <em class="italics">threads</em>. We'll explain them and their <a id="_idIndexMarker894"/>differences in the upcoming chapters, but for now, you should know that most operating systems treat both in basically the same way: as some tasks that need to <a id="_idIndexMarker895"/>be executed concurrently.</p>
			<p class="normal">An operating system needs to use a task scheduler to share the CPU cores among the many tasks, be they processes or threads, that are willing to use the CPU for their execution. When a new process or a new thread is created, it enters the scheduler queue as a new task, and it waits to obtain a CPU core before it starts running.</p>
			<p class="normal">In cases in which a <em class="italics">time-sharing</em> or <em class="italics">preemptive scheduler</em> is in place, if the task cannot finish its logic in a certain amount of time, then the CPU core will be taken back forcefully by the task scheduler and the task enters the queue again, just like in the telephone booth scenario.</p>
			<p class="normal">In this case, the task should wait in the queue until it obtains the CPU core once more, and then it can continue running. If it cannot finish its logic in the second round, the same process continues until it is able to finish.</p>
			<p class="normal">Every time a preemptive scheduler stops a process in the middle of running and puts another process into the running state, it is said that a <em class="italics">context switch</em> has occurred. The faster the context switches are, the more a user will feel as if the tasks are being run in parallel. Interestingly, most operating systems today use a preemptive scheduler, something that will be our main focus for the rest of this chapter.</p>
			<div>
				<div id="_idContainer103" class="note">
					<p class="Information-Box--PACKT-">From now on, all schedulers are assumed to be preemptive. I will specify in instances where this is not the case.</p>
				</div>
			</div>
			<p class="normal">When a task <a id="_idIndexMarker896"/>is running, it may experience hundreds or even thousands of context switches before being finished. However, context switches have a very bizarre and unique characteristic – they are not <em class="italics">predictable</em>. In other words, we are not able to predict when, or even at which instruction, a context switch is going to happen. Even in two remarkably close successive runs of a program on the same platform, the context switches will happen differently.</p>
			<p class="normal">The importance <a id="_idIndexMarker897"/>of this, and the impact it has, cannot be overstated; context switches cannot be predicted! Shortly, through the given examples, you'll observe the consequences of this for yourself.</p>
			<p class="normal">Context switches are highly unpredictable, to such an extent that the best way to deal with this uncertainty is to assume that the probability of having a context switch on a specific instruction is the same for all instructions. In other words, you should expect that all instructions are subject to experiencing a context switch in any given run. What this means, simply, is that you may have gaps between the execution of any two adjacent instructions.</p>
			<p class="normal">With that being said, let's now move on and take a look at the only certainties that do exist in a concurrent environment.</p>
			<h1 id="_idParaDest-187" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor353"/>Happens-before constraint</h1>
			<p class="normal">We established in the previous section that context switches are not predictable; there is uncertainty about the time at which they are likely to occur in our programs. Despite that, there <a id="_idIndexMarker898"/>is certainty about the instructions that are being executed concurrently.</p>
			<p class="normal">Let's continue with a simple example. To start with, we're going to work on the basis that we've got a task like the one you see next in <em class="italics">Code Box 13-1</em>, which has five instructions. Note that these instructions are abstract, and they don't represent any real instructions like C or machine instructions:</p>
			<p class="snippet code">Task P {</p>
			<p class="snippet code">    1. num = 5</p>
			<p class="snippet code">    2. num++</p>
			<p class="snippet code">    3. num = num – 2</p>
			<p class="snippet code">    4. x = 10</p>
			<p class="snippet code">    5. num = num + x</p>
			<p class="snippet code">}</p>
			<p class="packt_figref">Code Box 13-1: A simple task with 5 instructions</p>
			<p class="normal">As you can see, the instructions are ordered, which means that they <em class="italics">must</em> be executed in that specified order in order to satisfy the purpose of the task. We are certain about this. In technical terms, we say that we have a <em class="italics">happens-before constraint</em> between every two adjacent instructions. The instruction <code class="Code-In-Text--PACKT-">num++</code> must happen before <code class="Code-In-Text--PACKT-">num = num - 2</code> and this <a id="_idIndexMarker899"/>constraint must be kept satisfied no matter how the context switches are happening.</p>
			<p class="normal">Note that we still have uncertainty about when the context switches are going to happen; it's key to remember that they can happen anywhere between the instructions.</p>
			<p class="normal">Here, we are going to present two possible executions of the preceding task, with different context switches:</p>
			<p class="snippet code">Run 1:</p>
			<p class="snippet code">  1. num = 5</p>
			<p class="snippet code">  2. num++</p>
			<p class="snippet code">&gt;&gt;&gt;&gt;&gt; Context Switch &lt;&lt;&lt;&lt;&lt;</p>
			<p class="snippet code">  3. num = num – 2</p>
			<p class="snippet code">  4. x = 10</p>
			<p class="snippet code">&gt;&gt;&gt;&gt;&gt; Context Switch &lt;&lt;&lt;&lt;&lt;</p>
			<p class="snippet code">  5. num = num + x</p>
			<p class="packt_figref">Code Box 13-2: One possible run of the above task together with the context switches</p>
			<p class="normal">And for the second run, it is executed as the following:</p>
			<p class="snippet code">Run 2:</p>
			<p class="snippet code">  num = 5</p>
			<p class="snippet code">  &gt;&gt; Context Switch &lt;&lt;</p>
			<p class="snippet code">  num++</p>
			<p class="snippet code">  num = num – 2</p>
			<p class="snippet code">  &gt;&gt; Context Switch &lt;&lt;</p>
			<p class="snippet code">  x = 10</p>
			<p class="snippet code">  &gt;&gt; Context Switch &lt;&lt;</p>
			<p class="snippet code">  num = num + x</p>
			<p class="packt_figref">Code Box 13-3: Another possible run together with the context switches</p>
			<p class="normal">As you can see in <em class="italics">Code Box 13-2</em>, the number of context switches and the places they occur can both change in each run. Yet, as we said before, there are certain happens-before constraints that should be followed.</p>
			<p class="normal">This is the reason we can have an overall deterministic behavior for a specific task. No matter how context switches happen in different runs, the <em class="italics">overall state</em> of a task remains the same. By the overall state of a task, we mean the set of variables and their corresponding <a id="_idIndexMarker900"/>values after the execution of the last instruction in the task. For example, for the preceding task, we always have the final state, including the <code class="Code-In-Text--PACKT-">num</code> variables with a value of <code class="Code-In-Text--PACKT-">14</code>, and the variable <code class="Code-In-Text--PACKT-">x</code> with a value of <code class="Code-In-Text--PACKT-">10</code>, regardless of the context switches.</p>
			<p class="normal">By knowing that the overall state of a single task does not change in different runs, we might be tempted to conclude that due to having to follow the order of execution and the happens-before constraints, concurrency cannot affect the overall state of a task. Yet, we should be careful about this conclusion.</p>
			<p class="normal">Let's assume that we have a system of concurrent tasks, all having read/write permissions over a <em class="italics">shared resource</em>, say a variable. If all the tasks only read the shared variable and none of them are going to write to it (change its value), we can say that no matter how context switches are happening, and no matter how many times you run the tasks, we always get the same results. Note that this is also true about a system of concurrent tasks that have no shared variable at all.</p>
			<p class="normal">Yet, if just one of the tasks is going to write to the shared variable, then the context switches imposed by the task scheduler unit will affect the overall state of all tasks. This means that it can be different from one run to another! Consequently, a proper control mechanism should be employed to avoid any unwanted results. This is all due to the fact that context switches cannot be predicted, and the tasks' <em class="italics">intermediate states</em> can vary from one run to another. An intermediate state, as opposed to overall state, is a set of variables together with their values at a certain instruction. Every task has only one overall state that is determined when it is finished, but it has numerous intermediate states that correspond to the variables and their values after executing a certain instruction.</p>
			<p class="normal">In summary, when you have a concurrent system containing several tasks with a shared resource that can be written to by any of those tasks, then different runs of the system will yield different results. Hence, proper <em class="italics">synchronization</em> methods should be used in order to cancel <a id="_idIndexMarker901"/>the effect of context switches and obtain the same deterministic results in various runs.</p>
			<p class="normal">We now have some of the basic concepts of concurrency, which is the dominant topic of this chapter. The concepts explained in this section are fundamental to our understanding of many topics, and you will hear them again and again in future sections and chap<a id="_idTextAnchor354"/>ters of this book.</p>
			<p class="normal">You'll remember that we also said concurrency could be problematic and in turn, it can make things more complicated for us. So, you may be asking, when do we need it? In the next section of this chapter, we'll answer that question.</p>
			<h1 id="_idParaDest-188" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor355"/>When to use concurrency</h1>
			<p class="normal">Based on our <a id="_idIndexMarker902"/>explanations given so far, it seems that having only one task is less problematic than having multiple tasks do the same thing concurrently. This is quite right; if you can write a program that runs acceptably without introducing concurrency, it is highly recommended that you do so. There are some general patterns we can use to know when we have to use concurrency.</p>
			<p class="normal">In this section, we are going to walk through what these general patterns are, and how they lead us to split a program into concurrent flows.</p>
			<p class="normal">A program, regardless of the programming language used, is simply a set of instructions that should be executed in sequence. In other words, a given instruction won't be executed until the preceding instruction has been executed. We call this concept a <em class="italics">sequential execution</em>. It doesn't matter how long the current instruction takes to finish; the next instruction must wait until the current one has been completed. It is usually said that the current instruction is <em class="italics">blocking</em> the next instruction; this is sometimes described as the current instruction being a <em class="italics">blocking instruction</em>.</p>
			<p class="normal">In every program, all of the instructions are blocking, and the execution flow is sequential in each flow of execution. We can only say a program is running quickly if each instruction blocks the following instruction for a relatively short time in terms of a few milliseconds. Yet, what happens if a blocking instruction takes too much time (for example 2 seconds or 2000 milliseconds), or the time that it takes cannot be determined? These are two patterns that tell us we need to have a concurrent program.</p>
			<p class="normal">To elaborate further, every blocking instruction consumes an amount of time when trying to get completed. For us, the best scenario is that a given instruction takes a relatively short time to complete and after that, the next instruction can be executed immediately. However, we are not always so fortunate.</p>
			<p class="normal">There are certain scenarios where we cannot determine the time that a blocking instruction takes to complete. This usually happens when a blocking instruction is waiting either for a certain event to occur, or for some data to become available.</p>
			<p class="normal">Let's continue <a id="_idIndexMarker903"/>with an example. Suppose that we have a server program that is serving a number of client programs. There is an instruction in the server program that waits for a client program to get connected. From the server program's point of view, no one can say for sure when a new client is about to connect. Therefore, the next instruction cannot be executed on the server side because we don't know when we will be done with the current one. It depends entirely on the time at which a new client tries to connect.</p>
			<p class="normal">A simpler example is when you read a string from the user. From the program's point of view, no one can say for sure when the user will enter their input; hence, future instructions cannot be executed. This is the <em class="italics">first pattern</em> that leads to a concurrent system of tasks.</p>
			<p class="normal">The first pattern for concurrency then, is when you have an instruction that can block the flow of execution for an indefinite amount of time. At this point you should split the existing flow into two separate flows or tasks. You would do this if you need to have the later instructions being executed, and you cannot wait for the current instruction to complete first. More importantly for this scenario, we assume that the later instructions are not dependent on the result of the current instructions being completed.</p>
			<p class="normal">By splitting our preceding flow into two concurrent tasks, while one of the tasks is waiting for the blocking instruction to complete, the other task can continue and execute those instructions that were blocked in the<a id="_idTextAnchor356"/> preceding non-concurrent setup.</p>
			<p class="normal">The following example that we're going to focus on in this section shows how the first pattern can result in a system of concurrent tasks. We will be using pseudo-code to represent the instructions in each task.</p>
			<div>
				<div id="_idContainer104" class="note">
					<p class="Information-Box--PACKT-"><strong class="screen-text">Note</strong>:</p>
					<p class="Information-Box--PACKT-">No prior knowledge of computer networks is needed to understand the upcoming example.</p>
				</div>
			</div>
			<p class="normal">The example we're going to focus on is about a server program that has three objectives:</p>
			<ul>
				<li class="list">It calculates <a id="_idIndexMarker904"/>the sum of two numbers read from a client and returns the result back to the client.</li>
				<li class="list">It writes the number of served clients to a file regularly, regardless of whether any client is being served or not.</li>
				<li class="list">It must also be able to serve multiple clients at once.</li>
			</ul>
			<p class="normal">Before talking <a id="_idIndexMarker905"/>about the final concurrent system that satisfies preceding objectives, let's first suppose that in this example we are going to use only one task (or flow) and then we are going to show that a single task cannot accomplish the preceding objectives. You can see the pseudo-code for the server program, in a single-task setup, in <em class="italics">Code Box 13-4</em>:</p>
			<p class="snippet code">Calculator Server {</p>
			<p class="snippet code">    Task T1 {</p>
			<p class="snippet code">        1. N = 0</p>
			<p class="snippet code">        2. Prepare Server</p>
			<p class="snippet code">        3. Do Forever {</p>
			<p class="snippet code">        4.     Wait for a client C</p>
			<p class="snippet code">        5.     N = N + 1</p>
			<p class="snippet code">        6.     Read the first number from C and store it in X</p>
			<p class="snippet code">        7.     Read the second number from C and store it in Y</p>
			<p class="snippet code">        8.     Z = X + Y</p>
			<p class="snippet code">        9.     Write Z to C</p>
			<p class="snippet code">       10.     Close the connection to C</p>
			<p class="snippet code">       11.     Write N to file</p>
			<p class="snippet code">           }</p>
			<p class="snippet code">    }</p>
			<p class="snippet code">}</p>
			<p class="packt_figref">Code Box 13-4: A server program operating using a single task</p>
			<p class="normal">As you can see, our single flow waits for a client on the network to get connected. It then reads two numbers from the client, then calculates their sum and returns it to the client. Finally, it closes the client connection and writes the number of served clients to a file before continuing to wait for the next client to join in. Shortly, we'll show that the preceding code cannot satisfy our aforementioned objectives.</p>
			<p class="normal">This pseudo-code contains only one task, <code class="Code-In-Text--PACKT-">T1</code>. It has 12 lines of instructions, and as we've said before, they are executed sequentially, and all the instructions are blocking. So, what exactly is this code showing us? Let's walk through it:</p>
			<ul>
				<li class="list">The first instruction, <code class="Code-In-Text--PACKT-">N = 0</code>, is simple and finishes very quickly.</li>
				<li class="list">The second instruction, <code class="Code-In-Text--PACKT-">Prepare Server</code>, is expected to finish in a reasonable time so that it won't block the execution of the server program.</li>
				<li class="list">The third instruction is just starting the main loop and it should finish quickly as we proceed to go inside the loop.</li>
				<li class="list">The fourth command, <code class="Code-In-Text--PACKT-">Wait for a client C</code>, is a blocking instruction with an unknown completion time. Therefore, commands <em class="italics">5</em>, <em class="italics">6</em>, and the rest won't be executed. Hence, it seems that they must wait for a new client to join in, and only after that, these instructions can be executed.</li>
			</ul>
			<p class="normal">As we said before, having instructions <em class="italics">5</em> to <em class="italics">10</em> wait for a new client is must. In other words, those instructions are dependent on the output of instruction <em class="italics">4</em> and they cannot be executed <a id="_idIndexMarker906"/>without having a client accepted. However, instruction <em class="italics">11</em>, <code class="Code-In-Text--PACKT-">Write N to file</code>, needs to be executed regardless of having a client or not. This is dictated by the second objective that we've defined for this example. By the preceding configuration, we write <code class="Code-In-Text--PACKT-">N</code> to file only if we have a client, despite this being against our initial requirement, that is, we write <code class="Code-In-Text--PACKT-">N</code> to file <em class="italics">regardless</em> of whether we have a client or not.</p>
			<p class="normal">The preceding code has another problem in its flow of instructions; both instructions <em class="italics">6</em> and <em class="italics">7</em> can potentially block the flow of execution. These instructions wait for the client to enter two numbers, and since this is up to the client we cannot predict exactly when these instructions are going to finish. This prevents the program from continuing its execution.</p>
			<p class="normal">More than that, these instructions potentially block the program from accepting new clients. This is because the flow of executions won't reach the command <em class="italics">4</em> again, if commands <em class="italics">6</em> and <em class="italics">7</em> are going to take a long time to complete. Therefore, the server program cannot serve multiple clients at once, which is again not in accordance with our defined objectives.</p>
			<p class="normal">To resolve the aforementioned issues, we need to break our single task into three concurrent tasks that together will satisfy our requirements for the server program.</p>
			<p class="normal">In the following pseudo-code in <em class="italics">Code Box 13-5</em>, you will find three flows of execution, <code class="Code-In-Text--PACKT-">T1</code>, <code class="Code-In-Text--PACKT-">T2</code>, and <code class="Code-In-Text--PACKT-">T3</code>, that satisfy our defined objectives based on a concurrent solution:</p>
			<p class="snippet code">Calculator Server {</p>
			<p class="snippet code">    Shared Variable: N</p>
			<p class="snippet code">    Task T1 {</p>
			<p class="snippet code">        1. N = 0</p>
			<p class="snippet code">        2. Prepare Server</p>
			<p class="snippet code">        3. Spawn task T2</p>
			<p class="snippet code">        4. Do Forever {</p>
			<p class="snippet code">        5.     Write N to file</p>
			<p class="snippet code">        6.     Wait for 30 seconds</p>
			<p class="snippet code">           }</p>
			<p class="snippet code">    }</p>
			<p class="snippet code">    Task T2 {</p>
			<p class="snippet code">        1. Do Forever {</p>
			<p class="snippet code">        2.     Wait for a client C</p>
			<p class="snippet code">        3.     N = N + 1</p>
			<p class="snippet code">        4.     Spawn task T3 for C</p>
			<p class="snippet code">           }</p>
			<p class="snippet code">    }</p>
			<p class="snippet code">    Task T3 {</p>
			<p class="snippet code">        1. Read first number from C and store it in X</p>
			<p class="snippet code">        2. Read first number from C and store it in Y</p>
			<p class="snippet code">        3. Z = X + Y</p>
			<p class="snippet code">        4. Write Z to C</p>
			<p class="snippet code">        5. Close the connection to C</p>
			<p class="snippet code">    }</p>
			<p class="snippet code">}</p>
			<p class="packt_figref">Code Box 13-5: A server program operating using three concurrent tasks</p>
			<p class="normal">The program starts by executing task <code class="Code-In-Text--PACKT-">T1</code>. <code class="Code-In-Text--PACKT-">T1</code> is said to be the main task of the program because it is the first task that is going to be executed. Take note that each program has at least one <a id="_idIndexMarker907"/>task and that all other tasks are initiated by this task, either directly or indirectly.</p>
			<p class="normal">In the preceding code box, we have two other tasks that are spawned by the main task, <code class="Code-In-Text--PACKT-">T1</code>. There is also a shared variable, <code class="Code-In-Text--PACKT-">N</code>, which stores the number of served clients and can be accessed (read or written) by all the tasks.</p>
			<p class="normal">The program starts with the first instruction in task <code class="Code-In-Text--PACKT-">T1</code>; through this, it initializes the variable <code class="Code-In-Text--PACKT-">N</code> to zero. Then the second instruction prepares the server. As part of this instruction, some preliminary steps should be taken in order for the server program to be able to accept the incoming connections. Note that so far there hasn't been any other concurrent task running next to task <code class="Code-In-Text--PACKT-">T1</code>.</p>
			<p class="normal">The third instruction in task <code class="Code-In-Text--PACKT-">T1</code> creates a new <em class="italics">instance</em> of task <code class="Code-In-Text--PACKT-">T2</code>. The creation of a new task is usually fast and takes no time. Therefore, task <code class="Code-In-Text--PACKT-">T1</code> enters the infinite loop immediately after the creation of task <code class="Code-In-Text--PACKT-">T2</code>, where it continues to write the value of the shared variable <code class="Code-In-Text--PACKT-">N</code> to a file every 30 seconds. This was our first objective defined for the server program that has now been satisfied. Based on that, without having any interruption or blockage from other instructions, task <code class="Code-In-Text--PACKT-">T1</code> writes the value of <code class="Code-In-Text--PACKT-">N</code> to a file regularly, until the program finishes.</p>
			<p class="normal">Let's talk about <a id="_idIndexMarker908"/>the spawned task. The sole responsibility of task <code class="Code-In-Text--PACKT-">T2</code> is to accept the incoming clients as soon as they send the connection request. It's also worth remembering that all the instructions in task <code class="Code-In-Text--PACKT-">T2</code> are run inside an infinite loop. The second command in task <code class="Code-In-Text--PACKT-">T2</code> waits for a new client. Here, it blocks other instructions in task <code class="Code-In-Text--PACKT-">T2</code> from executing, but this is only applied to the instructions in task <code class="Code-In-Text--PACKT-">T2</code>. Note that if we had spawned two instances of task <code class="Code-In-Text--PACKT-">T2</code> instead of one, having instructions blocked in one of them would not block the instructions in the other instance.</p>
			<p class="normal">Other concurrent tasks, in this case only <code class="Code-In-Text--PACKT-">T1</code>, continue to execute their instructions without any blockage. This is what concurrency enables; while some tasks are blocked for a certain event, other tasks can continue their work without any interruption. As we said before, this has an important design principle at its core: <em class="italics">Whenever you have a blocking operation where either its finishing time is unknown, or it takes a long time to complete, then you should break the task into two concurrent tasks</em>.</p>
			<p class="normal">Now, suppose that a new client joins. We've already seen in <em class="italics">Code Box 13-4</em>, in the non-concurrent version of the server program, that the read operations could block the acceptance of new clients. Based on the design principle that we pointed out just now, since the read instructions are blocking, we need to break the logic into two concurrent tasks, which is why we have introduced task <code class="Code-In-Text--PACKT-">T3</code>.</p>
			<p class="normal">Whenever a new client joins, task <code class="Code-In-Text--PACKT-">T2</code> spawns a new instance of task <code class="Code-In-Text--PACKT-">T3</code> in order to communicate with the newly joined client. This was done by instruction <em class="italics">4</em> in task <code class="Code-In-Text--PACKT-">T2</code>, which, to remind you, was the following command:</p>
			<p class="snippet code">4.     Spawn task T3 for C</p>
			<p class="packt_figref">Code Box 13-6: Instruction 4 in task T2</p>
			<p class="normal">It's important to note that before spawning a new task, task <code class="Code-In-Text--PACKT-">T2</code> increments the value of the shared variable <code class="Code-In-Text--PACKT-">N</code> as an indication of having a new client served. Again, a spawn instruction is fairly quick and doesn't block the acceptance of new clients.</p>
			<p class="normal">In task <code class="Code-In-Text--PACKT-">T2</code>, when instruction <em class="italics">4</em> is finished, the loop continues, and it goes back to instruction <em class="italics">2</em>, which waits for another client to join. Note that based on the pseudo-code that we have, while we have only one instance of task <code class="Code-In-Text--PACKT-">T1</code> and one instance of task <code class="Code-In-Text--PACKT-">T2</code>, we can have multiple instances of <code class="Code-In-Text--PACKT-">T3</code> for every client.</p>
			<p class="normal">The sole responsibility of task <code class="Code-In-Text--PACKT-">T3</code> is to communicate to the client and read the input numbers. It then continues by calculating the sum and sending it back to the client. As pointed out before, the blocking instructions inside task <code class="Code-In-Text--PACKT-">T3</code> cannot block the execution of other tasks, and its blocking behavior is limited to the same instance of <code class="Code-In-Text--PACKT-">T3</code>. Even the blocking instructions in a specific instance of <code class="Code-In-Text--PACKT-">T3</code> cannot block the instructions in another instance of <code class="Code-In-Text--PACKT-">T3</code>. This way, the server program can satisfy all of our desired objectives in a concurrent way.</p>
			<p class="normal">So, the next <a id="_idIndexMarker909"/>question might be, when do the tasks finish? We know that generally, when all the instructions within a task are executed, the task is finished. That being said, when we have an infinite loop wrapping all instructions inside a task, the task won't finish, and its lifetime is dependent on its <em class="italics">parent task</em> that has spawned it. We will discuss this specifically regarding processes and threads in future chapters. For the sake of our example, in our preceding concurrent program the parent task of all instances of <code class="Code-In-Text--PACKT-">T3</code> is the only instance of task <code class="Code-In-Text--PACKT-">T2</code>. As you can see, a specific instance of task <code class="Code-In-Text--PACKT-">T3</code> finishes either when it closes the connection to the client after passing two blocking read instructions or when the only instance of task <code class="Code-In-Text--PACKT-">T2</code> is finished.</p>
			<p class="normal">In a rare but possible scenario, if all read operations take too much time to complete (and this can be either intentional or accidental), and the number of incoming clients increases rapidly, then there should be a moment where we have too many instances of task <code class="Code-In-Text--PACKT-">T3</code> running and all of them are waiting for their clients to provide their input numbers. This situation would result in consuming a considerable amount of resources. Then, after some time, by having more and more incoming connections, either the server program would be terminated by the operating system, or it simply cannot serve any more clients.</p>
			<p class="normal">Whatever happens in the preceding case, the server program ceases to serve the clients. When this occurs, it's called a <strong class="bold">denial of service</strong> (<strong class="bold">DoS</strong>). Systems with concurrent tasks should be designed in such a way to overcome these extreme situations that stop them from serving clients in a reasonable fashion.</p>
			<div>
				<div id="_idContainer105" class="note">
					<p class="Information-Box--PACKT-"><strong class="screen-text">Note</strong>:</p>
					<p class="Information-Box--PACKT-">When under a DoS attack, congestion of resources on a server machine occurs in order to bring it down and make it non-responsive. DoS attacks belong to a group of network attacks that try to block a certain service in order to make it unavailable to its clients. They cover a wide range of attacks, including <em class="italics">exploits</em>, with the intention of stopping a service. This can even include the <em class="italics">flooding</em> of a network in order to bring down the network infrastructure.</p>
				</div>
			</div>
			<p class="normal">In the preceding example of the server program, we described a situation in which we had a blocking instruction whose completion time could not be determined, and this was the first pattern for the use of concurrency. There is another pattern that is similar to this, but slightly different.</p>
			<p class="normal">If an instruction <a id="_idIndexMarker910"/>or a group of instructions take too much time to complete, then we can put them in a separate task and run the new task concurrent to the main task. This is different from the first pattern because, while we do have an estimate of the completion time, albeit not a very accurate one, we do know that it won't be soon.</p>
			<p class="normal">The last thing to note about the preceding example, regarding the shared variable, <code class="Code-In-Text--PACKT-">N</code>, is that one of the tasks, specifically the instance of task <code class="Code-In-Text--PACKT-">T2</code>, could change its value. Based on our previous discussions in this chapter, this system of concurrent tasks is therefore prone to concurrency problems because of it having a shared variable that can be modified by one of the tasks.</p>
			<p class="normal">It's important to note that the solution we proposed for the server program is far from perfect. In the next chapter, you'll be introduced to concurrency issues, and through it you will see that the preceding example suffers from a serious <em class="italics">data race</em> issue over the shared variable, <code class="Code-In-Text--PACKT-">N</code>. As a result, proper control mechanisms should be employed to resolve the issues created by concurrency.</p>
			<p class="normal">In the following and final section in this chapter, we are going to talk about the <em class="italics">states</em> that are shared between some concurrent tasks. We will also introduce <a id="_idTextAnchor357"/>the concept of <em class="italics">interleaving</em> and its important consequences for a concurrent system with a modifiable shared state.</p>
			<h1 id="_idParaDest-189" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor358"/>Shared states</h1>
			<p class="normal">In the previous section, we talked about the patterns suggesting that we require a concurrent system of tasks. Before that, we also briefly explained how the uncertainty in the pattern <a id="_idIndexMarker911"/>of context switches during the execution of a number of concurrent tasks, together with having a modifiable shared state, can lead to non-determinism occurring in the overall states of all tasks. This section provides an example to demonstrate how this non-determinism can be problematic in a simple program.</p>
			<p class="normal">In this section, we are going to continue our discussion and bring in <em class="italics">shared states</em> to see how they contribute to the non-determinism we talked about. As a programmer, the term <em class="italics">state</em> should remind you of a set of variables and their corresponding values at a specific time. Therefore, when we are talking about the <em class="italics">overall state</em> of a task, as we defined it in the first section, we are referring to the set of all existing non-shared variables, together with their corresponding values, at the exact moment when the last instruction of the task has been executed.</p>
			<p class="normal">Similarly, an <em class="italics">intermediate state</em> of a task is the set of all existing non-shared variables, together with their values when the task has executed a certain instruction. Therefore, a task has a different intermediate state for each of its instructions, and the number of intermediate states is equal to the number of instructions. According to our definitions, the last intermediate state is the same as the overall state of the task.</p>
			<p class="normal">A shared state <a id="_idIndexMarker912"/>is also a set of variables together with their corresponding values at a specific time which can be read or modified by a system of concurrent tasks. A shared state is not owned by a task (it is not local to a task), and it can be read or modified by any of the tasks running in the system, and of course at any time.</p>
			<p class="normal">Generally, we are not interested in shared states that are read-only. They are usually safe to be read by many concurrent tasks, and they don't yield any problem. However, a shared state that is modifiable usually yields some serious problems if it is not protected carefully. Therefore, all the shared states covered by this section are considered to be modifiable by at least by one of the tasks.</p>
			<p class="normal">Ask yourself this question: what can go wrong if a shared state is modified by one of the concurrent tasks in a system? To answer this, we start by giving an example of a system of two concurrent tasks accessing a single shared variable, which, in this case, is a simple integer variable.</p>
			<p class="normal">Let's suppose that we have the following system as displayed in <em class="italics">Code Box 13-7</em>:</p>
			<p class="snippet code">Concurrent System {</p>
			<p class="snippet code">    Shared State {</p>
			<p class="snippet code">        X : Integer = 0</p>
			<p class="snippet code">    }</p>
			<p class="snippet code">    </p>
			<p class="snippet code">    Task P {</p>
			<p class="snippet code">        A : Integer</p>
			<p class="snippet code">            1. A = X</p>
			<p class="snippet code">            2. A = A + 1</p>
			<p class="snippet code">            3. X = A</p>
			<p class="snippet code">            4. print X</p>
			<p class="snippet code">    }</p>
			<p class="snippet code">    Task Q {</p>
			<p class="snippet code">        B : Integer</p>
			<p class="snippet code">            1. B = X</p>
			<p class="snippet code">            2. B = B + 2</p>
			<p class="snippet code">            3. X = B</p>
			<p class="snippet code">            4. print X</p>
			<p class="snippet code">    }</p>
			<p class="snippet code">}</p>
			<p class="packt_figref">Code Box 13-7: A system of two concurrent tasks with a modifiable shared state</p>
			<p class="normal">Suppose that in the preceding system, tasks <code class="Code-In-Text--PACKT-">P</code> and <code class="Code-In-Text--PACKT-">Q</code> are not concurrently run. Therefore, they become executed sequentially. Suppose that the instructions in <code class="Code-In-Text--PACKT-">P</code> are executed first, before <code class="Code-In-Text--PACKT-">Q</code>. If that was the case, then the overall state of the whole system, regardless of the overall state of any individual task, would be the shared variable, <code class="Code-In-Text--PACKT-">X</code>, with a value of 3.</p>
			<p class="normal">If you run <a id="_idIndexMarker913"/>the system in reverse order, first the instructions in <code class="Code-In-Text--PACKT-">Q</code> and then the instructions in <code class="Code-In-Text--PACKT-">P</code>, you will get the same overall state. However, this is not usually the case and running two different tasks in a reversed order probably leads to a different overall state.</p>
			<p class="normal">As you can see, running these tasks sequentially produces a deterministic result without worrying about context switches.</p>
			<p class="normal">Now, suppose that they are run concurrently on the same CPU core. There are many possible scenarios for putting the instructions of <code class="Code-In-Text--PACKT-">P</code> and <code class="Code-In-Text--PACKT-">Q</code> into execution by considering various context switches occurring at various instructions.</p>
			<p class="normal">The following is a possible scenario:</p>
			<p class="snippet code">     Task P     |    Task Scheduler   |    Task Q    </p>
			<p class="snippet code">----------------------------------------------------</p>
			<p class="snippet code">                |    Context Switch   |             </p>
			<p class="snippet code">                |                     |  B = X</p>
			<p class="snippet code">                |                     |  B = B + 2</p>
			<p class="snippet code">                |    Context Switch   |</p>
			<p class="snippet code">  A = X         |                     |</p>
			<p class="snippet code">                |    Context Switch   |</p>
			<p class="snippet code">                |                     |  X = B</p>
			<p class="snippet code">                |    Context Switch   |</p>
			<p class="snippet code">  A = A + 1     |                     |</p>
			<p class="snippet code">  X = A         |                     |</p>
			<p class="snippet code">                |    Context Switch   |</p>
			<p class="snippet code">                |                     |  print X</p>
			<p class="snippet code">                |    Context Switch   |</p>
			<p class="snippet code">  print X       |                     |</p>
			<p class="snippet code">                |    Context Switch   |</p>
			<p class="packt_figref">Code Box 13-8: A possible interleaving of tasks P and Q when run concurrently</p>
			<p class="normal">This scenario is only one of many possible scenarios with context switches happening at certain places. Each scenario is called an <em class="italics">interleaving</em>. So, for a system of concurrent tasks, there are a number of possible interleavings based on the various places that context switches can happen, and in each run only one of these many interleavings will happen. This, as a result, makes them unpredictable.</p>
			<p class="normal">For the preceding interleaving, as you can see in the first and last column, the order of instructions and happens-before constraints are preserved, but there could be <em class="italics">gaps</em> between the executions. These gaps are not predictable, and as we trace the execution, the preceding interleaving leads to a surprising result. Process <code class="Code-In-Text--PACKT-">P</code> prints the value <code class="Code-In-Text--PACKT-">1</code> and process <code class="Code-In-Text--PACKT-">Q</code> prints the value <code class="Code-In-Text--PACKT-">2</code>, yet it was expected that both of them would print <code class="Code-In-Text--PACKT-">3</code> as their final result.</p>
			<p class="normal">Note that in <a id="_idIndexMarker914"/>the preceding example, the constraint for accepting the final result has been defined like this – the program should print two <code class="Code-In-Text--PACKT-">3</code>s in the output. This constraint could be something else, and independent of the visible output of the program. More than that, there exist other critical constraints that should remain <em class="italics">invariant</em> when facing unpredictable context switches. These could include not having any <em class="italics">data race</em> or <em class="italics">race condition</em>, having no memory leak at all, or even not to crash. All of these constraints are far more important than the visible output of the program. In many real applications, a program doesn't even have an output at all.</p>
			<p class="normal">The following in <em class="italics">Code Box 13-9</em> is another interleaving with a different result:</p>
			<p class="snippet code">   Task P    |    Task Scheduler   |    Task Q</p>
			<p class="snippet code">-------------------------------------------------</p>
			<p class="snippet code">             |    Context Switch   |</p>
			<p class="snippet code">             |                     |    B = X</p>
			<p class="snippet code">             |                     |    B = B + 2</p>
			<p class="snippet code">             |                     |    X = B</p>
			<p class="snippet code">             |    Context Switch   |</p>
			<p class="snippet code"> A = X       |                     |</p>
			<p class="snippet code"> A = A + 1   |                     |</p>
			<p class="snippet code">             |    Context Switch   |</p>
			<p class="snippet code">             |                     |    print X</p>
			<p class="snippet code">             |    Context Switch   |</p>
			<p class="snippet code"> X = A       |                     |</p>
			<p class="snippet code"> print X     |                     |</p>
			<p class="snippet code">             |    Context Switch   |</p>
			<p class="packt_figref">Code Box 13-9: Another possible interleaving of tasks P and Q when run concurrently</p>
			<p class="normal">In this interleaving, task <code class="Code-In-Text--PACKT-">P</code> prints <code class="Code-In-Text--PACKT-">3</code>, but task <code class="Code-In-Text--PACKT-">Q</code> prints <code class="Code-In-Text--PACKT-">2</code>. This occurs due to the fact that task <code class="Code-In-Text--PACKT-">P</code> hasn't been lucky enough to update the value of the shared variable <code class="Code-In-Text--PACKT-">X</code> before the third context switch. Therefore, task <code class="Code-In-Text--PACKT-">Q</code> just printed the value of <code class="Code-In-Text--PACKT-">X</code>, which was <code class="Code-In-Text--PACKT-">2</code> at that moment. This condition is called a <em class="italics">data race</em> over the variable <code class="Code-In-Text--PACKT-">X</code>, and we explain this further in the upcoming chapter.</p>
			<p class="normal">In a real C program, we usually write <code class="Code-In-Text--PACKT-">X++</code> or <code class="Code-In-Text--PACKT-">X = X + 1</code> instead of firstly copying <code class="Code-In-Text--PACKT-">X</code> into <code class="Code-In-Text--PACKT-">A</code> and then incrementing <code class="Code-In-Text--PACKT-">A</code>, and finally putting it back into <code class="Code-In-Text--PACKT-">X</code>. You will see an example of this in <em class="italics">Chapter 15</em>, <em class="italics">Thread Execution</em>.</p>
			<p class="normal">This clearly shows that a simple <code class="Code-In-Text--PACKT-">X++</code> statement in C consists of three smaller instructions that won't be executed in a single time slice. In other words, it is not an <em class="italics">atomic instruction</em>, but it has been made up of three smaller atomic instructions. An atomic instruction cannot be broken down into smaller operations <a id="_idIndexMarker915"/>and it cannot be interrupted by context switches. We will see more of this in later chapters regarding multithreading.</p>
			<p class="normal">There is another thing to consider regarding the preceding example. In the preceding example, the tasks <code class="Code-In-Text--PACKT-">P</code> and <code class="Code-In-Text--PACKT-">Q</code> were not the only running tasks in the system; there were also other tasks being executed concurrently to our tasks <code class="Code-In-Text--PACKT-">P</code> and <code class="Code-In-Text--PACKT-">Q</code>, but we didn't consider them in our analysis, and we only discussed those two tasks. Why is that?</p>
			<p class="normal">The answer to this question relies on the fact that the different interleavings between any of these two tasks and the other tasks in the system could not change the intermediate states of the task <code class="Code-In-Text--PACKT-">P</code> or <code class="Code-In-Text--PACKT-">Q</code>. In other words, the other tasks have no shared state with <code class="Code-In-Text--PACKT-">P</code> and <code class="Code-In-Text--PACKT-">Q</code>, and as we have explained before, when there are no shared resources between some tasks, interleavings won't matter, as we can see in this case. Therefore, we could assume that there are no other tasks besides <code class="Code-In-Text--PACKT-">P</code> and <code class="Code-In-Text--PACKT-">Q</code> in our hypothetical system.</p>
			<p class="normal">The only effect that the other tasks have upon <code class="Code-In-Text--PACKT-">P</code> and <code class="Code-In-Text--PACKT-">Q</code> is that, if there are too many of them, they can make <code class="Code-In-Text--PACKT-">P</code> and <code class="Code-In-Text--PACKT-">Q</code>'s execution slower. That's simply a result of having long gaps between two successive instructions in <code class="Code-In-Text--PACKT-">P</code> or <code class="Code-In-Text--PACKT-">Q</code>. In other words, the CPU core needs to be shared among more tasks. Therefore, tasks <code class="Code-In-Text--PACKT-">P</code> and <code class="Code-In-Text--PACKT-">Q</code> would need to wait in the queue more often than normally, delaying their execution.</p>
			<p class="normal">Using this example, you saw how even a single shared state between only two concurrent tasks could lead to a lack of determinism in the overall result. We have shown the problems associated with a lack of determinism; we don't want to have a program that yields to a different result in each run. The tasks in our example were relatively simple, containing four trivial instructions, but real concurrent applications that are present in the production environment are much more complex than this.</p>
			<p class="normal">More than that, we have various kinds of shared resources that don't necessarily reside in the memory, such as files or services that are available on the network.</p>
			<p class="normal">Likewise, the number of tasks trying to access a shared resource can be high, and therefore we need to study concurrency issues in a deeper sense and find mechanisms to bring back determinism. In the next chapter, we'll continue our discussions by talking about concurrency issues and the solutio<a id="_idTextAnchor359"/>ns to fix them.</p>
			<p class="normal">Before finishing <a id="_idIndexMarker916"/>this chapter, let's briefly talk about the task scheduler and how it works. If we only have one CPU core, then, at any given moment, we can only have one task using that CPU core.</p>
			<p class="normal">We also know that the task scheduler itself is a piece of program that needs a slice of the CPU core to be executed. So, how does it manage different tasks in order to use the CPU core when another task is using it? Let's suppose that the task scheduler itself is using the CPU core. Firstly, it selects a task from its queue before it sets a timer for a <em class="italics">timer interrupt</em> to happen, and it then leaves the CPU core and gives its resources over to the selected task. </p>
			<p class="normal">Now that we have assumed that the task scheduler will give each task a certain amount of time, there is a time that the interrupt will act, and the CPU core stops the execution of the current task and immediately loads the task scheduler back into the CPU. Now, the scheduler stores the latest status of the previous task and loads the next one from the queue. All of this goes on until the kernel is up and running. Regarding a machine having a CPU with multiple cores, this can change, and the kernel can use various cores while scheduling the tasks for other cores.</p>
			<p class="normal">In this section, we briefly went over the concept of shared states and the way they participate in concurrent systems. The discussions will be continued in the next chapter by talking about concurrency issues and synchronization techniques.</p>
			<h1 id="_idParaDest-190" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor360"/>Summary</h1>
			<p class="normal">In this chapter, we went through the basics of concurrency, and the essential concepts and terminology that you need to know in order to understand the upcoming topics of multithreading and multi-processing.</p>
			<p class="normal">Specifically, we discussed the following:</p>
			<ul>
				<li class="list">The definitions of concurrency and parallelism – the fact that each parallel task needs to have its own processor unit, while concurrent tasks can share a single processor.</li>
				<li class="list">Concurrent tasks use a single processor unit while a task scheduler manages the processor time and shares it between different tasks. This will lead to a number context switches and different interleavings for each task.</li>
				<li class="list">An introduction to blocking instructions. We also explained the patterns that suggest when we require concurrency, and the way we could break a single task into two or three concurrent tasks.</li>
				<li class="list">We described what a shared state is. We also showed how a shared state could lead to serious concurrency issues like data races when multiple tasks try to read and write the same shared state.</li>
			</ul>
			<p class="normal">In the following chapter, we complete our discussion on the topic of concurrency, and we explain the several types of issues that you will experience in a concurrent environment. Talking about the solutions to concurrency-related issues will also be a part of our discussions in the following chapter.</p>
		</div>
</body></html>