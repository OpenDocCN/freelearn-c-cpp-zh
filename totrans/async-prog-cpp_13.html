<html><head></head><body>
  <div><h1 class="chapter-number" id="_idParaDest-268">
    <a id="_idTextAnchor267">
    </a>
    
     13
    
   </h1>
   <h1 id="_idParaDest-269">
    <a id="_idTextAnchor268">
    </a>
    
     Improving Asynchronous Software Performance
    
   </h1>
   <p>
    
     In this chapter, we’ll introduce the performance aspects of asynchronous code.
    
    
     Code performance and optimization is a deep and complex subject, and we can’t cover everything in just one chapter.
    
    
     We aim to give you a good introduction to the subject with some examples of how to measure performance and optimize
    
    
     
      your code.
     
    
   </p>
   <p>
    
     This chapter will cover the following
    
    
     
      key topics:
     
    
   </p>
   <ul>
    <li>
     
      Performance measurement tools with a focus on
     
     
      
       multithreaded applications
      
     
    </li>
    <li>
     
      What’s false sharing, how to spot it, and how to fix/improve
     
     
      
       our code
      
     
    </li>
    <li>
     
      An introduction to modern CPUs’ memory
     
     
      
       cache architecture
      
     
    </li>
    <li>
     
      A review of
     
     <a id="_idIndexMarker996">
     </a>
     
      the
     
     <strong class="bold">
      
       single-producer-single-consumer
      
     </strong>
     
      (
     
     <strong class="bold">
      
       SPSC
      
     </strong>
     
      ) lock-free queue we implemented in
     
     <a href="B22219_05.xhtml#_idTextAnchor097">
      
       <em class="italic">
        
         Chapter 5
        
       </em>
      
     </a>
    </li>
   </ul>
   <h1 id="_idParaDest-270">
    <a id="_idTextAnchor269">
    </a>
    
     Technical requirements
    
   </h1>
   <p>
    
     Like in the previous chapters, you’ll need a modern C++ compiler that supports C++20.
    
    
     We’ll be using GCC 13 and Clang 18.
    
    
     You’ll also need a PC with an Intel/AMD multicore CPU running Linux.
    
    
     For this chapter, we used Ubuntu 24.04 LTS running on a workstation with a CPU AMD Ryzen Threadripper Pro 5975WX (32 cores).
    
    
     A CPU with 8 cores is ideal but 4 cores is enough to run
    
    
     
      the examples.
     
    
   </p>
   <p>
    
     We’ll also be using the Linux
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     tool.
    
    
     We’ll explain how to get and install these tools later in
    
    
     
      this book.
     
    
   </p>
   <p>
    
     The examples for this chapter can be found in this book’s GitHub
    
    
     
      repository:
     
    
    <a href="https://github.com/PacktPublishing/Asynchronous-Programming-with-CPP">
     
      
       https://github.com/PacktPublishing/Asynchronous-Programming-with-CPP
      
     
    </a>
    
     
      .
     
    
   </p>
   <h1 id="_idParaDest-271">
    <a id="_idTextAnchor270">
    </a>
    
     Performance measurement tools
    
   </h1>
   <p>
    
     To learn about the
    
    <a id="_idIndexMarker997">
    </a>
    
     performance of our applications, we need to be able to measure it.
    
    
     If there’s one key takeaway from this chapter, it’s to
    
    <em class="italic">
     
      never estimate or guess your code performance
     
    </em>
    
     .
    
    
     To know whether your program meets its performance requirements (either latency or throughput), you need to measure, measure, and then
    
    
     
      measure again.
     
    
   </p>
   <p>
    
     Once you have the data from your performance tests, you’ll know the hotspots in your code.
    
    
     Maybe they’re related to memory access patterns or thread contention (such as, for example, when multiple threads must wait to acquire a lock to access a resource).
    
    
     This is where the second most important takeaway comes into play:
    
    <em class="italic">
     
      set a goal when optimizing your application
     
    </em>
    
     .
    
    
     Don’t aim to achieve the best performance possible because there always will be room for improvement.
    
    
     The right thing to do is to set a clear specification with targets such as maximum processing time for a transaction or the number of network packets processed
    
    
     
      per second.
     
    
   </p>
   <p>
    
     With these two main ideas in mind, let’s start with the different methods we can use to measure
    
    
     
      code performance.
     
    
   </p>
   <h2 id="_idParaDest-272">
    <a id="_idTextAnchor271">
    </a>
    
     In-code profiling
    
   </h2>
   <p>
    
     A very simple but useful
    
    <a id="_idIndexMarker998">
    </a>
    
     way to start understanding the performance of our
    
    <a id="_idIndexMarker999">
    </a>
    
     code is
    
    <strong class="bold">
     
      in-code profiling
     
    </strong>
    
     , which consists of adding some extra code to measure the execution time of some code sections.
    
    
     This method is good to use as a tool while we’re writing the code (of course, we need to have access to the source code).
    
    
     This will allow us to find some performance issues in our code, as we’ll see later in
    
    
     
      this chapter.
     
    
   </p>
   <p>
    
     We’re going to use
    
    <strong class="source-inline">
     
      std::chrono
     
    </strong>
    
     as our initial approach to profiling
    
    
     
      our code.
     
    
   </p>
   <p>
    
     The following code snippet shows how we can use
    
    <strong class="source-inline">
     
      std::chrono
     
    </strong>
    
     to do some basic profiling of
    
    
     
      our code:
     
    
   </p>
   <pre class="source-code">
auto start = std::chrono::high_resolution_clock::now();
// processing to profile
auto end = std::chrono::high_resolution_clock::now();
auto duration = std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(end - start);
std::cout &lt; duration.count() &lt;&lt; " milliseconds\n";</pre>
   <p>
    
     Here, we get two time samples that call
    
    <strong class="source-inline">
     
      high_resolution_clock::now()
     
    </strong>
    
     and print the time lapse converted into milliseconds.
    
    
     Depending on the time we estimate the processing is going to take, we could use either microseconds or seconds, for example.
    
    
     With this simple technique, we can easily get an idea of how long the processing takes and we can easily compare
    
    
     
      different options.
     
    
   </p>
   <p>
    
     Here,
    
    <strong class="source-inline">
     
      std::chrono::high_resolution_clock
     
    </strong>
    
     is the clock type that offers the highest precision (smallest tick period provided by the implementation).
    
    
     The C++ Standard Library allows it to be an alias of either
    
    <strong class="source-inline">
     
      std::chrono::system_clock
     
    </strong>
    
     or
    
    <strong class="source-inline">
     
      std::chrono::steady_clock
     
    </strong>
    
     .
    
    
     libstdc++ has it aliased to
    
    <strong class="source-inline">
     
      std::chrono::system_clock
     
    </strong>
    
     , whereas libc++ uses
    
    <strong class="source-inline">
     
      std::chrono::steady_clock
     
    </strong>
    
     .
    
    
     For the examples in this chapter, we’ve used GCC and libstdc++.
    
    
     The
    
    <a id="_idIndexMarker1000">
    </a>
    
     clock resolution
    
    <a id="_idIndexMarker1001">
    </a>
    
     is
    
    
     
      1 nanosecond:
     
    
   </p>
   <pre class="source-code">
/**
 *  @brief Highest-resolution clock
 *
 *  This is the clock "with the shortest tick period." Alias to
 *  std::system_clock until higher-than-nanosecond definitions
 *  become feasible.
 *  @ingroup chrono
*/
using high_resolution_clock = system_clock;</pre>
   <p>
    
     Now, let’s see a full example
    
    <a id="_idIndexMarker1002">
    </a>
    
     of profiling two of the C++ Standard
    
    <a id="_idIndexMarker1003">
    </a>
    
     Library algorithms to sort vectors –
    
    <strong class="source-inline">
     
      std::sort
     
    </strong>
    
     
      and
     
    
    
     <strong class="source-inline">
      
       std::stable_sort
      
     </strong>
    
    
     
      :
     
    
   </p>
   <pre class="source-code">
#include &lt;algorithm&gt;
#include &lt;chrono&gt;
#include &lt;iostream&gt;
#include &lt;random&gt;
#include &lt;utility&gt;
int uniform_random_number(int min, int max) {
    static std::random_device rd;
    static std::mt19937 gen(rd());
    std::uniform_int_distribution dis(min, max);
    return dis(gen);
}
std::vector&lt;int&gt; random_vector(std::size_t n, int32_t min_val, int32_t max_val) {
    std::vector&lt;int&gt; rv(n);
    std::ranges::generate(rv, [&amp;] {
            return uniform_random_number(min_val, max_val);
        });
    return rv;
}
using namespace std::chrono;
int main() {
    constexpr uint32_t elements = 100000000;
    int32_t minval = 1;
    int32_t maxval = 1000000000;
    auto rv1 = random_vector(elements, minval, maxval);
    auto rv2 = rv1;
    auto start = high_resolution_clock::now();
    std::ranges::sort(rv1);
    auto end = high_resolution_clock::now();
    auto duration = duration_cast&lt;milliseconds&gt;(end - start);
    std::cout &lt;&lt; "Time to std::sort "
              &lt;&lt; elements &lt;&lt; " elements with values in ["
              &lt;&lt; minval &lt;&lt; "," &lt;&lt; maxval &lt;&lt; "] "
              &lt;&lt; duration.count() &lt;&lt; " milliseconds\n";
    start = high_resolution_clock::now();
    std::ranges::stable_sort(rv2);
    end = high_resolution_clock::now();
    duration = duration_cast&lt;milliseconds&gt;(end - start);
    std::cout &lt;&lt; "Time to std::stable_sort "
              &lt;&lt; elements &lt;&lt; " elements with values in ["
              &lt;&lt; minval &lt;&lt; "," &lt;&lt; maxval &lt;&lt; "] "
              &lt;&lt; duration.count() &lt;&lt; " milliseconds\n";
    return 0;
}</pre>
   <p>
    
     The preceding code generates a vector of normally distributed random numbers and then sorts the vector with both
    
    <strong class="source-inline">
     
      std::sort()
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      std::stable_sort()
     
    </strong>
    
     .
    
    
     Both functions sort the vector, but
    
    <strong class="source-inline">
     
      std::sort()
     
    </strong>
    
     uses a combination of quicksort and insertion sort algorithms called introsort, while
    
    <strong class="source-inline">
     
      std::stable_sort()
     
    </strong>
    
     uses merge sort.
    
    
     The sort is
    
    <em class="italic">
     
      stable
     
    </em>
    
     because equivalent keys have the same order in both the original and sorted vectors.
    
    
     For a vector of integers, this isn’t important, but if the vector has three elements with the same value, after sorting the vector, the numbers will be in the
    
    
     
      same order.
     
    
   </p>
   <p>
    
     After running the code, we get the
    
    
     
      following output:
     
    
   </p>
   <pre class="console">
Time to std::sort 100000000 elements with values in [1,1000000000] 6019 milliseconds
Time to std::stable_sort 100000000 elements with values in [1,1000000000] 7342 milliseconds</pre>
   <p>
    
     In this example,
    
    <strong class="source-inline">
     
      std::stable_sort()
     
    </strong>
    
     is slower
    
    
     
      than
     
    
    
     <strong class="source-inline">
      
       std::sort()
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     In this section, we learned about a simple way to measure the running time of sections of our code.
    
    
     This method is intrusive and requires that we modify the code; it’s mostly used while we
    
    <a id="_idIndexMarker1004">
    </a>
    
     develop
    
    <a id="_idIndexMarker1005">
    </a>
    
     our applications.
    
    
     In the next section, we’re going to introduce another way to measure execution time
    
    
     
      called micro-benchmarks.
     
    
   </p>
   <h2 id="_idParaDest-273">
    <a id="_idTextAnchor272">
    </a>
    
     Code micro-benchmarks
    
   </h2>
   <p>
    
     Sometimes, we just want
    
    <a id="_idIndexMarker1006">
    </a>
    
     to analyze
    
    <a id="_idIndexMarker1007">
    </a>
    
     a small section of code in isolation.
    
    
     We may need to run it more than once and then get the average running time or run it with different input data.
    
    
     In these cases, we can use a benchmark (also
    
    <a id="_idIndexMarker1008">
    </a>
    
     called a
    
    <strong class="bold">
     
      micro-benchmark
     
    </strong>
    
     ) library to do just that – execute small parts of our code in
    
    
     
      different conditions.
     
    
   </p>
   <p>
    
     Micro-benchmarks must be used as a guide.
    
    
     Bear in mind that the code runs in isolation, and this can give us very different results when we run all the code together due to the many complex interactions among different sections of our code.
    
    
     Use them carefully and be aware that micro-benchmarks can
    
    
     
      be misleading.
     
    
   </p>
   <p>
    
     There are many libraries we can use to benchmark our code.
    
    
     We’ll use
    
    <em class="italic">
     
      Google Benchmark
     
    </em>
    
     , a very good and
    
    
     
      well-known library.
     
    
   </p>
   <p>
    
     Let’s start by getting the code and compiling the library.
    
    
     To get the code, run the
    
    
     
      following commands:
     
    
   </p>
   <pre class="console">
git clone https://github.com/google/benchmark.git
cd benchmark
git clone https://github.com/google/googletest.git</pre>
   <p>
    
     Once we have the code for both the benchmark and Google Test libraries (the latter is required to compile the former), we’ll
    
    
     
      build it.
     
    
   </p>
   <p>
    
     Create a directory for
    
    
     
      the build:
     
    
   </p>
   <pre class="console">
mkdir build
cd build</pre>
   <p>
    
     With that, we’ve created the build directory inside the
    
    
     
      benchmark directory.
     
    
   </p>
   <p>
    
     Next, we’ll use CMake
    
    <a id="_idIndexMarker1009">
    </a>
    
     to configure the
    
    <a id="_idIndexMarker1010">
    </a>
    
     build and create all the necessary information
    
    
     
      for
     
    
    
     <strong class="source-inline">
      
       make
      
     </strong>
    
    
     
      :
     
    
   </p>
   <pre class="console">
cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBRARIES=ON -DCMAKE_INSTALL_PREFIX=/usr/lib/x86_64-linux-gnu/</pre>
   <p>
    
     Finally, run
    
    <strong class="source-inline">
     
      make
     
    </strong>
    
     to build and install
    
    
     
      the libraries:
     
    
   </p>
   <pre class="console">
make -j16
sudo make install</pre>
   <p>
    
     You also need to add the library to the
    
    <strong class="source-inline">
     
      CmakeLists.txt
     
    </strong>
    
     file.
    
    
     We’ve done that for you in the code for
    
    
     
      this book.
     
    
   </p>
   <p>
    
     Once Google Benchmark has been installed, we can work on an example with a few benchmark functions to learn how to use the library for some
    
    
     
      basic benchmarking.
     
    
   </p>
   <p>
    
     Note that both
    
    <strong class="source-inline">
     
      std::chrono
     
    </strong>
    
     and Google Benchmark aren’t specific tools for working with asynchronous/multithreaded code and are more like
    
    
     
      generic tools.
     
    
   </p>
   <p>
    
     This is our first
    
    <a id="_idIndexMarker1011">
    </a>
    
     example of
    
    <a id="_idIndexMarker1012">
    </a>
    
     using
    
    
     
      Google Benchmark:
     
    
   </p>
   <pre class="source-code">
#include &lt;benchmark/benchmark.h&gt;
#include &lt;algorithm&gt;
#include &lt;chrono&gt;
#include &lt;iostream&gt;
#include &lt;random&gt;
#include &lt;thread&gt;
void BM_vector_push_back(benchmark::State&amp; state) {
    for (auto _ : state) {
        std::vector&lt;int&gt; vec;
        for (int i = 0; i &lt; state.range(0); i++) {
            vec.push_back(i);
        }
    }
}
void BM_vector_emplace_back(benchmark::State&amp; state) {
    for (auto _ : state) {
        std::vector&lt;int&gt; vec;
        for (int i = 0; i &lt; state.range(0); i++) {
            vec.emplace_back(i);
        }
    }
}
void BM_vector_insert(benchmark::State&amp; state) {
    for (auto _ : state) {
        std::vector&lt;int&gt; vec;
        for (int i = 0; i &lt; state.range(0); i++) {
            vec.insert(vec.begin(), i);
        }
    }
}
BENCHMARK(BM_vector_push_back)-&gt;Range(1, 1000);
BENCHMARK(BM_vector_emplace_back)-&gt;Range(1, 1000);
BENCHMARK(BM_vector_insert)-&gt;Range(1, 1000);
int main(int argc, char** argv) {
    benchmark::Initialize(&amp;argc, argv);
    benchmark::RunSpecifiedBenchmarks();
    return 0;
}</pre>
   <p>
    
     We need to
    
    <a id="_idIndexMarker1013">
    </a>
    
     include the
    
    
     
      library
     
    
    
     <a id="_idIndexMarker1014">
     </a>
    
    
     
      header:
     
    
   </p>
   <pre class="source-code">
#include &lt;benchmark/benchmark.h&gt;</pre>
   <p>
    
     All benchmark functions have the
    
    
     
      following signature:
     
    
   </p>
   <pre class="source-code">
void benchmark_function(benchmark::State&amp; state);</pre>
   <p>
    
     This is a function with one parameter,
    
    <strong class="source-inline">
     
      benchmark::State&amp; state
     
    </strong>
    
     , that returns
    
    <strong class="source-inline">
     
      void
     
    </strong>
    
     .
    
    
     The
    
    <strong class="source-inline">
     
      benchmark::State
     
    </strong>
    
     parameter has a
    
    
     
      dual purpose:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Controlling the iteration loop
      
     </strong>
     
      : The
     
     <strong class="source-inline">
      
       benchmark::State
      
     </strong>
     
      object is used to control how many times a benchmarked function or piece of code should be executed.
     
     
      This helps measure the performance accurately by repeating the test enough times to minimize variability and collect
     
     
      
       meaningful data.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Measuring time and statistics
      
     </strong>
     
      : The
     
     <strong class="source-inline">
      
       state
      
     </strong>
     
      object keeps track of how long the benchmarked code takes to run, and it provides mechanisms to report metrics such as elapsed time, iterations, and
     
     
      
       custom counters.
      
     
    </li>
   </ul>
   <p>
    
     We’ve implemented three functions to benchmark adding elements to a
    
    <strong class="source-inline">
     
      std::vector
     
    </strong>
    
     sequence in different ways: the first function uses
    
    <strong class="source-inline">
     
      std::vector::push_back
     
    </strong>
    
     , the second uses
    
    <strong class="source-inline">
     
      std::vector::emplace_back
     
    </strong>
    
     , and the third uses
    
    <strong class="source-inline">
     
      std::vector::insert
     
    </strong>
    
     .
    
    
     The first two functions add elements at the end of the vector, while the third function adds elements at the beginning of
    
    
     
      the vector.
     
    
   </p>
   <p>
    
     Once we’ve implemented the benchmark functions, we need to tell the library that they must be run as
    
    
     
      a benchmark:
     
    
   </p>
   <pre class="source-code">
BENCHMARK(BM_vector_push_back)-&gt;Range(1, 1000);</pre>
   <p>
    
     We use the
    
    <strong class="source-inline">
     
      BENCHMARK
     
    </strong>
    
     macro to do this.
    
    
     For the benchmarks in this example, we set the number of elements to be inserted into the vector in each iteration.
    
    
     The range goes from
    
    <strong class="source-inline">
     
      1
     
    </strong>
    
     to
    
    <strong class="source-inline">
     
      1000
     
    </strong>
    
     and each iteration will insert eight times the number of elements of the previous iteration until it
    
    <a id="_idIndexMarker1015">
    </a>
    
     reaches
    
    <a id="_idIndexMarker1016">
    </a>
    
     the maximum.
    
    
     In this case, it will insert 1, 8, 64, 512, and
    
    
     
      1,000 elements.
     
    
   </p>
   <p>
    
     When we run our first benchmark program, we get the
    
    
     
      following output:
     
    
   </p>
   <pre class="console">
2024-10-17T05:02:37+01:00
Running ./13x02-benchmark_vector
Run on (64 X 3600 MHz CPU s)
CPU Caches:
  L1 Data 32 KiB (x32)
  L1 Instruction 32 KiB (x32)
  L2 Unified 512 KiB (x32)
  L3 Unified 32768 KiB (x4)
Load Average: 0.00, 0.02, 0.16
----------------------------------------------------------------------
Benchmark                            Time             CPU   Iterations
----------------------------------------------------------------------
BM_vector_push_back/1             10.5 ns         10.5 ns     63107997
BM_vector_push_back/8             52.0 ns         52.0 ns     13450361
BM_vector_push_back/64             116 ns          116 ns      6021740
BM_vector_push_back/512            385 ns          385 ns      1819732
BM_vector_push_back/1000           641 ns          641 ns      1093474
BM_vector_emplace_back/1          10.8 ns         10.8 ns     64570848
BM_vector_emplace_back/8          53.3 ns         53.3 ns     13139191
BM_vector_emplace_back/64          108 ns          108 ns      6469997
BM_vector_emplace_back/512         364 ns          364 ns      1924992
BM_vector_emplace_back/1000        616 ns          616 ns      1138392
BM_vector_insert/1                10.6 ns         10.6 ns     65966159
BM_vector_insert/8                58.6 ns         58.6 ns     11933446
BM_vector_insert/64                461 ns          461 ns      1485319
BM_vector_insert/512              7249 ns         7249 ns        96756
BM_vector_insert/1000            23352 ns        23348 ns        29742</pre>
   <p>
    
     First, the program
    
    <a id="_idIndexMarker1017">
    </a>
    
     prints information
    
    <a id="_idIndexMarker1018">
    </a>
    
     about the execution of the benchmark: the date and time, the name of the executable, and information about the CPU it’s
    
    
     
      running on.
     
    
   </p>
   <p>
    
     Take a look at the
    
    
     
      following line:
     
    
   </p>
   <pre class="console">
Load Average: 0.00, 0.02, 0.16</pre>
   <p>
    
     This line gives us an estimate of the CPU load: from 0.0 (no load at all or very low load) to 1.0 (fully loaded).
    
    
     The three numbers correspond to the CPU load for the last 5, 10, and 15
    
    
     
      minutes, respectively.
     
    
   </p>
   <p>
    
     After printing the CPU load information, the benchmark prints the results of each iteration.
    
    
     Here’s
    
    
     
      an example:
     
    
   </p>
   <pre class="console">
BM_vector_push_back/64             116 ns          116 ns      6021740</pre>
   <p>
    
     This means that
    
    <strong class="source-inline">
     
      BM_vector_push_back
     
    </strong>
    
     was called 6,021,740 times (the number of iterations) while inserting 64 elements into
    
    
     
      the vector.
     
    
   </p>
   <p>
    
     The
    
    <strong class="source-inline">
     
      Time
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      CPU
     
    </strong>
    
     columns give us the average time for
    
    
     
      each iteration:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Time
      
     </strong>
     
      : This is the real time that’s elapsed from the beginning to the end of each benchmark execution.
     
     
      It includes everything that happens during the benchmark: CPU computation, I/O operations, context switches,
     
     
      
       and more.
      
     
    </li>
    <li>
     <strong class="bold">
      
       CPU time
      
     </strong>
     
      : This is the amount of time the CPU spent processing the instructions of the benchmark.
     
     
      It can be smaller than or equal
     
     
      
       to
      
     
     
      <strong class="source-inline">
       
        Time
       
      </strong>
     
     
      
       .
      
     
    </li>
   </ul>
   <p>
    
     In our benchmark, because the operations are simple, we can see that
    
    <strong class="source-inline">
     
      Time
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      CPU
     
    </strong>
    
     are mostly
    
    
     
      the same.
     
    
   </p>
   <p>
    
     Looking at the results, we can come to the
    
    
     
      following conclusions:
     
    
   </p>
   <ul>
    <li>
     
      For simple objects such as 32-bit integers, both
     
     <strong class="source-inline">
      
       push_back
      
     </strong>
     
      and
     
     <strong class="source-inline">
      
       emplace_back
      
     </strong>
     
      take the same amount
     
     
      
       of time.
      
     
    </li>
    <li>
     
      Here,
     
     <strong class="source-inline">
      
       insert
      
     </strong>
     
      takes the same amount of time as
     
     <strong class="source-inline">
      
       push_back
      
     </strong>
     
      /
     
     <strong class="source-inline">
      
       emplace_back
      
     </strong>
     
      for a small number of elements but from 64 elements onwards, it takes considerably more time.
     
     
      This is because
     
     <strong class="source-inline">
      
       insert
      
     </strong>
     
      must copy all the elements after each insertion (we insert the elements at the beginning of
     
     
      
       the vector).
      
     
    </li>
   </ul>
   <p>
    
     The following
    
    <a id="_idIndexMarker1019">
    </a>
    
     example also sorts a
    
    <strong class="source-inline">
     
      std::vector
     
    </strong>
    
     sequence, but
    
    <a id="_idIndexMarker1020">
    </a>
    
     this time, we’ll use a micro-benchmark to measure
    
    
     
      execution time:
     
    
   </p>
   <pre class="source-code">
#include &lt;benchmark/benchmark.h&gt;
#include &lt;algorithm&gt;
#include &lt;chrono&gt;
#include &lt;iostream&gt;
#include &lt;random&gt;
#include &lt;thread&gt;
std::vector&lt;int&gt; rv1, rv2;
int uniform_random_number(int min, int max) {
    static std::random_device rd;
    static std::mt19937 gen(rd());
    std::uniform_int_distribution dis(min, max);
    return dis(gen);
}
std::vector&lt;int&gt; random_vector(std::size_t n, int32_t min_val, int32_t max_val) {
    std::vector&lt;int&gt; rv(n);
    std::ranges::generate(rv, [&amp;] {
        return uniform_random_number(min_val, max_val);
    });
    return rv;
}
static void BM_vector_sort(benchmark::State&amp; state, std::vector&lt;int&gt;&amp; vec) {
    for (auto _ : state) {
        std::ranges::sort(vec);
    }
}
static void BM_vector_stable_sort(benchmark::State&amp; state, std::vector&lt;int&gt;&amp; vec) {
    for (auto _ : state) {
        std::ranges::stable_sort(vec);
    }
}
BENCHMARK_CAPTURE(BM_vector_sort, vector, rv1)-&gt;Iterations(1)-&gt;Unit(benchmark::kMillisecond);
BENCHMARK_CAPTURE(BM_vector_stable_sort, vector, rv2)-&gt;Iterations(1)-&gt;Unit(benchmark::kMillisecond);
int main(int argc, char** argv) {
    constexpr uint32_t elements = 100000000;
    int32_t minval = 1;
    int32_t maxval = 1000000000;
    rv1 = random_vector(elements, minval, maxval);
    rv2 = rv1;
    benchmark::Initialize(&amp;argc, argv);
    benchmark::RunSpecifiedBenchmarks();
    return 0;
}</pre>
   <p>
    
     The preceding
    
    <a id="_idIndexMarker1021">
    </a>
    
     code
    
    <a id="_idIndexMarker1022">
    </a>
    
     generates a vector of random numbers.
    
    
     Here, we run two benchmark functions to sort the vector: one using
    
    <strong class="source-inline">
     
      std::sort
     
    </strong>
    
     and another using
    
    <strong class="source-inline">
     
      std::stable_sort
     
    </strong>
    
     .
    
    
     Note that we use two copies of the same vector, so the input is the same for
    
    
     
      both functions.
     
    
   </p>
   <p>
    
     The following line of code uses the
    
    <strong class="source-inline">
     
      BENCHMARK_CAPTURE
     
    </strong>
    
     macro.
    
    
     This macro allows us to pass parameters to our benchmark functions – in this case, a reference to
    
    <strong class="source-inline">
     
      std::vector
     
    </strong>
    
     (we pass by reference to avoid copying the vector and impacting the
    
    
     
      benchmark result).
     
    
   </p>
   <p>
    
     We specify the results to be in milliseconds instead
    
    
     
      of nanoseconds:
     
    
   </p>
   <pre class="source-code">
BENCHMARK_CAPTURE(BM_vector_sort, vector, rv1)-&gt;Iterations(1)-&gt;Unit(benchmark::kMillisecond);</pre>
   <p>
    
     Here are the results of
    
    
     
      the benchmark:
     
    
   </p>
   <pre class="console">
-------------------------------------------------------------------------
Benchmark                          Time         CPU   Iterations
-------------------------------------------------------------------------
BM_vector_sort                     5877 ms      5876 ms            1
BM_vector_stable_sort.             7172 ms      7171 ms            1</pre>
   <p>
    
     The results are consistent with the ones we got measuring time
    
    
     
      using
     
    
    
     <strong class="source-inline">
      
       std::chrono
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     For our last Google
    
    <a id="_idIndexMarker1023">
    </a>
    
     Benchmark
    
    <a id="_idIndexMarker1024">
    </a>
    
     example, we’ll create a
    
    
     
      thread (
     
    
    
     <strong class="source-inline">
      
       std::thread
      
     </strong>
    
    
     
      ):
     
    
   </p>
   <pre class="source-code">
#include &lt;benchmark/benchmark.h&gt;
#include &lt;algorithm&gt;
#include &lt;chrono&gt;
#include &lt;iostream&gt;
#include &lt;random&gt;
#include &lt;thread&gt;
static void BM_create_terminate_thread(benchmark::State&amp; state) {
    for (auto _ : state) {
        std::thread thread([]{ return -1; });
        thread.join();
    }
}
BENCHMARK(BM_create_terminate_thread)-&gt;Iterations(2000);
int main(int argc, char** argv) {
    benchmark::Initialize(&amp;argc, argv);
    benchmark::RunSpecifiedBenchmarks();
    return 0;
}</pre>
   <p>
    
     This example is simple:
    
    <strong class="source-inline">
     
      BM_create_terminate_thread
     
    </strong>
    
     creates a thread (doing nothing, just returning 0) and waits for it to end (
    
    <strong class="source-inline">
     
      thread.join())
     
    </strong>
    
     .
    
    
     We run
    
    <strong class="source-inline">
     
      2000
     
    </strong>
    
     iterations to get
    
    <a id="_idIndexMarker1025">
    </a>
    
     an estimation of the
    
    <a id="_idIndexMarker1026">
    </a>
    
     time it takes to create
    
    
     
      a thread.
     
    
   </p>
   <p>
    
     The results are
    
    
     
      as follows:
     
    
   </p>
   <pre class="console">
---------------------------------------------------------------
----------
Benchmark                        Time             CPU
Iterations
---------------------------------------------------------------
----------
BM_create_terminate_thread.       32424 ns        21216 ns     2000</pre>
   <p>
    
     In this section, we learned how to use the Google Benchmark library to create micro-benchmarks to measure the execution time of some functions.
    
    
     Again, micro-benchmarks are
    
    <a id="_idIndexMarker1027">
    </a>
    
     just an approximation
    
    <a id="_idIndexMarker1028">
    </a>
    
     and due to the isolated nature of the code being benchmarked, they may be misleading.
    
    
     Use
    
    
     
      them carefully.
     
    
   </p>
   <h2 id="_idParaDest-274">
    <a id="_idTextAnchor273">
    </a>
    
     The Linux perf tool
    
   </h2>
   <p>
    
     Using
    
    <strong class="source-inline">
     
      std::chrono
     
    </strong>
    
     in our
    
    <a id="_idIndexMarker1029">
    </a>
    
     code or a micro-benchmark
    
    <a id="_idIndexMarker1030">
    </a>
    
     library such as Google Benchmark requires gaining access to the code to be profiled and also being able to modify it by either adding extra calls to measure the execution time of code sections or running small snippets as
    
    
     
      micro-benchmark functions.
     
    
   </p>
   <p>
    
     With the Linux
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     tool, we can analyze the execution of a program without changing any of
    
    
     
      its code.
     
    
   </p>
   <p>
    
     The Linux
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     tool is a powerful, flexible, and widely used performance analysis and profiling utility for Linux systems.
    
    
     It provides detailed insights into system performance at the kernel and user
    
    
     
      space levels.
     
    
   </p>
   <p>
    
     Let’s consider the main uses
    
    
     
      of
     
    
    
     <strong class="source-inline">
      
       perf
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     First, we
    
    <a id="_idIndexMarker1031">
    </a>
    
     have
    
    <strong class="bold">
     
      CPU profiling
     
    </strong>
    
     .
    
    
     The
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     tool allows you to capture the execution profile of a process, measuring which functions consume most of the CPU time.
    
    
     This can be very useful in helping to identify CPU-intensive parts of the code
    
    
     
      and bottlenecks.
     
    
   </p>
   <p>
    
     The following command line will run
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     on the small
    
    <strong class="source-inline">
     
      13x07-thread_contention
     
    </strong>
    
     program we wrote to illustrate the basics of the tool.
    
    
     The code for this application can be found in this book’s
    
    
     
      GitHub repository:
     
    
   </p>
   <pre class="console">
perf record --call-graph dwarf ./13x07-thread_contention</pre>
   <p>
    
     The
    
    <strong class="source-inline">
     
      --call-graph
     
    </strong>
    
     option records the data of the function call hierarchy in a file called
    
    <strong class="source-inline">
     
      perf.data
     
    </strong>
    
     , while the
    
    <strong class="source-inline">
     
      dwarf
     
    </strong>
    
     option instructs
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     to use the dwarf file format to debug symbols (to get the
    
    
     
      function names).
     
    
   </p>
   <p>
    
     After the previous command, we must run
    
    
     
      the following:
     
    
   </p>
   <pre class="console">
 perf script &gt; out.perf</pre>
   <p>
    
     This will dump the recorded data (including the call stack) into a text file
    
    
     
      called
     
    
    
     <strong class="source-inline">
      
       out.perf
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     Now, we need to convert the text file into a picture with the call graph.
    
    
     To do this, we can run the
    
    
     
      following command:
     
    
   </p>
   <pre class="console">
gprof2dot -f perf out.perf -o callgraph.dot</pre>
   <p>
    
     This will generate a file called
    
    <strong class="source-inline">
     
      callgraph.dot
     
    </strong>
    
     that can be visualized
    
    
     
      using Graphviz.
     
    
   </p>
   <p>
    
     You may need to install
    
    <strong class="source-inline">
     
      gprof2dot
     
    </strong>
    
     .
    
    
     For this, you need Python installed on your PC.
    
    
     Run the following command to
    
    
     
      install
     
    
    
     <strong class="source-inline">
      
       gprof2dot
      
     </strong>
    
    
     
      :
     
    
   </p>
   <pre class="console">
pip install gprof2dot</pre>
   <p>
    
     Install Graphviz too.
    
    
     In Ubuntu, you can do this
    
    
     
      like so:
     
    
   </p>
   <pre class="console">
sudo apt-get install graphviz</pre>
   <p>
    
     Finally, you can generate the
    
    <strong class="source-inline">
     
      callgraph.png
     
    </strong>
    
     picture by running the
    
    
     
      following command:
     
    
   </p>
   <pre class="console">
dot -Tpng callgraph.dot -o callgraph.png</pre>
   <p>
    
     Another very common
    
    <a id="_idIndexMarker1032">
    </a>
    
     way to visualize the call graph
    
    <a id="_idIndexMarker1033">
    </a>
    
     of a program is by using a
    
    
     
      flame graph.
     
    
   </p>
   <p>
    
     To generate a flame graph, clone the
    
    
     <strong class="source-inline">
      
       FlameGraph
      
     </strong>
    
    
     
      repository:
     
    
   </p>
   <pre class="console">
git clone https://github.com/brendangregg/FlameGraph.git</pre>
   <p>
    
     In the
    
    <strong class="source-inline">
     
      FlameGraph
     
    </strong>
    
     folder, you’ll find the scripts to generate the
    
    
     
      flame graphs.
     
    
   </p>
   <p>
    
     Run the
    
    
     
      following command:
     
    
   </p>
   <pre class="console">
FlameGraph/stackcollapse-perf.pl out.perf &gt; out.folded</pre>
   <p>
    
     This command will collapse the stack traces into a format that can be used by the FlameGraph tool.
    
    
     Now, run the
    
    
     
      following command:
     
    
   </p>
   <pre class="console">
Flamegraph/flamegraph.pl out.folded &gt; flamegraph.svg</pre>
   <p>
    
     You can visualize the flame graph with a
    
    
     
      web browser:
     
    
   </p>
   <div><div><img alt="Figure 13.1: ﻿An overview of a flame graph" src="img/B22219_13_1.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 13.1: An overview of a flame graph
    
   </p>
   <p>
    
     Now, let’s learn how to gather the performance statistics of
    
    
     
      a program.
     
    
   </p>
   <p>
    
     The following command will show the number of instructions that have been executed and CPU cycles that were used during the execution of
    
    <strong class="source-inline">
     
      13x05-sort_perf
     
    </strong>
    
     .
    
    
     The number of instructions per cycle is the average number of instructions the CPU executes in each clock cycle.
    
    
     This metric is only useful when we microbenchmark or measure short parts of the code.
    
    
     For this example, we can see that the CPU is executing one instruction per cycle, which is average for a modern CPU.
    
    
     In multithreaded code, we can get a much bigger number due to the parallel nature of the execution, but this metric is generally used to measure and optimize code executed in a single CPU core.
    
    
     The number must be
    
    <a id="_idIndexMarker1034">
    </a>
    
     interpreted as how busy we keep the
    
    <a id="_idIndexMarker1035">
    </a>
    
     CPU because it depends on many factors, such as the number of memory reads/writes, memory access patterns (linear consecutive/no linear), level of branching in the code, and
    
    
     
      so on:
     
    
   </p>
   <pre class="console">
perf stat -e instructions,cycles ./13x05-sort_perf</pre>
   <p>
    
     After running the preceding command, we get the
    
    
     
      following result:
     
    
   </p>
   <pre class="console">
Performance counter stats for './13x05-sort_perf':
    30,993,024,309      instructions                     #     1.03   
             insn per cycle
    30,197,863,655      cycles
       6.657835162 seconds time elapsed
       6.502372000 seconds user
       0.155008000 seconds sys</pre>
   <p>
    
     Running the following command, you can get the list of all the predefined events you can analyze
    
    
     
      with
     
    
    
     <strong class="source-inline">
      
       perf
      
     </strong>
    
    
     
      :
     
    
   </p>
   <pre class="console">
perf list</pre>
   <p>
    
     Let’s do a
    
    
     
      few more:
     
    
   </p>
   <pre class="console">
perf stat -e branches ./13x05-sort_perf</pre>
   <p>
    
     The previous command measures the number of branch instructions that have been executed.
    
    
     We get the
    
    
     
      following result:
     
    
   </p>
   <pre class="console">
Performance counter stats for './13x05-sort_perf':
     5,246,138,882      branches
       6.712285274 seconds time elapsed
       6.551799000 seconds user
       0.159970000 seconds sys</pre>
   <p>
    
     Here, we can see
    
    <a id="_idIndexMarker1036">
    </a>
    
     that a sixth of the instructions that
    
    <a id="_idIndexMarker1037">
    </a>
    
     were executed are branching instructions, which is expected in a program that sorts
    
    
     
      large vectors.
     
    
   </p>
   <p>
    
     As mentioned previously, measuring the level of branching in our code is important, especially for short sections of the code (to avoid interactions that can impact what’s being measured).
    
    
     A CPU will run instructions much faster if there are no branches or there are just a few.
    
    
     The main issue with branches is that the CPU may need to rebuild the pipeline and that can be costly, especially if branches are in
    
    
     
      inner/critical loops.
     
    
   </p>
   <p>
    
     The following command will report the number of L1 cache data accesses (we will see the CPU cache in the
    
    
     
      next section):
     
    
   </p>
   <pre class="console">
perf stat -e all_data_cache_accesses ./13x05-sort_perf</pre>
   <p>
    
     We get the
    
    
     
      following result:
     
    
   </p>
   <pre class="console">
Performance counter stats for './13x05-sort_perf':
    21,286,061,764      all_data_cache_accesses
       6.718844368 seconds time elapsed
       6.561416000 seconds user
       0.157009000 seconds sys</pre>
   <p>
    
     Let’s go back to our lock contention example and gather some useful statistics
    
    
     
      with
     
    
    
     <strong class="source-inline">
      
       perf
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     Another benefit of using
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     is
    
    <strong class="bold">
     
      CPU migrations
     
    </strong>
    
     – that is, the
    
    <a id="_idIndexMarker1038">
    </a>
    
     number of times a thread was moved from one CPU core to another.
    
    
     Thread migration between cores can degrade cache performance
    
    <a id="_idIndexMarker1039">
    </a>
    
     since threads lose the benefit
    
    <a id="_idIndexMarker1040">
    </a>
    
     of cached data when moving to a new core (more on caches in the
    
    
     
      next section).
     
    
   </p>
   <p>
    
     Let’s run the
    
    
     
      following command:
     
    
   </p>
   <pre class="console">
perf stat -e cpu-migrations ./13x07-thread_contention</pre>
   <p>
    
     This results in the
    
    
     
      following output:
     
    
   </p>
   <pre class="console">
Performance counter stats for './13x08-thread_contention':
                45      cpu-migrations
      50.476706194 seconds time elapsed
      57.333880000 seconds user
     262.123060000 seconds sys</pre>
   <p>
    
     Let’s look at another advantage of using
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     :
    
    <strong class="bold">
     
      context switches
     
    </strong>
    
     .
    
    
     It
    
    <a id="_idIndexMarker1041">
    </a>
    
     counts the number of context switches (how many times a thread is swapped out and another thread is scheduled) during the execution.
    
    
     High-context switching can indicate that too many threads are competing for CPU time, leading to
    
    
     
      performance degradation.
     
    
   </p>
   <p>
    
     Let’s run the
    
    
     
      following command:
     
    
   </p>
   <pre class="console">
perf stat -e context-switches ./13x07-thread_contention</pre>
   <p>
    
     This results in the
    
    
     
      following output:
     
    
   </p>
   <pre class="console">
Performance counter stats for './13x08-thread_contention':
        13,867,866      cs
      47.618283562 seconds time elapsed
      52.931213000 seconds user
     247.033479000 seconds sys</pre>
   <p>
    
     That’s a wrap on
    
    <a id="_idIndexMarker1042">
    </a>
    
     this section.
    
    
     Here, we introduced
    
    <a id="_idIndexMarker1043">
    </a>
    
     the Linux
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     tool and some of its applications.
    
    
     We’ll study the CPU memory cache and false sharing in the
    
    
     
      next section.
     
    
   </p>
   <h1 id="_idParaDest-275">
    <a id="_idTextAnchor274">
    </a>
    
     False sharing
    
   </h1>
   <p>
    
     In this section, we’ll study a
    
    <a id="_idIndexMarker1044">
    </a>
    
     common issue with multithreaded applications called
    
    
     <strong class="bold">
      
       false sharing
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     We already know that the ideal implementation of a multithreaded application is minimizing the data that’s shared among its different threads.
    
    
     Ideally, we should share data just for read access because in that case, we don’t need to synchronize the threads to access the shared data and thus we don’t need to pay the runtime cost and deal with issues such as deadlock
    
    
     
      and livelock.
     
    
   </p>
   <p>
    
     Now, let’s consider a simple example: four threads run in parallel, generate random numbers, and calculate their sum.
    
    
     Each thread works independently, generating random numbers and calculating the sum stored in a variable just written by itself.
    
    
     This is the ideal (though for this example, a bit contrived) application, with threads working independently without any
    
    
     
      shared data.
     
    
   </p>
   <p>
    
     The following code is the full source for the example we’re going to analyze in this section.
    
    
     You can refer
    
    <a id="_idIndexMarker1045">
    </a>
    
     to it while you read
    
    
     
      the explanations:
     
    
   </p>
   <pre class="source-code">
#include &lt;chrono&gt;
#include &lt;iostream&gt;
#include &lt;random&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;
struct result_data {
    unsigned long result { 0 };
};
struct alignas(64) aligned_result_data {
    unsigned long result { 0 };
};
void set_affinity(int core) {
    if (core &lt; 0) {
        return;
    }
    cpu_set_t cpuset;
    CPU_ZERO(&amp;cpuset);
    CPU_SET(core, &amp;cpuset);
    if (pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &amp;cpuset) != 0) {
        perror("pthread_setaffinity_np");
        exit(EXIT_FAILURE);
    }
}
template &lt;typename T&gt;
auto random_sum(T&amp; data, const std::size_t seed, const unsigned long iterations, const int core) {
    set_affinity(core);
    std::mt19937 gen(seed);
    std::uniform_int_distribution dist(1, 5);
    for (unsigned long i = 0; i &lt; iterations; ++i) {
        data.result += dist(gen);
    }
}
using namespace std::chrono;
void sum_random_unaligned(int num_threads, uint32_t iterations) {
    auto* data = new(static_cast&lt;std::align_val_t&gt;(64)) result_data[num_threads];
    auto start = high_resolution_clock::now();
    std::vector&lt;std::thread&gt; threads;
    for (std::size_t i = 0; i &lt; num_threads; ++i) {
        set_affinity(i);
        threads.emplace_back(random_sum&lt;result_data&gt;, std::ref(data[i]), i, iterations, i);
    }
    for (auto&amp; thread : threads) {
        thread.join();
    }
    auto end = high_resolution_clock::now();
    auto duration = std::chrono::duration_cast&lt;milliseconds&gt;(end - start);
    std::cout &lt;&lt; "Non-aligned data: " &lt;&lt; duration.count() &lt;&lt; " milliseconds" &lt;&lt; std::endl;
    operator delete[] (data, static_cast&lt;std::align_val_t&gt;(64));
}
void sum_random_aligned(int num_threads, uint32_t iterations) {
    auto* aligned_data = new(static_cast&lt;std::align_val_t&gt;(64)) aligned_result_data[num_threads];
    auto start = high_resolution_clock::now();
    std::vector&lt;std::thread&gt; threads;
    for (std::size_t i = 0; i &lt; num_threads; ++i) {
        set_affinity(i);
        threads.emplace_back(random_sum&lt;aligned_result_data&gt;, std::ref(aligned_data[i]), i, iterations, i);
    }
    for (auto&amp; thread : threads) {
        thread.join();
    }
    auto end = high_resolution_clock::now();
    auto duration = std::chrono::duration_cast&lt;milliseconds&gt;(end - start);
    std::cout &lt;&lt; "Aligned data: " &lt;&lt; duration.count() &lt;&lt; " milliseconds" &lt;&lt; std::endl;
    operator delete[] (aligned_data, static_cast&lt;std::align_val_t&gt;(64));
}
int main() {
    constexpr unsigned long iterations{ 100000000 };
    constexpr unsigned int num_threads = 8;
    sum_random_unaligned(8, iterations);
    sum_random_aligned(8, iterations);
    return 0;
}</pre>
   <p>
    
     If you compile and run
    
    <a id="_idIndexMarker1046">
    </a>
    
     the previous code, you’ll get an output similar to
    
    
     
      the following:
     
    
   </p>
   <pre class="console">
Non-aligned data: 4403 milliseconds
Aligned data: 160 milliseconds</pre>
   <p>
    
     The program just calls two functions:
    
    <strong class="source-inline">
     
      sum_random_unaligned
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      sum_random_aligned
     
    </strong>
    
     .
    
    
     Both functions do the same: they create eight threads, and each thread generates random numbers and calculates their sum.
    
    
     No data is shared among the threads.
    
    
     You can see that the functions are pretty much the same and the main difference is that
    
    <strong class="source-inline">
     
      sum_random_unaligned
     
    </strong>
    
     uses the following data structure to store the sum of the generated
    
    
     
      random numbers:
     
    
   </p>
   <pre class="source-code">
struct result_data {
    unsigned long result { 0 };
};</pre>
   <p>
    
     The
    
    <strong class="source-inline">
     
      sum_random_aligned
     
    </strong>
    
     function uses a slightly
    
    
     
      different one:
     
    
   </p>
   <pre class="source-code">
struct alignas(64) aligned_result_data {
    unsigned long result { 0 };
};</pre>
   <p>
    
     The only difference is the use of
    
    <strong class="source-inline">
     
      alignas(64)
     
    </strong>
    
     in informing the compiler that the data structure instances must be aligned at a
    
    
     
      64-byte boundary.
     
    
   </p>
   <p>
    
     We can see that the difference in performance is quite dramatic because the threads are performing the same tasks.
    
    
     Just aligning the variables written by each thread to a 64-byte boundary greatly
    
    
     
      improves performance.
     
    
   </p>
   <p>
    
     To understand why
    
    <a id="_idIndexMarker1047">
    </a>
    
     this is happening, we need to consider a feature of modern CPUs – the
    
    
     
      memory cache.
     
    
   </p>
   <h1 id="_idParaDest-276">
    <a id="_idTextAnchor275">
    </a>
    
     CPU memory cache
    
   </h1>
   <p>
    
     Modern CPUs are very
    
    <a id="_idIndexMarker1048">
    </a>
    
     fast at computing and when we want to achieve maximum performance, memory access is the main bottleneck.
    
    
     A good estimate for memory access is about 150 nanoseconds.
    
    
     In that time, our 3.6 GHz CPU has gone through 540 clock cycles.
    
    
     As a rough estimate, if the CPU executes an instruction every two cycles, that’s 270 instructions.
    
    
     For a normal application, memory access is an issue, even though the compiler may reorder the instructions it generates and the CPU may also reorder the instructions to optimize memory access and try to run as many instructions
    
    
     
      as possible.
     
    
   </p>
   <p>
    
     Therefore, to improve the performance of modern CPUs, we have what’s called a
    
    <strong class="bold">
     
      CPU cache
     
    </strong>
    
     or
    
    <strong class="bold">
     
      memory cache
     
    </strong>
    
     , which is memory in the chip to store both data and instructions.
    
    
     This memory is much faster than RAM and allows the CPU to retrieve data much faster, significantly boosting
    
    
     
      overall performance.
     
    
   </p>
   <p>
    
     As an example of a real-life cache, think about a cook.
    
    
     They need some ingredients to make lunch for their restaurant clients.
    
    
     Now, imagine that they only buy those ingredients when a client comes to the restaurant and orders their food.
    
    
     That will be very slow.
    
    
     They can also go to the supermarket and buy ingredients for, say, a full day.
    
    
     Now, they can cook for all their clients and serve them their meals in a much
    
    
     
      shorter period.
     
    
   </p>
   <p>
    
     CPU caches follow the same concept: when the CPU needs to access a variable, say a 4-byte integer, it reads 64 bytes (this size may be different, depending on the CPU, but most modern CPUs use that size) of contiguous memory
    
    <em class="italic">
     
      just in case
     
    </em>
    
     it may need to access more
    
    
     
      contiguous data.
     
    
   </p>
   <p>
    
     Linear memory data structures such as
    
    <strong class="source-inline">
     
      std::vector
     
    </strong>
    
     will perform better from a memory access point of view because in these cases, the cache can be a big performance improvement.
    
    
     For other types of data structures, such as
    
    <strong class="source-inline">
     
      std::list
     
    </strong>
    
     , this won’t be the case.
    
    
     Of course, this is just about optimizing
    
    
     
      cache use.
     
    
   </p>
   <p>
    
     You may be wondering why if in-CPU cache memory is so good, isn’t all memory like that?
    
    
     The answer
    
    <a id="_idIndexMarker1049">
    </a>
    
     is cost.
    
    
     Cache memory is very fast (much faster than RAM), but it’s also
    
    
     
      very expensive.
     
    
   </p>
   <p>
    
     Modern CPUs employ a hierarchical cache structure, typically consisting of three levels called L1, L2,
    
    
     
      and L3:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       L1 cache
      
     </strong>
     
      is the smallest
     
     <a id="_idIndexMarker1050">
     </a>
     
      and fastest.
     
     
      It’s also the closest to the CPU, as well as the most expensive.
     
     
      It’s often split into two parts: an instruction cache for storing instructions and a data cache for storing data.
     
     
      The typical sizes are 64 Kb split into 32 Kb for instructions and 32 Kb for data.
     
     
      The typical access time to the L1 cache is between 1 and
     
     
      
       3 nanoseconds.
      
     
    </li>
    <li>
     <strong class="bold">
      
       L2 cache
      
     </strong>
     
      is larger and slightly slower than L1, but still much faster than RAM.
     
     
      Typical L2 cache sizes are between 128 Kb and 512 Kb (the CPU used to run the examples in this chapter has 512 Kb of L2 cache per core).
     
     
      Typical access times for L2 cache are about 3 to
     
     
      
       5 nanoseconds.
      
     
    </li>
    <li>
     <strong class="bold">
      
       L3 cache
      
     </strong>
     
      is the largest and slowest of the three.
     
     
      The L1 and L2 caches are per core (each core has its own L1 and L2 cache), but L3 is shared by more than one core.
     
     
      Our CPU has 32 Mb of L3 cache shared by each group of eight cores.
     
     
      The typical access
     
     <a id="_idIndexMarker1051">
     </a>
     
      time is about 10 to
     
     
      
       15 nanoseconds.
      
     
    </li>
   </ul>
   <p>
    
     With that, let’s turn our attention to another important concept related to
    
    
     
      memory cache.
     
    
   </p>
   <h2 id="_idParaDest-277">
    <a id="_idTextAnchor276">
    </a>
    
     Cache coherency
    
   </h2>
   <p>
    
     The CPU doesn’t access
    
    <a id="_idIndexMarker1052">
    </a>
    
     RAM directly.
    
    
     This access is always done through the cache, and RAM is accessed only if the CPU doesn’t find the data required in the cache.
    
    
     In multi-core systems, each core having its own cache means that one piece of RAM may be present in the cache of multiple cores at the same time.
    
    
     These copies need to be synchronized all the time; otherwise, computation results could
    
    
     
      be incorrect.
     
    
   </p>
   <p>
    
     So far, we’ve seen that each core has its own L1 cache.
    
    
     Let’s go back to our example and think about what happens when we run the function using
    
    
     
      non-aligned memory.
     
    
   </p>
   <p>
    
     In this case, each instance of
    
    <strong class="source-inline">
     
      result_data
     
    </strong>
    
     is 8 bytes.
    
    
     We create an array of 8 instances of
    
    <strong class="source-inline">
     
      result_data
     
    </strong>
    
     , one for each thread.
    
    
     The total memory that’s occupied will be 64 bytes and all the instances will be contiguous in memory.
    
    
     Every time a thread updates the sum of random numbers, it changes the value that’s stored in the cache.
    
    
     Remember that the CPU will always read and write 64 bytes in one go (something called a
    
    <strong class="bold">
     
      cache line
     
    </strong>
    
     – you can
    
    <a id="_idIndexMarker1053">
    </a>
    
     think of it as the smallest memory access unit).
    
    
     All the variables are in the same cache line and even if the threads don’t share them (each thread has its own variable –
    
    <strong class="source-inline">
     
      sum
     
    </strong>
    
     ), the CPU doesn’t know that and needs to make the changes visible for all
    
    
     
      the cores.
     
    
   </p>
   <p>
    
     Here, we have 8 cores, and each core is running a thread.
    
    
     Each core has loaded 64 bytes of memory from RAM into the L1 cache.
    
    
     Since the threads only read the variables, everything is OK, but as soon as one thread modifies its variable, the contents of the cache line
    
    
     
      are invalidated.
     
    
   </p>
   <p>
    
     Now, because the cache line is invalid in the remaining 7 cores, the CPU needs to propagate the changes to all the cores.
    
    
     As mentioned previously, even if the threads don’t share the variables, the CPU can’t possibly know that, and it updates all the cache lines for all the cores to keep the values consistent.
    
    
     This is called cache coherency.
    
    
     If the threads shared the variables, it would be incorrect not to propagate the changes to all
    
    
     
      the cores.
     
    
   </p>
   <p>
    
     In our example, the cache coherency protocol generates quite a lot of traffic inside the CPU because all the threads
    
    <em class="italic">
     
      share the memory region where the variables reside
     
    </em>
    
     , even though they don’t from the program’s point of view.
    
    
     This is the reason we call it false sharing: the variables are shared because of the way the cache and the cache coherency
    
    
     
      protocol work.
     
    
   </p>
   <p>
    
     When we align the data to a 64-byte boundary, each instance occupies 64 bytes.
    
    
     This guarantees that they are in their own cache line and no cache coherency traffic is necessary because in this case, there’s no data sharing.
    
    
     In this second case, performance is
    
    
     
      much better.
     
    
   </p>
   <p>
    
     Let’s use
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     to confirm that this is
    
    
     
      really happening.
     
    
   </p>
   <p>
    
     First, we run
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     while executing
    
    <strong class="source-inline">
     
      sum_random_unaligned
     
    </strong>
    
     .
    
    
     We want to see how many times the program accesses the cache and how many times there’s a cache miss.
    
    
     Each time the cache needs to be updated because it contains data that’s also in a cache line in another core
    
    <a id="_idIndexMarker1054">
    </a>
    
     counts as a
    
    
     
      cache miss:
     
    
   </p>
   <pre class="console">
perf stat -e cache-references,cache-misses ./13x07-false_sharing</pre>
   <p>
    
     We obtain
    
    
     
      these results:
     
    
   </p>
   <pre class="console">
Performance counter stats for './13x07-false_sharing':
       251,277,877      cache-references
       242,797,999      cache-misses
                        # 96.63% of all cache refs</pre>
   <p>
    
     Most of the cache references are cache misses.
    
    
     This is expected because of
    
    
     
      false sharing.
     
    
   </p>
   <p>
    
     Now, if we run
    
    <strong class="source-inline">
     
      sum_random_aligned
     
    </strong>
    
     , the results are
    
    
     
      quite different:
     
    
   </p>
   <pre class="console">
Performance counter stats for './13x07-false_sharing':
           851,506      cache-references
           231,703      cache-misses
                        # 27.21% of all cache refs</pre>
   <p>
    
     The number of both cache references and cache misses is much smaller.
    
    
     This is because there’s no need to constantly update the caches in all the cores to keep
    
    
     
      cache coherency.
     
    
   </p>
   <p>
    
     In this section, we saw one of the most common performance issues of multithreaded code: false sharing.
    
    
     We saw a function example with and without false sharing and the negative impact false
    
    <a id="_idIndexMarker1055">
    </a>
    
     sharing has
    
    
     
      on performance.
     
    
   </p>
   <p>
    
     In the next section, we’ll go back to the SPSC lock-free queue we implemented in
    
    <a href="B22219_05.xhtml#_idTextAnchor097">
     
      <em class="italic">
       
        Chapter 5
       
      </em>
     
    </a>
    
     and improve
    
    
     
      its performance.
     
    
   </p>
   <h1 id="_idParaDest-278">
    <a id="_idTextAnchor277">
    </a>
    
     SPSC lock-free queue
    
   </h1>
   <p>
    
     In
    
    <a href="B22219_05.xhtml#_idTextAnchor097">
     
      <em class="italic">
       
        Chapter 5
       
      </em>
     
    </a>
    
     , we implemented
    
    <a id="_idIndexMarker1056">
    </a>
    
     an SPSC lock-free queue as an example of how to synchronize access to a data structure from two threads without using locks.
    
    
     This queue is accessed by just two threads: one producer pushing data to the queue and one consumer popping data from the queue.
    
    
     It’s the easiest queue
    
    
     
      to synchronize.
     
    
   </p>
   <p>
    
     We used two atomic variables to represent the head (buffer index to read) and tail (buffer index to write) of
    
    
     
      the queue:
     
    
   </p>
   <pre class="source-code">
std::atomic&lt;std::size_t&gt; head_ { 0 };
std::atomic&lt;std::size_t&gt; tail_ { 0 };</pre>
   <p>
    
     To avoid false sharing, we can change the code to
    
    
     
      the following:
     
    
   </p>
   <pre class="source-code">
alignas(64) std::atomic&lt;std::size_t&gt; head_ { 0 };
alignas(64) std::atomic&lt;std::size_t&gt; tail_ { 0 };</pre>
   <p>
    
     After this change, we can run the code we implemented to measure the number of operations per second (push/pop) performed by the producer and consumer threads.
    
    
     The code can be found in this book’s
    
    
     
      GitHub repository.
     
    
   </p>
   <p>
    
     Now, we can
    
    
     
      run
     
    
    
     <strong class="source-inline">
      
       perf
      
     </strong>
    
    
     
      :
     
    
   </p>
   <pre class="console">
perf stat -e cache-references,cache-misses ./13x09-spsc_lock_free_queue</pre>
   <p>
    
     We’ll get the
    
    
     
      following results:
     
    
   </p>
   <pre class="console">
101559149 ops/sec
 Performance counter stats for ‹./13x09-spsp_lock_free_queue›:
       532,295,487      cache-references
       219,861,054      cache-misses                     #   41.30% of all cache refs
       9.848523651 seconds time elapsed</pre>
   <p>
    
     Here, we can see that the queue is capable of about 100 million operations per second.
    
    
     Also, there are roughly 41%
    
    
     
      cache misses.
     
    
   </p>
   <p>
    
     Let’s review how the queue works.
    
    
     Here, the producer is the only thread writing
    
    <strong class="source-inline">
     
      tail_
     
    </strong>
    
     and the consumer is the only thread writing
    
    <strong class="source-inline">
     
      head_
     
    </strong>
    
     .
    
    
     Still, both threads need to read
    
    <strong class="source-inline">
     
      tail_
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      head_
     
    </strong>
    
     .
    
    
     We’ve declared both atomic variables as
    
    <strong class="source-inline">
     
      aligned(64)
     
    </strong>
    
     so that they’re guaranteed to be in different cache lines and there’s no false sharing.
    
    
     However, there is true sharing.
    
    
     True sharing also generates cache
    
    
     
      coherency traffic.
     
    
   </p>
   <p>
    
     True sharing means that both threads have shared access to both variables, even if each variable is just written
    
    <a id="_idIndexMarker1057">
    </a>
    
     by one thread (and always the same thread).
    
    
     In this case, to improve performance, we must reduce sharing, avoiding as much of the read access from each thread to both variables as we can.
    
    
     We can’t avoid data sharing, but we can
    
    
     
      reduce it.
     
    
   </p>
   <p>
    
     Let’s focus on the producer (it’s the same mechanism for
    
    
     
      the consumer):
     
    
   </p>
   <pre class="source-code">
bool push(const T &amp;item) {
    std::size_t tail = tail_.load(std::memory_order_relaxed);
    std::size_t next_tail = (tail + 1) &amp; (capacity_ - 1);
    if (next_tail == cache_head_) {
        cache_head_ = head_.load(std::memory_order_acquire);
        if (next_tail == cache_head_) {
            return false;
        }
    }
    buffer_[tail] = item;
    tail_.store(next_tail, std::memory_order_release);
    return true;
}</pre>
   <p>
    
     The
    
    <strong class="source-inline">
     
      push()
     
    </strong>
    
     function is only called by
    
    
     
      the producer.
     
    
   </p>
   <p>
    
     Let’s analyze what the
    
    
     
      function does:
     
    
   </p>
   <ul>
    <li>
     
      It atomically reads the last index where an item was stored in the
     
     
      
       ring buffer:
      
     
     <pre class="source-code">
std::size_t tail = tail_.load(std::memory_order_relaxed);</pre>
    </li>
    <li>
     
      It calculates
     
     <a id="_idIndexMarker1058">
     </a>
     
      the index where the item will be stored in the
     
     
      
       ring buffer:
      
     
     <pre class="source-code">
std::size_t next_tail = (tail + 1) &amp; (capacity_ - 1);</pre>
    </li>
    <li>
     
      It checks whether the ring buffer is full.
     
     
      However, instead of reading
     
     <strong class="source-inline">
      
       head_
      
     </strong>
     
      ,  it reads the cached
     
     
      
       head value:
      
     
     <pre class="source-code">
    if (next_tail == cache_head_) {</pre>
     <p class="list-inset">
      
       Initially, both
      
      <strong class="source-inline">
       
        cache_head_
       
      </strong>
      
       and
      
      <strong class="source-inline">
       
        cache_tail_
       
      </strong>
      
       are set to zero.
      
      
       As mentioned previously, the goal of using these two variables is to minimize cache updates between cores.
      
      
       The cache variables technique works like so: every time
      
      <strong class="source-inline">
       
        push
       
      </strong>
      
       (or
      
      <strong class="source-inline">
       
        pop
       
      </strong>
      
       ) is called, we atomically read
      
      <strong class="source-inline">
       
        tail_
       
      </strong>
      
       (which is written by the same thread, so no cache updates are required) and generate the next index where we’ll store the item that’s passed as a parameter to the
      
      <strong class="source-inline">
       
        push
       
      </strong>
      
       function.
      
      
       Now, instead of using
      
      <strong class="source-inline">
       
        head_
       
      </strong>
      
       to check whether the queue is full, we use
      
      <strong class="source-inline">
       
        cache_head_
       
      </strong>
      
       , which is only accessed by one thread (the producer thread), avoiding any cache coherency traffic.
      
      
       If the queue is “full,” then we update
      
      <strong class="source-inline">
       
        cache_head_
       
      </strong>
      
       by atomically loading
      
      <strong class="source-inline">
       
        head_
       
      </strong>
      
       .
      
      
       After this update, we check again.
      
      
       If the second check results in the queue being full, then we
      
      
       
        return
       
      
      
       <strong class="source-inline">
        
         false
        
       </strong>
      
      
       
        .
       
      
     </p>
     <p class="list-inset">
      
       The advantage of using these local variables (
      
      <strong class="source-inline">
       
        cache_head_
       
      </strong>
      
       for the producer and
      
      <strong class="source-inline">
       
        cache_tail_
       
      </strong>
      
       for the consumer) is that they reduce true sharing – that is, accessing variables that may be updated in the cache of a different core.
      
      
       This will work better when the producer pushes several items in the queue before the consumer tries to get them (same for the consumer).
      
      
       Say that the producer inserts 10 items in the queue and the consumer tries to get one item.
      
      
       In this case, the first check with the cache variable will tell us that the queue is empty but after updating with the real value, it will be OK.
      
      
       The consumer can get nine more items just by checking whether the queue is empty by only reading the
      
      
       <strong class="source-inline">
        
         cache_tail_
        
       </strong>
      
      
       
        variable.
       
      
     </p>
    </li>
    <li>
     
      If the ring buffer
     
     <a id="_idIndexMarker1059">
     </a>
     
      is full, then
     
     
      
       update
      
     
     
      <strong class="source-inline">
       
        cache_head_
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
head_.load(std::memory_order_acquire);
        if (next_tail == cache_head_) {
            return false;
        }</pre>
    </li>
    <li>
     
      If the buffer is full (not just that
     
     <strong class="source-inline">
      
       cache_head_
      
     </strong>
     
      needs to be updated), then return
     
     <strong class="source-inline">
      
       false
      
     </strong>
     
      .
     
     
      The producer can’t push a new item to
     
     
      
       the queue.
      
     
    </li>
    <li>
     
      If the buffer isn’t full, add the item to the ring buffer and
     
     
      
       return
      
     
     
      <strong class="source-inline">
       
        true
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
buffer_[tail] = item;
    tail_.store(next_tail, std::memory_order_release);
    return true;</pre>
    </li>
   </ul>
   <p>
    
     We’ve potentially reduced the number of times the producer thread will access
    
    <strong class="source-inline">
     
      tail_
     
    </strong>
    
     and hence reduced cache coherency traffic.
    
    
     Think about this case: the producer and the consumer use the queue and the producer calls
    
    <strong class="source-inline">
     
      push()
     
    </strong>
    
     .
    
    
     When
    
    <strong class="source-inline">
     
      push()
     
    </strong>
    
     updates
    
    <strong class="source-inline">
     
      cache_head_
     
    </strong>
    
     , it may be more than one slot ahead of
    
    <strong class="source-inline">
     
      tail_
     
    </strong>
    
     , which means we don’t need to
    
    
     
      read
     
    
    
     <strong class="source-inline">
      
       tail_
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     The same principle applies to the consumer
    
    
     
      and
     
    
    
     <strong class="source-inline">
      
       pop()
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     Let’s run
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     again after modifying the code to reduce cache
    
    
     
      coherency traffic:
     
    
   </p>
   <pre class="console">
162493489 ops/sec
 Performance counter stats for ‹./13x09-spsp_lock_free_queue›:
       474,296,947      cache-references
       148,898,301      cache-misses                     #   31.39% of all cache refs
       6.156437788 seconds time elapsed
      12.309295000 seconds user
       0.000999000 seconds sys</pre>
   <p>
    
     Here, we can that performance has improved by about 60% and that there is a smaller number of cache
    
    <a id="_idIndexMarker1060">
    </a>
    
     references and
    
    
     
      cache misses.
     
    
   </p>
   <p>
    
     With that, we’ve learned how reducing access to shared data between two threads can
    
    
     
      improve performance.
     
    
   </p>
   <h1 id="_idParaDest-279">
    <a id="_idTextAnchor278">
    </a>
    
     Summary
    
   </h1>
   <p>
    
     In this chapter, we covered three methods you can use to profile your code:
    
    <strong class="source-inline">
     
      std::chrono
     
    </strong>
    
     , micro-benchmarking with the Google Benchmark library, and the Linux
    
    
     <strong class="source-inline">
      
       perf
      
     </strong>
    
    
     
      tool.
     
    
   </p>
   <p>
    
     We also saw how to improve multithreaded programs’ performance by both reducing/eliminating false sharing and reducing true sharing, reducing the cache
    
    
     
      coherency traffic.
     
    
   </p>
   <p>
    
     This chapter provided a basic introduction to some profiling techniques that will be very useful as a starting point for further studies.
    
    
     As we said at the beginning of this chapter, performance is a complex subject and deserves its
    
    
     
      own book.
     
    
   </p>
   <h1 id="_idParaDest-280">
    <a id="_idTextAnchor279">
    </a>
    
     Further reading
    
   </h1>
   <ul>
    <li>
     
      Fedor G.
     
     
      Pikus,
     
     <em class="italic">
      
       The Art of Writing Efficient Programs
      
     </em>
     
      , First Edition, Packt
     
     
      
       Publishing, 2021.
      
     
    </li>
    <li>
     
      Ulrich Drepper,
     
     <em class="italic">
      
       What Every Programmer Should Know About
      
     </em>
     
      <em class="italic">
       
        Memory
       
      </em>
     
     
      
       , 2007.
      
     
    </li>
    <li>
     
      Shivam Kunwar,
     
     <em class="italic">
      
       Optimizing Multithreading
      
     </em>
     
      <em class="italic">
       
        Performance
       
      </em>
     
     
      
       (
      
     
     <a href="https://www.youtube.com/watch?v=yN7C3SO4Uj8">
      
       
        https://www.youtube.com/watch?v=yN7C3SO4Uj8
       
      
     </a>
     
      
       ).
      
     
    </li>
   </ul>
  </div>
 </body></html>