- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analyzing and Optimizing the Performance of Our C++ System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will analyze the performance of our electronic trading ecosystem
    based on the measurements we added in the previous chapter, *Adding instrumentation
    and measuring performance*. Using the insights we develop about the performance
    of our trading systems based on this analysis, we will learn what areas to focus
    on in terms of potential performance bottlenecks and what areas we can improve.
    We will discuss tips and techniques for optimizing our C++ trading ecosystem.
    Finally, we will think about the future of our electronic trading ecosystem and
    what enhancements can be made in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the performance of our trading ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing tips and techniques for optimizing our C++ trading system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thinking about the future of our trading ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the book’s code can be found in the GitHub repository for this book at [https://github.com/PacktPublishing/Building-Low-Latency-Applications-with-CPP](https://github.com/PacktPublishing/Building-Low-Latency-Applications-with-CPP).
    The source code for this chapter is in the `Chapter12` directory in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is the concluding chapter of this book and we will discuss tips for
    improving the performance of the full electronic trading ecosystem as well as
    future enhancements, we expect you to have gone through all the preceding chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The specifications of the environment in which the source code for this book
    was developed are listed as follows. We have presented the details of this environment
    since all the C++ code presented in this book is not necessarily portable and
    might require some minor changes to work in your environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'OS: `Linux 5.19.0-41-generic #42~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Apr
    18 17:40:00 UTC 2 x86_64 x86_64` `x86_64 GNU/Linux`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GCC: `g++ (Ubuntu` `11.3.0-1ubuntu1~22.04.1) 11.3.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CMake: `cmake` `version 3.23.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ninja: `1.10.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, for those who are interested in running the *optional* Python
    Jupyter notebook included with this chapter, the following environment was used.
    We will not discuss the installation process for Python, Jupyter, and these libraries
    and assume that you will figure it out on your own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Analyzing the performance of our trading ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we analyze the performance of our electronic trading ecosystem, let us
    quickly recap the measurements we added in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the latencies we measure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We added two forms of measurement. The first one measures the performance of
    internal components and the second one generates timestamps at key points in our
    entire system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first form, which measures the latencies of internal components, generates
    differences in `RDTSC` values before and after calls to different functions, and
    generates log entries such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The second form, which measures the latencies at key points in the trading
    ecosystem, generates absolute timestamp values and generates log entries such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, let us move forward and analyze these latency measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To analyze these performance metrics, we have built a Python Jupyter notebook,
    which is available at `Chapter12/notebooks/perf_analysis.ipynb`. Note that since
    this is a book about C++ and low-latency applications, we will not discuss the
    source code in this notebook, but instead describe the analysis. Running the notebook
    is optional, so we also included an HTML file with the results of this analysis,
    which is available at `Chapter12/notebooks/perf_analysis.html`. To run this notebook,
    you will first have to launch the `jupyter notebook` server from the `Chapter12`
    root directory (where the log files exist) using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If your browser does not already launch the web page for this notebook, you
    can copy and paste the URL you receive and navigate to and open the `notebooks/perf_analysis.ipynb`
    notebook. Note that the preceding addresses are just examples for this specific
    run; you will receive a different address, which you should use. Once you open
    the notebook, you can run it using **Cell** | **Run All**, or the closest equivalent
    in your notebook instance, as shown in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Screenshot of the perf_analysis.ipynb notebook](img/B19434_12_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – Screenshot of the perf_analysis.ipynb notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we will not discuss the details of this notebook, we will briefly describe
    the analysis performed in it. This notebook performs the following steps in the
    order presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it looks for the log files generated by running the electronic trading
    ecosystem in the current working directory. Specifically, it looks for log files
    from the trading exchange; in the case of this notebook, we look for log files
    from the trading client with `ClientId=1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It opens each log file and looks for log entries that contain the `RDTSC` and
    `TTT` tokens in them to find the log entries corresponding to the measurements
    we discussed in the previous chapter and revisited in the preceding sub-section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then creates two `pandas` `DataFrame` instances containing each of the measurements
    it extracts from the log files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the measurement entries corresponding to the measurement of internal functions,
    which are tagged with the `RDTSC` token, we generate a scatter plot of those measurements
    as well as a rolling mean of those plots (to smooth the overall latency measurements).
    One crucial point here is that the measurement values in the log files represent
    the difference in `RDTSC` values, that is, the number of CPU cycles elapsed for
    a function call. In this notebook, we convert the CPU cycles into nanoseconds
    using a constant factor of 2.6 GHz, which is specific to our system and will differ
    based on your hardware; it will need to be adjusted. We will look at a few examples
    of these plots in the next sub-section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the measurement entries corresponding to the timestamps at key spots in
    our electronic trading ecosystem, which are tagged with the `TTT` token, we also
    generate a scatter plot and a plot of the rolling mean values. The difference
    here is that we display the transit times from one hop to the other. For instance,
    we will plot the time it takes from the hop at `T1_OrderServer_TCP_read` to the
    hop at `T2_OrderServer_LFQueue_write`, from `T2_OrderServer_LFQueue_write` to
    `T3_MatchingEngine_LFQueue_read`, from `T3_MatchingEngine_LFQueue_read` to `T4_MatchingEngine_LFQueue_write`,
    and so forth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of these inter-hop transits on the side of the exchange is shown in the
    following diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Flow of data between different hops at the electronic exchange](img/B19434_12_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – Flow of data between different hops at the electronic exchange
  prefs: []
  type: TYPE_NORMAL
- en: Each of these inter-hop transits on the side of the trading client is shown
    in the following diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Flow of data between different hops on the electronic trading
    client](img/B19434_12_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – Flow of data between different hops on the electronic trading
    client
  prefs: []
  type: TYPE_NORMAL
- en: In the next sub-section, we will observe the distribution of a few of these
    different latency metrics from both groups (`RDTSC` and `TTT`) and see what we
    can learn from them.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the output of our analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will present the distribution of the latencies for a subset
    of the measurements we added in the previous chapter and analyzed using the notebook
    presented in the previous sub-section. Our objective here is to gain some insight
    into the performance of different components and sub-components in our ecosystem.
    First, we will start with a few examples of latencies for internal function calls
    in the next sub-section. One thing to note is that for the sake of brevity, we
    will present and discuss a subset of all the performance plots available in the
    Python notebook in this chapter. Also, note that these are not arranged in any
    particular order; we simply picked some of the more interesting ones and left
    all possible plots in the notebook for you to inspect further.
  prefs: []
  type: TYPE_NORMAL
- en: Observing the latencies for internal function calls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first performance plot we present in this chapter is the distribution of
    the latency of calling the `Exchange::MEOrderBook::removeOrder()` method in the
    matching engine inside the trading exchange. That is presented as follows, but
    our key takeaway here is that this is a very well-behaved function; that is, the
    minimum and maximum latencies are within a tight range between 0.4 and 3.5 microseconds
    and the mean is relatively stable around the 1-to-1.5-microsecond range. There
    might be the possibility to make this faster, of course, but this seems quite
    well behaved for now and has low-performance latencies; we should evaluate whether
    this method is a bottleneck before trying to optimize it any further.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Latency distribution for the removeOrder() method in MEOrderBook
    for the matching engine](img/B19434_12_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Latency distribution for the removeOrder() method in MEOrderBook
    for the matching engine
  prefs: []
  type: TYPE_NORMAL
- en: 'The next plot presents the distribution of latencies for the `Exchange::FIFOSequencer::``     sequenceAndPublish()` method. This instance is more interesting because here we
    see that while this method has low average latencies in the 90 microseconds range,
    it experiences many spikes in latencies spiking up to values in the 500 to 1,200
    microseconds range. This behavior will result in jitter in the `OrderServer` component’s
    performance when it comes to processing client order requests and is something
    we might need to investigate.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Latency distribution of the sequenceAndPublish() method in
    FIFOSequencer for the matching engine](img/B19434_12_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – Latency distribution of the sequenceAndPublish() method in FIFOSequencer
    for the matching engine
  prefs: []
  type: TYPE_NORMAL
- en: The next plot shows another interesting distribution of latency values for the
    `Trading::PositionKeeper::addFill()` method. In this case, the average performance
    latency remains stable around the 50 microseconds range. However, between **15:28:00**
    and **15:29:00**, there are a few spikes in latency that warrant a closer look.
    The difference here compared to *Figure 12**.4* is that there the spikes were
    distributed evenly, but in this case, there appears to be a small patch of spikes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Latency distribution of the addFill() method in PositionKeeper
    for the trade engine](img/B19434_12_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 – Latency distribution of the addFill() method in PositionKeeper
    for the trade engine
  prefs: []
  type: TYPE_NORMAL
- en: 'We conclude this sub-section by presenting one more plot, this time of the
    `Trading::``     PositionKeeper::updateBBO()` method, which updates the PnL for open positions.
    This is another well-behaved method with an average performance latency of 10
    microseconds, and there seem to be many measurements close to 0 microseconds,
    which is slightly different from *Figure 12**.3*, where the minimum latency value
    was never remarkably close to 0.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Latency distribution of the updateBBO() method in PositionKeeper
    for the trade engine](img/B19434_12_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 – Latency distribution of the updateBBO() method in PositionKeeper
    for the trade engine
  prefs: []
  type: TYPE_NORMAL
- en: In the next sub-section, we will look at a few similar examples, but this time
    pertaining to the latencies between the different hops in our ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Observing the latencies between hops in the ecosystem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first plot we will look at is the time difference between when a trading
    client’s `OrderGateway` component writes a client request to the TCP socket (`T12`)
    up to the point when the exchange’s `OrderServer` component reads that client
    request from the TCP socket (`T1`). This represents the network transit time from
    the trading client to the trading exchange on the TCP connection. The average
    latency in this case is around 15 to 20 microseconds and the distribution is evenly
    distributed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Latency distribution between the T12_OrderGateway_TCP_write
    and T1_OrderServer_TCP_read hops](img/B19434_12_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 – Latency distribution between the T12_OrderGateway_TCP_write and
    T1_OrderServer_TCP_read hops
  prefs: []
  type: TYPE_NORMAL
- en: The next plot displays the distribution of the network transit time for the
    market data updates, from when the market data updates are written to the UDP
    socket by `MarketDataPublisher` (`T6`) to when they are read from the UDP socket
    by `MarketDataConsumer` (`T7`). There seems to be a great amount of variance in
    the latencies for this measurement, as the plot shows; however, this has lower
    overall latencies than the TCP path.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Latency distribution between the T6_MarketDataPublisher_UDP_write
    and T7_MarketDataConsumer_UDP_read hops](img/B19434_12_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.9 – Latency distribution between the T6_MarketDataPublisher_UDP_write
    and T7_MarketDataConsumer_UDP_read hops
  prefs: []
  type: TYPE_NORMAL
- en: The next diagram shows the distribution of latencies measured from `MarketDataConsumer`
    reading a market update from the UDP socket (`T7`) to the time when the market
    update is written to `LFQueue` connected to `TradeEngine` (`T8`). This path experiences
    huge spikes in latencies (up to 2,000 microseconds) compared to its average performance
    of around 100 microseconds, so this is something we need to investigate.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10 – Latency distribution between the T7_MarketDataConsumer_UDP_read
    and T8_MarketDataConsumer_LFQueue_write hops](img/B19434_12_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.10 – Latency distribution between the T7_MarketDataConsumer_UDP_read
    and T8_MarketDataConsumer_LFQueue_write hops
  prefs: []
  type: TYPE_NORMAL
- en: The next plot displays the distribution of the latencies between `MatchingEngine`
    reading a client request from `LFQueue` attached to `OrderServer` (`T3`) and the
    time `MatchingEngine` processes it and writes the client response to `LFQueue`
    back to `OrderServer` (`T4t`). This path also appears to be experiencing large
    latency spikes and should be investigated.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Latency distribution between the T3_MatchingEngine_LFQueue_read
    and T4t_MatchingEngine_LFQueue_write hops](img/B19434_12_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.11 – Latency distribution between the T3_MatchingEngine_LFQueue_read
    and T4t_MatchingEngine_LFQueue_write hops
  prefs: []
  type: TYPE_NORMAL
- en: This section was dedicated to the analysis of the different latency measurements
    in our ecosystem. In the next section, we will discuss some tips and techniques
    that we can use to optimize the design and implementation of the different components
    in our electronic trading ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Discussing tips and techniques for optimizing our C++ trading system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will present a few possible areas where we can optimize
    our C++ trading ecosystem. Note that these are only some examples and a lot more
    is possible, but we will leave you to measure and discover those inefficiencies,
    as well as improve on them. To reiterate what we have mentioned a few times before,
    you should measure the performance of various parts of your system with everything
    we learned in the previous chapter, *Adding instrumentation and measuring performance*.
    You should analyze them using the approach we discussed in this chapter and use
    the C++ discussions we had in the chapter *Exploring C++ Concepts from a Low-Latency
    Application’s Perspective* to improve on them further. Now, let us discuss some
    areas of improvement next. We have tried to arrange these loosely in order from
    least to most effort.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the release build
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first suggestion would be to try and optimize the release build we run for
    our system. Some simple things we can do in the code itself is remove the calls
    to `ASSERT()` from the release binaries. The motivation behind this is to remove
    the extra `if` condition this macro introduces in our code base wherever it gets
    used. However, this can be dangerous since we might allow exceptional conditions
    through. The optimal middle ground is to remove the use of this macro only from
    the critical code path wherever it is safe to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Another suggestion would be to reduce logging in the release build. We have
    made a decent amount of effort to make logging efficient and low-latency. Additionally,
    it is not wise to eliminate all logging since it makes troubleshooting difficult,
    if not impossible. However, logging is not free, so we should try to reduce logging
    on the critical path for release builds as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The most common method to perform optimizations, as we suggested here, that
    only apply to release builds is to define the NDEBUG (No Debug) preprocessor flag
    and check for its existence in our code base. If the flag is defined, we build
    a release build and skip non-essential code such as asserts and logging.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this for the `MemoryPool::deallocate()` method is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Another example for the `FIFOSequencer::sequenceAndPublish()` method is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Another thing to think about is whether the actual entries being logged can
    be output in a more optimal method. For instance, `Common:: getCurrentTimeStr()`,
    which gets called in each of our log lines in the current code base state itself,
    is quite expensive. This is because it performs string formatting operations using
    `sprintf()`, which is quite expensive, like most string formatting operations.
    Here, we have another optimization where in release builds, we can output a simple
    integer value representing time, instead of a formatted string, which, while more
    readable, is less efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: Let us move on to the next possible optimization area – managing thread affinity.
  prefs: []
  type: TYPE_NORMAL
- en: Setting thread affinity correctly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, in all the instances of creating and launching threads, we have passed
    the `core_id` parameter to be `-1` in the call to the `Common::createAndStartThread()`
    method; that is, the threads were not pinned to any specific core. This was done
    intentionally since, as we mentioned before, the `exchange_main` application instance
    creates and runs 10 threads and each `trading_main` application instance creates
    and runs 8 threads. Unless you are executing the source code for this book on
    a production-grade trading server, it is unlikely to have too many CPU cores.
    Our system, for example, has only four cores. In practice, however, each of the
    following performance-critical threads would be assigned a CPU core all to themselves.
    We present a sample core assignment next; however, this will change from server
    to server and might also depend on the NUMA architecture – but that is beyond
    the scope of this book. Note that these names refer to the names we passed to
    the method in the `name` string parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`core_id`=0 : `Exchange/MarketDataPublisher`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=1 : `Exchange/MatchingEngine`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=2 : `Exchange/OrderServer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=3 : `Trading/MarketDataConsumer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=4 : `Trading/OrderGateway`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=5 : `Trading/TradeEngine`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any additional performance critical threads get assigned the remaining core
    ids in a similar fashion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The remaining non-critical threads, as well as any Linux processes running
    on the server, would be given a block of CPU cores to be run on without any affinity
    settings. Specifically, in our system, they would be the following non-critical
    threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '`core_id`=-1 : `Exchange/SnapshotSynthesizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=-1 : `Common/Logger exchange_main.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=-1 : `Common/Logger exchange_matching_engine.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=-1 : `Common/Logger exchange_market_data_publisher.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=-1 : `Common/Logger exchange_snapshot_synthesizer.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=-1 : `Common/Logger exchange_order_server.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=-1 : `Common/Logger trading_main_1.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=-1 : `Common/Logger trading_engine_1.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=-1 : `Common/Logger trading_order_gateway_1.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`core_id`=-1 : `Common/Logger trading_market_data_con``sumer_1.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any other non-critical threads would also be assigned core id -1 i.e. these
    threads will not be pinned to any specific CPU code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note one additional detail: for this setup to be as optimized as possible,
    we need to make sure that the Linux process scheduler does not assign any OS processes
    to the CPU cores being used by the critical threads. This is achieved on Linux
    using the `isolcpus` kernel parameter, which we will not discuss in detail here.
    The `isolcpus` parameter tells the process scheduler which cores to ignore when
    deciding where to schedule a process.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Logger for strings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is an opportunity to optimize our `Logger` class to handle parameters
    of the `char*` type better. Remember that our implementation for logging `char*`
    parameters consists of calling the `Logger::pushValue(const char value)` method
    on each of the characters iteratively, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'One option here is to introduce a new enumeration value to the `LogType` enumeration.
    Let’s call it `STRING`, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll update the `LogElement` type to have a fixed-size `char*` array of *some*
    size, as shown. We are vague on the size of this array on purpose since this is
    pseudo-code and we want to focus more on the design and the idea and less on the
    implementation details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Then, finally, update `Logger::pushValue(const char *value)` and `Logger::flushQueue()`
    to copy and write the strings in blocks of characters rather than a single character
    at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Eliminating the use of std::function instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our code base, we used the `std::function<>` function wrapper in a couple
    of places, as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Common::McastSocket`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Common::TCPServer`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Common::TCPSocket`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Trading::TradeEngine`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Calling functions through these objects is slower than directly calling functions,
    and these incur similar costs as `virtual` functions. This mechanism of calling
    methods using the `std::function<>` objects can be replaced with templates. To
    refresh your memory on the drawbacks of calling functions indirectly, please revisit
    the chapter *Exploring C++ Concepts from a Low-Latency Application’s Perspective*,
    specifically the *Avoiding function pointers* sub-section of the *Calling functions
    efficiently* section. Additionally, revisit the *Using compile-time polymorphism*
    section in the same chapter, reviewing the discussion on the `std::function<>`
    instances in our code base, but we encourage those who are interested to attempt
    that improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the impact of these optimizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will not be able to investigate every optimization opportunity in detail,
    but before we finish this section, we will discuss the details of two optimizations
    that we discussed in this section. First, let us discuss the implementation and
    impact of the optimization on our `Logger` class for logging strings.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking Logger string optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To implement the Logger string optimization, we will change the `pushValue()`
    method for `char*` arguments as discussed before. For the sake of brevity, we
    will not look at the full class, which we implement in an alternate `OptLogger`
    class available in the `Chapter12/common/opt_logging.h` source file. The most
    important change is shown here, but please refer to the full source file to see
    the other minor changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: To benchmark this and compare it against the original `Logger` implementation,
    we will create a simple standalone binary called `logger_benchmark`. We do this
    so that we can check the performance impact in a controlled environment. Remember
    that running the full trading ecosystem introduces a lot of variance due to the
    number of processes and threads, the network activity, the trading activity, and
    so on, and it can be difficult to properly assess the impact of the `Logger` optimization.
    The source code for this benchmark application can be found in the `Chapter12/benchmarks/logger_benchmark.cpp`
    source file. Let us look at the implementation of this source file quickly before
    looking at the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will include the header files corresponding to the original `Logger`
    and the new `OptLogger` classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define a `random_string()` method, which simply generates random
    strings of a specified length. We will use this to generate random strings for
    the two loggers to log to compare the performance difference when it comes to
    strings. This uses a `charset()` lambda method, which returns a random alphanumeric
    (0-9, a-z, or A-z) character. It then uses the `std::generate_n()` method to generate
    a `std::string` with a length specified in the length argument by calling the
    `charset()` lambda method repeatedly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define a `benchmarkLogging()` method, which accepts a template
    parameter, `T`, which it expects to be an instance of one of the two loggers we
    are comparing here. It runs a loop 100,000 times and uses the `random_string()`
    method we built previously and the logger’s `log()` method to log 100,000 random
    strings. For each call to the `log()` method, it records and sums up the difference
    in clock cycles, using the `Common::rdtsc()` method we built in the previous chapter.
    Finally, it returns the average clock cycles by dividing the sum of each RDTSC
    difference by the loop count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can finally build the `main()` method, which is quite simple. It creates
    an instance of the old logger – `Common::Logger()` – calls the `benchmarkLogging()`
    method on it, and outputs the average clock cycle count to the screen. Then, it
    does exactly the same thing again, except this time it uses the new logger – `OptCommon::OptLogger()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This binary can be built using the same script as before, that is, by running
    `scripts/build.sh` from the `Chapter12` root directory. To run the binary, you
    can call it directly from the command line, as shown here, and, among other output,
    you will see the following two lines displaying the results of the benchmarking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that there will be some variance in the output for each run, and the results
    you get will likely be different due to system-dependent reasons, but amount the
    optimization has sped things up, should be somewhat similar to what we have shown.
    In this case, it seems like our optimization efforts have sped up the `log()`
    method for strings to be roughly 50 times faster. Next, let us look at another
    example of the optimization tips we discussed before, which is optimizing the
    binary for release builds.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking release build optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To benchmark an example of leaving out non-essential code from the release
    build, we picked the `MemPool` class. Note that this principle applies to all
    the components we built, but we arbitrarily picked a single one to limit the scope
    of our discussion. Similar to what we did for the `Logger` class, we create a
    new class called `OptMemPool`, which you will find in the `Chapter12/common/opt_mem_pool.h`
    source file. The primary change in this file compared to the `MemPool` class is
    that the calls to `ASSERT()` are only built for non-release builds. This is achieved
    by checking for the `NDEBUG` preprocessor flag, as shown in the following two
    examples. You can check out the full source code in the file we mentioned previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To benchmark this optimization, we will build a `release_benchmark` binary,
    and the code for that is available in the `Chapter12/benchmarks/release_benchmark.cpp`
    source file. First, let us look at the header files we need to include, most importantly
    the `mem_pool.h` and `opt_mem_pool.h` files. Since memory pools store structures,
    we will use `Exchange::MDPMarketUpdate` as an example, so we include the `market_update.h`
    header file for that as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to what we did with the `logger_benchmark.cpp` file, we will create
    a `benchmarkMemPool()` method, which accepts a template parameter, `T`, and expects
    it to be one of the two memory pools we are comparing. In this method, we will
    first allocate and save 256 `MDPMarketUpdate` objects from the memory pool, using
    the `allocate()` method. Then, we will deallocate each of these objects and return
    them to the memory pool, using the `deallocate()` method. We will run this loop
    100,000 times to find a reliable average over many iterations. We will measure
    and sum up the clock cycles elapsed for each call to `allocate()` and `deallocate()`
    as we did before with the logger benchmark. Finally, we return the average clock
    cycles by dividing the sum of elapsed clock cycles by the loop count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we build the `main()` method, which again is quite simple. It calls
    the `benchmarkMemPool()` method twice, once with an object of the `Common::MemPool`
    type and next with an object of the `OptCommon::OptMemPool` type, and outputs
    the average clock cycles elapsed for the `allocate()` and `deallocate()` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The process to build this benchmark binary remains the same, so we will not
    repeat it. Running the binary will yield something that resembles the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this case, our optimization efforts yielded a speed up of around 7 to 8 times
    for the `allocate()` and `deallocate()` methods.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we presented and explained a subset of optimization areas/ideas
    in our electronic trading ecosystem. The goal here is to get you to understand
    what these optimization areas can look like and how to approach them with the
    goal of optimizing performance. In the next section, we’ll discuss some more future
    improvements and enhancements that can be made to our electronic trading ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about the future of our trading ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we conclude this chapter and this book, we will discuss a few possible
    enhancements to our electronic trading ecosystem. In the previous section, we
    discussed some examples of things that can be optimized for those interested in
    maximizing the performance of the electronic trading system we built in this book.
    In this section, we will discuss some examples of how this ecosystem can be enhanced,
    not necessarily to reduce latency but to make the system more feature-rich and
    add functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Growing containers dynamically
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We built and used a few containers in this book, as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: The lock-free queue – `LFQueue` – which is used in multiple components for various
    object types, such as `MEMarketUpdate`, `MDPMarketUpdate`, `MEClientRequest`,
    and `MEClientResponse`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The memory pool – `MemPool` – which was used for multiple object types, such
    as instances of `MEMarketUpdate`, `MEOrder`, `MEOrdersAtPrice`, `MarketOrder`,
    and `MarketOrdersAtPrice`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all these cases, we assumed a safe maximum size value. In practice, that
    still leaves us open to the possibility that under some circumstances, we might
    exceed these limits and get in trouble. One enhancement we can make to this system
    is to improve our handling of this unlikely edge case.
  prefs: []
  type: TYPE_NORMAL
- en: One option would be to fail/exit if we encounter a scenario where `LFQueue`
    is full or `MemPool` is out of memory. Another option would be to fall back to
    dynamic memory allocation or a secondary inefficient container for this unlikely
    event; that is, we will be inefficient and slow in this extremely rare case that
    we run out of memory or space in our containers, but we will continue to function
    until it is resolved. Yet another option is to make these containers flexible
    where they can be grown if needed even though the task of growing these containers
    when needed will be extremely slow, since in practice we do not expect to encounter
    that condition.
  prefs: []
  type: TYPE_NORMAL
- en: Growing and enhancing the hash maps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this book, we used `std::array` in many contexts as a hash map by assuming
    a safe upper bound. For instance, by assuming that valid `TickerId` values fall
    in the range of 0 and `ME_MAX_TICKERS`, we used `std::array` instances of size
    `ME_MAX_TICKERS` as hash maps with `TickerId` keys. A similar design was used
    for containers such as `TradeEngineCfgHashMap`, `OrderHashMap`, `ClientOrderHashMap`,
    `OrdersAtPriceHashMap`, `OrderBookHashMap`, `MarketOrderBookHashMap`, and `OMOrderTickerSideHashMap`.
    While in practice, some of these can continue to exist, that is, valid and reasonable
    upper bounds can be decided and used, for some of these, this design will not
    scale up elegantly.
  prefs: []
  type: TYPE_NORMAL
- en: There are several different hash map implementations available – `std::unordered_map`,
    `absl::flat_hash_map`, `boost::` hash maps, `emhash7::HashMap`, `folly::AtomicHashmap`,
    `robin_hood::unordered_map`, `tsl::hopscotch_map`, and many more. Additionally,
    it is common to optimize and tweak these containers so that they perform best
    under our specific use cases. We’ll leave those of you who are interested with
    the task of exploring these and deciding which ones can replace the `std::array`-based
    hash maps in our system.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of demonstrating an example, we will replace the `std::array`-based
    hash maps in the limit order book that the matching engine builds and maintains
    (`MEOrderBook`) with `std::unordered_map` hash maps. We will then benchmark the
    two implementations to see how much of a difference it makes. Following the same
    pattern as we used in the benchmarking we performed earlier in this chapter, we
    will introduce a new `MEOrderBook` class, `UnorderedMapMEOrderBook`, where the
    only difference is the use of the `std::unordered_map` containers instead of the
    `std::array` containers. All the source code for this new class is available in
    the `Chapter12/exchange/matcher/unordered_map_me_order_book.h` and `Chapter12/exchange/matcher/unordered_map_me_order_book.cpp`
    source files. For the sake of brevity, we will not repeat the entire class implementation
    here, but we will discuss the important changes. The first important and obvious
    change is the inclusion of the `unordered_map` header file in the `unordered_map_me_order_book.h`
    header file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We change the `cid_oid_to_order_` data member to be `std::unordered_map<ClientId,
    std::unordered_map<OrderId, MEOrder *>>` instead of `ClientOrderHashMap`, which
    is a `typedef` for `std::array<OrderHashMap, ME_MAX_NUM_CLIENTS>`. This data member
    is a hash map from `ClientId` to `OrderId` to `MEOrder` objects. Remember that
    `ClientOrderHashMap` is actually a hash map of hash maps, that is, a `std::array`
    whose elements are also `std::array` objects. The other data member we change
    is the `price_orders_at_price_` member, which we change to `std::unordered_map<Price,
    MEOrdersAtPrice *>` instead of the `OrdersAtPriceHashMap` type. This data member
    is a hash map from `Price` to `MEOrdersAtPrice` objects. If you have forgotten
    what `MEOrder` and `MEOrdersAtPrice` are, please revisit the *Designing the exchange
    order book* sub-section in the *Defining the operations and interactions in our
    matching engine* section of the chapter *Building the C++ Matching Engine*. These
    changes are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to remove the following lines from the destructor since the `fill()`
    method does not apply to `std::unordered_map` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In terms of accessing these modified containers, we replace the calls to the
    `std::array::at()` method for `cid_oid_to_order_` and `price_orders_at_price_`
    with the `std::unordered_map::operator[]` method. These changes for `cid_oid_to_order_`
    are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to make similar changes in spots where we access the `price_orders_at_price_`
    container, which is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we present the `hash_benchmark` binary to measure the performance
    differences because of these changes. The source code for this binary can be found
    in the `Chapter12/benchmarks/hash_benchmark.cpp` source file. First, we include
    the header files shown as follows and also define a global `loop_count` variable
    as we have done in our previous benchmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As we have done before, we will define a `benchmarkHashMap()` method, which
    accepts a template parameter, `T`, to represent either `MEOrderBook` or `UnorderedMapMEOrderBook`.
    It also accepts a vector of `Exchange::MEClientRequest` messages, which will be
    processed in the benchmark. The actual processing is quite simple. It checks the
    type of `MEClientRequest` and then calls the `add()` method for `ClientRequestType::NEW`
    and the `cancel()` method for `ClientRequestType::CANCEL`. We use `Common::rdtsc()`
    to measure and sum up the clock cycles elapsed for each of these calls and then
    return the average at the end of this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can look at the `main()` method. We need `Logger` and a `MatchingEngine`
    object to create the `MEOrderBook` or `UnorderedMapMEOrderBook` object, but to
    create the `MatchingEngine` object, we need three lock-free queues as we have
    seen in the implementation of the `exchange_main` binary. So, we create these
    objects as shown here, even though we are not measuring the performance of any
    of these components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create a vector of 100,000 (`loop_count`) `MEClientRequest` objects,
    which will be composed of new order requests as well as requests to cancel these
    orders. We have seen similar code in the `trading_main` application for the random
    trading algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we end the `main()` method by calling the `benchmarkHashMap()` method
    twice – once with an instance of `MEOrderBook` and once with an instance of `UnorderedMapMEOrderBook`,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The process to build this application remains the same, which is by calling
    the `scripts/build.sh` script from the `Chapter12` root directory. Running the
    application by calling the `hash_benchmark` binary will yield output like what
    is shown here, with some variance between independent runs and depending on the
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Based on the output of this run, it appears that switching from a `std::array`
    hash map implementation to a `std::unordered_map` hash map implementation adds
    an approximate 6 to 7% extra overhead to the `MEOrderBook` `add()` and `cancel()`
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing snapshot messages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our design of the snapshot messages in the `MarketDataPublisher` component
    at the trading exchange, a full cycle of snapshot messages between the `START_SNAPSHOT`
    and `END_SNAPSHOT` messages contains the snapshot for all trading instruments,
    as shown in the following diagram (which we have seen before). In our `SnapshotSynthesizer`,
    this full snapshot for all trading instruments is published once every 60 seconds.
    What this means is that if the order books for each of these trading instruments
    have a lot of orders, then every 60 seconds, there is a huge spike in network
    traffic on the snapshot multicast channels followed by silence in the remaining
    60 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.12 – Current composition of snapshot messages](img/B19434_12_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.12 – Current composition of snapshot messages
  prefs: []
  type: TYPE_NORMAL
- en: It would be an enhancement to this design if we changed this such that these
    snapshots are spaced out more evenly and each snapshot cycle contained the snapshot
    messages corresponding to only one `TickerId`. As a simple example, instead of
    sending a snapshot message cycle for 6 instruments every 60 seconds, we can send
    6 snapshots each containing information for a single instrument, and each of these
    snapshots is spaced out with 10 seconds in between them. This hypothetical proposal
    is represented in the following diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.13 – A proposal for an optimized snapshot messaging format](img/B19434_12_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.13 – A proposal for an optimized snapshot messaging format
  prefs: []
  type: TYPE_NORMAL
- en: In this new proposal, as we mentioned, there are fewer spikes in network traffic
    since the full snapshot is distributed over time. This leads to a lower chance
    of dropping packets on the snapshot multicast stream for the `MarketDataConsumer`
    components in the trading client’s systems. This also leads to the client’s system
    synchronizing or catching up with the snapshot stream for each trading instrument
    faster, since it does not need to wait for the full snapshot across all trading
    instruments before it can mark some of the instruments as *recovered*.
  prefs: []
  type: TYPE_NORMAL
- en: Adding authentication and rejection messages to the Order protocol
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our electronic trading exchange right now has no concept of user authentication
    and is missing a lot of error checking and handling. By this, we mean that it
    does not check whether clients log in with the correct credentials and are authorized
    to trade the instruments they try to trade. Additionally, if the `ClientId` and
    `TCPSocket` instances do not match or there is a sequence number gap in the `ClientRequest`
    messages that a client sends, we quietly ignore it in `Exchange::OrderServer`.
    This is shown in the following code block from the `exchange/order_server/order_server.h`
    source file, which we have already discussed in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Silently ignoring errors like these is not ideal since the clients are not notified
    about these errors. An enhancement to this workflow would be to add a rejection
    message to the `ClientResponse` message protocol, which the `OrderServer` component
    can use to notify the clients about these errors. This enhancement is in addition
    to the enhancements we suggested to the order protocol to facilitate the authentication
    of the trading clients.
  prefs: []
  type: TYPE_NORMAL
- en: Supporting modify messages in the Order protocol
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our current order protocol for `ClientRequest` messages only supports `ClientRequestType::NEW`
    and `ClientRequestType::CANCEL` requests. An enhancement to this protocol would
    be to add a `ClientRequestType::MODIFY` message type so that client trading systems
    can modify their order’s price or quantity attributes. We would need to update
    the `OrderServer`, `MatchingEngine`, `MEOrderBook`, and other components on the
    exchange’s side and update the `OrderGateway`, `OrderManager`, `MarketMaker`,
    `TradeEngine`, and other components on the trading client’s side.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing trade engine components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The trade engine has several components that can be improved and/or enhanced.
    In this section, we provide brief descriptions of these improvements for each
    of the components with potential future enhancements.
  prefs: []
  type: TYPE_NORMAL
- en: Adding risk metrics to RiskManager
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the chapter *Designing Our Trading Ecosystem*, in the *Understanding the
    risk management systems* section, we described a couple of different risk metrics.
    `RiskManager` was built only with a small subset of those risk metrics and can
    be enhanced by adding additional risk measures, as described in that section.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing OrderManager
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`OrderManager` was built extremely simply – it supports a maximum of one active
    order on each side, that is, at most one buy order and one sell order. Obviously,
    this is an extremely simplified version and `OrderManager` can be enhanced to
    support much more complex order management.'
  prefs: []
  type: TYPE_NORMAL
- en: Enriching FeatureEngine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`FeatureEngine` was set up with two hardcoded features built into it. It can
    be enriched a lot to support complex configurations of features, a library of
    diverse types of features, complex interactions between these features, and so
    on.'
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing the trading algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`LiquidityTaker` and `MarketMaker` in this book were also extremely simple
    representations of realistic trading strategies. These can be enhanced/improved
    in many ways – improvements in terms of feature compositions, order management,
    efficient execution, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of future enhancement possibilities for our electronic
    trading ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first section of this chapter focused on analyzing the latency metrics we
    added to our electronic trading systems in the previous chapter. We discussed
    a few examples of latency measurements for internal functions, as well as a few
    examples of latency measurements between critical hops in our system. The goal
    was to understand the distribution of latencies in different cases so that you
    understand how to identify and investigate areas of potential problems or optimization
    opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: In the second section of this chapter, we discussed a few tips and techniques
    regarding how to approach potential performance optimization possibilities. We
    presented a few examples of what could be improved and discussed the performance
    problems that exist in the current design and solutions to those problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the concluding section, we described a roadmap for the future of the electronic
    trading ecosystem we built in this book. We discussed several different components,
    sub-components, and workflows that can be enriched to build a more mature electronic
    trading ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: The approach and principles we discussed in this book pertaining to latency-sensitive
    applications developed in C++ should guide you on your journey. The full end-to-end
    electronic trading ecosystem we built is a prime example of a low-latency application
    and hopefully provided a good practical example of how to build a low-latency
    application from scratch. Hopefully, this chapter added to the experience by providing
    you with tools to analyze the performance and iteratively improve the system.
    We wish you all the best as you continue your low-latency application development
    journey!
  prefs: []
  type: TYPE_NORMAL
