- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing Some Common Low Latency Applications in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will look at some applications in different fields from
    video streaming, online gaming, real-time data analysis, and electronic trading.
    We will understand their behavior, and what features need to be executed in real
    time under extremely low-latency considerations. We will introduce the electronic
    trading ecosystem, since we will use that as a case study in the rest of the book,
    and build a system from scratch in C++, with a focus on understanding and using
    low latency ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding low latency performance in live video streaming applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding which low latency constraints matter in gaming applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing the design of Internet-of-Things (IoT) and retail analytics systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring low latency electronic trading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter’s goal is to dig into some of the technical aspects of low latency
    applications in different business areas. By the end of this chapter, you should
    be able to understand and appreciate the technical challenges that applications
    such as real-time video streaming, offline and online gaming applications, IoT
    machines and applications, and electronic trading face. You will be able to understand
    the different solutions that advancements in technology provide to solve these
    problems and make these businesses viable and profitable.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding low latency performance in live video streaming applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will first discuss the details behind low latency performance
    in the context of video streaming applications. We will define the important concepts
    and terms relevant to live video streaming to build an understanding of the field
    and business use cases. We will understand what causes latencies in these applications
    and the business impact of those. Finally, we will discuss technologies, platforms,
    and solutions to build and support low latency video streaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: Defining important concepts in low latency streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we will first define a few important concepts and terms when it comes
    to low latency streaming applications. Let us get started with a few basics and
    build up from there into more complex concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Latency in video streaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Video streaming** is defined as audio-video content delivered in real time
    or near real time. Latency in general refers to the time delay between an input
    event and the output event. In the context of live video streaming applications,
    latency refers specifically to the time from when a live video stream hits the
    camera on the recording device and then gets transported to the target audience’s
    screens and gets rendered and displayed there. It should be easy to intuitively
    understand why this is also referred to as **glass-to-glass latency** in the context
    of live video streaming applications. Glass-to-glass latency in video streaming
    applications is quite important regardless of the actual application, whether
    it be a video call, live video streams for other applications, or online video
    game rendering. In live streaming, video latency is basically the delay between
    when the video frame captures at the recorder’s side to when the video frame is
    displayed at the viewer’s side. Another commonly encountered term is **lag**,
    which often just refers to a higher-than-expected glass-to-glass latency, which
    the user may perceive as reduced or jittery performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Video distribution services and content delivery networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Video distribution service** (**VDS**) is a fancy term for a relatively easy-to-understand
    concept. A VDS basically means the system responsible for taking multiple incoming
    streams of video and audio from the sources and presenting them to the viewers.
    One of the most well-known examples of a VDS would be a **content delivery network**
    (**CDN**). A CDN is a means of efficiently distributing context across the globe.'
  prefs: []
  type: TYPE_NORMAL
- en: Transcoding, transmuxing, and transrating
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let us discuss three concepts that relate to encoding the audio-video stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transcoding** refers to the process of decoding a media stream from one format
    (so lower-level details such as codec, video size, sampling rates, encoder formats,
    etc.) and possibly recoding it in a different format or with different parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transmuxing** is similar to transcoding but here, the delivery format changes
    without any changes to the encoding, as in the case of transcoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transrating** is also similar to transcoding, but we change the video bitrate;
    usually, it is compressed to a lower value. The video bitrate is the number of
    bits (or kilobits) being transferred per second and captures the information and
    quality in the video stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will understand the sources of latencies in low latency
    video streaming applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding sources of latency in video streaming applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us look at the details of what happens in the glass-to-glass journey. Our
    ultimate motivation in this section is to understand the sources of latencies
    in video streaming applications. This figure describes at a high level what happens
    in the glass-to-glass journey from the camera to the display:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Glass-to-glass journey in live video streaming applications](img/Figure_2.1_B19434.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Glass-to-glass journey in live video streaming applications
  prefs: []
  type: TYPE_NORMAL
- en: Discussing the steps in the glass-to-glass journey
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will begin by understanding all the steps and components involved in the
    glass-to-glass journey of low latency video streaming applications. There are
    two forms of latency – the initial startup latency and then the lag between video
    frames once the live stream starts. Typically for the user experience, a slightly
    longer startup latency is much preferred over lag between video frames, but there
    is usually a trade-off in trying to reduce one latency over the other. So, we
    need to understand which metric is more important for a specific use case and
    adjust the design and technical details appropriately. The following are the steps
    in the glass-to-glass journey from the broadcaster to the receivers:'
  prefs: []
  type: TYPE_NORMAL
- en: Camera capturing and processing the audio and video at the broadcaster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Video consumption and packaging at the broadcaster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encoders transcoding, transmuxing, and transrating the content
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sending the data over the network over the appropriate protocol(s)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distribution over a VDS such as a CDN
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reception at the receivers and buffering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decoding the content on the viewer’s device
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dealing with packet drops, network changes, and so on at the receiver end
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rendering of the audio-video content on the viewer’s device of choice
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Possibly collecting interactive inputs (selections, audio, video, etc.) from
    the viewer for interactive applications and sending them back to the broadcaster
    where needed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have described the details behind content delivery from the sender
    to the receiver and possibly back to the sender, in the next section, we will
    describe where we have possibilities of latencies on that path. Typically, each
    step does not take a long time, but higher latencies in multiple components can
    accumulate and cause significant degradation in user performance.
  prefs: []
  type: TYPE_NORMAL
- en: Describing possibilities of high latencies on the path
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will look at the reasons for high latencies in low latency video streaming
    applications. There are numerous reasons for this on each of the components of
    the glass-to-glass path we discussed in the previous subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Physical distance, server load, and internet quality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is an obvious one: the physical distance between the source and destination
    will affect the glass-to-glass latency. This is sometimes very obvious when streaming
    videos from a different country.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the distance, the quality of the internet connection itself can
    affect the streaming latency. Slow or limited bandwidth connections lead to instability,
    buffering, and lags.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on how many users are simultaneously streaming the videos and how
    much load that puts on the servers involved in the streaming path, the latency
    and user experience can vary. Overloaded servers lead to slower response times,
    higher latencies, buffering, and lag and can even make the streaming come to a
    grinding halt.
  prefs: []
  type: TYPE_NORMAL
- en: Capture equipment and hardware
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The video and audio capture devices have a big impact on the glass-to-glass
    latencies. Taking an audio and video frame and turning it into digital signals
    takes time. Advanced systems such as recorders, encoders, processors, re-encoders,
    decoders, and re-transmitters have a significant impact on the final user experience.
    The capture equipment and hardware will determine the latency values.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming protocol, transmission, and jitter buffer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given the availability of different streaming protocols (as we will discuss
    shortly), the final choice can determine the latency of video streaming applications.
    If the protocol is not optimized for dynamic adaptive streaming, it can increase
    delays. Overall, there are two categories of protocols for live video streaming
    – HTTP-based and non-HTTP-based – and there are differences in latencies and scalability
    between the two broad options, which will change the performance of the final
    system.
  prefs: []
  type: TYPE_NORMAL
- en: The internet routes chosen on the way through the VDS can change the glass-to-glass
    latency. These routes can also change over time, and packets can be queued on
    some hops and can even arrive out of order at the receiver. The software that
    handles these issues is known as a **jitter buffer**. If the CDN has issues, that
    can also cause additional delays. Then, there are constraints such as the encoded
    bitrate (lower bitrates mean less data being transferred per unit time and lead
    to lower latencies), which can change the latencies encountered.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding – transcoding and transrating
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The encoding process determines the compression, format, and so on of the final
    video output, and the choice and quality of encoding protocols will have a huge
    impact on the performance. Also, there are many options for viewer devices (TVs,
    phones, PCs, Macs, etc.) and networks (3G, 4G, 5G, LAN, Wi-Fi, etc.) and a streaming
    provider needs to implement **adaptive bitrate** (**ABR**) to handle these efficiently.
    The computer or server running the encoder needs to have adequate CPU and memory
    resources for the encoding process to keep up with the incoming audio-video data.
    Whether we use encoding software on a computer or encoding hardware such as *BoxCaster*
    or *Teradek*, we incur processing latencies ranging from a few milliseconds to
    seconds. The tasks that the encoder needs to perform are to ingest the raw video
    data, buffer the content, and decode, process, and re-encode it before forwarding
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Decoded and played on the viewer’s device
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Assuming the content makes it to the viewer’s device without incurring noticeable
    latencies, the client still must decode, playback, and render the content. Video
    players do not render video segments one at a time as they receive them, but instead,
    have a buffer of received segments, usually in memory. This means several segments
    are buffered before the video begins to play and, depending on the actual size
    of the segment chosen, can cause latency on the end user side. For instance, if
    we choose a segment length that contains 10 seconds of video, the player at the
    end user must at least receive a complete segment before it can play it and will
    introduce an extra 10-second delay between the sender and receiver. Typically,
    these segments are between 2 and 10 seconds, trying to balance between optimizing
    network efficiency and glass-to-glass latency. Obviously, factors such as the
    viewer’s device, platform, hardware, CPU, memory, and player efficiency can add
    to the glass-to-glass latencies.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring latencies in low latency video streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Measuring latencies in low latency video streaming applications is not extremely
    complicated, since the latency range we care about should at least be a few seconds
    to be perceptible to the end user as a delay or lag. The easiest ways to measure
    end-to-end video latency are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The first place to start would be to use a **clapperboard** application. A clapperboard
    is a tool used to synchronize video and audio during filmmaking, and apps are
    available to detect synchronization issues between two streams due to latency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another option is to publish the video stream back to yourself to measure whether
    there are any latencies in the capturing, encoding, decoding, and rendering steps
    by taking the network out of it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An obvious solution is to take a screenshot of two screens running the same
    live stream to spot differences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best solution to measure live video streaming latencies is to add a timestamp
    to the video stream itself at the source and then the receiver can use that to
    determine the glass-to-glass latencies. Obviously, the clocks used by the sender
    and the receiver need to be synchronized with each other reasonably well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the impact of high latencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we understand the impact of high latencies on low latency video streaming
    applications, first, we need to define what the acceptable latency is for different
    applications. For video streaming applications that do not require a lot of real-time
    interactions, anything up to 5 seconds is acceptable. For streaming applications
    that need to support live and interactive use cases, anything up to 1 second is
    enough for the users. Obviously, for video-on-demand, latency is not an issue
    since it is pre-recorded and there is no live-streaming component. Overall, high
    latency in real-time live-streaming applications negatively impacts an end user’s
    experience. The key motivation for real time is that viewers want to feel connected
    and get the feeling of being in person. Large delays in receiving and rendering
    the content destroy the feeling of watching something in real time. One of the
    most annoying experiences occurs when a real-time video regularly pauses and buffers
    due to latencies.
  prefs: []
  type: TYPE_NORMAL
- en: Let us briefly discuss the main negative impacts of real-time video streaming
    applications due to latency.
  prefs: []
  type: TYPE_NORMAL
- en: Low audio-video quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the components of the streaming system cannot achieve real-time latencies,
    that usually leads to higher levels of compression. Due to the high levels of
    compression on the audio-video data, the audio quality can sound scrambled and
    scratchy at times and the video quality can be blurry and pixelated, so it's just
    a worse user experience overall.
  prefs: []
  type: TYPE_NORMAL
- en: Buffering pauses and delays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Buffering is one of the worst things that can ruin the user experience since
    the viewer experiences a jittery performance with constant pauses instead of having
    a smooth experience. This is very frustrating for viewers if a video keeps pausing
    to buffer and catch up, and will likely lead to the viewer quitting the video,
    the platform, or the business itself and never returning.
  prefs: []
  type: TYPE_NORMAL
- en: Audio-video synchronization issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many implementations of real-time audio-video streaming applications, the
    audio data is sent separately from the video data and thus the audio data can
    reach the receiver faster than the video data. This is because by its nature,
    audio data is smaller in size than video data, and due to high latencies, video
    data might lag behind audio data at the receiver’s end. This leads to problems
    with synchronization and hurts the viewer’s experience with real-time video streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Playback – rewinding and fast-forwarding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: High latencies can cause issues with rewinding and fast-forwarding, even when
    the applications aren’t necessarily in 100% real time. This is because the audio-video
    data will have to be resent so that the end user’s player can re-sync with the
    newly selected location.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring technologies for low latency video streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at different technology protocols that apply to
    the encoding, decoding, streaming, and distribution of audio-video data. These
    protocols are specially designed for low latency video streaming applications
    and platforms. These protocols fall into one of two broad categories – HTTP-based
    protocols and non-HTTP-based protocols – but for low latency video streaming,
    typically, HTTP-based protocols are the way to go, as we will see in this section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Live video streaming latencies and technologies](img/Figure_2.2_B19434.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Live video streaming latencies and technologies
  prefs: []
  type: TYPE_NORMAL
- en: Non-HTTP-based protocols
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Non-HTTP-based protocols use a combination of **User Datagram Protocol** (**UDP**)
    and **Transmission Control Protocol** (**TCP**) to transfer data from the sender
    to the receiver. These protocols can be used for low latency applications, but
    many do not have advanced support for adaptive streaming technology and suffer
    from limited scalability. Two examples of these protocols are **Real-Time Streaming
    Protocol** (**RTSP**) and **Real-Time Messaging Protocol** (**RTMP**), which we
    will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: RTSP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RTSP is an application layer protocol that was used for the low latency streaming
    of videos. It also has playback capabilities to allow playing and pausing the
    video content and can handle multiple data streams. This, however, is no longer
    popular today and has been replaced by other more modern protocols, which we will
    see in later sections. RTSP was replaced by modern protocols such as HLS and DASH
    because a lot of receivers did not support RTSP; it was incompatible with HTTP
    and lost popularity with the advent of web-based streaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: Flash and RTMP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Flash-based applications were very popular once upon a time. They use RTMP and
    work well for low latency streaming use cases. However, Flash as a technology
    has declined a lot in popularity for many reasons, mostly security-related. Web
    browsers as well as CDNs have removed support for RTMP because it did not scale
    too well as demand grew. RTMP is a streaming protocol that accomplishes low latencies
    in streaming but, as mentioned before, is being replaced by other technologies
    now.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP-based protocols
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'HTTP-based protocols typically break down the continuous stream of audio-video
    data into small segments of 2 to 10 seconds in length. These segments are then
    transported through a CDN or a web service. These are the preferred protocols
    for low latency live streaming applications since they are still acceptably low
    latency but also feature-rich and scale better. These protocols, however, do have
    a disadvantage that we have mentioned before: the latency incurred depends on
    the length of the segments. The minimum latency is at least the length of the
    segment because the receiver needs to receive at least one full segment before
    it can play it. In some cases, the latency can be in the order of multiples of
    segment length depending on the video player devices’ implementation. For example,
    iOS buffers at least three to five segments before playing the first segment to
    ensure smooth rendering.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of HTTP-based protocols are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**HTTP Live** **Streaming** (**HLS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTTP Dynamic** **Streaming** (**HDS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microsoft Smooth** **Streaming** (**MSS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic Adaptive Streaming over** **HTTP** (**DASH**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Common Media Application** **Format** (**CMAF**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-Efficiency Stream** **Protocol** (**HESP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss some of these protocols in this section to understand how they
    work and how they achieve low latency performance in real-time video streaming
    applications. Overall, these protocols are designed to scale to millions of simultaneous
    receivers and support adaptive streaming and playback. HTTP-based streaming protocols
    use communication over standard HTTP protocol and require a server for distribution.
    In contrast to that, **Web Real-Time Communication** (**WebRTC**), which we will
    explore later, is a **Peer-to-Peer** (**P2P**) protocol that can technically establish
    direct communication between two machines and skip the need for an intermediate
    machine or server.
  prefs: []
  type: TYPE_NORMAL
- en: HLS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: HLS is used both for real-time and on-demand audio-video content delivery and
    can scale tremendously well. HLS is generally converted from RTMP by the video
    delivery platform. Using both RTMP and HLS is the best way to achieve low latency
    and stream to all devices. There is a variant of **Low Latency HLS** (**LL-HLS**)
    that can get latencies down to under 2 seconds, but it is still experimental.
    LL-HLS enables low latency audio-video real-time streaming by exploiting the ability
    to stream and render partial segments instead of requiring a full segment. The
    success of HLS and LL-HLS as the most widely used ABR streaming protocols comes
    from scalability to many users and compatibility with most kinds of devices, browsers,
    and players.
  prefs: []
  type: TYPE_NORMAL
- en: CMAF
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CMAF is relatively new; strictly speaking, it is not really a new format but
    instead packages and delivers various forms of protocols for video streaming.
    It works with HTTP-based protocols such as HLS and DASH to encode, package, and
    decode video segments. This typically helps businesses by reducing storage costs
    and audio-video streaming latencies.
  prefs: []
  type: TYPE_NORMAL
- en: DASH
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DASH was created from the work of the **Moving Picture Experts Group** (**MPEG**)
    and is an alternative to the HLS protocol we discussed before. It is quite similar
    to HLS because it prepares different quality levels of the audio-video content
    and divides them into small segments to enable ABR streaming. Under the hood,
    DASH still relies on CMAF and, to be specific, one of the features it relies on
    is **chunked encoding**, which facilitates breaking a segment into even smaller
    subsegments of a few milliseconds. The other feature it relies on is **chunked
    transfer encoding**, which takes these subsegments sent to the distribution layer
    and distributes them in real time.
  prefs: []
  type: TYPE_NORMAL
- en: HESP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: HESP is another ABR HTTP-based streaming protocol. This protocol has the ambitious
    goals of ultra-low latencies, increasing scalability, supporting currently popular
    CDNs, reducing bandwidth requirements, and reducing times to switch between streams
    (i.e., the latency to start a new audio video stream). Since it is extremely low
    latency (<500 milliseconds) it is a competitor to the WebRTC protocol, but HESP
    can be expensive since it is not an open source protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentally, the major difference in HESP relative to other protocols is that
    HESP relies on two streams rather than one. One of the streams (which only contains
    keyframes or snapshot frames) is known as the **initialization stream**. The other
    stream contains data that applies incremental changes to the frames in the initialization
    stream, and this stream is known as the **continuation stream**. So, while the
    keyframes from the initialization stream contain snapshot data and require higher
    bandwidth, they support the ability to quickly seek various locations in the video
    during playback. But the continuation stream is lower-bandwidth since it only
    contains changes and can be used to quickly play back once the receiver video
    player synchronizes with the initialization stream.
  prefs: []
  type: TYPE_NORMAL
- en: While, on paper, HESP might sound perfect, it has a few drawbacks such as higher
    costs for encoding and storing two streams instead of one, the need to encode
    and distribute two streams instead of one, and the need to update the players
    on the receivers’ platforms to decode and render the two streams.
  prefs: []
  type: TYPE_NORMAL
- en: WebRTC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: WebRTC is regarded as the new standard in the real-time video streaming industry
    and allows subsecond latencies so can be played back on most platforms and almost
    every browser (such as Safari, Chrome, Opera, Firefox, etc.). It is a P2P protocol
    (i.e., it creates a direct communication channel between devices or streaming
    applications). A big advantage of WebRTC is that it does not need additional plugins
    to support audio-video streaming and playback. It also supports ABR and adaptive
    video quality changes for bi-directional and real-time audio-video streaming.
    Even though WebRTC uses a P2P protocol and so can establish a direct connection
    for conferencing, the performance is still dependent on the hardware and network
    quality because that is still a consideration for all protocols, regardless of
    whether they're P2P or not.
  prefs: []
  type: TYPE_NORMAL
- en: WebRTC does have some challenges, such as needing its own multimedia server
    infrastructure, the need to encrypt data that is exchanged, security protocols
    to handle the gaps in UDP, trying to scale worldwide cost-effectively, and the
    engineering complexity that comes with dealing with the several protocols that
    WebRTC is a combination of.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring solutions and platforms for low latency streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explore some of the most popular solutions and commercially
    available platforms for low latency video streaming. These platforms build on
    all the technologies we discussed in the previous section to solve a lot of the
    business problems associated with high latencies in real-time audio-video streaming
    applications. Note that a lot of these platforms support and use multiple underlying
    streaming protocols, but we will mention the ones that are primarily used for
    these platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Twitch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Twitch is a very popular online platform, mostly used by video gamers who want
    to live-stream their gameplay in real time as well as interact with their target
    audience via chats, comments, donations, and so on. It goes without saying, but
    this requires low latency streaming as well as the ability to scale to a large
    community, which Twitch provides. Twitch uses RTMP for its broadcasting needs.
  prefs: []
  type: TYPE_NORMAL
- en: Zoom
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Zoom is one of the real-time video conferencing platforms that exploded in popularity
    during the COVID pandemic and working-from-home era. Zoom provides real-time low
    latency audio and video conferencing with little delays and supports many simultaneous
    users. It also provides features such as screen sharing and group chats while
    on video conferencing. Zoom primarily uses the WebRTC streaming protocol technology.
  prefs: []
  type: TYPE_NORMAL
- en: Dacast
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dacast is a platform for broadcasting events, and even though it is not as low
    latency as some other real-time streaming applications, it still has acceptable
    performance when it comes to broadcasting purposes. It is affordable and works
    well but does not allow for a lot of interactive workflows. Dacast uses the RTMP
    streaming protocol as well.
  prefs: []
  type: TYPE_NORMAL
- en: Ant Media Server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ant Media Server uses WebRTC technology to provide an extremely low latency
    video streaming platform and is intended to be used at an enterprise level on-premises
    or on the cloud. It is also used for live video monitoring and surveillance-based
    applications that require real-time video streaming at their core.
  prefs: []
  type: TYPE_NORMAL
- en: Vimeo
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vimeo is another very popular video streaming platform that, while not the fastest
    in the business, is still used quite extensively. It is mostly used to house real-time
    live event broadcasts and on-demand video distribution applications. Vimeo uses
    RTMP streaming by default but also supports others, including HLS.
  prefs: []
  type: TYPE_NORMAL
- en: Wowza
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Wowza has been around for a long time in the field of online real-time video
    streaming and is quite reliable and widely used. It is used by many large corporations
    such as Sony, Vimeo, and Facebook and focuses on providing video streaming services
    at a commercial and enterprise level on a very large scale. Wowza is another platform
    that uses RTMP streaming protocol technology.
  prefs: []
  type: TYPE_NORMAL
- en: Evercast
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Evercast is an ultra-low latency streaming platform that has found a lot of
    uses for collaborative content creation and editing applications, as well as live
    streaming applications. Since it can support ultra-low latency performance, multiple
    collaborators are able to stream their workspaces and create an environment of
    real-time and collaborative editing. The demand for such use cases has exploded
    in recent years due to the COVID pandemic, remote work and collaboration, and
    online collaboration education systems. Evercast primarily uses WebRTC on its
    streaming servers.
  prefs: []
  type: TYPE_NORMAL
- en: CacheFly
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CacheFly is another platform that provides live video streaming for live event
    broadcast purposes. It provides an acceptably low latency of single digit seconds
    and scales very well for real-time audio-video broadcasting applications. CacheFly
    uses a custom Websocket-based end-to-end streaming solution.
  prefs: []
  type: TYPE_NORMAL
- en: Vonage Video API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vonage Video API (previously known as **TokBox**) is another platform that provides
    live video streaming capabilities and targets large corporations to support enterprise-level
    applications. It supports data encryption, which is what makes it a preferred
    choice for enterprises, corporations, and healthcare companies looking for audio-video
    conferencing, meetings, and training online. Vonage uses RTMP as well as HLS as
    its broadcasting technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Open Broadcast Software (OBS)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OBS is another low latency video streaming platform that is also open source,
    which makes it popular in a lot of circles where enterprise-level solutions might
    be a deterrent. Many live streamers who stream several types of content use OBS,
    and even some platforms such as Facebook Live and Twitch use some parts of OBS.
    OBS supports multiple protocols such as RTMP and **Secure Reliable** **Transport**
    (**SRT**).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we conclude our discussion of low latency considerations for live video
    streaming applications. Next, we will transition into video gaming applications,
    which share some common traits when compared to live video streaming applications,
    especially when it comes to online video games.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what low latency constraints matter in gaming applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Video gaming has evolved greatly since it was first born in the 1960s and, these
    days, video games are not about playing alone or even playing along with or against
    the person physically next to you. These days, gaming involves many players all
    over the globe, and even the quality and complexity of these games have increased
    tremendously. It is no surprise that ultra-low latency and high scalability are
    non-negotiable requirements when it comes to modern gaming applications. With
    new technologies such as AR and VR, this only further increases the need for ultra-low
    latency performance. Additionally, with the advent of mobile gaming combined with
    online gaming, complex gaming applications have been ported to smartphones and
    need ultra-low latency content delivery systems, multiplayer systems, and super-fast
    processing speeds.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we discussed low latency real-time video streaming
    applications in detail, including streaming applications that are interactive.
    In this section, we will look at low latency considerations, high-latency impact,
    and techniques to facilitate low latency performance in video gaming applications.
    Since a lot of modern video games are either online or in the cloud, or have a
    strong online presence due to multiplayer features, a lot of what we learned in
    the previous section is still important here. Streaming and rendering video games
    in real time, preventing lag, and responding to player interactions quickly and
    efficiently are necessities when it comes to gaming applications. Additionally,
    there are some extra concepts, considerations, and techniques to maximize low
    latency gaming performance.
  prefs: []
  type: TYPE_NORMAL
- en: Concepts in low latency gaming applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we understand the impact of high latency in gaming applications and how
    to improve the latencies in these applications, we will define and explain a few
    concepts related to gaming applications and their performance. When it comes to
    low latency gaming applications, the most important concepts are the **refresh
    rate**, **response time**, and **input lag**. The main goal of these applications
    is to minimize the delay between the player and the character on screen that the
    player controls. Really, what this means is that any user input reflects the impact
    on the screen right away, and any changes to the character due to the gameplay
    environment are rendered on the screen right away. The optimal user experience
    is achieved when the gameplay feels very smooth, and the player feels like they
    really are inside the game world being rendered on screen. Now, let us jump into
    a discussion of the important concepts with regard to low latency gaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: Ping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In computer science and online video gaming applications, **ping** is the latency
    from when data is sent from the user’s computer to a server (or possibly a different
    player’s computer) until when the data is received back at the original user’s
    computer. Typically, the magnitude of the ping latency depends on the application;
    for low latency electronic trading, this will be in hundreds of microseconds,
    and for gaming applications, it is usually tens to hundreds of milliseconds. Ping
    latency basically measures how fast a server and client communicate with each
    other in the absence of any processing delays at the server or client machines.
  prefs: []
  type: TYPE_NORMAL
- en: The closer to real-time the requirements for the gaming application are, the
    lower the ping times need to be. This is typically required for games such as
    **first-person shooters** (**FPS**) and sports and racing games, while other games
    such as **massive multiplayer online** (**MMO**) games and some **real-time strategy**
    (**RTS**) games can tolerate higher ping latencies. It is common for the game
    interface itself to have a ping functionality or display ping statistics in real
    time. In general, 50 to 100 milliseconds is an acceptable ping time, above 100
    milliseconds can cause noticeable delays during gameplay, and any higher than
    that degrades the player’s experience too much to be viable. Typically, less than
    25 milliseconds is the ideal ping latency for good responsiveness, crisp rendering
    of visuals, and no gameplay lags.
  prefs: []
  type: TYPE_NORMAL
- en: Frames per second (FPS)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**FPS** (not to be confused with *first-person shooters*) is another important
    concept when it comes to online gaming applications. FPS measures how many frames
    or images can be rendered each second by the graphics card. FPS can also be measured
    for the monitor hardware itself instead of the graphics card (i.e., how many frames
    can be displayed or updated by the monitor hardware itself). Higher FPS typically
    leads to the smoother rendering of game worlds and the user experience feels more
    responsive to inputs and gameplay events. Lower FPS leads to the gameplay and
    rendering feeling like it is rigid, stuttering, and flickering, and overall, just
    leads to significantly reduced enjoyment and adoption.'
  prefs: []
  type: TYPE_NORMAL
- en: For a game to be functional or even playable, 30 FPS is the bare minimum necessity,
    and this can support console games and some PC games. As long as the FPS stays
    above 20 FPS, these games can continue to be playable without any noticeable lag
    and degradation. For most games, 60 FPS or more is the ideal performance range
    that is easily supported by most graphics cards, PCs, monitors, and TVs. Beyond
    60 FPS, the next milestone is 120 FPS, which is only needed and available for
    high-end gaming hardware connected to monitors that support at least 144-Hz refresh
    rates. Beyond this, 240 FPS is the maximum frame rate achievable and needs to
    be paired with 240-Hz refresh rate monitors. This high-end configuration is typically
    only needed for the biggest gaming enthusiasts out there.
  prefs: []
  type: TYPE_NORMAL
- en: Refresh rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**The refresh rate** is a concept that is very closely related to FPS, and
    even though technically they are slightly different, they do impact each other.
    The refresh rate also measures how quickly the screen refreshes and impacts the
    maximum possible FPS that hardware can support. Like FPS, the higher the refresh
    rate, the smoother the rendering transition when animating motion during gameplay
    on the screen. The maximum refresh rate controls the maximum FPS achievable because
    even though the graphics card can render faster than a monitor screen can refresh,
    the bottleneck then becomes the monitor screen refresh rate. When there are cases
    where the FPS exceeds the refresh rate, one of the display artifacts that we encounter
    is called **screen tearing**. Screen tearing is when the graphics card (GPU) is
    not synchronized with the monitor, so there are cases where the monitor paints
    an incomplete frame on top of the current frame, resulting in horizontal or vertical
    splits where the partial and full frames overlap on the screen. This does not
    completely break down the gameplay but, at the very least, can be distracting
    if it happens rarely, all the way to completely ruining the visual quality of
    the gameplay if quite frequent. There are various techniques to deal with screen
    tearing, which we will look at shortly, such as **vertical synchronization** (**V-Sync**),
    **adaptive sync**, **FreeSync**, **Fast Sync**, **G-Sync**, and **variable refresh**
    **rate** (**VRR**).'
  prefs: []
  type: TYPE_NORMAL
- en: Input lag
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Input lag** measures the latency between when a user generates an input (such
    as a keystroke, mouse movement, or a mouse click) to when the response to that
    input is rendered on the screen. This is basically the responsiveness of the hardware
    and the game to user inputs and interactions. Obviously, for all games, this is
    a non-zero value and is the sum of the hardware itself (the controller, mouse,
    keyboard, internet connection, processor, display monitor, etc.) or the game software
    itself (processing input, updating the game and character states, dispatching
    the graphics updates, rendering them via the graphics card, and refreshing the
    monitor). When there is high input lag, the game feels unresponsive and lagging,
    which can affect the player’s performance during multiplayer or online gameplay
    and even ruin the game completely for the user.'
  prefs: []
  type: TYPE_NORMAL
- en: Response time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Response time** is often mistaken for input lag, but they are different terms.
    Response time refers to the pixel response time, which basically means the time
    it takes for pixels to change colors. While input lag impacts the gameplay’s responsiveness,
    response time impacts the blurriness of rendered animations on the screen. Intuitively,
    if the pixel response time is high, the pixels take longer to change colors when
    rendering motion or animation on screen, thus causing blurriness. Lower pixel
    response times (1 millisecond or lower) lead to crisp and sharp image and animation
    quality, even for games that have fast camera movements. Good examples of such
    games would be first-person shooters and racing games. In cases where the response
    time is high, we encounter an artifact known as **ghosting**, which refers to
    trails and artifacts slowly fading off the screen when there is motion. Usually,
    ghosting and high pixel response times are not a problem, and modern hardware
    can easily provide response times of less than 5 milliseconds and render sharp
    animations.'
  prefs: []
  type: TYPE_NORMAL
- en: Network bandwidth
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Network bandwidth affects online gaming applications in the same way as it would
    affect real-time video streaming applications. Bandwidth measures how many megabits
    per second can be uploaded to or downloaded from the gaming application server.
    Bandwidth is also affected by packet losses, which we will look at next, and varies
    depending on the location of the players and the gaming server they connect to.
    **Contention** is another term to think about when it comes to network bandwidth.
    Contention is a measure of how many simultaneous users are trying to access the
    same server or shared resource and whether that causes the server to be overloaded
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: Network packet loss and jitter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Network packet loss is an inescapable fact of transmitting packets over the
    network. Network packet loss reduces the effective bandwidth and causes retransmission
    and recovery protocols, which introduce additional delays. Some packet losses
    are tolerable, but when the network packet losses are very high, they can degrade
    the user experience of online gaming applications and can even bring it to a grinding
    halt. **Jitter** is like packet losses except, in this case, packets arrive out
    of order. This introduces additional delays as the game software is on the user’s
    end because the receiver must save the out-of-order packets and wait for packets
    that have not arrived yet and then process the packets in order.
  prefs: []
  type: TYPE_NORMAL
- en: Networking protocols
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When it comes to networking protocols, there are broadly two protocols to transfer
    data across the internet: **TCP** and **UDP**. TCP provides a reliable transport
    protocol by tracking packets successfully delivered to the receiver and having
    mechanisms to retransmit lost packets. The advantage here is obvious where applications
    cannot operate with packet and information losses. The disadvantage here is that
    these additional mechanisms to detect and handle packet drops cause additional
    latencies (additional milliseconds) and use the available bandwidth less effectively.
    Examples of applications that must rely on TCP are online shopping and online
    banking, where it is critical to make sure the data is delivered correctly, even
    if it is delivered late. UDP instead focuses on making sure that the data is delivered
    as quickly as possible and with greater bandwidth effectiveness. However, it does
    so at the cost of not guaranteeing delivery or even guaranteeing in-order delivery
    of packets since it does not have mechanisms to retransmit dropped packets. UDP
    works well for applications that can tolerate some packet losses without completely
    breaking down and some dropped information is preferred over delayed information.
    Some examples of such applications are real-time video streaming and some components
    of online gaming applications. For instance, some video components or rendering
    components in online video games can be transported over UDP, but some components
    such as user input and game and player state updates need to be sent over TCP.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Components in an end-to-end video gaming system](img/Figure_2.3_B19434.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Components in an end-to-end video gaming system
  prefs: []
  type: TYPE_NORMAL
- en: Improving gaming application performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we discussed some concepts that apply to low latency
    gaming applications and their impact on the application and the user experience.
    In this section, we will explore additional details about the sources of high
    latency in gaming applications and discuss the steps we can take to improve gaming
    application latencies and performance and thus improve the user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Approaching gaming application optimization from the developer’s perspective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we look at the approach and techniques that game developers use to optimize
    gaming application performance. Let us quickly describe a few optimization techniques
    employed by developers – some that apply to all applications and some that only
    apply to gaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: Managing memory, optimizing cache access, and optimizing the hot path
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gaming applications, like other low latency applications, must use the available
    resources efficiently and maximize runtime performance. This includes managing
    the memory correctly to avoid memory leaks and pre-allocating and pre-initializing
    as many things as possible. Avoiding mechanisms such as garbage collection and
    dynamic memory allocations and deallocation on the critical path are also important
    to meet a certain runtime performance expectation. This is especially relevant
    to gaming applications because there are many objects in video games, especially
    ones that create and deal with large worlds.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect of most low latency applications is using the data
    and instruction cache as efficiently as possible. Gaming applications are no different,
    especially given the large amount of data that they must deal with.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of applications, including gaming applications, spend a lot of their time
    in a critical loop. For gaming applications, this can be a loop that checks for
    inputs, updates the game state, character states, and so on based on the physics
    engine ,and renders on screen, as well as generates audio output. Gaming developers
    typically spend a lot of time focusing on the operations performed in this critical
    path, like what we would do with any low latency application running in a tight
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: Frustum culling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In computer graphics, the term view **frustum** refers to the part of the game
    world that is currently visible on the screen. **Frustum culling** is a term that
    refers to the technique of determining which objects are visible on the screen
    and only rendering those objects on the screen. Another way to think about this
    is that most game engines minimize the amount of processing power being directed
    toward off-screen objects. This is typically achieved by separating the display
    or rendering functionality of an object from its data and logic management, such
    as the location, state, next move, and so on. Eliminating the overhead of rendering
    objects not currently on the screen reduces the processing cost to a fraction
    of what it would be. Another way to introduce this separation would be to have
    an update method that is used when the object is on screen and another update
    method to be used when the object is off-screen.
  prefs: []
  type: TYPE_NORMAL
- en: Caching calculations and using mathematical approximations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is an easy-to-understand optimization technique that applies to applications
    that need to perform a lot of expensive mathematical computations. Gaming applications
    especially are heavy on mathematical computations in their physics engines, especially
    for 3D games with large worlds and lots of objects in the world. Optimization
    techniques such as caching values instead of recomputing them each time, using
    lookup tables to trade memory usage for CPU usage to lookup values, and using
    mathematical approximations instead of extremely accurate but expensive expressions
    are used in such cases. These optimization techniques have been used for a very
    long time in the world of video games because, for a long time, hardware resources
    were extremely limited, and developing such systems needed to rely on these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The raycasting engine from **id Software** (which pioneered and powered games
    such as Wolfenstein, Doom, Quake, etc.) is an impressive masterpiece of low latency
    software development from back in the old days. Another example would be cases
    where we have side scrollers or top-down shooters where there are a lot of enemies
    on screen but a lot of them have similar movement patterns and can be reused instead
    of being recalculated.
  prefs: []
  type: TYPE_NORMAL
- en: Prioritizing critical tasks and leveraging CPU idle time
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A game engine that deals with many objects in a huge game world typically has
    many objects that update frequently. Instead of updating every object on every
    frame update, a game engine needs to prioritize tasks that need to be performed
    in the critical section (for example, objects whose visual properties have changed
    since the last frame). A simple implementation would be to have a member method
    for each object that the game engine can use to check whether it has changed since
    the last frame and prioritize the updates for those objects. For instance, some
    game components such as the scenery (stationary environment objects, weather,
    lighting, etc.) and **heads-up display** (**HUD**) do not change very frequently
    and typically have extremely limited animation sequences. The tasks related to
    updating these components are slightly lower-priority than some other game components.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying tasks into high-priority and low-priority tasks also means that
    the game engine has the option of guaranteeing a good gameplay experience by making
    sure high-priority tasks are performed in all hardware and game settings. If the
    game engine detects a lot of CPU idle time, it can add additional low-priority
    features (such as particle engines, lighting, shading, atmospheric effects, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Ordering draw calls depending on layer, depth, and texture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A game engine needs to determine which rendering or draw calls to issue to
    the graphics card. To optimize performance, the goal here is to not only minimize
    the number of draw calls issued but also to order and group these draw calls to
    perform them optimally. When rendering objects to the screen, we must think about
    the following layers or considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The fullscreen layer**: This comprises the HUD, the game layer, the translucent
    effects layer, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The viewport layer**: These exist if there are mirrors, portals, split screens,
    and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Depth considerations**: We need to draw objects in back-to-front order or
    farthest-to-closest order'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Texturing considerations**: These comprise textures, shading, lighting, and
    so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are various decisions to be made about the ordering of these different
    layers and components and in which order the draw calls are sent to the graphics
    card. An example would be in cases where translucent objects might be ordered
    back-to-front (i.e., sorted by depth first and texture second). For opaque objects,
    it might sort by texture first and eliminate draw calls for objects that are behind
    opaque objects.
  prefs: []
  type: TYPE_NORMAL
- en: Approaching gaming application optimization from the gamer’s perspective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For gaming applications, a lot of the performance depends on the end user’s
    hardware, OS, and game settings. This section describes a few things that end
    users can do to maximize gaming performance under different settings and different
    resource availability.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading hardware
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first obvious method to improve gaming application performance is to improve
    the hardware the end user’s game runs on. Some important candidates would be the
    gaming monitor, mouse, keyboard, and controllers used. Gaming monitors with higher
    refresh rates (such as 360-Hz monitors that support 1920 x 1080p (pixels) resolution
    and 240-Hz monitors that support 2560 x 1440p resolution) can provide high-quality
    rendering and fluid animations and enhance gameplay. We can also use a mouse with
    an extremely high polling rate, which allows for clicks and movements to be registered
    faster than before and reduce latency and lag. Similarly, for keyboards, gaming
    keyboards have much higher polling rates and can improve response times, especially
    for games where there are a lot of constant keystrokes, which can often be the
    case for RTS games. Another important point to mention here would be that using
    official and reputable controllers for specific consoles and platforms usually
    results in the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: Gaming monitor refresh rates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have discussed this aspect a few times before but with the rise of very high-quality
    images and animations, the quality and capacity of gaming monitors themselves
    have become quite important. Here, the key is to have a high refresh rate monitor
    that also has a low pixel response time so that animations can be rendered and
    updated quickly as well as smoothly. The configuration must also avoid the screen-tearing,
    ghosting, and blurring artifacts that we discussed before.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading your graphics card
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Upgrading the graphics card is another option that can result in significant
    improvements by increasing the frame rate and thus improving the gaming performance.
    NVIDIA found that upgrading the graphics card and the GPU drivers can help improve
    gaming performance by more than 20% in some cases. NVIDIA GeForce, ATI Radeon,
    Intel HD graphics, and so on are popular vendors that provide updated and optimized
    drivers that can be used to boost your gaming performance depending on which graphics
    cards are installed on the user’s platform.
  prefs: []
  type: TYPE_NORMAL
- en: Overclocking the graphics card
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another possible area of improvement instead of, or in addition to, upgrading
    the GPU is attempting to overclock the GPU with the aim of adding FPS. **Overclocking**
    the GPU works by increasing the frequency of the GPU and ultimately increasing
    the FPS output of the GPU. The one drawback of overclocking the GPU is increased
    internal temperatures, which can lead to overheating in extreme cases. So, when
    overclocking, you should monitor the increase in temperatures, increase the overclock
    levels gradually, monitor along the way, and make sure that the PC, laptop, or
    console has sufficient cooling in place. GPU overclocking can yield a performance
    boost of around 10%.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading your RAM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is another obvious general-purpose improvement technique that applies to
    low latency gaming applications as well. Adding additional RAM to the PC, smartphone,
    tablet, or console gives the game applications and graphics rendering tasks to
    perform their best. Thankfully, RAM costs have dropped tremendously over the last
    decade, so this is an easy way to boost the performance of gaming applications
    and is highly recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Tweaking the hardware, OS, and game settings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the previous subsection, we discussed some options for upgrading the hardware
    resources that lead to improved gaming applications performance. In this subsection,
    we will discuss settings that can be optimized for the hardware, the platform,
    the OS, and the game settings themselves to further push gaming application performance.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling game mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Game mode is a setting available for displays such as high-end TVs and similar
    high-end display monitors. Enabling game mode disables extra functionality in
    the display, which improves image and animation quality but comes at the price
    of higher latency. Enabling game mode will cause a slight deterioration in image
    quality but can help improve the low latency gaming application end user experience
    by reducing rendering latencies. An example of game mode is the Windows game mode
    on Windows 10, which optimizes gaming performance when enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Using high-performance mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The high-performance mode we discuss here refers to power settings. Different
    power settings try to optimize between battery usage and performance; high-performance
    mode drains battery power faster and possibly raises the internal temperature
    more than low-performance mode but it boosts the performance of the applications
    running.
  prefs: []
  type: TYPE_NORMAL
- en: Delaying automatic updates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Automatic updates are a feature most notably available in Windows, which downloads
    and installs security fixes automatically. While this is typically not a major
    problem, if a particularly large automatic update download and install starts
    while we are in the middle of an online gaming session, then it can affect the
    gaming performance and experience. Automatic updates can spike processor usage
    and bandwidth consumption if this coincides with a gaming session utilizing high
    processor usage and bandwidth, and can degrade the gaming performance. So, turning
    off or delaying automatic Windows updates is usually a good idea when running
    latency-sensitive and resource-intensive gaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: Turning off background services
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is another option similar to delaying automatic updates we just discussed.
    Here, we find and turn off apps and services that might be running in the background
    but are not necessarily needed for a low latency gaming session to function properly.
    In fact, turning these off prevents these applications from consuming hardware
    resources unexpectedly and non-deterministically during gaming sessions. This
    maximizes the low latency gaming application performance by making the maximum
    amount of resources available to that application.
  prefs: []
  type: TYPE_NORMAL
- en: Meeting or exceeding refresh rates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have discussed the concept of screen tearing before, so we at least need
    a system where the FPS meets or exceeds the refresh rate to prevent it. Technologies
    such as FreeSync and G-Sync facilitate smooth rendering without any screen tearing
    while still providing low latency performance. When frame rates exceed refresh
    rates, the latency continues to remain low, but if frame rates start exceeding
    refresh rates by a large magnitude, screen tearing can show up again. This can
    be addressed by using V-Sync technology or limiting FPS intentionally. FreeSync
    and G-Sync need hardware support, so you need a compatible GPU to use these technologies.
    But the upside with FreeSync and G-Sync is that you can completely disable V-Sync,
    which introduces latencies, and instead have a low latency and tear-free rendering
    experience as long as you have the hardware support for it.
  prefs: []
  type: TYPE_NORMAL
- en: Disabling triple buffering and V-Sync and running exclusively in full-screen
    mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have explained before that V-Sync can introduce additional latency due to
    the need to synchronize the frames rendered by the GPU with the display device.
    **Triple buffering** is just another form of V-Sync and has the same goal of reducing
    screen tearing. Triple buffering especially comes into play when running a game
    in windowed mode, in which the game runs inside a window instead of full screen.
    The key takeaway here is that to disable V-Sync and triple buffering to improve
    latency and performance, we have to run exclusively in full-screen mode.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing game settings for low latency and high frame rate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modern games come with a plethora of options and settings designed to maximize
    performance (sometimes at the cost of rendering quality), and the end user can
    optimize these parameters for their target hardware, platform, network resources,
    and performance requirements. Reducing settings such as **anti-aliasing** would
    be an example, and reducing resolution is another option. Finally, adjusting settings
    related to the viewing distance, texture rendering, shadows, and lighting can
    also maximize performance at the cost of lower rendering quality. Anti-aliasing
    seeks to render smooth edges instead of jagged edges when we try to render high-resolution
    images in low-resolution environments, so turning it down deteriorates image smoothness
    but accelerates low latency performance. Advanced rendering effects such as fire,
    water, motion blur, and lens flares can also be turned down if additional performance
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: Further optimizing your hardware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the last two subsections, we discussed options for optimizing low latency
    gaming applications by upgrading hardware resources and tweaking the hardware,
    OS, and game settings. In this final subsection, we will discuss how to squeeze
    the performance even further and what our options are to further optimize online
    low latency gaming application performance.
  prefs: []
  type: TYPE_NORMAL
- en: Installing DirectX 12 Ultimate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DirectX is a Windows graphics and gaming API developed by Microsoft. Upgrading
    DirectX to the latest version means the gaming platform gets access to the latest
    fixes and improvements and better performance. At this time, DirectX 12 Ultimate
    is the latest version, with DirectX 13 expected to be released at the end of 2022
    or early 2023.
  prefs: []
  type: TYPE_NORMAL
- en: Defragmenting and optimizing disks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Defragmentation of disks occurs as files are created and deleted on the hard
    disk and the free and used disk space blocks are spread out or fragmented causing
    lower driver performance. **Hard Disk Drives** (**HDD**) and **Solid State Drives**
    (**SSD**) are typically the two commonly used storage options for most gaming
    platforms. SSDs are significantly faster than HDDs and do not typically suffer
    from a lot of fragmentation-related issues but can still become suboptimal over
    time. Windows, for instance, has a defragmentation and optimization application
    to optimize the performance of the drives, which can improve gaming application
    performance as well.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring the laptop cools optimally
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When under heavy loads due to high processor, network, memory, and disk usage,
    the internal temperatures of the laptop or PC are raised. Other than being dangerous,
    it also forces the laptop to try and cool itself down by limiting resource consumption
    and thus, ultimately, performance. We mention laptops specifically to explain
    this issue because PCs typically have better airflow and cooling abilities than
    laptops. Ensuring that laptops cool effectively by clearing the vents and fans,
    removing dirt and dust, placing them on a hard, smooth, and flat surface, using
    an external power supply to not drain the battery, and possibly even using additional
    cooling stands can boost gaming performance on laptops.
  prefs: []
  type: TYPE_NORMAL
- en: Using NVIDIA Reflex low latency technology
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**NVIDIA Reflex** low latency technology seeks to minimize the input lag measured
    from the moment the user clicks the mouse or hits a key on the keyboard or controller
    to the time at which the impact of that action is rendered on screen. We have
    already discussed the sources of latency here, and NVIDIA breaks this down into
    nine chunks from the input device to the processors and the display. The NVIDIA
    Reflex software speeds up this critical path performance by improving communication
    paths between CPUs and GPUs, optimizing frame delivery and rendering by skipping
    unnecessary tasks and pauses, and accelerating the GPU rendering time. NVIDIA
    also provides an NVIDIA Reflex Latency Analyzer to measure the speed-up achieved
    by using these low latency enhancements.'
  prefs: []
  type: TYPE_NORMAL
- en: Discussing the design of IoT and retail analytics systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed **IoT** and retail analytics and many
    of the different use cases that they create. Our focus in this section will be
    to have a brief discussion about the technologies being used to achieve low latency
    performance for these applications and use cases. Note that IoT is a technology
    space that is still actively growing and evolving, so there are going to be a
    lot of breakthroughs and advancements in the coming years. Let us quickly recap
    some important use cases of IoT and retail data analytics. A lot of these new
    applications and future possibilities are fueled by the research and advancements
    in 5G wireless technology, **edge computing**, and **artificial intelligence**
    (**AI**). We will look at those aspects in the next section, along with other
    technologies that facilitate applications using low latency IoT and retail data
    analytics.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of applications fall under the remote inspection/analysis category, where
    drones can replace humans when it comes to being the first line of defense in
    fields such as remote technicians, monitoring infrastructure such as bridges,
    tunnels, railways, highways, and waterways, and even things such as transformers,
    utility wires, gas pipelines, and electricity and telephone lines. Incorporating
    AI into such applications enhances the complexity of the data analysis possible
    and thus creates new opportunities and use cases. Incorporating AR technology
    also increases the possibilities. Modern automobiles collect large amounts of
    data and, with the possibility of having autonomous driving vehicles at some point,
    the use cases for IoT only expand further. Automation in agriculture, shipping
    and logistics, supply chain management, inventory, and warehouse management, and
    managing fleets of vehicles creates numerous additional use cases for IoT technology
    and analyzes data generated from and collected by these devices.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring low latency in IoT devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at some considerations to facilitate low latency
    performance in IoT applications and retail analytics. Note that a lot of the considerations
    we discussed for real-time video streaming and online video gaming use cases apply
    here as well, such as hardware resources, encoding and decoding data streams,
    content delivery mechanisms, and hardware and system-level optimizations. We will
    not repeat those techniques here in the interest of brevity, but we will present
    additional low latency considerations that apply specifically to IoT and retail
    data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: P2P connectivity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: P2P connectivity for IoT devices establishes direct connectivity between different
    IoT devices or between an IoT device and the end user application. The user’s
    input from their device is directly sent to the IoT device that it is meant for
    without any third-party service or server in between to minimize latencies. Similarly,
    data from the IoT devices is streamed back to the other devices directly from
    the device. The P2P approach is an alternative to connecting to the IoT device
    through a cloud, which has extra latencies due to additional server databases,
    cloud worker instances, and so on. P2P is also referred to as a decentralized
    **Application Enablement Platform** (**AEP**) for IoT, which is an alternative
    to the cloud-based AEPs.
  prefs: []
  type: TYPE_NORMAL
- en: Using fifth-generation wireless (5G)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5G wireless technology delivers higher bandwidth, ultra-low latency, reliability,
    and scalability. Not only do the end users benefit from 5G but it also helps each
    step of the IoT devices and applications that require low latency and real-time
    data streaming and processing. 5G’s low latency facilitates faster and more reliable
    inventory tracking, transportation services and monitoring, real-time visibility
    into distribution logistics, and more. The 5G network was designed keeping all
    the different IoT use cases in mind, so it is an excellent fit for all kinds of
    IoT applications and more.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding edge computing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Edge computing** is a distributed processing technology where the key point
    is to bring the processing application and the data storage components as close
    to the source of data as possible, which, in this case, is the IoT devices that
    capture the data. Edge computing breaks the older paradigm where the data is recorded
    by remote devices and then transferred to a central storage and processing location
    and then results are transported back to the devices and client applications.
    This exciting new technology is revolutionizing how massive amounts of data generated
    by a lot of IoT devices are transported, stored, and processed. The main goals
    for edge computing are to reduce bandwidth costs to transfer massive amounts of
    data across wide distances and to support ultra-low latency to facilitate real-time
    applications that need to process massive amounts of data as quickly and efficiently
    as possible. Additionally, it also reduces costs for businesses because they do
    not necessarily need a centralized and cloud-based storage and processing solution.
    This point is especially important when it comes to IoT applications because the
    sheer scale of how many devices generate data means the bandwidth consumption
    will increase exponentially.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding all the details of the physical architecture of an edge computing
    system is difficult and outside the scope of this book. However, at a very high
    level, the client devices and IoT devices connect to edge modules that are available
    nearby. Typically, there are many gateways and servers that are deployed by service
    providers or enterprises looking to build their own edge network to support such
    edge computing operations. The devices that can use these edge modules range from
    IoT sensors, laptops and computers, smartphones and tablets, cameras, microphones,
    and anything else you can imagine.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the relationship between 5G and edge computing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We mentioned before that 5G was designed and developed keeping IoT and edge
    computing in mind. So IoT, 5G, and edge computing are all related to each other
    and work with each other to maximize the use cases and performance of these IoT
    applications. Theoretically, edge computing could be deployed to non-5G networks,
    but obviously, 5G is the preferred network. However, the reverse is not true;
    to leverage 5G’s true power, you need an edge computing infrastructure to really
    maximize the use of everything 5G offers. This is intuitive because, without an
    edge computing infrastructure, the data from the devices must travel long distances
    to get processed, and then results must travel a long distance to reach the end
    user’s applications or other devices. In those cases, even if you have a 5G network,
    the latency due to the data’s travel distance far outweighs the latency improvements
    gained by using 5G. So, edge computing is necessary when it comes to IoT applications
    and applications that need to analyze retail data in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the relationship between edge computing and AI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data analytics techniques, machine learning, and AI have revolutionized how
    the retail and non-retail data collected from IoT devices is analyzed to derive
    meaningful insights. NVIDIA is a pioneer when it comes to developing new hardware
    solutions to push not only edge computing but also AI processing to the maximum.
    Jetson AGX Orin is a particularly good example of how NVIDIA packages AI and robotics
    functionality together into a single product.
  prefs: []
  type: TYPE_NORMAL
- en: We will not go into too many details about the Jetson AGX Orin since that is
    neither the focus nor within the scope of this book. The Jetson AGX Orin has a
    few qualities that make it excellent for AI, robotics, and autonomous vehicles
    – it is compact, enormously powerful, and energy-efficient. The power and energy
    efficiency allows it to be used for AI applications and enable edge computing.
    This latest model in particular lets developers combine AI, robotics, **natural
    language processing** (**NLP**), computer vision, and so on into a compact package,
    making it excellent for robotics. This device also has multiple I/O connectors
    and is compatible with many different sensors (MIPI, USB, cameras, etc.). There
    are also additional hardware expansion slots to support storage, wireless, and
    so on. This powerful GPU-powered device makes it perfect for deep learning (in
    addition to classic machine learning) and computer vision applications such as
    robotics.
  prefs: []
  type: TYPE_NORMAL
- en: Buying and deploying edge computing systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When it comes to purchasing and setting up an edge computing infrastructure,
    businesses generally go down one of two routes: customize the components and build
    and manage the infrastructure in-house or use a vendor that provides and manages
    edge services for the enterprise.'
  prefs: []
  type: TYPE_NORMAL
- en: Building and managing the edge computing infrastructure in-house needs expertise
    from the IT, network, and business departments. Then, they can select the edge
    devices from hardware vendors (such as IBM, Dell, etc.) and architect and manage
    the 5G network infrastructure for the specific use case. This option only makes
    sense for a large enterprise that sees value in customizing its edge computing
    infrastructure for a specific use case. When it comes to the option of having
    a third-party vendor facilitate and manage the edge computing infrastructure,
    the vendor sets up the hardware, software, and networking architecture for a fee.
    This leaves the management of a complicated system such as edge computing infrastructure
    to firms such as GE and Siemens with expertise in this field and allows the client
    business to focus on building on top of this infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging proximity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have implicitly discussed this point in the previous sections, but now we
    will explicitly discuss it here. A key requirement of IoT applications is achieving
    ultra-low latency performance, and the key to achieving that is leveraging proximity
    between the different devices and applications involved in the IoT use case. It
    is no surprise that edge computing is the key to leveraging proximity for IoT
    applications to minimize latencies from capturing data to processing it and sharing
    results with other devices or client applications. As we have seen before, the
    biggest bottleneck with a non-edge computing infrastructure is the distance of
    data centers and processing resources from the source of the data and the destination
    of the results. This gets worse with additional distributed data centers spread
    out miles away from each other, and ultimately leads to critically high latencies
    and lags. Clearly, placing edge computing resources closer to the data sources
    is the key to driving IoT adoption, IoT use cases, and scaling IoT businesses
    to a huge number of devices and users.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing cloud costs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is another point we have discussed before, but we will have a formal conversation
    about it in this section. There are billions of IoT devices out there and they
    generate a continuous stream of data. Any effective IoT-driven business will need
    to scale extremely well to large increases in the number of devices and clients
    involved, which leads to exponential increases in the amount of data recorded
    and data processed by edge computing, and the results being transferred to other
    devices and clients. Data-heavy infrastructures that rely on centralized cloud
    infrastructure cannot support IoT applications in a cost-effective manner and
    the data and cloud infrastructure itself becomes a significant fraction of an
    enterprise’s expenses. The clear solution here is to find a low-cost edge solution
    (third-party or in-house) and use it to facilitate the IoT data capture, storage,
    and processing needs. This removes the costs associated with transferring data
    in and out of cloud solutions and can improve edge computing reliability and cut
    costs significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will conclude our discussion of low latency IoT applications by summarizing
    the current and future state of IoT applications in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Current and future states of IoT applications](img/Figure_2.4_B19434.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Current and future states of IoT applications
  prefs: []
  type: TYPE_NORMAL
- en: Exploring low latency electronic trading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final example of low latency applications is the applications used in low
    latency electronic trading and ultra-low latency electronic trading, also known
    as HFT. We will build a full end-to-end low latency electronic trading system
    from scratch in C++ in the rest of this book. So, in this section, we will briefly
    discuss the important considerations for electronic trading applications to achieve
    low latency performance and then build out the low-level details in the remaining
    chapters. *Developing High-Frequency Trading Systems* by Sebastian Donadio, Sourav
    Ghosh, and Romain Rossier would be an excellent book for understanding low latency
    electronic trading systems in greater detail for interested readers. Our focus
    in this book will be to design and build each component from scratch in C++ to
    learn about low latency application development, but that book can be used as
    a good reference for the additional theory behind the HFT business.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the need for low latency in modern electronic trading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the modernization of electronic trading and the rise of HFT, low latencies
    are more important than ever before for these applications. In many cases, achieving
    lower latency leads to a direct increase in trading revenue. In some cases, there
    is a constant race to try and reduce latencies more and more to maintain a competitive
    edge in the markets. And in extreme cases, if a participant falls behind in the
    arms race to the lowest possible latencies, they may go out of business.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trading opportunities in modern electronic markets are extremely short-lived,
    so only the market participants that can process market data and find such an
    opportunity and quickly send orders in reaction to this can be profitable. Failure
    to react quickly enough means you get a smaller piece of the opportunity, and
    it is common for only the fastest participant to get all the profit, and for all
    other slower participants to get nothing. Another nuance here is that if a participant
    is not quick enough to react to a market event, they can also be caught on the
    wrong side of the trade and lose money to people who were able to react to the
    event quickly enough. In such a case, trading profits are not just lower, but
    trading revenue can be negative (i.e., losses). To understand this better, let
    us present an example of something we will build in this book: market-making and
    liquidity-taking algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Without going into too many details, a market-making algorithm has orders in
    the market that other participants can trade against when needed. A market-making
    algorithm thus needs to constantly re-evaluate its active orders and change the
    prices and quantities for them depending on the market conditions. A liquidity-taking
    algorithm, however, does not always have active orders in the market. This algorithm
    instead waits for an opportunity to present itself and then trades against a market-making
    algorithm’s active order in the book. A simple view of the HFT market would be
    a constant battle between market-making and liquidity-taking algorithms because
    they naturally take opposite sides.
  prefs: []
  type: TYPE_NORMAL
- en: In this setup, a market-making algorithm loses money when it is slow at modifying
    its active orders in the market. For instance, say depending on market conditions,
    it is quite clear that the market prices are going to go up in the short term;
    a market-making algorithm will try to move or cancel its sell orders if they are
    at risk of being executed since it no longer wants to sell at those prices. A
    liquidity-taking algorithm, at the same time, will try to see whether it can send
    a buy order to trade against a market maker’s sell order at that price. In this
    race, if the market-making algorithm is slower than the liquidity-taking algorithms,
    it will not be able to modify or cancel its sell order. If the liquidity-taking
    algorithm is slow, it will not be able to execute against the orders it wanted
    to either because a different (and faster) algorithm was able to execute before
    it or because the market maker was able to move out of the way. This example should
    make it clear to you that latency directly affects trading revenue in electronic
    trading.
  prefs: []
  type: TYPE_NORMAL
- en: For HFT, trading applications on the client’s side can receive and process market
    data, analyze the information, look for opportunities, and send an order out to
    the exchange, all within sub-10-microsecond latency, and using **Field-Programmable
    Gate Arrays** (**FPGAs**), can reduce that to sub-1-microsecond latency. FPGAs
    are special hardware chips that are re-programmable and can be used to build extremely
    specialized and low latency functionality directly onto the chip itself. Understanding
    the details and developing and using FPGAs is an advanced topic beyond this book’s
    scope.
  prefs: []
  type: TYPE_NORMAL
- en: While we have referred to trading performance and revenue in the previous example,
    low latencies are also important in other aspects of electronic trading businesses
    that might not be immediately obvious. Obviously, trading revenues and performance
    are still the primary focus for trading applications; another important requirement
    for long-term business continuity is real-time risk management. Since each electronic
    market has many trading instruments and each of those continuously changes prices
    throughout the day, there is a tremendous amount of data that the risk management
    system needs to keep up with, across all the exchanges and all the products available
    throughout the day.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, since a firm employs HFT strategies across all these products
    and exchanges, the firm’s position on each of these products changes rapidly all
    day long. A real-time risk management system needs to evaluate the firm’s constantly
    evolving exposure across all these products against market prices to track profits
    and losses and risk throughout the day. The risk evaluation metrics and systems
    can themselves be quite complicated; for instance, in options trading, it is common
    to run Monte Carlo simulations to try and find worst-case risk evaluations in
    real time or very close to real time. Some risk management systems are also in
    charge of shutting down automated trading strategies if they exceed any of their
    risk limits. These risk systems are often added to multiple components – a central
    risk system, the order gateways, and the trading strategies themselves – but we
    will understand these details later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving the lowest latencies in electronic trading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will briefly discuss some of the higher-level ideas and
    concepts when it comes to implementing low latency electronic trading systems.
    We will, of course, revisit these with examples in greater detail as we work on
    building our electronic trading ecosystem in the coming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing trading server hardware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Getting powerful trading servers to support the low latency trading operations
    is a first step. Typically, the processing power of these servers depends on the
    architecture of the trading system processes, such as how many processes we expect
    to run, how many network resources we expect to consume, and how much memory we
    expect these applications to consume. Typically, low latency trading applications
    have high CPU usage, low kernel usage (system calls), low memory consumption,
    and relatively high network resource usage during busy trading periods. CPU registers,
    cache architecture, and capacity matter as well, and typically, we try to get
    larger sizes, if possible, but these can be quite expensive. Advanced considerations
    such as **Non-Uniform Memory Access** (**NUMA**), processor instruction sets,
    instruction pipelines and instruction parallelism, cache hierarchy architecture
    details, hyperthreading, and overclocked CPUs are often considered, but those
    are extremely advanced optimization techniques and outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Network Interface Cards, switches, and kernel bypass
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Trading servers that need to support ultra-low latency trading applications
    (especially ones that must read massive amounts of market data, update packets
    from the network, and process them) need specialized **Network Interface Cards**
    (**NICs**) and switches. The NICs preferred for such applications need to have
    very low latency performance, low jitter, and large buffer capacities to handle
    market data bursts without dropping packets. Also, optimal NICs for modern electronic
    trading applications support an especially low-latency path that avoids system
    calls and buffer copies, known as **kernel bypass**. One example is **Solarflare**,
    which provides **OpenOnload** and APIs such as **ef_vi** and **TCPDirect, which**
    bypass the kernel when using their NICs; **Exablaze** is another example of a
    specialized NIC that supports kernel bypass. Network switches show up in various
    places in the network topology, which support interconnectivity between trading
    servers and trading servers that are located far away from each other and between
    trading servers and electronic exchange servers. For network switches, one of
    the important considerations is the size of the buffer that the switch can support
    to buffer packets that need to be forwarded. Another important requirement is
    the latency between a switch receiving a packet and forwarding it to the correct
    interface known as **switching latency**. Switching latencies are generally very
    low, in the order of tens of hundreds of nanoseconds, but this applies to all
    inbound or outbound traffic going through the switch, so needs to be consistently
    low to not have a negative impact on trading performance.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multithreading, locks, context switches, and CPU scheduling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have discussed the closely related but technically different concepts of
    bandwidth and low latency in the previous chapter. It is sometimes incorrectly
    assumed that having an architecture with a larger number of threads is always
    lower-latency, but this is not always true. Multithreading adds value in certain
    areas of low latency electronic trading systems, and we will make use of it in
    the system we build in this book. But the point here is that we need to be careful
    when using additional threads in HFT systems because, while adding threads generally
    boosts throughput for applications that need it, it can sometimes end up increasing
    latencies in applications as well. As we increase the number of threads, we must
    think about concurrency and thread safety, and if we need to use locks for synchronization
    and concurrency between threads, that adds additional latencies and context switches.
    Context switches are not free because the scheduler and OS must save the state
    of the thread or process being switched out and load the state of the thread or
    process that will be run next. Many lock implementations are built on top of kernel
    system calls, which are more expensive than user space routines, thus increasing
    the latencies in a heavily multithreaded application even further. For optimal
    performance, we try to get the CPU scheduler to do little to no work (i.e., processes
    and threads that are scheduled to run are never context switched out and keep
    running in user space). Additionally, it is quite common to pin specific threads
    and processes to specific CPU cores, which eliminates context switching and the
    OS needing to find free cores to schedule tasks, and additionally, improves memory
    access efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamically allocating memory and managing memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dynamic memory allocation is a request for memory blocks of arbitrary sizes
    made at runtime. At a very high level, dynamic memory allocations and deallocations
    are handled by the OS by looking through a list of free memory blocks and trying
    to allocate a contiguous block as large as the program requested. Dynamic memory
    deallocations are handled by appending the freed blocks to the list of free blocks
    managed by the OS. Searching through this list can incur higher and higher latencies
    as the program runs through the day and memory gets increasingly fragmented. Additionally,
    if dynamic memory allocations and deallocations are on the same critical path,
    then they incur an additional overhead every single time. This is one of the major
    reasons we discussed before that led us to choose C++ as our preferred language
    for building low latency and resource-constrained applications. We will explore
    the performance impact of dynamic memory allocation and techniques to avoid it
    during later chapters in this book as we build our own trading system.
  prefs: []
  type: TYPE_NORMAL
- en: Static versus dynamic linking and compile time versus runtime
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Linking** is the compilation or translation step in the process of converting
    high-level programming language source code into machine code for the target architecture.
    Linking ties together pieces of code that might be in different libraries – these
    can be libraries internal to the code base or external standalone libraries. During
    the linking step, we have two choices: **static linking** or **dynamic linking**.'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic linking is when the linker does not incorporate the code from libraries
    into the final binary at linking time. Instead, when the main application requires
    code from the shared libraries for the first time, then the resolution is performed
    at runtime. Obviously, there is a particularly large extra cost incurred at runtime
    the first time the shared library code is called. The bigger downside is that
    since the compiler and linker do not incorporate the code at compilation and linking
    time, they are unable to perform possible optimizations, resulting in an application
    that can be inefficient overall.
  prefs: []
  type: TYPE_NORMAL
- en: Static linking is when the linker arranges the application code and the code
    for the library dependencies into a single binary executable file. The upside
    here is that the libraries are already linked at compile time so there is no need
    for the OS to find and resolve the dependencies at runtime startup by loading
    the dependent libraries before the application starts executing. The even bigger
    upside is that this creates an opportunity for the program to be super-optimized
    at compile and linking time to yield lower latencies at runtime. The downside
    to static linking over dynamic linking is that the application binary is much
    larger and each application binary that relies on the same set of external libraries
    has a copy of all the external library code compiled and linked into the binary.
    It is common for ultra-low latency electronic trading systems to link all dependent
    libraries statically to minimize runtime performance latencies.
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed compile time versus runtime processing in the previous chapter,
    and that approach tries to move the maximum amount of processing to the compilation
    step instead of at runtime. This increases compile times, but the runtime performance
    latencies are much lower because a lot of the work is already done at compile
    time. We will look at this aspect in detail specifically for C++ in the next few
    chapters and throughout the course of this book as we build our electronic trading
    system in C++.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at different low latency applications in different
    business areas. The goals were to understand how low latency applications impact
    businesses in different areas and the similarities that some of these applications
    share, such as the hardware requirements and optimization, software design, performance
    optimization, and different revolutionary technologies being used to achieve these
    performance requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The first applications we looked at in detail were real-time, low latency, online
    video streaming applications. We discussed different concepts and investigated
    where high latencies come from, and how that affects performance and businesses.
    Finally, we discussed different technologies and solutions, and platforms that
    facilitate low latency video streaming applications to be a success.
  prefs: []
  type: TYPE_NORMAL
- en: The next applications we looked at had a lot of overlap with video streaming
    applications – offline and online video gaming applications. We introduced some
    additional concepts and considerations that apply to offline and online gaming
    applications and explained their impact on the user experience and thus, ultimately,
    on business performance. We discussed a myriad of things to consider when trying
    to maximize the performance of these applications, ranging from a lot of factors
    that apply to live video streaming applications to additional hardware and software
    considerations for gaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: We then briefly discussed the requirement of low latency performance when it
    comes to IoT devices and retail data collection and analysis applications. This
    is a relatively new and fast-improving technology and is likely to continue growing
    aggressively over the next decade. Lots of research and advancements are being
    made for IoT devices and we find new business ideas and use cases as we make progress
    here. We discussed how 5G wireless and edge computing technologies are breaking
    the old paradigm of central data storage and processing and why that is critical
    for IoT devices and applications.
  prefs: []
  type: TYPE_NORMAL
- en: The last applications we also discussed briefly in this chapter were low latency
    electronic trading and HFT applications. We kept the discussion short and focused
    on the higher-level ideas when it comes to maximizing the performance of low latency
    and ultra-low latency electronic trading applications. We did so because we will
    build a full end-to-end C++ low latency electronic trading ecosystem from scratch
    in the remaining chapters of this book. When we do that, we will discuss, understand,
    and implement all the different low latency C++ concepts and ideas with examples
    and performance data, so there is a lot more to come on this application.
  prefs: []
  type: TYPE_NORMAL
- en: We will move on from this discussion of different low latency applications to
    a more in-depth discussion of the C++ programming language. We will discuss the
    correct approach to using C++ for low latency performance, the different modern
    C++ features, and how to unleash the power of modern C++ compiler optimizations.
  prefs: []
  type: TYPE_NORMAL
