# 第三章：3

# 分析和测量性能

由于这是一本关于编写高效运行的 C++代码的书，我们需要涵盖一些关于如何衡量软件性能和估算算法效率的基础知识。本章大部分主题并不特定于 C++，在面对性能问题时都可以使用。

您将学习如何使用大 O 符号估算算法效率。在选择 C++标准库中的算法和数据结构时，这是必不可少的知识。如果您对大 O 符号不熟悉，这部分可能需要一些时间来消化。但不要放弃！这是一个非常重要的主题，以便理解本书的其余部分，更重要的是，成为一个注重性能的程序员。如果您想要更正式或更实用的介绍这些概念，有很多专门讨论这个主题的书籍和在线资源。另一方面，如果您已经掌握了大 O 符号并知道摊销时间复杂度是什么，您可以略过下一节，转到本章的后面部分。

本章包括以下部分：

+   使用大 O 符号估算算法效率

+   优化代码的建议工作流程，这样您不会在没有充分理由的情况下花费时间微调代码

+   CPU 性能分析器——它们是什么以及为什么你应该使用它们

+   微基准测试

让我们首先看一下如何使用大 O 符号来估算算法效率。

# 渐近复杂度和大 O 符号

通常解决问题的方法不止一种，如果效率是一个问题，您应该首先专注于通过选择正确的算法和数据结构进行高级优化。评估和比较算法的一个有用方法是分析它们的渐近计算复杂性——也就是分析输入大小增加时运行时间或内存消耗的增长情况。此外，C++标准库为所有容器和算法指定了渐近复杂度，这意味着如果您使用这个库，对这个主题的基本理解是必须的。如果您已经对算法复杂度和大 O 符号有很好的理解，可以安全地跳过本节。

让我们以一个例子开始。假设我们想编写一个算法，如果在数组中找到特定的键，则返回`true`，否则返回`false`。为了找出我们的算法在不同大小的数组上的行为，我们希望分析这个算法的运行时间作为其输入大小的函数：

```cpp
bool linear_search(const std::vector<int>& vals, int key) noexcept { 
  for (const auto& v : vals) { 
    if (v == key) { 
      return true; 
    } 
  } 
  return false; 
} 
```

该算法很简单。它遍历数组中的元素，并将每个元素与键进行比较。如果我们幸运的话，在数组的开头找到键并立即返回，但我们也可能在整个数组中循环而根本找不到键。这将是算法的最坏情况，通常情况下，这是我们想要分析的情况。

但是当我们增加输入大小时，运行时间会发生什么变化？假设我们将数组的大小加倍。嗯，在最坏的情况下，我们需要比较数组中的所有元素，这将使运行时间加倍。输入大小和运行时间之间似乎存在线性关系。我们称这为线性增长率。

![](img/B15619_03_01.png)

图 3.1：线性增长率

现在考虑以下算法：

```cpp
struct Point { 
  int x_{}; 
  int y_{}; 
}; 

bool linear_search(const std::vector<Point>& a, const Point& key) { 
  for (size_t i = 0; i < a.size(); ++i) { 
    if (a[i].x_ == key.x_ && a[i].y_ == key.y_) { 
      return true; 
    } 
  } 
  return false; 
} 
```

我们比较的是点而不是整数，并且我们使用下标运算符的索引来访问每个元素。这些变化如何影响运行时间？绝对运行时间可能比第一个算法高，因为我们做了更多的工作——例如，比较点涉及两个整数，而不是数组中每个元素的一个整数。然而，在这个阶段，我们对算法表现的增长率感兴趣，如果我们将运行时间绘制成输入大小的函数，我们仍然会得到一条直线，如前图所示。

作为搜索整数的最后一个例子，让我们看看是否可以找到更好的算法，如果我们假设数组中的元素是排序的。我们的第一个算法将在元素的顺序无关紧要的情况下工作，但是如果我们知道它们是排序的，我们可以使用二分搜索。它通过查看中间的元素来确定它是否应该继续在数组的第一半或第二半中搜索。为简单起见，索引`high`，`low`和`mid`的类型为`int`，需要`static_cast`。更好的选择是使用迭代器，这将在后续章节中介绍。以下是算法：

```cpp
bool binary_search(const std::vector<int>& a, int key) {
  auto low = 0; 
  auto high = static_cast<int>(a.size()) - 1;
  while (low <= high) {
    const auto mid = std::midpoint(low, high); // C++20
    if (a[mid] < key) {
      low = mid + 1;
    } else if (a[mid] > key) {
      high = mid - 1;
    } else {
      return true;
    }
  }
  return false;
} 
```

正如您所看到的，这个算法比简单的线性扫描更难正确实现。它通过*猜测*数组中间的元素来寻找指定的键。如果不是，它将比较键和中间的元素，以决定应该在数组的哪一半中继续寻找键。因此，在每次迭代中，它将数组减半。

假设我们使用包含 64 个元素的数组调用`binary_search()`。在第一次迭代中，我们拒绝 32 个元素，在下一次迭代中，我们拒绝 16 个元素，在下一次迭代中，我们拒绝 8 个元素，依此类推，直到没有更多元素可以比较，或者直到我们找到键。对于输入大小为 64，最多将有 7 次循环迭代。如果我们将输入大小*加倍*到 128 呢？由于我们在每次迭代中将大小减半，这意味着我们只需要*再进行一次循环迭代*。显然，增长率不再是线性的——实际上是对数的。如果我们测量`binary_search()`的运行时间，我们将看到增长率看起来类似于以下内容：

![](img/B15619_03_02.png)

图 3.2：对数增长率

在我的机器上，对三种算法进行快速计时，每次调用 10,000 次，不同输入大小（*n*）产生了以下表中显示的结果：

| 算法 | n = 10 | n = 1,000 | n = 100,000 |
| --- | --- | --- | --- |
| 使用`int`的线性搜索 | 0.04 毫秒 | 4.7 毫秒 | 458 毫秒 |
| 使用`Point`的线性搜索 | 0.07 毫秒 | 6.7 毫秒 | 725 毫秒 |
| 使用`int`的二分搜索 | 0.03 毫秒 | 0.08 毫秒 | 0.16 毫秒 |

表 3.1：不同版本搜索算法的比较

比较算法 1 和 2，我们可以看到，比较点而不是整数需要更多时间，但即使输入大小增加，它们仍然处于相同数量级。然而，当输入大小增加时，比较所有三种算法时，真正重要的是算法表现出的增长率。通过利用数组已排序的事实，我们可以用很少的循环迭代来实现搜索功能。对于大数组，与线性扫描数组相比，二分搜索实际上是免费的。

在确定选择正确的算法和数据结构之前，花时间调整代码通常不是一个好主意。

如果我们能以一种有助于我们决定使用哪种算法的方式来表达算法的增长率，那不是很好吗？这就是大 O 符号表示法派上用场的地方。

以下是一个非正式的定义：

如果*f(n)*是一个指定算法在输入大小*n*的运行时间的函数，我们说*f(n)*是*O(g(n))*，如果存在一个常数*k*，使得![](img/B15619_03_001.png)。

这意味着我们可以说`linear_search()`的时间复杂度是*O(n)*，对于两个版本（一个操作整数，一个操作点），而`binary_search()`的时间复杂度是*O(log n)*或者*O(log n)*的大 O。

实际上，当我们想要找到一个函数的大 O 时，我们可以通过消除除了具有最大增长率的项之外的所有项，然后去掉任何常数因子来做到这一点。例如，如果我们有一个时间复杂度由*f(n) = 4n*² *+ 30n + 100*描述的算法，我们挑出具有最高增长率的项，4*n*²。接下来，我们去掉常数因子 4，最终得到*n*²，这意味着我们可以说我们的算法运行在*O(n*²*)*。找到算法的时间复杂度可能很难，但是当你在编写代码时开始思考它时，它会变得更容易。在大多数情况下，跟踪循环和递归函数就足够了。

让我们试着找出以下排序算法的时间复杂度：

```cpp
void insertion_sort(std::vector<int>& a) { 
  for (size_t i = 1; i < a.size(); ++i) { 
    auto j = i; 
    while (j > 0 && a[j-1] > a[j]) {  
      std::swap(a[j], a[j-1]); 
      --j;  
    } 
  } 
} 
```

输入大小是数组的大小。通过查看迭代所有元素的循环，可以大致估计运行时间。首先，有一个迭代*n - 1*个元素的外部循环。内部循环不同：第一次到达`while`循环时，`j`为 1，循环只运行一次。在下一次迭代中，`j`从 2 开始减少到 0。对于外部`for`循环的每次迭代，内部循环需要做更多的工作。最后，`j`从*n - 1*开始，这意味着在最坏的情况下，我们执行了`swap()`*1 + 2 + 3 + ... + (n - 1)*次。我们可以通过注意到这是一个等差数列来用*n*来表示这一点。数列的和是：

![](img/B15619_03_002.png)

因此，如果我们设*k = (n - 1)*，排序算法的时间复杂度是：

![](img/B15619_03_003.png)

我们现在可以通过首先消除除了具有最大增长率的项之外的所有项来找到这个函数的大 O，这让我们得到了*(1/2)n*²。之后，我们去掉常数*1/2*，得出排序算法的运行时间是*O(n*²*)*。

## 增长率

如前所述，找到复杂度函数的大 O 的第一步是消除除了具有最高增长率的项之外的所有项。为了能够做到这一点，我们必须知道一些常见函数的增长率。在下图中，我画出了一些最常见的函数：

![](img/B15619_03_03.png)

图 3.3：增长率函数的比较

增长率与机器或编码风格等无关。当两个算法之间的增长率不同时，当输入大小足够大时，增长率最慢的算法将始终获胜。让我们看看不同增长率的运行时间会发生什么，假设执行 1 单位的工作需要 1 毫秒。下表列出了增长函数、其常见名称和不同的输入大小*n*：

| 大 O | 名称 | n = 10 | n = 50 | n = 1000 |
| --- | --- | --- | --- | --- |
| *O(1)* | 常数 | 0.001 秒 | 0.001 秒 | 0.001 秒 |
| *O(log n)* | 对数 | 0.003 秒 | 0.006 秒 | 0.01 秒 |
| *O(n)* | 线性 | 0.01 秒 | 0.05 秒 | 1 秒 |
| *O(n log n)* | 线性对数或*n log n* | 0.03 秒 | 0.3 秒 | 10 秒 |
| *O(n*²*)* | 二次方 | 0.1 秒 | 2.5 秒 | 16.7 分钟 |
| *O(2*^n*)* | 指数 | 1 秒 | 35,700 年 | 3.4 * 10²⁹⁰年 |

表 3.2：不同增长率和各种输入大小的绝对运行时间

注意右下角的数字是一个 291 位数！将其与宇宙的年龄 13.7 * 10⁹年相比较，后者只是一个 11 位数。

接下来，我将介绍摊销时间复杂度，这在 C++标准库中经常使用。

## 摊销时间复杂度

通常，算法在不同的输入下表现不同。回到我们线性搜索数组中元素的算法，我们分析了一个关键字根本不在数组中的情况。对于该算法，这是最坏情况，即算法将需要*最多*的资源。最佳情况是指算法将需要*最少*的资源，而平均情况指定了算法在不同输入下平均使用的资源量。

标准库通常指的是对容器进行操作的函数的*摊销运行时间*。如果算法以恒定的摊销时间运行，这意味着它在几乎所有情况下都将以*O(1)*运行，只有极少数情况下会表现得更差。乍一看，摊销运行时间可能会与平均时间混淆，但正如您将看到的那样，它们并不相同。

为了理解摊销时间复杂度，我们将花一些时间思考`std::vector::push_back()`。假设向量在内部具有固定大小的数组来存储所有元素。当调用`push_back()`时，如果固定大小数组中还有空间可以存放更多元素，则该操作将在常数时间*O(1)*内运行，即不依赖于向量中已有多少元素，只要内部数组还有空间可以存放一个以上的元素：

```cpp
if (internal_array.size() > size) { 
  internal_array[size] = new_element; 
  ++size; 
} 
```

但是当内部数组已满时会发生什么？处理增长向量的一种方法是创建一个新的空内部数组，大小更大，然后将所有元素从旧数组移动到新数组。这显然不再是常数时间，因为我们需要对数组中的每个元素进行一次移动，即*O(n)*。如果我们认为这是最坏情况，那么这意味着`push_back()`是*O(n)*。然而，如果我们多次调用`push_back()`，我们知道昂贵的`push_back()`不会经常发生，因此如果我们知道`push_back()`连续调用多次，那么说`push_back()`是*O(n)*是悲观且不太有用的。

摊销运行时间用于分析一系列操作，而不是单个操作。我们仍然在分析最坏情况，但是针对一系列操作。摊销运行时间可以通过首先分析整个序列的运行时间，然后将其除以序列的长度来计算。假设我们执行一系列*m*个操作，总运行时间为*T(m)*：

![](img/B15619_03_004.png)

其中*t*[0] *= 1*, *t*[1] *= n*, *t*[2] *= 1*, *t*[3] *= n*，依此类推。换句话说，一半的操作在常数时间内运行，另一半在线性时间内运行。所有*m*个操作的总时间*T*可以表示如下：

![](img/B15619_03_005.png)

每个操作的摊销复杂度是总时间除以操作数，结果为*O(n)*：

![](img/B15619_03_006.png)

然而，如果我们可以保证昂贵操作的次数与常数时间操作的次数相比相差很大，我们将实现更低的摊销运行成本。例如，如果我们可以保证昂贵操作仅在序列*T(n) + T(1) + T(1) + ...*中发生一次，那么摊销运行时间为*O(1)*。因此，根据昂贵操作的频率，摊销运行时间会发生变化。

现在，回到`std::vector`。C++标准规定`push_back()`需要在摊销常数时间内运行，*O(1)*。库供应商是如何实现这一点的呢？如果每次向量变满时容量增加固定数量的元素，我们将会有一个类似于前面的情况，其中运行时间为*O(n)*。即使使用一个大常数，容量变化仍然会以固定间隔发生。关键的见解是向量需要呈指数增长，以便使昂贵的操作发生得足够少。在内部，向量使用增长因子，使得新数组的容量是当前大小乘以增长因子。

一个大的增长因子可能会浪费更多的内存，但会使昂贵的操作发生得更少。为了简化数学计算，让我们使用一个常见的策略，即每次向量需要增长时都加倍容量。现在我们可以估计昂贵调用发生的频率。对于大小为*n*的向量，我们需要增长内部数组*log*[2]*(n)*次，因为我们一直在加倍大小。每次增长数组时，我们需要移动当前数组中的所有元素。当我们增长数组的第*i*次时，将有*2*^i 个元素需要移动。因此，如果我们执行*m*次`push_back()`操作，增长操作的总运行时间将是：

![](img/B15619_03_007.png)

这是一个等比数列，也可以表示为：

![](img/B15619_03_008.png)

将这个除以序列的长度*m*，我们最终得到摊销运行时间*O(1)*。

正如我已经说过的，摊销时间复杂度在标准库中被广泛使用，因此了解这种分析是很有帮助的。思考`push_back()`如何在摊销常数时间内实现已经帮助我记住了摊销常数时间的简化版本：它几乎在所有情况下都是*O(1)*，只有极少数情况下会表现得更差。

这就是我们将要涵盖的关于渐近复杂度的全部内容。现在我们将继续讨论如何解决性能问题，并通过优化代码来有效地工作。

# 要测量什么以及如何测量？

优化几乎总是会给你的代码增加复杂性。高级优化，比如选择算法和数据结构，可以使代码的意图更清晰，但在大多数情况下，优化会使代码更难阅读和维护。因此，我们要确信我们添加的优化对我们在性能方面试图实现的目标有实际影响。我们真的需要让代码更快吗？以何种方式？代码真的使用了太多内存吗？为了了解可能的优化，我们需要对要求有一个很好的理解，比如延迟、吞吐量和内存使用。

优化代码是有趣的，但也很容易在没有可衡量的收益的情况下迷失方向。我们将从建议的工作流程开始，以便在调整代码时进行优化：

1.  **定义一个目标**：如果有一个明确定义的定量目标，那么知道如何优化以及何时停止优化会更容易。对于一些应用程序，从一开始就很明确要求是什么，但在许多情况下，要求往往更加模糊。即使代码运行太慢可能是显而易见的，但知道什么是足够好是很重要的。每个领域都有自己的限制，所以确保你了解与你的应用程序相关的限制。以下是一些例子，以使其更具体：

+   用户交互应用的响应时间为 100 毫秒；参考[`www.nngroup.com/articles/response-times-3-important-limits`](https://www.nngroup.com/articles/response-times-3-important-limits)。

+   每秒 60 帧（FPS）的图形给你每帧 16 毫秒。

+   以 44.1 kHz 的采样率和 128 个样本缓冲的实时音频意味着略低于 3 毫秒。

1.  **测量**：一旦我们知道要测量什么和限制是什么，我们就可以通过测量应用程序当前的性能来继续。从*步骤 1*开始，如果我们对平均时间、峰值、负载等感兴趣，那么很明显。在这一步中，我们只关心测量我们设定的目标。根据应用程序的不同，测量可以是从使用秒表到使用高度复杂的性能分析工具的任何事情。

1.  **找出瓶颈**：接下来，我们需要找出应用程序的瓶颈——那些太慢的部分，使应用程序变得无用。此时不要相信你的直觉！也许在*步骤 2*的不同点测量代码时你获得了一些见解——这很好，但通常你需要进一步对代码进行分析，以找到最重要的热点。

1.  **做出合理猜测**：提出一个如何提高性能的假设。可以使用查找表吗？我们可以缓存数据以获得整体吞吐量吗？我们可以改变代码以便编译器可以对其进行矢量化吗？我们可以通过重用内存来减少关键部分的分配次数吗？如果你知道这些只是合理的猜测，提出想法通常并不那么困难。错了也没关系——你以后会发现它们是否产生了影响。

1.  **优化**：让我们实现我们在*步骤 4*中勾画的假设。在知道它是否真的产生效果之前，不要在这一步上花费太多时间使其完美。准备拒绝这种优化。它可能没有预期的效果。

1.  **评估**：再次测量。做与*步骤 2*中完全相同的测试，并比较结果。我们得到了什么？如果我们没有得到任何东西，拒绝这段代码并返回*步骤 4*。如果优化实际上产生了积极的效果，你需要问自己是否值得再花更多时间。这种优化有多复杂？是否值得努力？这是一般性能提升还是高度特定于某种情况/平台？它是否可维护？我们能封装它吗，还是它散布在整个代码库中？如果你无法证明这种优化，返回*步骤 4*，否则继续进行最后一步。

1.  **重构**：如果你遵循了*步骤 5*中的指示，并且在一开始没有花太多时间编写完美的代码，那么现在是时候重构优化以使其更清晰了。优化几乎总是需要一些注释来解释为什么我们以一种不寻常的方式做事情。

遵循这个过程将确保你保持在正确的轨道上，不会最终得到没有动机的复杂优化。花时间定义具体目标和测量的重要性不可低估。为了在这个领域取得成功，你需要了解哪些性能特性对你的应用程序是相关的。

## 性能特性

在开始测量之前，你必须知道对你正在编写的应用程序来说哪些性能特性是重要的。在本节中，我将解释一些在测量性能时经常使用的术语。根据你正在编写的应用程序，有些特性比其他特性更相关。例如，如果你正在编写在线图像转换服务，吞吐量可能比延迟更重要，而在编写具有实时要求的交互式应用程序时，延迟就很关键。以下是一些在性能测量过程中值得熟悉的有价值的术语和概念：

+   **延迟/响应时间**：根据领域的不同，延迟和响应时间可能有非常精确和不同的含义。然而，在本书中，我指的是请求和操作响应之间的时间——例如，图像转换服务处理一个图像所需的时间。

+   **吞吐量**：这指的是每个时间单位处理的交易（操作，请求等）的数量，例如，图像转换服务每秒可以处理的图像数量。

+   **I/O 绑定或 CPU 绑定**：任务通常在 CPU 上计算大部分时间或等待 I/O（硬盘，网络等）。如果 CPU 速度更快，任务通常会更快，就称为 CPU 绑定。如果通过加快 I/O 速度，任务通常会更快，就称为 I/O 绑定。有时你也会听到内存绑定任务，这意味着主内存的数量或速度是当前的瓶颈。

+   **功耗**：这对于在带电池的移动设备上执行的代码来说非常重要。为了减少功耗，应用程序需要更有效地使用硬件，就像我们在优化 CPU 使用率，网络效率等一样。除此之外，应该避免高频率轮询，因为它会阻止 CPU 进入睡眠状态。

+   **数据聚合**：在进行性能测量时，当收集大量样本时通常需要对数据进行聚合。有时*平均值*足以成为程序性能的良好指标，但更常见的是*中位数*，因为它对异常值更具鲁棒性，可以更多地告诉你实际性能。如果你对异常值感兴趣，你可以测量*最小*和*最大*值（或者例如第 10 百分位数）。

这个列表并不是详尽无遗的，但这是一个很好的开始。在这里要记住的重要事情是，在测量性能时，我们可以使用已经确立的术语和概念。花一些时间来定义我们所说的优化代码实际意味着帮助我们更快地达到我们的目标。

## 执行时间的加速

当我们比较程序或函数的两个版本之间的相对性能时，通常习惯谈论**加速**。在这里我将给出一个比较执行时间（或延迟）时的加速定义。假设我们已经测量了某段代码的两个版本的执行时间：一个旧的较慢版本和一个新的较快版本。执行时间的加速可以相应地计算如下：

![](img/B15619_03_009.png)

其中*T*[old]是代码初始版本的执行时间，*T*[new]是优化版本的执行时间。这个加速的定义意味着加速比为 1 表示根本没有加速。

让我们通过一个例子来确保你知道如何测量相对执行时间。假设我们有一个函数，执行时间为 10 毫秒（*T*[old] = 10 毫秒），经过一些优化后我们设法让它在 4 毫秒内运行（*T*[new] = 4 毫秒）。然后我们可以计算加速比如下：

![](img/B15619_03_010.png)

换句话说，我们的新优化版本提供了 2.5 倍的加速。如果我们想将这种改进表示为百分比，我们可以使用以下公式将加速转换为百分比改进：

![](img/B15619_03_011.png)

然后我们可以说新版本的代码比旧版本快 60%，这对应着 2.5 倍的加速。在本书中，当比较执行时间时，我将一贯使用加速，而不是百分比改进。

最终，我们通常对执行时间感兴趣，但时间并不总是最好的衡量标准。通过检查硬件上的其他值，硬件可能会给我们一些其他有用的指导，帮助我们优化我们的代码。

## 性能计数器

除了显而易见的属性，比如执行时间和内存使用，有时候测量其他东西可能会更有益。要么是因为它们更可靠，要么是因为它们可以更好地帮助我们了解导致代码运行缓慢的原因。

许多 CPU 配备了硬件性能计数器，可以为我们提供诸如指令数、CPU 周期、分支错误预测和缓存未命中等指标。我在本书中尚未介绍这些硬件方面，我们也不会深入探讨性能计数器。但是，知道它们的存在以及所有主要操作系统都有现成的工具和库（通过 API 可访问）来收集运行程序时的**性能监视计数器**（**PMC**）是很有好处的。

性能计数器的支持因 CPU 和操作系统而异。英特尔提供了一个强大的工具称为 VTune，可用于监视性能计数器。FreeBSD 提供了`pmcstat`。macOS 自带 DTrace 和 Xcode Instruments。微软 Visual Studio 在 Windows 上提供了收集 CPU 计数器的支持。

另一个流行的工具是`perf`，它在 GNU/Linux 系统上可用。运行命令：

```cpp
perf stat ./your-program 
```

将显示许多有趣的事件，例如上下文切换的次数，页面错误，错误的预测分支等。以下是运行小程序时输出的示例：

```cpp
Performance counter stats for './my-prog':
     1 129,86 msec task-clock               # 1,000 CPUs utilized          
            8      context-switches         # 0,007 K/sec                  
            0      cpu-migrations           # 0,000 K/sec                  
       97 810      page-faults              # 0,087 M/sec                  
3 968 043 041      cycles                   # 3,512 GHz                    
1 250 538 491      stalled-cycles-frontend  # 31,52% frontend cycles idle
  497 225 466      stalled-cycles-backend   # 12,53% backend cycles idle    
6 237 037 204      instructions             # 1,57  insn per cycle         
                                            # 0,20  stalled cycles per insn
1 853 556 742      branches                 # 1640,516 M/sec                  
    3 486 026      branch-misses            # 0,19% of all branches        
  1,130355771 sec  time elapsed
  1,026068000 sec  user
  0,104210000 sec  sys 
```

我们现在将重点介绍一些测试和评估性能的最佳实践。

## 性能测试-最佳实践

由于某种原因，更常见的是回归测试涵盖功能要求，而不是性能要求或其他非功能要求在测试中得到覆盖。性能测试通常更加零星地进行，而且往往太晚了。我的建议是通过将性能测试添加到每晚的构建中，尽早测量并尽快检测到回归。

如果要处理大量输入，则明智地选择算法和数据结构，但不要没有充分理由就对代码进行微调。早期使用真实测试数据测试应用程序也很重要。在项目早期就询问数据大小的问题。应用程序应该处理多少表行并且仍然能够平稳滚动？不要只尝试 100 个元素并希望您的代码能够扩展-进行测试！

绘制数据是了解收集到的数据的一种非常有效的方式。今天有很多好用的绘图工具，所以没有理由不绘图。RStudio 和 Octave 都提供强大的绘图功能。其他例子包括 gnuplot 和 Matplotlib（Python），它们可以在各种平台上使用，并且在收集数据后需要最少的脚本编写来生成有用的图表。图表不一定要看起来漂亮才有用。一旦绘制了数据，您将能够看到通常在充满数字的表中很难找到的异常值和模式。

这结束了我们的*要测量和如何测量*？部分。接下来，我们将探索找到代码中浪费太多资源的关键部分的方法。

# 了解您的代码和热点

帕累托原则，或 80/20 法则，自 100 多年前意大利经济学家维尔弗雷多·帕累托首次观察到以来，已经在各个领域得到应用。他能够证明意大利人口的 20%拥有 80%的土地。在计算机科学中，它已被广泛使用，甚至可能被过度使用。在软件优化中，它表明代码的 20%负责程序使用的 80%资源。

当然，这只是一个经验法则，不应该被过于字面理解。尽管如此，对于尚未优化的代码，通常会发现一些相对较小的热点，它们消耗了绝大部分的资源。作为程序员，这实际上是个好消息，因为这意味着我们可以大部分时间编写代码而不需要为了性能而对其进行调整，而是专注于保持代码的清晰。这也意味着在进行优化时，我们需要知道*在哪里*进行优化；否则，我们很可能会优化对整体性能没有影响的代码。在本节中，我们将探讨寻找可能值得优化的代码中的 20%的方法和工具。

使用性能分析器通常是识别程序中热点的最有效方法。性能分析器分析程序的执行并输出函数或指令被调用的统计摘要，即性能分析结果。

此外，性能分析器通常还会输出一个调用图，显示函数调用之间的关系，即每个在分析期间被调用的函数的调用者和被调用者。在下图中，您可以看到`sort()`函数是从`main()`（调用者）调用的，而`sort()`又调用了`swap()`函数（被调用者）：

![](img/New_B15619_03_04.png)

图 3.4：调用图的示例。函数`sort()`被调用一次，并调用`swap()` 50 次。

性能分析器主要分为两类：采样性能分析器和插装性能分析器。这两种方法也可以混合使用，创建采样和插装的混合性能分析器。Unix 性能分析工具`gprof`就是一个例子。接下来的部分将重点介绍插装性能分析器和采样性能分析器。

## 插装性能分析器

通过插装，我指的是向程序中插入代码以便分析，以收集关于每个函数被执行频率的信息。通常，插入的插装代码记录每个入口和出口点。您可以通过手动插入代码来编写自己的原始插装性能分析器，或者您可以使用一个工具，在构建过程中自动插入必要的代码。

一个简单的实现可能对您的目的足够了，但要注意添加的代码对性能的影响，这可能会使性能分析结果产生误导。像这样的天真实现的另一个问题是，它可能会阻止编译器优化或者有被优化掉的风险。

仅仅举一个插装性能分析器的例子，这里是一个我在以前项目中使用过的计时器类的简化版本：

```cpp
class ScopedTimer { 
public: 
  using ClockType = std::chrono::steady_clock;
  ScopedTimer(const char* func) 
      : function_name_{func}, start_{ClockType::now()} {}
  ScopedTimer(const ScopedTimer&) = delete; 
  ScopedTimer(ScopedTimer&&) = delete; 
  auto operator=(const ScopedTimer&) -> ScopedTimer& = delete; 
  auto operator=(ScopedTimer&&) -> ScopedTimer& = delete;
  ~ScopedTimer() {
    using namespace std::chrono;
    auto stop = ClockType::now(); 
    auto duration = (stop - start_); 
    auto ms = duration_cast<milliseconds>(duration).count(); 
    std::cout << ms << " ms " << function_name_ << '\n'; 
  } 

private: 
  const char* function_name_{}; 
  const ClockType::time_point start_{}; 
}; 
```

`ScopedTimer`类将测量从创建到超出作用域（即析构）的时间。我们使用自 C++11 以来可用的`std::chrono::steady_clock`类，它专门用于测量时间间隔。`steady_clock`是单调的，这意味着在两次连续调用`clock_type::now()`之间它永远不会减少。这对于系统时钟来说并非如此，例如，系统时钟可以随时调整。

我们现在可以通过在每个函数的开头创建一个`ScopedTimer`实例来使用我们的计时器类：

```cpp
auto some_function() {
  ScopedTimer timer{"some_function"};
  // ...
} 
```

尽管我们通常不建议使用预处理宏，但这可能是使用预处理宏的一个案例：

```cpp
#if USE_TIMER 
#define MEASURE_FUNCTION() ScopedTimer timer{__func__} 
#else 
#define MEASURE_FUNCTION() 
#endif 
```

我们使用自 C++11 以来可用的唯一预定义的函数局部`__func__`变量来获取函数的名称。C++20 还引入了方便的`std::source_location`类，它为我们提供了`function_name()`、`file_name()`、`line()`和`column()`等函数。如果您的编译器尚不支持`std::source_location`，还有其他非标准的预定义宏被广泛支持，对于调试目的非常有用，例如`__FUNCTION__`、`__FILE__`和`__LINE__`。

现在，我们的`ScopedTimer`类可以像这样使用：

```cpp
auto some_function() { 
  MEASURE_FUNCTION(); 
  // ...
} 
```

假设我们在编译计时器时定义了`USE_TIMER`，那么每次`some_function()`返回时，它将产生以下输出：

```cpp
2.3 ms some_function 
```

我已经演示了如何通过在代码中插入打印两个代码点之间经过的时间的代码来手动检测我们的代码。虽然这对于某些情况来说是一个方便的工具，请注意这样一个简单工具可能产生误导性的结果。在下一节中，我将介绍一种不需要对执行代码进行任何修改的性能分析方法。

## 采样分析器

采样分析器通过在均匀间隔（通常为每 10 毫秒）查看运行程序的状态来创建概要。采样分析器通常对程序的实际性能影响很小，并且还可以在启用所有优化的发布模式下构建程序。采样分析器的缺点是它们的不准确性和统计方法，通常只要你意识到这一点，这通常不是问题。

下图显示了一个运行程序的采样会话，其中包含五个函数：`main()`、`f1()`、`f2()`、`f3()`和`f4()`。标签**t**[1] - **t**[10]表示每个样本的取样时间。方框表示每个执行函数的入口和出口点：

![](img/B15619_03_05.png)

图 3.5：采样分析器会话的示例

概要显示在下表中：

| 函数 | 总数 | 自身 |
| --- | --- | --- |
| `main()` | 100% | 10% |
| `f1()` | 80% | 10% |
| `f2()` | 70% | 30% |
| `f3()` | 50% | 50% |

表 3.3：对于每个函数，概要显示了它出现在调用堆栈中的总百分比（Total）以及它出现在堆栈顶部的百分比（Self）。

前表中的**Total**列显示了包含某个函数的调用堆栈的百分比。在我们的示例中，主函数在 10 个调用堆栈中都出现（100%），而`f2()`函数只在 7 个调用堆栈中被检测到，占所有调用堆栈的 70%。

**Self**列显示了每个函数在调用堆栈顶部出现的次数。`main()`函数在第五个样本**t**[5]中被检测到在调用堆栈顶部出现一次，而`f2()`函数在样本**t**[6]、**t**[8]和**t**[9]中出现在调用堆栈顶部，对应 3/10 = 30%。

`f3()`函数具有最高的**Self**值（5/10），每当检测到它时，它都位于调用堆栈的顶部。

在概念上，采样分析器以均匀的时间间隔存储调用堆栈的样本。它检测当前在 CPU 上运行的内容。纯采样分析器通常只检测当前在运行状态的线程中执行的函数，因为休眠线程不会被调度到 CPU 上。这意味着如果一个函数正在等待导致线程休眠的锁，那么这段时间不会显示在时间概要中。这很重要，因为您的瓶颈可能是由线程同步引起的，这可能对采样分析器是不可见的。

`f4()`函数发生了什么？根据图表，它在样本二和三之间被`f2()`函数调用，但它从未出现在我们的统计概要中，因为它从未在任何调用堆栈中注册过。这是采样分析器的一个重要特性。如果每个样本之间的时间太长或总采样会话时间太短，那么短且不经常调用的函数将不会出现在概要中。这通常不是问题，因为这些函数很少是您需要调整的函数。您可能注意到`f3()`函数也在**t**[5]和**t**[6]之间被错过了，但由于`f3()`被频繁调用，它对概要产生了很大的影响。

确保您了解您的时间分析器实际上记录了什么。要充分利用它，要意识到它的局限性和优势。

# 微基准测试

分析可以帮助我们找到代码中的瓶颈。如果这些瓶颈是由低效的数据结构（见*第四章*，*数据结构*）、算法选择错误（见*第五章*，*算法*）或不必要的争用（见*第十一章*，*并发*）引起的，那么应该首先解决这些更大的问题。但有时我们会发现需要优化的小函数或小代码块，在这种情况下，我们可以使用一种称为**微基准测试**的方法。通过这个过程，我们创建一个微基准测试——一个在程序的其余部分中孤立运行小代码片段的程序。微基准测试的过程包括以下步骤：

1.  找到需要调整的热点，最好使用分析器。

1.  将其与其余代码分离并创建一个孤立的微基准测试。

1.  优化微基准测试。使用基准测试框架在优化过程中测试和评估代码。

1.  将新优化的代码集成到程序中，然后*重新测量*，看看当代码在更大的上下文中运行时，优化是否相关。

该过程的四个步骤如下图所示：

![](img/B15619_03_06.png)

图 3.6：微基准测试过程

微基准测试很有趣。然而，在着手尝试加快特定函数之前，我们应该首先确保：

+   运行程序时在函数内部花费的时间显着影响我们想要加速的程序的整体性能。分析和阿姆达尔定律将帮助我们理解这一点。下面将解释阿姆达尔定律。

+   我们无法轻易减少函数被调用的次数。消除对昂贵函数的调用通常是优化程序整体性能最有效的方法。

使用微基准测试来优化代码通常应该被视为最后的手段。预期的整体性能提升通常很小。然而，有时我们无法避免需要通过调整实现来加快相对较小的代码片段的运行速度，而在这些情况下，微基准测试可以非常有效。

接下来，您将了解微基准测试的加速比如何影响程序的整体加速比。

## 阿姆达尔定律

在使用微基准测试时，要牢记孤立代码的优化对整个程序的影响有多大（或多小）是至关重要的。我们的经验是，有时在改进微基准测试时很容易有点过于兴奋，只是意识到整体效果几乎可以忽略不计。使用健全的分析技术部分地解决了这种无法前进的风险，同时也要牢记优化的整体影响。

假设我们正在优化程序中的一个孤立部分的微基准测试。然后可以使用阿姆达尔定律计算整个程序的整体加速比的上限。为了计算整体加速比，我们需要知道两个值：

+   首先，我们需要知道孤立部分的执行时间在整体执行时间中所占的比例。我们用字母*p*来表示这个*比例执行*时间的值。

+   其次，我们需要知道我们正在优化的部分的加速比——即微基准测试的。我们用字母*s*来表示这个*本地加速比*的值。

使用*p*和*s*，我们现在可以使用阿姆达尔定律来计算整体加速比：

![](img/B15619_03_012.png)

希望这看起来不会太复杂，因为当投入使用时，这是非常直观的。为了直观理解阿姆达尔定律，可以看看在使用各种极端*p*和*s*值时整体加速比会变成什么样：

+   设置*p = 0*和*s = 5x*意味着我们优化的部分对整体执行时间没有影响。因此，无论*s*的值如何，整体加速比始终为 1x。

+   设置*p = 1*，*s = 5x*意味着我们优化了整个程序执行时间的一部分，在这种情况下，整体加速将始终等于我们在优化部分所实现的加速——在这种情况下是 5 倍。

+   设置*p = 0.5*和*s = ∞*意味着我们完全删除了程序执行时间的一半。整体加速将是 2 倍。

结果总结在下表中：

| p | s | 整体加速 |
| --- | --- | --- |
| 0 | 5x | 1x |
| 1 | 5x | 5x |
| 0.5 | ∞ | 2x |

表 3.4：p 和 s 的极端值及实现的整体加速

一个完整的例子将演示我们如何在实践中使用阿姆达尔定律。假设你正在优化一个函数，使得优化版本比原始版本快 2 倍，即*2x (s = 2)*的加速。此外，让我们假设这个函数只占程序整体执行时间的 1%（*p = 0.01*），那么整个程序的整体加速可以计算如下：

![](img/B15619_03_013.png)

因此，即使我们设法使我们的孤立代码快 2 倍，整体加速只有 1.005 倍的因素——并不是说这种加速必然是可以忽略的，但我们不断需要回过头来看我们的收益与整体情况的比例。

## 微基准测试的陷阱

在一般情况下测量软件性能和特别是微基准测试时，有很多隐藏的困难。在这里，我将列出在处理微基准测试时需要注意的事项：

+   有时结果被过度概括，并被视为普遍真理。

+   编译器可能会以不同于在完整程序中优化的方式来优化孤立的代码。例如，在微基准测试中可能会内联一个函数，但在完整程序中编译时可能不会内联。或者，编译器可能能够预先计算微基准测试的部分。

+   在基准测试中未使用的返回值可能会使编译器删除我们试图测量的函数。

+   在微基准测试中提供的静态测试数据可能会使编译器在优化代码时获得不切实际的优势。例如，如果我们硬编码循环将执行的次数，并且编译器知道这个硬编码的值恰好是 8 的倍数，它可以以不同的方式对循环进行矢量化，跳过可能与 SIMD 寄存器大小不对齐的部分的序言和尾声。然后在真实代码中，这个硬编码的编译时常量被替换为运行时值，这种优化就不会发生。

+   不切实际的测试数据可能会影响运行基准测试时的分支预测。

+   多次测量之间的结果可能会有所不同，因为频率缩放、缓存污染和其他进程的调度等因素。

+   代码性能的限制因素可能是缓存未命中，而不是实际执行指令所需的时间。因此，在许多情况下，微基准测试的一个重要规则是，在测量之前必须清除缓存，否则你实际上并没有在测量任何东西。

我希望有一个简单的公式来避免上面列出的所有陷阱，但不幸的是，我没有。然而，在下一节中，我们将通过使用微基准测试支持库来看一个具体的例子，看看如何通过使用微基准测试支持库来解决其中一些陷阱。

## 一个微基准测试的例子

我们将通过回到本章的线性搜索和二分搜索的初始例子，并演示如何使用基准测试框架对它们进行基准测试来结束这一章。

我们开始这一章节，比较了在`std::vector`中搜索整数的两种方法。如果我们知道向量已经排序，我们可以使用二分搜索，这比简单的线性搜索算法效果更好。我不会在这里重复函数的定义，但声明看起来是这样的：

```cpp
bool linear_search(const std::vector<int>& v, int key);
bool binary_search(const std::vector<int>& v, int key); 
```

一旦输入足够大，这些函数的执行时间差异是非常明显的，但它将作为我们目的的一个足够好的例子。我们将首先只测量`linear_search()`。然后，当我们有一个可用的基准测试时，我们将添加`binary_search()`并比较这两个版本。

为了制作一个测试程序，我们首先需要一种方法来生成一个排序的整数向量。以下是一个简单的实现，对我们的需求来说足够了：

```cpp
auto gen_vec(int n) {
  std::vector<int> v;
  for (int i = 0; i < n; ++i) { 
    v.push_back(i); 
  }
  return v;
} 
```

返回的向量将包含 0 到*n-1*之间的所有整数。一旦我们有了这个，我们就可以创建一个像这样的简单测试程序：

```cpp
int main() { // Don't do performance tests like this!
  ScopedTimer timer("linear_search");
  int n = 1024;
  auto v = gen_vec(n);
  linear_search(v, n);
} 
```

我们正在搜索值`n`，我们知道它不在向量中，所以算法将展示其在这个测试数据中的最坏情况性能。这是这个测试的好部分。除此之外，它还有许多缺陷，这将使得这个基准测试无用：

+   使用优化编译这段代码很可能会完全删除代码，因为编译器可以看到函数的结果没有被使用。

+   我们不想测量创建和填充`std::vector`所需的时间。

+   只运行一次`linear_search()`函数，我们将无法获得统计上稳定的结果。

+   测试不同的输入大小是很麻烦的。

让我们看看如何通过使用微基准支持库来解决这些问题。有各种各样的用于基准测试的工具/库，但我们将使用**Google Benchmark**，[`github.com/google/benchmark`](https://github.com/google/benchmark)，因为它被广泛使用，而且作为一个奖励，它也可以在[`quick-bench.com`](http://quick-bench.com)页面上轻松在线测试，而无需任何安装。

这是使用 Google Benchmark 时`linear_search()`的一个简单微基准测试的样子：

```cpp
#include <benchmark/benchmark.h> // Non-standard header
#include <vector>
bool linear_search(const std::vector<int>& v, int key) { /* ... */ }
auto gen_vec(int n) { /* ... */ }
static void bm_linear_search(benchmark::State& state) {
  auto n = 1024;
  auto v = gen_vec(n);
  for (auto _ : state) {
    benchmark::DoNotOptimize(linear_search(v, n));
  }
}
BENCHMARK(bm_linear_search); // Register benchmarking function
BENCHMARK_MAIN(); 
```

就是这样！我们还没有解决的唯一问题是输入大小被硬编码为 1024。我们稍后会解决这个问题。编译和运行这个程序将生成类似这样的东西：

```cpp
-------------------------------------------------------------------
Benchmark                Time   CPU           Iterations
-------------------------------------------------------------------
bm_linear_search         361 ns 361 ns        1945664 
```

右侧列中报告的迭代次数报告了循环需要执行多少次才能获得统计上稳定的结果。传递给我们基准测试函数的`state`对象确定了何时停止。每次迭代的平均时间在两列中报告：**时间**是挂钟时间，**CPU**是主线程在 CPU 上花费的时间。在这种情况下，它们是相同的，但如果`linear_search()`被阻塞等待 I/O（例如），CPU 时间将低于挂钟时间。

另一个重要的事情要注意的是生成向量的代码不包括在报告的时间内。唯一被测量的代码是这个循环内的代码：

```cpp
for (auto _ : state) {   // Only this loop is measured
  benchmark::DoNotOptimize(binary_search(v, n));
} 
```

从我们的搜索函数返回的布尔值被包裹在`benchmark::DoNotOptimize()`中。这是用来确保返回值不被优化掉的机制，这可能会使对`linear_search()`的整个调用消失。

现在让我们通过改变输入大小使这个基准测试更有趣。我们可以通过使用`state`对象向我们的基准测试函数传递参数来做到这一点。以下是如何做到的：

```cpp
static void bm_linear_search(benchmark::State& state) {
  auto n = state.range(0);
  auto v = gen_vec(n);
  for (auto _ : state) {
    benchmark::DoNotOptimize(linear_search(v, n));
  }
}
BENCHMARK(bm_linear_search)->RangeMultiplier(2)->Range(64, 256); 
```

这将从输入大小为 64 开始，每次加倍大小，直到达到 256。在我的机器上，测试生成了以下输出：

```cpp
-------------------------------------------------------------------
Benchmark                Time    CPU          Iterations
-------------------------------------------------------------------
bm_linear_search/64      17.9 ns 17.9 ns      38143169
bm_linear_search/128     44.3 ns 44.2 ns      15521161
bm_linear_search/256     74.8 ns 74.7 ns      8836955 
```

最后，我们将使用可变输入大小对`linear_search()`和`binary_search()`函数进行基准测试，并尝试让框架估计我们函数的时间复杂度。这可以通过使用`SetComplexityN()`函数向`state`对象提供输入大小来实现。完整的微基准测试示例如下：

```cpp
#include <benchmark/benchmark.h>
#include <vector>
bool linear_search(const std::vector<int>& v, int key) { /* ... */ }
bool binary_search(const std::vector<int>& v, int key) { /* ... */ }
auto gen_vec(int n) { /* ... */ }
static void bm_linear_search(benchmark::State& state) {
  auto n = state.range(0); 
  auto v = gen_vec(n);
  for (auto _ : state) { 
    benchmark::DoNotOptimize(linear_search(v, n)); 
  }
  state.SetComplexityN(n);
}
static void bm_binary_search(benchmark::State& state) {
  auto n = state.range(0); 
  auto v = gen_vec(n);
  for (auto _ : state) { 
    benchmark::DoNotOptimize(binary_search(v, n)); 
  }
  state.SetComplexityN(n);
}
BENCHMARK(bm_linear_search)->RangeMultiplier(2)->
  Range(64, 4096)->Complexity();
BENCHMARK(bm_binary_search)->RangeMultiplier(2)->
  Range(64, 4096)->Complexity();
BENCHMARK_MAIN(); 
```

运行基准测试时，将在控制台上打印以下结果：

```cpp
-------------------------------------------------------------------
Benchmark                Time     CPU         Iterations
-------------------------------------------------------------------
bm_linear_search/64      18.0 ns  18.0 ns     38984922
bm_linear_search/128     45.8 ns  45.8 ns     15383123
...
bm_linear_search/8192    1988 ns  1982 ns     331870
bm_linear_search_BigO    0.24 N   0.24 N
bm_linear_search_RMS        4 %   4 %
bm_binary_search/64      4.16 ns  4.15 ns     169294398
bm_binary_search/128     4.52 ns  4.52 ns     152284319
...
bm_binary_search/4096    8.27 ns  8.26 ns     80634189
bm_binary_search/8192    8.90 ns  8.90 ns     77544824
bm_binary_search_BigO    0.67 lgN 0.67 lgN
bm_binary_search_RMS        3 %   3 % 
```

图 3.7：绘制不同输入大小的执行时间，显示了搜索函数的增长率

输出与本章初步结果一致，我们得出结论，这些算法分别表现出线性运行时间和对数运行时间。如果我们将数值绘制在表中，我们可以清楚地看到函数的线性和对数增长率。

输出结果如下：

总结

以下图是使用 Python 和 Matplotlib 生成的：

> “测量让您领先于不需要测量的专家。”
> 
> 您现在拥有了许多工具和见解，可以找到并改进代码的性能。在处理性能时，我再次强调测量和设定目标的重要性。Andrei Alexandrescu 的一句话将结束本节：

# 在本章中，您学会了如何使用大 O 符号比较算法的效率。您现在知道 C++标准库为算法和数据结构提供了复杂性保证。所有标准库算法都指定它们的最坏情况或平均情况性能保证，而容器和迭代器指定摊销或精确复杂度。

-Andrei Alexandrescu，2015 年，编写快速代码 I，code::dive conference 2015，https://codedive.pl/2015/writing-fast-code-part-1。

您还了解了如何通过测量延迟和吞吐量来量化软件性能。

最后，您学会了如何使用 CPU 分析器检测代码中的热点，并如何执行微基准测试来改进程序的孤立部分。

在下一章中，您将了解如何有效使用 C++标准库提供的数据结构。
