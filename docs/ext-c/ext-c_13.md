# 第十三章

# 并发

在接下来的两章中，我们将讨论**并发**以及开发并发程序所需的理论背景，这不仅适用于 C 语言，也必然适用于其他语言。因此，这两章将不包含任何 C 代码，而是使用伪代码来表示并发系统和它们的内在属性。

由于并发主题的长度，它已经被分为两个章节。在本章中，我们将探讨关于并发的核心基本概念，然后转向第十四章，**同步**，我们将讨论与并发相关的问题以及并发程序中用于解决这些问题的**同步**机制。这两个章节的总体目标是为你提供足够的理论知识，以便在后续章节中继续讨论多线程和多进程主题。

本章建立的知识背景在处理我们全书使用的**POSIX 线程库**时也将非常有用。

在本章关于并发的第一部分，我们将致力于理解：

+   并行系统与并发系统的区别

+   当我们需要并发时

+   **任务调度器**是什么，以及广泛使用的调度算法有哪些

+   并发程序是如何运行的以及什么是交错

+   共享状态是什么以及各种任务如何访问它

让我们从对并发概念的介绍开始，广泛地了解它对我们意味着什么。

# 介绍并发

并发简单来说就是程序中有多个逻辑部分同时执行。现代软件系统通常是并发的，因为程序需要同时运行多个逻辑部分。因此，并发是今天每个程序都在一定程度上使用的。

我们可以说，并发是一种强大的工具，它允许你编写能够同时管理不同任务的程序，并且对它的支持通常位于内核中，这是操作系统的核心。

有许多例子表明，一个普通程序可以同时管理多个任务。例如，你可以在下载文件的同时上网冲浪。在这种情况下，任务是在浏览器进程的上下文中并发执行的。另一个值得注意的例子是在**视频流**场景中，比如你在 YouTube 上观看视频时。视频播放器可能正在下载视频的后续片段，而你仍在观看之前下载的片段。

即使是简单的文字处理软件也有几个并发任务在后台运行。当我在这本 Microsoft Word 上写这一章时，拼写检查器和格式化器正在后台运行。如果你在 iPad 上的 Kindle 应用程序上阅读这本书，你认为作为 Kindle 程序的一部分，可能正在并发运行哪些程序？

同时运行多个程序听起来很神奇，但就像大多数技术一样，并发除了带来好处外，还会带来一些头痛的问题。确实，并发给计算机科学历史带来了最痛苦的头痛问题！这些“头痛”问题，我们将在后面讨论，它们可能长时间隐藏，甚至在发布后数月，而且通常很难找到、重现和解决。

我们在本节开始时将并发描述为同时执行任务，或者说是并发执行。这种描述意味着任务是在并行运行，但这并不完全正确。这样的描述过于简单，也不准确，因为*并发不同于并行*，我们还没有解释这两者之间的区别。两个并发程序与两个并行程序不同，我们本章的一个目标就是阐明这些区别，并给出该领域官方文献中使用的某些定义。

在接下来的章节中，我们将解释一些基本的并发相关概念，例如*任务*、*调度*、*交错*、*状态*和*共享状态*，这些是在这本书中你将经常遇到的术语。值得注意的是，这些概念大多是抽象的，可以应用于任何并发系统，而不仅仅是 C 语言。

为了理解并行和并发之间的区别，我们将简要介绍并行系统。

注意，在本章中，我们坚持简单的定义。我们的唯一目的是给你一个并发系统如何工作的基本概念，因为超出这个范围就不在本书的 C 语言范畴之内了。

# 并行

并行简单来说就是同时运行两个任务，或者说是*并行运行*。短语“并行运行”是区分并行和并发的关键元素。为什么是这样呢？因为并行意味着两件事情同时发生。在并发系统中并不是这样；在并发系统中，你需要暂停一个任务以便让另一个任务继续执行。请注意，这个定义可能过于简单且不完整，特别是在现代并发系统中，但它足以让我们对基本概念有一个基本的了解。

我们在日常生活中经常遇到并行。当你和你的朋友同时进行两个不同的任务时，这些任务就是在并行进行。为了使多个任务并行，我们需要独立的、隔离的*处理单元*，每个单元被分配给特定的任务。例如，在计算机系统中，每个*CPU 核心*都是一个处理单元，一次可以处理一个任务。

暂时将你自己视为这本书的唯一读者。你不能并行阅读两本书；你不得不暂停阅读其中一本以便阅读另一本。然而，如果你让你的朋友加入进来，那么两本书就可以并行阅读了。

如果你有第三本书需要阅读会发生什么？既然你们两个人都不能同时阅读两本书，那么其中一个人在阅读自己的书时就需要暂停，以便继续阅读第三本书。这仅仅意味着你们中的任何一个人或者你的朋友都需要合理分配时间，以便阅读这三本书。

在计算机系统中，必须至少有两个独立且分离的处理单元，才能在该系统上执行两个并行任务。现代 CPU 内部有多个**核心**，这些核心是实际的处理单元。例如，一个 4 核心 CPU 有 4 个处理单元，因此可以同时支持 4 个并行任务运行。为了简化，在本章中，我们将假设我们的想象中的 CPU 内部只有一个核心，因此不能执行并行任务。在相关章节中，我们将在稍后讨论多核 CPU。

假设你得到两台装有我们想象中的 CPU 的笔记本电脑，一台播放音乐，另一台求解微分方程。它们都在并行工作，但如果你想在同一台笔记本电脑上使用单个 CPU 和单个核心同时完成这两项任务，那么这**不可能**是并行的，实际上它是并发的。

并行化是指可以并行化的任务。这意味着实际的算法可以被分割并在多个处理器单元上运行。但截至目前，我们编写的算法大多数都是**顺序的**，而不是并行的。即使在多线程中，每个线程也有一定数量的顺序指令，这些指令不能被分解成一些并行的**执行流程**。

换句话说，顺序算法不能被操作系统自动轻易地分解成一些并行执行流程，这需要程序员来完成。因此，尽管拥有多核 CPU，你仍然需要将每个执行流程分配给特定的 CPU 核心，并且在该核心中，如果你分配了多个流程，你不能让它们同时并行运行，你将立即观察到并发行为。

简而言之，当然，将两个流程分别分配给不同的核心，可以最终实现两个并行流程，但将它们分配给同一个核心，就会导致两个并发流程。在多核 CPU 中，我们实际上观察到的是一种混合行为，既有核心间的并行性，也有同一核心上的并发性。

尽管并行化具有简单的意义和无数的日常例子，但在计算机体系结构中，它是一个复杂且困难的话题。实际上，它是一个与并发性分开的独立学术领域，拥有自己的理论、书籍和文献。能够拥有一个可以将顺序算法分解成一些并行执行流程的操作系统是一个开放的研究领域，而当前的操作系统无法做到这一点。

正如所述，本章的目的不是深入探讨并行性，而是只为这个概念提供一个初步的定义。由于关于并行性的进一步讨论超出了本书的范围，让我们从并发概念开始。

首先，我们将讨论并发系统以及它与并行性的真正含义。

# 并发

你可能听说过**多任务处理**——好吧，并发有同样的理念。如果你的系统正在同时管理多个任务，你需要理解这并不一定意味着任务正在并行运行。相反，中间可能有一个**任务调度器**；它只是非常快速地在不同的任务之间切换，并在相对较短的时间内执行每个任务的一小部分。

当你只有一个处理器单元时，这当然会发生。在本节接下来的讨论中，我们假设我们正在仅对一个处理器单元进行操作。

如果任务调度器足够**快**和**公平**，你不会注意到任务之间的**切换**，它们在你看来就像是并行运行的。这就是并发的魔法，也是它被广泛应用于大多数知名操作系统（包括 Linux、macOS 和 Microsoft Windows）中的根本原因。

并发可以看作是使用单个处理器单元模拟并行执行任务。实际上，整个想法可以被称为一种形式的人工并行性。对于只有单个 CPU、只有一个核心的旧系统来说，人们能够以多任务的方式使用那个单一核心是一项巨大的进步。

作为旁注，*Multics* 是最早设计为多任务处理和同时管理进程的操作系统之一。你可能会记得，在*第十章*，*Unix – 历史 和 架构*中，Unix 是基于从 Multics 项目中获得的想法构建的。

正如我们之前解释的，几乎所有的操作系统都可以通过多任务执行并发任务，尤其是符合 POSIX 标准的操作系统，因为这种能力在 POSIX 标准中得到了明确的体现。

# 任务调度器单元

正如我们之前所说的，所有多任务操作系统都需要在其内核中有一个**任务调度器**单元，或者简单地称为**调度器单元**。在本节中，我们将看到这个单元是如何工作的，以及它是如何有助于一些并发任务的无缝执行的。

关于任务调度器单元的一些事实如下：

+   调度器有一个用于等待执行的任务**队列**。**任务**或**作业**仅仅是应该在不同执行流中执行的工作片段。

+   这个队列通常是**优先级**排序的，高优先级任务会被优先选择开始执行。

+   处理器单元由任务调度器管理和共享。当处理器单元空闲（没有任务在使用它）时，任务调度器必须在让任务使用处理器单元之前，从其队列中选择另一个任务。当任务完成时，它释放处理器单元并使其再次可用，然后任务调度器选择另一个任务。这个过程持续进行。这被称为*任务调度*，这是任务调度器唯一的责任。

+   有许多*调度算法*供任务调度器操作，但它们都应该满足特定的要求。例如，所有算法都应该*公平*，并且没有任何任务因为长时间未被选中而在队列中*饥饿*。

+   根据选择的*调度策略*，调度器应该为任务分配特定的*时间片*或*时间量子*以使用处理器单元，或者调度器必须等待任务释放处理器单元。

+   如果调度策略是*抢占式*的，调度器应该能够强制从正在运行的任务中收回 CPU 核心，以便将其分配给下一个任务。这被称为*抢占式调度*。还有一种方案是任务自愿释放 CPU，这被称为*协作式调度*。

+   抢占式调度算法试图在不同任务之间公平地平均分配*时间片*。优先级较高的任务可能会被更频繁地选中，或者根据调度器的实现，它们甚至可能获得更长的时间片。

任务是一个通用的抽象概念，用来指代在并发系统中应该完成的工作，不一定是计算机系统中的工作。我们很快就会看看这些非计算机系统究竟是什么。同样，CPU 也不是唯一可以共享给任务的资源。人类在存在以来就一直对任务进行调度和优先级排序，当我们面临无法同时完成的工作时。在接下来的几段中，我们将考虑这种情况作为理解调度的良好例子。

假设我们处于 20 世纪初，街上只有一个电话亭，有 10 个人在等待使用电话。在这种情况下，这 10 个人应该遵循一种调度算法，以便在他们之间公平地共享电话亭。

首先，他们需要排队。在这种情况下，文明心智做出的最基本决定就是排队并等待你的轮次。然而，这还不够；我们还需要一些规则来支持这种方法。目前正在使用电话的第一个人，在还有九个人在等待隔间时，不能像他们可能希望的那样说很多话。第一个人必须在一段时间后离开隔间，以便让队列中的下一个人有机会。

在极少数情况下，如果他们还没有结束对话，第一个人应该在一段时间后停止使用电话，离开隔间，并回到队伍的末尾。然后他们必须等待他们的下一轮，以便继续他们的谈话。这样，10 个人中的每一个人都需要继续进入隔间，直到他们完成他们的对话。

这只是一个例子。我们每天都会遇到多个消费者之间共享资源的例子，人类已经发明了许多方法来在这些资源之间公平地共享——直到人类本性允许的程度！在下一节中，我们将回到考虑计算机系统中的调度。

# 进程和线程

在整本书中，我们主要对计算机系统中的任务调度感兴趣。在操作系统内，任务要么是*进程*，要么是*线程*。我们将在接下来的章节中解释它们及其区别，但就目前而言，你应该知道大多数操作系统基本上以相同的方式处理它们：作为需要并发执行的一些任务。

操作系统需要使用任务调度器来在许多任务之间共享 CPU 核心，这些任务无论是进程还是线程，都愿意使用 CPU 来执行它们的任务。当创建一个新的进程或一个新的线程时，它作为新的任务进入调度队列，并在开始运行之前等待获取 CPU 核心。

在存在*时间共享*或*抢占式调度器*的情况下，如果任务在一段时间内无法完成其逻辑，那么任务调度器将强制收回 CPU 核心，并将任务再次放入队列，就像电话亭场景中一样。

在这种情况下，任务应该在队列中等待，直到再次获得 CPU 核心，然后才能继续运行。如果它无法在第二轮中完成其逻辑，这个过程将继续进行，直到它能够完成。

每次抢占式调度器在运行过程中停止一个进程并将另一个进程放入运行状态时，就会发生一次*上下文切换*。上下文切换越快，用户就会感觉任务似乎是在并行运行。有趣的是，今天的大多数操作系统都使用抢占式调度器，这是我们本章剩余部分的主要焦点。

从现在开始，所有调度器都被假定为抢占式的。在不适用此情况时，我会进行说明。

当一个任务运行时，它可能经历数百甚至数千次上下文切换，才能完成。然而，上下文切换有一个非常奇特和独特的特性——它们是不可预测的。换句话说，我们无法预测上下文切换何时发生，甚至无法预测在哪个指令上发生。即使在同一平台上两个非常接近的连续程序运行中，上下文切换也会有所不同。

这一点的重要性及其影响不容小觑；上下文切换是不可预测的！简而言之，通过给出的例子，你将亲自观察到这一点带来的后果。

上下文切换高度不可预测，到了这种程度，处理这种不确定性的最佳方式是假设在特定指令上发生上下文切换的概率对所有指令都是相同的。换句话说，你应该预期所有指令在任何给定运行中都可能经历上下文切换。简单来说，这意味着你可能在任何两个相邻指令的执行之间有间隔。

话虽如此，我们现在继续前进，看看在并发环境中唯一确定存在的事情。

# 发生之前约束

在上一节中，我们确定了上下文切换是不可预测的；在我们程序中，它们可能发生的时间存在不确定性。尽管如此，同时执行的指令是确定的。

让我们继续用一个简单的例子来说明。首先，我们将基于我们有一个类似于你在*代码框 13-1*中看到的任务，它有五条指令。请注意，这些指令是抽象的，它们不代表任何真实的指令，如 C 或机器指令：

```cpp
Task P {
    1\. num = 5
    2\. num++
    3\. num = num – 2
    4\. x = 10
    5\. num = num + x
}
```

代码框 13-1：一个包含 5 条指令的简单任务

如你所见，指令是有序的，这意味着它们*必须*按照指定的顺序执行，以满足任务的目的是。我们对此是确定的。从技术角度来说，我们说我们在每两个相邻指令之间有一个*发生之前约束*。指令 `num++` 必须在 `num = num - 2` 之前发生，并且这个约束必须得到满足，无论上下文切换如何发生。

注意，我们仍然对上下文切换何时发生存在不确定性；记住这一点很重要，它们可以在指令之间任何地方发生。

在这里，我们将展示前述任务两种可能的执行方式，具有不同的上下文切换：

```cpp
Run 1:
  1\. num = 5
  2\. num++
>>>>> Context Switch <<<<<
  3\. num = num – 2
  4\. x = 10
>>>>> Context Switch <<<<<
  5\. num = num + x
```

代码框 13-2：上述任务与上下文切换的可能运行之一

对于第二次运行，它执行如下：

```cpp
Run 2:
  num = 5
  >> Context Switch <<
  num++
  num = num – 2
  >> Context Switch <<
  x = 10
  >> Context Switch <<
  num = num + x
```

代码框 13-3：另一个与上下文切换一起的可能运行

正如你在*代码框 13-2*中可以看到的，上下文切换的次数和它们发生的位置在每个运行中都可以改变。然而，正如我们之前所说的，有一些应该遵循的发生之前约束。

这就是为什么我们可以对特定任务有一个总体确定性行为的原因。无论上下文切换在不同运行中如何发生，任务的*总体状态*都保持不变。当我们说任务的总体状态时，我们指的是在任务中最后一条指令执行后，变量及其对应值的集合。例如，对于前面的任务，我们总是有最终状态，包括`num`变量值为`14`，以及变量`x`的值为`10`，无论上下文切换如何。

通过知道单个任务的总体状态在不同运行中不会改变，我们可能会倾向于得出结论，由于必须遵循执行顺序和发生之前约束，并发性不会影响任务的总体状态。然而，我们对此结论应持谨慎态度。

假设我们有一个并发任务系统，所有任务都有对*共享资源*的读写权限，比如一个变量。如果所有任务只读取共享变量，而没有任务将要写入它（改变其值），我们可以说，无论上下文切换如何发生，无论你运行任务多少次，我们总是得到相同的结果。请注意，这同样适用于没有共享变量的并发任务系统。

然而，如果只有一个任务将要写入共享变量，那么由任务调度器单元强加的上下文切换将影响所有任务的总体状态。这意味着它可能从一个运行到另一个运行而不同！因此，应该采用适当的控制机制来避免任何不希望的结果。这一切都是由于上下文切换无法预测，并且任务的*中间状态*可能从一个运行到另一个运行而变化。与总体状态相对，中间状态是在某个指令下变量及其值的集合。每个任务只有一个总体状态，这是在任务完成时确定的，但它有多个中间状态，对应于执行某个指令后的变量及其值。

总结来说，当你有一个包含多个任务且这些任务可以写入共享资源的并发系统时，系统的不同运行将产生不同的结果。因此，应该使用适当的*同步*方法来取消上下文切换的影响，并在各种运行中获得相同的确定性行为。

我们现在有一些并发的基本概念，这是本章的主题。本节中解释的概念对于理解许多主题是基本的，你将在本书未来的章节中反复听到它们。

你会记得我们也说过并发可能会出现问题，进而，它可能会使事情对我们来说更加复杂。所以，你可能想知道，我们什么时候需要它？在本章的下一节中，我们将回答这个问题。

# 何时使用并发

根据我们到目前为止的解释，似乎只有一个任务比多个任务同时做同一件事的问题要小得多。这是完全正确的；如果你能编写一个不需要引入并发就能正常运行得不错的程序，那么强烈建议你这样做。我们可以使用一些通用模式来了解何时必须使用并发。

在本节中，我们将探讨这些通用模式是什么，以及它们是如何引导我们将程序拆分为并发流程的。

不论使用哪种编程语言，程序本质上是一组应该按顺序执行的指令。换句话说，给定的指令不会执行，直到前面的指令已经执行。我们称这个概念为*顺序执行*。当前指令完成所需的时间有多长并不重要；下一条指令必须等待当前一条指令完成。通常说，当前指令正在*阻塞*下一条指令；这有时被描述为当前指令是一个*阻塞指令*。

在每个程序中，所有的指令都是阻塞的，每个执行流程的执行流程都是顺序的。我们只能说程序运行得快，如果每个指令在几个毫秒内阻塞下一条指令的时间相对较短。然而，如果阻塞指令花费了太多时间（例如 2 秒或 2000 毫秒），或者它所需的时间无法确定，会发生什么？这两个模式告诉我们我们需要一个并发程序。

为了进一步阐述，每个阻塞指令在尝试完成时会消耗一定的时间。对我们来说，最佳情况是，给定的指令完成所需的时间相对较短，然后，下一条指令可以立即执行。然而，我们并不总是这么幸运。

有一些场景，我们无法确定阻塞指令完成所需的时间。这通常发生在阻塞指令正在等待某个事件发生，或者某些数据变得可用时。

让我们用一个例子继续。假设我们有一个服务器程序，它正在为多个客户端程序提供服务。服务器程序中有一个指令等待客户端程序连接。从服务器程序的角度来看，没有人能确切地说何时会有新的客户端连接。因此，下一条指令不能在服务器端执行，因为我们不知道何时才能完成当前的指令。这完全取决于新客户端尝试连接的时间。

一个更简单的例子是当你从用户那里读取一个字符串时。从程序的角度来看，没有人能确定用户何时会输入他们的输入；因此，未来的指令无法执行。这是导致并发任务系统的*第一个模式*。

并发的第一个模式是当你有一个可以无限期阻塞执行流程的指令。在这种情况下，你应该将现有的流程分成两个独立的流程或任务。如果你需要执行后续指令，而又不能等待当前指令首先完成，你会这样做。对于这个场景来说，更重要的是，我们假设后续指令不依赖于当前指令完成的结果。

通过将前面的流程分成两个并发任务，当其中一个任务正在等待阻塞指令完成时，另一个任务可以继续执行先前非并发设置中阻塞的指令。

在本节中，我们将关注的下一个例子将展示第一个模式如何导致并发任务系统。我们将使用伪代码来表示每个任务中的指令。

**注意**：

理解即将到来的例子不需要任何计算机网络知识。

我们将要关注的例子是关于一个具有三个目标的服务器程序：

+   它计算从客户端读取的两个数字的和，并将结果返回给客户端。

+   它定期将已服务的客户端数量写入文件，无论是否正在服务任何客户端。

+   它还必须能够同时服务多个客户端。

在讨论满足上述目标的最终并发系统之前，让我们首先假设在这个例子中我们只使用一个任务（或流程），然后我们将展示单个任务无法完成上述目标。你可以看到服务器程序的伪代码，在单任务设置中，在*代码框 13-4*中：

```cpp
Calculator Server {
    Task T1 {
        1\. N = 0
        2\. Prepare Server
        3\. Do Forever {
        4\.     Wait for a client C
        5\.     N = N + 1
        6\.     Read the first number from C and store it in X
        7\.     Read the second number from C and store it in Y
        8\.     Z = X + Y
        9\.     Write Z to C
       10\.     Close the connection to C
       11\.     Write N to file
           }
    }
}
```

代码框 13-4：使用单个任务操作的服务器程序

正如你所见，我们的单个流程等待网络上的客户端连接。然后从客户端读取两个数字，然后计算它们的和并将其返回给客户端。最后，它关闭客户端连接，在继续等待下一个客户端加入之前将已服务的客户端数量写入文件。很快，我们将展示前面的代码无法满足我们上述的目标。

这个伪代码只包含一个任务，`T1`。它有 12 条指令，正如我们之前所说的，它们是顺序执行的，并且所有指令都是阻塞的。那么，这段代码究竟向我们展示了什么呢？让我们一步步来看：

+   第一条指令，`N = 0`，很简单，并且完成得很快。

+   第二条指令，`准备服务器`，预计会在合理的时间内完成，这样就不会阻塞服务器程序的执行。

+   第三条指令只是启动主循环，并且随着我们进入循环，它应该很快完成。

+   第四条指令，`等待客户 C`，是一个具有未知完成时间的阻塞指令。因此，指令*5*、*6*以及其余的指令将不会执行。因此，它们似乎必须等待新客户加入，只有在此之后，这些指令才能执行。

正如我们之前所说的，指令*5*到*10*等待新客户是必须的。换句话说，这些指令依赖于指令*4*的输出，并且在没有接受客户的情况下不能执行。然而，指令*11*，`将 N 写入文件`，需要执行，无论是否有客户。这是由我们为这个例子定义的第二项目标所决定的。根据前面的配置，我们只有在有客户的情况下才将`N`写入文件，尽管这与我们的初始要求相矛盾，即，无论是否有客户，我们都将`N`写入文件。

前面的代码在其指令流程中还有一个问题；指令*6*和*7*都有可能阻塞执行流程。这些指令等待客户输入两个数字，由于这取决于客户，我们无法准确预测这些指令何时会完成。这阻止了程序继续执行。

不仅如此，这些指令可能会阻止程序接受新的客户。这是因为如果指令*6*和*7*需要很长时间才能完成，执行流程将不会再次达到指令*4*。因此，服务器程序不能同时服务多个客户，这再次不符合我们定义的目标。

为了解决上述问题，我们需要将单个任务分解为三个并发任务，这三个任务共同满足我们对服务器程序的要求。

在*代码框 13-5*中的后续伪代码中，您将找到三个执行流程，`T1`、`T2`和`T3`，它们基于并发解决方案满足我们定义的目标：

```cpp
Calculator Server {
    Shared Variable: N
    Task T1 {
        1\. N = 0
        2\. Prepare Server
        3\. Spawn task T2
        4\. Do Forever {
        5\.     Write N to file
        6\.     Wait for 30 seconds
           }
    }
    Task T2 {
        1\. Do Forever {
        2\.     Wait for a client C
        3\.     N = N + 1
        4\.     Spawn task T3 for C
           }
    }
    Task T3 {
        1\. Read first number from C and store it in X
        2\. Read first number from C and store it in Y
        3\. Z = X + Y
        4\. Write Z to C
        5\. Close the connection to C
    }
}
```

代码框 13-5：使用三个并发任务运行的服务器程序

程序首先执行任务`T1`。`T1`被称为程序的主要任务，因为它将是第一个要执行的任务。请注意，每个程序至少有一个任务，并且所有其他任务都是由这个任务直接或间接启动的。

在前面的代码框中，我们还有两个由主任务`T1`派生的其他任务。还有一个共享变量`N`，它存储已服务的客户数量，并且可以被所有任务访问（读取或写入）。

程序从任务 `T1` 的第一条指令开始；通过这条指令，它将变量 `N` 初始化为零。然后第二条指令准备服务器。作为这条指令的一部分，应该采取一些初步步骤，以便服务器程序能够接受传入的连接。请注意，到目前为止，还没有其他并发任务在任务 `T1` 旁边运行。

任务 `T1` 中的第三条指令创建了一个新的 *实例*，用于任务 `T2`。创建新任务通常很快，且不耗时。因此，任务 `T1` 在创建任务 `T2` 后立即进入无限循环，每 30 秒将共享变量 `N` 的值写入文件。这是我们为服务器程序定义的第一个目标，现在已经实现。基于此，在没有其他指令的干扰或阻塞的情况下，任务 `T1` 会定期将 `N` 的值写入文件，直到程序完成。

让我们谈谈派生的任务。任务 `T2` 的唯一责任是当客户端发送连接请求时立即接受它们。也值得记住的是，任务 `T2` 中的所有指令都在一个无限循环中运行。任务 `T2` 中的第二条指令等待新的客户端。在这里，它阻止了任务 `T2` 中其他指令的执行，但这仅适用于任务 `T2` 中的指令。请注意，如果我们派生了两个 `T2` 实例而不是一个，其中一个实例中的指令被阻塞不会阻止另一个实例中的指令。

其他并发任务，在这种情况下只有 `T1`，会继续执行它们的指令而没有任何阻塞。这正是并发所实现的；当一些任务因为某个事件而阻塞时，其他任务可以继续它们的工作而不会受到任何干扰。正如我们之前所说的，这有一个重要的设计原则作为其核心：*每当遇到一个阻塞操作，要么其完成时间未知，要么完成时间很长，那么你应该将任务分成两个并发任务*。

现在，假设有一个新的客户端加入。我们已经在 *代码框 13-4* 中看到，在服务器程序的并发版本中，读取操作可能会阻塞新客户端的接受。基于我们刚才指出的设计原则，由于读取指令是阻塞的，我们需要将逻辑分成两个并发任务，这就是为什么我们引入了任务 `T3`。

每当有新的客户端加入时，任务 `T2` 会派生一个新的任务 `T3` 实例，以便与新加入的客户端通信。这是通过任务 `T2` 中的指令 *4* 实现的，提醒一下，这是以下命令：

```cpp
4\.     Spawn task T3 for C
```

代码框 13-6：任务 T2 中的指令 4

在派生新任务之前，任务 `T2` 会增加共享变量 `N` 的值，以表示已为新客户端提供服务。再次强调，派生指令相当快，不会阻塞新客户端的接受。

在任务 `T2` 中，当指令 *4* 执行完毕后，循环继续，并回到指令 *2*，等待另一个客户端加入。请注意，根据我们已有的伪代码，虽然我们只有一个任务 `T1` 的实例和一个任务 `T2` 的实例，但我们可以为每个客户端拥有多个 `T3` 的实例。

任务 `T3` 的唯一责任是与客户端通信并读取输入数字。然后，它继续计算总和并将结果发送回客户端。正如之前指出的，任务 `T3` 内部的阻塞指令不能阻止其他任务的执行，其阻塞行为仅限于同一实例的 `T3`。即使是特定实例的 `T3` 内的阻塞指令也不能阻止另一个实例的 `T3` 内的指令。这样，服务器程序可以以并发的方式满足我们所有的期望目标。

那么，下一个问题可能是，任务何时完成？我们知道，通常情况下，当任务内的所有指令都执行完毕后，任务就完成了。但当我们有一个无限循环包裹着任务内的所有指令时，任务就不会完成，其生命周期取决于创建它的**父任务**。我们将在未来的章节中具体讨论关于进程和线程的内容。为了我们的例子，在我们先前的并发程序中，所有 `T3` 实例的父任务是唯一的 `T2` 实例。正如你所看到的，一个特定的 `T3` 实例完成要么是在通过两个阻塞读取指令后关闭与客户端的连接，要么是唯一的 `T2` 实例完成。

在一种罕见但可能的情况中，如果所有读取操作完成所需的时间过长（这可能是有意的或意外的），并且进入的客户端数量迅速增加，那么我们可能会有一段时间内运行着过多的 `T3` 实例，并且它们都在等待客户端提供输入数字。这种情况会导致消耗大量的资源。然后，经过一段时间，由于越来越多的进入连接，服务器程序可能会被操作系统终止，或者它简单地无法再服务任何客户端。

无论前一种情况发生什么，服务器程序都会停止服务客户端。当这种情况发生时，我们称之为**拒绝服务**（**DoS**）。对于具有并发任务的系统，应该设计成能够克服这些极端情况，从而以合理的方式为客户端提供服务。

**注意**：

当受到 DoS 攻击时，服务器机器上的资源拥塞发生，以使其崩溃并使其无响应。DoS 攻击属于试图阻止某些服务以使其对客户端不可用的网络攻击组。它们包括广泛的攻击，包括*漏洞利用*，目的是停止服务。这甚至包括为了使网络基础设施崩溃而对网络进行*洪水攻击*。

在服务器程序的先前列举的示例中，我们描述了一种情况，其中我们有一个阻塞指令，其完成时间无法确定，这是并发使用的第一个模式。还有一个与此类似但略有不同的模式。

如果一条指令或一组指令需要太长时间才能完成，那么我们可以将它们放入一个单独的任务中，并使新任务与主任务并发运行。这与第一个模式不同，因为虽然我们确实有一个完成时间的估计，尽管不是非常准确，但我们确实知道它不会很快完成。

关于前面示例中提到的共享变量`N`的最后一件事需要注意，那就是其中一个任务，特别是任务`T2`的实例，可能会改变其值。根据我们在本章前面的讨论，因此这个并发任务系统因此容易受到并发问题的困扰，因为它有一个可以被其中一个任务修改的共享变量。

重要的是要注意，我们为服务器程序提出的解决方案远非完美。在下一章中，你将了解到并发问题，通过它你将看到前面的示例在共享变量`N`上存在严重的*数据竞争*问题。因此，应该采用适当的控制机制来解决并发产生的问题。

在本章接下来的最后一节中，我们将讨论一些并发任务之间共享的*状态*。我们还将介绍*交错*的概念及其对具有可修改共享状态的并发系统的重要影响。

# 共享状态

在上一节中，我们讨论了表明我们需要一个并发任务系统的模式。在那之前，我们也简要解释了在执行多个并发任务期间，上下文切换模式的不确定性，以及有一个可修改的共享状态，可能会导致所有任务的总体状态中出现非确定性。本节提供了一个示例，以说明这种非确定性在简单程序中可能带来的问题。

在本节中，我们将继续我们的讨论，并引入*共享状态*，看看它们如何导致我们之前讨论的非确定性。作为一个程序员，术语*状态*应该让你想到一组变量及其在特定时间的对应值。因此，当我们谈论任务的*整体状态*时，正如我们在第一部分定义的那样，我们指的是在任务执行最后一条指令的确切时刻，所有现有非共享变量及其对应值的集合。

同样，一个任务的*中间状态*是所有现有非共享变量及其在任务执行特定指令时的值的集合。因此，一个任务对于其每条指令都有一个不同的中间状态，中间状态的数量等于指令的数量。根据我们的定义，最后一个中间状态与任务的整体状态相同。

共享状态也是一组变量及其在特定时间的对应值，这些变量可以被并发任务系统读取或修改。共享状态不属于任何任务（它不是任务本地的），它可以被系统中的任何任务读取或修改，当然是在任何时间。

通常，我们对只读的共享状态不感兴趣。它们通常可以安全地被许多并发任务读取，并且不会产生任何问题。然而，如果一个共享状态是可修改的，并且没有仔细保护，通常会导致一些严重的问题。因此，本节中涵盖的所有共享状态都被认为是至少可以被一个任务修改的。

自问一下这个问题：如果一个共享状态被系统中的一个并发任务修改，可能会出什么问题？为了回答这个问题，我们首先给出一个例子，即两个并发任务访问单个共享变量的系统，在这种情况下，是一个简单的整数变量。

假设我们有一个如下所示的系统，如*代码框 13-7*所示：

```cpp
Concurrent System {
    Shared State {
        X : Integer = 0
    }

    Task P {
        A : Integer
            1\. A = X
            2\. A = A + 1
            3\. X = A
            4\. print X
    }
    Task Q {
        B : Integer
            1\. B = X
            2\. B = B + 2
            3\. X = B
            4\. print X
    }
}
```

代码框 13-7：具有可修改共享状态的两个并发任务系统

假设在前面的系统中，任务`P`和`Q`不是并发运行的。因此，它们变成了顺序执行。假设首先执行`P`中的指令，然后是`Q`。如果是这样，那么整个系统整体状态，无论任何单个任务的整体状态如何，都将是一个值为 3 的共享变量`X`。

如果你以相反的顺序运行系统，首先执行`Q`中的指令，然后执行`P`中的指令，你将得到相同的状态。然而，这通常不是情况，以相反的顺序运行两个不同的任务可能会导致不同的整体状态。

如你所见，按顺序运行这些任务会产生一个确定的结果，无需担心上下文切换。

现在，假设它们在同一个 CPU 核心上并发运行。根据各种指令处的上下文切换，将 `P` 和 `Q` 的指令放入执行有许多可能的场景。

以下是一个可能的场景：

```cpp
     Task P     |    Task Scheduler   |    Task Q    
----------------------------------------------------
                |    Context Switch   |             
                |                     |  B = X
                |                     |  B = B + 2
                |    Context Switch   |
  A = X         |                     |
                |    Context Switch   |
                |                     |  X = B
                |    Context Switch   |
  A = A + 1     |                     |
  X = A         |                     |
                |    Context Switch   |
                |                     |  print X
                |    Context Switch   |
  print X       |                     |
                |    Context Switch   |
```

代码框 13-8：当并发运行任务 P 和 Q 时另一种可能的交错

这种场景只是许多可能场景中的一种，这些场景涉及在特定位置发生上下文切换。每个场景都称为 *交错*。因此，对于并发任务系统，根据上下文切换可能发生的各种位置，存在多种可能的交错方式，而在每次运行中，只有这些众多交错方式中的一个会发生。这，结果，使得它们不可预测。

对于前面的交错，如您在第一列和最后一列中看到的那样，指令和 happens-before 约束的顺序得到了保留，但执行之间可能存在 *间隙*。这些间隙是不可预测的，并且当我们跟踪执行时，前面的交错导致了一个令人惊讶的结果。进程 `P` 打印值 `1`，进程 `Q` 打印值 `2`，但预期它们都会打印 `3` 作为它们的最终结果。

注意，在前面的例子中，接受最终结果的约束被定义为这样——程序应该在输出中打印两个 `3`。这个约束可能是其他东西，并且与程序的可视输出无关。更重要的是，存在其他关键的约束，在面临不可预测的上下文切换时应该保持 *不变*。这包括没有任何 *数据竞争* 或 *竞争条件*，没有任何内存泄漏，甚至不崩溃。所有这些约束都比程序的可视输出更重要。在许多实际应用中，程序甚至没有输出。

以下在 *代码框 13-9* 中是另一种具有不同结果的交错：

```cpp
   Task P    |    Task Scheduler   |    Task Q
-------------------------------------------------
             |    Context Switch   |
             |                     |    B = X
             |                     |    B = B + 2
             |                     |    X = B
             |    Context Switch   |
 A = X       |                     |
 A = A + 1   |                     |
             |    Context Switch   |
             |                     |    print X
             |    Context Switch   |
 X = A       |                     |
 print X     |                     |
             |    Context Switch   |
```

代码框 13-9：当并发运行任务 P 和 Q 时另一种可能的交错

在这种交错中，任务 `P` 打印 `3`，但任务 `Q` 打印 `2`。这是由于任务 `P` 在第三次上下文切换之前没有足够幸运地更新共享变量 `X` 的值。因此，任务 `Q` 只打印了 `X` 在那一刻的值，即 `2`。这种情况被称为变量 `X` 的 *数据竞争*，我们将在下一章中进一步解释。

在实际的 C 程序中，我们通常编写 `X++` 或 `X = X + 1` 而不是首先将 `X` 复制到 `A` 中，然后增加 `A`，最后将其放回 `X`。您将在 *第十五章*，*线程执行* 中看到这个例子。

这清楚地表明，C 语言中的简单 `X++` 语句实际上由三个更小的指令组成，这些指令不会在单个时间片中执行。换句话说，它不是一个 *原子指令*，但它由三个更小的原子指令组成。原子指令不能被分解成更小的操作，也不能被上下文切换中断。我们将在关于多线程的后续章节中看到更多关于这一点的内容。

在前一个例子中，还有另一件事需要考虑。在前面的例子中，任务 `P` 和 `Q` 并不是系统中唯一正在运行的任务；还有其他任务与我们的任务 `P` 和 `Q` 同时执行，但我们没有在分析中考虑它们，我们只讨论了这两个任务。为什么是这样？

这个问题的答案在于，这两个任务与系统中其他任务之间的不同交错组合不会改变任务 `P` 或 `Q` 的中间状态。换句话说，其他任务与 `P` 和 `Q` 没有共享状态，正如我们之前解释的，当某些任务之间没有共享资源时，交错组合就不会重要，就像在这个例子中我们看到的那样。因此，我们可以假设在我们的假设系统中除了 `P` 和 `Q` 之外没有其他任务。

其他任务对 `P` 和 `Q` 唯一的影响是，如果它们的数量太多，它们可以使 `P` 和 `Q` 的执行变慢。这仅仅是 `P` 或 `Q` 中两个连续指令之间有长间隔的结果。换句话说，CPU 核心需要被更多任务共享。因此，任务 `P` 和 `Q` 需要更频繁地在队列中等待，从而延迟它们的执行。

通过这个例子，你看到了即使是两个并发任务之间的单一共享状态也可能导致整体结果缺乏确定性。我们已经展示了与缺乏确定性相关的问题；我们不希望有一个在不同的运行中产生不同结果的程序。我们例子中的任务相对简单，包含四个平凡的指令，但实际存在于生产环境中的并发应用程序比这要复杂得多。

更重要的是，我们有许多不同类型的共享资源，这些资源不一定驻留在内存中，例如网络上的文件或服务。

同样，尝试访问共享资源的任务数量可能很高，因此我们需要更深入地研究并发问题，并找到恢复确定性的机制。在下一章中，我们将继续讨论并发问题和解决这些问题的方法。

在结束这一章之前，让我们简要地谈谈任务调度器以及它是如何工作的。如果我们只有一个 CPU 核心，那么在任何给定时刻，我们只能有一个任务使用那个 CPU 核心。

我们也知道，任务调度器本身是一个需要占用 CPU 核心执行片段的程序。那么，当另一个任务正在使用 CPU 核心时，它是如何管理不同任务的？让我们假设任务调度器本身正在使用 CPU 核心。首先，它在设置一个定时器以发生*定时中断*之前，从其队列中选择一个任务，然后它离开 CPU 核心并将资源交给所选任务。

现在我们假设任务调度器会给每个任务一定的时间，那么中断将会发生，CPU 核心停止当前任务的执行，并立即将任务调度器加载回 CPU。现在，调度器存储前一个任务的最新状态，并从队列中加载下一个任务。所有这些都会一直进行，直到内核启动并运行。关于具有多核心 CPU 的机器，这可能会改变，内核可以在调度其他核心的任务时使用多个核心。

在本节中，我们简要介绍了共享状态的概念以及它们如何在并发系统中参与。讨论将在下一章继续，我们将讨论并发问题和同步技术。

# 摘要

在本章中，我们介绍了并发的基础知识，以及为了理解即将到来的多线程和多处理多进程主题，你需要了解的基本概念和术语。

具体来说，我们讨论了以下内容：

+   并发和并行性的定义——即每个并行任务都需要自己的处理器单元，而并发任务可以共享单个处理器。

+   并发任务使用单个处理器单元，而任务调度器管理处理器时间，并在不同任务之间共享。这会导致每个任务都有多个上下文切换和不同的交织。

+   阻塞指令的介绍。我们还解释了表明我们需要并发的情况的模式，以及我们如何将单个任务分解成两个或三个并发任务。

+   我们描述了什么是共享状态。我们还展示了共享状态如何导致严重的并发问题，如当多个任务尝试读取和写入相同的共享状态时，会出现数据竞争。

在下一章中，我们完成对并发主题的讨论，并解释了在并发环境中你将遇到的各种问题。关于并发相关问题的解决方案也将是我们下一章讨论的一部分。
