<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Augmented Reality-based Visualization on Mobile or Wearable Platforms</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Getting started I: Setting up OpenCV on Android </li><li class="listitem" style="list-style-type: disc">Getting started II: Accessing the camera live feed using OpenCV</li><li class="listitem" style="list-style-type: disc">Displaying real-time video processing with texture mapping</li><li class="listitem" style="list-style-type: disc">Augmented reality-based data visualization over real-world scenes</li></ul></div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec53"/>Introduction</h1></div></div></div><p>The field of digital graphics has traditionally been living within its own virtual world since computers were invented. Often, computer-generated content has no awareness of the user and how the information is relevant to the user in the real world. The application is always simply waiting for a user command such as the mouse or keyboard input. One major limiting factor in the early design of computer applications is that computers are typically sitting on a desk in an office or in a home environment. The lack of mobility and the inability to interact with its environment or user ultimately limited the development of real-world interactive visualization applications.</p><p>Today, with the evolution of mobile computing, we have redefined many of our daily interactions with the world—for example, through applications that enable navigation with GPS using a mobile phone. However, instead of enabling users to seamlessly interact with the world, mobile devices still draw users away from the real world. In particular, as in previous generations of desktop computing, users are still required to look away from the real world into a virtual world (in many cases, just a tiny mobile screen).</p><p>The notion of <strong>Augmented Reality</strong> (<strong>AR</strong>)<a id="id452" class="indexterm"/> is a step towards reconnecting the user with the real world through the fusion of the virtual world (generated by the computer) with the real world. This is distinctly different from virtual reality, in which the user is immersed into the virtual world and detached from the real world. For example, a typical embodiment of AR involves the use of a video see-through display in which virtual content (such as a computer-generated map) is combined with a real-world scene (captured continuously with a built-in camera). Now, the user is engaged with the real world—a step closer to a truly human-centric application.</p><p>Ultimately, the emergence of AR-enabled wearable computing devices (such as Meta's AR eyeglasses, which features the world's first holographic interface with 3D gesture detection and 3D stereoscopic display) will create a new era of computing that will greatly revolutionize the way humans interact with computers. Developers interested in data visualization now have another set of tools that are significantly more human-centric and intuitive. Such a design, needless to say, truly connects human, machine, and the real world together. Having information directly overlaid onto the real world (for example, by overlaying a virtual guidance map for navigation) is so much more powerful and meaningful.</p><p>This final chapter introduces the fundamental building blocks for creating your first AR-based application on a commodity Android-based mobile device: OpenCV for computer vision, OpenGL for graphics rendering, as well as Android's sensor framework for interaction. With these tools, the graphics rendering capability that used to only exist in Hollywood movie production can now be made available at everyone's fingertips. While we will only focus on the use of an Android-based mobile device in this chapter, the conceptual framework for AR-based data visualization introduced in this chapter can be similarly extended to state-of-the-art wearable computing platforms, such as Meta's AR eyeglasses.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec54"/>Getting started I: Setting up OpenCV on Android</h1></div></div></div><p>In this section, we will<a id="id453" class="indexterm"/> outline<a id="id454" class="indexterm"/> the steps to set up the OpenCV library on the Android platform, which is needed to enable access to the live camera stream central to any Augmented Reality applications.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec138"/>Getting ready</h2></div></div></div><p>We assume that the Android SDK and NDK are configured exactly as discussed in <a class="link" href="ch07.html" title="Chapter 7. An Introduction to Real-time Graphics Rendering on a Mobile Platform using OpenGL ES 3.0">Chapter 7</a>, <em>An Introduction to Real-time Graphics Rendering on a Mobile Platform Using OpenGL ES 3.0</em>. Here, we add in the support of OpenCV for Android. We will import and integrate the OpenCV library into our existing code structure from the previous chapter.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec139"/>How to do it...</h2></div></div></div><p>Here, we describe the <a id="id455" class="indexterm"/>major steps for setting up the OpenCV library, mainly<a id="id456" class="indexterm"/> path setup and pre-configuration of the Java SDK project setup:</p><div><ol class="orderedlist arabic"><li class="listitem">Download the OpenCV for <a id="id457" class="indexterm"/>Android SDK package, Version 3.0.0 (<code class="literal">OpenCV-3.0.0-android-sdk-1.zip</code>) at <a class="ulink" href="http://sourceforge.net/projects/opencvlibrary/files/opencv-android/3.0.0/OpenCV-3.0.0-android-sdk-1.zip">http://sourceforge.net/projects/opencvlibrary/files/opencv-android/3.0.0/OpenCV-3.0.0-android-sdk-1.zip</a>.</li><li class="listitem">Move the package (<code class="literal">OpenCV-3.0.0-android-sdk-1.zip</code>) to the <code class="literal">3rd_party/android</code> folder created in <a class="link" href="ch07.html" title="Chapter 7. An Introduction to Real-time Graphics Rendering on a Mobile Platform using OpenGL ES 3.0">Chapter 7</a>,<em> An Introduction to Real-time Graphics Rendering on a Mobile Platform Using OpenGL ES 3.0</em>.</li><li class="listitem">Unzip the package with the following commands<div><pre class="programlisting">
<strong>cd 3rd_party/android &amp;&amp; unzip OpenCV-3.0.0-android-sdk-1.zip</strong>
</pre></div></li><li class="listitem">Then in the project folder (for example <code class="literal">ch9/code/opencv_demo_1</code>), run the following script to initialize the project for Android. Note that the <code class="literal">3rd_party</code> folder is assumed to be in the same top-level directory as in previous chapters:<div><pre class="programlisting">#!/bin/bash
ANDROID_SDK_PATH="../../../3rd_party/android/android-sdk-macosx"
OPENCV_SDK_PATH="../../../3rd_party/android/OpenCV-android-sdk"

#initialize the SDK Java library
$ANDROID_SDK_PATH/tools/android update project -p $OPENCV_SDK_PATH/sdk/java -s --target "android-18"
$ANDROID_SDK_PATH/tools/android update project -p . -s --target "android-18" --library $OPENCV_SDK_PATH/sdk/java</pre></div></li><li class="listitem">Finally, include the OpenCV path in the build script <code class="literal">jni/Android.mk</code>.<div><pre class="programlisting">LOCAL_PATH:= $(call my-dir)
#build the OpenGL + OpenCV code in JNI
include $(CLEAR_VARS)
#including OpenCV SDK
include ../../../3rd_party/android/OpenCV-android-sdk/sdk/native/jni/OpenCV.mk</pre></div></li></ol></div><p>Now, the project is linked to the OpenCV library, both from the Java side as well as from the native side.</p><p>Next we must install the OpenCV Manager on the mobile phone. The OpenCV Manager allows us to create<a id="id458" class="indexterm"/> applications without statically linking all the <a id="id459" class="indexterm"/>required libraries, and it is recommended. To install the package, we can execute the following <code class="literal">adb</code> command from the same project folder (<code class="literal">ch9/code/opencv_demo_1</code>). Again, note the relative location of the <code class="literal">3rd_party</code> folder. You can also execute this command within the Android SDK folder and modify the relative path of the <code class="literal">3rd_party</code> folder accordingly.</p><div><pre class="programlisting">$ANDROID_SDK_PATH/platform-tools/adb install ../../../3rd_party/android/OpenCV-android-sdk/apk/OpenCV_3.0.0_Manager_3.00_armeabi-v7a.apk</pre></div><p>After we have successfully completed the setup, we are ready to create our first OpenCV Android application on the phone.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec140"/>See also</h2></div></div></div><p>Windows users should consult the following tutorials<a id="id460" class="indexterm"/> on Android development with OpenCV for setup instructions: <a class="ulink" href="http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/android_dev_intro.html">http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/android_dev_intro.html</a> and <a class="ulink" href="http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/dev_with_OCV_on_Android.html#native-c">http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/dev_with_OCV_on_Android.html#native-c</a>.</p><p>For further information on using OpenCV in an <a id="id461" class="indexterm"/>Android application, consult the online documentation at <a class="ulink" href="http://opencv.org/platforms/android.html">http://opencv.org/platforms/android.html</a>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec55"/>Getting started II: Accessing the camera live feed using OpenCV</h1></div></div></div><p>Next we need to <a id="id462" class="indexterm"/>demonstrate <a id="id463" class="indexterm"/>how to integrate OpenCV into our Android-based development framework. The following block diagram illustrates the core functions and relationship among the classes that will be implemented in this chapter (only the functions or classes relevant to the introduction of OpenCV will be discussed in this section):</p><div><img src="img/9727OS_09_01.jpg" alt="Getting started II: Accessing the camera live feed using OpenCV"/></div><p>In particular, we will<a id="id464" class="indexterm"/> demonstrate how to extract an image<a id="id465" class="indexterm"/> frame from the camera video stream for further image processing steps. The OpenCV library provides camera support for accessing the live camera feed (the raw data buffer of the video data stream) as well as controlling the camera parameters. This feature allows us to get the raw frame data from the live preview camera with optimal resolution, frame rate, and image format.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec141"/>Getting ready</h2></div></div></div><p>The demos in this chapter build upon the basic structure introduced in the sample code of <a class="link" href="ch08.html" title="Chapter 8. Interactive Real-time Data Visualization on Mobile Devices">Chapter 8</a>, <em>Interactive Real-time Data Visualization on Mobile Devices</em> which utilizes the multi-touch interface and motion sensor inputs to enable interactive real-time data visualization on mobile devices. The major changes that are made to support OpenCV will be highlighted. For the complete code, download the code package from the Packt Publishing website.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec142"/>How to do it...</h2></div></div></div><p>First, we will highlight the changes to the Java source files required to enable the use of OpenCV and the OpenCV camera module. Rename <code class="literal">GL3JNIActivity.java</code> (<code class="literal">src/com/android/gl3jni/</code>) as <code class="literal">GL3OpenCVDemo.java</code> and modify the code as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Include the packages for the OpenCV library:<div><pre class="programlisting">package com.android.gl3jni;
...
import org.opencv.android.BaseLoaderCallback;
import org.opencv.android.LoaderCallbackInterface;
import org.opencv.android.OpenCVLoader;
import org.opencv.android.CameraBridgeViewBase;
import org.opencv.android.CameraBridgeViewBase.CvCameraViewFrame;
import org.opencv.android.CameraBridgeViewBase.CvCameraViewListener2;
import org.opencv.core.CvType;
import org.opencv.core.Mat;

import android.widget.RelativeLayout;
import android.view.SurfaceView;</pre></div></li><li class="listitem">Add the <code class="literal">CvCameraViewListener2</code> interface to the <code class="literal">GL3OpenCVDemo</code> class:<div><pre class="programlisting">public class GL3OpenCVDemo extends Activity implements SensorEventListener, CvCameraViewListener2{</pre></div></li><li class="listitem">Create the variables to handle the camera view:<div><pre class="programlisting">  private GL3JNIView mView=null;
  ...
  private boolean gl3_loaded = false;
  private CameraBridgeViewBase mOpenCvCameraView;
  private RelativeLayout l_layout;</pre></div></li><li class="listitem">Implement <a id="id466" class="indexterm"/>the <code class="literal">BaseLoaderCallback</code> function<a id="id467" class="indexterm"/> for <code class="literal">OpenCVLoader</code>:<div><pre class="programlisting">  private BaseLoaderCallback mLoaderCallback = new BaseLoaderCallback(this) {
  @Override 
  public void onManagerConnected(int status) {
    switch (status) {
      case LoaderCallbackInterface.SUCCESS:{
        Log.i("OpenCVDemo", "OpenCV loaded successfully");
        // load the library *AFTER* we have OpenCV lib ready!
        System.loadLibrary("gl3jni");
        gl3_loaded = true;

        //load the view as we have all JNI loaded 
        mView = new GL3JNIView(getApplication());
        l_layout.addView(mView);
        setContentView(l_layout);

        /* enable the camera, and push the images to the OpenGL layer */
        mOpenCvCameraView.enableView();
      } break;
      default:{
        super.onManagerConnected(status);
      } break;
    }
  }
};</pre></div></li><li class="listitem">Implement the OpenCV camera callback functions and pass the image data to the JNI C/C++ side for processing and rendering:<div><pre class="programlisting">public void onCameraViewStarted(int width, int height) {
}
public void onCameraViewStopped() {
}
public Mat onCameraFrame(CvCameraViewFrame inputFrame) {
  //Log.i("OpenCVDemo", "Got Frame\n");
  Mat input = inputFrame.rgba();
  if(gl3_loaded){
    GL3JNILib.setImage(input.nativeObj);
  }
  //don't show on the java side
  return null;
}</pre></div></li><li class="listitem">Initialize the <a id="id468" class="indexterm"/>camera in the <code class="literal">onCreate</code> function, upon<a id="id469" class="indexterm"/> starting the application:<div><pre class="programlisting">@Override protected void onCreate(Bundle icicle) {
  super.onCreate(icicle);
  ...
  //setup the Java Camera with OpenCV
  setContentView(R.layout.ar);
  l_layout = (RelativeLayout)findViewById(R.id.linearLayoutRest);
  mOpenCvCameraView = (CameraBridgeViewBase)findViewById(R.id.opencv_camera_surface_view);
  mOpenCvCameraView.setVisibility( SurfaceView.VISIBLE );
  mOpenCvCameraView.setMaxFrameSize(1280, 720); /* cap it at 720 for performance issue */
  mOpenCvCameraView.setCvCameraViewListener(this);
  mOpenCvCameraView.disableView();
}</pre></div></li><li class="listitem">Load the OpenCV library using the asynchronized initialization function called <code class="literal">initAsync</code> from the <code class="literal">OpenCVLoader</code> class. This event is captured by the <code class="literal">BaseLoaderCallback mLoaderCallback</code> function defined earlier:<div><pre class="programlisting">@Override
protected void onResume() {
  super.onResume();
  OpenCVLoader.initAsync(OpenCVLoader.OPENCV_VERSION_3_0_0, this, mLoaderCallback);  
  ...
}</pre></div></li><li class="listitem">Finally, handle the <code class="literal">onPause</code> event, which pauses the camera preview when the application is no longer running in the foreground:<div><pre class="programlisting">@Override
protected void onPause() {
  super.onPause();
  mSensorManager.unregisterListener(this);
  //stop the camera
  if(mView!=null){
    mView.onPause();
  }
  if (mOpenCvCameraView != null)
    mOpenCvCameraView.disableView();
  gl3_loaded = false;
}</pre></div></li><li class="listitem">Now<a id="id470" class="indexterm"/> inside <code class="literal">GL3JNILib.java</code> (<code class="literal">src/com/android/gl3jni/</code>), add the native <code class="literal">setImage</code> function to pass the<a id="id471" class="indexterm"/> camera raw data. The entire source file is shown here, given its simplicity:<div><pre class="programlisting">package com.android.gl3jni;

public class GL3JNILib { 
  public static native void init(int width, int height);
  public static native void step();

  //pass the image to JNI C++ side
  public static native void setImage(long imageRGBA);
  
  //pass the device rotation angles and the scaling factor
  public static native void resetRotDataOffset();
  public static native void setRotMatrix(float[] rotMatrix);
  public static native void setScale(float scale);
}</pre></div></li><li class="listitem">Finally, the source code inside <code class="literal">GL3JNIView.java</code> is virtually identical except that we offer the option to reset the rotation data and call the <code class="literal">setZOrderOnTop</code> function to ensure that the OpenGL layer is on top of the Java layer:<div><pre class="programlisting">class GL3JNIView extends GLSurfaceView {
  ...
  public GL3JNIView(Context context) {
    super(context);
    // Pick an EGLConfig with RGB8 color, 16-bit depth, no stencil 
    setZOrderOnTop(true);
    setEGLConfigChooser(8, 8, 8, 8, 16, 0);
    setEGLContextClientVersion(3);
    getHolder().setFormat(PixelFormat.TRANSLUCENT);
    renderer = new Renderer();
    setRenderer(renderer);
    //handle gesture input
    mScaleDetector = new ScaleGestureDetector(context, new ScaleListener());
  }
  ...
  @Override
  public boolean onTouchEvent(MotionEvent ev) {
    mScaleDetector.onTouchEvent(ev);
    int action = ev.getActionMasked();
    switch (action) {
      case MotionEvent.ACTION_DOWN:
        GL3JNILib.resetRotDataOffset();
        break;
    }
    return true;
  }
  ...
}</pre></div></li><li class="listitem">Finally, define the <a id="id472" class="indexterm"/>JNI prototypes to interface<a id="id473" class="indexterm"/> with the Java side in the <code class="literal">main.cpp</code> file that connects all components.<div><pre class="programlisting">//external calls for Java
extern "C" {
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_setImage(JNIEnv * jenv, jobject, jlong imageRGBA);
};
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_setImage(
    JNIEnv * jenv, jobject, jlong imageRGBA) {
      cv::Mat* image = (cv::Mat*) imageRGBA;
      /* use mutex lock to ensure the write/read operations are synced (to avoid corrupting the frame) */
      pthread_mutex_lock(&amp;count_mutex);
      frame = image-&gt;clone();
      pthread_mutex_unlock(&amp;count_mutex);
      //LOGI("Got Image: %dx%d\n", frame.rows, frame.cols);
}</pre></div></li><li class="listitem">To access the device camera, the following elements must be declared in the <code class="literal">AndroidManifest.xml</code> file to ensure we have the permission to control the camera. In our current example, we request access to the front and back cameras with autofocus support.<div><pre class="programlisting">&lt;uses-permission android:name="android.permission.CAMERA"/&gt;
&lt;uses-feature android:name="android.hardware.camera" android:required="false"/&gt;
&lt;uses-feature android:name="android.hardware.camera.autofocus" android:required="false"/&gt;
&lt;uses-feature android:name="android.hardware.camera.front" android:required="false"/&gt;
&lt;uses-feature android:name="android.hardware.camera.front.autofocus" android:required="false"/&gt;</pre></div></li></ol></div><p>At this point, we have <a id="id474" class="indexterm"/>developed a full demo application that <a id="id475" class="indexterm"/>supports OpenCV and real-time camera feed. In the next section, we will connect the camera raw data stream to the OpenGL layer and perform real-time feature extraction with OpenCV in C/C++.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec143"/>How it works...</h2></div></div></div><p>On the Java side, we have integrated the OpenCV Manager (installed previously) to handle the dynamic loading of all libraries at runtime. Upon starting the application, we must call the <code class="literal">OpenCVLoader.initAsync</code> function; all OpenCV-related JNI libraries must only be called after the OpenCV libraries are successfully loaded. To synchronize these actions in our case, the <code class="literal">callback</code> function (<code class="literal">BaseLoaderCallback</code>) checks the status of the initialization of OpenCV, and we proceed with the <code class="literal">System.loadLibrary</code> function to initialize OpenGL and other components only if the OpenCV loader returns success (<code class="literal">LoaderCallbackInterface.SUCCESS</code>). For simplicity, we did not include the implementation to handle library loading exceptions in this demo.</p><p>On the sensor side, we have also changed the implementation for the <code class="literal">SensorManager</code> function to return the rotation matrix instead of the Euler angles to avoid the issue of Gimbal lock<a id="id476" class="indexterm"/> (refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Gimbal_lock">http://en.wikipedia.org/wiki/Gimbal_lock</a>). We also remapped the coordinates (from device orientation to OpenGL camera orientation) using the <code class="literal">SensorManager.remapCoordinateSystem</code> function. Then the rotation matrix is directed to the OpenGL side with the native calls <code class="literal">GL3JNILib.setRotMatrix</code>. Also, we can allow the user to reset the default orientation by touching the screen. This is achieved by calling the <code class="literal">GL3JNILib.resetRotDataOffset</code> function, which resets the rotation matrix with the touch event.</p><p>Additionally, we have added the <code class="literal">OpenCV CvCameraViewListener2</code> interface and <code class="literal">CameraBridgeViewBase</code> class to enable native camera access. The <code class="literal">CameraBridgeViewBase</code> class is a basic class that handles the interaction with the Android Camera class and OpenCV library. It is responsible for controlling the camera, such as resolution, and<a id="id477" class="indexterm"/> processing the frame, such as changing the image <a id="id478" class="indexterm"/>format. The client implements <code class="literal">CvCameraViewListener</code> to receive callback events. In the current implementation, we manually set the resolution as 1280 x 720. However, we can increase or decrease the resolution based on the application needs. Finally, the color frame buffers are returned in RGBA format, and the data stream will be transferred to the JNI C/C++ side and rendered using texture mapping.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec56"/>Displaying real-time video using texture mapping</h1></div></div></div><p>Today, most <a id="id479" class="indexterm"/>mobile phones are equipped with <a id="id480" class="indexterm"/>cameras that are capable of capturing high-quality photos as well as videos. For example, the Samsung Galaxy Note 4 is equipped with a 16MP back-facing camera as well as a 3.7MP front-facing camera for video conferencing applications. With these built-in cameras, we can record high-definition videos with exceptional image quality in both outdoor and indoor environments. The ubiquity of these imaging sensors, as well as the increasing computational capability of mobile processors, now enable us to develop much more interactive applications such as real-time tracking of objects or faces.</p><p>By combining OpenGL with the OpenCV library, we can create interactive applications that perform real-time video processing of the real world to register and augment 3D virtual information onto real-world objects. Since both libraries are hardware-accelerated (GPU and CPU optimized), it is important that we explore the use of these libraries to obtain real-time performance.</p><p>In the previous section, we introduced the framework that provides access to the live camera feed. Here, we will create a full demo that displays real-time video using OpenGL-based texture mapping techniques (similar to those introduced in <a class="link" href="ch04.html" title="Chapter 4. Rendering 2D Images and Videos with Texture Mapping">Chapter 4</a>, <em>Rendering 2D Images and Videos with Texture Mapping</em> to <a class="link" href="ch06.html" title="Chapter 6. Rendering Stereoscopic 3D Models using OpenGL">Chapter 6</a>, <em>Rendering Stereoscopic 3D Models using OpenGL</em>, except we will deploy OpenGL ES for mobile platforms), and processes the video stream to perform corner detection using OpenCV. To help readers understand the additional code needed to finalize the demo, here is an overview diagram of the implementation:</p><div><img src="img/9727OS_09_02.jpg" alt="Displaying real-time video using texture mapping"/></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec144"/>Getting ready</h2></div></div></div><p>This demo<a id="id481" class="indexterm"/> requires the completion of <a id="id482" class="indexterm"/>all the <em>Getting ready</em> steps to enable the capture of the real-time video stream using OpenCV on an Android device. The implementation of the shader program and texture mapping code is based on the demos from <a class="link" href="ch08.html" title="Chapter 8. Interactive Real-time Data Visualization on Mobile Devices">Chapter 8</a>, <em>Interactive Real-time Data Visualization on Mobile Devices</em>.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec145"/>How to do it...</h2></div></div></div><p>On the native code side, create two new files called <code class="literal">VideoRenderer.hpp</code> and <code class="literal">VideoRenderer.cpp</code>. These files contain the implementation to render the video using texture mapping. Also, we will import the <code class="literal">Texture.cpp</code> and <code class="literal">Texture.hpp</code> files from the previous chapter to handle texture creation.</p><p>Inside the <code class="literal">VideoRenderer.hpp</code> file, define the <code class="literal">VideoRenderer</code> class as follows (the details of each function will be discussed next):</p><div><pre class="programlisting">#ifndef VIDEORENDERER_H_
#define VIDEORENDERER_H_
//The shader program and basic OpenGL calls
#include &lt;Shader.hpp&gt;
//for texture support
#include &lt;Texture.hpp&gt;
//opencv support
#include &lt;opencv2/core/core.hpp&gt;
#include &lt;opencv2/imgproc/imgproc.hpp&gt;
#include &lt;opencv2/highgui/highgui.hpp&gt;

class VideoRenderer {
  public:
    VideoRenderer();
    virtual ~VideoRenderer();
    //setup all shader program and texture mapping variables
    bool setup();
    bool initTexture(cv::Mat frame);
    //render the frame on screen
    void render(cv::Mat frame);

  private:
    //this handles the generic camera feed view
    GLuint gProgram;
    GLuint gvPositionHandle;
    GLuint vertexUVHandle;
    GLuint textureSamplerID;
    GLuint texture_id;
    Shader shader;
};

#endif /* VIDEORENDERER_H_ */</pre></div><p>Inside<a id="id483" class="indexterm"/> the <code class="literal">VideoRenderer.cpp</code> file, we<a id="id484" class="indexterm"/> implement each of the three key member functions (<code class="literal">setup</code>, <code class="literal">initTexture</code>, and <code class="literal">render</code>). Here is the complete implementation:</p><div><ol class="orderedlist arabic"><li class="listitem">Include the <code class="literal">VideoRenderer.hpp</code> header file, define functions to print debug messages, and define the constructor and destructor:<div><pre class="programlisting">#include "VideoRenderer.hpp"

#define  LOG_TAG    "VideoRenderer"
#define  LOGI(...) __android_log_print(ANDROID_LOG_INFO,LOG_TAG,__VA_ARGS__)
#define  LOGE(...) __android_log_print(ANDROID_LOG_ERROR,LOG_TAG,__VA_ARGS__)

VideoRenderer::VideoRenderer() {
}

VideoRenderer::~VideoRenderer() {
}</pre></div></li><li class="listitem">Define the <a id="id485" class="indexterm"/>vertex and fragment shaders <a id="id486" class="indexterm"/>as well as associated configuration steps (similar to <a class="link" href="ch08.html" title="Chapter 8. Interactive Real-time Data Visualization on Mobile Devices">Chapter 8</a>, <em>Interactive Real-time Data Visualization on Mobile Devices</em>):<div><pre class="programlisting">bool VideoRenderer::setup(){
  // Vertex shader source code
  const char g_vshader_code[] =
  "#version 300 es\n"
  "layout(location = 1) in vec4 vPosition;\n"
  "layout(location = 2) in vec2 vertexUV;\n"
  "out vec2 UV;\n"
  "void main() {\n"
    "  gl_Position = vPosition;\n"
    "  UV=vertexUV;\n"
  "}\n";
  // fragment shader source code
  const char g_fshader_code[] =
  "#version 300 es\n"
  "precision mediump float;\n"
  "out vec4 color;\n"
  "uniform sampler2D textureSampler;\n"
  "in vec2 UV;\n"
  "void main() {\n"
    "  color = vec4(texture(textureSampler, UV).rgb, 1.0);\n"
  "}\n";

  LOGI("setupVideoRenderer");
  gProgram =   shader.createShaderProgram(g_vshader_code, g_fshader_code);
  if (!gProgram) {
    LOGE("Could not create program.");
    return false;
  }

  gvPositionHandle = glGetAttribLocation(gProgram, "vPosition");
  shader.checkGlError("glGetAttribLocation");
  LOGI("glGetAttribLocation(\"vPosition\") = %d\n",
  gvPositionHandle);

  vertexUVHandle = glGetAttribLocation(gProgram, "vertexUV");
  shader.checkGlError("glGetAttribLocation");
  LOGI("glGetAttribLocation(\"vertexUV\") = %d\n",
  vertexUVHandle);

  textureSamplerID = glGetUniformLocation(gProgram, "textureSampler");
  shader.checkGlError("glGetUniformLocation");
  LOGI("glGetUniformLocation(\"textureSampler\") =   %d\n", textureSamplerID);

  return true;
}</pre></div></li><li class="listitem">Initialize and <a id="id487" class="indexterm"/>bind the <a id="id488" class="indexterm"/>texture:<div><pre class="programlisting">bool VideoRenderer::initTexture(cv::Mat frame){
  texture_id = initializeTexture(frame.data, frame.size().width, frame.size().height);
  //binds our texture in Texture Unit 0
  glActiveTexture(GL_TEXTURE0);
  glBindTexture(GL_TEXTURE_2D, texture_id);
  glUniform1i(textureSamplerID, 0);

  return true;
}</pre></div></li><li class="listitem">Render the camera feed on the screen with texture mapping:<div><pre class="programlisting">void VideoRenderer::render(cv::Mat frame){
  //our vertices
  const GLfloat g_vertex_buffer_data[] = {
    1.0f,1.0f,0.0f,
    -1.0f,1.0f,0.0f,
    -1.0f,-1.0f,0.0f,
    1.0f,1.0f
    ,0.0f,
    -1.0f,-1.0f,0.0f,
    1.0f,-1.0f,0.0f
  };
  //UV map for the vertices
  const GLfloat g_uv_buffer_data[] = {
    1.0f, 0.0f,
    0.0f, 0.0f,
    0.0f, 1.0f,
    1.0f, 0.0f,
    0.0f, 1.0f,
    1.0f, 1.0f
  };

  glUseProgram(gProgram);
  shader.checkGlError("glUseProgram");

  glEnableVertexAttribArray(gvPositionHandle);
  shader.checkGlError("glEnableVertexAttribArray");

  glEnableVertexAttribArray(vertexUVHandle);
  shader.checkGlError("glEnableVertexAttribArray");

  glVertexAttribPointer(gvPositionHandle, 3, GL_FLOAT, GL_FALSE, 0, g_vertex_buffer_data);
  shader.checkGlError("glVertexAttribPointer");

  glVertexAttribPointer(vertexUVHandle, 2, GL_FLOAT, GL_FALSE, 0, g_uv_buffer_data);
  shader.checkGlError("glVertexAttribPointer");

  updateTexture(frame.data, frame.size().width, frame.size().height, GL_RGBA);

  //draw the camera feed on the screen
  glDrawArrays(GL_TRIANGLES, 0, 6);
  shader.checkGlError("glDrawArrays");

  glDisableVertexAttribArray(gvPositionHandle);
  glDisableVertexAttribArray(vertexUVHandle);
}</pre></div></li></ol></div><p>To further enhance<a id="id489" class="indexterm"/> the readability of the code, we <a id="id490" class="indexterm"/>encapsulate the handling of the shader program and texture mapping inside <code class="literal">Shader.hpp</code> (<code class="literal">Shader.cpp</code>) and <code class="literal">Texture.hpp</code> (<code class="literal">Texture.cpp</code>), respectively. We will only show the header files here for completeness and refer readers to the code package on the Packt Publishing website for the detailed implementation of each function.</p><p>Here is the <code class="literal">Shader.hpp</code> file:</p><div><pre class="programlisting">#ifndef SHADER_H_
#define SHADER_H_

#define GLM_FORCE_RADIANS
#include &lt;jni.h&gt;
#include &lt;android/log.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;math.h&gt;
#include &lt;GLES3/gl3.h&gt;
#include &lt;glm/glm.hpp&gt;
#include &lt;glm/gtc/matrix_transform.hpp&gt;

class Shader {
  public:
  Shader();
  virtual ~Shader();
  GLuint loadShader(GLenum shader_type, const char*p_source);
  GLuint createShaderProgram(const char*vertex_shader_code, const char*fragment_shader_code);
  void printGLString(const char *name, GLenum s) ;
  void checkGlError(const char* op);
};

#endif /* SHADER_H_ */</pre></div><p>The<a id="id491" class="indexterm"/> <code class="literal">Texture.hpp</code> file<a id="id492" class="indexterm"/> should read:</p><div><pre class="programlisting">#ifndef TEXTURE_HPP
#define TEXTURE_HPP

#include &lt;GLES3/gl3.h&gt;

class Texture {
  public:
    Texture();
    virtual ~Texture();
    GLuint initializeTexture(const unsigned char *image_data, int width, int height);
    void updateTexture(const unsigned char *image_data, int width, int height, GLenum format);
};

#endif</pre></div><p>Finally, we integrate everything inside the <code class="literal">main.cpp</code> file with the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Include all headers. In particular, include <code class="literal">pthread.h</code> to handle synchronization and OpenCV libraries for image processing.<div><pre class="programlisting">...
#include &lt;pthread.h&gt;
#include &lt;Texture.hpp&gt;
#include &lt;Shader.hpp&gt;
#include &lt;VideoRenderer.hpp&gt;

//including opencv headers
#include &lt;opencv2/core/core.hpp&gt;
#include &lt;opencv2/imgproc/imgproc.hpp&gt;
#include &lt;opencv2/highgui/highgui.hpp&gt;
#include &lt;opencv2/features2d/features2d.hpp&gt;
...</pre></div></li><li class="listitem">Define<a id="id493" class="indexterm"/> the <code class="literal">VideoRenderer</code> and <code class="literal">Shader</code> objects, as <a id="id494" class="indexterm"/>well as the <code class="literal">pthread_mutex_t</code> lock variable to handle synchronization for data copying using a mutex lock.<div><pre class="programlisting">//mutex lock for data copying
pthread_mutex_t count_mutex;
...
//pre-set image size.
const int IMAGE_WIDTH = 1280;
const int IMAGE_HEIGHT = 720; 

bool enable_process = true;
//main camera feed from the Java side
cv::Mat frame;
//all shader related code
Shader shader;
//for video rendering
VideoRenderer videorenderer;</pre></div></li><li class="listitem">Set up the <code class="literal">VideoRenderer</code> object in the <code class="literal">setupGraphics</code> function and initialize the texture.<div><pre class="programlisting">bool setupGraphics(int w, int h) {
  ...
  videorenderer.setup();
  //template for the first texture
  cv::Mat frameM(IMAGE_HEIGHT, IMAGE_WIDTH, CV_8UC4, cv::Scalar(0,0,0,255));
  videorenderer.initTexture(frameM);
  frame = frameM;
  ...
  return true;
}</pre></div></li><li class="listitem">Create a <code class="literal">processFrame</code> helper<a id="id495" class="indexterm"/> function to handle <a id="id496" class="indexterm"/>feature extraction with the OpenCV <code class="literal">goodFeaturesToTrack</code> function. The function also draws the result directly on the frame for visualization.<div><pre class="programlisting">void processFrame(cv::Mat *frame_local){
  int maxCorners = 1000;
  if( maxCorners &lt; 1 ) { maxCorners = 1; }
  cv::RNG rng(12345);
  // Parameters for Shi-Tomasi algorithm
  std::vector&lt;cv::Point2f&gt; corners;
  double qualityLevel = 0.05;
  double minDistance = 10;
  int blockSize = 3;
  bool useHarrisDetector = false;
  double k = 0.04;

  // Copy the source image
  cv::Mat src_gray;
  cv::Mat frame_small;
  cv::resize(*frame_local, frame_small, cv::Size(), 0.5, 0.5, CV_INTER_AREA);
  cv::cvtColor(frame_small, src_gray, CV_RGB2GRAY );
  
  // Apply feature extraction
  cv::goodFeaturesToTrack( src_gray, corners, maxCorners, qualityLevel, minDistance, cv::Mat(), blockSize, useHarrisDetector, k );
  
  // Draw corners detected on the image
  int r = 10;
  for( int i = 0; i &lt; corners.size(); i++ )
  {
    cv::circle(*frame_local, 2*corners[i], r, cv::Scalar(rng.uniform(0,255), 
    rng.uniform(0,255), rng.uniform(0,255), 255), -1, 8, 0 );
  }
  //LOGI("Found %d features", corners.size());
}</pre></div></li><li class="listitem">Implement frame<a id="id497" class="indexterm"/> copying with mutex<a id="id498" class="indexterm"/> lock synchronization (to avoid frame corruption due to shared memory and race condition) in the <code class="literal">renderFrame</code> function. Process the frame with the OpenCV library and render the result using OpenGL texture-mapping techniques.<div><pre class="programlisting">void renderFrame() {
  shader.checkGlError("glClearColor");
  glClearColor(0.0f, 0.0f, 0.0f, 0.0f);
  glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
  
  shader.checkGlError("glClear");
  
  pthread_mutex_lock(&amp;count_mutex);
  cv::Mat frame_local = frame.clone();
  pthread_mutex_unlock(&amp;count_mutex);

  if(enable_process)
    processFrame(&amp;frame_local);
  
  //render the video feed on screen
  videorenderer.render(frame_local);
  //LOGI("Rendering OpenGL Graphics");
}</pre></div></li><li class="listitem">Define the JNI prototypes and implement the <code class="literal">setImage</code> function, which receives the raw camera image data from the Java side using a mutex lock to ensure data copying is protected. Also, implement the <code class="literal">toggleFeatures</code> function to turn feature tracking on and off upon touching the screen.<div><pre class="programlisting">extern "C" {
..
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_setImage(JNIEnv * jenv, jobject, jlong imageRGBA);
  //toggle features
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_toggleFeatures(JNIEnv * jenv, jobject);
};

JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_toggleFeatures(JNIEnv * env, jobject obj){
  //toggle the processing on/off 
  enable_process = !enable_process;
}
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_setImage(
  JNIEnv * jenv, jobject, jlong imageRGBA) {
  cv::Mat* image = (cv::Mat*) imageRGBA;
  /* use mutex lock to ensure the write/read operations are synced (to avoid corrupting the frame) */
  pthread_mutex_lock(&amp;count_mutex);
  frame = image-&gt;clone();
  pthread_mutex_unlock(&amp;count_mutex);
  //LOGI("Got Image: %dx%d\n", frame.rows, frame.cols);
}</pre></div><div><img src="img/9727OS_09_03.jpg" alt="How to do it..."/></div></li></ol></div><p>The <a id="id499" class="indexterm"/>resulting<a id="id500" class="indexterm"/> image is a post-processed frame from OpenCV. In addition to displaying the raw video frame, we demonstrate that our implementation can easily be extended to support real-time video processing with OpenCV. The <code class="literal">processFrame</code> function uses the OpenCV <code class="literal">goodFeaturesToTrack</code> corner detection function and we overlay all corners extracted from the scene on the image.</p><p>Image features are the fundamental elements for many tracking algorithms such as <strong>Simultaneous localization and Mapping</strong> (<strong>SLAM</strong>)<a id="id501" class="indexterm"/> as well as recognition algorithms such as<a id="id502" class="indexterm"/> image-based matching. For example, with the <a id="id503" class="indexterm"/>SLAM algorithm, we can construct a map of the environment and, at the same time, keep track of the position of the device in space. Such techniques are particularly useful in AR applications as we always need to align the virtual world with the real world. Next, we can see a feature extraction algorithm (corner detection) running in real-time on a mobile phone.</p><div><img src="img/9727OS_09_04.jpg" alt="How to do it..."/></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec146"/>How it works...</h2></div></div></div><p>The <code class="literal">VideoRenderer</code> class has two primary functions:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Creating the shader <a id="id504" class="indexterm"/>program that handles texture mapping (<code class="literal">Shader.cpp</code> and <code class="literal">Texture.cpp</code>).</li><li class="listitem" style="list-style-type: disc">Updating the texture memory with the OpenCV raw camera frame. Each time a new frame is retrieved from OpenCV, we call the render function, which updates the texture memory and also draws the frame on the screen.</li></ul></div><p>The <code class="literal">main.cpp</code> file<a id="id505" class="indexterm"/> connects all the components of the <a id="id506" class="indexterm"/>implementation, and encapsulates all the logics for the interaction. It interfaces with the Java side (for example, <code class="literal">setImage</code>) and we offload all computationally intensive tasks to the C++ native side. For example, the <code class="literal">processFrame</code> function handles the OpenCV video processing pipeline, and we can efficiently handle memory I/O and parallelization. On the other hand, the <code class="literal">VideoRenderer</code> class accelerates rendering with OpenGL for real-time performance on the mobile platform.</p><p>One may notice that the implementations of OpenGL and OpenCV on Android are mostly identical to the desktop version. That's the key reason why we employ such cross-platform languages as we can easily extend our code to any future platform with minimal effort.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec147"/>See also</h2></div></div></div><p>On a mobile platform, computational resources<a id="id507" class="indexterm"/> are particularly limited and thus it is important to optimize the use of all available hardware resources. With OpenGL-based hardware acceleration, we can reduce most of our overhead in rendering graphics in 2D and 3D on the graphics processor. In the near future, especially with the emergence of mobile processors supporting<a id="id508" class="indexterm"/> GPGPU (for example, Nvidia's K1 mobile processor), we will enable more parallelized processing for computer vision algorithms and offer real-time performance for many applications on a mobile device. For example, Nvidia<a id="id509" class="indexterm"/> now officially supports CUDA<a id="id510" class="indexterm"/> for all its upcoming mobile processors, so we will see many more real-time image processing, machine learning (such as deep learning algorithms), and high-performance graphics emerging on the mobile platform. See the <a id="id511" class="indexterm"/>following website for more information: <a class="ulink" href="https://developer.nvidia.com/embedded-computing">https://developer.nvidia.com/embedded-computing</a>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec57"/>Augmented reality-based data visualization over real-world scenes</h1></div></div></div><p>In our ultimate demo, we <a id="id512" class="indexterm"/>will introduce the basic framework for AR-based data visualization by overlaying 3D data on real-world objects and scenes. We apply the same GPU-accelerated simulation model and register it to the world with a sensor-based tracking approach. The following diagram illustrates the<a id="id513" class="indexterm"/> final architecture of the implementation in this chapter:</p><div><img src="img/9727OS_09_08.jpg" alt="Augmented reality-based data visualization over real-world scenes"/></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec148"/>Getting ready</h2></div></div></div><p>This final demo <a id="id514" class="indexterm"/>integrates together all the concepts previously introduced in this chapter and requires the capture (and possibly processing) of a real-time video stream using OpenCV on an Android-based phone. To reduce the complexity of the code, we have created the Augmented Reality layer (<code class="literal">AROverlayRenderer</code>) and we can improve the registration, alignment, and calibration of the layer with more advanced algorithms in the future.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec149"/>How to do it...</h2></div></div></div><p>Let's define a new class called <code class="literal">AROverlayRenderer</code> inside the <code class="literal">AROverlayRenderer.hpp</code> file:</p><div><pre class="programlisting">#ifndef AROVERLAYRENDERER_H_
#define AROVERLAYRENDERER_H_

#include&lt;Shader.hpp&gt;

class AROverlayRenderer {
  public:
    AROverlayRenderer();
    virtual ~AROverlayRenderer();
    void render();
    bool setup();
    void setScale(float s);

    void setOldRotMatrix(glm::mat4 r_matrix);
    void setRotMatrix(glm::mat4 r_matrix);
    void resetRotMatrix();
    void setScreenSize(int width, int height);
    void setDxDy (float dx, float dy);
  private:
    //this renders the overlay view
    GLuint gProgramOverlay;
    GLuint gvOverlayPositionHandle;
    GLuint gvOverlayColorHandle;
    GLuint matrixHandle;
    GLuint sigmaHandle;
    GLuint scaleHandle;

  //vertices for the grid
  int grid_size;
  GLfloat *gGrid;
  GLfloat sigma;

  //for handling the object rotation from user
  GLfloat dx, dy;
  GLfloat rotX, rotY;

  //the view matrix and projection matrix
  glm::mat4 g_view_matrix;
  glm::mat4 g_projection_matrix;

  //initial position of the camera
  glm::vec3 g_position;
  //FOV of the virtual camera in OpenGL
  float g_initial_fov;

  glm::mat4 rotMatrix;
  glm::mat4 old_rotMatrix;

  float scale;
  int width;
  int height;

  Shader shader;
  void computeProjectionMatrices();
  void computeGrid();
};

#endif /* AROVERLAYRENDERER_H_ */</pre></div><p>Now <a id="id515" class="indexterm"/>implement the <code class="literal">AROverlayRenderer</code> member functions inside the <code class="literal">AROverlayRenderer.cpp</code> file:</p><div><ol class="orderedlist arabic"><li class="listitem">Include the <code class="literal">AROverlayRenderer.hpp</code> header file and define functions to print messages as well as the constructor and destructor:<div><pre class="programlisting">#include "AROverlayRenderer.hpp"

#define  LOG_TAG    "AROverlayRenderer"
#define  LOGI(...) __android_log_print(ANDROID_LOG_INFO,LOG_TAG,__VA_ARGS__)
#define  LOGE(...) __android_log_print(ANDROID_LOG_ERROR,LOG_TAG,__VA_ARGS__)

AROverlayRenderer::AROverlayRenderer() {
  //initial position of the camera
  g_position = glm::vec3( 0.0f, 0.0f, 0.0f );

  //FOV of the virtual camera in OpenGL
  //45 degree FOV
  g_initial_fov = 45.0f*glm::pi&lt;float&gt;()/180.0f;

  /* scale for the panel and other objects, allow for zooming in with pinch. */
  scale = 1.0f;
  dx=0.0f; dy=0.0f;
  rotX=0.0f, rotY=0.0f;
  sigma = 0;

  grid_size = 400;
  //allocate memory for the grid
  gGrid = (GLfloat*) malloc(sizeof(GLfloat)*grid_size*grid_size*3);
}

AROverlayRenderer::~AROverlayRenderer() {
  //delete all dynamically allocated objects here
  free(gGrid);
}</pre></div></li><li class="listitem">Initialize<a id="id516" class="indexterm"/> the grid pattern for the simulation:<div><pre class="programlisting">void AROverlayRenderer::computeGrid(){
  float grid_x = grid_size;
  float grid_y = grid_size;
  unsigned int data_counter = 0;
  //define a grid ranging from -1 to +1
  for(float x = -grid_x/2.0f; x&lt;grid_x/2.0f; x+=1.0f){
    for(float y = -grid_y/2.0f; y&lt;grid_y/2.0f; y+=1.0f){
      float x_data = x/grid_x;
      float y_data = y/grid_y;
      gGrid[data_counter] = x_data;
      gGrid[data_counter+1] = y_data;
      gGrid[data_counter+2] = 0;
      data_counter+=3;
    }
  }
}</pre></div></li><li class="listitem">Set up the<a id="id517" class="indexterm"/> shader program to overlay graphics:<div><pre class="programlisting">bool AROverlayRenderer::setup(){
  // Vertex shader source code
  static const char g_vshader_code_overlay[] =
    "#version 300 es\n"
    "in vec4 vPosition;\n"
    "uniform mat4 MVP;\n"
    "uniform float sigma;\n"
    "uniform float scale;\n"
    "out vec4 color_based_on_position;\n"
    "// Heat map generator                \n"
    "vec4 heatMap(float v, float vmin, float vmax){\n"
    "    float dv;\n"
    "    float r=1.0, g=1.0, b=1.0;\n"
    "  if (v &lt; vmin){\n"
    "    v = vmin;}\n"
    "  if (v &gt; vmax){\n"
    "    v = vmax;}\n"
    "  dv = vmax - vmin;\n"
    "  if (v &lt; (vmin + 0.25 * dv)) {\n"
      "    r = 0.0;\n"
      "    g = 4.0 * (v - vmin) / dv;\n"
    "  } else if (v &lt; (vmin + 0.5 * dv)) {\n"
      "    r = 0.0;\n"
      "    b = 1.0 + 4.0 * (vmin + 0.25 * dv - v) / dv;\n"
    "  } else if (v &lt; (vmin + 0.75 * dv)) {\n"
      "    r = 4.0 * (v - vmin - 0.5 * dv) / dv;\n"
      "    b = 0.0;\n"
    "  } else {\n"
      "    g = 1.0 + 4.0 * (vmin + 0.75 * dv - v) / dv;\n"
      "    b = 0.0;\n"
    "  }\n"
    "    return vec4(r, g, b, 0.1);\n"
  "}\n"
  "void main() {\n"
    "  //Simulation on GPU \n"
    "  float x_data = vPosition.x;\n"
    "  float y_data = vPosition.y;\n"
    "  float sigma2 = sigma*sigma;\n"
    "  float z = exp(-0.5*(x_data*x_data)/(sigma2)-0.5*(y_data*y_data)/(sigma2));\n"
    "  vec4 position = vPosition;\n"
    "  position.z = z*scale;\n"
    "  position.x = position.x*scale;\n"
    "  position.y = position.y*scale;\n"
    "  gl_Position = MVP*position;\n"
    "  color_based_on_position = heatMap(position.z, 0.0, 0.5);\n"
    "  gl_PointSize = 5.0*scale;\n"
  "}\n";

  // fragment shader source code
  static const char g_fshader_code_overlay[] =
    "#version 300 es\n"
      "precision mediump float;\n"
    "in vec4 color_based_on_position;\n"
    "out vec4 color;\n"
    "void main() {\n"
      "  color = color_based_on_position;\n"
    "}\n";

    //setup the shader for the overlay
    gProgramOverlay = shader.createShaderProgram(g_vshader_code_overlay, g_fshader_code_overlay);
  if (!gProgramOverlay) {
    LOGE("Could not create program for overlay.");
    return false;
  }
  //get handlers for the overlay side
  matrixHandle = glGetUniformLocation(gProgramOverlay, "MVP");
  shader.checkGlError("glGetUniformLocation");
  LOGI("glGetUniformLocation(\"MVP\") = %d\n",
      matrixHandle);

  gvOverlayPositionHandle = glGetAttribLocation(gProgramOverlay, "vPosition");
  shader.checkGlError("glGetAttribLocation");
  LOGI("glGetAttribLocation(\"vPosition\") = %d\n",
      gvOverlayPositionHandle);

  sigmaHandle = glGetUniformLocation(gProgramOverlay, "sigma");
  shader.checkGlError("glGetUniformLocation");
  LOGI("glGetUniformLocation(\"sigma\") = %d\n",
      sigmaHandle);

  scaleHandle = glGetUniformLocation(gProgramOverlay, "scale");
  shader.checkGlError("glGetUniformLocation");
  LOGI("glGetUniformLocation(\"scale\") = %d\n",
    scaleHandle);

  computeGrid();
}</pre></div></li><li class="listitem">Create<a id="id518" class="indexterm"/> helper functions to set the scale, screen size, and rotation variables from the touch interface:<div><pre class="programlisting">void AROverlayRenderer::setScale(float s) {
  scale = s;
}

void AROverlayRenderer::setScreenSize(int w, int h) {
  width = w;
  height = h;
}

void AROverlayRenderer::setRotMatrix(glm::mat4 r_matrix){
  rotMatrix= r_matrix;
}

void AROverlayRenderer::setOldRotMatrix(glm::mat4 r_matrix){
  old_rotMatrix = r_matrix;
}

void AROverlayRenderer::resetRotMatrix(){
  old_rotMatrix = rotMatrix;
}

void AROverlayRenderer::setDxDy(float dx, float dy){
  //update the angle of rotation for each
  rotX += dx/width;
  rotY += dy/height;
}</pre></div></li><li class="listitem">Compute the projection and view matrices based on the camera parameters:<div><pre class="programlisting">void AROverlayRenderer::computeProjectionMatrices(){
  //direction vector for z
  glm::vec3 direction_z(0.0, 0.0, -1.0);
  //up vector
  glm::vec3 up = glm::vec3(0.0, -1.0, 0.0);

  float aspect_ratio = (float)width/(float)height;
  float nearZ = 0.01f;
  float farZ = 50.0f;
  float top = tan(g_initial_fov/2*nearZ);
  float right = aspect_ratio*top;
  float left = -right;
  float bottom = -top;
  g_projection_matrix = glm::frustum(left, right, bottom, top, nearZ, farZ);

  g_view_matrix = glm::lookAt(
    g_position,           // camera position
    g_position+direction_z, //viewing direction 
    up                  // up direction
  );
}</pre></div></li><li class="listitem">Render the <a id="id519" class="indexterm"/>graphics on the screen:<div><pre class="programlisting">void AROverlayRenderer::render(){
  //update the variables for animations
  sigma+=0.002f;
  if(sigma&gt;0.5f){
    sigma = 0.002f;
  }
  glUseProgram(gProgramOverlay);
  /* Retrieve the View and Model matrices and apply them to the rendering */
  computeProjectionMatrices();
  glm::mat4 projection_matrix = g_projection_matrix;
  glm::mat4 view_matrix = g_view_matrix;
  glm::mat4 model_matrix = glm::mat4(1.0);

  model_matrix = glm::translate(model_matrix, glm::vec3(0.0f, 0.0f, scale-5.0f));
  //X,Y reversed for the screen orientation
  model_matrix = glm::rotate(model_matrix, rotY*glm::pi&lt;float&gt;(), glm::vec3(-1.0f, 0.0f, 0.0f));
  model_matrix = glm::rotate(model_matrix, rotX*glm::pi&lt;float&gt;(), glm::vec3(0.0f, -1.0f, 0.0f));
  model_matrix = glm::rotate(model_matrix, 90.0f*glm::pi&lt;float&gt;()/180.0f, glm::vec3(0.0f, 0.0f, 1.0f));
  /* the inverse of rotational matrix is to counter-  rotate the graphics to the center. This allows us to reset the camera orientation since R*inv(R) = I. */
  view_matrix = rotMatrix*glm::inverse(old_rotMatrix)*view_matrix;

  //create the MVP (model view projection) matrix
  glm::mat4 mvp = projection_matrix * view_matrix * model_matrix;
  glUniformMatrix4fv(matrixHandle, 1, GL_FALSE, &amp;mvp[0][0]);
  shader.checkGlError("glUniformMatrix4fv");
  glEnableVertexAttribArray(gvOverlayPositionHandle);
  shader.checkGlError("glEnableVertexAttribArray");
  glVertexAttribPointer(gvOverlayPositionHandle, 3, GL_FLOAT, GL_FALSE, 0, gGrid);
  shader.checkGlError("glVertexAttribPointer");
  glUniform1f(sigmaHandle, sigma);
  shader.checkGlError("glUniform1f");

  glUniform1f(scaleHandle, 1.0f);
  shader.checkGlError("glUniform1f");

  //draw the overlay graphics
  glDrawArrays(GL_POINTS, 0, grid_size*grid_size);
  shader.checkGlError("glDrawArrays");
  glDisableVertexAttribArray(gvOverlayPositionHandle);
}</pre></div></li><li class="listitem">Finally, we <a id="id520" class="indexterm"/>only need to make minor modifications to the <code class="literal">main.cpp</code> file used in the previous demo to enable the AR overlay on top of the real-time video stream (real-world scene). Only the relevant code snippets that highlight the required modifications are shown here (download the complete code from the Packt Publishing website):<div><pre class="programlisting">...
#include &lt;AROverlayRenderer.hpp&gt;
...
AROverlayRenderer aroverlayrenderer;
...
bool setupGraphics(int w, int h) {
  ...
  videorenderer.setup();
  aroverlayrenderer.setup();
  ...
  videorenderer.initTexture(frame);
  aroverlayrenderer.setScreenSize(width, height);
}

void renderFrame() {
  ...
  videorenderer.render(frame);
  aroverlayrenderer.render();
}
...
extern "C" {
  ...
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_setScale(JNIEnv * env, jobject obj,  jfloat jscale);
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_resetRotDataOffset(JNIEnv * env, jobject obj);
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_setRotMatrix (JNIEnv *env, jobject obj, jfloatArray ptr);
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_setDxDy(JNIEnv *env, jobject obj,  jfloat dx,  jfloat dy);
};
...
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_resetRotDataOffset (JNIEnv * env, jobject obj){
  aroverlayrenderer.resetRotMatrix();
}
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_setScale (JNIEnv * env, jobject obj, jfloat jscale)
{
  aroverlayrenderer.setScale(jscale);
  LOGI("Scale is %lf", scale);
}
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_resetRotDataOffset (JNIEnv * env, jobject obj){
  aroverlayrenderer.resetRotMatrix();
}
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_setRotMatrix (JNIEnv *env, jobject obj, jfloatArray ptr) {
  jsize len = env-&gt;GetArrayLength(ptr);
  jfloat *body = env-&gt;GetFloatArrayElements(ptr,0);
  //should be 16 elements from the rotation matrix
  glm::mat4 rotMatrix(1.0f);
  int count = 0;
  for(int i = 0; i&lt;4; i++){
    for(int j=0; j&lt;4; j++){
      rotMatrix[i][j] = body[count];
      count++;
    }
  }
  env-&gt;ReleaseFloatArrayElements(ptr, body, 0);
  aroverlayrenderer.setRotMatrix(rotMatrix);
}
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_setDxDy(JNIEnv * env, jobject obj, jfloat dx, jfloat dy){
  aroverlayrenderer.setDxDy(dx, dy);
}</pre></div></li></ol></div><p>With this framework, one<a id="id521" class="indexterm"/> can overlay virtually any dataset on different real-world objects or surfaces and enable truly interactive applications, using the built-in sensors and gesture interface on mobile devices and emerging state-of-the-art wearable AR eyeglasses. Following are the results demonstrating a real-time, interactive, AR-based visualization of a 3-D dataset (in this case, a Gaussian distribution) overlaid on real-world scenes:</p><div><img src="img/9727OS_09_05.jpg" alt="How to do it..."/></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec150"/>How it works...</h2></div></div></div><p>The key element for <a id="id522" class="indexterm"/>enabling an AR application is the ability is overlay information onto the real world. The <code class="literal">AROverlayRenderer</code> class implements the core functions essential to all AR applications. First, we create a virtual camera that matches the parameters of the actual camera on the mobile phone. Parameters such as the <strong>field of view</strong> (<strong>FOV</strong>)<a id="id523" class="indexterm"/> and aspect ratio of the camera are currently hard-coded, but we can easily modify them in the <code class="literal">computeProjectionMatrices</code> function. Then, to perform the registration between the real world and virtual world, we control the orientation of the virtual camera based on the orientation of the device. The orientation values are fed through the rotation matrix passed from the Java side (the <code class="literal">setRotMatrix</code> function) and we apply this directly to the OpenGL camera view matrix (<code class="literal">view_matrix</code>). Also, we use the multi-touch interface of the mobile phone to reset the default orientation of the rotation matrix. This is achieved by storing the rotational matrix value upon the touch event (the <code class="literal">resetRotDataOffset</code> function) and we apply the inverse to the rotational matrix to the view matrix (this is equivalent to rotating the camera in the opposite direction).</p><p>In terms of user interaction, we have enabled the pinch and drag option to support dynamic interaction with the virtual object. Upon the pinch event, we take the scale factor and we position the rendered object at a farther distance by applying the <code class="literal">glm::translate</code> function on the <code class="literal">model_matrix</code> variable. In addition, we rotate the virtual object by capturing the dragging action from the Java side (the <code class="literal">setDxDy</code> function). The user can control the orientation of the virtual object by dragging a finger across the screen. Together, these multi-touch gestures enable a highly interactive application interface that allows users to change the perspective of the rendered object intuitively.</p><p>Due to the underlying complexity of the calibration process, we will not cover these details here. However, advanced users may consult the following <a id="id524" class="indexterm"/>website for a more in-depth discussion: <a class="ulink" href="http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html">http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html</a>.</p><p>Also, the current registration process is purely based on the IMU, and it does not support translation (that is, the virtual object does not move exactly with the real world). To address this, we can apply various image-processing techniques such as mean shift tracking, feature-based tracking, and marker-based tracking to recover the full 6 DOF (degree of freedom) model of the camera. SLAM, for example, is a great candidate to recover the 6 DOF camera model, but its detailed implementation is beyond the scope of this chapter.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec151"/>See also</h2></div></div></div><p>Indeed, in this chapter, we have only covered the fundamentals of AR. The field of AR is becoming an increasingly hot topic in both academia and industry. If you are interested in implementing AR data visualization applications on the latest wearable computing platforms (such as the one provided by Meta that features 3D gesture input and 3D stereoscopic output), visit the following websites:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://www.getameta.com/">https://www.getameta.com/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://www.eyetap.org/publications/">http://www.eyetap.org/publications/</a></li></ul></div><p>For further technical details on AR eyeglasses<a id="id525" class="indexterm"/>, please consult the following publications:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Raymond Lo, Alexander Chen, Valmiki Rampersad, Jason Huang, Han Wu, Steve Mann (2013). "Augmediated reality system based on 3D camera selfgesture sensing," IEEE International Symposium on Technology and Society (ISTAS) 2013, pp. 20-31.</li><li class="listitem" style="list-style-type: disc">Raymond Lo, Valmiki Rampersad, Jason Huang, Steve Mann (2013). "Three Dimensional High Dynamic Range Veillance for 3D Range-Sensing Cameras," IEEE International Symposium on Technology and Society (ISTAS) 2013, pp. 255-265.</li><li class="listitem" style="list-style-type: disc">Raymond Chun Hing Lo, Steve Mann, Jason Huang, Valmiki Rampersad, and Tao Ai. 2012. "High Dynamic Range (HDR) Video Image Processing For Digital Glass." In Proceedings of the 20th ACM international conference on Multimedia (MM '12). ACM, New York, NY, USA, pp. 1477-1480.</li><li class="listitem" style="list-style-type: disc">Steve Mann, Raymond Lo, Jason Huang, Valmiki Rampersad, Ryan Janzen, Tao Ai (2012). "HDRchitecture: Real-Time stereoscopic HDR Imaging for Extreme Dynamic Range," In ACM SIGGRAPH 2012 Emerging Technologies (SIGGRAPH '12).</li></ul></div></div></div></body></html>