<html><head></head><body>
        

            
                <h1 class="header-title">Intel Edison and Security System</h1>
            

            
                
<p>In previous chapters, we learned how we can use the Intel Edison to develop applications related to IoT where we displayed live sensor data and also controlled the Edison itself. We also learned the development of an Android and a WPF app that was used to control the Intel Edison. Well, this chapter is more on the local front of the Intel Edison where we are going to use the built-in features of the device. This chapter is concentrated mainly on two key points:</p>
<ul>
<li>Speech and voice processing with the Intel Edison</li>
<li>Image processing with the Intel Edison</li>
</ul>
<p>All the codes are to be written in Python, so some parts of the chapter will concentrate on Python programming as well. In this chapter, we'll operate the Intel Edison using voice commands and then ultimately detect faces using the Intel Edison and a webcam. This chapter will thus explore the core capabilities of the Intel Edison. Since most of the code is in Python, it is advisable to download Python for your PC from the following website:</p>
<p><a href="https://www.python.org/downloads/">https://www.python.org/downloads/</a></p>
<p>This chapter will be divided into two parts. The first part will concentrate on only speech or voice processing and we'll do a mini-project based on that while the second part, which will be a bit lengthy, will concentrate on the image processing aspect of it using OpenCV.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Speech/voice processing using Edison</h1>
            

            
                
<p>Speech processing typically refers to the various mathematical techniques that are applied on an audio signal to process it. It may be some simple mathematical operation or some complex operation. It's a special case of digital signal processing. However, we are not typically dealing with speech processing as a whole entity. We are interested only in a specific area of speech to text conversion. It is to be noted that everything in this chapter is to be performed by the Edison itself without accessing any cloud services. The scenario that this chapter will tackle initially is that we'll make the Edison perform some tasks based on our voice commands. We'll be using a lightweight speech processing tool, but before we proceed further with all the code and circuits, make sure you have the following devices with you. Initially, we'll walk you through switching an LED on and off. Next we'll deal with controlling a servo motor using voice commands.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Devices required</h1>
            

            
                
<p>Along with the Intel Edison, we need a couple of more devices, as listed here:</p>
<ul>
<li>Power adapter of 9V-1 A for the Intel Edison</li>
<li>USB sound card</li>
<li>USB hub, preferably powered</li>
</ul>
<p>This project will use the Edison on external power and the USB port will be used for the sound card. A non-powered USB hub also works, but because of the current it's recommended to use a powered USB hub.</p>
<p>Make sure that the USB sound card is supported on a Linux environment. The selector switch should be towards the USB port. That is because the Edison will be powered through the DC adapter and we need power in the USB port that is activated only when we provide DC power.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Speech processing library</h1>
            

            
                
<p>For this project we are going to use PocketSphinx. It's a lightweight version of CMU Sphinx, a project created by Carnegie Mellon University. It's a lightweight speech recognition engine meant for mobile and handheld devices and wearables. The greatest advantage of using this over any cloud-based service is that it is available offline.</p>
<p>More information about PocketSphinx can be accessed from the following links:</p>
<p><a href="http://cmusphinx.sourceforge.net/wiki/develop">http://cmusphinx.sourceforge.net/wiki/develop</a></p>
<p><a href="https://github.com/cmusphinx/pocketsphinx">https://github.com/cmusphinx/pocketsphinx</a></p>
<p>Setting up the library will be discussed in a later section of this chapter.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Initial configuration</h1>
            

            
                
<p>In the first chapter, we performed some very basic configuration for the Intel Edison. Here we need to configure our device with the required libraries and sound setup with the sound card. For this you need to connect the Intel Edison to only one micro USB port. This will be used to communicate using the PuTTY console and transfer files using the FileZilla FTP client:</p>
<div><img class=" image-border" height="384" src="img/6639_04_01.jpg" width="677"/></div>
<p>Arduino expansion board components</p>
<p>Connect the Intel Edison to the Micro B USB for serial interface to your PC.</p>
<p>Some of the steps were covered in <a href="c225d705-919a-4442-adc8-7b22d33437fc.xhtml" target="_blank">Chapter 1</a>, <em>Setting up Intel Edison;</em> however, we'll show all the steps from the beginning. Open your PuTTY console and log in to your device. Use the <kbd>configure_edison -wifi</kbd> to connect to your Wi-Fi network.</p>
<p>Initially, we'll add AlexT's unofficial <kbd>opkg</kbd> repository. To add this, edit the <kbd>/etc/opkg/base-feeds.conf</kbd> file.</p>
<p>Add these lines to the preceding file:</p>
<pre>
<strong>src/gz all http://repo.opkg.net/edison/repo/all</strong><br/><strong>src/gz edison http://repo.opkg.net/edison/repo/edison</strong><br/><strong>src/gz core2-32 http://repo.opkg.net/edison/repo/core2-32</strong>  
</pre>
<p>To do that, execute the following command:</p>
<pre>
<strong>echo "src/gz all http://repo.opkg.net/edison/repo/all</strong><br/><strong>src/gz edison http://repo.opkg.net/edison/repo/edison</strong><br/><strong>src/gz core2-32 http://repo.opkg.net/edison/repo/core2-32" &gt;&gt; /etc/opkg/base-feeds.conf         </strong>
</pre>
<p>Update the package manager:</p>
<pre>
<strong>opkg update</strong>  
</pre>
<p>Install <kbd>git</kbd> using the package manager:</p>
<pre>
<strong>opkg install git</strong>  
</pre>
<p>We will now install Edison helper scripts to simplify things a bit:</p>
<ol>
<li>First <kbd>clone</kbd> the package:</li>
</ol>
<pre>
<strong>      git clone https://github.com/drejkim/edison-scripts.git ~/edison<br/>      scripts</strong>
</pre>
<ol start="2">
<li>Now we have to add <kbd>~/edison-scripts</kbd> to the path:</li>
</ol>
<pre>
<strong>      echo'export PATH=$PATH:~/edison-scripts'&gt;&gt;~/.profile</strong><br/><strong>      source~/.profile</strong>  
</pre>
<ol start="3">
<li>Next we will run the following scripts:</li>
</ol>
<pre>
<strong>      # Resize /boot -- we need the extra space to add an additional<br/>      kernel<br/><br/></strong><strong>          resizeBoot.sh</strong><br/><br/><strong><br/>      # Install pip, Python's package manager<br/><br/></strong><strong>          installPip.sh</strong><br/><br/><strong><br/>      # Install MRAA, the low level skeleton library for IO<br/>      communication on, Edison, and other platforms<br/><br/></strong><strong>          installMraa.sh</strong>
</pre>
<p style="padding-left: 90px">The initial configuration is done. Now we'll configure the Edison for sound.</p>
<ol start="4">
<li>Now <kbd>install</kbd> the modules for USB devices, including USB webcams, microphone, and speakers. Make sure that your sound card is connected to the Intel Edison:</li>
</ol>
<pre>
      <strong>opkg install kernel-modules</strong>
</pre>
<ol start="5">
<li>The next target is to check whether the USB device is getting detected or not. To check that, type the <kbd>lsusb</kbd> command:</li>
</ol>
<div><img class=" image-border" height="144" src="img/6639_04_02.jpg" width="504"/></div>
<p>USB sound card</p>
<p>The device that is connected to the Intel Edison is shown in the preceding screenshot. It is highlighted in the box. Once we get the device that is connected to the Edison, we can proceed further.</p>
<p>Now we'll check whether <kbd>alsa</kbd> is able to detect the sound card or not. Type in the following command:</p>
<pre>
<strong>aplay -Ll</strong>  
</pre>
<div><img class=" image-border" height="367" src="img/6639_04_03.png" width="572"/></div>
<p>Alsa device check</p>
<p>It is noted that our device is getting detected as card 2, named as <kbd>Device</kbd>.</p>
<p>Now we have to create a <kbd>~/.asoundrc</kbd> file where we need to add the following line. Please note that <kbd>Device</kbd> must be replaced with the device name that is detected on your system:</p>
<pre>
<strong>pcm.!default sysdefault:Device</strong>
</pre>
<p>Now, once this is done, exit and save the file. Next, to test whether everything is working or not, execute the following command and you must hear something on the headphone connected:</p>
<pre>
<strong>aplay /usr/share/sounds/alsa/Front_Center.wav</strong>
</pre>
<p>You should hear the words <kbd>Front Center</kbd>.</p>
<p>Now, our target is to record something and interpret the result. So let's test whether recording is working or not.</p>
<p>To record a clip, type in the following command:</p>
<pre>
<strong>arecord ~/test.wav</strong>
</pre>
<p>Press <em>Ctrl </em>+ <em>c</em> to stop recording. To play the preceding recording, type the following:</p>
<pre>
<strong>aplay ~/test.wav</strong>
</pre>
<p>You must hear what you have recorded. If you are not able to hear the sound, type <kbd>alsamixer</kbd> and adjust the playback and record volumes. Initially, you need to select the device:</p>
<div><img class=" image-border" src="img/6639_04_04.png"/></div>
<p>Alsamixer—1</p>
<p>Next, adjust the volume using the arrow keys:</p>
<div><img class=" image-border" height="340" src="img/6639_04_05.png" width="570"/></div>
<p>Alsamixer—2</p>
<p>Now everything related to sound is set up. The next aim is to <kbd>install</kbd> the packages for speech recognition.</p>
<p>Initially, use Python's <kbd>pip</kbd> to <kbd>install cython</kbd>:</p>
<pre>
<strong>pip install cython</strong>
</pre>
<p>The preceding package takes a lot of time to install. Once that's done, there are some shell scripts that are required to be executed. I have created a GitHub repository for this that contains the required files and the code. Use the git command to clone the repository (<a href="https://github.com/avirup171/Voice-Recognition-using-Intel-Edison.git">https://github.com/avirup171/Voice-Recognition-using-Intel-Edison.git</a>):</p>
<pre>
 <strong>git clone </strong>
</pre>
<p>Next in the bin folder, you will find the packages. Before typing the commands to execute those shell scripts, we need to provide permissions. Type the following command to add permissions:</p>
<pre>
<strong>chmod +x &lt;FILE_NAME&gt;</strong>
</pre>
<p>Next type the filename to execute them. Installation of the packages may take a bit of time:</p>
<pre>
<strong>./installSphinxbase.sh</strong>
</pre>
<p>Next type these for adding to the path:</p>
<pre>
<strong>echo 'export LD_LIBRARY_PATH=/usr/local/lib' &gt;&gt; ~/.profile</strong><br/><strong>echo 'export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig' &gt;&gt; ~/.profile</strong><br/><strong>source ~/.profile</strong>
</pre>
<p>Next install <kbd>Pocketsphinx</kbd>:</p>
<pre>
<strong>./installPocketsphinx.sh</strong>
</pre>
<p>Finally, install <kbd>PyAudio</kbd>:</p>
<pre>
<strong>./installPyAudio.sh</strong>
</pre>
<p>After this step, all the configurations are set up and we are good to go with the coding. PocketSphinx works with some specific sets of commands. We need to create a language mode and a dictionary for the words to be used. We'll do that using the Sphinx knowledge base tool:</p>
<p><a href="http://www.speech.cs.cmu.edu/tools/lmtool-new.html">http://www.speech.cs.cmu.edu/tools/lmtool-new.html</a></p>
<p>Upload a text file containing the set of commands that we want the engine to decode. Then click on COMPILE KNOWLEDGE BASE. Download the <kbd>.tgz</kbd> file that contains the necessary files that are required. Once we have those files, copy it to the Edison using FileZilla. Note the names of the files that contain the following extension. Ideally each file should have the same name:</p>
<ul>
<li><kbd>.dic</kbd></li>
<li><kbd>.lm</kbd></li>
</ul>
<p>Move the entire set to the Edison.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Writing the code</h1>
            

            
                
<p><strong>Problem statement</strong>: To turn on and off an LED using voice commands such as <kbd>ON</kbd> and <kbd>OFF</kbd>.</p>
<p>Before writing the code, let us discuss the algorithm first. Please note that I am writing the algorithm in plain text so that it is easier for the reader to understand.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Let's start with the algorithm</h1>
            

            
                
<p>Perform the following steps to begin with the algorithm:</p>
<ol>
<li>Import all the necessary packages.</li>
<li>Set the LED pin.</li>
<li>Start an infinite loop. From here on, all the parts or blocks will be inside the while loop.</li>
<li>Store two variables in the path for the <kbd>.lm</kbd> and <kbd>.dic</kbd> files.</li>
<li>Record and save a <kbd>.wav</kbd> file for <kbd>3</kbd> seconds.</li>
<li>Pass the <kbd>.wav</kbd> file as a parameter to the speech recognition engine.</li>
<li>Get the resultant text.</li>
<li>With an <kbd>if else</kbd> block test for the <kbd>ON</kbd> and <kbd>OFF</kbd> texts and use the <kbd>mraa</kbd> library to turn on and off an LED.</li>
</ol>
<p>The algorithm is pretty much straightforward. Compare the following code with the preceding algorithm to get a full grip of it:</p>
<pre>
import collections <br/>import mraa <br/>import os <br/>import sys <br/>import time <br/><br/># Import things for pocketsphinx <br/>import pyaudio <br/>import wave <br/>import pocketsphinx as ps <br/>import sphinxbase <br/><br/>led = mraa.Gpio(13)   <br/>led.dir(mraa.DIR_OUT) <br/><br/><br/>print("Starting") <br/>while 1: <br/>         #PocketSphinx parameters <br/>         LMD   = "/home/root/vcreg/5608.lm" <br/>         DICTD = "/home/root/vcreg/5608.dic" <br/>         CHUNK = 1024 <br/>         FORMAT = pyaudio.paInt16 <br/>         CHANNELS = 1 <br/>         RATE = 16000 <br/>         RECORD_SECONDS = 3 <br/>         PATH = 'vcreg' <br/>         p = pyaudio.PyAudio() <br/>         speech_rec = ps.Decoder(lm=LMD, dict=DICTD) <br/>         #Record audio <br/>         stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,<br/>         input=True, frames_per_buffer=CHUNK) <br/>         print("* recording") <br/>         frames = [] <br/>         fori in range(0, int(RATE / CHUNK * RECORD_SECONDS)): <br/>               data = stream.read(CHUNK) <br/>               frames.append(data) <br/>         print("* done recording") <br/>         stream.stop_stream() <br/>         stream.close() <br/>         p.terminate() <br/>         # Write .wav file <br/>         fn = "test.wav" <br/>         #wf = wave.open(os.path.join(PATH, fn), 'wb') <br/>         wf = wave.open(fn, 'wb') <br/>         wf.setnchannels(CHANNELS) <br/>         wf.setsampwidth(p.get_sample_size(FORMAT)) <br/>         wf.setframerate(RATE) <br/>         wf.writeframes(b''.join(frames)) <br/>         wf.close() <br/><br/>         # Decode speech <br/>         #wav_file = os.path.join(PATH, fn) <br/>         wav_file=fn <br/>         wav_file = file(wav_file,'rb') <br/>         wav_file.seek(44) <br/>         speech_rec.decode_raw(wav_file) <br/>         result = speech_rec.get_hyp() <br/>         recognised= result[0] <br/>         print("* LED section begins") <br/>         print(recognised) <br/>         if recognised == 'ON.': <br/>               led.write(1) <br/>         else: <br/>               led.write(0) <br/>         cm = 'espeak "'+recognised+'"' <br/>         os.system(cm) 
</pre>
<p>Let's go line by line:</p>
<pre>
import collections <br/>import mraa <br/>import os <br/>import sys <br/>import time <br/><br/># Import things for pocketsphinx <br/>import pyaudio <br/>import wave <br/>import pocketsphinx as ps <br/>import Sphinxbase 
</pre>
<p>The preceding segment is just to <kbd>import</kbd> all the libraries and packages:</p>
<pre>
led = mraa.Gpio(13)   <br/>led.dir(mraa.DIR_OUT) 
</pre>
<p>We set the LED pin and set its direction as the output. Next we will begin the infinite while loop:</p>
<pre>
#PocketSphinx and Audio recording parameters <br/>         LMD   = "/home/root/vcreg/5608.lm" <br/>         DICTD = "/home/root/vcreg/5608.dic" <br/>         CHUNK = 1024 <br/>         FORMAT = pyaudio.paInt16 <br/>         CHANNELS = 1 <br/>         RATE = 16000 <br/>         RECORD_SECONDS = 3 <br/>         PATH = 'vcreg' <br/>         p = pyaudio.PyAudio() <br/>         speech_rec = ps.Decoder(lm=LMD, dict=DICTD) 
</pre>
<p>The preceding chunk of code is just the parameters for PocketSphinx and for audio recording. We will be recording for <kbd>3</kbd> seconds. We have also provided the path for the <kbd>.lmd</kbd> and <kbd>.dic</kbd> files and some other audio recording parameters:</p>
<pre>
#Record audio <br/>         stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,<br/>         input=True, frames_per_buffer=CHUNK) <br/>         print("* recording") <br/>         frames = [] <br/>         fori in range(0, int(RATE / CHUNK * RECORD_SECONDS)): <br/>               data = stream.read(CHUNK) <br/>               frames.append(data) <br/>         print("* done recording") <br/>         stream.stop_stream() <br/>         stream.close() <br/>         p.terminate() 
</pre>
<p>In the preceding code, we record the audio for the specific time interval.</p>
<p>Next, we save it as a <kbd>.wav</kbd> file:</p>
<pre>
# Write .wav file <br/>         fn = "test.wav" <br/>         #wf = wave.open(os.path.join(PATH, fn), 'wb') <br/>         wf = wave.open(fn, 'wb') <br/>         wf.setnchannels(CHANNELS) <br/>         wf.setsampwidth(p.get_sample_size(FORMAT)) <br/>         wf.setframerate(RATE) <br/>         wf.writeframes(b''.join(frames)) <br/>         wf.close() 
</pre>
<p>The final step contains the decoding of the file and comparing it to affect the LED:</p>
<pre>
# Decode speech <br/>         #wav_file = os.path.join(PATH, fn) <br/>         wav_file=fn <br/>         wav_file = file(wav_file,'rb') <br/>         wav_file.seek(44) <br/>         speech_rec.decode_raw(wav_file) <br/>         result = speech_rec.get_hyp() <br/>         recognised= result[0] <br/>         print("* LED section begins") <br/>         print(recognised) <br/>         if recognised == 'ON.': <br/>               led.write(1) <br/>         else: <br/>               led.write(0) <br/>         cm = 'espeak "'+recognised+'"' <br/>         os.system(cm) 
</pre>
<p>In the preceding code, we initially pass the <kbd>.wav</kbd> file as a parameter to the speech processing engine and then use the result to compare the output. Finally, we switch on and off the LEDs based on the output of the speech processing engine. Another activity carried out by the preceding code is that whatever is recognized is spoken back using <kbd>espeak</kbd>. <kbd>espeak</kbd> is a text to speech engine. It uses spectral formant synthesis by default, which sounds robotic, but can be configured to use Klatt formant synthesis or MBROLA to give it a more natural sound.</p>
<p>Transfer the code to your device using FileZilla. Let's assume that the code is saved by the file named <kbd>VoiceRecognitionTest.py</kbd>.</p>
<p>Before executing the code, you may want to attach an LED to GPIO pin 13 or just use the on board LED for the purpose.</p>
<p>To execute the code, type the following:</p>
<pre>
<strong>python VoiceRecognitionTest.py</strong> 
</pre>
<p>Initially, the console says <kbd>*recording</kbd>, speak <kbd>on</kbd>:</p>
<div><img class=" image-border" src="img/image_04_009.png"/></div>
<p>Voice recognition—1</p>
<p>Then, after you speak, the speech recognition engine will recognize the word that you spoke from the existing language model:</p>
<div><img class=" image-border" src="img/image_04_010.png"/></div>
<p>Voice recognition—2</p>
<p>It is noted that on is displayed. That means that the speech recognition engine has successfully decoded the speech we just spoke. Similarly, the other option stands when we speak off on the microphone:</p>
<div><img class=" image-border" src="img/image_04_011.png"/></div>
<p>Voice recognition—3</p>
<p>So now we have a voice recognition proof of concept ready. Now, we are going to use this concept with small modifications to lock and unlock the door.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Door lock/unlock based on voice commands</h1>
            

            
                
<p>In this section, we'll just open and close a door based on voice commands. Similar to the previous section, where we switched an LED on and off using voice commands such as <kbd>ON</kbd> and <kbd>OFF</kbd>, here we are going to do a similar thing using a servo motor. The main target is to make the readers understand the core concepts of the Intel Edison where we use voice commands to perform different tasks. The question may arise, why are we using servo motors?</p>
<p>A servo motor, unlike normal DC motors, rotates up to a specific angle set by the operator. In normal scenarios, controlling the lock of a door may use a relay. The usage of relays was discussed in <a href="3bd53219-a287-4d8f-9a58-5a06c5b14062.xhtml" target="_blank">Chapter 3</a>, <em>Intel Edison and IoT (Home Automation)</em>.</p>
<p>Let us also explore the use of servo motors so that we can widen the spectrum of controlling devices. In this case, when a servo is set to <kbd>0</kbd> degrees, it is unlocked and when it is set to <kbd>90</kbd> degrees, it is locked. The control of servo motors requires the use of pulse width modulation pins. Intel Edison has four PWM pins:</p>
<div><img class=" image-border" height="179" src="img/6639_04_09.jpg" width="279"/></div>
<p>Servo motor. Picture credits: <a href="https://circuitdigest.com">https://circuitdigest.com</a></p>
<p>There are three operating lines to operate a servo:</p>
<ul>
<li>Vcc</li>
<li>Gnd</li>
<li>Signal</li>
</ul>
<p>The typical color coding goes like this:</p>
<ul>
<li>Black—ground</li>
<li>Red or brown—power supply</li>
<li>Yellow or white—control signal</li>
</ul>
<p>We are using a 5V servo motor; therefore the Edison is enough to supply power. The Edison and the servo motor must share a common ground. Finally, the signal pin is connected to the PWM pin on the Intel Edison. As we move further with this mini-project, things will get clearer.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Circuit diagram</h1>
            

            
                
<p>The following is the circuit diagram for voice recognition:</p>
<div><img class=" image-border" height="170" src="img/6639_04_10-1.jpg" width="453"/></div>
<p>Circuit diagram for voice recognition</p>
<p>As already mentioned, the servo motor requires PWM pins for operation and the Intel Edison has a total of six PWM pins. Here we are using digital pin 6 for servo control and digital pin 13 for the LED. As far as the peripheral devices are concerned, connect your USB sound card to the USB of the Intel Edison and you are all set.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Configuring the servo library for Python</h1>
            

            
                
<p>To control a servo, we need to send some signals through the PWM pins. We opt for using a library for controlling the servo motors.</p>
<p>Use the following link to get access to a <kbd>Servo.py</kbd> Python script from a GitHub repository:</p>
<p><a href="https://github.com/MakersTeam/Edison/blob/master/Python-Examples/Servo/Servo.py">https://github.com/MakersTeam/Edison/blob/master/Python-Examples/Servo/Servo.py</a></p>
<p>Download the file and push it to your Edison device. After that, just execute the file similar to executing a Python script:</p>
<pre>
<strong>python Servo.py</strong>
</pre>
<p>Now once you have done, you are ready to use the servo with your Intel Edison using Python.</p>
<p>Getting back to the hardware, the servo must be connected to digital pin <kbd>6</kbd>, which is a PWM pin. Let's write a Python script that will test if the library is functioning or not:</p>
<pre>
from Servo import * <br/>import time <br/>myServo = Servo("Servo") <br/>myServo.attach(6) <br/>while True: <br/>   # From 0 to 180 degrees <br/>   for angle in range(0,180): <br/>         myServo.write(angle) <br/>         time.sleep(0.005) <br/>   # From 180 to 0 degrees <br/>   for angle in range(180,-1,-1): <br/>         myServo.write(angle) <br/>         time.sleep(0.005)             
</pre>
<p>The preceding code basically sweeps from <kbd>0</kbd> to <kbd>180</kbd> degrees and back to <kbd>0</kbd> degrees. The circuit remains the same as discussed before. Initially, we attach the servo to the servo pin. Then as the standard goes, we put the entire logic in an infinite loop. To rotate the servo to a specific angle, we use <kbd>.write(angle)</kbd>. In the two for loops, initially we rotate from <kbd>0</kbd> to <kbd>180</kbd> degrees and in the second one, we rotate from <kbd>180</kbd> to <kbd>0</kbd> degrees.</p>
<p>It is also to be noted that <kbd>time.sleep(time_interval)</kbd> is used to pause the code for some miliseconds. When you execute the preceding code, the servo should rotate and come back to the initial position.</p>
<p>Now, we have all the things in place. We'll just put them in the right place and your voice controlled door will be ready. Initially, we controlled an LED and then we learned how we can operate a servo using Python. Now let's create a language model using the Sphinx knowledge base tool.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Language model</h1>
            

            
                
<p>For this project, we'll be using the following set of commands. To keep things simple, we're using only two sets of commands:</p>
<ul>
<li><kbd>door open</kbd></li>
<li><kbd>door close</kbd></li>
</ul>
<p>Follow the process that was discussed earlier and create a text file and just write the three unique words:</p>
<pre>
<strong>door open close</strong> 
</pre>
<p>Save it and upload it to the Sphinx knowledge base tool and compile it.</p>
<p>Once you have the compressed file downloaded, move on to the next step with this code:</p>
<pre>
import collections <br/>import mraa <br/>import os <br/>import sys <br/>import time <br/><br/># Import things for pocketsphinx <br/>import pyaudio <br/>import wave <br/>import pocketsphinx as ps <br/>import sphinxbase <br/># Import for Servo  <br/>from Servo import * <br/><br/>led = mraa.Gpio(13)   <br/>led.dir(mraa.DIR_OUT) <br/>myServo = Servo("First Servo") <br/>myServo.attach(6) <br/><br/>print("Starting") <br/>while 1: <br/>         #PocketSphinx parameters <br/>         LMD   = "/home/root/Voice-Recognition-using-Intel-Edison/8578.lm" <br/>         DICTD = "/home/root/Voice-Recognition-using-Intel-Edison/8578.dic" <br/>         CHUNK = 1024 <br/>         FORMAT = pyaudio.paInt16 <br/>         CHANNELS = 1 <br/>         RATE = 16000 <br/>         RECORD_SECONDS = 3 <br/>         PATH = 'Voice-Recognition-using-Intel-Edison' <br/>         p = pyaudio.PyAudio() <br/>         speech_rec = ps.Decoder(lm=LMD, dict=DICTD) <br/>         #Record audio <br/>         stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK) <br/>         print("* recording") <br/>         frames = [] <br/>         fori in range(0, int(RATE / CHUNK * RECORD_SECONDS)): <br/>               data = stream.read(CHUNK) <br/>               frames.append(data) <br/>         print("* done recording") <br/>         stream.stop_stream() <br/>         stream.close() <br/>         p.terminate() <br/>         # Write .wav file <br/>         fn = "test.wav" <br/>         #wf = wave.open(os.path.join(PATH, fn), 'wb') <br/>         wf = wave.open(fn, 'wb') <br/>         wf.setnchannels(CHANNELS) <br/>         wf.setsampwidth(p.get_sample_size(FORMAT)) <br/>         wf.setframerate(RATE) <br/>         wf.writeframes(b''.join(frames)) <br/>         wf.close() <br/><br/>         # Decode speech <br/>         #wav_file = os.path.join(PATH, fn) <br/>         wav_file=fn <br/>         wav_file = file(wav_file,'rb') <br/>         wav_file.seek(44) <br/>         speech_rec.decode_raw(wav_file) <br/>         result = speech_rec.get_hyp() <br/>         recognised= result[0] <br/>         print("* LED section begins") <br/>         print(recognised) <br/>         ifrecognised == 'DOOR OPEN': <br/>               led.write(1) <br/>               myServo.write(90) <br/>         else: <br/>               led.write(0) <br/>               myServo.write(0) <br/>         cm = 'espeak "'+recognised+'"' <br/>         os.system(cm) 
</pre>
<p>The preceding code is more or less similar to the code for switching an LED on and off. The only difference is that the servo control mechanism is added into the existing code. In a simple if else block, we check for the <kbd>door open</kbd> and <kbd>door close</kbd> conditions. Finally based on what is triggered, we set the LED and the servo to a <kbd>90</kbd> degrees or <kbd>0</kbd> degree position.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Conclusion of speech processing using the Intel Edison</h1>
            

            
                
<p>From the projects discussed before, we explored one of the core capabilities of the Intel Edison and explored a whole new scenario of controlling the Intel Edison by voice. A popular use case that implements the preceding procedure can be the case of home automation, which was implemented in the earlier chapter. Another use case is building a virtual voice based assistant using your Intel Edison. There are multiple opportunities that can be used using voice-based control. It's up to the reader's imagination as to what they want to explore.</p>
<p>In the next part, we'll be dealing with the implementation of image processing using the Intel Edison.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Image processing using the Intel Edison</h1>
            

            
                
<p>Image processing or computer vision is one such field that requires tremendous amounts of research. However, we're not going to do rocket science here. We are opting for an open source computer vision library called <kbd>OpenCV</kbd>. <kbd>OpenCV</kbd> supports multiple languages and we are going to use Python as our programming language to perform face detection.</p>
<p>Typically, an image processing application has an input image; we process the input image and we get an output processed image.</p>
<p>Intel Edison doesn't have a display unit. So essentially we will run the Python script on our PC first. Then after the successful working of the code in the PC, we'll modify the code to run on the Edison. Things will get clearer when we do the practical implementation.</p>
<p>Our target is to perform face detection and, if detected, perform some action.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Initial configuration</h1>
            

            
                
<p>The initial configuration will include installing the <kbd>openCV</kbd> package both on the Edison device as well as the PC.</p>
<p>For the PC, download Python from <a href="https://www.python.org/downloads/windows/">https://www.python.org/downloads/windows/</a>. Next install Python on your system. Also download the latest version of openCV from <a href="https://sourceforge.net/projects/opencvlibrary/">https://sourceforge.net/projects/opencvlibrary/</a>.</p>
<p>After you download openCV, move the extracted folder to <kbd>C:\</kbd>. Next, browse to <kbd>C:\opencv\build\python\2.7\x86</kbd>.</p>
<p>Finally, copy the <kbd>cv2.pyd</kbd> file to <kbd>C:\Python27\Lib\site-packages</kbd>.</p>
<p>We need to install <kbd>numpy</kbd> as well. Numpy stands for <strong>Numerical Python</strong>. Download and install it.</p>
<p>Once you install all the components, we need to test whether everything is installed or not. To do that, open up the idle Python GUI and type the following:</p>
<pre>
<strong>importnumpy</strong><br/><strong>import cv2</strong> 
</pre>
<p>If this proceeds without any error, then everything is installed and in place as far as the PC configuration is concerned. Next, we'll configure for our device.</p>
<p>To configure your Edison with openCV, initially execute the following:</p>
<pre>
<strong>opkg update</strong><br/><strong>opkg upgrade</strong>
</pre>
<p>Finally, after the preceding is successfully executed, run the following:</p>
<pre>
<strong>opkg install python-numpy python-opencv</strong>
</pre>
<p>This should install all the necessary components. To check whether everything is set up or not, type the following:</p>
<pre>
<strong>python</strong> 
</pre>
<p>And press the <em>Enter</em> key. This should enter into the Python shell mode. Next, type the following:</p>
<pre>
<strong>importnumpy</strong><br/><strong>import cv2</strong> 
</pre>
<p>Here is the screenshot of this:</p>
<div><img class=" image-border" src="img/6639_04_11.png"/></div>
<p>Python shell</p>
<p>If this doesn't return any error message, then you are all set to go.</p>
<p>At first we will be covering everything in the PC and after that we'll move on to deploy it to the Intel Edison.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Real-time video display using OpenCV</h1>
            

            
                
<p>Before we move on to face detection, let's first see whether we can access our camera or not. To do that, let's write a very simple Python script to display the webcam video feed:</p>
<pre>
import cv2 <br/><br/>cap = cv2.VideoCapture(0) <br/><br/>while(True): <br/>    # Capture frame-by-frame <br/>    ret, frame = cap.read() <br/><br/>    # Our operations on the frame come here <br/>    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) <br/><br/>    # Display the resulting frame <br/>    cv2.imshow('frame',gray) <br/>    if cv2.waitKey(1) &amp; 0xFF == ord('q'): <br/>      break <br/><br/># When everything done, release the capture <br/>cap.release() <br/>cv2.destroyAllWindows() 
</pre>
<p>In the preceding code, we initially import the openCV module as <kbd>import cv2</kbd>.</p>
<p>Next we initialize the video capture device and set the index to zero as we're using the default webcam that comes with the laptop. For desktop users, you may need to vary the parameter.</p>
<p>After the initialization, in an infinite loop, we read the incoming video frame by frame using <kbd>cap.read()</kbd>:</p>
<pre>
<strong>ret, frame = cap.read()</strong>
</pre>
<p>Next we apply some operations on the incoming video feed. Here in the sample, we convert the RGB video frame to a grayscale image:</p>
<pre>
<strong>gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</strong>
</pre>
<p>Finally, the frames are displayed in a separate window:</p>
<pre>
<strong>if cv2.waitKey(1) &amp; 0xFF == ord('q'):</strong><br/><strong>break</strong>
</pre>
<p>In the preceding two lines, we implement the mechanism of keyboard interrupts. When someone presses <em>q</em> or presses the <em>Esc</em> key, the display will close.</p>
<p>Once you get the incoming video feed, then we are ready to move to face detection.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Face detection theory</h1>
            

            
                
<p>Face detection is a very specific case of object recognition. There are many approaches to face recognition. However, we are going to discuss the two given here:</p>
<ul>
<li>Segmentation based on color</li>
<li>Feature-based recognition</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Segmentation based on color</h1>
            

            
                
<p>In this technique, the face is segmented out based on skin color. The input of this is typically an RGB image, while in the processing stage we shift it to <strong>Hue saturation value</strong> (<strong>HSV</strong>)<strong> </strong>or YIQ ( Luminance (Y), In-phase Quadrature) color formats. In this process, each pixel is classified as a skin-color pixel or a non-skin-color pixel. The reason behind the use of other color models other than RGB is that sometimes RGB isn't able to distinguish skin colors in different light conditions. This significantly improves while using other color models.</p>
<p>This algorithm won't be used here.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Feature-based recognition</h1>
            

            
                
<p>In this technique, we go for certain features and based on that we do the recognition. Use of the haar feature-based cascade for face detection is an effective object detection method proposed by Paul Viola and Michael Jones in their paper "<em>Rapid Object Detection using a Boosted Cascade of Simple Features</em>" in 2001. It is a machine learning based approach where a cascade function is trained against a set of positive and negative images. Then it is used to detect objects in other images.</p>
<p>The algorithm initially needs a lot of positive images. In our case, these are images of faces, while negative images which don't contain images of faces. Then we need to extract features from it.</p>
<p>For this purpose, the haar features shown in the following figure are used. Each of the features is a single value obtained by subtracting the sum of pixels under a white rectangle from sum of pixels under a black rectangle:</p>
<div><img class=" image-border" height="180" src="img/6639_04_12.jpg" width="213"/></div>
<p>Haar features</p>
<p>The haar classifiers need to be trained for face, eyes, smile, and so on. OpenCV contains a set of predefined classifiers. They are available in the <kbd>C:\opencv\build\etc\haarcascades</kbd> folder. Now that we know how we can approach face detection, we are going to use the pre-trained haar classifiers for face detection.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Code for face detection</h1>
            

            
                
<p>The following is the code for face detection:</p>
<pre>
import cv2 <br/>import sys <br/>import os <br/><br/>faceCascade = cv2.CascadeClassifier('C:/opencv/build/haarcascade_frontalface_default.xml') <br/>video_capture = cv2.VideoCapture(0) <br/>while (1): <br/>    # Capture frame-by-frame <br/>    ret, frame = video_capture.read() <br/><br/>    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) <br/>    faces = faceCascade.detectMultiScale(gray, 1.3, 5)  <br/>    # Draw a rectangle around the faces <br/><br/>    for (x, y, w, h) in faces: <br/>      cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2) <br/><br/><br/>    # Display the resulting frame <br/>    cv2.imshow('Video', frame) <br/><br/>    if cv2.waitKey(25) == 27: <br/>      video_capture.release() <br/>      break <br/><br/># When everything is done, release the capture <br/>video_capture.release() <br/>cv2.destroyAllWindows() 
</pre>
<p>Let's look at the code line by line:</p>
<pre>
import cv2 <br/>import sys <br/>import os 
</pre>
<p>Import all the required modules:</p>
<pre>
faceCascade = cv2.CascadeClassifier('C:/opencv/build/haarcascade_frontalface_default.xml') <br/>video_capture = cv2.VideoCapture(0) 
</pre>
<p>We select the cascade classifier file. Also we select the video capture device. Make sure you mention the path correctly:</p>
<pre>
ret, frame = video_capture.read() <br/>gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) 
</pre>
<p>In the preceding lines, which are inside the infinite while loop, we read the video frame and convert it from RGB to grayscale:</p>
<pre>
faces = faceCascade.detectMultiScale(gray, 1.3, 5)  
</pre>
<p>The preceding line is the most important part of the code. We have actually applied the operation on the incoming feed.</p>
<p><kbd>detectMultiScale</kbd> consists of three important parameters. It is a general function for detecting images and since we are applying the face haar cascade, therefore we are detecting faces:</p>
<ul>
<li>The first parameter is the input image that needs to be processed. Here we have passed the grayscale version of the original image.</li>
<li>The second parameter is the scale factor, which provides us with the factor for the creation of a scale pyramid. Typically, around 1.01-1.5 is an appropriate one. The higher the value, the speed increases, but the accuracy decreases.</li>
<li> The third parameter is <kbd>minNeighbours</kbd> which affects the quality of the detected regions. A higher value results in less detection. A range of 3-6 is good:</li>
</ul>
<pre>
      # Draw a rectangle around the faces     <br/>      for (x, y, w, h) in faces: <br/>      cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2) 
</pre>
<p>The preceding lines simply draw rectangles around the faces.</p>
<p>Finally, we display the resultant frame and use the keyboard interrupts to release the video capture device and destroy the window.</p>
<p>Now press <em>F5</em> to run the code. Initially, it will ask to save the file, and then the execution will begin:</p>
<div><img class=" image-border" height="367" src="img/image_04_017.png" width="461"/></div>
<p>Screenshot of the image window with a face detected</p>
<p>Until now, if everything is carried out in a proper way, you must have a brief idea about face detection and how it can be accomplished using openCV. But now, we need to transfer it to the Intel Edison. Also we need to alter certain parts to meet the capabilities of the device as it doesn't have a display unit and above all it has a RAM of 1 GB.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Intel Edison code</h1>
            

            
                
<p>For the Intel Edison, let's find out what is actually possible. We don't have a display, so we can rely only on console messages and LED, perhaps, for visual signals. Next, we may need to optimize the code to run on the Intel Edison. But first let's edit the code discussed previously to include an LED and some kind of messages to the picture:</p>
<pre>
import cv2 <br/>import numpy as np <br/>import sys <br/>import os <br/><br/>faceCascade = cv2.CascadeClassifier('C:/opencv/build/haarcascade_frontalface_default.xml') <br/>video_capture = cv2.VideoCapture(0) <br/>led = mraa.Gpio(13)   <br/>led.dir(mraa.DIR_OUT) <br/>while (1): <br/>led.write(0) <br/>    # Capture frame-by-frame <br/>    ret, frame = video_capture.read() <br/><br/>    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) <br/>    faces = faceCascade.detectMultiScale(gray, 2, 4) <br/>    iflen(faces) &gt; 0: <br/>      print("Detected") <br/>        led.write(1) <br/>    else: <br/>      print("You are clear to proceed") <br/>        led.write(0) <br/>    if cv2.waitKey(25) == 27: <br/>      video_capture.release() <br/>      break <br/><br/># When everything is done, release the capture <br/>video_capture.release() <br/>cv2.destroyAllWindows() 
</pre>
<p>Since the Intel Edison has only one USB port, therefore we have mentioned the parameter of <kbd>cv2.VideoCapture</kbd> as <kbd>0</kbd>. Also notice the following line:</p>
<pre>
faces = faceCascade.detectMultiScale(gray, 2, 4) 
</pre>
<p>You will notice that the parameters have been changed to optimize them for the Intel Edison. You can easily tamper with the parameters to get a good result.</p>
<p>We have included some lines for LED on and off:</p>
<div><img class=" image-border" src="img/6639_04_14.png"/></div>
<p>Console output for face detection in images using openCV</p>
<p>This is when you begin to notice that the Intel Edison is simply not meant for image processing because of the RAM.</p>
<p>Now when you are dealing with high processing applications, we cannot rely on the processing power of the Intel Edison alone.</p>
<p>In those cases, we opt for cloud-based solutions. For cloud-based solutions, there are multiple frameworks that exist. One of them is Project Oxford by Microsoft (<a href="https://www.microsoft.com/cognitive-services">https://www.microsoft.com/cognitive-services</a>).</p>
<p>Microsoft Cognitive Services provides us with APIs for face detection, recognition, speech recognition, and many more. Use the preceding link to learn more about them.</p>
<p>After all the discussions that we've had in this chapter, we now know that voice recognition performs reasonably well. However, things are not so good with image processing. But why are we focused on using it? The answer lies in that the Intel Edison can definitely be used as an image gathering device while other processing can be carried out on the cloud:</p>
<div><img class=" image-border" height="301" src="img/6639_04_15.jpg" width="448"/></div>
<p>Security-based systems architecture at a glance</p>
<p>Processing can either be performed at the device end or at the cloud end. It all depends on the use case and the availability of resources.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Open-ended task for the reader</h1>
            

            
                
<p>The task for this chapter may require a bit of time, but the end result is going to be awesome. Implement Microsoft Cognitive Services to perform facial recognition. Use the Edison to gather data from the user and send it to the service for processing and perform some actions based on the result.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Summary</h1>
            

            
                
<p>Throughout this chapter, we have learned some techniques of voice recognition using the Intel Edison. We also learned how image processing can be done in Python and implemented the same on the Intel Edison. Finally, we explored how a real life security-based system would look like and an open-ended question for Microsoft Cognitive Services.</p>
<p><a href="45fccd6a-a75e-465d-89dc-dad31f528ac1.xhtml" target="_blank">Chapter 5</a>, <em>Autonomous Robotics with Intel Edison</em>, will be entirely dedicated to robotics and how the Intel Edison can be used with robotics. We'll be covering both autonomous and manual robotics.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    </body></html>