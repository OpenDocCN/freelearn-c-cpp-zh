<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Lighting</h1>
            </header>

            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Rendering a geometry with a vertex diffuse lighting</li>
<li>Rendering a geometry with a fragment specular lighting</li>
<li>Rendering a normal mapped geometry</li>
<li>Drawing a reflective and refractive geometry using cubemaps</li>
<li>Adding shadows to the scene</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Introduction</h1>
            </header>

            <article>
                
<p>Lighting is one of the most important factors influencing the way we perceive everything that surrounds us. Most of the information our brains gather about the world comes from our eyes. Human sight is very sensitive to even the slightest change in lighting conditions. That's why lighting is also very important for creators of 3D applications, games, and movies.</p>
<p>In the times when 3D graphics libraries supported only a fixed-function pipeline, lighting calculations were performed according to a predefined set of rules--developers could only select colors for a light source and a lit object. This led most games and applications that used a given library to have a similar look and feel. The next step in the evolution of graphics hardware was the introduction of fragment shaders: their main purpose was to calculate the final color of a fragment (pixel). Fragment shaders literally shaded the geometry, so the name shader was a natural choice. Their main advantage was that they were programmable. They could perform not only lighting calculations, but also realize almost any other algorithm. Nowadays, graphics hardware is much more sophisticated. There are other types of programmable parts of graphics hardware which have also adopted the term shaders for their names. There are many different algorithms and approaches which all use shaders to display interesting images in games, 3D applications, and movies. The basic purpose of shader programs is still very important even today--lighting calculations must be performed if we want to achieve interesting, eye-catching results.</p>
<p>In this chapter, we will learn about commonly used lighting techniques from simple object diffuse lighting calculations, to a shadow mapping algorithm.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Rendering a geometry with a vertex diffuse lighting</h1>
            </header>

            <article>
                
<p>A basic diffuse lighting algorithm lies at the core of most lighting calculations. It is used to simulate the appearance of matte surfaces that reflect the light, scattering it in many different directions. In this recipe, we will see how to achieve geometry rendering using vertex and fragment shaders that implement the diffuse lighting algorithm.</p>
<p>An example of an image generated with this recipe looks like the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="301" src="assets/image_11_001.png" width="504"/></div>
<div class="packt_tip">The following recipe is very detailed so that you can understand and follow all the steps more easily. Further recipes will be based on the knowledge described here, so they are shorter but also more general.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Getting ready</h1>
            </header>

            <article>
                
<p>Diffuse lighting is based on a <strong>cosine law</strong> introduced by <strong>Johann Heinrich Lambert</strong>. It says that the lighting intensity of an observed surface is proportional to the cosine of the angle between the direction from the surface to the source of light (a light vector) and the surface normal vector:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="219" src="assets/image_11_002.png" width="228"/></div>
<p>This law is easily implemented inside shaders. Normal vectors are provided from the application as one of the vertex attributes. The positions of all vertices are also known, so we only need to provide a light direction or the position of a light source to calculate the light vector inside shaders. Both normal and light vectors must be normalized (both must have a length equal to <kbd>1.0</kbd>). The next step is to calculate a cosine of the angle between these two vectors. This is done with a single <kbd>dot()</kbd> function like this:</p>
<pre>
max( 0.0, dot( normal_vector, light_vector ) )
</pre>
<p>We must remember that a cosine can give negative results. This happens when we calculate lighting for points on a surface that point in the opposite direction to a light source. Such points cannot be lit by a given light source (they are in shadow from the perspective of a given light source), so we must disregard such results and clamp them to the <kbd>0</kbd> value.</p>
<p>In all the following recipes we will use objects of a <kbd>VkDestroyer</kbd> class, which allow us to automatically destroy Vulkan resources. For convenience, a <kbd>InitVkDestroyer()</kbd> function is also introduced. Its purpose is to wrap a given resource in the <kbd>VkDestroyer</kbd> object and connect it to a created logical device.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">How to do it...</h1>
            </header>

            <article>
                
<ol>
<li>Create a Vulkan instance and a logical device with a set of enabled swapchain extensions. Also store the handle of a physical device from which the logical device was created (refer to the <em>Creating a Vulkan Instance with WSI extensions enabled</em> and <em>Creating a logical device with WSI extensions enabled</em> recipes from <a href="45eb1180-672a-4745-bd85-f13c7bb658b7.xhtml"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Image Presentation</em>).</li>
<li>Acquire handles of graphics and presentation queues from the logical device (refer to the <em>Getting a device queue</em> recipe from <a href="d10e8284-6122-4d0a-8f86-ab0bc0bba47e.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Instance and Devices</em>).</li>
<li>Create a swapchain with a desired set of parameters. Store the swapchain's size (image dimensions) and format (refer to the <em>Creating a swapchain with R8G8B8A8 format and a MAILBOX present mode</em> recipe from <a href="45eb1180-672a-4745-bd85-f13c7bb658b7.xhtml"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Image Presentation</em>).</li>
<li>Get handles of all swapchain images (refer to the <em>Getting handles of swapchain images</em> recipe from <a href="45eb1180-672a-4745-bd85-f13c7bb658b7.xhtml"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Image Presentation</em>).</li>
<li>Create image views for all swapchain images (refer to the <em>Creating an image view</em> recipe from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml">Chapter 4</a>, <em>Resources and Memory</em>).</li>
<li>Create a set of resources required to generate frames of animation--command pool and command buffers, semaphores (at least two for acquiring a swapchain image and to indicate when a frame rendering is finished, which is required during swapchain image presentation), fences and framebuffers. Create at least one such set, but more can be created if we want to render more frames separately (refer to the <em>Increasing the performance through increasing the number of separately rendered frames</em> recipe from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
<li>Load a 3D model data with vertex positions and normal vectors into a variable of type <kbd>Mesh</kbd> named <kbd>Model</kbd> (refer to the <em>Loading a 3D model from an OBJ file</em> recipe from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml"><span class="ChapterrefPACKT">Chapter 10</span></a>, <em>Helper Recipes</em>).</li>
<li>Create a buffer that will serve as a vertex buffer and will support <kbd>VK_BUFFER_USAGE_TRANSFER_DST_BIT</kbd> and <kbd>VK_BUFFER_USAGE_VERTEX_BUFFER_BIT</kbd> usages (refer to the <em>Creating a buffer recipe</em> from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
</ol>
<ol start="9">
<li>Allocate a memory object with a <kbd>VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT</kbd> property and bind it to the vertex buffer (refer to the <em>Allocating and binding memory object to a buffer</em> recipe from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>Copy vertex data from the <kbd>Data</kbd> member of the <kbd>Model</kbd> variable into the vertex buffer using a staging buffer (refer to the <em>Using staging buffer to update buffer with a device-local memory bound</em> recipe from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>Create a uniform buffer with <kbd>VK_BUFFER_USAGE_TRANSFER_DST_BIT</kbd> and <kbd>VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT</kbd> usages that is big enough to hold data for two 16-element matrices of floating-point values (refer to the <em>Creating a uniform buffer</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Create a descriptor set layout with only one uniform buffer accessed by a vertex shader stage (refer to the <em>Creating a descriptor set layout</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Create a descriptor pool from which a descriptor for one uniform buffer can be allocated (refer to the <em>Creating a descriptor pool</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Allocate a descriptor set from the created pool using the prepared layout (refer to <em>Allocating descriptor sets</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Update the descriptor set with the uniform buffer's handle (refer to the <em>Updating descriptor sets</em> recipes from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Prepare parameters for a render pass creation. First, specify descriptions of two attachments (refer to the <em>Specifying attachments descriptions</em> recipe from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Render Passes and Framebuffers</em>):
<ul>
<li>The first attachment should have the same format as the swapchain images. It should be cleared on a render pass start and its contents should be stored at the end of the render pass. Its initial layout can be undefined, but a final layout must be a <kbd>VK_IMAGE_LAYOUT_PRESENT_SRC_KHR</kbd>.</li>
<li>The second attachment should have one of the supported depth formats (<kbd>VK_FORMAT_D16_UNORM</kbd> format must always be supported, and at least one of <kbd>VK_FORMAT_X8_D24_UNORM_PACK32</kbd> or <kbd>VK_FORMAT_D32_SFLOAT</kbd> must be supported). It must be cleared on the render pass start, but its contents don't need to be preserved after the render pass. Its initial layout may be undefined and the final layout should be the same as a layout specified in a subpass (to avoid unnecessary layout transitions).</li>
</ul>
</li>
</ol>
<ol start="17">
<li>Specify one subpass for the render pass, in which the first render pass attachment will be provided as a color attachment with a <kbd>VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL</kbd> layout, and the second attachment will be used as a depth attachment with a <kbd>VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL</kbd> layout (refer to the <em>Specifying subpass descriptions</em> recipe from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Render Passes and Framebuffers</em>).</li>
<li>Specify two subpass dependencies for the render pass (refer to the <em>Specifying dependencies between subpasses</em> recipe from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Render Passes and Framebuffers</em>). Use the following values for the first dependency:
<ul>
<li><kbd>VK_SUBPASS_EXTERNAL</kbd> value for <kbd>srcSubpass</kbd></li>
<li><kbd>0</kbd> value for <kbd>dstSubpass</kbd></li>
<li><kbd>VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT</kbd> value for <kbd>srcStageMask</kbd></li>
<li><kbd>VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT</kbd> value for <kbd>dstStageMask</kbd></li>
<li><kbd>VK_ACCESS_MEMORY_READ_BIT</kbd> value for <kbd>srcAccessMask</kbd></li>
<li><kbd>VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT</kbd> value for <kbd>dstAccessMask</kbd></li>
<li><kbd>VK_DEPENDENCY_BY_REGION_BIT</kbd> value for <kbd>dependencyFlags</kbd></li>
</ul>
</li>
<li>Use the following values for the second render pass dependency:
<ul>
<li><kbd>0</kbd> value for <kbd>srcSubpass</kbd></li>
<li><kbd>VK_SUBPASS_EXTERNAL</kbd> value for <kbd>dstSubpass</kbd></li>
<li><kbd>VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT</kbd> value for <kbd>srcStageMask</kbd></li>
<li><kbd>VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT</kbd> value for <kbd>dstStageMask</kbd></li>
<li><kbd>VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT</kbd> value for <kbd>srcAccessMask</kbd></li>
<li><kbd>VK_ACCESS_MEMORY_READ_BIT</kbd> value for <kbd>dstAccessMask</kbd></li>
<li><kbd>VK_DEPENDENCY_BY_REGION_BIT</kbd> value for <kbd>dependencyFlags</kbd></li>
</ul>
</li>
<li>Create the render pass using the prepared parameters (refer to the <em>Creating a render pass</em> recipe from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Render Passes and Framebuffers</em>).</li>
</ol>
<ol start="21">
<li>Create a pipeline layout using the prepared descriptor set layout with only a uniform buffer (refer to the <em>Creating a pipeline layout</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
</ol>
<ol start="22">
<li>Create a shader module for a vertex shader stage using a SPIR-V assembly generated from the following GLSL code (refer to the <em>Converting GLSL shaders to SPIR-V assemblies</em> recipe from <a href="97217f0d-bed7-4ae1-a543-b4d599f299cf.xhtml"><span class="ChapterrefPACKT">Chapter 7</span></a>, <em>Shaders</em> and to the <em>Creating a shader module</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>):</li>
</ol>
<pre>
      #version 450 
      layout( location = 0 ) in vec4 app_position; 
      layout( location = 1 ) in vec3 app_normal; 
      layout( set = 0, binding = 0 ) uniform UniformBuffer { 
        mat4 ModelViewMatrix; 
        mat4 ProjectionMatrix; 
      }; 
      layout( location = 0 ) out float vert_color; 
      void main() { 
        gl_Position = ProjectionMatrix * ModelViewMatrix * 
        app_position; 
        vec3 normal = mat3( ModelViewMatrix ) * app_normal; 

        vert_color = max( 0.0, dot( normal, vec3( 0.58, 0.58, 0.58 ) )          ) + 0.1; 
      }
</pre>
<ol start="23">
<li>Create a shader module for a fragment shader stage using a SPIR-V assembly generated from the following GLSL code:</li>
</ol>
<pre>
      #version 450 
      layout( location = 0 ) in float vert_color; 
      layout( location = 0 ) out vec4 frag_color; 
      void main() { 
        frag_color = vec4( vert_color ); 
      }
</pre>
<ol start="24">
<li>Specify pipeline shader stages with vertex and fragment shaders, both using a <kbd>main</kbd> function from respective shader modules (refer to the <em>Specifying pipeline shader stages</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Specify a pipeline vertex input state with two attributes that are read from the same 0 binding. Binding should be created with data read per vertex and a stride equal to <kbd>6 * sizeof( float )</kbd> (refer to the <em>Specifying pipeline vertex input state</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>). The first attribute should have the following parameters:
<ul>
<li><kbd>0</kbd> value for <kbd>location</kbd></li>
<li><kbd>0</kbd> value for <kbd>binding</kbd></li>
<li><kbd>VK_FORMAT_R32G32B32_SFLOAT</kbd> value for <kbd>format</kbd></li>
<li><kbd>0</kbd> value for <kbd>offset</kbd></li>
</ul>
</li>
</ol>
<ol start="26">
<li>The second vertex attribute should be specified using the following values:
<ul>
<li><kbd>1</kbd> value for <kbd>location</kbd></li>
<li><kbd>0</kbd> value for <kbd>binding</kbd></li>
<li><kbd>VK_FORMAT_R32G32B32_SFLOAT</kbd> value for <kbd>format</kbd></li>
<li><kbd>3 * sizeof( float)</kbd> value for <kbd>offset</kbd></li>
</ul>
</li>
<li>Specify a pipeline input assembly state with a <kbd>VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST</kbd> topology and without primitive restart (refer to the <em>Specifying pipeline input assembly state</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Specify a pipeline viewport and scissor test state with only one viewport and scissor test state. Initial values don't matter as they will be set dynamically (refer to the <em>Specifying pipeline viewport and scissor test state</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Specify a pipeline rasterization state without depth clamp, with no rasterization discard, with a <kbd>VK_POLYGON_MODE_FILL</kbd><kbd>,</kbd> <kbd>VK_CULL_MODE_BACK_BIT</kbd> and <kbd>VK_FRONT_FACE_COUNTER_CLOCKWISE</kbd>, without a depth bias and with line widths of <kbd>1.0f</kbd> (refer to the <em>Specifying pipeline rasterization state</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Specify a pipeline multisample state with only a single sample and without sample shading, sample masks, alpha to coverage, or alpha to one (refer to the <em>Specifying pipeline multisample state</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Specify a pipeline depth state with a depth test and depth writes enabled, with a <kbd>VK_COMPARE_OP_LESS_OR_EQUAL</kbd> operator and without depth bounds or stencil tests (refer to the <em>Specifying pipeline depth and stencil state</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Specify a pipeline blend state with both logical operations and blending disabled (refer to the <em>Specifying pipeline blend state</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Specify the viewport and scissor test as dynamic states of a pipeline (refer to the <em>Specifying pipeline dynamic states</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Create a graphics pipeline using the prepared parameters (refer to the <em>Creating graphics pipelines</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
</ol>
<ol start="35">
<li>Create a staging buffer supporting a <kbd>VK_BUFFER_USAGE_TRANSFER_SRC_BIT</kbd> usage, which can hold data of two matrices, each with 16 floating-point elements. The buffer's memory object should be allocated on a memory that is host-visible (refer to the <em>Creating a buffer</em> and <em>Allocating and binding memory object to a buffer</em> recipes from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>Create a 2D image (with an appropriate memory object) and an image view with the same format as the render pass's depth attachment, with the same size as the size of swapchain images, with one mipmap level and array layer. The image must support a <kbd>VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT</kbd> usage (refer to the <em>Creating a 2D image and view</em> recipe from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>). Remember that these resources (along with the swapchain) must be recreated each time the application's window is resized.</li>
<li>Prepare a model matrix, which can be a multiplication of rotation, scaling and translation matrices (refer to the <em>Preparing a translation matrix</em>, <em>Preparing a rotation matrix</em> and <em>Preparing a scaling matrix</em> recipes from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml"><span class="ChapterrefPACKT">Chapter 10</span></a>, <em>Helper Recipes</em>). Copy the contents of the concatenated matrix to the staging buffer at a <kbd>0</kbd> offset (refer to the <em>Mapping, updating and unmapping host-visible memory</em> recipe from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>Prepare a perspective projection matrix based on the aspect ratio of swapchain's dimensions (refer to the <em>Preparing a perspective projection matrix recipe</em> from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml"><span class="ChapterrefPACKT">Chapter 10</span></a>, <em>Helper Recipes</em>). Copy contents of the matrix to the staging buffer at an offset equal to the number of elements in the model matrix (16) multiplied by the size of a single element (<kbd>sizeof(float)</kbd>). Remember to recreate the projection matrix and copy it to the staging buffer each time the application's window is resized (refer to <em>Mapping, updating and unmapping host-visible memory</em> recipe from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>Inside a rendering loop, for each loop iteration, prepare a frame of animation by acquiring one of the swapchain images, creating a framebuffer with the acquired swapchain image and the image serving as a depth attachment, recording the command buffer as described below, submitting it to the graphics queue and presenting the acquired image (refer to the <em>Preparing a single frame of animation</em> recipe from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
<li>To record the command buffer:
<ul>
<li>Begin recording the command buffer specifying a <kbd>VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT</kbd> usage (refer to the <em>Beginning a command buffer recording operation</em> recipe from <a href="fc38e0ae-51aa-4f6f-8fb3-551861273018.xhtml"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Command Buffers and Synchronization</em>).</li>
<li>If the staging buffer has been updated since the last frame, set a buffer memory barrier for the uniform buffer to inform the driver that the buffer's memory will be accessed in a different way, copy data from the staging buffer to the uniform buffer, and set up another buffer memory barrier (refer to the <em>Setting a buffer memory barrier</em> and <em>Copying data between buffers</em> recipes from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>When graphics and presentation queues are different, transform the ownership for the acquired swapchain image from the presentation queue to the graphics queue using an image memory barrier (refer to the <em>Setting an image memory barrier</em> from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>Begin the render pass (refer to the <em>Beginning a render pass</em> recipe from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Render Passes and Framebuffers</em>).</li>
<li>Set the viewport and scissor test states dynamically providing the current swapchain dimensions (refer to the <em>Setting viewport state dynamically</em> and <em>Setting scissors state dynamically</em> recipes from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
<li>Bind the vertex buffer to the <kbd>0</kbd> binding (refer to the <em>Binding vertex buffers</em> recipe from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
<li>Bind the descriptor set to the <kbd>0</kbd> index (refer to the <em>Binding descriptor sets</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Bind the graphics pipeline (refer to the <em>Binding a pipeline object</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Draw the model geometry (refer to the <em>Drawing a geometry</em> recipe from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
<li>End the render pass (refer to the <em>Ending a render pass</em> recipe from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Render Passes and Framebuffers</em>).</li>
<li>If the graphics and presentation queues are different, transform the ownership for the acquired swapchain image from the graphics queue to the presentation queue using an image memory barrier (refer to the <em>Setting an image memory barrier</em> from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>End the command buffer's recording operation (refer to the <em>Ending a command buffer recording operation</em> recipe from <a href="fc38e0ae-51aa-4f6f-8fb3-551861273018.xhtml"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Command Buffers and Synchronization</em>).</li>
</ul>
</li>
<li>To increase the performance of an application, prepare multiple animation frames using separate sets of resources (refer to the <em>Increasing the performance through increasing the number of separately rendered frames</em> recipe from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
</ol>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">How it works...</h1>
            </header>

            <article>
                
<p>Assume we have created a Vulkan Instance and a logical device with enabled WSI extensions. We also created a swapchain object (the full source code for these operations can be found in the accompanying code samples).</p>
<p>To render any geometry, we need to first load a 3D model. Its data needs to be copied to a vertex buffer, so we also need to create a vertex buffer, allocate and bind a memory to it, and we need to copy the model data using a staging buffer:</p>
<pre>
if( !Load3DModelFromObjFile( "Data/Models/knot.obj", true, false, false, true, Model ) ) { 
  return false; 
} 
InitVkDestroyer( LogicalDevice, VertexBuffer ); 
if( !CreateBuffer( *LogicalDevice, sizeof( Model.Data[0] ) * Model.Data.size(), VK_BUFFER_USAGE_TRANSFER_DST_BIT | VK_BUFFER_USAGE_VERTEX_BUFFER_BIT, *VertexBuffer ) ) { 
  return false; 
} 
InitVkDestroyer( LogicalDevice, VertexBufferMemory ); 
if( !AllocateAndBindMemoryObjectToBuffer( PhysicalDevice, *LogicalDevice, *VertexBuffer, VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT, *VertexBufferMemory ) ) { 
  return false; 
} 
if( !UseStagingBufferToUpdateBufferWithDeviceLocalMemoryBound( PhysicalDevice, *LogicalDevice, sizeof( Model.Data[0] ) * Model.Data.size(), &amp;Model.Data[0], *VertexBuffer, 0, 0, VK_ACCESS_TRANSFER_WRITE_BIT, VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, VK_PIPELINE_STAGE_VERTEX_INPUT_BIT, GraphicsQueue.Handle, FrameResources.front().CommandBuffer, {} ) ) { 
  return false; 
}
</pre>
<p>Next, a uniform buffer is required. Using the uniform buffer we will provide transformation matrices to the shaders:</p>
<pre>
InitVkDestroyer( LogicalDevice, UniformBuffer ); 
InitVkDestroyer( LogicalDevice, UniformBufferMemory ); 
if( !CreateUniformBuffer( PhysicalDevice, *LogicalDevice, 2 * 16 * sizeof( float ), VK_BUFFER_USAGE_TRANSFER_DST_BIT | VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT, 
  *UniformBuffer, *UniformBufferMemory ) ) { 
  return false; 
}
</pre>
<p>The uniform buffer will be accessed in a vertex shader. For this purpose, we need a descriptor set layout, a descriptor pool, and a single descriptor set, which will be updated (populated) with the created uniform buffer:</p>
<pre>
VkDescriptorSetLayoutBinding descriptor_set_layout_binding = { 
  0, 
  VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
  1, 
  VK_SHADER_STAGE_VERTEX_BIT, 
  nullptr 
}; 
InitVkDestroyer( LogicalDevice, DescriptorSetLayout ); 
if( !CreateDescriptorSetLayout( *LogicalDevice, { descriptor_set_layout_binding }, *DescriptorSetLayout ) ) { 
  return false; 
} 
VkDescriptorPoolSize descriptor_pool_size = { 
  VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
  1 
}; 
InitVkDestroyer( LogicalDevice, DescriptorPool ); 
if( !CreateDescriptorPool( *LogicalDevice, false, 1, { descriptor_pool_size }, *DescriptorPool ) ) { 
  return false; 
} 
if( !AllocateDescriptorSets( *LogicalDevice, *DescriptorPool, { *DescriptorSetLayout }, DescriptorSets ) ) { 
  return false; 
} 
BufferDescriptorInfo buffer_descriptor_update = { 
  DescriptorSets[0], 
  0, 
  0, 
  VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
  { 
    { 
      *UniformBuffer, 
      0, 
      VK_WHOLE_SIZE 
    } 
  } 
}; 
UpdateDescriptorSets( *LogicalDevice, {}, { buffer_descriptor_update }, {}, {} );
</pre>
<p>Rendering operations can only be performed inside render passes. We need a render pass with two attachments: the first is a swapchain image; and second is an image created by us that will serve as a depth attachment. As we will render only a single model without any postprocessing techniques, it is enough for the render pass to have just one subpass.</p>
<pre>
std::vector&lt;VkAttachmentDescription&gt; attachment_descriptions = { 
  { 
    0, 
    Swapchain.Format, 
    VK_SAMPLE_COUNT_1_BIT, 
    VK_ATTACHMENT_LOAD_OP_CLEAR, 
    VK_ATTACHMENT_STORE_OP_STORE, 
    VK_ATTACHMENT_LOAD_OP_DONT_CARE, 
    VK_ATTACHMENT_STORE_OP_DONT_CARE, 
    VK_IMAGE_LAYOUT_UNDEFINED, 
    VK_IMAGE_LAYOUT_PRESENT_SRC_KHR 
  }, 
  { 
    0, 
    DepthFormat, 
    VK_SAMPLE_COUNT_1_BIT, 
    VK_ATTACHMENT_LOAD_OP_CLEAR, 
    VK_ATTACHMENT_STORE_OP_DONT_CARE, 
    VK_ATTACHMENT_LOAD_OP_DONT_CARE, 
    VK_ATTACHMENT_STORE_OP_DONT_CARE, 
    VK_IMAGE_LAYOUT_UNDEFINED, 
    VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL 
  } 
}; 
VkAttachmentReference depth_attachment = { 
  1, 
  VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL 
}; 
std::vector&lt;SubpassParameters&gt; subpass_parameters = { 
  { 
    VK_PIPELINE_BIND_POINT_GRAPHICS, 
    {}, 
    { 
      { 
        0, 
        VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL, 
      } 
    }, 
    {}, 
    &amp;depth_attachment, 
    {} 
  } 
}; 
std::vector&lt;VkSubpassDependency&gt; subpass_dependencies = { 
  { 
    VK_SUBPASS_EXTERNAL, 
    0, 
    VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, 
    VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT, 
    VK_ACCESS_MEMORY_READ_BIT, 
    VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT, 
    VK_DEPENDENCY_BY_REGION_BIT 
  }, 
  { 
    0, 
    VK_SUBPASS_EXTERNAL, 
    VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT, 
    VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, 
    VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT, 
    VK_ACCESS_MEMORY_READ_BIT, 
    VK_DEPENDENCY_BY_REGION_BIT 
  } 
}; 
InitVkDestroyer( LogicalDevice, RenderPass ); 
if( !CreateRenderPass( *LogicalDevice, attachment_descriptions, subpass_parameters, subpass_dependencies, *RenderPass ) ) { 
  return false; 
}
</pre>
<p>We also need a staging buffer. It will be used to transfer data from the application to the uniform buffer:</p>
<pre>
InitVkDestroyer( LogicalDevice, StagingBuffer ); 
if( !CreateBuffer( *LogicalDevice, 2 * 16 * sizeof(float), VK_BUFFER_USAGE_TRANSFER_SRC_BIT, *StagingBuffer ) ) { 
  return false; 
} 
InitVkDestroyer( LogicalDevice, StagingBufferMemory ); 
if( !AllocateAndBindMemoryObjectToBuffer( PhysicalDevice, *LogicalDevice, *StagingBuffer, VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT, *StagingBufferMemory ) ) { 
  return false; 
}
</pre>
<p>Before we can render a frame, we need to do one last thing: create a graphics pipeline. As the code required to create one is pretty straightforward, we will skip it (it can be seen in the code samples accompanying this book).</p>
<p>To see the model, we need to prepare model and projection matrices. A model matrix is used to place a model in a virtual world--it can be moved, scaled, or rotated. Such a matrix is usually combined with a view matrix, which is used to move a camera in our scene. Here, for simplicity, we won't use a view transformation; but we still need a projection matrix. Because the values in the projection matrix depend on the framebuffer's aspect ratio (in this case the size of the application's window), it must be recomputed every time the application window's dimensions are changed:</p>
<pre>
Matrix4x4 rotation_matrix = PrepareRotationMatrix( vertical_angle, { 1.0f, 0.0f, 0.0f } ) * PrepareRotationMatrix( horizontal_angle, { 0.0f, -1.0f, 0.0f } ); 
Matrix4x4 translation_matrix = PrepareTranslationMatrix( 0.0f, 0.0f, -4.0f ); 
Matrix4x4 model_view_matrix = translation_matrix * rotation_matrix; 
if( !MapUpdateAndUnmapHostVisibleMemory( *LogicalDevice, *StagingBufferMemory, 0, sizeof( model_view_matrix[0] ) * model_view_matrix.size(), &amp;model_view_matrix[0], true, nullptr ) ) { 
  return false; 
} 
Matrix4x4 perspective_matrix = PreparePerspectiveProjectionMatrix( static_cast&lt;float&gt;(Swapchain.Size.width) / static_cast&lt;float&gt;(Swapchain.Size.height), 
  50.0f, 0.5f, 10.0f ); 
if( !MapUpdateAndUnmapHostVisibleMemory( *LogicalDevice, *StagingBufferMemory, sizeof( model_view_matrix[0] ) * model_view_matrix.size(), 
  sizeof( perspective_matrix[0] ) * perspective_matrix.size(), &amp;perspective_matrix[0], true, nullptr ) ) { 
  return false;
</pre>
<p>Finally, the last thing we need to do is to prepare an animation frame. This is usually performed inside a rendering loop, where for each loop iteration a separate (new) frame is rendered.</p>
<p>First, we need to check if the uniform buffer's contents need to be updated and whether the data needs to be copied from the staging buffer to the uniform buffer:</p>
<pre>
if( !BeginCommandBufferRecordingOperation( command_buffer, VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT, nullptr ) ) { 
  return false; 
} 
if( UpdateUniformBuffer ) { 
  UpdateUniformBuffer = false; 
  BufferTransition pre_transfer_transition = { 
    *UniformBuffer, 
    VK_ACCESS_UNIFORM_READ_BIT, 
    VK_ACCESS_TRANSFER_WRITE_BIT, 
    VK_QUEUE_FAMILY_IGNORED, 
    VK_QUEUE_FAMILY_IGNORED 
  }; 
  SetBufferMemoryBarrier( command_buffer, VK_PIPELINE_STAGE_BOTTOM_OF_PIPE_BIT, VK_PIPELINE_STAGE_TRANSFER_BIT, { pre_transfer_transition } ); 
  std::vector&lt;VkBufferCopy&gt; regions = { 
    { 
      0, 
      0, 
      2 * 16 * sizeof( float ) 
    } 
  }; 
  CopyDataBetweenBuffers( command_buffer, *StagingBuffer, *UniformBuffer, regions ); 
  BufferTransition post_transfer_transition = { 
    *UniformBuffer, 
    VK_ACCESS_TRANSFER_WRITE_BIT, 
    VK_ACCESS_UNIFORM_READ_BIT, 
    VK_QUEUE_FAMILY_IGNORED, 
    VK_QUEUE_FAMILY_IGNORED 
  }; 
  SetBufferMemoryBarrier( command_buffer, VK_PIPELINE_STAGE_TRANSFER_BIT, VK_PIPELINE_STAGE_VERTEX_SHADER_BIT, { post_transfer_transition } ); 
}
</pre>
<p>Next, we transfer queue ownership for swapchain images (in a situation when graphics and present queues are different). After that, we start the render pass and set up all the states required to render a geometry: we set viewport and scissor test states, bind the vertex buffer, descriptor set and the graphics pipeline. After that, the geometry is drawn and the render pass is finished. Once again, we need to transfer the queue ownership back to the presentation queue (if the graphics queue is different) and we stop recording the command buffer. Now it can be submitted to the queue:</p>
<pre>
if( PresentQueue.FamilyIndex != GraphicsQueue.FamilyIndex ) { 
  ImageTransition image_transition_before_drawing = { 
    Swapchain.Images[swapchain_image_index], 
    VK_ACCESS_MEMORY_READ_BIT, 
    VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT, 
    VK_IMAGE_LAYOUT_UNDEFINED, 
    VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL, 
    PresentQueue.FamilyIndex, 
    GraphicsQueue.FamilyIndex, 
    VK_IMAGE_ASPECT_COLOR_BIT 
  }; 
  SetImageMemoryBarrier( command_buffer, VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT, VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT, { image_transition_before_drawing } ); 
} 
BeginRenderPass( command_buffer, *RenderPass, framebuffer, { { 0, 0 }, Swapchain.Size }, { { 0.1f, 0.2f, 0.3f, 1.0f },{ 1.0f, 0 } }, VK_SUBPASS_CONTENTS_INLINE ); 
VkViewport viewport = { 
  0.0f, 
  0.0f, 
  static_cast&lt;float&gt;(Swapchain.Size.width), 
  static_cast&lt;float&gt;(Swapchain.Size.height), 
  0.0f, 
  1.0f, 
}; 
SetViewportStateDynamically( command_buffer, 0, { viewport } ); 
VkRect2D scissor = { 
  { 
    0, 
    0 
  }, 
  { 
    Swapchain.Size.width, 
    Swapchain.Size.height 
  } 
}; 
SetScissorsStateDynamically( command_buffer, 0, { scissor } ); 
BindVertexBuffers( command_buffer, 0, { { *VertexBuffer, 0 } } ); 
BindDescriptorSets( command_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS, *PipelineLayout, 0, DescriptorSets, {} ); 
BindPipelineObject( command_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS, *Pipeline ); 
for( size_t i = 0; i &lt; Model.Parts.size(); ++i ) { 
  DrawGeometry( command_buffer, Model.Parts[i].VertexCount, 1, Model.Parts[i].VertexOffset, 0 ); 
} 
EndRenderPass( command_buffer ); 
if( PresentQueue.FamilyIndex != GraphicsQueue.FamilyIndex ) { 
  ImageTransition image_transition_before_present = { 
    Swapchain.Images[swapchain_image_index], 
    VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT, 
    VK_ACCESS_MEMORY_READ_BIT, 
    VK_IMAGE_LAYOUT_PRESENT_SRC_KHR, 
    VK_IMAGE_LAYOUT_PRESENT_SRC_KHR, 
    GraphicsQueue.FamilyIndex, 
    PresentQueue.FamilyIndex, 
    VK_IMAGE_ASPECT_COLOR_BIT 
  }; 
  SetImageMemoryBarrier( command_buffer, VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT, VK_PIPELINE_STAGE_BOTTOM_OF_PIPE_BIT, { image_transition_before_present } ); 
} 
if( !EndCommandBufferRecordingOperation( command_buffer ) ) { 
  return false; 
} 
return true;
</pre>
<p>When we prepare the preceding frame, normal vectors and vertex positions are automatically fetched from the vertex buffer. Positions are used, not only to display a geometry, but along with the normal vectors, they are also used for lighting calculations.</p>
<pre>
gl_Position = ProjectionMatrix * ModelViewMatrix * app_position; 
vec3 normal = mat3( ModelViewMatrix ) * app_normal; 
vert_color = max( 0.0, dot( normal, vec3( 0.58, 0.58, 0.58 ) ) ) + 0.1;
</pre>
<p>For simplicity, the light vector is hardcoded in the vertex shader, but normally it should be provided using a uniform buffer or a push constant. In this case, the light vector always points in the same direction (for all vertices), so it simulates a directional light, which usually represents the sun.</p>
<p>In the preceding code, all lighting calculations are performed in the view space. We can perform such calculations in any coordinate system we want, but all vectors (normal, light vectors, view vectors, and so on) must be transformed to the same space in order for the calculations to be correct.</p>
<p>After calculating the diffuse term, we also add a constant value to the calculated color. Usually this is referred to as ambient lighting, which is used to brighten up the scene (otherwise all shadows/unlit surfaces would be too dark).</p>
<p>Below we can see diffuse lighting calculated at each vertex applied to geometry with a different number of polygons: on the left, a detailed geometry (high-polygon); and, on the right model, with a much smaller amount of detail (low-polygon):</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="178" src="assets/image_11_003.png" width="533"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">See also</h1>
            </header>

            <article>
                
<ul>
<li>In <a href="45eb1180-672a-4745-bd85-f13c7bb658b7.xhtml"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Image Presentation</em>, see the following recipes:
<ul>
<li><em>Creating a Vulkan Instance with WSI extensions enabled</em></li>
<li><em>Creating a logical device with WSI extensions enabled</em></li>
<li><em>Creating a swapchain with R8G8B8A8 format and a MAILBOX present mode</em></li>
</ul>
</li>
<li>In <a href="fc38e0ae-51aa-4f6f-8fb3-551861273018.xhtml"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Command Buffers and Synchronization</em>, the recipe <em>Beginning a command buffer recording operation</em></li>
<li>In <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>, the recipe <em>Creating graphics pipelines</em></li>
<li>In <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>, the recipe <em>increasing the performance by increasing the number of separately rendered frames</em></li>
<li>In <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml"><span class="ChapterrefPACKT">Chapter 10</span></a>, <em>Helper Recipes</em>, the recipe<em> Loading a 3D model from an OBJ file</em></li>
<li class="packt_nosymbol">The following recipes in this chapter:
<ul>
<li><em>Rendering a geometry with a fragment specular lighting</em></li>
<li><em>Rendering a normal mapped geometry</em></li>
</ul>
</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Rendering a geometry with a fragment specular lighting</h1>
            </header>

            <article>
                
<p>Specular lighting allows us to add bright highlights or reflections on the surface of a model. This way rendered geometry looks shinier and more glossy.</p>
<p>An example of an image generated with this recipe looks like the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="307" src="assets/image_11_004.png" width="514"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Getting ready</h1>
            </header>

            <article>
                
<p>The most commonly used algorithm describing the way surfaces are lit is a <strong>Blinn-Phong</strong> model. It is an empirical model, which isn't physically correct but gives results that are more plausible in situations where rendered geometry is simplified. So it is well suited for 3D real-time graphics.</p>
<p>The <strong>Blinn-Phong</strong> model describes light leaving a given surface as a sum of four components:</p>
<div style="margin-left: 2em">
<ul>
<li><strong>Emissive</strong>: The amount of light emitted by the surface</li>
<li>: The amount of reflected light that is scattered around the whole scene and doesn't have any visible source (used to brighten up the geometry)</li>
</ul>
</div>
<p>Ambient: The amount of reflected light that is scattered around the whole scene and doesn't have any visible source (used to brighten up the geometry) Diffuse: Describes the light reflected by rough surfaces (based on the Lambert lighting equation)</p>
<ul>
<li><strong>Specular</strong>: Describes the light reflected by shiny, slick surfaces</li>
</ul>
<p>Each of the above components may have a different color, which describes the surface material (diffuse color is usually taken from a texture). Each light source may also be represented with a separate color for each component (except the emissive). We can interpret it as how much given light source influences the ambient light available in the scene, how much diffuse lighting is emitted by the light source, and so on. We can, of course, modify the preceding algorithm to adjust it to our needs. This way we can achieve various results that are easy to calculate.</p>
<p>In this recipe, we will focus on the diffuse lighting and specular reflections. The former are described in the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe. The latter are calculated with a dot product of a surface normal vector and a half vector. A half vector is a vector that is halfway between a view vector (from the lit point to the viewer) and the light vector (from the lit point to the light source):</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="271" src="assets/image_11_005.png" width="417"/></div>
<p>The calculated dot product value is responsible for creating shinny light reflections on slick surfaces. As the area lit this way may be too big, the calculated value is then raised to the power. The higher the power value, the smaller and more concentrated are light reflexes on an object's surface. In the shader, these are calculated like this:</p>
<pre>
pow( dot( half_vector, normal_vector ), shinniness );
</pre>
<p>Normal vector is usually loaded along the geometry and provided by the application. The half vector is calculated as follows:</p>
<pre>
vec3 view_vector = normalize( eye_position.xyz - vert_position.xyz ); 
vec3 light_vector = normalize( light_position.xyz - vert_position.xyz ); 
vec3 half_vector = normalize( view_vector + light_vector );
</pre>
<p>To achieve correct results, all vectors must be normalized. Of course, specular highlights are not visible when the surface is not lit (or doesn't face the light source). So they should be calculated only when the diffuse component is greater than <kbd>0</kbd>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">How to do it...</h1>
            </header>

            <article>
                
<ol>
<li>Prepare Vulkan resources as described in the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe.</li>
<li>Create a pipeline layout using the prepared descriptor set layout with only a uniform buffer and also with a single push constant range accessed by a fragment shader stage, beginning at a 0<sup>th</sup> offset and of a <kbd>4 * sizeof( float )</kbd> size (refer to the <em>Creating a pipeline layout</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Create a shader module for a vertex shader stage using a SPIR-V assembly generated from the following GLSL code (refer to the <em>Converting GLSL shaders to SPIR-V assemblies</em> recipe from <a href="97217f0d-bed7-4ae1-a543-b4d599f299cf.xhtml"><span class="ChapterrefPACKT">Chapter 7</span></a>, <em>Shaders</em> and to <em>Creating a shader module</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>):</li>
</ol>
<pre>
      #version 450 
      layout( location = 0 ) in vec4 app_position; 
      layout( location = 1 ) in vec3 app_normal; 
      layout( set = 0, binding = 0 ) uniform UniformBuffer { 
        mat4 ModelViewMatrix; 
        mat4 ProjectionMatrix; 
      }; 
      layout( location = 0 ) out vec3 vert_position; 
      layout( location = 1 ) out vec3 vert_normal; 
      void main() { 
        vec4 position = ModelViewMatrix * app_position; 

        vert_position = position.xyz; 
        vert_normal = mat3( ModelViewMatrix ) * app_normal; 
        gl_Position = ProjectionMatrix * position; 
      }
</pre>
<ol start="4">
<li>Create a shader module for a fragment shader stage using a SPIR-V assembly generated from the following GLSL code:</li>
</ol>
<pre>
      #version 450 
      layout( location = 0 ) in vec3 vert_position; 
      layout( location = 1 ) in vec3 vert_normal; 
      layout( push_constant ) uniform LightParameters { 
        vec4 Position; 
      } Light; 
      layout( location = 0 ) out vec4 frag_color; 
      void main() { 
        vec3 normal_vector = normalize( vert_normal ); 
        vec3 light_vector = normalize( Light.Position.xyz - 
      vert_position ); 
        float diffuse_term = max( 0.0, dot( normal_vector, light_vector       ) ); 

        frag_color = vec4( diffuse_term + 0.1 ); 
        if( diffuse_term &gt; 0.0 ) { 
          vec3 view_vector = normalize( vec3( 0.0, 0.0, 0.0 ) - 
      vert_position.xyz ); 
          vec3 half_vector = normalize( view_vector + light_vector ); 
          float shinniness = 60.0; 
          float specular_term = pow( dot( half_vector, normal_vector ),       shinniness ); 
          frag_color += vec4( specular_term ); 
        } 
      }
</pre>
<ol start="5">
<li>Specify the pipeline shader stages with vertex and fragment shaders, both using a main function from the respective shader modules (refer to the <em>Specifying pipeline shader stages</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Create a graphics pipeline using the pipeline layout and shader stages presented previously. The rest of pipeline parameters remain identical to those presented in the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe.</li>
<li>Prepare a command buffer recording (or a rendering) function executed each frame. To do this, we need to begin a command buffer recording, copy data from the staging buffer to the uniform buffer (if needed), set an image memory barrier to transfer queue ownership for an image acquired from the swapchain, begin the render pass, set the viewport and scissor test states dynamically, bind the vertex buffers, descriptor sets and the graphics pipeline (refer to the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe).</li>
</ol>
<ol start="8">
<li>Prepare a position of a light source and provide it to the shaders through push constants. For this operation, provide the pipeline layout, a <kbd>VK_SHADER_STAGE_FRAGMENT_BIT</kbd> shader stage, <kbd>0</kbd> offset and a size of <kbd>sizeof( float ) * 4</kbd>, and a pointer to the data, in which the light source's position is stored (refer to the <em>Providing data to shaders through push constants</em> recipe from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
<li>Finalize the command buffer by recording the model drawing operation, ending the render pass, setting another image memory barrier for the swapchain image and ending the command buffer.</li>
<li>Submit the command buffer to the graphics queue and present an image (refer to the <em>Preparing a single frame of animation</em> and <em>Increasing the performance through increasing the number of separately rendered frames</em> recipes from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
</ol>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">How it works...</h1>
            </header>

            <article>
                
<p>The whole source code is almost identical to the one presented in the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe. The most important difference is in the vertex and fragment shaders, which perform lighting calculations based on data provided from the application. This time a light vector is not hardcoded in the shader. Instead, it is calculated using the data provided from the application. Positions and normal vectors are automatically read as vertex attributes. The position of a light source is read using a push constant, so we need to include a push constant range, when we create a pipeline layout:</p>
<pre>
std::vector&lt;VkPushConstantRange&gt; push_constant_ranges = { 
  { 
    VK_SHADER_STAGE_FRAGMENT_BIT,   // VkShaderStageFlags     stageFlags 
    0,                              // uint32_t               offset 
    sizeof( float ) * 4             // uint32_t               size 
  } 
}; 
InitVkDestroyer( LogicalDevice, PipelineLayout ); 
if( !CreatePipelineLayout( *LogicalDevice, { *DescriptorSetLayout }, push_constant_ranges, *PipelineLayout ) ) { 
  return false; 
}
</pre>
<p>Data to a push constant is provided during the command buffer recording operation:</p>
<pre>
BindVertexBuffers( command_buffer, 0, { { *VertexBuffer, 0 } } ); 
BindDescriptorSets( command_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS, *PipelineLayout, 0, DescriptorSets, {} ); 
BindPipelineObject( command_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS, *Pipeline ); 
std::array&lt;float, 4&gt; light_position = { 5.0f, 5.0f, 0.0f, 0.0f }; 
ProvideDataToShadersThroughPushConstants( command_buffer, *PipelineLayout, VK_SHADER_STAGE_FRAGMENT_BIT, 0, sizeof( float ) * 4, &amp;light_position[0] );
</pre>
<p>Through the push constant, we provide a position of a light source. This way our shaders becomes more universal, as we can calculate light vector directly in a shader and use it for lighting calculations.</p>
<p>In the following image, we can see the results of rendering a geometry lit with a diffuse and specular lighting calculated inside a fragment shader. The results of lighting calculations performed in the fragment shader are much better than if the same calculations were performed in the vertex shader. The lighting looks good even if the geometry is quite simple. But, of course, this comes with reduced performance.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_11_006.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">See also</h1>
            </header>

            <article>
                
<ul>
<li>In <a href="97217f0d-bed7-4ae1-a543-b4d599f299cf.xhtml"><span class="ChapterrefPACKT">Chapter 7</span></a>, <em>Shaders</em>, the recipe <em>Converting GLSL shaders to SPIR-V assemblies</em></li>
<li>In <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>, see the following recipes:
<ul>
<li><em>Creating a shader module</em></li>
<li><em>Specifying pipeline shader stages</em></li>
<li><em>Creating a pipeline layout</em></li>
</ul>
</li>
<li>In <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml">Chapter 9</a>, <em>Command Recording and Drawing</em>, see the following recipes::
<ul>
<li><em>Providing data to shaders through push constants</em></li>
</ul>
</li>
<li>The following recipe in this chapter:
<ul>
<li><em>Rendering a geometry with a vertex diffuse lighting</em></li>
</ul>
</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Rendering a normal mapped geometry</h1>
            </header>

            <article>
                
<p>Normal mapping is a technique that allows us to increase the details of a model's surface without increasing its geometrical complexity. Using this technique, normal vectors associated with vertices are not used during lighting calculations. They are replaced with normal vectors read from an image (a texture). This way, the shape of a model is unchanged, so we don't need additional processing power to transform vertices. However, the lighting quality is much better and depends only on the quality of a normal map image instead of the complexity of the model.</p>
<p>An example of an image generated with this recipe looks like the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_11_007.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Getting ready</h1>
            </header>

            <article>
                
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">Normal map is an image, in which normal vectors acquired from a highly detailed geometry are stored. It is used to simulate the high amount of surface details on a simple (low-polygon) geometry.</p>
<p><img class="alignnone size-full wp-image-1018 image-border" height="129" src="assets/image_11_008-1.png" width="647"/></p>
<p>For simple lighting calculations, we just need to load positions and normal vectors, but normal mapping requires us to load (or generate) much more data for a given 3D model. Apart from the above attributes, we also need texture coordinates, so we can sample a normal map inside fragment shaders, and two additional vectors: tangent and bitangent. The normal vector is perpendicular to the surface at a given point and points in a direction that is away from the surface. Tangent and bitangent vectors are tangential to the surface. Tangent vector points in the direction on object's surface, in which texture image advances <em>horizontally</em>, from left to right (<kbd>s</kbd> component of texture coordinates is increasing). Bitangent points in the direction on object's surface, in which the texture image advances <em>vertically</em>, from top to bottom (<kbd>t</kbd> component of texture coordinates is decreasing). Additionally, all three vectors - a normal, tangent and bitangent - should be perpendicular to each other (small deviations are acceptable) and have a length equal to <kbd>1.0</kbd>.</p>
<p>Normal, tangent and bitangent vectors are not used directly for lighting calculations. Instead, they form a rotation matrix, which can be used to convert a vector from texture (or tangent) space to a local model space or vice versa. This way we don't need to create a texture with normal vectors that can only be applied to a dedicated model, but we can prepare a general normal map and use it with an arbitrary geometry. Using the so called TBN matrix, we can load a normal vector from the texture and use it for lighting calculations performed in a coordinate system that is more convenient for us.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">How to do it...</h1>
            </header>

            <article>
                
<ol>
<li>Prepare Vulkan resources as described in the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe.</li>
<li>Load texture data from a file with a normal map (refer to the <em>Loading texture data from a file</em> recipe from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml">Chapter 10</a>, <em>Helper Recipes</em>).</li>
<li>Create a two-dimensional combined image sampler that has a color aspect and format (in example <kbd>VK_FORMAT_R8G8B8A8_UNORM</kbd>) and supports <kbd>VK_IMAGE_USAGE_SAMPLED_BIT</kbd> and <kbd>VK_IMAGE_USAGE_TRANSFER_DST_BIT</kbd> usages (refer to the <em>Creating a combined image sampler</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Copy data loaded from a normal map into the created image using a staging buffer (refer to the <em>Using staging buffer to update an image with a device-local memory bound</em> recipe from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>Load a 3D model from a file. Apart from the vertex positions and normal vectors, load also texture coordinates and load or generate tangent and bitangent vectors. Create a (vertex) buffer and copy loaded model data to the buffer's memory using a staging buffer (refer to the <em>Loading a 3D model from an OBJ file</em> from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml"><span class="ChapterrefPACKT">Chapter 10</span></a>, <em>Helper Recipes</em>).</li>
<li>Create a descriptor set layout with one uniform buffer accessed by a vertex shader at 0<sup>th</sup> binding and with one combined image sampler accessed by a fragment shader at 1<sup>st</sup> binding (refer to the <em>Creating a descriptor set layout</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Create a descriptor pool from which one uniform buffer descriptor and one combined image sampler descriptor can be allocated (refer to the <em>Creating a descriptor pool</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Allocate one descriptor set from the created pool using a descriptor set layout with one uniform buffer and one combined image sampler (refer to <em>Allocating descriptor sets</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Update the descriptor set with the uniform buffer accessed at the 0<sup>th</sup> binding and with the created combined image sampler with normal map data accessed at the 1<sup>st</sup> binding. Provide a <kbd>VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL</kbd> value as the image's layout (refer to the <em>Updating descriptor sets</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
</ol>
<ol start="10">
<li>Create a pipeline layout using the prepared descriptor set layout that also specifies a single push constant range accessed by a fragment shader stage, beginning at a 0<sup>th</sup> offset and of a <kbd>4 * sizeof( float )</kbd> size (refer to the <em>Creating a pipeline layout</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Create a shader module for a vertex shader stage using a SPIR-V assembly generated from the following GLSL code (refer to the <em>Converting GLSL shaders to SPIR-V assemblies</em> recipe from <a href="97217f0d-bed7-4ae1-a543-b4d599f299cf.xhtml"><span class="ChapterrefPACKT">Chapter 7</span></a>, <em>Shaders and to Creating a shader module</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>):</li>
</ol>
<pre>
      #version 450 
      layout( location = 0 ) in vec4 app_position; 
      layout( location = 1 ) in vec3 app_normal; 
      layout( location = 2 ) in vec2 app_texcoord; 
      layout( location = 3 ) in vec3 app_tangent; 
      layout( location = 4 ) in vec3 app_bitangent; 
      layout( set = 0, binding = 0 ) uniform UniformBuffer { 
        mat4 ModelViewMatrix; 
        mat4 ProjectionMatrix; 
      }; 
      layout( location = 0 ) out vec3 vert_position; 
      layout( location = 1 ) out vec2 vert_texcoord; 
      layout( location = 2 ) out vec3 vert_normal; 
      layout( location = 3 ) out vec3 vert_tanget; 
      layout( location = 4 ) out vec3 vert_bitanget; 
      void main() { 
        vec4 position = ModelViewMatrix * app_position; 
        gl_Position = ProjectionMatrix * position; 

        vert_position = position.xyz; 
        vert_texcoord = app_texcoord; 
        vert_normal = mat3( ModelViewMatrix ) * app_normal; 
        vert_tanget = mat3( ModelViewMatrix ) * app_tangent; 
        vert_bitanget = mat3( ModelViewMatrix ) * app_bitangent; 
      }
</pre>
<ol start="12">
<li>Create a shader module for a fragment shader stage using a SPIR-V assembly generated from the following GLSL code:</li>
</ol>
<pre>
      #version 450 
      layout( location = 0 ) in vec3 vert_position; 
      layout( location = 1 ) in vec2 vert_texcoord; 
      layout( location = 2 ) in vec3 vert_normal; 
      layout( location = 3 ) in vec3 vert_tanget; 
      layout( location = 4 ) in vec3 vert_bitanget; 
      layout( set = 0, binding = 1 ) uniform sampler2D ImageSampler; 
      layout( push_constant ) uniform LightParameters { 
        vec4 Position; 
      } Light; 
      layout( location = 0 ) out vec4 frag_color; 
      void main() { 
        vec3 normal = 2 * texture( ImageSampler, vert_texcoord ).rgb - 
      1.0; 
        vec3 normal_vector = normalize( mat3( vert_tanget, 
      vert_bitanget, vert_normal) * normal ); 
        vec3 light_vector = normalize( Light.Position.xyz - 
      vert_position ); 
        float diffuse_term = max( 0.0, dot( normal_vector, light_vector 
      ) ) * max( 0.0, dot( vert_normal, light_vector ) ); 
        frag_color = vec4( diffuse_term + 0.1 ); 
        if( diffuse_term &gt; 0.0 ) { 
          vec3 half_vector = normalize(normalize( -vert_position.xyz  ) 
      + light_vector); 
          float specular_term = pow( dot( half_vector, normal_vector ), 
      60.0 ); 
          frag_color += vec4( specular_term ); 
        } 
      }
</pre>
<ol start="13">
<li>Specify pipeline shader stages with vertex and fragment shaders, both using a <kbd>main</kbd> function from respective shader modules (refer to the <em>Specifying pipeline shader stages</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Specify a pipeline vertex input state with five attributes that are read from the same 0<sup>th</sup> binding. The binding should be created with data read per vertex and a stride equal to <kbd>14 * sizeof( float )</kbd> (refer to the <em>Specifying pipeline vertex input state</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>). The first attribute should have the following parameters:
<ul>
<li><kbd>0</kbd> value for <kbd>location</kbd></li>
<li><kbd>0</kbd> value for <kbd>binding</kbd></li>
<li><kbd>VK_FORMAT_R32G32B32_SFLOAT</kbd> value for <kbd>format</kbd></li>
<li><kbd>0</kbd> value for <kbd>offset</kbd></li>
</ul>
</li>
<li>The second attribute should be defined as follows:
<ul>
<li><kbd>1</kbd> value for <kbd>location</kbd></li>
<li><kbd>0</kbd> value for <kbd>binding</kbd></li>
<li><kbd>VK_FORMAT_R32G32B32_SFLOAT</kbd> value for <kbd>format</kbd></li>
<li><kbd>3 * sizeof( float )</kbd> value for <kbd>offset</kbd></li>
</ul>
</li>
</ol>
<ol start="16">
<li>The third attribute should have the following definition:
<ul>
<li><kbd>2</kbd> value for <kbd>location</kbd></li>
<li><kbd>0</kbd> value for <kbd>binding</kbd></li>
<li><kbd>VK_FORMAT_R32G32_SFLOAT</kbd> value for <kbd>format</kbd></li>
<li><kbd>6 * sizeof( float )</kbd> value for <kbd>offset</kbd></li>
</ul>
</li>
</ol>
<ol start="17">
<li>The fourth attribute should be specified as follows:
<ul>
<li><kbd>3</kbd> value for <kbd>location</kbd></li>
<li><kbd>0</kbd> value for <kbd>binding</kbd></li>
<li><kbd>VK_FORMAT_R32G32B32_SFLOAT</kbd> value for <kbd>format</kbd></li>
<li><kbd>8 * sizeof( float )</kbd> value for <kbd>offset</kbd></li>
</ul>
</li>
<li>The fifth attribute should use these values:
<ul>
<li><kbd>4</kbd> value for <kbd>location</kbd></li>
<li><kbd>0</kbd> value for <kbd>binding</kbd></li>
<li><kbd>VK_FORMAT_R32G32B32_SFLOAT</kbd> value for <kbd>format</kbd></li>
<li><kbd>11 * sizeof( float )</kbd> value for <kbd>offset</kbd></li>
</ul>
</li>
<li>In each frame of animation, record a command buffer, inside which copy data from the staging buffer to the uniform buffer, begin the render pass, set the viewport and scissor test state dynamically, bind the vertex buffer, the descriptor set and the graphics pipeline (refer to the <em>Rendering a geometry with a vertex diffuse lighting recipe</em>).</li>
<li>Prepare the position of a light source and provide it to shaders through push constants. For this operation, provide the pipeline layout, a <kbd>VK_SHADER_STAGE_FRAGMENT_BIT</kbd> shader stage, <kbd>0</kbd> offset and a size of <kbd>sizeof( float ) * 4</kbd>, and a pointer to the data, in which light source's position is stored (refer to the <em>Providing data to shaders through push constants</em> recipe from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
<li>Draw the model, record the rest of the required operations into the command buffer, submit the command buffer to the graphics queue, and present an image (refer to the P<em>reparing a single frame of animation and Increasing the performance through increasing the number of separately rendered frames</em> recipes from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
</ol>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">How it works...</h1>
            </header>

            <article>
                
<p>To use normal mapping in our application, we need to prepare an image, in which normal vectors are stored. We must load the image's contents, create an image, and copy the data to the image's memory. We also need to create a sampler that, along with the image, will form a combined image sampler:</p>
<pre>
int width = 1; 
int height = 1; 
std::vector&lt;unsigned char&gt; image_data; 
if( !LoadTextureDataFromFile( "Data/Textures/normal_map.png", 4, image_data, &amp;width, &amp;height ) ) { 
  return false; 
} 
InitVkDestroyer( LogicalDevice, Sampler ); 
InitVkDestroyer( LogicalDevice, Image ); 
InitVkDestroyer( LogicalDevice, ImageMemory ); 
InitVkDestroyer( LogicalDevice, ImageView ); 
if( !CreateCombinedImageSampler( PhysicalDevice, *LogicalDevice, VK_IMAGE_TYPE_2D, VK_FORMAT_R8G8B8A8_UNORM, { (uint32_t)width, (uint32_t)height, 1 }, 
  1, 1, VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT, VK_IMAGE_VIEW_TYPE_2D, VK_IMAGE_ASPECT_COLOR_BIT, VK_FILTER_LINEAR, 
  VK_FILTER_LINEAR, VK_SAMPLER_MIPMAP_MODE_NEAREST, VK_SAMPLER_ADDRESS_MODE_REPEAT, VK_SAMPLER_ADDRESS_MODE_REPEAT, 
  VK_SAMPLER_ADDRESS_MODE_REPEAT, 0.0f, false, 1.0f, false, VK_COMPARE_OP_ALWAYS, 0.0f, 1.0f, VK_BORDER_COLOR_FLOAT_OPAQUE_BLACK, 
  false, *Sampler, *Image, *ImageMemory, *ImageView ) ) { 
  return false; 
} 
VkImageSubresourceLayers image_subresource_layer = { 
  VK_IMAGE_ASPECT_COLOR_BIT,    // VkImageAspectFlags     aspectMask 
  0,                            // uint32_t               mipLevel 
  0,                            // uint32_t               baseArrayLayer 
  1                             // uint32_t               layerCount 
}; 
if( !UseStagingBufferToUpdateImageWithDeviceLocalMemoryBound( PhysicalDevice, *LogicalDevice, static_cast&lt;VkDeviceSize&gt;(image_data.size()), 
  &amp;image_data[0], *Image, image_subresource_layer, { 0, 0, 0 }, { (uint32_t)width, (uint32_t)height, 1 }, VK_IMAGE_LAYOUT_UNDEFINED, 
  VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL, 0, VK_ACCESS_SHADER_READ_BIT, VK_IMAGE_ASPECT_COLOR_BIT, VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, 
  VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, GraphicsQueue.Handle, FrameResources.front().CommandBuffer, {} ) ) { 
  return false; 
}
</pre>
<p>After that, we need to load a 3D model. We need to load positions, normal vectors, and texture coordinates. Tangent and bitangent vectors must also be loaded, but as the <kbd>.obj</kbd> format cannot store so many different attributes, we must generate them (this is performed inside the <kbd>Load3DModelFromObjFile()</kbd>):</p>
<pre>
uint32_t vertex_stride = 0; 
if( !Load3DModelFromObjFile( "Data/Models/ice.obj", true, true, true, true, Model, &amp;vertex_stride ) ) { 
  return false; 
}
</pre>
<p>Now we need to modify the descriptor set described in the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe. First, we start by creating a proper layout:</p>
<pre>
std::vector&lt;VkDescriptorSetLayoutBinding&gt; descriptor_set_layout_bindings = { 
  { 
    0, 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1, 
    VK_SHADER_STAGE_VERTEX_BIT, 
    nullptr 
  }, 
  { 
    1, 
    VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
    1, 
    VK_SHADER_STAGE_FRAGMENT_BIT, 
    nullptr 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorSetLayout ); 
if( !CreateDescriptorSetLayout( *LogicalDevice, descriptor_set_layout_bindings, *DescriptorSetLayout ) ) { 
  return false; 
}
</pre>
<p>Next, a descriptor pool is required. From it a descriptor set is allocated:</p>
<pre>
std::vector&lt;VkDescriptorPoolSize&gt; descriptor_pool_sizes = { 
  { 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1 
  }, 
  { 
    VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
    1 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorPool ); 
if( !CreateDescriptorPool( *LogicalDevice, false, 1, descriptor_pool_sizes, *DescriptorPool ) ) { 
  return false; 
} 
if( !AllocateDescriptorSets( *LogicalDevice, *DescriptorPool, { *DescriptorSetLayout }, DescriptorSets ) ) { 
  return false; 
}
</pre>
<p>When the descriptor set is allocated, we can update it with handles of the uniform buffer and the combined image sampler:</p>
<pre>
BufferDescriptorInfo buffer_descriptor_update = { 
  DescriptorSets[0], 
  0, 
  0, 
  VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
  { 
    { 
      *UniformBuffer, 
      0, 
      VK_WHOLE_SIZE 
    } 
  } 
}; 
ImageDescriptorInfo image_descriptor_update = { 
  DescriptorSets[0], 
  1, 
  0, 
  VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
  { 
    { 
      *Sampler, 
      *ImageView, 
      VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL 
    } 
  } 
}; 
UpdateDescriptorSets( *LogicalDevice, { image_descriptor_update }, { buffer_descriptor_update }, {}, {} );
</pre>
<p>This time we have not two but five vertex attributes, so we also need to modify the vertex input state:</p>
<pre>
std::vector&lt;VkVertexInputBindingDescription&gt; vertex_input_binding_descriptions = { 
  { 
    0, 
    vertex_stride, 
    VK_VERTEX_INPUT_RATE_VERTEX 
  } 
}; 
std::vector&lt;VkVertexInputAttributeDescription&gt; vertex_attribute_descriptions = { 
  { 
    0, 
    0, 
    VK_FORMAT_R32G32B32_SFLOAT, 
    0 
  }, 
  { 
    1, 
    0, 
    VK_FORMAT_R32G32B32_SFLOAT, 
    3 * sizeof( float ) 
  }, 
  { 
    2, 
    0, 
    VK_FORMAT_R32G32_SFLOAT, 
    6 * sizeof( float ) 
  }, 
  { 
    3, 
    0, 
    VK_FORMAT_R32G32B32_SFLOAT, 
    8 * sizeof( float ) 
  }, 
  { 
    4, 
    0, 
    VK_FORMAT_R32G32B32_SFLOAT, 
    11 * sizeof( float ) 
  } 
}; 
VkPipelineVertexInputStateCreateInfo vertex_input_state_create_info; 
SpecifyPipelineVertexInputState( vertex_input_binding_descriptions, vertex_attribute_descriptions, vertex_input_state_create_info );
</pre>
<p>The preceding attributes are read in the vertex shader, which transforms the vertex position to a clip space using the model-view and projection matrices. Additionally, the view-space position and unmodified texture coordinates are passed to the fragment shader. Normal, tangent and bitangent vectors are also passed to the fragment shader, but they are first transformed to a view space with the model-view matrix:</p>
<pre>
vec4 position = ModelViewMatrix * app_position; 
gl_Position = ProjectionMatrix * position; 

vert_position = position.xyz; 
vert_texcoord = app_texcoord; 
vert_normal = mat3( ModelViewMatrix ) * app_normal; 
vert_tangent = mat3( ModelViewMatrix ) * app_tangent; 
vert_bitangent = mat3( ModelViewMatrix ) * app_bitangent;
</pre>
<p>The most important part, from the normal mapping perspective, takes place in the fragment shader. It first reads the normal vector from the texture. Usually, textures store values that are in the <kbd>0.0</kbd> - <kbd>1.0</kbd> range inclusive (unless we use signed-normalized texture formats: <kbd>SNORM</kbd>). However, all the components of normal vectors may have values in the <kbd>-1.0</kbd> - <kbd>1.0</kbd> range, so we need to expand the loaded normal vector like this:</p>
<pre>
vec3 normal = 2 * texture( ImageSampler, vert_texcoord ).rgb - 1.0;
</pre>
<p>The fragment shader calculates diffuse and specular lighting in the same way as described in the <em>Rendering a geometry with a fragment specular lighting</em> recipe. It just takes the normal vector loaded from the texture instead of the one provided from the vertex shader. There is just one additional thing it needs to perform: all the vectors (light and view) are in the view space, but the normal vector stored in the normal map is in the tangent space, so it also needs to be converted to the same view space. This is done with a TBN matrix formed from the normal, tangent and bitangent vectors. They are provided from the vertex shader. Because the vertex shader transforms them from the model space into a view space (by multiplying them by the model-view matrix), the created TBN matrix converts the normal vector from the tangent space directly into the view space:</p>
<pre>
vec3 normal_vector = normalize( mat3( vert_tanget, vert_bitanget, vert_normal) * normal );
</pre>
<p><kbd>mat3()</kbd> is a constructor for creating a 3x3 matrix from three-component vectors. Using such a matrix, we can perform rotations and scaling, but no translation. Since we want to transform directions (unit-length vectors), this is exactly what we need in this situation.</p>
<p>Normal mapping can give us impressive lighting even on very simple (low-poly) geometry. In the image below, on the left we can see normal mapped geometry with many polygons; while on the right, similar geometry is presented but with fewer vertices.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_11_009.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">See also</h1>
            </header>

            <article>
                
<ul>
<li>In <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>, the recipe <em>Using staging buffer to update an image with a device local memory bound</em></li>
<li>In <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>, the recipe <em>Creating a combined image sampler</em></li>
<li>In <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml"><span class="ChapterrefPACKT">Chapter 10</span></a>, <em>Helper Recipes</em>, see the following recipes:
<ul>
<li><em>Loading texture data from a file</em></li>
<li><em>Loading a 3D model from an OBJ file</em></li>
</ul>
</li>
<li>Also, look at the following recipes in the same chapter:
<ul>
<li><em>Rendering a geometry with a vertex diffuse lighting</em></li>
<li><em>Rendering a geometry with a fragment specular lighting<br/></em></li>
</ul>
</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Drawing a reflective and refractive geometry using cubemaps</h1>
            </header>

            <article>
                
<p>In real-life, transparent objects both transmit light rays, and also reflect them. If an object's surface is viewed from high angles, we see more light being reflected. Looking at an object's surface more directly, we see more light being transmitted through the object. Simulating such an effect may generate very plausible results. In this recipe, we will see how to render a geometry that is both refractive and reflective.</p>
<p>An example of an image generated with this recipe looks like:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="306" src="assets/image_11_010.png" width="513"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Getting ready</h1>
            </header>

            <article>
                
<p>Cubemaps are textures with images covering six sides of a cube. They usually store the view of a scene from a given position. The most common use for cubemaps are skyboxes. They are also handy when we want to map reflections on a surface of a given model. Another example of common use is to simulate transparent objects (that is made of glass), which refract light rays. Very low resolution cubemaps (in example 4x4 pixels) can even be used directly for ambient lighting.</p>
<p>Cubemaps contain six two-dimensional images. All of them are square and have the same size. In Vulkan, cubemaps are created using 2D images with six array layers, for which a cubemap image view is created. Through it, the six array layers are interpreted as cubemap faces in the following order: <kbd>+X</kbd>, <kbd>-X</kbd>, <kbd>+Y</kbd>, <kbd>-Y</kbd>, <kbd>+Z</kbd>, <kbd>-Z</kbd>.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_11_011.png"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Images courtesy of Emil Persson (<a href="http://www.humus.name">h t t p ://w w w . h u m u s . n a m e</a>)</div>
<p>Six sides of a cubemap correspond to six directions, as if we stayed in one position, turned around, and took photos of the world around us. Using such a texture, we can simulate the world being reflected from the surface of the object or being transmitted through the object. However, when an object moves too far from the place the texture was created for, the illusion is broken until we apply a new texture that is valid for the new position.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">How to do it...</h1>
            </header>

            <article>
                
<ol>
<li>Prepare Vulkan resources as described in the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe.</li>
<li>Load a 3D model data from file with vertex positions and normal vectors. This model will be displayed as the one reflecting and transmitting the environment (refer to the <em>Loading a 3D model from an OBJ file</em> recipe from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml"><span class="ChapterrefPACKT">Chapter</span> 10</a>, <em>Helper Recipes</em>).</li>
<li>Create a (vertex) buffer with a memory object and use it store the vertex data for our model (refer to the <em>Creating a buffer, Allocating and binding memory object to a buffer</em> and <em>Using staging buffer to update buffer with a device-local memory bound</em> recipes from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>Load a 3D model containing vertex positions of a cube. This model will be used to display the environment being reflected (refer to the <em>Drawing a skybox</em> recipe from <a href="82938796-18a3-413b-b05d-47816d70e49a.xhtml"><span class="ChapterrefPACKT">Chapter 12</span></a>, <em>Advanced Rendering Techniques</em>).</li>
<li>Create a buffer, along with a memory object bound it, to hold the vertex data of the environment (skybox).</li>
<li>Create a two-dimensional combined image sampler with six array layers and a cube image view. It must support <kbd>VK_IMAGE_USAGE_SAMPLED_BIT</kbd> and <kbd>VK_IMAGE_USAGE_TRANSFER_DST_BIT</kbd> uses. A <kbd>VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE</kbd> sampler address mode must be used for all addressing dimensions (refer to the <em>Creating a combined image sampler</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Load texture data for all six sides of a cubemap from files (refer to the <em>Loading texture data from a file</em> recipe from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml"><span class="ChapterrefPACKT">Chapter 10</span></a>, <em>Helper Recipes</em>).</li>
<li>Upload each loaded texture to a separate array layer of a created combined image sampler. Textures should be uploaded in the following order: positive and negative X, positive and negative Y, positive and negative Z (refer to the <em>Using staging buffer to update an image with a device-local memory bound</em> from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>Create a descriptor set layout with two descriptor resources: a uniform buffer accessed in a vertex shader at 0<sup>th</sup> binding and with a combined image sampler accessed in a fragment shader at 1<sup>st</sup> binding (refer to the <em>Creating a descriptor set layout</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Create a descriptor pool, from which one uniform buffer descriptor and one combined image sampler descriptor can be allocated (refer to the <em>Creating a descriptor pool</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
</ol>
<ol start="11">
<li>Allocate a descriptor set from the created pool using the descriptor set layout with a uniform buffer and a combined image sampler resources (refer to the <em>Allocating descriptor sets</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Update (populate) the descriptor set with the uniform buffer accessed at the 0<sup>th</sup> binding and with the created combined image sampler (cubemap) accessed at the 1<sup>st</sup> binding. Provide a <kbd>VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL</kbd> value as the cubemap's layout (refer to the <em>Updating descriptor sets</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Create a pipeline layout using the prepared descriptor set layout that also specifies a single push constant range accessed by a fragment shader stage, beginning at a 0<sup>th</sup> offset and of a <kbd>4 * sizeof( float )</kbd> size (refer to the <em>Creating a pipeline layout</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Create a graphics pipeline used for drawing a reflective and refractive model. Start by creating a shader module for a vertex shader stage using a SPIR-V assembly generated from the following GLSL code (refer to the <em>Converting GLSL shaders to SPIR-V assemblies</em> recipe from <a href="97217f0d-bed7-4ae1-a543-b4d599f299cf.xhtml"><span class="ChapterrefPACKT">Chapter 7</span></a>, <em>Shaders</em> and to Creating a shader module recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>):</li>
</ol>
<pre>
      #version 450 
      layout( location = 0 ) in vec4 app_position; 
      layout( location = 1 ) in vec3 app_normal; 
      layout( set = 0, binding = 0 ) uniform UniformBuffer { 
        mat4 ModelViewMatrix; 
        mat4 ProjectionMatrix; 
      }; 
      layout( location = 0 ) out vec3 vert_position; 
      layout( location = 1 ) out vec3 vert_normal; 
      void main() { 
        vert_position = app_position.xyz; 
        vert_normal = app_normal; 

        gl_Position = ProjectionMatrix * ModelViewMatrix * 
      app_position; 
      }
</pre>
<ol start="15">
<li>Create a shader module for a fragment shader stage using a SPIR-V assembly generated from the following GLSL code:</li>
</ol>
<pre>
      #version 450 
      layout( location = 0 ) in vec3 vert_position; 
      layout( location = 1 ) in vec3 vert_normal; 
      layout( set = 0, binding = 1 ) uniform samplerCube Cubemap; 
      layout( push_constant ) uniform LightParameters { 
        vec4 Position; 
      } Camera; 
      layout( location = 0 ) out vec4 frag_color; 
      void main() { 
        vec3 view_vector = vert_position - Camera.Position.xyz; 

        float angle = smoothstep( 0.3, 0.7, dot( normalize( -
      view_vector ), vert_normal ) ); 

        vec3 reflect_vector = reflect( view_vector, vert_normal ); 
        vec4 reflect_color = texture( Cubemap, reflect_vector ); 

        vec3 refrac_vector = refract( view_vector, vert_normal, 0.3 ); 
        vec4 refract_color = texture( Cubemap, refrac_vector ); 

        frag_color = mix( reflect_color, refract_color, angle ); 
      }
</pre>
<ol start="16">
<li>Specify pipeline shader stages with vertex and fragment shaders, both using a <kbd>main</kbd> function from the respective shader modules (refer to the <em>Specifying pipeline shader stages</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Create a graphics pipeline for drawing a model using the preceding pipeline shader stages definition, with the rest of the pipeline's parameters defined in the same way as in the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe.</li>
<li>Create a graphics pipeline for drawing an environment being reflected--a skybox (refer to the <em>Drawing a skybox</em> recipe from <a href="82938796-18a3-413b-b05d-47816d70e49a.xhtml"><span class="ChapterrefPACKT">Chapter 12</span></a>, <em>Advanced Rendering Techniques</em>).</li>
<li>To render a frame, record a command buffer in each iteration of a rendering loop. In the command buffer, copy data from the staging buffer to the uniform buffer, begin the render pass, set the viewport and scissor test states dynamically and bind the descriptor set (refer to the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe).</li>
<li>Bind the graphics pipeline and the vertex buffer created for the reflective/refractive model.</li>
<li>Prepare the position of a camera, from which the scene is observed and provide it to shaders through push constants. For this operation, provide the pipeline layout, a <kbd>VK_SHADER_STAGE_FRAGMENT_BIT</kbd> shader stage, <kbd>0</kbd> offset and a size of <kbd>sizeof( float ) * 4</kbd>, and a pointer to the data, in which the camera's position is stored (refer to the <em>Providing data to shaders through push constants</em> recipe from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
<li>Draw the model (refer to the <em>Drawing a geometry</em> recipe from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
</ol>
<ol start="23">
<li>Bind the graphics pipeline and the vertex buffer created for the skybox and draw it.</li>
<li>Record the rest of the required operations into the command buffer, submit the command buffer to the graphics queue, and present an image (refer to the <em>Preparing a single frame of animation and Increasing the performance through increasing the number of separately rendered frames</em> recipes from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>).</li>
</ol>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">How it works...</h1>
            </header>

            <article>
                
<p>We start this recipe by loading and preparing buffers for two models: the first being the one simulating our main scene (reflective/refractive model); and second being used to draw the environment itself (a skybox). We need to copy vertex data using staging buffers to both vertex buffers.</p>
<p>Next, we need to create a cubemap. We do this by creating a combined image sampler. The image must be of a 2D type, must have six array layers, and must support <kbd>VK_IMAGE_USAGE_SAMPLED_BIT</kbd> and <kbd>VK_IMAGE_USAGE_TRANSFER_DST_BIT</kbd> usages. The format of the image depends on the case, but usually a <kbd>VK_FORMAT_R8G8B8A8_UNORM</kbd> would be a good choice. The created sampler must use a <kbd>VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE</kbd> addressing mode for all sampling dimensions (<kbd>u</kbd>, <kbd>v</kbd>, and <kbd>w</kbd>), otherwise we might see the edges of all cubemap faces:</p>
<pre>
if( !CreateCombinedImageSampler( PhysicalDevice, *LogicalDevice, VK_IMAGE_TYPE_2D, VK_FORMAT_R8G8B8A8_UNORM, { 1024, 1024, 1 }, 1, 6, 
  VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT, VK_IMAGE_VIEW_TYPE_CUBE, VK_IMAGE_ASPECT_COLOR_BIT, VK_FILTER_LINEAR, 
  VK_FILTER_LINEAR, VK_SAMPLER_MIPMAP_MODE_NEAREST, VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, 
  VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, 0.0f, false, 1.0f, false, VK_COMPARE_OP_ALWAYS, 0.0f, 1.0f, VK_BORDER_COLOR_FLOAT_OPAQUE_BLACK, 
  false, *CubemapSampler, *CubemapImage, *CubemapImageMemory, *CubemapImageView ) ) { 
  return false; 
}
</pre>
<p>Next, we need to upload data to the cubemap image. In this sample we load data from six separate files and copy it to six layers of an image like this:</p>
<pre>
std::vector&lt;std::string&gt; cubemap_images = { 
  "Data/Textures/Skansen/posx.jpg", 
  "Data/Textures/Skansen/negx.jpg", 
  "Data/Textures/Skansen/posy.jpg", 
  "Data/Textures/Skansen/negy.jpg", 
  "Data/Textures/Skansen/posz.jpg", 
  "Data/Textures/Skansen/negz.jpg" 
}; 
for( size_t i = 0; i &lt; cubemap_images.size(); ++i ) { 
  std::vector&lt;unsigned char&gt; cubemap_image_data; 
  int image_data_size; 
  if( !LoadTextureDataFromFile( cubemap_images[i].c_str(), 4, cubemap_image_data, nullptr, nullptr, nullptr, &amp;image_data_size ) ) { 
    return false; 
  } 
  VkImageSubresourceLayers image_subresource = { 
    VK_IMAGE_ASPECT_COLOR_BIT, 
    0, 
    static_cast&lt;uint32_t&gt;(i), 
    1 
  }; 
  UseStagingBufferToUpdateImageWithDeviceLocalMemoryBound( PhysicalDevice, *LogicalDevice, image_data_size, &amp;cubemap_image_data[0], 
    *CubemapImage, image_subresource, { 0, 0, 0 }, { 1024, 1024, 1 }, VK_IMAGE_LAYOUT_UNDEFINED, VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL, 
    0, VK_ACCESS_SHADER_READ_BIT, VK_IMAGE_ASPECT_COLOR_BIT, VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, 
    GraphicsQueue.Handle, FrameResources.front().CommandBuffer, {} ); 
}
</pre>
<p>We also need a descriptor set through which a fragment shader will be able to access the cubemap. To allocate a descriptor set its layout is required:</p>
<pre>
std::vector&lt;VkDescriptorSetLayoutBinding&gt; descriptor_set_layout_bindings = { 
  { 
    0, 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1, 
    VK_SHADER_STAGE_VERTEX_BIT, 
    nullptr 
  }, 
  { 
    1, 
    VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
    1, 
    VK_SHADER_STAGE_FRAGMENT_BIT, 
    nullptr 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorSetLayout ); 
if( !CreateDescriptorSetLayout( *LogicalDevice, descriptor_set_layout_bindings, *DescriptorSetLayout ) ) { 
  return false; 
}
</pre>
<p>Descriptor sets are allocated from pools. So now we create one and allocate the descriptor set itself:</p>
<pre>
std::vector&lt;VkDescriptorPoolSize&gt; descriptor_pool_sizes = { 
  { 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1 
  }, 
  { 
    VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
    1 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorPool ); 
if( !CreateDescriptorPool( *LogicalDevice, false, 1, descriptor_pool_sizes, *DescriptorPool ) ) { 
  return false; 
} 
if( !AllocateDescriptorSets( *LogicalDevice, *DescriptorPool, { *DescriptorSetLayout }, DescriptorSets ) ) { 
  return false; 
}
</pre>
<p>One last step connected with descriptor resources is to update the created set with handles of resources that should be accessed in shaders:</p>
<pre>
BufferDescriptorInfo buffer_descriptor_update = { 
  DescriptorSets[0], 
  0, 
  0, 
  VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
  { 
    { 
      *UniformBuffer, 
      0, 
      VK_WHOLE_SIZE 
    } 
  } 
}; 
ImageDescriptorInfo image_descriptor_update = { 
  DescriptorSets[0], 
  1, 
  0, 
  VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
  { 
    { 
      *CubemapSampler, 
      *CubemapImageView, 
      VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL 
    } 
  } 
}; 
UpdateDescriptorSets( *LogicalDevice, { image_descriptor_update }, { buffer_descriptor_update }, {}, {} );
</pre>
<p>After descriptor sets, it's time to create a render pass and a graphics pipeline, or rather two pipelines: one for drawing the model, and one for drawing the environment (a skybox). A graphics pipeline used for the model is very similar to the one created in the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe, except it uses different shader programs and a push constant range, so we need to include it during pipeline layout creation:</p>
<pre>
std::vector&lt;VkPushConstantRange&gt; push_constant_ranges = { 
  { 
    VK_SHADER_STAGE_FRAGMENT_BIT, 
    0, 
    sizeof( float ) * 4 
  } 
}; 
InitVkDestroyer( LogicalDevice, PipelineLayout ); 
if( !CreatePipelineLayout( *LogicalDevice, { *DescriptorSetLayout }, push_constant_ranges, *PipelineLayout ) ) { 
  return false; 
}
</pre>
<p>The vertex shader, as usual, calculates the clip space position of a vertex and passes the unmodified position and normal vector to the fragment shader:</p>
<pre>
vert_position = app_position.xyz; 
vert_normal = app_normal; 

gl_Position = ProjectionMatrix * ModelViewMatrix * app_position;
</pre>
<p>Calculating reflections or refractions is most easily done in the world space and we should transform both vectors to this coordinate system. However, to simplify the recipe, the above vertex shader makes an assumption that the model is already provided in the world space, that's why unmodified vectors (position and normal) are passed to the fragment shader. It then takes these vectors and uses them to calculate both reflected and refracted vectors with built-in <kbd>reflect()</kbd> and <kbd>refract()</kbd> functions. Calculated vectors are used to read values from the cubemap. They are then mixed together based on the viewing angle:</p>
<pre>
vec3 view_vector = vert_position - Camera.Position.xyz; 

float angle = smoothstep( 0.3, 0.7, dot( normalize( -view_vector ), vert_normal ) ); 

vec3 reflect_vector = reflect( view_vector, vert_normal ); 
vec4 reflect_color = texture( Cubemap, reflect_vector ); 

vec3 refrac_vector = refract( view_vector, vert_normal, 0.3 ); 
vec4 refract_color = texture( Cubemap, refrac_vector ); 

frag_color = mix( reflect_color, refract_color, angle );
</pre>
<p>As for the creation of a graphics pipeline used for the skybox rendering, there is a dedicated <em>Drawing a skybox</em> recipe in <a href="82938796-18a3-413b-b05d-47816d70e49a.xhtml"><span class="ChapterrefPACKT">Chapter 12</span></a>, <em>Advanced Rendering Techniques</em>.</p>
<p>One last thing we should focus on is a command buffer recording. Here we render two objects, not one, so first we need to set an appropriate state required to properly draw the model:</p>
<pre>
BindDescriptorSets( command_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS, *PipelineLayout, 0, DescriptorSets, {} ); 
BindPipelineObject( command_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS, *ModelPipeline ); 
BindVertexBuffers( command_buffer, 0, { { *ModelVertexBuffer, 0 } } ); 
ProvideDataToShadersThroughPushConstants( command_buffer, *PipelineLayout, VK_SHADER_STAGE_FRAGMENT_BIT, 0, sizeof( float ) * 4, &amp;Camera.GetPosition()[0] ); 
for( size_t i = 0; i &lt; Model.Parts.size(); ++i ) { 
  DrawGeometry( command_buffer, Model.Parts[i].VertexCount, 1, Model.Parts[i].VertexOffset, 0 ); 
}
</pre>
<p>Immediately after the preceding code, we render the skybox:</p>
<pre>
BindPipelineObject( command_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS, *SkyboxPipeline ); 
BindVertexBuffers( command_buffer, 0, { { *SkyboxVertexBuffer, 0 } } ); 
for( size_t i = 0; i &lt; Skybox.Parts.size(); ++i ) { 
  DrawGeometry( command_buffer, Skybox.Parts[i].VertexCount, 1, Skybox.Parts[i].VertexOffset, 0 ); 
}
</pre>
<p>Of course, we don't need to render the environment--the reflections (and refractions) are stored in the texture. However, usually we also want to see the environment being reflected, not only the reflections.</p>
<p>All the knowledge in this recipe combined with the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe, should generate the results seen in the following image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="199" src="assets/image_11_012.png" width="475"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">See also</h1>
            </header>

            <article>
                
<ul>
<li>In <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em><span>, the recipe</span> <em>Creating a combined image sampler</em></li>
<li>In <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Command Recording and Drawing</em>, the recipe <em>Providing data to shaders through push constants</em></li>
<li>In <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml"><span class="ChapterrefPACKT">Chapter 10</span></a>, <em>Helper Recipes</em>, see the following recipes:
<ul>
<li><em>Loading texture data from a file</em></li>
<li><em>Loading a 3D model from an OBJ file</em></li>
</ul>
</li>
<li>The recipe <em>Rendering a geometry with a vertex diffuse lighting</em>, in this chapter</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Adding shadows to the scene</h1>
            </header>

            <article>
                
<p>Lighting is one of the most important operations performed by 3D applications. Unfortunately, due to the specifics of graphics libraries and the graphics hardware itself, lighting calculations have one major drawback--they don't have information about positions of all drawn objects. That's why generating shadows requires a special approach and advanced rendering algorithms.</p>
<p>There are several popular techniques targeted at efficient generation of natural looking shadows. Now we will learn about a technique called shadow mapping.</p>
<p>An example of an image generated with this recipe looks like:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="247" src="assets/image_11_013.png" width="414"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Getting ready</h1>
            </header>

            <article>
                
<p>The shadow mapping technique requires us to render a scene twice. Firstly, we render objects that cast shadows. They are rendered from the light's point of view. This way we store depth values in a depth attachment (color values are not required).</p>
<p>Then, in the second step, we render the scene as we normally do, from the camera's point of view. Inside shaders we use the shadow map generated in the first step. The vertex position is projected onto the shadow map and its distance from the light position is compared with the value read from the shadow map. If it is greater, it means a given point is covered in shadow, otherwise it is normally lit.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">How to do it...</h1>
            </header>

            <article>
                
<ol>
<li>Prepare the Vulkan resources as described in the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe.</li>
<li>Load the 3D models with vertex positions and normal vectors. Store loaded data in a (vertex) buffer.</li>
<li>Create a uniform buffer with <kbd>VK_BUFFER_USAGE_TRANSFER_DST_BIT</kbd> and <kbd>VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT</kbd> usages that are big enough to hold data for three 16-element matrices of floating-point values (refer to the <em>Creating a uniform buffer</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Create a staging buffer supporting a <kbd>VK_BUFFER_USAGE_TRANSFER_SRC_BIT</kbd> usage, which is able to hold data for three matrices each with 16 floating-point elements. The buffer's memory object should be allocated on a memory that is host-visible (refer to the <em>Creating a buffer and Allocating and binding memory object to a buffer</em> recipes from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>Create a combined image sampler that should act as a shadow map. The image should be two-dimensional with one of the supported depth formats (<kbd>VK_FORMAT_D16_UNORM</kbd> must always be supported), and should support <kbd>VK_IMAGE_USAGE_SAMPLED_BIT</kbd> and <kbd>VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT</kbd> usages (refer to the <em>Creating a combined image sampler</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Create a descriptor set layout with two descriptor resources: a uniform buffer accessed in a vertex shader at 0<sup>th</sup> binding and with a combined image sampler accessed in a fragment shader at 1<sup>st</sup> binding (refer to the <em>Creating a descriptor set layout</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>Create a descriptor pool, from which one uniform buffer descriptor and one combined image sampler descriptor can be allocated (refer to the <em>Creating a descriptor pool</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li>A descriptor set from the created pool using the descriptor set layout with a uniform buffer and a combined image sampler resources (refer to the <em>Allocating descriptor sets</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
<li class="packt_nosymbol">Update (populate) the descriptor set with the uniform buffer accessed at the 0<sup>th</sup> binding and with the created combined image sampler (shadow map) accessed at the 1<sup>st</sup> binding. Provide a <kbd>VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL</kbd> value as the image's layout (refer to the <em>Updating descriptor sets</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Descriptor Sets</em>).</li>
</ol>
<ol start="10">
<li class="packt_nosymbol">Prepare data for a render pass used for drawing the whole scene into the shadow map. This render pass should have only one attachment, which has the same format as the combined image sampler's format. The image should be cleared on load, its initial layout may be undefined. The image contents should be stored at the end of the render pass and the final layout should be set to a <kbd>VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL</kbd> (refer to the <em>Specifying attachments descriptions</em> recipe from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Render Passes and Framebuffers</em>).</li>
<li class="packt_nosymbol">The render pass used for shadow map generation should have just one subpass with only a depth attachment, for which framebuffer's 0<sup>th</sup> attachment with a <kbd>VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL</kbd> layout should be used (refer to the <em>Specifying subpass descriptions</em> recipe from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml">Chapter 6</a>, <em>Render Passes and Framebuffers</em>).</li>
<li>Specify two subpass dependencies for the render pass (refer to the <em>Specifying dependencies between subpasses</em> recipe from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Render Passes and Framebuffers</em>). Use the following values for the first dependency:
<ul>
<li><kbd>VK_SUBPASS_EXTERNAL</kbd> value for <kbd>srcSubpass</kbd></li>
<li><kbd>0</kbd> value for <kbd>dstSubpass</kbd></li>
<li><kbd>VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT</kbd> value for <kbd>srcStageMask</kbd></li>
<li><kbd>VK_PIPELINE_STAGE_EARLY_FRAGMENT_TESTS_BIT</kbd> value for <kbd>dstStageMask</kbd></li>
<li><kbd>VK_ACCESS_SHADER_READ_BIT</kbd> value for <kbd>srcAccessMask</kbd></li>
<li><kbd>VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_WRITE_BIT</kbd> value for <kbd>dstAccessMask</kbd></li>
<li><kbd>VK_DEPENDENCY_BY_REGION_BIT</kbd> value for <kbd>dependencyFlags</kbd></li>
</ul>
</li>
<li>Use the following values for the second render pass dependency:
<ul>
<li><kbd>0</kbd> value for <kbd>srcSubpass</kbd></li>
<li><kbd>VK_SUBPASS_EXTERNAL</kbd> value for <kbd>dstSubpass</kbd></li>
<li><kbd>VK_PIPELINE_STAGE_LATE_FRAGMENT_TESTS_BIT</kbd> value for <kbd>srcStageMask</kbd></li>
<li><kbd>VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT</kbd> value for <kbd>dstStageMask</kbd></li>
<li><kbd>VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_WRITE_BIT</kbd> value for <kbd>srcAccessMask</kbd></li>
<li><kbd>VK_ACCESS_SHADER_READ_BIT</kbd> value for <kbd>dstAccessMask</kbd></li>
<li><kbd>VK_DEPENDENCY_BY_REGION_BIT</kbd> value for <kbd>dependencyFlags</kbd></li>
</ul>
</li>
</ol>
<ol start="14">
<li>Create a render pass using the above parameters (refer to the <em>Creating a render pass</em> recipe from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Render Passes and Framebuffers</em>).</li>
<li>Create a framebuffer compatible with the created render pass. The Framebuffer should have one attachment, for which the image view created along with the shadow map's combined image sampler should be used. Framebuffer should also have the same dimensions as the shadow map image (refer to the <em>Creating a framebuffer</em> recipe from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Render Passes and Framebuffers</em>).</li>
<li>Create a second render pass used for drawing the scene normally into a swapchain (refer to the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe).</li>
<li>Create a pipeline layout using the prepared descriptor set layout. Also, specify a single push constant range accessed by a vertex shader stage, beginning at a 0<sup>th</sup> offset and of a <kbd>4 * sizeof( float )</kbd> size (refer to the <em>Creating a pipeline layout</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Create a graphics pipeline used for drawing a scene into the shadow map. Start by creating a shader module for a vertex shader stage using a SPIR-V assembly generated from the following GLSL code (refer to the <em>Converting GLSL shaders to SPIR-V assemblies</em> recipe from <a href="97217f0d-bed7-4ae1-a543-b4d599f299cf.xhtml">Chapter 7</a>, <em>Shaders</em> and to <em>Creating a shader module</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>):</li>
</ol>
<pre>
      #version 450 
      layout( location = 0 ) in vec4 app_position; 
      layout( set = 0, binding = 0 ) uniform UniformBuffer { 
        mat4 ShadowModelViewMatrix; 
        mat4 SceneModelViewMatrix; 
        mat4 ProjectionMatrix; 
      }; 
      void main() { 
        gl_Position = ProjectionMatrix * ShadowModelViewMatrix *       
      app_position; 
      }
</pre>
<ol start="19">
<li>Specify pipeline shader stages with a vertex shader only, which uses a <kbd>main</kbd> function from the prepared shader module (refer to the <em>Specifying pipeline shader</em> stages recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
</ol>
<ol start="20">
<li>Specify a pipeline vertex input state with one attribute that is read from the 0<sup>th</sup> binding. The binding should be created with data read per vertex and a stride equal to <kbd>6 * sizeof( float )</kbd> (refer to the <em>Specifying pipeline vertex input state</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter</span> 8</a>, <em>Graphics and Compute Pipelines</em>). The attribute should have the following parameters:
<ul>
<li><kbd>0</kbd> value for <kbd>location</kbd></li>
<li><kbd>0</kbd> value for <kbd>binding</kbd></li>
<li><kbd>VK_FORMAT_R32G32B32_SFLOAT</kbd> value for <kbd>format</kbd></li>
<li><kbd>0</kbd> value for <kbd>offset</kbd></li>
</ul>
</li>
<li>Specify the viewport and scissor test parameters with one viewport, whose dimensions match the size of the shadow map image (refer to the <em>Specifying pipeline viewport and scissor test state</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter</span> 8</a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Create a graphics pipeline using the previously specified parameters. Skip the blending state, because the render pass used for the shadow map generation doesn't have any color attachments (rasterization must be enabled though, because otherwise no fragments will be generated and their depth won't be stored in the shadow map). Also, don't use dynamic states, because the size of the shadow map doesn't change (refer to the <em>Creating graphics pipelines</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Create another graphics pipeline used for rendering a shadowed scene. This time, create a shader module for a vertex shader stage using a SPIR-V assembly generated from the following GLSL code:</li>
</ol>
<pre>
      #version 450 

      layout( location = 0 ) in vec4 app_position; 
      layout( location = 1 ) in vec3 app_normal; 

      layout( set = 0, binding = 0 ) uniform UniformBuffer { 
        mat4 ShadowModelViewMatrix; 
        mat4 SceneModelViewMatrix; 
        mat4 ProjectionMatrix; 
      }; 

      layout( push_constant ) uniform LightParameters { 
        vec4 Position; 
      } Light; 

      layout( location = 0 ) out vec3 vert_normal; 
      layout( location = 1 ) out vec4 vert_texcoords; 
      layout( location = 2 ) out vec3 vert_light; 

      const mat4 bias = mat4(  
        0.5, 0.0, 0.0, 0.0, 
        0.0, 0.5, 0.0, 0.0, 
        0.0, 0.0, 1.0, 0.0, 
        0.5, 0.5, 0.0, 1.0 ); 

      void main() { 
        gl_Position = ProjectionMatrix * SceneModelViewMatrix * 
      app_position; 

        vert_normal = mat3( SceneModelViewMatrix ) * app_normal; 
        vert_texcoords = bias * ProjectionMatrix * 
      ShadowModelViewMatrix * app_position; 
        vert_light = (SceneModelViewMatrix * vec4( Light.Position.xyz, 
      0.0 ) ).xyz; 
      }
</pre>
<ol start="24">
<li>Create a shader module for a fragment shader stage using a SPIR-V assembly generated from the following GLSL code:</li>
</ol>
<pre>
#version 450 

layout( location = 0 ) in vec3 vert_normal; 
layout( location = 1 ) in vec4 vert_texcoords; 
layout( location = 2 ) in vec3 vert_light; 

layout( set = 0, binding = 1 ) uniform sampler2D ShadowMap; 

layout( location = 0 ) out vec4 frag_color; 

void main() { 
  float shadow = 1.0; 
  vec4 shadow_coords = vert_texcoords / vert_texcoords.w; 

  if( texture( ShadowMap, shadow_coords.xy ).r &lt; shadow_coords.z - 0.005 ) { 
    shadow = 0.5; 
  } 

  vec3 normal_vector = normalize( vert_normal ); 
  vec3 light_vector = normalize( vert_light ); 
  float diffuse_term = max( 0.0, dot( normal_vector, light_vector ) ); 

  frag_color = shadow * vec4( diffuse_term ) + 0.1; 
}
</pre>
<ol start="25">
<li>Specify pipeline shader stages with vertex and fragment shaders, both using a <kbd>main</kbd> function from a respective shader modules.</li>
<li>Specify a pipeline vertex input state with two attributes that are read from the same 0<sup>th</sup> binding. The binding should be created with data read per vertex and a stride equal to <kbd>6 * sizeof( float )</kbd>. The first attribute should have the following parameters:
<ul>
<li><kbd>0</kbd> value for <kbd>location</kbd></li>
<li><kbd>0</kbd> value for <kbd>binding</kbd></li>
<li><kbd>VK_FORMAT_R32G32B32_SFLOAT</kbd> value for <kbd>format</kbd></li>
<li>0 value for <kbd>offset</kbd></li>
</ul>
</li>
<li>The second attribute should have the following definition:
<ul>
<li><kbd>1</kbd> value for <kbd>location</kbd></li>
<li><kbd>0</kbd> value for <kbd>binding</kbd></li>
<li><kbd>VK_FORMAT_R32G32B32_SFLOAT</kbd> value for <kbd>format</kbd></li>
<li><kbd>3 * sizeof( float )</kbd> value for <kbd>offset</kbd></li>
</ul>
</li>
<li>Create a graphics pipeline for rendering the shadowed scene using the above shader stages and two attributes, with the rest of the parameters similar to those defined in the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe.</li>
<li>Prepare a view matrix, which can be a multiplication of rotation, scaling and translation matrices used to draw the scene from the light's perspective (refer to the <em>Preparing a translation matrix</em>, <em>Preparing a rotation matrix</em> and <em>Preparing a scaling matrix</em> recipes from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml"><span class="ChapterrefPACKT">Chapter 10</span></a>, <em>Helper Recipes</em>). Copy the contents of the concatenated matrix to the staging buffer at a <kbd>0</kbd> offset (refer to the <em>Mapping, updating and unmapping host-visible memory</em> recipe from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
</ol>
<ol start="30">
<li>Prepare a view matrix used to draw the scene normally, from the camera's perspective. Copy the contents of this matrix to the staging buffer at a <kbd>16 * sizeof( float )</kbd> offset.</li>
<li>Prepare a perspective projection matrix based on the aspect ratio of the swapchain's dimensions (refer to the <em>Preparing a perspective projection matrix</em> recipe from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml"><span class="ChapterrefPACKT">Chapter 10</span></a>, <em>Helper Recipes</em>). Copy the contents of the matrix to the staging buffer at a <kbd>32 * sizeof( float )</kbd>. Remember to recreate the projection matrix and copy it to the staging buffer each time the application's window is resized (refer to the <em>Mapping, updating and unmapping host-visible memory</em> recipe from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>In each frame of animation, record a command buffer. Start by checking whether any of the view or projection matrices were modified: if they were, copy the contents of the staging buffer to the uniform buffer, guarded by the proper pipeline barriers (refer to the <em>Copying data between buffers</em> recipe from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Resources and Memory</em>).</li>
<li>Begin a render pass used for drawing the scene from the light's perspective into the shadow map. Bind the vertex buffer, descriptor set, and the pipeline used to fill the shadow map. Draw the geometry and end the render pass.</li>
<li>Transfer ownership of the acquired swapchain image if necessary. Set the viewport and scissor test states dynamically, bind the graphics pipeline created for rendering the shadowed scene, and draw the geometry once again. End the command buffer recording, submit the command buffer to the queue, and present an image.</li>
</ol>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">How it works...</h1>
            </header>

            <article>
                
<p>We start by creating a combined image sampler, in which depth information from the light's perspective will be stored:</p>
<pre>
if( !CreateCombinedImageSampler( PhysicalDevice, *LogicalDevice, VK_IMAGE_TYPE_2D, DepthFormat, { 512, 512, 1 }, 1, 1, 
  VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT | VK_IMAGE_USAGE_SAMPLED_BIT, VK_IMAGE_VIEW_TYPE_2D, VK_IMAGE_ASPECT_DEPTH_BIT, VK_FILTER_LINEAR, 
  VK_FILTER_LINEAR, VK_SAMPLER_MIPMAP_MODE_NEAREST, VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, 
  VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, 0.0f, false, 1.0f, false, VK_COMPARE_OP_ALWAYS, 0.0f, 1.0f, VK_BORDER_COLOR_FLOAT_OPAQUE_BLACK, 
  false, *ShadowMapSampler, *ShadowMap.Image, *ShadowMap.Memory, *ShadowMap.View ) ) { 
  return false; 
}
</pre>
<p>The combined image sampler, along with uniform buffer, will be accessed in shaders, so we need a descriptor set through which shaders will have access to both. Despite the fact that we render the scene twice using two different pipelines, we can use one descriptor set to avoid unnecessary state switching:</p>
<pre>
std::vector&lt;VkDescriptorSetLayoutBinding&gt; descriptor_set_layout_bindings = { 
  { 
    0, 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1, 
    VK_SHADER_STAGE_VERTEX_BIT, 
    nullptr 
  }, 
  { 
    1, 
    VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
    1, 
    VK_SHADER_STAGE_FRAGMENT_BIT, 
    nullptr 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorSetLayout ); 
if( !CreateDescriptorSetLayout( *LogicalDevice, descriptor_set_layout_bindings, *DescriptorSetLayout ) ) { 
  return false; 
} 

std::vector&lt;VkDescriptorPoolSize&gt; descriptor_pool_sizes = { 
  { 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1 
  }, 
  { 
    VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
    1 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorPool ); 
if( !CreateDescriptorPool( *LogicalDevice, false, 1, descriptor_pool_sizes, *DescriptorPool ) ) { 
  return false; 
} 
if( !AllocateDescriptorSets( *LogicalDevice, *DescriptorPool, { *DescriptorSetLayout }, DescriptorSets ) ) { 
  return false; 
}
</pre>
<p>We also need to populate the descriptor set with the handles of a uniform buffer and the combined image sampler:</p>
<pre>
BufferDescriptorInfo buffer_descriptor_update = { 
  DescriptorSets[0], 
  0, 
  0, 
  VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
  { 
    { 
      *UniformBuffer, 
      0, 
      VK_WHOLE_SIZE 
    } 
  } 
}; 

ImageDescriptorInfo image_descriptor_update = { 
  DescriptorSets[0], 
  1, 
  0, 
  VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
  { 
    { 
      *ShadowMapSampler, 
      *ShadowMap.View, 
      VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL 
    } 
  } 
}; 
UpdateDescriptorSets( *LogicalDevice, { image_descriptor_update }, { buffer_descriptor_update }, {}, {} );
</pre>
<p>The next step is to create a dedicated render pass for storing the depth information in the shadow map. It doesn't use any color attachments, because we only need depth data. We also create a framebuffer. It can have fixed dimensions as we don't change the size of the shadow map:</p>
<pre>
std::vector&lt;VkAttachmentDescription&gt; shadow_map_attachment_descriptions = { 
  { 
    0 
    DepthFormat, 
    VK_SAMPLE_COUNT_1_BIT, 
    VK_ATTACHMENT_LOAD_OP_CLEAR, 
    VK_ATTACHMENT_STORE_OP_STORE, 
    VK_ATTACHMENT_LOAD_OP_DONT_CARE, 
    VK_ATTACHMENT_STORE_OP_DONT_CARE, 
    VK_IMAGE_LAYOUT_UNDEFINED, 
    VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL 
  } 
}; 
VkAttachmentReference shadow_map_depth_attachment = { 
  0, 
  VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL 
}; 
std::vector&lt;SubpassParameters&gt; shadow_map_subpass_parameters = { 
  { 
    VK_PIPELINE_BIND_POINT_GRAPHICS, 
    {}, 
    {}, 
    {}, 
    &amp;shadow_map_depth_attachment, 
    {} 
  } 
}; 
std::vector&lt;VkSubpassDependency&gt; shadow_map_subpass_dependencies = { 
  { 
    VK_SUBPASS_EXTERNAL, 
    0, 
    VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, 
    VK_PIPELINE_STAGE_EARLY_FRAGMENT_TESTS_BIT, 
    VK_ACCESS_SHADER_READ_BIT, 
    VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_WRITE_BIT, 
    VK_DEPENDENCY_BY_REGION_BIT 
  }, 
  { 
    0, 
    VK_SUBPASS_EXTERNAL, 
    VK_PIPELINE_STAGE_LATE_FRAGMENT_TESTS_BIT, 
    VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, 
    VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_WRITE_BIT, 
    VK_ACCESS_SHADER_READ_BIT, 
    VK_DEPENDENCY_BY_REGION_BIT 
  } 
}; 
InitVkDestroyer( LogicalDevice, ShadowMapRenderPass ); 
if( !CreateRenderPass( *LogicalDevice, shadow_map_attachment_descriptions, shadow_map_subpass_parameters, shadow_map_subpass_dependencies, 
  *ShadowMapRenderPass ) ) { 
  return false; 
} 
InitVkDestroyer( LogicalDevice, ShadowMap.Framebuffer ); 
if( !CreateFramebuffer( *LogicalDevice, *ShadowMapRenderPass, { *ShadowMap.View }, 512, 512, 1, *ShadowMap.Framebuffer ) ) { 
  return false; 
}
</pre>
<p>Next, we create two graphics pipelines. They both use the same push constant range to lower the number of variables (though only the second pipeline uses it in shaders):</p>
<pre>
std::vector&lt;VkPushConstantRange&gt; push_constant_ranges = { 
  { 
    VK_SHADER_STAGE_VERTEX_BIT, 
    0, 
    sizeof( float ) * 4 
  } 
}; 
InitVkDestroyer( LogicalDevice, PipelineLayout ); 
if( !CreatePipelineLayout( *LogicalDevice, { *DescriptorSetLayout }, push_constant_ranges, *PipelineLayout ) ) { 
  return false; 
}
</pre>
<p>The first pipeline is for the shadow map generation. It uses very simple shaders that read only vertex positions and render the scene from the light's point of view.</p>
<p>The second pipeline renders the scene normally into the swapchain image. Its shaders are more complicated. A vertex shader calculates the position normally, but also converts the normal vector and the light vector into the view space for correct lighting calculations:</p>
<pre>
vert_normal = mat3( SceneModelViewMatrix ) * app_normal; 
vert_light = (SceneModelViewMatrix * vec4( Light.Position.xyz, 0.0 ) ).xyz;
</pre>
<p>The most important thing that the vertex shader does is to calculate the vertex's position in the light source's view space. To do this, we multiply it by the light's model-view and projection matrices (perspective division is done in the fragment shader). The acquired result is used to fetch data from the shadow map. However, the calculated position values (after perspective division) are in the <kbd>-1.0 - 1.0</kbd> range and reading data from textures using normalized texture coordinates requires providing the values in the <kbd>0.0 - 1.0</kbd> range. That's why we need to bias the result:</p>
<pre>
vert_texcoords = bias * ProjectionMatrix * ShadowModelViewMatrix * app_position;
</pre>
<p>This way the fragment shader can project the interpolated position onto the shadow map and read the value from a proper coordinate:</p>
<pre>
float shadow = 1.0; 
vec4 shadow_coords = vert_texcoords / vert_texcoords.w; 
if( texture( ShadowMap, shadow_coords.xy ).r &lt; shadow_coords.z - 0.005 ) { 
  shadow = 0.5; 
}
</pre>
<p>The value read from the shadow map is compared with the point's distance from the light's position (offset by a small value). If the distance is greater than the value stored in the shadow map, the point is lying in a shadow and shouldn't be lit. We need to add the small offset, so the surface of an object doesn't cast shadows on itself (only on parts that are further away). We also don't fully discard the lighting to avoid the shadows being too dark, hence the value <kbd>0.5</kbd> assigned to the shadow variable.</p>
<p>The above calculations can be performed using a <kbd>textureProj()</kbd>and a <kbd>sampler2DShadow</kbd>. This way perspective division, offsetting the distance and comparing it to a reference value is performed automatically.</p>
<p>The rest of the resources created in this recipe are similar to those presented in the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe. Rendering/recording a command buffer requires us, apart from the usual stuff, to render the scene twice. Firstly, we fill the shadow map by drawing all objects from the light's perspective. The shadow map is then used during the rendering of all the objects normally from the camera's perspective:</p>
<pre>
BeginRenderPass( command_buffer, *ShadowMapRenderPass, *ShadowMap.Framebuffer, { { 0, 0, }, { 512, 512 } }, { { 1.0f, 0 } }, VK_SUBPASS_CONTENTS_INLINE ); 
BindVertexBuffers( command_buffer, 0, { { *VertexBuffer, 0 } } ); 
BindDescriptorSets( command_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS, *PipelineLayout, 0, DescriptorSets, {} ); 
BindPipelineObject( command_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS, *ShadowMapPipeline ); 
DrawGeometry( command_buffer, Scene[0].Parts[0].VertexCount + Scene[1].Parts[0].VertexCount, 1, 0, 0 ); 
EndRenderPass( command_buffer ); 
if( PresentQueue.FamilyIndex != GraphicsQueue.FamilyIndex ) { 
  ImageTransition image_transition_before_drawing = { 
    Swapchain.Images[swapchain_image_index], 
    VK_ACCESS_MEMORY_READ_BIT, 
    VK_ACCESS_MEMORY_READ_BIT, 
    VK_IMAGE_LAYOUT_UNDEFINED, 
    VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL, 
    PresentQueue.FamilyIndex, 
    GraphicsQueue.FamilyIndex, 
    VK_IMAGE_ASPECT_COLOR_BIT 
  }; 
  SetImageMemoryBarrier( command_buffer, VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT, VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT, { image_transition_before_drawing } ); 
} 
BeginRenderPass( command_buffer, *SceneRenderPass, framebuffer, { { 0, 0 }, Swapchain.Size }, { { 0.1f, 0.2f, 0.3f, 1.0f }, { 1.0f, 0 } }, VK_SUBPASS_CONTENTS_INLINE ); 

VkViewport viewport = { 
  0.0f, 
  0.0f, 
  static_cast&lt;float&gt;(Swapchain.Size.width), 
  static_cast&lt;float&gt;(Swapchain.Size.height), 
  0.0f, 
  1.0f, 
}; 
SetViewportStateDynamically( command_buffer, 0, { viewport } ); 

VkRect2D scissor = { 
  { 
    0, 
    0 
  }, 
  { 
    Swapchain.Size.width, 
    Swapchain.Size.height 
  } 
}; 
SetScissorsStateDynamically( command_buffer, 0, { scissor } ); 
BindPipelineObject( command_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS, *ScenePipeline ); 
ProvideDataToShadersThroughPushConstants( command_buffer, *PipelineLayout, VK_SHADER_STAGE_VERTEX_BIT, 0, sizeof( float ) * 4, &amp;LightSource.GetPosition()[0] ); 
DrawGeometry( command_buffer, Scene[0].Parts[0].VertexCount + Scene[1].Parts[0].VertexCount, 1, 0, 0 ); 
EndRenderPass( command_buffer );
</pre>
<p>The following image shows different models casting shadows on a flat plane:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_11_014.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">See also</h1>
            </header>

            <article>
                
<ul>
<li><span class="ChapterrefPACKT">In <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a></span>, <em>Descriptor Sets</em>, see the recipe:
<ul>
<li><em>Creating a combined image sampler</em></li>
</ul>
</li>
<li>In <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Render Passes and Framebuffers</em>, see the following recipes:
<ul>
<li><em>Creating a render pass</em></li>
<li><em>Creating a framebuffer</em></li>
</ul>
</li>
<li>The recipe <em>Rendering a geometry with a vertex diffuse lighting</em>, in this chapter</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>