# Chapter 4. Basic IR Transformations

Until now, we have seen how the IR is independent of its target and how it can be used to generate code for a specific backend. To generate efficient code for the backend, we optimize the IR generated by the frontend by running a series of analysis and transformation passes using the LLVM pass manager. We must note that most of the optimizations that happen in a compiler take place on the IR, one of the reasons being that the IR is retargetable and the same set of optimizations would be valid for a number of targets. It reduces the effort of writing the same optimization for every target. There are some target-specific optimizations too; they happen at the selection DAG level, which we will see later. Another reason for IR being the target of optimization is that LLVM IR is in SSA form, which means every variable is assigned only once and every new assignment to a variable is a new variable itself. One very visible benefit of this representation is that we don't have to do reaching definition analysis where some variable is assigned a value of another variable. SSA representation also helps in a number of optimizations such as constant propagation, dead code elimination, and so on. Going ahead, we will see some of the important optimizations in LLVM, what is the role of LLVM Pass Infrastructure, and how we can use the opt tool to perform different optimizations.

In this chapter, we will cover following topics:

*   The opt tool
*   Pass and Pass Manager
*   Using other Pass info in own pass
*   IR simplification examples
*   IR combination examples

# Opt Tool

**Opt** is the LLVM Optimizer and analyzer tool that is run on LLVM IR to optimize the IR or produce an analysis about it. We saw in the first chapter a very basic introduction to the opt tool, and how to use it to run analysis and transformation passes. In this section, we will see what else the opt tool does. We must note that opt is a developer tool and all the optimizations that it provides can be invoked from the frontend as well.

With the opt tool, we can specify the level of optimization that we need, which means we can specify the optimization levels from `O0`, `O1`, `O2`, to `O3`(`O0` being the least optimized code and `O3` being the most optimized code). Apart from these, there is also an optimization level `Os` or `Oz`, which deals with space optimization. The syntax to invoke one of these optimizations is:

[PRE0]

Here, x represents the optimization level, which can have a value from 0 to 3 or s or z. These optimization levels are similar to what Clang frontend specifies. `-O0` represents no optimization whereas `–O1` means only few optimizations are enabled. `–O2` is a moderate level of optimization and `–O3` is the highest level of optimization, which is similar to `–O2` but it allows optimization that takes longer to perform or may generate larger code (the O3 level does not guarantee that the code will be the most optimized and efficient, it just says that the compiler will try more to optimize the code and in the process may break things also). `–Os` means optimization for size, basically not running optimizations which increase code size (for example, it removes `slp-vectorizer` optimization) and perform optimizations that reduce code size (for example, instruction combining optimization).

We can direct the opt tool to run a specific pass that we require. These passes can be one of the already defined passes listed at [http://llvm.org/docs/Passes.html](http://llvm.org/docs/Passes.html) or one of the passes we have written ourselves. The passes listed in the above link are also run in the optimization levels of `-O1`, `-O2`, and `-O3`. To view which pass is being run at a certain optimization level, use the `-debug-pass=Structure` command-line option along with the opt tool.

Let's take an example to demonstrate the difference between the `O1` and `O2` level of optimization. The `O3` level generally has one or two more passes from `O2`. So, let's take an example and see how much the `O2` level of optimization optimizes the code. Write the test code in the `test.ll` file:

[PRE1]

In this test code, the `callercaller` function calls the `caller` function, which in turn calls the `test` function, which performs an addition of two numbers and returns the value to its caller, which in turn returns the value to the `callercaller` function.

Now, run the `O1` and `O2` levels of optimization, as shown:

[PRE2]

The following screenshot shows the difference in the optimized code for the `O1` and `O2` levels:

![Opt Tool](img/00003.jpeg)

As we can see, the code in `O2` has optimized the calls to the function and the `Add` operations as well and returns the result directly from the `callercaller` function. This is obtained due to the fact that `O2` optimization runs the passes **always-inline** which inlines all the function calls and treats the code as one big function. Then, in also runs the `globaldce` pass, which eliminates unreachable internals from the code. After this, it runs `constmerge` which merges duplicate global constants into a single constant. It also performs a global value numbering pass that eliminates partially or fully redundant instructions and eliminates redundant load instructions.

# Pass and Pass Manager

LLVM's `Pass` infrastructure is one of the many important features of the LLVM system. There are a number of analysis and optimization passes that can be run using this infrastructure. The starting point for LLVM passes is the `Pass` class, which is a superclass of all the passes. We need to inherit from some predefined subclasses taking into account what our pass is going to implement.

*   **ModulePass**: This is the most general superclass. By inheriting this class we allow the entire module to be analyzed at once. The functions within the module may not be referred to in a particular order. To use it, write a subclass that inherits from the `ModulePass` subclass and overloads the `runOnModule` function.

    ### Note

    Before going ahead with the discussion of other `Pass` classes, let's look into the three virtual methods that the `Pass` classes override:

    *   **doInitialization**: This is meant to do initialization stuff that does not depend on the current function being processed.
    *   **runOn{Passtype}**: This is the method where we should implement our subclass for the functionality of the pass. This will be `runOnFunction` for `FunctionPass`, `runOnLoop` for `LoopPass`, and so on.
    *   **doFinalization**: This is called when `runOn{Passtype}` has finished doing the job for every function in the program.

*   **FunctionPass**: These passes execute on each function present in the module, independent from other functions in the module. There is no defined order in which the functions will be processed. They are not allowed to modify functions other than the one being processed, and any addition or deletion of functions from the current module is also not allowed. To implement `FunctionPass` we might need to overload the three virtual functions mentioned earlier by implementing in the `runOnFunction` method.
*   **BasicBlockPass**: These passes work on basic blocks one at a time, independently of other basic blocks present in the program. They are not allowed to add or delete any new basic block or change the CFG. They are also not allowed to do things that `FunctionPass` is not allowed to. To implement, they can override the `doInitialization` and `doFinalization` methods of `FunctionPass`, or overload their own virtual methods for the two methods mentioned earlier and the `runOnBasicBlock` method.
*   **LoopPass**: These passes work on each loop in the function, independent of all other loops within the function. Loops are processed in such a way that the outermost loop is executed the last. To implement `LoopPass` we need to overload the `doInitialization`, `doFinalization`, and `runOnLoop` methods.

Now, let's see how to get started with writing a custom pass. Let's write a pass that will print the names of all the functions.

Before getting started with writing the implementation of the pass, we need to make changes in a few places in the code so that the pass is recognized and can be run.

We need to create a directory under the LLVM tree. Let's make a directory, `lib/Transforms/FnNamePrint`. In this directory, we need to create a `Makefile` with the following contents, which will allow our pass to be compiled:

[PRE3]

This specifies that all `.cpp` files should be compiled and linked into a shared object that will be available in the `lib` folder of the `build-folder` (`build-folder/lib/FnNamePrint.so`).

Now, let's get started with writing the actual pass implementation. We need to create the source file for the pass in `lib/Transforms/FnNamePrint`: let's name it `FnNamePrint.cpp`. The first step now is to choose the correct subclass. In this case, as we are trying to print names of each function, the `FunctionPass` class will serve our purpose by processing one function at a time. Also, we are only printing the name of function and not modifying anything within it, so we are choosing `FunctionPass` for simplicity. We could use `ModulePass` as well because it is an `Immutable Pass`.

Now, let's write the source code for the pass implementation, which looks like this:

[PRE4]

In the preceding code we `include` the necessary headers first and use an `llvm` namespace:

[PRE5]

We declare our pass as a structure, `FnNamePrint`, which is a subclass of `FunctionPass`. In `runOnFunction` we implement the logic to print the function name. The `bool` value returned in the end signifies whether we have made any modification within the function. A `True` value is returned if some modifications was made, otherwise, `false` is returned. In our case, we are not making any modifications, so we return `false`.

[PRE6]

Then, we declare the `ID` for the pass, which is used to identify the pass:

[PRE7]

Finally, we need to register the passes with the Pass Manager. The first argument is the Pass name used by the `opt` tool to identify this pass. The second argument is the actual Pass name. The third and fourth arguments specify whether the pass modified the `cfg` and whether it is an analysis pass.

[PRE8]

### Note

The implementation of the pass is done. Now, before we use it, we need to build LLVM using the `make` command, which will build the shared object in the `lib` folder within the build (`build-folder/lib/FnNamePrint.so`).

Now, we can run the pass over a test case using the `opt` tool in the following way:

[PRE9]

The `load` command line option specifies the path from where to pick the shared object of the pass and `–funcnameprint` is the option to opt tool to tell it to run the pass we have written. The Pass will print the names of all the function present in the testcase. For the example in the first section it will print out:

[PRE10]

So, we got started with writing a Pass. Now, we will see the significance of the `PassManager` class in LLVM.

The `PassManager` class schedules the passes to be run efficiently. The `PassManager` is used by all LLVM tools that run passes for the execution of these passes. It is the responsibility of the `PassManager` to make sure the interaction between the passes is correctly done. As it tries to execute the passes in an optimized way, it must have information regarding how the passes interact with each other and what the different dependencies between the passes are.

A pass itself can specify the dependency on other passes, that is, which passes need to be run before the execution of the current pass. Also, it can specify the passes that will be invalidated by the execution of the current pass. The `PassManager` gets the analysis results before a pass is executed. We will later see how a pass can specify such dependencies.

The main work of the `PassManager` is to avoid the calculation of analysis results time and again. This is done by keeping track of which analyses are available, which are invalidated, and which analyses are required. The `PassManager` tracks the lifetimes of the analysis results and frees the memory holding the analysis results when not required, allowing for optimal memory use.

The `PassManager` pipelines the passes together to get better memory and cache results, improving the cache behavior of the compiler. When a series of consecutive `FunctionPass` are given, it will execute all the `FunctionPass` on the first function, then all the `FunctionPass` on the second function, and so on. This improves cache behavior as it is only dealing with the single function part of the LLVM representation and not the entire program.

The `PassManager` also specifies the `–debug-pass` option with which we can see how one pass interacts with other passes. We can see what all passes are run using the `–debug-pass=Argument` option. We can use the `–debug-pass=Structure` option to see how the passes had run. It will also give us the names of the passes that ran. Let's take the example of the test code in the first section of this chapter:

[PRE11]

In the output, the `Pass Arguments` gives us the passes that are run and the following list is the structure used to run each pass. The Passes just after `ModulePass` `Manager` will show the passes run per module (here, it is empty). The passes in hierarchy of `FunctionPass` `Manager` show that these passes were run per function (`Function Name Print` and `Module Verifier`), which is the expected result.

The `PassManger` also provides some other useful flags, some of which are the following:

*   **time-passes**: This gives time information about the pass along with the other passes that are lined up.
*   **stats**: This prints statistics about each pass.
*   **instcount**: This collects the count of all instructions and reports them. `–stats` must also be Passes to the opt tool so that the results of `instcount` are visible.

# Using other Pass info in current Pass

For the Pass Manager to work optimally it needs to know the dependencies between the Passes. Each of the passes can itself declare its dependencies: the analysis passes that need to be executed before this pass is executed and the passes that will get invalidated after the current pass is run. To specify these dependencies, a pass needs to implement the `getAnalysisUsage` method.

[PRE12]

Using this method the current pass can specify the required and invalidated sets by filling in the details in the `AnalysisUsage` object. To fill in the information the Pass needs to call any of the following methods:

## AnalysisUsage::addRequired<> method

This method arranges for the execution of a Pass prior to the current Pass. One example of this is: for memory copy optimization it needs the results of an alias analysis:

[PRE13]

By adding the pass required to run, it is made sure that `Alias Analysis Pass` is run before the `MemCpyOpt` Pass. Also, this makes sure that if the `Alias Analysis` has been invalidated by some other Pass, it will be run before the `MemCpyOpt` Pass is run.

## AnalysisUsage:addRequiredTransitive<> method

When an analysis chains to other analyses for results, this method should be used instead of the `addRequired` method. That is, when we need to preserve the order in which the analysis passes are run we use this method. For example:

[PRE14]

Here, `DependenceAnalysis` chains to `AliasAnalysis`, `ScalarEvolution` and `LoopInfo` Passes for the results.

## AnalysisUsage::addPreserved<> method

By using this method a Pass can specify which analyses of other Passes it will not invalidate on running: that is, it will preserve the information already present, if any. This means that the subsequent passes that require the analysis would not need to run this again.

For example, in the case of the `MemCpyOpt` Pass seen earlier, it required the `AliasAnalysis` results and it also preserved them. Also:

[PRE15]

To get a detailed understanding of how everything is linked and works together, you can pick up any of the transformation passes and go through the source code and you will know how they are getting information from other passes and how they are using it.

# Instruction simplification example

In this section, we will see how we fold instructions into simpler forms in LLVM. Here, the creation of new instructions will not take place. Instruction simplification does constant folding:

[PRE16]

That is, it simplifies the `sub` instruction to a constant value `1`.

It can handle non-constant operands as well:

[PRE17]

It returns a value of variable `%x`

[PRE18]

In this case, it returns an already existing value.

The implementations for the methods that simplify instructions are located in `lib/Analysis/InstructionSimplify.cpp`.

Some of the important methods of dealing with the simplification of instructions are:

*   **SimplifyBinOp method**: This is used to simplify binary operations such as addition, subtraction, and multiplication, and so on. It has the function signature as follows:

    [PRE19]

Here, by `Opcode`, we mean the operator instruction that we are trying to simplify. LHS and RHS are the operands on either side of the operator. `MaxRecurse` is the recursion level we specify after which the routine must stop trying simplification of the instruction.

In this method, we have a switch case on the `Opcode`:

[PRE20]

Using this `Opcode,` the method decides which function it needs to call for simplification. Some of the methods are as follows:

*   **SimplifyAddInst**: This method tries to fold the result of the `Add` operator when the operands are known. Some of the folding is as follows:

    [PRE21]

The code for the last simplification in the function `static` `Value *SimplifyAddInst(Value *Op0, Value *Op1, bool isNSW, bool isNUW, const Query &Q, unsigned MaxRecurse )` looks something like this:

[PRE22]

Here, the first condition matches the `(Y-X)` value in the expression as `Operand1: m_Value(Y)` denotes value of `Y` and `m_Specific(Op0)` denotes `X`. As soon as it is matched it folds the expression to a constant value `Y` and returns it. The case is similar for the second part of our condition:

*   **SimplifySubInst**: This method tries to fold the result of `subtract` operator when the operators are known. Some examples for the same are as follows:

    [PRE23]

The matching of instructions and folding is done similar to as shown in `SimplifyAddInst`:

*   **SimplifyAndInst**: Similar to the two preceding methods, it tries to fold the result for the logical operator And. Some examples of this are:

    [PRE24]

The code for this, in the method looks like:

[PRE25]

Here, it tries to match `A` and `~A` and returns a `Null` value, 0, when it matches the condition.

So, we have seen a bit of instruction simplification. Now, what do we do if we can replace a set of instructions with a more effective set of instructions?

# Instruction Combining

Instruction combining is a LLVM Pass and compiler technique in which we replace a sequence of instructions with instructions that are more effective and give the same result on execution in a smaller number of machine cycles. Instruction combining does not alter the CFG of the program and is mainly used for algebraic simplification. The major difference between instruction combining and instruction simplification is that in instruction simplification we cannot generate new instructions, which is possible in instruction combining. This pass is run by specifying the `–instcombine` argument to the opt tool and is implemented in the `lib/transforms/instcombine` folder. The `instcombine` Pass combines

[PRE26]

It has removed one redundant `add` instruction and hence combined the two `add` instructions to one.

The LLVM page states that this pass guarantees that the following canonicalizations are performed on the program:

*   Constant operand of a binary operator is moved to RHS.
*   Bitwise operators with constant operands are grouped together with shifts being performed first then 'or' operations, 'and' operations and then 'xor operations'
*   If possible, comparison operators are converted from <,>,<=,>= to == or != .
*   All `cmp` instructions operating on Boolean values are replaced with logical operations.
*   Add X, X is represented by X*2 , that is X<<1
*   Multipliers with a power-of-two constant argument are transformed into shifts.

This pass starts from `bool InstCombiner::runOnFunction(Function &F)` located in the `InstructionCombining.cpp` file. There are different files under the `lib/Transform/InstCombine` folder to combine instructions related to different instructions. The methods, before trying to combine instructions, try to simplify them. Some of these methods for simplification of the `instcombine` module are:

*   **SimplifyAssociativeOrCommutative function**: It performs simplification for operators that are associative or commutative. For commutative operators, it orders the operands from right to left in the order of increasing complexity. For associative operations of the form "`(X op Y) op Z`", it converts it to "`X op (Y op Z)`" if (Y op Z) can be simplified.
*   **tryFactorization function**: This method tries to simplify binary operations by factoring out common terms using commutative and distributive property of the operator. For example, `(A*B)+(A*C)` is simplified to `A*(B+C)`.

Now, let's look at instruction combining. As described earlier, various functionalities are implemented in different files. Let's take an example testcode and see where to add code so that instruction combining happens for our testcode.

Let's write the testcode in `test.ll` for the pattern `(A | (B ^ C)) ^ ((A ^ C) ^ B)`, which can be reduced to `(A & (B ^ C))`:

[PRE27]

The code in LLVM for the handling of operators such as "And", "Or", and "Xor" lies in the `lib/Transforms/InstCombine/InstCombineAndOrXor.cpp` file.

In the `InstCombineAndOrXor.cpp` file, in the `InstCombiner::visitXor(BinaryOperator &I)` function, go to the `if` condition `If` `(Op0I && Op1I)` and add the following snippet of code:

[PRE28]

As it is quite clear, the code added is to match the pattern `(A | (B ^ C)) ^ ((A ^ C) ^ B)` and return `(A & (B ^ C))` when matched.

To test the code, build LLVM and run the `instcombine` Pass with this test code and see the output.

[PRE29]

So the output shows that now only one `xor` `and` one and operation is required instead of four `xor` and one `or` earlier`.`

To understand and add more transformations you can look into the source code in the `InstCombine` folder.

# Summary

So, in this chapter, we looked into how simple transformations can be applied to IR. We looked into the opt tool, LLVM Pass infrastructure, the `Passmanager` and how to use information of one Pass in another Pass. We ended the chapter with examples of instruction simplification and instruction combining. In the next chapter, we will see some more advanced optimizations like Loop Optimization, Scalar Evolution, and others, where we will operate at a block of code rather than individual instructions.