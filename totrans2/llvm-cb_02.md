# Chapter 2. Steps in Writing a Frontend

In this chapter, we will cover the following recipes:

*   Defining a TOY language
*   Implementing a lexer
*   Defining Abstract Syntax Tree
*   Implementing a parser
*   Parsing simple expressions
*   Parsing binary expressions
*   Invoking a driver for parsing
*   Running lexer and parser on our TOY language
*   Defining IR code generation methods for each AST class
*   Generating IR code for expressions
*   Generating IR code for functions
*   Adding IR optimization support

# Introduction

In this chapter, you will get to know about how to write a frontend for a language. By making use of a custom-defined TOY language, you will have recipes on how to write a lexer and a parser, and how to generate IR code from the **Abstract Syntax Tree** (**AST**) generated by the frontend.

# Defining a TOY language

Before implementing a lexer and parser, the syntax and grammar of the language need to be determined first. In this chapter, a TOY language is used to demonstrate how a lexer and a parser can be implemented. The purpose of this recipe is to show how a language is skimmed through. For this purpose, the TOY language to be used is simple but meaningful.

A language typically has some variables, some function calls, some constants, and so on. To keep things simple, our TOY language in consideration has only numeric constants of 32-bit Integer type A, a variable that need not declare its type (like Python, in contrast to C/C++/Java, which require a type declaration) in the TOY language.

## How to do it…

The grammar can be defined as follows (the production rules are defined below, with non-terminals on **Left Hand Side** (**LHS**) and a combination of terminals and non-terminals on **Right Hand Side** (**RHS**); when LHS is encountered, it yields appropriate RHS defined in the production rule):

1.  A numeric expression will give a constant number:

    [PRE0]

2.  A parenthesis expression will have an expression in between an opening and a closing bracket:

    [PRE1]

3.  An identifier expression will either yield an identifier or a function call:

    [PRE2]

4.  If identifier `_expr` is a function call, it will either have no arguments or list of arguments separated by a comma:

    [PRE3]

5.  There will be some primary expression, the starting point of the grammar, which may yield an identifier expression, a numeric expression, or a parenthesis expression:

    [PRE4]

6.  An expression can lead to a binary expression:

    [PRE5]

7.  A binary operation with RHS can yield combinations of binary operators and expressions:

    [PRE6]

8.  A function declaration can have grammar as follows:

    [PRE7]

9.  A function definition is distinguished by a `def` keyword followed by a function declaration and an expression that defines its body:

    [PRE8]

10.  Finally, there will be a top level expression that will yield an expression:

    [PRE9]

An example of the TOY language based on the previously defined grammar can be written as follows:

[PRE10]

Since we have defined the grammar, the next step is to write a lexer and parser for it.

# Implementing a lexer

Lexer is a part of the first phase in compiling a program. Lexer tokenizes a stream of input in a program. Then parser consumes these tokens to construct an AST. The language to tokenize is generally a context-free language. A token is a string of one or more characters that are significant as a group. The process of forming tokens from an input stream of characters is called tokenization. Certain delimiters are used to identify groups of words as tokens. There are lexer tools to automate lexical analysis, such as **LEX**. In the TOY lexer demonstrated in the following procedure is a handwritten lexer using C++.

## Getting ready

We must have a basic understanding of the TOY language defined in the recipe. Create a file named `toy.cpp` as follows:

[PRE11]

All the code that follows will contain all the lexer, parser, and code generation logic.

## How to do it…

While implementing a lexer, types of tokens are defined to categorize streams of input strings (similar to states of an automata). This can be done using the **enumeration** (**enum**) type:

1.  Open the `toy.cpp` file as follows:

    [PRE12]

2.  Write the `enum` in the `toy.cpp` file as follows:

    [PRE13]

    Following is the term list for the preceding example:

    *   `EOF_TOKEN`: It states the end of file
    *   `NUMERIC_TOKEN:` The current token is of numeric type
    *   `IDENTIFIER_TOKEN:` The current token is identifier
    *   `PARAN_TOKEN:` The current token is parenthesis
    *   `DEF_TOKEN`: The current token `def` states that whatever follows is a function definition

3.  To hold numeric values, a static variable is defined in the `toy.cpp` file as follows:

    [PRE14]

4.  To hold the `Identifier` string name, a static variable can be defined in the `toy.cpp` file as follows:

    [PRE15]

5.  Now the lexer function can be defined by using library functions such as `isspace()`, `isalpha()`, and `fgetc()` in the `toy.cpp` file, as shown in the following:

    [PRE16]

## How it works…

The example TOY language defined earlier was as follows:

[PRE17]

The lexer will get the preceding program as input. It will come across the `def` keyword and determine that whatever follows is a definition token, and hence returns the enum value `DEF_TOKEN`. After this, it will come across the function definition and its arguments. Then, there is an expression that involves two binary operators, two variables, and a numeric constant. How these are stored in data structures is demonstrated in the following recipes.

## See also

*   See more sophisticated and detailed handwritten lexer for the C++ language is written in Clang, at [http://clang.llvm.org/doxygen/Lexer_8cpp_source.html](http://clang.llvm.org/doxygen/Lexer_8cpp_source.html)

# Defining Abstract Syntax Tree

AST is a tree representation of the abstract syntactic structure of the source code of a programming language. The ASTs of programming constructs, such as expressions, flow control statements, and so on, are grouped into operators and operands. ASTs represent relationships between programming constructs, and not the ways they are generated by grammar. ASTs ignore unimportant programming elements such as punctuations and delimiters. ASTs generally contain additional properties of every element in it, which are useful in further compilation phases. Location of source code is one such property, which can be used to throw an error line number if an error is encountered in determining the correctness of the source code in accordance with the grammar (location, line number, column number, and so on, and other related properties are stored in an object of the `SourceManager` class in Clang frontend for C++).

The AST is used intensively during semantic analysis, where the compiler checks for correct usage of the elements of the program and the language. The compiler also generates symbol tables based on the AST during semantic analysis. A complete traversal of the tree allows verification of the correctness of the program. After verifying correctness, the AST serves as the base for code generation.

## Getting ready

We must have run the lexer by now to obtain the tokens that will be used in generating the AST. The languages we intend to parse consist of expressions, function definitions, and function declarations. Again we have various types of expressions—variables, binary operators, numeric expressions, and so on.

## How to do it…

To define AST structure, proceed with the following steps:

1.  Open the `toy.cpp` file as follows:

    [PRE18]

    Below the lexer code, define ASTs.

2.  A `base` class is defined for parsing an expression as follows:

    [PRE19]

    Then, several derived classes are defined for every type of expression to be parsed.

3.  An AST class for variable expressions is defined as follows:

    [PRE20]

4.  The language has some numeric expressions. The `AST` class for such numeric expressions can be defined as follows:

    [PRE21]

5.  For expressions involving binary operation, the `AST` class can be defined as follows:

    [PRE22]

6.  The `AST` class for function declaration can be defined as follows:

    [PRE23]

7.  The `AST` class for function definition can be defined as follows:

    [PRE24]

8.  The `AST` class for function call can be defined as follows:

    [PRE25]

The basic skeleton of the AST is now ready to use.

## How it works…

The AST acts as a data structure for storing various information about the tokens given by the lexer. This information is generated in the parser logic and ASTs are filled up according to the type of token being parsed.

## See also

*   Having generated the AST, we will implement the parser, and only after that will we see an example where both lexer and parser will be invoked. For a more detailed AST structure of C++ in Clang, refer to: [http://clang.llvm.org/docs/IntroductionToTheClangAST.html](http://clang.llvm.org/docs/IntroductionToTheClangAST.html).

# Implementing a parser

Parser analyzes a code syntactically according to the rules of the language's grammar. The parsing phase determines if the input code can be used to form a string of tokens according to the defined grammar. A parse tree is constructed in this phase. Parser defines functions to organize language into a data structure called AST. The parser defined in this recipe uses a recursive decent parser technique which is a top-down parser, and uses mutually recursive functions to build the AST.

## Getting ready

We must have the custom-defined language, that is the TOY language in this case, and also a stream of tokens generated by the lexer.

## How to do it…

Define some basic value holders in our TOY parser as shown in the following:

1.  Open the `toy.cpp` file as follows:

    [PRE26]

2.  Define a global static variable to hold the current token from the lexer as follows:

    [PRE27]

3.  Define a function to get the next token from the input stream from the lexer as follows:

    [PRE28]

4.  The next step is to define functions for expression parsing by using the AST data structure defined in the previous section.
5.  Define a generic function to call specific parsing functions according to the types of tokens determined by the lexer, as shown in the following:

    [PRE29]

## How it works…

The stream of input is tokenized and fed to the parser. `Current_token` holds the token to be processed. The type of token is known at this stage and the corresponding parser functions are called to initialize ASTs.

## See also

*   In next few recipes, you will learn how to parse different expressions. For more detailed parsing of the C++ language implemented in Clang, refer to it works: [http://clang.llvm.org/doxygen/classclang_1_1Parser.html](http://clang.llvm.org/doxygen/classclang_1_1Parser.html).

# Parsing simple expressions

In this recipe, you will learn how to parse a simple expression. A simple expression may consist of numeric values, identifiers, function calls, a function declaration, and function definitions. For each type of expression, individual parser logic needs to be defined.

## Getting ready

We must have the custom-defined language—that is, the TOY language in this case—and also stream of tokens generated by lexer. We already defined ASTs above. Further, we are going to parse the expression and invoke AST constructors for every type of expression.

## How to do it…

To parse simple expressions, proceed with the following code flow:

1.  Open the `toy.cpp` file as follows:

    [PRE30]

    We already have lexer logic present in the `toy.cpp` file. Whatever code follows needs to be appended after the lexer code in the `toy.cpp` file.

2.  Define the `parser` function for numeric expression as follows:

    [PRE31]

3.  Define the `parser` function for an identifier expression. Note that identifier can be a variable reference or a function call. They are distinguished by checking if the next token is `(`. This is implemented as follows:

    [PRE32]

4.  Define the `parser` function for the function declaration as follows:

    [PRE33]

5.  Define the `parser` function for the function definition as follows:

    [PRE34]

    Note that the function called `expression_parser` used in the preceding code, parses the expression. The function can be defined as follows:

    [PRE35]

## How it works…

If a numeric token is encountered, the constructor for the numeric expression is invoked and the AST object for the numeric value is returned by the parser, filling up the AST for numeric values with the numeric data.

Similarly, for identifier expressions, the parsed data will either be a variable or a function call. For function declaration and definitions, the name of the function and function arguments is parsed and the corresponding AST class constructors are invoked.

# Parsing binary expressions

In this recipe, you will learn how to parse a binary expression.

## Getting ready

We must have the custom-defined language—that is, the toy language in this case—and also stream of tokens generated by lexer. The binary expression parser requires precedence of binary operators for determining LHS and RHS in order. An STL map can be used to define precedence of binary operators.

## How to do it…

To parse a binary expression, proceed with the following code flow:

1.  Open the `toy.cpp` file as follows:

    [PRE36]

2.  Declare a `map` for operator precedence to store the precedence at global scope in the `toy.cpp` file as follows:

    [PRE37]

    The TOY language for demonstration has 4 operators where precedence of operators is defined as `-`< `+` < `/` < `*`.

3.  A function to initialize precedence—that is, to store precedence value in `map`—can be defined in global scope in the `toy.cpp` file as follows:

    [PRE38]

4.  A helper function to return precedence of binary operator can be defined as follows:

    [PRE39]

5.  Now, the `binary` operator parser can be defined as follows:

    [PRE40]

    Here, precedence of current operator is checked with the precedence of old operator, and the outcome is decided according to LHS and RHS of binary operators. Note that the binary operator parser is recursively called since the RHS can be an expression and not just a single identifier.

6.  A `parser` function for parenthesis can be defined as follows:

    [PRE41]

7.  Some top-level functions acting as wrappers around these `parser` functions can be defined as follows:

    [PRE42]

## See also

*   All of the remaining recipes in this chapter pertain to user objects. For detailed parsing of expressions, and for C++ parsing, please refer to: [http://clang.llvm.org/doxygen/classclang_1_1Parser.html](http://clang.llvm.org/doxygen/classclang_1_1Parser.html).

# Invoking a driver for parsing

In this recipe, you will learn how to call the parser function from the main function of our TOY parser.

## How to do it…

To invoke a driver program to start parsing, define the driver function as shown in the following:

1.  Open the `toy.cpp` file:

    [PRE43]

2.  A `Driver` function called from the main function, and a parser can now be defined as follows:

    [PRE44]

3.  The `main()` function to run the whole program can be defined as follows:

    [PRE45]

## How it works…

The main function is responsible for calling the lexer and parser so that both can act over a piece of code that is being input to the compiler frontend. From the main function, driver function is invoked to start the process of parsing.

## See also

*   For details on how the main function and driver function work for c++ parsing in Clang, refer to [http://llvm.org/viewvc/llvm-project/cfe/trunk/tools/driver/cc1_main.cpp](http://llvm.org/viewvc/llvm-project/cfe/trunk/tools/driver/cc1_main.cpp)

# Running lexer and parser on our TOY language

Now that a full-fledged lexer and parser for our TOY language grammar are defined, it's time to run it on example TOY language.

## Getting ready

To do this, you should have understanding of TOY language grammar and all the previous recipes of this chapter.

## How to do it…

Run and test the Lexer and Parser on TOY Language, as shown in the following:

1.  First step is to compile the `toy.cpp` program into an executable:

    [PRE46]

2.  The `toy` executable is our TOY compiler frontend. The `toy` language to be parsed is in a file called `example`:

    [PRE47]

3.  This file is passed as argument to be processed by the `toy` compiler:

    [PRE48]

## How it works…

The TOY compiler will open the `example` file in read mode. Then, it will tokenize the stream of words. It will come across the def keyword and return `DEF_TOKEN`. Then, the `HandleDefn()` function will be called, which will store the function name and the argument. It will recursively check for the type of token and then call the specific token handler functions to store them into respective ASTs.

## See also

*   The aforementioned lexer and parser do not handle errors in syntax except a few trivial ones. To implement Error handling, refer to [http://llvm.org/docs/tutorial/LangImpl2.html#parser-basics](http://llvm.org/docs/tutorial/LangImpl2.html#parser-basics).

# Defining IR code generation methods for each AST class

Now, since the AST is ready with all the necessary information in its data structure, the next phase is to generate LLVM IR. LLVM APIs are used in this code generation. LLVM IR has a predefined format that is generated by the inbuilt APIs of LLVM.

## Getting ready

You must have created the AST from any input code of the TOY language.

## How to do it…

In order to generate LLVM IR, a virtual `CodeGen` function is defined in each AST class (the AST classes were defined earlier in the AST section; these functions are additional to those classes) as follows:

1.  Open the `toy.cpp` file as follows:

    [PRE49]

2.  In the `BaseAST` class defined earlier, append the `Codegen()` functions as follows:

    [PRE50]

    This virtual `Codegen()` function is included in every AST class we defined.

    This function returns an LLVM Value object, which represents **Static Single Assignment** (**SSA**) value in LLVM. A few more static variables are defined that will be used during Codegen.

3.  Declare the following static variables in global scope as follows:

    [PRE51]

## How it works…

The `Module_Ob` module contains all the functions and variables in the code.

The `Builder` object helps to generate LLVM IR and keeps track of the current point in the program to insert LLVM instructions. The `Builder` object has functions to create new instructions.

The `Named_Values` map keeps track of all the values defined in the current scope like a symbol table. For our language, this map will contain function parameters.

# Generating IR code for expressions

In this recipe, you will see how IR code gets generated for an expression using the compiler frontend.

## How to do it…

To implement LLVM IR code generation for our TOY language, proceed with the following code flow:

1.  Open the `toy.cpp` file as follows:

    [PRE52]

2.  The function to generate code for numeric values can be defined as follows:

    [PRE53]

    In LLVM IR, integer constants are represented by the `ConstantInt` class whose numeric value is held by the `APInt` class.

3.  The function for generating code for variable expressions can be defined as follows:

    [PRE54]

4.  The `Codegen()` function for binary expression can be defined as follows:

    [PRE55]

    If the code above emits multiple `addtmp` variables, LLVM will automatically provide each one with an increasing, unique numeric suffix.

## See also

*   The next recipe shows how to generate IR code for function; we will learn how the code generation actually works.

# Generating IR code for functions

In this recipe you, will learn how to generate IR code for a function.

## How to do it…

Do the following steps:

1.  The `Codegen()` function for the function call can be defined as follows:

    [PRE56]

    Once we have the function to call, we recursively call the `Codegen()` function for each argument that is to be passed in and create an LLVM call instruction.

2.  Now that the `Codegen()` function for a function call has been defined, it's time to define the `Codegen()` functions for declarations and function definitions.

    The `Codegen()` function for function declarations can be defined as follows:

    [PRE57]

    The `Codegen()` function for function definition can be defined as follows:

    [PRE58]

3.  That's it! LLVMIR is now ready. These `Codegen()` functions can be called in the wrappers written to parse top-level expressions as follows:

    [PRE59]

    So, after parsing successfully, the respective `Codegen()` functions are called to generate the LLVM IR. The `dump()` function is called to print the generated IR.

## How it works…

The `Codegen()` functions use LLVM inbuilt function calls to generate IR. The header files to include for this purpose are `llvm/IR/Verifier.h`, `llvm/IR/DerivedTypes.h`, `llvm/IR/IRBuilder.h`, and `llvm/IR/LLVMContext.h`, `llvm/IR/Module.h`.

1.  While compiling, this code needs to be linked with LLVM libraries. For this purpose, the `llvm-config` tool can be used as follows:

    [PRE60]

2.  For this purpose, the `toy` program is recompiled with additional flags as follows:

    [PRE61]

3.  When the `toy` compiler is now run on `example` code, it will generate LLVM IR as follows:

    [PRE62]

    Another `example2` file has a function `call.$ cat example2`:

    [PRE63]

    Its LLVM IR will be dumped as follows:

    [PRE64]

## See also

*   For details on how `Codegen()` functions for C++ in Clang, refer to [http://llvm.org/viewvc/llvm-project/cfe/trunk/lib/CodeGen/](http://llvm.org/viewvc/llvm-project/cfe/trunk/lib/CodeGen/)

# Adding IR optimization support

LLVM provides a wide variety of optimization passes. LLVM allows a compiler implementation to decide which optimizations to use, their order, and so on. In this recipe, you will learn how to add IR optimization support.

## How to do it…

Do the following steps:

1.  To start with the addition of IR optimization support, first of all a static variable for function manager has to be defined as follows:

    [PRE65]

2.  Then, a function pass manager needs to be defined for the `Module` object used previously. This can be done in the `main()` function as follows:

    [PRE66]

3.  Now a pipeline of various optimizer passes can be added in the `main()` function as follows:

    [PRE67]

4.  Now the static global function Pass Manager is assigned to this pipeline as follows:

    [PRE68]

    This PassManager has a run method, which we can run on the function IR generated before returning from `Codegen()` of the function definition. This is demonstrated as follows:

    [PRE69]

This is a lot more beneficial as it optimizes the function in place, improving the code generated for the function body.

## See also

*   How to add our own optimization pass and its run method will be demonstrated in the later chapters