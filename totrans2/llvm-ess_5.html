<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05" class="calibre1"/>Chapter 5. Advanced IR Block Transformations</h1></div></div></div><p class="calibre7">In the previous chapter, we have gone through some of the optimizations, which were mainly at instruction level. In this chapter, we will look at optimizations on block level where we will be optimizing a block of code to a simpler form, which makes the code more effective. We will start by looking at how loops are represented in LLVM, use the concept of dominance and CFG to optimize loops. We will use <a id="id165" class="calibre1"/>
<strong class="calibre2">Loop Simplification</strong> (<code class="email">LoopSimplify</code>)and <a id="id166" class="calibre1"/>
<strong class="calibre2">Loop Invariant Code Motion</strong> optimizations for loop processing. We will then see how a scalar value changes during program execution and how the result of this <strong class="calibre2">Scalar Evolution Optimization</strong> <a id="id167" class="calibre1"/>can be used in other optimizations. Then we will look into how LLVM represents its in build functions called as <a id="id168" class="calibre1"/>LLVM intrinsics. Finally, we will look into how LLVM deals with concepts of parallelism by understanding its approach towards vectorization.</p><p class="calibre7">In this chapter, we will look into the following topics:</p><div><ul class="itemizedlist"><li class="listitem">Loop processing</li><li class="listitem">Scalar evolution</li><li class="listitem">LLVM intrinsics</li><li class="listitem">Vectorization</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch05lvl1sec35" class="calibre1"/>Loop processing</h1></div></div></div><p class="calibre7">Before getting started with loop processing<a id="id169" class="calibre1"/> and optimization, we must have a little heads up about the concepts of CFG and dominance information. A CFG is the control flow graph of the program that gives a look into how the program may be executed through the various basic blocks. By dominance information, we get to know about the relation between the various basic blocks in the CFG.</p><p class="calibre7">In a CFG, we say a node <code class="email">d</code> dominates a node <code class="email">n</code> if every path (from the input towards output) that passes through <code class="email">n</code> must also pass through <code class="email">d</code>. This is denoted by <code class="email">d -&gt; n</code>. The graph <code class="email">G = (V, E)</code>, where <code class="email">V</code> is the set of basic blocks and <code class="email">E</code> is the dominance relation defined on <code class="email">V</code>, is called dominator tree.</p><p class="calibre7">Let's take an example to show the <a id="id170" class="calibre1"/>CFG of a program and the corresponding dominator tree.</p><p class="calibre7">Put example code here:</p><div><pre class="programlisting">void fun() {
  int iter, a, b;

  for (iter = 0; iter &lt; 10; iter++) {
    a = 5;
    if (iter == a)
      b = 2;
    else
      b = 5;
  }
}</pre></div><p class="calibre7">The CFG for the preceding code looks like the following:</p><div><img src="img/00004.jpeg" alt="Loop processing" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">From what you have <a id="id171" class="calibre1"/>learned about dominance and dominator trees, the dominator tree for the preceding CFG looks something like the following:</p><div><img src="img/00005.jpeg" alt="Loop processing" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">The first figure shows <a id="id172" class="calibre1"/>the CFG of the preceding code and the next figure shows the dominator tree for the same CFG. We have numbered each of the CFG components and we can see that 2 dominates 3 in the CFG, and 2 also dominates 4, 5, and 6. 3 dominates 4, 5, and 6 and is the immediate dominator of these. There is no dominance relation between 4 and 5. 6 is not dominated by 5 because there is another path available through 4 and for the same reasons, 4 does not dominate 6.</p><p class="calibre7">All the loop optimizations and transformation in LLVM are derived from the <code class="email">LoopPass</code> class implemented in the <code class="email">LoopPass.cpp</code> file located in <code class="email">lib/Analysis</code>. The <code class="email">LPPassManager</code> class is responsible for the handling of all <code class="email">LoopPasses</code>.</p><p class="calibre7">The most important class to get started with loop processing is the <code class="email">LoopInfo</code> Class, which is used to identify the natural loops<a id="id173" class="calibre1"/> in the code and to know the depth of various nodes in the CFG. Natural loops are the cyclic structures in a CFG. To define a natural loop in a CFG, we must know what a backedge is: it is an edge in the CFG where the source dominates the target. A natural loop can be defined by a backedge <code class="email">a-&gt;d</code> that defines a subgraph of the CFG, where <code class="email">d</code> is the header node and it contains all other basic blocks that can reach a without having to reach <code class="email">d</code>.</p><p class="calibre7">We can see in the preceding diagram that the backedge <code class="email">6-&gt;2</code> forms a natural loop consisting of the nodes <code class="email">2</code>, <code class="email">3</code>, <code class="email">4</code>, <code class="email">5</code>, and <code class="email">6</code>.</p><p class="calibre7">The next important <a id="id174" class="calibre1"/>step is loop simplification that transforms the loop into a canonical form, which includes the insertion of a preheader to the loop, which in turn ensures that there is a single entry edge to the loop header from outside the loop. It also inserts loop exit blocks, which ensure that all exit blocks from the loop have predecessors only from within the loop. These insertion of pre-header and exit blocks help in later loop optimizations, such as Loop Independent Code Motion. </p><p class="calibre7">Loop Simplification also ensures that the loop will have only one backedge, that is if the loop header is having more than two predecessors, (from the pre header block and multiple latches to the loop) we adjust only this loop latch. One way of doing this is by inserting a new block which is the target of all the backedges and make this new block jump to loop header. Let's take a look at how a loop looks after<a id="id175" class="calibre1"/> <strong class="calibre2">Loop Simplify Pass</strong>. We will be able to see that a preheader node is inserted, new exit blocks are created, and there is only one backedge.</p><div><img src="img/00006.jpeg" alt="Loop processing" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">Now, after getting the required <a id="id176" class="calibre1"/>information from <code class="email">LoopInfo</code> and simplifying the loop to a canonical form, we will look into some of the loop optimizations.</p><p class="calibre7">One of the main loop optimizations is<a id="id177" class="calibre1"/> <strong class="calibre2">Loop Invariant Code Motion</strong> (<strong class="calibre2">LICM</strong>) optimization. This pass tries to remove as much code from the body of the loop as possible. The condition for removal of the code is that this piece of code is invariant inside the loop, that is the output of this part of code not dependent on loop execution and it will remain same in every iteration of the loop. This is done by moving this piece of code either in the preheader block or moving the code to exit blocks. This pass is implemented in the <code class="email">lib/TransformsScalar/LICM.cpp</code> file. If we look into the code of the loop, we see it requires <code class="email">LoopInfo</code> and <code class="email">LoopSimplify</code> passes to be run before it. Also, it needs the <code class="email">AliasAnalysis</code> information. Alias analysis is needed to move loop invariant loads and calls out of the loop. If there is no load and call inside the loop that aliases anything stored, we can move these out of the loop. This also helps in scalar promotion of memory.</p><p class="calibre7">Let's look at an example to see how LICM is getting done.</p><p class="calibre7">Let's write the testcase in a file <code class="email">licm.ll</code>:</p><div><pre class="programlisting">
<strong class="calibre2">$ cat licm.ll</strong>
<strong class="calibre2">define void @func(i32 %i) {</strong>
<strong class="calibre2">Entry:</strong>
<strong class="calibre2">        br label %Loop</strong>
<strong class="calibre2">Loop:</strong>
<strong class="calibre2">        %j = phi i32 [ 0, %Entry ], [ %Val, %Loop ]</strong>
<strong class="calibre2">        %loopinvar = mul i32 %i, 17</strong>
<strong class="calibre2">        %Val = add i32 %j, %loopinvar</strong>
<strong class="calibre2">        %cond = icmp eq i32 %Val, 0</strong>
<strong class="calibre2">        br i1 %cond, label %Exit, label %Loop</strong>
<strong class="calibre2">Exit:            </strong>
<strong class="calibre2">        ret void</strong>
<strong class="calibre2">}</strong>
</pre></div><p class="calibre7">This <code class="email">testcase</code> has a loop <a id="id178" class="calibre1"/>denoted by Loop block in the test code with the loop condition being <code class="email">br i1 %cond</code>, <code class="email">label %Exit</code>, <code class="email">label %Loop</code> (Latch part of the loop). We can see the <code class="email">%j</code> value, which is being used as the induction variable is derived after using the phi instruction. Basically, it tells to choose the value <code class="email">0</code> if the control is coming from the Entry block and <code class="email">%Val</code> if the control is coming from Loop block. In this, the invariant code can be seen as <code class="email">%loopinvar = mul i32 %i, 17</code>, as <code class="email">%loopinvar</code> value is independent of the iteration of loop and depends on the function argument only. So when we run the LICM pass, we expect this value to be hoisted out of the loop, thus preventing its computation in every iteration of the loop.</p><p class="calibre7">Let's run the <code class="email">licm</code> pass and see the output:</p><div><pre class="programlisting">
<strong class="calibre2">$ opt -licm licm.ll -o licm.bc</strong>
<strong class="calibre2">$ llvm-dis licm.bc -o licm_opt.ll</strong>
<strong class="calibre2">$ cat licm_opt.ll</strong>
<strong class="calibre2">; ModuleID = 'licm.bc'</strong>

<strong class="calibre2">define void @func(i32 %i) {</strong>
<strong class="calibre2">Entry:</strong>
<strong class="calibre2">  %loopinvar = mul i32 %i, 17</strong>
<strong class="calibre2">  br label %Loop</strong>

<strong class="calibre2">Loop:                                             </strong>
<strong class="calibre2">; preds = %Loop, %Entry</strong>
<strong class="calibre2">  %j = phi i32 [ 0, %Entry ], [ %Val, %Loop ]</strong>
<strong class="calibre2">  %Val = add i32 %j, %loopinvar</strong>
<strong class="calibre2">  %cond = icmp eq i32 %Val, 0</strong>
<strong class="calibre2">  br i1 %cond, label %Exit, label %Loop</strong>

<strong class="calibre2">Exit:                                             </strong>
<strong class="calibre2">; preds = %Loop</strong>
<strong class="calibre2">  ret void</strong>
<strong class="calibre2">}</strong>
</pre></div><p class="calibre7">As we can see in the<a id="id179" class="calibre1"/> output, the calculation <code class="email">%loopinvar = mul i32 %i, 17</code> is hoisted out of the loop, which is the expected output.</p><p class="calibre7">We have many other loop optimizations such as <a id="id180" class="calibre1"/>
<strong class="calibre2">Loop Rotation</strong>, <a id="id181" class="calibre1"/>
<strong class="calibre2">Loop Interchange</strong>, <a id="id182" class="calibre1"/>
<strong class="calibre2">Loop Unswitch</strong>, and so on. The source codes for these can be looked under the LLVM folder <code class="email">lib/Transforms/Scalar</code> to get more understanding about these optimizations. In the next section, we will see the concept of scalar evolution.</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec36" class="calibre1"/>Scalar evolution</h1></div></div></div><p class="calibre7">By scalar evolution, we <a id="id183" class="calibre1"/>mean how the value of a scalar changes in a program with the execution of code. We look at a particular scalar value and see how it is getting derived, what all other elements it is dependent on, whether this is known at compile time or not, and what all operations are being performed. We need to look into a block of code rather than looking into individual instructions. A scalar value is build up from two elements, a variable and an operation of constant step. The variable element that builds up this scalar value is unknown at compile time and its value can be known at run time only. The other element is the constant part. These elements themselves may be recursively broken into other elements such as a constant, an unknown value or an arithmetic operation. </p><p class="calibre7">The main idea here is to look at complete scalar value containing the unknown part at compile time and see how this value will evolve during execution and try to use this for optimization. One example is removing a redundant value for which the scalar evolution is similar to some other value in the same program.</p><p class="calibre7">In LLVM, we can use scalar evolution to analyze code that contains common integer arithmetic operations.</p><p class="calibre7">In LLVM <code class="email">ScalarEvolution</code> class is implemented in <code class="email">include/llvm/Analysis</code>, which is a LLVM pass and can be used analyze scalar expressions in a loop. It is able to recognize general induction variables (a variable in loop whose value is a function of loop iteration number) and represent them using object of SCEV class, which is used to represent analyzed expression in a program. Using this analysis trip count and other important analysis can be obtained. This scalar evolution analysis is mainly used in induction variable substitution and strength reduction of loops.</p><p class="calibre7">Let's take an example now and<a id="id184" class="calibre1"/> run the scalar evolution pass on it and see what output it generates.</p><p class="calibre7">Write a testcase <code class="email">scalevl.ll</code> with a loop and some scalar values within the loop.</p><div><pre class="programlisting">
<strong class="calibre2">$ cat scalevl.ll</strong>
<strong class="calibre2">define void @fun() {</strong>
<strong class="calibre2">entry:</strong>
<strong class="calibre2">        br label %header</strong>
<strong class="calibre2">header:</strong>
<strong class="calibre2">        %i = phi i32 [ 1, %entry ], [ %i.next, %body ]</strong>
<strong class="calibre2">        %cond = icmp eq i32 %i, 10</strong>
<strong class="calibre2">        br i1 %cond, label %exit, label %body</strong>
<strong class="calibre2">body:</strong>
<strong class="calibre2">        %a = mul i32 %i, 5</strong>
<strong class="calibre2">        %b = or i32 %a, 1</strong>
<strong class="calibre2">        %i.next = add i32 %i, 1</strong>
<strong class="calibre2">        br label %header</strong>
<strong class="calibre2">exit:        </strong>
<strong class="calibre2">        ret void</strong>
<strong class="calibre2">}</strong>
</pre></div><p class="calibre7">In this test case, we have a loop consisting of header and body blocks with <code class="email">%a</code> and <code class="email">%b</code> being the scalars in loop body of interest. Let's run the scalar evolution pass on this and see the output:</p><div><pre class="programlisting">
<strong class="calibre2">$ opt -analyze -scalar-evolution scalevl.ll</strong>
<strong class="calibre2">Printing analysis 'Scalar Evolution Analysis' for function 'fun':</strong>
<strong class="calibre2">Classifying expressions for: @fun</strong>
<strong class="calibre2">  %i = phi i32 [ 1, %entry ], [ %i.next, %body ]</strong>
<strong class="calibre2">  --&gt;  {1,+,1}&lt;%header&gt; U: [1,11) S: [1,11)    Exits: 10</strong>
<strong class="calibre2">  %a = mul i32 %i, 5</strong>
<strong class="calibre2">  --&gt;  {5,+,5}&lt;%header&gt; U: [5,51) S: [5,51)    Exits: 50</strong>
<strong class="calibre2">  %b = or i32 %a, 1</strong>
<strong class="calibre2">  --&gt;  %b U: [1,0) S: full-set                 Exits: 51</strong>
<strong class="calibre2">  %i.next = add i32 %i, 1</strong>
<strong class="calibre2">  --&gt;  {2,+,1}&lt;%header&gt; U: [2,12) S: [2,12)    Exits: 11</strong>
<strong class="calibre2">Determining loop execution counts for: @fun</strong>
<strong class="calibre2">Loop %header: backedge-taken count is 9</strong>
<strong class="calibre2">Loop %header: max backedge-taken count is 9</strong>
</pre></div><p class="calibre7">As we can see, the<a id="id185" class="calibre1"/> output of scalar evolution pass shows the range of values for a particular variable (<code class="email">U</code> stands for unsigned range and <code class="email">S</code> for signed range, here both are same) and the exit value, the value in that variable when the loop runs its last iteration. For example, the value <code class="email">%i</code> has the range as <code class="email">[1,11)</code>, that is the starting iteration value is <code class="email">1</code> and when the value of <code class="email">%i</code> becomes <code class="email">11</code> the condition <code class="email">%cond = icmp eq i32 %i, 10</code> becomes false and the loop breaks. So, the the value of <code class="email">%i</code> when it exited the loop was <code class="email">10</code>, which is denoted by <code class="email">Exits: 10</code> in the output.</p><p class="calibre7">The value in the form of <code class="email">{x,+,y}</code> representation, such as <code class="email">{2,+,1}</code>, represents add recurrence, that is the expressions changing value during loop execution where x represents the base value at 0th iteration and y represents the value added to it on each subsequent iteration.</p><p class="calibre7">The output also shows the number of times the loop has iterated after the first run. Here, it shows the value <code class="email">9</code> for backedge-taken, that is the loop has run <code class="email">10</code> times in total. The max backedge-taken value is the least value which can never be less than the backedge-taken value, which here is <code class="email">9</code>.</p><p class="calibre7">This is the output for this example, you can try some other test cases and see what this pass outputs.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec37" class="calibre1"/>LLVM intrinsics</h1></div></div></div><p class="calibre7">An intrinsic function is a <a id="id186" class="calibre1"/>function built in to the compiler. The compiler knows how to best implement the functionality in the most optimized way for these functions and replaces with a set of machine instruction for a particular backend. Often, the code for the function is inserted inline thus avoiding the overhead of function call (In many cases, we do call the library function. For example, for the functions<a id="id187" class="calibre1"/> listed in <a class="calibre1" href="http://llvm.org/docs/LangRef.html#standard-c-library-intrinsics">http://llvm.org/docs/LangRef.html#standard-c-library-intrinsics</a> we make a call to <code class="email">libc</code>). These are also called built-in functions for other compilers.</p><p class="calibre7">In LLVM these intrinsics are introduced during code optimization at IR level (Intrinsics written in program code can be emitted through frontend directly). These function names will start with a prefix "<code class="email">llvm.</code>", which is a reserved word in LLVM. These functions are always external and a user cannot specify the body for these functions in his/her code. In our code, we can only call these intrinsic functions.</p><p class="calibre7">In this section, we will not go much deep into details. We will take an example and see how LLVM optimizes certain part of code with its own intrinsic functions.</p><p class="calibre7">Let's write a simple code:</p><div><pre class="programlisting">
<strong class="calibre2">$ cat intrinsic.cpp</strong>
<strong class="calibre2">int func()</strong>
<strong class="calibre2">{</strong>
<strong class="calibre2">        int a[5];</strong>

<strong class="calibre2">        for (int i = 0; i != 5; ++i)</strong>
<strong class="calibre2">                a[i] = 0;</strong>

<strong class="calibre2">        return a[0];</strong>
<strong class="calibre2">}</strong>
</pre></div><p class="calibre7">Now use Clang to generate the IR file. Using the command given below, we will get the <code class="email">intrinsic.ll</code> file that contains the unoptimized IR without any intrinsic function.</p><div><pre class="programlisting">
<strong class="calibre2">$ clang -emit-llvm -S intrinsic.cpp</strong>
</pre></div><p class="calibre7">Now, use the opt tool to optimize the IR with O1 level of optimization.</p><div><pre class="programlisting">
<strong class="calibre2">$ opt -O1 intrinsic.ll -S -o -</strong>
<strong class="calibre2">; ModuleID = 'intrinsic.ll'</strong>
<strong class="calibre2">target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"</strong>
<strong class="calibre2">target triple = "x86_64-unknown-linux-gnu"</strong>

<strong class="calibre2">; Function Attrs: nounwind readnone uwtable</strong>
<strong class="calibre2">define i32 @_Z4funcv() #0 {</strong>
<strong class="calibre2">  %a = alloca [5 x i32], align 16</strong>
<strong class="calibre2">  %a2 = bitcast [5 x i32]* %a to i8*</strong>
<strong class="calibre2">  call void @llvm.memset.p0i8.i64(i8* %a2, i8 0, i64 20, i32 16, i1 false)</strong>
<strong class="calibre2">  %1 = getelementptr inbounds [5 x i32], [5 x i32]* %a, i64 0, i64 0</strong>
<strong class="calibre2">  %2 = load i32, i32* %1, align 16</strong>
<strong class="calibre2">  ret i32 %2</strong>
<strong class="calibre2">}</strong>

<strong class="calibre2">; Function Attrs: nounwind argmemonly</strong>
<strong class="calibre2">declare void @llvm.memset.p0i8.i64(i8* nocapture, i8, i64, i32, i1) #1</strong>
</pre></div><p class="calibre7">The important <a id="id188" class="calibre1"/>optimization to be noted here is the call to LLVM intrinsic function <code class="email">llvm.memset.p0i8.i64</code> to fill the array with value <code class="email">0</code>. The intrinsic functions may be used to implement vectorization and parallelization in the code, leading to better code generation. It might call the most optimized version of the <code class="email">memset</code> call from the <code class="email">libc</code> library and may choose to completely omit this function if there is no usage of this.</p><p class="calibre7">The first argument in the call specifies the array "<code class="email">a</code>", that is the destination array where the value needs to be filled. The second argument specifies the value to be filled. The third argument to the call is specification about number of bytes to be filled. The fourth argument specifies the alignment of destination value. The last argument is to determine whether this is a <a id="id189" class="calibre1"/>volatile operation or not.</p><p class="calibre7">There is a list of such <a id="id190" class="calibre1"/>intrinsic functions in LLVM, a list of which can be found at <a class="calibre1" href="http://llvm.org/docs/LangRef.html#intrinsic-functions">http://llvm.org/docs/LangRef.html#intrinsic-functions</a>.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec38" class="calibre1"/>Vectorization</h1></div></div></div><p class="calibre7">
<strong class="calibre2">Vectorization</strong>
<a id="id191" class="calibre1"/> is an important optimization for compilers where we can vectorize code to execute an instruction on multiple datasets in one go. Advance target architecture typically have vector registers set and vector instructions—where broad range of data type (typically 128/246 bit) can be loaded into the vector registers and operations can be performed on those register set, performing two, four, and sometimes eight operations at the same time, with the cost of one scalar operation.</p><p class="calibre7">There are two types of vectorization in LLVM—<strong class="calibre2">Superword-Level Parallelism</strong> (<strong class="calibre2">SLP</strong>) <a id="id192" class="calibre1"/>and <a id="id193" class="calibre1"/>loop vectorization. Loop vectorization deals with vectorization opportunities in a loop, while SLP vectorization deals with vectorizing straight-line code in a basic block.</p><p class="calibre7">A vector instruction performs <a id="id194" class="calibre1"/>
<strong class="calibre2">Single-instruction multiple-data</strong> (<strong class="calibre2">SIMD</strong>) operations; the same operation on multiple data lanes (in parallel).</p><div><img src="img/00007.jpeg" alt="Vectorization" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">Let's look at how SLP Vectorization is implemented in LLVM infrastructure.</p><p class="calibre7">As the code <a id="id195" class="calibre1"/>itself attributes, the implementation of SLP Vectorization in LLVM is inspired by the work described in the paper <em class="calibre8">Loop-Aware SLP in GCC</em> by Ira Rosen, Dorit Nuzman, and Ayal Zaks. LLVM SLP Vectorization Pass implements the Bottom Up SLP vectorizer. It detects consecutive stores that can be put together into vector-stores. Next, it attempts to stores that can be put together into vector-stores. Next, it attempts to construct vectorizable tree using the <code class="email">use-def</code> chains. If a profitable tree was found, the SLP vectorizer performs vectorization on the tree.</p><p class="calibre7">There are three stages to<a id="id196" class="calibre1"/> SLP Vectorization:</p><div><ul class="itemizedlist"><li class="listitem">Identify the pattern  and determine if it is a valid Vectorization pattern</li><li class="listitem">Determine if it is profitable to vectorize the code</li><li class="listitem">If step 1 and 2 are true, then vectorize the code</li></ul></div><p class="calibre7">Let's look at an example:</p><p class="calibre7">Consider addition of <code class="email">4</code> consecutive elements of two arrays into third array.</p><div><pre class="programlisting">int a[4], b[4], c[4];

void addsub() {
a[0] = b[0] + c[0];
a[1] = b[1] + c[1];
a[2] = b[2] + c[2];
a[3] = b[3] + c[3];
}</pre></div><p class="calibre7">The IR for the <a id="id197" class="calibre1"/>preceding kind of expression will look like this:</p><div><pre class="programlisting">; ModuleID = 'addsub.c'

@a = global [4 x i32] zeroinitializer, align 4
@b = global [4 x i32] zeroinitializer, align 4
@c = global [4 x i32] zeroinitializer, align 4

; Function Attrs: nounwind
define void @addsub() {
entry:
  %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0)
  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0)
  %add = add nsw i32 %1, %0
  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0)
  %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1)
  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1)
  %add1 = add nsw i32 %3, %2
  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1)
  %4 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2)
  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2)
  %add2 = add nsw i32 %5, %4
  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2)
  %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3)
  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)
  %add3 = add nsw i32 %7, %6
  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)
  ret void
}</pre></div><p class="calibre7">The expression tree for the preceding pattern can be visualized as a chain of stores and loads:</p><div><img src="img/00008.jpeg" alt="Vectorization" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">For the <a id="id198" class="calibre1"/>preceding expression tree, the bottom-up SLP Vectorization first constructs a chain that starts with a store instruction:</p><div><pre class="programlisting">// Use the bottom up slp vectorizer to construct chains that start
// with store instructions.
 BoUpSLP R(&amp;F, SE, TTI, TLI, AA, LI, DT, AC);</pre></div><p class="calibre7">It then scans the tree already built in the preceding code for all the stores in the given basic block:</p><div><pre class="programlisting">    // Scan the blocks in the function in post order.
    for (auto BB : post_order(&amp;F.getEntryBlock())) {
      // Vectorize trees that end at stores.
      if (unsigned count = collectStores(BB, R)) {
        (void)count;
        DEBUG(dbgs() &lt;&lt; "SLP: Found " &lt;&lt; count &lt;&lt; " stores to vectorize.\n");
        Changed |= vectorizeStoreChains(R);
      }
      // Vectorize trees that end at reductions.
      Changed |= vectorizeChainsInBlock(BB, R);
    }</pre></div><p class="calibre7">The <code class="email">collectStores()</code> function collects all the store references.</p><div><pre class="programlisting">unsigned SLPVectorizer::collectStores(BasicBlock *BB, BoUpSLP &amp;R) {
  unsigned count = 0;
  StoreRefs.clear();
  const DataLayout &amp;DL = BB-&gt;getModule()-&gt;getDataLayout();
  for (Instruction &amp;I : *BB) {
    StoreInst *SI = dyn_cast&lt;StoreInst&gt;(&amp;I);
    if (!SI)
      continue;

    // Don't touch volatile stores.
    if (!SI-&gt;isSimple())
      continue;

    // Check that the pointer points to scalars.
    Type *Ty = SI-&gt;getValueOperand()-&gt;getType();
    if (!isValidElementType(Ty))
      continue;

    // Find the base pointer.
    Value *Ptr = GetUnderlyingObject(SI-&gt;getPointerOperand(), DL);

    // Save the store locations.
    StoreRefs[Ptr].push_back(SI);
    count++;
  }
  return count;
}</pre></div><p class="calibre7">The<a id="id199" class="calibre1"/> function <code class="email">SLPVectorizer::vectorizeStoreChains()</code> has three steps and function calls to each three steps:</p><div><pre class="programlisting">bool SLPVectorizer::vectorizeStoreChain(ArrayRef&lt;Value *&gt; Chain,
                                        int CostThreshold, BoUpSLP &amp;R,
                                        unsigned VecRegSize) {
   … 
   …
    <strong class="calibre2">R.buildTree(Operands);</strong>

    <strong class="calibre2">int Cost = R.getTreeCost();</strong>

    DEBUG(dbgs() &lt;&lt; "SLP: Found cost=" &lt;&lt; Cost &lt;&lt; " for VF=" &lt;&lt; VF &lt;&lt; "\n");
    <strong class="calibre2">if (Cost &lt; CostThreshold)</strong> {
      DEBUG(dbgs() &lt;&lt; "SLP: Decided to vectorize cost=" &lt;&lt; Cost &lt;&lt; "\n");
      <strong class="calibre2">R.vectorizeTree();</strong>
… 
…
}</pre></div><p class="calibre7">The first step is to identify pattern. The function <code class="email">buildTree()</code> subsequently builds up the tree recursively as the preceding visualization.</p><div><pre class="programlisting">void BoUpSLP::buildTree(ArrayRef&lt;Value *&gt; Roots,
                        ArrayRef&lt;Value *&gt; UserIgnoreLst) {
    … 
    …
    <strong class="calibre2">buildTree_rec(Roots, 0);</strong>
    …
    … 
}</pre></div><p class="calibre7">For our given<a id="id200" class="calibre1"/> example, it will identify that all the store operations have binary addition operations as their operands:</p><div><pre class="programlisting">void BoUpSLP::buildTree_rec(ArrayRef&lt;Value *&gt; VL, unsigned Depth) {
…
…
case Instruction::Add:
newTreeEntry(VL, true);
    DEBUG(dbgs() &lt;&lt; "SLP: added a vector of bin op.\n");

    // Sort operands of the instructions so that each side is more 
    // likely to have the sam opcode 
    if (isa&lt;BinaryOperator&gt;(VL0) &amp;&amp; VL0-&gt;isCommutative()) {
      ValueList Left, Right;
      reorderInputsAccordingToOpcode(VL, Left, Right);
      buildTree_rec(Left, Depth + 1);
      buildTree_rec(Right, Depth + 1);
      return;
    }
…
…
}</pre></div><p class="calibre7">When the binary operation <code class="email">ADD</code> is encountered, it again recursively builds tree (calling the same function) on LHS and RHS operands of the ADD operation, which in our case are both <code class="email">Load</code>:</p><div><pre class="programlisting">case Instruction::Load: {
    // Check that a vectorized load would load the same memory as a // scalar load.
    // For example we don't want vectorize loads that are smaller than 8 bit.
    // Even though we have a packed struct {&lt;i2, i2, i2, i2&gt;} LLVM treats
    // loading/storing it as an i8 struct. If we vectorize loads/stores from
    // such a struct we read/write packed bits disagreeing with the
    // unvectorized version.
    const DataLayout &amp;DL = F-&gt;getParent()-&gt;getDataLayout();
    Type *ScalarTy = VL[0]-&gt;getType();

    if (DL.getTypeSizeInBits(ScalarTy) != DL.getTypeAllocSizeInBits(ScalarTy)) {
      BS.cancelScheduling(VL);
      newTreeEntry(VL, false);
      DEBUG(dbgs() &lt;&lt; "SLP: Gathering loads of non-packed type.\n");
      return;
    }
    // Check if the loads are consecutive or of we need to swizzle them.
    for (unsigned i = 0, e = VL.size() - 1; i &lt; e; ++i) {
      LoadInst *L = cast&lt;LoadInst&gt;(VL[i]);
      if (!L-&gt;isSimple()) {
        BS.cancelScheduling(VL);
        newTreeEntry(VL, false);
        DEBUG(dbgs() &lt;&lt; "SLP: Gathering non-simple loads.\n");
        return;
      }

      if (!isConsecutiveAccess(VL[i], VL[i + 1], DL)) {
        if (VL.size() == 2 &amp;&amp; isConsecutiveAccess(VL[1], VL[0], DL)) {
          ++NumLoadsWantToChangeOrder;
        }
        BS.cancelScheduling(VL);
        newTreeEntry(VL, false);
        DEBUG(dbgs() &lt;&lt; "SLP: Gathering non-consecutive loads.\n");
        return;
      }
    }
    ++NumLoadsWantToKeepOrder;
    newTreeEntry(VL, true);
    DEBUG(dbgs() &lt;&lt; "SLP: added a vector of loads.\n");
    return;
  }</pre></div><p class="calibre7">While building the tree, there are several checks that validate if the tree can be vectorized. For example, in the preceding case, when loads are encountered across trees, it is checked whether they are consecutive loads or not. In our expression tree, the loads across trees in LHS—b[0], b[1], b[2], and b[3] are accessing consecutive memory location. Similarly, loads across tress in RHS—c[0], c[1], c[2] and c[3] are accessing consecutive memory location. If any of the checks fail for a given operation, the building of a tree is aborted and code is not vectorized.</p><p class="calibre7">After the <a id="id201" class="calibre1"/>pattern is identified and the vector tree is built, the next step is to get the cost of vectorizing the built tree. This effectively refers to the cost of the tree if it is vectorized compared to the cost of tree in current scalar form. If the vector cost is less than the scalar cost, it is beneficial to vectorize the tree:</p><div><pre class="programlisting">int BoUpSLP::getTreeCost() {
  int Cost = 0;
  DEBUG(dbgs() &lt;&lt; "SLP: Calculating cost for tree of size "
               &lt;&lt; VectorizableTree.size() &lt;&lt; ".\n");

  // We only vectorize tiny trees if it is fully vectorizable.
  if (VectorizableTree.size() &lt; 3 &amp;&amp; !isFullyVectorizableTinyTree()) {
    if (VectorizableTree.empty()) {
      assert(!ExternalUses.size() &amp;&amp; "We should not have any external users");
    }
    return INT_MAX;
  }

  unsigned BundleWidth = VectorizableTree[0].Scalars.size();

  for (unsigned i = 0, e = VectorizableTree.size(); i != e; ++i) {
    int C = getEntryCost(&amp;VectorizableTree[i]);
    DEBUG(dbgs() &lt;&lt; "SLP: Adding cost " &lt;&lt; C &lt;&lt; " for bundle that starts with " &lt;&lt; *VectorizableTree[i].Scalars [0] &lt;&lt; " . \n" );
    Cost += C;
  }

  SmallSet&lt;Value *, 16&gt; ExtractCostCalculated;
  int ExtractCost = 0;
  for (UserList::iterator I = ExternalUses.begin(), E = ExternalUses.end();
       I != E; ++I) {
    // We only add extract cost once for the same scalar.
    if (!ExtractCostCalculated.insert(I-&gt;Scalar).second)
      continue;

    // Uses by ephemeral values are free (because the ephemeral value will be
    // removed prior to code generation, and so the extraction will be
    // removed as well).
    if (EphValues.count(I-&gt;User))
      continue;

    VectorType *VecTy = VectorType::get(I-&gt;Scalar-&gt;getType(), BundleWidth);
    ExtractCost +=
        <strong class="calibre2">TTI-&gt;getVectorInstrCost(Instruction::ExtractElement, VecTy, I-&gt;Lane);</strong>
  }

  Cost += getSpillCost();

  DEBUG(dbgs() &lt;&lt; "SLP: Total Cost " &lt;&lt; Cost + ExtractCost &lt;&lt; ".\n");
  return Cost + ExtractCost;
}</pre></div><p class="calibre7">An important<a id="id202" class="calibre1"/> interface to focus on here is the <a id="id203" class="calibre1"/>
<strong class="calibre2">TargetTransformInfo</strong> (<strong class="calibre2">TTI</strong>), which provides access to the codegen interfaces that are needed for IR-level transformations. In our SLP Vectorization, TTI is used to get the cost of the vector instruction of the built vector tree:</p><div><pre class="programlisting">int BoUpSLP::getEntryCost(TreeEntry *E) {
…
…
case Instruction::Store: {
    // We know that we can merge the stores. Calculate the cost.
    int ScalarStCost = VecTy-&gt;getNumElements() *
                       TTI-&gt;getMemoryOpCost(Instruction::Store, ScalarTy, 1, 0);
    int VecStCost = TTI-&gt;getMemoryOpCost(Instruction::Store, VecTy, 1, 0);
    return VecStCost - ScalarStCost;
  }
…
…
}</pre></div><p class="calibre7">In the same function, the cost of vector add is also calculated:</p><div><pre class="programlisting">case Instruction::Add:  {
// Calculate the cost of this instruction.
    int ScalarCost = 0;
    int VecCost = 0;
    if (Opcode == Instruction::FCmp || Opcode == Instruction::ICmp ||
        Opcode == Instruction::Select) {
      VectorType *MaskTy = VectorType::get(Builder.getInt1Ty(), VL.size());
      ScalarCost =
          VecTy-&gt;getNumElements() *
          TTI-&gt;getCmpSelInstrCost(Opcode, ScalarTy, Builder.getInt1Ty());
      VecCost = TTI-&gt;getCmpSelInstrCost(Opcode, VecTy, MaskTy);
    } else {
      // Certain instructions can be cheaper to vectorize if they have
      // a constant second vector operand.
      TargetTransformInfo::OperandValueKind Op1VK =
          TargetTransformInfo::OK_AnyValue;
      TargetTransformInfo::OperandValueKind Op2VK =
          TargetTransformInfo::OK_UniformConstantValue;
      TargetTransformInfo::OperandValueProperties Op1VP =
          TargetTransformInfo::OP_None;
      TargetTransformInfo::OperandValueProperties Op2VP =
          TargetTransformInfo::OP_None;

      // If all operands are exactly the same ConstantInt then set the
      // operand kind to OK_UniformConstantValue.
      // If instead not all operands are constants, then set the operand kind
      // to OK_AnyValue. If all operands are constants but not the 
     // same, then set the operand kind to OK_NonUniformConstantValue.
      ConstantInt *CInt = nullptr;
      for (unsigned i = 0; i &lt; VL.size(); ++i) {
        const Instruction *I = cast&lt;Instruction&gt;(VL[i]);
        if (!isa&lt;ConstantInt&gt;(I-&gt;getOperand(1))) {
          Op2VK = TargetTransformInfo::OK_AnyValue;
          break;
        }
        if (i == 0) {
          CInt = cast&lt;ConstantInt&gt;(I-&gt;getOperand(1));
          continue;
        }
        if (Op2VK == TargetTransformInfo::OK_UniformConstantValue &amp;&amp;
            CInt != cast&lt;ConstantInt&gt;(I-&gt;getOperand(1)))
          Op2VK = TargetTransformInfo::OK_NonUniformConstantValue;
      }
      // FIXME: Currently cost of model modification for division by
      // power of 2 is handled only for X86. Add support for other 
      // targets.
      if (Op2VK == TargetTransformInfo::OK_UniformConstantValue &amp;&amp; CInt &amp;&amp;
          CInt-&gt;getValue().isPowerOf2())
        Op2VP = TargetTransformInfo::OP_PowerOf2;

      <strong class="calibre2">ScalarCost = VecTy-&gt;getNumElements() *</strong>
<strong class="calibre2">                  TTI-&gt;getArithmeticInstrCost(Opcode, ScalarTy, Op1VK, Op2VK, Op1VP, Op2VP);</strong>
<strong class="calibre2">      VecCost = TTI-&gt;getArithmeticInstrCost(Opcode, VecTy, Op1VK, Op2VK, Op1VP, Op2VP);</strong>
    }
    return VecCost - ScalarCost;
  }</pre></div><p class="calibre7">In our example, the total cost of the whole expression tree comes out to be <code class="email">-12</code>, which indicates that it is profitable to vectorize the tree.</p><p class="calibre7">Finally, the tree is vectorized by the function call <code class="email">R.vectorizeTree()</code> on the tree:</p><div><pre class="programlisting">Value *BoUpSLP::vectorizeTree() {
  …
  …
  <strong class="calibre2">vectorizeTree(&amp;VectorizableTree[0]);</strong>
  …
  …
}</pre></div><p class="calibre7">Lets see all the steps the <a id="id204" class="calibre1"/>Vectorization process follows for our example. Note that this will require a '<code class="email">Debug</code>' build of the '<code class="email">opt</code>' tool.</p><div><pre class="programlisting">
<strong class="calibre2">$ opt -S -basicaa -slp-vectorizer -mtriple=aarch64-unknown-linuxgnu -mcpu=cortex-a57 addsub.ll –debug</strong>

<strong class="calibre2">Features:</strong>
<strong class="calibre2">CPU:cortex-a57</strong>

<strong class="calibre2">SLP: Analyzing blocks in addsub.</strong>
<strong class="calibre2">SLP: Found 4 stores to vectorize.</strong>
<strong class="calibre2">SLP: Analyzing a store chain of length 4.</strong>
<strong class="calibre2">SLP: Analyzing a store chain of length 4</strong>
<strong class="calibre2">SLP: Analyzing 4 stores at offset 0</strong>
<strong class="calibre2">SLP:  bundle:   store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0)</strong>
<strong class="calibre2">SLP:  initialize schedule region to   store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0)</strong>
<strong class="calibre2">SLP:  extend schedule region end to   store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1)</strong>
<strong class="calibre2">SLP:  extend schedule region end to   store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2)</strong>
<strong class="calibre2">SLP:  extend schedule region end to   store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)</strong>
<strong class="calibre2">SLP: try schedule bundle [  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0);  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1);  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2);  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)] in block entry</strong>
<strong class="calibre2">SLP:       update deps of [  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0);  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1);  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2);  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)]</strong>
<strong class="calibre2">SLP:       update deps of /   store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1)</strong>
<strong class="calibre2">SLP:       update deps of /   store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2)</strong>
<strong class="calibre2">SLP:       update deps of /   store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)</strong>
<strong class="calibre2">SLP:     gets ready on update:   store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0)</strong>
<strong class="calibre2">SLP: We are able to schedule this bundle.</strong>
<strong class="calibre2">SLP: added a vector of stores.</strong>
<strong class="calibre2">SLP:  bundle:   %add = add nsw i32 %1, %0</strong>
<strong class="calibre2">SLP:  extend schedule region start to   %add = add nsw i32 %1, %0</strong>
<strong class="calibre2">SLP: try schedule bundle [  %add = add nsw i32 %1, %0;  %add1 = add nsw i32 %3, %2;  %add2 = add nsw i32 %5, %4;  %add3 = add nsw i32 %7, %6] in block entry</strong>
<strong class="calibre2">SLP:       update deps of [  %add = add nsw i32 %1, %0;  %add1 = add nsw i32 %3, %2;  %add2 = add nsw i32 %5, %4;  %add3 = add nsw i32 %7, %6]</strong>
<strong class="calibre2">SLP:       update deps of /   %add1 = add nsw i32 %3, %2</strong>
<strong class="calibre2">SLP:       update deps of /   %add2 = add nsw i32 %5, %4</strong>
<strong class="calibre2">SLP:       update deps of /   %add3 = add nsw i32 %7, %6</strong>
<strong class="calibre2">SLP:   schedule [  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0);  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1);  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2);  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)]</strong>
<strong class="calibre2">SLP:    gets ready (def): [  %add = add nsw i32 %1, %0;  %add1 = add nsw i32 %3, %2;  %add2 = add nsw i32 %5, %4;  %add3 = add nsw i32 %7, %6]</strong>
<strong class="calibre2">SLP: We are able to schedule this bundle.</strong>
<strong class="calibre2">SLP: added a vector of bin op.</strong>
<strong class="calibre2">SLP:  bundle:   %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0)</strong>
<strong class="calibre2">SLP:  extend schedule region start to   %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0)</strong>
<strong class="calibre2">SLP: try schedule bundle [  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0);  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1);  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2);  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)] in block entry</strong>
<strong class="calibre2">SLP:       update deps of [  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0);  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1);  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2);  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)]</strong>
<strong class="calibre2">SLP:       update deps of /   %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1)</strong>
<strong class="calibre2">SLP:       update deps of /   %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2)</strong>
<strong class="calibre2">SLP:       update deps of /   %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)</strong>
<strong class="calibre2">SLP:   schedule [  %add = add nsw i32 %1, %0;  %add1 = add nsw i32 %3, %2;  %add2 = add nsw i32 %5, %4;  %add3 = add nsw i32 %7, %6]</strong>
<strong class="calibre2">SLP:    gets ready (def): [  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0);  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1);  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2);  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)]</strong>
<strong class="calibre2">SLP: We are able to schedule this bundle.</strong>
<strong class="calibre2">SLP: added a vector of loads.</strong>
<strong class="calibre2">SLP:  bundle:   %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0)</strong>
<strong class="calibre2">SLP:  extend schedule region start to   %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0)</strong>
<strong class="calibre2">SLP: try schedule bundle [  %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0);  %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1);  %4 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2);  %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3)] in block entry</strong>
<strong class="calibre2">SLP:       update deps of [  %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0);  %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1);  %4 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2);  %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3)]</strong>
<strong class="calibre2">SLP:       update deps of /   %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1)</strong>
<strong class="calibre2">SLP:       update deps of /   %4 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2)</strong>
<strong class="calibre2">SLP:       update deps of /   %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3)</strong>
<strong class="calibre2">SLP:     gets ready on update:   %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0)</strong>
<strong class="calibre2">SLP: We are able to schedule this bundle.</strong>
<strong class="calibre2">SLP: added a vector of loads.</strong>
<strong class="calibre2">SLP: Checking user:  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0).</strong>
<strong class="calibre2">SLP:   Internal user will be removed:  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0).</strong>
<strong class="calibre2">SLP: Checking user:  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1).</strong>
<strong class="calibre2">SLP:   Internal user will be removed:  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1).</strong>
<strong class="calibre2">SLP: Checking user:  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2).</strong>
<strong class="calibre2">SLP:   Internal user will be removed:  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2).</strong>
<strong class="calibre2">SLP: Checking user:  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3).</strong>
<strong class="calibre2">SLP:   Internal user will be removed:  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3).</strong>
<strong class="calibre2">SLP: Checking user:  %add = add nsw i32 %1, %0.</strong>
<strong class="calibre2">SLP:   Internal user will be removed:  %add = add nsw i32 %1, %0.</strong>
<strong class="calibre2">SLP: Checking user:  %add1 = add nsw i32 %3, %2.</strong>
<strong class="calibre2">SLP:   Internal user will be removed:  %add1 = add nsw i32 %3, %2.</strong>
<strong class="calibre2">SLP: Checking user:  %add2 = add nsw i32 %5, %4.</strong>
<strong class="calibre2">SLP:   Internal user will be removed:  %add2 = add nsw i32 %5, %4.</strong>
<strong class="calibre2">SLP: Checking user:  %add3 = add nsw i32 %7, %6.</strong>
<strong class="calibre2">SLP:   Internal user will be removed:  %add3 = add nsw i32 %7, %6.</strong>
<strong class="calibre2">SLP: Checking user:  %add = add nsw i32 %1, %0.</strong>
<strong class="calibre2">SLP:   Internal user will be removed:  %add = add nsw i32 %1, %0.</strong>
<strong class="calibre2">SLP: Checking user:  %add1 = add nsw i32 %3, %2.</strong>
<strong class="calibre2">SLP:   Internal user will be removed:  %add1 = add nsw i32 %3, %2.</strong>
<strong class="calibre2">SLP: Checking user:  %add2 = add nsw i32 %5, %4.</strong>
<strong class="calibre2">SLP:   Internal user will be removed:  %add2 = add nsw i32 %5, %4.</strong>
<strong class="calibre2">SLP: Checking user:  %add3 = add nsw i32 %7, %6.</strong>
<strong class="calibre2">SLP:   Internal user will be removed:  %add3 = add nsw i32 %7, %6.</strong>
<strong class="calibre2">SLP: Calculating cost for tree of size 4.</strong>
<strong class="calibre2">SLP: Adding cost -3 for bundle that starts with   store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0) .</strong>
<strong class="calibre2">SLP: Adding cost -3 for bundle that starts with   %add = add nsw i32 %1, %0 .</strong>
<strong class="calibre2">SLP: Adding cost -3 for bundle that starts with   %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0) .</strong>
<strong class="calibre2">SLP: Adding cost -3 for bundle that starts with   %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0) .</strong>
<strong class="calibre2">SLP: #LV: 0, Looking at   %add = add nsw i32 %1, %0</strong>
<strong class="calibre2">SLP: #LV: 1 add, Looking at   %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0)</strong>
<strong class="calibre2">SLP: #LV: 2  , Looking at   %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0)</strong>
<strong class="calibre2">SLP: SpillCost=0</strong>
<strong class="calibre2">SLP: Total Cost -12.</strong>
<strong class="calibre2">SLP: Found cost=-12 for VF=4</strong>
<strong class="calibre2">SLP: Decided to vectorize cost=-12</strong>
<strong class="calibre2">SLP: schedule block entry</strong>
<strong class="calibre2">SLP:    initially in ready list:   store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0)</strong>
<strong class="calibre2">SLP:   schedule [  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0);  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1);  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2);  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)]</strong>
<strong class="calibre2">SLP:    gets ready (def): [  %add = add nsw i32 %1, %0;  %add1 = add nsw i32 %3, %2;  %add2 = add nsw i32 %5, %4;  %add3 = add nsw i32 %7, %6]</strong>
<strong class="calibre2">SLP:   schedule [  %add = add nsw i32 %1, %0;  %add1 = add nsw i32 %3, %2;  %add2 = add nsw i32 %5, %4;  %add3 = add nsw i32 %7, %6]</strong>
<strong class="calibre2">SLP:    gets ready (def): [  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0);  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1);  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2);  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)]</strong>
<strong class="calibre2">SLP:    gets ready (def): [  %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0);  %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1);  %4 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2);  %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3)]</strong>
<strong class="calibre2">SLP:   schedule [  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0);  %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1);  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2);  %4 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)]</strong>
<strong class="calibre2">SLP:   schedule [  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0);  %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1);  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2);  %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3)]</strong>
<strong class="calibre2">SLP: Extracting 0 values .</strong>
<strong class="calibre2">SLP:   Erasing scalar:  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0).</strong>
<strong class="calibre2">SLP:   Erasing scalar:  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1).</strong>
<strong class="calibre2">SLP:   Erasing scalar:  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2).</strong>
<strong class="calibre2">SLP:   Erasing scalar:  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3).</strong>
<strong class="calibre2">SLP:   Erasing scalar:  %add = add nsw i32 %8, %3.</strong>
<strong class="calibre2">SLP:   Erasing scalar:  %add1 = add nsw i32 %7, %2.</strong>
<strong class="calibre2">SLP:   Erasing scalar:  %add2 = add nsw i32 %6, %1.</strong>
<strong class="calibre2">SLP:   Erasing scalar:  %add3 = add nsw i32 %5, %0.</strong>
<strong class="calibre2">SLP:   Erasing scalar:  %8 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0).</strong>
<strong class="calibre2">SLP:   Erasing scalar:  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1).</strong>
<strong class="calibre2">SLP:   Erasing scalar:  %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2).</strong>
<strong class="calibre2">SLP:   Erasing scalar:  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3).</strong>
<strong class="calibre2">SLP:   Erasing scalar:  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0).</strong>
<strong class="calibre2">SLP:   Erasing scalar:  %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1).</strong>
<strong class="calibre2">SLP:   Erasing scalar:  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2).</strong>
<strong class="calibre2">SLP:   Erasing scalar:  %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3).</strong>
<strong class="calibre2">SLP: Optimizing 0 gather sequences instructions.</strong>
<strong class="calibre2">SLP: vectorized "addsub"</strong>
</pre></div><p class="calibre7">The final vectorized output is:</p><div><pre class="programlisting">
<strong class="calibre2">; ModuleID = 'addsub.ll'</strong>
<strong class="calibre2">target triple = "aarch64-unknown-linuxgnu"</strong>

<strong class="calibre2">@a = global [4 x i32] zeroinitializer, align 4</strong>
<strong class="calibre2">@b = global [4 x i32] zeroinitializer, align 4</strong>
<strong class="calibre2">@c = global [4 x i32] zeroinitializer, align 4</strong>

<strong class="calibre2">define void @addsub()  {</strong>
<strong class="calibre2">entry:</strong>
<strong class="calibre2">  %0 = load &lt;4 x i32&gt;, &lt;4 x i32&gt;* bitcast ([4 x i32]* @b to &lt;4 x i32&gt;*), align 4</strong>
<strong class="calibre2">  %1 = load &lt;4 x i32&gt;, &lt;4 x i32&gt;* bitcast ([4 x i32]* @c to &lt;4 x i32&gt;*), align 4</strong>
<strong class="calibre2">  %2 = add nsw &lt;4 x i32&gt; %1, %0</strong>
<strong class="calibre2">  store &lt;4 x i32&gt; %2, &lt;4 x i32&gt;* bitcast ([4 x i32]* @a to &lt;4 x i32&gt;*), align 4</strong>
<strong class="calibre2">  ret void</strong>
<strong class="calibre2">}</strong>
</pre></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec39" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">In this chapter, we concluded the optimizer part of the compiler where we had seen block level optimizations. We took the examples of loop optimization, Scalar Evolution, Vectorization, and LLVM Intrinsic functions. We also saw how SLP Vectorization is handled in LLVM. However, there are many other such optimizations that you can look into and get a hold of.</p><p class="calibre7">In the next chapter, we will see how this IR is converted to <strong class="calibre2">Directed Acyclic Graph</strong>. We have some optimizations at <code class="email">selectionDAG</code> level as well, which we will take a look at.</p></div></body></html>