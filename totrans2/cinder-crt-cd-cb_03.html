<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Using Image Processing Techniques"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Using Image Processing Techniques</h1></div></div></div><p>In this chapter we will cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Transforming image contrast and brightness</li><li class="listitem" style="list-style-type: disc">Integrating with OpenCV</li><li class="listitem" style="list-style-type: disc">Detecting edges</li><li class="listitem" style="list-style-type: disc">Detecting faces</li><li class="listitem" style="list-style-type: disc">Detecting features in image</li><li class="listitem" style="list-style-type: disc">Converting images to vector graphics</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec29"/>Introduction</h1></div></div></div><p>In this chapter, we will show examples of using image processing techniques implemented in Cinder and using<a id="id195" class="indexterm"/> third-party libraries. In most of the examples, we will use the following famous test image widely used to illustrate computer vision algorithms and techniques:</p><div class="mediaobject"><img src="graphics/8703OS_03_01.jpg" alt="Introduction"/></div><p>You can <a id="id196" class="indexterm"/>download Lenna's image from Wikipedia (<a class="ulink" href="http://en.wikipedia.org/wiki/File:Lenna.png">http://en.wikipedia.org/wiki/File:Lenna.png</a>).</p></div></div>
<div class="section" title="Transforming image contrast and brightness"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec30"/>Transforming image contrast and brightness</h1></div></div></div><p>In this recipe we will cover<a id="id197" class="indexterm"/>
<a id="id198" class="indexterm"/> basic image color transformations using the <code class="literal">Surface</code> class for pixel manipulation.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec79"/>Getting ready</h2></div></div></div><p>To change the values of contrast and brightness we will use <code class="literal">InterfaceGl</code> covered in <a class="link" href="ch02.html" title="Chapter 2. Preparing for Development">Chapter 2</a>, <span class="emphasis"><em>Preparing for Development in the Setting up GUI for parameters tweaking</em></span> recipe. We will need a sample image to proceed with; save it in your <code class="literal">assets</code> folder as <code class="literal">image.png</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec80"/>How to do it...</h2></div></div></div><p>We will create an application with simple GUI for contrast and brightness manipulation on the sample image. Perform the following steps to do so:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Include necessary headers:<div class="informalexample"><pre class="programlisting">#include "cinder/gl/gl.h"
#include "cinder/gl/Texture.h"
#include "cinder/Surface.h"
#include "cinder/ImageIo.h"</pre></div></li><li class="listitem">Add properties to the main class:<div class="informalexample"><pre class="programlisting">float mContrast,mContrastOld;
float mBrightness,mBrightnessOld;
Surface32f  mImage, mImageOutput;</pre></div></li><li class="listitem">In the <code class="literal">setup</code> method an image is loaded for processing and the <code class="literal">Surface</code> object is prepared to store processed image:<div class="informalexample"><pre class="programlisting">mImage = loadImage( loadAsset("image.png") );
mImageOutput = Surface32f(mImage.getWidth(), 
        mImage.getHeight(), false);</pre></div></li><li class="listitem">Set window size to default values:<div class="informalexample"><pre class="programlisting">setWindowSize(1025, 512);
mContrast = 0.f;
mContrastOld = -1.f;
mBrightness = 0.f;
mBrightnessOld = -1.f;</pre></div></li><li class="listitem">Add parameter controls to the <code class="literal">InterfaceGl</code> window:<div class="informalexample"><pre class="programlisting">mParams.addParam("Contrast", &amp;mContrast, 
"min=-0.5 max=1.0 step=0.01");
mParams.addParam("Brightness", &amp;mBrightness, 
      "min=-0.5 max=0.5 step=0.01");</pre></div></li><li class="listitem">Implement the <code class="literal">update</code> method as follows:<div class="informalexample"><pre class="programlisting">if(mContrastOld != mContrast || mBrightnessOld != mBrightness) {
float c = 1.f + mContrast;
    Surface32f::IterpixelIter = mImage.getIter();
    Surface32f::IterpixelOutIter = mImageOutput.getIter();

    while( pixelIter.line() ) {
    pixelOutIter.line();
    while( pixelIter.pixel() ) {
    pixelOutIter.pixel();

    // contrast transformation
    pixelOutIter.r() = (pixelIter.r() - 0.5f) * c + 0.5f;
    pixelOutIter.g() = (pixelIter.g() - 0.5f) * c + 0.5f;
    pixelOutIter.b() = (pixelIter.b() - 0.5f) * c + 0.5f;

    // brightness transformation
    pixelOutIter.r() += mBrightness;
    pixelOutIter.g() += mBrightness;
    pixelOutIter.b() += mBrightness;

        }
    }

mContrastOld = mContrast;
mBrightnessOld = mBrightness;
}</pre></div></li><li class="listitem">Lastly, we will draw the original and processed images by adding the following lines of code inside the <code class="literal">draw</code> method:<div class="informalexample"><pre class="programlisting">gl::draw(mImage);
gl::draw(mImageOutput, Vec2f(512.f+1.f, 0.f));</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec81"/>How it works...</h2></div></div></div><p>The most important part is <a id="id199" class="indexterm"/>
<a id="id200" class="indexterm"/>inside the <code class="literal">update</code> method. In step 6 we checked if the parameters for contrast and brightness had been changed. If they have, we iterate through all the pixels of the original image and store recalculated color values in <code class="literal">mImageOutput</code>. While modifying the brightness is just increasing or decreasing each color component, calculating contrast is a little more complicated. For each color component we are using the multiplying formula, <span class="emphasis"><em>color = (color - 0.5) * contrast + 0.5</em></span>, where contrast is a number between 0.5 and 2. In the GUI we are setting a value between -0.5 and 1.0, which is more natural range; it is then recalculated at the beginning of step 6. While processing the image we have to change color value of all pixels, so later in step 6, you can see that we iterate through<a id="id201" class="indexterm"/>
<a id="id202" class="indexterm"/> later columns of each row of the pixels using two <code class="literal">while</code> loops. To move to the next row we invoked the <code class="literal">line</code> method on the <code class="literal">Surface</code> iterator and then the <code class="literal">pixel</code> method to move to the next pixel of the current row. This method is much faster than using, for example, the <code class="literal">getPixel</code> and <code class="literal">setPixel</code> methods.</p><div class="mediaobject"><img src="graphics/8703OS_03_02.jpg" alt="How it works..."/></div><p>Our application is rendering the original image on the left-hand side and the processed image on the right-hand side, so you can compare the results of color adjustment.</p></div></div>
<div class="section" title="Integrating with OpenCV"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec31"/>Integrating with OpenCV</h1></div></div></div><p>OpenCV is a very powerful <a id="id203" class="indexterm"/>open-source library for computer vision. The library is written in C++ so it can be easily integrated in your Cinder application. There is a very useful OpenCV Cinder block provided within Cinder package available at the GitHub repository<a id="id204" class="indexterm"/> (<a class="ulink" href="https://github.com/cinder/Cinder-OpenCV">https://github.com/cinder/Cinder-OpenCV</a>).</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec82"/>Getting ready</h2></div></div></div><p>Make sure you have Xcode up and running with a Cinder project opened.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec83"/>How to do it…</h2></div></div></div><p>We will add OpenCV Cinder<a id="id205" class="indexterm"/> block to your project, which also illustrates the usual way of adding any other Cinder block to your project. Perform the following steps to do so:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Add a new group to our Xcode project root and name it <code class="literal">Blocks.</code> Next, drag the <code class="literal">opencv</code> folder inside the <code class="literal">Blocks</code> group. Be sure to select the <span class="strong"><strong>Create groups for any added folders</strong></span> radio button, as shown in the following screenshot:<div class="mediaobject"><img src="graphics/8703OS_03_03.jpg" alt="How to do it…"/></div></li><li class="listitem">You will need only the <code class="literal">include</code> folder inside the <code class="literal">opencv</code> folder in your project structure, so delete any reference to others. The final project structure should look like the following screenshot:<div class="mediaobject"><img src="graphics/8703OS_03_04.jpg" alt="How to do it…"/></div></li><li class="listitem">Add the paths to the OpenCV<a id="id206" class="indexterm"/> library files in the <span class="strong"><strong>Other Linker Flags</strong></span> section of your project's build settings, for example:<div class="informalexample"><pre class="programlisting">$(CINDER_PATH)/blocks/opencv/lib/macosx/libopencv_imgproc.a
$(CINDER_PATH)/blocks/opencv/lib/macosx/libopencv_core.a
$(CINDER_PATH)/blocks/opencv/lib/macosx/libopencv_objdetect.a</pre></div><p>These paths are shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/8703OS_03_05.jpg" alt="How to do it…"/></div></li><li class="listitem">Add the paths to the OpenCV Cinder block headers you are going to use in the <span class="strong"><strong>User Header Search Paths</strong></span> section of your project's build settings:<div class="informalexample"><pre class="programlisting">$(CINDER_PATH)/blocks/opencv/include</pre></div><p>This path is shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/8703OS_03_06.jpg" alt="How to do it…"/></div></li><li class="listitem">Include OpenCV Cinder block header file:<div class="informalexample"><pre class="programlisting">#include "CinderOpenCV.h"</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec84"/>How it works…</h2></div></div></div><p>OpenCV Cinder block provides<a id="id207" class="indexterm"/> the <code class="literal">toOcv</code> and <code class="literal">fromOcv</code> functions for data exchange between Cinder and OpenCV. After setting up your project you can use them, as shown in the following short example:</p><div class="informalexample"><pre class="programlisting">Surface mImage, mImageOutput;
mImage = loadImage( loadAsset("image.png") );
cv::Mat ocvImage(toOcv(mImage));
cv::cvtColor(ocvImage, ocvImage, CV_BGR2GRAY ); 
mImageOutput = Surface(fromOcv(ocvImage));</pre></div><p>You can use the <code class="literal">toOcv</code> and <code class="literal">fromOcv</code> functions<a id="id208" class="indexterm"/> to convert between Cinder and OpenCV types, storing image data such as <code class="literal">Surface</code> or <code class="literal">Channel</code> handled through the <code class="literal">ImageSourceRef</code> type; there are also other types, as shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Cinder types</p>
</th><th style="text-align: left" valign="bottom">
<p>OpenCV types</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">ImageSourceRef</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">Mat</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">Color</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">Scalar</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">Vec2f</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">Point2f</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">Vec2i</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">Point</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">Area</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">Rect</code></p>
</td></tr></tbody></table></div><p>In this example we are linking against<a id="id209" class="indexterm"/> the following three files from the OpenCV package:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">libopencv_imgproc.a</code>: This image processing module includes image manipulation functions, filters, feature detection, and more<a id="id210" class="indexterm"/></li><li class="listitem" style="list-style-type: disc"><code class="literal">libopencv_core.a</code>: This module provides core functionality and data structures<a id="id211" class="indexterm"/></li><li class="listitem" style="list-style-type: disc"><code class="literal">libopencv_objdetect.a</code>: This module has object detection tools such as cascade classifiers<a id="id212" class="indexterm"/></li></ul></div><p>You can find the documentation on all OpenCV modules at <a class="ulink" href="http://docs.opencv.org/index.html">http://docs.opencv.org/index.html</a>.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec85"/>There's more…</h2></div></div></div><p>There are some features that are not available in precompiled OpenCV libraries packaged in OpenCV Cinder block, but you can always compile your own OpenCV libraries and still use exchange functions from OpenCV Cinder block in your project.</p></div></div>
<div class="section" title="Detecting edges"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec32"/>Detecting edges</h1></div></div></div><p>In this recipe, we will demonstrate<a id="id213" class="indexterm"/> how to use edge detection function, which is one of the image processing functions implemented directly in Cinder.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec86"/>Getting ready</h2></div></div></div><p>Make sure you have Xcode up and running with an empty Cinder project opened. We will need a sample image to proceed, so save it in your assets folder as <code class="literal">image.png</code>.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec87"/>How to do it…</h2></div></div></div><p>We will process the sample image with the edge detection function. Perform the following steps to do so:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Include necessary headers:<div class="informalexample"><pre class="programlisting">#include "cinder/gl/Texture.h"
#include "cinder/Surface.h"
#include "cinder/ImageIo.h"

#include "cinder/ip/EdgeDetect.h"
#include "cinder/ip/Grayscale.h"</pre></div></li><li class="listitem">Add two properties to your main class:<div class="informalexample"><pre class="programlisting">Surface8u mImageOutput;</pre></div></li><li class="listitem">Load the source image <a id="id214" class="indexterm"/>and set up <code class="literal">Surface</code> for processed images inside the <code class="literal">setup</code> method:<div class="informalexample"><pre class="programlisting">mImage = loadImage( loadAsset("image.png") );
mImageOutput = Surface8u(mImage.getWidth(), mImage.getHeight(), false);</pre></div></li><li class="listitem">Use image processing functions:<div class="informalexample"><pre class="programlisting">ip::grayscale(mImage, &amp;mImage);
ip::edgeDetectSobel(mImage, &amp;mImageOutput);</pre></div></li><li class="listitem">Inside the <code class="literal">draw</code> method add the following two lines of code for drawing images:<div class="informalexample"><pre class="programlisting">gl::draw(mImage);
gl::draw(mImageOutput, Vec2f(512.f+1.f, 0.f));</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec88"/>How it works…</h2></div></div></div><p>As you can see, detecting edges in Cinder is pretty easy because of implementation of basic image processing functions<a id="id215" class="indexterm"/> directly in Cinder, so you don't have to include any third-party libraries. In this case we are using the <code class="literal">grayscale</code> function to convert the original image color space to grayscale. It is a commonly used feature in image processing because many algorithms work more efficiently on grayscale images or are even designed to work only with grayscale source images. The edge detection is implemented with the <code class="literal">edgeDetectSobel</code> function<a id="id216" class="indexterm"/> and uses the Sobel algorithm. In this case, the first parameter is the source original grayscale image and the second parameter, is the output <code class="literal">Surface</code> object in which the result will be stored.</p><p>Inside the <code class="literal">draw</code> method we are drawing both images, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/8703OS_03_07.jpg" alt="How it works…"/></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec89"/>There's more…</h2></div></div></div><p>You may find the image processing functions implemented in Cinder insufficient, so you can also include to your project,<a id="id217" class="indexterm"/> third-party library such as OpenCV. We explained how we can use Cinder and OpenCV together in the preceding recipe, <span class="emphasis"><em>Integrating with OpenCV</em></span>.</p><p>Other useful functions in the context of edge detection are <code class="literal">Canny</code> and <code class="literal">findContours</code>. The following is the example of how we can use them:</p><div class="informalexample"><pre class="programlisting">vector&lt;vector&lt;cv::Point&gt; &gt; contours; 
cv::Mat inputMat( toOcv( frame ) );
// blur
cv::cvtColor( inputMat, inputMat, CV_BGR2GRAY );
cv::Mat blurMat;
cv::medianBlur(inputMat, blurMat, 11);

// threshold
cv::Mat thresholdMat;
cv::threshold(blurMat, thresholdMat, 50, 255, CV_8U );

// erode
cv::Mat erodeMat;
cv::erode(thresholdMat, erodeMat, 11);

// Detect edges
cv::Mat cannyMat;
int thresh = 100;
cv::Canny(erodeMat, cannyMat, thresh, thresh*2, 3 );

// Find contours
cv::findContours(cannyMat, contours, CV_RETR_TREE, CV_CHAIN_APPROX_SIMPLE);</pre></div><p>After executing the preceding code, the points, which form the contours are stored in the <code class="literal">contours</code> variable.<a id="id218" class="indexterm"/></p></div></div>
<div class="section" title="Detecting faces"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec33"/>Detecting faces</h1></div></div></div><p>In this recipe, we will examine <a id="id219" class="indexterm"/>how our application can be used to recognize human faces. Thanks to the OpenCV library, it is really easy.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec90"/>Getting ready</h2></div></div></div><p>We will be using the OpenCV library, so please refer to the <span class="emphasis"><em>Integrating with OpenCV</em></span> recipe for information on how to set up your project. We will need a sample image to proceed, so save it in your <code class="literal">assets</code> folder as <code class="literal">image.png</code>. Put the Haar cascade classifier file for frontal face recognition inside the <code class="literal">assets</code> directory. The cascade file can be found inside the downloaded OpenCV package or in the online public repository, located at <a class="ulink" href="https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_frontalface_alt.xml">https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_frontalface_alt.xml</a>.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec91"/>How to do it…</h2></div></div></div><p>We will create an application that demonstrates the usage of cascade classifier from OpenCV with Cinder. Perform the following steps to do so:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Include necessary headers:<div class="informalexample"><pre class="programlisting">#include "cinder/gl/Texture.h"
#include "cinder/Surface.h"
#include "cinder/ImageIo.h"</pre></div></li><li class="listitem">Add the following members to your main class:<div class="informalexample"><pre class="programlisting">Surface8u mImage;
cv::CascadeClassifier  mFaceCC;
std::vector&lt;Rectf&gt;  mFaces;</pre></div></li><li class="listitem">Add the following code snippet to the <code class="literal">setup</code> method:<div class="informalexample"><pre class="programlisting">mImage = loadImage( loadAsset("image.png") );
mFaceCC.load( getAssetPath( "haarcascade_frontalface_alt.xml" ).string() );</pre></div></li><li class="listitem">Also add the following code snippet at the end of the <code class="literal">setup</code> method:<div class="informalexample"><pre class="programlisting">cv::Mat cvImage( toOcv( mImage, CV_8UC1 ) );
std::vector&lt;cv::Rect&gt; faces;
mFaceCC.detectMultiScale( cvImage, faces );
std::vector&lt;cv::Rect&gt;::const_iterator faceIter;
for(faceIter = faces.begin(); faceIter != faces.end(); ++faceIter ) {
  Rectf faceRect( fromOcv( *faceIter ) );
  mFaces.push_back( faceRect );
}</pre></div></li><li class="listitem">At the end of the <code class="literal">draw</code> method add the following code snippet:<div class="informalexample"><pre class="programlisting">gl::color( Color::white() );
gl::draw(mImage);
gl::color( ColorA( 1.f, 0.f, 0.f, 0.45f ) );
std::vector&lt;Rectf&gt;::const_iterator faceIter;
for(faceIter = mFaces.begin(); faceIter != mFaces.end(); ++faceIter ) {
  gl::drawStrokedRect( *faceIter );
}</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec92"/>How it works…</h2></div></div></div><p>In step 3 we loaded an image file for processing<a id="id220" class="indexterm"/> and an XML classifier file, which has description of the object features to be recognized. In step 4 we performed an image detection by invoking the <a id="id221" class="indexterm"/>
<code class="literal">detectMultiScale</code> function on the <code class="literal">mFaceCC</code> object, where we pointed to <code class="literal">cvImage</code> as an input and stored the result in a vector structure, <code class="literal">cvImage</code> is converted from <code class="literal">mImage</code> as an 8-bit, single channel image (<code class="literal">CV_8UC1</code>). What we did next was iterating through all the detected faces and storing <code class="literal">Rectf</code> variable, which describes a bounding box around the detected face. Finally, in step 5 we drew our original image and all the recognized faces as stroked rectangles.</p><p>We are using cascade classifier implemented in OpenCV, which can be trained to detect a specific object in the image. More on training and using cascade classifier for object detection can be found in the OpenCV documentation, located at <a class="ulink" href="http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html">http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html</a>.</p><div class="mediaobject"><img src="graphics/8703OS_03_08.jpg" alt="How it works…"/></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec93"/>There's more…</h2></div></div></div><p>You can use a video stream from your <a id="id222" class="indexterm"/>camera and process each frame to track faces of people in real time. Please refer to the <span class="emphasis"><em>Capturing from the camera</em></span> recipe in <a class="link" href="ch11.html" title="Chapter 11. Sensing and Tracking Input from the Camera">Chapter 11</a>, <span class="emphasis"><em>Sensing and Tracking Input from the Camera</em></span>.</p></div></div>
<div class="section" title="Detecting features in an image"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec34"/>Detecting features in an image</h1></div></div></div><p>In this recipe we will use one of the methods of finding characteristic features in the image. We will use the SURF<a id="id223" class="indexterm"/> algorithm implemented by the OpenCV library.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec94"/>Getting ready</h2></div></div></div><p>We will be using the OpenCV library, so please refer to the <span class="emphasis"><em>Integrating with OpenCV</em></span> recipe for information on how to set up your project. We will need a sample image to proceed, so save it in your <code class="literal">assets</code> folder as <code class="literal">image.png</code>, then save a copy of the sample image as <code class="literal">image2.png</code> and perform some transformation on it, for example rotation.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec95"/>How to do it…</h2></div></div></div><p>We will create an application that visualizes matched features between two images. Perform the following steps to do so:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Add the paths to the OpenCV library files in the <span class="strong"><strong>Other Linker Flags</strong></span> section of your project's build settings, for example:<div class="informalexample"><pre class="programlisting">$(CINDER_PATH)/blocks/opencv/lib/macosx/libopencv_imgproc.a
$(CINDER_PATH)/blocks/opencv/lib/macosx/libopencv_core.a
$(CINDER_PATH)/blocks/opencv/lib/macosx/libopencv_objdetect.a
$(CINDER_PATH)/blocks/opencv/lib/macosx/libopencv_features2d.a
$(CINDER_PATH)/blocks/opencv/lib/macosx/libopencv_flann.a</pre></div></li><li class="listitem">Include necessary headers:<div class="informalexample"><pre class="programlisting">#include "cinder/gl/Texture.h"
#include "cinder/Surface.h"
#include "cinder/ImageIo.h"</pre></div></li><li class="listitem">In your main class declaration add the method and properties:<div class="informalexample"><pre class="programlisting">int matchImages(Surface8u img1, Surface8u img2);

Surface8u   mImage, mImage2;
gl::Texture mMatchesImage;</pre></div></li><li class="listitem">Inside the <code class="literal">setup</code> method <a id="id224" class="indexterm"/>load the images and invoke the matching method:<div class="informalexample"><pre class="programlisting">mImage = loadImage( loadAsset("image.png") );
mImage2 = loadImage( loadAsset("image2.png") );

int numberOfmatches = matchImages(mImage, mImage2);</pre></div></li><li class="listitem">Now you have to implement previously declared <code class="literal">matchImages</code> method<a id="id225" class="indexterm"/>:<div class="informalexample"><pre class="programlisting">int MainApp::matchImages(Surface8u img1, Surface8u img2)
{
  cv::Mat image1(toOcv(img1));
  cv::cvtColor( image1, image1, CV_BGR2GRAY );

  cv::Mat image2(toOcv(img2));
  cv::cvtColor( image2, image2, CV_BGR2GRAY );

  // Detect the keypoints using SURF Detector
  std::vector&lt;cv::KeyPoint&gt; keypoints1, keypoints2;

  cv::SurfFeatureDetector detector;
  detector.detect( image1, keypoints1 );
  detector.detect( image2, keypoints2 );

  // Calculate descriptors (feature vectors)
  cv::SurfDescriptorExtractor extractor;
  cv::Mat descriptors1, descriptors2;

  extractor.compute( image1, keypoints1, descriptors1 );
  extractor.compute( image2, keypoints2, descriptors2 );

  // Matching
  cv::FlannBasedMatcher matcher;
  std::vector&lt;cv::DMatch&gt; matches;
  matcher.match( descriptors1, descriptors2, matches );

  double max_dist = 0; 
  double min_dist = 100;

  for( int i = 0; i&lt; descriptors1.rows; i++ )
    {
  double dist = matches[i].distance;
  if( dist&lt;min_dist ) min_dist = dist;
  if( dist&gt;max_dist ) max_dist = dist;
      }

  std::vector&lt;cv::DMatch&gt; good_matches;

  for( int i = 0; i&lt; descriptors1.rows; i++ )
      {
  if( matches[i].distance&lt;2*min_dist )
  good_matches.push_back( matches[i]);
      }

  // Draw matches
  cv::Matimg_matches;
  cv::drawMatches(image1, keypoints1, image2, keypoints2,
  good_matches, img_matches, cv::Scalar::all(-1),cv::Scalar::all(-1),
  std::vector&lt;char&gt;(), cv::DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS );

  mMatchesImage = gl::Texture(fromOcv(img_matches));

  return good_matches.size();
    }</pre></div></li><li class="listitem">The last thing is to visualize the matches, so put the following line of code inside the <code class="literal">draw</code> method:<div class="informalexample"><pre class="programlisting">gl::draw(mMatchesImage);</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec96"/>How it works…</h2></div></div></div><p>Let's discuss the code under step 5.<a id="id226" class="indexterm"/> First we are converting <code class="literal">image1</code> and <code class="literal">image2</code> to an OpenCV Mat structure. Then we are converting both images to grayscale. Now we can start processing images with SURF, so we are detecting keypoints – the characteristic points of the image calculated by this algorithm. We can use calculated keypoints from these two images and match them using FLANN, or more precisely the <code class="literal">FlannBasedMatcher</code> class<a id="id227" class="indexterm"/>. After filtering out the proper matches and storing them in the <code class="literal">good_matches</code> vector we can visualize them, as follows:</p><div class="mediaobject"><img src="graphics/8703OS_03_09.jpg" alt="How it works…"/></div><p>Please notice that second image<a id="id228" class="indexterm"/> is rotated, however the algorithm can still find and link the corresponding keypoints.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec97"/>There's more…</h2></div></div></div><p>Detecting characteristic features in the images is crucial for matching pictures and is part of more advanced algorithms used in augmented reality applications.</p><div class="section" title="If images match"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec14"/>If images match</h3></div></div></div><p>It is possible to determine if one <a id="id229" class="indexterm"/>of the images is a copy of another or is it rotated. You can use a number of matches returned by the <code class="literal">matchImages</code> method.</p></div><div class="section" title="Other possibilities"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec15"/>Other possibilities</h3></div></div></div><p>SURF is rather a slow algorithm for real-time matching so you can try the FAST algorithm for your project if you need to <a id="id230" class="indexterm"/>process frames from the camera at real time. The FAST algorithm is also included in the OpenCV library.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec98"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The comparison of the OpenCV's feature detection algorithms can be found at <a class="ulink" href="http://computer-vision-talks.com/2011/01/comparison-of-the-opencvs-feature-detection-algorithms-2/">http://computer-vision-talks.com/2011/01/comparison-of-the-opencvs-feature-detection-algorithms-2/</a></li></ul></div></div></div>
<div class="section" title="Converting images to vector graphics"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec35"/>Converting images to vector graphics</h1></div></div></div><p>In this recipe, we will <a id="id231" class="indexterm"/>try to convert simple, hand-drawn sketches to vector graphics using image processing functions from the OpenCV library and Cairo library for vector drawing and exporting.</p><div class="section" title="Getting started"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec99"/>Getting started</h2></div></div></div><p>We will be using the OpenCV library, so please refer to the <span class="emphasis"><em>Integrating with OpenCV</em></span> recipe earlier in this chapter for information on how to set up your project. You may want to prepare your own drawing to be processed. In this example we are using a photo of some simple geometric shapes sketched on paper.</p><div class="mediaobject"><img src="graphics/8703OS_03_10.jpg" alt="Getting started"/></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec100"/>How to do it…</h2></div></div></div><p>We will create an application <a id="id232" class="indexterm"/>to illustrate the conversion to vector shapes. Perform the following steps to do so:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Include necessary headers:<div class="informalexample"><pre class="programlisting">#include "cinder/gl/Texture.h"
#include "cinder/Surface.h"
#include "cinder/ImageIo.h"
#include "cinder/cairo/Cairo.h"</pre></div></li><li class="listitem">Add the following declarations to your main class:<div class="informalexample"><pre class="programlisting">void renderDrawing( cairo::Context&amp;ctx );

Surface mImage, mIPImage;
std::vector&lt;std::vector&lt;cv::Point&gt; &gt;mContours, mContoursApprox;
double mApproxEps;
int mCannyThresh;</pre></div></li><li class="listitem">Load your drawing and set default values inside the <code class="literal">setup</code> method:<div class="informalexample"><pre class="programlisting">mImage = loadImage( loadAsset("drawing.jpg") );

mApproxEps = 1.0;
mCannyThresh = 200;</pre></div></li><li class="listitem">At the end of the <code class="literal">setup</code> method<a id="id233" class="indexterm"/> add the following code snippet:<div class="informalexample"><pre class="programlisting">cv::Mat inputMat( toOcv( mImage ) );

cv::Mat bgr, gray, outputFrame;
cv::cvtColor(inputMat, bgr, CV_BGRA2BGR);
double sp = 50.0;
double sr = 55.0;
cv::pyrMeanShiftFiltering(bgr.clone(), bgr, sp, sr);

cv::cvtColor(bgr, gray, CV_BGR2GRAY);
cv::cvtColor(bgr, outputFrame, CV_BGR2BGRA);
mIPImage = Surface(fromOcv(outputFrame));
cv::medianBlur(gray, gray, 7);

// Detect edges using
cv::MatcannyMat;
cv::Canny(gray, cannyMat, mCannyThresh, mCannyThresh*2.f, 3 );
mIPImage = Surface(fromOcv(cannyMat));

// Find contours
cv::findContours(cannyMat, mContours, CV_RETR_LIST, CV_CHAIN_APPROX_SIMPLE);

// prepare outline
for( int i = 0; i&lt;mContours.size(); i++ )
{
std::vector&lt;cv::Point&gt; approxCurve;
cv::approxPolyDP(mContours[i], approxCurve, mApproxEps, true);
mContoursApprox.push_back(approxCurve);
}</pre></div></li><li class="listitem">Add implementation for the <code class="literal">renderDrawing</code> method:<div class="informalexample"><pre class="programlisting">void MainApp::renderDrawing( cairo::Context&amp;ctx )
{
  ctx.setSource( ColorA( 0, 0, 0, 1 ) );
  ctx.paint();
  
  ctx.setSource( ColorA( 1, 1, 1, 1 ) );
  for( int i = 0; i&lt;mContoursApprox.size(); i++ )
    {
  ctx.newSubPath();
  ctx.moveTo(mContoursApprox[i][0].x, mContoursApprox[i][0].y);
  for( int j = 1; j &lt;mContoursApprox[i].size(); j++ )
        {
ctx.lineTo(mContoursApprox[i][j].x, mContoursApprox[i][j].y);
        }
ctx.closePath();
ctx.fill();

ctx.setSource(Color( 1, 0, 0 ));
for( int j = 1; j &lt;mContoursApprox[i].size(); j++ )
        {
ctx.circle(mContoursApprox[i][j].x, mContoursApprox[i][j].y, 2.f);
        }
ctx.fill();
    }
}</pre></div></li><li class="listitem">Implement your <code class="literal">draw</code> method<a id="id234" class="indexterm"/> <a id="id235" class="indexterm"/>as follows:<div class="informalexample"><pre class="programlisting">  gl::clear( Color( 0.1f, 0.1f, 0.1f ) );

  gl::color(Color::white());

  gl::pushMatrices();
  gl::scale(Vec3f(0.5f,0.5f,0.5f));
  gl::draw(mImage);
  gl::draw(mIPImage, Vec2i(0, mImage.getHeight()+1));
  gl::popMatrices();

  gl::pushMatrices();
  gl::translate(Vec2f(mImage.getWidth()*0.5f+1.f, 0.f));
  gl::color( Color::white() );

  cairo::SurfaceImage vecSurface( mImage.getWidth(), mImage.getHeight() );
  cairo::Context ctx( vecSurface );
  renderDrawing(ctx);
  gl::draw(vecSurface.getSurface());

  gl::popMatrices();</pre></div></li><li class="listitem">Inside the <code class="literal">keyDown</code> method<a id="id236" class="indexterm"/> insert the following code snippet:<div class="informalexample"><pre class="programlisting">if( event.getChar() == 's' ) {
cairo::Context ctx( cairo::SurfaceSvg( getAppPath() / fs::path("..") / "output.svg",mImage.getWidth(), mImage.getHeight() ) );
renderDrawing( ctx );
}</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec101"/>How it works…</h2></div></div></div><p>The key part is implemented in<a id="id237" class="indexterm"/> step 4 where we are detecting edges in the image and then finding contours. We are drawing vector representation of processed shapes in step 5, inside the <a id="id238" class="indexterm"/>
<code class="literal">renderDrawing</code> method. For drawing vector graphics we are using the Cairo library, which is also able to save results into a file in several vector formats. As you can see in the following screenshot, there is an original image in the upper-left corner and just under it is the preview of the detected contours. The vector version of our simple hand-drawn image is on the right-hand side:</p><div class="mediaobject"><img src="graphics/8703OS_03_11.jpg" alt="How it works…"/></div><p>Each shape is a filled path with<a id="id239" class="indexterm"/> black color. Paths consist of points calculated in step 4. The following is the visualization with highlighted points:</p><div class="mediaobject"><img src="graphics/8703OS_03_12.jpg" alt="How it works…"/></div><p>You can save a vector graphic as a file by pressing the <span class="emphasis"><em>S</em></span> key<a id="id240" class="indexterm"/>. The file will be saved in the same folder as application executable <a id="id241" class="indexterm"/>under the name <code class="literal">output.svg</code>. SVG is only one of the following available exporting options:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Method</p>
</th><th style="text-align: left" valign="bottom">
<p>Usage</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">SurfaceSvg</code>
<a id="id242" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p>Preparing context for SVG file rendering</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">SurfacePdf</code>
<a id="id243" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p>Preparing context for PDF file rendering</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">SurfacePs</code>
<a id="id244" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p>Preparing context for PostScript file rendering</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">SurfaceEps</code>
<a id="id245" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p>Preparing context for Illustrator EPS file rendering</p>
</td></tr></tbody></table></div><p>The exported graphics look as follows:</p><div class="mediaobject"><img src="graphics/8703OS_03_13.jpg" alt="How it works…"/></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec102"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Cairo</strong></span>: <a class="ulink" href="http://cairographics.org/">http://cairographics.org/</a><a id="id246" class="indexterm"/></li></ul></div></div></div></body></html>