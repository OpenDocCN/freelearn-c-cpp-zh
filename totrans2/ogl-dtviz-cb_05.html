<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Rendering of Point Cloud Data for 3D Range-sensing Cameras"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Rendering of Point Cloud Data for 3D Range-sensing Cameras</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Getting started with the Microsoft Kinect (PrimeSense) 3D range-sensing camera</li><li class="listitem" style="list-style-type: disc">Capturing raw data from depth-sensing cameras</li><li class="listitem" style="list-style-type: disc">OpenGL point cloud rendering with texture mapping and overlays</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec35"/>Introduction</h1></div></div></div><p>The purpose of this chapter is to introduce the techniques to visualize another interesting and emerging class of data: depth information from 3D range-sensing cameras. Devices with 3D depth sensors are hitting the market everyday, and companies such as Intel, Microsoft, SoftKinetic, PMD, Structure Sensor, and Meta (wearable Augmented Reality eyeglasses) are all using these novel 3D sensing devices to track user inputs, such as hand gestures for interaction and/or tracking a user's environment. An interesting integration of 3D sensors with OpenGL is the ability to look at a scene in 3D from different perspectives, thereby enabling a virtual 3D fly-through of a scene captured with the depth sensors. In our case, for data visualization, being able to walk through a massive 3D dataset could be particularly powerful in scientific computing, urban planning, and many other applications that involve the visualization of 3D structures of a scene.</p><p>In this chapter, we propose a simplified pipeline that takes any 3D point data (<span class="emphasis"><em>X</em></span>, <span class="emphasis"><em>Y</em></span>, <span class="emphasis"><em>Z</em></span>) with color (<span class="emphasis"><em>r</em></span>, <span class="emphasis"><em>g</em></span>, <span class="emphasis"><em>b</em></span>) and renders these point clouds on the screen in real time. The point clouds will be obtained directly from real-world data using a 3D range-sensing camera. We will also provide ways to fly around the point cloud and have dynamic ways to adjust the camera's parameters. This chapter will build on the OpenGL graphics rendering pipeline discussed in the previous chapter, and we will show you a few additional tricks to filter the data with GLSL. We will display our depth information using our heat map generator to see the depth in 2D and remap this data to a 3D point cloud using texture mapping and perspective projection. This will allow us to see the real-life depth-based rendering of a scene and navigate around the scene from any perspective.</p></div></div>
<div class="section" title="Getting started with the Microsoft Kinect (PrimeSense) 3D range-sensing camera"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec36"/>Getting started with the Microsoft Kinect (PrimeSense) 3D range-sensing camera</h1></div></div></div><p>The Microsoft Kinect 3D range-sensing camera<a id="id193" class="indexterm"/> based on the PrimeSense technology is an interesting piece of equipment that enables the estimation of the 3D geometry of a scene through depth-sensing using light patterns. The 3D sensor has an active infrared laser projector, which emits encoded speckle light patterns. The sensors allow users to capture color images and provide a 3D depth map at a resolution of 640 x 480. Since the Kinect sensor is an active sensor, it is invariant to indoor lighting condition (that is, it even works in the dark) and enables many applications, such as gesture and pose tracking as well as 3D scanning and reconstruction.</p><p>In this section, we will demonstrate how to set up this type of range-sensing camera, as an example. While we do not require readers to purchase a 3D range-sensing camera for this chapter (since we will provide the raw data captured on this device for the purpose of running our demos), we will demonstrate how one can set up the device to capture data directly, primarily for those who are interested in further experimenting with real-time 3D data.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec101"/>How to do it...</h2></div></div></div><p>Windows users can download the OpenNI 2 SDK and driver from <a class="ulink" href="http://structure.io/openni">http://structure.io/openni</a> (or using the direct download link: <a class="ulink" href="http://com.occipital.openni.s3.amazonaws.com/OpenNI-Windows-x64-2.2.0.33.zip">http://com.occipital.openni.s3.amazonaws.com/OpenNI-Windows-x64-2.2.0.33.zip</a>) and follow the<a id="id194" class="indexterm"/> on-screen instructions. Linux users can download the OpenNI 2 SDK from the same website at <a class="ulink" href="http://structure.io/openni">http://structure.io/openni</a>.</p><p>Mac users can install the OpenNI2 driver as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Install libraries with Macport:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo port install libtool</strong></span>
<span class="strong"><strong>sudo port install libusb +universal</strong></span>
</pre></div></li><li class="listitem">Download OpenNI2 from <a class="ulink" href="https://github.com/occipital/openni2">https://github.com/occipital/openni2</a>.</li><li class="listitem">Compile the source code with the following commands:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd OpenNI2-master</strong></span>
<span class="strong"><strong>make</strong></span>
<span class="strong"><strong>cd Bin/x64-Release/</strong></span>
</pre></div></li><li class="listitem">Run the <code class="literal">SimpleViewer</code> executable:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./SimpleViewer</strong></span>
</pre></div></li></ol></div><p>If you are using <a id="id195" class="indexterm"/>a computer with a USB 3.0 interface, it is important that you first upgrade the firmware for the PrimeSense sensor to version 1.0.9 (<a class="ulink" href="http://dasl.mem.drexel.edu/wiki/images/5/51/FWUpdate_RD109-112_5.9.2.zip">http://dasl.mem.drexel.edu/wiki/images/5/51/FWUpdate_RD109-112_5.9.2.zip</a>). This upgrade requires a Windows platform. Note that the Windows driver for the PrimeSense sensor must be installed (<a class="ulink" href="http://structure.io/openni">http://structure.io/openni</a>) for you to proceed. Execute the <code class="literal">FWUpdate_RD109-112_5.9.2.exe</code> file, and the firmware will be automatically upgraded. Further details on the firmware can be found at <a class="ulink" href="http://dasl.mem.drexel.edu/wiki/index.php/4._Updating_Firmware_for_Primesense">http://dasl.mem.drexel.edu/wiki/index.php/4._Updating_Firmware_for_Primesense</a>.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec102"/>See also</h2></div></div></div><p>Detailed technical specifications of the Microsoft Kinect 3D system can be obtained from <a class="ulink" href="http://msdn.microsoft.com/en-us/library/jj131033.aspx">http://msdn.microsoft.com/en-us/library/jj131033.aspx</a>, and further installation instructions and prerequisites to build OpenNI2 drivers can be found at <a class="ulink" href="https://github.com/occipital/openni2">https://github.com/occipital/openni2</a>.</p><p>In addition, Microsoft Kinect V2 is also available and is compatible with Windows. The new sensor provides higher resolution images and better depth fidelity. More information about the sensor, as well as the Microsoft Kinect SDK, can be found at <a class="ulink" href="https://www.microsoft.com/en-us/kinectforwindows">https://www.microsoft.com/en-us/kinectforwindows</a>.</p></div></div>
<div class="section" title="Capturing raw data from depth-sensing cameras"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec37"/>Capturing raw data from depth-sensing cameras</h1></div></div></div><p>Now that<a id="id196" class="indexterm"/> you have installed the prerequisite libraries and drivers, we will demonstrate how to capture raw data from your depth-sensing camera.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec103"/>How to do it...</h2></div></div></div><p>To <a id="id197" class="indexterm"/>capture sensor data directly in a binary format, implement the following function:</p><div class="informalexample"><pre class="programlisting">void writeDepthBuffer(openni::VideoFrameRef depthFrame){
  static int depth_buffer_counter=0;
  char file_name [512];
  sprintf(file_name, "%s%d.bin", "depth_frame", depth_buffer_counter);
  openni::DepthPixel *depthPixels = new openni::DepthPixel[depthFrame.getHeight()*depthFrame.getWidth()];
  memcpy(depthPixels, depthFrame.getData(), depthFrame.getHeight()*depthFrame.getWidth()*sizeof(uint16_t));
  std::fstream myFile (file_name, std::ios::out |std::ios::binary);
  myFile.write ((char*)depthPixels, depthFrame.getHeight()*depthFrame.getWidth()*sizeof(uint16_t));
  depth_buffer_counter++;
  printf("Dumped Depth Buffer %d\n",depth_buffer_counter);
  myFile.close();
  delete depthPixels;
}</pre></div><p>Similarly, we <a id="id198" class="indexterm"/>also capture the raw RGB<a id="id199" class="indexterm"/> color data with the following implementation:</p><div class="informalexample"><pre class="programlisting">  void writeColorBuffer(openni::VideoFrameRef colorFrame){
    static int color_buffer_counter=0;
    char file_name [512];
    sprintf(file_name, "%s%d.bin", "color_frame", color_buffer_counter);
    //basically unsigned char*
    const openni::RGB888Pixel* imageBuffer = (const openni::RGB888Pixel*)colorFrame.getData();
    std::fstream myFile (file_name, std::ios::out | std::ios::binary);
    myFile.write ((char*)imageBuffer, colorFrame.getHeight()*colorFrame.getWidth()*sizeof(uint8_t)*3);
    color_buffer_counter++;
    printf("Dumped Color Buffer %d, %d, %d\n", colorFrame.getHeight(), colorFrame.getWidth(), color_buffer_counter);
    myFile.close();
  }</pre></div><p>The preceding code snippet can be added to any sample code within the OpenNI2 SDK that provides depth and color data visualization (to enable raw data capture). We recommend that you modify the <code class="literal">Viewer.cpp</code> file in the <code class="literal">OpenNI2-master/Samples/SimpleViewer</code> folder. The modified sample code is included in our code package. To capture raw data, press <span class="emphasis"><em>R</em></span> and the data will be stored in the <code class="literal">depth_frame0.bin</code> and <code class="literal">color_frame0.bin</code> files.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec104"/>How it works...</h2></div></div></div><p>The depth sensor returns two streams of data in real time. One data stream is a 3D depth map, which is stored in 16-bits unsigned short data type (see the following figure on the left-hand side). Another data stream is a color image (see the following figure on the right-hand side), which is stored in a 24 bits per pixel, RGB888 format (that is, the memory is aligned in the R, G, and B order, and <span class="emphasis"><em>8 bits * 3 channels = 24 bits</em></span> are used per pixel).</p><div class="mediaobject"><img src="graphics/9727OS_05_01.jpg" alt="How it works..."/></div><p>The binary data <a id="id200" class="indexterm"/>is written directly to the<a id="id201" class="indexterm"/> hard disk without compression or modification to the data format. On the client side, we read the binary files as if there is a continuous stream of data and color data pairs arriving synchronously from the hardware device. The OpenNI2 driver provides the mechanism to interface with the PrimeSense-based sensors (Microsoft Kinect or PS1080).</p><p>The <code class="literal">openni::VideoFrameRef depthFrame</code> variable, for example, stores the reference to the depth data buffer. By calling the <code class="literal">depthFrame.getData()</code> function, we obtain a pointer to the buffer in the <code class="literal">DepthPixel</code> format, which is equivalent to the unsigned short data type. Then, we write the binary data to a file using the <code class="literal">write()</code> function in the <code class="literal">fstream</code> library. Similarly, we perform the same task with the color image, but the data is stored in the RGB888 format.</p><p>Additionally, we can enable the <code class="literal">setImageRegistrationMode</code> (<code class="literal">openni::IMAGE_REGISTRATION_DEPTH_TO_COLOR</code>) depth map registration function in OpenNI2 to automatically compute and map a depth value onto the color image. The depth map is overlaid onto the color image and is shown in the following figure:</p><div class="mediaobject"><img src="graphics/9727OS_05_02.jpg" alt="How it works..."/></div><p>In the next section, we <a id="id202" class="indexterm"/>will assume that<a id="id203" class="indexterm"/> the raw depth map is precalibrated with image registration by OpenNI2 and can be used to compute the real-world coordinates and UV mapping indices directly.</p></div></div>
<div class="section" title="OpenGL point cloud rendering with texture mapping and overlays"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec38"/>OpenGL point cloud rendering with texture mapping and overlays</h1></div></div></div><p>We will build <a id="id204" class="indexterm"/>on the OpenGL framework discussed in the previous chapter for point cloud rendering in this section. The texture mapping technique introduced in the previous chapter can also be applied in the point cloud format. Basically, the depth sensor provides a set of vertices in real-world space (the depth map), and the color camera provides us with the color information of the vertices. UV mapping is a simple lookup table once the depth map and color camera are calibrated.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec105"/>Getting ready</h2></div></div></div><p>Readers should use the raw data provided for the subsequent demo or obtain their own raw data from a 3D range-sensing camera. In either case, we assume these filenames will be used to denote the raw data files: <code class="literal">depth_frame0.bin</code> and <code class="literal">color_frame0.bin</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec106"/>How to do it...</h2></div></div></div><p>Similar to the previous chapter, we will divide the program into three major components: the main program (<code class="literal">main.cpp</code>), shader programs (<code class="literal">shader.cpp</code>, <code class="literal">shader.hpp</code>, <code class="literal">pointcloud.vert</code>, <code class="literal">pointcloud.frag</code>), and texture-mapping functions (<code class="literal">texture.cpp</code>, <code class="literal">texture.hpp</code>). The main program performs the essential tasks to set up the demo, while the shader programs perform the specialized processing. The texture-mapping functions provide a mechanism to load and map the color information onto the vertices. Finally, we modify the <code class="literal">control.cpp</code> file to provide more refined controls over the <span class="strong"><strong>fly-through</strong></span> experience<a id="id205" class="indexterm"/> through various additional keyboard inputs (using the up, down, left, and right arrow keys to zoom in and out in addition to adjusting the rotation angles using the <span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>s</em></span>, <span class="emphasis"><em>x</em></span>, and <span class="emphasis"><em>z</em></span> keys).</p><p>First, let's<a id="id206" class="indexterm"/> take a look at the shader programs. We will create two vertex and fragment shader programs inside the <code class="literal">pointcloud.vert</code> and <code class="literal">pointcloud.frag</code> files that are compiled and loaded by the program at runtime by using the <code class="literal">LoadShaders</code> function in the <code class="literal">shader.cpp</code> file.</p><p>For the <code class="literal">pointcloud.vert</code> file, we implement the following:</p><div class="informalexample"><pre class="programlisting">#version 150 core
// Input vertex data
in vec3 vertexPosition_modelspace;
in vec2 vertexUV;
// Output data: interpolated for each fragment.
out vec2 UV;
out vec4 color_based_on_position;
// Values that stay constant for the whole mesh
uniform mat4 MVP;
//heat map generator
vec4 heatMap(float v, float vmin, float vmax){
  float dv;
  float r=1.0f, g=1.0f, b=1.0f;
  if (v &lt; vmin)
    v = vmin;
  if (v &gt; vmax)
    v = vmax;
  dv = vmax - vmin;
  if (v &lt; (vmin + 0.25f * dv)) {
    r = 0.0f;
    g = 4.0f * (v - vmin) / dv;
  } else if (v &lt; (vmin + 0.5f * dv)) {
    r = 0.0f;
    b = 1.0f+4.0f*(vmin+0.25f*dv-v)/dv;
  } else if (v &lt; (vmin + 0.75f * dv)) {
    r = 4.0f*(v-vmin-0.5f*dv)/dv;
    b = 0.0f;
  } else {
    g = 1.0f+4.0f*(vmin+0.75f*dv-v)/dv;
    b = 0.0f;
  }
  return vec4(r, g, b, 1.0);
}
void main(){
  // Output position of the vertex, in clip space: MVP * position
  gl_Position =  MVP * vec4(vertexPosition_modelspace,1);
  color_based_on_position = heatMap(vertexPosition_modelspace.z, -3.0, 0.0f);
  UV = vertexUV;
}</pre></div><p>For the <code class="literal">pointcloud.frag</code> file, we<a id="id207" class="indexterm"/> implement the following:</p><div class="informalexample"><pre class="programlisting">#version 150 core
in vec2 UV;
out vec4 color;
uniform sampler2D textureSampler;
in vec4 color_based_on_position;
void main(){
  //blend the depth map color with RGB
  color = 0.5f*texture(textureSampler, UV).rgba+0.5f*color_based_on_position;
}</pre></div><p>Finally, let's put everything together with the <code class="literal">main.cpp</code> file:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Include prerequisite libraries and the shader program header files inside the common folder:<div class="informalexample"><pre class="programlisting">#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
//GLFW and GLEW libraries
#include &lt;GL/glew.h&gt;
#include &lt;GLFW/glfw3.h&gt;
//GLM library
#include &lt;glm/glm.hpp&gt;
#include &lt;glm/gtc/matrix_transform.hpp&gt;
#include "../common/shader.hpp"
#include "../common/texture.hpp"
#include "../common/controls.hpp"
#include "../common/common.h"
#include &lt;fstream&gt;</pre></div></li><li class="listitem">Create a global variable for the GLFW window:<div class="informalexample"><pre class="programlisting">GLFWwindow* window;</pre></div></li><li class="listitem">Define the width and height of the input depth dataset as well as other window/camera properties for rendering:<div class="informalexample"><pre class="programlisting">const int WINDOWS_WIDTH = 640;
const int WINDOWS_HEIGHT = 480;
const int IMAGE_WIDTH = 320;
const int IMAGE_HEIGHT = 240;
float z_offset = 0.0f;
float rotateY = 0.0f;
float rotateX = 0.0f;</pre></div></li><li class="listitem">Define<a id="id208" class="indexterm"/> helper functions to parse the raw depth and color data:<div class="informalexample"><pre class="programlisting">unsigned short *readDepthFrame(const char *file_path){
  int depth_buffer_size = DEPTH_WIDTH*DEPTH_HEIGHT*sizeof(unsigned short);
  unsigned short *depth_frame = (unsigned short*)malloc(depth_buffer_size);
  char *depth_frame_pointer = (char*)depth_frame;
  //read the binary file
  ifstream myfile;
  myfile.open (file_path, ios::binary | ios::in);
  myfile.read(depth_frame_pointer, depth_buffer_size);
  return depth_frame;
}
unsigned char *readColorFrame(const char *file_path){
  int color_buffer_size = DEPTH_WIDTH*DEPTH_HEIGHT*sizeof(unsigned char)*3;
  unsigned char *color_frame = (unsigned char*)malloc(color_buffer_size);
  //read the binary file
  ifstream myfile;
  myfile.open (file_path, ios::binary | ios::in);
  myfile.read((char *)color_frame, color_buffer_size);
  return color_frame;
}</pre></div></li><li class="listitem">Create callback functions to handle key strokes:<div class="informalexample"><pre class="programlisting">static void key_callback(GLFWwindow* window, int key, int scancode, int action, int mods)
{
  if (action != GLFW_PRESS &amp;&amp; action != GLFW_REPEAT)
    return;
  switch (key)
  {
    case GLFW_KEY_ESCAPE:
      glfwSetWindowShouldClose(window, GL_TRUE);
      break;
    case GLFW_KEY_SPACE:
      rotateX=0;
      rotateY=0;
      break;
    case GLFW_KEY_Z:
      rotateX+=0.01;
      break;
    case GLFW_KEY_X:
      rotateX-=0.01;
      break;
    case GLFW_KEY_A:
      rotateY+=0.01;
      break;
    case GLFW_KEY_S:
      rotateY-=0.01;
      break;
    default:
      break;
  }
}</pre></div></li><li class="listitem">Start the main program with the initialization of the GLFW library:<div class="informalexample"><pre class="programlisting">int main(int argc, char **argv)
{
  if(!glfwInit()){
    fprintf( stderr, "Failed to initialize GLFW\n" );
    exit(EXIT_FAILURE);
  }</pre></div></li><li class="listitem">Set<a id="id209" class="indexterm"/> up the GLFW window:<div class="informalexample"><pre class="programlisting">  glfwWindowHint(GLFW_SAMPLES, 4);
  glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3);
  glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 2);
  glfwWindowHint(GLFW_OPENGL_FORWARD_COMPAT, GL_TRUE);
  glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE);</pre></div></li><li class="listitem">Create the GLFW window object and make it current for the calling thread:<div class="informalexample"><pre class="programlisting">  g_window = glfwCreateWindow(WINDOWS_WIDTH, WINDOWS_HEIGHT, "Chapter 5 - 3D Point Cloud Rendering", NULL, NULL);
  if(!g_window){
    fprintf( stderr, "Failed to open GLFW window. If you have an Intel GPU, they are not 3.3 compatible. Try the 2.1 version of the tutorials.\n" );
    glfwTerminate();
    exit(EXIT_FAILURE);
  }
  glfwMakeContextCurrent(g_window);
  glfwSwapInterval(1);</pre></div></li><li class="listitem">Initialize <a id="id210" class="indexterm"/>the GLEW library and include support for experimental drivers:<div class="informalexample"><pre class="programlisting">  glewExperimental = true;
  if (glewInit() != GLEW_OK) {
    fprintf(stderr, "Final to Initialize GLEW\n");
    glfwTerminate();
    exit(EXIT_FAILURE);
  }</pre></div></li><li class="listitem">Set up keyboard callback:<div class="informalexample"><pre class="programlisting">  glfwSetInputMode(g_window,GLFW_STICKY_KEYS,GL_TRUE);
  glfwSetKeyCallback(g_window, key_callback);</pre></div></li><li class="listitem">Set up the shader programs:<div class="informalexample"><pre class="programlisting">  GLuint program_id = LoadShaders("pointcloud.vert", "pointcloud.frag");</pre></div></li><li class="listitem">Create the vertex (<span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>y</em></span>, <span class="emphasis"><em>z</em></span>) for all depth pixels:<div class="informalexample"><pre class="programlisting">  GLfloat *g_vertex_buffer_data = (GLfloat*)malloc(IMAGE_WIDTH*IMAGE_HEIGHT * 3*sizeof(GLfloat));
  GLfloat *g_uv_buffer_data = (GLfloat*)malloc(IMAGE_WIDTH*IMAGE_HEIGHT * 2*sizeof(GLfloat));</pre></div></li><li class="listitem">Read the raw data using the helper functions defined previously:<div class="informalexample"><pre class="programlisting">  unsigned short *depth_frame = readDepthFrame("depth_frame0.bin");
  unsigned char *color_frame = readColorFrame("color_frame0.bin");</pre></div></li><li class="listitem">Load the color information into a texture object:<div class="informalexample"><pre class="programlisting">  GLuint texture_id = loadRGBImageToTexture(color_frame, IMAGE_WIDTH, IMAGE_HEIGHT);</pre></div></li><li class="listitem">Create a set of vertices in a real-world space based on the depth map and also define the UV mapping for the color mapping:<div class="informalexample"><pre class="programlisting">  //divided by two due to 320x240 instead of 640x480 resolution
  float cx = 320.0f/2.0f;
  float cy = 240.0f/2.0f;
  float fx = 574.0f/2.0f;
  float fy = 574.0f/2.0f;
  for(int y=0; y&lt;IMAGE_HEIGHT; y++){
    for(int x=0; x&lt;IMAGE_WIDTH; x++){
      int index = y*IMAGE_WIDTH+x;
      float depth_value = (float)depth_frame[index]/1000.0f; //in meter
      int ver_index = index*3;
      int uv_index = index*2;
      if(depth_value != 0){
        g_vertex_buffer_data[ver_index+0] = ((float)x- cx)*depth_value/fx;
        g_vertex_buffer_data[ver_index+1] = ((float)y- cy)*depth_value/fy;
        g_vertex_buffer_data[ver_index+2] = -depth_value;
        g_uv_buffer_data[uv_index+0] = (float)x/IMAGE_WIDTH;
        g_uv_buffer_data[uv_index+1] = (float)y/IMAGE_HEIGHT;
      }
    }
  }
  //Enable depth test to ensure occlusion:
  //uncommented glEnable(GL_DEPTH_TEST);</pre></div></li><li class="listitem">Get the <a id="id211" class="indexterm"/>location for various uniform and attribute variables:<div class="informalexample"><pre class="programlisting">  GLuint matrix_id = glGetUniformLocation(program_id, "MVP");
  GLuint texture_sampler_id = glGetUniformLocation(program_id, "textureSampler");
  GLint attribute_vertex, attribute_uv;
  attribute_vertex = glGetAttribLocation(program_id, "vertexPosition_modelspace");
  attribute_uv = glGetAttribLocation(program_id, "vertexUV");</pre></div></li><li class="listitem">Generate the vertex array object:<div class="informalexample"><pre class="programlisting">  GLuint vertex_array_id;
  glGenVertexArrays(1, &amp;vertex_array_id);
  glBindVertexArray(vertex_array_id);</pre></div></li><li class="listitem">Initialize the vertex buffer memory:<div class="informalexample"><pre class="programlisting">  GLuint vertex_buffer;
  glGenBuffers(1, &amp;vertex_buffer);
  glBindBuffer(GL_ARRAY_BUFFER, vertex_buffer);
  glBufferData(GL_ARRAY_BUFFER, 
IMAGE_WIDTH*IMAGE_HEIGHT*2* sizeof(GLfloat), 
g_uv_buffer_data, GL_STATIC_DRAW);</pre></div></li><li class="listitem">Create and bind the UV buffer memory:<div class="informalexample"><pre class="programlisting">  GLuint uv_buffer;
  glGenBuffers(1, &amp;uv_buffer);
  glBindBuffer(GL_ARRAY_BUFFER, uv_buffer);
  glBufferData(GL_ARRAY_BUFFER, 
IMAGE_WIDTH*IMAGE_HEIGHT*3* sizeof(GLfloat), 
g_vertex_buffer_data, GL_STATIC_DRAW);</pre></div></li><li class="listitem">Use our shader program:<div class="informalexample"><pre class="programlisting">  glUseProgram(program_id);</pre></div></li><li class="listitem">Bind<a id="id212" class="indexterm"/> the texture in Texture Unit 0:<div class="informalexample"><pre class="programlisting">  glActiveTexture(GL_TEXTURE0);
  glBindTexture(GL_TEXTURE_2D, texture_id);
  glUniform1i(texture_sampler_id, 0);</pre></div></li><li class="listitem">Set up attribute buffers for vertices and UV mapping:<div class="informalexample"><pre class="programlisting">  glEnableVertexAttribArray(attribute_vertex);
  glBindBuffer(GL_ARRAY_BUFFER, vertex_buffer);
  glVertexAttribPointer(attribute_vertex, 3, GL_FLOAT, GL_FALSE, 0, (void*)0);
  glEnableVertexAttribArray(attribute_uv);
  glBindBuffer(GL_ARRAY_BUFFER, uv_buffer);
  glVertexAttribPointer(attribute_uv, 2, GL_FLOAT, GL_FALSE, 0, (void*)0);</pre></div></li><li class="listitem">Run the draw functions and loop:<div class="informalexample"><pre class="programlisting">  do{
    //clear the screen
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
    glClearColor(1.0f, 1.0f, 1.0f, 0.0f);
    //compute the MVP matrix from keyboard and mouse input
    computeViewProjectionMatrices(g_window);
    //get the View and Model Matrix and apply to the rendering
    glm::mat4 projection_matrix = getProjectionMatrix();
    glm::mat4 view_matrix = getViewMatrix();
    glm::mat4 model_matrix = glm::mat4(1.0);
    model_matrix = glm::rotate(model_matrix, glm::pi&lt;float&gt;() * rotateY, glm::vec3(0.0f, 1.0f, 0.0f));
      model_matrix = glm::rotate(model_matrix, glm::pi&lt;float&gt;() * rotateX, glm::vec3(1.0f, 0.0f, 0.0f));
    glm::mat4 mvp = projection_matrix * view_matrix * model_matrix;
    //send our transformation to the currently bound
    //shader in the "MVP" uniform variable
    glUniformMatrix4fv(matrix_id, 1, GL_FALSE, &amp;mvp[0][0]);
    glPointSize(2.0f);
    //draw all points in space
    glDrawArrays(GL_POINTS, 0, IMAGE_WIDTH*IMAGE_HEIGHT);
    //swap buffers
    glfwSwapBuffers(g_window);
    glfwPollEvents();
  }
  // Check if the ESC key was pressed or the window was closed
  while(!glfwWindowShouldClose(g_window) &amp;&amp; glfwGetKey(g_window, GLFW_KEY_ESCAPE )!=GLFW_PRESS);</pre></div></li><li class="listitem">Clean<a id="id213" class="indexterm"/> up and exit the program:<div class="informalexample"><pre class="programlisting">  glDisableVertexAttribArray(attribute_vertex);
  glDisableVertexAttribArray(attribute_uv);
  glDeleteBuffers(1, &amp;vertex_buffer);
  glDeleteBuffers(1, &amp;uv_buffer);
  glDeleteProgram(program_id);
  glDeleteTextures(1, &amp;texture_id);
  glDeleteVertexArrays(1, &amp;vertex_array_id);
  glfwDestroyWindow(g_window);
  glfwTerminate();
  exit(EXIT_SUCCESS);
}</pre></div></li><li class="listitem">In <code class="literal">texture.cpp</code>, we implement the additional image-loading functions based on the previous chapter:<div class="informalexample"><pre class="programlisting">/* Handle loading images to texture memory and setting up the parameters */
GLuint loadRGBImageToTexture(const unsigned char * image_buffer, int width, int height){
  int channels;
  GLuint textureID=0;
  textureID=initializeTexture(image_buffer, width, height, GL_RGB);
  return textureID;
}
GLuint initializeTexture(const unsigned char *image_data, int width, int height, GLenum input_format){
  GLuint textureID=0;
  //for the first time we create the image,
  //create one texture element
  glGenTextures(1, &amp;textureID);
  //bind the one element
  glBindTexture(GL_TEXTURE_2D, textureID);
  glPixelStorei(GL_UNPACK_ALIGNMENT,1);
  /* Specify the target texture. Parameters describe the format and type of image data */
  glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, width, height, 0, input_format, GL_UNSIGNED_BYTE, image_data);
  /* Set the magnification method to linear, which returns an weighted average of 4 texture elements */
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP);
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP);
  /* Set the magnification method to linear, which //returns an weighted average of 4 texture elements */
  //closest to the center of the pixel 
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
  /* Choose the mipmap that most closely matches the size of the pixel being textured and use the GL_NEAREST criterion (texture element nearest to the center of the pixel) to produce texture value. */
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR_MIPMAP_LINEAR);
  glGenerateMipmap(GL_TEXTURE_2D);
  return textureID;
}</pre></div></li><li class="listitem">In <code class="literal">texture.hpp</code>, we<a id="id214" class="indexterm"/> simply define the function prototypes:<div class="informalexample"><pre class="programlisting">GLuint loadRGBImageToTexture(const unsigned char *image_data, int width, int height);
GLuint initializeTexture(const unsigned char *image_data, int width, int height, GLenum input_format = GL_RGBA);</pre></div></li><li class="listitem">In <code class="literal">control.cpp</code>, we modify the <code class="literal">computeViewProjectionMatrices</code> function with the following code to support additional translation controls:<div class="informalexample"><pre class="programlisting">//initial position of the camera
glm::vec3 g_position = glm::vec3( 0, 0, 3.0 );
const float speed = 3.0f; // 3 units / second
float g_initial_fov = glm::pi&lt;float&gt;()*0.25f;
//compute the view matrix and projection matrix based on 
//user input
void computeViewProjectionMatrices(GLFWwindow* window){
  static double last_time = glfwGetTime();
  // Compute time difference between current and last frame
  double current_time = glfwGetTime();
  float delta_time = float(current_time - last_time);
  int width, height;
  glfwGetWindowSize(window, &amp;width, &amp;height);
  //direction vector for movement
  glm::vec3 direction_z(0, 0, -0.5);
  glm::vec3 direction_y(0, 0.5, 0);
  glm::vec3 direction_x(0.5, 0, 0);
  //up vector
  glm::vec3 up = glm::vec3(0,-1,0);
  if (glfwGetKey( window, GLFW_KEY_UP ) == GLFW_PRESS){
    g_position += direction_y * delta_time * speed;
  }
  else if (glfwGetKey( window, GLFW_KEY_DOWN ) == GLFW_PRESS){
    g_position -= direction_y * delta_time * speed;
  }
  else if (glfwGetKey( window, GLFW_KEY_RIGHT ) == GLFW_PRESS){
    g_position += direction_z * delta_time * speed;
  }
  else if (glfwGetKey( window, GLFW_KEY_LEFT ) == GLFW_PRESS){
    g_position -= direction_z * delta_time * speed;
  }
  else if (glfwGetKey( window, GLFW_KEY_PERIOD ) == GLFW_PRESS){
    g_position -= direction_x * delta_time * speed;
  }
  else if (glfwGetKey( window, GLFW_KEY_COMMA ) == GLFW_PRESS){
    g_position += direction_x * delta_time * speed;
  }
  /* update projection matrix: Field of View, aspect ratio, display range : 0.1 unit &lt;-&gt; 100 units */
  g_projection_matrix = glm::perspective(g_initial_fov, (float)width/(float)height, 0.01f, 100.0f);
  
  // update the view matrix
  g_view_matrix = glm::lookAt(
    g_position,           // camera position
    g_position+direction_z, //viewing direction
    up                  // up direction
  );
  last_time = current_time;
}</pre></div></li></ol></div><p>Now we have<a id="id215" class="indexterm"/> created a way to visualize the depth sensor information in a 3D fly-through style; the following figure shows the rendering of the point cloud with a virtual camera at the central position of the frame:</p><div class="mediaobject"><img src="graphics/9727OS_05_03.jpg" alt="How to do it..."/></div><p>By rotating and<a id="id216" class="indexterm"/> translating the virtual camera, we can create various representations of the scene from different perspectives. With a bird's eye view or side view of the scene, we can see the contour of the face and hand more apparently from these two angles, respectively:</p><div class="mediaobject"><img src="graphics/9727OS_05_04.jpg" alt="How to do it..."/></div><p>This is <a id="id217" class="indexterm"/>the side view of the same scene:</p><div class="mediaobject"><img src="graphics/9727OS_05_05.jpg" alt="How to do it..."/></div><p>By adding an <a id="id218" class="indexterm"/>additional condition to the remapping loop, we can render the unknown regions (holes) from the scene where the depth camera fails to reconstruct due to occlusion, field of view limitation, range limitation, and/or surface properties such as reflectance:</p><div class="informalexample"><pre class="programlisting">if(depth_value != 0){
  g_vertex_buffer_data[ver_index+0] = ((float)x-cx)*depth_value/fx;
  g_vertex_buffer_data[ver_index+1] = ((float)y-cy)*depth_value/fy;
  g_vertex_buffer_data[ver_index+2] = -depth_value;
  g_uv_buffer_data[uv_index+0] = (float)x/IMAGE_WIDTH;
  g_uv_buffer_data[uv_index+1] = (float)y/IMAGE_HEIGHT;
}
else{
  g_vertex_buffer_data[ver_index+0] = ((float)x-cx)*0.2f/fx;
  g_vertex_buffer_data[ver_index+1] = ((float)y-cy)*0.2f/fy;
  g_vertex_buffer_data[ver_index+2] = 0;
}</pre></div><p>This condition allows us to segment the region and project the regions with depth values of 0 onto a plane that is 0.2 meters away from the virtual camera, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/9727OS_05_06.jpg" alt="How to do it..."/></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec107"/>How it works...</h2></div></div></div><p>In this chapter, we<a id="id219" class="indexterm"/> exploited the GLSL pipeline and texture-mapping technique to create an interactive point cloud visualization tool that enables the 3D navigation of a scene captured with a 3D range-sensing camera. The shader program also combines the result with the color image to produce our desired effect. The program reads two binary images: the calibrated depth map image and the RGB color image. The color is loaded into a texture object directly using the new <code class="literal">loadRGBImageToTexture()</code> function, which converts the data from <code class="literal">GL_RGB</code> to <code class="literal">GL_RGBA</code>. Then, the depth map data is converted into point cloud data in real-world coordinates based on the intrinsic value of the cameras as well as the depth value at each pixel, as follows:</p><div class="mediaobject"><img src="graphics/9727OS_05_07.jpg" alt="How it works..."/></div><p>Here, <span class="emphasis"><em>d</em></span> is the depth value in millimeter, <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> are the positions of the depth value in pixel (projective) space, <span class="inlinemediaobject"><img src="graphics/9727OS_05_11.jpg" alt="How it works..."/></span> and <span class="inlinemediaobject"><img src="graphics/9727OS_05_12.jpg" alt="How it works..."/></span> are the principle axes of the depth camera, <span class="inlinemediaobject"><img src="graphics/9727OS_05_13.jpg" alt="How it works..."/></span> and <span class="inlinemediaobject"><img src="graphics/9727OS_05_14.jpg" alt="How it works..."/></span> are the focal lengths of the camera, and <span class="inlinemediaobject"><img src="graphics/9727OS_05_15.jpg" alt="How it works..."/></span> is the position of the point cloud in the real-world coordinate.</p><p>In our example, we <a id="id220" class="indexterm"/>do not require fine alignment or registration as our visualizer uses a primitive estimation of the intrinsic parameters:</p><div class="mediaobject"><img src="graphics/9727OS_05_16.jpg" alt="How it works..."/></div><p>These numbers could be estimated with the camera calibration tools in OpenCV. The details of these tools are beyond the scope of this chapter.</p><p>For our application, we are provided a set of 3D points (<span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>y</em></span>, <span class="emphasis"><em>z</em></span>) as well as the corresponding color information (<span class="emphasis"><em>r</em></span>, <span class="emphasis"><em>g</em></span>, <span class="emphasis"><em>b</em></span>) to compute the point cloud representation. However, the point visualization does not support dynamic lighting and other more advanced rendering techniques. To address this, we can extend the point cloud further into a mesh (that is, a set of triangles to represent surfaces), which will be discussed in the next chapter.</p></div></div></body></html>