<html><head></head><body><div><div><div><div><div><div><h1 class="title"><a id="ch10"/>Chapter 10. A Chapter You Shouldnt Skip - Final Optimizations</h1></div></div></div><p>What's the most important aspect of any game? According to a very famous e-celebrity, it's being able to play it. Fancy graphics and advanced techniques definitely add a necessary touch of polish to a medium as visual and interactive as video games, but if that gets in the way of enjoying the most fundamental experience of smooth gameplay, the whole thing might as well just be a fancy screensaver. Optimizing code, even when the application runs fine on higher-end machines, is extremely important, since every iteration excludes potential machines that are older but could still be used to expand the fan base of a game.</p><p>In this chapter, we will be covering the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The basics of profiling and reading code metrics</li><li class="listitem" style="list-style-type: disc">Analyzing and repairing inefficiencies in our code</li><li class="listitem" style="list-style-type: disc">The basics of light culling</li></ul></div><p>Let's not waste any more clock cycles and get to cleaning up some of those inefficiencies!</p><div><div><div><div><h1 class="title"><a id="ch10lvl1sec89"/>Use of third-party software</h1></div></div></div><p>As expected, we can't do all of this work with no additional tools. Profiling applications is a subject that requires a backend of established software, used to neatly organize and present us with the data of performance subtleties. <em>CodeXL</em> is an application we have already covered in <a class="link" href="ch09.html" title="Chapter 9.  The Speed of Dark - Lighting and Shadows">Chapter 9
</a>, <em>The Speed of Dark - Lighting and Shadows</em> and although we used it to view runtime OpenGL states, it also has quite a suite of options used to profile both CPU and GPU code. It can be found and downloaded here: <a class="ulink" href="http://gpuopen.com/compute-product/codexl/">http://gpuopen.com/compute-product/codexl/</a>.</p><p>Of course, if we don't have AMD hardware, only a very limited set of tools for profiling are available. Although we can get by with the limited CPU profiling options, GPU profiling on an Nvidia card, for example, would require a different tool. There are some choices out there, but one notable option is <em>Nvidia Nsight</em>: <a class="ulink" href="http://www.nvidia.com/object/nsight.html">http://www.nvidia.com/object/nsight.html</a>.</p><p>It's worth mentioning, however, that the newest versions of Nsight don't support some of the legacy functions SFML invokes, so the functionalities are, once again, rather limited.</p></div></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch10lvl1sec90"/>The devil's in the details</h1></div></div></div><p>They say that a master craftsman knows not only how, but also when to use their tools. Many programmers often enough arrive at the false conclusion that they must constantly write beautiful, efficient, and overall perfect code that will never fail. In practice, this couldn't be farther from the truth. Many find this out the hard way. As <em>Donald Knuth</em> said:</p><div><blockquote class="blockquote"><p><em>"Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil."</em></p></blockquote></div><p>This doesn't mean that one shouldn't take performance into consideration. Things such as designing a class with later features in mind, or even picking the right algorithm for the job both fall under that remaining 3%. Rather, it simply means that, unless the application is noticeably slow, tackling performance issues in code should be one of the final tasks at all times.</p><p>Another common mistake programmers often make is relying on intuition when it comes to evaluating performance. It's easy to forget that a program has tons of underlying complexity and moving parts, which is why it's incredibly hard to always know exactly how a specific chunk of code is going to behave unless properly tested. That's the key here always profile! Is the game running slow? Break out the profiler and take it for a spin. Feeling like enemy path-finding code is really weighing down on performance? Don't feel, just profile! The same thing can be said about the state of your code after optimizations have been made. Don't just replace a ton of code and assume it runs faster. Take a base measurement, make the appropriate changes, and profile the final result to make sure the new code runs faster. Starting to see the picture? Good. With that out of the way, let's jump straight into the basics of profiling!</p></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch10lvl1sec91"/>Profiling basics</h1></div></div></div><p>There's a variety of different ways an application can be profiled. Anything from branching and individual instructions, to usage of caches and patterns of data access can be tracked in a project. Since our game isn't exactly overflowing with complexity, however, we really only need to worry about time-based profiling.</p><p>There are three basic ways a profiler can gather information about an application:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Sampling</strong>: This is a periodic application stack capture that yields relatively inaccurate results, yet has very little overhead.</li><li class="listitem" style="list-style-type: disc"><strong>Event collection</strong>: This involves tapping into the compilation process and configuring it in such a way that allows certain information to be sent to the profiling DLLs. A higher amount of overhead with a higher precision.</li><li class="listitem" style="list-style-type: disc"><strong>Instrumentation</strong>: This involves direct code injection into the application during run time that allows for the most precise results and has the highest level of overhead.</li></ul></div><p>Any of these techniques can be utilized, depending on what software is being used and the data it needs to collect. Because we don't really need incredibly precise results to locate the hotspots of our code, it's best to go with a sampling approach.</p><div><div><h3 class="title"><a id="tip48"/>Tip</h3><p>As we have already established, profiling is not a free task. In some instances, it can slow down an application to a crawl, which, depending on the task, can be absolutely normal.</p></div></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec108"/>Time-based sampling</h2></div></div></div><p>Using the time-based sampling technique will create a rough estimate of all of the application's function/method calls, initializations/destructions, virtually anything that can be created or invoked, and assign a sample value to them. This even includes underlying libraries, such as STL, SFML, OpenGL, and so on. If your code uses it, it will be on the list.</p><p>This sample value represents how much time is spent executing a certain line of code. There are two types of time samples:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Inclusive</strong>: This involves all of the time spent inside a specific line/chunk of code, including all of the time it took to execute other functions that may have been called.</li><li class="listitem" style="list-style-type: disc"><strong>Exclusive</strong>: This involves only the amount of time that a specific line/chunk of code took to execute on its own.</li></ul></div><p>We are not necessarily going to be dealing with exclusive sample counts, but it's important to understand these terms nonetheless.</p><p>Lastly, it's important to understand that samples are relative. If a program is running slowly, fewer samples will be captured across the board. This particular benchmark shouldn't be interpreted based on quantity, but rather in comparison to the rest of the code.</p><div><div><h3 class="title"><a id="note49"/>Note</h3><p>Sampling should always be done with all of the relevant project's optimizations enabled, because it strips away unnecessary code that's used for debugging, which would interfere with the results. In the case of Visual Studio, the release mode should be used when sampling.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec109"/>Sampling our application</h2></div></div></div><p>Now that we have the fundamentals covered, let's actually fire up a profile and get started! The first important aspect of this process is actually spending enough time sampling the state of the desired application. In our case, sampling should be done in the <code class="literal">Game</code> state, and for at least 20 seconds, to capture enough information. It isn't going to help us understand the time complexities of the entity component system, for example, if the majority of the application sampling time is spent inside the menu state.</p><p>Secondly, we should probably test our application in a stressful state, just to make sure it holds up well under nonideal conditions. For our purposes, we will simply add a bunch more entities, particle emitters, and lights to the scene until a <strong>stress test</strong> that looks like the following is constructed:</p><div><img src="img/image_10_001.jpg" alt="Sampling our application"/></div><p>It isn't pretty, but then again, neither are performance issues.</p><p>Once the application has been sampled enough and is terminated, most profilers will show a <em>profile overview</em> of all the processes that have been running during the sampling process. It's important to select only our game by clicking on it, because that's what we're interested in.</p><p>After navigating to the <strong>Function</strong> tab, we should be left with something similar to this:</p><div><img src="img/image_10_002.jpg" alt="Sampling our application"/></div><p>By clicking on the <strong>Timer</strong> tab and sorting entries in a descending order, we can view the functions that have the highest amount of samples, thus taking the most time to run. This is where you will find the trade-off between using a general purpose library such as <em>SFML</em> and sacrificing some performance. While it's true that writing case-specific code would probably be more optimal in terms of performance, it's still a worthy price to pay when considering how versatile SFML is for small to medium-sized projects.</p><p>While it's obvious our own SFML rendering code could probably use some work and utilize vertex arrays when rendering sprites and tiles to reduce this bottleneck, we're not going to concern ourselves with SFML-specific optimizations this time. Instead, let's analyze the highlighted entries on the list first. As the <code class="literal">glm::</code> namespace seems to suggest, the OpenGL math library we're using for various calculations is the culprit. Upon closer inspection, it seems that all three of these hotspots have to do with matrix operations; the most expensive of which is the <code class="literal">glm::operator*</code>. By right-clicking on the entry, we can view it in a call graph.</p><div><div><h3 class="title"><a id="note50"/>Note</h3><p>A <strong>call graph</strong> is a tool that helps locate all points in our code that use a specific function.</p></div></div><p>By simply analyzing the information onscreen, we're now able to see what code uses the GLM matrices in such a way that causes performance problems:</p><div><img src="img/image_10_003.jpg" alt="Sampling our application"/></div><p>As the <strong>Parents</strong> section indicates, the majority of time samples regarding this particular bottleneck are located inside the <code class="literal">GL_Transform::GetModelMatrix()</code> method. Double-clicking on the function allows us to also view the code and specific hotspots of each individual line:</p><div><img src="img/image_10_004.jpg" alt="Sampling our application"/></div><p>This should all start to add up by now. The two most sampled lines in relation to matrices were <code class="literal">glm::tmat4x4&lt;float,0&gt;::tmat4x4&lt;float,0&gt;(float const&amp;)</code>, which is the matrix constructor, and <code class="literal">glm::rotate</code>, which we're calling three times. Every time we want to obtain a model matrix from this class (which is once for each shadow caster every frame), a bunch of new matrices are constructed and filled out using quite expensive GLM function calls, not to mention being multiplied later as well.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec110"/>Finding GPU bottlenecks</h2></div></div></div><p>Finding GPU bottlenecks is fairly similar to what we did with the CPU. It also utilizes time sampling and will generate similar looking reports that list OpenGL code based on how long it took to execute, as follows:</p><div><img src="img/image_10_005.jpg" alt="Finding GPU bottlenecks"/></div><p>We're not going to be covering GPU optimizations heavily here, but the idea is exactly the same: finding bottlenecks, re-implementing code in a more efficient manner, and testing again.</p><div><div><h3 class="title"><a id="note51"/>Note</h3><p>Some GPU profiling tools, such as Nvidia Nsight, don't support legacy OpenGL API calls made by SFML.</p></div></div></div></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch10lvl1sec92"/>Improving CPU code performance</h1></div></div></div><p>After establishing a baseline reading, we can begin making changes to our code. Some of these changes involve simply understanding the libraries we're using and being more cautious about the way they're deployed, while others revolve around making better design choices, applying faster and more appropriate algorithms, designing data structures better, and using the newest features of the C++ standard. Let's begin by taking a look at some easy changes that can be made to our code.</p><div><div><div><div><h2 class="title"><a id="ch10lvl2sec111"/>Optimizing the three most obvious bottlenecks</h2></div></div></div><p>Judging by the profiler's results, there is quite a bit of room for improvement of the code we've written so far. In this section, we're going to be addressing three of the most inefficient implementations and how they can be fixed.</p><div><div><div><div><h3 class="title"><a id="ch10lvl3sec25"/>GL_Transform optimizations</h3></div></div></div><p>The very first example we used to illustrate how time sampling works is a perfect candidate for improvement. There really is nothing subtle about it. First, the recalculation of all matrices involved in obtaining a model matrix every time one is requested is incredibly inefficient. To add insult to injury, all <em>7</em> of those matrices has to be created all over again. That's a lot of clock cycles wasted for no reason. Let's see how that can be quickly improved:</p><pre class="programlisting">class GL_Transform { 
public: 
  ... 
  const glm::mat4&amp; GetModelMatrix(); 
private: 
  ... 
  bool m_needsUpdate; 
  glm::mat4 m_matPos; 
  glm::mat4 m_matScale; 
  glm::mat4 m_matRotX; 
  glm::mat4 m_matRotY; 
  glm::mat4 m_matRotZ; 
  glm::mat4 m_matRotCombined; 
  glm::mat4 m_modelMatrix; // Final matrix. 
}; 
</pre><p>Firstly, notice the change of the return parameter of <code class="literal">GetModelMatrix</code> to a <em>const</em>
<em>reference</em>. This ensures that we're not returning a newly constructed matrix each time. Additionally, we have added a Boolean flag that will help us keep track of whether the position, scale, or rotation of the object has changed and whether the model matrix needs to be updated to reflect that. Lastly, we're storing all 7 matrices inside the transform object now, so that they are created only once. This is important, because we don't want to recalculate three rotational matrices along with its combined matrix, for example, just because the position of the object was changed.</p><p>Next, let's actually implement these changes, starting with the setters of this class:</p><pre class="programlisting">void GL_Transform::SetPosition(const glm::vec3&amp; l_pos) { 
  if (l_pos == m_position) { return; } 
  m_position = l_pos; 
  m_matPos = glm::translate(m_position); 
  m_needsUpdate = true; 
} 
</pre><p>The general idea here is to first check whether the argument provided to the setter method isn't already the current value of whatever parameter it's supposed to override. If it isn't, the position is changed and the position matrix is updated, along with the <code class="literal">m_needsUpdate</code> flag being set to <code class="literal">true</code>. This will ensure that the model matrix is updated later on.</p><p>Rotation follows the exact same principle:</p><pre class="programlisting">void GL_Transform::SetRotation(const glm::vec3&amp; l_rot) { 
  if (l_rot == m_rotation) { return; } 
  if (l_rot.x != m_rotation.x) { 
    m_matRotX = glm::rotate(m_rotation.x, glm::vec3(1, 0, 0)); 
  } 
  if (l_rot.y != m_rotation.y) { 
    m_matRotY = glm::rotate(m_rotation.y, glm::vec3(0, 1, 0)); 
  } 
  if (l_rot.z != m_rotation.z) { 
    m_matRotZ = glm::rotate(m_rotation.z, glm::vec3(0, 0, 1)); 
  } 
  m_matRotCombined = m_matRotZ * m_matRotY * m_matRotX; 
  m_rotation = l_rot; 
  m_needsUpdate = true; 
} 
</pre><p>Before the assignment is committed to, however, we must check each individual member of the vector class, because each one of them has their own matrix. The point, as it's becoming clearer and clearer now, is to only calculate what we absolutely have to.</p><p>Scale, once again, follows this idea exactly:</p><pre class="programlisting">void GL_Transform::SetScale(const glm::vec3&amp; l_scale) { 
  if (l_scale == m_scale) { return; } 
  m_scale = l_scale; 
  m_matScale = glm::scale(m_scale); 
  m_needsUpdate = true; 
} 
</pre><p>The <code class="literal">GetModelMatrix</code> method should now be implemented this way:</p><pre class="programlisting">const glm::mat4&amp; GL_Transform::GetModelMatrix() { 
  if (m_needsUpdate) { 
    m_modelMatrix = m_matPos * m_matRotCombined * m_matScale; 
    m_needsUpdate = false; 
  } 
  return m_modelMatrix; 
} 
</pre><p>First, the update flag is checked to determine whether the matrix needs to be updated. If it does, all three relevant matrices are multiplied and the flag is reset back to <code class="literal">false</code>. We then return the const reference to the <code class="literal">m_modelMatrix </code>data member, ensuring one isn't created just to be thrown away later.</p><p>Let's follow our own advice and profile the application again to make sure our changes worked:</p><div><img src="img/image_10_006.jpg" alt="GL_Transform optimizations"/></div><p>All three of the previously highlighted lines to do with <code class="literal">glm::</code> have now completely disappeared from the top of the list! The highlighted exception in this illustration was taken during the sampling of <code class="literal">GL_Transform::GetModelMatrix()</code> that <strong>did not</strong> return by const reference, just to show that our approach does indeed work. When the method does return a const reference, even the highlighted function completely vanishes. This perfectly illustrates how avoiding useless copies of data can vastly improve overall performance.</p></div><div><div><div><div><h3 class="title"><a id="ch10lvl3sec26"/>Particle system optimizations</h3></div></div></div><p>Another massive bottleneck that's right at the top of the sample list is the <code class="literal">ParticleSystem::Draw</code> method. In fact, it's the highest sampled piece of code that we have actually written. It's understandable that rendering so many particles would be taxing, but in this case, the unoptimized version of this method knocks the frame rate of our game down to 10 FPS:</p><div><img src="img/image_10_007.jpg" alt="Particle system optimizations"/></div><div><div><h3 class="title"><a id="tip52"/>Tip</h3><p><strong>Fraps</strong> is a free piece of screen capture software that can record video, take screenshots, and most importantly for our purposes, show the frame rate! Although it's Windows specific, there are other tools like it for Linux and OSX. The frame rate counter can also be easily implemented by simply counting the frames in our code and displaying the result using SFML.</p></div></div><p>That is absolutely unforgivable, so let's break out the profiler and dissect the <code class="literal">Draw</code> method:</p><div><img src="img/image_10_008.jpg" alt="Particle system optimizations"/></div><p>Judging by the sample count, the main inefficiency lies somewhere inside the material value shader pass, where each particle is rendered for the normal and specular passes. There is something slightly weird going on, though, and that's the fact that the normal pass samples seem to be really low, but when the time comes to render for the specular pass, they suddenly jump much higher. This may look especially weird considering all we're doing is setting a <code class="literal">vec3</code> uniform and drawing to a render texture. This is where further digging through the function stack and understanding of how SFML needs to handle things behind the scenes comes in:</p><p>
</p><div><img src="img/image_10_009.jpg" alt="Particle system optimizations"/></div><p>
</p><p>Because of context switching and the way render textures work behind the scenes, it's extremely inefficient to render two different types of material maps as we did. Switching textures too many times during runtime can cause serious performance bottlenecks, which is why sprite and tile sheets are used by games, rather than individual images.</p><p>Let's try and split up these two different types into two separate loops, making sure only one texture is being rendered two at a time:</p><pre class="programlisting">void ParticleSystem::Draw(MaterialMapContainer&amp; l_materials, ...) { 
  ... 
  if (renderer-&gt;UseShader("MaterialValuePass")) { 
    auto shader = renderer-&gt;GetCurrentShader(); 
    // Normal pass. 
    auto texture = l_materials[MaterialMapType::Normal].get(); 
    shader-&gt;setUniform("material", 
      sf::Glsl::Vec3(0.5f, 0.5f, 1.f)); 
    for (size_t i = 0; i &lt; container-&gt;m_countAlive; ++i) { 
      ... 
      renderer-&gt;Draw(drawables[i], texture); 
    } 
 
    // Specular pass. 
    texture = l_materials[MaterialMapType::Specular].get(); 
    shader-&gt;setUniform("material", sf::Glsl::Vec3(0.f, 0.f, 0.f)); 
    for (size_t i = 0; i &lt; container-&gt;m_countAlive; ++i) { 
      ... 
      renderer-&gt;Draw(drawables[i], texture); 
    } 
  } 
  ... 
} 
</pre><p>Note that the material uniform is also moved outside of the loop to prevent unnecessary copies from being constructed and sent to the shader every time. By just running the application now, a very obvious jump in performance will quickly become apparent. Let's see how much faster it got by simply splitting up the little bit of code we had into two pieces:</p><div><img src="img/image_10_010.jpg" alt="Particle system optimizations"/></div><p>We just jumped from 10 FPS to 65 FPS by simply separating the normal and specular material passes! That's more like it! You will notice that this sudden jump in performance will increase the sample count drastically:</p><div><img src="img/image_10_011.jpg" alt="Particle system optimizations"/></div><p>This is because the game is running much faster now and doesn't indicate that the function is taking more time to execute. Remember, the samples are relative. Upon looking through the list, the two previously highlighted bits of code are found much lower now, with sample counts in the 20s. That's only slightly lower than before, but because the samples are relative and they all jumped up to about 6 times higher, it indicates a huge performance gain.</p></div><div><div><div><div><h3 class="title"><a id="ch10lvl3sec27"/>Light culling</h3></div></div></div><p>The last main inefficiency we have to fix has to do with the lighting system implemented in <a class="link" href="ch08.html" title="Chapter 8.  Let There Be Light - An Introduction to Advanced Lighting">Chapter 8
</a>, <em>Let There Be Light! - An Introduction to Advanced Lighting</em>, and <a class="link" href="ch09.html" title="Chapter 9.  The Speed of Dark - Lighting and Shadows">Chapter 9
</a>, <em>The Speed of Dark - Lighting and Shadows</em>.</p><p>Dealing with multiple lights in a scene by using multipass shading/rendering is a great technique, but it can quickly become inefficient when those passes start to add up. The first obvious step to fixing that issue is not rendering lights that will not affect anything in the final image. The technique of reducing the number of objects being rendered by eliminating those that cannot be directly observed by the scene's view frustum, also known as <strong>culling</strong>, is going to help out with that.</p><p>Since we're only dealing with omni-directional point lights at this moment, culling lights can be achieved by simply checking for circle on rectangle collisions.</p><p>Let's set up some helper functions to help us do that:</p><pre class="programlisting">inline float GetDistance(const sf::Vector2f&amp; l_1, 
  const sf::Vector2f&amp; l_2) 
{ 
  return std::sqrt(std::pow(l_1.x - l_2.x, 2) + 
    std::pow(l_1.y - l_2.y, 2)); 
} 
 
inline bool CircleInView(const sf::View&amp; l_view, 
  const sf::Vector2f&amp; l_circleCenter, float l_circleRad) 
{ 
  auto HalfSize = l_view.getSize() / 2.f; 
  float OuterRadius = std::sqrt((HalfSize.x * HalfSize.x) + 
    (HalfSize.y * HalfSize.y)); 
  float AbsoluteDistance = GetDistance(l_view.getCenter(), 
    l_circleCenter); 
  if (AbsoluteDistance &gt; OuterRadius + l_circleRad) { 
    return false; 
  } 
  float InnerRadius = std::min(l_view.getSize().x, 
    l_view.getSize().y) / 2.f; 
  if (AbsoluteDistance &lt; InnerRadius + l_circleRad){return true;} 
  glm::vec2 dir = { 
    l_circleCenter.x - l_view.getCenter().x, 
    l_circleCenter.y - l_view.getCenter().y 
  }; 
  dir = glm::normalize(dir); 
  sf::Vector2f point = l_circleCenter + 
    sf::Vector2f(l_circleRad * dir.x, l_circleRad * dir.y); 
  auto rect = sf::FloatRect( 
    l_view.getCenter() - HalfSize, 
    l_view.getSize()); 
  return rect.contains(point); 
} 
</pre><p>The function works by first creating an outer radius around the view's rectangle, so that we can default to a circle-on-circle collision checking for the majority of cases where the light is nowhere near the view frustum. The distance between the view's centre and circle's centre is obtained and checked for exceeding the sum of the view's outer bounding circle's radius summed with the circle's radius. This is the easiest way to check whether the light circle is anywhere close to the view's rectangle.</p><p>If the light is closer to the view, another circle radius for the view's rectangle is constructed. This time, the circle is inside the view and only has the radius of the smaller dimension of the rectangle's size. If the distance between the light and the view's center is lower than the sum of the inner radius and the circle's radius, we know for sure that we have a collision. That's another common case that we can scratch off the list, before defaulting to a more complicated algorithm.</p><p>Finally, if we know the light may be intersecting with one of the corners, its direction towards the view is normalized and used to obtain the closest point that is then checked for intersecting a constructed <code class="literal">sf::FloatRect</code> that represents our view.</p><p>Actual changes to the <code class="literal">RenderScene()</code> method of the light manager class simply involve storing a new list of lights that are definitely affecting something on screen, so that they can be passed to the shader:</p><pre class="programlisting">void LightManager::RenderScene() { 
  ... 
  std::vector&lt;LightBase*&gt; unculled; 
  for (auto&amp; light : m_lights) { 
    if (!Utils::CircleInView(currentView, 
      { light.m_lightPos.x, light.m_lightPos.y }, 
      light.m_radius)) 
    { continue; } 
    unculled.emplace_back(&amp;light); 
  } 
  auto&amp; beginning = unculled.begin(); 
  auto passes = static_cast&lt;int&gt;(std::ceil( 
    static_cast&lt;float&gt;(unculled.size()) / LightsPerPass)); 
  if (passes == 0) { passes = 1; } 
   
  for (int pass = 0; pass &lt; passes; ++pass) { 
    ... 
    for (int lightID = 0; lightID &lt; LightsPerPass; ++lightID) { 
      ... 
      DrawShadowMap(ShadowPassShaderHandle, **light, lightID); 
      ... 
    } 
    ... 
    for (int lightID = 0; lightID &lt; LightCount; ++lightID) { 
      ... 
      SubmitLightUniforms(LightPassShaderHandle,lightID, **light); 
      ... 
    } 
    ... 
    renderer-&gt;SwapTextures(); 
  } 
  ... 
} 
</pre><p>Note that we're not taking into account the light's falloff or how it's attenuated in the shader to determine whether it should be culled or not.</p><p>After all the unnecessary lights have been culled, only the very busy areas will experience some performance loss. At this point, it's the area of level design that should show concern and improve the architecture of the map.</p></div></div></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch10lvl1sec93"/>Summary</h1></div></div></div><p>Congratulations on making it all the way to the end! It has been quite a journey, and we can certainly say that a lot of things have been covered here that should inspire confidence in advanced game development in anyone. Even so, as always, there's still a ton of features, optimizations, techniques, and topics that we either just briefly touched upon, or haven't even acknowledged yet. Use that as inspiration to seek greatness, because, as we have already established, master craftsmen know not only how, but when to use their tools. While we have covered the basics, there are still many more tools to add to your tool belt. Use them, abuse them, break them and replace them. Do whatever it takes, but always remember to take something out of it and make it better next time.</p><p>With that, may your next project exhibit that extra level of polish, and run just a little bit faster! Thanks for reading!</p></div></div></div></body></html>