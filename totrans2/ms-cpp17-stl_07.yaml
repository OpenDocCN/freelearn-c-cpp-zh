- en: Concurrency
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发
- en: 'In the previous chapter, we discussed how `std::shared_ptr<T>` implements reference-counting
    memory management, so that an object''s lifetime can be cooperatively controlled
    by stakeholders who might be otherwise unaware of each other--for example, the
    stakeholders might live in different threads. In C++ before C++11, this would
    have posed a stumbling block right away: if one stakeholder decrements the reference
    count while, simultaneously, a stakeholder in another thread is in the process
    of decrementing the reference count, then don''t we have a *data race* and therefore
    undefined behavior?'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了`std::shared_ptr<T>`如何实现引用计数内存管理，以便对象的生存期可以由可能彼此不了解的利益相关者共同控制——例如，利益相关者可能生活在不同的线程中。在C++11之前，这会立即成为一个障碍：如果一个利益相关者在减少引用计数的同时，另一个线程中的利益相关者正在减少引用计数的进程中，那么我们不是有*数据竞争*和因此是未定义的行为吗？
- en: In C++ before C++11, the answer was generally "yes." (In fact, C++ before C++11
    didn't have a standard concept of "threads," so another reasonable answer might
    have been that the question itself was irrelevant.) In C++ as of 2011, though,
    we have a standard memory model that accounts for concepts such as "threading"
    and "thread-safety," and so the question is meaningful and the answer is categorically
    "No!" Accesses to the reference count of `std::shared_ptr` are guaranteed not
    to race with each other; and in this chapter we'll show you how you can implement
    similarly thread-safe constructions using the tools the standard library provides.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在C++11之前，答案通常是“是的。”（事实上，C++11之前的C++没有“线程”的标准概念，所以另一个合理的答案可能是这个问题本身就不相关。）然而，自2011年以来，C++有一个标准内存模型，它考虑了诸如“线程”和“线程安全”等概念，因此这个问题是有意义的，答案是明确地“不！”对`std::shared_ptr`的引用计数的访问保证不会相互竞争；在本章中，我们将向您展示如何使用标准库提供的工具实现类似的线程安全构造。
- en: 'In this chapter we''ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: The difference between `volatile T` and `std::atomic<T>`
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`volatile T`和`std::atomic<T>`之间的区别'
- en: '`std::mutex`, `std::lock_guard<M>`, and `std::unique_lock<M>`'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`std::mutex`、`std::lock_guard<M>`和`std::unique_lock<M>`'
- en: '`std::recursive_mutex` and `std::shared_mutex`'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`std::recursive_mutex`和`std::shared_mutex`'
- en: '`std::condition_variable` and `std::condition_variable_any`'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`std::condition_variable`和`std::condition_variable_any`'
- en: '`std::promise<T>` and `std::future<T>`'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`std::promise<T>`和`std::future<T>`'
- en: '`std::thread` and `std::async`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`std::thread`和`std::async`'
- en: The dangers of `std::async`, and how to build a thread pool to replace it
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`std::async`的危险以及如何构建线程池来替代它'
- en: The problem with volatile
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不稳定的**问题**
- en: If you've been living under a rock for the past ten years--or if you're coming
    from old-style C--you might ask, "What's wrong with the `volatile` keyword? When
    I want to make sure some access really hits memory, I make sure it's done `volatile`."
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你过去十年一直生活在山洞里——或者如果你来自旧式的C语言——你可能会问：“`volatile`关键字有什么问题？当我想要确保某些访问真正触达内存时，我会确保它是`volatile`的。”
- en: 'The official semantics of `volatile` are that volatile accesses are evaluated
    strictly according to the rules of the abstract machine, which means, more or
    less, that the compiler is not allowed to reorder them or combine multiple accesses
    into one. For example, the compiler cannot assume that the value of `x` remains
    the same between these two loads; it must generate machine code that performs
    two loads, one on either side of the store to `y`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`volatile`的官方语义是，volatile访问将严格根据抽象机的规则进行评估，这意味着，或多或少，编译器不允许重新排序它们或将多个访问组合成一个。例如，编译器不能假设在这两次加载之间`x`的值保持不变；它必须生成执行两个加载的机器代码，一个在存储到`y`之前，一个在之后：'
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If `x` were not volatile, then the compiler would be perfectly within its rights
    to reorder the code like this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`x`不是`volatile`，那么编译器完全有权像这样重新排序代码：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The compiler could do this (if `x` weren't volatile) because the write to a
    `bool` variable `y` cannot possibly affect the value of the `int` variable `x`.
    However, since `x` is volatile, this reordering optimization is not allowed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`x`不是`volatile`，编译器可以这样做（因为写入`bool`变量`y`不可能影响`int`变量`x`的值）。然而，由于`x`是`volatile`，这种重新排序优化是不允许的。
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So that's what `volatile` does. But why can't we use `volatile` to make our
    programs thread-safe? In essence, the problem with `volatile` is that it's too
    old. The keyword has been in C++ ever since we split off from C, and it was in
    C since the original standard in 1989\. Back then, there was very little concern
    about multithreading, and compilers were simpler, which meant that some potentially
    problematic optimizations had not yet been dreamt of. By the late 1990s and early
    2000s, when C++'s lack of a thread-aware memory model started to become a real
    concern, it was too late to make `volatile` do everything that was required for
    thread-safe memory access, because every vendor had already implemented `volatile`
    and documented exactly what it did. Changing the rules at that point would have
    broken a lot of people's code--and the code that would have broken would have
    been low-level hardware interface code, the kind of code you really don't want
    bugs in.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是`volatile`的作用。但为什么我们不能使用`volatile`来使我们的程序线程安全呢？本质上，`volatile`的问题在于它太老了。自从我们从C语言分离出来，C++就包含了这个关键字，而它在1989年的原始标准中就已经存在了。当时，对多线程的关注非常少，编译器也更简单，这意味着一些可能存在问题的优化还没有被想到。到了1990年代末和2000年代初，当C++缺乏线程感知的内存模型开始成为一个真正的问题时，已经太晚了，无法让`volatile`完成线程安全内存访问所需的全部工作，因为每个供应商都已经实现了`volatile`并详细记录了它所做的一切。在那个时刻改变规则将会破坏很多人的代码——而且会被破坏的代码将是低级硬件接口代码，这种代码你真的不希望出现错误。
- en: 'Here are a couple of examples of the kind of guarantee we need in order to
    get thread-safe memory accesses:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些我们需要确保的保证类型，以便获得线程安全的内存访问：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Suppose `thread_A`, `thread_B`, and `thread_C` are all running concurrently
    in different threads. How could this code go wrong? Well, `thread_B` is checking
    that `x` always holds exactly either zero or `0x42'00000042`. On a 32-bit computer,
    however, it may not be possible to make that guarantee; the compiler might have
    to implement the assignment in `thread_A` as a pair of assignments "set the upper
    half of `x` to 42; set the lower half of `x` to 42." If the test in `thread_B`
    happens to run at the right (wrong) time, it could end up seeing `x` as `0x42'00000000`.
    Making `x` volatile will not help with this one; in fact, nothing will, because
    our 32-bit hardware simply doesn't support this operation! It would be nice for
    the compiler to detect that we're trying to get an atomic 64-bit assignment, and
    give us a compile-time error if it knows our goal is impossible. In other words,
    `volatile` accesses are not guaranteed to be *atomic*. In practice, they often
    are atomic--and so are non-volatile accesses, but they aren't *guaranteed* to
    be, and sometimes you have to go down to the machine code level to figure out
    whether you're getting the code you expected. We'd like a way to guarantee that
    an access will be atomic (or if that's impossible, we'd like a compiler error).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`thread_A`、`thread_B`和`thread_C`都在不同的线程中并发运行。这段代码可能会出错吗？嗯，`thread_B`正在检查`x`始终保持正好是零或`0x42'00000042`。然而，在32位计算机上，可能无法做出这样的保证；编译器可能不得不将`thread_A`中的赋值实现为两个赋值：“将`x`的高半部分设置为42；将`x`的低半部分设置为42。”如果`thread_B`中的测试恰好（错误地）在正确的时间运行，它最终可能会看到`x`为`0x42'00000000`。将`x`声明为`volatile`并不能解决这个问题；事实上，什么都不能，因为我们的32位硬件根本不支持这个操作！如果编译器能够检测到我们正在尝试进行原子64位赋值，并且如果它知道我们的目标是不可实现的，就会给出编译时错误。换句话说，`volatile`访问并不保证是原子的。在实践中，它们通常是原子的——非`volatile`访问也是如此，但它们并不保证是原子的，有时你必须下降到机器代码级别才能确定你是否得到了预期的代码。我们希望有一种方法可以保证访问将是原子的（或者如果这是不可能的，我们希望编译器报错）。
- en: Now consider `thread_C`. It's checking that *if* the value of `y` is visibly
    true, *then* the value of `x` must already be set to its final value. In other
    words, it's checking that the write to `x` "happened before" the write to `y`.
    This is definitely true from the point of view of `thread_A`, at least if `x`
    and `y` are both volatile, because we have seen that the compiler is not allowed
    to reorder volatile accesses. However, the same is not necessarily true from the
    point of view of `thread_C`! If `thread_C` is running on a different physical
    CPU, with its own data cache, then it may become aware of the updated values of
    `x` and `y` at different times, depending on when it refreshes their respective
    cache lines. We would like a way to say that when the compiler loads from `y`,
    it must also ensure that its entire cache is up-to-date--that it will never read
    a "stale" value for `x`. However, on some processor architectures, that requires
    special instructions, or additional memory-barrier logic. The compiler doesn't
    generate those instructions for "old-style" volatile accesses, because threading
    wasn't a concern when `volatile` was invented; and the compiler can't be *made*
    to generate those instructions for volatile accesses, because that would unnecessarily
    slow down or maybe even break, all the existing low-level code that uses old-style
    `volatile` for its old-style meaning. So we're left with the problem that even
    though volatile accesses happen in sequential order from the point of view of
    their own thread, they may well appear in a different order from the point of
    view of another thread. In other words, `volatile` accesses are not guaranteed
    to be *sequentially consistent*. We'd like a way to guarantee that an access will
    be sequentially consistent with respect to other accesses.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑`thread_C`。它在检查，如果`y`的值是可见的为真，那么`x`的值必须已经设置为它的最终值。换句话说，它正在检查对`x`的写入“发生在”对`y`的写入之前。从`thread_A`的角度来看，这绝对是正确的，至少如果`x`和`y`都是易失性的，因为我们已经看到编译器不允许重新排序易失性访问。然而，从`thread_C`的角度来看，这并不一定正确！如果`thread_C`在不同的物理CPU上运行，有自己的数据缓存，那么它可能会在不同的时间意识到`x`和`y`的更新值，这取决于它何时刷新各自的缓存行。我们希望有一种方式来说明，当编译器从`y`加载时，它必须确保其整个缓存是最新的--它永远不会读取一个“过时”的`x`值。然而，在某些处理器架构上，这需要特殊的指令或额外的内存屏障逻辑。编译器不会为“旧式”的易失性访问生成这些指令，因为当`volatile`被发明时，线程并不是一个关注点；并且编译器不能被*强制*生成这些指令，因为这会不必要地减慢速度，甚至可能破坏所有使用旧式`volatile`进行旧式意义访问的现有底层代码。因此，我们面临的问题是，尽管从它们自己的线程的角度来看，易失性访问是按顺序发生的，但它们可能从另一个线程的角度来看以不同的顺序出现。换句话说，易失性访问不能保证是*顺序一致的*。我们希望有一种方式来保证访问将与其他访问顺序一致。
- en: The solution to both of our problems was added to C++ in 2011\. That solution
    is `std::atomic`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 解决我们两个问题的方案在2011年被添加到C++中。这个方案是`std::atomic`。
- en: Using std::atomic<T> for thread-safe accesses
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`std::atomic<T>`进行线程安全的访问
- en: 'In C++11 and later, the `<atomic>` header contains the definition of class
    template `std::atomic<T>`. There are two different ways you can think about `std::atomic`:
    you can think of it as a class template just like `std::vector`, with overloaded
    operators that just happen to implement thread-safe operations; or you can think
    of it as a magical built-in family of types whose names just happen to contain
    angle brackets. The latter way of thinking about it is actually pretty useful,
    because it suggests--correctly--that `std::atomic` is partly built into the compiler,
    and so the compiler will usually generate optimal code for atomic operations.
    The latter also suggests a way in which `atomic` is different from `vector`: with
    `std::vector<T>`, the `T` can be pretty much anything you like. With `std::atomic<T>`,
    the `T` is *can* be anything you like, but in practice it is a bad idea to use
    any `T` that doesn''t belong to a small set of *atomic-friendly* types. More on
    this topic in a moment.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在C++11及以后版本中，`<atomic>`头文件包含了类模板`std::atomic<T>`的定义。你可以从两种不同的方式来考虑`std::atomic`：你可以将其视为一个类模板，就像`std::vector`一样，它重载了操作符，这些操作符恰好实现了线程安全的操作；或者你可以将其视为一个神奇的内建类型家族，其名称恰好包含尖括号。后一种思考方式实际上非常有用，因为它正确地表明`std::atomic`部分是内建到编译器中的，因此编译器通常会为原子操作生成最优代码。后一种还表明了`atomic`与`vector`的不同之处：在`std::vector<T>`中，`T`可以是几乎任何你想要的东西。在`std::atomic<T>`中，`T`可以是任何你想要的东西，但在实践中，使用不属于一小组*原子友好*类型的任何`T`都是不明智的。关于这个话题的更多内容，稍后讨论。
- en: 'The *atomic-friendly* types are the integral types (at least, those no bigger
    than a machine register) and the pointer types. Generally speaking, on common
    platforms, you''ll find that operations on `std::atomic` objects of these types
    will do exactly what you want:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 原子友好的类型是整数类型（至少，那些不超过机器寄存器大小的类型）和指针类型。一般来说，在常见的平台上，你会发现对这些类型的 `std::atomic`
    对象的操作会正好满足你的需求：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`std::atomic<T>` overloads its assignment operator to perform atomic, thread-safe
    assignment; and likewise its `++`, `--`, `+=`, and `-=` operators; and for integral
    types, also the `&=`, `|=`, and `^=` operators.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::atomic<T>` 重载了赋值运算符以执行原子、线程安全的赋值；同样地，它的 `++`、`--`、`+=` 和 `-=` 运算符；对于整数类型，还包括
    `&=`、`|=` 和 `^=` 运算符。'
- en: 'It''s important to bear in mind the difference between *objects* of type `std::atomic<T>`
    (which conceptually live "out there" in memory) and short-lived *values* of type
    `T` (which conceptually live "right here," close at hand; for example, in CPU
    registers). So, for example, there is no copy-assignment operator for `std::atomic<int>`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住 `std::atomic<T>` 类型（在概念上生活在内存“那里”）和类型 `T` 的短暂值（在概念上生活在“这里”，手头附近；例如，在
    CPU 寄存器中）之间的区别。因此，例如，`std::atomic<int>` 没有复制赋值运算符：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'There''s no copy-assignment operator (nor move-assignment operator) because
    it wouldn''t have a clear meaning: Does the programmer mean that the computer
    should load the value of `b` into a register and then store the value of that
    register into `a`? That sounds like two different atomic operations, not one operation!
    Or the programmer might mean that the computer should copy the value from `b`
    to `a` in a single atomic operation; but that involves touching two different
    memory locations in a single atomic operation, which is not within the capabilities
    of most computer hardware. So instead, C++ requires that you write out explicitly
    what you mean: a single atomic load from object `b` into a register (represented
    in C++ by a non-atomic stack variable), and then a single atomic store into object
    `a`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 没有复制赋值运算符（也没有移动赋值运算符），因为这没有明确的含义：程序员的意思是计算机应该将 `b` 的值加载到一个寄存器中，然后将该寄存器的值存储到
    `a` 中吗？这听起来像是两个不同的原子操作，而不是一个操作！或者程序员可能意味着计算机应该在单个原子操作中将 `b` 的值复制到 `a` 中；但这涉及到在单个原子操作中触及两个不同的内存位置，而这超出了大多数计算机硬件的能力。因此，C++
    要求你明确写出你的意图：从对象 `b` 到寄存器（在 C++ 中由非原子栈变量表示）的单个原子加载，然后是到对象 `a` 的单个原子存储：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`std::atomic<T>` provides the member functions `.load()` and `.store(v)` for
    the benefit of programmers who like to see what they''re doing at every step.
    Using them is optional:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::atomic<T>` 提供了 `.load()` 和 `.store(v)` 成员函数，以方便那些喜欢在每一步都看到他们所做事情的程序员。使用它们是可选的：'
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In fact, by using these member functions, you *could* write the assignment in
    a single line of code as `b.store(a.load())`; but I advise strongly against doing
    that. Writing both function calls on one line of code does *not* mean that they'll
    happen "closer together" in time, and *certainly* doesn't mean they'll happen
    "atomically" (as we've just seen, that's impossible on most hardware), but writing
    both function calls on one line of code might very well *deceive you into thinking*
    that the calls happen "together."
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，通过使用这些成员函数，你*可以*将赋值操作写在一行代码中，例如 `b.store(a.load())`；但我强烈建议你不要这样做。在一行代码中写上两个函数调用*并不意味着*它们会在时间上“更接近”，当然也不意味着它们会“原子性地”发生（正如我们刚才看到的，在大多数硬件上这是不可能的），但将两个函数调用写在一行代码中可能会让你误以为这些调用是“同时”发生的。
- en: Dealing with threaded code is hard enough when you're doing only one thing at
    a time. If you start getting clever and doing several things at once, in a single
    line of code, the potential for bugs skyrockets. Stick to a single atomic operation
    per source line; you'll find that it clarifies your thinking process and incidentally
    makes your code easier to read.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当你一次只做一件事时，处理线程代码已经足够困难了。如果你开始变得聪明，同时做几件事，在一行代码中，错误的可能性会急剧增加。坚持每行源代码一个原子操作；你会发现这会澄清你的思考过程，并且意外地使你的代码更容易阅读。
- en: Doing complicated operations atomically
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原子性地执行复杂操作
- en: You may have noticed that the operators `*=`, `/=`, `%=`, `<<=`, and `>>=` were
    omitted from the list of overloaded operators in the preceding section. These
    operators were deleted by `std::atomic<int>` and all the rest of the integral
    atomic types because they were perceived as being difficult to provide efficiently
    on any real hardware. However, even among the operations that were included in
    `std::atomic<int>`, most of them require a slightly expensive implementation trick.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，操作符 `*=`, `/=`, `%=`, `<<=`, 和 `>>=` 在上一节中省略了重载操作符的列表。这些操作符被 `std::atomic<int>`
    和所有其他整型原子类型删除，因为它们被认为在任何实际硬件上难以高效实现。然而，即使在 `std::atomic<int>` 中包含的操作中，大多数也需要一个稍微昂贵的实现技巧。
- en: Let's suppose that our hardware doesn't have an "atomic multiply" instruction,
    but we'd still like to implement `operator*=`). How would we do it? The trick
    is to use a primitive atomic operation known as "compare and swap," or in C++
    "compare-exchange."
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的硬件没有“原子乘法”指令，但我们仍然想实现 `operator*=`。我们该如何做？诀窍是使用一种原始的原子操作，称为“比较并交换”，或者在
    C++ 中称为“比较交换”。
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The meaning of `a.compare_exchange_weak(expected, desired)` is that the processor
    should look at `a`; and *if* its value is currently `expected`, then set its value
    to `desired`; otherwise don't. The function call returns `true` if `a` was set
    to `desired` and `false` otherwise.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`a.compare_exchange_weak(expected, desired)` 的含义是处理器应该查看 `a`；并且 *如果* 它的当前值是
    `expected`，则将其值设置为 `desired`；否则不设置。如果 `a` 被设置为 `desired`，则函数调用返回 `true`，否则返回 `false`。'
- en: But there's one more thing it does, too. Notice that every time through the
    preceding loop, we're loading the value of `a` into `expected`; but the compare-exchange
    function is also loading the value of `a` in order to compare it with `expected`.
    The second time we go through the loop, we'd prefer not to load `a` a second time;
    we'd prefer simply to set `expected` to the value that the compare-exchange function
    saw. Fortunately, `a.compare_exchange_weak(expected, desired)` anticipates this
    desire of ours, and preemptively--if it would return `false`--updates `expected`
    to the value it saw. That is, whenever we use `compare_exchange_weak`, we must
    provide a modifiable value for `expected` because the function takes it by reference.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 但它还有另一个功能。注意，每次通过前面的循环时，我们都会将 `a` 的值加载到 `expected` 中；但比较交换函数也会加载 `a` 的值以便与 `expected`
    进行比较。当我们第二次通过循环时，我们更希望不要再次加载 `a`；我们更希望直接将 `expected` 设置为比较交换函数看到的值。幸运的是，`a.compare_exchange_weak(expected,
    desired)` 预见了我们的这个愿望，并且如果它将返回 `false`，则会预先更新 `expected` 为它看到的值。也就是说，每次我们使用 `compare_exchange_weak`
    时，我们必须提供一个可修改的 `expected` 值，因为函数是通过引用来获取它的。
- en: 'Therefore, we should really write our example like this:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们实际上应该这样编写我们的示例：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `desired` variable isn't really necessary except if it helps to clarify
    the code.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`desired` 变量实际上并不是必需的，除非它有助于澄清代码。'
- en: The dirty little secret of `std::atomic` is that most of the compound assignment
    operations are implemented as compare-exchange loops just like this. On RISC processors,
    this is practically always the case. On x86 processors, this is the case only
    if you want to use the return value of the operator, as in `x = (a += b)`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::atomic` 的一个不为人知的秘密是，大多数的复合赋值操作实际上都是通过类似于这样的比较交换循环来实现的。在 RISC 处理器上，这几乎是始终如此。在
    x86 处理器上，这种情况只在你想要使用操作符的返回值时才会发生，例如 `x = (a += b)`。'
- en: When the atomic variable `a` isn't being modified very frequently by other threads,
    there's no harm in doing a compare-exchange loop. But when `a` is being frequently
    modified--when it is highly *contended*--then we might see the loop being taken
    several times before it succeeds. In an absolutely pathological case, we might
    even see starvation of the looping thread; it might just keep looping forever,
    until the contention died down. However, notice that every time our compare-exchange
    returns `false` and we loop around again, it is because the value of `a` in memory
    has changed; which means that some other thread must have made a little bit of
    progress. Compare-exchange loops by themselves will never cause the program to
    enter a state where *nobody* is making progress (a state known technically as
    "livelock").
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当原子变量 `a` 不是被其他线程频繁修改时，进行比较交换循环是没有害处的。但当 `a` 被频繁修改——当它高度 *竞争* 时——我们可能会看到循环多次尝试才能成功。在绝对病态的情况下，我们甚至可能会看到循环线程的饥饿；它可能只是无限循环，直到竞争减弱。然而，请注意，每次我们的比较交换返回
    `false` 并再次循环时，都是因为内存中 `a` 的值已经改变；这意味着其他线程必须已经取得了一点点进展。比较交换循环本身永远不会导致程序进入一个没有人取得进展的状态（技术上称为“活锁”）。
- en: 'The previous paragraph probably sounds scarier than it ought to. There''s generally
    no need to worry about this pathological behavior, since it manifests itself only
    under really high contention and even then doesn''t really cause any terrible
    problem. The real takeaway you should take from this section is how you can use
    a compare-exchange loop to implement complicated, non-built-in "atomic" operations
    on `atomic<T>` objects. Just remember the order of the parameters to `a.compare_exchange_weak(expected,
    desired)` by remembering what it does to `a`: "if `a` has the expected value,
    give it the desired value."'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 前一段文字可能听起来比它应该的更可怕。通常没有必要担心这种病态行为，因为它只在高度竞争的情况下才会出现，即使那样也不会真正造成任何严重问题。你应该从这个部分学到的真正要点是如何使用比较交换循环在
    `atomic<T>` 对象上实现复杂的、非内置的“原子”操作。只需记住 `a.compare_exchange_weak(expected, desired)`
    参数的顺序，通过记住它对 `a` 做了什么：“如果 `a` 有预期的值，就给它想要的值。”
- en: Big atomics
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大原子操作
- en: The compiler will recognize and generate optimal code for `std::atomic<T>` when
    `T` is an integral type (including `bool`), or when `T` is a pointer type such
    as `void *`. But what if `T` is a bigger type, such as `int[100]`? In that case,
    the compiler will generally call out to a routine in the C++ runtime library which
    will perform the assignment under a *mutex*. (We'll look at mutexes in a moment.)
    Since the assignment is being performed out in a library which doesn't know how
    to copy arbitrary user-defined types, the C++17 standard restricts `std::atomic<T>`
    to work only with types that are *trivially copyable*, which is to say they can
    be copied safely using `memcpy`. So, if you wanted `std::atomic<std::string>`,
    tough luck--you'll have to write that one yourself.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器会识别并生成针对 `std::atomic<T>` 的最佳代码，当 `T` 是整数类型（包括 `bool`）或 `T` 是指针类型，如 `void
    *` 时。但如果 `T` 是更大的类型，例如 `int[100]` 呢？在这种情况下，编译器通常会调用 C++ 运行时库中的一个例程，该例程将在一个 *互斥锁*
    下执行赋值操作。（我们稍后会讨论互斥锁。）由于赋值操作是在一个不知道如何复制任意用户定义类型的库中执行的，C++17 标准将 `std::atomic<T>`
    限制为只能与那些可以 *简单复制* 的类型一起工作，也就是说，它们可以使用 `memcpy` 安全地复制。因此，如果你想要 `std::atomic<std::string>`，那很遗憾——你必须自己编写它。
- en: The other catch when using big (trivially copyable) types with `std::atomic`
    is that the relevant C++ runtime routines often live in a different place from
    the rest of the C++ standard library. On some platforms, you'd be required to
    add `-latomic` to your linker command line. But this is only a problem if you
    actually do use big types with `std::atomic`, and as you really shouldn't, there's
    generally no reason to worry.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大（简单可复制）类型与 `std::atomic` 时的另一个问题是，相关的 C++ 运行时例程通常位于 C++ 标准库的其他地方。在某些平台上，你可能需要在链接器命令行中添加
    `-latomic`。但这只在你实际上使用大类型与 `std::atomic` 一起使用时才是问题。因为你实际上不应该这样做，所以通常没有必要担心。
- en: Now let's look at how you'd write that atomic string class!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何编写那个原子字符串类！
- en: Taking turns with std::mutex
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 std::mutex 交替使用
- en: Suppose we want to write a class type that behaves basically like `std::atomic<std::string>`
    would, if it existed. That is, we'd like to make it support atomic, thread-safe
    loads and stores, so that if two threads are accessing the `std::string` concurrently,
    neither one will ever observe it in a "halfway assigned" state, the way we observed
    a "halfway assigned" `int64_t` in the code sample in the previous section "The
    problem with volatile."
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要编写一个类类型，其行为基本上类似于如果存在`std::atomic<std::string>`会有的行为。也就是说，我们希望它支持原子、线程安全的加载和存储，这样如果两个线程正在并发访问`std::string`，则任何一个都不会观察到它处于“半分配”状态，就像我们在上一节“`volatile`的问题”中的代码示例中观察到的“半分配”`int64_t`一样。
- en: The best way to write this class is to use a standard library type called `std::mutex`.
    The name "mutex" is so common in technical circles that these days it basically
    just stands for itself, but originally its name is derived from "*mut*ual *ex*clusion."
    This is because a mutex acts as a way to ensure that only one thread is allowed
    into a particular section of code (or set of sections of code) at once--that is,
    to ensure that the possibilities "thread A is executing this code" and "thread
    B is executing this code" are *mutually exclusive* possibilities.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 编写此类最佳方式是使用一个名为 `std::mutex` 的标准库类型。在技术领域，“mutex”这个名字非常常见，以至于如今它基本上就是其自身的代名词，但最初它的名字来源于“*mut*ual
    *ex*clusion”（互斥）。这是因为互斥锁充当了一种确保一次只允许一个线程进入特定代码段（或一组代码段）的方式——也就是说，确保“线程A正在执行此代码”和“线程B正在执行此代码”是互斥的可能性。
- en: At the start of such a critical section, to indicate that we don't want to be
    disturbed by any other thread, we *take a lock* on the associated mutex. When
    we leave the critical section, we *release the lock*. The library takes care of
    making sure that no two threads can hold locks on the same mutex at the same time.
    Specifically, this means that if thread B comes in while thread A is already holding
    the lock, thread B must *wait* until thread A leaves the critical section and
    releases the lock. As long as thread A holds the lock, thread B's progress is
    *blocked*; therefore this phenomenon is referred to as either *waiting* or *blocking*,
    interchangeably.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样一个关键段的开始，为了表明我们不希望被任何其他线程打扰，我们会在相关的互斥锁上*加锁*。当我们离开关键段时，我们*释放锁*。库会负责确保没有两个线程可以在同一时间对同一个互斥锁持有锁。具体来说，这意味着如果线程B在线程A已经持有锁的情况下进入，线程B必须*等待*直到线程A离开关键段并释放锁。只要线程A持有锁，线程B的进度就会被*阻塞*；因此，这种现象被称为*等待*或*阻塞*，可以互换使用。
- en: '"Taking a lock on a mutex" is often shortened to "locking the mutex," and "releasing
    the lock" shortened to "unlocking the mutex."'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: “在互斥锁上加锁”通常简称为“锁定互斥锁”，“释放锁”简称为“解锁互斥锁”。
- en: Sometimes (albeit rarely) it can be useful to test whether a mutex is currently
    locked. For this purpose `std::mutex` exposes not only the member functions `.lock()`
    and `.unlock()` but also the member function `.try_lock()`, which returns `true`
    if it was able to acquire a lock on the mutex (in which case the mutex will be
    locked) and `false` if the mutex was already locked by some thread.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有时（尽管很少见）测试一个互斥锁是否当前被锁定是有用的。为此目的，`std::mutex`不仅公开了成员函数`.lock()`和`.unlock()`，还公开了成员函数`.try_lock()`，如果它能够获取互斥锁的锁（在这种情况下，互斥锁将被锁定），则返回`true`，如果互斥锁已经被某个线程锁定，则返回`false`。
- en: 'In some languages, like Java, each object carries with it its own mutex; this
    is how Java implements its `synchronized` blocks, for example. In C++, a mutex
    is its own object type; when you want to use a mutex to control a section of code,
    you need to think about the lifetime semantics of the mutex object itself. Where
    can you put the mutex so that there will be just a single mutex object that is
    visible to everyone who wants to use it? Sometimes, if there is just one critical
    section that needs protection, you can put the mutex in a function-scoped static
    variable:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些语言中，例如Java，每个对象都携带自己的互斥锁；这就是Java实现其`synchronized`块的方式。在C++中，互斥锁是其自己的对象类型；当你想要使用互斥锁来控制代码段时，你需要考虑互斥锁对象的生存期语义。你可以在哪里放置互斥锁，以便只有一个对希望使用它的每个人可见的互斥锁对象？有时，如果只有一个需要保护的关键段，你可以在函数作用域的静态变量中放置互斥锁：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `static` keyword here is very important! If we had omitted it, then `m`
    would have been a plain old stack variable, and each thread that entered `log`
    would have received its own distinct copy of `m`. That wouldn't have helped us
    with our goal, because the library merely ensures that no two threads have a lock
    on the *same* mutex object at once. If each thread is locking and unlocking its
    own distinct mutex object, then the library has nothing to do; none of the mutexes
    are being *contended*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `static` 关键字非常重要！如果我们省略了它，那么 `m` 就会是一个普通的栈变量，每个进入 `log` 的线程都会收到 `m` 的一个独特副本。这不会帮助我们实现目标，因为库仅仅确保一次不会有两个线程锁定同一个互斥量对象。如果每个线程都在锁定和解锁它自己的独特互斥量对象，那么库就没有什么可做的；没有任何互斥量正在被*竞争*。
- en: 'If we want to make sure that two different functions are mutually exclusive
    with each other, such that only one thread is allowed in either `log1` or `log2`
    at any given time, we must put the mutex object somewhere that can be seen by
    both critical sections:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要确保两个不同的函数彼此互斥，即在任何给定时间只允许一个线程进入 `log1` 或 `log2`，我们必须将互斥量对象放在两个关键部分都能看到的地方：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Generally, if you find yourself needing to do this, you should try to eliminate
    the global variable by creating a class type and making the mutex object a member
    variable of that class, like this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果你发现自己需要这样做，你应该尝试通过创建一个类类型并使互斥量对象成为该类的成员变量来消除全局变量，如下所示：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now messages printed by one `Logger` may interleave with messages printed by
    another `Logger`, but concurrent accesses to the same `Logger` object will take
    locks on the same `m_mtx`, which means they will block each other and nicely take
    turns entering the critical functions `log1` and `log2`, one at a time.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在由一个 `Logger` 打印的消息可能会与另一个 `Logger` 打印的消息交织在一起，但同时对同一个 `Logger` 对象的并发访问将会锁定在同一个
    `m_mtx` 上，这意味着它们将互相阻塞并很好地轮流进入关键函数 `log1` 和 `log2`，一次一个。
- en: '"Taking locks" the right way'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “正确地获取锁”
- en: 'Recall from [Chapter 6](part0093.html#2OM4A0-2fdac365b8984feebddfbb9250eaf20d),
    *Smart Pointers*, that one of the major problems of programs written in C and
    "old-style" C++ is the presence of pointer bugs--memory leaks, double-frees, and
    heap corruption--and that the way we eliminate those bugs from "new-style" C++
    programs is via the use of RAII types such as `std::unique_ptr<T>`. Multi-threaded
    programming with raw mutexes have failure modes that are analogous to the failure
    modes of heap programming with raw pointers:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下 [第 6 章](part0093.html#2OM4A0-2fdac365b8984feebddfbb9250eaf20d)，*智能指针*，C
    和“旧式”C++ 编写的程序的一个主要问题是存在指针错误--内存泄漏、双重释放和堆损坏--以及我们从“新式”C++ 程序中消除这些错误的方法是通过使用 RAII
    类型，如 `std::unique_ptr<T>`。使用原始互斥量的多线程编程具有与使用原始指针进行堆编程的故障模式类似：
- en: '**Lock leaks***:* You might take a lock on a particular mutex, and accidentally
    forget to write the code that frees it.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**锁泄漏**：你可能会锁定特定的互斥量，并意外忘记编写释放它的代码。'
- en: '**Lock leaks**: You might have written that code, but due to an early return
    or an exception being thrown, the code never runs and the mutex remains locked!'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**锁泄漏**：你可能已经编写了那段代码，但由于早期返回或抛出异常，代码从未运行，互斥量仍然被锁定！'
- en: '**Use-outside-of-lock**: Because a raw mutex is just another variable, it is
    physically disassociated from the variables it "guards." You might accidentally
    access one of those variables without taking the lock first.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**锁外使用**：因为原始互斥量只是另一个变量，它与它“保护”的变量在物理上是分离的。你可能会在没有先获取锁的情况下意外访问这些变量之一。'
- en: '**Deadlock**: Suppose thread A takes a lock on mutex 1 and thread B takes a
    lock on mutex 2\. Then, thread A attempts to acquire a lock on mutex 2 (and blocks);
    and while thread A is still blocked, thread B attempts to acquire a lock on mutex
    1 (and blocks). Now both threads are blocked, and will never make progress again.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**死锁**：假设线程 A 锁定了互斥量 1，而线程 B 锁定了互斥量 2。然后，线程 A 尝试获取互斥量 2 的锁（并阻塞）；而在线程 A 仍然阻塞的同时，线程
    B 尝试获取互斥量 1 的锁（并阻塞）。现在两个线程都处于阻塞状态，并且将永远不会再次取得进展。'
- en: This is not an exhaustive list of concurrency pitfalls; for example, we've already
    briefly mentioned "livelock" in connection with `std::atomic<T>`. For a thorough
    treatment of concurrency bugs and how to avoid them, consult a book on multithreaded
    or concurrent programming.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是并发陷阱的详尽列表；例如，我们已经在与 `std::atomic<T>` 相关的上下文中简要提到了“活锁”。对于并发错误及其避免方法的彻底处理，请参阅关于多线程或并发编程的书籍。
- en: The C++ standard library has some tools that help us eliminate these bugs from
    our multithreaded programs. Unlike the situation with memory management, the standard
    library's solutions in this case are not 100 percent guaranteed to fix your issues--multithreading
    is much harder than single-threaded programming, and in fact a good rule of thumb
    is *not to do it* if you can help it. But if you must do concurrent programming,
    the standard library can help somewhat.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: C++ 标准库有一些工具可以帮助我们从多线程程序中消除这些错误。与内存管理的情况不同，在这种情况下，标准库的解决方案并不能保证100%修复你的问题--多线程编程比单线程编程要复杂得多，实际上，如果你能避免的话，一个好的经验法则是*不要做*。但是，如果你必须进行并发编程，标准库可以在一定程度上帮助你。
- en: 'Just as in [Chapter 6](part0093.html#2OM4A0-2fdac365b8984feebddfbb9250eaf20d),
    *Smart Pointers,* we can eliminate bugs related to "lock leaks" by the conscientious
    use of RAII. You might have noticed that I have been consistently using the phrase
    "take a lock on the mutex" instead of "lock the mutex"; now we''ll see why. In
    the phrase "lock the mutex," "lock" is a *verb*; this phrasing corresponds exactly
    to the C++ code `mtx.lock()`. But in the phrase "take a lock *on* the mutex,"
    "lock" is a *noun*. Let''s invent a type that reifies the idea of "lock"; that
    is, that turns it into a noun (an RAII class) instead of a verb (a method on a
    non-RAII class):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 [第6章](part0093.html#2OM4A0-2fdac365b8984feebddfbb9250eaf20d) 中所提到的 *智能指针*，我们可以通过谨慎使用
    RAII 来消除与“锁泄漏”相关的错误。你可能已经注意到，我一直在一致地使用“在互斥锁上获取锁”这个短语，而不是“锁定互斥锁”；现在我们将看到为什么。在短语“锁定互斥锁”中，“锁定”是一个*动词*；这种说法与
    C++ 代码 `mtx.lock()` 完全对应。但在短语“在互斥锁上获取锁”中，“锁定”是一个*名词*。让我们发明一个类型，将“锁定”的概念具体化；也就是说，将其变成一个名词（一个
    RAII 类）而不是动词（一个非 RAII 类的方法）：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As suggested by the name, `std::unique_lock<M>` is a "unique ownership" RAII
    class, similar in spirit to `std::unique_ptr<T>`. If you stick to using the noun
    `unique_ptr` instead of the verbs `new` and `delete`, you'll never forget to free
    a pointer; and if you stick to using the noun `unique_lock` instead of the verbs
    `lock` and `unlock`, you'll never forget to release a mutex lock.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，`std::unique_lock<M>` 是一个“唯一所有权”RAII 类，在精神上类似于 `std::unique_ptr<T>`。如果你坚持使用名词
    `unique_ptr` 而不是动词 `new` 和 `delete`，你就永远不会忘记释放指针；同样，如果你坚持使用名词 `unique_lock` 而不是动词
    `lock` 和 `unlock`，你就永远不会忘记释放互斥锁。
- en: '`std::unique_lock<M>` does expose the member functions `.lock()` and `.unlock()`,
    but generally you will not need to use those. They can be useful if you need to
    acquire or release a lock in the middle of a block of code, far away from the
    natural point of destruction of the `unique_lock` object. We will also see in
    the next section a function that takes as a parameter a locked `unique_lock`,
    which the function unlocks and re-locks as part of its functionality.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::unique_lock<M>` 确实公开了成员函数 `.lock()` 和 `.unlock()`，但通常你不需要使用这些。如果需要在代码块中间获取或释放锁，远离
    `unique_lock` 对象的自然销毁点，它们可能是有用的。我们将在下一节中看到一个接受已锁定 `unique_lock` 作为参数的函数，该函数在功能的一部分中解锁并重新锁定。'
- en: Notice that because `unique_lock` is movable, it must have a "null" or "empty"
    state, just like `unique_ptr`. In most cases, you won't need to move your locks
    around; you'll just unconditionally take the lock at the start of some scope,
    and unconditionally release it at the end of the scope. For this use-case, there's
    `std::lock_guard<M>`. `lock_guard` is much like `unique_lock`, but it is not movable,
    nor does it have the `.lock()` and `.unlock()` member functions. Therefore, it
    doesn't need to carry around an `m_locked` member, and its destructor can unconditionally
    unlock the mutex the object has been guarding, without any extra tests.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，因为 `unique_lock` 是可移动的，它必须有一个“null”或“empty”状态，就像 `unique_ptr` 一样。在大多数情况下，你不需要移动你的锁；你只需在某个作用域的开始无条件地获取锁，并在作用域的末尾无条件地释放它。对于这种用例，有
    `std::lock_guard<M>`。`lock_guard` 与 `unique_lock` 很相似，但它不可移动，也没有 `.lock()` 和 `.unlock()`
    成员函数。因此，它不需要携带 `m_locked` 成员，并且它的析构函数可以在没有任何额外测试的情况下无条件地解锁它所保护的互斥锁。
- en: 'In both cases (`unique_lock` and `lock_guard`), the class template is parameterized
    on the kind of mutex being locked. (We''ll look at a couple more kinds of mutexes
    in a minute, but almost invariably, you''ll want to use `std::mutex`.) C++17 has
    a new language feature called *class template argument deduction* that, in most
    cases, allows you to elide the template parameter: to write simply `std::unique_lock`
    instead of `std::unique_lock<std::mutex>`, for example. This is one of the very
    few cases where I would personally recommend relying on class template argument
    deduction, because writing out the parameter type `std::mutex` really adds so
    little information for your reader.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况（`unique_lock`和`lock_guard`）中，类模板是根据被锁定的互斥锁的类型进行参数化的。（我们将在下一分钟查看更多种类的互斥锁，但几乎总是，你将想要使用`std::mutex`。）C++17有一个新的语言特性叫做*类模板参数推导*，在大多数情况下，它允许你省略模板参数：例如，简单地写`std::unique_lock`而不是`std::unique_lock<std::mutex>`。这是我个人会推荐依赖类模板参数推导的极少数情况之一，因为写出参数类型`std::mutex`对读者来说真的增加了很少的信息。
- en: 'Let''s see some examples of `std::lock_guard`, with and without class template
    argument deduction:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`std::lock_guard`的一些示例，包括带有和不带有类模板参数推导的情况：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Before we can see similarly practical examples of `std::unique_lock`, we'll
    have to explain a good reason to use `std::unique_lock` in the first place.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够看到`std::unique_lock`的类似实际示例之前，我们首先需要解释为什么最初要使用`std::unique_lock`。
- en: Always associate a mutex with its controlled data
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总是将与受控数据关联的互斥锁
- en: Consider the following sketch of a thread-safe `StreamingAverage` class. There
    is a bug here; can you find it?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下线程安全的`StreamingAverage`类的草图。这里有一个bug；你能找到它吗？
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The bug is the line `A`, which writes to `this->m_count` in the producer thread,
    races with line `D`, which reads from `this->m_count` in the consumer thread.
    Line `A` correctly takes a lock on `this->m_mtx` before writing, but line `D`
    fails to take a similar lock, which means that it will happily barge in and attempt
    to read `m_count` even while line `A` is writing to it.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: bug是行`A`，在生产者线程中写入`this->m_count`，与行`D`在消费者线程中读取`this->m_count`发生竞争。行`A`在写入之前正确地锁定`this->m_mtx`，但行`D`未能采取类似的锁定，这意味着它将愉快地闯入并尝试读取`m_count`，即使行`A`正在写入它。
- en: Lines `B` and `C` look superficially similar, which is probably how the bug
    originally crept in. Line `C` doesn't need to take a lock; why should line `D`
    have to? Well, line `C` is called only from the consumer thread, which is the
    same thread that writes to `m_last_average` on line `B`. Since lines `B` and `C`
    are executed only by the single consumer thread, they can't both be executed simultaneously--at
    least as long as the rest of the program conforms to the comments! (Let's assume
    the code comments are correct. This is often sadly untrue in practice, but for
    the sake of this example let's assume it.)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 行`B`和`C`表面上看起来很相似，这可能是bug最初悄悄进入的原因。行`C`不需要加锁；为什么行`D`必须加锁呢？好吧，行`C`只被消费者线程调用，而这个线程与在行`B`上写入`m_last_average`的线程是同一个。由于行`B`和`C`只由单个消费者线程执行，它们不能同时执行--至少在程序的其他部分遵守注释的情况下是这样！（让我们假设代码注释是正确的。在实践中，这通常很遗憾地不正确，但为了这个例子，让我们假设它是正确的。）
- en: 'We have a recipe for confusion here: Locking `m_mtx` is required when touching
    `m_sum` or `m_count`, but it is not required when touching `m_last_average`. If
    this class becomes even more complicated, it might even have several mutexes involved
    (although at that point, it would clearly be violating the Single Responsibility
    Principle and would probably benefit from refactoring into smaller components).
    Therefore, a very good practice when dealing with mutexes is to place the mutex
    in the tightest possible relationship to the variables it "guards." One way to
    do this is simply via careful naming:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里有一个混淆的配方：当接触`m_sum`或`m_count`时需要锁定`m_mtx`，但当接触`m_last_average`时则不需要。如果这个类变得更加复杂，它甚至可能涉及多个互斥锁（尽管在那个阶段，它显然违反了单一职责原则，并且可能从重构为更小的组件中受益）。因此，在处理互斥锁时，一个非常好的实践是将互斥锁放置在与它“保护”的变量最紧密的关系中。一种方法是通过仔细命名来实现这一点：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'A better way is via a nested struct definition:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的方法是使用嵌套结构定义：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The hope above is that when the programmer is forced to write `this->m_guarded_sc.sum`,
    it reminds him to make sure he's already gotten a lock on `this->m_guarded_sc.mtx`.
    We could use the GNU extension of "anonymous struct members" to avoid retyping
    `m_guarded_sc` all over our code; but this would defeat the purpose of this approach,
    which is to make sure that every place the data is accessed *must* use the word
    "guarded," reminding the programmer to take that lock on `this->m_guarded_sc.mtx`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上述希望是，当程序员被迫编写`this->m_guarded_sc.sum`时，它会提醒他确保他已经获取了`this->m_guarded_sc.mtx`的锁。我们可以使用GNU的“匿名结构成员”扩展来避免在我们的代码中重复输入`m_guarded_sc`；但这样会违背这种方法的目的，即确保每个访问数据的地方都必须使用“guarded”这个词，提醒程序员在`this->m_guarded_sc.mtx`上获取那个锁。
- en: 'An even more bulletproof, but somewhat inflexible, approach is to place the
    mutex in a class that allows access to its private members only when the mutex
    is locked, by returning an RAII handle. The handle-returning class would look
    more or less like this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更加牢不可破但相对不灵活的方法是将互斥锁放在一个类中，该类仅在互斥锁被锁定时允许访问其私有成员，通过返回一个RAII句柄。返回句柄的类看起来大致如下：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And our `StreamingAverage` class could use it like this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`StreamingAverage`类可以这样使用它：
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This pattern is included in Facebook's Folly library under the name `folly::Synchronized<T>`,
    and many more variations on it are available in Ansel Sermersheim and Barbara
    Geller's "libGuarded" template library.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式包含在Facebook的Folly库中，名为`folly::Synchronized<T>`，Ansel Sermersheim和Barbara
    Geller的“libGuarded”模板库中还有更多基于它的变体。
- en: Notice the use of `std::unique_lock<std::mutex>` in the `Handle` class! We're
    using `unique_lock` here, not `lock_guard`, because we want the ability to pass
    this lock around, return it from functions, and so on--so it needs to be movable.
    This is the main reason you'd reach into your toolbox for `unique_lock`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在`Handle`类中使用了`std::unique_lock<std::mutex>`！我们在这里使用`unique_lock`而不是`lock_guard`，因为我们希望有传递这个锁、从函数返回它等功能，因此它需要是可移动的。这就是你会在工具箱里找到`unique_lock`的主要原因。
- en: 'Do be aware that this pattern does not solve all lock-related bugs--it solves
    only the simplest "forget to lock the mutex" cases--and it might encourage programming
    patterns that lead to more concurrency bugs of other types. For example, consider
    the following rewrite of `StreamingAverage::get_current_average`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种模式并不能解决所有与锁相关的错误——它只解决了最简单的“忘记锁定互斥锁”的情况——并且可能会鼓励导致更多其他类型并发错误的编程模式。例如，考虑以下对`StreamingAverage::get_current_average`的重新编写：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Because of the two calls to `m_sc.lock()`, there is a gap between the read
    of `m_sum` and the read of `m_count`. If the producer thread calls `add_value`
    during this gap, we will compute an incorrect average (too low by a factor of
    `1 / m_count`). And if we try to "fix" this bug by taking a lock around the entire
    computation, we''ll find ourselves in deadlock:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有两个`m_sc.lock()`调用，`m_sum`的读取和`m_count`的读取之间存在一个间隙。如果生产线程在这个间隙期间调用`add_value`，我们将计算出一个错误的平均值（比实际低一个`1
    / m_count`的因子）。如果我们尝试通过在整个计算周围获取锁来“修复”这个错误，我们会发现自己陷入了死锁：
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The line marked `LOCK 1` causes the mutex to become locked; then, on the line
    marked `LOCK 2`, we try to lock the mutex again. The general rule with mutexes
    is, if you're trying to lock a mutex and it's already locked, then you must *block*
    and wait for it to become unlocked. So our thread blocks and waits for the mutex
    to unlock--which will never happen, since the lock is being held by our own thread!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 标记为`LOCK 1`的行会导致互斥锁被锁定；然后，在标记为`LOCK 2`的行上，我们尝试再次锁定互斥锁。关于互斥锁的一般规则是，如果你试图锁定一个已经锁定的互斥锁，你必须*阻塞*并等待它解锁。所以我们的线程会阻塞并等待互斥锁解锁——但这是不可能发生的，因为锁是由我们自己的线程持有的！
- en: This problem (deadlock with oneself) should generally be dealt with by careful
    programming--that is, you should try not to take locks you already hold! But if
    taking locks this way is unavoidably part of your design, then the standard library
    has your back, so let's talk about `recursive_mutex`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题（自死锁）通常应该通过仔细的编程来解决——也就是说，你应该尽量避免获取你已经持有的锁！但如果以这种方式获取锁不可避免地成为你设计的一部分，那么标准库会支持你，所以让我们来谈谈`recursive_mutex`。
- en: Special-purpose mutex types
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特殊用途的互斥锁类型
- en: Recall that `std::lock_guard<M>` and `std::unique_lock<M>` are parameterized
    on the mutex type. So far we've seen only `std::mutex`. However, the standard
    library does contain a few other mutex types which can be useful in special circumstances.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，`std::lock_guard<M>`和`std::unique_lock<M>`是根据互斥锁类型参数化的。到目前为止，我们只看到了`std::mutex`。然而，标准库确实包含了一些其他互斥锁类型，在特殊情况下可能很有用。
- en: '`std::recursive_mutex` is like `std::mutex`, but remembers *which* thread has
    locked it. If that particular thread tries to lock it a second time, the recursive
    mutex will merely increment an internal reference count of "how many times I''ve
    been locked." If some other thread tries to lock the recursive mutex, that thread
    will block until the original thread has unlocked the mutex the appropriate number
    of times.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::recursive_mutex` 类似于 `std::mutex`，但它会记住 *哪个* 线程已经锁定了它。如果该特定线程尝试再次锁定它，递归互斥锁将仅增加内部引用计数“我已被锁定的次数”。如果其他线程尝试锁定递归互斥锁，该线程将阻塞，直到原始线程已适当地解锁互斥锁。'
- en: '`std::timed_mutex` is like `std::mutex`, but is aware of the passage of time.
    It has as member functions not only the usual `.try_lock()`, but also `.try_lock_for()`
    and `.try_lock_until()`, which interact with the standard `<chrono>` library.
    Here''s an example of `try_lock_for`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::timed_mutex` 类似于 `std::mutex`，但它能够感知时间的流逝。它不仅具有常用的 `.try_lock()` 成员函数，还有
    `.try_lock_for()` 和 `.try_lock_until()` 成员函数，这些函数与标准 `<chrono>` 库交互。下面是 `try_lock_for`
    的一个示例：'
- en: '[PRE22]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And here''s an example of `try_lock_until`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 `try_lock_until` 的一个示例：
- en: '[PRE23]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Incidentally, the `count_ms` function being used here is just a little lambda
    that factors out some of the usual `<chrono>` boilerplate:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，这里使用的 `count_ms` 函数只是一个提取了一些 `<chrono>` 常用模板代码的小型 lambda 表达式：
- en: '[PRE24]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In both of the preceding examples, pay attention to our use of `std::atomic<bool>`
    to synchronize threads `A` and `B`. We simply initialize the atomic variable to
    `false`, and then loop until it becomes `true`. The body of the polling loop is
    a call to `std::this_thread::sleep_for`, which is a sufficient hint to the compiler
    that the value of the atomic variable might change. Be careful never to write
    a polling loop that does not contain a sleep, because in that case the compiler
    is within its rights to collapse all the consecutive loads of `ready` down into
    a single load and a (necessarily infinite) loop.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述两个示例中，请注意我们使用 `std::atomic<bool>` 来同步线程 `A` 和 `B` 的方式。我们只需将原子变量初始化为 `false`，然后循环直到它变为
    `true`。轮询循环的主体是调用 `std::this_thread::sleep_for`，这足以向编译器暗示原子变量的值可能会改变。务必注意永远不要编写不包含睡眠的轮询循环，因为在这种情况下，编译器有权将所有连续的
    `ready` 加载合并为单个加载和一个（必然是无限期的）循环。
- en: '`std::recursive_timed_mutex` is like you took `recursive_mutex` and `timed_mutex`
    and smushed them together; it provides the "counting" semantics of `recursive_mutex`,
    *plus* the `try_lock_for` and `try_lock_until` methods of `timed_mutex`.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::recursive_timed_mutex` 就像是将 `recursive_mutex` 和 `timed_mutex` 合并在一起；它提供了
    `recursive_mutex` 的“计数”语义，*加上* `timed_mutex` 的 `try_lock_for` 和 `try_lock_until`
    方法。'
- en: '`std::shared_mutex` is perhaps poorly named. It implements behavior that in
    most concurrency textbooks would be called a *read-write lock* (also known as
    a *rwlock* or *readers-writer lock*). The defining characteristic of a read-write
    lock, or `shared_mutex`, is that it can be "locked" in two different ways. You
    can take a normal exclusive ("write") lock by calling `sm.lock()`, or you can
    take a non-exclusive ("read") lock by calling `sm.lock_shared()`. Many different
    threads are allowed to take read locks at the same time; but if *anybody* is reading,
    then *nobody* can be writing; and if *anybody* is writing, then *nobody* can be
    doing anything else (neither reading nor writing). These happen to be fundamentally
    the same rules that define "race conditions" in the C++ memory model: two threads
    reading from the same object simultaneously is fine, as long as no thread is writing
    to it at the same time. What `std::shared_mutex` adds to the mix is safety: it
    ensures that if anyone *does* try to write (at least if they play nice and take
    a write lock on the `std::shared_mutex` first), they''ll block until all the readers
    have exited and it''s safe to write.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::shared_mutex` 的命名可能不太恰当。它实现的行为在大多数并发教科书中被称为 *读写锁*（也称为 *rwlock* 或 *readers-writer
    lock*）。读写锁或 `shared_mutex` 的定义特征是它可以以两种不同的方式“锁定”。你可以通过调用 `sm.lock()` 来获取一个普通的排他性（“写入”）锁，或者你可以通过调用
    `sm.lock_shared()` 来获取一个非排他性（“读取”）锁。许多不同的线程可以同时获取读取锁；但如果 *任何人* 正在读取，那么 *任何人* 都不能写入；如果
    *任何人* 正在写入，那么 *任何人* 都不能进行其他操作（既不能读取也不能写入）。这些恰好是定义 C++ 内存模型中“竞态条件”的基本规则：如果有两个线程同时从同一个对象读取，这是可以的，只要没有线程同时写入。`std::shared_mutex`
    增加的是安全性：它确保如果有人 *确实* 尝试写入（至少如果他们表现得很好，首先在 `std::shared_mutex` 上获取写入锁），他们将阻塞，直到所有读取者都已退出并且安全写入。'
- en: '`std::unique_lock<std::shared_mutex>` is the noun corresponding to an exclusive
    ("write") lock on a `std::shared_mutex`. As you might expect, the standard library
    also provides `std::shared_lock<std::shared_mutex>` to reify the idea of a non-exclusive
    ("read") lock on a `std::shared_mutex`.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::unique_lock<std::shared_mutex>` 是对应于 `std::shared_mutex` 上的独占（“写”）锁的名词。正如你所期望的，标准库也提供了
    `std::shared_lock<std::shared_mutex>` 来具体化 `std::shared_mutex` 上的非独占（“读”）锁的概念。'
- en: Upgrading a read-write lock
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 升级读写锁
- en: Suppose you have a read lock on a `shared_mutex` (that is to say, you have a
    `std::shared_lock<std::shared_mutex> lk` such that `lk.owns_lock()`), and you
    want to get a write lock. Can you "upgrade" your lock?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个 `shared_mutex` 的读锁（也就是说，你有一个 `std::shared_lock<std::shared_mutex> lk`，使得
    `lk.owns_lock()`），并且你想要获取写锁。你能“升级”你的锁吗？
- en: No, you can't. Consider what would happen if threads `A` and `B` both hold read
    locks, and simultaneously attempt to upgrade to write locks without first releasing
    their read locks. Neither one would be able to acquire a write lock, and so they'd
    deadlock with each other.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 不可以。考虑一下，如果线程 `A` 和 `B` 都持有读锁，并且同时尝试升级到写锁而不先释放它们的读锁会发生什么。它们两个都无法获取写锁，因此它们会相互死锁。
- en: 'There *are* third-party libraries that attempt to solve this problem, such
    as `boost::thread::upgrade_lock`, which works with `boost::thread::shared_mutex`;
    but they are outside the scope of this book. The standard solution is that if
    you hold a read lock and want a write lock, you must release your read lock and
    then go stand in line for a write lock with everyone else:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有第三方库试图解决这个问题，例如 `boost::thread::upgrade_lock`，它与 `boost::thread::shared_mutex`
    一起工作；但它们超出了本书的范围。标准的解决方案是，如果你持有读锁并想要写锁，你必须释放你的读锁，然后和其他人一样排队等待写锁：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Downgrading a read-write lock
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降级读写锁
- en: Suppose you have an exclusive write lock on a `shared_mutex` and you want to
    get a read lock. Can you "downgrade" your lock?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个 `shared_mutex` 的独占写锁，并且你想要获取读锁。你能“降级”你的锁吗？
- en: 'In principle the answer is yes, it should be possible to downgrade a write
    lock to a read lock; but in standard C++17 the answer is no, you can''t do it
    directly. As in the upgrade case, you can use `boost::thread::shared_mutex`. The
    standard solution is that if you hold a write lock and want a read lock, you must
    release your write lock and then go stand in line for a read lock with everyone
    else:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上答案是肯定的，应该可以降级写锁到读锁；但在标准 C++17 中，答案是不了，你不能直接这样做。与升级的情况一样，你可以使用 `boost::thread::shared_mutex`。标准的解决方案是，如果你持有写锁并想要读锁，你必须释放你的写锁，然后和其他人一样排队等待读锁：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see from these examples, C++17's `std::shared_mutex` is a bit half-baked
    at the moment. If your architectural design calls for a read-write lock, I strongly
    recommend using something like `boost::thread::shared_mutex`, which comes "batteries
    included."
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如这些示例所示，C++17 的 `std::shared_mutex` 目前有些半成品。如果你的架构设计需要读写锁，我强烈建议使用类似 `boost::thread::shared_mutex`
    的东西，它“电池组”齐全。
- en: You may have noticed that since new readers can come in while a read lock is
    held, but new writers cannot, it is conceivable and even likely for a prospective
    writer thread to be "starved" by a steady stream of prospective readers, unless
    the implementation goes out of its way to provide a strong "no starvation" guarantee.
    `boost::thread::shared_mutex` provides such a guarantee (at least, it avoids starvation
    if the underlying operating system's scheduler does). The standard wording for
    `std::shared_mutex` provides no such guarantee, although any implementation that
    allowed starvation in practice would be considered a pretty poor one. In practice
    you'll find that your standard library vendor's implementation of `shared_mutex`
    is pretty close to the Boost one, except for the missing upgrade/downgrade functionality.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，由于在持有读锁的同时可能会有新的读者加入，但不会有新的写者，因此一个潜在的写者线程可能会因为连续的潜在读者流而“饿死”，除非实现者特意提供强有力的“无饿死”保证。`boost::thread::shared_mutex`
    提供了这样的保证（至少，它避免了饿死，如果底层操作系统的调度器这样做的话）。`std::shared_mutex` 的标准措辞没有提供这样的保证，尽管任何在实践中允许饿死的实现都会被认为是非常差的。实际上，你会发现你的标准库供应商对
    `shared_mutex` 的实现非常接近 Boost 的实现，除了缺少升级/降级功能。
- en: Waiting for a condition
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等待条件
- en: In the section titled "Special-purpose mutex types," we launched a task in a
    separate thread and then needed to wait until a certain bit of initialization
    was done before continuing. We used a polling loop around a `std::atomic<bool>`
    in that case. But there are better ways to wait!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在标题为“专用互斥锁类型”的部分，我们在一个单独的线程中启动了一个任务，然后需要在继续之前等待某些初始化完成。在那个情况下，我们使用了一个围绕`std::atomic<bool>`的轮询循环。但还有更好的等待方式！
- en: The problem with our 50-millisecond polling loop is that it *never* spends the
    right amount of time asleep. Sometimes our thread will wake up, but the condition
    it's waiting for hasn't been satisfied, so it'll go back to sleep--that means
    we didn't sleep long enough the first time. Sometimes our thread will wake up
    and see that the condition it's waiting for *has* been satisfied, sometime in
    the past 50 milliseconds, but we don't know how long ago--that means we've *overslept*
    by about 25 milliseconds on average. Whatever happens, the chance that we slept
    *just precisely the right amount of time* is slim to none.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们50毫秒的轮询循环的问题在于它**从未**在睡眠中花费正确的时间。有时我们的线程会醒来，但它等待的条件还没有满足，所以它会再次入睡——这意味着我们第一次没有睡够。有时我们的线程会醒来，看到它等待的条件在过去50毫秒中的某个时刻已经满足，但我们不知道具体是多久以前——这意味着我们平均超睡大约25毫秒。无论发生什么，我们恰好睡够正确时间的几率几乎为零。
- en: So, if we don't want to waste time, the right thing to do is to avoid polling
    loops. The standard library provides a way to wait just the *right* amount of
    time; it's called `std::condition_variable`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们不想浪费时间，正确的做法是避免轮询循环。标准库提供了一种等待恰好正确时间的方法；它被称为`std::condition_variable`。
- en: Given a variable `cv` of type `std::condition_variable`, our thread can "wait
    on" `cv` by calling `cv.wait(lk)`; that puts our thread to sleep. Calling `cv.notify_one()`
    or `cv.notify_all()` wakes up one, or all of, the threads currently waiting on
    `cv`. However, this is not the only way that those threads might wake up! It's
    possible that an interrupt from outside (such as a POSIX signal) might jar your
    thread awake without anybody's having called `notify_one`. This phenomenon is
    called a *spurious wakeup*. The usual way to guard against spurious wakeups is
    to check your condition when you wake up. For example, if you're waiting for some
    input to arrive in a buffer `b`, then when you wake up, you ought to check `b.empty()`
    and, if it's empty, go back to waiting.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个类型为`std::condition_variable`的变量`cv`，我们的线程可以通过调用`cv.wait(lk)`来“等待”`cv`；这将使我们的线程进入睡眠状态。调用`cv.notify_one()`或`cv.notify_all()`会唤醒一个或所有当前正在等待`cv`的线程。然而，这不是唤醒那些线程的唯一方式！可能来自外部的中断（例如POSIX信号）可能会在没有任何人调用`notify_one`的情况下将你的线程唤醒。这种现象被称为**虚假唤醒**。防止虚假唤醒的常用方法是醒来时检查你的条件。例如，如果你正在等待某个输入到达缓冲区`b`，那么当你醒来时，你应该检查`b.empty()`，如果它是空的，就回去等待。
- en: By definition, some other thread is going to be putting that data into `b`;
    so when you read `b.empty()`, you'd better do it under some kind of mutex. Which
    means the first thing you'll do when you wake up is take a lock on that mutex,
    and the last thing you'll do when you go back to sleep is release your lock on
    that mutex. (In fact, you need to release your lock on that mutex atomically with
    the going-to-sleep operation, so that nobody can slip in, modify `b`, and call
    `cv.notify_one()` before you've managed to get to sleep.) This chain of logic
    leads us to the reason that `cv.wait(lk)` takes that parameter `lk`--it's a `std::unique_lock<std::mutex>`
    that will be released upon going to sleep and regained upon awaking!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，其他线程将会把数据放入`b`中；因此，当你读取`b.empty()`时，你最好在某种类型的互斥锁下进行。这意味着当你醒来时，你首先会锁定那个互斥锁，当你再次入睡时，你最后会释放对那个互斥锁的锁定。（实际上，你需要原子性地在入睡操作中释放那个互斥锁，这样就没有人可以在你成功入睡之前溜进来，修改`b`并调用`cv.notify_one()`。）这个逻辑链引导我们理解为什么`cv.wait(lk)`需要那个参数`lk`——它是一个`std::unique_lock<std::mutex>`，在入睡时会释放，在醒来时会重新获得！
- en: 'Here''s an example of waiting for some condition to be satisfied. First the
    simple but wasteful polling loop on a `std::atomic` variable:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个等待某些条件满足的例子。首先是一个简单的但低效的轮询循环，针对一个`std::atomic`变量：
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And now the preferable and more efficient `condition_variable` implementation:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是更受欢迎且更高效的`condition_variable`实现：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If we''re waiting to read from a structure protected by a read-write lock (that
    is, a `std::shared_mutex`), then we don''t want to pass in a `std::unique_lock<std::mutex>`;
    we want to pass in a `std::shared_lock<std::shared_mutex>`. We can do this, if
    (and sadly only if) we plan ahead and define our condition variable to be of type
    `std::condition_variable_any` instead of `std::condition_variable`. In practice,
    there is unlikely to be any performance difference between `std::condition_variable_any`
    and `std::condition_variable`, which means you should choose between them based
    on your program''s needs, or, if either one would serve, then based on the clarity
    of the resulting code. Generally this means saving four characters and using `std::condition_variable`.
    However, notice that because of the layer of insulating abstraction provided by
    `std::shared_lock`, the actual code for waiting on `cv` under a read-write lock
    is almost identical to the code for waiting on `cv` under a plain old mutex. Here
    is the read-write lock version:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在等待从受读写锁保护的结构中读取（即，一个`std::shared_mutex`），那么我们不想传递一个`std::unique_lock<std::mutex>`；我们希望传递一个`std::shared_lock<std::shared_mutex>`。如果我们提前计划并定义我们的条件变量为`std::condition_variable_any`类型而不是`std::condition_variable`类型，我们可以这样做（但遗憾的是，只有在这种情况下才能做到）。实际上，`std::condition_variable_any`和`std::condition_variable`之间可能没有任何性能差异，这意味着你应该根据你的程序需求来选择它们，或者，如果两者都可以满足需求，那么根据代码的清晰度来选择。一般来说，这意味着节省四个字符并使用`std::condition_variable`。然而，请注意，由于`std::shared_lock`提供的隔离抽象层，在读写锁下等待`cv`的实际代码几乎与在普通互斥锁下等待`cv`的代码相同。以下是读写锁版本的代码：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This is perfectly correct code, and as efficient as it can be. However, manually
    fiddling with mutex locks and condition variables is almost as dangerous to one's
    health as fiddling with raw mutexes or raw pointers. We can do better! The better
    solution is the subject of our next section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一段完全正确且尽可能高效的代码。然而，手动操作互斥锁和条件变量几乎和直接操作原始互斥锁或原始指针一样危险。我们可以做得更好！更好的解决方案是我们下一节的主题。
- en: Promises about futures
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于未来的承诺
- en: 'If you haven''t encountered concurrent programming topics before, the last
    few sections probably got progressively more and more challenging. Mutexes are
    pretty simple to understand because they model a familiar idea from daily life:
    getting exclusive access to some resource by putting a lock on it. Read-write
    locks (`shared_mutex`) aren''t much harder to understand. However, we then took
    a significant jump upward in esotericism with condition variables--which are hard
    to grasp partly because they seem to model not a noun (like "padlock") but a sort
    of prepositional verb phrase: "sleep until, but also, wake." Their opaque name
    doesn''t help much either.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前没有遇到过并发编程主题，那么最后几个部分可能越来越具有挑战性。互斥锁相对容易理解，因为它们模拟了日常生活中熟悉的概念：通过加锁来获取对某些资源的独占访问。读写锁（`shared_mutex`）也不难理解。然而，我们随后在神秘性方面迈出了重大的一步，条件变量很难掌握，部分原因在于它们似乎模拟的不是名词（如“挂锁”），而是一种介词动词短语：“直到……但也要……唤醒。”它们晦涩的名字也没有多大帮助。
- en: 'Now we continue our journey into concurrent programming with a topic that may
    be unfamiliar even if you''ve taken an undergraduate course in concurrent programming,
    but is well worth the learning: *promises* and *futures*.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续我们的并发编程之旅，探讨一个即使你已修过并行编程本科课程可能也感到陌生的主题：*承诺*和*未来*。
- en: 'In C++11, the types `std::promise<T>` and `std::future<T>` always appear in
    pairs. Someone coming from the Go language might think of a promise-future pair
    as a sort of *channel*, in that if one thread shoves a value (of type `T`) into
    the "promise" side of the pair, that value will eventually emerge at the "future"
    side (which is typically in a different thread by then). However, promise-future
    pairs are also like unstable wormholes: as soon as you''ve shoved a single value
    through the wormhole, it collapses.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在C++11中，`std::promise<T>`和`std::future<T>`类型总是成对出现。来自Go语言的人可能会把承诺-未来对看作是一种*通道*，即如果一个线程将一个值（类型为`T`）推入这对的“承诺”一侧，那么这个值最终会在“未来”一侧出现（那时通常在另一个线程中）。然而，承诺-未来对也像不稳定的时间隧道：一旦你将一个值推过这个隧道，它就会立即坍塌。
- en: We might say that a promise-future pair is like a directed, portable, one-shot
    wormhole. It's "directed" because you're allowed to shove data into only the "promise"
    side and retrieve data only via the "future" side. It's "portable" because if
    you own one end of the wormhole, you can move that end around and even move it
    between threads; you won't break the tunnel between the two ends. And it's "one-shot"
    because once you've shoved one piece of data into the "promise" end, you can't
    shove any more.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，promise-future对就像一个有方向的、可携带的、一次性的虫洞。它是有方向的，因为您只能将数据推入“promise”端，并通过“future”端检索数据。它是可携带的，因为如果您拥有虫洞的一端，您可以移动该端，甚至在不同线程之间移动它；您不会破坏两端之间的隧道。而且它是一次性的，因为一旦您将一块数据推入“promise”端，您就不能再推入更多。
- en: 'Another metaphor for the pair is suggested by their names: A `std::future<T>`
    is not actually a value of type `T`, but it is in some sense a *future* value--it
    will, at some point in the future, give you access to a `T`, but "not yet." (In
    this way, it is also something like a thread-safe `optional<T>`.) Meanwhile, a
    `std::promise<T>` object is like an unfulfilled promise, or an I-O-U. The holder
    of the promise object *promises* to put a value of type `T` into it at some point;
    if he doesn''t ever put in a value, then he''s "broken his promise."'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这对的另一个隐喻是由它们的名称所提出的：`std::future<T>`实际上不是类型`T`的值，但在某种意义上它是一个未来的值——它将在未来的某个时刻为您提供访问`T`的权限，但“尚未”。（以这种方式，它也类似于线程安全的`optional<T>`。）同时，`std::promise<T>`对象就像一个未履行的承诺，或者一个I-O-U。承诺对象的所有者承诺在某个时刻将类型`T`的值放入其中；如果他从未放入值，那么他就“违背了他的承诺”。
- en: Generally speaking, you use a promise-future pair by first creating a `std::promise<T>`,
    where `T` is the type of data you're planning to send through it; then creating
    the wormhole's "future" end by calling `p.get_future()`. When you're ready to
    fulfill the promise, you call `p.set_value(v)`. Meanwhile, in some other thread,
    when you're ready to retrieve the value, you call `f.get()`. If a thread calls
    `f.get()` before the promise has been fulfilled, that thread will block until
    the promise is fulfilled and the value is ready to retrieve. On the other hand,
    when the promise-holding thread calls `p.set_value(v)`, if nobody's waiting, that's
    fine; `set_value` will just record the value `v` in memory so that it's ready
    and waiting whenever anyone *does* ask for it via `f.get()`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，您首先创建一个`std::promise<T>`，其中`T`是您计划通过它发送的数据类型；然后通过调用`p.get_future()`创建虫洞的“future”端。当您准备好履行承诺时，您调用`p.set_value(v)`。同时，在另一个线程中，当您准备好检索值时，您调用`f.get()`。如果线程在承诺得到履行之前调用`f.get()`，那么该线程将阻塞，直到承诺得到履行且值准备好检索。另一方面，当持有承诺的线程调用`p.set_value(v)`时，如果没有人在等待，那也无所谓；`set_value`只需将值`v`记录在内存中，以便任何人通过`f.get()`请求时都能准备好并等待。
- en: Let's see `promise` and `future` in action!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`promise`和`future`的实际应用！
- en: '[PRE30]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: (For the definition of `count_ms`, see the previous section, *Special-purpose
    mutex types*.)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: （有关`count_ms`的定义，请参阅上一节，*专用互斥类型*。）
- en: 'One nice detail about the standard library''s `std::promise` is that it has
    a specialization for `void`. The idea of `std::future<void>` might seem a little
    silly at first--what good is a wormhole if the only data type you can shove through
    it is a type with no values? But in fact `future<void>` is extremely useful, whenever
    we don''t care so much about the *value* that was received as about the fact that
    some signal was received at all. For example, we can use `std::future<void>` to
    implement yet a third version of our "wait for thread B to launch" code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 关于标准库的`std::promise`的一个不错的细节是，它为`void`类型有一个特化。`std::future<void>`的想法一开始可能看起来有点愚蠢——如果唯一可以推入虫洞的数据类型是没有值的类型，那么虫洞有什么用？但事实上`future<void>`非常有用，无论我们是否关心接收到的*值*，而是关心是否收到了信号。例如，我们可以使用`std::future<void>`来实现我们“等待线程B启动”代码的第三个版本：
- en: '[PRE31]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Compare this version to the code samples from the section titled "Waiting for
    a condition." This version is much cleaner! There's practically no cruft, no boilerplate
    at all. The "signal B's readiness" and "wait for B's readiness" operations both
    take only a single line of code. So this is definitely the preferred way to signal
    between a single pair of threads, as far as syntactic cleanliness is concerned.
    For yet a fourth way to signal from one thread to a group of threads, see this
    chapter's subsection titled "Identifying individual threads and the current thread."
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 将此版本与标题为“等待条件”的章节中的代码示例进行比较。这个版本要干净得多！实际上没有冗余，没有任何样板代码。"信号 B 的就绪"和"等待 B 的就绪"操作都只需要一行代码。因此，从语法清洁度的角度来看，这绝对是单对线程之间进行信号的最佳方式。至于从单个线程向一组线程发送信号的第四种方式，请参阅本章标题为“识别单个线程和当前线程”的小节。
- en: 'There *is* a price to pay for `std::future`, though. The price is dynamic memory
    allocation. You see, `promise` and `future` both need access to a shared storage
    location, so that when you store `42` in the promise side, you''ll be able to
    pull it out from the future side. (That shared storage location also holds the
    mutex and condition variable required for synchronizing between the threads. The
    mutex and condition variable haven''t disappeared from our code; they''ve just
    moved down a layer of abstraction so that we don''t have to worry about them.)
    So, `promise` and `future` both act as a sort of "handle" to this shared state;
    but they''re both movable types, so neither of them can actually hold the shared
    state as a member. They need to allocate the shared state on the heap, and hold
    pointers to it; and since the shared state isn''t supposed to be freed until *both*
    handles are destroyed, we''re talking about shared ownership via something like
    `shared_ptr` (see [Chapter 6](part0093.html#2OM4A0-2fdac365b8984feebddfbb9250eaf20d),
    *Smart Pointers*). Schematically, `promise` and `future` look like this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 `std::future` 有其代价。这个代价是动态内存分配。你看，`promise` 和 `future` 都需要访问一个共享存储位置，这样当你将
    `42` 存储在 `promise` 一侧时，你将能够从 `future` 一侧取出它。（这个共享存储位置还包含了线程间同步所需的互斥锁和条件变量。互斥锁和条件变量并没有从我们的代码中消失；它们只是向下移动了一层抽象层，这样我们就不必担心它们了。）因此，`promise`
    和 `future` 都充当了这种共享状态的“句柄”；但它们都是可移动类型，所以它们都不能作为成员持有共享状态。它们需要在堆上分配共享状态，并持有其指针；由于共享状态不应该在两个句柄都被销毁之前释放，我们谈论的是通过类似
    `shared_ptr`（见第 6 章 [Chapter 6](part0093.html#2OM4A0-2fdac365b8984feebddfbb9250eaf20d)，*智能指针*）的东西进行共享所有权。从图示上看，`promise`
    和 `future` 看起来是这样的：
- en: '![](img/00023.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00023.jpeg)'
- en: 'The shared state in this diagram will be allocated with `operator new`, unless
    you use a special "allocator-aware" version of the constructor `std::promise`.
    To use `std::promise` and `std::future` with an allocator of your choice, you''d
    write the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 此图中的共享状态将使用 `operator new` 分配，除非你使用特殊的“分配器感知”版本的构造函数 `std::promise`。要使用你选择的分配器与
    `std::promise` 和 `std::future` 一起使用，你应该编写以下代码：
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`std::allocator_arg` is defined in the `<memory>` header. See [Chapter 8](part0129.html#3R0OI0-2fdac365b8984feebddfbb9250eaf20d),
    *Allocators*, for the details of `MyAllocator`.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::allocator_arg` 在 `<memory>` 头文件中定义。有关 `MyAllocator` 的详细信息，请参阅第 8 章 [Chapter
    8](part0129.html#3R0OI0-2fdac365b8984feebddfbb9250eaf20d)，*分配器*。'
- en: Packaging up tasks for later
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将任务打包以供稍后使用
- en: Another thing to notice about the preceding diagram is that the shared state
    doesn't just contain an `optional<T>`; it actually contains a `variant<T, exception_ptr>`
    (for `variant` and `optional`, see [Chapter 5](part0074.html#26I9K0-2fdac365b8984feebddfbb9250eaf20d),
    *Vocabulary Types*). This implies that not only can you shove data of type `T`
    through the wormhole; you can also shove *exceptions* through. This is particularly
    convenient and symmetrical because it allows `std::future<T>` to represent all
    the possible outcomes of calling a function with the signature `T()`. Maybe it
    returns a `T`; maybe it throws an exception; and of course maybe it never returns
    at all. Similarly, a call to `f.get()` may return a `T`; or throw an exception;
    or (if the promise-holding thread loops forever) might never return at all. In
    order to shove an exception through the wormhole, you'd use the method `p.set_exception(ex)`,
    where `ex` is an object of type `std::exception_ptr` such as might be returned
    from `std::current_exception()` inside a catch handler.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 关于前面图表的另一个需要注意的事项是，共享状态不仅仅包含一个`optional<T>`；实际上它包含一个`variant<T, exception_ptr>`（关于`variant`和`optional`，请参阅[第5章](part0074.html#26I9K0-2fdac365b8984feebddfbb9250eaf20d)，*词汇类型*）。这意味着你不仅可以将类型为`T`的数据通过虫洞传递；你还可以传递*异常*。这特别方便且对称，因为它允许`std::future<T>`表示调用具有签名`T()`的函数的所有可能结果。也许它会返回一个`T`；也许它会抛出异常；当然，也许它根本不会返回。同样，调用`f.get()`可能会返回一个`T`；或者抛出异常；或者（如果持有承诺的线程无限循环）可能根本不会返回。为了通过虫洞传递异常，你会使用`p.set_exception(ex)`方法，其中`ex`是`std::exception_ptr`类型的对象，这种对象可能来自catch处理程序中的`std::current_exception()`。
- en: 'Let''s take a function of signature `T()` and package it up in a future of
    type `std::future<T>`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拿一个签名为`T()`的函数，并将其封装在类型为`std::future<T>`的未来中：
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This class superficially resembles the standard library type `std::packaged_task<R(A...)>`;
    the difference is that the standard library type takes arguments, and uses an
    extra layer of indirection to make sure that it can hold even move-only functor
    types. Back in [Chapter 5](part0074.html#26I9K0-2fdac365b8984feebddfbb9250eaf20d),
    *Vocabulary Types*, we showed you some workarounds for the fact that `std::function`
    can''t hold move-only function types; fortunately those workarounds are not needed
    when dealing with `std::packaged_task`. On the other hand, you''ll probably never
    have to deal with `std::packaged_task` in your life. It''s interesting mainly
    as an example of how to compose promises, futures, and functions together into
    user-friendly class types with externally very simple interfaces. Consider for
    a moment: The `simple_packaged_task` class above uses type-erasure in `std::function`,
    and then has the `std::promise` member, which is implemented in terms of `std::shared_ptr`,
    which does reference counting; and the shared state pointed to by that reference-counted
    pointer holds a mutex and a condition variable. That''s quite a lot of ideas and
    techniques packed into a very small volume! And yet the interface to `simple_packaged_task`
    is indeed simple: construct it with a function or lambda of some kind, then call
    `pt.get_future()` to get a future that you can `f.get()`; and meanwhile call `pt()`
    (probably from some other thread) to actually execute the stored function and
    shove the result through the wormhole into `f.get()`.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类在表面上类似于标准库类型`std::packaged_task<R(A...)>`；区别在于标准库类型接受参数，并使用额外的间接层来确保它可以持有甚至只移动的函数类型。回到[第5章](part0074.html#26I9K0-2fdac365b8984feebddfbb9250eaf20d)，*词汇类型*，我们向你展示了`std::function`不能持有只移动函数类型的一些解决方案；幸运的是，当处理`std::packaged_task`时，这些解决方案是不必要的。另一方面，你可能一生中都不需要处理`std::packaged_task`。它主要作为一个例子，展示了如何将承诺、未来和函数组合成用户友好的类类型，同时具有非常简单的接口。考虑一下：上面的`simple_packaged_task`类在`std::function`中使用类型擦除，然后有一个`std::promise`成员，该成员通过`std::shared_ptr`实现，它执行引用计数；那个引用计数的指针指向的共享状态包含一个互斥锁和一个条件变量。这相当多的想法和技术被压缩在一个非常小的体积中！然而，`simple_packaged_task`的接口确实很简单：用某种函数或lambda表达式构造它，然后调用`pt.get_future()`以获取一个你可以调用`f.get()`的未来；同时调用`pt()`（可能来自其他线程）以实际执行存储的函数并将结果通过虫洞传递到`f.get()`。
- en: If the stored function throws an exception, then `packaged_task` will catch
    that exception (in the promise-holding thread) and shove it into the wormhole.
    Then, whenever the other thread calls `f.get()` (or maybe it already called it
    and it's blocked inside `f.get()` right now), `f.get()` will throw that exception
    out into the future-holding thread. In other words, by using promises and futures,
    we can actually "teleport" exceptions across threads. The exact mechanism of this
    teleportation, `std::exception_ptr`, is unfortunately outside the scope of this
    book. If you do library programming in a codebase that uses a lot of exceptions,
    it is definitely worth becoming familiar with `std::exception_ptr`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存储函数抛出异常，那么`packaged_task`将捕获该异常（在持有承诺的线程中）并将其推入虫洞。然后，无论何时其他线程调用`f.get()`（或者它可能已经调用了，并且现在正阻塞在`f.get()`中），`f.get()`都会将异常抛出到持有未来的线程。换句话说，通过使用承诺和未来，我们实际上可以在线程之间“传送”异常。这种传送的确切机制`std::exception_ptr`，不幸的是超出了本书的范围。如果你在一个使用大量异常的代码库中进行库编程，那么熟悉`std::exception_ptr`绝对是值得的。
- en: The future of futures
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来的未来
- en: As with `std::shared_mutex`, the standard library's own version of `std::future`
    is only half-baked. A much more complete and useful version of `future` is coming,
    perhaps in C++20, and there are very many third-party libraries that incorporate
    the best features of the upcoming version. The best of these libraries include
    `boost::future` and Facebook's `folly::Future`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 与`std::shared_mutex`一样，标准库自己的`std::future`版本只是半成品。一个更完整、更有用的`future`版本可能出现在C++20中，并且有许多第三方库结合了即将推出的版本的最佳特性。其中最好的库包括`boost::future`和Facebook的`folly::Future`。
- en: 'The major problem with `std::future` is that it requires "touching down" in
    a thread after each step of a potentially multi-step computation. Consider this
    pathological usage of `std::future`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::future`的主要问题是它需要在多步计算中的每一步之后“降落”到线程中。考虑这种对`std::future`的病态使用：'
- en: '[PRE34]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Notice the line marked `DANGER`: each of the three thread bodies has the same
    bug, which is that they fail to catch and `.set_exception()` when an exception
    is thrown. The solution is a `try...catch` block, just like we used in our `simple_packaged_task`
    in the preceding section; but since that would get tedious to write out every
    time, the standard library provides a neat wrapper function called `std::async()`,
    which takes care of creating a promise-future pair and spawning a new thread.
    Using `std::async()`, we have this much cleaner-looking code:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意标记为`DANGER`的行：三个线程体中每个都存在相同的错误，即它们在抛出异常时未能捕获并调用`.set_exception()`。解决方案是一个`try...catch`块，就像我们在前一个部分中使用的`simple_packaged_task`一样；但由于每次都要写出来会变得很繁琐，标准库提供了一个整洁的包装函数`std::async()`，它负责创建承诺-未来对并创建一个新的线程。使用`std::async()`，我们有了这样更干净看起来更好的代码：
- en: '[PRE35]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: However, this code is cleaner only in its aesthetics; it's equally horrifically
    bad for the performance and robustness of your codebase. This is *bad* code!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这段代码在美学上更干净，但它对你的代码库的性能和健壮性同样糟糕。这是**糟糕**的代码！
- en: 'Every time you see a `.get()` in that code, you should think, "What a waste
    of a context switch!" And every time you see a thread being spawned (whether explicitly
    or via `async`), you should think, "What a possibility for the operating system
    to run out of kernel threads and for my program to start throwing unexpected exceptions
    from the constructor of `std::thread`!" Instead of either of the preceding codes,
    we''d prefer to write something like this, in a style that might look familiar
    to JavaScript programmers:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你在代码中看到`.get()`时，你应该想，“多么浪费上下文切换啊！”每次你看到线程被创建（无论是显式还是通过`async`），你应该想，“操作系统可能耗尽内核线程，我的程序可能从`std::thread`的构造函数开始抛出意外异常的可能性有多大！”我们宁愿不写前面的任何一种代码，而是写一些可能看起来熟悉JavaScript程序员的代码：
- en: '[PRE36]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, there are no calls to `.get()` except at the very end, when we have nothing
    to do but wait for the final answer; and there is one fewer thread spawned. Instead,
    before `f1` finishes its task, we attach a "continuation" to it, so that when
    `f1` does finish, the promise-holding thread can immediately segue right into
    working on the continuation task (if original task of `f1` threw an exception,
    we won't enter this continuation at all. The library should provide a symmetrical
    method, `f1.on_error(continuation)`, to deal with the exceptional codepath).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，除了在最后什么也不做只是等待最终答案时调用 `.get()` 之外，没有其他调用；并且产生的线程数量少了一个。相反，在 `f1` 完成任务之前，我们将其附加到一个“延续”上，这样当
    `f1` 完成时，持有承诺的线程可以立即过渡到继续任务（如果 `f1` 的原始任务抛出异常，我们根本不会进入这个延续。库应该提供一个对称的方法，`f1.on_error(continuation)`，来处理异常代码路径）。
- en: Something like this is already available in Boost; and Facebook's Folly library
    contains a particularly robust and fully featured implementation even better than
    Boost's. While we wait for C++20 to improve the situation, my advice is to use
    Folly if you can afford the cognitive overhead of integrating it into your build
    system. The single advantage of `std::future` is that it's standard; you'll be
    able to use it on just about any platform without needing to worry about downloads,
    include paths, or licensing terms.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的东西在 Boost 中已经可用；Facebook 的 Folly 库包含一个特别健壮且功能齐全的实现，甚至比 Boost 的还要好。在我们等待 C++20
    改善这种情况的同时，我的建议是，如果你能承担将其集成到你的构建系统中的认知开销，就使用 Folly。`std::future` 的单一优势在于它是标准的；你几乎可以在任何平台上使用它，而无需担心下载、包含路径或许可条款。
- en: Speaking of threads...
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 说到线程...
- en: Throughout this entire chapter, we've been using the word "thread" without ever
    defining exactly what we mean by it; and you've probably noticed that many of
    our multithreaded code examples have used the class type `std::thread` and the
    namespace `std::this_thread` without much explanation. We've been focusing on
    *how* to synchronize behavior between different threads of execution, but so far
    we have glossed over exactly *who* is doing the executing!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个这一章中，我们一直在使用“线程”这个词，而没有明确地定义我们所说的“线程”是什么；你可能已经注意到，我们许多多线程代码示例都使用了 `std::thread`
    类类型和 `std::this_thread` 命名空间，而没有太多解释。我们一直专注于*如何*在不同执行线程之间同步行为，但到目前为止，我们一直忽略了*谁*在执行！
- en: 'To put it another way: When execution reaches the expression `mtx.lock()`,
    where `mtx` is a locked mutex, the semantics of `std::mutex` say that the current
    thread of execution should block and wait. While that thread is blocked, what
    is happening? Our C++ program is still "in charge" of what''s going on, but clearly
    *this particular C++ code* is no longer executing; so who *is* executing? The
    answer is: another thread. We specify the existence of other threads, and the
    code we want them to execute, by using the standard library class `std::thread`,
    defined in the `<thread>` header.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 换个说法：当执行到达表达式 `mtx.lock()`，其中 `mtx` 是一个已锁定的互斥量时，`std::mutex` 的语义表明当前执行线程应该阻塞并等待。当这个线程阻塞时，发生了什么？我们的
    C++ 程序仍然“负责”正在发生的事情，但显然*这个特定的 C++ 代码*不再执行；那么*谁*在执行？答案是：另一个线程。我们通过使用标准库类 `std::thread`
    来指定其他线程的存在以及我们希望它们执行的操作，该类定义在 `<thread>` 头文件中。
- en: 'To spawn a new thread of execution, simply construct an object of type `std::thread`,
    and pass a single argument to the constructor: a lambda or function that tells
    you what code you want to run in the new thread. (Technically, you are allowed
    to pass multiple arguments; all arguments after the first will be passed along
    to the first argument as *its* function parameters, after undergoing `reference_wrapper`
    decay as described in [Chapter 5](part0074.html#26I9K0-2fdac365b8984feebddfbb9250eaf20d),
    *Vocabulary Types*. As of C++11, lambdas have made the extra arguments to the
    `thread` constructor unnecessary and even error-prone; I recommend avoiding them.)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个新的执行线程，只需构造一个 `std::thread` 类型的对象，并将单个参数传递给构造函数：一个 lambda 或函数，它告诉你你希望在新的线程中运行的代码。技术上，你可以传递多个参数；所有第一个参数之后的参数都将经过
    `reference_wrapper` 衰减（如第 5 章[26I9K0-2fdac365b8984feebddfbb9250eaf20d]中所述，*词汇类型*），然后作为*它的*函数参数传递。从
    C++11 开始，lambdas 使得 `thread` 构造函数的额外参数变得不必要，甚至可能出错；我建议避免使用它们。）
- en: The new thread will immediately start running; if you want it to "start up paused,"
    you'll have to build that functionality yourself using one of the synchronization
    tricks shown in the section titled "Waiting for a condition," or the alternative
    trick shown in "Identifying individual threads and the current thread."
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 新线程将立即开始运行；如果你想让它“启动时暂停”，你必须自己使用“等待条件”部分中展示的同步技巧之一或“识别单个线程和当前线程”中展示的替代技巧来构建该功能。
- en: 'The new thread will run through the code it''s given, and when it gets to the
    end of the lambda or function you provided to it, it will "become joinable." This
    idea is very similar to what happens with `std::future` when it "becomes ready":
    the thread has completed its computation and is ready to deliver the result of
    that computation to you. Just as with `std::future<void>`, the result of that
    computation is "valueless"; but the very fact that the computation *has finished*
    is valuable nonetheless--no pun intended!'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 新线程将执行它所提供的代码，当它到达你提供的 lambda 表达式或函数的末尾时，它将“变为可连接”。这个想法与 `std::future` 在“变为就绪”时发生的情况非常相似：线程已经完成了其计算，并准备好将计算结果传递给你。就像
    `std::future<void>` 一样，该计算的结果是“无值的”；但计算 *已经完成* 的这一事实仍然非常有价值——不是字面意义上的！
- en: Unlike `std::future<void>`, though, it is not permitted to destroy a `std::thread`
    object without fetching that valueless result. By default, if you destroy any
    new thread without dealing with its result, the destructor will call `std::terminate`,
    which is to say, it will bluntly kill your program. The way to avoid this fate
    is to indicate to the thread that you see and acknowledge its completion--"Good
    job, thread, well done!"--by calling the member function `t.join()`. Alternatively,
    if you do not expect the thread to finish (for example if it is a background thread
    running an infinite loop) or don't care about its result (for example if it represents
    some short-lived "fire and forget" task), you can dismiss it to the background--"Go
    away, thread, I don't want to hear from you again!"--via `t.detach()`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `std::future<void>` 不同，不允许在未获取无值结果的情况下销毁 `std::thread` 对象。默认情况下，如果你不处理任何新线程的结果就销毁它，析构函数将调用
    `std::terminate`，也就是说，它会直接杀死你的程序。避免这种命运的方法是通过调用成员函数 `t.join()` 来向线程表明你看到了并承认了它的完成——“干得好，线程，做得好！”——或者，如果你不期望线程完成（例如，如果它是一个运行无限循环的后台线程）或者不关心它的结果（例如，如果它代表一些短暂的“点火并忘记”任务），你可以将其发送到后台——“走开，线程，我不想再听到你的消息！”——通过
    `t.detach()`。
- en: 'Here are some complete examples of how to use `std::thread`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些使用 `std::thread` 的完整示例：
- en: '[PRE37]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Identifying individual threads and the current thread
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别单个线程和当前线程
- en: Objects of type `std::thread`, like every other type described in this chapter,
    do not support `operator==`. You can't directly ask "Are these two thread objects
    the same?" This also means that you can't use `std::thread` objects as the keys
    in an associative container such as `std::map` or `std::unordered_map`. However,
    you *can* ask about equality indirectly, via a feature called *thread-ids*.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 类型为 `std::thread` 的对象，就像本章中描述的每一种其他类型一样，不支持 `operator==`。你不能直接询问“这两个线程对象是否相同？”这也意味着你不能将
    `std::thread` 对象用作关联容器（如 `std::map` 或 `std::unordered_map`）中的键。然而，你可以通过一个称为 *thread-ids*
    的特性间接地询问相等性。
- en: The member function `t.get_id()` returns a unique identifier of type `std::thread::id`,
    which, although it is technically a class type, behaves an awful lot like an integer
    type. You can compare thread-ids using operators `<` and `==`; and you can use
    thread-ids as keys in associative containers. Another valuable feature of thread-id
    objects is that they can be *copied*, unlike `std::thread` objects themselves,
    which are move-only. Remember, each `std::thread` object represents an actual
    thread of execution; if you could copy `thread` objects, you would be "copying"
    threads of execution, which doesn't make a whole lot of sense--and would certainly
    lead to some interesting bugs!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 成员函数 `t.get_id()` 返回一个唯一的标识符，类型为 `std::thread::id`，尽管它技术上是一个类类型，但它的行为非常类似于整数类型。你可以使用
    `<` 和 `==` 操作符比较线程标识符；并且你可以将线程标识符用作关联容器的键。线程标识符对象的一个宝贵特性是它们可以被 *复制*，而 `std::thread`
    对象本身只能移动。记住，每个 `std::thread` 对象代表一个实际的执行线程；如果你可以复制 `thread` 对象，你就是在“复制”执行线程，这没有太多意义——而且肯定会引发一些有趣的错误！
- en: The third valuable feature of `std::thread::id` is that it is possible to get
    the thread-id of the *current* thread, or even of the main thread. From within
    a thread, there is no way to say "Please give me the `std::thread` object that
    manages this thread." (This would be a trick analogous to `std::enable_shared_from_this<T>`
    from [Chapter 6](part0093.html#2OM4A0-2fdac365b8984feebddfbb9250eaf20d), *Smart
    Pointers*; but as we've seen, such a trick requires support from the part of the
    library that creates managed resources--which in this case would be the constructor
    of `std::thread`.) And the main thread, the thread in which `main` begins execution,
    doesn't have a corresponding `std::thread` object at all. But it still has a thread-id!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, thread-ids are convertible in some implementation-defined manner to
    a string representation, which is guaranteed to be unique--that is, `to_string(id1)
    == to_string(id2)` if and only if `id1 == id2`. Unfortunately this string representation
    is exposed only via the stream operator (see [Chapter 9](part0144.html#49AH00-2fdac365b8984feebddfbb9250eaf20d),
    *Iostreams*); if you want to use the syntax `to_string(id1)` you need to write
    a simple wrapper function:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can get the thread-id of the current thread (including of the main thread,
    if that happens to be your current thread) by calling the free function `std::this_thread::get_id()`.
    Look carefully at the syntax! `std::thread` is the name of a class, but `std::this_thread`
    is the name of a *namespace*. In this namespace live some free functions (unassociated
    with any C++ class instance) that manipulate the current thread. `get_id()` is
    one of those functions. Its name was chosen to be reminiscent of `std::thread::get_id()`,
    but in fact it is a completely different function: `thread::get_id()` is a member
    function and `this_thread::get_id()` is a free function.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Using two thread-ids, you can find out, for example, which of an existing list
    of threads represents your current thread:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'What you cannot do, ever, is go the other direction; you cannot reconstruct
    the `std::thread` object corresponding to a given `std::thread::id`. Because if
    you could, you''d have two different objects in your program representing that
    thread of execution: the original `std::thread` wherever it is, and the one you
    just reconstructed from its thread-id. And you can never have two `std::thread`
    objects controlling the same thread.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'The two other free functions in the `std::this_thread` namespace are `std::this_thread::sleep_for(duration)`,
    which you''ve seen me use extensively in this chapter, and `std::this_thread::yield()`,
    which is basically the same thing as `sleep_for(0ms)`: it tells the runtime that
    it would be a good idea to context-switch to a different thread right now, but
    doesn''t connote any *particular* time delay on the current thread.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Thread exhaustion and std::async
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter''s section *The future of futures*, we introduced `std::async`,
    which is a simple wrapper around a thread constructor with the result captured
    into a `std::future`. Its implementation looks more or less like this:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的 *未来的未来* 节中，我们介绍了 `std::async`，它是一个围绕线程构造函数的简单包装器，结果被捕获到 `std::future` 中。它的实现看起来大致如下：
- en: '[PRE40]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Notice the commented-out lines indicating a special behavior "on destruction"
    of the `std::future` returned from `std::async`. This is a strange and awkward
    behavior of the standard library''s `std::async` implementation, and a good reason
    to avoid or reimplement `std::async` in your own code: The futures returned from
    `std::async` have destructors that call `.join()` on their underlying threads!
    This means that their destructors can block, and that the task certainly will
    not be "executing in the background" as you might naturally expect. If you call
    `std::async` and don''t assign the returned future to a variable, the return value
    will be destroyed right then and there, which means ironically that a line containing
    nothing but a call to `std::async` will actually execute the specified function
    *synchronously:*'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意被注释掉的行，它们指示了从 `std::async` 返回的 `std::future` 的特殊行为 "在销毁时"。这是标准库中 `std::async`
    实现的一个奇怪且尴尬的行为，也是避免或在自己的代码中重新实现 `std::async` 的好理由：`std::async` 返回的未来具有调用其底层线程的
    `.join()` 的析构函数！这意味着它们的析构函数可能会阻塞，而且任务肯定不会像你自然期望的那样“在后台执行”。如果你调用 `std::async` 而不将返回的未来分配给变量，返回值将立即被销毁，这从讽刺的角度来看意味着只包含对
    `std::async` 调用的行实际上会同步执行指定的函数：
- en: '[PRE41]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The original reason for this limitation seems to have been a concern that if
    `std::async` launched background threads in the usual way, it would lead to people
    overusing `std::async` and possibly introducing dangling-reference bugs, as in
    this example:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这种限制的原由似乎是出于对如果 `std::async` 以通常的方式启动后台线程，可能会导致人们过度使用 `std::async` 并可能引入悬垂引用错误的担忧，就像这个例子中那样：
- en: '[PRE42]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: If we didn't wait for the result of this future, the function `test()` might
    return to its caller before the new thread got a chance to run; then, when the
    new thread did finally run and attempt to increment `i`, it would be accessing
    a stack variable that no longer existed. So, rather than run the risk of people
    writing such buggy code, the Standards Committee decided that `std::async` should
    return futures with special, "magic" destructors that join their threads automatically.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有等待这个未来的结果，函数 `test()` 可能会在新线程有机会运行之前就返回给调用者；然后，当新线程最终运行并尝试增加 `i` 时，它会访问一个不再存在的栈变量。因此，为了避免人们编写这样的有缺陷的代码，标准委员会决定
    `std::async` 应该返回具有特殊、“魔法”析构函数的未来，这些析构函数会自动连接它们的线程。
- en: 'Anyway, overuse of `std::async` is problematic for other reasons as well. The
    biggest reason is that on all popular operating systems, `std::thread` represents
    a *kernel thread*--a thread whose scheduling is under the control of the OS kernel.
    Because the OS has only finite resources to track these threads, the number of
    threads available to any one process is fairly limited: often only a few tens
    of thousands. If you''re using `std::async` as your thread manager, spawning a
    new `std::thread` every time you have another task that might benefit from concurrency,
    you''ll quickly find yourself running out of kernel threads. When this happens,
    the constructor of `std::thread` will start throwing exceptions of type `std::system_error`,
    often with the text `Resource temporarily unavailable`.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，过度使用 `std::async` 也有其他问题。最大的原因是，在所有流行的操作系统上，`std::thread` 代表一个 *内核线程*——一个调度受操作系统内核控制的线程。因为操作系统只有有限的资源来跟踪这些线程，所以任何进程可用的线程数量相当有限：通常只有几万。如果你将
    `std::async` 作为你的线程管理器，每次有另一个可能从并发中受益的任务时都创建一个新的 `std::thread`，你很快就会发现自己没有足够的内核线程可用。当这种情况发生时，`std::thread`
    的构造函数将开始抛出 `std::system_error` 类型的异常，通常带有文本 `Resource temporarily unavailable`。
- en: Building your own thread pool
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建自己的线程池
- en: If you use `std::async` to spawn a thread every time you have a new task, you
    risk exhausting the kernel's number of available threads for your process. A better
    way to run tasks concurrently is to use a *thread pool*--a small number of "worker
    threads" whose sole job is to run tasks as they are provided by the programmer.
    If there are more tasks than workers, the excess tasks are placed in a *work queue*.
    Whenever a worker finishes a task, it checks the work queue for new tasks.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你每次有新任务时都使用`std::async`来启动一个线程，你可能会耗尽内核为你进程提供的线程数量。运行任务的一种更好的方式是使用*线程池*——一小部分“工作线程”，它们唯一的任务是运行程序员提供的任务。如果有比工作者更多的任务，额外的任务将被放置在*工作队列*中。每当一个工作者完成一个任务时，它会检查工作队列中是否有新任务。
- en: This is a well-known idea, but has not yet been taken up into the standard library
    as of C++17\. However, you can combine the ideas shown in this chapter to create
    your own production-quality thread pool. I'll walk through a simple one here;
    it's not "production quality" in terms of performance, but it *is* properly thread-safe
    and correct in all its functionality. Some performance tweaks will be discussed
    at the end of the walkthrough.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个众所周知的思想，但截至C++17标准库还没有采用。然而，你可以结合本章中展示的思想来创建自己的生产级线程池。我将在这里介绍一个简单的例子；从性能的角度来看，它不是“生产级”的，但它*确实是*正确线程安全的，并且在其所有功能上都是正确的。在浏览结束时将讨论一些性能调整。
- en: 'We''ll start with the member data. Notice that we are using the rule that all
    the data controlled by a mutex should be located together under a single visual
    namespace; in this case, a nested struct definition. We''re also going to use
    `std::packaged_task<void()>` as our move-only function type; if your codebase
    already has a move-only function type, you''ll probably want to use that instead.
    If you don''t already have a move-only function type, consider adopting Folly''s
    `folly::Function` or Denis Blank''s `fu2::unique_function`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从成员数据开始。请注意，我们使用的是规则，即所有由互斥锁控制的数据都应该位于单个视觉命名空间下；在这种情况下，一个嵌套的结构定义。我们还将使用`std::packaged_task<void()>`作为我们的移动函数类型；如果你的代码库已经有一个移动函数类型，你可能想使用那个类型。如果你还没有移动函数类型，考虑采用Folly的`folly::Function`或Denis
    Blank的`fu2::unique_function`：
- en: '[PRE43]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The `work_queue` variable will hold tasks as they come in to us. The member
    variable `m_state.aborting` will be set to `true` when it's time for all the workers
    to stop working and "come home to rest." `m_workers` holds the worker threads
    themselves; and `m_state.mtx` and `m_cv` are just for synchronization. (The workers
    will spend much of their time asleep when there's no work to do. When a new task
    comes in and we need to wake up some worker, we'll notify `m_cv`.)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`work_queue`变量将保存传入的任务。成员变量`m_state.aborting`将在所有工作者停止工作并“回家休息”时设置为`true`。`m_workers`保存工作线程本身；而`m_state.mtx`和`m_cv`只是用于同步。（当没有工作要做时，工作者将花费大部分时间处于睡眠状态。当有新任务进来并且我们需要唤醒一些工作者时，我们将通知`m_cv`。）'
- en: 'The constructor of `ThreadPool` spawns worker threads and populates the `m_workers`
    vector. Each worker thread will be running the member function `this->worker_loop()`,
    which we''ll see in a minute:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`ThreadPool`的构造函数会启动工作线程并填充`m_workers`向量。每个工作线程将运行成员函数`this->worker_loop()`，我们将在下一分钟看到：'
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As promised, the destructor sets `m_state.aborting` to `true` and then waits
    for all of the worker threads to notice the change and terminate. Notice that
    when we touch `m_state.aborting`, it's only under a lock on `m_state.mtx`; we
    are following good hygiene in order to avoid bugs!
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如承诺的那样，析构函数将`m_state.aborting`设置为`true`，然后等待所有工作线程注意到这个变化并终止。请注意，当我们接触`m_state.aborting`时，它只在`m_state.mtx`的锁下；我们正在遵循良好的卫生习惯，以避免错误！
- en: '[PRE45]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now let''s see how we enqueue tasks into the work queue. (We have not yet seen
    how workers grab tasks out; we''ll see that happening in the `worker_loop` member
    function.) It''s very straightforward; we just have to make sure that we access
    `m_state` only under the mutex lock, and that once we have enqueued the task,
    we call `m_cv.notify_one()` so that some worker will wake up to handle the task:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何将任务入队到工作队列中。（我们还没有看到工作者如何获取任务；我们将在`worker_loop`成员函数中看到这一点。）这非常简单；我们只需要确保只在互斥锁下访问`m_state`，并且一旦我们入队了任务，我们就调用`m_cv.notify_one()`，这样某个工作者就会醒来处理任务：
- en: '[PRE46]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'At last, here is the worker loop. This is the member function that each worker
    runs:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这里是工作循环。这是每个工作者运行的成员函数：
- en: '[PRE47]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Notice the inevitable loop around `m_cv.wait(lk)`, and notice that we hygienically
    access `m_state` only under the mutex lock. Also notice that when we actually
    call out to perform `task`, we release the mutex lock first; this ensures that
    we are not holding the lock for a very long time while the user''s task executes.
    If we *were* to hold the lock for a long time, then no other worker would be able
    to get in and grab its next task--we''d effectively reduce the concurrency of
    our pool. Also, if we were to hold the lock during `task`, and if `task` itself
    tried to enqueue a new task on this pool (which requires taking the lock itself),
    then `task` would deadlock and our whole program would freeze up. This is a special
    case of the more general rule never to call a user-provided callback while holding
    a mutex lock: that''s generally a recipe for deadlock.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `m_cv.wait(lk)` 附近的必然循环，并注意我们只在互斥锁下卫生地访问 `m_state`。此外，注意当我们实际调用 `task` 来执行任务时，我们首先释放互斥锁；这确保了我们不会在用户任务执行期间长时间持有锁。如果我们
    *确实* 要长时间持有锁，那么就没有其他工作线程能够进入并获取其下一个任务--我们实际上会减少池的并发性。此外，如果我们要在 `task` 期间持有锁，并且如果
    `task` 本身尝试在这个池中排队一个新任务（这需要自己获取锁），那么 `task` 将发生死锁，整个程序都会冻结。这是更一般规则的一个特殊情况，即永远不要在持有互斥锁的同时调用用户提供的回调：这通常会导致死锁。
- en: 'Finally, let''s round out our `ThreadPool` class by implementing a safe version
    of `async`. Our version will allow calling `tp.async(f)` for any `f` that is callable
    without arguments, and just like `std::async`, we''ll return a `std::future` via
    which our caller can retrieve the result of `f` once it''s ready. Unlike the futures
    returned from `std::async`, our futures will be safe to drop on the floor: If
    the caller decides that he doesn''t want to wait for the result after all, the
    task will remain enqueued and will eventually be executed, and the result will
    simply be ignored:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们通过实现 `async` 的安全版本来完善我们的 `ThreadPool` 类。我们的版本将允许对任何无参数可调用的 `f` 调用 `tp.async(f)`，就像
    `std::async` 一样，我们将通过返回一个 `std::future` 来获取 `f` 的结果，一旦它准备好。与 `std::async` 返回的
    `future` 不同，我们的 `future` 将是安全的：如果调用者最终决定他不想等待结果，任务将保持排队并最终执行，结果将被简单地忽略：
- en: '[PRE48]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can use our `ThreadPool` class to write code like this function, which creates
    60,000 tasks:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们的 `ThreadPool` 类来编写如下函数，该函数创建了 60,000 个任务：
- en: '[PRE49]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We could try to do the same with `std::async`, but we'd likely run into thread
    exhaustion when we tried to create 60,000 kernel threads. The preceding example
    uses only four kernel threads, as indicated by the parameter to the `ThreadPool`
    constructor.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试用 `std::async` 做同样的事情，但当我们尝试创建 60,000 个内核线程时，我们可能会遇到线程耗尽的问题。前面的例子只使用了四个内核线程，正如
    `ThreadPool` 构造函数的参数所示。
- en: When you run this code, you'll see at least the numbers 0 through 42 printed
    to standard output, in some order. We know that 42 must be printed because the
    function definitely waits for `futures[42]` to be ready before it exits, and all
    the previous numbers must be printed because their tasks were placed in the work
    queue ahead of task number 42\. The numbers 43 through 59,999 might or might not
    be printed, depending on the scheduler; because as soon as task 42 is completed,
    we exit `test` and thus destroy the thread pool. The thread pool's destructor,
    as we've seen, notifies all of its workers to stop working and come home after
    they complete their current tasks. So it is likely that we'll see a few more numbers
    printed, but then all the workers will come home and the remaining tasks will
    be dropped on the floor.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这段代码时，你会在标准输出中看到至少从 0 到 42 的数字，以某种顺序打印出来。我们知道 42 必须被打印出来，因为函数肯定在退出前等待 `futures[42]`
    准备就绪，并且所有前面的数字都必须打印出来，因为它们的任务在任务编号 42 之前被放入工作队列。数字 43 到 59,999 可能会打印出来，也可能不会，这取决于调度器；因为一旦任务
    42 完成，我们就退出 `test` 并因此销毁线程池。正如我们所见，线程池的析构函数会通知所有工作线程在完成当前任务后停止工作并回家。因此，我们可能会看到更多数字被打印出来，然后所有工作线程都会回家，剩余的任务将被简单地丢弃。
- en: Of course if you wanted the destructor of `ThreadPool` to block until all enqueued
    tasks were completed, you could do that, by changing the code of the destructor.
    However, typically when you're destroying a thread pool, it's because your program
    (such as a web server) is exiting, and that's because you've received a signal
    such as the user pressing *Ctrl* + *C*. In that situation, you *probably* want
    to exit as soon as you can, as opposed to trying to clear the queue. Personally,
    I'd prefer to add a member function `tp.wait_for_all_enqueued_tasks()`, so that
    the user of the thread pool could decide whether they want to block or just drop
    everything on the floor.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果您希望 `ThreadPool` 的析构函数阻塞，直到所有入队任务都完成，您可以通过更改析构函数的代码来实现这一点。然而，通常当您销毁线程池时，是因为您的程序（如
    Web 服务器）正在退出，这是由于您收到了用户按下 *Ctrl* + *C* 等信号。在这种情况下，您可能希望尽快退出，而不是尝试清空队列。我个人更愿意添加一个成员函数
    `tp.wait_for_all_enqueued_tasks()`，这样线程池的使用者就可以决定他们是要阻塞还是直接放弃所有任务。
- en: Improving our thread pool's performance
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高我们的线程池性能
- en: The biggest performance bottleneck in our `ThreadPool` is that every worker
    thread is vying for the same mutex, `this->m_state.mtx`. The reason they're all
    contending that mutex is because that is the mutex that guards `this->m_state.work_queue`,
    and every worker needs to touch that queue in order to find out its next job.
    So one way to reduce contention and speed up our program is to find a way of distributing
    work to our workers that doesn't involve a single central work queue.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 `ThreadPool` 的最大性能瓶颈在于每个工作线程都在争夺同一个互斥锁，`this->m_state.mtx`。它们之所以争夺这个互斥锁，是因为这是保护
    `this->m_state.work_queue` 的互斥锁，每个工作线程都需要接触这个队列以找出其下一个工作。因此，减少竞争并加快我们程序的一种方法就是找到一种将工作分配给工作线程的方法，而不涉及单一的中心工作队列。
- en: The simplest solution is to give each worker its own "to-do list"; that is,
    to replace our single `std::queue<Task>` with a whole `std::vector<std::queue<Task>>`,
    with one entry for each worker thread. Of course then we'd also need a `std::vector<std::mutex>`
    so that we had one mutex for each work queue. The `enqueue_task` function distributes
    tasks to the work queues in a round-robin fashion (using atomic increments of
    a `std::atomic<int>` counter to deal with simultaneous enqueues).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的解决方案是为每个工作线程提供自己的“待办事项列表”；也就是说，用整个 `std::vector<std::queue<Task>>` 替换我们单一的
    `std::queue<Task>`，为每个工作线程提供一个条目。当然，我们还需要一个 `std::vector<std::mutex>`，以便为每个工作队列有一个互斥锁。`enqueue_task`
    函数以轮询方式将任务分配给工作队列（使用 `std::atomic<int>` 计数器的原子递增来处理同时入队的情况）。
- en: You could alternatively use a `thread_local` counter per enqueuing thread, if
    you are fortunate enough to work on a platform that supports C++11's `thread_local`
    keyword. On x86-64 POSIX platforms, access to a `thread_local` variable is approximately
    as fast as access to a plain old global variable; all the complication of setting
    up thread-local variables happens under the hood and only when you spawn a new
    thread. However, because that complication *does* exist and needs runtime support,
    many platforms do not yet support the `thread_local` storage class specifier.
    (On those that do, `thread_local int x` is basically the same thing as `static
    int x`, except that when your code accesses `x` by name, the actual memory address
    of `x` will vary depending on `std::this_thread::get_id()`. In principle, there
    is a whole array of `x` somewhere behind the scenes, indexed by thread-id and
    populated by the C++ runtime as threads are created and destroyed.)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以为每个入队线程使用一个 `thread_local` 计数器，如果您有幸在一个支持 C++11 的 `thread_local` 关键字的平台上工作。在
    x86-64 POSIX 平台上，访问 `thread_local` 变量的速度大约与访问普通全局变量的速度相同；设置线程局部变量的所有复杂性都在幕后发生，并且仅在您创建新线程时才会出现。然而，由于这种复杂性确实存在并且需要运行时支持，许多平台尚未支持
    `thread_local` 存储类指定符。（在那些支持的平台中，`thread_local int x` 与 `static int x` 基本上是同一回事，只是当您的代码通过名称访问
    `x` 时，`x` 的实际内存地址将根据 `std::this_thread::get_id()` 而变化。原则上，在幕后有一个由线程 ID 索引的整个 `x`
    数组，由 C++ 运行时在创建和销毁线程时填充。）
- en: 'The next significant performance improvement to our `ThreadPool` would be "work-stealing":
    now that each worker has its own to-do list, it might happen by chance or malice
    that one worker becomes overworked while all the other workers lie idle. In this
    case, we want the idle workers to scan the queues of the busy workers and "steal"
    tasks if possible. This re-introduces lock contention among the workers, but only
    when an inequitable assignment of tasks has already produced inefficiency--inefficiency
    which we are hoping to *correct* via work-stealing.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 `ThreadPool` 的下一个显著的性能改进将是“工作窃取”：现在每个工作线程都有自己的待办事项列表，可能会偶然或恶意地发生一个工作线程变得过度劳累，而所有其他工作线程都闲置。在这种情况下，我们希望空闲的工作线程扫描忙碌工作线程的队列，并在可能的情况下“窃取”任务。这再次在工作线程之间引入了锁竞争，但只有在任务分配不均已经产生了效率低下——我们希望通过工作窃取来纠正这种效率低下。
- en: Implementing separate work queues and work-stealing is left as an exercise for
    the reader; but I hope that after seeing how simple the basic `ThreadPool` turned
    out, you won't be too daunted by the idea of modifying it to include those extra
    features.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 实现单独的工作队列和工作窃取作为练习留给读者；但我希望你在看到基本的 `ThreadPool` 实现如此简单之后，不会对修改它以包含这些额外功能感到过于畏惧。
- en: 'Of course, there also exists professionally written thread-pool classes. Boost.Asio
    contains one, for example, and Asio is on track to be brought into the standard
    perhaps in C++20\. Using Boost.Asio, our `ThreadPool` class would look like this:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，也存在专业编写的线程池类。例如，Boost.Asio 包含了一个，而且 Asio 正在朝着可能在 C++20 中成为标准库的方向发展。使用 Boost.Asio，我们的
    `ThreadPool` 类将看起来像这样：
- en: '[PRE50]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: An explanation of Boost.Asio is, of course, far outside the scope of this book.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Boost.Asio 的解释当然超出了本书的范围。
- en: Any time you use a thread pool, be careful that the tasks you enqueue never
    block indefinitely on conditions controlled by other tasks in the same thread
    pool. A classic example would be a task A that waits on a condition variable,
    expecting that some later task B will notify the condition variable. If you make
    a `ThreadPool` of size 4 and enqueue four copies of task A followed by four copies
    of task B, you'll find that task B never runs--the four worker threads in your
    pool are all occupied by the four copies of task A, which are all asleep waiting
    for a signal that will never come! "Handling" this scenario is tantamount to writing
    your own user-space threading library; if you don't want to get into that business,
    then the only sane answer is to be careful that the scenario cannot arise in the
    first place.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 任何时候你使用线程池，都要小心确保你入队的任务不会在由同一线程池中的其他任务控制的条件下无限期地阻塞。一个经典的例子是任务 A 等待一个条件变量，期望稍后某个任务
    B 会通知这个条件变量。如果你创建一个大小为 4 的 `ThreadPool` 并将四个任务 A 的副本和四个任务 B 的副本入队，你会发现任务 B 永远不会运行——你池中的四个工作线程都被四个任务
    A 的副本占用，它们都在等待一个永远不会到来的信号！“处理”这种情况相当于编写自己的用户空间线程库；如果你不想涉足这一领域，那么唯一的明智答案是确保这种情况根本不会发生。
- en: Summary
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Multithreading is a difficult and subtle subject, with many pitfalls that are
    obvious only in hindsight. In this chapter we have learned:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 多线程是一个困难和微妙的话题，有许多只有在事后才会明显的问题。在本章中，我们学习了：
- en: '`volatile`, while useful for dealing directly with hardware, is insufficient
    for thread-safety. `std::atomic<T>` for scalar `T` (up to the size of a machine
    register) is the right way to access shared data without races and without locks.
    The most important primitive atomic operation is compare-and-swap, which in C++
    is spelled `compare_exchange_weak`.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`volatile` 虽然对于直接处理硬件很有用，但不足以保证线程安全。对于标量 `T`（大小不超过机器寄存器），`std::atomic<T>` 是访问共享数据而不发生竞争和不使用锁的正确方式。最重要的原始原子操作是比较并交换，在
    C++ 中表示为 `compare_exchange_weak`。'
- en: To force threads to take turns accessing shared non-atomic data, we use `std::mutex`.
    Always lock mutexes via an RAII class such as `std::unique_lock<M>`. Remember
    that although C++17 class template argument deduction allows us to omit the `<M>`
    from these templates' names, that is just a syntactic convenience; they remain
    template classes.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强制线程轮流访问共享的非原子数据，我们使用 `std::mutex`。始终通过 RAII 类如 `std::unique_lock<M>` 锁定互斥量。记住，尽管
    C++17 类模板参数推导允许我们从这些模板的名称中省略 `<M>`，但这只是一个语法便利；它们仍然是模板类。
- en: Always clearly indicate which data is controlled by each mutex in your program.
    One good way to do this is with a nested struct definition.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 总是清楚地指出程序中每个互斥量控制的数据。一个很好的方法是使用嵌套的结构定义。
- en: '`std::condition_variable` allows us to "sleep until" some condition is satisfied.
    If the condition can be satisfied only once, such as a thread becoming "ready,"
    then you probably want to use a promise-future pair instead of a condition variable.
    If the condition can be satisfied over and over again, consider whether your problem
    can be rephrased in terms of the *work queue* pattern.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::condition_variable` 允许我们“等待直到”某个条件得到满足。如果条件只能满足一次，例如线程变为“就绪”状态，那么你可能想使用承诺-未来对而不是条件变量。如果条件可以反复满足，考虑是否可以将你的问题重新表述为
    *工作队列* 模式。'
- en: '`std::thread` reifies the idea of a thread of execution. The "current thread"
    is not directly manipulable as a `std::thread` object, but a limited set of operations
    are available as free functions in the `std::this_thread` namespace. The most
    important of these operations are `sleep_for` and `get_id`. Each `std::thread`
    must always be joined or detached before it can be destroyed. Detaching is useful
    only for background threads that you will never need to shut down cleanly.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::thread` 实现了执行线程的概念。当前线程不能直接作为一个 `std::thread` 对象来操作，但在 `std::this_thread`
    命名空间中有一组有限的操作作为免费函数可用。其中最重要的操作是 `sleep_for` 和 `get_id`。每个 `std::thread` 在被销毁之前都必须始终加入或分离。分离对于你永远不会需要干净关闭的后台线程是有用的。'
- en: The standard function `std::async` takes a function or lambda for execution
    on some other thread, and returns a `std::future` that becomes ready when the
    function is done executing. While `std::async` itself is fatally flawed (destructors
    that `join`; kernel thread exhaustion) and thus should not be used in production
    code, the general idea of dealing with concurrency via futures is a good one.
    Prefer to use an implementation of promises and futures that supports the `.then`
    method. Folly's implementation is the best.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 标准函数 `std::async` 接收一个用于在其他线程上执行的功能或lambda表达式，并在函数执行完毕时返回一个 `std::future` 对象。虽然
    `std::async` 本身存在致命缺陷（析构函数中的 `join`；内核线程耗尽）因此不应在生产代码中使用，但通过future处理并发的总体思路是好的。最好使用支持
    `.then` 方法的承诺和future的实现。Folly的实现是最好的。
- en: '*Multithreading is a difficult and subtle subject, with many pitfalls that
    are* *obvious only in hindsight.*'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*多线程是一个困难和微妙的话题，其中许多陷阱只有在事后才会变得明显*。'
