<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch11"/>Chapter 11. Sensing and Tracking Input from the Camera</h1></div></div></div><p>In this chapter, we will learn how to receive and process data from input devices such as a camera or a Microsoft Kinect sensor.</p><p>The following recipes will be covered:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Capturing from the camera</li><li class="listitem" style="list-style-type: disc">Tracking an object based on color</li><li class="listitem" style="list-style-type: disc">Tracking motion using optical flow</li><li class="listitem" style="list-style-type: disc">Object tracking</li><li class="listitem" style="list-style-type: disc">Reading QR code</li><li class="listitem" style="list-style-type: disc">Building UI navigation and gesture recognition with Kinect</li><li class="listitem" style="list-style-type: disc">Building an augmented reality with Kinect</li></ul></div><div><div><div><div><h1 class="title"><a id="ch11lvl1sec93"/>Capturing from the camera</h1></div></div></div><p>In this recipe we will learn how to capture <a id="id672" class="indexterm"/>and display frames from a camera.<a id="id673" class="indexterm"/></p><div><div><div><div><h2 class="title"><a id="ch11lvl2sec290"/>Getting ready</h2></div></div></div><p>Include the necessary files to capture images from a camera and draw them to OpenGL textures:</p><div><pre class="programlisting">#include "cinder/gl/gl.h"
#include "cinder/gl/Texture.h"
#include "cinder/Capture.h"</pre></div><p>Also add the following <code class="literal">using</code> statements:</p><div><pre class="programlisting">using namespace ci;
using namespace ci::app;
using namespace std;</pre></div></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec291"/>How to do it…</h2></div></div></div><p>We will now capture and<a id="id674" class="indexterm"/> draw frames <a id="id675" class="indexterm"/>from the camera.</p><div><ol class="orderedlist arabic"><li class="listitem">Declare the following members in your application class:<div><pre class="programlisting">    Capture mCamera;
    gl::Texture mTexture;</pre></div></li><li class="listitem">In the <code class="literal">setup</code> method we will initialize <code class="literal">mCamera</code>:<div><pre class="programlisting">    try{
        mCamera = Capture( 640, 480 );
        mCamera.start();
    } catch( ... ){
        console() &lt;&lt; "Could not initialize the capture" &lt;&lt; endl;</pre></div></li><li class="listitem">In the <code class="literal">update</code> method, we will check if <code class="literal">mCamera</code> was successfully initialized. Also if there is any new frame available, copy the camera's image into <code class="literal">mTexture</code>:<div><pre class="programlisting">    if( mCamera ){
        if( mCamera.checkNewFrame() ){
            mTexture = gl::Texture( mCamera.getSurface() );
        }
    }</pre></div></li><li class="listitem">In the <code class="literal">draw</code> method, we will simply clear the background, check if <code class="literal">mTexture</code> has been initialized, and draw it's image on the screen:<div><pre class="programlisting">  gl::clear( Color( 0, 0, 0 ) ); 
    if( mTexture ){
        gl::draw( mTexture, getWindowBounds() );
    }</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec292"/>How it works…</h2></div></div></div><p>The <code class="literal">ci::Capture</code> is a class that wraps around Quicktime on Apple computers, AVFoundation on iOS platforms, and Directshow on Windows. Under the hood it uses these lower level frameworks<a id="id676" class="indexterm"/> to access and capture frames from a webcam.</p><p>Whenever a new frame is found, it's pixels are copied into the <code class="literal">ci::Surface method</code>. In the previous code we check on every <code class="literal">update</code> method<a id="id677" class="indexterm"/> if there is a new frame by calling the <code class="literal">ci::Capture::checkNewFrame</code> method, and update our texture with its surface.</p></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec293"/>There's more…</h2></div></div></div><p>It is also possible to get a list of available capture devices and choose which one you wish to start with.</p><p>To ask for a list of devices and print their information, we could write the following code:</p><div><pre class="programlisting">vector&lt;Capture::DeviceRef&gt; devices = Capture::getDevices();
for( vector&lt;Capture::DeviceRef&gt;::iterator it = devices.begin(); 
 it != devices.end(); ++it ){
 Capture::DeviceRef device = *it;
 console() &lt;&lt; "Found device:" 
  &lt;&lt; device-&gt;getName() 
  &lt;&lt; " with ID:" &lt;&lt; device-&gt;getUniqueId() &lt;&lt; endl;
}</pre></div><p>To initialize <code class="literal">mCapture</code> using a specific device, you simply pass <code class="literal">ci::Capture::DeviceRef</code> as a third parameter in the constructor.</p><p>For example, if you wanted to initialize <code class="literal">mCapture</code> with the first device, you should write the following code:</p><div><pre class="programlisting">vector&lt;Capture::DeviceRef&gt; devices = Capture::getDevices();
mCapture = Capture( 640, 480 devices[0] );</pre></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch11lvl1sec94"/>Tracking an object based on color</h1></div></div></div><p>In this recipe we will show how to<a id="id678" class="indexterm"/> track objects with a specified color using the OpenCV library.</p><div><div><div><div><h2 class="title"><a id="ch11lvl2sec294"/>Getting ready</h2></div></div></div><p>In this recipe we will use OpenCV, so please refer to the <em>Integrating with OpenCV</em> recipe from <a class="link" href="ch03.html" title="Chapter 3. Using Image Processing Techniques">Chapter 3</a>, <em>Using Image Processing Techniques</em>. We will also need InterfaceGl which is covered in the <em>Setting up a GUI for parameter tweaking</em> recipe from <a class="link" href="ch02.html" title="Chapter 2. Preparing for Development">Chapter 2</a>, <em>Preparing for Development</em>.</p></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec295"/>How to do it…</h2></div></div></div><p>We will create an application that tracks an object with a selected color.</p><div><ol class="orderedlist arabic"><li class="listitem">Include the necessary header files:<div><pre class="programlisting">#include "cinder/gl/gl.h"
#include "cinder/gl/Texture.h"
#include "cinder/Surface.h"
#include "cinder/ImageIo.h"
#include "cinder/Capture.h"
#include "cinder/params/Params.h"
#include  "CinderOpenCV.h"</pre></div></li><li class="listitem">Add members to store the original and processed frame:<div><pre class="programlisting">Surface8u   mImage;</pre></div></li><li class="listitem">Add members to store the tracked object's coordinates:<div><pre class="programlisting">vector&lt;cv::Point2f&gt; mCenters;
vector&lt;float&gt; mRadius;</pre></div></li><li class="listitem">Add members <a id="id679" class="indexterm"/>to store the parameters that will be passed to the tracking algorithms:<div><pre class="programlisting">double mApproxEps;
int mCannyThresh;

ColorA      mPickedColor;
cv::Scalar  mColorMin;
cv::Scalar  mColorMax;</pre></div></li><li class="listitem">Add members to handle the capturing device and frame texture:<div><pre class="programlisting">Capture     mCapture;
gl::Texture mCaptureTex;</pre></div></li><li class="listitem">In the <code class="literal">setup</code> method we will set the window dimensions and initialize capturing device:<div><pre class="programlisting">setWindowSize(640, 480);

try {
  mCapture = Capture( 640, 480 );
  mCapture.start();
}
catch( ... ) {
  console() &lt;&lt;"Failed to initialize capture"&lt;&lt;std::endl;
}</pre></div></li><li class="listitem">In the <code class="literal">setup</code> method we have to initialize variables and setup the GUI for a preview of the tracked color value:<div><pre class="programlisting">mApproxEps = 1.0;
mCannyThresh = 200;

mPickedColor = Color8u(255, 0, 0);
setTrackingHSV();

// Setup the parameters
mParams = params::InterfaceGl( "Parameters", Vec2i( 200, 150 ) );
mParams.addParam( "Picked Color", &amp;mPickedColor, "readonly=1" );</pre></div></li><li class="listitem">In the <code class="literal">update</code> method, <a id="id680" class="indexterm"/>check if there is any new frame to process and convert it to <code class="literal">cv::Mat</code>, which is necessary for further OpenCV operations:<div><pre class="programlisting">if( mCapture&amp;&amp;mCapture.checkNewFrame() ) {
  mImage = mCapture.getSurface();
  mCaptureTex = gl::Texture( mImage );

  cv::Mat inputMat( toOcv( mImage) );
  cv::resize(inputMat, inputMat, cv::Size(320, 240) );

  cv::Mat inputHSVMat, frameTresh;
  cv::cvtColor(inputMat, inputHSVMat, CV_BGR2HSV);</pre></div></li><li class="listitem">Process the captured frame:<div><pre class="programlisting">  cv::inRange(inputHSVMat, mColorMin, mColorMax, frameTresh);

  cv::medianBlur(frameTresh, frameTresh, 7);

  cv::Mat cannyMat;
  cv::Canny(frameTresh, cannyMat, mCannyThresh, mCannyThresh*2.f, 3 );
 vector&lt; std::vector&lt;cv::Point&gt; &gt;  contours;
 cv::findContours(cannyMat, contours, CV_RETR_LIST, 
  CV_CHAIN_APPROX_SIMPLE);
 mCenters = vector&lt;cv::Point2f&gt;(contours.size());
 mRadius = vector&lt;float&gt;(contours.size());
 for( int i = 0; i &lt; contours.size(); i++ ) {
  std::vector&lt;cv::Point&gt; approxCurve;
  cv::approxPolyDP(contours[i], approxCurve, 
   mApproxEps, true);
  cv::minEnclosingCircle(approxCurve, mCenters[i], 
   mRadius[i]);
 }</pre></div></li><li class="listitem">Close the <code class="literal">if</code> statement's body.<div><pre class="programlisting">}</pre></div></li><li class="listitem">Implement the method <code class="literal">setTrackingHSV</code>, which sets color's values for tracking:<div><pre class="programlisting">void MainApp::setTrackingHSV()
{
void MainApp::setTrackingHSV() {
 Color8u col = Color( mPickedColor );
 Vec3f colorHSV = col.get(CM_HSV);
 colorHSV.x *= 179;
 colorHSV.y *= 255;
 colorHSV.z *= 255;
 mColorMin = cv::Scalar(colorHSV.x-5, colorHSV.y -50, 
  colorHSV.z-50);
 mColorMax = cv::Scalar(colorHSV.x+5, 255, 255);
}</pre></div></li><li class="listitem">Implement the<a id="id681" class="indexterm"/> <code class="literal">mouseDown</code> event handler:<div><pre class="programlisting">void MainApp::mouseDown(MouseEvent event) {
 if( mImage&amp;&amp;mImage.getBounds().contains( event.getPos() ) ) {
  mPickedColor = mImage.getPixel( event.getPos() );
  setTrackingHSV();
 } 
}</pre></div></li><li class="listitem">Implement the <code class="literal">draw</code> method as follows:<div><pre class="programlisting">void MainApp::draw()
{
 gl::clear( Color( 0.1f, 0.1f, 0.1f ) );
 gl::color(Color::white());
 if(mCaptureTex) {
  gl::draw(mCaptureTex);
  gl::color(Color::white());
  for( int i = 0; i &lt;mCenters.size(); i++ )
  {
   Vec2f center = fromOcv(mCenters[i])*2.f;
   gl::begin(GL_POINTS);
   gl::vertex( center );
   gl::end();
   gl::drawStrokedCircle(center, mRadius[i]*2.f );
  }
 }
 params::InterfaceGl::draw();
}</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec296"/>How it works…</h2></div></div></div><p>By preparing the captured<a id="id682" class="indexterm"/> frame for processing we are converting it into a <strong>hue, saturation, and value</strong> (<strong>HSV</strong>) <a id="id683" class="indexterm"/>color space description method, which will be very useful in this case. Those are the properties describing the color in the HSV color space in a more intuitive way for color tracking. We can set a fixed hue value for detection, while saturation and value can vary with in a specified range. This can eliminate a noise caused by constantly changing light in the camera view. Take a look at the first step of the frame image processing; we are using the <code class="literal">cv::inRange</code> function to get a mask of pixels that fits our tracking color range. The range of the tracking colors is calculated from the color value picked by clicking inside the window, which is implemented inside the <code class="literal">mouseDown</code> handler and the <code class="literal">setTrackingHSV</code> method.</p><p>As you can see inside <code class="literal">setTrackingHSV</code>, we are calculating <code class="literal">mColorMin</code> and <code class="literal">mColorMax</code> by simply widening the range. You may have to adjust these calculations depending on your camera noise and lighting conditions.</p></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec297"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">HSV on Wikipedia: <a class="ulink" href="http://en.wikipedia.org/wiki/HSL_and_HSV">http://en.wikipedia.org/wiki/HSL_and_HSV</a></li><li class="listitem" style="list-style-type: disc">The OpenCV documentation: <a class="ulink" href="http://opencv.willowgarage.com/documentation/cpp/">http://opencv.willowgarage.com/documentation/cpp/</a></li></ul></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch11lvl1sec95"/>Tracking motion using optical flow</h1></div></div></div><p>In this recipe we<a id="id684" class="indexterm"/> will learn how to track motion in the images produced from a webcam using OpenCV using the popular Lucas Kanade optical flow algorithm.<a id="id685" class="indexterm"/></p><div><div><div><div><h2 class="title"><a id="ch11lvl2sec298"/>Getting ready</h2></div></div></div><p>We will need to use OpenCV in this recipe, so please refer to the <em>Integrating with OpenCV</em> recipe from <a class="link" href="ch03.html" title="Chapter 3. Using Image Processing Techniques">Chapter 3</a>, <em>Using Image Processing Techniques</em> and add OpenCV and it's CinderBlock to your project. Include the following files to your source file:</p><div><pre class="programlisting">#include "cinder/Capture.h"
#include "cinder/gl/Texture.h"
#include "CinderOpenCV.h"</pre></div><p>Add the following <code class="literal">using</code> statements:</p><div><pre class="programlisting">using namespace ci;
using namespace ci::app;
using namespace std;</pre></div></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec299"/>How to do it…</h2></div></div></div><p>We will read frames from<a id="id686" class="indexterm"/> the camera and track motion.<a id="id687" class="indexterm"/></p><div><ol class="orderedlist arabic"><li class="listitem">Declare the <code class="literal">ci::gl::Texture</code> and <code class="literal">ci::Capture</code> objects to display and capture from a camera. Also, declare a <code class="literal">cv::Mat</code> object as the previous frame, two <code class="literal">std::vector&lt;cv::Point2f&gt;</code> objects to store the current and previous features, and a <code class="literal">std::vector&lt;uint8_t&gt;</code> object to store the status of each feature:<div><pre class="programlisting">    gl::Texture mTexture;
    Capture mCamera;
    cv::Mat mPreviousFrame;
    vector&lt; cv::Point2f &gt; mPreviousFeatures, mFeatures;
    vector&lt; uint8_t &gt; mFeatureStatuses;</pre></div></li><li class="listitem">In the <code class="literal">setup</code> method we will initialize <code class="literal">mCamera</code>:<div><pre class="programlisting">try{
        mCamera = Capture( 640, 480 );
        mCamera.start();
    } catch( ... ){
        console() &lt;&lt; "unable to initialize device" &lt;&lt; endl;
    }</pre></div></li><li class="listitem">In the <code class="literal">update</code> method we need to check if <code class="literal">mCamera</code> has been correctly initialized and whether it has a new frame available:<div><pre class="programlisting">    if( mCamera ){
        if( mCamera.checkNewFrame() ){</pre></div></li><li class="listitem">After those <code class="literal">if</code> statements we will get a reference to <code class="literal">ci::Surface</code> of <code class="literal">mCamera</code> and then copy it to our <code class="literal">mTexture</code> for drawing:<div><pre class="programlisting">            Surface surface = mCamera.getSurface();
            mTexture = gl::Texture( surface );</pre></div></li><li class="listitem">Now let's create a <code class="literal">cv::Mat</code> with the current camera frame. We will also check if <code class="literal">mPreviousFrame</code> contains any initialized data, calculate the good features to track, and calculate their motion from the previous camera frame to the current frame:<div><pre class="programlisting">            cv::Mat frame( toOcv( Channel( surface ) ) );
            if( mPreviousFrame.data != NULL ){
                cv::goodFeaturesToTrack( frame, mFeatures, 300, 0.005f, 3.0f );
                vector&lt;float&gt; errors;
                mPreviousFeatures = mFeatures;
                cv::calcOpticalFlowPyrLK( mPreviousFrame, frame, mPreviousFeatures, mFeatures, mFeatureStatuses, errors );
            }</pre></div></li><li class="listitem">Now we<a id="id688" class="indexterm"/> just need to<a id="id689" class="indexterm"/> copy the frame to <code class="literal">mPreviousFrame</code> and close the initial <code class="literal">if</code> statements:<div><pre class="programlisting">            mPreviousFrame = frame;
        }
    }</pre></div></li><li class="listitem">In the <code class="literal">draw</code> method we will begin by clearing the background with black and drawing <code class="literal">mTexture</code>:<div><pre class="programlisting">  gl::clear( Color( 0, 0, 0 ) ); 
    if( mTexture ){
        gl::color( Color::white() );
        gl::draw( mTexture, getWindowBounds() );
    }</pre></div></li><li class="listitem">Next, we will draw red lines on the features we have tracked, using <code class="literal">mFeatureStatus</code> to draw the features that have been matched:<div><pre class="programlisting">    glColor4f( 1.0f, 0.0f, 0.0f, 1.0f );
    for( int i=0; i&lt;mFeatures.size(); i++ ){
        if( (bool)mFeatureStatuses[i] == false ) continue;
        gl::drawSolidCircle( fromOcv( mFeatures[i] ), 5.0f );
    }</pre></div></li><li class="listitem">Finally, we will draw a line between the previous features and the current ones, also using <code class="literal">mFeatureStatus</code> to draw one of the features that has been matched:<div><pre class="programlisting">    for( int i=0; i&lt;mFeatures.size(); i++ ){
        if( (bool)mFeatureStatuses[i] == false ) continue;
        Vec2f pt1 = fromOcv( mFeatures[i] );
        Vec2f pt2 = fromOcv( mPreviousFeatures[i] );
        gl::drawLine( pt1, pt2 );
    }</pre></div><p>In the following image, the red dots represent good features to track:</p><div><img src="img/8703OS_11_01.jpg" alt="How to do it…"/></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec300"/>How it works…</h2></div></div></div><p>The optical flow algorithm will <a id="id690" class="indexterm"/>make an estimation of how<a id="id691" class="indexterm"/> much the tracked point has moved from one frame to the other.</p></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec301"/>There's more…</h2></div></div></div><p>In this recipe we are using the <code class="literal">cv::goodFeaturesToTrack</code> object to calculate which features are optimal for tracking, but it is also possible to manually choose which points we wish to track. All we have to do is populate <code class="literal">mFeatures</code> manually with whatever points we wish to track and pass it to the <code class="literal">cv::calcOpticalFlowPyrLK</code>. object</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch11lvl1sec96"/>Object tracking</h1></div></div></div><p>In this recipe, we will learn how<a id="id692" class="indexterm"/> to track specific planar objects in our webcam using OpenCV and it's corresponding CinderBlock.</p><div><div><div><div><h2 class="title"><a id="ch11lvl2sec302"/>Getting ready</h2></div></div></div><p>You will need an image depiction of the physical object you wish to track in the camera. For this recipe place that image in the <code class="literal">assets</code> folder and name it <code class="literal">object.jpg</code>.</p><p>We will use the OpenCV CinderBlock in this recipe, so please refer to the <em>Integrating with OpenCV</em> recipe from <a class="link" href="ch03.html" title="Chapter 3. Using Image Processing Techniques">Chapter 3</a>, <em>Using Image Processing Techniques</em> and add OpenCV and it's CinderBlock to your project.</p><p>If you are using a Mac, you will need to compile the OpenCV static libraries yourself, because the OpenCV CinderBlock is missing some needed libraries on OSX (it will work fine on Windows). You can download the correct version from the following link: <a class="ulink" href="http://sourceforge.net/projects/opencvlibrary/files/opencv-unix/2.3/">http://sourceforge.net/projects/opencvlibrary/files/opencv-unix/2.3/</a>.</p><p>You will need to compile the static libraries yourself using the provided <code class="literal">CMake</code> files. Once your libraries are correctly added to your project, include the following files:</p><div><pre class="programlisting">#include "cinder/Capture.h"
#include "cinder/gl/Texture.h"
#include "cinder/ImageIo.h"</pre></div><p>Add the following <code class="literal">using</code> statements:</p><div><pre class="programlisting">using namespace ci;
using namespace ci::app;
using namespace std;</pre></div></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec303"/>How to do it…</h2></div></div></div><p>We will track an object <a id="id693" class="indexterm"/>in the camera frames based on an image depicting the object</p><div><ol class="orderedlist arabic"><li class="listitem">Let's begin by creating a <code class="literal">struct</code> method to store the necessary objects for feature tracking and matching. Add the following code before your application class declaration:<div><pre class="programlisting">struct DetectionInfo{
    vector&lt;cv::Point2f&gt; goodPoints;
    vector&lt;cv::KeyPoint&gt; keyPoints;
    cv::Mat image, descriptor;
    gl::Texture texture;
};</pre></div></li><li class="listitem">In your class declaration add the following member objects:<div><pre class="programlisting">DetectionInfo mObjectInfo, mCameraInfo;
    cv::Mat mHomography;
    cv::SurfFeatureDetector mFeatureDetector;
    cv::SurfDescriptorExtractor mDescriptorExtractor;
    cv::FlannBasedMatcher mMatcher;
    vector&lt;cv::Point2f&gt; mCorners;</pre></div></li><li class="listitem">In the <code class="literal">setup</code> method let's start by initializing the camera:<div><pre class="programlisting">    try{
        mCamera = Capture( 640, 480 );
        mCamera.start();
    } catch( ... ){
        console() &lt;&lt; "could not initialize capture" &lt;&lt; endl;
    }</pre></div></li><li class="listitem">Lets resize<a id="id694" class="indexterm"/> <code class="literal">mCorners</code>, load our object image, and calculate its <code class="literal">image</code>, <code class="literal">keyPoints</code>, <code class="literal">texture</code>, and <code class="literal">descriptor</code>:<div><pre class="programlisting">mCorners.resize( 4 );
    Surface objectSurface = loadImage( loadAsset( "object.jpg" ) );
    mObjectInfo.texture = gl::Texture( objectSurface );
    mObjectInfo.image = toOcv( Channel( objectSurface ) );
    mFeatureDetector.detect( mObjectInfo.image, mObjectInfo.keyPoints );
    mDescriptorExtractor.compute( mObjectInfo.image, mObjectInfo.keyPoints, mObjectInfo.descriptor );</pre></div></li><li class="listitem">In the <code class="literal">update</code> method, we will check if <code class="literal">mCamera</code> has been initialized and whether we have a new frame to process:<div><pre class="programlisting">    if( mCamera ){
        if( mCamera.checkNewFrame() ){</pre></div></li><li class="listitem">Now let's get the surface of <code class="literal">mCamera</code> and initialize <code class="literal">texture</code> and <code class="literal">image</code> objects of <code class="literal">mCameraInfo</code>. We will create a <code class="literal">ci::Channel</code> object from <code class="literal">cameraSurface</code> that converts color surfaces to gray channel surfaces:<div><pre class="programlisting">Surface cameraSurface = mCamera.getSurface();
mCameraInfo.texture = gl::Texture( cameraSurface );
mCameraInfo.image = toOcv( Channel( cameraSurface ) );</pre></div></li><li class="listitem">Let's calculate <code class="literal">features</code> and <code class="literal">descriptor</code> values of <code class="literal">mCameraInfo</code>:<div><pre class="programlisting">mFeatureDetector.detect( mCameraInfo.image, mCameraInfo.keyPoints);
mDescriptorExtractor.compute( mCameraInfo.image, mCameraInfo.keyPoints, mCameraInfo.descriptor );</pre></div></li><li class="listitem">Now let's use <code class="literal">mMatcher</code> to calculate the matches between <code class="literal">mObjectInfo</code> and <code class="literal">mCameraInfo</code>:<div><pre class="programlisting">            vector&lt;cv::DMatch&gt; matches;
            mMatcher.match( mObjectInfo.descriptor, mCameraInfo.descriptor, matches );</pre></div></li><li class="listitem">To perform a<a id="id695" class="indexterm"/> test to check for false matches, we will calculate the minimum distance between matches:<div><pre class="programlisting">double minDist = 640.0;
for( int i=0; i&lt;mObjectInfo.descriptor.rows; i++ ){
                double dist = matches[i].distance;
                if( dist &lt; minDist ){
                    minDist = dist;
                }
            }</pre></div></li><li class="listitem">Now we will add all the points whose distance is less than <code class="literal">minDist*3.0</code> to <code class="literal">mObjectInfo.goodPoints.clear();</code><div><pre class="programlisting">
mCameraInfo.goodPoints.clear();
for( vector&lt;cv::DMatch&gt;::iterator it = matches.begin(); 
 it != matches.end(); ++it ){
 if( it-&gt;distance &lt; minDist*3.0 ){
  mObjectInfo.goodPoints.push_back( 
   mObjectInfo.keyPoints[ it-&gt;queryIdx ].pt );
  mCameraInfo.goodPoints.push_back( 
   mCameraInfo.keyPoints[ it-&gt;trainIdx ].pt );
 }
</pre></div></li><li class="listitem"><code class="literal">}</code> With all our points calculated and matched, we need to calculate the homography between the points of <code class="literal">mObjectInfo</code> and <code class="literal">mCameraInfo</code>:<div><pre class="programlisting">mHomography = cv::findHomography( mObjectInfo.goodPoints, mCameraInfo.goodPoints, CV_RANSAC );</pre></div></li><li class="listitem">Let's create <code class="literal">vector&lt;cv::Point2f&gt;</code> with the corners of our object and perform a perspective transform to calculate the corners of our object in the camera image:<div><div><h3 class="title"><a id="tip05"/>Tip</h3><p>Don't forget to close the brackets we opened earlier.</p></div></div><div><pre class="programlisting">  vector&lt;cv::Point2f&gt; objCorners( 4 );
  objCorners[0] = cvPoint( 0.0f, 0.0f );
  objCorners[1] = cvPoint( mObjectInfo.image.cols, 0.0f);
  objCorners[2] = cvPoint( mObjectInfo.image.cols, 
   mObjectInfo.image.rows );
  objCorners[3] = cvPoint( 0.0f, mObjectInfo.image.rows);
  mCorners = vector&lt; cv::Point2f &gt;( 4 );
  cv::perspectiveTransform( objCorners, mCorners, 
   mHomography );
 } 
}</pre></div></li><li class="listitem">Let's move<a id="id696" class="indexterm"/> to the <code class="literal">draw</code> method and begin by clearing the background and drawing the camera and object textures:<div><pre class="programlisting">  gl::clear( Color( 0, 0, 0 ) );

    gl::color( Color::white() );
    if( mCameraInfo.texture ){
        gl::draw( mCameraInfo.texture, getWindowBounds() );
    }

    if( mObjectInfo.texture ){
        gl::draw( mObjectInfo.texture );
    }</pre></div></li><li class="listitem">Now let's iterate over <code class="literal">goodPoints</code> values in both <code class="literal">mObjectInfo</code> and <code class="literal">mCameraInfo</code> and draw them:<div><pre class="programlisting">for( int i=0; i&lt;mObjectInfo.goodPoints.size(); i++ ){
 gl::drawStrokedCircle( fromOcv( mObjectInfo.goodPoints[ i ] ),
  5.0f );
 gl::drawStrokedCircle( fromOcv( mCameraInfo.goodPoints[ i ] ),
  5.0f );
 gl::drawLine( fromOcv( mObjectInfo.goodPoints[ i ] ), 
  fromOcv( mCameraInfo.goodPoints[ i ] ) );
}</pre></div></li><li class="listitem">Now let's iterate over <code class="literal">mCorners</code> and draw the corners of the found object:<div><pre class="programlisting">gl::color( Color( 1.0f, 0.0f, 0.0f ) );
    gl::begin( GL_LINE_LOOP );
    for( vector&lt;cv::Point2f&gt;::iterator it = mCorners.begin(); it != mCorners.end(); ++it ){
        gl::vertex( it-&gt;x, it-&gt;y );
    }
    gl::end();</pre></div></li><li class="listitem">Build and run the application. Grab the physical object you depicted in the <code class="literal">object.jpg</code> image and put it in front of the image. The program will try to track that object in the camera image and draw it's corners in the image.</li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec304"/>How it works…</h2></div></div></div><p>We are using a <strong>Speeded Up Robust Features</strong> (<strong>SURF</strong>)<a id="id697" class="indexterm"/> feature detector and descriptor to identify features.<a id="id698" class="indexterm"/> In the step 4, we are calculating the features and descriptor. We use a <code class="literal">cv::SurfFeatureDetect</code> object <code class="literal">or</code> that calculates good features to track on our object. The <code class="literal">cv::SurfDescriptorExtractor</code> object then uses these features to create a description of our object. In the step 7, we do the same for the camera image.</p><p>In the step 8, we then use a <strong>Fast Library for Approximate Nearest Neighbor</strong>
<a id="id699" class="indexterm"/> (<strong>FLANN</strong>) called <code class="literal">cv::FlannBasedMatcher</code>. This matcher takes the description from both the camera frame and our object, and calculates matches between them.</p><p>In steps 9 and 10, we use the minimum distance between matches to eliminate the possible false matches. The result is passed into <code class="literal">mObjectInfo.goodPoints</code> and <code class="literal">mCameraInfo.goodPoints</code>.</p><p>In the step 11, we calculate the homography between image and camera. A homography is a projection transformation from one space to another using projective geometry. We use it in the step 12 to apply a perspective transformation to <code class="literal">mCorners</code> to identify the object corners in the camera image.</p></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec305"/>There's more…</h2></div></div></div><p>To learn more about what SURF is and how it works, please refer to the following web page: <a class="ulink" href="http://en.wikipedia.org/wiki/SURF">http://en.wikipedia.org/wiki/SURF</a>.</p><p>To learn more about FLANN, please refer to the web page <a class="ulink" href="http://en.wikipedia.org/wiki/Nearest_neighbor_search">http://en.wikipedia.org/wiki/Nearest_neighbor_search</a>.</p><p>To learn more about homography please refer to the following web page:</p><p><a class="ulink" href="http://en.wikipedia.org/wiki/Homography">http://en.wikipedia.org/wiki/Homography</a>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch11lvl1sec97"/>Reading QR code</h1></div></div></div><p>In this example we will use the ZXing library <a id="id700" class="indexterm"/>for QR code reading.</p><div><div><div><div><h2 class="title"><a id="ch11lvl2sec306"/>Getting ready</h2></div></div></div><p>Please download the Cinder ZXing block from GitHub and unpack it to the <code class="literal">blocks</code> folder: <a class="ulink" href="https://github.com/dawidgorny/Cinder-ZXing">https://github.com/dawidgorny/Cinder-ZXing</a></p></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec307"/>How to do it…</h2></div></div></div><p>We will <a id="id701" class="indexterm"/>now create a QR code reader:</p><div><ol class="orderedlist arabic"><li class="listitem">Add a header search path to the build settings of your project:<div><pre class="programlisting">$(CINDER_PATH)/blocks/zxing/include</pre></div></li><li class="listitem">Add a path from the precompiled ZXing library to the build settings of your project: <code class="literal">$(CINDER_PATH)/blocks/zxing/lib/macosx/libzxing.a</code>. For a debug configuration, use <code class="literal">$(CINDER_PATH)/blocks/zxing/lib/macosx/libzxing_d.a</code>.</li><li class="listitem">Add Cinder ZXing block files to your project structure as follows:<div><img src="img/8703OS_11_02.jpg" alt="How to do it…"/></div></li><li class="listitem">Add the <code class="literal">libiconv.dylib</code> library to the <code class="literal">Link Binary With Libraries</code> list:<div><img src="img/8703OS_11_03.jpg" alt="How to do it…"/></div></li><li class="listitem">Add the<a id="id702" class="indexterm"/> necessary header files:<div><pre class="programlisting">#include "cinder/gl/Texture.h"
#include "cinder/Surface.h"
#include "cinder/Capture.h"

#include &lt;zxing/qrcode/QRCodeReader.h&gt;
#include &lt;zxing/common/GlobalHistogramBinarizer.h&gt;
#include &lt;zxing/Exception.h&gt;
#include &lt;zxing/DecodeHints.h&gt;

#include "CinderZXing.h"</pre></div></li><li class="listitem">Add the following members to your main application class:<div><pre class="programlisting">Capture     mCapture;
Surface8u   mCaptureImg;
gl::Texture mCaptureTex;
bool        mDetected;
string      mData;</pre></div></li><li class="listitem">Inside the <code class="literal">setup</code> method, set window dimensions and initialize capturing from camera:<div><pre class="programlisting">setWindowSize(640, 480);

mDetected = false;

try {
    mCapture = Capture( 640, 480 );
    mCapture.start();
}
catch( ... ) {
    console() &lt;&lt;"Failed to initialize capture"&lt;&lt; std::endl;
}</pre></div></li><li class="listitem">Implement <a id="id703" class="indexterm"/>the <code class="literal">update</code> function as follows:<div><pre class="programlisting">if( mCapture &amp;&amp; mCapture.checkNewFrame() ) {
    mCaptureImg = mCapture.getSurface();
    mCaptureTex = gl::Texture( mCaptureImg );

    mDetected = false;

try {
        zxing::Ref&lt;zxing::SurfaceBitmapSource&gt; source(new zxing::SurfaceBitmapSource(mCaptureImg));

        zxing::Ref&lt;zxing::Binarizer&gt; binarizer(NULL);
        binarizer = new zxing::GlobalHistogramBinarizer(source);

        zxing::Ref&lt;zxing::BinaryBitmap&gt; image(new zxing::BinaryBitmap(binarizer));
        zxing::qrcode::QRCodeReader reader;
        zxing::DecodeHints hints(zxing::DecodeHints::BARCODEFORMAT_QR_CODE_HINT);

        zxing::Ref&lt;zxing::Result&gt; result( reader.decode(image, hints) );

        console() &lt;&lt;"READ("&lt;&lt; result-&gt;count() &lt;&lt;") : "&lt;&lt; result-&gt;getText()-&gt;getText() &lt;&lt; endl;

if( result-&gt;count() ) {
            mDetected = true;
            mData = result-&gt;getText()-&gt;getText();
        }

    } catch (zxing::Exception&amp; e) {
        cerr &lt;&lt;"Error: "&lt;&lt; e.what() &lt;&lt; endl;
    }

}</pre></div></li><li class="listitem">Implement the <code class="literal">draw</code> function as follows:<div><pre class="programlisting">gl::clear( Color( 0.1f, 0.1f, 0.1f ) );

gl::color(Color::white());

if(mCaptureTex) {
    gl::draw(mCaptureTex);

}

if(mDetected) {
    Vec2f pos = Vec2f( getWindowWidth()*0.5f, getWindowHeight()-100.f );
    gl::drawStringCentered(mData, pos);
}</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec308"/>How it works…</h2></div></div></div><p>We are using regular<a id="id704" class="indexterm"/> ZXing library methods. The <code class="literal">SurfaceBitmapSource</code> class delivered by the Cinder ZXing block provides integration with Cinder <code class="literal">Surface</code> type objects. While the QR code is detected and read, the <code class="literal">mDetected</code> flag is set to <code class="literal">true</code> and the read data is stored in the <code class="literal">mData</code> member.</p><div><img src="img/8703OS_11_06.jpg" alt="How it works…"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch11lvl1sec98"/>Building UI navigation and gesture recognition with Kinect</h1></div></div></div><p>In this recipe we will create<a id="id705" class="indexterm"/> interactive GUI controlled with a Kinect sensor.<a id="id706" class="indexterm"/></p><div><div><h3 class="title"><a id="tip06"/>Tip</h3><p>Since the <strong>Kinect for Windows SDK</strong> is available only for Windows, this recipe is written for Windows users only.</p></div></div><div><img src="img/8703OS_11_04.jpg" alt="Building UI navigation and gesture recognition with Kinect"/></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec309"/>Getting ready</h2></div></div></div><p>In this exampl<a id="id707" class="indexterm"/>e we are using the <code class="literal">InteractiveObject</code> class that we covered in the <em>Creating an interactive object that responds to the mouse</em> recipe from <a class="link" href="ch10.html" title="Chapter 10. Interacting with the User">Chapter 10</a>, <em>Interacting with the User</em>.</p><p>Download and install <a id="id708" class="indexterm"/>the Kinect for Windows SDK from <a class="ulink" href="http://www.microsoft.com/en-us/kinectforwindows/">http://www.microsoft.com/en-us/kinectforwindows/</a>.</p><p>Download the KinectSDK CinderBlock from GitHub at <a class="ulink" href="https://github.com/BanTheRewind/Cinder-KinectSdk">https://github.com/BanTheRewind/Cinder-KinectSdk</a>, and unpack it to the <code class="literal">blocks</code> directory.</p></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec310"/>How to do it…</h2></div></div></div><p>We will now create a <a id="id709" class="indexterm"/>Cinder application controlled with hand gestures.</p><div><ol class="orderedlist arabic"><li class="listitem">Include the necessary header files:<div><pre class="programlisting">#include "cinder/Rand.h"
#include "cinder/gl/Texture.h"
#include "cinder/Utilities.h"

#include "Kinect.h"
#include "InteractiveObject.h";</pre></div></li><li class="listitem">Add the Kinect SDK using the following statement:<div><pre class="programlisting">using namespace KinectSdk;</pre></div></li><li class="listitem">Implement <a id="id710" class="indexterm"/>the class for a waving hand gesture recognition as follows:<div><pre class="programlisting">class WaveHandGesture {
public:
  enum GestureCheckResult { Fail, Pausing, Suceed };

private:
  GestureCheckResult checkStateLeft( const Skeleton &amp; skeleton ) {
    // hand above elbow
    if (skeleton.at(JointName::NUI_SKELETON_POSITION_HAND_RIGHT).y &gt; skeleton.at(JointName::NUI_SKELETON_POSITION_ELBOW_RIGHT).y)
    {
      // hand right of elbow
      if (skeleton.at(JointName::NUI_SKELETON_POSITION_HAND_RIGHT).x &gt; skeleton.at(JointName::NUI_SKELETON_POSITION_ELBOW_RIGHT).x)
      {
        return Suceed;
      }
      return Pausing;
    }
    return Fail;
  }
  GestureCheckResult checkStateRight( const Skeleton &amp; skeleton ) {
    // hand above elbow
    if (skeleton.at(JointName::NUI_SKELETON_POSITION_HAND_RIGHT).y &gt; skeleton.at(JointName::NUI_SKELETON_POSITION_ELBOW_RIGHT).y)
    {
      // hand left of elbow
      if (skeleton.at(JointName::NUI_SKELETON_POSITION_HAND_RIGHT).x &lt; skeleton.at(JointName::NUI_SKELETON_POSITION_ELBOW_RIGHT).x)
      {
        return Suceed;
      }
      return Pausing;
    }
    return Fail;
  }

  int currentPhase;

public:
  WaveHandGesture() {
    currentPhase = 0;
  }

  GestureCheckResult check( const Skeleton &amp; skeleton )
  {
    GestureCheckResult res;
    switch(currentPhase) {
    case0: // start on left
    case2: 
      res = checkStateLeft(skeleton);
      if( res == Suceed ) { currentPhase++; }
      elseif( res == Fail ) { currentPhase = 0; return Fail; }
      return Pausing;
      break;
    case1: // to the right
    case3: 
      res = checkStateRight(skeleton);
      if( res == Suceed ) { currentPhase++; }
      elseif( res == Fail ) { currentPhase = 0; return Fail; }
      return Pausing;
      break;
    case4: // to the left
      res = checkStateLeft(skeleton);
      if( res == Suceed ) { currentPhase = 0; return Suceed; }
      elseif( res == Fail ) { currentPhase = 0; return Fail; }
      return Pausing;
      break;
    }

    return Fail;
  }
};</pre></div></li><li class="listitem">Implement <code class="literal">NuiInteractiveObject</code> extending<a id="id711" class="indexterm"/> the <code class="literal">InteractiveObject</code> class:<div><pre class="programlisting">class NuiInteractiveObject: public InteractiveObject {
public:
  NuiInteractiveObject(const Rectf &amp; rect) : InteractiveObject(rect) {
    mHilight = 0.0f;
  }

  void update(bool activated, const Vec2f &amp; cursorPos) {
    if(activated &amp;&amp; rect.contains(cursorPos)) {
      mHilight += 0.08f;
    } else {
      mHilight -= 0.005f;
    }
    mHilight = math&lt;float&gt;::clamp(mHilight);
  }

  virtualvoid draw() {
    gl::color(0.f, 0.f, 1.f, 0.3f+0.7f*mHilight);
    gl::drawSolidRect(rect);
  }

  float mHilight;
};</pre></div></li><li class="listitem">Implement the <code class="literal">NuiController</code> class<a id="id712" class="indexterm"/> that manages the active objects:<div><pre class="programlisting">class NuiController {
public:
  NuiController() {}

  void registerObject(NuiInteractiveObject *object) {
    objects.push_back( object );
  }

  void unregisterObject(NuiInteractiveObject *object) {
    vector&lt;NuiInteractiveObject*&gt;::iterator it = find(objects.begin(), objects.end(), object);
    objects.erase( it );
  }

  void clear() { objects.clear(); }

  void update(bool activated, const Vec2f &amp; cursorPos) {
    vector&lt;NuiInteractiveObject*&gt;::iterator it;
    for(it = objects.begin(); it != objects.end(); ++it) {
      (*it)-&gt;update(activated, cursorPos);
    }
  }

  void draw() {
    vector&lt;NuiInteractiveObject*&gt;::iterator it;
    for(it = objects.begin(); it != objects.end(); ++it) {
      (*it)-&gt;draw();
    }
  }

  vector&lt;NuiInteractiveObject*&gt; objects;
};</pre></div></li><li class="listitem">Add the members<a id="id713" class="indexterm"/> to you main application class for handling<a id="id714" class="indexterm"/> Kinect devices and data:<div><pre class="programlisting">KinectSdk::KinectRef          mKinect;
vector&lt;KinectSdk::Skeleton&gt;   mSkeletons;
gl::Texture                   mVideoTexture;</pre></div></li><li class="listitem">Add members to store the calculated cursor position:<div><pre class="programlisting">Rectf  mPIZ;
Vec2f  mCursorPos;</pre></div></li><li class="listitem">Add the members that we will use for gesture recognition and user activation:<div><pre class="programlisting">vector&lt;WaveHandGesture*&gt;    mGestureControllers;
bool  mUserActivated;
int  mActiveUser;</pre></div></li><li class="listitem">Add a member to handle <code class="literal">NuiController</code>:<div><pre class="programlisting">NuiController* mNuiController;</pre></div></li><li class="listitem">Set window settings by implementing <code class="literal">prepareSettings</code>:<div><pre class="programlisting">void MainApp::prepareSettings(Settings* settings)
{
  settings-&gt;setWindowSize(800, 600);
}</pre></div></li><li class="listitem">In the <code class="literal">setup</code> method, set the default values for members:<div><pre class="programlisting">mPIZ = Rectf(0.f,0.f, 0.85f,0.5f);
mCursorPos = Vec2f::zero();

mUserActivated = false;
mActiveUser = 0;</pre></div></li><li class="listitem">In the <code class="literal">setup</code> method initialize <a id="id715" class="indexterm"/>Kinect and gesture recognition for <code class="literal">10</code> users:<div><pre class="programlisting">mKinect = Kinect::create();
mKinect-&gt;enableDepth( false );
mKinect-&gt;enableVideo( false );
mKinect-&gt;start();

for(int i = 0; i &lt;10; i++) {
    mGestureControllers.push_back( new WaveHandGesture() );
}</pre></div></li><li class="listitem">In the <code class="literal">setup</code> <a id="id716" class="indexterm"/>method, initialize the user interface consisting of objects of type <code class="literal">NuiInterativeObject</code>:<div><pre class="programlisting">mNuiController = new NuiController();

float cols = 10.f;
float rows = 10.f;

Rectf rect = Rectf(0.f,0.f, getWindowWidth()/cols - 1.f, getWindowHeight()/rows - 1.f);

or(int ir = 0; ir &lt; rows; ir++) {
 for(int ic = 0; ic &lt; cols; ic++) {
  Vec2f offset = (rect.getSize()+Vec2f::one()) 
   * Vec2f(ic,ir);
  Rectf r = Rectf( offset, offset+rect.getSize() );
  mNuiController-&gt;registerObject( 
   new NuiInteractiveObject® );
 } 
}</pre></div></li><li class="listitem">In the <code class="literal">update</code> method,<a id="id717" class="indexterm"/> we are checking <a id="id718" class="indexterm"/>if the Kinect device is capturing, getting tracked skeletons, and iterating:<div><pre class="programlisting">if ( mKinect-&gt;isCapturing() ) {
 if ( mKinect-&gt;checkNewSkeletons() ) {
  mSkeletons = mKinect-&gt;getSkeletons();
 }
 uint32_t i = 0;
 vector&lt;Skeleton&gt;::const_iterator skeletonIt;
 for (skeletonIt = mSkeletons.cbegin(); 
   skeletonIt != mSkeletons.cend(); ++skeletonIt, i++ ) {</pre></div></li><li class="listitem">Inside the loop, we are checking if the skeleton is complete and deactivating the cursor controls if it is not complete:<div><pre class="programlisting">  if(mUserActivated &amp;&amp; i == mActiveUser 
   &amp;&amp; skeletonIt-&gt;size() != 
   JointName::NUI_SKELETON_POSITION_COUNT ) {
   mUserActivated = false;
  }</pre></div></li><li class="listitem">Inside the loop check if the skeleton is valid. Notice we are only processing 10 skeletons. You can modify this number, but remember to provide sufficient number of gesture controllers in <code class="literal">mGestureControllers</code>:<div><pre class="programlisting">if ( skeletonIt-&gt;size() == JointName::NUI_SKELETON_POSITION_COUNT &amp;&amp; i &lt;10 ) {</pre></div></li><li class="listitem">Inside the loop and the <code class="literal">if</code> statement, check for the completed activation gesture. While the skeleton is activated, we are calculating person interaction zone:<div><pre class="programlisting">if( !mUserActivated || ( mUserActivated &amp;&amp; i != mActiveUser ) ) {
    WaveHandGesture::GestureCheckResult res;
    res = mGestureControllers[i]-&gt;check( *skeletonIt );

    if( res == WaveHandGesture::Suceed &amp;&amp; ( !mUserActivated || i != mActiveUser ) ) {
        mActiveUser = i;

     float armLen = 0;
        Vec3f handRight = skeletonIt-&gt;at(JointName::NUI_SKELETON_POSITION_HAND_RIGHT);
        Vec3f elbowRight = skeletonIt-&gt;at(JointName::NUI_SKELETON_POSITION_ELBOW_RIGHT);
        Vec3f shoulderRight = skeletonIt-&gt;at(JointName::NUI_SKELETON_POSITION_SHOULDER_RIGHT);

        armLen += handRight.distance( elbowRight );
        armLen += elbowRight.distance( shoulderRight );

        mPIZ.x2 = armLen;
        mPIZ.y2 = mPIZ.getWidth() / getWindowAspectRatio();

        mUserActivated = true;
    }
}</pre></div></li><li class="listitem">Inside the loop <a id="id719" class="indexterm"/>and the <code class="literal">if</code> statement, we are calculating cursor positions for active users:<div><pre class="programlisting">if(mUserActivated &amp;&amp; i == mActiveUser) {
    Vec3f handPos = skeletonIt-&gt;at(JointName::NUI_SKELETON_POSITION_HAND_RIGHT);

    Rectf piz = Rectf(mPIZ);
    piz.offset( skeletonIt-&gt;at(JointName::NUI_SKELETON_POSITION_SPINE).xy() );

    mCursorPos = handPos.xy() - piz.getUpperLeft();
    mCursorPos /= piz.getSize();
    mCursorPos.y = (1.f - mCursorPos.y);
    mCursorPos *= getWindowSize();
}</pre></div></li><li class="listitem">Close the <a id="id720" class="indexterm"/>opened <code class="literal">if</code> statements and the <code class="literal">for</code> loop:<div><pre class="programlisting">        }
    }
}</pre></div></li><li class="listitem">At the end of the <code class="literal">update</code> method, update the <code class="literal">NuiController</code> object:<div><pre class="programlisting">mNuiController-&gt;update(mUserActivated, mCursorPos);</pre></div></li><li class="listitem">Implement the <code class="literal">draw</code> method as follows:<div><pre class="programlisting">void MainApp::draw()
{
  // Clear window
  gl::setViewport( getWindowBounds() );
  gl::clear( Color::white() );
  gl::setMatricesWindow( getWindowSize() );
  gl::enableAlphaBlending();

  mNuiController-&gt;draw();

  if(mUserActivated) {
    gl::color(1.f,0.f,0.5f, 1.f);
    glLineWidth(10.f);
    gl::drawStrokedCircle(mCursorPos, 25.f);
  }
}</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec311"/>How it works…</h2></div></div></div><p>The application is <a id="id721" class="indexterm"/>tracking users using Kinect SDK. Skeleton data of the active user are used to calculate the cursor position by following the guidelines provided by Microsoft with Kinect SDK documentation. Activation is invoked by a hand waving gesture.<a id="id722" class="indexterm"/></p><p>This is an example of UI responsive to cursor controlled by a user's hand. Elements of the grid light up under the cursor and fade out on roll-out.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch11lvl1sec99"/>Building an augmented reality with Kinect</h1></div></div></div><p>In this recipe we will learn<a id="id723" class="indexterm"/> how to combine both Kinect's <a id="id724" class="indexterm"/>depth and image frames to create augmented reality application.</p><div><div><h3 class="title"><a id="tip07"/>Tip</h3><p>Since Kinect for Windows SDK is available only for Windows, this recipe is written for Windows users only.</p></div></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec312"/>Getting ready</h2></div></div></div><p>Download and install Kinect for Windows SDK from <a class="ulink" href="http://www.microsoft.com/en-us/kinectforwindows/">http://www.microsoft.com/en-us/kinectforwindows/</a>.</p><p>Download KinectSDK CinderBlock from GitHub at <a class="ulink" href="https://github.com/BanTheRewind/Cinder-KinectSdk">https://github.com/BanTheRewind/Cinder-KinectSdk</a>, and unpack it to the <code class="literal">blocks</code> directory.</p><p>In this example, we are using assets from one of the sample programs delivered with the Cinder package. Please copy the <code class="literal">ducky.mshducky.png</code>, <code class="literal">phong_vert.glsl</code>, and <code class="literal">phong_frag.glsl</code> files from <code class="literal">cinder_0.8.4_mac/samples/Picking3D/resources/</code> into your <code class="literal">assets</code> folder.</p></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec313"/>How to do it…</h2></div></div></div><p>We will now create an augmented <a id="id725" class="indexterm"/>reality application using<a id="id726" class="indexterm"/> a sample 3D model.</p><div><ol class="orderedlist arabic"><li class="listitem">Include the necessary header files:<div><pre class="programlisting">#include "cinder/app/AppNative.h"
#include "cinder/gl/Texture.h"
#include "cinder/gl/GlslProg.h"
#include "cinder/TriMesh.h"
#include "cinder/ImageIo.h"
#include "cinder/MayaCamUI.h"
#include "cinder/params/Params.h"
#include "cinder/Utilities.h"

#include "Kinect.h"</pre></div></li><li class="listitem">Add the <code class="literal">using</code> statement of the Kinect SDK:<div><pre class="programlisting">using namespace KinectSdk;</pre></div></li><li class="listitem">Add the members to you main application class for handling Kinect device and data:<div><pre class="programlisting">KinectSdk::KinectRef            mKinect;
vector&lt;KinectSdk::Skeleton&gt;     mSkeletons;
gl::Texture                     mVideoTexture;</pre></div></li><li class="listitem">Add members to store 3D camera scene properties:<div><pre class="programlisting">CameraPersp        mCam;
Vec3f              mCamEyePoint;
float              mCamFov;</pre></div></li><li class="listitem">Add members to store calibration settings:<div><pre class="programlisting">Vec3f    mPositionScale;
float    mActivationDist;</pre></div></li><li class="listitem">Add members that will store geometry, texture, and shader program for 3D object:<div><pre class="programlisting">gl::GlslProg  mShader;
gl::Texture   mTexture;
TriMesh       mMesh;</pre></div></li><li class="listitem">Inside the <code class="literal">setup</code> method, set the window dimensions and initial values:<div><pre class="programlisting">setWindowSize(800, 600);

mCamEyePoint = Vec3f(0.f,0.f,1.f);
mCamFov = 33.f;

mPositionScale = Vec3f(1.f,1.f,-1.f);
mActivationDist = 0.6f;</pre></div></li><li class="listitem">Inside the <code class="literal">setup</code> method<a id="id727" class="indexterm"/> load geometry, texture, and shader program for 3D object:<div><pre class="programlisting">mMesh.read( loadFile( getAssetPath("ducky.msh") ) );

gl::Texture::Format format;
format.enableMipmapping(true);
ImageSourceRef img = loadImage( getAssetPath("ducky.png") );
if(img) mTexture = gl::Texture( img, format );

mShader = gl::GlslProg( loadFile(getAssetPath("phong_vert.glsl")), loadFile(getAssetPath("phong_frag.glsl")) );</pre></div></li><li class="listitem">Inside the <code class="literal">setup</code> method, initialize the Kinect device and start capturing:<div><pre class="programlisting">mKinect = Kinect::create();
mKinect-&gt;enableDepth( false );
mKinect-&gt;start();</pre></div></li><li class="listitem">At the end <a id="id728" class="indexterm"/>of the <code class="literal">setup</code> method, create GUI for parameter tweaking:<div><pre class="programlisting">mParams = params::InterfaceGl( "parameters", Vec2i( 200, 500 ) );
mParams.addParam("Eye Point", &amp;mCamEyePoint);
mParams.addParam("Camera FOV", &amp;mCamFov);
mParams.addParam("Position Scale", &amp;mPositionScale);
mParams.addParam("Activation Distance", &amp;mActivationDist);</pre></div></li><li class="listitem">Implement the <code class="literal">update</code> method as follows:<div><pre class="programlisting">void MainApp::update()
{
  mCam.setPerspective( mCamFov, getWindowAspectRatio(), 0.1, 10000 );
  mCam.setEyePoint(mCamEyePoint);
  mCam.setViewDirection(Vec3f(0.f,0.f, -1.f*mCamEyePoint.z));

  if ( mKinect-&gt;isCapturing() ) {
    if ( mKinect-&gt;checkNewVideoFrame() ) {
      mVideoTexture = gl::Texture( mKinect-&gt;getVideo() );
    }
    if ( mKinect-&gt;checkNewSkeletons() ) {
      mSkeletons = mKinect-&gt;getSkeletons();
    }
  }
}</pre></div></li><li class="listitem">Implement<a id="id729" class="indexterm"/> the <a id="id730" class="indexterm"/><code class="literal">drawObject</code> method<a id="id731" class="indexterm"/> that will draw our 3D model with the texture and shading applied:<div><pre class="programlisting">void MainApp::drawObject()
{

  mTexture.bind();
  mShader.bind();
  mShader.uniform("tex0", 0);

  gl::color( Color::white() );
  gl::pushModelView();
  gl::scale(0.05f,0.05f,0.05f);
  gl::rotate(Vec3f(0.f,-30.f,0.f));
  gl::draw( mMesh );
  gl::popModelView();

  mShader.unbind();
  mTexture.unbind();
}</pre></div></li><li class="listitem">Implement the <code class="literal">draw</code> method as follows:<div><pre class="programlisting">void MainApp::draw()
{
  gl::setViewport( getWindowBounds() );
  gl::clear( Colorf( 0.1f, 0.1f, 0.1f ) );
  gl::setMatricesWindow( getWindowSize() );

  if ( mKinect-&gt;isCapturing() &amp;&amp; mVideoTexture ) {
    gl::color( ColorAf::white() );
    gl::draw( mVideoTexture, getWindowBounds() );
    draw3DScene();
  }

  params::InterfaceGl::draw();
}</pre></div></li><li class="listitem">The last thing that is missing is the <code class="literal">draw3DScene</code> method invoked inside the <code class="literal">draw</code> method. <a id="id732" class="indexterm"/>Implement the <code class="literal">draw3DScene</code> method <a id="id733" class="indexterm"/>as follows:<div><pre class="programlisting">gl::enableDepthRead();
gl::enableDepthWrite();

Vec3f mLightDirection = Vec3f( 0, 0, -1 );
ColorA mColor = ColorA( 0.25f, 0.5f, 1.0f, 1.0f );

gl::pushMatrices();
gl::setMatrices( mCam );

vector&lt;KinectSdk::Skeleton&gt;::const_iterator skelIt;
for ( skelIt = mSkeletons.cbegin(); skelIt != mSkeletons.cend(); ++skelIt ) {

if ( skelIt-&gt;size() == JointName::NUI_SKELETON_POSITION_COUNT ) {
        KinectSdk::Skeleton skel = *skelIt;

        Vec3f pos, dV;
float armLen = 0;
        Vec3f handRight = skeletonIt-&gt;at(JointName::NUI_SKELETON_POSITION_HAND_RIGHT);
        Vec3f elbowRight = skeletonIt-&gt;at(JointName::NUI_SKELETON_POSITION_ELBOW_RIGHT);
        Vec3f shoulderRight = skeletonIt-&gt;at(JointName::NUI_SKELETON_POSITION_SHOULDER_RIGHT);

        armLen += handRight.distance( elbowRight );
        armLen += elbowRight.distance( shoulderRight );

        pos = skel[JointName::NUI_SKELETON_POSITION_HAND_RIGHT];
        dV = pos - skel[JointName::NUI_SKELETON_POSITION_SHOULDER_RIGHT];
if( dV.z &lt; -armLen*mActivationDist ) {
            gl::pushMatrices();
            gl::translate(pos*mPositionScale);
            drawObject();
            gl::popMatrices();
        }
    }
}

gl::popMatrices();

gl::enableDepthRead(false);
gl::enableDepthWrite(false);</pre></div></li><li class="listitem">Implement<a id="id734" class="indexterm"/> the <code class="literal">shutdown</code> method<a id="id735" class="indexterm"/> to <a id="id736" class="indexterm"/>stop capturing from Kinect on program termination:<div><pre class="programlisting">void MainApp::shutdown()
{
  mKinect-&gt;stop();
}</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch11lvl2sec314"/>How it works…</h2></div></div></div><p>The application is tracking users using the Kinect SDK. Skeleton data of the users are used to calculate the coordinates of the 3D duck model taken from one of the Cinder sample programs. The 3D model is rendered right above the right hand of the user when the user's hand is in front of the user. The activation distance is calculated using the <code class="literal">mActivationDist</code> member value.</p><div><img src="img/8703OS_11_05.jpg" alt="How it works…"/></div><p>To properly <a id="id737" class="indexterm"/>overlay<a id="id738" class="indexterm"/> 3D scene onto a video frame, you have to set the camera FOV according to the Kinect video camera. To do this, we are using the <code class="literal">Camera FOV</code> property.</p></div></div></body></html>