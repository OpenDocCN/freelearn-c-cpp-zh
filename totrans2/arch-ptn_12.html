<html><head></head><body>
        

                            
                    <h1 class="header-title">Big Data Architecture and Design Patterns</h1>
                
            
            
                
<p>Big data is the digital trace that gets generated in today's digital world when we use the internet and other digital technology. Whatever we do digitally leaves a massive volume of data. Interestingly, we can do far smarter analysis with those traces and so, therefore, make smarter decisions and much more. For example, when you log in to any website it shows an advertisement for a product that you searched or browsed earlier, even if it was on an entirely different website. So by showing the product that you are interested in, regardless of the specific product selling site, the results of big data analysis and a smart way of selling means that the end user might like the product and be more likely to buy it.</p>
<p>This chapter intends to introduce readers to the more common big data architectural patterns. Some brief details on the core parts of big data, its core principles, and characteristics are outlined,  including analytics principles, big data workload patterns, and optimal decision-making patterns.</p>
<p>Please be aware that this chapter is a mere introduction to the patterns. Readers need to refer to other materials (references sections) that are available online and offline.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The four V's of big data</h1>
                
            
            
                
<p>Big data has many definitions and many different implementations across various sectors. However, there are four common elements of any big data definition, which are popularly referred to as the V's of big data. They are as follows:</p>
<ul>
<li><strong>Velocity</strong>: This refers to the speed of data accumulation</li>
<li><strong>Volume</strong>: This refers to the scale of data or the phase that data storage grows</li>
<li><strong>Variety</strong>: This refers to the diversity of the data, such as structured, semi-structured, unstructured, and so on</li>
<li><strong>Veracity</strong>: This refers to collected data's accuracy and its reflection of facts</li>
</ul>
<p>The latest addition to the V's group is <strong>value</strong>. This refers to our ability and needs to turn accumulated data into things of value. That is not just business value, but it can also be any significant added value for social, medical, and common causes.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Big data analysis and technology concepts</h1>
                
            
            
                
<p>Let's start with the technology prerequisites for big data analysis, and then we will cover the life cycle of big data analysis. The prerequisites are:</p>
<ul>
<li>Flexible architectures, that supports various data types and patterns</li>
<li>Upstream use of analytics for data relevance optimization</li>
<li>Advanced analytics and real-time visualization to accelerate actions and understandings</li>
<li>Collaborative approaches for aligning stakeholders</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Data analysis life cycle</h1>
                
            
            
                
<p>Big data analysis life cycle provides a step-by-step methodology for organizing the data activities and tasks related to data acquiring, processing, analyzing and repurposing. The following are the stages of data analysis life cycle with a brief overview of each of them.</p>
<ul>
<li><strong>Data discovery</strong>: Learn the business domain, frame the business problems as analytics challenges, and strategize and formulate initial hypotheses to start learning data.</li>
<li><strong>Data preparations</strong>: Data <strong>Extraction</strong>, <strong>Load</strong>, and <strong>Transform</strong> (<strong>ELT</strong>) and data <strong>Extraction</strong>, <strong>Transform</strong>, and <strong>Load</strong> (<strong>ETL</strong>) should be used to become familiarized with the data.</li>
<li><strong>Model planning</strong>: Determine and formulate techniques, workflows, and best practices to follow. Learn about relationships between variables and choose the most suitable methods.</li>
<li><strong>Model building</strong>: Develop datasets for testing, training, and production deployments. Evaluate tools to run the models and suggests additional tools, workflows, and execution environments, if needed.</li>
<li><strong>Communicate results</strong>: Identify critical findings, quantify the business values of the current exercise, the success criteria, risks, and mitigations, and present them to stakeholders.</li>
<li><strong>Operationalize</strong>: Deliver proofs of concepts, final reports, and technical documents.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Big data analysis and data science</h1>
                
            
            
                
<p>Big data is the result of collecting and managing large amounts of diverse data; data mining is all about searching data for unrecognized patterns.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data analysis</h1>
                
            
            
                
<p>Data analysis is about breaking the mined data and assessing the impact of those unrecognized methods. It may even create new patterns over time and help to develop working applications.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data science</h1>
                
            
            
                
<p>Data science is the process of cleaning, mining, and analyzing the data to derive insights of value from it. Extract data insights through a combination of exploratory data analysis and modeling. Data science is the process of distilling insights from data to inform decisions.</p>
<p>Data science creates models that capture the underlying patterns of complex systems and helps those models to become working applications:</p>
<div><img height="548" width="500" src="img/e0d1e39f-094f-4e83-81b7-75f707a404a4.png"/></div>
<p>The preceding diagram intends to represent the data science process followed by a data scientist.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Big data platform</h1>
                
            
            
                
<p>Any software or hardware platform should support large datasets; otherwise, it is hard to support those large datasets with traditional database tools:</p>
<div><img height="454" width="578" class="image-border" src="img/219cf4d5-3588-42f2-b4a5-758c44eb3790.png"/></div>
<p>The preceding diagram depicts a sample big data platform with supported sample tools, servers, hardware, and so on.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Big data engineering</h1>
                
            
            
                
<p>Big data engineering gets the most value out of the vast amount of disparate data, data staging, profiling, and data cleansing in any big data platform. Also, it represents optimal ways of migrating the data from back office systems to the front office to help data analysts and data scientists:</p>
<div><img height="180" width="500" src="img/1e90ffc5-e2f5-421e-a54a-9f9d1a459364.png"/></div>
<p>The preceding diagram accounts for a sample ecosystem of a big data engineering landscape. One can find numerous tools in each stage of the big data landscape. The following are some examples of those tools: Hadoop, Oozie, Flume, Hive, HBase, Apache Pig, Apache Spark, MapReduce, YARN, Sqoop, ZooKeeper, text analytics, and so on. However, we are not going to discuss all those tools here as it is out side of the scope of this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Big data governance</h1>
                
            
            
                
<p>Any big data enterprise would need to develop and enhance broader enterprise information governance by bringing rules or policies for optimization and privacy and also find avenues for monetizing (value) at the same time as ensuring regulatory compliance and facilitating prudent risk management:</p>
<div><img height="297" width="525" src="img/6c4de7b7-c471-4d44-8414-63ccfd796e02.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Big data architecture landscape and layers</h1>
                
            
            
                
<p>You should be able to extract valuable, meaningful information (<strong>insights</strong>) from the enormous volumes of data to improve an organization's decisions that involve various challenges, such as data regulations, faster decisions, interactions with customers, dealing with legacy systems, disparate data sources, and so on. So, to address all those challenges efficiently, researchers came up with a unified architecture consisting of layers at different levels:</p>
<div><img height="274" width="289" src="img/9fb2ccd3-3281-41b6-8c76-92e6c7159802.png"/></div>
<p>The preceding pyramid depicts the significant attributes of big data layers and the problems that are addressed in each layer. As we have mentioned earlier, big data is not a single technology or a framework solving just a set of use cases; it is a set of tools, processes, technologies, and a system infrastructure that helps businesses to make much smarter analysis and take smarter decisions based on the massive volume of data traces.</p>
<p>Unified big data architecture consists of various layers. It provides a way to organize different components to address problems and it represents unique functions:</p>
<ul>
<li><strong>Big data sources</strong>: Data coming from several channels, such as handheld devices, software applications, sensors, legacy databases, and so on</li>
<li><strong>Data messaging and storage</strong>: Acquires data from the data sources, data compliance, and storage formatting</li>
<li><strong>Data analysis</strong>: Data model management, analytics engines<strong>,</strong> and access to data message stores</li>
<li><strong>Data consumption</strong>: Dashboards, insights, reporting, and so on</li>
</ul>
<div><img src="img/b39a5a89-b0e5-4863-b4c2-a44cc530ff9f.png"/></div>
<p>The preceding diagram depicts different levels and layers of the big data landscape. These layers perhaps may be considered as a summary of our earlier introductions of big data concepts and the realization of values in each layer.</p>
<p>Before we look at patterns, let's summarize the big data architecture principle as follows:</p>
<ul>
<li>Decoupled data bus</li>
<li>Right tool usage for the job</li>
<li>Data structure, latency, throughput, access patterns</li>
<li>Lambda architecture</li>
<li>Immutable logs, batch/speed/serving layer</li>
<li>Cloud-based infrastructure</li>
<li>System maintenance with low or no admin</li>
<li>Cost-effective</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Big data architecture patterns</h1>
                
            
            
                
<p>In this section, we will take you through big data design patterns, based on the following big data architectural patterns, and give a brief overview of the big data architectural patterns.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">MapReduce pattern</h1>
                
            
            
                
<p>MapReduce is a software framework implementation that processes and generates big datasets by applying parallel and distributed algorithms on a cluster infrastructure.</p>
<p>The primary methods of MapReduce are as follows:</p>
<ul>
<li><strong>Map</strong>: Responsible for filtering and sorting</li>
<li><strong>Reduce</strong>: Responsible for operations (for example, counting the number of records)</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Lambda architecture pattern</h1>
                
            
            
                
<p>To address big data challenges (described earlier in this chapter), there needs to be a data processing architecture to handle massive quantities of data to process rapidly with batch processing and stream processing methods.</p>
<p>Some fundamental characteristics of the Lambda architecture are as follows:</p>
<ul>
<li>It is dependent on underlying data principles of append-only, immutable, and atomic</li>
<li>It thrives on balancing latency, throughput, and fault-tolerance</li>
<li>It correlates with the growth of big data and real-time analytics</li>
<li>It helps to mitigate the latencies of MapReduce</li>
</ul>
<div><img height="292" width="251" src="img/e3933109-37af-40bb-966e-0e2ea821b541.png"/></div>
<p>The preceding diagram depicts the Lambda architecture with three primary layers called the batch processing layer, the speed or real-time processing layer, and serving layers for responding to queries.</p>
<p>The three primary layers are explained here:</p>
<ul>
<li><strong>Batch layer</strong>: This precomputes results, using a distributed processing system output to the read-only data store, and updates views by replacing the existing precomputed views. Data accuracy in the views is high with batch jobs (accuracy over latency).</li>
<li><strong>Speed</strong>/<strong>Real-time layer</strong>: This processes data streams in real time and the views are almost instantaneous, but maybe with less data accuracy (latency over accuracy). However, those views can be updated later by batch methods (accuracy over latency).</li>
<li><strong>Serving layer</strong>: This stores outputs from the batch and speed layers to respond to ad-hoc queries either by precomputed views or new views from the processed data.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Data lake architecture pattern</h1>
                
            
            
                
<p>In established enterprises, the most common business case is to make use of existing data infrastructure along with big data implementations. The data lake architecture pattern provides efficient ways to achieve reusing most of the data infrastructure and, at the same time, get the benefits of big data paradigm shifts.</p>
<p>Data lakes have the following essential characteristics to address:</p>
<ul>
<li>Manage abundant unprocessed data</li>
<li>Retain data as long as possible</li>
<li>Ability to manage the data transformation</li>
<li>Support dynamic schema</li>
</ul>
<p>The following diagram depicts a data lake pattern implementation. It is getting raw data into data storage from different data sources. Also, the received data needs to be retained as long as possible in the data warehouse. Conditioning is conducted only after a data source has been identified for immediate use in the mainline analytics:</p>
<div><img height="250" width="312" src="img/d0dd5344-41ee-4370-8237-50b606e33ad2.png"/></div>
<p>Data lakes provide a mechanism for capturing and exploring potentially useful data without incurring additional transactional systems storage costs, or any conditioning effort to bring data sources into those transactional systems.</p>
<p>Data lake implementation includes HDFS, AWS S3, distributed file systems, and so on. Microsoft, Amazon, EMC, Teradata, and Hortonworks are prominent vendors with data lake implementation among their products and they sell these technologies. Data lakes can also be a cloud <strong>Infrastructure as a Service </strong>(<strong>IaaS</strong>).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Big data design patterns</h1>
                
            
            
                
<p>This section covers most prominent big data design patterns by various data layers such as data sources and ingestion layer, data storage layer and data access layer.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data sources and ingestion layer</h1>
                
            
            
                
<p>Enterprise big data systems face a variety of data sources with non-relevant information (noise) alongside relevant (signal) data. Noise ratio is very high compared to signals, and so filtering the noise from the pertinent information, handling high volumes, and the velocity of data is significant. This is the responsibility of the ingestion layer. The common challenges in the ingestion layers are as follows:</p>
<ul>
<li>Multiple data source load and prioritization</li>
<li>Ingested data indexing and tagging</li>
<li>Data validation and cleansing</li>
<li>Data transformation and compression</li>
</ul>
<div><img height="181" width="518" src="img/5b98d83a-71c5-4fae-bbbf-8b4f8945c931.png"/></div>
<p>The preceding diagram depicts the building blocks of the ingestion layer and its various components. We need patterns to address the challenges of <strong>data sources</strong> to ingestion layer communication that takes care of performance, scalability, and availability requirements.</p>
<p>In this section, we will discuss the following ingestion and streaming patterns and how they help to address the challenges in ingestion layers. We will also touch upon some common workload patterns as well, including:</p>
<ul>
<li>Multisource extractor</li>
<li>Multidestination</li>
<li>Protocol converter</li>
<li><strong>Just-in-time</strong> (<strong>JIT</strong>) transformation</li>
<li>Real-time streaming pattern</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Multisource extractor</h1>
                
            
            
                
<p>An approach to ingesting multiple data types from multiple data sources efficiently is termed a <em>Multisource extractor</em>. Efficiency represents many factors, such as data velocity, data size, data frequency, and managing various data formats over an unreliable network, mixed network bandwidth, different technologies, and systems:</p>
<div><img height="229" width="314" src="img/0a8b3607-a5af-43df-883b-a87b32846c76.png"/></div>
<p>The multisource extractor system ensures high availability and distribution. It also confirms that the vast volume of data gets segregated into multiple batches across different nodes. The single node implementation is still helpful for lower volumes from a handful of clients, and of course, for a significant amount of data from multiple clients processed in batches. Partitioning into small volumes in clusters produces excellent results.</p>
<p>Data enrichers help to do initial data aggregation and data cleansing. Enrichers ensure file transfer reliability, validations, noise reduction, compression, and transformation from native formats to standard formats. Collection agent nodes represent intermediary cluster systems, which helps final data processing and data loading to the destination systems.</p>
<p>The following are the benefits of the multisource extractor:</p>
<ul>
<li>Provides reasonable speed for storing and consuming the data</li>
<li>Better data prioritization and processing</li>
<li>Drives improved business decisions</li>
<li>Decoupled and independent from data production to data consumption</li>
<li>Data semantics and detection of changed data</li>
<li>Scaleable and fault tolerance system</li>
</ul>
<p>The following are the impacts of the multisource extractor:</p>
<ul>
<li>Difficult or impossible to achieve near real-time data processing</li>
<li>Need to maintain multiple copies in enrichers and collection agents, leading to data redundancy and mammoth data volume in each node</li>
<li>High availability trade-off with high costs to manage system capacity growth</li>
<li>Infrastructure and configuration complexity increases to maintain batch processing</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Multidestination pattern</h1>
                
            
            
                
<p>In multisourcing, we saw the raw data ingestion to HDFS, but in most common cases the enterprise needs to ingest raw data not only to new HDFS systems but also to their existing traditional data storage, such as Informatica or other analytics platforms. In such cases, the additional number of data streams leads to many challenges, such as storage overflow, data errors (also known as data regret), an increase in time to transfer and process data, and so on.</p>
<p>The multidestination pattern is considered as a better approach to overcome all of the challenges mentioned previously. This pattern is very similar to multisourcing until it is ready to integrate with multiple destinations (refer to the following diagram). The router publishes the improved data and then broadcasts it to the subscriber destinations (already registered with a publishing agent on the router). Enrichers can act as publishers as well as subscribers:</p>
<div><img height="456" width="477" src="img/712d489f-f2d2-45a0-aac2-f7448b4fa5a6.png"/></div>
<p>Deploying routers in the cluster environment is also recommended for high volumes and a large number of subscribers.</p>
<p>The following are the benefits of the multidestination pattern:</p>
<ul>
<li>Highly scalable, flexible, fast, resilient to data failure, and cost-effective</li>
<li>Organization can start to ingest data into multiple data stores, including its existing RDBMS as well as NoSQL data stores</li>
<li>Allows you to use simple query language, such as Hive and Pig, along with traditional analytics</li>
<li>Provides the ability to partition the data for flexible access and decentralized processing</li>
<li>Possibility of decentralized computation in the data nodes</li>
<li>Due to replication on HDFS nodes, there are no data regrets</li>
<li>Self-reliant data nodes can add more nodes without any delay</li>
</ul>
<p>The following are the impacts of the multidestination pattern:</p>
<ul>
<li>Needs complex or additional infrastructure to manage distributed nodes</li>
<li>Needs to manage distributed data in secured networks to ensure data security</li>
<li>Needs enforcement, governance, and stringent practices to manage the integrity and consistency of data</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Protocol converter</h1>
                
            
            
                
<p>This is a mediatory approach to provide an abstraction for the incoming data of various systems. The protocol converter pattern provides an efficient way to ingest a variety of unstructured data from multiple data sources and different protocols.</p>
<p>The message exchanger handles synchronous and asynchronous messages from various protocol and handlers as represented in the following diagram. It performs various mediator functions, such as file handling, web services message handling, stream handling, serialization, and so on:</p>
<div><img height="347" width="451" class="image-border" src="img/f546fb4c-3683-48df-9e61-9319ceb237a7.png"/></div>
<p>In the protocol converter pattern, the ingestion layer holds responsibilities such as identifying the various channels of incoming events, determining incoming data structures, providing mediated service for multiple protocols into suitable sinks, providing one standard way of representing incoming messages, providing handlers to manage various request types, and providing abstraction from the incoming protocol layers.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Just-In-Time (JIT) transformation pattern</h1>
                
            
            
                
<p>The JIT transformation pattern is the best fit in situations where raw data needs to be preloaded in the data stores before the transformation and processing can happen. In this kind of business case, this pattern runs independent preprocessing batch jobs that clean, validate, corelate, and transform, and then store the transformed information into the same data store (HDFS/NoSQL); that is, it can coexist with the raw data:</p>
<div><img height="269" width="414" class="image-border" src="img/a73c69f8-f253-4aaf-a429-6cacab8bc841.png"/></div>
<p>The preceding diagram depicts the datastore with raw data storage along with transformed datasets. Please note that the data enricher of the multi-data source pattern is absent in this pattern and more than one batch job can run in parallel to transform the data as required in the big data storage, such as HDFS, Mongo DB, and so on.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Real-time streaming pattern</h1>
                
            
            
                
<p>Most modern businesses need continuous and real-time processing of unstructured data for their enterprise big data applications.</p>
<p>Real-time streaming implementations need to have the following characteristics:</p>
<ul>
<li>Minimize latency by using large in-memory</li>
<li>Event processors are atomic and independent of each other and so are easily scalable</li>
<li>Provide API for parsing the real-time information</li>
<li>Independent deployable script for any node and no centralized master node implementation</li>
</ul>
<p>The real-time streaming pattern suggests introducing an optimum number of event processing nodes to consume different input data from the various data sources and introducing listeners to process the generated events (from event processing nodes) in the event processing engine:</p>
<div><img height="320" width="379" class="image-border" src="img/820dee03-e1d2-4fb3-9689-a2e597c8e019.png"/></div>
<p>Event processing engines (event processors) have a sizeable in-memory capacity, and the event processors get triggered by a specific event. The trigger or alert is responsible for publishing the results of the in-memory big data analytics to the enterprise business process engines and, in turn, get redirected to various publishing channels (mobile, CIO dashboards, and so on).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Big data workload patterns</h1>
                
            
            
                
<p>Workload patterns help to address data workload challenges associated with different domains and business cases efficiently. The big data design pattern manifests itself in the solution construct, and so the workload challenges can be mapped with the right architectural constructs and thus service the workload.</p>
<p>The following diagram depicts a snapshot of the most common workload patterns and their associated architectural constructs:</p>
<div><img class="image-border" src="img/e00eebef-de3c-4519-8a86-436753437962.png"/></div>
<p>Workload design patterns help to simplify and decompose the business use cases into workloads. Then those workloads can be methodically mapped to the various building blocks of the big data solution architecture.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data storage layer</h1>
                
            
            
                
<p>Data storage layer is responsible for acquiring all the data that are gathered from various data sources and it is also liable for converting (if needed) the collected data to a format that can be analyzed. The following sections discuss more on data storage layer patterns. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">ACID versus BASE versus CAP</h1>
                
            
            
                
<p>Traditional RDBMS follows <strong>atomicity</strong>, <strong>consistency</strong>, <strong>isolation</strong>, <strong>and durability</strong> (<strong>ACID</strong>) to provide reliability for any user of the database. However, searching high volumes of big data and retrieving data from those volumes consumes an enormous amount of time if the storage enforces ACID rules. So, big data follows <strong>basically available</strong>, <strong>soft state</strong>, <strong>eventually consistent</strong> (<strong>BASE</strong>), a phenomenon for undertaking any search in big data space.</p>
<p>Database theory suggests that the NoSQL big database may predominantly satisfy two properties and relax standards on the third, and those properties are <strong>consistency</strong>, <strong>availability</strong>, <strong>and partition tolerance</strong> (<strong>CAP</strong>).</p>
<p>With the ACID, BASE, and CAP paradigms, the big data storage design patterns have gained momentum and purpose. We will look at those patterns in some detail in this section. The patterns are:</p>
<ul>
<li>Façade pattern</li>
<li>NoSQL pattern</li>
<li>Polyglot pattern</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Façade pattern</h1>
                
            
            
                
<p>This pattern provides a way to use existing or <strong>traditional existing data warehouses</strong> along with big data storage (such as Hadoop). It can act as a façade for the enterprise data warehouses and business intelligence tools.</p>
<p>In the façade pattern, the data from the different data sources get aggregated into <strong>HDFS</strong> before any transformation, or even before loading to the <strong>traditional existing data warehouses</strong>:</p>
<div><img height="272" width="488" class="image-border" src="img/b003b1dd-2ba3-4c27-9ab9-42f843c49d1a.png"/></div>
<p>The façade pattern allows structured data storage even after being ingested to HDFS in the form of structured storage in an <strong>RDBMS</strong>, or in NoSQL databases, or in a memory cache. The façade pattern ensures reduced data size, as only the necessary data resides in the structured storage, as well as faster access from the storage.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">NoSQL pattern</h1>
                
            
            
                
<p>This pattern entails getting NoSQL alternatives in place of traditional RDBMS to facilitate the rapid access and querying of big data. The NoSQL database stores data in a columnar, non-relational style. It can store data on local disks as well as in HDFS, as it is HDFS aware. Thus, data can be distributed across data nodes and fetched very quickly.</p>
<p>Let's look at four types of NoSQL databases in brief:</p>
<ul>
<li><strong>Column-oriented DBMS</strong>: Simply called a columnar store or big table data store, it has a massive number of columns for each tuple. Each column has a column key. Column family qualifiers represent related columns so that the columns and the qualifiers are retrievable, as each column has a column key as well. These data stores are suitable for fast writes.</li>
</ul>
<div><img height="98" width="429" class="image-border" src="img/73b124c4-a9df-4803-8b42-1c013882d495.png"/></div>
<ul>
<li><strong>Key-value pair database</strong>: A key-value database is a data store that, when presented with a simple string (key), returns an arbitrarily large data (value). The key is bound to the value until it gets a new value assigned into or from a database. The key-value data store does not need to have a query language. It provides a way to add and remove key-value pairs. A key-value store is a dictionary kind of data store, where it has a list of words and each word represents one or more definitions.</li>
<li><strong>Graph database</strong>: This is a representation of a system that contains a sequence of nodes and relationships that creates a graph when combined. A graph represents three data fields: nodes, relationships, and properties. Some types of graph store are referred to as triple stores because of their node-relationship-node structure. You may be familiar with applications that provide evaluations of similar or likely characteristics as part of the search (for example, a user bought this item also bought... is a good illustration of graph store implementations).</li>
</ul>
<div><img height="105" width="237" class="image-border" src="img/aa3ed6f0-9794-4b41-bebe-6daf15b720fd.png"/></div>
<ul>
<li><strong>Document database</strong>: We can represent a graph data store as a tree structure. Document trees have a single root element or sometimes even multiple root elements as well. Note that there is a sequence of branches, sub-branches, and values beneath the root element. Each branch can have an expression or relative path to determine the traversal path from the origin node (root) and to any given branch, sub-branch, or value. Each branch may have a value associated with that branch. Sometimes the existence of a branch of the tree has a specific meaning, and sometimes a branch must have a given value to be interpreted correctly.</li>
</ul>
<div><img height="311" width="398" class="image-border" src="img/00750a0b-f843-45f4-942e-7faa2c83fb8b.png"/></div>
<p>The following table summarizes some of the NoSQL use cases, providers, tools and scenarios that might need NoSQL pattern considerations. Most of this pattern implementation is already part of various vendor implementations, and they come as out-of-the-box implementations and as plug and play so that any enterprise can start leveraging the same quickly.</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<div><strong>NoSQL DB to Use</strong></div>
</td>
<td>
<div><strong>Scenario</strong></div>
</td>
<td>
<div><strong>Vendor / Application / Tools</strong></div>
</td>
</tr>
<tr>
<td>
<p>Columnar database</p>
</td>
<td>
<p>Application that needs to fetch entire related columnar family based on a given string: for example, search engines</p>
</td>
<td>
<p>SAP HANA / IBM DB2 BLU / ExtremeDB / EXASOL / IBM Informix / MS SQL Server / MonetDB</p>
</td>
</tr>
<tr>
<td>
<p>Key Value Pair database</p>
</td>
<td>
<p>Needle in haystack applications (refer to the <em>Big data workload patterns</em> given in this section)</p>
</td>
<td>
<p>Redis / Oracle NoSQL DB / Linux DBM / Dynamo / Cassandra</p>
</td>
</tr>
<tr>
<td>
<p>Graph database</p>
</td>
<td>
<p>Recommendation engine: application that provides evaluation of <em>Similar to / Like</em>: for example, <em>User that bought this item also bought</em></p>
</td>
<td>
<p>ArangoDB / Cayley / DataStax / Neo4j / Oracle Spatial and Graph / Apache Orient DB / Teradata Aster</p>
</td>
</tr>
<tr>
<td>
<p>Document database</p>
</td>
<td>
<p>Applications that evaluate churn management of social media data or non-enterprise data</p>
</td>
<td>
<p>Couch DB / Apache Elastic Search / Informix / Jackrabbit / Mongo DB / Apache SOLR</p>
</td>
</tr>
</tbody>
</table>
<p> </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Polyglot pattern</h1>
                
            
            
                
<p>Traditional (RDBMS) and multiple storage types (files, CMS, and so on) coexist with big data types (NoSQL/HDFS) to solve business problems.</p>
<p>Most modern business cases need the coexistence of legacy databases. At the same time, they would need to adopt the latest big data techniques as well. Replacing the entire system is not viable and is also impractical. The polyglot pattern provides an efficient way to combine and use multiple types of storage mechanisms, such as Hadoop, and RDBMS. Big data appliances coexist in a storage solution:</p>
<div><img height="370" width="461" class="image-border" src="img/1319b88d-5143-4afa-b94a-c25853e98785.png"/></div>
<p>The preceding diagram represents the polyglot pattern way of storing data in different storage types, such as RDBMS, key-value stores, NoSQL database, CMS systems, and so on. Unlike the traditional way of storing all the information in one single data source, polyglot facilitates any data coming from all applications across multiple sources (RDBMS, CMS, Hadoop, and so on) into different storage mechanisms, such as in-memory, RDBMS, HDFS, CMS, and so on.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data access layer</h1>
                
            
            
                
<p>Data access in traditional databases involves JDBC connections and HTTP access for documents. However, in big data, the data access with conventional method does take too much time to fetch even with cache implementations, as the volume of the data is so high.</p>
<p>So we need a mechanism to fetch the data efficiently and quickly, with a reduced development life cycle, lower maintenance cost, and so on.</p>
<p>Data access patterns mainly focus on accessing big data resources of two primary types:</p>
<ul>
<li>End-to-end user-driven API (access through simple queries)</li>
<li>Developer API (access provision through API methods)</li>
</ul>
<p>In this section, we will discuss the following data access patterns that held efficient data access, improved performance, reduced development life cycles, and low maintenance costs for broader data access:</p>
<ul>
<li>Connector pattern</li>
<li>Lightweight stateless pattern</li>
<li>Service locator pattern</li>
<li>Near real-time pattern</li>
<li>Stage transform pattern</li>
</ul>
<div><img height="251" width="501" class="image-border" src="img/9c91c144-6ee1-4389-b8fe-a0bb68524e77.png"/></div>
<p>The preceding diagram represents the big data architecture layouts where the big data access patterns help data access. We discuss the whole of that mechanism in detail in the following sections.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Connector pattern</h1>
                
            
            
                
<p>The developer API approach entails fast data transfer and data access services through APIs. It creates optimized data sets for efficient loading and analysis. Some of the big data appliances abstract data in NoSQL DBs even though the underlying data is in HDFS, or a custom implementation of a filesystem so that the data access is very efficient and fast.</p>
<p>The connector pattern entails providing developer API and SQL like query language to access the data and so gain significantly reduced development time. As we saw in the earlier diagram, big data appliances come with connector pattern implementation. The big data appliance itself is a complete big data ecosystem and supports virtualization, redundancy, <strong>replication using protocols</strong> (<strong>RAID</strong>), and some appliances host NoSQL databases as well.</p>
<div><img height="261" width="400" class="image-border" src="img/d2b3223c-4262-4a25-beac-edf8ff85b1b7.png"/></div>
<p>The preceding diagram shows a sample connector implementation for Oracle big data appliances. The data connector can connect to Hadoop and the big data appliance as well. It is an example of a custom implementations that we described earlier to facilitate faster data access with less development time.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Lightweight stateless pattern</h1>
                
            
            
                
<p>This pattern entails providing data access through web services, and so it is independent of platform or language implementations. The data is fetched through restful HTTP calls, making this pattern the most sought after in cloud deployments. WebHDFS and HttpFS are examples of lightweight stateless pattern implementation for HDFS HTTP access. It uses the HTTP REST protocol. The HDFS system exposes the REST API (web services) for consumers who analyze big data. This pattern reduces the cost of ownership (pay-as-you-go) for the enterprise, as the implementations can be part of an <strong>integration Platform as a Service</strong> (<strong>iPaaS</strong>):</p>
<div><img height="249" width="506" class="image-border" src="img/5b23b296-cfd1-4802-92bd-cec481f41d73.png"/></div>
<p>The preceding diagram depicts a sample implementation for HDFS storage that exposes HTTP access through the HTTP web interface.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Service locator pattern</h1>
                
            
            
                
<p>In a big data storage landscape, there are different types of data format (polyglot persistence), and if one needs to select and analyze a specific storage type from the list of stored data, then the service locator pattern comes in handy. It provides the flexibility to manipulate, filter, select, and co-relate services from the service catalog when storage access is with a SaaS model:</p>
<div><img height="195" width="503" class="image-border" src="img/7f6b5f03-ee8f-4dee-9d2c-702a2657415f.png"/></div>
<p>The preceding diagram shows a sample implementation of a service locator pattern. Observed data from various sources get aggregated and exposed through a service catalog and is available for visualization, or perhaps for further analysis. Service aggregators can aggregate services within or outside of enterprises. Different visualization tools can mix and match these services to show enterprise data alongside social media which is a different format than the other data source formats.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Near real-time pattern</h1>
                
            
            
                
<p>For any enterprise to implement real-time data access or near real-time data access, the key challenges to be addressed are:</p>
<ul>
<li><strong>Rapid determination of data</strong>: Ensure rapid determination of data and make swift decisions (within a few seconds, not in minutes) before the data becomes meaningless</li>
<li><strong>Rapid analysis</strong>: Ability to analyze the data in real time and spot anomalies and relate them to business events, provide visualization, and generate alerts at the moment that the data arrived</li>
</ul>
<p>Some examples of systems that would need real-time data analysis are:</p>
<ul>
<li>Radar systems</li>
<li>Customer services applications</li>
<li>ATMs</li>
<li>Social media platforms</li>
<li>Intrusion detection systems</li>
</ul>
<p>Storm and in-memory applications such as Oracle Coherence, Hazelcast IMDG, SAP HANA, TIBCO, Software AG (Terracotta), VMware, and Pivotal GemFire XD are some of the in-memory computing vendor/technology platforms that can implement near real-time data access pattern applications:</p>
<div><img height="212" width="380" class="image-border" src="img/7ce82b71-635a-4f3a-9898-c544a26e3b26.png"/></div>
<p>As shown in the preceding diagram, with multi-cache implementation at the ingestion phase, and with filtered, sorted data in multiple storage destinations (here one of the destinations is a cache), one can achieve near real-time access. The cache can be of a NoSQL database, or it can be any in-memory implementations tool, as mentioned earlier. The preceding diagram depicts a typical implementation of a log search with SOLR as a search engine.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Stage transform pattern</h1>
                
            
            
                
<p>In the big data world, a massive volume of data can get into the data store. However, all of the data is not required or meaningful in every business case. The stage transform pattern provides a mechanism for reducing the data scanned and fetches only relevant data.</p>
<p>HDFS has raw data and business-specific data in a NoSQL database that can provide application-oriented structures and fetch only the relevant data in the required format:</p>
<div><img height="287" width="315" class="image-border" src="img/b15c6261-0f0d-46bc-87f9-257807dd1945.png"/></div>
<p>Combining the stage transform pattern and the NoSQL pattern is the recommended approach in cases where a reduced data scan is the primary requirement. The preceding diagram depicts one such case for a recommendation engine where we need a significant reduction in the amount of data scanned for an improved customer experience.</p>
<p>The implementation of the virtualization of data from HDFS to a NoSQL database, integrated with a big data appliance, is a highly recommended mechanism for rapid or accelerated data fetch. We have already seen that in the near real-time implementation shown earlier in this section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Rapid data analysis pattern</h1>
                
            
            
                
<p>For faster data processing and access, the enterprise can choose any of the following tools in its data landscape. Each implementation has its own merits and purpose; we suggest reading each implementation in detail from the references that we have provided and choose the best for your enterprise needs:</p>
<ul>
<li class="mce-root">Apache Hadoop </li>
<li>Bash Reduce</li>
<li>Disco (Nokia Research)</li>
<li>Apache Spark</li>
<li>Graph Lab</li>
<li>Apache Storm</li>
<li>Google Big Query</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Data discovery and analysis layer</h1>
                
            
            
                
<p>Data discovery and analysis in big data is different from the traditional analysis of structured RDBMS data from limited sets. Big data analysis needs a more sophisticated mechanism, as it involves natural language processing, unstructured texts, videos and images, RFID data, and so on. This section touches upon some data discovery and analysis patterns and mentions the tools that are supporting these patterns. Readers are encouraged to read other referenced materials to get a more profound understanding of each pattern:</p>
<div><img height="236" width="523" class="image-border" src="img/ece67b31-700a-4344-bf5f-73ad7dc1b3ba.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Data queuing pattern</h1>
                
            
            
                
<p>It is a most common situation that a system needs to handle spikes while analyzing data. This pattern introduces a workflow or process to queue additional chunks of data and then route them to available nodes:</p>
<div><img height="158" width="480" class="image-border" src="img/ef55dc1c-a7b0-45d5-8771-b7c964733aa5.png"/></div>
<p>The preceding diagram depicts a sample implementation of a data queue and processors for additional workflows and routes to available nodes (of multiple nodes).</p>
<p>Using cloud IaaS is the best option to handle the spikes dynamically and yield better cost savings. It spins additional virtual machines as needed, with more when there is a spike, and fewer when traffic is slow or average).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Index-based insight pattern</h1>
                
            
            
                
<p>This pattern defines indexes (keys) based on the inputs from the users who interact with customers. Iteratively, finding a range of indexes is the mechanism suggested by the index-based insight pattern. It sets the analysis mechanism or pattern to index a variable and to provide insight into common behaviors such as parents buying toys, and all children aged above 13 in a neighborhood. This pattern helps to find a crucial efficient lookup for rapid scanning but keeps related columns together.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Machine learning pattern</h1>
                
            
            
                
<p>This pattern helps to find a pattern of data inputs generated from heterogeneous devices, such as RFID devices, energy meters, signal devices, weather-related devices, and so on.</p>
<p>Understanding data generated by automated systems, or devices without manual intervention, is a challenging task, and one needs to rely on algorithms and statistical methods. Fortunately, there are excellent algorithms that help to analyze this data, and some of the conventional algorithms are as follows:</p>
<ul>
<li class="mce-root">Naïve Bayes classifier algorithm</li>
<li>K Means clustering algorithm</li>
<li>Support vector machine algorithm</li>
<li>Apriori algorithm</li>
<li>Linear regression</li>
<li>Hypothesis testing</li>
<li>Clustering</li>
<li>ANOVA</li>
<li>Logistic regression</li>
<li>Neural networks / artificial neural networks</li>
<li>Random forests</li>
<li>Decision trees</li>
<li>Nearest neighbors</li>
<li>Principal component analysis</li>
<li>Conjoint analysis</li>
<li>Ensemble methods</li>
</ul>
<p>We can use one or more combinations of these algorithms as needed. Readers are encouraged to refer to other materials to get an insight into each algorithm, as covering them is not in the scope of this section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Converge(r) pattern</h1>
                
            
            
                
<p>In most business cases, as we have seen earlier, enterprises need to deal with traditional (structured) data and at the same time make use of big data to get enterprise-wide insights. The converge(r) pattern provides an efficient way to merge unstructured data with structured data and get insights and make decisions.</p>
<p>In some business cases, enterprises may need to understand the sentiments (views and opinions) of their product from social media. The converge(r) pattern, combining external data formats with internal enterprise data formats, is one of the best options. This pattern entails combining those views and opinions from social media with internal data analysis to get combined data insights.</p>
<p>The data convergence needs to happen before the enterprise data is analyzed. So we can use the façade pattern (refer to the <em>Data storage layer</em> section in this chapter), and also use machine learning patterns to use the grouped data from the social media (for impacts, revenues, brand images, churn rates, and so on).</p>
<p>Tools such as DrivenData, TianChi, Crowd Analytics, InfoChimps, Kaggle, and TopCoder provide out-of-the-box converge(r) implementation, and we can use those tools along with ETL tools for data transformation, cleansing, and enrichment, and get insights by combining the data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data visualization layer</h1>
                
            
            
                
<p>Data visualization's primary responsibility is to provide more insights from the massive volume of data by using visual representations, such as statistical reports, charts, and so on. Visualization of insights is the most visible portion to the stakeholders and sponsors; it is the most impactful part of the whole big data paradigm.</p>
<p>As visualization is most impactful and considering the vastness of the visualization, this section aims to provide only a brief introduction to a few of the common visualization patterns. However, we encourage readers to explore the exclusive visualization materials that we have provided in the reference sections.</p>
<div><img height="228" width="544" class="image-border" src="img/10614bfc-e33e-473e-82f9-3f2b84e52a0f.png"/></div>
<p>The preceding diagram depicts data visualization patterns in a sample big data landscape. Visualization patterns need to support high-level views and also granular level details as visual representations. Moreover, visualization patterns can be used in conjunction with data access patterns to leverage the rapid access of data and its presentation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">First glimpse pattern</h1>
                
            
            
                
<p>As the name suggests, this is an approach that provides primary or minimalistic visualization data and pulls detailed information only on demand.</p>
<p>This pattern entails fetching only the most critical and essential data (which may be decided by machine learning patterns, rankings, scores, and so on) as a first glimpse and fetches drill-down data on demand. An example could be a search application displaying search results as only one page (the first page) and providing more data when the user needs it on subsequent pages.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Portal pattern</h1>
                
            
            
                
<p>With most common cases where the enterprise already has reporting applications and intends to reuse the same for the visualization of big data, then this pattern entails enhancing the web application (portal) with scripting frameworks to enhance the legacy visualization, thus saving the enterprise the cost of having a new visualization tool.</p>
<p>The following lists some of the scripting frameworks one may want to include and enhance with enterprise portal and realize the portal pattern:</p>
<ul>
<li class="mce-root">D3.js</li>
<li>Chart.js</li>
<li>HighChart.js</li>
<li>ChartList.js</li>
<li>Raphel</li>
<li>Processing.js</li>
<li>Pixi.js</li>
<li>Webix</li>
<li>AnyChart</li>
<li>Flot</li>
<li>Pykcharts</li>
<li>Cytoscape.js</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Mashup view pattern</h1>
                
            
            
                
<p>Mashup view creates an aggregated mashup view from heterogeneous data stores such as Hadoop, cache, and RDBMS, thereby reducing the analysis time by aggregating the results of the queries.</p>
<p>It helps to achieve higher performance for the queries by storing an aggregated mashups view in the HIVE layer, similar to the traditional data warehouse. The updates to the data warehouse are made as offline batch jobs:</p>
<table>
<tbody>
<tr>
<td>
<div><strong>Some mashup view supported (vendor) tools</strong></div>
</td>
<td>
<div><strong>Some data integration mashup tools</strong></div>
</td>
</tr>
<tr>
<td>
<ul>
<li class="mce-root">IBM Netezza</li>
<li>Cassandra</li>
<li>Vertica, Cloudera Impala</li>
<li>Hortonworks Stinger</li>
</ul>
</td>
<td>
<ul>
<li class="mce-root">Damia</li>
<li>Yahoo Pipes</li>
<li>MS Popfly</li>
<li><strong>Google Mashup Editor</strong> (<strong>GME</strong>)</li>
<li>Exhibit</li>
<li>Apatar</li>
<li>MashMaker</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Table 12.2: Mashup view supported tools and data integration tools</p>
<p>Some drawbacks with mashups that you may need to be aware of are text/data mismatch, object identifiers, schema mismatches, abstract level mismatches, and lower data quality or accuracy (due to data integration from independent sources).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Compression pattern</h1>
                
            
            
                
<p>Compression is one of the data reduction methods of big data analysis, as reduced data size is computationally less expensive.</p>
<p>The compression pattern provides a mechanism in situations where the enterprise needs to access data without aggregation or mashups. The compression pattern can help with faster data access from data storage by having standardized formats (with the need to transform to a standardized format regardless of data sources). The advantage of having formats is to ensure data correctness and consistency.</p>
<p>The most popular compression data analysis platform is R, and one can explore in-memory compression with ReRams as well.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exploder pattern</h1>
                
            
            
                
<p>This is a pattern to help data analysts to look at different datasets, finding a relation between different datasets, and also providing different perspectives. The exploder pattern is a useful pattern in cases where an enterprise need various views (visuals) for the data and there are no restrictions with the same kind of visual patterns.</p>
<p>It also allows one to drill down from one view to a different chart type or visualization pattern with a click.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Although the development field of data analytics is not new, it has become more critical than ever as it experiences prodigious quantities of data generated by businesses, sensors, applications, and so on. Once the generated data gets stored, it can give extraordinary insights and helps not only business enterprises but also government and non-government enterprises, social communities, the economy, and much more.</p>
<p>In current technology trends, big data has been involved in many evolutions, from just buzzwords to crunching data from machine learning algorithms. With the exponential explosion of high velocity, high volume, high variety, and the veracity of data sources and streams (the four V's), big data has become the inevitable representative of the architectures, tools, and technologies that handle enterprises increasingly demanding requirements.</p>
<p>In this chapter, we have gone through a brief introduction of the four V's of big data, data analysis technology, and concepts. We also touched upon the big data life cycle and how it helps different stakeholders to achieve and realize their data insights. A brief section covered big data landscapes, and the data layers, as well as most of the architectural patterns associated with big data, involving data pipelines: that is an ordered combination of data acquisition, integration, ingestion, fast processing, storage, rapid access, and analytics stages.</p>
<p>The most crucial theme of this book is architectural patterns, and this chapter reflects it in its big data architecture, and design patterns section, in a sequence of architecture patterns, such as MapReduce, Lambda, and data lake. Then we have covered most common big data (application) design patterns by layers: that is patterns in various big data architectural layers, such as data sources and the ingestion layer, the data storage layer, the data access layer, the data discovery and analysis layer, and the data visualization layer.</p>
<p>Covering big data architectural patterns in one chapter has been very challenging for us, and we have tried our best by providing samples of big data concepts and the most common patterns that help data architects and other data technology stakeholders. We hope this chapter provides them with a head start on their big data journey. As mentioned in many places across this chapter, we strongly encourage readers to refer to the citations section should they need to get exclusive patterns and details of implementations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">References</h1>
                
            
            
                
<p>Citations and reference materials:</p>
<ul>
<li><strong>Big data</strong>: <em>Application Architecture Q and A, A Problem-Solution Approach</em> by Nitin Sawant and Himanshu Shah (Apress 2013)</li>
<li><strong>Big data governance</strong>: <em>An Emerging Imperative</em> by Sunil Soares, (MC Press, October 2012)</li>
</ul>
<p>Other sources:</p>
<ul>
<li><a href="http://assured-cloud-computing.illinois.edu/files/2015/02/Cristina_Abad.pdf">http://assured-cloud-computing.illinois.edu/files/2015/02/Cristina_Abad.pdf</a></li>
<li><a href="http://bigr.io/architecture/">http://bigr.io/architecture/</a></li>
<li><a href="http://blog.flutura.com//2012/08/11-core-big-data-workload-design.html">http://blog.flutura.com//2012/08/11-core-big-data-workload-design.html</a></li>
<li><a href="http://ercoppa.github.io/HadoopInternals/HadoopArchitectureOverview.html">http://ercoppa.github.io/HadoopInternals/HadoopArchitectureOverview.html</a></li>
<li><a href="http://insightdatascience.com">http://insightdatascience.com</a></li>
<li><a href="http://www.bcs.org/upload/pdf/enterprise-architecture-patterns-201016.pdf">http://www.bcs.org/upload/pdf/enterprise-architecture-patterns-201016.pdf</a></li>
<li><a href="http://www.bigdatapatterns.org/design_patterns/automated_dataset_execution">http://www.bigdatapatterns.org/design_patterns/automated_dataset_execution</a></li>
<li><a href="http://www.bigdatapatterns.org/overview">http://www.bigdatapatterns.org/overview</a></li>
<li><a href="http://www.bigdatascienceschool.com/selfstudy">http://www.bigdatascienceschool.com/selfstudy</a></li>
<li><a href="http://www.infoworld.com/article/2616959/big-data/7-top-tools-for-taming-big-data.html">http://www.infoworld.com/article/2616959/big-data/7-top-tools-for-taming-big-data.html</a></li>
<li><a href="http://www.pentaho.com/sites/default/files/uploads/resources/forrester_patterns_in_big_data.pdf">http://www.pentaho.com/sites/default/files/uploads/resources/forrester_patterns_in_big_data.pdf</a></li>
<li><a href="http://www.refcodes.org/resources/Big%20data%20processing%20the%20lean%20way%20-%20a%20case%20study%20-%20v1.7.pdf">http://www.refcodes.org/resources/Big%20data%20processing%20the%20lean%20way%20-%20a%20case%20study%20-%20v1.7.pdf</a></li>
<li><a href="http://www.yottastor.com/design-principles-big-data">http://www.yottastor.com/design-principles-big-data</a></li>
<li><a href="https://arxiv.org/ftp/arxiv/papers/1201/1201.4479.pdf">https://arxiv.org/ftp/arxiv/papers/1201/1201.4479.pdf</a></li>
<li><a href="https://bigdatawg.nist.gov/_uploadfiles/M0060_v1_8912129783.pdf">https://bigdatawg.nist.gov/_uploadfiles/M0060_v1_8912129783.pdf</a></li>
<li><a href="https://blogs.msmvps.com/abu/2010/10/16/data-architecture-patterns-design-patterns-and-solution-patterns/">https://blogs.msmvps.com/abu/2010/10/16/data-architecture-patterns-design-patterns-and-solution-patterns/</a></li>
<li><a href="https://conferences.oreilly.com/strata/big-data-conference-ca-2015/public/schedule/detail/38774">https://conferences.oreilly.com/strata/big-data-conference-ca-2015/public/schedule/detail/38774</a></li>
<li><a href="https://conferences.oreilly.com/strata/strataeu2014/public/schedule/detail/37305">https://conferences.oreilly.com/strata/strataeu2014/public/schedule/detail/37305</a></li>
<li><a href="https://hackernoon.com/ingestion-and-processing-of-data-for-big-data-and-iot-solutions-659431e37b52">https://hackernoon.com/ingestion-and-processing-of-data-for-big-data-and-iot-solutions-659431e37b52</a></li>
<li><a href="https://iwringer.wordpress.com/2015/08/03/patterns-for-streaming-realtime-analytics/">https://iwringer.wordpress.com/2015/08/03/patterns-for-streaming-realtime-analytics/</a></li>
<li><a href="https://link.springer.com/book/10.1007%2F978-1-4302-6293-0">https://link.springer.com/book/10.1007%2F978-1-4302-6293-0</a></li>
<li><a href="https://static1.squarespace.com/static/55007c24e4b001deff386756/t/564a2b7de4b0c1a8406915fb/1447701373291/Maniyam%2C+Sujee.pdf">https://static1.squarespace.com/static/55007c24e4b001deff386756/t/564a2b7de4b0c1a8406915fb/1447701373291/Maniyam%2C+Sujee.pdf</a></li>
<li><a href="https://vision.cloudera.com/the-six-principles-of-modern-data-architecture/">https://vision.cloudera.com/the-six-principles-of-modern-data-architecture/</a></li>
<li><a href="https://www.datameer.com/wp-content/uploads/pdf/white_paper/Data-Preparation-Modern-BI-Common-Design-Patterns.pdf">https://www.datameer.com/wp-content/uploads/pdf/white_paper/Data-Preparation-Modern-BI-Common-Design-Patterns.pdf</a></li>
<li><a href="https://www.dezyre.com/article/types-of-analytics-descriptive-predictive-prescriptive-analytics/209">https://www.dezyre.com/article/types-of-analytics-descriptive-predictive-prescriptive-analytics/209</a></li>
<li><a href="https://www.ibm.com/developerworks/library/bd-archpatterns1/index.html">https://www.ibm.com/developerworks/library/bd-archpatterns1/index.html</a></li>
<li><a href="https://www.import.io/post/best-big-data-tools-use/">https://www.import.io/post/best-big-data-tools-use/</a></li>
<li><a href="https://www.linkedin.com/pulse/top-10-guiding-principles-big-data-architecture-ram-narasimhan">https://www.linkedin.com/pulse/top-10-guiding-principles-big-data-architecture-ram-narasimhan</a></li>
<li><a href="https://www.researchgate.net/publication/296634867_Device_Data_Ingestion_for_Industrial_Big_Data_Platforms_with_a_Case_Study">https://www.researchgate.net/publication/296634867_Device_Data_Ingestion_for_Industrial_Big_Data_Platforms_with_a_Case_Study</a></li>
<li><a href="https://www.slideshare.net/AmazonWebServices/big-data-architectural-patterns-and-best-practices">https://www.slideshare.net/AmazonWebServices/big-data-architectural-patterns-and-best-practices</a></li>
<li><a href="https://www.slideshare.net/AsterData/sas-ny-big-analytics-conference">https://www.slideshare.net/AsterData/sas-ny-big-analytics-conference</a></li>
<li><a href="https://www.slideshare.net/cscyphers/big-data-platforms-an-overview">https://www.slideshare.net/cscyphers/big-data-platforms-an-overview</a></li>
<li><a href="https://www.slideshare.net/ZachGemignani/7-design-principles-44395597">https://www.slideshare.net/ZachGemignani/7-design-principles-44395597</a></li>
</ul>
<p> </p>
<p> </p>
<p class="mce-root"/>


            

            
        
    </body></html>