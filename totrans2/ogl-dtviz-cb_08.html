<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Interactive Real-time Data Visualization on Mobile Devices</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Visualizing real-time data from built-in Inertial Measurement Units (IMUs)</li><li class="listitem" style="list-style-type: disc">Part I – handling multi-touch interface and motion sensor inputs</li><li class="listitem" style="list-style-type: disc">Part II – interactive, real-time data visualization with mobile GPUs</li></ul></div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec49"/>Introduction</h1></div></div></div><p>In this chapter, we will demonstrate how to visualize data interactively using built-in motion sensors called <a id="id366" class="indexterm"/>
<strong>Inertial Measurement Units</strong> (<strong>IMUs</strong>) and the multi-touch interface on mobile devices. We will further explore the use of shader programs to accelerate computationally intensive operations to enable real-time visualization of 3D data with mobile graphics hardware. We will assume familiarity with the basic framework for building an Android-based OpenGL ES 3.0 application introduced in the previous chapter and add significantly more complexity in the implementation in this chapter to achieve interactive, real-time 3D visualization of a Gaussian function using both motion sensors and the multi-touch gesture interface. The final demo is designed to work on any Android-based mobile device with proper sensor hardware support.</p><p>Here, we will first introduce how to extract data directly from the IMUs and plot the real-time data stream acquired on an Android device. We will divide the final demo into two parts given its complexity. In part I, we will demonstrate how to handle the multi-touch interface and motion sensor inputs on the Java side. In part II, we will demonstrate how to implement the shader program in OpenGL ES 3.0 and other components of the native code to finish our interactive demo.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec50"/>Visualizing real-time data from built-in Inertial Measurement Units (IMUs)</h1></div></div></div><p>Many<a id="id367" class="indexterm"/> modern <a id="id368" class="indexterm"/>mobile devices now integrate a plethora of built-in sensors including various motion and position sensors (such as an accelerometer, gyroscope, and magnetometer/digital compass) to enable novel forms of user interaction (such as complex gesture and motion control) as well as other environmental sensors, which can measure environmental conditions (such as an ambient light sensor and proximity sensor) to enable smart wearable applications. The Android Sensor Framework provides a comprehensive interface to access many types of sensors, which can be either hardware-based (physical sensors) or software-based (virtual sensors that derive inputs from hardware sensors). In general, there are three major categories of sensors—motion sensors, position sensors, and environmental sensors.</p><p>In this section, we will demonstrate how to utilize the Android Sensor Framework to communicate with the sensors available on your device, register sensor event listeners to monitor changes in the sensors, and acquire raw sensor data for display on your mobile device. To create this demo, we will implement the Java code and native code using the same framework design introduced in the previous chapter. The following block diagram illustrates the core functions and the relationship among the classes that will be implemented in this demo:</p><div><img src="img/9727OS_08_01.jpg" alt="Visualizing real-time data from built-in Inertial Measurement Units (IMUs)"/></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec129"/>Getting ready</h2></div></div></div><p>This demo <a id="id369" class="indexterm"/>requires <a id="id370" class="indexterm"/>an Android device with OpenGL ES 3.0 support as well as physical sensor hardware support. Unfortunately, at the moment these functions cannot be simulated with an emulator shipped with the Android SDK. Specifically, an Android mobile device with the following set of sensors, which are now commonly available, would be required to run this demo: an accelerometer, gyroscope, and magnetometer (digital compass).</p><p>In addition, we assume that the Android SDK and Android NDK are configured as discussed in <a class="link" href="ch07.html" title="Chapter 7. An Introduction to Real-time Graphics Rendering on a Mobile Platform using OpenGL ES 3.0">Chapter 7</a>, <em>An I</em>
<em>ntroduction to Real-time Graphics Rendering on a Mobile Platform using OpenGL ES 3.0</em>.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec130"/>How to do it…</h2></div></div></div><p>First, we will create the core Java source files similar to the previous chapter. Since the majority of the code is similar, we will only discuss the new and significant elements that are introduced in the current code. The rest of the code is abbreviated with the "…" notation. Please download the complete source code from the official Packt Publishing website.</p><p>In the <code class="literal">GL3JNIActivity.java</code> file, we first integrate Android Sensor Manager, which allows <a id="id371" class="indexterm"/>us to<a id="id372" class="indexterm"/> read and parse sensor data. The following steps are required to complete the integration:</p><div><ol class="orderedlist arabic"><li class="listitem">Import the classes for the Android Sensor Manager:<div><pre class="programlisting">package com.android.gl3jni;
…
import android.hardware.Sensor;
import android.hardware.SensorEvent;
import android.hardware.SensorEventListener;
import android.hardware.SensorManager;
…</pre></div></li><li class="listitem">Add the <code class="literal">SensorEventListener</code> interface to interact with the sensors:<div><pre class="programlisting">public class GL3JNIActivity extends Activity implements SensorEventListener{</pre></div></li><li class="listitem">Define the <code class="literal">SensorManager</code> and the <code class="literal">Sensor</code> variables to handle the data from the accelerometer, gyroscope, and magnetometer:<div><pre class="programlisting">  …
  private SensorManager mSensorManager;
  private Sensor mAccelerometer;
  private Sensor mGyro;
  private Sensor mMag;</pre></div></li><li class="listitem">Initialize the <code class="literal">SensorManager</code> as well as all other sensor services:<div><pre class="programlisting">@Override protected void onCreate(Bundle icicle) {
  super.onCreate(icicle);
  setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_LANDSCAPE);
  
  mSensorManager = (SensorManager)getSystemService(SENSOR_SERVICE);
  mAccelerometer = mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER);
  mGyro = mSensorManager.getDefaultSensor(Sensor.TYPE_GYROSCOPE);
  mMag = mSensorManager.getDefaultSensor(Sensor.TYPE_MAGNETIC_FIELD);
  mView = new GL3JNIView(getApplication());
  setContentView(mView);
}</pre></div></li><li class="listitem">Register the<a id="id373" class="indexterm"/> callback<a id="id374" class="indexterm"/> functions and start listening to these events:<div><pre class="programlisting">@Override protected void onPause() {
  super.onPause();
  mView.onPause();
  //unregister accelerometer and other sensors
  mSensorManager.unregisterListener(this, mAccelerometer);
  mSensorManager.unregisterListener(this, mGyro);
  mSensorManager.unregisterListener(this, mMag);
}

@Override protected void onResume() {
  super.onResume();
  mView.onResume();
  /* register and activate the sensors. Start streaming data and handle with callback functions */
  mSensorManager.registerListener(this, mAccelerometer, SensorManager.SENSOR_DELAY_GAME);
  mSensorManager.registerListener(this, mGyro, SensorManager.SENSOR_DELAY_GAME);
  mSensorManager.registerListener(this, mMag, SensorManager.SENSOR_DELAY_GAME);
}</pre></div></li><li class="listitem">Handle the <code class="literal">sensor</code> events. The <code class="literal">onSensorChanged</code> and <code class="literal">onAccuracyChanged</code> functions capture any changes detected and the <code class="literal">SensorEvent</code> variable holds all the information about the sensor type, time-stamp, accuracy, and so on:<div><pre class="programlisting">@Override
public void onAccuracyChanged(Sensor sensor, int accuracy) {
  //included for completeness
}
@Override
public void onSensorChanged(SensorEvent event) {
  //handle the accelerometer data
  //All values are in SI units (m/s^2)
  if (event.sensor.getType() == Sensor.TYPE_ACCELEROMETER) {
    float ax, ay, az;
    ax = event.values[0];
    ay = event.values[1];
    az = event.values[2];
    GL3JNILib.addAccelData(ax, ay, az);
  }
  /* All values are in radians/second and measure the rate of rotation around the device's local X, Y, and Z axes */
  if (event.sensor.getType() == Sensor.TYPE_GYROSCOPE) {
    float gx, gy, gz;
    //angular speed
    gx = event.values[0];
    gy = event.values[1];
    gz = event.values[2];
    GL3JNILib.addGyroData(gx, gy, gz);
  }
  //All values are in micro-Tesla (uT) and measure the ambient magnetic field in the X, Y and Z axes.
  if (event.sensor.getType() == Sensor.TYPE_MAGNETIC_FIELD) {
    float mx, my, mz;
    mx = event.values[0];
    my = event.values[1];
    mz = event.values[2];
    GL3JNILib.addMagData(mx, my, mz);
  }
}</pre></div></li></ol></div><p>Next<a id="id375" class="indexterm"/> implement<a id="id376" class="indexterm"/> the <code class="literal">GL3JNIView</code> class, which handles OpenGL rendering, in the <code class="literal">GL3JNIView.java</code> source file inside the <code class="literal">src/com/android/gl3jni/</code> directory. Since this implementation is identical to content in the <a class="link" href="ch07.html" title="Chapter 7. An Introduction to Real-time Graphics Rendering on a Mobile Platform using OpenGL ES 3.0">Chapter 7</a>, <em>An Introduction to Real-time Graphics Rendering on a Mobile Platform using OpenGL ES 3.0</em>, we will not discuss it again here.</p><p>Finally, integrate all the new features in the <code class="literal">GL3JNILib</code> class, which handles native library loading and calling, in the <code class="literal">GL3JNILib.java</code> file inside the <code class="literal">src/com/android/gl3jni</code> directory:</p><div><pre class="programlisting">package com.android.gl3jni;

public class GL3JNILib {
  static {
    System.loadLibrary("gl3jni");
  }

  public static native void init(int width, int   height);
  public static native void step();
  
  public static native void addAccelData(float ax, float ay, float az);
  public static native void addGyroData(float gx, float gy, float gz);
  public static native void addMagData(float mx, float my, float mz);
}</pre></div><p>Now, on the<a id="id377" class="indexterm"/> JNI/C++ side, create<a id="id378" class="indexterm"/> a class called <code class="literal">Sensor</code> for managing the data buffer for each sensor, including the accelerometer, gyroscope, and magnetometer (digital compass). First, create a header file for the <code class="literal">Sensor</code> class called <code class="literal">Sensor.h</code>:</p><div><pre class="programlisting">#ifndef SENSOR_H_
#define SENSOR_H_
#include &lt;stdlib.h&gt;
#include &lt;jni.h&gt;
#include &lt;GLES3/gl3.h&gt;
#include &lt;math.h&gt;

class Sensor {
  public:
    Sensor();
    Sensor(unsigned int size);
    virtual ~Sensor();

    //Resize buffer size dynamically with this function
    void init(unsigned int size);
    //Append new data to the buffer
    void appendAccelData(GLfloat x, GLfloat y,GLfloat z);
    void appendGyroData(GLfloat x, GLfloat y, GLfloat z);
    void appendMagData(GLfloat x, GLfloat y, GLfloat z);

    //Get sensor data buffer
    GLfloat *getAccelDataPtr(int channel);
    GLfloat *getGyroDataPtr(int channel);
    GLfloat *getMagDataPtr(int channel);
    GLfloat *getAxisPtr();

    //Auto rescale factors based on max and min
    GLfloat getAccScale();
    GLfloat getGyroScale();
    GLfloat getMagScale();

  unsigned int getBufferSize();

private:
  unsigned int buffer_size;

  GLfloat **accel_data;
  GLfloat **gyro_data;
  GLfloat **mag_data;
  GLfloat *x_axis;

  GLfloat abs_max_acc;
  GLfloat abs_max_mag;
  GLfloat abs_max_gyro;

  void createBuffers(unsigned int size);
  void free_all();

  void findAbsMax(GLfloat *src, GLfloat *max);
  void appendData(GLfloat *src, GLfloat data);
  void setNormalizedAxis(GLfloat *data, unsigned int size, float min, float max);
};

#endif /* SENSOR_H_ */</pre></div><p>Then, implement<a id="id379" class="indexterm"/> the <code class="literal">Sensor</code> class in the <code class="literal">Sensor.cpp</code> file with the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Implement the constructor<a id="id380" class="indexterm"/> and destructor for the <code class="literal">Sensor</code> class. Set the default size of the buffer to <code class="literal">256</code>:<div><pre class="programlisting">#include "Sensor.h"
Sensor::Sensor() {
  //use default size
  init(256);
}
// Initialize with different buffer size
Sensor::Sensor(unsigned int size) {
  init(size);
}
Sensor::~Sensor() {
  free_all();
}</pre></div></li><li class="listitem">Add the initialization function, which sets all default parameters, and allocate and deallocate memory at runtime:<div><pre class="programlisting">void Sensor::init(unsigned int size){
  buffer_size = size;
  //delete the old memory if already exist
  free_all();
  //allocate the memory for the buffer
  createBuffers(size);
  setNormalizedAxis(x_axis, size, -1.0f, 1.0f);
  abs_max_acc = 0;
  abs_max_gyro = 0;
  abs_max_mag = 0;
}</pre></div></li><li class="listitem">Implement <a id="id381" class="indexterm"/>the <code class="literal">createBuffers</code> function<a id="id382" class="indexterm"/> for memory allocation:<div><pre class="programlisting">// Allocate memory for all sensor data buffers
void Sensor::createBuffers(unsigned int buffer_size){
  accel_data = (GLfloat**)malloc(3*sizeof(GLfloat*));
  gyro_data = (GLfloat**)malloc(3*sizeof(GLfloat*));
  mag_data = (GLfloat**)malloc(3*sizeof(GLfloat*));

  //3 channels for accelerometer
  accel_data[0] = (GLfloat*)calloc(buffer_size,sizeof(GLfloat));
  accel_data[1] = (GLfloat*)calloc(buffer_size,sizeof(GLfloat));
  accel_data[2] = (GLfloat*)calloc(buffer_size,sizeof(GLfloat));

  //3 channels for gyroscope
  gyro_data[0] = (GLfloat*)calloc(buffer_size,sizeof(GLfloat));
  gyro_data[1] = (GLfloat*)calloc(buffer_size,sizeof(GLfloat));
  gyro_data[2] = (GLfloat*)calloc(buffer_size,sizeof(GLfloat));

  //3 channels for digital compass
  mag_data[0] = (GLfloat*)calloc(buffer_size,sizeof(GLfloat));
  mag_data[1] = (GLfloat*)calloc(buffer_size,sizeof(GLfloat));
  mag_data[2] = (GLfloat*)calloc(buffer_size,sizeof(GLfloat));

  //x-axis precomputed
  x_axis = (GLfloat*)calloc(buffer_size,sizeof(GLfloat));
}</pre></div></li><li class="listitem">Implement<a id="id383" class="indexterm"/> the <code class="literal">free_all</code> function <a id="id384" class="indexterm"/>for deallocating memory:<div><pre class="programlisting">// Deallocate all memory
void Sensor::free_all(){
  if(accel_data){
    free(accel_data[0]);
    free(accel_data[1]);
    free(accel_data[2]);
    free(accel_data);
  }
  if(gyro_data){
    free(gyro_data[0]);
    free(gyro_data[1]);
    free(gyro_data[2]);
    free(gyro_data);
  }
  if(mag_data){
    free(mag_data[0]);
    free(mag_data[1]);
    free(mag_data[2]);
    free(mag_data);
  }
  if(x_axis){
    free(x_axis);
  }
}</pre></div></li><li class="listitem">Create routines for appending data to the data buffer of each sensor:<div><pre class="programlisting">// Append acceleration data to the buffer
void Sensor::appendAccelData(GLfloat x, GLfloat y, GLfloat z){
  abs_max_acc = 0;
  float data[3] = {x, y, z};
  for(int i=0; i&lt;3; i++){
    appendData(accel_data[i], data[i]);
    findAbsMax(accel_data[i], &amp;abs_max_acc);
  }
}

// Append the gyroscope data to the buffer
void Sensor::appendGyroData(GLfloat x, GLfloat y, GLfloat z){
  abs_max_gyro = 0;
  float data[3] = {x, y, z};
  for(int i=0; i&lt;3; i++){
    appendData(gyro_data[i], data[i]);
    findAbsMax(gyro_data[i], &amp;abs_max_gyro);
  }
}

// Append the magnetic field data to the buffer
void Sensor::appendMagData(GLfloat x, GLfloat y, GLfloat z){
  abs_max_mag = 0;
  float data[3] = {x, y, z};
  for(int i=0; i&lt;3; i++){
    appendData(mag_data[i], data[i]);
    findAbsMax(mag_data[i], &amp;abs_max_mag);
  }
}

// Append Data to the end of the buffer
void Sensor::appendData(GLfloat *src, GLfloat data){
  //shift the data by one
  int i;
  for(i=0; i&lt;buffer_size-1; i++){
    src[i]=src[i+1];
  }
  //set the last element with the new data
  src[buffer_size-1]=data;
}</pre></div></li><li class="listitem">Create <a id="id385" class="indexterm"/>routines <a id="id386" class="indexterm"/>for returning the pointer to the memory buffer of each sensor:<div><pre class="programlisting">// Return the x-axis buffer
GLfloat* Sensor::getAxisPtr() {
  return x_axis;
}

// Get the acceleration data buffer
GLfloat* Sensor::getAccelDataPtr(int channel) {
  return accel_data[channel];
}

// Get the Gyroscope data buffer
GLfloat* Sensor::getGyroDataPtr(int channel) {
  return gyro_data[channel];
}
// Get the Magnetic field data buffer
GLfloat* Sensor::getMagDataPtr(int channel) {
  return mag_data[channel];
}</pre></div></li><li class="listitem">Implement <a id="id387" class="indexterm"/>methods for<a id="id388" class="indexterm"/> displaying/plotting the data stream properly from each sensor (for example, determining the maximum value of the data stream from each sensor to scale the data properly):<div><pre class="programlisting">// Return buffer size
unsigned int Sensor::getBufferSize() {
  return buffer_size;
}

/* Return the global max for the acceleration data buffer (for rescaling and fitting purpose) */
GLfloat Sensor::getAccScale() {
  return abs_max_acc;
}

/* Return the global max for the gyroscope data buffer (for rescaling and fitting purpose) */
GLfloat Sensor::getGyroScale() {
  return abs_max_gyro;
}

/* Return the global max for the magnetic field data buffer (for rescaling and fitting purpose) */
GLfloat Sensor::getMagScale() {
  return abs_max_mag;
}

// Pre-compute the x-axis for the plot
void Sensor::setNormalizedAxis(GLfloat *data, unsigned int size, float min, float max){
  float step_size = (max - min)/(float)size;
  for(int i=0; i&lt;size; i++){
    data[i]=min+step_size*i;
  }
}

// Find the absolute maximum from the buffer
void Sensor::findAbsMax(GLfloat *src, GLfloat *max){
  int i=0;
  for(i=0; i&lt;buffer_size; i++){
    if(*max &lt; fabs(src[i])){
      *max= fabs(src[i]);
    }
  }
}</pre></div></li></ol></div><p>Finally, we<a id="id389" class="indexterm"/> describe the<a id="id390" class="indexterm"/> implementation of the OpenGL ES 3.0 native code to complete the demo application (<code class="literal">main_sensor.cpp</code>). The code is built upon the structure introduced in the previous chapter, so only new changes and modifications will be described in the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">In the project directory, create a file named <code class="literal">main_sensor.cpp</code> and store it inside the <code class="literal">jni</code> directory.</li><li class="listitem">Include all necessary header files, including <code class="literal">Sensor.h</code> at the beginning of the file:<div><pre class="programlisting">#include &lt;Sensor.h&gt;
...</pre></div></li><li class="listitem">Declare shader program handlers and variables for handling sensor data:<div><pre class="programlisting">GLuint gProgram;
GLuint gxPositionHandle;
GLuint gyPositionHandle;
GLuint gColorHandle;
GLuint gOffsetHandle;
GLuint gScaleHandle;
static Sensor g_sensor_data;</pre></div></li><li class="listitem">Define the shader program code for both the vertex shader and fragment shader to render points and lines:<div><pre class="programlisting">// Vertex shader source code
static const char g_vshader_code[] = "#version 300 es\n"
  "in float yPosition;\n"
  "in float xPosition;\n"
  "uniform float scale;\n"
  "uniform float offset;\n"
  "void main() {\n"
    "  vec4 position = vec4(xPosition, yPosition*scale+offset, 0.0, 1.0);\n"
    "  gl_Position = position;\n"
  "}\n";

// fragment shader source code
static const char g_fshader_code[] = "#version 300 es\n"
  "precision mediump float;\n"
  "uniform vec4 color;\n"
  "out vec4 color_out;\n"
  "void main() {\n"
    "  color_out = color;\n"
  "}\n";</pre></div></li><li class="listitem">Set up all <a id="id391" class="indexterm"/>attribute variables in the <code class="literal">setupGraphics</code> function. These variables will be used to communicate <a id="id392" class="indexterm"/>with the shader programs:<div><pre class="programlisting">bool setupGraphics(int w, int h) {

  ...

  gxPositionHandle = glGetAttribLocation(gProgram,"xPosition");
  checkGlError("glGetAttribLocation");
  LOGI("glGetAttribLocation(\"vPosition\") = %d\n", gxPositionHandle);

  gyPositionHandle = glGetAttribLocation(gProgram,   "yPosition");
  checkGlError("glGetAttribLocation");
  LOGI("glGetAttribLocation(\"vPosition\") = %d\n", gyPositionHandle);

  gColorHandle = glGetUniformLocation(gProgram, "color");
  checkGlError("glGetUniformLocation");
  LOGI("glGetUniformLocation(\"color\") = %d\n", gColorHandle);

  gOffsetHandle = glGetUniformLocation(gProgram, "offset");
  checkGlError("glGetUniformLocation");
  LOGI("glGetUniformLocation(\"offset\") = %d\n", gOffsetHandle);

  gScaleHandle = glGetUniformLocation(gProgram, "scale");
  checkGlError("glGetUniformLocation");
  LOGI("glGetUniformLocation(\"scale\") = %d\n", gScaleHandle);

  glViewport(0, 0, w, h);
  width = w;
  height = h;

  checkGlError("glViewport");

  return true;
}</pre></div></li><li class="listitem">Create a<a id="id393" class="indexterm"/> function <a id="id394" class="indexterm"/>for drawing 2D plots to display real-time sensor data:<div><pre class="programlisting">void draw2DPlot(GLfloat *data, unsigned int size, GLfloat scale, GLfloat offset){
  glVertexAttribPointer(gyPositionHandle, 1, GL_FLOAT, GL_FALSE, 0, data);
  checkGlError("glVertexAttribPointer");

  glEnableVertexAttribArray(gyPositionHandle);
  checkGlError("glEnableVertexAttribArray");

  glUniform1f(gOffsetHandle, offset);
  checkGlError("glUniform1f");

  glUniform1f(gScaleHandle, scale);
  checkGlError("glUniform1f");

  glDrawArrays(GL_LINE_STRIP, 0, g_sensor_data.getBufferSize());
  checkGlError("glDrawArrays");
}</pre></div></li><li class="listitem">Set up the rendering function which draws the various 2D time series with the data stream from the sensors:<div><pre class="programlisting">void renderFrame() {
  glClearColor(0.0f, 0.0f, 0.0f, 1.0f);
  checkGlError("glClearColor");

  glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
  checkGlError("glClear");

  glUseProgram(gProgram);
  checkGlError("glUseProgram");
  
  glVertexAttribPointer(gxPositionHandle, 1, GL_FLOAT, GL_FALSE, 0, g_sensor_data.getAxisPtr());
  checkGlError("glVertexAttribPointer");

  glEnableVertexAttribArray(gxPositionHandle);
  checkGlError("glEnableVertexAttribArray");

  //Obtain the scaling factor based on the dataset
  //0.33f for 1/3 of the screen for each graph
  float acc_scale = 0.33f/g_sensor_data.getAccScale();
  float gyro_scale = 0.33f/g_sensor_data.getGyroScale();
  float mag_scale = 0.33f/g_sensor_data.getMagScale();

  glLineWidth(4.0f);

  //set the rendering color
  glUniform4f(gColorHandle, 1.0f, 0.0f, 0.0f, 1.0f);
  checkGlError("glUniform1f");
  /* Render the accelerometer, gyro, and digital compass data. As the vertex shader does not use any projection matrix, every visible vertex has to be in the range of [-1, 1].  0.67f, 0.0f, and -0.67f define the vertical positions of each graph */
  draw2DPlot(g_sensor_data.getAccelDataPtr(0), g_sensor_data.getBufferSize(), acc_scale, 0.67f);
  draw2DPlot(g_sensor_data.getGyroDataPtr(0), g_sensor_data.getBufferSize(), gyro_scale, 0.0f);
  draw2DPlot(g_sensor_data.getMagDataPtr(0), g_sensor_data.getBufferSize(), mag_scale, -0.67f);

  glUniform4f(gColorHandle, 0.0f, 1.0f, 0.0f, 1.0f);
  checkGlError("glUniform1f");
  draw2DPlot(g_sensor_data.getAccelDataPtr(1), g_sensor_data.getBufferSize(), acc_scale, 0.67f);
  draw2DPlot(g_sensor_data.getGyroDataPtr(1), g_sensor_data.getBufferSize(), gyro_scale, 0.0f);
  draw2DPlot(g_sensor_data.getMagDataPtr(1), g_sensor_data.getBufferSize(), mag_scale, -0.67f);

  glUniform4f(gColorHandle, 0.0f, 0.0f, 1.0f, 1.0f);
  checkGlError("glUniform1f");
  draw2DPlot(g_sensor_data.getAccelDataPtr(2), g_sensor_data.getBufferSize(), acc_scale, 0.67f);
  draw2DPlot(g_sensor_data.getGyroDataPtr(2), g_sensor_data.getBufferSize(), gyro_scale, 0.0f);
  draw2DPlot(g_sensor_data.getMagDataPtr(2), g_sensor_data.getBufferSize(), mag_scale, -0.67f);
}</pre></div></li><li class="listitem">Define the JNI <a id="id395" class="indexterm"/>prototypes <a id="id396" class="indexterm"/>that connect to the Java side. These calls are the interfaces for communicating between the Java code and C/C++ native code:<div><pre class="programlisting">//external calls for Java
extern "C" {
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_init(JNIEnv * env, jobject obj, jint width, jint height);
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_step(JNIEnv * env, jobject obj);
  JNIEXPORT void JNICALL
    Java_com_android_gl3jni_GL3JNILib_addAccelData
   (JNIEnv * env, jobject obj, jfloat ax, jfloat ay, jfloat az);
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_addGyroData (JNIE nv * env, jobject obj, jfloat gx, jfloat gy, jfloat gz);
  JNIEXPORT void JNICALL 
Java_com_android_gl3jni_GL3JNILib_addMagData
(JNIEnv * env, jobject obj, jfloat mx, jfloat my, jfloat mz)
{
  g_sensor_data.appendMagData(mx, my, mz);
}
};

//link to internal calls
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_init(JNIEnv * env, jobject obj,  jint width, jint height)
{
  setupGraphics(width, height);
}
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_step(JNIEnv * env, jobject obj)
{
  renderFrame();
}
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_addAccelData(JNIEnv * env, jobject obj,  jfloat ax, jfloat ay, jfloat az){
  g_sensor_data.appendAccelData(ax, ay, az);
}
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_addGyroData(JNIEnv * env, jobject obj,  jfloat gx, jfloat gy, jfloat gz){
  g_sensor_data.appendGyroData(gx, gy, gz);
}
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_addMagData(JNIEnv * env, jobject obj,  jfloat mx, jfloat my, jfloat mz){
  g_sensor_data.appendMagData(mx, my, mz);
}</pre></div></li></ol></div><p>Finally, we need to compile and install the Android application with the same instructions as outlined in the previous chapter.</p><p>The following <a id="id397" class="indexterm"/>screenshots <a id="id398" class="indexterm"/>show the real-time sensor data stream from the accelerometer, gyroscope, and digital compass (top panel, middle panel, and bottom panel, respectively) on our Android device. Red, green, and blue are used to differentiate the channels from each sensor data stream. For example, the red plot in the top panel represents the acceleration value of the device along the <em>x</em> axis (the blue plot for the <em>y</em> axis and the green plot for the <em>z</em> axis). In the first example, we rotated the phone freely at various orientations and the plots show the corresponding changes in the sensor values. The visualizer also provides an auto-scale function, which automatically computes the maximum values to rescale the plots accordingly:</p><div><img src="img/9727OS_08_02.jpg" alt="How to do it…"/></div><p>Next, we positioned<a id="id399" class="indexterm"/> the phone <a id="id400" class="indexterm"/>on a stationary surface and we plotted the values of the sensors. Instead of observing constant values over time, the time series plots show that there are some very small changes (jittering) in the sensor values due to sensor noise. Depending on the application, you will often need to apply filtering techniques to ensure that the user experience is jitter-free. One simple solution is to apply a low-pass filter to smooth out any high-frequency noise. More <a id="id401" class="indexterm"/>details on the implementation of such filters can be found at <a class="ulink" href="http://developer.android.com/guide/topics/sensors/sensors_motion.html">http://developer.android.com/guide/topics/sensors/sensors_motion.html</a>.</p><div><img src="img/9727OS_08_03.jpg" alt="How to do it…"/></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec131"/>How it works…</h2></div></div></div><p>The Android Sensor Framework allows users to access the raw data from various types of sensors on a mobile device. This framework is part of the <code class="literal">android.hardware</code> package and the sensor package includes a set of classes and interfaces for sensor-specific features.</p><p>The <code class="literal">SensorManager</code> class provides an interface and methods for accessing and listing the available sensors <a id="id402" class="indexterm"/>from the <a id="id403" class="indexterm"/>device. Some common hardware sensors include the accelerometer, gyroscope, proximity sensor, and the magnetometer (digital compass). These sensors are represented by constant variables (such as <code class="literal">TYPE_ACCELEROMETER</code> for the accelerometer, <code class="literal">TYPE_MAGNETIC_FIELD</code> for the magnetometer, and <code class="literal">TYPE_GYROSCOPE</code> for the gyroscope) and the <code class="literal">getDefaultSensor</code> function returns an instance of the <code class="literal">Sensor</code> object based on the type requested.</p><p>To enable data streaming, we must register the sensor to the <code class="literal">SensorEventListener</code> class such that the raw data is reported back to the application upon updates. The <code class="literal">registerListener</code> function then creates the callback to handle updates to the sensor value or sensor accuracy. The <code class="literal">SensorEvent</code> variable stores the name of the sensor, the timestamp and accuracy of the event, as well as the raw data.</p><p>The raw data stream from each sensor is reported back with the <code class="literal">onSensorChange</code> function. Since sensor data may be acquired and streamed at a high rate, it is important that we do not block callback function calls or perform any computationally intensive processes within the <code class="literal">onSensorChange</code> function. In addition, it is a good practice to reduce the data rate of the sensor based on your application requirements. In our case, we set the sensor to run at the optimal rate for gaming purposes by passing the constant preset variable <code class="literal">SENSOR_DELAY_GAME</code> to the <code class="literal">registerListener</code> function.</p><p>The <code class="literal">GL3JNILib</code> class <a id="id404" class="indexterm"/>then handles <a id="id405" class="indexterm"/>all the data passing to the native code using the new functions. For simplicity, we have created separate functions for each sensor type, which makes it easier for the reader to understand the data flow for each sensor.</p><p>At this point, we have created the interfaces that redirect data to the native side. However, to plot the sensor data on the screen, we need to create a simple buffering mechanism that stores the data points over some period of time. We have created a custom <code class="literal">Sensor</code> class in C++ to handle data creation, updates, and processing needed to manage these interactions. The implementation of the class is straightforward, and we preset the buffer size to store 256 data points by default.</p><p>On the OpenGL ES side, we create the 2D plot by appending the data stream to our vertex buffer. The scale of the data stream is adjusted dynamically based on the current values to ensure that the values fit on the screen. Notice that we have also performed all data scaling and translation on the vertex shader to reduce any overhead in the CPU computation.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec132"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For more information on the <a id="id406" class="indexterm"/>Android Sensor Framework, consult the documentation online at <a class="ulink" href="http://developer.android.com/guide/topics/sensors/sensors_overview.html">http://developer.android.com/guide/topics/sensors/sensors_overview.html</a>.</li></ul></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec51"/>Part I – handling multi-touch interface and motion sensor inputs</h1></div></div></div><p>Now that we have<a id="id407" class="indexterm"/> introduced<a id="id408" class="indexterm"/> the basics of handling sensor inputs, we will develop an interactive, sensor-based data visualization tool. In addition to using motion sensors, we will introduce a multi-touch interface for user interaction. The following is a preview of the final application, integrating all the elements in this chapter:</p><div><img src="img/9727OS_08_06.jpg" alt="Part I – handling multi-touch interface and motion sensor inputs"/></div><p>In this section, we will focus <a id="id409" class="indexterm"/>solely on the Java side of the<a id="id410" class="indexterm"/> implementation and the native code will be described in part II. The following class diagram illustrates the various components of the Java code (part I) that provide the basic interface for user interaction on the mobile device and demonstrates how the native code (part II) completes the entire implementation:</p><div><img src="img/9727OS_08_04.jpg" alt="Part I – handling multi-touch interface and motion sensor inputs"/></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec133"/>How to do it…</h2></div></div></div><p>First, we will <a id="id411" class="indexterm"/>create the core Java source files that are essential to an <a id="id412" class="indexterm"/>Android application. These files serve as a wrapper for our OpenGL ES 3.0 native code. The code structure is based on the <code class="literal">gl3jni</code> package described in the previous section. Here we will highlight the major changes made to the code and discuss the interaction of these new components.</p><p>In the project directory, modify the <code class="literal">GL3JNIActivity</code> class in the <code class="literal">GL3JNIActivity.java</code> file within the <code class="literal">src/com/android/gl3jni</code> directory. Instead of using the raw sensor data, we will utilize the Android sensor fusion algorithm, which intelligently combines all sensor data to recover the orientation of the device as a rotation vector. The <a id="id413" class="indexterm"/>steps to enable this feature are described as <a id="id414" class="indexterm"/>follows:</p><div><ol class="orderedlist arabic"><li class="listitem">In the <code class="literal">GL3JNIActivity</code> class, add the new variables for handling the rotation matrix and vector:<div><pre class="programlisting">public class GL3JNIActivity extends Activity implements SensorEventListener{
  GL3JNIView mView;
  private SensorManager mSensorManager;
  private Sensor mRotate;
  private float[] mRotationMatrix=new float[16];
  private float[] orientationVals=new float[3];</pre></div></li><li class="listitem">Initialize the <code class="literal">Sensor</code> variable with the <code class="literal">TYPE_ROTATION_VECTOR</code> type, which returns the device orientation as a rotation vector/matrix:<div><pre class="programlisting">@Override protected void onCreate(Bundle icicle) {
  super.onCreate(icicle);
  //lock the screen orientation for this demo
  //otherwise the canvas will rotate
  setRequestedOrientation (ActivityInfo.SCREEN_ORIENTATION_LANDSCAPE);
    
  mSensorManager = (SensorManager)getSystemService(SENSOR_SERVICE);
    //TYPE_ROTATION_VECTOR for device orientation
    mRotate = mSensorManager.getDefaultSensor(Sensor.TYPE_ROTATION_VECTOR);
        
    mView = new GL3JNIView(getApplication());
    setContentView(mView);
}</pre></div></li><li class="listitem">Register the Sensor Manager object and set the sensor response rate to <code class="literal">SENSOR_DELAY_GAME</code>, which is used for gaming or real-time applications:<div><pre class="programlisting">@Override protected void onResume() {
  super.onResume();
  mView.onResume();
  mSensorManager.registerListener(this, mRotate, SensorManager.SENSOR_DELAY_GAME);
}</pre></div></li><li class="listitem">Retrieve the device orientation and save the event data as a rotation matrix. Then convert the rotation matrix into Euler angles that are passed to the native code:<div><pre class="programlisting">@Override
public void onSensorChanged(SensorEvent event) {
  if (event.sensor.getType() == Sensor.TYPE_ROTATION_VECTOR){
    SensorManager.getRotationMatrixFromVector (mRotationMatrix,event.values);
    SensorManager.getOrientation (mRotationMatrix, orientationVals);
    GL3JNILib.addRotData(orientationVals[0], orientationVals[1],orientationVals[2]);
  }
}</pre></div></li></ol></div><p>Next, modify<a id="id415" class="indexterm"/> the <code class="literal">GL3JNIView</code> class, which handles OpenGL <a id="id416" class="indexterm"/>rendering, in the <code class="literal">GL3JNIView.java</code> file inside the <code class="literal">src/com/android/gl3jni/</code> directory. To make the application interactive, we also integrate the touch-based gesture detector that handles multi-touch events. Particularly, we add the <code class="literal">ScaleGestureDetector</code> class that enables the pinch gesture for scaling the 3D plot. To implement this feature, we make the following modifications to the <code class="literal">GL3JNIView.java</code> file:</p><div><ol class="orderedlist arabic"><li class="listitem">Import the <code class="literal">MotionEvent</code> and <code class="literal">ScaleGestureDetector</code> classes:<div><pre class="programlisting">package com.android.gl3jni;
...
import android.view.MotionEvent;
import android.view.ScaleGestureDetector;
...</pre></div></li><li class="listitem">Create a <code class="literal">ScaleGestureDetector</code> variable and initialize with <code class="literal">ScaleListener</code>:<div><pre class="programlisting">class GL3JNIView extends GLSurfaceView {
  private ScaleGestureDetector mScaleDetector;
  ...
  
  public GL3JNIView(Context context) {
    super(context);
    ...
    //handle gesture input
    mScaleDetector = new ScaleGestureDetector (context, new ScaleListener());
  }</pre></div></li><li class="listitem">Pass the motion event to the gesture detector when a touch screen event occurs (<code class="literal">onTouchEvent</code>):<div><pre class="programlisting">@Override
public boolean onTouchEvent(MotionEvent ev) {
  // Let ScaleGestureDetector inspect all events.
  mScaleDetector.onTouchEvent(ev);
  return true;
}</pre></div></li><li class="listitem">Implement <code class="literal">SimpleOnScaleGestureListener</code> and handle the callback (<code class="literal">onScale</code>) on pinch gesture events:<div><pre class="programlisting">private class ScaleListener extends ScaleGestureDetector.SimpleOnScaleGestureListener {
  private float mScaleFactor = 1.f;
  @Override
  public boolean onScale(ScaleGestureDetector detector) 
  {
    //scaling factor
    mScaleFactor *= detector.getScaleFactor();
    //Don't let the object get too small/too large.
    mScaleFactor = Math.max(0.1f, Math.min(mScaleFactor, 5.0f));
    invalidate();
    GL3JNILib.setScale(mScaleFactor);
    return true;
  }
}</pre></div></li></ol></div><p>Finally, in <a id="id417" class="indexterm"/>the <code class="literal">GL3JNILib</code> class, we implement the functions to<a id="id418" class="indexterm"/> handle native library loading and calling in the <code class="literal">GL3JNILib.java</code> file inside the <code class="literal">src/com/android/gl3jni</code> directory:</p><div><pre class="programlisting">package com.android.gl3jni;

public class GL3JNILib {
  static {
    System.loadLibrary("gl3jni");
  }
  
  public static native void init(int width, int height);
  public static native void step();
  
  /* pass the rotation angles and scaling factor to the native code */
  public static native void addRotData(float rx, float ry, float rz);
  public static native void setScale(float scale);
}</pre></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec134"/>How it works…</h2></div></div></div><p>Similar to the <a id="id419" class="indexterm"/>previous demo, we will use the Android Sensor<a id="id420" class="indexterm"/> Framework to handle the sensor inputs. Notice that, in this demo, we specify <code class="literal">TYPE_ROTATION_VECTOR</code> for the sensor type inside the <code class="literal">getDefaultSensor</code> function in <code class="literal">GL3JNIActivity.java</code>, which allows us to detect the device orientation. This is a software type sensor in which all IMUs data (from the accelerometer, gyroscope, and magnetometer) are fused together to create the rotation vector. The device orientation data is first stored in the rotation matrix <code class="literal">mRotationMatrix</code> using the <code class="literal">getRotationMatrixFromVector</code> function and the azimuth, pitch, and roll angles (rotation around the <em>x</em>, <em>y</em>, and <em>z</em> axes, respectively) are retrieved using the <code class="literal">getOrientation</code> function. Finally, we pass the three orientation angles to the native code portion of the implementation using the <code class="literal">GL3JNILib.addRotData</code> call. This allows us to control 3D graphics based on the device's orientation.</p><p>Next we will explain how the multi-touch interface works. Inside the <code class="literal">GL3JNIView</code> class, you will notice that we have created an instance (<code class="literal">mScaleDetector</code>) of a new class called <code class="literal">ScaleGestureDetector</code>. The <code class="literal">ScaleGestureDetector</code> class detects scaling transformation gestures (pinching with two fingers) using the <code class="literal">MotionEvent</code> class from the multi-touch screen. The algorithm returns the scale factor that can be redirected to the OpenGL pipeline to update the graphics in real time. The <code class="literal">SimpleOnScaleGestureListener</code> class provides a callback function for the <code class="literal">onScale</code> event and we pass the scale factor (<code class="literal">mScaleFactor</code>) to the native code using the <code class="literal">GL3JNILib.setScale</code> call.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec135"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For further information on the <a id="id421" class="indexterm"/>Android multi-touch interface, see the detailed documentation at <a class="ulink" href="http://developer.android.com/training/gestures/index.html">http://developer.android.com/training/gestures/index.html</a>.</li></ul></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec52"/>Part II – interactive, real-time data visualization with mobile GPUs</h1></div></div></div><p>Now we will<a id="id422" class="indexterm"/> complete<a id="id423" class="indexterm"/> our demo with the native code implementation to create our highly interactive Android-based data visualization application with OpenGL ES 3.0 as well as the Android sensor and gesture control interface.</p><p>The following <a id="id424" class="indexterm"/>class<a id="id425" class="indexterm"/> diagram highlights what remains to be implemented on the C/C++ side:</p><div><img src="img/9727OS_08_05.jpg" alt="Part II – interactive, real-time data visualization with mobile GPUs"/></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec136"/>How to do it…</h2></div></div></div><p>Here, we describe <a id="id426" class="indexterm"/>the implementation of the OpenGL ES 3.0 native code to complete the <a id="id427" class="indexterm"/>demo application. We will preserve the same code structure from <a class="link" href="ch07.html" title="Chapter 7. An Introduction to Real-time Graphics Rendering on a Mobile Platform using OpenGL ES 3.0">Chapter 7</a>, <em>An Introduction to Real-time Graphics Rendering on a Mobile Platform using OpenGL ES 3.0</em>. In the following steps, only the new codes are highlighted, and all changes are implemented in the <code class="literal">main.cpp</code> file inside the <code class="literal">jni</code> folder:</p><div><ol class="orderedlist arabic"><li class="listitem">Include all necessary header files, including <code class="literal">JNI</code>, OpenGL ES 3.0, and the <code class="literal">GLM</code> library:<div><pre class="programlisting">#define GLM_FORCE_RADIANS

//header for JNI
#include &lt;jni.h&gt;
...

//header for GLM library
#include &lt;glm/glm.hpp&gt;
#include &lt;glm/gtc/matrix_transform.hpp&gt;</pre></div></li><li class="listitem">Declare the shader program variables:<div><pre class="programlisting">//shader program handlers
GLuint gProgram;
GLuint gvPositionHandle;
GLuint matrixHandle;
GLuint sigmaHandle;
GLuint scaleHandle;</pre></div></li><li class="listitem">Declare variables for setting up the camera as well as other relevant variables such as the rotation angles and grid:<div><pre class="programlisting">//the view matrix and projection matrix
glm::mat4 g_view_matrix;
glm::mat4 g_projection_matrix;

//initial position of the camera
glm::vec3 g_position = glm::vec3( 0, 0, 4 );

//FOV of the camera
float g_initial_fov = glm::pi&lt;float&gt;()*0.25f;
//rotation angles, set by sensors or by touch screen
float rx, ry, rz;
float scale=1.0f; 
//vertices for the grid
const unsigned int GRID_SIZE=400;
GLfloat gGrid[GRID_SIZE*GRID_SIZE*3]={0};</pre></div></li><li class="listitem">Define the shader program code for both the vertex shader and fragment shader. Note the <a id="id428" class="indexterm"/>similarity in the heat map generation code between this implementation in OpenGL ES 3.0 and an earlier implementation<a id="id429" class="indexterm"/> in standard OpenGL (see chapters 4-6):<div><pre class="programlisting">// Vertex shader source code
static const char g_vshader_code[] = 
  "#version 300 es\n"
    "in vec4 vPosition;\n"
    "uniform mat4 MVP;\n"
  "uniform float sigma;\n"
  "uniform float scale;\n"
    "out vec4 color_based_on_position;\n"
    "// Heat map generator                \n"
    "vec4 heatMap(float v, float vmin, float vmax){\n"
    "    float dv;\n"
    "    float r=1.0, g=1.0, b=1.0;\n"
    "  if (v &lt; vmin){\n"
    "    v = vmin;}\n"
    "  if (v &gt; vmax){\n"
    "    v = vmax;}\n"
    "  dv = vmax - vmin;\n"
    "  if (v &lt; (vmin + 0.25 * dv)) {\n"
    "    r = 0.0;\n"
    "    g = 4.0 * (v - vmin) / dv;\n"
    "  } else if (v &lt; (vmin + 0.5 * dv)) {\n"
    "    r = 0.0;\n"
    "    b = 1.0 + 4.0 * (vmin + 0.25 * dv - v) /   dv;\n"
    "  } else if (v &lt; (vmin + 0.75 * dv)) {\n"
    "    r = 4.0 * (v - vmin - 0.5 * dv) / dv;\n"
    "    b = 0.0;\n"
    "  } else {\n"
    "    g = 1.0 + 4.0 * (vmin + 0.75 * dv - v) /   dv;\n"
    "    b = 0.0;\n"
    "  }\n"
    "    return vec4(r, g, b, 0.1);\n"
    "}\n"
    "void main() {\n"
  "  //Simulation on GPU \n"
    "  float x_data = vPosition.x;\n"
    "  float y_data = vPosition.y;\n"
    "  float sigma2 = sigma*sigma;\n"
    "  float z = exp(-0.5*(x_data*x_data)/(sigma2)-0.5*(y_data*y_data)/(sigma2));\n"
    "  vec4 position = vPosition;\n"
  // scale the graphics based on user gesture input
  "  position.z = z*scale;\n"
  "  position.x = position.x*scale;\n"
  "  position.y = position.y*scale;\n"
  "  gl_Position = MVP*position;\n"
    "  color_based_on_position = heatMap(position.z, 0.0, 0.5);\n"
  "  gl_PointSize = 5.0*scale;\n"
    "}\n";

// fragment shader source code
static const char g_fshader_code[] =
  "#version 300 es\n"
    "precision mediump float;\n"
    "in vec4 color_based_on_position;\n"
  "out vec4 color;\n"
    "void main() {\n"
    "  color = color_based_on_position;\n"
    "}\n";</pre></div></li><li class="listitem">Initialize the <a id="id430" class="indexterm"/>grid pattern for <a id="id431" class="indexterm"/>data visualization:<div><pre class="programlisting">void computeGrid(){
  float grid_x = GRID_SIZE;
  float grid_y = GRID_SIZE;
  unsigned int data_counter = 0;
  //define a grid ranging from -1 to +1
  for(float x = -grid_x/2.0f; x&lt;grid_x/2.0f; x+=1.0f){
    for(float y = -grid_y/2.0f; y&lt;grid_y/2.0f; y+=1.0f){
      float x_data = 2.0f*x/grid_x;
      float y_data = 2.0f*y/grid_y;
      gGrid[data_counter] = x_data;
      gGrid[data_counter+1] = y_data;
      gGrid[data_counter+2] = 0;
      data_counter+=3;
    }
  }
}</pre></div></li><li class="listitem">Set the rotation angles that are used to control the model viewing angles. These angles (device orientation) are passed from the Java side:<div><pre class="programlisting">void setAngles(float irx, float iry, float irz){
  rx = irx;
  ry = iry;
  rz = irz;
}</pre></div></li><li class="listitem">Compute the projection and view matrices based on camera parameters:<div><pre class="programlisting">void computeProjectionMatrices(){
  //direction vector for z 
  glm::vec3 direction_z(0, 0, -1.0);
  //up vector
  glm::vec3 up = glm::vec3(0,-1,0);
  
  float aspect_ratio = (float)width/(float)height;
  float nearZ = 0.1f;
  float farZ = 100.0f;
  float top = tan(g_initial_fov/2*nearZ);
  float right = aspect_ratio*top;
  float left = -right;
  float bottom = -top;
  g_projection_matrix = glm::frustum(left, right, bottom, top, nearZ, farZ);
  
  // update the view matrix
  g_view_matrix = glm::lookAt( 
    g_position,             // camera position
    g_position+direction_z, // view direction
    up                      // up direction
  );
}</pre></div></li><li class="listitem">Create a <a id="id432" class="indexterm"/>function for handling the initialization of all<a id="id433" class="indexterm"/> attribute variables for the shader program and other one-time setups, such as the memory allocation and initialization for the grid:<div><pre class="programlisting">bool setupGraphics(int w, int h) {
  ...
  gvPositionHandle = glGetAttribLocation(gProgram, "vPosition");
  checkGlError("glGetAttribLocation");
  LOGI("glGetAttribLocation(\"vPosition\") = %d\n", gvPositionHandle);

  matrixHandle = glGetUniformLocation(gProgram, "MVP");
  checkGlError("glGetUniformLocation");
  LOGI("glGetUniformLocation(\"MVP\") = %d\n", matrixHandle);

  sigmaHandle = glGetUniformLocation(gProgram, "sigma");
  checkGlError("glGetUniformLocation");
  LOGI("glGetUniformLocation(\"sigma\") = %d\n", sigmaHandle);

  scaleHandle = glGetUniformLocation(gProgram, "scale");
  checkGlError("glGetUniformLocation");
  LOGI("glGetUniformLocation(\"scale\") = %d\n", scaleHandle);

  ...

  computeGrid();
  return true;
}</pre></div></li><li class="listitem">Set up the <a id="id434" class="indexterm"/>rendering <a id="id435" class="indexterm"/>function for the 3D plot of the Gaussian function:<div><pre class="programlisting">void renderFrame() {
  glEnable(GL_BLEND);
  glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);

  static float sigma;

  //update the variables for animations
  sigma+=0.002f;
  if(sigma&gt;0.5f){
    sigma = 0.002f;
  }

  /* gets the View and Model Matrix and apply to the rendering */
  computeProjectionMatrices();
  glm::mat4 projection_matrix = g_projection_matrix;
  glm::mat4 view_matrix = g_view_matrix;
  glm::mat4 model_matrix = glm::mat4(1.0);
  model_matrix = glm::rotate(model_matrix, rz, glm::vec3(-1.0f, 0.0f, 0.0f));
  model_matrix = glm::rotate(model_matrix, ry, glm::vec3(0.0f, -1.0f, 0.0f));
  model_matrix = glm::rotate(model_matrix, rx, glm::vec3(0.0f, 0.0f, 1.0f));
  glm::mat4 mvp = projection_matrix * view_matrix * model_matrix;

  glClearColor(0.0f, 0.0f, 0.0f, 1.0f);
  checkGlError("glClearColor");

  glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
  checkGlError("glClear");

  glUseProgram(gProgram);
  checkGlError("glUseProgram");
  
  glUniformMatrix4fv(matrixHandle, 1, GL_FALSE, &amp;mvp[0][0]);
  checkGlError("glUniformMatrix4fv");

  glUniform1f(sigmaHandle, sigma);
  checkGlError("glUniform1f");

  glUniform1f(scaleHandle, scale);
  checkGlError("glUniform1f");

  glVertexAttribPointer(gvPositionHandle, 3, GL_FLOAT, GL_FALSE, 0, gGrid);
  checkGlError("glVertexAttribPointer");

  glEnableVertexAttribArray(gvPositionHandle);
  checkGlError("glEnableVertexAttribArray");

  glDrawArrays(GL_POINTS, 0, GRID_SIZE*GRID_SIZE);
  checkGlError("glDrawArrays");
}</pre></div></li><li class="listitem">Define the <a id="id436" class="indexterm"/>JNI prototypes<a id="id437" class="indexterm"/> that connect to the Java side. These calls are the interfaces for communicating between the Java code and C/C++ native code:<div><pre class="programlisting">extern "C" {
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_init(JNIEnv * env, jobject obj, jint width, jint height);
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_step(JNIEnv * env, jobject obj);
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_addRotData(JNIEnv * env, jobject obj, jfloat rx, jfloat ry, jfloat rz);
  JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_setScale(JNIEnv * env, jobject obj,  jfloat jscale);
};</pre></div></li><li class="listitem">Set up the internal function calls with the helper functions:<div><pre class="programlisting">JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_init(JNIEnv * env, jobject obj,  jint width, jint height)
{
  setupGraphics(width, height);
}
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_step(JNIEnv * env, jobject obj)
{
  renderFrame();
}
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_addRotData(JNIEnv * env, jobject obj, jfloat rx, jfloat ry, jfloat rz)
{
  setAngles(rx, ry, rz);
}
JNIEXPORT void JNICALL Java_com_android_gl3jni_GL3JNILib_setScale(JNIEnv * env, jobject obj, jfloat jscale)
{
  scale = jscale;
  LOGI("Scale is %lf", scale);
}</pre></div></li></ol></div><p>Finally, in terms<a id="id438" class="indexterm"/> of the <a id="id439" class="indexterm"/>compilation steps, modify the build files <code class="literal">Android.mk</code> and <code class="literal">Application.mk</code> accordingly as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Add in the GLM path to the <code class="literal">LOCAL_C_INCLUDES</code> variable in <code class="literal">Android.mk</code>:<div><pre class="programlisting">LOCAL_PATH:= $(call my-dir)

include $(CLEAR_VARS)

LOCAL_MODULE    := libgl3jni
LOCAL_CFLAGS    := -Werror
LOCAL_SRC_FILES := main.cpp
LOCAL_LDLIBS    := -llog -lGLESv3
#The GLM library is installed in one of these two folders by default
LOCAL_C_INCLUDES := /opt/local/include /usr/local/include 

include $(BUILD_SHARED_LIBRARY)</pre></div></li><li class="listitem">Add in <code class="literal">gnustl_static</code> to the <code class="literal">APP_STL</code> variable to use GNU STL as a static library. This allows for all runtime supports from C++, which is needed by the GLM library. See more at <a class="ulink" href="http://www.kandroid.org/ndk/docs/CPLUSPLUS-SUPPORT.html">http://www.kandroid.org/ndk/docs/CPLUSPLUS-SUPPORT.html</a>:<div><pre class="programlisting">APP_ABI := armeabi-v7a
#required for GLM and other static libraries
APP_STL := gnustl_static</pre></div></li><li class="listitem">Run the <a id="id440" class="indexterm"/>compilation <a id="id441" class="indexterm"/>script (this is similar to what we did in the previous chapter). Please note that the <code class="literal">ANDROID_SDK_PATH</code> and <code class="literal">ANDROID_NDK_PATH</code> variables should be changed to the correct directories based on the local environment setup:<div><pre class="programlisting">
<strong>#!/bin/bash</strong>
<strong>ANDROID_SDK_PATH="../../../3rd_party/android/android-sdk-macosx"</strong>
<strong>ANDROID_NDK_PATH="../../../3rd_party/android/android-ndk-r10e"</strong>

<strong>$ANDROID_SDK_PATH/tools/android update project -p . -s --target "android-18"</strong>
<strong>$ANDROID_NDK_PATH/ndk-build</strong>
<strong>ant debug</strong>
</pre></div></li><li class="listitem">Install the <a id="id442" class="indexterm"/><strong>Android Application Package</strong> (<strong>APK</strong>) on the Android phone, using the following commands in the terminal:<div><pre class="programlisting">
<strong>ANDROID_SDK_PATH="../../../3rd_party/android/android-sdk-macosx"</strong>
<strong>$ANDROID_SDK_PATH/platform-tools/adb install -r bin/GL3JNIActivity-debug.apk</strong>
</pre></div></li></ol></div><p>The final results of our implementation are shown next. By changing the orientation of the phone, the Gaussian function can be viewed from different angles. This provides a very intuitive way to visualize 3D datasets. Here is a photo showing the Gaussian function when the device is oriented parallel to the ground:</p><div><img src="img/9727OS_08_07.jpg" alt="How to do it…"/></div><p>Finally, we test <a id="id443" class="indexterm"/>our <a id="id444" class="indexterm"/>multi-touch gesture interface by pinching on the touch screen with 2 fingers. This provides an intuitive way to zoom into and out of the 3D data. Here is the first photo that shows the close-up view after zooming into the data:</p><div><img src="img/9727OS_08_08.jpg" alt="How to do it…"/></div><p>Here is another<a id="id445" class="indexterm"/> photo that <a id="id446" class="indexterm"/>shows what the data looks like when you zoom out by pinching your fingers:</p><div><img src="img/9727OS_08_09.jpg" alt="How to do it…"/></div><p>Finally, here is a screenshot of the demo application that shows a Gaussian distribution in 3D rendered in real-time with our OpenGL ES 3.0 shader program:</p><div><img src="img/9727OS_08_10.jpg" alt="How to do it…"/></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec137"/>How it works…</h2></div></div></div><p>In the second <a id="id447" class="indexterm"/>part of the<a id="id448" class="indexterm"/> demo, we demonstrated the use of a shader program written in OpenGL ES 3.0 to perform all the simulation and heat map-based 3D rendering steps to visualize a Gaussian distribution on a mobile GPU. Importantly, the shader code in OpenGL ES 3.0 is very similar to the code written in standard OpenGL 3.2 and above (see chapters 4 to 6). However, we recommend that you consult the specification to ensure that a particular feature of interest co-exists in both versions. More details on the OpenGL ES 3.0 specifications<a id="id449" class="indexterm"/> can be found at <a class="ulink" href="https://www.khronos.org/registry/gles/specs/3.0/es_spec_3.0.0.pdf">https://www.khronos.org/registry/gles/specs/3.0/es_spec_3.0.0.pdf</a>.</p><p>The hardware-accelerated portion of the code is programmed within the vertex shader program and stored inside the <code class="literal">g_vshader_code</code> variable; then the fragment shade program passes the processed color information onto the screen's color buffer. The vertex program handles the computation related to the simulation (in our case, we have a Gaussian function with a time-varying sigma value as demonstrated in <a class="link" href="ch03.html" title="Chapter 3. Interactive 3D Data Visualization">Chapter 3</a>, <em>Interactive 3D Data Visualization</em>) in the graphics hardware. We pass in the sigma value as a uniform variable and it is used to compute the surface height. In addition, we also compute the heat map color value within the shader program based on the height value. With this approach, we have significantly improved the speed of the graphic rendering step by completely eliminating the use of the CPU cycles on these numerous floating point operations.</p><p>In addition, we have included the GLM library used in previous chapters into the Android platform by adding the headers as well as the GLM path in the build script <code class="literal">Android.mk</code>. The GLM library handles the view and projection matrix computation and also allows us to migrate most of our previous work, such as setting up 3D rendering, to the Android platform.</p><p>Finally, our<a id="id450" class="indexterm"/> Android-based <a id="id451" class="indexterm"/>application also utilizes the inputs from the multi-touch screen interface and the device orientation derived from the motion sensor data. These values are passed through the JNI directly to the shader program as uniform variables.</p></div></div></body></html>