- en: Chapter 5. Advanced IR Block Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we have gone through some of the optimizations, which
    were mainly at instruction level. In this chapter, we will look at optimizations
    on block level where we will be optimizing a block of code to a simpler form,
    which makes the code more effective. We will start by looking at how loops are
    represented in LLVM, use the concept of dominance and CFG to optimize loops. We
    will use **Loop Simplification** (`LoopSimplify`)and **Loop Invariant Code Motion**
    optimizations for loop processing. We will then see how a scalar value changes
    during program execution and how the result of this **Scalar Evolution Optimization**
    can be used in other optimizations. Then we will look into how LLVM represents
    its in build functions called as LLVM intrinsics. Finally, we will look into how
    LLVM deals with concepts of parallelism by understanding its approach towards
    vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look into the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Loop processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalar evolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLVM intrinsics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loop processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before getting started with loop processing and optimization, we must have a
    little heads up about the concepts of CFG and dominance information. A CFG is
    the control flow graph of the program that gives a look into how the program may
    be executed through the various basic blocks. By dominance information, we get
    to know about the relation between the various basic blocks in the CFG.
  prefs: []
  type: TYPE_NORMAL
- en: In a CFG, we say a node `d` dominates a node `n` if every path (from the input
    towards output) that passes through `n` must also pass through `d`. This is denoted
    by `d -> n`. The graph `G = (V, E)`, where `V` is the set of basic blocks and
    `E` is the dominance relation defined on `V`, is called dominator tree.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example to show the CFG of a program and the corresponding dominator
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Put example code here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The CFG for the preceding code looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loop processing](img/00004.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'From what you have learned about dominance and dominator trees, the dominator
    tree for the preceding CFG looks something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loop processing](img/00005.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The first figure shows the CFG of the preceding code and the next figure shows
    the dominator tree for the same CFG. We have numbered each of the CFG components
    and we can see that 2 dominates 3 in the CFG, and 2 also dominates 4, 5, and 6\.
    3 dominates 4, 5, and 6 and is the immediate dominator of these. There is no dominance
    relation between 4 and 5\. 6 is not dominated by 5 because there is another path
    available through 4 and for the same reasons, 4 does not dominate 6.
  prefs: []
  type: TYPE_NORMAL
- en: All the loop optimizations and transformation in LLVM are derived from the `LoopPass`
    class implemented in the `LoopPass.cpp` file located in `lib/Analysis`. The `LPPassManager`
    class is responsible for the handling of all `LoopPasses`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important class to get started with loop processing is the `LoopInfo`
    Class, which is used to identify the natural loops in the code and to know the
    depth of various nodes in the CFG. Natural loops are the cyclic structures in
    a CFG. To define a natural loop in a CFG, we must know what a backedge is: it
    is an edge in the CFG where the source dominates the target. A natural loop can
    be defined by a backedge `a->d` that defines a subgraph of the CFG, where `d`
    is the header node and it contains all other basic blocks that can reach a without
    having to reach `d`.'
  prefs: []
  type: TYPE_NORMAL
- en: We can see in the preceding diagram that the backedge `6->2` forms a natural
    loop consisting of the nodes `2`, `3`, `4`, `5`, and `6`.
  prefs: []
  type: TYPE_NORMAL
- en: The next important step is loop simplification that transforms the loop into
    a canonical form, which includes the insertion of a preheader to the loop, which
    in turn ensures that there is a single entry edge to the loop header from outside
    the loop. It also inserts loop exit blocks, which ensure that all exit blocks
    from the loop have predecessors only from within the loop. These insertion of
    pre-header and exit blocks help in later loop optimizations, such as Loop Independent
    Code Motion.
  prefs: []
  type: TYPE_NORMAL
- en: Loop Simplification also ensures that the loop will have only one backedge,
    that is if the loop header is having more than two predecessors, (from the pre
    header block and multiple latches to the loop) we adjust only this loop latch.
    One way of doing this is by inserting a new block which is the target of all the
    backedges and make this new block jump to loop header. Let's take a look at how
    a loop looks after **Loop Simplify Pass**. We will be able to see that a preheader
    node is inserted, new exit blocks are created, and there is only one backedge.
  prefs: []
  type: TYPE_NORMAL
- en: '![Loop processing](img/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now, after getting the required information from `LoopInfo` and simplifying
    the loop to a canonical form, we will look into some of the loop optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main loop optimizations is **Loop Invariant Code Motion** (**LICM**)
    optimization. This pass tries to remove as much code from the body of the loop
    as possible. The condition for removal of the code is that this piece of code
    is invariant inside the loop, that is the output of this part of code not dependent
    on loop execution and it will remain same in every iteration of the loop. This
    is done by moving this piece of code either in the preheader block or moving the
    code to exit blocks. This pass is implemented in the `lib/TransformsScalar/LICM.cpp`
    file. If we look into the code of the loop, we see it requires `LoopInfo` and
    `LoopSimplify` passes to be run before it. Also, it needs the `AliasAnalysis`
    information. Alias analysis is needed to move loop invariant loads and calls out
    of the loop. If there is no load and call inside the loop that aliases anything
    stored, we can move these out of the loop. This also helps in scalar promotion
    of memory.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example to see how LICM is getting done.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write the testcase in a file `licm.ll`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This `testcase` has a loop denoted by Loop block in the test code with the loop
    condition being `br i1 %cond`, `label %Exit`, `label %Loop` (Latch part of the
    loop). We can see the `%j` value, which is being used as the induction variable
    is derived after using the phi instruction. Basically, it tells to choose the
    value `0` if the control is coming from the Entry block and `%Val` if the control
    is coming from Loop block. In this, the invariant code can be seen as `%loopinvar
    = mul i32 %i, 17`, as `%loopinvar` value is independent of the iteration of loop
    and depends on the function argument only. So when we run the LICM pass, we expect
    this value to be hoisted out of the loop, thus preventing its computation in every
    iteration of the loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the `licm` pass and see the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the output, the calculation `%loopinvar = mul i32 %i, 17` is
    hoisted out of the loop, which is the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: We have many other loop optimizations such as **Loop Rotation**, **Loop Interchange**,
    **Loop Unswitch**, and so on. The source codes for these can be looked under the
    LLVM folder `lib/Transforms/Scalar` to get more understanding about these optimizations.
    In the next section, we will see the concept of scalar evolution.
  prefs: []
  type: TYPE_NORMAL
- en: Scalar evolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By scalar evolution, we mean how the value of a scalar changes in a program
    with the execution of code. We look at a particular scalar value and see how it
    is getting derived, what all other elements it is dependent on, whether this is
    known at compile time or not, and what all operations are being performed. We
    need to look into a block of code rather than looking into individual instructions.
    A scalar value is build up from two elements, a variable and an operation of constant
    step. The variable element that builds up this scalar value is unknown at compile
    time and its value can be known at run time only. The other element is the constant
    part. These elements themselves may be recursively broken into other elements
    such as a constant, an unknown value or an arithmetic operation.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea here is to look at complete scalar value containing the unknown
    part at compile time and see how this value will evolve during execution and try
    to use this for optimization. One example is removing a redundant value for which
    the scalar evolution is similar to some other value in the same program.
  prefs: []
  type: TYPE_NORMAL
- en: In LLVM, we can use scalar evolution to analyze code that contains common integer
    arithmetic operations.
  prefs: []
  type: TYPE_NORMAL
- en: In LLVM `ScalarEvolution` class is implemented in `include/llvm/Analysis`, which
    is a LLVM pass and can be used analyze scalar expressions in a loop. It is able
    to recognize general induction variables (a variable in loop whose value is a
    function of loop iteration number) and represent them using object of SCEV class,
    which is used to represent analyzed expression in a program. Using this analysis
    trip count and other important analysis can be obtained. This scalar evolution
    analysis is mainly used in induction variable substitution and strength reduction
    of loops.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example now and run the scalar evolution pass on it and see what
    output it generates.
  prefs: []
  type: TYPE_NORMAL
- en: Write a testcase `scalevl.ll` with a loop and some scalar values within the
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this test case, we have a loop consisting of header and body blocks with
    `%a` and `%b` being the scalars in loop body of interest. Let''s run the scalar
    evolution pass on this and see the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the output of scalar evolution pass shows the range of values
    for a particular variable (`U` stands for unsigned range and `S` for signed range,
    here both are same) and the exit value, the value in that variable when the loop
    runs its last iteration. For example, the value `%i` has the range as `[1,11)`,
    that is the starting iteration value is `1` and when the value of `%i` becomes
    `11` the condition `%cond = icmp eq i32 %i, 10` becomes false and the loop breaks.
    So, the the value of `%i` when it exited the loop was `10`, which is denoted by
    `Exits: 10` in the output.'
  prefs: []
  type: TYPE_NORMAL
- en: The value in the form of `{x,+,y}` representation, such as `{2,+,1}`, represents
    add recurrence, that is the expressions changing value during loop execution where
    x represents the base value at 0th iteration and y represents the value added
    to it on each subsequent iteration.
  prefs: []
  type: TYPE_NORMAL
- en: The output also shows the number of times the loop has iterated after the first
    run. Here, it shows the value `9` for backedge-taken, that is the loop has run
    `10` times in total. The max backedge-taken value is the least value which can
    never be less than the backedge-taken value, which here is `9`.
  prefs: []
  type: TYPE_NORMAL
- en: This is the output for this example, you can try some other test cases and see
    what this pass outputs.
  prefs: []
  type: TYPE_NORMAL
- en: LLVM intrinsics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An intrinsic function is a function built in to the compiler. The compiler knows
    how to best implement the functionality in the most optimized way for these functions
    and replaces with a set of machine instruction for a particular backend. Often,
    the code for the function is inserted inline thus avoiding the overhead of function
    call (In many cases, we do call the library function. For example, for the functions
    listed in [http://llvm.org/docs/LangRef.html#standard-c-library-intrinsics](http://llvm.org/docs/LangRef.html#standard-c-library-intrinsics)
    we make a call to `libc`). These are also called built-in functions for other
    compilers.
  prefs: []
  type: TYPE_NORMAL
- en: In LLVM these intrinsics are introduced during code optimization at IR level
    (Intrinsics written in program code can be emitted through frontend directly).
    These function names will start with a prefix "`llvm.`", which is a reserved word
    in LLVM. These functions are always external and a user cannot specify the body
    for these functions in his/her code. In our code, we can only call these intrinsic
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will not go much deep into details. We will take an example
    and see how LLVM optimizes certain part of code with its own intrinsic functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write a simple code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now use Clang to generate the IR file. Using the command given below, we will
    get the `intrinsic.ll` file that contains the unoptimized IR without any intrinsic
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, use the opt tool to optimize the IR with O1 level of optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The important optimization to be noted here is the call to LLVM intrinsic function
    `llvm.memset.p0i8.i64` to fill the array with value `0`. The intrinsic functions
    may be used to implement vectorization and parallelization in the code, leading
    to better code generation. It might call the most optimized version of the `memset`
    call from the `libc` library and may choose to completely omit this function if
    there is no usage of this.
  prefs: []
  type: TYPE_NORMAL
- en: The first argument in the call specifies the array "`a`", that is the destination
    array where the value needs to be filled. The second argument specifies the value
    to be filled. The third argument to the call is specification about number of
    bytes to be filled. The fourth argument specifies the alignment of destination
    value. The last argument is to determine whether this is a volatile operation
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: There is a list of such intrinsic functions in LLVM, a list of which can be
    found at [http://llvm.org/docs/LangRef.html#intrinsic-functions](http://llvm.org/docs/LangRef.html#intrinsic-functions).
  prefs: []
  type: TYPE_NORMAL
- en: Vectorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Vectorization** is an important optimization for compilers where we can vectorize
    code to execute an instruction on multiple datasets in one go. Advance target
    architecture typically have vector registers set and vector instructions—where
    broad range of data type (typically 128/246 bit) can be loaded into the vector
    registers and operations can be performed on those register set, performing two,
    four, and sometimes eight operations at the same time, with the cost of one scalar
    operation.'
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of vectorization in LLVM—**Superword-Level Parallelism**
    (**SLP**) and loop vectorization. Loop vectorization deals with vectorization
    opportunities in a loop, while SLP vectorization deals with vectorizing straight-line
    code in a basic block.
  prefs: []
  type: TYPE_NORMAL
- en: A vector instruction performs **Single-instruction multiple-data** (**SIMD**)
    operations; the same operation on multiple data lanes (in parallel).
  prefs: []
  type: TYPE_NORMAL
- en: '![Vectorization](img/00007.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Let's look at how SLP Vectorization is implemented in LLVM infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: As the code itself attributes, the implementation of SLP Vectorization in LLVM
    is inspired by the work described in the paper *Loop-Aware SLP in GCC* by Ira
    Rosen, Dorit Nuzman, and Ayal Zaks. LLVM SLP Vectorization Pass implements the
    Bottom Up SLP vectorizer. It detects consecutive stores that can be put together
    into vector-stores. Next, it attempts to stores that can be put together into
    vector-stores. Next, it attempts to construct vectorizable tree using the `use-def`
    chains. If a profitable tree was found, the SLP vectorizer performs vectorization
    on the tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three stages to SLP Vectorization:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the pattern and determine if it is a valid Vectorization pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine if it is profitable to vectorize the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If step 1 and 2 are true, then vectorize the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider addition of `4` consecutive elements of two arrays into third array.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The IR for the preceding kind of expression will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The expression tree for the preceding pattern can be visualized as a chain
    of stores and loads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Vectorization](img/00008.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the preceding expression tree, the bottom-up SLP Vectorization first constructs
    a chain that starts with a store instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'It then scans the tree already built in the preceding code for all the stores
    in the given basic block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `collectStores()` function collects all the store references.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The function `SLPVectorizer::vectorizeStoreChains()` has three steps and function
    calls to each three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The first step is to identify pattern. The function `buildTree()` subsequently
    builds up the tree recursively as the preceding visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For our given example, it will identify that all the store operations have
    binary addition operations as their operands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When the binary operation `ADD` is encountered, it again recursively builds
    tree (calling the same function) on LHS and RHS operands of the ADD operation,
    which in our case are both `Load`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: While building the tree, there are several checks that validate if the tree
    can be vectorized. For example, in the preceding case, when loads are encountered
    across trees, it is checked whether they are consecutive loads or not. In our
    expression tree, the loads across trees in LHS—b[0], b[1], b[2], and b[3] are
    accessing consecutive memory location. Similarly, loads across tress in RHS—c[0],
    c[1], c[2] and c[3] are accessing consecutive memory location. If any of the checks
    fail for a given operation, the building of a tree is aborted and code is not
    vectorized.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the pattern is identified and the vector tree is built, the next step
    is to get the cost of vectorizing the built tree. This effectively refers to the
    cost of the tree if it is vectorized compared to the cost of tree in current scalar
    form. If the vector cost is less than the scalar cost, it is beneficial to vectorize
    the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'An important interface to focus on here is the **TargetTransformInfo** (**TTI**),
    which provides access to the codegen interfaces that are needed for IR-level transformations.
    In our SLP Vectorization, TTI is used to get the cost of the vector instruction
    of the built vector tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the same function, the cost of vector add is also calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In our example, the total cost of the whole expression tree comes out to be
    `-12`, which indicates that it is profitable to vectorize the tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the tree is vectorized by the function call `R.vectorizeTree()` on
    the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Lets see all the steps the Vectorization process follows for our example. Note
    that this will require a '`Debug`' build of the '`opt`' tool.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The final vectorized output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we concluded the optimizer part of the compiler where we had
    seen block level optimizations. We took the examples of loop optimization, Scalar
    Evolution, Vectorization, and LLVM Intrinsic functions. We also saw how SLP Vectorization
    is handled in LLVM. However, there are many other such optimizations that you
    can look into and get a hold of.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how this IR is converted to **Directed Acyclic
    Graph**. We have some optimizations at `selectionDAG` level as well, which we
    will take a look at.
  prefs: []
  type: TYPE_NORMAL
