<html><head></head><body><div><div><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7.  One Step Forward, One Level Down - OpenGL Basics </h1></div></div></div><p>Often times it's easy to take a library like SFML for granted. After all, the ideas and concepts offered by it seem quite intuitive. Building something rather simple can take as little as a couple of minutes, and there are no major headaches to deal with. In a perfect world, we could just offload those troubles to someone else and simply rely on increasingly higher levels of abstraction to get the job done. However, what happens when certain limitations make us slam face-first into a brick wall? In order to know the way around them, it's necessary to know the fundamentals that SFML was built on. In other words, at that point, downward is the only way forward.</p><p>In this chapter, we are going to be covering:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Setting up and using OpenGL with a window from SFML</li><li class="listitem" style="list-style-type: disc">Shaping and submitting data to the GPU</li><li class="listitem" style="list-style-type: disc">Creating, building, and using shaders for rendering</li><li class="listitem" style="list-style-type: disc">Applying textures to geometry</li><li class="listitem" style="list-style-type: disc">Looking at various coordinate spaces and model transformations</li><li class="listitem" style="list-style-type: disc">Implementing a camera</li></ul></div><p>That is quite a laundry list of things to do, so let us not waste any time and jump right in!</p><div><div><div><div><h1 class="title"><a id="ch07lvl1sec54"/>Use of copyrighted resources</h1></div></div></div><p>As always, let us acknowledge those who deserve to be acknowledged, and give credit where credit's due. These are the resources used in this chapter:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Old wall texture by <code class="literal">texturelib.com
</code> under the CC0: <a class="ulink" href="http://texturelib.com/texture/?path=/Textures/brick/medieval/brick_medieval_0121">license:http://texturelib.com/texture/?path=/Textures/brick/medieval/brick_medieval_0121</a></li><li class="listitem" style="list-style-type: disc">STB public domain image loader by <em>Sean Barrett</em> under the CC0 license: <a class="ulink" href="https://github.com/nothings/stb/blob/master/stb_image.h">https://github.com/nothings/stb/blob/master/stb_image.h</a></li></ul></div></div></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec55"/>Setting up OpenGL</h1></div></div></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">In order to have access to the latest version of OpenGL, we need to download two libraries. One is named the OpenGL Extension Wrangler Library. It loads and makes available all OpenGL extensions that are supported on the target platform. The library can be downloaded here <a class="ulink" href="http://glew.sourceforge.net/">http://glew.sourceforge.net/</a>.</li><li class="listitem" style="list-style-type: disc">The other library we need is called OpenGL Mathematics, or GLM for short. It is a header-only library that adds a lot of extra data types and functions, which come in handy more often than not. Anything from simple vector data types to functions used to calculate cross products are added in by this library. It can be found here <a class="ulink" href="http://glm.g-truc.net/0.9.8/index.html">http://glm.g-truc.net/0.9.8/index.html</a>.</li></ul></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec55"/>Setting up a Visual Studio project</h2></div></div></div><p>Alongside the usual SFML includes, which we are still going to need for creating a window, we also need to add the GLEW and GLM <code class="literal">include</code> folders in the <strong>Include Directories</strong> field under <strong>VC++ Directories</strong>.</p><p>The GLEW <strong>Additional Library Directory</strong> must be added in as well in the <strong>General</strong> section under <strong>Linker</strong>. The library files are located inside the <code class="literal">Release</code> folder, which holds a couple of directories: <code class="literal">Win32</code> and <code class="literal">x64</code>. These need to be set up correctly for different build configurations.</p><p>Finally, the <code class="literal">glew32.lib</code> file has to be added to the <strong>Additional Dependencies</strong> field in the <strong>Input</strong> section under <strong>Linker</strong>, as well as the <code class="literal">OpenGL32.lib</code> file. It can be linked statically, in which case, <code class="literal">glew32s.lib</code> needs to be added instead of the regular <code class="literal">glew32.lib</code>. If linking statically, the <code class="literal">GLEW_STATIC</code> <strong>Preprocessor Definition</strong> in the <strong>Preprocessor</strong> section under <strong>C/C++</strong> needs to be added as well.</p></div></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec56"/>Using GLEW</h1></div></div></div><p>The first thing we are going to need if we are working with OpenGL is a window. Luckily, window creation isn't OpenGL specific, so one can be made using almost any library out there that supports it, including SFML. For our purposes, we'll be reusing the Window class with some minor adjustments to it, including the actual SFML window type:</p><pre class="programlisting">class GL_Window { 
  ... 
private: 
  ... 
  <strong>sf::Window</strong> m_window; 
  ... 
}; 
</pre><p>Note the data type of the <code class="literal">m_window</code> data member. If actual SFML is not used to draw anything, we do not need an instance of <code class="literal">sf::RenderWindow</code> and can instead work with <code class="literal">sf::Window</code>. This means that any task that does not have anything to do with the actual window has to be handled separately. This even includes clearing the window:</p><pre class="programlisting">void GL_Window::BeginDraw() { 
  glClearColor(0.f, 0.f, 0.f, 1.f); // BLACK 
  glClear(GL_COLOR_BUFFER_BIT); 
} 
</pre><p>Here we get a glimpse at the first two GL functions we are going to be using. Because GLEW is a C API, code that looks like this will be quite common. There are no classes to manage, as every task is performed via function calls and a shared state. Case in point, our first function <code class="literal">glClearColor()</code> actually sets up the color that the screen will be cleared with, including the alpha channel.</p><div><div><h3 class="title"><a id="note17"/>Note</h3><p>This specific function, as well as many others, takes in what is known as a <strong>normalized</strong> vector. It is useful when representing proportion. For example, clearing the screen to the color purple would mean passing the value <em>0.5f </em>as the first and the third parameter, which would mean half of the colour is red, and the other half is blue.</p></div></div><p>The second function call actually performs the clearing with the stored value. It takes in a single argument, which is essentially just a bitmask, defined using the <code class="literal">#define</code> pre-processor directive. This specific implementation detail allows more masks to be passed into the function call by utilizing <strong>bitwise</strong> or operations, represented by the pipe <em>|</em> symbol. This concept will be revisited by us eventually.</p><p>With that out of the way, let us actually create the window and initialize the <code class="literal">GLEW</code> library:</p><pre class="programlisting">Game::Game() : m_window("Chapter 7", sf::Vector2u(800, 600)) 
{ 
  ... 
  std::cout &lt;&lt; glGetString(GL_VERSION) &lt;&lt; std::endl; 
  GLenum status = glewInit(); 
  if (status != GLEW_OK) { 
    std::cout &lt;&lt; "GLEW failed!" &lt;&lt; std::endl; 
  } 
  ... 
} 
</pre><p>All we need to do in order to initialize GLEW is to call a single function <code class="literal">glewInit()</code>. It returns a value, which represents the success/failure of the operation. Another useful function to keep around is <code class="literal">glGetString()</code>. It returns a static string that represents specific information about the OpenGL version that is supported by the computer it is executed on. In this case, we specifically want to check the version of OpenGL and print it out, but it can also be used to determine the OpenGL extensions, the supported GLSL version, the name of the hardware rendering platform, and so on.</p></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec57"/>The rendering pipeline</h1></div></div></div><p>When drawing something on the screen, a certain sequence of steps must be followed in order to submit the geometry, convert it to pixels, and color them all appropriately. This particular sequence of steps is often referred to as the <strong>rendering pipeline</strong>. How it functions depends entirely on the version of OpenGL you are using. Versions below <em>3.0</em> use what is called a <strong>fixed function pipeline</strong>, while newer OpenGL releases of <em>3.0 +</em> utilize the <strong>programmable pipeline</strong>. The former is now deprecated and is referred to as <strong>legacy</strong> OpenGL, while the latter is widely used and applied, even on mobile devices, and has become the standard.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec56"/>Fixed function pipeline</h2></div></div></div><p>Actually drawing things on screen with the fixed function pipeline is much easier than the modern way of doing things, but it comes at a price. Consider the following example:</p><pre class="programlisting">glBegin(GL_TRIANGLES); 
  
glColor3f(1.0f, 0.0f, 0.0f); // Red 
glVertex3f(-0.5f, -0.5f, 0.5f); 
  
glColor3f(0.0f, 1.0f, 0.0f); // Green 
glVertex3f(-0.5f, 0.5f, 0.5f); 
    
glColor3f(0.0f, 0.0f, 1.0f); // Blue 
glVertex3f(0.5f,  0.5f, 0.5f); 
  
glEnd(); 
</pre><p>This particular block of code is quite easily readable, which is one advantage of the legacy method. We begin by invoking the <code class="literal">glBegin()</code> method and passing in a value, which signifies how the actual vertices should be interpreted as they are being submitted. We are working with triangles, which means that every three vertices submitted in a row will be connected and turned into a triangle. Note the calls to <code class="literal">glColor3f</code> as well. The color of the vertices is set as they are being submitted, and the same can be done with texture coordinates as well. The final call to the <code class="literal">glEnd()</code> method flushes all of the submitted data to the GPU for rendering.</p><p>While this is very readable and easy to understand for newcomers, the vertex data has to be resubmitted to the GPU every frame, which heavily impacts performance. Small applications would not notice the difference, but the memory transfer overhead really starts to add up after a significant number of primitives have been submitted.</p><p>Another issue with this approach is how limited it is. Certain effects, if at all possible, can be extremely slow to pull off with the fixed function pipeline.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec57"/>Programmable pipeline</h2></div></div></div><p>Using the programmable pipeline is quite a bit more complicated for small tasks, but it proves invaluable for larger projects. Just like the fixed function pipeline, there are steps that are static and unchanging. The programmable pipeline does, however, provide a way to customize certain aspects of how the data submitted to the GPU is processed. This is where <strong>shaders</strong> come in. Shaders have already been briefly covered in the previous chapter; however, there is much more to them that has not yet been explained. They are the programs that can be written in a C-like language and executed on the GPU instead of the CPU. As it turns out, shaders are used to customize certain parts of the programmable pipeline. Consider the following diagram:</p><div><img src="img/image_07_001.jpg" alt="Programmable pipeline"/></div><p>Just like the fixed-function pipeline, vertex data is submitted to the GPU. However, this data is not re-submitted every frame. Instead, the vertex data lives on the GPU and can be referred to when it needs to be rendered. Once a call has been made to draw a specific set of vertices, they're passed in to be processed and relayed to the vertex shader.</p><p>The <strong>Vertex shader</strong> is one of few programmable bits of this pipeline. It is often used to calculate the positions of vertices in the appropriate coordinate system, and pass these vertices down the pipeline to be processed further.</p><p>The <strong>Tessellation</strong> stage essentially is responsible for performing sub-divisions of our existing geometry into smaller primitives. It actually ends up connecting the vertices and passing these primitives further down the pipeline. There are two shaders in this stage that can be written and used; however, we are not going to be doing that.</p><p>All of the primitive data is then passed down to a <strong>Geometry shader</strong>, which just like the two tessellation shaders is optional. It can be used to generate more vertices from the existing geometry.</p><p>After the primitives have been properly assembled, they are passed further down and handled by the rasterizer.</p><p><strong>Rasterization</strong> is the actual process of turning vertex and primitive information into pixel data. These pixels are then passed further down the pipeline.</p><p>The last programmable bit of this pipeline receives all of the pixel information from the previous stage. It is called the <strong>Fragment shader</strong> (that is, pixel shader), and can be used to determine the value of each individual pixel within the geometry we are rendering. Anything from assigning specific colors, to actually sampling pixels of a texture is done at this stage. These pixels are then pushed further down to be handled by other stages.</p><p>The <strong>Depth &amp; Stencil</strong> stage performs various tests in order to clip unneeded pixels that should not be drawn on screen. If a pixel is outside of the window area or even behind another bit of geometry, it is dropped at this stage.</p><p>Unclipped pixels are then blended onto the existing frame buffer, which is used to draw everything on screen. Before they are blended, however, the <strong>Dithering</strong> process takes place, making sure pixels are correctly rounded up or down if the render image has less or more precision than the value we have.</p><p>Although it may be hard to grasp this concept at first, the programmable pipeline is a superior approach to modern rendering. Out of all of these stages covered, we only really need to write the vertex and fragment shaders to get started. We will be covering that very soon.</p></div></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec58"/>Storing and drawing primitives</h1></div></div></div><p>All of our primitive data has to be represented as a set of vertices. Whether we are dealing with a triangle or a sprite on screen, or if it is a huge, complex model of a monster, it can all be broken down to this fundamental type. Let us take a look at a class that represents it:</p><pre class="programlisting">enum class VertexAttribute{ Position, COUNT }; 
 
struct GL_Vertex { 
  GL_Vertex(const glm::vec3&amp; l_pos): m_pos(l_pos) {} 
 
  glm::vec3 m_pos; // Attribute 1. 
  // ... 
}; 
</pre><p>As you can see, it is only a simple <code class="literal">struct</code> that holds a 3D vector that represents a position. Later on, we might want to store other information about a vertex, such as texture coordinates, its color, and so on. These different pieces of information about a specific vertex are usually referred to as <strong>attributes</strong>. For convenience, we are also enumerating different attributes to make the rest of our code more clear.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec58"/>Vertex storage</h2></div></div></div><p>Before any primitives can be drawn, their data must be stored on the GPU. In OpenGL, this task is achieved by utilizing <strong>Vertex Array Objects </strong>(<strong>VAO</strong>) and <strong>Vertex Buffer Objects </strong>(<strong>VBO</strong>).</p><p>A vertex buffer object can simply be thought of as space that gets allocated on the GPU for storing data. That data can be anything. It could be vertex positions, colors, texture coordinates, and so on. We are going to use VBOs to store all of our primitive information.</p><p>A vertex array object is like a parent to a VBO, or even multiple VBOs. It stores information about how data that lives inside a VBO should be accessed, how information can be passed into various shader stages, and many more details that together form a state. If a VBO is the actual data pool, a VAO can be thought of as an instruction set of how to access that data.</p><p>Both VAO and VBO instances are identified by simple integers, which get returned after the space is allocated. These integers will be used to differentiate different buffers and array objects.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec59"/>The model class</h2></div></div></div><p>With that bit of information out of the way, we can finally get down to actually implementing a model class! A model, in our case, is any set of triangles that can form a shape. With enough triangles, any shape can be modelled. Let us take a look at the class header:</p><pre class="programlisting">class GL_Model { 
public: 
  GL_Model(GL_Vertex* l_vertices, unsigned int l_vertCount); 
  ~GL_Model(); 
 
  void Draw(); 
private: 
  GLuint m_VAO; 
  GLuint m_vertexVBO; 
  unsigned int m_drawCount; 
}; 
</pre><p>As you can tell, it is quite simple. The constructor takes in two arguments for now: a pointer to the first instance of a vertex, and the number of vertices we are actually submitting. This makes it easy for us to load a model quickly from a simple array of vertices, although it may not be the best way of loading more complex meshes.</p><p>Note that the class also has a <code class="literal">Draw()</code> method, which will be used later on to actually submit its vertices to the rendering pipeline and begin the drawing process.</p><p>Lastly, we have the two <em>GL unsigned integer</em> types: <code class="literal">m_VAO</code> and <code class="literal">m_vertexVBO</code>. These integers will refer to the actual vertex array object that is used with this model, as well as the vertex buffer object, used to store all of the vertex information. We also have an <em>unsigned integer</em>, <code class="literal">m_drawCount</code>, which is going to store the number of vertices this particular model has in order to draw them all.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec9"/>Implementing the model class</h3></div></div></div><p>With that out of the way, let us begin allocating and filling in our data structures! The constructor of the <code class="literal">GL_Model</code> class is going to be helping us with that task:</p><pre class="programlisting">GL_Model::GL_Model(GL_Vertex* l_vertices, 
  unsigned int l_vertCount) 
{ 
  m_drawCount = l_vertCount; 
 
  glGenVertexArrays(1, &amp;m_VAO); 
  glBindVertexArray(m_VAO); 
  glGenBuffers(1, &amp;m_vertexVBO); 
 
  glBindBuffer(GL_ARRAY_BUFFER, m_vertexVBO); 
  glBufferData(GL_ARRAY_BUFFER, 
    l_vertCount * sizeof(l_vertices[0]), 
    l_vertices, GL_STATIC_DRAW); 
  glEnableVertexAttribArray( 
    static_cast&lt;GLuint&gt;(VertexAttribute::Position)); 
  glVertexAttribPointer( 
    static_cast&lt;GLuint&gt;(VertexAttribute::Position), 3, GL_FLOAT, 
    GL_FALSE, 0, 0); 
 
  glBindVertexArray(0); 
} 
</pre><p>We begin by copying the amount of vertices to the <code class="literal">m_drawCount</code> data member. This is going to be useful later, as we need to know exactly how many vertices need to be drawn before actually rendering them. Some space for a VAO is then allocated, using the <code class="literal">glGenVertexArrays</code> function. Its first argument is the amount of objects that need to be created, while the second one takes a pointer to a variable that is going to store the returned identifiers.</p><p>The next function call, <code class="literal">glBindVertexArray()</code>, actually enables a vertex array object with the provided identifier, so that any subsequent function call after this one modifies the vertex array object that was passed in as the argument. Any vertex array object manipulation from this point on will be performed on the VAO with the identifier, <code class="literal">m_VAO</code>.</p><div><div><h3 class="title"><a id="note18"/>Note</h3><p>Because GLEW is a C API, the idea of binding and unbinding something dominates most aspects of it. In order to modify or do anything with certain data that lives on the GPU, the appropriate buffer must be bound first.</p></div></div><p>Just like the VAO, the vertex buffer object also needs to be generated. The function <code class="literal">glGenBuffers</code> does just that. In this case, we only need one buffer object, which is what the first argument denotes. Once it is generated, just like the VAO, we need to bind to this buffer in order to modify it. This is where the <code class="literal">glBindBuffer</code> function comes in. As it is bound to, we also need to specify the type of buffer we are going to treat it as. Because we just want an array of data, <code class="literal">GL_ARRAY_BUFFER</code> is used.</p><p>Now that we have a buffer created, we can push some data to it! A call to <code class="literal">glBufferData</code> does just that. The first argument, just like the previous function, determines what kind of buffer we are dealing with. The second argument is the <strong>byte</strong> size of the data chunk we want to submit, which OpenGL has to know in order to allocate enough space for the buffer to hold all of the data. In this case, it is just the number of vertices multiplied by the number of bytes the first element takes up. The third argument is just a pointer to the actual data structure we want to submit. How much of that is read in is determined by the second argument, which, in this case, is all of it. Finally, the last argument is used as a hint for OpenGL to manage the data storage as efficiently as possible depending on its use. It stores the data differently depending on what we do with it. <code class="literal">GL_STATIC_DRAW</code> means we are not going to be modifying the data, so that it can store it a certain way that is most efficient for this situation.</p><p>With all of the data buffered, we can begin working with the VAO again and give it information about how the vertex information should be accessed. Because the vertex position has to be passed to the fragment shader, we need to enable it as an attribute and store information about how it should be processed in the VAO. This is where <code class="literal">glEnableVertexAttribArray()</code> and <code class="literal">glVertexAttribPointer()</code> functions come in.</p><p>The former function simply enables a certain attribute to be used by the vertex shader. <code class="literal">VertexAttribute::Position</code> evaluates to <code class="literal">0</code>, so the <em>0th</em> attribute in the vertex shader is enabled for use. The latter, however, actually specifies how this data is read and processed before it gets piped down the vertex shader. In this case, the <em>0th</em> attribute is defined as a set of three variables, all of which are floats. The next argument can be useful if we want to normalize the data before it gets sent to the vertex shader. In this case, we do not need to do that, so <code class="literal">GL_FALSE</code> is passed in instead. The last two arguments are the byte stride and byte offset of the data we are interested in inside the buffer. Because we are only storing the vertexes position inside the <code class="literal">GL_Vertex</code> structure so far, both of these values are <code class="literal">0</code>. However, what would happen if we had more attributes? Consider the following diagram:</p><div><img src="img/image_07_002.jpg" alt="Implementing the model class"/></div><p>Imagine we have all of the data inside a buffer, which was shown previously. For each vertex in there, its position is followed by its color, and then by another vertexes position. If we just want to filter out the position data, for example, stride and offset can be very useful. The stride argument is the number of bytes that have to be jumped from the beginning of one data segment to another. Effectively, stride can be thought of as the size of the entire vertex's data structure, which, in this case, is the sum of the size of the position vector, as well as the color vector. To put it simply, it's the number of bytes from the beginning of one vertex, to the beginning of another.</p><p>Offset, on the other hand, is just the number of bytes we need to move from the beginning of whichever structure we happen to be reading in order to reach the desired element. Accessing the color element would mean the offset would have to be the size of the position vector. To put it simply, the offset is the number of bytes from the beginning of the structure to the beginning of the desired element.</p><p>After our data is submitted and accounted for, we can use <code class="literal">glBindVertexArray</code> again to bind to <em>0</em>, which would show that we're done with the VAO.</p><p>All of this allocated data actually has to be disposed of when it's no longer needed. The destructor can help us here:</p><pre class="programlisting">GL_Model::~GL_Model() { 
  glDeleteBuffers(1, &amp;m_vertexVBO); 
  glDeleteVertexArrays(1, &amp;m_VAO); 
} 
</pre><p>First, the vertex buffer object needs to be disposed of. We pass the number of VBOs, as well as the pointer to the first identifier to the <code class="literal">glDeleteBuffers</code> function, which purges all of the buffer data on the GPU. The VAO follows a similar procedure afterwards.</p><p>Finally, we can implement the <code class="literal">Draw</code> method of our <code class="literal">Model</code> class:</p><pre class="programlisting">void GL_Model::Draw() { 
  glBindVertexArray(m_VAO); 
  glDrawArrays(GL_TRIANGLES, 0, m_drawCount); 
  glBindVertexArray(0); 
} 
</pre><p>Before drawing something, we need to specify which data the pipeline should use. All of the vertex information sits safely in our buffer object that is managed by the VAO, so we bind it. The <code class="literal">glDrawArrays</code> function is then invoked. As the name states, it draws arrays of vertices. Its first argument is the type of primitive we want to draw, which in this case is triangles. Lines, points, and other types can also be drawn like this. The second argument is the starting index inside the buffer array object. Since we want to draw everything from the beginning, this is set to <em>0</em>. Lastly, the number of vertices to be drawn is passed in. The call to this function actually initiates the rendering pipeline, sending all of the vertex data into the vertex shader. The final call is to the <code class="literal">glBindVertexArray()</code> function that simply unbinds our VAO.</p></div></div></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec59"/>Using shaders</h1></div></div></div><p>The standardization of the programmable pipeline now means shaders have to be written for certain tasks, including the basic ones. This means that simply submitting our vertex data and rendering it would do nothing, as the two fundamental chunks of the rendering pipeline, the vertex and fragment shaders, are non-existent. In this section, we are going to cover how shaders are loaded, built, and applied to our virtual geometry, in turn producing those glorious pixels on the screen.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec60"/>Loading shader files</h2></div></div></div><p>Before we can use shaders, we must first discuss how they are loaded. All we technically need to create a shader is a string, containing all of its code. A very simple helper function can be written to parse a file and return it as a string, as shown here:</p><pre class="programlisting">inline std::string ReadFile(const std::string&amp; l_filename) { 
  std::ifstream file(l_filename); 
  if (!file.is_open()) { return ""; } 
  std::string output; 
  std::string line; 
  while (std::getline(file, line)) { 
    output.append(line + "\n"); 
  } 
  file.close(); 
  return output; 
} 
</pre><p>This is nothing we have not seen before, when it comes to file reading and parsing. A string is created, then appended to with each new line of the file being read, and finally returned.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec61"/>Creating shader programs</h2></div></div></div><p>OpenGL shaders themselves are part of programs that are used throughout the rendering pipeline. If we have a vertex shader and a fragment shader we wish to utilize, both of them are actually joined into one program, which is then bound to so that the pipeline can use the appropriate shader at the right time. This is important, because it shapes the way the <code class="literal">GL_Shader</code> data structure is built:</p><pre class="programlisting">enum class ShaderType{ Vertex, Fragment, COUNT }; 
 
class GL_Shader { 
public: 
  GL_Shader(const std::string&amp; l_fileName); 
  ~GL_Shader(); 
 
  void Bind() const; 
private: 
  static void CheckError(GLuint l_shader, GLuint l_flag, 
    bool l_program, const std::string&amp; l_errorMsg); 
  static GLuint BuildShader(const std::string&amp; l_src, 
    unsigned int l_type); 
 
  GLuint m_program; 
  GLuint m_shader[static_cast&lt;unsigned int&gt;(ShaderType::COUNT)]; 
}; 
</pre><p>First, we enumerate the shader types we are going to be using. For basic purposes, vertex and fragment shaders are more than enough.</p><p>The constructor of the class takes a filename of the shader(s) we are going to be loading. There is also a <code class="literal">Bind()</code> method, which will be used to enable a specific shader program before rendering begins.</p><p>We also have two static helper methods, used for printing out errors inside shaders, and actually building them. Yes, shaders need to be compiled and linked before they can be used, much like C/C++.</p><p>Finally, we need two <em>GL unsigned integer</em> data members, the latter of which is an array. The first integer is going to represent the shader program, which contains all attached shaders. The array of integers keeps track of identifiers of all types of shaders that are in the program.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec10"/>Implementing the shader class</h3></div></div></div><p>Let us get down to actually creating some shaders! As always, a good place to start is the constructor:</p><pre class="programlisting">GL_Shader::GL_Shader(const std::string&amp; l_fileName) { 
  auto src_vert = Utils::ReadFile(l_fileName + ".vert"); 
  auto src_frag = Utils::ReadFile(l_fileName + ".frag"); 
  if (src_vert.empty() &amp;&amp; src_frag.empty()) { return; } 
 
  m_program = glCreateProgram(); // Create a new program. 
  m_shader[static_cast&lt;GLuint&gt;(ShaderType::Vertex)] = 
    BuildShader(src_vert, GL_VERTEX_SHADER); 
  m_shader[static_cast&lt;GLuint&gt;(ShaderType::Fragment)] = 
    BuildShader(src_frag, GL_FRAGMENT_SHADER); 
 
  for (GLuint i = 0; 
    i &lt; static_cast&lt;GLuint&gt;(ShaderType::COUNT); ++i) 
  { 
    glAttachShader(m_program, m_shader[i]); 
  } 
 
  glBindAttribLocation(m_program, 
    static_cast&lt;GLuint&gt;(VertexAttribute::Position), "position"); 
 
  glLinkProgram(m_program); 
  CheckError(m_program,GL_LINK_STATUS,<strong>true</strong>,"Shader link error:"); 
  glValidateProgram(m_program); 
  CheckError(m_program,GL_VALIDATE_STATUS,<strong>true</strong>,"Invalid shader:"); 
} 
</pre><p>Before any shader compilation is done, we first need to have the actual source code loaded in memory. OpenGL does not do this for you, so we are going to be utilizing the <code class="literal">ReadFile</code> function implemented earlier. Once both types of shaders are loaded and checked for not being empty, a new shader program is created using <code class="literal">glCreateProgram()</code>. It returns an identifier that we need to keep track of if we want to use the shaders when rendering.</p><p>For the actual vertex and fragment shaders the static <code class="literal">BuildShader()</code> method is invoked, and the returned identifier is stored inside the <code class="literal">m_shader</code> array for the relevant type of shader. Note the <code class="literal">GL_VERTEX_SHADER</code> and <code class="literal">GL_FRAGMENT_SHADER</code> definitions being passed to the method call. These are the shader types OpenGL needs in order to build the shaders.</p><p>After the shaders have been built, they need to be attached to our created program. For this we can simply use a loop and invoke <code class="literal">glAttachShader</code>, which takes the ID of the program, as well as an ID of the shader to be attached to said program.</p><p>Shaders need to have some sort of input as they are being executed. Remember that our model rendering begins with a binding to a VAO, which holds the information about how certain attributes of a VBO should be accessed, followed by a draw call. In order for the data feeding to work properly, our shader class needs to bind a name and attribute location. This can be done by calling <code class="literal">glBindAttribLocation</code>, and passing in the ID of the program, the actual attribute location, which is enumerated as <code class="literal">VertexAttribute</code>, and the name of the attribute variable that's going to be used inside the shader program. This step ensures that the data being fed into the vertex shader will be accessible through a <em>position</em> variable. This will be covered more in the <em>Writing basic shaders</em> section.</p><p>After the shaders are built and have their attributes bound, all we have left is linking and validation, the latter of which determines if the shader executable can run given the current OpenGL state. Both <code class="literal">glLinkProgram</code> and <code class="literal">glValidateProgram</code> simply take the ID of the program. After each of these function calls, we also invoke the other static helper method, <code class="literal">CheckError</code>. It is responsible for actually fetching a string of information, pertaining to any sort of errors during the linking and compilation stages. This method takes in the program ID, a flag that is used to determine what stage of the shader building process we are actually interested in, a <em>Boolean</em> value that signifies whether the whole shader program is being checked or if it is just an individual shader, and a string to be split out into the console window before the actual error is printed.</p><p>Shaders, just like any resource, need to be cleaned up once we are done with them:</p><pre class="programlisting">GL_Shader::~GL_Shader() { 
  for (GLuint i = 0; 
    i &lt; static_cast&lt;GLuint&gt;(ShaderType::COUNT); ++i) 
  { 
    glDetachShader(m_program, m_shader[i]); 
    glDeleteShader(m_shader[i]); 
  } 
  glDeleteProgram(m_program); 
} 
</pre><p>Thanks to the <code class="literal">ShaderType</code> enumeration, we know exactly how many shader types we support, and so we are able to simply run a loop for each one during cleanup. For each shader type, we must first detach it from the shader program using <code class="literal">glDetachShader</code>, which takes the program ID and the shader ID, and then deletes it using <code class="literal">glDeleteShader</code>. Once all the shaders are removed, the program itself is deleted through the <code class="literal">glDeleteProgram()</code> function call.</p><p>As discussed previously, OpenGL operates using function calls and a shared state. This means that certain resources such as shaders, for example, must be bound to before being used for rendering:</p><pre class="programlisting">void GL_Shader::Bind() const { glUseProgram(m_program); } 
</pre><p>In order to use a shader for a specific set of primitives to be drawn, we simply need to call <code class="literal">glUseProgram</code> and pass in the ID of the shader program.</p><p>Let us take a look at one of our helper methods, used to determine if there were any errors during the various stages of the shader program setup:</p><pre class="programlisting">void GL_Shader::CheckError(GLuint l_shader, GLuint l_flag, 
  bool l_program, const std::string&amp; l_errorMsg) 
{ 
  GLint success = 0; 
  GLchar error[1024] = { 0 }; 
  if (l_program) { glGetProgramiv(l_shader, l_flag, &amp;success); } 
  else { glGetShaderiv(l_shader, l_flag, &amp;success); } 
 
  if (success) { return; } 
  if (l_program) { 
    glGetProgramInfoLog(l_shader, sizeof(error), nullptr, error); 
  } else { 
    glGetShaderInfoLog(l_shader, sizeof(error), nullptr, error); 
  } 
  std::cout &lt;&lt; l_errorMsg &lt;&lt; error &lt;&lt; std::endl; 
} 
</pre><p>First, a few local variables are set up for storing the state information: a success flag, and a buffer for an error message to be put in. If the <code class="literal">l_program</code> flag is true, it means we are trying to fetch information about the actual shader program. Otherwise, we are only interested in an individual shader. To obtain the parameter that signifies success or failure of link/validation/compilation stages of a shader/program, we need to use <code class="literal">glGetProgramiv</code> or <code class="literal">glGetShaderiv</code>. Both of them take an ID to a shader or program being checked, a flag of the parameter we are interested in, and a pointer to the return value that is to be overwritten with either <code class="literal">GL_TRUE</code> or <code class="literal">GL_FALSE</code> in this case.</p><p>If whichever stage of the shader building process we are interested in finished successfully, we simply return from the method. Otherwise, we invoke either  <code class="literal">glGetProgramInfoLog()</code> or <code class="literal">glGetShaderInfoLog()</code> to fetch the error information of the program or individual shader. Both of these functions take the identifier of either the program or shader being checked, the size of the error message buffer we have allocated, a pointer to a variable that would be used to store the length of the string returned, which we do not really need so <code class="literal">nullptr</code> is passed in, and a pointer to the error message buffer that is to be written to. Afterwards, it is as simple as printing out our <code class="literal">l_errorMsg</code> prefix, followed by the actual error written to the <code class="literal">error</code> message buffer.</p><p>Last, but definitely not least, let us see what it takes to build an individual shader:</p><pre class="programlisting">GLuint GL_Shader::BuildShader(const std::string&amp; l_src, 
  unsigned int l_type) 
{ 
  GLuint shaderID = glCreateShader(l_type); 
  if (!shaderID) { 
    std::cout &lt;&lt; "Bad shader type!" &lt;&lt; std::endl; return 0; 
  } 
  const GLchar* sources[1]; 
  GLint lengths[1]; 
  sources[0] = l_src.c_str(); 
  lengths[0] = l_src.length(); 
  glShaderSource(shaderID, 1, sources, lengths); 
  glCompileShader(shaderID); 
  CheckError(shaderID, GL_COMPILE_STATUS, false, 
    "Shader compile error: "); 
  return shaderID; 
} 
</pre><p>First, an individual shader has to be created using the <code class="literal">glCreateShader()</code> method. It takes in a shader type, such as <code class="literal">GL_VERTEX_SHADER</code>, which we used in the constructor of this class. If, for some reason, the shader creation failed, an error message is written to the console window and the method returns a <code class="literal">0</code>. Otherwise, two arrays of GL types are set up: one for the sources of potentially multiple shaders, and one for the lengths of each source string. For now we're only going to be dealing with one source per shader, but it is possible to handle multiple sources later on, should we ever want to.</p><p>After the source code and its length have been written to the arrays we just set up, <code class="literal">glShaderSource</code> is used to submit the code to a buffer before it gets compiled. The function takes in the ID of the newly created shader, the number of source strings we're passing in, a pointer to the source array, as well as a pointer to the source length array. The shader is then actually compiled using <code class="literal">glCompileShader</code>, and the <code class="literal">CheckError</code> helper method is invoked to print out any possible compilation errors. Note the <code class="literal">GL_COMPILE_STATUS</code> flag being passed in, as well as the false flag, showing that we're interested in checking the status of an individual shader, rather than the whole shader program.</p></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec62"/>Writing basic shaders</h2></div></div></div><p>As our <code class="literal">GL_Shader</code> class is done, we can finally get to write some basic shaders for our application! Let us get started by taking a look at a file named <code class="literal">basic.vert</code>, which is our vertex shader:</p><pre class="programlisting">#version 450
attribute vec3 position; 
 
void main(){ 
   gl_Position = vec4(position, 1.0); 
} 
</pre><p>First, we set up the <code class="literal">attribute</code> of the shader that is going to be written to by OpenGL. It is an attribute of type <code class="literal">vec3</code>, and is going to represent our <strong>vertex position</strong> information that gets fed into this shader one by one. This type was set up inside the <code class="literal">GL_Model</code> class constructor using the <code class="literal">glVertexAttribPointer</code>, and then named in the <code class="literal">GL_Shader</code> class constructor, using the <code class="literal">glBindVertexAttribLocation</code> function.</p><p>The body of the shader has to have a main function, where all of the magic happens. In this case, all we need to do is set the internal OpenGL variable <code class="literal">gl_Position</code> to the position we want our vertex to have. It requires a <code class="literal">vec4</code> type, so the position attribute is converted to it, with the last vector value, which is used for clipping purposes, being set to <code class="literal">1.0</code>. For now, we do not need to worry about this. Just keep in mind that the actual vertex position in normalized device coordinates (coordinates in a range of <em>(-1,-1)</em> and <em>(1,1)</em>) are set here.</p><div><div><h3 class="title"><a id="tip19"/>Tip</h3><p>Note the version number on the very first line. If your computer does not support OpenGL 4.5, this can be changed to anything else, especially because we are not doing anything that older versions do not support.</p></div></div><p>After the vertex information is processed, we also need to worry about shading the individual pixels that make up our geometry correctly. This is where the fragment shader comes in:</p><pre class="programlisting">#version 450 
 
void main(){ 
   gl_FragColor = vec4(1.0, 1.0, 1.0, 1.0); // White. 
} 
</pre><p>This shader also uses an internal OpenGL variable. This time it is named <code class="literal">gl_FragColor</code>, and, predictably enough, is used for setting the color of the pixel we are processing. For now, let us just shade all of the pixels of our geometry white.</p></div></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec60"/>Drawing our first triangle</h1></div></div></div><p>We have our model class that handles all of the geometry data, as well as the shader class, which deals with processing our data at various points of the programmable rendering pipeline. With that out of the way, all we have left to do is actually set up and use these classes. Let us start by adding them as data members to the <code class="literal">Game</code> object:</p><pre class="programlisting">class Game{ 
  ... 
private: 
  ... 
  std::unique_ptr&lt;GL_Shader&gt; m_shader; 
  std::unique_ptr&lt;GL_Model&gt; m_model; 
  ... 
}; 
</pre><p>They can then be set up in the constructor of our <code class="literal">Game</code> class:</p><pre class="programlisting">Game::Game() ... { 
  ... 
  m_shader = std::make_unique&lt;GL_Shader&gt;( 
    Utils::GetWorkingDirectory() + "GL/basic"); 
 
  GL_Vertex vertices[] = { 
    //        |-----POSITION----| 
    //            X     Y    Z 
    GL_Vertex({ -0.5, -0.5, 0.5 }, // 0 
    GL_Vertex({ -0.5, 0.5, 0.5 }, // 1 
    GL_Vertex({ 0.5, 0.5, 0.5 }, // 2 
  }; 
 
  m_model = std::make_unique&lt;GL_Model&gt;(vertices, 3); 
} 
</pre><p>First, the shader class is created and a path with a filename is passed to it, so that the <code class="literal">basic.vert</code> and <code class="literal">basic.frag</code> shaders inside the <code class="literal">GL</code> directory of our executable directory can be loaded. An array of vertices is then set up, with each one being initialized to a particular position in normalized device coordinates. This particular arrangement creates three vertices in the middle of the screen, which will be connected into a triangle. The coordinates here fall within the range of what is known as <strong>normalized device coordinates</strong>. It is the coordinate system that the window uses, as shown here:</p><div><img src="img/image_07_003.jpg" alt="Drawing our first triangle"/></div><p>A <code class="literal">GL_Model</code> object is then created, with the vertex array and vertex count being passed in as arguments. The <code class="literal">GL_Model</code> then goes on to push this data to the GPU, as discussed previously.</p><p>Lastly, let us take a look at how we can render our triangle on screen:</p><pre class="programlisting">void Game::Render() { 
  m_window.BeginDraw(); 
  // Render here. 
  m_shader-&gt;Bind(); 
  m_model-&gt;Draw(); 
  // Finished rendering. 
  m_window.EndDraw(); 
} 
</pre><p>After the window is cleared inside the <code class="literal">BeginDraw()</code> method, the shader program is bound to, so that the vertex and fragment shaders we wrote earlier are used when the vertex data of our <code class="literal">GL_Model</code> is being pushed through the rendering pipeline. The models <code class="literal">Draw()</code> method is then invoked, to begin the rendering process. After successful program compilation and execution, this is what we should see on screen:</p><div><img src="img/image_07_004.jpg" alt="Drawing our first triangle"/></div><p>Hooray! After about 20 pages of theory, we have a triangle. This may be a little bit discouraging, but keep in mind that everything from this point on is going to get much, much easier. Congratulations on making it this far!</p></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec61"/>Using textures</h1></div></div></div><p>A basic, white triangle is not very exciting to look at. The next obvious improvement to make to our code is making textures available to the fragment shader, so that they can be sampled and applied to our geometry. Unfortunately, OpenGL does not provide a way of actually loading image data, especially since there are so many different formats to keep up with. For that, we are going to use one of our resources listed at the beginning of this chapter, the STB image loader. It is a small, single header C library, used to load image data into a buffer that can later be used by OpenGL, or any other library for that matter.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec63"/>The texture class</h2></div></div></div><p>Remember the remark that everything is going to get much easier at this point? It is true. Let us breeze through the texturing process, starting with a class definition for a texture object:</p><pre class="programlisting">class GL_Texture { 
public: 
  GL_Texture(const std::string&amp; l_fileName); 
  ~GL_Texture(); 
 
  void Bind(unsigned int l_unit); 
private: 
  GLuint m_texture; 
}; 
</pre><p>Although OpenGL does not actually handle loading texture data, it is still going to be handled within the confines of this class. Because of that, the constructor of our texture class is still going to take a path to the texture file to be loaded. Also, much like the shader class, we are going to need to bind to a specific texture before it can be used when rendering geometry. For now, ignore the argument it takes. It will be explained down the line.</p><p>The OpenGL textures, just like shaders or geometry data, have to be stored on the GPU. Because of that, it stands to reason that texture data will be referred to by a <code class="literal">GLuint</code> identifier, just like shaders or buffers.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec11"/>Implementing the texture class</h3></div></div></div><p>Let us take a look at what needs to be done in order to successfully load textures from the hard disk, and push them into the GPU:</p><pre class="programlisting">GL_Texture::GL_Texture(const std::string&amp; l_fileName) { 
  int width, height, nComponents; 
  unsigned char* imageData = stbi_load(l_fileName.c_str(), 
    &amp;width, &amp;height, &amp;nComponents, 4); 
  if (!imageData) { return; } 
 
  glGenTextures(1, &amp;m_texture); 
  glBindTexture(GL_TEXTURE_2D, m_texture); 
 
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT); 
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT); 
  glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER,GL_LINEAR); 
  glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER,GL_LINEAR); 
 
  glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, width, height, 0, 
    GL_RGBA, GL_UNSIGNED_BYTE, imageData); 
 
  stbi_image_free(imageData); 
} 
</pre><p>First, a few integers are created in order to be filled in with information about the texture that is going to be loaded. We then invoke the <code class="literal">stbi_load()</code> function, which is part of the STB image loading library, passing in a path to the texture file, pointers to the width, height, and the component count variables that are about to be written to, as well as the number of components the file is expected to have. The data is stored in the form of an <em>unsigned char</em>, a pointer to which is returned by the <code class="literal">stbi_load()</code> function. If <code class="literal">nullptr</code> was returned, we obviously need to return, as the loading process failed.</p><p>The number of components an image has is simply the number of color channels. Passing in a value of 0 would mean the image data is loaded as is, while any other value <em>forces</em> the data to contain other color channel information. The component number to channel configuration can be evaluated like so:</p><div><table border="1"><colgroup><col/><col/></colgroup><tbody><tr><td>
<p><strong>Components</strong></p>
</td><td>
<p><strong>Channels</strong></p>
</td></tr><tr><td>
<p>1</p>
</td><td>
<p>Gray</p>
</td></tr><tr><td>
<p>2</p>
</td><td>
<p>Gray, alpha</p>
</td></tr><tr><td>
<p>3</p>
</td><td>
<p>Red, green, blue</p>
</td></tr><tr><td>
<p>4</p>
</td><td>
<p>Red, green, blue, alpha</p>
</td></tr></tbody></table></div><p>From this point on, we follow what should be a familiar pattern by now. First, a texture object is generated using <code class="literal">glGenTextures</code>, to which the number of textures we want is passed as the first argument, and the pointer to the texture identifier or a list of them as the second argument. We then bind to the newly created texture using <code class="literal">glBindTexture</code>. The first argument of this function simply lets OpenGL know what kind of texture we are dealing with. In this case, <code class="literal">GL_TEXTURE_2D</code> is used, because it is a basic 2D image.</p><div><div><h3 class="title"><a id="tip20"/>Tip</h3><p>OpenGL supports a myriad of different types of textures for various tasks, including 3D textures, cube maps, and so on.</p></div></div><p>Once a texture is bound to, we can manipulate various details it comes with. For textures, the parameter manipulation function is named <code class="literal">glTexParameter()</code>. There are many different types of this single function, all with different suffixes that give a hint to the programmer of what data type it is expecting. For our purposes, we are going to be using two types: <em>integer</em> and <em>float</em>, appropriately ended by letters <em>i</em> and <em>f</em>.</p><p>The first two lines deal with defining behavior for cases when texture data is being read outside of the boundaries of its size, that is, how the texture is wrapped. The <code class="literal">GL_TEXTURE_WRAP_S</code> parameter deals with wrapping on the <em>X</em> axis, while the <code class="literal">GL_TEXTURE_WRAP_T</code> parameter deals with the <em>Y</em> axis. Why <em>S</em> and <em>T</em> you may ask? The answer to that is simple. Positional vectors, color data, and texture coordinates are enumerated differently, but they both mean roughly the same thing. Consider the following table:</p><div><table border="1"><colgroup><col/><col/><col/><col/><col/></colgroup><tbody><tr><td>
</td><td>
<p>1</p>
</td><td>
<p>2</p>
</td><td>
<p>3</p>
</td><td>
<p>4</p>
</td></tr><tr><td>
<p>Position</p>
</td><td>
<p>X</p>
</td><td>
<p>Y</p>
</td><td>
<p>Z</p>
</td><td>
<p>W</p>
</td></tr><tr><td>
<p>Color</p>
</td><td>
<p>R</p>
</td><td>
<p>G</p>
</td><td>
<p>B</p>
</td><td>
<p>A</p>
</td></tr><tr><td>
<p>Textures</p>
</td><td>
<p>S</p>
</td><td>
<p>T</p>
</td><td>
<p>P</p>
</td><td>
<p>Q</p>
</td></tr></tbody></table></div><p>They are all vectors of four values. Accessing the position <em>X</em> value is the same as accessing the red channel of a color structure, and so on.</p><p>The next two function calls deal with how the texture is interpolated when being sized down or up. Both cases specify the <code class="literal">GL_LINEAR</code> parameter, which means the pixels will be linearly interpolated.</p><p>Finally, we actually submit the loaded pixel information to the GPU by invoking the <code class="literal">glTexImage2D()</code> method. Its first argument, once again, lets OpenGL know what type of texture we are submitting. The second argument is the texture's level of detail, which will be used for mip-mapping. The value <code class="literal">0</code> simply means it is the base level texture.</p><div><div><h3 class="title"><a id="note21"/>Note</h3><p>Mip-mapping is an optional technique that can be utilized by OpenGL, in which multiple versions of the same texture, but of different resolutions, are loaded and submitted to the GPU, and later applied to geometry depending on how far it is from the viewer. If it is further away, a lower resolution texture (with a higher mip-mapping level) is used. This can be done for performance reasons, when necessary.</p></div></div><p>The third argument lets OpenGL know what arrangement the pixel information data is in. This is necessary, because certain formats may store it in varying configurations. The width and height information is passed in next, along with a number of pixels that can be used to add a border to the texture. We are not going to be using that feature, which is why <code class="literal">0</code> is passed in. The next argument is, once again, a flag for a certain arrangement of pixels. This time it lets OpenGL know which arrangement we want it to store the pixel data in. Finally, a flag for the type that our loaded texture is in is passed, along with a pointer to the actual texture data. We are using the <code class="literal">GL_UNSIGNED_BYTE</code> parameter, because that is what the STB image loader returns, and the <em>char</em> type is exactly one byte long.</p><p>After the texture information is submitted to the GPU, we no longer need to keep the image data buffer around. It's destroyed by calling <code class="literal">stbi_image_free</code>, and passing in the pointer to the buffer.</p><p>The data we submitted to the GPU needs to be released once we no longer need the texture:</p><pre class="programlisting">GL_Texture::~GL_Texture() { glDeleteTextures(1, &amp;m_texture); } 
</pre><p>The <code class="literal">glDeleteTextures</code> function takes the number of textures we want to dispose of, as well as a pointer to an array of <em>GLuint</em> identifiers.</p><p>Finally, let's implement the <code class="literal">Bind()</code> method, which is going to give us the ability to use the texture when rendering:</p><pre class="programlisting">void GL_Texture::Bind(unsigned int l_unit) { 
  assert(l_unit &gt;= 0 &amp;&amp; l_unit &lt; 32); 
  glActiveTexture(GL_TEXTURE0 + l_unit); 
  glBindTexture(GL_TEXTURE_2D, m_texture); 
} 
</pre><p>OpenGL actually supports the ability for multiple textures to be bound all at once while rendering, so that complex geometry can be textured more efficiently. The exact number, at least at the time of writing, is <code class="literal">32</code> units. Most of the time we are not going to need that many, but it is nice to have the option. The identifier of the unit we want to use is passed in as an argument to the <code class="literal">Bind()</code> method. In order to avoid confusion, we are going to perform an <code class="literal">assert()</code> method and make sure that the <code class="literal">l_unit</code> value is in the right range first.</p><p>In order to enable a specific unit for a texture, the <code class="literal">glActiveTexture()</code> method needs to be called. It takes a single argument, which is the enumerated texture unit. It ranges from <code class="literal">GL_TEXTURE0</code> all the way to <code class="literal">GL_TEXTURE31</code>. Because those values are sequential, a neat trick is to simply add the <code class="literal">l_unit</code> to the <code class="literal">GL_TEXTURE0</code> definition, which will give us the right unit enumeration. After that, we simply bind to the texture as before, using the <code class="literal">glBindTexture()</code> method and passing in the type of texture we have, along with its identifier.</p></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec64"/>Model and shader class changes</h2></div></div></div><p>To add support for textured geometry, we first need to make some changes to the vertex information that gets stored. Let us take a look at the <code class="literal">GL_Vertex</code> structure to see what needs to be added:</p><pre class="programlisting">enum class VertexAttribute{ Position, TexCoord, COUNT }; 
 
struct GL_Vertex { 
  GL_Vertex(const glm::vec3&amp; l_pos, 
    <strong>const glm::vec2&amp; l_texCoord</strong>) 
    : m_pos(l_pos), <strong>m_texCoord(l_texCoord</strong>) {} 
 
  glm::vec3 m_pos; // Attribute 1. 
  <strong>glm::vec2 m_texCoord;</strong> // Attribute 2. 
  // ... 
}; 
</pre><p>As you can see, we need an additional vertex attribute, which is the coordinate of the texture a vertex is associated with. It is a simple two-dimensional vector that represents the texture coordinates, as shown here:</p><div><img src="img/image_07_005.jpg" alt="Model and shader class changes"/></div><p>The great thing about representing texture coordinates in this fashion is the fact that it makes the coordinates resolution-independent. A point <em>(0.5,0.5)</em> on a smaller texture is going to be the exact same point on its larger counterpart.</p><p>Because we now have more information about a single vertex that needs to be stored and accessed, the VAO needs to know exactly how to do so:</p><pre class="programlisting">GL_Model::GL_Model(GL_Vertex* l_vertices,unsigned int l_vertCount) 
{ 
  ... 
  <strong>auto stride = sizeof(l_vertices[0]);</strong>
<strong>  auto texCoordOffset = sizeof(l_vertices[0].m_pos);</strong> 
 
  glBindBuffer(GL_ARRAY_BUFFER, m_vertexVBO); 
  glBufferData(GL_ARRAY_BUFFER, 
    l_vertCount * sizeof(l_vertices[0]), 
    l_vertices, GL_STATIC_DRAW); 
  glEnableVertexAttribArray( 
    static_cast&lt;GLuint&gt;(VertexAttribute::Position)); 
  glVertexAttribPointer( 
    static_cast&lt;GLuint&gt;(VertexAttribute::Position), 3, GL_FLOAT, 
    GL_FALSE, <strong>stride</strong>, <strong>0</strong>); 
  glEnableVertexAttribArray( 
    static_cast&lt;GLuint&gt;(VertexAttribute::TexCoord)); 
  glVertexAttribPointer( 
    static_cast&lt;GLuint&gt;(VertexAttribute::TexCoord), 2, GL_FLOAT, 
    GL_FALSE, <strong>stride</strong>, <strong>(void*)texCoordOffset</strong>); 
  ... 
} 
</pre><p>We now get to utilize the stride and offset parameters that were discussed previously! The stride is, of course, the full size of a <code class="literal">GL_Vertex</code> structure, while the offset to obtain texture coordinates is the size of the vertex position vector, because that is the amount by which the pointer needs to be offset.</p><p>After the data is submitted to the buffer, we enable the vertex position attribute and provide its pointer with the <code class="literal">stride</code>. The offset remains <code class="literal">0</code>, because <code class="literal">Position</code> is the first attribute.</p><p>We also need to enable the <code class="literal">TexCoord</code> attribute, because it will be passed to the shaders as well. Its pointer is set up similarly to that of position, except we have <code class="literal">2</code> floats instead of <code class="literal">3</code>, and the offset now needs to be applied, so that the position data is skipped.</p><div><div><h3 class="title"><a id="note22"/>Note</h3><p>Note the <code class="literal">void*</code> cast for the last argument. This is because the offset actually takes a pointer, rather than a number of bytes. It is one of the leftover <em>legacy</em> details, and only means the number of bytes in newer versions.</p></div></div><p>The final change to our C++ code pertains to updating the <code class="literal">GL_Shader</code> class, in order to register the new attribute that is going to be passed in to the vertex shader:</p><pre class="programlisting">GL_Shader::GL_Shader(const std::string&amp; l_fileName) { 
  ... 
  glBindAttribLocation(m_program, 
    static_cast&lt;GLuint&gt;(VertexAttribute::Position), "position"); 
  <strong>glBindAttribLocation(m_program, 
    static_cast&lt;GLuint&gt;(VertexAttribute::TexCoord), 
    "texCoordVert");</strong> 
  ... 
} 
</pre><p>It simply establishes a name for our texture coordinate attribute, which is now <code class="literal">"texCoordVert"</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec65"/>Updating the shaders</h2></div></div></div><p>The actual sampling of the texture takes place inside the fragment shader. However, as the data is actually received in the vertex shader first, let us see how it needs to be updated to cater to our needs:</p><pre class="programlisting">#version 450 
 
attribute vec3 position; 
<strong>attribute vec2 texCoordVert;</strong>
<strong>varying vec2 texCoord; // Pass to fragment shader.</strong> 
void main(){ 
  gl_Position = vec4(position, 1.0); 
  <strong>texCoord = texCoordVert; // Pass to fragment shader. </strong>
} 
</pre><p>As you can see, the <code class="literal">texCoordVert</code> attribute is established here, along with a <code class="literal">varying</code> 2D vector named <code class="literal">texCoord</code>. A varying type simply means that its data is going to be passed down the rendering pipeline and received by the next shader in line. In our case, <code class="literal">texCoord</code> is going to be accessible inside the fragment shader. Its value is set to the input attribute of <code class="literal">texCoordVert</code>. Why? Because varying data received by any shader down the line is <strong>interpolated</strong>. That's right. Take a look at the following diagram:</p><div><img src="img/image_07_006.jpg" alt="Updating the shaders"/></div><p>In order to accurately sample color information for each pixel of our geometry, we do not really need to do any math by ourselves. Interpolation, or weighted averaging, takes care of that for us. If one vertex has texture coordinates of, let's say <strong>(1,1)</strong>, and the opposite vertex has the coordinates <strong>(0,0)</strong>, the fragment shader executing on a pixel somewhere in between those vertices will receive the <strong>interpolated</strong> value of <strong>(0.5, 0.5)</strong>. This makes coloring a pixel as easy as this:</p><pre class="programlisting">#version 450 
<strong>uniform sampler2D texture; 
varying vec2 texCoord; // Receiving it from vertex shader.</strong> 
 
void main(){ 
  gl_FragColor = <strong>texture2D</strong>(texture, texCoord); 
} 
</pre><p>First, note the <code class="literal">uniform</code> variable of type <code class="literal">sampler2D</code>, called <code class="literal">texture</code>. We do not need to manually pass this into our shaders as it is done behind the scenes. It simply provides access to the data of the current texture that it is bound to. Next, we set up the varying variable <code class="literal">texCoord</code>, which completes the <em>piping</em> of data from the vertex shader to the fragment shader. The fragment color is then set to a <code class="literal">vec4</code>, which gets returned from the <code class="literal">texture2D()</code> function that takes in the texture received by the fragment shader, as well as the coordinates we want to sample. Since the <code class="literal">vec4</code> that gets returned represents the color of the pixel, that is all that it takes to texture geometry!</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec66"/>Using a texture</h2></div></div></div><p>Applying the texture to our geometry is quite simple at this point. First, the <code class="literal">GL_Texture</code> class needs to be added as a data member to the <code class="literal">Game</code> object. We can then proceed to set everything else up as follows:</p><pre class="programlisting">Game::Game() ... { 
  ... 
  GL_Vertex vertices[] = { 
    //           |---POSITION----| |TEXTURE| 
    //            X     Y    Z      X  Y 
    GL_Vertex({ -0.5, -0.5, 0.5 }, { 0, 0 }), // 0 
    GL_Vertex({ -0.5, 0.5, 0.5 }, { 0, 1 }), // 1 
    GL_Vertex({ 0.5, 0.5, 0.5 }, { 1, 1 }), // 2 
  }; 
  m_texture = std::make_unique&lt;GL_Texture&gt;( 
    Utils::GetWorkingDirectory() + "GL/brick.jpg"); 
  ... 
} 
 
void Game::Render() { 
  m_window.BeginDraw(); 
  // Render here. 
  <strong>m_texture-&gt;Bind(0);</strong> 
  m_shader-&gt;Bind(); 
  m_model-&gt;Draw(); 
  m_window.EndDraw(); 
} 
</pre><p>The <code class="literal">GL_Vertex</code> objects now take an additional argument, which represents the texture coordinates of the vertex. We also load the brick texture in the constructor, which is then bound to in the <code class="literal">Render()</code> method, right before the shader. When our model is rendered, it should look as follows:</p><div><img src="img/image_07_007.jpg" alt="Using a texture"/></div><p>We now have a motionless model with a texture applied. Still not very exciting, but we are getting there!</p></div></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec62"/>Applying transformations</h1></div></div></div><p>Moving, rotating, and otherwise manipulating vertex data may seem quite straight forward. One may even be tempted to simply update the vertex position information and simply resubmit that data back to the VBO. While things may have been done that way for a while in the past, there are much more efficient, albeit more math-intensive ways of performing this task. Displacing vertices is now done in the vertex shader by simply multiplying the vertex positions by something called a <strong>matrix</strong>.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec67"/>Matrix basics</h2></div></div></div><p>Matrices are extremely useful in graphics programming, because they can represent any kind of rotation, scale, or displacement manipulation that can be applied to a vector. There are many different types of matrices, but they are all just blocks of information that look similar to this:</p><div><img src="img/image_07_008.jpg" alt="Matrix basics"/></div><p>This particular matrix is a 4x4 identity matrix, but a variety of differently sized matrices exist, such as 3x3, 2x3, 3x2, and so on. There are rules when it comes to adding, subtracting, multiplying, or dividing them. We are not really going to get into that as it is beyond the scope of this chapter. The nice thing is that the <code class="literal">glm</code> library abstracts all of this away for us, so it is not absolutely necessary to know much about this for now. A thing to take away from this is that positional vectors can be transformed when added to or multiplied by matrices.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec68"/>The world space</h2></div></div></div><p>Up until this point, we have been working with vertex positions that are specified in the normalized device coordinate space. This means that each vertex coordinate is actually relative to the center of the screen. In order to properly deal with transformations; however, we want to treat our geometry as being relative to an origin point that falls within the <strong>model space</strong>, as shown here:</p><div><img src="img/image_07_009.jpg" alt="The world space"/></div><p>If a model has an origin, it can also have a global position within our world, where its origin is relative to some arbitrary point within the game world we have constructed. This global position, as well as some other attributes, such as the scale and rotation of the object, can be represented by a matrix. Applying these attributes to the vertex coordinates that are in model space, which is exactly what happens when they are multiplied by the <strong>model matrix</strong>, allows us to bring those coordinates into what is known as <strong>world space</strong>, as shown here:</p><div><img src="img/image_07_010.jpg" alt="The world space"/></div><p>This transformation simply means that the vertices are now relative to the world's origin, rather than the model origin, allowing us to accurately represent models in our own coordinate system before drawing them on screen.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec69"/>The transform class</h2></div></div></div><p>Before any transformations can be applied, they should be properly grouped together and represented by a single data structure. The <code class="literal">GL_Transform</code> class is going to do exactly that for us:</p><pre class="programlisting">
<strong>#include &lt;glm.hpp&gt; 
#include &lt;gtx/transform.hpp&gt;</strong> 
class GL_Transform { 
public: 
  GL_Transform(const glm::vec3&amp; l_pos = { 0.f, 0.f, 0.f }, 
    const glm::vec3&amp; l_rot = { 0.f, 0.f, 0.f }, 
    const glm::vec3&amp; l_scale = { 1.f, 1.f, 1.f }); 
 
  glm::vec3 GetPosition()const; 
  glm::vec3 GetRotation()const; 
  glm::vec3 GetScale()const; 
 
  void SetPosition(const glm::vec3&amp; l_pos); 
  void SetRotation(const glm::vec3&amp; l_rot); 
  void SetScale(const glm::vec3&amp; l_scale); 
 
  glm::mat4 GetModelMatrix(); 
private: 
  void RecalculateMatrix(); 
  glm::vec3 m_position; 
  glm::vec3 m_rotation; 
  glm::vec3 m_scale; 
}; 
</pre><p>First, note the included headers on top. These are necessary for the data types and transformation functions that are going to be used in this class. Outside of that, we have three vectors that are going to represent the model's position, rotation, and scale, which are going to be used for calculating the model matrix that transforms vertices into world coordinates.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec12"/>Implementing the transform class</h3></div></div></div><p>The constructor simply takes in the appropriate arguments and sets up some data members using the initializer list:</p><pre class="programlisting">GL_Transform::GL_Transform(const glm::vec3&amp; l_pos, 
  const glm::vec3&amp; l_rot, const glm::vec3&amp; l_scale) 
  : m_position(l_pos), m_rotation(l_rot), m_scale(l_scale) 
{} 
</pre><p>The meat of this class is the <code class="literal">GetModelMatrix()</code> method, as it deals with all the necessary math:</p><pre class="programlisting">glm::mat4 GL_Transform::GetModelMatrix() { 
  glm::mat4 matrix_pos = glm::translate(m_position); 
  glm::mat4 matrix_scale = glm::scale(m_scale); 
  // Represent each stored rotation as a different matrix, because 
  // we store angles. 
  //          x  y  z 
  glm::mat4 matrix_rotX = glm::rotate(m_rotation.x, 
    glm::vec3(1, 0, 0)); 
  glm::mat4 matrix_rotY = glm::rotate(m_rotation.y, 
    glm::vec3(0, 1, 0)); 
  glm::mat4 matrix_rotZ = glm::rotate(m_rotation.z, 
    glm::vec3(0, 0, 1)); 
  // Create a rotation matrix. 
  // Multiply in reverse order it needs to be applied. 
  glm::mat4 matrix_rotation = matrix_rotZ*matrix_rotY*matrix_rotX; 
  // Apply transforms in reverse order they need to be applied in. 
  return matrix_pos * matrix_rotation * matrix_scale; 
} 
</pre><p>The model matrix is going to be a result of many other matrices being multiplied together, thus making it contain all of the necessary transformation information. We begin by creating what is known as a <strong>translation matrix</strong>. Calling <code class="literal">glm::translate</code> creates one for us, with the position information of <code class="literal">m_position</code>. It is used to bring the positions of our vertices into world space.</p><p>We then create a <strong>scale matrix</strong>, which is responsible for representing scaling or shrinking of a model. For example, if a model should be drawn as twice as big as it's stored on the GPU, the scale matrix will be used to adjust the positions of all vertices to make it look that way. Using <code class="literal">glm::scale</code> and passing in the scale vector as the argument will construct one for us.</p><p>The final type of matrix we need is the <strong>rotation matrix</strong>. It obviously represents different rotation values of an object, thus displacing all the vertices around an origin point. This one, however, is not quite so straightforward due to the fact that we are storing rotation information as a vector of <strong>degrees</strong>. Because of that, matrices of each axis need to be created using the <code class="literal">glm::rotate</code> function, which takes the degree of rotation, as well as a <strong>directional vector</strong>, representing the axis around which the rotation is desired. It simply means setting a value of <code class="literal">1</code> for the <em>x</em>, <em>y</em>, or <em>z</em> component, depending on which axis we are dealing with. The final rotational matrix is then calculated by multiplying all three previous matrices together. Using a different multiplication order will produce different results. Generally, a rule of thumb is to multiply all matrices in reverse order of application.</p><p>Finally, we can calculate the model matrix by multiplying all previous matrices together like so:</p><div><img src="img/image_07_011.jpg" alt="Implementing the transform class"/></div><p>The resulting model matrix is then returned.</p><p>The rest of this class is fairly straightforward, as there is nothing else except setters and getters left:</p><pre class="programlisting">glm::vec3 GL_Transform::GetPosition() const { return m_position; } 
glm::vec3 GL_Transform::GetRotation() const { return m_rotation; } 
glm::vec3 GL_Transform::GetScale() const { return m_scale; } 
 
void GL_Transform::SetPosition(const glm::vec3&amp; l_pos) 
{ m_position = l_pos; } 
void GL_Transform::SetRotation(const glm::vec3&amp; l_rot) 
{ m_rotation = l_rot; } 
void GL_Transform::SetScale(const glm::vec3&amp; l_scale) 
{ m_scale = l_scale; } 
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec70"/>Updating the shader class</h2></div></div></div><p>Once again, we are going to be using the shader class to submit the necessary matrix information to the vertex shader, where it will be used. The reason this is done inside the vertex shader is because the GPU is optimized for operations like this. Let us take a look at what we need to change:</p><pre class="programlisting">
<strong>enum class UniformType{ Transform, COUNT };</strong> 
 
class GL_Shader { 
public: 
  ... 
  void Update(GL_Transform&amp; l_transform); 
  ... 
private: 
  ... 
  GLuint m_uniform[static_cast&lt;unsigned int&gt;(UniformType::COUNT)]; 
}; 
</pre><p>First, note a new enumeration that we have established. It enumerates all the uniform variable types that our shaders need, which, for now, consists of only one.</p><div><div><h3 class="title"><a id="note23"/>Note</h3><p>A uniform performs a different task from the usual shader attributes or varying variables. Attributes are <em>filled in</em> by OpenGL behind the scenes, using data from the VBO. Varying shader variables are passed between shaders. A uniform variable is actually passed into the shader by our C++ code, which is why we need to treat it differently.</p></div></div><p>The <code class="literal">GL_Shader</code> class now also needs an <code class="literal">Update()</code> method, which is going to take in a reference to the <code class="literal">GL_Transform</code> class and use it to pass the model matrix to the vertex shader. Lastly, we need to store identifiers that are used to locate uniform variables within shaders, so that they can be used. The <code class="literal">m_uniform </code>data member exists for that exact purpose.</p><p>Let's see how a uniform variable location can be obtained and stored:</p><pre class="programlisting">GL_Shader::GL_Shader(const std::string&amp; l_fileName) { 
  ... 
  m_uniform[static_cast&lt;unsigned int&gt;(UniformType::Transform)] = 
    glGetUniformLocation(m_program, "transform"); 
} 
</pre><p>As you can see, OpenGL provides a nice function for that, called <code class="literal">glGetUniformLocation</code>. It takes an identifier of the program we are using, as well as the name of the uniform variable inside the shader, which is <code class="literal">"transform"</code>.</p><p>Setting the value of a uniform variable also comes down to a single function call:</p><pre class="programlisting">void GL_Shader::Update(GL_Transform&amp; l_transform) { 
  glm::mat4 modelMatrix = l_transform.GetModelMatrix(); 
   
  glUniformMatrix4fv(static_cast&lt;GLint&gt;( 
    m_uniform[static_cast&lt;unsigned int&gt;(UniformType::Transform)]), 
    1, GL_FALSE, &amp;modelMatrix[0][0]); 
} 
</pre><p>First, we obtain the model matrix from the transform class. The <code class="literal">glUniform</code> function is then called. It has a suffix of the exact data type we are submitting, which, in this case, is a 4x4 matrix of floats. The uniform ID we stored earlier is used as the first argument. The amount of data being submitted is the second argument, which in this case is only <code class="literal">1</code>, as one matrix is being submitted. The third argument is a flag that lets us transpose the matrix. We do not need to do that, so <code class="literal">GL_FALSE</code> is passed in. Finally, a pointer to the first element of the matrix is passed as the last argument. OpenGL knows exactly how big the matrix is, as we are calling the appropriate function, which allows it to read the entire matrix.</p><p>Lastly, we need to modify the vertex shader in order to actually perform the transformation:</p><pre class="programlisting">#version 450 
 
attribute vec3 position; 
attribute vec2 texCoordVert; 
varying vec2 texCoord; // Pass to fragment shader. 
 
<strong>uniform mat4 transform; // Passed in by the shader class.</strong> 
 
void main(){ 
 <strong> gl_Position = transform * vec4(position, 1.0);</strong> 
  texCoord = texCoordVert; // Pass to fragment shader. 
} 
</pre><p>Note the <code class="literal">uniform</code> of type <code class="literal">mat4</code> being added. We simply need to multiply it by the position in the main function, which gives us our transformed vertex position.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec71"/>Manipulating the triangle</h2></div></div></div><p>Once again, all we have left to do in order to apply the code we have written is add it to the <code class="literal">Game</code> class:</p><pre class="programlisting">class Game{ 
  ... 
private: 
  ... 
  GL_Transform m_transform; 
  ... 
}; 
</pre><p>It really does not need any more setting up than that. We can jump straight to manipulation of the transform's properties, by editing the <code class="literal">Update()</code> method:</p><pre class="programlisting">void Game::Update() { 
  ... 
  auto rotation = m_transform.GetRotation(); 
  rotation.x += 0.001f; 
  rotation.y += 0.0002f; 
  rotation.z += 0.002f; 
  if (rotation.x &gt;= 360.f) { rotation.x = 0.f; } 
  if (rotation.y &gt;= 360.f) { rotation.y = 0.f; } 
  if (rotation.z &gt;= 360.f) { rotation.z = 0.f; } 
  m_transform.SetRotation(rotation); 
  <strong>m_shader-&gt;Update(m_transform);</strong> 
} 
</pre><p>In this case, we are simply playing around with rotations along all axes. After making those modifications, it is important to pass the transform object to the <code class="literal">GL_ShaderUpdate()</code> method, so that the vertices can be properly transformed, giving us this resulting rotation:</p><div><img src="img/image_07_012.jpg" alt="Manipulating the triangle"/></div><p>Now we are getting somewhere! Still, we have no interaction with the scene. This whole time we are just sitting still while the geometry just spins around. At best, this is just a very elaborate screensaver. Let's actually implement something that will give us some <em>mobility</em>.</p></div></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec63"/>Creating a camera</h1></div></div></div><p>OpenGL, unlike SFML, does not offer any means of actually moving around the view or the camera. While this may seem odd at first, that is mainly because there is no camera or view to move around. Yes, you heard that right. No camera, no views, just vertex data, shaders, and raw math to the rescue. How? Let's take a look!</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec72"/>View projection essentials</h2></div></div></div><p>All of the rendering and programming trickery that lots of libraries abstract away is exactly that - tricks. When it comes to moving around the game world, there is no real <em>camera</em> that conveniently films the right sides of geometry to be rendered. The camera is just an illusion, used to abstract away concepts that are not intuitive. Moving around a game world involves nothing else except additional matrix math that is performed on the <strong>vertices themselves</strong>. The act of rotating the <em>camera</em> around the scene simply comes down to the exact opposite of that: rotating the scene around a point in space that is referred to as the camera. Once again, we are going to be transforming our vertices to be relative to yet another point of origin, and this time, it is the camera itself. Consider the following diagram:</p><div><img src="img/image_07_013.jpg" alt="View projection essentials"/></div><p>In order to implement the camera class and be able to <em>move around</em> the world, we need to know a few basics. First of all, we have to decide how wide the view angle of the camera should be. This affects how much we can actually see. The other important detail is correctly setting up the <strong>view frustum</strong>. Think of it as a pyramid shaped piece of geometry that defines the range of the camera's view. It determines how close certain things can be until they are no longer seen, as well as what's the maximum distance of an object from the camera until it's no longer rendered.</p><p>The aspect ratio of our window, as well as the field of view, near/far distances of the view frustum, and the position of the camera all add up to a total of two matrices we are going to calculate: the <strong>view matrix</strong> and <strong>projection matrix</strong>. The former deals with positioning vertices relative to the camera's position, while the latter adjusts and warps them, which depends on how close or far away they are from the view frustum, the field of view, and other attributes.</p><p>There are mainly two projection types we can work with: <strong>perspective</strong> and <strong>orthographic</strong>. The perspective projection offers a realistic result where objects can appear to be further away from the camera, while orthographic projection is more of a fixed depth feel, making objects look the same size regardless of their distances. We are going to be using the perspective projection for our purposes.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec73"/>The camera class</h2></div></div></div><p>With all of this information covered, we are finally ready for the smoke and mirrors that is the <code class="literal">GL_Camera</code> class. Let us see what it takes in order to manoeuvre around our world:</p><pre class="programlisting">class GL_Camera { 
public: 
  GL_Camera(const glm::vec3&amp; l_pos, float l_fieldOfView, 
    float l_aspectRatio, float l_frustumNear, float l_frustumFar); 
 
  glm::mat4 GetViewProjectionMatrix(); 
private: 
  void RecalculatePerspective(); 
 
  float m_fov; 
  float m_aspect; 
  float m_frustumNear; 
  float m_frustumFar; 
 
  glm::vec3 m_position; 
  glm::vec3 m_forwardDir; 
  glm::vec3 m_upDir; 
 
  glm::mat4 m_perspectiveMatrix; 
}; 
</pre><p>As you can see, we are storing all of the covered details, as well as a couple of new ones. Along with the field of view angle, the aspect ratio, and the near and far frustum values, we also need to keep around the position, a forward direction vector, and the up direction vector. The <code class="literal">m_forwardDir</code> is a normalized directional vector that represents which way the camera is looking. The <code class="literal">m_upDir</code> is also a normalized directional vector, but it simply stores the <em>up</em> direction. This will all start to make sense soon.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec13"/>Implementing the camera class</h3></div></div></div><p>Let us see what the constructor of this class looks like:</p><pre class="programlisting">GL_Camera::GL_Camera(const glm::vec3&amp; l_pos, float l_fieldOfView, 
  float l_aspectRatio, float l_frustumNear, float l_frustumFar) 
  : m_position(l_pos), m_fov(l_fieldOfView), 
  m_aspect(l_aspectRatio), m_frustumNear(l_frustumNear), 
  m_frustumFar(l_frustumFar) 
{ 
  RecalculatePerspective(); 
  m_forwardDir = glm::vec3(0.f, 0.f, 1.f); 
  m_upDir = glm::vec3(0.f, 1.f, 0.f); 
} 
</pre><p>Outside of initializing our data members, the constructor has three tasks. It recalculates the perspective matrix, which only needs to be done once unless the window is resized, and it sets up both the forward direction, and the up direction. The camera starts out looking towards the positive <em>Z</em> axis, which is literally <em>towards</em> the screen, if you imagine it in those terms. The <em>up</em> direction is the positive Y axis.</p><p>Calculating the perspective matrix is quite simple, thanks to the <code class="literal">glm</code> library:</p><pre class="programlisting">void GL_Camera::RecalculatePerspective() { 
  m_perspectiveMatrix = glm::perspective(m_fov, m_aspect, 
    m_frustumNear, m_frustumFar); 
} 
</pre><p>Our matrix is constructed by the <code class="literal">glm::perspective</code> function, which takes in the field of view, the aspect ratio, and both frustum distances.</p><p>Finally, we can obtain the <strong>view projection matrix</strong>, which is simply a combination of the view and projection matrix:</p><pre class="programlisting">glm::mat4 GL_Camera::GetViewProjectionMatrix() { 
  glm::mat4 viewMatrix = glm::lookAt(m_position, 
    m_position + m_forwardDir, m_upDir); 
  return m_perspectiveMatrix * viewMatrix; 
} 
</pre><p>We begin by calculating the view matrix, using the <code class="literal">glm::lookAt</code> function. It takes in the position of the camera, the point the camera is looking at, and the <em>up</em> direction. Afterwards, the multiplication of our perspective matrix and the view matrix results in obtaining the view projection matrix, which is returned for later use.</p></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec74"/>Updating the rest of the code</h2></div></div></div><p>Because our geometry needs to, once again, be transformed relative to yet another origin, we need to update the <code class="literal">GL_Shader</code> class:</p><pre class="programlisting">void GL_Shader::Update(GL_Transform&amp; l_transform, 
 <strong> GL_Camera&amp; l_camera</strong>) 
{ 
  glm::mat4 modelMatrix = l_transform.GetModelMatrix(); 
  <strong>glm::mat4 viewProjMatrix = l_camera.GetViewProjectionMatrix();</strong>
  
  <strong>glm::mat4 modelViewMatrix = viewProjMatrix * modelMatrix;</strong> 
  glUniformMatrix4fv(static_cast&lt;GLint&gt;( 
    m_uniform[static_cast&lt;unsigned int&gt;(UniformType::Transform)]), 
    1, GL_FALSE, <strong>&amp;modelViewMatrix[0][0]</strong>); 
} 
</pre><p>Because the vertex shader is already multiplying its position by a transform, we can simply change which matrix it uses inside the <code class="literal">Update()</code> method. After the model matrix is obtained, we also grab the view projection matrix and multiply the two together. The resulting <strong>model view matrix</strong> is then passed down to the vertex shader.</p><p>Finally, the camera needs to be created inside the <code class="literal">Game</code> class:</p><pre class="programlisting">class Game{ 
  ... 
private: 
  ... 
  std::unique_ptr&lt;GL_Camera&gt; m_camera; 
}; 
</pre><p>It also needs to be set up with the appropriate information:</p><pre class="programlisting">Game::Game() ... { 
  ... 
  float aspectRatio = 
    static_cast&lt;float&gt;(m_window.GetWindowSize().x) / 
    static_cast&lt;float&gt;(m_window.GetWindowSize().y); 
  float frustum_near = 1.f; 
  float frustum_far = 100.f; 
 
  m_camera = std::make_unique&lt;GL_Camera&gt;( 
    glm::vec3(0.f, 0.f, -5.f), 70.f, aspectRatio, 
    frustum_near, frustum_far); 
} 
</pre><p>We begin by calculating the window's aspect ratio, which is its width divided by its height. After the <code class="literal">frustum_near</code> and <code class="literal">frustum_far</code> values are set up, they get passed in to the camera's constructor, along with its initial position, the field of view angle, and the aspect ratio of the window.</p><p>Finally, we just need to update the shader class with the camera's information:</p><pre class="programlisting">void Game::Update() { 
  ... 
  m_shader-&gt;Update(m_transform, *m_camera); 
} 
</pre><p>Upon successful compilation and execution, we should see our triangle slightly further away from the camera, because its position was set to <code class="literal">-5.f</code>on the <em>Z</em> axis.</p></div></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec64"/>Moving the camera around</h1></div></div></div><p>Having a programmable camera is nice, but it still does not allow us to freely roam the scene. Let 's actually give our camera class the ability to be manipulated in real time, so that we can have the illusion of floating around the world:</p><pre class="programlisting">enum class GL_Direction{ Up, Down, Left, Right, Forward, Back }; 
 
class GL_Camera { 
public: 
  ... 
  void MoveBy(GL_Direction l_dir, float l_amount); 
  void OffsetLookBy(float l_speed, float l_x, float l_y); 
  ... 
}; 
</pre><p>As you can see, we are going to use two methods for that: one for moving the camera, and another for rotating it. We are also defining a helpful enumeration of all six possible directions.</p><p>Moving a position vector is fairly simple. Assume we have a scalar value that represents the speed of the camera. If we multiply it by a direction vector, we get a proportional position change based on which direction the vector was pointed at, like so:</p><div><img src="img/image_07_014.jpg" alt="Moving the camera around"/></div><p>With that in mind, let us implement the <code class="literal">MoveBy()</code> method:</p><pre class="programlisting">void GL_Camera::MoveBy(GL_Direction l_dir, float l_amount) { 
  if (l_dir == GL_Direction::Forward) { 
    m_position += m_forwardDir * l_amount; 
  } else if (l_dir == GL_Direction::Back) { 
    m_position -= m_forwardDir * l_amount; 
  } else if (l_dir == GL_Direction::Up) { 
    m_position += m_upDir * l_amount; 
  } else if (l_dir == GL_Direction::Down) { 
    m_position -= m_upDir * l_amount; 
  } ... 
} 
</pre><p>If we are moving the camera forwards or backwards, the <code class="literal">l_amount</code> scalar value is multiplied by the forward direction. Moving the camera up and down is equally as simple, since the up direction can be used for that.</p><p>Moving left or right is slightly more complex. We cannot just statically change the position, because the camera's idea of <em>left</em> or <em>right</em> depends on which way we are looking. This is where the <strong>cross product</strong> comes in:</p><div><img src="img/image_07_015.jpg" alt="Moving the camera around"/></div><p>The cross product of two vectors is a slightly harder formula to memorize, but it is very useful. It gives us a vector that is <strong>orthogonal</strong> to the vectors <em>a</em> and <em>b</em>. Consider the following diagram:</p><div><img src="img/image_07_016.jpg" alt="Moving the camera around"/></div><p>An orthogonal vector is one way of saying that the direction of that vector is <strong>perpendicular</strong> to the plane the other two vectors form. Knowing that, we can implement left and right strafing with relative ease:</p><pre class="programlisting">} else if (l_dir == GL_Direction::Left) { 
  glm::vec3 cross = glm::cross(m_forwardDir, m_upDir); 
  m_position -= cross * l_amount; 
} else if (l_dir == GL_Direction::Right) { 
  glm::vec3 cross = glm::cross(m_forwardDir, m_upDir); 
  m_position += cross * l_amount; 
} ... 
</pre><p>After obtaining the cross product of the forward and up vectors, we simply multiply it by the scalar and add the result to the camera's position, creating left and right movement.</p><p>Rotating the camera is slightly more involved, but not trivial:</p><pre class="programlisting">void GL_Camera::OffsetLookBy(float l_speed, float l_x, float l_y) 
{ 
  glm::vec3 rotVector = glm::cross(m_forwardDir, m_upDir); 
  glm::mat4 rot_matrix = glm::rotate(-l_x * l_speed, m_upDir) * 
               glm::rotate(-l_y * l_speed, rotVector); 
  m_forwardDir = glm::mat3(rot_matrix) * m_forwardDir; 
} 
</pre><p>Once again, we use the cross product to obtain the orthogonal vector of the forward direction and up direction vectors plane. A rotation matrix is then calculated, by multiplying two rotation matrices of <em>X</em> and <em>Y</em> axes. For the <em>X</em> axis, we are simply rotating around the up direction vector, as shown here:</p><div><img src="img/image_07_017.jpg" alt="Moving the camera around"/></div><p>The <em>Y</em> axis rotation is made available by rotating along the orthogonal vector of the view direction and up vector's plane:</p><div><img src="img/image_07_018.jpg" alt="Moving the camera around"/></div><p>Having this functionality now allows us to program in actual camera movement, like so:</p><pre class="programlisting">void Game::Update() { 
  ... 
  m_mouseDifference = sf::Mouse::getPosition( 
    *m_window.GetRenderWindow()) - m_mousePosition; 
  m_mousePosition = sf::Mouse::getPosition( 
    *m_window.GetRenderWindow()); 
 
  float moveAmount = 0.005f; 
  float rotateSpeed = 0.004f; 
 
  if (sf::Keyboard::isKeyPressed(sf::Keyboard::W)) { 
    // Forward. 
    m_camera-&gt;MoveBy(GL_Direction::Forward, moveAmount); 
  } else if (sf::Keyboard::isKeyPressed(sf::Keyboard::S)) { 
    // Back. 
    m_camera-&gt;MoveBy(GL_Direction::Back, moveAmount); 
  } 
   
  if (sf::Keyboard::isKeyPressed(sf::Keyboard::A)) { 
    // Left. 
    m_camera-&gt;MoveBy(GL_Direction::Left, moveAmount); 
  } else if (sf::Keyboard::isKeyPressed(sf::Keyboard::D)) { 
    // Right. 
    m_camera-&gt;MoveBy(GL_Direction::Right, moveAmount); 
  } 
   
  if (sf::Keyboard::isKeyPressed(sf::Keyboard::Q)) { 
    // Up. 
    m_camera-&gt;MoveBy(GL_Direction::Up, moveAmount); 
  } else if (sf::Keyboard::isKeyPressed(sf::Keyboard::Z)) { 
    // Down. 
    m_camera-&gt;MoveBy(GL_Direction::Down, moveAmount); 
  } 
 
  if (sf::Mouse::isButtonPressed(sf::Mouse::Left)) { 
    m_camera-&gt;OffsetLookBy(rotateSpeed, 
      static_cast&lt;float&gt;(m_mouseDifference.x), 
      static_cast&lt;float&gt;(m_mouseDifference.y)); 
  } 
  ... 
} 
</pre><p>We are using the keyboard keys <em>W</em>, <em>S</em>, <em>A</em>, and <em>D</em> to move around the camera, and mouse position changes as scalar values to rotate it, provided the left mouse button is pressed.</p></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec65"/>Drawing with vertex indices</h1></div></div></div><p>One last thing that is quite important for us before moving on is covering a more efficient way of rendering shapes. Our current method is fine for rendering a single triangle, but it can get inefficient really quickly when rendering something more complex, like a cube. If we are using vertices only, it would require a grand total of <em>36</em> to render <em>six</em> cube faces. A much more efficient approach would obviously be submitting <em>eight</em> vertices for each corner of the cube and then reusing them to draw each face. Luckily, there is a way to do just that by using an <strong>index array</strong>.</p><p>Using indices simply means that for each model we are drawing, we also need to store an array of indices that represent the draw order of vertices. Each vertex in a model is given an index, starting from <em>0</em>. An array of these indices would then be used to connect the vertices, instead of having to re-submit them. Let's implement this functionality, starting with the <code class="literal">GL_Model</code> class:</p><pre class="programlisting">class GL_Model { 
  ... 
private: 
  ... 
  GLuint m_indexVBO; 
  ... 
}; 
</pre><p>As the new data member suggests, we need to store these indices in their own VBO, all of which happens inside the constructor:</p><pre class="programlisting">GL_Model::GL_Model(GL_Vertex* l_vertices, 
  unsigned int l_vertCount, <strong>unsigned int* l_indices,</strong>
<strong>unsigned int l_indexCount</strong>) 
{ 
  m_drawCount = <strong>l_indexCount</strong>; 
 
  glGenVertexArrays(1, &amp;m_VAO); 
  glBindVertexArray(m_VAO); 
  glGenBuffers(1, &amp;m_vertexVBO); 
  <strong>glGenBuffers(1, &amp;m_indexVBO);</strong> 
  ... 
  glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_indexVBO); 
  glBufferData(GL_ELEMENT_ARRAY_BUFFER, 
    l_indexCount * (sizeof(l_indices[0])), 
    l_indices, GL_STATIC_DRAW); 
  ... 
} 
</pre><p>The constructor needs to take two extra arguments: a pointer to an array of indices, and the count of indices in that array. Note that <code class="literal">m_drawCount</code> is now being set to <code class="literal">l_indexCount</code>. This is because we only need <em>eight</em> vertices for a cube model, but there are <em>36</em> indices that describe how to draw it.</p><p>After a new VBO is generated for the indices, we bind to it and submit the index data pretty much in the same way as before. The main difference here is the <code class="literal">GL_ELEMENT_ARRAY_BUFFER</code> flag. We cannot use <code class="literal">GL_ARRAY_BUFFER</code>, as the indices actually refer to the vertex data, which is located inside another VBO.</p><p>Obviously, this new data needs to be released once the model is no longer needed:</p><pre class="programlisting">GL_Model::~GL_Model() { 
  glDeleteBuffers(1, &amp;m_vertexVBO); 
  <strong>glDeleteBuffers(1, &amp;m_indexVBO);</strong> 
  glDeleteVertexArrays(1, &amp;m_VAO); 
} 
</pre><p>Drawing our model using indices requires a different <code class="literal">Draw()</code> call altogether:</p><pre class="programlisting">void GL_Model::Draw() { 
  glBindVertexArray(m_VAO); 
  <strong>glDrawElements(GL_TRIANGLES, m_drawCount, GL_UNSIGNED_INT, 0);</strong> 
  glBindVertexArray(0); 
} 
</pre><p>The call to the <code class="literal">glDrawElements()</code> method takes four arguments: the type of primitives we are going to be drawing, the total number of indices, the data type that these indices are represented by, and an offset that can be used to skip them.</p><p>That is all there is to drawing geometry using indices! Now let's set up a more exciting model to show it off:</p><pre class="programlisting">Game::Game() ... { 
  ... 
  GL_Vertex vertices[] = { 
    //      |---POSITION----| |TEXTURE| 
    //        X    Y  Z      X, Y 
    GL_Vertex({ -0.5, -0.5, 0.5 }, { 0, 0 }), // 0 
    GL_Vertex({ -0.5, 0.5, 0.5 }, { 0, 1 }), // 1 
    GL_Vertex({ 0.5, 0.5, 0.5 }, { 1, 1 }), // 2 
    GL_Vertex({ 0.5, -0.5, 0.5 }, { 1, 0 }), // 3 
    GL_Vertex({ -0.5, -0.5, -0.5f }, { 1, 0 }), // 4 
    GL_Vertex({ -0.5, 0.5, -0.5f }, { 1, 1 }), // 5 
    GL_Vertex({ 0.5, 0.5, -0.5f }, { 0, 0 }), // 6 
    GL_Vertex({ 0.5, -0.5, -0.5f }, { 0, 1 }) // 7 
  }; 
 
  unsigned int indices[] = { 
    2, 1, 0, 0, 3, 2, // Back 
    5, 4, 0, 0, 1, 5, // Right 
    3, 7, 6, 6, 2, 3, // Left 
    6, 7, 4, 4, 5, 6, // Front 
    1, 2, 6, 6, 5, 1, // Top 
    0, 4, 7, 7, 3, 0 // Bottom 
  }; 
 
  m_model = std::make_unique&lt;GL_Model&gt;(vertices, 8, indices, 36); 
  ... 
} 
</pre><p>As you can see, we've now set up <code class="literal">8</code> vertices, and we've created another array for indices. Once the model is rendered, we would see something like this:</p><div><img src="img/image_07_019.jpg" alt="Drawing with vertex indices"/></div><p>Note that the bottom face is actually rendered on top for some reason. This is caused by OpenGL not knowing which geometry to render on top, and this will be solved in the next section.</p></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec66"/>Face culling and depth buffer</h1></div></div></div><p>One way of solving the draw order issues is by using a <strong>depth buffer</strong>. In the simplest terms, a depth buffer, also commonly known as the <strong>Z-buffer</strong>, is basically a texture managed by OpenGL in the background that contains depth information of each pixel. When a pixel is being rendered, its depth (<em>Z</em> value) is checked against that on the depth buffer. If a pixel being rendered has a lower <em>Z</em> value, the pixel is overwritten, as it is clearly on top.</p><p>Enabling the depth buffer only comes down to a single <code class="literal">glEnable()</code> method call:</p><pre class="programlisting">Game::Game() ... { 
  ... 
  glEnable(GL_DEPTH_TEST); 
  ... 
} 
</pre><p>Keep in mind that the depth buffer is a texture. It is imperative to make sure it gets allocated when the window is created, and it has enough data to work with. We can make sure of that by creating an <code class="literal">sf::ContextSettings</code> structure and filling out its <code class="literal">depthBits</code> data member before passing it to the SFML's window <code class="literal">Create()</code> method:</p><pre class="programlisting">void GL_Window::Create() { 
  ... 
  sf::ContextSettings settings; 
  <strong>settings.depthBits = 32; // 32 bits.</strong> 
  settings.stencilBits = 8; 
  settings.antialiasingLevel = 0; 
  settings.majorVersion = 4; 
  settings.minorVersion = 5; 
 
  m_window.create(sf::VideoMode(m_windowSize.x, 
    m_windowSize.y, 32), m_windowTitle, style, <strong>settings</strong>); 
} 
</pre><p>If we just ran the code as is, the screen would be completely blank. Why? Well, remember that the Z-buffer is a texture. A texture, just like the display, needs to be cleared every cycle. We can accomplish that like so:</p><pre class="programlisting">void GL_Window::BeginDraw() { 
  glClearColor(0.f, 0.f, 0.f, 1.f); // BLACK 
  glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); 
} 
</pre><p>Adding the pipe symbol allows us to perform a bitwise or operation on the <code class="literal">glClear</code>'s argument, joining in the <code class="literal">GL_DEPTH_BUFFER_BIT</code> definition. This ensures that the depth buffer is also cleared to black, and we can finally enjoy our cube:</p><div><img src="img/image_07_020.jpg" alt="Face culling and depth buffer"/></div></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec67"/>Back face culling</h1></div></div></div><p>In order to save on performance, it is a good idea to let OpenGL know that we would like to cull faces that are not visible from the current perspective. This feature can be enabled like so:</p><pre class="programlisting">Game::Game() ... { 
  ... 
  glEnable(GL_DEPTH_TEST); 
  <strong>glEnable(GL_CULL_FACE); 
  glCullFace(GL_BACK);</strong> 
  ... 
} 
</pre><p>After we <code class="literal">glEnable</code> face culling, the <code class="literal">glCullFace</code> function is invoked to let OpenGL know which faces to cull. This will work right out of the box, but we may notice weird artifacts like this if our model data is not set up correctly:</p><div><img src="img/image_07_021.jpg" alt="Back face culling"/></div><p>This is because the order our vertices are rendered in actually defines whether a face of a piece of geometry is facing inwards or outwards. For example, if the vertices of a face are rendered in a clockwise sequence, the face, by default, is considered to be facing <strong>inwards</strong> of the model and vice versa. Consider the following diagram:</p><div><img src="img/image_07_022.jpg" alt="Back face culling"/></div><p>Setting up the model draw order correctly allows us to save on performance by not drawing invisible faces, and having our cube back just the way it was.</p></div></div></div>
<div><div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec68"/>Summary</h1></div></div></div><p>That may have been quite a lot to take in, but if you have made it all the way to the end, congratulations! The hard part is now over, and you are familiar with the modern versions of OpenGL, the programmable pipeline and general-purpose rendering. Even SFML itself was built around basic principles like the ones we have gone over, some of which we have already covered extensively.</p><p>In the next chapter, we are going to be covering the basics of lighting to create a more dynamic feel to our world. See you there!</p></div></div></div></body></html>