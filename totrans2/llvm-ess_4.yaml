- en: Chapter 4. Basic IR Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we have seen how the IR is independent of its target and how it can
    be used to generate code for a specific backend. To generate efficient code for
    the backend, we optimize the IR generated by the frontend by running a series
    of analysis and transformation passes using the LLVM pass manager. We must note
    that most of the optimizations that happen in a compiler take place on the IR,
    one of the reasons being that the IR is retargetable and the same set of optimizations
    would be valid for a number of targets. It reduces the effort of writing the same
    optimization for every target. There are some target-specific optimizations too;
    they happen at the selection DAG level, which we will see later. Another reason
    for IR being the target of optimization is that LLVM IR is in SSA form, which
    means every variable is assigned only once and every new assignment to a variable
    is a new variable itself. One very visible benefit of this representation is that
    we don't have to do reaching definition analysis where some variable is assigned
    a value of another variable. SSA representation also helps in a number of optimizations
    such as constant propagation, dead code elimination, and so on. Going ahead, we
    will see some of the important optimizations in LLVM, what is the role of LLVM
    Pass Infrastructure, and how we can use the opt tool to perform different optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The opt tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass and Pass Manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using other Pass info in own pass
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IR simplification examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IR combination examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opt Tool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Opt** is the LLVM Optimizer and analyzer tool that is run on LLVM IR to optimize
    the IR or produce an analysis about it. We saw in the first chapter a very basic
    introduction to the opt tool, and how to use it to run analysis and transformation
    passes. In this section, we will see what else the opt tool does. We must note
    that opt is a developer tool and all the optimizations that it provides can be
    invoked from the frontend as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the opt tool, we can specify the level of optimization that we need, which
    means we can specify the optimization levels from `O0`, `O1`, `O2`, to `O3`(`O0`
    being the least optimized code and `O3` being the most optimized code). Apart
    from these, there is also an optimization level `Os` or `Oz`, which deals with
    space optimization. The syntax to invoke one of these optimizations is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, x represents the optimization level, which can have a value from 0 to
    3 or s or z. These optimization levels are similar to what Clang frontend specifies.
    `-O0` represents no optimization whereas `–O1` means only few optimizations are
    enabled. `–O2` is a moderate level of optimization and `–O3` is the highest level
    of optimization, which is similar to `–O2` but it allows optimization that takes
    longer to perform or may generate larger code (the O3 level does not guarantee
    that the code will be the most optimized and efficient, it just says that the
    compiler will try more to optimize the code and in the process may break things
    also). `–Os` means optimization for size, basically not running optimizations
    which increase code size (for example, it removes `slp-vectorizer` optimization)
    and perform optimizations that reduce code size (for example, instruction combining
    optimization).
  prefs: []
  type: TYPE_NORMAL
- en: We can direct the opt tool to run a specific pass that we require. These passes
    can be one of the already defined passes listed at [http://llvm.org/docs/Passes.html](http://llvm.org/docs/Passes.html)
    or one of the passes we have written ourselves. The passes listed in the above
    link are also run in the optimization levels of `-O1`, `-O2`, and `-O3`. To view
    which pass is being run at a certain optimization level, use the `-debug-pass=Structure`
    command-line option along with the opt tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example to demonstrate the difference between the `O1` and `O2`
    level of optimization. The `O3` level generally has one or two more passes from
    `O2`. So, let''s take an example and see how much the `O2` level of optimization
    optimizes the code. Write the test code in the `test.ll` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this test code, the `callercaller` function calls the `caller` function,
    which in turn calls the `test` function, which performs an addition of two numbers
    and returns the value to its caller, which in turn returns the value to the `callercaller`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, run the `O1` and `O2` levels of optimization, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the difference in the optimized code for the
    `O1` and `O2` levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Opt Tool](img/00003.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the code in `O2` has optimized the calls to the function and
    the `Add` operations as well and returns the result directly from the `callercaller`
    function. This is obtained due to the fact that `O2` optimization runs the passes
    **always-inline** which inlines all the function calls and treats the code as
    one big function. Then, in also runs the `globaldce` pass, which eliminates unreachable
    internals from the code. After this, it runs `constmerge` which merges duplicate
    global constants into a single constant. It also performs a global value numbering
    pass that eliminates partially or fully redundant instructions and eliminates
    redundant load instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Pass and Pass Manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLVM's `Pass` infrastructure is one of the many important features of the LLVM
    system. There are a number of analysis and optimization passes that can be run
    using this infrastructure. The starting point for LLVM passes is the `Pass` class,
    which is a superclass of all the passes. We need to inherit from some predefined
    subclasses taking into account what our pass is going to implement.
  prefs: []
  type: TYPE_NORMAL
- en: '**ModulePass**: This is the most general superclass. By inheriting this class
    we allow the entire module to be analyzed at once. The functions within the module
    may not be referred to in a particular order. To use it, write a subclass that
    inherits from the `ModulePass` subclass and overloads the `runOnModule` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before going ahead with the discussion of other `Pass` classes, let''s look
    into the three virtual methods that the `Pass` classes override:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**doInitialization**: This is meant to do initialization stuff that does not
    depend on the current function being processed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**runOn{Passtype}**: This is the method where we should implement our subclass
    for the functionality of the pass. This will be `runOnFunction` for `FunctionPass`,
    `runOnLoop` for `LoopPass`, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**doFinalization**: This is called when `runOn{Passtype}` has finished doing
    the job for every function in the program.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FunctionPass**: These passes execute on each function present in the module,
    independent from other functions in the module. There is no defined order in which
    the functions will be processed. They are not allowed to modify functions other
    than the one being processed, and any addition or deletion of functions from the
    current module is also not allowed. To implement `FunctionPass` we might need
    to overload the three virtual functions mentioned earlier by implementing in the
    `runOnFunction` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BasicBlockPass**: These passes work on basic blocks one at a time, independently
    of other basic blocks present in the program. They are not allowed to add or delete
    any new basic block or change the CFG. They are also not allowed to do things
    that `FunctionPass` is not allowed to. To implement, they can override the `doInitialization`
    and `doFinalization` methods of `FunctionPass`, or overload their own virtual
    methods for the two methods mentioned earlier and the `runOnBasicBlock` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LoopPass**: These passes work on each loop in the function, independent of
    all other loops within the function. Loops are processed in such a way that the
    outermost loop is executed the last. To implement `LoopPass` we need to overload
    the `doInitialization`, `doFinalization`, and `runOnLoop` methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's see how to get started with writing a custom pass. Let's write a
    pass that will print the names of all the functions.
  prefs: []
  type: TYPE_NORMAL
- en: Before getting started with writing the implementation of the pass, we need
    to make changes in a few places in the code so that the pass is recognized and
    can be run.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to create a directory under the LLVM tree. Let''s make a directory,
    `lib/Transforms/FnNamePrint`. In this directory, we need to create a `Makefile`
    with the following contents, which will allow our pass to be compiled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This specifies that all `.cpp` files should be compiled and linked into a shared
    object that will be available in the `lib` folder of the `build-folder` (`build-folder/lib/FnNamePrint.so`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s get started with writing the actual pass implementation. We need
    to create the source file for the pass in `lib/Transforms/FnNamePrint`: let''s
    name it `FnNamePrint.cpp`. The first step now is to choose the correct subclass.
    In this case, as we are trying to print names of each function, the `FunctionPass`
    class will serve our purpose by processing one function at a time. Also, we are
    only printing the name of function and not modifying anything within it, so we
    are choosing `FunctionPass` for simplicity. We could use `ModulePass` as well
    because it is an `Immutable Pass`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s write the source code for the pass implementation, which looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code we `include` the necessary headers first and use an `llvm`
    namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We declare our pass as a structure, `FnNamePrint`, which is a subclass of `FunctionPass`.
    In `runOnFunction` we implement the logic to print the function name. The `bool`
    value returned in the end signifies whether we have made any modification within
    the function. A `True` value is returned if some modifications was made, otherwise,
    `false` is returned. In our case, we are not making any modifications, so we return
    `false`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we declare the `ID` for the pass, which is used to identify the pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we need to register the passes with the Pass Manager. The first argument
    is the Pass name used by the `opt` tool to identify this pass. The second argument
    is the actual Pass name. The third and fourth arguments specify whether the pass
    modified the `cfg` and whether it is an analysis pass.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The implementation of the pass is done. Now, before we use it, we need to build
    LLVM using the `make` command, which will build the shared object in the `lib`
    folder within the build (`build-folder/lib/FnNamePrint.so`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can run the pass over a test case using the `opt` tool in the following
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `load` command line option specifies the path from where to pick the shared
    object of the pass and `–funcnameprint` is the option to opt tool to tell it to
    run the pass we have written. The Pass will print the names of all the function
    present in the testcase. For the example in the first section it will print out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: So, we got started with writing a Pass. Now, we will see the significance of
    the `PassManager` class in LLVM.
  prefs: []
  type: TYPE_NORMAL
- en: The `PassManager` class schedules the passes to be run efficiently. The `PassManager`
    is used by all LLVM tools that run passes for the execution of these passes. It
    is the responsibility of the `PassManager` to make sure the interaction between
    the passes is correctly done. As it tries to execute the passes in an optimized
    way, it must have information regarding how the passes interact with each other
    and what the different dependencies between the passes are.
  prefs: []
  type: TYPE_NORMAL
- en: A pass itself can specify the dependency on other passes, that is, which passes
    need to be run before the execution of the current pass. Also, it can specify
    the passes that will be invalidated by the execution of the current pass. The
    `PassManager` gets the analysis results before a pass is executed. We will later
    see how a pass can specify such dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: The main work of the `PassManager` is to avoid the calculation of analysis results
    time and again. This is done by keeping track of which analyses are available,
    which are invalidated, and which analyses are required. The `PassManager` tracks
    the lifetimes of the analysis results and frees the memory holding the analysis
    results when not required, allowing for optimal memory use.
  prefs: []
  type: TYPE_NORMAL
- en: The `PassManager` pipelines the passes together to get better memory and cache
    results, improving the cache behavior of the compiler. When a series of consecutive
    `FunctionPass` are given, it will execute all the `FunctionPass` on the first
    function, then all the `FunctionPass` on the second function, and so on. This
    improves cache behavior as it is only dealing with the single function part of
    the LLVM representation and not the entire program.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `PassManager` also specifies the `–debug-pass` option with which we can
    see how one pass interacts with other passes. We can see what all passes are run
    using the `–debug-pass=Argument` option. We can use the `–debug-pass=Structure`
    option to see how the passes had run. It will also give us the names of the passes
    that ran. Let''s take the example of the test code in the first section of this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the output, the `Pass Arguments` gives us the passes that are run and the
    following list is the structure used to run each pass. The Passes just after `ModulePass`
    `Manager` will show the passes run per module (here, it is empty). The passes
    in hierarchy of `FunctionPass` `Manager` show that these passes were run per function
    (`Function Name Print` and `Module Verifier`), which is the expected result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `PassManger` also provides some other useful flags, some of which are the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**time-passes**: This gives time information about the pass along with the
    other passes that are lined up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stats**: This prints statistics about each pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**instcount**: This collects the count of all instructions and reports them.
    `–stats` must also be Passes to the opt tool so that the results of `instcount`
    are visible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using other Pass info in current Pass
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the Pass Manager to work optimally it needs to know the dependencies between
    the Passes. Each of the passes can itself declare its dependencies: the analysis
    passes that need to be executed before this pass is executed and the passes that
    will get invalidated after the current pass is run. To specify these dependencies,
    a pass needs to implement the `getAnalysisUsage` method.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this method the current pass can specify the required and invalidated
    sets by filling in the details in the `AnalysisUsage` object. To fill in the information
    the Pass needs to call any of the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: AnalysisUsage::addRequired<> method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This method arranges for the execution of a Pass prior to the current Pass.
    One example of this is: for memory copy optimization it needs the results of an
    alias analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: By adding the pass required to run, it is made sure that `Alias Analysis Pass`
    is run before the `MemCpyOpt` Pass. Also, this makes sure that if the `Alias Analysis`
    has been invalidated by some other Pass, it will be run before the `MemCpyOpt`
    Pass is run.
  prefs: []
  type: TYPE_NORMAL
- en: AnalysisUsage:addRequiredTransitive<> method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When an analysis chains to other analyses for results, this method should be
    used instead of the `addRequired` method. That is, when we need to preserve the
    order in which the analysis passes are run we use this method. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, `DependenceAnalysis` chains to `AliasAnalysis`, `ScalarEvolution` and
    `LoopInfo` Passes for the results.
  prefs: []
  type: TYPE_NORMAL
- en: AnalysisUsage::addPreserved<> method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By using this method a Pass can specify which analyses of other Passes it will
    not invalidate on running: that is, it will preserve the information already present,
    if any. This means that the subsequent passes that require the analysis would
    not need to run this again.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the case of the `MemCpyOpt` Pass seen earlier, it required
    the `AliasAnalysis` results and it also preserved them. Also:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To get a detailed understanding of how everything is linked and works together,
    you can pick up any of the transformation passes and go through the source code
    and you will know how they are getting information from other passes and how they
    are using it.
  prefs: []
  type: TYPE_NORMAL
- en: Instruction simplification example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see how we fold instructions into simpler forms in
    LLVM. Here, the creation of new instructions will not take place. Instruction
    simplification does constant folding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: That is, it simplifies the `sub` instruction to a constant value `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can handle non-constant operands as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It returns a value of variable `%x`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this case, it returns an already existing value.
  prefs: []
  type: TYPE_NORMAL
- en: The implementations for the methods that simplify instructions are located in
    `lib/Analysis/InstructionSimplify.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the important methods of dealing with the simplification of instructions
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SimplifyBinOp method**: This is used to simplify binary operations such as
    addition, subtraction, and multiplication, and so on. It has the function signature
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, by `Opcode`, we mean the operator instruction that we are trying to simplify.
    LHS and RHS are the operands on either side of the operator. `MaxRecurse` is the
    recursion level we specify after which the routine must stop trying simplification
    of the instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this method, we have a switch case on the `Opcode`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this `Opcode,` the method decides which function it needs to call for
    simplification. Some of the methods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SimplifyAddInst**: This method tries to fold the result of the `Add` operator
    when the operands are known. Some of the folding is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code for the last simplification in the function `static` `Value *SimplifyAddInst(Value
    *Op0, Value *Op1, bool isNSW, bool isNUW, const Query &Q, unsigned MaxRecurse
    )` looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the first condition matches the `(Y-X)` value in the expression as `Operand1:
    m_Value(Y)` denotes value of `Y` and `m_Specific(Op0)` denotes `X`. As soon as
    it is matched it folds the expression to a constant value `Y` and returns it.
    The case is similar for the second part of our condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SimplifySubInst**: This method tries to fold the result of `subtract` operator
    when the operators are known. Some examples for the same are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The matching of instructions and folding is done similar to as shown in `SimplifyAddInst`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SimplifyAndInst**: Similar to the two preceding methods, it tries to fold
    the result for the logical operator And. Some examples of this are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code for this, in the method looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Here, it tries to match `A` and `~A` and returns a `Null` value, 0, when it
    matches the condition.
  prefs: []
  type: TYPE_NORMAL
- en: So, we have seen a bit of instruction simplification. Now, what do we do if
    we can replace a set of instructions with a more effective set of instructions?
  prefs: []
  type: TYPE_NORMAL
- en: Instruction Combining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instruction combining is a LLVM Pass and compiler technique in which we replace
    a sequence of instructions with instructions that are more effective and give
    the same result on execution in a smaller number of machine cycles. Instruction
    combining does not alter the CFG of the program and is mainly used for algebraic
    simplification. The major difference between instruction combining and instruction
    simplification is that in instruction simplification we cannot generate new instructions,
    which is possible in instruction combining. This pass is run by specifying the
    `–instcombine` argument to the opt tool and is implemented in the `lib/transforms/instcombine`
    folder. The `instcombine` Pass combines
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: It has removed one redundant `add` instruction and hence combined the two `add`
    instructions to one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLVM page states that this pass guarantees that the following canonicalizations
    are performed on the program:'
  prefs: []
  type: TYPE_NORMAL
- en: Constant operand of a binary operator is moved to RHS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bitwise operators with constant operands are grouped together with shifts being
    performed first then 'or' operations, 'and' operations and then 'xor operations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If possible, comparison operators are converted from <,>,<=,>= to == or != .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All `cmp` instructions operating on Boolean values are replaced with logical
    operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add X, X is represented by X*2 , that is X<<1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multipliers with a power-of-two constant argument are transformed into shifts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This pass starts from `bool InstCombiner::runOnFunction(Function &F)` located
    in the `InstructionCombining.cpp` file. There are different files under the `lib/Transform/InstCombine`
    folder to combine instructions related to different instructions. The methods,
    before trying to combine instructions, try to simplify them. Some of these methods
    for simplification of the `instcombine` module are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SimplifyAssociativeOrCommutative function**: It performs simplification for
    operators that are associative or commutative. For commutative operators, it orders
    the operands from right to left in the order of increasing complexity. For associative
    operations of the form "`(X op Y) op Z`", it converts it to "`X op (Y op Z)`"
    if (Y op Z) can be simplified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tryFactorization function**: This method tries to simplify binary operations
    by factoring out common terms using commutative and distributive property of the
    operator. For example, `(A*B)+(A*C)` is simplified to `A*(B+C)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's look at instruction combining. As described earlier, various functionalities
    are implemented in different files. Let's take an example testcode and see where
    to add code so that instruction combining happens for our testcode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write the testcode in `test.ll` for the pattern `(A | (B ^ C)) ^ ((A
    ^ C) ^ B)`, which can be reduced to `(A & (B ^ C))`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The code in LLVM for the handling of operators such as "And", "Or", and "Xor"
    lies in the `lib/Transforms/InstCombine/InstCombineAndOrXor.cpp` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `InstCombineAndOrXor.cpp` file, in the `InstCombiner::visitXor(BinaryOperator
    &I)` function, go to the `if` condition `If` `(Op0I && Op1I)` and add the following
    snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As it is quite clear, the code added is to match the pattern `(A | (B ^ C))
    ^ ((A ^ C) ^ B)` and return `(A & (B ^ C))` when matched.
  prefs: []
  type: TYPE_NORMAL
- en: To test the code, build LLVM and run the `instcombine` Pass with this test code
    and see the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: So the output shows that now only one `xor` `and` one and operation is required
    instead of four `xor` and one `or` earlier`.`
  prefs: []
  type: TYPE_NORMAL
- en: To understand and add more transformations you can look into the source code
    in the `InstCombine` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, in this chapter, we looked into how simple transformations can be applied
    to IR. We looked into the opt tool, LLVM Pass infrastructure, the `Passmanager`
    and how to use information of one Pass in another Pass. We ended the chapter with
    examples of instruction simplification and instruction combining. In the next
    chapter, we will see some more advanced optimizations like Loop Optimization,
    Scalar Evolution, and others, where we will operate at a block of code rather
    than individual instructions.
  prefs: []
  type: TYPE_NORMAL
