<html><head></head><body>
        

            
                <h1 class="header-title">Advanced Rendering Techniques</h1>
            

            
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Drawing a skybox</li>
<li>Drawing billboards using geometry shaders</li>
<li>Drawing particles using compute and graphics pipelines</li>
<li>Rendering a tessellated terrain</li>
<li>Rendering a full-screen quad for post-processing</li>
<li>Using input attachments for a color correction post-process effect</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Introduction</h1>
            

            
                
<p>Creating 3D applications, such as games, benchmarks or CAD tools, usually requires, from a rendering perspective, the preparation of various resources, including meshes or textures, drawing multiple objects on the scene, and implementing algorithms for object transformations, lighting calculations, and image processing. They all can be developed in any way we want, in a way that is most suitable for our purpose. But there are also many useful techniques that are commonly used in the 3D graphics industry. Descriptions for these can be found in books and tutorials with examples implemented using various 3D graphics APIs.</p>
<p>Vulkan is still a relatively new graphics API, so there aren't too many resources that present common rendering algorithms implemented with the Vulkan API. In this chapter, we will learn how to use Vulkan to prepare various graphics techniques. We will learn about important concepts from a collection of popular, advanced rendering algorithms found in games and benchmarks and how they match with the Vulkan resources.</p>
<p>In this chapter, we will focus only on the code parts that are important from the perspective of a given recipe. Resources that are not described (for example, command pool or render pass creation) are created as usual (refer to the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe from <a href="45108a92-6d49-4759-9495-3f1166e69128.xhtml">Chapter 11</a>, <em>Lighting</em>).</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Drawing a skybox</h1>
            

            
                
<p>Rendering 3D scenes, especially open world ones with vast viewing distances, requires many objects to be drawn. However, the processing power of current graphics hardware is still too limited to render as many objects as we see around us every day. So, to lower the number of drawn objects and to draw the background for our scene, we usually prepare an image (or a photo) of distant objects and draw just the image instead.</p>
<p>In games where players can freely move and look around, we can't draw a single image. We must draw images in all directions. Such images form a cube, and an object on which background images are placed is called a skybox. We render it in such a way that it is always in the background, at the furthest depth value available.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Getting ready</h1>
            

            
                
<p>Drawing a skybox requires the preparation of a cubemap. It contains six square images containing a view in all world directions (right, left, up, down, backward, forward), as in the following image:</p>
<div><img class=" image-border" height="394" src="img/image_12_001.png" width="522"/></div>
<p>Images courtesy of Emil Persson (<a href="http://www.humus.name">h t t p ://w w w . h u m u s . n a m e</a>)</p>
<p>In Vulkan, cubemaps are special image views created for images with six array layers (or a multiple of six). Layers must contain images in the <em>+X</em>, -<em>X</em>, <em>+Y</em>, -<em>Y</em>, <em>+Z</em>, -<em>Z</em> order.</p>
<p>Cubemaps can be used not only for drawing skyboxes. We can use them to draw reflections or transparent objects. They can be used for lighting calculations as well (refer to the <em>Drawing a reflective and refractive geometry using cubemaps</em> recipe in Chapter 11, <em>Lighting</em>).</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">How to do it...</h1>
            

            
                
<ol>
<li>Load a 3D model of a cube from a file and store vertex data in a vertex buffer. Only vertex positions are required (refer to the <em>Loading a 3D model from an OBJ file</em> recipe in <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml">Chapter 10</a>, <em>Helper Recipes</em>).</li>
</ol>
<ol start="2">
<li>Create a combined image sampler with a square <kbd>VK_IMAGE_TYPE_2D</kbd> image that has six array layers (or a multiple of six), a sampler that uses a <kbd>VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE</kbd> addressing mode for all coordinates and a <kbd>VK_IMAGE_VIEW_TYPE_CUBE</kbd> image view (refer to the <em>Creating a combined image sampler</em> recipe in <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
<li>Load image data for all six sides of a cube and upload it to the image's memory using a staging buffer. Image data must be uploaded to six array layers in the following order: <em>+X</em>, -<em>X</em>, <em>+Y</em>, -<em>Y</em>, <em>+Z</em>, -<em>Z</em> (refer to the <em>Loading texture data from a file</em> recipe in <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml">Chapter 10</a>, Helper Recipes, and to the <em>Using a staging buffer to update an image with a device-local memory bound</em> recipe in <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml">Chapter 4</a>, <em>Resources and Memory</em>).</li>
<li>Create a uniform buffer in which transformation matrices will be stored (refer to the <em>Creating a uniform buffer</em> recipe in <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
<li>Create a descriptor set layout with the uniform buffer accessed by a vertex stage and a combined image sampler accessed by a fragment stage. Allocate a descriptor set using the preceding layout. Update the descriptor set with the uniform buffer and the cubemap/combined image sampler (refer to the <em>Creating a descriptor set layout, Allocating descriptor sets</em> and <em>Updating descriptor sets</em> recipes in <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
<li>Create a shader module with a vertex shader created from the following GLSL code (refer to the <em>Creating a shader module recipe</em> in <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>):</li>
</ol>
<pre>
       #version 450 
       layout( location = 0 ) in vec4 app_position; 
       layout( set = 0, binding = 0 ) uniform UniformBuffer { 
        mat4 ModelViewMatrix; 
        mat4 ProjectionMatrix; 
       }; 

       layout( location = 0 ) out vec3 vert_texcoord; 

       void main() { 
         vec3 position = mat3(ModelViewMatrix) * app_position.xyz; 
         gl_Position = (ProjectionMatrix * vec4( position, 0.0 )).xyzz; 
         vert_texcoord = app_position.xyz; 
       }
</pre>
<ol start="7">
<li>Create a shader module with a fragment shader created from the following GLSL code:</li>
</ol>
<pre>
       #version 450 
       layout( location = 0 ) in vec3 vert_texcoord; 
       layout( set = 0, binding = 1 ) uniform samplerCube Cubemap; 
       layout( location = 0 ) out vec4 frag_color; 

       void main() { 
         frag_color = texture( Cubemap, vert_texcoord ); 
       }
</pre>
<ol start="8">
<li>Create a graphics pipeline from the preceding modules with vertex and fragment shaders. The pipeline should use one vertex attribute with three components (vertex positions) and a <kbd>VK_CULL_MODE_FRONT_BIT</kbd> value for the rasterization state's culling mode. Blending should be disabled. The pipeline's layout should allow access to the uniform buffer and the cubemap/combined image sampler (refer to the <em>Specifying pipeline shader stages</em>, <em>Specifying pipeline vertex input state</em>, <em>Specifying pipeline rasterization state</em>, <em>Specifying pipeline blend state</em>, <em>Creating a pipeline layout</em>, <em>Specifying graphics pipeline creation parameters</em> and <em>Creating a graphics pipeline</em> recipes from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Draw the cube with the rest of a rendered geometry (refer to the <em>Binding descriptor sets</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>, to the <em>Binding a pipeline object</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em> and to the <em>Binding vertex buffers</em> and <em>Drawing a geometry</em> recipes from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml">Chapter 9</a>, <em>Command Recording and Drawing</em>).</li>
<li>Update a model view matrix in the uniform buffer each time a user (a camera) moves in the scene. Update a projection matrix in the uniform buffer each time the application window is resized.</li>
</ol>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">How it works...</h1>
            

            
                
<p>To render a skybox we need to load or prepare a geometry forming a cube. Only positions are required as they can also be used for texture coordinates.</p>
<p>Next, we load six cubemap images and create a combined image sampler with a cube image view:</p>
<pre>
InitVkDestroyer( LogicalDevice, CubemapImage ); 
InitVkDestroyer( LogicalDevice, CubemapImageMemory ); 
InitVkDestroyer( LogicalDevice, CubemapImageView ); 
InitVkDestroyer( LogicalDevice, CubemapSampler ); 
if( !CreateCombinedImageSampler( PhysicalDevice, *LogicalDevice, VK_IMAGE_TYPE_2D, VK_FORMAT_R8G8B8A8_UNORM, { 1024, 1024, 1 }, 1, 6, 
  VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT, VK_IMAGE_VIEW_TYPE_CUBE, VK_IMAGE_ASPECT_COLOR_BIT, VK_FILTER_LINEAR, 
  VK_FILTER_LINEAR, VK_SAMPLER_MIPMAP_MODE_NEAREST, VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, 
  VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, 0.0f, false, 1.0f, false, VK_COMPARE_OP_ALWAYS, 0.0f, 1.0f, VK_BORDER_COLOR_FLOAT_OPAQUE_BLACK, 
  false, *CubemapSampler, *CubemapImage, *CubemapImageMemory, *CubemapImageView ) ) { 
  return false; 
} 
std::vector&lt;std::string&gt; cubemap_images = { 
  "Data/Textures/Skansen/posx.jpg", 
  "Data/Textures/Skansen/negx.jpg", 
  "Data/Textures/Skansen/posy.jpg", 
  "Data/Textures/Skansen/negy.jpg", 
  "Data/Textures/Skansen/posz.jpg", 
  "Data/Textures/Skansen/negz.jpg" 
}; 
for( size_t i = 0; i &lt; cubemap_images.size(); ++i ) { 
  std::vector&lt;unsigned char&gt; cubemap_image_data; 
  int image_data_size; 
  if( !LoadTextureDataFromFile( cubemap_images[i].c_str(), 4, cubemap_image_data, nullptr, nullptr, nullptr, &amp;image_data_size ) ) { 
    return false; 
  } 
  VkImageSubresourceLayers image_subresource = { 
    VK_IMAGE_ASPECT_COLOR_BIT, 
    0, 
    static_cast&lt;uint32_t&gt;(i), 
    1 
  }; 
  UseStagingBufferToUpdateImageWithDeviceLocalMemoryBound( PhysicalDevice, *LogicalDevice, image_data_size, &amp;cubemap_image_data[0], 
    *CubemapImage, image_subresource, { 0, 0, 0 }, { 1024, 1024, 1 }, VK_IMAGE_LAYOUT_UNDEFINED, VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL, 
    0, VK_ACCESS_SHADER_READ_BIT, VK_IMAGE_ASPECT_COLOR_BIT, VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, 
    GraphicsQueue.Handle, FrameResources.front().CommandBuffer, {} ); 
}
</pre>
<p>The created cube image view, along with a sampler, is then provided to the shaders through a descriptor set. We also need a uniform buffer in which transformation matrices will be stored and accessed in the shaders:</p>
<pre>
std::vector&lt;VkDescriptorSetLayoutBinding&gt; descriptor_set_layout_bindings = { 
  { 
    0, 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1, 
    VK_SHADER_STAGE_VERTEX_BIT, 
    nullptr 
  }, 
  { 
    1, 
    VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
    1, 
    VK_SHADER_STAGE_FRAGMENT_BIT, 
    nullptr 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorSetLayout ); 
if( !CreateDescriptorSetLayout( *LogicalDevice, descriptor_set_layout_bindings, *DescriptorSetLayout ) ) { 
  return false; 
} 

std::vector&lt;VkDescriptorPoolSize&gt; descriptor_pool_sizes = { 
  { 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1 
  }, 
  { 
    VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
    1 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorPool ); 
if( !CreateDescriptorPool( *LogicalDevice, false, 1, descriptor_pool_sizes, *DescriptorPool ) ) { 
  return false; 
} 

if( !AllocateDescriptorSets( *LogicalDevice, *DescriptorPool, { *DescriptorSetLayout }, DescriptorSets ) ) { 
  return false; 
} 

BufferDescriptorInfo buffer_descriptor_update = { 
  DescriptorSets[0], 
  0, 
  0, 
  VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
  { 
    { 
      *UniformBuffer, 
      0, 
      VK_WHOLE_SIZE 
    } 
  } 
}; 

ImageDescriptorInfo image_descriptor_update = { 
  DescriptorSets[0], 
  1, 
  0, 
  VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
  { 
    { 
      *CubemapSampler, 
      *CubemapImageView, 
      VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL 
    } 
  } 
}; 

UpdateDescriptorSets( *LogicalDevice, { image_descriptor_update }, { buffer_descriptor_update }, {}, {} );
</pre>
<p>To draw a skybox we don't need a separate, dedicated <em>render pass</em>, as we can render it along the normal geometry. What's more, to save processing power (image fill rate), we usually draw a skybox after the (opaque) geometry and before the transparent objects. It is rendered in such a way so that its vertices are always at the far clipping plane. This way it doesn't cover geometry that had been already drawn and doesn't get clipped away either. This effect is achieved with a special vertex shader. Its most important part is the following code:</p>
<pre>
vec3 position = mat3(ModelViewMatrix) * app_position.xyz; 
gl_Position = (ProjectionMatrix * vec4( position, 0.0 )).xyzz;
</pre>
<p>First, we multiply the position by a modelview matrix. We take only the rotation part of the matrix. A player should always be in the center of the skybox, or the illusion will be broken. That's why we don't want to move the skybox, we need only to rotate it as a response to the player looking around.</p>
<p>Next, we multiply the viewspace position of a vertex by a projection matrix. The result is stored in a 4-element vector, with the last two components being the same and equal to the z component of the result. In modern graphics hardware, a perspective projection is performed by dividing the position vector by its <kbd>w</kbd> component. After that, all vertices, whose <kbd>x</kbd> and <kbd>y</kbd> components fit into the &lt;<kbd>-1, 1</kbd>&gt; range (inclusive) and <kbd>z</kbd> component fits into the &lt;<kbd>0, 1</kbd>&gt; range (inclusive), are inside the clipping volume and are visible (unless they are obscured by something else). So, calculating the vertex position in a way that makes its last two components equal, guarantees that the vertex will lie on the far clipping plane.</p>
<p>Apart from the vertex shader and a cube image view, skybox needs only one additional special treatment. We need to remember polygon facingness. Usually, we draw geometry with backface culling, as we want to see its external surface. For the skybox, we want to render its internal surface, because we look at it from the inside. That's why, if we don't have a mesh prepared especially for the skybox, we probably want to cull front faces during skybox rendering. We can prepare the pipeline rasterization info like this:</p>
<pre>
VkPipelineRasterizationStateCreateInfo rasterization_state_create_info; 
SpecifyPipelineRasterizationState( false, false, VK_POLYGON_MODE_FILL, VK_CULL_MODE_FRONT_BIT, VK_FRONT_FACE_COUNTER_CLOCKWISE, false, 0.0f, 1.0f, 0.0f, 1.0f, rasterization_state_create_info );
</pre>
<p>Apart from that, the graphics pipeline is created in the usual way. To use it for drawing, we need to bind the descriptor set, the vertex buffer, and the pipeline itself:</p>
<pre>
BindVertexBuffers( command_buffer, 0, { { *VertexBuffer, 0 } } ); 

BindDescriptorSets( command_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS, *PipelineLayout, 0, DescriptorSets, {} ); 

BindPipelineObject( command_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS, *Pipeline ); 

for( size_t i = 0; i &lt; Skybox.Parts.size(); ++i ) { 
  DrawGeometry( command_buffer, Skybox.Parts[i].VertexCount, 1, Skybox.Parts[i].VertexOffset, 0 ); 
}
</pre>
<p>The following images have been generated using this recipe:</p>
<div><img class=" image-border" src="img/image_12_002.png"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">See also</h1>
            

            
                
<ul>
<li>In <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>, see the following recipes:
<ul>
<li><em>Creating a combined image sampler</em></li>
<li><em>Creating a descriptor set layout</em></li>
<li><em>Allocating descriptor sets</em></li>
<li><em>Updating descriptor sets</em></li>
<li><em>Binding descriptor sets</em></li>
</ul>
</li>
<li>In <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>, see the following recipes:
<ul>
<li><em>Creating a shader module</em></li>
<li><em>Specifying pipeline shader stages</em></li>
<li><em>Creating a graphics pipeline</em></li>
<li><em>Binding a pipeline object</em></li>
</ul>
</li>
<li>In <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml">Chapter 9</a>, <em>Command Recording and Drawing</em>, see the following recipes:
<ul>
<li><em>Binding vertex buffers</em></li>
<li><em>Drawing a geometry</em></li>
</ul>
</li>
<li>In <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml">Chapter 10</a>, <em>Helper Recipes</em>, see the following recipes:
<ul>
<li><em>Loading texture data from a file</em></li>
<li><em>Loading a 3D model from an OBJ file</em></li>
</ul>
</li>
<li>In <a href="45108a92-6d49-4759-9495-3f1166e69128.xhtml">Chapter 11</a>, <em>Lighting</em>, see the following recipe:
<ul>
<li><em>Drawing a reflective and refractive geometry using cubemaps</em></li>
</ul>
</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Drawing billboards using geometry shaders</h1>
            

            
                
<p>Simplifying geometry drawn in a distance is a common technique for lowering the processing power needed to render the whole scene. The simplest geometry that can be drawn is a flat quad (or a triangle) with an image depicting the look of an object. For the effect to be convincing, the quad must always be facing camera:</p>
<div><img class=" image-border" src="img/image_12_003.png"/></div>
<p>Flat objects that are always facing camera are called billboards. They are used not only for distant objects as the lowest level of detail of a geometry, but also for particle effects.</p>
<p>One straightforward technique for drawing billboards is to use geometry shaders.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">How to do it...</h1>
            

            
                
<ol start="1">
<li>Create a logical device with the <kbd>geometryShader</kbd> feature enabled (refer to the <em>Getting features and properties of a physical device</em> and <em>Creating a logical device</em> recipes from <a href="d10e8284-6122-4d0a-8f86-ab0bc0bba47e.xhtml">Chapter 1</a>, <em>Instance and Devices</em>).</li>
<li>Prepare positions for all billboards with one vertex per single billboard. Store them in a vertex buffer (refer to the <em>Creating a buffer</em> recipe from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml">Chapter 4</a>, <em>Resources and Memory</em>).</li>
<li>Create a uniform buffer for at least two 4x4 transformation matrices (refer to the <em>Creating a uniform buffer</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
<li>If billboards should use a texture, create a combined image sampler and upload the texture data loaded from a file to the image's memory (refer to the <em>Creating a combined image sampler</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em> and to the <em>Loading texture data from a file</em> recipe from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml">Chapter 10</a>, <em>Helper Recipes</em>).</li>
<li>Prepare a descriptor set layout for a uniform buffer accessed by vertex and geometry stages and, if billboards need a texture, a combined image sampler accessed by a fragment shader stage. Create a descriptor set and update it with the created uniform buffer and the combined image sampler (refer to the <em>Creating a descriptor set layout</em>, <em>Allocating descriptor sets,</em> and <em>Updating descriptor sets</em> recipes from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
<li>Create a shader module with a vertex shader created from the following GLSL code (refer to the <em>Creating a shader module</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>):</li>
</ol>
<pre>
       #version 450 
       layout( location = 0 ) in vec4 app_position; 
       layout( set = 0, binding = 0 ) uniform UniformBuffer { 
         mat4 ModelViewMatrix; 
         mat4 ProjectionMatrix; 
       }; 

       layout( push_constant ) uniform TimeState { 
        float Time; 
       } PushConstant; 

       void main() { 
         gl_Position = ModelViewMatrix * app_position; 
       }
</pre>
<ol start="7">
<li>Create a shader module containing a geometry shader created from the following GLSL code:</li>
</ol>
<pre>
       #version 450 
       layout( points ) in; 
       layout( set = 0, binding = 0 ) uniform UniformBuffer { 
        mat4 ModelViewMatrix; 
        mat4 ProjectionMatrix; 
       }; 

       layout( triangle_strip, max_vertices = 4 ) out; 
       layout( location = 0 ) out vec2 geom_texcoord; 

       const float SIZE = 0.1; 

       void main() { 
         vec4 position = gl_in[0].gl_Position; 

         gl_Position = ProjectionMatrix * (gl_in[0].gl_Position + vec4( 
         -SIZE, SIZE, 0.0, 0.0 )); 
         geom_texcoord = vec2( -1.0, 1.0 ); 
         EmitVertex(); 

         gl_Position = ProjectionMatrix * (gl_in[0].gl_Position + vec4( 
         -SIZE, -SIZE, 0.0, 0.0 )); 
         geom_texcoord = vec2( -1.0, -1.0 ); 
         EmitVertex(); 

         gl_Position = ProjectionMatrix * (gl_in[0].gl_Position + vec4( 
         SIZE, SIZE, 0.0, 0.0 )); 
         geom_texcoord = vec2( 1.0, 1.0 ); 
         EmitVertex(); 

         gl_Position = ProjectionMatrix * (gl_in[0].gl_Position + vec4( 
         SIZE, -SIZE, 0.0, 0.0 )); 
         geom_texcoord = vec2( 1.0, -1.0 ); 
         EmitVertex(); 

         EndPrimitive(); 
       }
</pre>
<ol start="8">
<li>Create a shader module with a fragment shader that uses a SPIR-V assembly generated from the following GLSL code:</li>
</ol>
<pre>
       #version 450 
       layout( location = 0 ) in vec2 geom_texcoord; 
       layout( location = 0 ) out vec4 frag_color; 

       void main() { 
         float alpha = 1.0 - dot( geom_texcoord, geom_texcoord ); 
         if( 0.2 &gt; alpha ) { 
           discard; 
         } 
         frag_color = vec4( alpha ); 
       }
</pre>
<ol start="9">
<li>Create a graphics pipeline. It must use shader modules with the preceding vertex, geometry and fragment shaders. Only one vertex attribute (a position) is needed. It will be used to draw geometry using a <kbd>VK_PRIMITIVE_TOPOLOGY_POINT_LIST</kbd> primitive. The pipeline should have access to the uniform buffer with transformation matrices and (if needed) a combined image texture (refer to the <em>Specifying pipeline shader stages</em>, <em>Specifying pipeline vertex input state</em>, <em>Specifying pipeline input assembly state</em>, <em>Creating a pipeline layout</em>, <em>Specifying graphics pipeline creation parameters</em>, and <em>Creating a graphics pipeline</em> recipes from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Draw the geometry inside a render pass (refer to the <em>Binding descriptor sets</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>, to the <em>Binding a pipeline object</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>, and to the <em>Binding vertex buffers</em> and <em>Drawing a geometry recipes</em> from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml">Chapter 9</a>, <em>Command Recording and Drawing)</em>.</li>
<li>Update a modelview matrix in the uniform buffer each time the user (a camera) moves in the scene. Update a projection matrix in the uniform buffer each time the application window is resized.</li>
</ol>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">How it works...</h1>
            

            
                
<p>First, we start by preparing the positions for billboards. Billboards are drawn as point primitives, so one vertex corresponds to a one billboard. How we prepare the geometry is up to us and we don't need other attributes. A geometry shader converts a single vertex into a camera-facing quad and calculates texture coordinates.</p>
<p>In this example we don't use a texture, but we will use texture coordinates to draw circles. All we need to access are transformation matrices stored in a uniform buffer generated like this:</p>
<pre>
std::vector&lt;VkDescriptorSetLayoutBinding&gt; descriptor_set_layout_bindings = { 
  { 
    0, 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1, 
    VK_SHADER_STAGE_VERTEX_BIT | VK_SHADER_STAGE_GEOMETRY_BIT, 
    nullptr 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorSetLayout ); 
if( !CreateDescriptorSetLayout( *LogicalDevice, descriptor_set_layout_bindings, *DescriptorSetLayout ) ) { 
  return false; 
} 

std::vector&lt;VkDescriptorPoolSize&gt; descriptor_pool_sizes = { 
  { 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorPool ); 
if( !CreateDescriptorPool( *LogicalDevice, false, 1, descriptor_pool_sizes, *DescriptorPool ) ) { 
  return false; 
} 

if( !AllocateDescriptorSets( *LogicalDevice, *DescriptorPool, { *DescriptorSetLayout }, DescriptorSets ) ) { 
  return false; 
} 

BufferDescriptorInfo buffer_descriptor_update = { 
  DescriptorSets[0], 
  0, 
  0, 
  VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
  { 
    { 
      *UniformBuffer, 
      0, 
      VK_WHOLE_SIZE 
    } 
  } 
}; 

UpdateDescriptorSets( *LogicalDevice, {}, { buffer_descriptor_update }, {}, {} );
</pre>
<p>The next step is to create a graphics pipeline. It uses a single vertex attribute (a position) defined in the following way:</p>
<pre>
std::vector&lt;VkVertexInputBindingDescription&gt; vertex_input_binding_descriptions = { 
  { 
    0, 
    3 * sizeof( float ), 
    VK_VERTEX_INPUT_RATE_VERTEX 
  } 
}; 

std::vector&lt;VkVertexInputAttributeDescription&gt; vertex_attribute_descriptions = { 
  { 
    0, 
    0, 
    VK_FORMAT_R32G32B32_SFLOAT, 
    0 
  } 
}; 

VkPipelineVertexInputStateCreateInfo vertex_input_state_create_info; 
SpecifyPipelineVertexInputState( vertex_input_binding_descriptions, vertex_attribute_descriptions, vertex_input_state_create_info );
</pre>
<p>We draw vertices as points, so we need to specify an appropriate primitive type during the pipeline creation:</p>
<pre>
VkPipelineInputAssemblyStateCreateInfo input_assembly_state_create_info; 
SpecifyPipelineInputAssemblyState( VK_PRIMITIVE_TOPOLOGY_POINT_LIST, false, input_assembly_state_create_info );
</pre>
<p>The rest of the pipeline parameters are fairly typical. The most important parts are the shaders.</p>
<p>A vertex shader transforms the vertex from the local space to the view space. Billboards must always face the camera, so it is easier to perform calculations directly in the view space.</p>
<p>A geometry shader does almost all the work. It takes one vertex (a point) and emits a triangle strip with four vertices (a quad). Each new vertex is offset a bit to the left/right and up/down to form a quad:</p>
<div><img class=" image-border" src="img/image_12_004.png"/></div>
<p>Additionally, a texture coordinate is assigned to the generated vertex based on the direction/offset. In our example, the first vertex is prepared like this:</p>
<pre>
vec4 position = gl_in[0].gl_Position; 

gl_Position = ProjectionMatrix * (gl_in[0].gl_Position + vec4( -SIZE, SIZE, 0.0, 0.0 )); 
geom_texcoord = vec2( -1.0, 1.0 ); 
EmitVertex();
</pre>
<p>The remaining vertices are emitted in a similar way. As we transformed vertices to the view space in the vertex shader, the generated quad is always facing the screen plane. All we need to do is to multiply generated vertices by a projection matrix to transform them to the clip space.</p>
<p>A fragment shader is used to discard some fragments to form a circle from the quad:</p>
<pre>
float alpha = 1.0 - dot( geom_texcoord, geom_texcoord ); 
if( 0.2 &gt; alpha ) { 
  discard; 
}
</pre>
<p>In the following example, we can see billboards rendered in the positions of a mesh's vertices. The circles seen in the image are flat; they are not spheres:</p>
<div><img class=" image-border" height="176" src="img/image_12_005.png" width="483"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">See also</h1>
            

            
                
<ul>
<li>In <a href="d10e8284-6122-4d0a-8f86-ab0bc0bba47e.xhtml">Chapter 1</a>, <em>Instances and Devices</em>, see the following recipes:
<ul>
<li><em>Getting features and properties of a physical device</em></li>
<li><em>Creating a logical device</em></li>
</ul>
</li>
<li>In <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>, see the following recipes:
<ul>
<li><em>Creating a uniform buffer</em></li>
<li><em>Creating a descriptor set layout</em></li>
<li><em>Allocating descriptor sets</em></li>
<li><em>Updating descriptor sets</em></li>
<li><em>Binding descriptor sets</em></li>
</ul>
</li>
<li>In <a href="97217f0d-bed7-4ae1-a543-b4d599f299cf.xhtml">Chapter 7</a>, <em>Shaders</em>, see the <em>Writing geometry shaders</em> recipe</li>
<li>In <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>, see the following recipes:
<ul>
<li><em>Creating a shader module</em></li>
<li><em>Specifying pipeline shader stages</em></li>
<li><em>Specifying a pipeline vertex binding description, attribute description, and input state</em></li>
<li><em>Specifying a pipeline input assembly state</em></li>
<li><em>Creating a pipeline layout</em></li>
<li><em>Specifying graphics pipeline creation parameters</em></li>
<li><em>Creating a graphics pipeline</em></li>
<li><em>Binding a pipeline object</em></li>
</ul>
</li>
<li>In <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml">Chapter 9</a>, <em>Command Recording and Drawing</em>, see the following recipes:
<ul>
<li><em>Binding vertex buffers</em></li>
<li><em>Drawing geometry recipes</em></li>
</ul>
</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Drawing particles using compute and graphics pipelines</h1>
            

            
                
<p>Due to the nature of graphics hardware and the way objects are processed by the graphics pipeline, it is quite hard to display phenomena such as clouds, smoke, sparks, fire, falling rain, and snow. Such effects are usually simulated with particle systems, which are a large number of small sprites that behave according to the algorithms implemented for the system.</p>
<p>Because of the very large number of independent entities, it is convenient to implement the behavior and mutual interactions of particles using compute shaders. Sprites mimicking the look of each particle are usually displayed as billboards with geometry shaders.</p>
<p>In the following example, we can see an image generated with this recipe:</p>
<div><img class=" image-border" height="311" src="img/image_12_006.png" width="417"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">How to do it...</h1>
            

            
                
<ol>
<li>Create a logical device with the <kbd>geometryShader</kbd> feature enabled. Request a queue that supports graphics operations and a queue that supports compute operations (refer to the <em>Getting features and properties of a physical device</em> and <em>Creating a logical device</em> recipes from <a href="d10e8284-6122-4d0a-8f86-ab0bc0bba47e.xhtml">Chapter 1</a>, <em>Instance and Devices</em>).</li>
<li>Generate the initial data (attributes) for a particle system.</li>
<li>Create a buffer that will serve both as a vertex buffer and a storage texel buffer. Copy the generated particle data to the buffer (refer to the <em>Creating a storage texel buffer</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>, and to the <em>Using staging buffer to update buffer with a device-local memory bound</em> recipe from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml">Chapter 4</a>, <em>Resources and Memory</em>).</li>
<li>Create a uniform buffer for two transformation matrices. Update it each time the camera is moved or the window is resized (refer to the <em>Creating a uniform buffer</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
<li>Create two descriptor set layouts: one with a uniform buffer accessed by vertex and geometry stages; and the second with a storage texel buffer accessed by a compute stage. Create a descriptor pool and allocate two descriptor sets using the above layouts. Update them with the uniform buffer and the storage texel buffer (refer to the <em>Creating a descriptor set layout, Creating a descriptor pool</em>, <em>Allocating descriptor sets</em> and <em>Updating descriptor sets</em>, recipes from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
<li>Create a shader module with a compute shader created from the following GLSL code (refer to the <em>Creating a shader module</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>):</li>
</ol>
<pre>
       #version 450 
       layout( local_size_x = 32, local_size_y = 32 ) in; 
       layout( set = 0, binding = 0, rgba32f ) uniform imageBuffer 
       StorageTexelBuffer; 

       layout( push_constant ) uniform TimeState { 
         float DeltaTime; 
       } PushConstant; 

       const uint PARTICLES_COUNT = 2000; 

       void main() { 
         if( gl_GlobalInvocationID.x &lt; PARTICLES_COUNT ) { 
           vec4 position = imageLoad( StorageTexelBuffer, 
           int(gl_GlobalInvocationID.x * 2) ); 
           vec4 color = imageLoad( StorageTexelBuffer, 
           int(gl_GlobalInvocationID.x * 2 + 1) ); 

           vec3 speed = normalize( cross( vec3( 0.0, 1.0, 0.0 ), 
           position.xyz ) ) * color.w; 

           position.xyz += speed * PushConstant.DeltaTime; 

           imageStore( StorageTexelBuffer, int(gl_GlobalInvocationID.x 
           * 
           2), position ); 
         } 
       }
</pre>
<ol start="7">
<li>Create a compute pipeline that uses the shader module with the compute shader and has access to the storage texel buffer and a push constant range with one floating point value (refer to the <em>Specifying pipeline shader stages</em>, <em>Creating a pipeline layout recipe</em>, and <em>Creating a compute pipeline</em> recipes from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Create a graphics pipeline with vertex, geometry, and fragment shaders as described in the <em>Drawing billboards using geometry shaders</em> recipe. The graphics pipeline must fetch two vertex attributes, draw vertices as <kbd>VK_PRIMITIVE_TOPOLOGY_POINT_LIST</kbd> primitives and must have blending enabled (refer to the <em>Specifying pipeline vertex input state</em>, <em>Specifying pipeline input assembly state</em>, <em>Specifying pipeline blend state</em>, and <em>Creating a graphics pipeline</em> recipes from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>To render a frame, record a command buffer that dispatches compute work and submit it to a queue that supports compute operations. Provide a semaphore to be signaled when the queue finishes processing the submitted command buffer (refer to the <em>Providing data to shaders through push</em> <em>constants</em> and <em>Dispatching a compute work recipes</em> from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml">Chapter 9</a>, <em>Command Recording</em>, and the <em>Submitting command buffers to the queue</em> recipe from <a href="fc38e0ae-51aa-4f6f-8fb3-551861273018.xhtml">Chapter 3</a>, <em>Command Buffers and Synchronization</em>).</li>
<li>Also, in each frame record a command buffer that draws billboards as described in the <em>Drawing billboards using geometry shaders</em> recipe. Submit it to the queue that supports graphics operations. During submission, provide a semaphore, which is signaled by the compute queue. Provide it as a wait semaphore (refer to the <em>Synchronizing two command buffers</em> recipe from <a href="fc38e0ae-51aa-4f6f-8fb3-551861273018.xhtml">Chapter 3</a>, <em>Command Buffers and Synchronization</em>).</li>
</ol>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">How it works...</h1>
            

            
                
<p>Drawing particle systems can be divided in two steps:</p>
<ul>
<li>We calculate and update positions of all particles with compute shaders</li>
<li>We draw particles in updated positions using graphics pipeline with vertex, geometry, and fragment shaders</li>
</ul>
<p>To prepare a particle system, we need to think about the data needed to calculate positions and draw all particles. In this example we will use three parameters: position, speed and color. Each set of these parameters will be accessed by a vertex shader through a vertex buffer, and the same data will be read in a compute shader. A simple and convenient way to access a very large number of entries in stages other than the vertex shader is to use a texel buffer. As we want to both read and store data, we will need a storage texel buffer. It allows us to fetch data from a buffer treated as a 1-dimensional image (refer to the <em>Creating a storage texel buffer</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</p>
<p>First, we need to generate initial data for our particle system. For the data available in a storage texel buffer to be properly read, it must be stored according to a selected format. Storage texel buffers have a limited set of formats that are mandatory, so we need to pack the parameters of our particles to one of them. Positions and colors require at least three values each. In our example, particles will move around the center of the whole system, so the velocity can be easily calculated based on the particle's current position. We just need to differentiate the speed of particles. For this purpose one value for scaling our velocity vector is enough.</p>
<p>So we end up with seven values. We will pack them into two RGBA vectors of floating-point values. First we have three <kbd>X</kbd>, <kbd>Y</kbd>, <kbd>Z</kbd> components of a position attribute. The next value is unused in our particle system, but for the data to be correctly read, it needs to be included. We will store a <kbd>1.0f</kbd> value as a fourth component of the position attribute. After that there are <kbd>R</kbd>, <kbd>G</kbd>, <kbd>B</kbd> values for a color, and a value scaling the speed vector of a particle. We randomly generate all values and store them in a vector:</p>
<pre>
std::vector&lt;float&gt; particles; 

for( uint32_t i = 0; i &lt; PARTICLES_COUNT; ++i ) { 
  Vector3 position = /* generate position */; 
  Vector3 color = /* generate color */; 
  float speed = /* generate speed scale */; 
  particles.insert( particles.end(), position.begin(), position.end() ); 
  particles.push_back( 1.0f ); 
  particles.insert( particles.end(), color.begin(), color.end() ); 
  particles.push_back( speed ); 
}
</pre>
<p>The generated data is copied to the buffer. We create a buffer that will serve both as a vertex buffer during rendering and as a storage texel buffer during position calculations:</p>
<pre>
InitVkDestroyer( LogicalDevice, VertexBuffer ); 
InitVkDestroyer( LogicalDevice, VertexBufferMemory ); 
InitVkDestroyer( LogicalDevice, VertexBufferView ); 
if( !CreateStorageTexelBuffer( PhysicalDevice, *LogicalDevice, VK_FORMAT_R32G32B32A32_SFLOAT, sizeof( particles[0] ) * particles.size(), 
  VK_BUFFER_USAGE_TRANSFER_DST_BIT | VK_BUFFER_USAGE_VERTEX_BUFFER_BIT | VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT, false, 
  *VertexBuffer, *VertexBufferMemory, *VertexBufferView ) ) { 
  return false; 
} 

if( !UseStagingBufferToUpdateBufferWithDeviceLocalMemoryBound( PhysicalDevice, *LogicalDevice, sizeof( particles[0] ) * particles.size(), 
  &amp;particles[0], *VertexBuffer, 0, 0, VK_ACCESS_TRANSFER_WRITE_BIT, VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, VK_PIPELINE_STAGE_VERTEX_INPUT_BIT, 
  GraphicsQueue.Handle, FrameResources.front().CommandBuffer, {} ) ) { 
  return false; 
}
</pre>
<p>Additionally, we need a uniform buffer, through which we will provide transformation matrices. A uniform buffer along the storage texel buffer will be provided to shaders through descriptor sets. Here we will have two separate sets. In the first set, we will have only a uniform buffer accessed by vertex and geometry shaders. The second descriptor set is used in a compute shader to access storage texel buffer. For this purpose we need two separate descriptor set layouts:</p>
<pre>
std::vector&lt;VkDescriptorSetLayoutBinding&gt; descriptor_set_layout_bindings = { 
  { 
    0, 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1, 
    VK_SHADER_STAGE_VERTEX_BIT | VK_SHADER_STAGE_GEOMETRY_BIT, 
    nullptr 
  }, 
  { 
    0, 
    VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER, 
    1, 
    VK_SHADER_STAGE_COMPUTE_BIT, 
    nullptr 
  } 
}; 

DescriptorSetLayout.resize( 2 ); 
InitVkDestroyer( LogicalDevice, DescriptorSetLayout[0] ); 
InitVkDestroyer( LogicalDevice, DescriptorSetLayout[1] ); 
if( !CreateDescriptorSetLayout( *LogicalDevice, { descriptor_set_layout_bindings[0] }, *DescriptorSetLayout[0] ) ) { 
  return false; 
} 
if( !CreateDescriptorSetLayout( *LogicalDevice, { descriptor_set_layout_bindings[1] }, *DescriptorSetLayout[1] ) ) { 
  return false; 
}
</pre>
<p>Next, we need a pool from which we can allocate two descriptor sets:</p>
<pre>
std::vector&lt;VkDescriptorPoolSize&gt; descriptor_pool_sizes = { 
  { 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1 
  }, 
  { 
    VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER, 
    1 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorPool ); 
if( !CreateDescriptorPool( *LogicalDevice, false, 2, descriptor_pool_sizes, *DescriptorPool ) ) { 
  return false; 
}
</pre>
<p>After that, we can allocate two descriptors sets and update them with the created buffer and buffer view:</p>
<pre>
if( !AllocateDescriptorSets( *LogicalDevice, *DescriptorPool, { *DescriptorSetLayout[0], *DescriptorSetLayout[1] }, DescriptorSets ) ) { 
  return false; 
} 

BufferDescriptorInfo buffer_descriptor_update = { 
  DescriptorSets[0], 
  0, 
  0, 
  VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
  { 
    { 
      *UniformBuffer, 
      0, 
      VK_WHOLE_SIZE 
    } 
  } 
}; 

TexelBufferDescriptorInfo storage_texel_buffer_descriptor_update = { 
  DescriptorSets[1], 
  0, 
  0, 
  VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER, 
  { 
    { 
      *VertexBufferView 
    } 
  } 
}; 

UpdateDescriptorSets( *LogicalDevice, {}, { buffer_descriptor_update }, { storage_texel_buffer_descriptor_update }, {} );
</pre>
<p>The next important step is the creation of graphics and compute pipelines. When a movement is involved, calculations must be performed based on real-time values, as we usually cannot rely on fixed time intervals. So the compute shader must have access to a value of time that has elapsed since the last frame. Such a value may be provided through a push constant range. We can see the code required to create the compute pipeline here:</p>
<pre>
std::vector&lt;unsigned char&gt; compute_shader_spirv; 
if( !GetBinaryFileContents( "Data/Shaders/Recipes/12 Advanced Rendering Techniques/03 Drawing particles using compute and graphics pipelines/shader.comp.spv", compute_shader_spirv ) ) { 
  return false; 
} 

VkDestroyer&lt;VkShaderModule&gt; compute_shader_module( LogicalDevice ); 
if( !CreateShaderModule( *LogicalDevice, compute_shader_spirv, *compute_shader_module ) ) { 
  return false; 
} 
std::vector&lt;ShaderStageParameters&gt; compute_shader_stage_params = { 
  { 
    VK_SHADER_STAGE_COMPUTE_BIT, 
    *compute_shader_module, 
    "main", 
    nullptr 
  } 
}; 
std::vector&lt;VkPipelineShaderStageCreateInfo&gt; compute_shader_stage_create_infos; 
SpecifyPipelineShaderStages( compute_shader_stage_params, compute_shader_stage_create_infos ); 
VkPushConstantRange push_constant_range = { 
  VK_SHADER_STAGE_COMPUTE_BIT, 
  0, 
  sizeof( float ) 
}; 

InitVkDestroyer( LogicalDevice, ComputePipelineLayout ); 
if( !CreatePipelineLayout( *LogicalDevice, { *DescriptorSetLayout[1] }, { push_constant_range }, *ComputePipelineLayout ) ) { 
  return false; 
} 

InitVkDestroyer( LogicalDevice, ComputePipeline ); 
if( !CreateComputePipeline( *LogicalDevice, 0, compute_shader_stage_create_infos[0], *ComputePipelineLayout, VK_NULL_HANDLE, VK_NULL_HANDLE, *ComputePipeline ) ) { 
  return false; 
}
</pre>
<p>Compute shaders read data from the storage texel buffer defined as follows:</p>
<pre>
layout( set = 0, binding = 0, rgba32f ) uniform imageBuffer StorageTexelBuffer;
</pre>
<p>Data from the storage texel buffer is read using the <kbd>imageLoad()</kbd> function:</p>
<pre>
vec4 position = imageLoad( StorageTexelBuffer, int(gl_GlobalInvocationID.x * 2) ); 
vec4 color = imageLoad( StorageTexelBuffer, int(gl_GlobalInvocationID.x * 2 + 1) );
</pre>
<p>We read two values so we need two <kbd>imageLoad()</kbd> calls, because each such operation returns one element of a format defined for the buffer (in this case, a 4-component vector of floats). We access the buffer based on a unique value of a current compute shader instance.</p>
<p>Next, we perform calculations and update the positions of the vertices. Calculations are performed so the particles move around the center of the scene based on the position and an up vector. A new vector (speed) is calculated using the <kbd>cross()</kbd> function:</p>
<div><img class=" image-border" src="img/image_12_007.png"/></div>
<p>This calculated speed vector is added to the fetched position and the result is stored in the same buffer using the <kbd>imageStore()</kbd> function:</p>
<pre>
imageStore( StorageTexelBuffer, int(gl_GlobalInvocationID.x * 2), position );
</pre>
<p>We don't update a color or speed, so we store only one value.</p>
<p>Because we access the data of only one particle, we can read values from and store values in the same buffer. In more complicated scenarios, such as when there are interactions between particles, we can't use the same buffer. The order in which compute shader invocations are executed is unknown, so we would end up with some invocations accessing unmodified values, but others would read data that has already been updated. This would impact the accuracy of performed calculations and probably result in an unpredictable system.</p>
<p>Graphics pipeline creation is very similar to the one presented in the <em>Drawing billboards using geometry shaders</em> recipe. The difference is that it fetches two attributes instead of one:</p>
<pre>
std::vector&lt;VkVertexInputBindingDescription&gt; vertex_input_binding_descriptions = { 
  { 
    0,    VK_VERTEX_INPUT_RATE_VERTEX 
  } 
}; 

std::vector&lt;VkVertexInputAttributeDescription&gt; vertex_attribute_descriptions = { 
  { 
    0, 
    0, 
    VK_FORMAT_R32G32B32A32_SFLOAT, 
    0 
  }, 
  { 
    1, 
    0, 
    VK_FORMAT_R32G32B32A32_SFLOAT, 
    4 * sizeof( float ) 
  } 
}; 

VkPipelineVertexInputStateCreateInfo vertex_input_state_create_info; 
SpecifyPipelineVertexInputState( vertex_input_binding_descriptions, vertex_attribute_descriptions, vertex_input_state_create_info );
</pre>
<p>We also render vertices as point primitives:</p>
<pre>
VkPipelineInputAssemblyStateCreateInfo input_assembly_state_create_info; 
SpecifyPipelineInputAssemblyState( VK_PRIMITIVE_TOPOLOGY_POINT_LIST, false, input_assembly_state_create_info );
</pre>
<p>One last difference is that here we enable additive blending, so the particles look like they are glowing:</p>
<pre>
std::vector&lt;VkPipelineColorBlendAttachmentState&gt; attachment_blend_states = { 
  { 
    true, 
    VK_BLEND_FACTOR_SRC_ALPHA, 
    VK_BLEND_FACTOR_ONE, 
    VK_BLEND_OP_ADD, 
    VK_BLEND_FACTOR_ONE, 
    VK_BLEND_FACTOR_ONE, 
    VK_BLEND_OP_ADD, 
    VK_COLOR_COMPONENT_R_BIT | 
    VK_COLOR_COMPONENT_G_BIT | 
    VK_COLOR_COMPONENT_B_BIT | 
    VK_COLOR_COMPONENT_A_BIT 
  } 
}; 
VkPipelineColorBlendStateCreateInfo blend_state_create_info; 
SpecifyPipelineBlendState( false, VK_LOGIC_OP_COPY, attachment_blend_states, { 1.0f, 1.0f, 1.0f, 1.0f }, blend_state_create_info );
</pre>
<p>The drawing process is also divided into two steps. First, we record a command buffer that dispatches compute work. Some hardware platforms may have a queue family that is dedicated to math calculations, so it may be preferable to submit command buffers with compute shaders to that queue:</p>
<pre>
if( !BeginCommandBufferRecordingOperation( ComputeCommandBuffer, VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT, nullptr ) ) { 
  return false; 
} 

BindDescriptorSets( ComputeCommandBuffer, VK_PIPELINE_BIND_POINT_COMPUTE, *ComputePipelineLayout, 0, { DescriptorSets[1] }, {} ); 

BindPipelineObject( ComputeCommandBuffer, VK_PIPELINE_BIND_POINT_COMPUTE, *ComputePipeline ); 

float time = TimerState.GetDeltaTime(); 
ProvideDataToShadersThroughPushConstants( ComputeCommandBuffer, *ComputePipelineLayout, VK_SHADER_STAGE_COMPUTE_BIT, 0, sizeof( float ), &amp;time ); 

DispatchComputeWork( ComputeCommandBuffer, PARTICLES_COUNT / 32 + 1, 1, 1 ); 

if( !EndCommandBufferRecordingOperation( ComputeCommandBuffer ) ) { 
  return false; 
} 

if( !SubmitCommandBuffersToQueue( ComputeQueue.Handle, {}, { ComputeCommandBuffer }, { *ComputeSemaphore }, *ComputeFence ) ) { 
  return false; 
}
</pre>
<p>Drawing is performed in the normal way. We just need to synchronize the graphics queue with a compute queue. We do this by providing an additional wait semaphore when we submit a command buffer to the graphics queue. This semaphore must be signaled by a compute queue when it finishes processing the submitted command buffer in which the<br/>
compute shaders are dispatched.<br/></p>
<p>The following sample images show the same particle system rendered with different numbers of particles:</p>
<div><img class=" image-border" height="502" src="img/image_12_008.png" width="617"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">See also</h1>
            

            
                
<ul>
<li>In <a href="d10e8284-6122-4d0a-8f86-ab0bc0bba47e.xhtml">Chapter 1</a>, <em>Instances and Devices</em>, see the Getting features and properties of a physical device recipe</li>
<li>In <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>, see the following recipes:
<ul>
<li><em>Creating a storage texel buffer</em></li>
<li><em>Creating a descriptor set layout</em></li>
<li><em>Creating a descriptor pool</em></li>
<li><em>Allocating descriptor sets</em></li>
<li><em>Updating descriptor sets</em></li>
</ul>
</li>
<li>In <a href="97217f0d-bed7-4ae1-a543-b4d599f299cf.xhtml">Chapter 7</a>, <em>Shaders</em>, see the Writing compute shaders recipe</li>
<li>In <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>, see the following recipes:
<ul>
<li><em>Creating a shader module</em></li>
<li><em>Creating a compute pipeline</em></li>
<li><em>Creating a graphics pipeline</em></li>
</ul>
</li>
<li>In <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml">Chapter 9</a>, <em>Command Recording and Drawing</em>, see the following recipes:
<ul>
<li><em>Providing data to shaders through push constants</em></li>
<li><em>Drawing a geometry</em></li>
<li><em>Dispatching a compute work</em></li>
</ul>
</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Rendering a tessellated terrain</h1>
            

            
                
<p>3D scenes with open worlds and long rendering distances usually also contain vast terrains. Drawing ground is a very complex topic and can be performed in many different ways. Terrain in a distance cannot be too complex, as it will take up too much memory and processing power to display it. On the other hand, the area near the player must be detailed enough to look convincing and natural. That's why we need a way to lower the number of details with increasing distance or to increase the terrain's fidelity near the camera.</p>
<p>This is an example of how the tessellation shaders can be used to achieve high quality rendered images. For a terrain, we can use a flat plane with low number of vertices. Using tessellation shaders, we can increase the number of primitives of the ground near the camera. We can then offset generated vertices by the desired amount to increase or decrease the height of a terrain.</p>
<p>The following screenshot is an example of an image generated using this recipe:</p>
<div><img class=" image-border" src="img/image_12_009.png"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Getting ready</h1>
            

            
                
<p>Drawing a terrain usually requires the preparation of height data. This can be generated on the fly, procedurally, according to some desired formulae. However, it can also be prepared earlier in the form of a texture called a height map. It contains information about the terrain's height above (or below) a specified altitude, in which a lighter color indicates a greater height and a darker color indicates a lower height. An example of such a height map can be seen in the following image:</p>
<div><img class=" image-border" src="img/image_12_010.png"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">How to do it...</h1>
            

            
                
<ol>
<li>Load or generate a model of a flat, horizontally-aligned plane. Two attributes--position and texture coordinate--will be needed. Upload the vertex data to a vertex buffer (refer to the <em>Loading a 3D model from an OBJ file</em> recipe from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml">Chapter 10</a>, <em>Helper Recipes</em> and to the <em>Creating a buffer</em> and <em>Using staging buffer to update buffer with a device-local memory bound</em> recipes from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml">Chapter 4</a>, <em>Resources and Memory</em>).</li>
<li>Create a uniform buffer for two transformation matrices (refer to the <em>Creating a uniform buffer</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
<li>Load height information from an image file (refer to the <em>Loading texture data from a file</em> recipe from <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml">Chapter 10</a>, <em>Helper Recipes</em>). Create a combined image sampler and copy the loaded height data to image's memory (refer to the <em>Creating a combined image sampler</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em> and to the <em>Using the staging buffer to update an image with a device-local memory bound</em> recipe from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml">Chapter 4</a>, <em>Resources and Memory</em>).</li>
<li>Create a descriptor set layout with one uniform buffer accessed by tessellation control and geometry stages and one combined image sampler accessed by tessellation control and evaluation stages (refer to the <em>Creating a descriptor set layout</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>). Allocate a descriptor set using the prepared layout. Update it with the created uniform buffer and sampler and image view handles (refer to the <em>Allocating descriptor sets</em> and <em>Updating descriptor sets</em> recipes from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
<li>Create a shader module with a SPIR-V assembly for a vertex shader created from the following GLSL code (refer to the <em>Creating a shader module</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>):</li>
</ol>
<pre>
       #version 450 
       layout( location = 0 ) in vec4 app_position; 
       layout( location = 1 ) in vec2 app_texcoord; 
       layout( location = 0 ) out vec2 vert_texcoord; 

       void main() { 
         gl_Position = app_position; 
         vert_texcoord = app_texcoord; 
       }
</pre>
<ol start="6">
<li>Create a shader module for a tessellation control stage. Use the following GLSL code to generate a SPIR-V assembly from:</li>
</ol>
<pre>
       #version 450 
       layout( location = 0 ) in vec2 vert_texcoord[]; 
       layout( set = 0, binding = 0 ) uniform UniformBuffer { 
        mat4 ModelViewMatrix; 
        mat4 ProjectionMatrix; 
       }; 

       layout( set = 0, binding = 1 ) uniform sampler2D ImageSampler; 
       layout( vertices = 3 ) out; 
       layout( location = 0 ) out vec2 tesc_texcoord[]; 

       void main() { 
         if( 0 == gl_InvocationID ) { 
           float distances[3]; 
           float factors[3]; 

           for( int i = 0; i &lt; 3; ++i ) { 
             float height = texture( ImageSampler, vert_texcoord[i] 
             ).x; 
             vec4 position = ModelViewMatrix * (gl_in[i].gl_Position + 
             vec4( 0.0, height, 0.0, 0.0 )); 
             distances[i] = dot( position, position ); 
           } 
           factors[0] = min( distances[1], distances[2] ); 
           factors[1] = min( distances[2], distances[0] ); 
           factors[2] = min( distances[0], distances[1] ); 

           gl_TessLevelInner[0] = max( 1.0, 20.0 - factors[0] ); 
           gl_TessLevelOuter[0] = max( 1.0, 20.0 - factors[0] ); 
           gl_TessLevelOuter[1] = max( 1.0, 20.0 - factors[1] ); 
           gl_TessLevelOuter[2] = max( 1.0, 20.0 - factors[2] ); 
         } 
         gl_out[gl_InvocationID].gl_Position = 
         gl_in[gl_InvocationID].gl_Position; 
         tesc_texcoord[gl_InvocationID] = 
         vert_texcoord[gl_InvocationID]; 
       }
</pre>
<ol start="7">
<li>Create a shader module for a tessellation evaluation shader created from the following GLSL code:</li>
</ol>
<pre>
       #version 450 
       layout( triangles, fractional_even_spacing, cw ) in; 
       layout( location = 0 ) in vec2 tesc_texcoord[]; 
       layout( set = 0, binding = 1 ) uniform sampler2D HeightMap; 
       layout( location = 0 ) out float tese_height; 
       void main() { 
         vec4 position = gl_in[0].gl_Position * gl_TessCoord.x + 
                         gl_in[1].gl_Position * gl_TessCoord.y + 
                         gl_in[2].gl_Position * gl_TessCoord.z; 
         vec2 texcoord = tesc_texcoord[0] * gl_TessCoord.x + 
                         tesc_texcoord[1] * gl_TessCoord.y + 
                         tesc_texcoord[2] * gl_TessCoord.z; 
         float height = texture( HeightMap, texcoord ).x; 
         position.y += height; 
         gl_Position = position; 
         tese_height = height; 
       }
</pre>
<ol start="8">
<li>Create a shader module for a geometry shader and use the following GLSL code:</li>
</ol>
<pre>
       #version 450 

       layout( triangles ) in; 
       layout( location = 0 ) in float tese_height[]; 

       layout( set = 0, binding = 0 ) uniform UniformBuffer { 
         mat4 ModelViewMatrix; 
         mat4 ProjectionMatrix; 
       }; 
       layout( triangle_strip, max_vertices = 3 ) out; 
       layout( location = 0 ) out vec3  geom_normal; 
       layout( location = 1 ) out float geom_height; 

       void main() { 
         vec3 v0v1 = gl_in[1].gl_Position.xyz - 
         gl_in[0].gl_Position.xyz; 
         vec3 v0v2 = gl_in[2].gl_Position.xyz - 
         gl_in[0].gl_Position.xyz; 
         vec3 normal = normalize( cross( v0v1, v0v2 ) ); 

         for( int vertex = 0; vertex &lt; 3; ++vertex ) { 
           gl_Position = ProjectionMatrix * ModelViewMatrix * 
           gl_in[vertex].gl_Position; 
           geom_height = tese_height[vertex]; 
           geom_normal = normal; 
           EmitVertex(); 
         } 

         EndPrimitive(); 
       }
</pre>
<ol start="9">
<li>Create a shader module that contains a source code of a fragment shader. Generate a SPIR-V assembly from the following GLSL code:</li>
</ol>
<pre>
       #version 450 
       layout( location = 0 ) in vec3  geom_normal; 
       layout( location = 1 ) in float geom_height; 
       layout( location = 0 ) out vec4 frag_color; 

       void main() { 
         const vec4 green = vec4( 0.2, 0.5, 0.1, 1.0 ); 
         const vec4 brown = vec4( 0.6, 0.5, 0.3, 1.0 ); 
         const vec4 white = vec4( 1.0 ); 
         vec4 color = mix( green, brown, smoothstep( 0.0, 0.4, 
         geom_height ) ); 
         color = mix( color, white, smoothstep( 0.6, 0.9, geom_height ) 
         ); 

         float diffuse_light = max( 0.0, dot( geom_normal, vec3( 0.58, 
         0.58, 0.58 ) ) ); 
         frag_color = vec4( 0.05, 0.05, 0.0, 0.0 ) + diffuse_light * 
         color; 
       }
</pre>
<ol start="10">
<li>Create a graphics pipeline using the above five shader modules. The pipeline should fetch two vertex attributes: a 3-component position and a 2-component texture coordinate. It must use <kbd>VK_PRIMITIVE_TOPOLOGY_PATCH_LIST</kbd> primitives. A patch should consist of three control points (refer to the <em>Specifying pipeline input assembly state</em>, <em>Specifying pipeline tessellation state</em>, <em>Specifying graphics pipeline creation parameters</em>, and <em>Creating a graphics pipeline</em> recipes from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Create the remaining resources and draw the geometry (refer to the <em>Rendering a geometry with a vertex diffuse lighting</em> recipe from <a href="45108a92-6d49-4759-9495-3f1166e69128.xhtml">Chapter 11</a>, <em>Lighting</em>).</li>
</ol>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">How it works...</h1>
            

            
                
<p>We start the process of drawing a terrain by loading a model of a flat plane. It may be a simple quad with a little bit more than four vertices. Generating too many vertices in a tessellation stage may be too expensive performance-wise, so we need to find a balance between the complexity of a base geometry and the tessellation factors. We can see a plane used as a base for the tessellated terrain in the following image:</p>
<div><img class=" image-border" height="504" src="img/image_12_011.png" width="500"/></div>
<p>In this example, we will load height information from a texture. We do this in the same way as we load data from files. Then we create a combined image sampler and upload loaded data to its memory:</p>
<pre>
int width = 1; 
int height = 1; 
std::vector&lt;unsigned char&gt; image_data; 
if( !LoadTextureDataFromFile( "Data/Textures/heightmap.png", 4, image_data, &amp;width, &amp;height ) ) { 
  return false; 
} 

InitVkDestroyer( LogicalDevice, HeightSampler ); 
InitVkDestroyer( LogicalDevice, HeightMap ); 
InitVkDestroyer( LogicalDevice, HeightMapMemory ); 
InitVkDestroyer( LogicalDevice, HeightMapView ); 
if( !CreateCombinedImageSampler( PhysicalDevice, *LogicalDevice, VK_IMAGE_TYPE_2D, VK_FORMAT_R8G8B8A8_UNORM, { (uint32_t)width, (uint32_t)height, 1 }, 
  1, 1, VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT, VK_IMAGE_VIEW_TYPE_2D, VK_IMAGE_ASPECT_COLOR_BIT, VK_FILTER_LINEAR, 
  VK_FILTER_LINEAR, VK_SAMPLER_MIPMAP_MODE_NEAREST, VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, 
  VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, 0.0f, false, 1.0f, false, VK_COMPARE_OP_ALWAYS, 0.0f, 1.0f, VK_BORDER_COLOR_FLOAT_OPAQUE_BLACK, 
  false, *HeightSampler, *HeightMap, *HeightMapMemory, *HeightMapView ) ) { 
  return false; 
} 

VkImageSubresourceLayers image_subresource_layer = { 
  VK_IMAGE_ASPECT_COLOR_BIT, 
  0, 
  0, 
  1 
}; 
if( !UseStagingBufferToUpdateImageWithDeviceLocalMemoryBound( PhysicalDevice, *LogicalDevice, static_cast&lt;VkDeviceSize&gt;(image_data.size()), 
&amp;image_data[0], *HeightMap, image_subresource_layer, { 0, 0, 0 }, { (uint32_t)width, (uint32_t)height, 1 }, VK_IMAGE_LAYOUT_UNDEFINED, 
VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL, 0, VK_ACCESS_SHADER_READ_BIT, VK_IMAGE_ASPECT_COLOR_BIT, VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, 
VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, GraphicsQueue.Handle, FrameResources.front().CommandBuffer, {} ) ) { 
return false; 
}
</pre>
<p>A uniform buffer with transformation matrices is also required, so the vertices can be transformed from local space to a view space and to the clip space:</p>
<pre>
InitVkDestroyer( LogicalDevice, UniformBuffer ); 
InitVkDestroyer( LogicalDevice, UniformBufferMemory ); 
if( !CreateUniformBuffer( PhysicalDevice, *LogicalDevice, 2 * 16 * sizeof( float ), VK_BUFFER_USAGE_TRANSFER_DST_BIT | VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT, 
  *UniformBuffer, *UniformBufferMemory ) ) { 
  return false; 
} 

if( !UpdateStagingBuffer( true ) ) { 
  return false; 
}
</pre>
<p>The next step is to create a descriptor set for the uniform buffer and the combined image sampler. A uniform buffer is accessed in the tessellation control and geometry stages. Height information is read in the tessellation control and evaluation stages:</p>
<pre>
std::vector&lt;VkDescriptorSetLayoutBinding&gt; descriptor_set_layout_bindings = { 
  { 
    0, 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1, 
    VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT | VK_SHADER_STAGE_GEOMETRY_BIT, 
    nullptr 
  }, 
  { 
    1, 
    VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
    1, 
    VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT | VK_SHADER_STAGE_TESSELLATION_EVALUATION_BIT, 
    nullptr 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorSetLayout ); 
if( !CreateDescriptorSetLayout( *LogicalDevice, descriptor_set_layout_bindings, *DescriptorSetLayout ) ) { 
  return false; 
} 

std::vector&lt;VkDescriptorPoolSize&gt; descriptor_pool_sizes = { 
  { 
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
    1 
  }, 
  { 
    VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
    2 
  } 
}; 
InitVkDestroyer( LogicalDevice, DescriptorPool ); 
if( !CreateDescriptorPool( *LogicalDevice, false, 1, descriptor_pool_sizes, *DescriptorPool ) ) { 
  return false; 
} 

if( !AllocateDescriptorSets( *LogicalDevice, *DescriptorPool, { *DescriptorSetLayout }, DescriptorSets ) ) { 
  return false; 
}
</pre>
<p>Next, we can update the descriptor set with the uniform buffer handle and with sampler and image view handles as they don't change during the lifetime of our application (that is, we don't need to recreate them when the window size is modified).</p>
<pre>
BufferDescriptorInfo buffer_descriptor_update = { 
  DescriptorSets[0], 
  0, 
  0, 
  VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 
  { 
    { 
      *UniformBuffer, 
      0, 
      VK_WHOLE_SIZE 
    } 
  } 
}; 

std::vector&lt;ImageDescriptorInfo&gt; image_descriptor_updates = { 
  { 
    DescriptorSets[0], 
    1, 
    0, 
    VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
    { 
      { 
        *HeightSampler, 
        *HeightMapView, 
        VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL 
      } 
    } 
  } 
}; 

UpdateDescriptorSets( *LogicalDevice, image_descriptor_updates, { buffer_descriptor_update }, {}, {} );
</pre>
<p>The next step is to create a graphics pipeline. This time we have a very complex pipeline with all five programmable graphics stages enabled:</p>
<pre>
std::vector&lt;ShaderStageParameters&gt; shader_stage_params = { 
  { 
    VK_SHADER_STAGE_VERTEX_BIT, 
    *vertex_shader_module, 
    "main", 
    nullptr 
  }, 
  { 
    VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT, 
    *tessellation_control_shader_module, 
    "main", 
    nullptr 
},
  { 
    VK_SHADER_STAGE_TESSELLATION_EVALUATION_BIT, 
    *tessellation_evaluation_shader_module, 
    "main", 
    nullptr 
  }, 
  { 
    VK_SHADER_STAGE_GEOMETRY_BIT, 
    *geometry_shader_module, 
    "main", 
    nullptr 
  }, 
  { 
    VK_SHADER_STAGE_FRAGMENT_BIT, 
    *fragment_shader_module, 
    "main", 
    nullptr 
  } 

}; 

std::vector&lt;VkPipelineShaderStageCreateInfo&gt; shader_stage_create_infos; 
SpecifyPipelineShaderStages( shader_stage_params, shader_stage_create_infos );
</pre>
<p>Why do we need all five stages? A vertex shader is always required. This time it only reads two input attributes (position and texcoord) and passes it further down the pipeline.</p>
<p>When tessellation is enabled, we need both the control and evaluation shader stages. The tessellation control shader, as the name suggests, controls the tessellation level of processed patches (the amount of generated vertices). In this recipe, we generate vertices based on the distance from the camera: the closer the vertices of a patch are to the camera, the more vertices are generated by the tessellator. This way, the terrain in the distance is simple and doesn't take much processing power to be rendered; but, the closer to the camera, the more complex the terrain becomes.</p>
<p>We can't choose one tessellation level for the whole patch (in this case a triangle). When two neighboring triangles are tessellated with different factors, different number of vertices will be generated on their common edge. Vertices from each triangle will be placed in different locations and they will be offset by different values. This will create holes in our ground:</p>
<div><img class=" image-border" src="img/image_12_012.png"/></div>
<p>In the preceding image we see two triangles: the left formed from vertices L0-L1-L7, and right formed from vertices R0-R1-R4. The other vertices are generated by the tessellator. Triangles share an edge: L1-L7 or R1-R4 (points L1 and R4 indicate the same vertex; similarly points L7 and R1 indicate the same vertex); but the edge is tessellated with different factors. This causes discontinuities (indicated by stripes) in the surface formed by the two triangles.</p>
<p>To avoid this problem, we need to calculate a tessellation factor for each triangle edge in such a way that it is fixed across triangles that share the same edge. In this example, we will calculate tessellation factors based on the distance of a vertex from the camera. We will do this for all vertices in a triangle. Then, for a given triangle edge, we will choose a greater tessellation factor that was calculated from one of the edge's vertices:</p>
<pre>
float distances[3]; 
float factors[3]; 

for( int i = 0; i &lt; 3; ++i ) { 
  float height = texture( ImageSampler, vert_texcoord[i] ).x; 
  vec4 position = ModelViewMatrix * (gl_in[i].gl_Position + vec4( 0.0, 
  height, 0.0, 0.0 )); 
  distances[i] = dot( position, position ); 
} 
factors[0] = min( distances[1], distances[2] ); 
factors[1] = min( distances[2], distances[0] ); 
factors[2] = min( distances[0], distances[1] ); 

gl_TessLevelInner[0] = max( 1.0, 20.0 - factors[0] ); 
gl_TessLevelOuter[0] = max( 1.0, 20.0 - factors[0] ); 
gl_TessLevelOuter[1] = max( 1.0, 20.0 - factors[1] ); 
gl_TessLevelOuter[2] = max( 1.0, 20.0 - factors[2] );
</pre>
<p>In the preceding tessellation control shader code, we calculate a distance (squared) from all vertices to the camera. We need to offset positions by the amount read from the height map, so the whole patch is in the correct place and the distance is properly calculated.</p>
<p>Next, for all triangle edges, we take the smaller distance of edge's two vertices. As we want a tessellation factor to increase with decreasing distance, we need to invert the calculated factor. Here we take a hardcoded value of <kbd>20</kbd> and subtract a chosen distance value. As we don't want the tessellation factor to be smaller than <kbd>1.0</kbd>, we perform additional clamping.</p>
<p>The tessellation factor calculated like this exaggerates the effect of decreasing the number of generated vertices with increasing distance. This is done on purpose so that we can see how triangles are tessellated and how the number of details increases near the camera. However, in real-life examples we should prepare such a formula so that the effect is barely visible.</p>
<p>Next, a tessellation evaluation shader takes the weights of generated vertices to calculate a valid position of the new vertices. We do the same for texture coordinates, as we need to load height information from the height map:</p>
<pre>
vec4 position = gl_in[0].gl_Position * gl_TessCoord.x + 
                gl_in[1].gl_Position * gl_TessCoord.y + 
                gl_in[2].gl_Position * gl_TessCoord.z; 

vec2 texcoord = tesc_texcoord[0] * gl_TessCoord.x + 
                tesc_texcoord[1] * gl_TessCoord.y + 
                tesc_texcoord[2] * gl_TessCoord.z;
</pre>
<p>After the position of a new vertex is calculated, we need to offset it, so the vertex is placed at an appropriate height:</p>
<pre>
float height = texture( HeightMap, texcoord ).x; 
position.y += height; 
gl_Position = position;
</pre>
<p>The tessellation evaluation shader stage is followed by the geometry shader stage. We can omit it but here we use it to calculate the normal vector of the generated triangle. We take one normal vector for all the triangle's vertices, so we will perform a flat shading in this sample.</p>
<p>The normal vector is calculated with the <kbd>cross()</kbd> function, which takes two vectors and returns a vector that is perpendicular to those provided. We provide vectors forming two edges of a triangle:</p>
<pre>
vec3 v0v1 = gl_in[1].gl_Position.xyz - gl_in[0].gl_Position.xyz; 
vec3 v0v2 = gl_in[2].gl_Position.xyz - gl_in[0].gl_Position.xyz; 
vec3 normal = normalize( cross( v0v1, v0v2 ) );
</pre>
<p>Finally, the geometry shader calculates the clip space positions of all vertices and emits them:</p>
<pre>
for( int vertex = 0; vertex &lt; 3; ++vertex ) { 
  gl_Position = ProjectionMatrix * ModelViewMatrix * gl_in[vertex].gl_Position; 
  geom_height = tese_height[vertex]; 
  geom_normal = normal; 
  EmitVertex(); 
} 

EndPrimitive();
</pre>
<p>To simplify the recipe, a fragment shader is also simple. It mixes three colors based on the height above ground: green for grass in the lower parts, grey/brown for rocks in the middle, and white for snow in mountain tops. It also performs simple lighting calculations using the diffuse/Lambert light model.</p>
<p>The preceding shaders form a graphics pipeline used to draw a tessellated terrain. During pipeline creation we must remember to think about primitive topology. Because of the enabled tessellation stages, we need to use a <kbd>VK_PRIMITIVE_TOPOLOGY_PATCH_LIST</kbd> topology. We also need to provide a tessellation state during pipeline creation. As we want to operate on triangles, we specify that a patch contains three control points:</p>
<pre>
VkPipelineInputAssemblyStateCreateInfo input_assembly_state_create_info; 
SpecifyPipelineInputAssemblyState( VK_PRIMITIVE_TOPOLOGY_PATCH_LIST, false, input_assembly_state_create_info ); 
VkPipelineTessellationStateCreateInfo tessellation_state_create_info; 
SpecifyPipelineTessellationState( 3, tessellation_state_create_info );
</pre>
<p>The remaining parameters used for pipeline creation are defined in the usual way. We also don't need to do anything special during rendering. We just draw a plane with the preceding graphics pipeline bound, and we should see a geometry resembling a terrain. We can see examples of results generated with this recipe in the following images:</p>
<div><img class=" image-border" height="348" src="img/image_12_013.png" width="522"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">See also</h1>
            

            
                
<ul>
<li>In <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml">Chapter 4</a>, <em>Resources and Memory</em>, see the <em>Creating a buffer</em> recipe</li>
<li>In <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>, see the following recipes:
<ul>
<li><em>Creating a combined image sampler</em></li>
<li><em>Creating a uniform buffer</em></li>
<li><em>Creating a descriptor set layout</em></li>
<li><em>Allocating descriptor sets</em></li>
<li><em>Updating descriptor sets</em></li>
</ul>
</li>
<li>In <a href="97217f0d-bed7-4ae1-a543-b4d599f299cf.xhtml">Chapter 7</a>, <em>Shaders</em>, see the following recipes:
<ul>
<li><em>Writing tessellation control shaders</em></li>
<li><em>Writing tessellation evaluation shaders</em></li>
<li><em>Writing geometry shaders</em></li>
</ul>
</li>
<li>In <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>, see the following recipes:
<ul>
<li><em>Creating a shader module</em></li>
<li><em>Specifying the pipeline input assembly state</em></li>
<li><em>Specifying the pipeline tessellation state</em></li>
<li><em>Creating a graphics pipeline</em></li>
</ul>
</li>
<li>In <a href="1b6b28e0-2101-47a4-8551-c30eb9bfb573.xhtml">Chapter 10</a>, <em>Helper Recipes</em>, see the following recipes:
<ul>
<li><em>Loading texture data from a file</em></li>
<li><em>Loading a 3D model from an OBJ file</em></li>
</ul>
</li>
<li>In <a href="45108a92-6d49-4759-9495-3f1166e69128.xhtml">Chapter 11</a>, <em>Lighting</em>, see the <em>Rendering a geometry with a vertex diffuse lighting recipe</em></li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Rendering a full-screen quad for post-processing</h1>
            

            
                
<p>Image processing is another class of techniques commonly used in 3D graphics. Human eyes perceive the world around us in a way that is almost impossible to simulate directly. There are many effects which cannot be displayed by just drawing a geometry. For example, bright areas seem larger than dark areas (this is usually referred to as bloom); objects seen at our focus point are sharp, but the further from the focus distance, these objects become more fuzzy or blurred (we call this effect a depth of field); color can be perceived differently during the day and at night, when with very little lighting, everything seems more blueish.</p>
<p>These phenomena are easily implemented as post-processing effects. We render the scene normally into an image. After that, we perform another rendering, this time taking the data stored in an image and processing it according to a chosen algorithm. To render an image, we need to place it on a quad that covers the whole scene. Such a geometry is usually called a fullscreen quad.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">How to do it...</h1>
            

            
                
<ol>
<li>Prepare vertex data for the quad's geometry. Use the following values for four vertices (add texture coordinates if needed):
<ul>
<li><kbd>{ -1.0f, -1.0f, 0.0f }</kbd> for top left vertex</li>
<li><kbd>{ -1.0f, 1.0f, 0.0f }</kbd> for bottom left vertex</li>
<li><kbd>{ 1.0f, -1.0f, 0.0f }</kbd> for top right vertex</li>
<li><kbd>{ 1.0f, 1.0f, 0.0f }</kbd> for bottom right vertex</li>
</ul>
</li>
<li>Create a buffer that will serve as a vertex buffer. Allocate a memory object and bind it to the buffer. Upload vertex data to the buffer using a staging resource (refer to the <em>Creating a buffer</em>, <em>Allocating and binding memory object to a buffer</em>, and <em>Using staging buffer to update buffer with a device-local memory bound</em> recipes from <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml">Chapter 4</a>, <em>Resources and Memory</em>).</li>
<li>Create a combined image sampler. Remember to provide valid uses that depend on the way the image will be accessed during rendering and post-processing: rendering a scene into an image requires a <kbd>VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT</kbd>; sampling an image (reading data using a sampler) requires a <kbd>VK_IMAGE_USAGE_SAMPLED_BIT</kbd>; for image load/stores we must provide a <kbd>VK_IMAGE_USAGE_STORAGE_BIT</kbd>; other uses may also be necessary (refer to the <em>Creating a combined image sampler</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
<li>Create a descriptor set layout with one combined image sampler. Create a descriptor pool and allocate a descriptor set from it using the created layout. Update the descriptor set with the image view's and sampler handles. Do it each time an application window is resized and an image is recreated (refer to the <em>Creating a descriptor set layout</em>, <em>Creating a descriptor pool</em>, <em>Allocating descriptor sets</em>, and <em>Updating descriptor sets</em> recipes from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
</ol>
<ol start="5">
<li>If we want to access many different image coordinates, create a separate, dedicated render pass with one color attachment and at least one subpass (refer to the <em>Specifying attachments descriptions</em>, <em>Specifying subpass descriptions</em>, <em>Specifying dependencies between subpasses</em>, and <em>Creating a render pass</em> recipes from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml">Chapter 6</a>, <em>Render Passes and Framebuffers</em>).</li>
<li>Create a shader module with a SPIR-V assembly for a vertex shader created from the following GLSL code (refer to the <em>Creating a shader module</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>):</li>
</ol>
<pre>
       #version 450 
       layout( location = 0 ) in vec4 app_position; 
       void main() { 
         gl_Position = app_position; 
       }
</pre>
<ol start="7">
<li>Create a shader module for a fragment shader created from the following GLSL code:</li>
</ol>
<pre>
       #version 450 
       layout( set = 0, binding = 0 ) uniform sampler2D Image; 
       layout( location = 0 ) out vec4 frag_color; 
       void main() { 
        vec4 color = vec4( 0.5 ); 
        color -= texture( Image, gl_FragCoord.xy + vec2( -1.0,  0.0 ) ); 
        color += texture( Image, gl_FragCoord.xy + vec2(  1.0,  0.0 ) ); 

        color -= texture( Image, gl_FragCoord.xy + vec2(  0.0, -1.0 ) ); 
        color += texture( Image, gl_FragCoord.xy + vec2(  0.0,  1.0 ) ); 

        frag_color = abs( 0.5 - color ); 
       }
</pre>
<ol start="8">
<li>Create a graphics pipeline using the preceding shader modules. It must read one vertex attribute with vertex positions (and potentially a second attribute with texture coordinates). Use a <kbd>VK_PRIMITIVE_TOPOLOGY_TRIANGLE_STRIP</kbd> topology and disable face culling (refer to the <em>Specifying pipeline vertex input state</em>, <em>Specifying pipeline input assembly state</em>, <em>Specifying pipeline rasterization state</em>, and <em>Creating a graphics pipeline</em> recipes from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>).</li>
<li>Render a scene into the created image. Next, start another render pass and draw the full-screen quad using the prepared graphics pipeline (refer to the Beginning a render pass and Ending a render pass recipes from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml">Chapter 6</a>, <em>Render Passes and Framebuffers</em>, to the <em>Binding descriptor sets</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>, and to the <em>Binding vertex buffers</em> and <em>Drawing a geometry recipes</em> from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml">Chapter 9</a>, <em>Command Recording and Drawing</em>).</li>
</ol>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">How it works...</h1>
            

            
                
<p>Image post-processing can be performed using compute shaders. However, when we want to display an image on screen, we must use a swapchain. Storing data in an image from within shaders requires images to be created with the storage image use. Unfortunately, such usage may not be supported on swapchain images, so it would require the creation of additional, intermediate resources, which further increase the complexity of a code.</p>
<p>Using a graphics pipeline allows us to process image data inside fragment shaders and store the results in color attachments. Such usage is mandatory for swapchain images, so this way feels more natural for image processing implemented with the Vulkan API. On the other hand, the graphics pipeline requires us to draw a geometry, so we need not only vertex data, and vertex and fragment shaders, but also a render pass and a framebuffer as well. That's why using compute shaders may be more efficient. So, everything depends on the features supported by the graphics hardware (available swapchain image usages) and the given situation.</p>
<p>In this recipe, we will present the method to draw a full-screen quad during an image postprocessing phase. First, we need the vertex data itself. It can be prepared directly in the clip space. This way we can create a much simpler vertex shader and avoid multiplying the vertex position by a projection matrix. After the perspective division, for the vertices to fit into a view, values stored in <kbd>x</kbd> and <kbd>y</kbd> components of their positions must fit into a &lt;<kbd>-1, 1</kbd>&gt; range (inclusive) and a value in a <kbd>z</kbd> component must be inside a &lt;<kbd>0, 1</kbd>&gt; range. So, if we want to cover the whole screen, we need the following set of vertices:</p>
<pre>
std::vector&lt;float&gt; vertices = { 
  -1.0f, -1.0f, 0.0f, 
  -1.0f,  1.0f, 0.0f, 
   1.0f, -1.0f, 0.0f, 
   1.0f,  1.0f, 0.0f, 
};
</pre>
<p>We can add normalized texture coordinates if needed or we can rely on the built-in <kbd>gl_FragCoord</kbd> value (when writing GLSL shaders), which contain screen coordinates of a currently processed shader. When we use input attachments, we even don't need texture coordinates, as we can access only the sample associated with the currently processed fragment.</p>
<p>Vertex data needs to be stored in a buffer serving as a vertex buffer. So we need to create it, allocate a memory object and bind it to the buffer and upload vertex data to the buffer:</p>
<pre>
InitVkDestroyer( LogicalDevice, VertexBuffer ); 
if( !CreateBuffer( *LogicalDevice, sizeof( vertices[0] ) * vertices.size(), VK_BUFFER_USAGE_TRANSFER_DST_BIT | VK_BUFFER_USAGE_VERTEX_BUFFER_BIT, *VertexBuffer ) ) { 
  return false; 
} 

InitVkDestroyer( LogicalDevice, BufferMemory ); 
if( !AllocateAndBindMemoryObjectToBuffer( PhysicalDevice, *LogicalDevice, *VertexBuffer, VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT, *BufferMemory ) ) { 
  return false; 
} 

if( !UseStagingBufferToUpdateBufferWithDeviceLocalMemoryBound( PhysicalDevice, *LogicalDevice, sizeof( vertices[0] ) * vertices.size(), &amp;vertices[0], *VertexBuffer, 0, 0, 
  VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT, VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, VK_PIPELINE_STAGE_VERTEX_INPUT_BIT, GraphicsQueue.Handle, FrameResources.front().CommandBuffer, {} ) ) { 
  return false;
</pre>
<p>Next, we need a way to access texel data inside the fragment shader. We can use an input attachment if we want to access data stored in a color attachment from any of the previous subpasses in the same render pass. We can use a storage image, separate the sampler and the sampled image or a combined image sampler. The latter is used in this recipe. To simplify this recipe and the code, we read texture data from a file. But usually we will have an image into which the scene will be rendered:</p>
<pre>
int width = 1; 
int height = 1; 
std::vector&lt;unsigned char&gt; image_data; 
if( !LoadTextureDataFromFile( "Data/Textures/sunset.jpg", 4, image_data, &amp;width, &amp;height ) ) { 
  return false; 
} 

InitVkDestroyer( LogicalDevice, Sampler ); 
InitVkDestroyer( LogicalDevice, Image ); 
InitVkDestroyer( LogicalDevice, ImageMemory ); 
InitVkDestroyer( LogicalDevice, ImageView ); 
if( !CreateCombinedImageSampler( PhysicalDevice, *LogicalDevice, VK_IMAGE_TYPE_2D, VK_FORMAT_R8G8B8A8_UNORM, { (uint32_t)width, (uint32_t)height, 1 }, 
  1, 1, VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT, VK_IMAGE_VIEW_TYPE_2D, VK_IMAGE_ASPECT_COLOR_BIT, VK_FILTER_NEAREST, 
  VK_FILTER_NEAREST, VK_SAMPLER_MIPMAP_MODE_NEAREST, VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, 
  VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE, 0.0f, false, 1.0f, false, VK_COMPARE_OP_ALWAYS, 0.0f, 1.0f, VK_BORDER_COLOR_FLOAT_OPAQUE_BLACK, true, 
  *Sampler, *Image, *ImageMemory, *ImageView ) ) { 
  return false; 
} 

VkImageSubresourceLayers image_subresource_layer = { 
  VK_IMAGE_ASPECT_COLOR_BIT, 
  0, 
  0, 
  1 
}; 
if( !UseStagingBufferToUpdateImageWithDeviceLocalMemoryBound( PhysicalDevice, *LogicalDevice, static_cast&lt;VkDeviceSize&gt;(image_data.size()), 
  &amp;image_data[0], *Image, image_subresource_layer, { 0, 0, 0 }, { (uint32_t)width, (uint32_t)height, 1 }, VK_IMAGE_LAYOUT_UNDEFINED, 
  VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL, 0, VK_ACCESS_SHADER_READ_BIT, VK_IMAGE_ASPECT_COLOR_BIT, VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, 
  VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, GraphicsQueue.Handle, FrameResources.front().CommandBuffer, {} ) ) { 
  return false; 
}
</pre>
<p>In the preceding code, we create a combined image sampler and specify that we will access it with unnormalized texture coordinates. Usually we provide coordinates in the &lt;0.0, 1.0&gt; range (inclusive). This way we don't need to worry about the image's size. On the other hand, for post-processing we usually want to address the texture image using screen space coordinates, and that's when unnormalized texture coordinates are used--they correspond to the image's dimensions.</p>
<p>To access an image, we also need a descriptor set. We don't need a uniform buffer as we don't transform the geometry-drawn vertices are already in the correct space (the clip space). Before we can allocate a descriptor set, we create a layout with one combined image sampler accessed in a fragment shader stage. After that, a pool is created and one descriptor set is allocated from the pool:</p>
<pre>
VkDescriptorSetLayoutBinding descriptor_set_layout_binding = { 
  0, 
  VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
  1, 
  VK_SHADER_STAGE_FRAGMENT_BIT, 
  nullptr 
}; 
InitVkDestroyer( LogicalDevice, DescriptorSetLayout ); 
if( !CreateDescriptorSetLayout( *LogicalDevice, { descriptor_set_layout_binding }, *DescriptorSetLayout ) ) { 
  return false; 
} 

VkDescriptorPoolSize descriptor_pool_size = { 
  VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
  1 
}; 
InitVkDestroyer( LogicalDevice, DescriptorPool ); 
if( !CreateDescriptorPool( *LogicalDevice, false, 1, { descriptor_pool_size }, *DescriptorPool ) ) { 
  return false; 
} 

if( !AllocateDescriptorSets( *LogicalDevice, *DescriptorPool, { *DescriptorSetLayout }, DescriptorSets ) ) { 
  return false; 
} 

ImageDescriptorInfo image_descriptor_update = { 
  DescriptorSets[0], 
  0, 
  0, 
  VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 
  { 
    { 
      *Sampler, 
      *ImageView, 
      VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL 
    } 
  } 
}; 

UpdateDescriptorSets( *LogicalDevice, { image_descriptor_update }, {}, {}, {} );
</pre>
<p>In the preceding code we also update the descriptor set with the handles of created sampler and image view. Unfortunately, the image into which we render a scene will usually fit into a screen. This means that we must recreate it when the size of an application's window is changed and, to do that, we must destroy the old image and create a new one with new dimensions. After such an operation we must update the descriptor set again with the handle of the new image (the sampler doesn't need to be recreated). So we must remember to update the descriptor set each time the application window size is changed.</p>
<p>One last thing is the creation of a graphics pipeline. It uses only two shader stages: vertex and fragment. The number of attributes fetched by the vertex shader depend on whether we need texture coordinates (and other dedicated attributes) or not. The full-screen quad's geometry should be drawn using a <kbd>VK_PRIMITIVE_TOPOLOGY_TRIANGLE_STRIP</kbd> topology. We also don't need any blending.</p>
<p>The most important part of the post-processing is performed inside the fragment shader. The work to be done depends on the technique we want to implement. In this recipe, we present an edge detection algorithm:</p>
<pre>
vec4 color = vec4( 0.5 ); 

color -= texture( Image, gl_FragCoord.xy + vec2( -1.0,  0.0 ) ); 
color += texture( Image, gl_FragCoord.xy + vec2(  1.0,  0.0 ) ); 

color -= texture( Image, gl_FragCoord.xy + vec2(  0.0, -1.0 ) ); 
color += texture( Image, gl_FragCoord.xy + vec2(  0.0,  1.0 ) ); 

frag_color = abs( 0.5 - color );
</pre>
<p>In the preceding fragment shader code, we sample four values around the fragment being processed. We take a negated value from one sample to the left and add a value read from one sample to the right. This way we know the difference between samples in a horizontal direction. When the difference is big, we know there is an edge.</p>
<p>We do the same operation for a vertical direction to detect horizontal lines too (the vertical difference, or a gradient, is used to detect horizontal edges; the horizontal gradient allows us to detect vertical edges). After that we store a value in the output variable. We additionally take the <kbd>abs()</kbd> value, but this is done only for visualization purposes.</p>
<p>In the preceding fragment shader, we access multiple texture coordinates. This can be done on combined image samplers (input attachments allow us to access only a single coordinate associated with a fragment being processed). However, to bind an image to a descriptor set as a resource other than an input attachment, we must end the current render pass and start another one. In a given render pass, images cannot be used for attachments and for any other non-attachment purpose at the same time.</p>
<p>Using the preceding setup, we should see the following result (on the right) with the original image seen on the left:</p>
<div><img class=" image-border" height="180" src="img/image_12_014.png" width="517"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">See also</h1>
            

            
                
<ul>
<li>In <a href="f1332ca0-b5a2-49bd-ac41-e37068e31042.xhtml">Chapter 4</a>, <em>Resources and Memory</em>, see the following recipes:
<ul>
<li><em>Creating a buffer</em></li>
<li><em>Allocating and binding a memory object to a buffer</em></li>
<li><em>Using the staging buffer to update a buffer with a device-local memory bound</em></li>
</ul>
</li>
<li>In <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>, see the following recipes:
<ul>
<li><em>Creating a combined image sampler</em></li>
<li><em>Creating a descriptor set layout</em></li>
<li><em>Allocating descriptor sets</em></li>
<li><em>Binding descriptor sets</em></li>
<li><em>Updating descriptor sets</em></li>
</ul>
</li>
<li>In <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml">Chapter 6</a>, <em>Render Passes and Framebuffers</em>, see the following recipes:
<ul>
<li><em>Beginning a render pass</em></li>
<li><em>Ending a render pass</em></li>
</ul>
</li>
<li>In <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>, see the following recipes:
<ul>
<li><em>Creating a shader module</em></li>
<li><em>Specifying the pipeline vertex input state</em></li>
<li><em>Specifying the pipeline input assembly state</em></li>
<li><em>Specifying the pipeline rasterization state</em></li>
<li><em>Creating a graphics pipeline</em></li>
</ul>
</li>
<li>In <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml">Chapter 9</a>, <em>Command Recording and Drawing</em>, see the following recipes:
<ul>
<li><em>Binding vertex buffers</em></li>
<li><em>Drawing a geometry</em></li>
</ul>
</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Using input attachments for a color correction post-process effect</h1>
            

            
                
<p>There are many various post-process techniques used in 3D applications. Color correction is one of them. This is relatively simple, but it can give impressive results and greatly improve the look and feel of a rendered scene. Color correction can change the mood of the scene and induce the desired feelings for the users.</p>
<p>Usually, a color correction effect requires us to read data of a single, currently processed sample. Thanks to this property, we can implement this effect using input attachments. This allows us to perform post-processing inside the same render pass in which the whole scene is rendered, thus improving the performance of our application.</p>
<p>The following is an example of an image generated with this recipe:</p>
<div><img class=" image-border" src="img/image_12_015.png"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">How to do it...</h1>
            

            
                
<ol>
<li>Create a fullscreen quad with additional resources required during postprocessing phase (refer to the <em>Rendering a full-screen quad for post processing</em> recipe).</li>
<li>Create a descriptor set layout with one input attachment accessed in a fragment shader stage. Allocate a descriptor set using the prepared layout (refer to the <em>Creating a descriptor set layout</em> and <em>Allocating descriptor sets</em> recipes from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
<li>Create a 2D image (along with a memory object and an image view) into which the scene will be drawn. Specify not only a <kbd>VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT</kbd> usage, but also a <kbd>VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT</kbd> usage during image creation. Recreate the image each time the application's window is resized (refer to the <em>Creating an input attachment</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>).</li>
</ol>
<ol start="4">
<li>Update the descriptor set with input attachment using the handle of the created image. Do it each time an application window is resized and an image is recreated (refer to the <em>Updating descriptor sets</em> recipe from Chapter 5, <em>Descriptor Sets</em>).</li>
<li>Prepare all the resource required to normally render the scene. When creating a render pass used for rendering the scene, add one additional subpass at the end of the render pass. Specify the attachment used in previous subpasses as a color attachment to be an input attachment in the additional subpass. A swapchain image should be used as a color attachment in the additional subpass (refer to the <em>Specifying subpass descriptions</em> and <em>Creating a render pass</em> recipes from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml">Chapter 6</a>, <em>Render Passes and Framebuffers</em>).</li>
<li>Create a shader module with a vertex shader created from the following GLSL code (refer to the <em>Creating a shader module</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>):</li>
</ol>
<pre>
       #version 450 
       layout( location = 0 ) in vec4 app_position; 
       void main() { 
         gl_Position = app_position; 
       }
</pre>
<ol start="7">
<li>Create a shader module with a fragment shader created from the following GLSL code:</li>
</ol>
<pre>
       #version 450 
       layout( input_attachment_index = 0, set = 0, binding = 0 ) 
       uniform subpassInput InputAttachment; 
       layout( location = 0 ) out vec4 frag_color; 

       void main() { 
         vec4 color = subpassLoad( InputAttachment ); 
         float grey = dot( color.rgb, vec3( 0.2, 0.7, 0.1 ) ); 
         frag_color = grey * vec4( 1.5, 1.0, 0.5, 1.0 ); 
       }
</pre>
<ol start="8">
<li>Create a graphics pipeline used for drawing a post-process phase. Use the preceding vertex and fragment shader modules. Prepare the rest of the pipeline parameters according to the <em>Rendering a fullscreen quad for postprocessing</em> recipe.</li>
<li>In each frame of animation, draw the scene normally into a created image, then progress to the next subpass (refer to the <em>Progressing to the next subpass</em> recipe from <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml">Chapter 6</a>, <em>Render Passes and Framebuffers</em>). Bind the created graphics pipeline used for post-processing, bind the descriptor set with the input attachment, bind the vertex buffer with full-screen quad data, and draw the full-screen quad (refer to the <em>Binding descriptor sets</em> recipe from <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>, to the <em>Binding a pipeline object</em> recipe from <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>, and to the <em>Binding vertex buffers</em> and <em>Drawing a geometry</em> recipes from <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml">Chapter 9</a>, <em>Command Recording and Drawing</em>).</li>
</ol>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">How it works...</h1>
            

            
                
<p>Creating a postprocessing effect that is rendered inside the same render pass as the scene is performed in two steps.</p>
<p>In the first step, we need to prepare resources for the base scene: its geometry, textures, descriptor sets, and pipeline objects, among others. In the second step, we do the same for the full-screen quad, as described in the <em>Rendering a fullscreen quad for postprocessing</em> recipe.</p>
<p>The two most important resources prepared solely for the post-processing phase are an image and a graphics pipeline. The image will serve as a color attachment when we are rendering the scene in a normal way. We just render the scene into the image instead of rendering it into a swapchain image. The image must serve both as a color attachment during scene rendering, but also as an input attachment during post-processing. We must also remember to recreate it when the size of the application's window is changed:</p>
<pre>
InitVkDestroyer( LogicalDevice, SceneImage ); 
InitVkDestroyer( LogicalDevice, SceneImageMemory ); 
InitVkDestroyer( LogicalDevice, SceneImageView ); 
if( !CreateInputAttachment( PhysicalDevice, *LogicalDevice, VK_IMAGE_TYPE_2D, Swapchain.Format, { Swapchain.Size.width, 
Swapchain.Size.height, 1 }, VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT | VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT, VK_IMAGE_VIEW_TYPE_2D, 
VK_IMAGE_ASPECT_COLOR_BIT, *SceneImage, *SceneImageMemory, *SceneImageView ) ) { 
return false; 
}
</pre>
<p>Accessing an image as an input attachment requires us to use a descriptor set. It must contain at least our input attachment, so we need to create a proper layout. Input attachments can be accessed only inside fragment shaders, so the creation of a descriptor set layout, a descriptor pool, and an allocation of a descriptor set may look like this:</p>
<pre>
std::vector&lt;VkDescriptorSetLayoutBinding&gt; scene_descriptor_set_layout_bindings = { 
  { 
    0, 
    VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT, 
    1, 
    VK_SHADER_STAGE_FRAGMENT_BIT, 
    nullptr 
  } 
}; 
InitVkDestroyer( LogicalDevice, PostprocessDescriptorSetLayout ); 
if( !CreateDescriptorSetLayout( *LogicalDevice, scene_descriptor_set_layout_bindings, *PostprocessDescriptorSetLayout ) ) { 
  return false; 
} 

std::vector&lt;VkDescriptorPoolSize&gt; scene_descriptor_pool_sizes = { 
  { 
    VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT, 
    1 
  } 
}; 
InitVkDestroyer( LogicalDevice, PostprocessDescriptorPool ); 
if( !CreateDescriptorPool( *LogicalDevice, false, 1, scene_descriptor_pool_sizes, *PostprocessDescriptorPool ) ) { 
  return false; 
} 

if( !AllocateDescriptorSets( *LogicalDevice, *PostprocessDescriptorPool, { *PostprocessDescriptorSetLayout }, PostprocessDescriptorSets ) ) { 
  return false; 
}
</pre>
<p>We must also update the descriptor set with the handle of our color attachment/input attachment image. As the image gets recreated when the size of the application's window is changed, we must update the descriptor too:</p>
<pre>
ImageDescriptorInfo scene_image_descriptor_update = { 
  PostprocessDescriptorSets[0], 
  0, 
  0, 
  VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT, 
  { 
    { 
      VK_NULL_HANDLE, 
      *SceneImageView, 
      VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL 
    } 
  } 
}; 

UpdateDescriptorSets( *LogicalDevice, { scene_image_descriptor_update }, {}, {}, {} );
</pre>
<p>The next thing we need to describe is the preparation of a render pass. In this recipe the render pass is common for both the scene rendering and the post-processing phase. The scene is rendered in its own, dedicated subpass (or subpasses). The post-processing phase adds an additional subpass for rendering a full-screen quad.</p>
<p>Usually, we define two render pass attachments: a color attachment (a swapchain image) and a depth attachment (an image with a depth format). This time we need three attachments: the first one is a color attachment for which the created image will be used; the depth attachment is the same as usual; and the third attachment is also a color attachment, for which a swapchain image will be used. This way, the scene is rendered normally into two (color and depth attachments). Then, the first attachment is used as an input attachment during post-processing; and the full-screen quad is rendered into the second color attachment (a swapchain image) so the final image appears on screen.</p>
<p>The following code sets up the render pass attachment:</p>
<pre>
std::vector&lt;VkAttachmentDescription&gt; attachment_descriptions = { 
  { 
    0, 
    Swapchain.Format, 
    VK_SAMPLE_COUNT_1_BIT, 
    VK_ATTACHMENT_LOAD_OP_CLEAR, 
    VK_ATTACHMENT_STORE_OP_DONT_CARE, 
    VK_ATTACHMENT_LOAD_OP_DONT_CARE, 
    VK_ATTACHMENT_STORE_OP_DONT_CARE, 
    VK_IMAGE_LAYOUT_UNDEFINED, 
    VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL 
  }, 
  { 
    0, 
    DepthFormat, 
    VK_SAMPLE_COUNT_1_BIT, 
    VK_ATTACHMENT_LOAD_OP_CLEAR, 
    VK_ATTACHMENT_STORE_OP_DONT_CARE, 
    VK_ATTACHMENT_LOAD_OP_DONT_CARE, 
    VK_ATTACHMENT_STORE_OP_DONT_CARE, 
    VK_IMAGE_LAYOUT_UNDEFINED, 
    VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL 
  }, 
  { 
    0, 
    Swapchain.Format, 
    VK_SAMPLE_COUNT_1_BIT, 
    VK_ATTACHMENT_LOAD_OP_DONT_CARE, 
    VK_ATTACHMENT_STORE_OP_STORE, 
    VK_ATTACHMENT_LOAD_OP_DONT_CARE, 
    VK_ATTACHMENT_STORE_OP_DONT_CARE, 
    VK_IMAGE_LAYOUT_UNDEFINED, 
    VK_IMAGE_LAYOUT_PRESENT_SRC_KHR 
  } 
};
</pre>
<p>The render pass has two subpasses defined as follows:</p>
<pre>
VkAttachmentReference depth_attachment = { 
  1, 
  VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL 
}; 

std::vector&lt;SubpassParameters&gt; subpass_parameters = { 
  { 
    VK_PIPELINE_BIND_POINT_GRAPHICS, 
    {}, 
    { 
      { 
        0, 
        VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL, 
      } 
    }, 
    {}, 
    &amp;depth_attachment, 
    {} 
  }, 
  { 
    VK_PIPELINE_BIND_POINT_GRAPHICS, 
    { 
      { 
        0, 
        VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL, 
      } 
    }, 
    { 
      { 
        2, 
        VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL, 
      } 
    }, 
    {}, 
    nullptr, 
    {} 
  } 
};
</pre>
<p>We also can't forget about the render pass subpass dependencies. They are very important here as they synchronize the two subpasses. We can't read data from a texture until the data is written into it, so we need dependencies between the 0 and the 1 subpass (for the image serving as color and input attachment. Similarly, dependencies are needed for a swapchain image:</p>
<pre>
std::vector&lt;VkSubpassDependency&gt; subpass_dependencies = { 
  { 
    0, 
    1, 
    VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT, 
    VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, 
    VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT, 
    VK_ACCESS_INPUT_ATTACHMENT_READ_BIT, 
    VK_DEPENDENCY_BY_REGION_BIT 
  }, 
  { 
    VK_SUBPASS_EXTERNAL, 
    1, 
    VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, 
    VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT, 
    VK_ACCESS_MEMORY_READ_BIT, 
    VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT, 
    VK_DEPENDENCY_BY_REGION_BIT 
  }, 
  { 
    1, 
    VK_SUBPASS_EXTERNAL, 
    VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT, 
    VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, 
    VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT, 
    VK_ACCESS_MEMORY_READ_BIT, 
    VK_DEPENDENCY_BY_REGION_BIT 
  } 
};
</pre>
<p>The graphics pipeline used during post-processing phase is a standard one. Only two things are different: the graphics pipeline is used inside the subpass with index <kbd>1</kbd> (not <kbd>0</kbd> as in other recipes--the scene is rendered in the subpass <kbd>0</kbd>); and the fragment shader loads color data, not from the combined image sampler, but from the input attachment. The input attachment inside the fragment shader is defined as follows:</p>
<pre>
layout( input_attachment_index = 0, set = 0, binding = 0 ) uniform subpassInput InputAttachment;
</pre>
<p>We read data from it using the <kbd>subpassLoad()</kbd> function. It takes only the uniform variable. Texture coordinates are unnecessary, because through an input attachment we can read data only from the coordinate associated with the fragment being processed.</p>
<pre>
vec4 color = subpassLoad( InputAttachment );
</pre>
<p>The fragment shader then takes the loaded color, calculates a sepia color from it, and stores it in an output variable (a color attachment). All this combined should lead us to create the following results. On the left we see the scene rendered normally. On the right we see a post-processing effect applied:</p>
<div><img class=" image-border" src="img/image_12_016.png"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">See also</h1>
            

            
                
<ul>
<li>In <a href="fe2cb528-9d22-49db-a05b-372bce2f87ee.xhtml">Chapter 5</a>, <em>Descriptor Sets</em>, see the following recipes:
<ul>
<li><em>Creating an input attachment</em></li>
<li><em>Creating a descriptor set layout</em></li>
<li><em>Allocating descriptor sets</em></li>
<li><em>Updating descriptor sets</em></li>
<li><em>Binding descriptor sets</em></li>
</ul>
</li>
<li>In <a href="2de4339d-8912-440a-89a6-fd1f84961448.xhtml">Chapter 6</a>, <em>Render Passes and Framebuffers</em>, see the following recipes:
<ul>
<li><em>Specifying subpass descriptions</em></li>
<li><em>Creating a render pass</em></li>
<li><em>Progressing to the next subpass</em></li>
</ul>
</li>
<li>In <a href="5744ea05-b18a-4f84-a1df-250b549dfea5.xhtml">Chapter 8</a>, <em>Graphics and Compute Pipelines</em>, see the following recipes:
<ul>
<li><em>Creating a shader module</em></li>
<li><em>Binding a pipeline object</em></li>
</ul>
</li>
<li>In <a href="0a69f5b5-142e-422b-aa66-5cb09a6467b3.xhtml">Chapter 9</a>, <em>Command Recording and Drawing</em>, see the following recipes:
<ul>
<li>Binding vertex buffers</li>
<li>Drawing a geometry</li>
</ul>
</li>
<li>The recipe <em>Rendering a full-screen quad for post-processing</em>, in this chapter</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    </body></html>