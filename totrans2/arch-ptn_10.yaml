- en: Patterns for Containerized and Reliable Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器化和可靠应用程序的模式
- en: 'The Docker-enabled containerization paradigm is on the right track to becoming
    an impactful and insightful technology with a number of crucial advancements being
    brought in by a growing array of third-party products and tool vendors. Especially,
    the future belongs to containerized cloud environments with the ready availability
    of proven container development, deployment, networking, and composition technologies
    and tools. The Docker-enabled containers in association with orchestration, governance,
    monitoring, measurement, and management platforms such as Kubernetes, Mesos, and
    so on, are to contribute immensely to setting up and sustaining next-generation
    containerized cloud environments that are very famous for delivering enterprise-class,
    microservices-based, event-driven, service-oriented, cloud-hosted, knowledge-filled,
    insights-attached, AI-enabled, people-centric, carrier-grade, production-ready,
    and infrastructure-aware applications. Besides containers, the concepts of microservices
    and microservices-centric applications acquire special significance. The basic
    requirement for building reliable applications lies with the faster realization
    of resilient microservices, which are being positioned as the standard and optimized
    building-block and deployment unit for the next-generation applications. This
    chapter focuses on the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Docker启用的容器化范式正在正确轨道上，有望成为一项有影响力和洞察力的技术，随着越来越多的第三方产品和工具供应商带来的关键进步，它正在取得成功。特别是，未来属于具有现成的容器开发、部署、网络和组合技术和工具的容器化云环境。与编排、治理、监控、测量和管理平台（如Kubernetes、Mesos等）结合的Docker容器将极大地促进建立和维持下一代容器化云环境，这些环境因其提供企业级、基于微服务、事件驱动、面向服务的、云托管、知识丰富、洞察力相关、人工智能赋能、以人为本、运营商级、生产就绪和基础设施感知的应用程序而闻名。除了容器之外，微服务和以微服务为中心的应用程序的概念也具有特殊意义。构建可靠应用程序的基本要求是更快地实现弹性微服务，这些微服务被定位为下一代应用程序的标准和优化构建块和部署单元。本章重点介绍以下主题：
- en: The containerization patterns
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器化模式
- en: Resilient microservices patterns
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性微服务模式
- en: Reliable applications patterns
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可靠的应用程序模式
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Undeniably, Docker is the most popular and powerful technology these days in
    the **information technology** (**IT**) sector. There are two principal trends
    in the Docker landscape. Firstly, the open-source Docker platform is being continuously
    equipped with more right and relevant features and functionalities in order to
    make it the most exemplary IT platform, not only for software developers, but
    also for on-premises as well as off-premises IT operational teams. The second
    trend is the unprecedented adoption of the Docker-inspired containerization technology
    by various IT service and solution providers across the globe in order to bring
    forth a growing array of premium offerings to their venerable consumers and clients.
    The enhanced simplicity in developing fresh software applications, the automated
    and accelerated deployment of Docker containers, and the extreme maneuverability
    of Docker containers are being widely touted as the key differentiators for its
    unprecedented success.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，Docker是目前**信息技术**（**IT**）领域最受欢迎和最强大的技术。在Docker生态系统中存在两个主要趋势。首先，开源的Docker平台正在不断被赋予更多权利和相关的功能，以便使其成为最典范的IT平台，不仅对软件开发者，也对本地和远程IT运营团队来说都是如此。第二个趋势是全球各地的IT服务和解决方案提供商前所未有的采用Docker启发的容器化技术，以向其尊贵的消费者和客户提供越来越多的优质产品。开发全新软件应用的简化、Docker容器的自动化和加速部署，以及Docker容器的极端灵活性，被广泛宣传为其前所未有的成功的关键差异化因素。
- en: We would like to shed more light on Docker and show why it is being touted as
    the next best thing for the impending digital, idea, API, knowledge and insightful
    economy.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望进一步阐明Docker，并展示为什么它被吹捧为即将到来的数字、理念、API、知识和洞察经济的下一件大事。
- en: The key drivers for containerization
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器化的关键驱动因素
- en: The first and foremost driver for Docker-enabled containerization is to competently
    and completely overcome the widely expressed limitations of the virtualization
    paradigm. Actually, we have been working on the proven virtualization techniques
    and tools for quite a long time now in order to realize the much-demanded software
    portability. That is, with the goal of decimating the inhibiting dependency between
    software and hardware, there have been several initiatives that incidentally include
    the matured and stabilized virtualization paradigm. Virtualization is a kind of
    beneficial abstraction, which is accomplished through the incorporation of an
    additional layer of indirection between hardware resources and software components.
    Through this freshly introduced abstraction layer (hypervisor or **virtual machine
    monitor** (**VMM**)), any kind of software application can run on any underlying
    hardware without any hitch or hurdle. In short, the software portability is being
    achieved through this middleware layer. However, the much-published portability
    target is not fully met even by the virtualization technique. The hypervisor software
    and different data encapsulation formats from different vendors come in the way
    of ensuring the much-needed application portability. Furthermore, the distribution,
    version, edition, and patching differences of operating systems and application
    workloads hinder the smooth portability of workloads across systems and locations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Docker启用容器化的首要驱动因素是有效地完全克服虚拟化范式被广泛表达的局限性。实际上，我们已经对经过验证的虚拟化技术和工具进行了相当长一段时间的工作，以实现迫切需要的软件可移植性。也就是说，为了消除软件与硬件之间的抑制性依赖，已经出现了几个意外包括成熟的和稳定的虚拟化范式。虚拟化是一种有益的抽象，它通过在硬件资源和软件组件之间引入额外的间接层来实现。通过这个新引入的抽象层（虚拟机管理程序或**虚拟机监控程序**（**VMM**）），任何类型的软件应用程序都可以在任何底层硬件上无缝运行。简而言之，软件可移植性是通过这个中间件层实现的。然而，即使是通过虚拟化技术，广泛发布的可移植性目标也没有完全实现。虚拟机管理程序软件和来自不同供应商的不同数据封装格式阻碍了确保所需的应用程序可移植性。此外，操作系统和应用程序工作负载的分布、版本、版本和修补差异阻碍了工作负载在系统和位置之间的顺畅迁移。
- en: Similarly, there are various other drawbacks being attached with the virtualization
    paradigm. In data centers and server farms, the virtualization technique is typically
    used for creating multiple VMs out of physical machines and each VM has its own
    **operating system** (**OS**). Through this solid and sound isolation enacted
    through automated tools and controlled resource-sharing, multiple and heterogeneous
    applications are being accommodated in a physical machine. That is, the hardware-assisted
    virtualization enables disparate applications to be run simultaneously on a single
    physical server. With the virtualization paradigm, various kinds of IT infrastructures
    (server machines, storage appliances, and networking solutions) become open, programmable,
    remotely monitorable, manageable, and maintainable. However, because of the verbosity
    and bloatedness (every VM carries its own operating system), VM provisioning typically
    takes a few minutes. This is a big setback for real-time and on-demand scalability.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，虚拟化范式还伴随着各种其他缺点。在数据中心和服务器农场中，虚拟化技术通常用于将物理机转换为多个虚拟机（VM），每个虚拟机都有自己的**操作系统**（**OS**）。通过通过自动化工具和受控资源共享实现的这种坚实和合理的隔离，可以在物理机上容纳多个和异构的应用程序。也就是说，硬件辅助的虚拟化使得不同的应用程序可以在单个物理服务器上同时运行。在虚拟化范式下，各种IT基础设施（服务器机器、存储设备、网络解决方案）变得开放、可编程、可远程监控、可管理和可维护。然而，由于冗长和臃肿（每个虚拟机都携带自己的操作系统），虚拟机的配置通常需要几分钟。这对实时和按需可扩展性是一个很大的障碍。
- en: The other widely expressed drawback that is being closely associated with virtualization
    is that the performance of virtualized systems also goes down due to the excessive
    usage of precious and expensive IT resources (processing, memory, storage, network
    bandwidth, and so on). The execution time of virtual machines is on the higher
    side because of multiple layers ranging from a guest OS, a hypervisor, and the
    underlying hardware.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个被广泛表达的缺点是与虚拟化紧密相关，那就是由于过度使用宝贵的昂贵IT资源（处理能力、内存、存储、网络带宽等），虚拟化系统的性能也会下降。由于从客户操作系统、虚拟机管理程序到底层硬件的多层结构，虚拟机的执行时间较高。
- en: Finally, the compute virtualization has flourished, whereas the other closely
    associated network and storage virtualization concepts are just taking off.  Precisely
    speaking, building distributed applications and fulfilling varying business expectations
    mandate for the faster and flexible provisioning, high availability, reliability,
    scalability, and maneuverability of all the participating IT resources. Compute,
    storage, and networking components need to work together in accomplishing the
    varying IT and business needs. With more virtualized elements and entities in
    an IT environment, the operational complexity is bound to grow rapidly.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，计算虚拟化已经蓬勃发展，而其他与之紧密相关的网络和存储虚拟化概念才刚刚起步。确切地说，构建分布式应用程序和满足不断变化的企业期望需要更快和更灵活的资源分配、高可用性、可靠性、可扩展性和灵活性。计算、存储和网络组件需要协同工作以满足各种IT和业务需求。在IT环境中，随着虚拟化元素和实体的增加，运营复杂性必然会迅速增长。
- en: Move over to the world of containerization; all the preceding barriers get resolved
    in a single stroke. That is, the evolving concept of application containerization
    coolly and confidently contributes to the unprecedented success of the software
    portability goal. A container generally contains an application/service/process.
    Along with the primary application, all of its relevant libraries, binaries, files,
    and other dependencies are stuffed and squeezed together to be packaged and presented
    as a comprehensive yet compact container. The application containers can be readily
    shipped, run, and managed in any local as well as remote environments. Containers
    are exceptionally lightweight, highly portable, rapidly deployable, extensible,
    horizontally scalable, and so on. Furthermore, many industry leaders have come
    together to form a kind of consortium to embark on a decisive and deft journey
    towards the systematic production, packaging, and delivery of industry-strength
    and standardized containers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 转向容器化的世界；所有先前的障碍都一扫而空。也就是说，不断发展的应用程序容器化概念冷静而自信地贡献于软件可移植性目标的空前成功。容器通常包含一个应用程序/服务/进程。除了主要应用程序外，所有相关的库、二进制文件、文件和其他依赖项都被打包并压缩在一起，形成一个全面而紧凑的容器。应用程序容器可以轻松地在任何本地和远程环境中运输、运行和管理。容器异常轻量级，高度便携，快速部署，可扩展，水平可扩展，等等。此外，许多行业领导者聚集在一起，形成了一种联盟，开始了一次决定性和灵巧的系统化生产、包装和交付行业级和标准化容器的旅程。
- en: This conscious and collective move makes Docker deeply penetrative and pervasive.
    The open-source community is simultaneously spearheading the containerization
    conundrum through an assortment of concerted activities for simplifying and streamlining
    the containerization concept. The containerization life cycle steps are being
    automated through a variety of third-party tools.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种有意识和集体的转变使得Docker深入且广泛地渗透。开源社区同时通过一系列协调活动领导着容器化难题，旨在简化并精炼容器化概念。容器化生命周期步骤正在通过各种第三方工具实现自动化。
- en: The Docker ecosystem also grows fast in order to bring in as much automation
    as possible in the IT landscape. Container clustering and orchestration are gaining
    a lot of attention, thereby geographically distributed containers and their clusters
    can be readily linked up to produce bigger and better process-aware and composite
    containers. The new concept of containerization assists with distributed computing.
    Containers enable the formation of federated cloud environments in order to accomplish
    specialized business targets. Cloud service providers and enterprise IT environments
    are all set to embrace this unique compartmentalization technology in order to
    escalate the resource utilization and to take the much-insisted infrastructure
    optimization to the next level. On the performance side, there are sufficient
    tests showcasing Docker containers achieving the bare metal server performance.
    In short, the IT agility through the DevOps aspect is being guaranteed through
    the smart leverage of the Docker-enabled containerization and this, in turn, leads
    to business agility, adaptivity, and affordability.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 IT 环境中尽可能实现自动化，Docker 生态系统也在快速发展。容器集群和编排正在受到越来越多的关注，因此地理上分布的容器及其集群可以迅速连接起来，产生更大、更好的过程感知和复合容器。容器化的新概念有助于分布式计算。容器能够形成联邦云环境，以实现特定的业务目标。云服务提供商和企业
    IT 环境都准备好拥抱这种独特的分区技术，以提高资源利用率，并将基础设施优化提升到新的水平。在性能方面，有足够的测试表明 Docker 容器达到了裸金属服务器的性能。简而言之，通过
    DevOps 方面的智能利用 Docker 功能容器，确保了 IT 的敏捷性，这反过来又导致了商业的敏捷性、适应性和可负担性。
- en: Design patterns for Docker containers
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker 容器的设计模式
- en: The Docker-enabled containerization is fast emerging and evolving. With the
    complexity of the container lifecycle management escalating, the need for enabling
    patterns is being felt. The concerned professionals and pundits are working in
    unison to formulate and firm up various container-specific patterns. In the days
    ahead, we will come across many more patterns. Whatever is widely articulated
    and accepted is concisely presented in this section and in the forthcoming sections.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 带有 Docker 功能的容器化正在迅速兴起和演变。随着容器生命周期管理的复杂性不断升级，对启用模式的需要正在感受到。相关的专业人士和评论家正在齐心协力制定和巩固各种特定于容器的模式。在未来的日子里，我们将遇到更多模式。本节和即将到来的各节中，将简洁地呈现广泛阐述和接受的内容。
- en: With the unprecedented proliferation of the Docker-enabled containers in cloud
    environments (public, private, and fog/edge), Docker enthusiasts, evangelists,
    and experts consciously bring forth a bevy of enabling patterns. The readers can
    find them in this section. Let us start with container building patterns. Building
    Docker images and containers is constrained with a number of challenges and concerns.
    The Docker patterns need to reach a level of stability.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随着在云环境中（公共、私有和雾/边缘）Docker 功能容器的无与伦比的激增，Docker 爱好者、传教士和专家有意识地提出了一系列启用模式。读者们可以在本节中找到它们。让我们从容器构建模式开始。构建
    Docker 镜像和容器受到许多挑战和问题的限制。Docker 模式需要达到一个稳定的水平。
- en: Container building patterns
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器构建模式
- en: 'This section describes a few common ways to build Docker images. As per Alex
    Collins ([https://alexecollins.com/developing-with-docker-building-patterns/](https://alexecollins.com/developing-with-docker-building-patterns/)),
    there are several choices: `scratch + binary`, `language stack`, and `distribution+
    package manager`. The `scratch + binary - scratch` is the most basic base image
    and it does not contain any files or programs at all. We must build a *standalone
    binary application* to use this. Here is an example. Firstly, we will build a
    standalone binary application using Docker. The steps are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了构建 Docker 镜像的一些常见方法。根据 Alex Collins ([https://alexecollins.com/developing-with-docker-building-patterns/](https://alexecollins.com/developing-with-docker-building-patterns/))
    的说法，有几种选择：`scratch + binary`、`language stack` 和 `distribution+ package manager`。`scratch
    + binary - scratch` 是最基本的基础镜像，它根本不包含任何文件或程序。我们必须构建一个 *独立二进制应用程序* 来使用它。以下是一个示例。首先，我们将使用
    Docker 构建一个独立二进制应用程序。步骤如下：
- en: 'Create an empty directory and then create a `main.go` application:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个空目录，然后创建一个 `main.go` 应用程序：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Compile the application:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译应用程序：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a Dockerfile for the application:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为应用程序创建一个 Dockerfile：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, build and run the image:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，构建并运行镜像：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This outputs `Hello World` in the terminal.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在终端中输出 `Hello World`。
- en: This is suitable for applications that can be packaged as standalone binaries.
    As there is no language runtime, larger applications are bound to consume more
    disk space.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这适用于可以打包为独立二进制文件的应用程序。因为没有语言运行时，较大的应用程序必然需要更多的磁盘空间。
- en: 'Docker provides a number of pre-built base images for the runtime for common
    languages. Here is an example as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Docker为常见语言提供了许多预构建的运行时基础镜像。以下是一个示例：
- en: 'Create a new empty directory and detail the `Main.java` application:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的空目录并详细说明`Main.java`应用程序：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, compile this application using the **Java Development Kit** (**JDK**):'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用**Java开发工具包**（**JDK**）编译此应用程序：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create the following Dockerfile with the **Java Runtime Environment** (**JRE**):'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**Java运行环境**（**JRE**）创建以下Dockerfile：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, build and run this Docker image:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，构建并运行此Docker镜像：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It is faster to deploy this application once the base image is downloaded, and
    if the same base image is used for many other applications, then the additional
    layer needed is very small.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下载基础镜像后部署此应用程序会更快，如果许多其他应用程序使用相同的基镜像，那么所需的额外层非常小。
- en: To build an image that is not on a supported language stack, it is necessary
    to roll your own image starting with a distribution, and then it is all about
    using a package manager to add the mandated dependencies. Linux always contains
    a package manager.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个不在支持的语言堆栈上的镜像，需要从一个发行版开始创建自己的镜像，然后就是使用包管理器添加必需的依赖项。Linux总是包含一个包管理器。
- en: 'This comment installs the JRE:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这条注释安装了JRE：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, build and run this base image:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，构建并运行此基础镜像：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The advantage is that we can build an application and it is possible to put
    multiple applications into a single image (using `systemd`).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 优点是我们可以构建一个应用程序，并且有可能将多个应用程序放入单个镜像中（使用`systemd`）。
- en: Docker image building patterns
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker镜像构建模式
- en: As we all know, Docker containers are a fantastic way to optimally and organically
    encapsulate complex build processes. Typically, any software package requires
    a host of dependencies. As indicated in [Chapter 9](https://cdp.packtpub.com/architectural_patterns/wp-admin/post.php?post=271&action=edit),
    *Microservices Architecture Patterns*, every microservice is being developed and
    delivered as a Docker image. Each microservice has its own code repository (GitHub)
    and its own CI build job.  Microservices can be coded using any programming language.
    Let us focus on the Java language here. If a service is built and run using a
    compiled language (Java, Go, and so on), then the build environment can be separated
    from the runtime environment. A Java service's `Dockerfile.build` is from the
    `openjdk-7-jdk` directory and its Dockerfile is from the `openjdk-7-jre` directory
    which is substantially smaller than JDK.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，Docker容器是优化和有机封装复杂构建过程的绝佳方式。通常，任何软件包都需要大量的依赖项。正如[第9章](https://cdp.packtpub.com/architectural_patterns/wp-admin/post.php?post=271&action=edit)《微服务架构模式》中所述，每个微服务都是作为一个Docker镜像开发和交付的。每个微服务都有自己的代码仓库（GitHub）和自己的CI构建任务。微服务可以使用任何编程语言进行编码。让我们在这里关注Java语言。如果一个服务是使用编译语言（Java、Go等）构建和运行的，那么构建环境可以与运行环境分离。Java服务的`Dockerfile.build`来自`openjdk-7-jdk`目录，其`Dockerfile`来自`openjdk-7-jre`目录，这个目录比JDK小得多。
- en: For the Java programming language, it requires additional tooling and processes
    before its microservices become executable. However, the JDK are not required
    when a compiled program is running. Another reason is that the JDK is a bigger
    package when compared with the **Java Runtime Environment** (**JRE**). Furthermore,
    it seems farsighted to develop and reuse a repeatable process and a uniform environment
    for deploying microservices. It is therefore paramount to package the Java tools
    and packages into containers. This setup allows the building of Java-based microservices
    on any machine, including a CI server, without any specific environmental requirements
    such as JDK version, profiling and testing tools, OS, Maven, environment variables,
    and so on.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Java编程语言，在它的微服务成为可执行之前需要额外的工具和流程。然而，当编译程序运行时，不需要JDK。另一个原因是与**Java运行环境**（**JRE**）相比，JDK是一个更大的包。此外，开发和重用可重复的过程和统一的环境来部署微服务似乎是一种远见。因此，将Java工具和包打包到容器中至关重要。这种设置允许在任何机器上构建基于Java的微服务，包括CI服务器，而无需任何特定的环境要求，如JDK版本、分析测试工具、操作系统、Maven、环境变量等。
- en: 'Resultantly, for every service, there are two Dockerfiles: one for service
    runtime and the second is packed with the required tools to build the service.
    First, it is all about crafting the `Dockerfile.build` file, which can speed up
    the Maven build. Now, it is straightforward to compile and run the microservice
    on any machine (local or remote). This segregated approach goes a long way in
    simplifying the **continuous integration** (**CI**) process.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipe is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '**Build file**: Have one Dockerfile with all the tools and packages required
    to build any service. Name it `Dockerfile.build`.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Run file**: Have another Dockerfile with all the packages required to run
    the service. Keep both files along with the service code.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a new builder image, create a container from it, and extract build artifacts
    using volumes or the `docker cp` command.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the service image.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Thus, segregating the building process from the runtime process stands well
    for the intended success of the containerization paradigm. One is to perform a
    build and another is to ship the results of the first build without the penalty
    of the build-chain and tooling in the first image. Terra Nullius has posted the
    relevant details at [http://blog.terranillius.com/post/docker_builder_pattern/](http://blog.terranillius.com/post/docker_builder_pattern/).
    The builder pattern describes the setup that developers have to follow for building
    a container. It generally involves two Docker images:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: A *build* image with all the build tools installed, capable of creating production-ready
    application files
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *service* image capable of running the application
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The basic idea behind the builder pattern is simple: create additional Docker
    images with the required tools (compilers, linkers, and testing tools), and use
    these images to produce lean, secure, and production-ready Docker images.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Multi-stage image building pattern
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The latest Docker release facilitates the creation of a single Dockerfile that
    can build multiple helper images with compilers, tools, and tests, and use files
    from images to produce the *final* Docker image, as vividly illustrated in the
    following section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'The Docker platform can build Docker images by reading the instructions from
    a Dockerfile. A Dockerfile is a text file that contains a list of all the commands
    needed to build a new Docker image. The syntax and core principle of a Dockerfile is
    pretty simple and straightforward as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That is, every Dockerfile creates a Docker image. This principle works just
    fine for basic use cases, but for creating advanced, secure, and lean Docker images,
    a single Dockerfile is just not enough.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Multi-stage builds are a new feature incorporated in the latest Docker version,
    and this is interesting for anyone who has struggled to optimize Dockerfiles while
    keeping them easy to read and maintain. One of the biggest challenges when building
    Docker images is keeping the image size down. Each instruction in the Dockerfile
    adds a layer to the image. The software engineer has to clean up any artifacts
    that are not needed before moving on to the next layer. To write a really efficient
    Dockerfile, he traditionally needs to employ the shell tricks and other logic
    to keep the layers as lean and light as possible and to ensure that each layer
    has the artifacts it needs from the previous layer and nothing else.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: It is always common to have one Dockerfile for development and a slimmed-down
    version of the Dockerfile for production. Maintaining two Dockerfiles is not ideal.
    With multi-stage builds, he can use multiple `FROM` statements in his Dockerfile.
    Each `FROM` instruction can use a different base, and each of them begins a new
    stage of the build. He can selectively copy artifacts from one stage to another,
    leaving behind everything he doesn't want in the final image. The end result is
    the same tiny production image as before, with a significant reduction in complexity.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The pattern for file sharing between containers
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker is a popular containerization tool used to package and provide software
    applications with a filesystem that contains everything they need to run. Docker
    containers are ephemeral in the sense that they can run for as long as it takes
    for the command issued in the container to complete. There are occasions wherein
    applications need access to data, to share data to, or do data persistence after
    a container is deleted. Typically, Docker images are not suitable for databases;
    user-generated content for a website and log files that applications have to access
    to do the required processing. The much-needed persistent access to data is provided
    with Docker volumes. At some point, the production-ready application files need
    to be copied from the build container to the host machine. There are two ways
    of accomplishing that:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Using `docker cp`
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `bind-mount volumes`
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matthias Noback ([https://matthiasnoback.nl/2017/04/docker-build-patterns/](https://matthiasnoback.nl/2017/04/docker-build-patterns/))
    has supplied the description for both along with an easy-to-understand example.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Using bind-mount volumes
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is not good to have the compilation step as a part of the build process of
    the container. The overwhelming expectation is that Docker images need to be highly
    reusable. If the source code is modified, then it is necessary to rebuild the
    build image, but it is desired to *run the same build image* again.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the compilation step has to be moved to the ENTRYPOINT ([https://docs.docker.com/engine/reference/builder/#entrypoint](https://docs.docker.com/engine/reference/builder/#entrypoint))
    or CMD instruction. The source/files shouldn't be part of the build context and
    instead, mounted as a bind-mount volume inside the running build container.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，编译步骤必须移动到 ENTRYPOINT ([https://docs.docker.com/engine/reference/builder/#entrypoint](https://docs.docker.com/engine/reference/builder/#entrypoint))
    或 CMD 指令。源文件不应该作为构建上下文的一部分，而应该作为 bind-mount 卷挂载到正在运行的 build 容器内部。
- en: The advantages are many here. Every time one runs the build container, it will
    compile the files in the `/project/source/` and produce a new executable in the `/project/target/`.
    Since `/project` is a bind-mount volume, the executable file is automatically
    available on the host machine in `target/`. There is no need to explicitly copy
    it from the container. Once the application files are on the host machine, it
    will be easy to copy them to the service image, since that can be done using the
    regular `COPY` instruction.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多优势。每次运行 build 容器时，它都会编译 `/project/source/` 中的文件，并在 `/project/target/` 产生一个新的可执行文件。由于 `/project`
    是一个绑定挂载卷，可执行文件会自动在主机机器上的 `target/` 目录中可用。无需显式地从容器中复制它。一旦应用程序文件在主机机器上，就很容易将它们复制到服务镜像中，因为可以使用常规的 `COPY`
    指令来完成此操作。
- en: Pipes and filters pattern
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道和过滤器模式
- en: An application is required to perform a variety of tasks of varying complexity
    on the information that it receives. A monolithic module could do this, but there
    are several inflexibilities. Suppose an application receives and processes data
    from two sources. The data from each source is processed by a separate module
    that performs a series of tasks to transform this data, before passing the result
    to the business logic of the application. The processing tasks performed by each
    module or the deployment requirements for each task could change. Some tasks might
    be compute-intensive and could benefit from running on powerful hardware, while
    others might not require such expensive resources. Also, additional processing
    might be required in the future, or the order in which the tasks are performed
    by the processing could change.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一个应用程序来对其接收到的信息执行各种不同复杂度的任务。一个单体模块可以做到这一点，但存在一些不灵活性。假设一个应用程序从两个来源接收并处理数据。每个来源的数据都由一个单独的模块处理，该模块执行一系列任务以转换这些数据，然后将结果传递给应用程序的业务逻辑。每个模块执行的处理任务或每个任务的部署要求可能会变化。一些任务可能是计算密集型的，可能需要运行在强大的硬件上，而其他任务可能不需要这样昂贵的资源。此外，未来可能需要额外的处理，或者处理任务执行的顺序可能会改变。
- en: The viable solution is to break down the processing required for each data stream
    into a set of separate components (or filters), each performing a single task.
    By standardizing the format of the data that each component receives and sends,
    these filters can be combined together into a pipeline. This helps to avoid duplicating
    code and makes it easy to remove, replace, or integrate additional components
    if the processing requirements change.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可行的解决方案是将每个数据流所需的处理分解成一系列单独的组件（或过滤器），每个组件执行单一任务。通过标准化每个组件接收和发送的数据格式，这些过滤器可以组合成一个管道。这有助于避免代码重复，并在处理需求发生变化时，便于移除、替换或集成额外的组件。
- en: The time it takes to process a single request depends on the speed of the slowest
    filter in the pipeline. One or more filters could be a bottleneck, especially
    if a large number of requests appear in a stream from a particular data source.
    A key advantage of the pipeline structure is that it provides opportunities for
    running parallel instances of slow filters, enabling the system to spread the
    load and improve throughput.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 处理单个请求所需的时间取决于管道中最慢的过滤器的速度。一个或多个过滤器可能成为瓶颈，特别是如果来自特定数据源的请求流中出现大量请求时。管道结构的优势之一是它提供了运行慢速过滤器并行实例的机会，从而使系统能够分散负载并提高吞吐量。
- en: The filters that make up a pipeline can run on different machines, enabling
    them to be scaled independently and take advantage of the elasticity that many
    cloud environments provide. A filter that is computationally intensive can run
    on high-performance hardware, while other less demanding filters can be hosted
    on less expensive commodity hardware.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 构成管道的过滤器可以在不同的机器上运行，这使得它们可以独立扩展并利用许多云环境提供的弹性。计算密集型的过滤器可以在高性能硬件上运行，而其他要求较低的过滤器可以托管在成本较低的通用硬件上。
- en: If the input and output of a filter are structured as a stream, it is possible
    to perform the processing for each filter in parallel. The first filter in the
    pipeline can start its work and output its results, which are passed directly
    on to the next filter in the sequence before the first filter has completed its
    work. If a filter fails or the machine it's running on is no longer available,
    the pipeline can reschedule the work that the filter was performing and direct
    this work to another instance of the component.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个过滤器的输入和输出都结构化为流，那么可以并行地对每个过滤器进行处理。管道中的第一个过滤器可以开始工作并输出其结果，这些结果在第一个过滤器完成其工作之前直接传递给序列中的下一个过滤器。如果一个过滤器失败或运行它的机器不再可用，管道可以重新安排该过滤器正在执行的工作，并将这项工作指向该组件的另一个实例。
- en: By using the proven pipes and filters pattern in conjunction with the compensating
    transaction pattern, there is an alternative approach to implement the complex
    distributed transactions. A distributed transaction can be broken down into separate
    and compensable tasks, each of which can be implemented by using a filter that
    also implements the compensating transaction pattern. The filters in a pipeline
    can be implemented as separate hosted tasks running close to the data that they
    maintain, thus there emerge newer possibilities.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合使用经过验证的管道和过滤器模式以及补偿事务模式，可以采用一种替代方法来实现复杂的分布式事务。分布式事务可以被分解为单独且可补偿的任务，每个任务都可以通过使用也实现了补偿事务模式的过滤器来实现。管道中的过滤器可以实施为在它们维护的数据附近运行的独立托管任务，从而出现新的可能性。
- en: 'For the container world, the preceding pattern is beneficial. That is, for
    taking the generated files out of a container, streaming the file to `stdout`
    leveraging the preceding *pipes and filters* pattern is being made out as an interesting
    workaround. This streaming has many advantages too:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于容器世界来说，前面的模式是有益的。也就是说，为了将生成的文件从容器中取出，利用前面的*管道和过滤器*模式将文件流式传输到`stdout`被视为一个有趣的解决方案。这种流式传输也有许多优点：
- en: The data doesn't have to end up in a file anymore as it can stay in memory.
    This offers faster access to the data.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据不再需要最终存储在文件中，因为它可以保持在内存中。这提供了对数据的更快访问。
- en: Using `stdout` allows sending the output directly to some other process using
    the pipe operator (`|`). Other processes may modify the output, then do the same
    thing, or store the final result in a file.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`stdout`允许通过管道操作符（`|`）直接将输出发送到其他进程。其他进程可以修改输出，然后执行相同操作，或者将最终结果存储在文件中。
- en: The exact location of files becomes irrelevant. There is no coupling through
    the filesystem if we only use `stdin` and `stdout`. The build container would
    not have to put its files in `/target`, and the build script would not have to
    look in `/target`, they just pass along data.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件的确切位置变得无关紧要。如果我们只使用`stdin`和`stdout`，则不会通过文件系统进行耦合。构建容器不需要将其文件放在`/target`中，构建脚本也不需要查找`/target`，它们只需传递数据。
- en: Containerized applications - Autopilot pattern
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器化应用程序 - 自动驾驶模式
- en: Deploying containerized applications and connecting them together is a definite
    challenge because typically, cloud-native applications are made up of hundreds
    of microservices. Microservice architectures provide organizations with a tool
    to manage the burgeoning complexity of the development process, and application
    containers provide a new means to manage the dependencies to accelerate the deployment
    of those microservices. But deploying and connecting those services together is
    still a challenge.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 部署容器化应用程序并将它们连接起来是一个明确的挑战，因为通常，云原生应用程序由数百个微服务组成。微服务架构为组织提供了一个管理开发过程日益增长的复杂性的工具，而应用程序容器提供了一种管理依赖关系的新方法，以加速微服务的部署。但是，部署和连接这些服务仍然是一个挑战。
- en: Operationalizing microservices-based applications brings forth several challenges.
    Developers have to embed several things inside for simplified deployment and delivery.
    Autopilot applications are a powerful design pattern for solving these problems.
    The autopilot pattern automates in the code the repetitive and boring operational
    tasks of an application, including start-up, shutdown, scaling, and recovery from
    anticipated failure conditions for reliability, ease of use, and improved productivity.
    By embedding the distinct responsibility and the operational tasks into the application,
    the workload on operational team members is bound to come down.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The autopilot pattern is for both developers and operators. It is for operators
    that want to bring sanity to their lives and for developers who want to make their
    applications easy to use. It is primarily for microservices applications and multi-tiered
    applications. Most importantly, it is designed to live and grow with our applications
    at all stages of development and operations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: The autopilot pattern automates the life cycle of each component of the application.
    There can be multiple components in any application. Web and application server,
    DB server, in-memory cache, reverse proxy, and so on, are the most prominent components
    for any application. Each of these components can be containerized and each container
    contributing for the application has its own life cycle. Most autopilot pattern
    implementations embrace single-purpose or single-service containers. The autopilot
    pattern does require developers and operators to think about how the application
    is operated at critical points in the life cycle of each component. The author
    of this unique pattern has provided some valid questions at [http://autopilotpattern.io/](http://autopilotpattern.io/),
    and those questions come in handy while designing the autopilot pattern.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: There are some applications emerging with at least some of this logic built
    in. Traefik is a proxy server with automatic discovery of its backends using Consul
    or other service catalogs. Traefik does not self-register in those service catalogs
    so that it can be used by other applications. ContainerPilot, a helper written
    in Golang that lives inside the container, can help with this.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: ContainerPilot provides microservices architectures with application orchestration,
    dependency management, health checks, error handling, lifecycle management, and
    linear and non-linear scaling of stateful services. Furthermore, it provides a
    private init system designed to live inside the container. It acts as a process
    supervisor, reaps zombies, runs health checks, registers the app in the service
    catalog, watches the service catalog for changes, and runs your user-specified
    code at events in the life cycle of the container to make it all work correctly.
    ContainerPilot uses Consul to coordinate global state among the application containers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Using a small configuration file, ContainerPilot can trigger events inside the
    container to automate operations on these events, including preStart (formerly onStart), health,
    onChange, preStop, and postStop.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个小型配置文件，ContainerPilot 可以在容器内部触发事件，从而自动化对这些事件的操作，包括 preStart（之前称为 onStart）、health、onChange、preStop
    和 postStop。
- en: Here is a sample scenario (readers can find the details at [http://autopilotpattern.io/example](http://autopilotpattern.io/example)).
    The author of this example has started with two services, sales, and customers.
    Nginx acts as a reverse proxy. Requests for `/customers/ go` to customer's, and
    `/sales/` to sales. The sales service needs to get some data from the customer's
    service to serve its requests, and vice versa. There are a few crucial problems
    here. The configuration is static. This prevents adding new instances and makes
    it harder to work with a failed instance. Configuring this stack via configuration
    management tools means packaging new dependencies with this application, but configuring
    statically means redeploying most of the containers every time a new instance
    gets added. There is a need for a mechanism to have the applications self-assemble
    and self-manage the everyday tasks, and hence there is a surging popularity for
    the autopilot pattern.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个示例场景（读者可以在 [http://autopilotpattern.io/example](http://autopilotpattern.io/example)
    找到详细信息）。本例的作者从两个服务开始，即销售和客户服务。Nginx 作为反向代理。对 `/customers/` 的请求会转发到客户服务，对 `/sales/`
    的请求会转发到销售服务。销售服务需要从客户服务获取一些数据来处理其请求，反之亦然。这里有几个关键问题。配置是静态的。这阻止了添加新实例，并使得处理失败的实例变得更加困难。通过配置管理工具配置此堆栈意味着将新的依赖项打包到该应用程序中，但静态配置意味着每次添加新实例时都需要重新部署大多数容器。需要一个机制让应用程序能够自我组装和自我管理日常任务，因此
    autopilot 模式越来越受欢迎。
- en: Engaging autopilot! The author of this autopilot design pattern has come out
    with the appropriate Dockerfile for the customer's service. It's a small Node.js
    application that listens on port `4000`. He uses Consul for service discovery
    and each service will send TTL heartbeats to Consul. All nodes know about all
    other nodes, and hence there is no need to use an external proxy or load balancer
    for communicating between the nodes. The diagram vividly illustrates everything.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 启用autopilot！这个autopilot设计模式的作者为客户服务推出了适当的Dockerfile。这是一个监听端口 `4000` 的小型Node.js应用程序。他使用Consul进行服务发现，每个服务都会向Consul发送TTL心跳。所有节点都知道所有其他节点，因此不需要使用外部代理或负载均衡器来在节点之间进行通信。图表生动地展示了所有内容。
- en: However, there is a need to make each of the services aware of Consul. For that,
    the author uses ContainerPilot. The source code and other implementation details
    are given at [https://github.com/autopilotpattern/workshop](https://github.com/autopilotpattern/workshop).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要让每个服务都意识到 Consul 的存在。为此，作者使用了 ContainerPilot。源代码和其他实现细节可以在 [https://github.com/autopilotpattern/workshop](https://github.com/autopilotpattern/workshop)
    找到。
- en: A re-usable Nginx base image got implemented according to the autopilot pattern for
    automatic discovery and configuration. The goal is to create a Nginx image that
    can be reused across environments without having to rebuild the entire image.
    The configuration of Nginx is entirely through ContainerPilot jobs and watch handlers,
    which update the Nginx configuration on disk through consul-template. The relevant
    details are supplied at [https://github.com/autopilotpattern/nginx](https://github.com/autopilotpattern/nginx).
    Similarly, there are autopilot implementations for other popular applications
    such as WordPress. Bringing a bevy of automation into various microservices-based
    software applications is gaining a lot of momentum.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 根据autopilot模式实现了可重用的Nginx基础镜像，用于自动发现和配置。目标是创建一个可以在不同环境中重用的 Nginx 镜像，而无需重建整个镜像。Nginx
    的配置完全通过 ContainerPilot 作业和 watch 处理器进行，这些处理器通过 consul-template 更新磁盘上的 Nginx 配置。相关细节可以在
    [https://github.com/autopilotpattern/nginx](https://github.com/autopilotpattern/nginx)
    找到。类似地，还有其他流行应用程序（如 WordPress）的 autopilot 实现。将大量自动化引入各种基于微服务的软件应用程序正在获得很大的动力。
- en: As indicated previously, a number of manual tasks are getting automated at different
    layers and levels, especially some of the crucial automation requirements are
    increasingly implemented at the application level. With the faster maturity and
    stability of the Docker platform, Docker containers are spreading their wings
    fast and wide. With the widespread availability of container management software
    solutions, microservices-based applications are gaining a lot of market and mind
    shares. Furthermore, there are a few service mesh frameworks, and hence the days
    of resilient microservices and reliable applications are not too far away. A growing
    bunch of automation capabilities is being attached to these applications, and
    this advancement enables the applications to exhibit adaptive behavior. Now, the
    autopilot pattern plays a key role in adding additional automation features and
    facilities.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，许多手动任务正在不同层次和级别上实现自动化，特别是某些关键的自动化需求越来越多地在应用层实现。随着Docker平台的快速成熟和稳定，Docker容器正在迅速扩展其翅膀。随着容器管理软件解决方案的广泛可用，基于微服务应用程序正在获得大量的市场份额和心智份额。此外，还有一些服务网格框架，因此弹性微服务和可靠应用程序的日子并不遥远。越来越多的自动化能力被附加到这些应用程序上，这种进步使得应用程序能够表现出适应性。现在，自动驾驶模式在添加额外的自动化功能和设施方面发挥着关键作用。
- en: Containers - persistent storage patterns
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器 - 持久化存储模式
- en: Typically, the container space originates with application containers that are
    not for permanently storing data. That is, when a container collapses, the data
    stored or buffered in the container gets lost. However, the aspect of data persistence
    is insisted for several reasons, including the realization of stateful applications,
    and hence fresh mechanisms are being worked out in order to safely and securely
    persist data in containers. Therefore, for persisting data, additional container
    types, such as data or volume containers were introduced.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，容器空间起源于非永久存储数据的应用程序容器。也就是说，当容器崩溃时，存储或缓存在容器中的数据会丢失。然而，出于几个原因，包括实现有状态应用程序，数据持久性方面被强调，因此正在开发新的机制以确保在容器中安全地持久化数据。因此，为了持久化数据，引入了额外的容器类型，例如数据或卷容器。
- en: The context for persistent storages
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化存储的上下文
- en: There is a concern widely expressed that containers are great for stateless
    applications, but are not so good for stateful applications that persist data.
    Thus, persistent storage patterns are acquiring special significance in the container
    world. A brief description about stateless and stateful applications is given
    as follows paragraph.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛表达的一个担忧是，容器非常适合无状态应用程序，但并不适合持久化数据的具有状态的应用程序。因此，持久化存储模式在容器世界中获得了特殊的重要性。以下段落简要介绍了无状态和有状态应用程序。
- en: A random number generator is stateless because we get a different value from
    it every time we run it. We could easily Dockerize it and if the instance fails,
    we can have it running in another host instantaneously to continue the service
    without any break and lag. The instance's behavior remains the same in the new
    host as well. However, that is not the case with our bank accounts. If the bank
    account application has to be re-provisioned on a new server, it has to have the
    original data from the first server instance.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 随机数生成器是无状态的，因为我们每次运行它都会得到不同的值。我们可以轻松地将其Docker化，如果实例失败，我们可以在另一个主机上瞬间启动它以继续服务，而不会出现任何中断和延迟。在新主机上，实例的行为也保持不变。然而，我们的银行账户并非如此。如果银行账户应用程序需要在新的服务器上重新部署，它必须具有来自第一个服务器实例的原始数据。
- en: Here is a stateful data categorization. Typically, configuration data, including
    keys and other secrets, is often written to disk in various files. This data is
    easy to recover when provisioning instances. User-generated content includes text,
    video, or bank transactions. There are dynamic configuration details. The suitable
    example is of those services *A* and *B* connecting with one another. Connecting
    an application/service to its backend database system is another prominent example.
    Typically, applications/services treat this discovery and connectivity as configuration
    data along with other configuration details. In order for an application to be
    scalable and resilient, it is necessary to update this configuration information
    while the application is running. That is, as we add or remove instances of a
    service, we have to update all the other service instances that connect to the
    service. Otherwise, the intended performance increment would not happen. Other
    pertinent configuration details can include performance-tuning parameters. These
    configuration details can be stocked in the application repository in order to
    facilitate the application/service versioning and easier tracking. The other option
    for configuration information is leveraging the dynamic storage so they can be
    changed without re-building and re-deploying the application. It is also possible
    to do automatic replication of repository contents to the configuration store
    using a tool such as `git2consul`. The best practice is to keep configuration
    data and templates in a consistent distributed key/value data store.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有状态的数据分类。通常，配置数据，包括密钥和其他秘密，经常被写入各种文件中。在配置实例时，这些数据很容易恢复。用户生成的内容包括文本、视频或银行交易。存在动态配置细节。合适的例子是服务
    *A* 和 *B* 之间的连接。将应用程序/服务连接到其后端数据库系统是另一个突出的例子。通常，应用程序/服务将这种发现和连接视为配置数据，以及其他配置细节。为了使应用程序可扩展和具有弹性，在应用程序运行时更新此配置信息是必要的。也就是说，当我们添加或删除服务的实例时，我们必须更新所有连接到该服务的其他服务实例。否则，预期的性能提升就不会发生。其他相关的配置细节可能包括性能调整参数。这些配置细节可以存储在应用程序仓库中，以方便应用程序/服务版本控制和更容易的跟踪。配置信息的另一种选择是利用动态存储，这样它们可以在不重新构建和重新部署应用程序的情况下进行更改。使用像
    `git2consul` 这样的工具自动复制仓库内容到配置存储也是可能的。最佳实践是将配置数据和模板保存在一致的分布式键/值数据存储中。
- en: The persistent storage options
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久性存储选项
- en: Containers are meant to be ephemeral, and so scale pretty well for stateless
    applications. Stateful containers, however, need to be treated differently. For
    stateful applications, a persistent storage mechanism has to be there for the
    container idea to be right and relevant. Containers can be developed and dismantled
    without the data persistence. The data resides within the container. If there
    is any change, then the data gets lost. For some situations, this data loss is
    not a big issue. For certain scenarios, the data loss is not accepted; the data
    persistence feature has to be there. The solution approach prescribed by Docker
    is given in the following section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 容器旨在是短暂的，因此对于无状态应用程序来说，扩展得相当好。然而，有状态容器需要被不同对待。对于有状态应用程序，容器概念要正确且相关，必须存在持久性存储机制。容器可以在没有数据持久性的情况下开发和拆卸。数据位于容器内。如果发生任何变化，则数据会丢失。对于某些情况，这种数据丢失不是一个大问题。对于某些场景，数据丢失是不被接受的；必须存在数据持久性功能。Docker
    提供的解决方案在下一节中给出。
- en: 'It is possible to store data within the writable layer of a container, but
    there are a few downsides:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器的可写层中存储数据是可能的，但有一些缺点：
- en: The data won't persist when that container is no longer running, and it can
    be difficult to get the data out of the container if another process needs it.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当该容器不再运行时，数据不会持久化，如果另一个进程需要这些数据，从容器中获取数据可能会很困难。
- en: A container's writable layer is tightly coupled to the host machine where the
    container is running. Moving the data somewhere else is a difficult affair.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器的可写层与容器运行的宿主机紧密耦合。将数据移动到其他地方是一件困难的事情。
- en: Writing into a container's writable layer requires a storage driver to manage
    the filesystem. The storage driver provides a union filesystem, using the Linux
    kernel. This extra abstraction reduces performance as compared to using *data
    volumes*, which write directly to the host filesystem.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向容器的可写层写入需要**存储驱动**来管理文件系统。存储驱动通过使用Linux内核提供联合文件系统。这种额外的抽象与使用直接写入宿主机文件系统的**数据卷**相比会降低性能。
- en: 'Docker offers three different ways to mount data into a container from the
    Docker host: *volumes*, *bind mounts*, or *tmpfs** mounts*. Volumes are almost
    always the right choice. Volumes are the preferred mechanism for persisting data
    generated by and used by Docker containers. While bind mounts are dependent on
    the directory structure of the host machine, volumes are completely managed by
    Docker. Volumes have several advantages over bind mounts:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Docker提供了三种不同的方式将数据从Docker宿主机挂载到容器中：**卷**、**绑定挂载**或**tmpfs**挂载。卷几乎总是正确的选择。卷是用于持久化由Docker容器生成和使用的数据的首选机制。虽然绑定挂载依赖于宿主机的目录结构，但卷完全由Docker管理。卷相对于绑定挂载有以下几个优点：
- en: Volumes are easier to back up or migrate than bind mounts
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷比绑定挂载更容易备份或迁移
- en: Volumes are easy to manage by using Docker CLI commands or the Docker API
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker CLI命令或Docker API，卷易于管理
- en: Volumes work on both Linux and Windows containers
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷可以在Linux和Windows容器上工作
- en: Volumes can be more safely shared among multiple containers
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷可以在多个容器之间更安全地共享
- en: Volume drivers allow storing volumes on remote hosts or cloud providers, to
    encrypt the contents of volumes, or to add other functionality
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷驱动允许在远程主机或云提供商上存储卷，加密卷的内容，或添加其他功能
- en: A new volume's contents can be pre-populated by a container
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新卷的内容可以被容器预先填充
- en: Volumes are often a better choice than persisting data in a container's writable
    layer, because using a volume does not increase the size of containers using it,
    and the volume's contents exist outside the life cycle of a given container.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 卷通常比在容器的可写层持久化数据是一个更好的选择，因为使用卷不会增加使用它的容器的尺寸，并且卷的内容存在于特定容器的生命周期之外。
- en: If a container generates non-persistent state data, then consider using a tmpfs
    mount to avoid storing the data anywhere permanently, and to increase the container's
    performance by avoiding writing into the container's writable layer.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器生成非持久状态数据，那么考虑使用**tmpfs**挂载来避免永久存储数据，并通过避免写入容器的可写层来提高容器的性能。
- en: 'All the three options are discussed as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种选项如下所述：
- en: '**Volumes** are stored in a part of the host filesystem that is *managed by
    Docker* (`/var/lib/docker/volumes/` on Linux). Non-Docker processes cannot modify
    this part of the filesystem.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷**存储在宿主机文件系统中由Docker管理的部分（在Linux上为`/var/lib/docker/volumes/`）。非Docker进程不能修改文件系统的这部分。'
- en: '**Bind mounts** may be stored *anywhere* on the host system. They may even
    be important system files or directories. Non-Docker processes on the Docker host
    or a Docker container can modify them at any time.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**绑定挂载**可以存储在宿主系统的任何位置。它们甚至可能是重要的系统文件或目录。Docker宿主机上的非Docker进程或Docker容器可以在任何时间修改它们。'
- en: '**The tmpfs ****mounts** are stored in the host system''s memory only and are
    never written to the host system''s filesystem.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tmpfs**挂载仅存储在宿主系统的内存中，并且永远不会写入宿主系统的文件系统。'
- en: Let's discuss more about them.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地讨论它们。
- en: Volumes
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷
- en: We can create a volume explicitly using the `docker volume create` command,
    or Docker can create a volume during container or service creation. When we create
    a volume, it is stored in a directory on the Docker host. When we mount the volume
    into a container, this is the directory that is mounted on the container. This
    is similar to the way that bind mounts work, except that volumes are managed by
    Docker and are isolated from the core functionality of the host machine.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`docker volume create`命令显式创建卷，或者Docker可以在创建容器或服务时创建卷。当我们创建卷时，它存储在Docker宿主机上的一个目录中。当我们将卷挂载到容器中时，这就是挂载到容器上的目录。这与绑定挂载的工作方式类似，只是卷由Docker管理，并且与宿主机的核心功能隔离。
- en: A given volume can be mounted into multiple containers simultaneously. When
    no running container is using a volume, the volume is still available to Docker
    and is not removed automatically. You can remove unused volumes using `docker
    volume prune`. Volumes also support the use of *volume drivers*, which allow the
    storing of data on remote hosts, cloud providers, and so on.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 某个卷可以同时挂载到多个容器中。当没有正在运行的容器使用卷时，该卷仍然对 Docker 可用，并且不会自动删除。您可以使用 `docker volume
    prune` 命令删除未使用的卷。卷还支持使用 *volume drivers*，这允许在远程主机、云提供商等地方存储数据。
- en: Bind mounts
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绑定挂载
- en: When we use a bind mount, a file or directory on the *host machine* is mounted
    on a container. The file or directory is referenced by its full path on the host
    machine. The file or directory does not need to exist on the Docker host already
    and it can be created on demand. Bind mounts are very performant, but they rely
    on the host machine's filesystem having a specific directory structure available.
    It is not possible to use Docker CLI commands to directly manage bind mounts.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用绑定挂载时，主机机器上的文件或目录会被挂载到容器中。文件或目录通过主机机器上的完整路径进行引用。文件或目录不需要在 Docker 主机上已经存在，并且可以在需要时创建。绑定挂载性能非常好，但它们依赖于主机机器的文件系统具有特定的目录结构。无法使用
    Docker CLI 命令直接管理绑定挂载。
- en: The tmpfs mounts
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: tmpfs 挂载
- en: A tmpfs mount is not persisted on disk either on the Docker host or within a
    container. It can be used by a container during the lifetime of the container,
    to store non-persistent state or sensitive information. For instance, internally,
    swarm services use tmpfs mounts to mount secrets into a service's containers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: tmpfs 挂载在 Docker 主机或容器内部都不会持久化到磁盘。它可以在容器生命周期内被容器使用，以存储非持久状态或敏感信息。例如，内部，swarm
    服务使用 tmpfs 挂载将秘密挂载到服务的容器中。
- en: Docker compose configuration pattern
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Compose 配置模式
- en: We are increasingly hearing, reading, and even experiencing multi-container
    applications. That is, composite applications are being achieved through multi-container
    composition. The composition technique acquires special significance because of
    two key trends. Firstly, the powerful concept of microservices is gradually changing
    the IT industry. That is, large monolithic services are slowly giving way to swarms
    of small and autonomous microservices. Different and distributed microservices
    are being found, checked and chained together to create and run business-class,
    production-ready, process-aware, mission-critical, enterprise-grade, composite
    applications. The second is that the Docker-enabled containerization changes not
    only the architecture of services but also the structure of environments used
    to create them. Now, software gets methodically containerized, stocked, and distributed
    and developers gain the full freedom to choose the preferred applications. Resultantly,
    even complex environments such as **continuous integration** (**CI**) servers
    with database backend systems and analytical infrastructure can be instantiated
    within seconds. In short, software development, deployment, and delivery become
    easier and faster.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们越来越频繁地听到、阅读，甚至体验到多容器应用。也就是说，复合应用是通过多容器组合来实现的。这种组合技术由于两个关键趋势而具有特殊意义。首先，强大的微服务概念正在逐渐改变
    IT 行业。也就是说，大型单体服务正在逐渐让位于小型和自主的微服务集群。不同的分布式微服务正在被发现、检查并连接在一起，以创建和运行业务级、生产就绪、流程感知、任务关键、企业级复合应用。第二是
    Docker 容器化不仅改变了服务的架构，也改变了创建它们的环境的结构。现在，软件被系统地容器化、存储和分发，开发者获得了选择首选应用程序的完全自由。结果，即使是像
    **持续集成** (**CI**) 服务器这样的复杂环境，带有数据库后端系统和分析基础设施，也可以在几秒钟内实例化。简而言之，软件开发、部署和交付变得更加容易和快速。
- en: 'Docker Compose is a tool for defining and running complex applications with
    Docker. With Compose, it is possible to define a multi-container application in
    a single file, and then spin the application up in a single command that does
    everything that needs to be done to get it running. Using Compose is basically
    a three-step process:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Compose 是一个用于定义和运行复杂应用的 Docker 工具。使用 Compose，可以在单个文件中定义一个多容器应用，然后通过一个命令启动应用，该命令会完成所有必要的操作以使其运行。使用
    Compose 基本上是一个三步过程：
- en: Define your application's environment with a Dockerfile so it can be reproduced
    anywhere
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Dockerfile 定义您应用程序的环境，以便它可以在任何地方重现
- en: Define the services that make up the application in `docker-compose.yml` so
    they can be run together in an isolated environment
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`docker-compose.yml`中定义构成应用程序的服务，以便它们可以在隔离环境中一起运行
- en: Lastly, run `docker-compose up` and compose will start and run the entire application
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，运行`docker-compose up`，Compose将启动并运行整个应用程序
- en: We can pass in environment variables via Docker Compose in order to realize
    a container image once and reuse it on any environment (development, staging,
    and production). With this approach, it is possible to develop compose-centric
    containers that require a piece of configuration management for handling pre-start
    events based on the values of the environment variables. The author of this pattern
    has detailed the source code at [https://github.com/jay-johnson/docker-schema-prototyping-with-mysql](https://github.com/jay-johnson/docker-schema-prototyping-with-mysql).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过Docker Compose传递环境变量，以实现一次创建容器镜像并在任何环境中（开发、测试和生产）重用。采用这种方法，可以开发以Compose为中心的容器，这些容器需要一些配置管理来处理基于环境变量值的启动前事件。该模式的作者详细介绍了源代码在[https://github.com/jay-johnson/docker-schema-prototyping-with-mysql](https://github.com/jay-johnson/docker-schema-prototyping-with-mysql)。
- en: The author has built this project for rapid-prototyping a database schema using
    a MySQL Docker container that deploys its own ORM schema file and populates the
    initial records on startup. By setting a couple of environment variables, it is
    possible to provide our own Docker container with a usable MySQL instance, browser-ready
    phpMyAdmin server, and our database, including the tables, initialized exactly
    how we want. Interested readers are requested to visit the preceding page to get
    finer details on this unique pattern.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 作者构建了这个项目，用于使用MySQL Docker容器快速原型设计数据库模式，该容器部署自己的ORM模式文件并在启动时填充初始记录。通过设置几个环境变量，我们可以为我们自己的Docker容器提供一个可用的MySQL实例、浏览器就绪的phpMyAdmin服务器以及我们的数据库，包括表，初始化成我们想要的样子。有兴趣的读者请访问前面的页面，以获取有关这种独特模式的更详细信息。
- en: Docker container anti-patterns
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker容器反模式
- en: We have discussed most of the available container-specific patterns in the previous
    section. Many exponents and evangelists of Docker-enabled containerization have
    brought in a few anti-patterns based on their vast experience in developing, deploying,
    and delivering containerized services and applications. This section is exclusively
    allocated for conveying the anti-patterns discovered and disseminated by Docker
    practitioners.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中讨论了大多数可用的容器特定模式。许多Docker容器化的倡导者和专家基于他们在开发、部署和交付容器化服务和应用程序方面的丰富经验，引入了一些反模式。本节专门用于传达Docker实践者发现和传播的反模式。
- en: Container creation and deployment are becoming easier and faster with the ready
    availability of both open-source and commercial-grade tools. DevOps team members
    ought to learn some of the techniques and tips in order to avoid mistakes when
    migrating to Docker.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 随着开源和商业级工具的随时可用，容器创建和部署正变得越来越容易和快速。DevOps团队成员应该学习一些技术和技巧，以避免在迁移到Docker时犯错误。
- en: Installing an OS inside a Docker container
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Docker容器内安装操作系统
- en: There is rarely a good reason to host an entire OS inside a container using
    Docker. There are platforms for generating and running system containers. The
    Docker platform is specially crafted and fine-tuned for producing application
    containers. That is, applications and their runtime dependencies are being stuffed
    together, packaged, and transmitted to their destinations.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Docker在容器内托管整个操作系统很少有一个合理的理由。有平台可以生成和运行系统容器。Docker平台专门设计和微调，用于生成应用程序容器。也就是说，应用程序及其运行时依赖项被组合在一起，打包并传输到目的地。
- en: Go for optimized Docker images
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择优化的Docker镜像
- en: When building container images, we should include only the services that are
    absolutely essential for the application the container will host. Anything extra
    wastes resources and widens the potential attack vector that could ultimately
    lead to security problems. For example, it is not good to run an SSH server inside
    the container because we can use the Docker *exec *call to interact with the containerized
    application. The related suggestions here are to create a new directory and include
    the Dockerfile and other relevant files in that directory. Also consider using
    `.dockerignore` to remove any logs, source code, and so on before creating the
    image. Furthermore, make it a habit to remove any downloaded artifacts after they
    are unzipped.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建容器镜像时，我们应该只包含容器将要托管的应用程序绝对必需的服务。任何额外的服务都会浪费资源并扩大潜在的攻击向量，最终可能导致安全问题。例如，在容器内运行SSH服务器是不好的，因为我们可以使用Docker的*exec*调用来与容器化应用程序交互。相关的建议是创建一个新的目录，并将Dockerfile和其他相关文件包含在该目录中。还应该考虑在创建镜像之前使用`.dockerignore`来删除任何日志、源代码等。此外，养成在解压后删除任何下载的工件的习惯。
- en: It is not correct to use different images or even different tags in development,
    testing, staging, and production environments. The image that is the *source of
    truth* should be created once and pushed to a repository. That image should be
    used for different environments going forward. Any system integration testing
    should be done on the image that will be pushed into production.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发、测试、预生产和生产环境中使用不同的镜像或甚至不同的标签是不正确的。作为*真相来源*的镜像应该一次性创建并推送到仓库。该镜像应继续用于不同的环境。任何系统集成测试都应该在将要推送到生产的镜像上进行。
- en: The containers produced by the Docker image should be as ephemeral as possible.
    By *ephemeral*, it is meant that it can be stopped and destroyed and a new one
    can be built and put in place with an absolute minimum of setup and configuration.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由Docker镜像生成的容器应尽可能短暂。这里的“短暂”是指它可以被停止和销毁，并且可以以绝对最小的设置和配置来构建和部署新的容器。
- en: The best practice is to not keep critical data inside containers. There are
    two prime reasons for this. When containers collapse inadvertently or deliberately,
    the data inside them gets lost immediately. The second reason is that the security
    situation of containers is not as good as virtual machines, and hence storing
    confidential, critical, customer, and corporate information, inside containers
    is not a way forward. For persisting data, there are mechanisms to be used. The
    popular ELK stack could be used to store and process logs. If managed volumes
    are used during the early testing process, then it is recommended to remove them
    using the `-v` switch with the `docker rm` command.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是不在容器中保留关键数据。有两个主要原因。当容器意外或故意崩溃时，容器内的数据会立即丢失。第二个原因是容器的安全性不如虚拟机，因此将机密、关键、客户和公司信息存储在容器中并不是一个好的发展方向。对于持久化数据，有可用的机制。流行的ELK堆栈可以用来存储和处理日志。如果在早期测试过程中使用了管理卷，那么建议使用`docker
    rm`命令的`-v`开关来删除它们。
- en: Also, do not store any security credentials in the Dockerfile. They are in clear
    text and this makes them completely vulnerable. Do not forget to use `-e` to specify
    passwords as runtime environment variables. Alternatively, `--env-file` can be
    used to read environment variables from a file. Also, go for CMD or ENTRYPOINT to
    specify a script, and this script will pull the credentials from a third party
    and then configure the application.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，不要在Dockerfile中存储任何安全凭证。它们是明文形式，这使得它们完全易受攻击。不要忘记使用`-e`来指定密码作为运行时环境变量。或者，可以使用`--env-file`从文件中读取环境变量。此外，选择`CMD`或`ENTRYPOINT`来指定脚本，该脚本将从第三方获取凭证然后配置应用程序。
- en: Storing container images only inside a container registry
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仅在容器注册表中存储容器镜像
- en: A container registry is designed solely for the purpose of hosting container
    images. It is not good to use the registry as a general-purpose repository for
    hosting other types of data.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 容器注册表专为托管容器镜像而设计。将注册表用作通用存储库来托管其他类型的数据是不合适的。
- en: Hosting only one service inside a container
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仅在容器内托管一个服务
- en: In the microservices world, applications are being partitioned into a dynamic
    collection of interactive, single-purpose, autonomous, API-driven, easily manageable,
    and composable services*.* Containers emerge as the best-in-class runtime environment
    for microservices. Thus, it is logical to have one service inside a container.
    Thus, for running an application, multiple containers need to be leveraged for
    running many services. For example, one container would install and use MySQL,
    WordPress, possibly even phpMyAdmin, nginx, and an SSH daemon. Also, multiple
    instances of a service can be hosted in different containers. The redundancy capability
    being achieved through containers goes a long way in ensuring the business continuity
    through fault-tolerance, high availability, horizontal scalability, independent
    deployment, and so on. Now, with the emergence of powerful container orchestration
    platforms, distributed and multiple containers can be linked up to come out with
    composite applications. An advantage of containerization is the ability to quickly
    re-build images in the case of a security issue, for example, and roll out a whole
    new set of containers quickly. And because containers are single-concern, there
    is no need to redeploy the cloud infrastructure every time. Similarly, multiple
    Docker images can be built from a base image. Furthermore, containers can be also
    converted to new images.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: We can use the CMD and ENTRYPOINT commands while formulating a Dockerfile. Often,
    CMD will use a script that will perform some configurations of the image and then
    start the container. It is better to avoid starting multiple processes using that
    script. This will make managing containers, collecting logs, and updating each
    individual process hard. That is, we need to follow the *separation of concerns* pattern
    when creating Docker images. Breaking up an application into multiple containers
    and managing them separately is the way forward.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Latest doesn't mean best
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is incredibly tempting when writing a Dockerfile to grab the latest version
    of every dependency. The *golden rule *though is to create containers with known
    and stable versions of the system and dependencies that we know our software will
    work on.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Docker containers with SSH
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A related and equally unfortunate practice is to bake an SSH daemon into an
    image. Having an SSH daemon inside a container may lead to undocumented, untraceable
    changes to the container infrastructure, but Docker containers are being touted
    as the immutable infrastructure.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few use cases for SSHing into a container:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Update the OS, services, or dependencies
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git pull or update any application in some other fashion
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check logs
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backup some files
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restart a service
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instead of using SSH, it is recommended to use the following mechanisms:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Make the change in the container Dockerfile, rebuild the image, and deploy the
    container.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use an environment variable or configuration file accessible via volume sharing
    to make the change and possibly restart the container.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用环境变量或通过卷共享可访问的配置文件来执行更改并可能重新启动容器。
- en: As indicated before, use `docker exec`. The `docker exec` command starts a new command
    in a running container, and hence has to be the last resort.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，使用`docker exec`。`docker exec`命令在运行的容器中启动一个新的命令，因此必须是最后的手段。
- en: IP addresses of a container
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器的IP地址
- en: Each container get assigned with an IP address. In a containerized environment,
    multiple containers have to interact with one another in order to achieve business
    goals. Also, containers are terminated often and fresh containers are being created.
    Thus, relying upon IP addresses of containers for initiating container communication
    is beset with real challenges. The preferred approach is to create services. This
    will provide a logical name that can be referred to independent of the growing
    and shrinking number of containers. And it also provides a basic load balancing.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 每个容器都会分配一个IP地址。在容器化环境中，多个容器必须相互交互以实现业务目标。此外，容器经常被终止，新的容器正在被创建。因此，依赖于容器的IP地址来启动容器通信面临着真正的挑战。首选的方法是创建服务。这将提供一个逻辑名称，可以独立于容器数量的增加和减少而被引用。它还提供了基本的负载均衡。
- en: Also, do not use `-p` to publish all the exposed ports. This facilitates in
    running multiple containers and publishing their exposed ports. But this comes
    with a price. That is, all the ports will be published, resulting in a security
    risk. Instead, use `-p` to publish specific ports.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，不要使用`-p`来发布所有暴露的端口。这有助于运行多个容器并发布它们的暴露端口。但这也带来了一定的代价。那就是，所有端口都将被发布，从而带来安全风险。相反，使用`-p`来发布特定的端口。
- en: Root user
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Root用户
- en: This is a security-mitigation tip. Don't run containers as a root user. The
    host and the container share the same kernel. If the container is compromised,
    a root user can do more damage to the underlying hosts. Instead, create a group
    and a user in it. Use the user instruction to switch to that user. Each user creates
    a new layer in the image. Also, avoid switching the user back and forth to reduce
    the number of layers.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个安全缓解技巧。不要以root用户身份运行容器。主机和容器共享相同的内核。如果容器被入侵，root用户可以对底层主机造成更大的损害。相反，创建一个组并在其中创建一个用户。使用用户指令切换到该用户。每个用户在镜像中创建一个新的层。此外，避免来回切换用户以减少层的数量。
- en: Dependency between containers
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器之间的依赖关系
- en: Often, applications rely upon containers to be started in a certain order. For
    example, a database container must be up before an application can connect to
    it. The application should be resilient to such changes as the containers may
    be terminated or started at any time. In this case, have the application container
    wait for the database connection to succeed before proceeding further. Do not
    use *wait-for* scripts in a Dockerfile for the containers to start up in a specific
    order.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，应用程序依赖于容器以特定顺序启动。例如，数据库容器必须启动，应用程序才能连接到它。应用程序应该对这种变化具有弹性，因为容器可能随时被终止或启动。在这种情况下，让应用程序容器在继续之前等待数据库连接成功。不要在Dockerfile中使用*wait-for*脚本来指定容器的启动顺序。
- en: In conclusion, containers are the new and powerful unit of development, deployment,
    and execution. Business applications, IT platforms, databases, and middleware
    are formally containerized and stocked in publically available and accessible
    image repositories so that software developers can pick up and leverage them for
    their software-building requirements. The system portability is a key advantage.
    The easier and faster maneuverability, testability, and composability of container
    images are being touted as the most promising and potential advantages of containerization.
    The inevitability of distributed computing is greatly simplified by the concept
    of containerization. Multiple containers across clusters can be easily linked
    to realizing smart and sophisticated applications.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，容器是开发、部署和执行的新颖而强大的单元。业务应用程序、IT平台、数据库和中间件正式容器化并存储在公开可用的和可访问的镜像存储库中，以便软件开发人员可以挑选并利用它们来满足他们的软件开发需求。系统可移植性是一个关键优势。容器镜像的更容易、更快的机动性、可测试性和可组合性被吹捧为容器化的最有希望和最有潜力的优势。分布式计算的概念极大地简化了分布式计算的必然性。跨集群的多个容器可以轻松连接，以实现智能和复杂的应用程序。
- en: Patterns for highly reliable applications
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可靠性应用的模式
- en: 'The IT systems are indispensable for business automation. The widely articulated
    challenge for our IT and business applications is to showcase high reliability.
    Systems ought to be responsive, resilient, elastic, and secure in order to intrinsically
    demonstrate the required dependability. Systems are increasingly multimodal and
    multimedia. Systems have to capture, understand, and exhibit the appropriate behavior.
    Also, systems have to respond all the time under any circumstance. Also, with
    the dawn of big data, distributed computing is all set to the mainstream compute
    model. In this section, we will discuss the prominent patterns for constructing
    reliable systems for professional as well as personal requirements. The promising
    approaches include:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: IT系统对于业务自动化是不可或缺的。我们IT和业务应用广泛面临的挑战是展示高可靠性。系统应该具有响应性、弹性、可伸缩性和安全性，以内在地展示所需的可靠性。系统越来越多地是多模态和多媒体的。系统必须捕捉、理解和展示适当的行为。此外，系统必须在任何情况下都能随时响应。此外，随着大数据时代的到来，分布式计算已经成为主流的计算模型。在本节中，我们将讨论构建可靠系统的突出模式，以满足专业和个人需求。有希望的方法包括：
- en: Reactive and cognitive programming techniques
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反应性和认知编程技术
- en: Resilient microservices
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性微服务
- en: Containerized cloud environments
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器化云环境
- en: In a distributed system, failures are bound to happen because of multiple moving
    parts and the sickening dependencies between the participating systems' modules.
    Hardware can fail, the application may go down, and the network can have transient
    failures. Rarely, an entire service or region may experience a disruption. Clouds
    are emerging as the one-stop IT solution.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式系统中，由于多个移动部分和参与系统模块之间令人恶心的依赖关系，故障是不可避免的。硬件可能失败，应用程序可能崩溃，网络可能发生瞬态故障。很少，整个服务或区域可能会出现中断。云正在成为一站式IT解决方案。
- en: '**Resiliency** is the ability of a system to withstand and tolerate faults
    in order to function continuously. Even if it fails, it has the wherewithal to
    bounce back to the original state. Precisely speaking, it is all about not avoiding
    failures but how quickly it can recover from the failures to serve without any
    breakdown and slowdown. Also, a fault in a component of a system should not cascade
    into other components in order to bring down the whole system. There are resiliency
    strategies, patterns, best practices, approaches, and techniques.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性**是系统承受和容忍故障以持续运行的能力。即使它失败了，它也有能力反弹回原始状态。确切地说，这关乎的不是避免故障，而是它从故障中恢复的速度有多快，以便在没有中断和减速的情况下提供服务。此外，系统组件中的故障不应级联到其他组件，以使整个系统崩溃。有弹性策略、模式、最佳实践、方法和技术。 '
- en: '**High availability **(**HA**) is the ability of the application to continue
    running in a healthy state, without significant downtime. That is, the application
    continues to be responsive to users'' requests.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**高可用性**（**HA**）是应用程序在健康状态下继续运行的能力，没有显著的中断时间。也就是说，应用程序继续响应用户的请求。'
- en: '**Disaster recovery **(**DR**) is the ability to recover from rare but major
    incidents: non-transient, wide-scale failures, such as service disruption that
    affects an entire region. Disaster recovery includes data backup and archiving,
    and may include manual intervention, such as restoring a database from backup.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**灾难恢复**（**DR**）是从罕见但重大的事件中恢复的能力：非瞬态、广泛范围的故障，例如影响整个区域的服务中断。灾难恢复包括数据备份和存档，可能包括手动干预，例如从备份中恢复数据库。'
- en: 'Resiliency must be designed into the system, and here is a general model to
    follow:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性必须设计到系统中，以下是一个通用的模型供参考：
- en: '**Define** the application availability requirements based on business needs.'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义**基于业务需求的应用可用性要求。'
- en: '**Design** the application architecture for resiliency. Start with an architecture
    that follows proven practices and architectural decisions, and then identify the
    possible failure points in that architecture. Take care of the dependencies. Also,
    choose the best-in-class architectural patterns and styles that intrinsically
    support resiliency.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设计**具有弹性的应用程序架构。从一个遵循既定实践和架构决策的架构开始，然后确定该架构中可能出现的故障点。注意处理依赖关系。同时，选择支持弹性的最佳架构模式和风格。'
- en: '**Develop** the application using the appropriate design patterns and incorporate strategies
    to detect and recover from failures.'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**开发**应用程序时使用适当的设计模式，并纳入检测和从故障中恢复的策略。'
- en: '**Build and test** the implementation by simulating faults and triggering forced
    failovers and debug the identified issues to the fullest.'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过模拟故障和触发强制故障转移来**构建和测试**实现，并彻底调试识别出的问题。
- en: '**Decide** the infrastructure capacity accordingly and **provision** them.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要**决定**基础设施容量并**提供**它们。
- en: '**Deploy** the application into production using a reliable and repeatable
    process.'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署** 应用程序到生产环境，使用可靠且可重复的过程。'
- en: '**Monitor** the application to detect failures. The monitoring activity helps
    to gauge the health of the application. The health check comes in handy in providing
    instantaneous responses.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监控** 应用程序以检测故障。监控活动有助于评估应用程序的健康状况。健康检查在提供即时响应时非常有用。'
- en: '**Respond** if there are incidents that require any kind of manual interventions.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**响应** 如果发生需要任何手动干预的事件。'
- en: Resiliency implementation strategies
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性实现策略
- en: As the resiliency requirement is insisted, IT departments of various business
    enterprises are exploring various ways and means in order to build and release
    resilient application services. At different levels (infrastructure, platform,
    database, middleware, network, and application), the virtue of resiliency is being
    mandated so that the whole system and environment become resilient.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 随着弹性要求的坚持，各个商业企业的IT部门正在探索各种方法和手段，以构建和发布弹性应用程序服务。在不同的级别（基础设施、平台、数据库、中间件、网络和应用程序）上，弹性价值正在被强制执行，以便整个系统和环境变得弹性。
- en: 'In this section, we will dig deeper and describe how the elusive target of
    resiliency is being endeavored and enunciated to see the reality. There are a
    few noteworthy failures. The key ones are include as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨，描述如何努力实现和阐述弹性目标，以了解现实情况。有一些值得注意的故障。其中关键的一些包括以下内容：
- en: '**Retry transient failures**: Transient failures can occur due to many causes,
    deficiencies, and disturbances. Often, a temporary failure can be resolved simply
    by retrying the request. However, each retry adds to the total latency. Also,
    too many failed requests can cause a bottleneck as pending requests accumulate
    in the queue. These blocked requests might hold critical system resources such
    as memory, threads, database connections, and so on. A sellable workaround here
    is to increase the delay between each retry attempt and limit the total number
    of failed requests.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重试短暂故障**：短暂故障可能由许多原因、缺陷和干扰引起。通常，通过重试请求可以简单地解决临时故障。然而，每次重试都会增加总延迟。此外，过多的失败请求可能导致瓶颈，因为挂起的请求在队列中积累。这些阻塞的请求可能持有关键系统资源，如内存、线程、数据库连接等。在这种情况下，一个可行的解决方案是增加每次重试尝试之间的延迟，并限制失败请求的总数。'
- en: '**Load balance across instances**: This is a common thing happening in IT environments.
    A **load balancer** (**LB**) instance in front of an application facilitates adding
    more application instances in order to improve resiliency.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨实例负载均衡**：这是IT环境中常见的事情。位于应用程序前面的**负载均衡器**（**LB**）实例有助于添加更多应用程序实例，以提高弹性。'
- en: '**Replicating data**: It has been a standard approach for handling non-transient
    failures in a database and filesystem. The data storage technologies innately
    provide built-in replication. However, to fulfill the high-availability requirement,
    replicas are being made and put up in geographically distributed locations. So,
    if one region goes down, the other region can take care of the business continuity.
    However, this significantly increases the latency when replicating the data across
    the regions. Typically, considering the long distance between regions, the data
    replication happens in an asynchronous fashion. In this case, we can not expect
    real-time and strong consistency. Instead, we need to settle for eventual consistency.
    Corporates have to tolerate for potential data loss if a replica fails.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制数据**：在数据库和文件系统中处理非短暂故障已经是一种标准方法。数据存储技术本身提供内置的复制功能。然而，为了满足高可用性要求，副本正在被制作并放置在地理上分布的位置。因此，如果一个区域发生故障，另一个区域可以处理业务连续性。然而，这显著增加了跨区域复制数据的延迟。通常，考虑到区域之间的长距离，数据复制以异步方式进行。在这种情况下，我们无法期望实时和强一致性。相反，我们需要满足最终一致性。如果副本失败，企业必须容忍潜在的数据丢失。'
- en: '**Degrade gracefully**: If a service fails and there is no failover path, the
    application may be able to degrade gracefully while still providing an acceptable
    user experience.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优雅降级**：如果一个服务失败且没有故障转移路径，应用程序可能能够在仍然提供可接受的用户体验的同时优雅降级。'
- en: '**Throttle high-volume users**: Sometimes, a small number of users create excessive
    load. This can have a bad impact on other users. The application might throttle
    the high-volume users for a certain period of time. Throttling does not imply
    the users are acting maliciously. The throttling starts if the number of requests
    exceeds the threshold.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**限制高流量用户**：有时，少数用户会创建过度的负载。这可能会对其他用户产生不良影响。应用程序可能会在一定时间内限制高流量用户。限制并不意味着用户正在恶意操作。如果请求数量超过阈值，则开始限制。'
- en: The testing approaches for resiliency
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性测试方法
- en: 'Testers have to test how the end-to-end workload performs under failure conditions
    that only occur intermittently, there are two types as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 测试人员必须测试在仅偶尔发生的故障条件下端到端工作负载的表现，有以下两种类型：
- en: '**Fault injection testing**: This is one way of testing the resiliency of the
    system during failures, either by triggering actual failures or by simulating
    them.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障注入测试**：这是在故障期间测试系统弹性的方法之一，可以通过触发实际故障或模拟它们来实现。'
- en: '**Load testing**: There are open source as well as commercial-grade load generation
    tools, and through those tools load testing of the application is being insisted.
    Load testing is crucial for identifying failures that only happen under loads
    such as the backend database being overwhelmed or service throttling. Test for
    peak load, using production data or synthetic data that is as close to production
    data as possible. The goal is to see how the application behaves under real-world
    conditions.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载测试**：有开源和商业级的负载生成工具，通过这些工具进行应用程序的负载测试。负载测试对于识别仅在负载下发生的故障至关重要，例如后端数据库过载或服务限制。使用生产数据或尽可能接近生产数据的合成数据进行峰值负载测试。目标是观察应用程序在真实世界条件下的行为。'
- en: The resilient deployment approaches
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性部署方法
- en: 'Software deployment is an important facet for establishing and sustaining resiliency.
    After applications are deployed in production-grade servers, the software updates
    also can be a source for errors. Any incomplete and bad update results in system
    breakdown. There are a few proven deployment and update methods in order to avoid
    any kind of downtime. The proper checks have to be in place before deployment
    and subsequent updates. Deployment typically includes provisioning of various
    server, network, and storage resources, deploying the curated and refined application
    code, and applying the required and right configuration settings. An update may
    involve all three or a subset of the three tasks. It is therefore recommended
    to have a tool-assisted, automated, and idempotent process in place. There are
    two major concepts related to resilient deployment:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 软件部署是建立和维持弹性重要的一环。在应用程序部署到生产级服务器后，软件更新也可能成为错误源。任何不完整或错误的更新都可能导致系统崩溃。有一些经过验证的部署和更新方法可以避免任何形式的停机。在部署和后续更新之前必须进行适当的检查。部署通常包括提供各种服务器、网络和存储资源，部署经过精选和优化的应用程序代码，以及应用所需的正确配置设置。更新可能涉及所有三个任务或这三个任务的一个子集。因此，建议实施一个辅助工具、自动化和幂等的过程。与弹性部署相关的有两个主要概念：
- en: '**Infrastructure as code** is the practice of using code to provision and configure
    infrastructure. Infrastructure as code may use a declarative approach or an imperative
    approach, or a combination of both.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施即代码**是使用代码来提供和配置基础设施的实践。基础设施即代码可能使用声明性方法或命令性方法，或者两者的结合。'
- en: '**Immutable infrastructure** complies with the principle that the infrastructure
    should not be disturbed or modified after it has gone to production.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可变基础设施**符合基础设施在生产后不应被干扰或修改的原则。'
- en: The deployment patterns
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署模式
- en: '**Blue-green deployment** is a technique where an update is deployed into a
    production environment separate from the live application. After the deployment
    gets validated, then switch the traffic routing to the updated version.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**蓝绿部署**是一种技术，其中更新被部署到一个与实时应用程序分开的生产环境中。在部署得到验证后，然后切换流量路由到更新版本。'
- en: In the case of **canary releases**, instead of switching all traffic to the
    updated version, we can roll out the update to a small percentage of users, by
    routing a portion of the traffic to the new deployment. If there is a problem,
    back off and revert to the old deployment. Otherwise, route more of the traffic
    to the new version, until it gets 100% of the traffic.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在**金丝雀发布**的情况下，我们不必将所有流量切换到更新版本，而是可以通过将部分流量路由到新部署来将更新推出给一小部分用户。如果有问题，就退回到旧部署。否则，将更多流量路由到新版本，直到它获得100%的流量。
- en: Whatever approach is preferred, it is mandatory to make sure that we can roll
    back to the last-known good deployment, in case the new version is not functioning
    as per the expectation. Also, if errors occur, the application logs must indicate
    which version caused the error.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 无论采用何种方法，都必须确保我们可以在新版本不符合预期的情况下回滚到最后已知的好部署。此外，如果发生错误，应用程序日志必须指出是哪个版本导致了错误。
- en: Monitoring and diagnostics
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控和诊断
- en: Continuous and tools-assisted monitoring of applications is crucial for achieving
    resiliency. If something drags, lags, or fails, the operational team has to be
    informed immediately along with all the right and relevant details to consider
    and proceed with a correct course of action. As we all agree, monitoring a large-scale
    distributed system poses a greater challenge. With the overwhelming acceptance
    of the *divide and conquer* technique, the number of moving parts of any enterprise-scale
    application has grown steadily and sharply. Today, as a part of the compartmentalization,
    we have virtualization and containerization concepts widely accepted and adopted.
    The number of VMs in any IT environment is growing. Furthermore, due to the lightweight
    nature, the number of containers being leveraged to run any mission-critical application
    has escalated rapidly and remarkably. In short, monitoring bare metal servers,
    VMs, and containers precisely is definitely a challenge for operational teams.
    Also, every kind of software and hardware generates a lot of log files resulting
    in massive operational data. It has become common to subject all sorts of operational
    data to extract actionable insights. Not only are the IT systems distributed,
    but they are also extremely dynamic. The monitoring, measuring, and management
    complexities of tomorrow's data centers and server farms are consistently on the
    climb.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对应用程序进行持续和工具辅助的监控对于实现弹性至关重要。如果出现拖沓、滞后或失败的情况，运营团队必须立即被告知，并附带所有正确和相关的细节，以便采取正确的行动。正如我们大家所同意的，监控大规模分布式系统是一个更大的挑战。随着“分而治之”技术的广泛接受，任何企业级应用程序的移动部件数量稳步且急剧增长。今天，作为模块化的一部分，我们已经广泛接受和采用虚拟化和容器化概念。任何IT环境中的虚拟机数量都在增长。此外，由于轻量级特性，用于运行任何关键任务应用程序的容器数量也在迅速且显著地增加。简而言之，精确监控裸金属服务器、虚拟机和容器对于运营团队来说确实是一个挑战。此外，各种软件和硬件都会生成大量的日志文件，导致大量运营数据。将各种运营数据用于提取可操作见解已成为一种常见做法。不仅IT系统是分布式的，而且它们也非常动态。明天数据中心和服务器农场监控、测量和管理复杂性持续上升。
- en: Monitoring is not the same as failure detection. For example, our application
    might detect a transient error and retry, resulting in no downtime. But it should
    also log the retry operation so that we can monitor the error rate, in order to
    get an overall picture of application health.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 监控并不等同于故障检测。例如，我们的应用程序可能检测到一个短暂错误并重试，从而避免了停机时间。但同时也应该记录重试操作，以便我们可以监控错误率，从而获得应用程序整体健康状况的全面了解。
- en: The resiliency strategy is essential to ensure the service resiliency of IT
    systems and business applications. As enterprises increasingly embrace the cloud
    model, the cloud service providers are focusing on enhancing the resiliency capability
    of their cloud servers, storage, and networks. Application developers are also
    learning the tricks and techniques fast in order to bring forth resilient applications.
    With the combination of resilient infrastructures, platforms, and applications,
    the days of the resilient IT, which is mandatory towards agile, dynamic, productive,
    and adaptive businesses, is not too far away.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性策略对于确保IT系统和业务应用程序的服务弹性至关重要。随着企业越来越多地采用云模式，云服务提供商正专注于增强其云服务器、存储和网络的可恢复能力。应用程序开发者也在快速学习技巧和技术，以便推出具有弹性的应用程序。结合弹性基础设施、平台和应用程序，实现弹性IT的日子，这对于敏捷、动态、高效和适应性强的企业来说是越来越近了。
- en: Resiliency realization patterns
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性实现模式
- en: Patterns are always a popular and peerless mechanism for unearthing and articulating
    competent solutions for a variety of recurring problems. We will look at a host
    of promising and proven design patterns for accomplishing the most important goal
    of resiliency.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 模式始终是挖掘和阐述针对各种重复性问题的高效解决方案的一种流行且无与伦比的机制。我们将探讨一系列有前景且经过验证的设计模式，以实现最重要的目标——弹性。
- en: Circuit breaker pattern
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 电路断路器模式
- en: The circuit breaker pattern can prevent an application from repeatedly trying
    an operation that is likely to fail. The circuit breaker wraps the calls to a
    service. It can handle faults that might take a variable amount of time to recover
    from when connecting to a remote service or resource. This can improve the stability
    and resiliency of an application.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 电路断路器模式可以防止应用程序反复尝试可能失败的操作。电路断路器封装了对服务的调用。它可以处理在连接到远程服务或资源时可能需要不同时间恢复的故障。这可以提高应用程序的稳定性和弹性。
- en: '**The problem description**—Remote connectivity is common in a distributed
    application. Due to a host of transient faults such as slow network speed, timeouts,
    the service unavailability, or the huge load on the service, calls to remote application
    services can fail. These faults, being transient, typically correct themselves
    after a short period of time. The retry pattern strategy suggests that a robust
    cloud application can handle these transient faults easily in order to meet up
    the service requests.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题描述**——远程连接在分布式应用程序中很常见。由于网络速度慢、超时、服务不可用或服务负载过重等大量短暂故障，对远程应用程序服务的调用可能会失败。这些故障通常是短暂的，通常在短时间内自行纠正。重试模式策略建议，一个健壮的云应用程序可以轻松处理这些短暂故障，以满足服务请求。'
- en: However, there can also be situations wherein the faults are due to bigger issues.
    The severity levels vary from temporary connectivity loss to the complete failure
    of the service due to various reasons and causes. Here, it is illogical to continuously
    retry to establish the broken connectivity. Instead, the application has to understand
    and accept the situation to handle the failure in a graceful manner.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也可能存在故障是由于更大问题的情况。严重程度从暂时性连接丢失到由于各种原因和原因导致的服务完全失败不等。在这里，不断重试建立已损坏的连接是不合逻辑的。相反，应用程序必须理解和接受这种情况，以优雅地处理失败。
- en: Suppose the requested service is very busy, then there is a possibility for
    the whole system to break down.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 假设请求的服务非常繁忙，那么整个系统崩溃的可能性就存在。
- en: Generally, an operation that invokes a service is configured to implement a
    timeout and to reply with a failure message if the service fails to respond within
    the indicated time period. However, this strategy could cause many concurrent
    requests to the same operation to be blocked until the timeout period expires.
    These blocked requests might hold critical system resources such as memory, threads,
    database connections, and so on. Finally, the resources could become exhausted,
    causing failure of other associated and even unrelated system components. The
    idea is to facilitate the operation to fail immediately and only to attempt to
    invoke the service again if it is likely to succeed. The point here is to set
    up a timeout intelligently because a shorter timeout might help to resolve this
    problem but the shorter timeout may cause the operation to fail most of the time.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，调用服务的操作被配置为实施超时，并在服务未能响应指定的时间段内回复失败消息。然而，这种策略可能导致许多并发请求同一操作的请求被阻塞，直到超时期间结束。这些阻塞的请求可能会占用关键系统资源，如内存、线程、数据库连接等。最终，资源可能会耗尽，导致其他相关甚至无关的系统组件失败。其理念是使操作立即失败，并且只有在可能成功的情况下才尝试再次调用服务。这里的要点是智能地设置超时，因为较短的超时可能有助于解决这个问题，但较短的超时可能会使操作大部分时间都失败。
- en: '**The solution** **approach**—The solution is the proven circuit breaker pattern,
    which can prevent an application from repeatedly trying to execute an operation
    that''s likely to fail. This allows it to continue without waiting for the fault
    to be fixed or wasting CPU cycles while it determines that the fault is long lasting.
    The circuit breaker pattern also enables an application to detect whether the
    fault has been resolved. If the problem appears to have been fixed, the application
    can try to invoke the operation.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案方法**—解决方案是经过验证的断路器模式，它可以防止应用程序反复尝试执行可能失败的操作。这允许它在等待故障修复或确定故障持续时间较长时，不浪费CPU周期继续运行。断路器模式还使应用程序能够检测故障是否已解决。如果问题似乎已经修复，应用程序可以尝试调用该操作。'
- en: 'The retry pattern enables an application to retry an operation in the expectation
    that it will succeed. On the other hand, the circuit breaker pattern prevents
    an application from performing an operation that is likely to fail. An application
    can combine these two patterns by using the retry pattern to invoke an operation
    through a circuit breaker. However, the retry logic should be highly sensitive
    to any exceptions returned by the circuit breaker and abandon retry attempts if
    the circuit breaker indicates that a fault is not transient. Also, a circuit breaker
    acts as a proxy for operations that might fail. The proxy should monitor the number
    of recent failures that have occurred, and use this information to decide whether
    to allow the operation to proceed, or simply return an exception immediately.
    The proxy can be implemented as a state machine with the following states:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 重试模式允许应用程序在预期操作将成功的情况下重试操作。另一方面，断路器模式防止应用程序执行可能失败的操作。应用程序可以通过使用断路器通过重试模式调用操作来结合这两种模式。然而，重试逻辑应该对断路器返回的任何异常高度敏感，如果断路器指示故障不是瞬时的，则应放弃重试尝试。此外，断路器充当可能失败的操作的代理。代理应监控最近发生的失败次数，并使用这些信息来决定是否允许操作继续，或者简单地立即返回异常。代理可以作为一个具有以下状态的状态机实现：
- en: '**Closed**: This is the original state of the circuit breaker. Therefore, the
    circuit breaker sends requests to the service and a counter continuously tracks
    the number of recent failures. If the failure count goes above the threshold level
    within a given time period, then the circuit breaker switches to the *open* state.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关闭状态**：这是断路器的原始状态。因此，断路器向服务发送请求，并有一个计数器持续跟踪最近失败的次数。如果在给定的时间段内，失败计数超过阈值水平，则断路器切换到*打开*状态。'
- en: '**Open**: In this state, the circuit breaker opens up and immediately fails
    all requests without calling the service. The application instead has to make
    use of a mitigation path such as reading data from a replica database or simply
    returning an error to the user. When the circuit breaker switches to the open
    state, it starts a timer. When the timer expires, the circuit breaker switches
    to the half-open state.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**打开状态**：在此状态下，断路器打开并立即失败所有请求，而不调用服务。应用程序必须使用缓解路径，例如从副本数据库读取数据或简单地向用户返回错误。当断路器切换到打开状态时，它启动一个计时器。当计时器到期时，断路器切换到半开状态。'
- en: '**Half-open**: In this state, the circuit breaker lets a limited number of
    requests go through to the service. If they succeed, the service is assumed to
    be recovered and the circuit breaker switches back to the original closed state.
    Otherwise, it reverts to the open state. The half-open state prevents a recovering
    service from suddenly being inundated with a series of service requests.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半开状态**：在此状态下，断路器允许有限数量的请求通过到服务。如果它们成功，则假定服务已恢复，断路器切换回原始关闭状态。否则，它将恢复到打开状态。半开状态防止恢复中的服务突然被一系列服务请求淹没。'
- en: The circuit breaker pattern ensures the system's stability while the system
    slowly yet steadily recovers from a failure and minimizes the impact on the system's
    performance. It can help to maintain the response time of the system by quickly
    rejecting a request for an operation that is likely to fail rather than waiting
    for the operation to time out. If the circuit breaker raises an event each time,
    it changes the state. This information can be used to monitor the health of the
    part of the system protected by the circuit breaker or to alert an administrator
    when a circuit breaker trips to the open state.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 断路器模式确保系统在从故障中缓慢而稳定地恢复的同时保持系统稳定性，并最小化对系统性能的影响。它可以通过快速拒绝可能失败的操作的请求，而不是等待操作超时，来帮助保持系统的响应时间。如果断路器每次改变状态时都引发一个事件，则可以使用这些信息来监控由断路器保护的系统部分的健康状况，或者在断路器跳转到开启状态时提醒管理员。
- en: The pattern is highly customizable and can be adapted according to the type
    of the possible failure. For example, it is possible to use an increasing timeout
    timer to a circuit breaker. We can place the circuit breaker in the open state
    for a few seconds initially and if the failure hasn't yet been resolved, then
    increase the timeout to a few minutes, and so on. In some cases, rather than the open state
    returning a failure and raising an exception, it could be useful to return a default
    value that is meaningful to the application.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 该模式高度可定制，可以根据可能出现的故障类型进行调整。例如，可以将一个增加的超时计时器用于断路器。我们最初可以将断路器置于开启状态几秒钟，如果故障尚未解决，则将超时时间增加到几分钟，依此类推。在某些情况下，与其让开启状态返回一个故障并引发异常，不如返回一个对应用程序有意义的默认值。
- en: 'In summary, this pattern is used to prevent an application from trying to invoke
    a remote service or access a shared resource if this operation is highly likely
    to fail. This pattern is not:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，此模式用于防止应用程序尝试调用远程服务或访问共享资源，如果这种操作高度可能失败。此模式不是：
- en: For handling access to local private resources in an application, such as an
    in-memory data structure
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于处理应用程序中对本地私有资源的访问，例如内存中的数据结构
- en: As a substitute for handling exceptions in the business logic of your applications
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为处理应用程序业务逻辑中异常的替代方案
- en: The circuit breaker pattern is becoming very common with microservices, emerging
    as the most optimized way of partitioning massive applications and presenting
    applications as an organized collection of microservices.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 断路器模式在微服务中变得越来越常见，成为分割大型应用程序和将应用程序呈现为有组织的微服务集合的最优化方式。
- en: Bulkhead pattern
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bulkhead pattern
- en: '**The problem description**—cloud applications typically comprise multiple
    and inter-linked services. A service can run on different and distributed services
    as service instances. There can be multiple requests from multiple consumers for
    each of those service instances. When the consumer sends a request to a service
    that is misconfigured or not responding, the resources used by the client''s request
    may not be freed in a timely manner.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题描述**——云应用通常由多个相互关联的服务组成。一个服务可以作为服务实例在不同的和分布式的服务上运行。对于每个服务实例，可能有多个消费者请求。当消费者向一个配置错误或未响应的服务发送请求时，客户端请求使用的资源可能无法及时释放。'
- en: As requests to the service continue incessantly, those resources may soon be
    exhausted. The resources occupied include the database connection. The ultimate
    result is that any request to other services of the cloud application gets impacted.
    Eventually, the cloud application may not be available to the consumer. This is
    the case with other consumers too. In short, a large number of requests originating
    from one client may exhaust available resources in the service. This is the cascading
    effect and this pattern comes in handy in surmounting this issue.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对服务的请求持续不断，这些资源可能很快就会耗尽。占用的资源包括数据库连接。最终结果是，对云应用中其他服务的任何请求都会受到影响。最终，云应用可能无法向消费者提供服务。其他消费者也是如此。简而言之，来自一个客户端的大量请求可能会耗尽服务中的可用资源。这是级联效应，而此模式在克服这一问题中非常有用。
- en: '**The solution approach**—the solution is to smartly partition service instances
    into different groups, based on consumer load and availability requirements. This
    design helps to isolate failures, and allows sustaining service functionality
    for some consumers, even during a failure. A consumer can also partition resources,
    to ensure that resources used to call one service don''t affect the resources
    used to call another service. For example, choosing different connection pools
    for different services is a workable option. Thus, the collapse of one connection
    pool does not stop other connections.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案方法**——解决方案是将服务实例智能地分区到不同的组，基于消费者负载和可用性要求。这种设计有助于隔离故障，并允许在故障期间为某些消费者维持服务功能。消费者还可以分区资源，以确保用于调用一个服务的资源不会影响用于调用另一个服务的资源。例如，为不同的服务选择不同的连接池是一个可行的选项。因此，一个连接池的崩溃不会停止其他连接。'
- en: 'The benefits of this pattern include the following:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式的优点包括以下内容：
- en: This isolates service consumers and services from cascading failures. This isolation
    firmly prevents an entire solution from going down.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这隔离了服务消费者和服务，防止级联故障。这种隔离坚决防止整个解决方案崩溃。
- en: The instance-level isolation helps to retain the other instances of the services.
    Thus, the service availability is guaranteed and similarly, other services of
    the application continue to deliver their assigned functionality.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例级别的隔离有助于保留服务的其他实例。因此，服务可用性得到保证，同样地，应用程序的其他服务继续执行其分配的功能。
- en: This helps to identify the demands of consuming applications and accordingly
    allows deploying services that offer a different **Quality of Service** (**QoS**).
    That is, a high-priority consumer pool can be configured to use high-priority
    services.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这有助于识别消费应用的需求，并相应地允许部署提供不同**服务质量**（**QoS**）的服务。也就是说，可以配置一个高优先级的消费者池来使用高优先级的服务。
- en: In summary, any sort of failures in one subsystem can sometimes cascade to other
    components resulting in the system breakdown. To avoid this, we need to partition
    a system into a few isolated groups, so that any failure in one partition does
    not percolate to others. Containerization in conjunction with polyglot microservices
    is an overwhelming option for having partitioned and problem-free systems.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，一个子系统的任何类型的故障有时会级联到其他组件，导致系统崩溃。为了避免这种情况，我们需要将系统分区成几个隔离的组，以便一个分区的故障不会渗透到其他分区。容器化与多语言微服务的结合是拥有分区和无忧系统的强大选择。
- en: Compensating transaction pattern
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 补偿事务模式
- en: This is a transaction that undoes the effects of another completed transaction.
    In a distributed system, it can be very difficult to achieve strong transactional
    consistency. Compensating transactions are a way to achieve consistency by using
    a series of smaller and individual transactions that can be undone at each step.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个撤销另一个已完成事务效果的交易。在分布式系统中，实现强事务一致性可能非常困难。补偿事务是通过使用一系列较小且独立的交易来实现一致性的方法，这些交易可以在每个步骤中撤销。
- en: '**The problem description**—a typical business operation consists of a series
    of separate steps. While these steps are being performed, the overall view of
    the system state might be inconsistent, but when the operation has completed and
    all of the steps have been executed, the system should become consistent again.
    A challenge in the eventual consistency model is how to handle a step that has
    failed. In this case, it might be necessary to undo all of the work completed
    by the previous steps in the operation. However, the data can''t simply be rolled
    back because other concurrent instances of the application might have changed
    it. Even in cases where the data hasn''t been changed by a concurrent instance,
    undoing a step might not simply be a matter of restoring the original state. This
    mandates the application of various business-specific rules. If an operation that
    implements eventual consistency spans several heterogeneous data stores, undoing
    the steps in the operation will require visiting each data store in turn. The
    work performed in every data store must be undone reliably to prevent the system
    from remaining inconsistent.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题描述**——典型的业务操作由一系列独立的步骤组成。当这些步骤正在执行时，系统的整体状态视图可能是不一致的，但是当操作完成并且所有步骤都已执行时，系统应该再次变得一致。最终一致性模型中的挑战是如何处理失败的步骤。在这种情况下，可能需要撤销操作中之前步骤完成的所有工作。然而，数据不能简单地回滚，因为其他并发应用程序实例可能已经更改了它。即使在数据没有被并发实例更改的情况下，撤销一个步骤可能不仅仅是恢复原始状态的问题。这要求应用各种特定于业务规则。如果一个实现最终一致性的操作跨越几个异构数据存储，撤销操作中的步骤将需要依次访问每个数据存储。必须在每个数据存储中可靠地撤销执行的工作，以防止系统保持不一致。'
- en: In a **service-oriented architecture** (**SOA**) environment, an operation could
    invoke an action in a service and cause a change in the state held by that service.
    To undo the operation, this state change must also be undone. This can involve
    invoking the service again and perform another action that reverses the effects
    of the first.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在**面向服务的架构**（**SOA**）环境中，一个操作可能会在服务中调用一个动作并导致该服务持有的状态发生变化。为了撤销操作，这种状态变化也必须被撤销。这可能涉及到再次调用服务并执行另一个动作，以逆转第一个动作的效果。
- en: '**The solution approach**—the solution is to implement a compensating transaction.
    The steps in a compensating transaction must undo the effects of the steps in
    the original operation. A compensating transaction might not be able to simply
    replace the current state with the state the system was in at the start of the
    operation because this approach could overwrite changes made by other concurrent
    instances of an application. Instead, it must be an intelligent process that takes
    into account any work done by concurrent instances. This process will usually
    be application specific, driven by the nature of the work performed by the original
    operation.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案方法**——解决方案是实施一个补偿事务。补偿事务中的步骤必须撤销原始操作中步骤的效果。补偿事务可能无法简单地用操作开始时系统的状态替换当前状态，因为这种方法可能会覆盖其他并发应用程序实例所做的更改。相反，它必须是一个智能过程，考虑到任何并发实例完成的工作。这个过程通常将是特定于应用程序的，由原始操作执行的工作的性质驱动。'
- en: A common approach is to use a workflow to implement an eventually consistent
    operation that requires compensation. As the original operation proceeds, the
    system records information about each step and how the work performed by that
    step can be undone. If the operation fails at any point, the workflow rewinds
    back through the steps it has completed and performs the work that reverses each
    step.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的方法是使用工作流来实现需要补偿的最终一致操作。随着原始操作的进行，系统记录有关每个步骤的信息以及该步骤执行的工作如何被撤销。如果在任何点上操作失败，工作流将回滚到已完成的步骤，并执行逆转每个步骤的工作。
- en: It is recommended to use this pattern only for operations that must be undone
    if they fail. If possible, design solutions to avoid the complexity of requiring
    compensating transactions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 建议仅在使用失败时必须撤销的操作中使用此模式。如果可能，设计解决方案以避免需要补偿事务的复杂性。
- en: Health endpoint monitoring pattern
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 健康端点监控模式
- en: '**The problem description**—applications and their services need to be continuously
    monitored to gain a firm grip on their availability and performance levels and
    patterns. Monitoring services running in off-premises, on-demand, and online environments
    are quite difficult compared to any on-premises services. There are many factors
    that affect cloud-hosted applications, such as network latency, the performance
    and availability of the underlying compute and storage systems, and the network
    bandwidth between them. The service can fail entirely or partially due to any
    of these factors. Therefore, we must verify at regular intervals that the service
    is performing correctly.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题描述**——应用程序及其服务需要持续监控，以获得对其可用性和性能水平和模式的牢固把握。与任何本地服务相比，在本地、按需和在线环境中运行的监控服务相当困难。有许多因素会影响云托管应用程序，例如网络延迟、底层计算和存储系统的性能和可用性，以及它们之间的网络带宽。服务可能会完全或部分失败，这可能是由于任何这些因素造成的。因此，我们必须定期验证服务是否正常运行。'
- en: '**The solution approach**—we need to do health monitoring by sending requests
    to an endpoint on the application. The application should perform the necessary
    checks and return an indication of its status. A health-monitoring check typically
    combines two factors:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案方法**——我们需要通过向应用程序的端点发送请求来进行健康监控。应用程序应执行必要的检查，并返回其状态的指示。健康监控检查通常结合两个因素：'
- en: The assigned checks performed by the application or service in response to the
    request to the health verification endpoint
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序或服务在响应健康验证端点请求时执行的分配检查。
- en: The analysis of the results by the health-monitoring tool that performs the
    health verification check.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行健康验证检查的健康监控工具对结果的分析。
- en: There are several parameters and conditions being checked by a health-monitoring
    tool in order to completely and concisely understand the state of the application.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 健康监控工具正在检查几个参数和条件，以便完全且简洁地了解应用程序的状态。
- en: It is also useful to run these checks from different on-premises or hosted locations
    to measure and compare response times. As customers are geographically distributed,
    the checks have to be initiated and implemented from those locations that are
    close to customers.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 从不同的本地或托管位置运行这些检查以测量和比较响应时间也是有用的。由于客户在地理上分布，检查必须从靠近客户的位置启动和实施。
- en: Another point is to expose at least one endpoint for the core services that
    the application uses and another for lower priority services. This allows different
    levels of importance to be assigned to each monitoring result. Also, it is good
    to consider exposing more endpoints such as one for each core service for additional
    monitoring granularity. Increasingly, health-verification checks are being done
    on the database, storage, and other critical services. The uptime and response
    time decide the quality of applications.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点是要公开至少一个应用使用的核心服务的端点，以及另一个用于低优先级服务的端点。这允许为每个监控结果分配不同的重要性级别。同时，考虑公开更多端点，例如为每个核心服务公开一个端点以实现额外的监控粒度也是好的。越来越频繁地，数据库、存储和其他关键服务都在进行健康验证检查。正常运行时间和响应时间决定了应用程序的质量。
- en: This pattern is extremely useful for checking the health condition of websites,
    web and mobile applications, and cloud-hosted applications.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式对于检查网站、Web和移动应用程序以及云托管应用程序的健康状况非常有用。
- en: Leader election pattern
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 领导选举模式
- en: '**The problem description**—a typical cloud application has many tasks working
    in a coordinated manner. These tasks could all be instances running the same code
    and requiring access to the same resources, or they might be working together
    in parallel to perform the individual parts of a complex calculation. The task
    instances might run separately for much of the time, but it might also be necessary
    to coordinate the actions of each instance to ensure that they don''t conflict,
    cause contention for shared resources, or accidentally interfere with the work
    that other task instances are performing.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题描述**——典型的云应用程序有许多任务以协调的方式工作。这些任务可能都是运行相同代码的实例，需要访问相同的资源，或者它们可能并行工作以执行复杂计算的各个部分。任务实例可能大部分时间都是独立运行的，但有时也可能需要协调每个实例的动作，以确保它们不会冲突，不会对共享资源产生竞争，或者意外地干扰其他任务实例正在进行的工作。'
- en: For example, cloud systems guarantee scalability through scale-up or scale-out.
    In the case of scale-out (horizontal scaling), there can be multiple instances
    of the same task/service. Each instance serves different users. If these instances
    write to a shared resource, then it is necessary to coordinate their actions to
    prevent each instance from overwriting the changes made by the others. Similarly,
    if the tasks are performing individual elements of a complex calculation in parallel,
    the results need to be duly aggregated to give the final answer. The task instances
    are all peers, so there isn't a natural leader that can act as the coordinator
    or aggregator.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，云系统通过扩展或扩展来保证可伸缩性。在扩展（水平扩展）的情况下，可能会有多个相同任务/服务的实例。每个实例服务于不同的用户。如果这些实例写入共享资源，那么协调它们的行动以防止每个实例覆盖其他实例所做的更改是必要的。同样，如果任务正在并行执行复杂计算的各个单独元素，那么结果需要适当汇总以给出最终答案。任务实例都是对等的，因此没有自然领导者可以充当协调器或聚合器。
- en: '**The solution approach**—a single task instance should be elected to act as
    the leader, and this instance should coordinate the actions of the other subordinate
    task instances. If all of the task instances are running the same code, they are
    each capable of acting as the leader. Therefore, the election process must be
    managed carefully to prevent two or more instances taking over the leader role
    at the same time. The system must provide a robust mechanism for selecting the
    leader. This method has to cope with events such as network outages or process
    failures. In many solutions, the subordinate task instances monitor the leader
    through some type of heartbeat method or by polling. If the designated leader
    terminates unexpectedly, or a network failure makes the leader unavailable to
    the subordinate task instances, it''s necessary for them to elect a new leader.
    This is like choosing a cluster head in a sensor mesh.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案方法**——应该选举一个单独的任务实例来充当领导者，并且这个实例应该协调其他下属任务实例的行动。如果所有任务实例都在运行相同的代码，它们各自都能充当领导者。因此，选举过程必须得到妥善管理，以防止两个或多个实例同时接管领导者角色。系统必须提供一种强大的机制来选择领导者。这种方法必须应对网络中断或进程故障等事件。在许多解决方案中，下属任务实例通过某种心跳方法或轮询来监控领导者。如果指定的领导者意外终止，或者网络故障使领导者对下属任务实例不可用，它们必须选举一个新的领导者。这就像在传感器网络中选择集群头节点一样。'
- en: This pattern performs best when the tasks in a distributed application, such
    as a cloud-hosted solution, need careful coordination and there is no natural
    leader. It is prudent to avoid making the leader a bottleneck in the system. The
    purpose of the leader is to coordinate the work of the subordinate tasks, and
    it doesn't necessarily have to participate in this work itself—although it should
    be able to do so if the task isn't elected as the leader.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式在分布式应用程序中的任务，例如云托管解决方案，需要仔细协调且没有自然领导者时表现最佳。避免将领导者作为系统瓶颈是谨慎的做法。领导者的目的是协调下属任务的工作，它本身并不一定必须参与这项工作——尽管如果任务没有被选为领导者，它应该能够这样做。
- en: Queue-based load leveling pattern
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于队列的负载均衡模式
- en: Applications may experience sudden spikes in traffic, which can bombard backend
    systems. If a backend service cannot respond to requests quickly enough, it may
    cause requests to queue (back up), or it can cause the service to throttle the
    application. To avoid this, we can use a queue as a buffer. When there is a new
    work item, instead of calling the backend service immediately, the application
    queues a work item to run asynchronously. The queue acts as a buffer that smooths
    out peaks in the load.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可能会经历突然的交通高峰，这可能会轰炸后端系统。如果后端服务无法快速响应请求，可能会导致请求排队（积压），或者可能导致服务限制应用程序。为了避免这种情况，我们可以使用队列作为缓冲。当有新的工作项时，不是立即调用后端服务，而是应用程序将工作项排队以异步运行。队列充当缓冲区，可以平滑负载的峰值。
- en: '**The problem description**—for arriving at competent and composite applications
    that are business-centric and process-aware, cloud applications ought to interact
    with one another. The services can be locally available or accessible remotely.
    Various enthusiastic software developers bring modern applications and provide
    them for worldwide subscribers for a small fee, or sometimes for free. Similarly,
    there are **independent software vendors** (**ISVs**) contracting with hosted
    service providers to run their software to be found and bound. That is, various
    cloud services have to connect and collaborate with many others in order to be
    right and relevant to their consumers. In this intertwined environment, if a service
    is subjected to intermittent heavy loads, it can potentially cause performance
    or reliability issues. The predictability of the number of service users at a
    particular time is also a tough affair. Thus, static capacity planning is out
    of the discussion. *Dynamism* is the new buzzword in the IT landscape.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: As indicated previously, an application can be segmented into multiple services.
    Each service can be run in different containers as separate instances. That is,
    multiple instances of a service can be run in an IT environment. In the service
    world, everything is API-enabled in order to be found and leveraged by other services.
    A service can be used by many tasks concurrently. A service could be part of the
    same application as the tasks that use it or it could be provided by a third-party
    service provider. For example, the service can be a resource service, such as
    a cache or a storage service.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: A service might experience peaks in demand that cause it to overload and be
    unable to respond to requests in a timely manner. Flooding a service with a large
    number of concurrent requests can also result in the service failing if it's unable
    to handle the contention these requests cause.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '**The solution approach**—it is suggested to refactor the solution and introduce
    a queue between the task and the service. The task and the service run asynchronously.
    The task posts a message containing the data required by the service to a queue.
    The queue acts as a buffer, storing the message until it is retrieved by the service.
    The service retrieves the messages from the queue and processes them. Requests
    from a number of tasks, which can be generated at a highly variable rate, can
    be passed to the service through the same message queue.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'This pattern provides the following benefits:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: It can help to maximize availability of applications because delays arising
    in services will not have an immediate and direct impact on the application, which
    can continue to post messages to the queue even when the service is not available
    or is not currently processing messages
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can help to maximize scalability because both the number of queues and the
    number of services can be varied to meet demand
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can help to control costs because the number of service instances deployed
    only has to be adequate to meet the average load rather than the peak load
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这有助于控制成本，因为部署的服务实例数量只需足够满足平均负载，而不是峰值负载。
- en: Retry pattern
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重试模式
- en: '**Problem description**—we have discussed a bit about this pattern previously.
    Applications are distributed in the sense that the application components are
    being expressed and exposed as a service and delivered from different IT environments
    (private, public, and edge clouds). Typically, the IT spans across embedded, enterprise,
    and cloud domains. With the fast-growing device ecosystem, the connectivity has
    grown to various devices at the ground level. That is the reason that we very
    often hear, read, and even experience **cyber-physical system** (**CPS**). Also,
    the enterprise-scale applications (both legacy and modern) are accordingly modernized
    and moved to cloud environments to reap the distinct benefits of the cloud idea.
    However, certain applications, due to some specific reasons, are being kept in
    enterprise servers/private clouds. With embedded and networked devices joining
    in the mainstream computing, edge/fog devices are being enabled to form kind of
    ad hoc clouds to facilitate real-time data capture, storage, processing, and decision-making.
    The point to be noted here is that application services ought to connect to other
    services in the vicinity and remotely hold services over different networks. Faults
    can occur, stampeding the application calls. As articulated previously, there
    are temporary faults impacting the service connectivity, interaction, and execution.
    However, these faults are typically self-correcting and if the action that triggered
    a fault is repeated after a suitable delay, the connectivity and accessibility
    may go through.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题描述**—我们之前已经讨论过这种模式的一些内容。应用是以服务的形式表达和暴露的，并且从不同的IT环境中（私有、公共和边缘云）进行分发。通常，IT跨越嵌入式、企业和云领域。随着设备生态系统的快速增长，连接性已经扩展到地面层级的各种设备。这就是我们经常听到、读到甚至体验到**网络物理系统**（**CPS**）的原因。此外，企业规模的应用（无论是传统还是现代）相应地进行了现代化改造，并迁移到云环境中以获得云理念的独特优势。然而，由于某些特定原因，某些应用仍然保留在企业服务器/私有云中。随着嵌入式和网络设备的加入主流计算，边缘/雾设备被启用以形成某种临时的云，以促进实时数据捕获、存储、处理和决策。需要注意的是，应用服务应该连接到附近的其他服务，并在远程网络上持有服务。可能会发生故障，导致应用调用激增。正如之前所述，存在暂时性的故障，影响服务的连接性、交互和执行。然而，这些故障通常是自我纠正的，如果在适当的延迟后重复触发故障的动作，连接性和可访问性可能会恢复。'
- en: '**The solution approach**—in cloud environments, transient faults are common
    and an application should be designed to handle them elegantly and transparently.
    This minimizes the effects faults can have on the business tasks the application
    is duly performing. If an application detects a failure when it tries to send
    a request to a remote service, it can handle the failure using the following strategies:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案方法**—在云环境中，暂时性故障很常见，应用应该设计得能够优雅且透明地处理这些故障。这最小化了故障对应用正当执行的业务任务可能产生的影响。如果应用在尝试向远程服务发送请求时检测到故障，它可以采用以下策略来处理故障：'
- en: '**Cancellation**: If the fault indicates that the failure is not temporary
    (that is, persists for more time), or is likely to be unsuccessful if repeated,
    the application should cancel the operation and report an exception.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**取消**：如果故障表明失败不是暂时的（即持续更长时间），或者如果重复尝试，则可能无法成功，应用应取消操作并报告异常。'
- en: '**Retry**: If the specific fault reported is unusual or rare, it might have
    been caused by some unusual circumstances such as a network packet getting corrupted
    while it was being transmitted. In this case, the application can try again as
    the subsequent request may attain the required success.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重试**：如果报告的具体故障不寻常或罕见，可能是由于一些不寻常的情况，例如在网络传输过程中数据包被损坏。在这种情况下，应用可以再次尝试，因为后续的请求可能会获得所需的成功。'
- en: '**Retry after delay**: If the fault is caused by one of the more commonplace
    connectivity or busy failures, then the application has to wait for some time
    and try again.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟后重试**：如果故障是由更常见的连接或繁忙失败引起的，那么应用必须等待一段时间后再尝试。'
- en: The application should wrap all attempts to access a remote service in code
    that implements a retry policy matching one of the strategies listed previously.
    Requests sent to different services can be subjected to different policies. Some
    vendors provide libraries that implement retry policies, where the application
    can specify the maximum number of retries, the time between retry attempts, and
    other parameters. An application should log the details of faults and failing
    operations. This information is useful to operators. If a service is frequently
    unavailable or busy, it's often because the service has exhausted its resources.
    We can reduce the frequency of these faults by scaling out the service. For example,
    if a database service is continually overloaded, it might be beneficial to partition
    the database and spread the load across multiple servers.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序应将所有尝试访问远程服务的尝试封装在实现重试策略的代码中，该策略与之前列出的策略之一相匹配。发送到不同服务的请求可以受到不同策略的影响。一些供应商提供了实现重试策略的库，其中应用程序可以指定最大重试次数、重试尝试之间的时间以及其他参数。应用程序应记录故障和失败操作的详细信息。这些信息对操作员很有用。如果一个服务频繁不可用或繁忙，通常是因为该服务已经耗尽了其资源。我们可以通过扩展服务来减少这些故障的频率。例如，如果一个数据库服务持续过载，可能有益于对数据库进行分区并将负载分散到多台服务器上。
- en: In conclusion, having understood the strategic significance that the resiliency,
    robustness, and reliability of next-generation IT systems are to fulfil the various
    business and people needs with all the QoS and **Quality of Experience** (**QoE**)
    traits and tenets enshrined and etched, IT industry professionals, academic professors,
    and researchers are investing their talents, treasures, and time to unearth scores
    of easy-to-understand and useful techniques, tips, and tricks to simplify and
    streamline software and infrastructure engineering tasks. I ask the readers to
    visit [https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency) for
    further reading.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在理解了下一代IT系统的弹性、鲁棒性和可靠性对于满足所有QoS（服务质量）和**服务质量体验**（**QoE**）特性和原则的重要性之后，IT行业专业人士、学术教授和研究人员正在投入他们的才智、财富和时间，挖掘大量易于理解且实用的技术、技巧和窍门，以简化并优化软件和基础设施工程任务。我建议读者访问[https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency)以获取更多阅读材料。
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Both legacy and modern applications are remedied to be a collection of interactive
    microservices. Microservices can be hosted and run inside containers. There can
    be multiple instances for each microservice. Each container can run a microservice
    instance. Thus, in a typical IT environment, there can be hundreds of physical
    machines (also called **bare metal servers**). Each physical machine, in turn,
    is capable of running hundreds of containers. Thus, there will be tens of thousands
    of containers. The management and operational complexities are therefore bound
    to escalate. This pattern comes handy in successfully running microservice-hosted
    containers. There are technologies, such as Istio and Linkerd, for ensuring the
    resiliency of microservices. This resiliency ultimately ensures the application's
    reliability. Together with software-defined cloud infrastructures, reliable applications
    ensure the reliability of cloud environments for hosting and delivering next-generation
    business workloads.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是传统应用还是现代应用，都应被修复为微服务的集合。微服务可以在容器内托管和运行。每个微服务可以有多个实例。每个容器可以运行一个微服务实例。因此，在典型的IT环境中，可能会有数百台物理机器（也称为**裸机服务器**）。每台物理机器反过来又能够运行数百个容器。因此，将会有数万个容器。管理和运营的复杂性因此必然会增加。这种模式在成功运行托管微服务的容器方面非常有用。有一些技术，如Istio和Linkerd，可以确保微服务的弹性。这种弹性最终确保了应用程序的可靠性。与软件定义的云基础设施一起，可靠的应用程序确保了云环境在托管和交付下一代业务负载时的可靠性。
- en: The forthcoming chapters will discuss the various software-defined cloud application
    design and deployment patterns.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将讨论各种软件定义的云应用程序设计和部署模式。
