<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Rendering Stereoscopic 3D Models using OpenGL</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Installing the Open Asset Import Library (Assimp)</li><li class="listitem" style="list-style-type: disc">Loading the first 3D model in the Wavefront Object (.obj) format</li><li class="listitem" style="list-style-type: disc">Rendering 3D models with points, lines, and triangles</li><li class="listitem" style="list-style-type: disc">Stereoscopic 3D rendering</li></ul></div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec39"/>Introduction</h1></div></div></div><p>In this chapter, we will demonstrate how to visualize data with stunning stereoscopic 3D technology using OpenGL. Stereoscopic 3D devices are becoming increasingly popular, and the latest generation's wearable computing devices (such as the 3D vision glasses from NVIDIA, Epson, and more recently, the augmented reality 3D glasses from Meta) can now support this feature natively.</p><p>The ability to visualize data in a stereoscopic 3D environment provides a powerful and highly intuitive platform for the interactive display of data in many applications. For example, we may acquire data from the 3D scan of a model (such as in architecture, engineering, and dentistry or medicine) and would like to visualize or manipulate 3D objects in real time.</p><p>Unfortunately, OpenGL does not provide any mechanism to load, save, or manipulate 3D models. Thus, to support this, we will integrate a new library named <a id="id221" class="indexterm"/>
<strong>Open Asset Import Library</strong> (<strong>Assimp</strong>) into our code. The source code in this chapter is built on top of the <em>OpenGL point cloud rendering with texture mapping and overlays</em> recipe in <a class="link" href="ch05.html" title="Chapter 5. Rendering of Point Cloud Data for 3D Range-sensing Cameras">Chapter 5</a>, <em>Rendering of Point Cloud Data for 3D Range-sensing Cameras</em>. The main dependencies include the GLFW library that requires OpenGL version 3.2 and higher. We will assume that you have all the prerequisite packages installed from earlier chapters.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec40"/>Installing the Open Asset Import Library (Assimp)</h1></div></div></div><p>Assimp is an <a id="id222" class="indexterm"/>open source library that loads and processes 3D geometric scenes from various 3D model data formats. The library provides a unified interface to load many different data formats, such as <a id="id223" class="indexterm"/>
<strong>Wavefront Object</strong> (<strong>.obj</strong>), <strong>3ds Max 3DS</strong> (<strong>.3ds</strong>), and <strong>Stereolithography</strong> (<strong>.stl</strong>). Moreover, this<a id="id224" class="indexterm"/> library is written<a id="id225" class="indexterm"/> in portable, ISO-compliant C++, and thus, it allows further customization and long-term support. Since the library is cross-platform, we can easily install it in Mac OS X, Linux, as well as Windows with the instructions given in the next section.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec108"/>How to do it...</h2></div></div></div><p>To obtain the <a id="id226" class="indexterm"/>library source files or binary library for <a id="id227" class="indexterm"/>Assimp 3.0, download them directly from Assimp's official website at <a class="ulink" href="http://sourceforge.net/projects/assimp/files/assimp-3.0/">http://sourceforge.net/projects/assimp/files/assimp-3.0/</a>. Alternatively, for Linux and Mac OS X users, use the command-line interface to simplify the installation steps described next.</p><p>In Mac OS X, install Assimp using the MacPort's command-line interface. It automatically resolves all dependencies, so this is recommended:</p><div><pre class="programlisting">
<strong>sudo port install assimp</strong>
</pre></div><p>In Linux, install Assimp using the <code class="literal">apt-get</code> command interface:</p><div><pre class="programlisting">
<strong>sudo apt-get install install libassimp-dev</strong>
</pre></div><p>After the installation, modify the Makefile to ensure the libraries are linked to the source files by appending the following to the <code class="literal">LIBS</code> variable:</p><div><pre class="programlisting">
<strong>`pkg-config --static --libs assimp`</strong>
</pre></div><p>and the <code class="literal">INCLUDES</code> path variable, respectively:</p><div><pre class="programlisting">
<strong>`pkg-config --cflags assimp`</strong>
</pre></div><p>The final Makefile is shown here for your reference:</p><div><pre class="programlisting">
<strong>PKG_CONFIG_PATH=/usr/local/lib/pkgconfig/</strong>
<strong>CFILES = ../common/shader.cpp ../common/controls.cpp ../common/ObjLoader.cpp main.cpp </strong>
<strong>CFLAGS = -c</strong>
<strong>OPT = -O3</strong>
<strong>INCLUDES = -I../common -I/usr/include -I/usr/include/SOIL -I.  `pkg-config --cflags glfw3` `pkg-config --cflags assimp`</strong>
<strong>LIBS = -lm -L/usr/local/lib -lGLEW `pkg-config --static --libs glfw3` `pkg-config --static --libs assimp`</strong>
<strong>CC = g++</strong>
<strong>OBJECTS=$(CFILES:.cpp=.o)</strong>
<strong>EXECUTABLE=main</strong>
<strong>all: $(CFILES) $(EXECUTABLE)</strong>
<strong>$(EXECUTABLE): $(OBJECTS)</strong>
<strong>$(CC) $(OPT) $(INCLUDES) $(OBJECTS) -o $@ $(LIBS)</strong>
<strong>.cpp.o:</strong>
<strong>$(CC) $(OPT) $(CFLAGS) $(INCLUDES) $&lt; -o $@</strong>
<strong>clean:</strong>
<strong>rm -v -f *~ ../common/*.o *.o $(EXECUTABLE)</strong>
</pre></div><p>To install <a id="id228" class="indexterm"/>Assimp in Windows, first, download the <a id="id229" class="indexterm"/>binary library from this link: <a class="ulink" href="http://sourceforge.net/projects/assimp/files/assimp-3.0/assimp--3.0.1270-full.zip/download">http://sourceforge.net/projects/assimp/files/assimp-3.0/assimp--3.0.1270-full.zip/download</a>.</p><p>Then, we configure the environment with the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Unpack <code class="literal">assimp--3.0.1270-full.zip</code> and save it in <code class="literal">C:/Program Files (x86)/</code>.</li><li class="listitem">Add the DLL path, <code class="literal">C:/Program Files (x86)/assimp--3.0.1270-sdk/bin/assimp_release-dll_win32</code>, to the PATH environment variable.</li><li class="listitem">Include the <code class="literal">CMakeLists.txt</code> file to the project:<div><pre class="programlisting">cmake_minimum_required (VERSION 2.8)
set(CMAKE_CONFIGURATION_TYPES Debug Release)
set(PROGRAM_PATH "C:/Program Files \(x86\)")
set(OpenCV_DIR ${PROGRAM_PATH}/opencv/build)
project (code)
#modify these path based on your configuration
#OpenCV
find_package(OpenCV REQUIRED )
INCLUDE_DIRECTORIES(${OpenCV_INCLUDE_DIRS})
INCLUDE_DIRECTORIES(${PROGRAM_PATH}/glm)
INCLUDE_DIRECTORIES(${PROGRAM_PATH}/glew-1.10.0/include)
LINK_DIRECTORIES(${PROGRAM_PATH}/glew-1.10.0/lib/Release/Win32)
INCLUDE_DIRECTORIES(${PROGRAM_PATH}/glfw-3.0.4/include)
LINK_DIRECTORIES(${PROGRAM_PATH}/glfw-3.0.4/lib)
INCLUDE_DIRECTORIES(${PROGRAM_PATH}/Simple\ OpenGL\ Image\ Library/src)
INCLUDE_DIRECTORIES(${PROGRAM_PATH}/assimp--3.0.1270-sdk/include/assimp)
LINK_DIRECTORIES(${PROGRAM_PATH}/assimp--3.0.1270-sdk/lib/assimp_release-dll_win32)
add_subdirectory (../common common)
add_executable (main main.cpp)
target_link_libraries (main LINK_PUBLIC shader controls texture glew32s glfw3 opengl32 assimp ObjLoader)</pre></div></li></ol></div><p>Finally, generate <a id="id230" class="indexterm"/>the build files with the same steps as described in <a class="link" href="ch04.html" title="Chapter 4. Rendering 2D Images and Videos with Texture Mapping">Chapter 4</a>, <em>Rendering 2D Images and Videos with Texture Mapping</em> and <a class="link" href="ch05.html" title="Chapter 5. Rendering of Point Cloud Data for 3D Range-sensing Cameras">Chapter 5</a>, <em>Rendering of Point Cloud Data for 3D Range-sensing Cameras</em>.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec109"/>See also</h2></div></div></div><p>In addition to importing 3D model objects, Assimp also supports the exporting of 3D models in <code class="literal">.obj</code>, <code class="literal">.stl</code>, and <code class="literal">.ply</code> formats. By combining this library with the OpenGL graphics rendering engine, we have created a simple yet powerful mechanism to visualize and exchange 3D models collaboratively or remotely. The Assimp library can also handle some postprocessing tasks of 3D scenes after importing the model (for example, splitting large meshes to overcome certain GPU limitations on vertex count). These additional features are documented on the official<a id="id231" class="indexterm"/> website and may be of interest to advanced users (<a class="ulink" href="http://assimp.sourceforge.net/lib_html/index.html">http://assimp.sourceforge.net/lib_html/index.html</a>).</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec41"/>Loading the first 3D model in the Wavefront Object (.obj) format</h1></div></div></div><p>Now, we<a id="id232" class="indexterm"/> are ready to integrate a 3D object loader into our code. The first step is to create an empty class called <code class="literal">ObjLoader</code> along with<a id="id233" class="indexterm"/> the source (<code class="literal">.cpp</code>) and header (<code class="literal">.h</code>) files. This class handles all the functions related to 3D object loading, parsing, and drawing using the OpenGL and Assimp libraries. The headers of the class will include the Assimp core functions for the handling of the data structures and all I/O mechanisms of the 3D data format:</p><div><pre class="programlisting">#include &lt;cimport.h&gt;
#include &lt;scene.h&gt;
#include &lt;postprocess.h&gt;</pre></div><p>In the <code class="literal">ObjLoader.h</code> file, we provide interfaces for the main program to create, destroy, load, and display the 3D data. In the <code class="literal">ObjLoader.cpp</code> file, we implement a set of functions to parse the scene (a hierarchical representation of the 3D objects in terms of meshes and faces) using the built-in functions from Assimp.</p><p>The Assimp library can support various 3D model data formats; however, in our example, we will focus on the Wavefront Object (<code class="literal">.obj</code>) format due to its simplicity. The <code class="literal">.obj</code> file is a simple geometric <a id="id234" class="indexterm"/>definition file that was first developed by Wavefront Technologies. The file contains the core elements of graphics, such as vertex, vertex position, normal face and so <a id="id235" class="indexterm"/>on, and is stored in a simple text format. Since the files are stored in ASCII text, we can easily open and examine the files without any parsers. For example, the following is the <code class="literal">.obj</code> file of a front-facing square:</p><div><pre class="programlisting"># This is a comment.
# Front facing square.
# vertices [x, y, z]
v 0 0 0   # Bottom left.
v 1 0 0   # Bottom right.
v 1 1 0   # Top    right.
v 0 1 0   # Top    left.
# List of faces: 
f 1 2 3 4       # Square.</pre></div><p>As we can see from the preceding example, the representation is quite simple and intuitive for beginners. The vertices can be read and extracted one line at a time, and then they can be modified.</p><p>In the next section, we will show the full implementation, which allows users to load the <code class="literal">.obj</code> file, store the scene in a vertex buffer object, and display the scene.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec110"/>How to do it...</h2></div></div></div><p>First, we create the <code class="literal">ObjLoader.h</code> file in the common folder and append the class function definitions and variables that will be used in our implementation:</p><div><pre class="programlisting">#ifndef OBJLOADER_H_
#define OBJLOADER_H_
/* Assimp include files. These three are usually needed. */
#include &lt;cimport.h&gt;
#include &lt;scene.h&gt;
#include &lt;postprocess.h&gt;
#include &lt;common.h&gt;
#define aisgl_min(x,y) (x&lt;y?x:y)
#define aisgl_max(x,y) (y&gt;x?y:x)
class ObjLoader {
  public:
  ObjLoader();
  virtual ~ObjLoader();
  int loadAsset(const char* path);
  void setScale(float scale);
  unsigned int getNumVertices();
  void draw(const GLenum draw_mode);
  void loadVertices(GLfloat *g_vertex_buffer_data);
private:
  //helper functions and variables
  const struct aiScene* scene;
  GLuint scene_list;
  aiVector3D scene_min, scene_max, scene_center;
  float g_scale;
  unsigned int num_vertices;
  unsigned int recursiveDrawing(const struct aiNode* nd, unsigned int v_count, const GLenum);
  unsigned int recursiveVertexLoading(const struct aiNode *nd, GLfloat *g_vertex_buffer_data, unsigned int v_counter);
  unsigned int recursiveGetNumVertices(const struct aiNode *nd);
  void get_bounding_box (aiVector3D* min, aiVector3D* max);
  void get_bounding_box_for_node (const struct aiNode* nd, aiVector3D* min, aiVector3D* max, aiMatrix4x4* trafo);
};
#endif</pre></div><p>The names <a id="id236" class="indexterm"/>of classes from the Assimp library <a id="id237" class="indexterm"/>are preceded by the prefix <code class="literal">ai-</code> (for example, <code class="literal">aiScene</code> and <code class="literal">aiVector3D</code>). The <code class="literal">ObjLoader</code> file provides ways to dynamically load and draw the object loaded into the memory. It also handles simple dynamic scaling so that the object will fit on the screen.</p><p>In the source file, <code class="literal">ObjLoader.cpp</code>, we start by adding the constructor for the class:</p><div><pre class="programlisting">#include &lt;ObjLoader.h&gt;
ObjLoader::ObjLoader() {
  g_scale=1.0f;
  scene = NULL; //empty scene
  scene_list = 0;
  num_vertices = 0;
}</pre></div><p>Then, we implement the file-loading mechanism with the <code class="literal">aiImportFile</code> function. The scene is processed to extract the bounding box size for proper scaling to fit the screen. The number of vertices of the scene is then used to allow dynamic vertex buffer creation in later steps:</p><div><pre class="programlisting">int ObjLoader::loadAsset(const char *path){
  scene = aiImportFile(path, aiProcessPreset_TargetRealtime_MaxQuality);
  if (scene) {
    get_bounding_box(&amp;scene_min,&amp;scene_max);
    scene_center.x = (scene_min.x + scene_max.x) / 2.0f;
    scene_center.y = (scene_min.y + scene_max.y) / 2.0f;
    scene_center.z = (scene_min.z + scene_max.z) / 2.0f;
    printf("Loaded file %s\n", path);
    g_scale =4.0/(scene_max.x-scene_min.x);

    printf("Scaling: %lf", g_scale);
    num_vertices = recursiveGetNumVertices(scene-&gt;mRootNode);
    printf("This Scene has %d vertices.\n", num_vertices);
    return 0;
  }
  return 1;
}</pre></div><p>To extract <a id="id238" class="indexterm"/>the total number of vertices required to <a id="id239" class="indexterm"/>draw the scene, we recursively walk through every node in the tree hierarchy. The implementation requires a simple recursive function that returns the number of vertices in each node, and then the total is calculated based on the summation of all nodes upon the return of the function:</p><div><pre class="programlisting">unsigned int ObjLoader::recursiveGetNumVertices(const struct aiNode *nd){
  unsigned int counter=0;
  unsigned int i;
  unsigned int n = 0, t;
  // draw all meshes assigned to this node
  for (; n &lt; nd-&gt;mNumMeshes; ++n) {
    const struct aiMesh* mesh = scene-&gt; mMeshes[nd-&gt;mMeshes[n]];
    for (t = 0; t &lt; mesh-&gt;mNumFaces; ++t) {
      const struct aiFace* face = &amp;mesh-&gt; mFaces[t];
      counter+=3*face-&gt;mNumIndices;
    }
    printf("recursiveGetNumVertices: mNumFaces 	%d\n", mesh-&gt;mNumFaces);
  }
  //traverse all children nodes
  for (n = 0; n &lt; nd-&gt;mNumChildren; ++n) {
    counter+=recursiveGetNumVertices(nd-&gt; mChildren[n]);
  }
  printf("recursiveGetNumVertices: counter %d\n", counter);
  return counter;
}</pre></div><p>Similarly, to calculate the size of the bounding box (that is, the minimum volume that is required to contain the scene), we recursively examine each node and extract the points that are farthest away from the center of the object:</p><div><pre class="programlisting">void ObjLoader::get_bounding_box (aiVector3D* min, aiVector3D* max)
{
  aiMatrix4x4 trafo;
  aiIdentityMatrix4(&amp;trafo);
  min-&gt;x = min-&gt;y = min-&gt;z =  1e10f;
  max-&gt;x = max-&gt;y = max-&gt;z = -1e10f;
  get_bounding_box_for_node(scene-&gt; mRootNode,min,max,&amp;trafo);
}
void ObjLoader::get_bounding_box_for_node (const struct aiNode* nd, aiVector3D* min, aiVector3D* max, aiMatrix4x4* trafo) 
{
  aiMatrix4x4 prev;
  unsigned int n = 0, t;
  prev = *trafo;
  aiMultiplyMatrix4(trafo,&amp;nd-&gt;mTransformation);
  for (; n &lt; nd-&gt;mNumMeshes; ++n) {
    const struct aiMesh* mesh = scene-&gt; mMeshes[nd-&gt;mMeshes[n]];
    for (t = 0; t &lt; mesh-&gt;mNumVertices; ++t) {
      aiVector3D tmp = mesh-&gt;mVertices[t];
      aiTransformVecByMatrix4(&amp;tmp,trafo);
      min-&gt;x = aisgl_min(min-&gt;x,tmp.x);
      min-&gt;y = aisgl_min(min-&gt;y,tmp.y);
      min-&gt;z = aisgl_min(min-&gt;z,tmp.z);
      max-&gt;x = aisgl_max(max-&gt;x,tmp.x);
      max-&gt;y = aisgl_max(max-&gt;y,tmp.y);
      max-&gt;z = aisgl_max(max-&gt;z,tmp.z);
    }
  }
  for (n = 0; n &lt; nd-&gt;mNumChildren; ++n) {
    get_bounding_box_for_node(nd-&gt; mChildren[n],min,max,trafo);
  }
  *trafo = prev;
}</pre></div><p>The <a id="id240" class="indexterm"/>resulting <a id="id241" class="indexterm"/>bounding box allows us to calculate the scaling factor and recenter the object coordinate to fit within the viewable screen.</p><p>In the <code class="literal">main.cpp</code> file, we integrate the code by first inserting the header file:</p><div><pre class="programlisting">#include &lt;ObjLoader.h&gt;</pre></div><p>Then, we create the <code class="literal">ObjLoader</code> object and load the model with the given filename in the main function:</p><div><pre class="programlisting">ObjLoader *obj_loader = new ObjLoader();
int result = 0;
if(argc &gt; 1){
  result = obj_loader-&gt;loadAsset(argv[1]);
}
else{
  result = obj_loader-&gt; loadAsset("dragon.obj");
}
if(result){
  fprintf(stderr, "Final to Load the 3D file\n");
  glfwTerminate();
  exit(EXIT_FAILURE);
}</pre></div><p>The <code class="literal">ObjLoader</code> contains <a id="id242" class="indexterm"/>an algorithm that recursively examines each mesh and computes the bounding box and the number of vertices in the scene. Then, we dynamically allocate the vertex buffer based on the number of vertices and load the vertices into the buffer:</p><div><pre class="programlisting">GLfloat *g_vertex_buffer_data = (GLfloat*) 
malloc (obj_loader-&gt;getNumVertices()*sizeof(GLfloat));
//load the scene data to the vertex buffer
obj_loader-&gt;loadVertices(g_vertex_buffer_data);</pre></div><p>Now, we have<a id="id243" class="indexterm"/> all the necessary vertex information for display with our custom shader program written in OpenGL.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec111"/>How it works...</h2></div></div></div><p>Assimp provides the mechanism to load and parse the 3D data format efficiently. The key feature we utilized is the hierarchical way to import 3D objects, which allows us to unify our rendering pipeline regardless of the 3D format. The <code class="literal">aiImportFile</code> function reads the given file and returns its content in the <code class="literal">aiScene</code> structure. The second parameter of this function specifies the optional postprocessing steps to be executed after a successful import. The <code class="literal">aiProcessPreset_TargetRealtime_MaxQuality</code> flag is a predefined variable, which combines the following set of parameters:</p><div><pre class="programlisting">( \
  aiProcessPreset_TargetRealtime_Quality | \
  aiProcess_FindInstances | \
  aiProcess_ValidateDataStructure | \
  aiProcess_OptimizeMeshes | \
  aiProcess_Debone | \
0 )</pre></div><p>These <a id="id244" class="indexterm"/>postprocessing options are described in further detail at <a class="ulink" href="http://assimp.sourceforge.net/lib_html/postprocess_8h.html#a64795260b95f5a4b3f3dc1be4f52e410">http://assimp.sourceforge.net/lib_html/postprocess_8h.html#a64795260b95f5a4b3f3dc1be4f52e410</a>. Advanced users can look into each option and understand whether these functions need to be enabled or disabled based on the content.</p><p>At this point, we have a simple mechanism to load graphics into the Assimp <code class="literal">aiScene</code> object, present the bounding box size, as well as extract the number of vertices required to render the scene. Next, we will create a simple shader program as well as various drawing functions to visualize the content with different styles. In short, by integrating this with the OpenGL graphics rendering engine, we now have a flexible way to visualize 3D models using the <a id="id245" class="indexterm"/>various tools we developed in the previous chapters.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec42"/>Rendering 3D models with points, lines, and triangles</h1></div></div></div><p>The<a id="id246" class="indexterm"/> next step after importing the 3D model is to display the content on the screen using an intuitive and aesthetically pleasing way. Many complex scenes consist of multiple surfaces (meshes) and many vertices. In the previous chapter, we implemented a<a id="id247" class="indexterm"/> simple shader program to visualize the point cloud at various depth values based on a heat map. In this section, we will <a id="id248" class="indexterm"/>utilize very simple primitives (points, lines, and triangles) with transparency to create skeleton-like rendering effects.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec112"/>How to do it...</h2></div></div></div><p>We will continue the implementation of the <code class="literal">ObjLoader</code> class to support loading vertices and draw the graphics for each mesh in the scene.</p><p>In the source file of <code class="literal">ObjLoader.cpp</code>, we add a recursive function to extract all vertices from the scene and store them in a single vertex buffer array. This allows us to reduce the number of vertex buffers to be managed, thus reducing the complexity of the code:</p><div><pre class="programlisting">void ObjLoader::loadVertices(GLfloat *g_vertex_buffer_data)
{
  recursiveVertexLoading(scene-&gt;mRootNode, g_vertex_buffer_data, 0);
}
unsigned int ObjLoader::recursiveVertexLoading (const struct aiNode *nd, GLfloat *g_vertex_buffer_data, unsigned int v_counter)
{
  unsigned int i;
  unsigned int n = 0, t;
  /* save all data to the vertex array, perform offset and scaling to reduce the computation */
  for (; n &lt; nd-&gt;mNumMeshes; ++n) {
    const struct aiMesh* mesh = scene-&gt; mMeshes[nd-&gt;mMeshes[n]];
    for (t = 0; t &lt; mesh-&gt;mNumFaces; ++t) {
      const struct aiFace* face = &amp;mesh-&gt;mFaces[t];
      for(i = 0; i &lt; face-&gt;mNumIndices; i++) {
        int index = face-&gt;mIndices[i];
        g_vertex_buffer_data[v_counter]=
          (mesh-&gt;mVertices[index].x-scene_center.x)*g_scale;
        g_vertex_buffer_data[v_counter+1]=
          (mesh-&gt;mVertices[index].y-scene_center.y)*g_scale;
        g_vertex_buffer_data[v_counter+2]=
          (mesh-&gt;mVertices[index].z-scene_center.z)*g_scale;
        v_counter+=3;
      }
    }
  }
  //traverse all children nodes
  for (n = 0; n &lt; nd-&gt;mNumChildren; ++n) {
    v_counter = recursiveVertexLoading(nd-&gt; mChildren[n], g_vertex_buffer_data, v_counter);
  }
  return v_counter;
}</pre></div><p>To <a id="id249" class="indexterm"/>draw the<a id="id250" class="indexterm"/> graphics, we traverse the <code class="literal">aiScene</code> object from<a id="id251" class="indexterm"/> the root node and draw the meshes one piece at a time:</p><div><pre class="programlisting">void ObjLoader::draw(const GLenum draw_mode){
  recursiveDrawing(scene-&gt;mRootNode, 0, draw_mode);
}
unsigned int ObjLoader::recursiveDrawing(const struct aiNode* nd, unsigned int v_counter, const GLenum draw_mode){
  /* break up the drawing, and shift the pointer to draw different parts of the scene */
  unsigned int i;
  unsigned int n = 0, t;
  unsigned int total_count = v_counter;
  // draw all meshes assigned to this node
  for (; n &lt; nd-&gt;mNumMeshes; ++n) {
    unsigned int count=0;
    const struct aiMesh* mesh = scene-&gt; mMeshes[nd-&gt;mMeshes[n]];
    for (t = 0; t &lt; mesh-&gt;mNumFaces; ++t) {
      const struct aiFace* face = &amp;mesh-&gt; mFaces[t];
      count+=3*face-&gt;mNumIndices;
    }
    glDrawArrays(draw_mode, total_count, count);
      total_count+=count;
  }
  v_counter = total_count;
  // draw all children nodes recursively
  for (n = 0; n &lt; nd-&gt;mNumChildren; ++n) {
    v_counter = recursiveDrawing(nd-&gt; mChildren[n], v_counter, draw_mode);
  }
  return v_counter;
}</pre></div><p>In the vertex<a id="id252" class="indexterm"/> shader, <code class="literal">pointcloud.vert</code>, we compute the color of vertices based on their positions in space. The<a id="id253" class="indexterm"/> remapping<a id="id254" class="indexterm"/> algorithm creates a heat map representation of the object in space, and it serves as an important depth cue for the human eye (depth perception):</p><div><pre class="programlisting">#version 150 core
// Input
in vec3 vertexPosition_modelspace;
// Output 
out vec4 color_based_on_position;
// Uniform/constant variable.
uniform mat4 MVP;
//heat map generator
vec4 heatMap(float v, float vmin, float vmax){
  float dv;
  float r=1.0f, g=1.0f, b=1.0f;
  if (v &lt; vmin)
    v = vmin;
  if (v &gt; vmax)
    v = vmax;
  dv = vmax - vmin;
  if (v &lt; (vmin + 0.25f * dv)) {
    r = 0.0f;
    g = 4.0f * (v - vmin) / dv;
  } else if (v &lt; (vmin + 0.5f * dv)) {
    r = 0.0f;
    b = 1.0f + 4.0f * (vmin + 0.25f * dv - v) / dv;
  } else if (v &lt; (vmin + 0.75f * dv)) {
    r = 4.0f * (v - vmin - 0.5f * dv) / dv;
    b = 0.0f;
  } else {
    g = 1.0f + 4.0f * (vmin + 0.75f * dv - v) / dv;
    b = 0.0f;
  }
  //with 0.2 transparency - can be dynamic if we pass in variables
  return vec4(r, g, b, 0.2f);
} 

void main () {
  // Output position of the vertex, in clip space : MVP * position
  gl_Position =  MVP * vec4(vertexPosition_modelspace, 1.0f);
  // remapping the color based on the depth (z) value.
  color_based_on_position = heatMap(vertexPosition_modelspace.z, -1.0f, 1.0f);
}</pre></div><p>The vertex <a id="id255" class="indexterm"/>shader passes the heat-mapped color information<a id="id256" class="indexterm"/> along to the fragment shader through the <code class="literal">color_based_on_position</code> variable. Then, the final color is returned through the fragment shader (<code class="literal">pointcloud.frag</code>) directly without further processing. The implementation of <a id="id257" class="indexterm"/>such a simple pipeline is shown as follows:</p><div><pre class="programlisting">#version 150 core
out vec4 color;
in vec4 color_based_on_position;
void main(){
  color = color_based_on_position;
}</pre></div><p>Finally, we draw the scene with various styles: lines, points, and triangles with transparency. The following is the code snippet inside the drawing loop:</p><div><pre class="programlisting">//draw the left eye (but full screen)
glViewport(0, 0, width, height);
//compute the MVP matrix from the IOD and virtual image plane distance
computeStereoViewProjectionMatrices(g_window, IOD, depthZ, true);
//get the View and Model Matrix and apply to the rendering
glm::mat4 projection_matrix = getProjectionMatrix();
glm::mat4 view_matrix = getViewMatrix();
glm::mat4 model_matrix = glm::mat4(1.0);
model_matrix = glm::translate(model_matrix, glm::vec3(0.0f, 0.0f, -depthZ));
model_matrix = glm::rotate(model_matrix, 
glm::pi&lt;float&gt;()*rotateY, glm::vec3(0.0f, 1.0f, 0.0f));
model_matrix = glm::rotate(model_matrix, 
glm::pi&lt;float&gt;()*rotateX, glm::vec3(1.0f, 0.0f, 0.0f));
glm::mat4 mvp = projection_matrix * view_matrix * model_matrix;
//send our transformation to the currently bound shader,
//in the "MVP" uniform variable
glUniformMatrix4fv(matrix_id, 1, GL_FALSE, &amp;mvp[0][0]);
/* render scene with different modes that can be enabled separately to get different effects */
obj_loader-&gt;draw(GL_TRIANGLES);
if(drawPoints)
  obj_loader-&gt;draw(GL_POINTS);
if(drawLines)
  obj_loader-&gt;draw(GL_LINES);</pre></div><p>The series of <a id="id258" class="indexterm"/>screenshots that follow illustrate the aesthetically pleasing results we can achieve with our custom shader. The color mapping based on the depth position using the heat map shader <a id="id259" class="indexterm"/>provides a strong depth perception that helps us understand the 3D structure of the objects more easily. Furthermore, we can enable and disable various rendering <a id="id260" class="indexterm"/>options separately to achieve various effects. For example, the same object can be rendered with different styles: points, lines, and triangles (surfaces) with transparency.</p><p>To demonstrate the effects, we will first render two objects with points only. The first example is a dragon model:</p><div><img src="img/9727OS_06_01.jpg" alt="How to do it..."/></div><p>The second example is an architectural model:</p><div><img src="img/9727OS_06_02.jpg" alt="How to do it..."/></div><p>The <a id="id261" class="indexterm"/>point-based rendering <a id="id262" class="indexterm"/>style is great for visualizing a large dataset with unknown relations <a id="id263" class="indexterm"/>or distribution. Next, we will render the same objects with lines only:</p><div><img src="img/9727OS_06_03.jpg" alt="How to do it..."/></div><p>Here's the architectural model rendered with lines only:</p><div><img src="img/9727OS_06_04.jpg" alt="How to do it..."/></div><p>With the<a id="id264" class="indexterm"/> lines, now we can see the structure of the object <a id="id265" class="indexterm"/>more easily. This rendering technique is great for simple <a id="id266" class="indexterm"/>structures, such as architectural models and other well-defined models. In addition, we can render the scene with both points and lines enabled, as shown here:</p><div><img src="img/9727OS_06_05.jpg" alt="How to do it..."/></div><p>Here's <a id="id267" class="indexterm"/>the <a id="id268" class="indexterm"/>architectural <a id="id269" class="indexterm"/>model rendered with points and lines enabled:</p><div><img src="img/9727OS_06_06.jpg" alt="How to do it..."/></div><p>The combination of both <a id="id270" class="indexterm"/>points and lines provides<a id="id271" class="indexterm"/> additional visual cue to the structure of the object (that is, emphasis on the intersection points). Finally, we <a id="id272" class="indexterm"/>render the scene with all options enabled: points, lines, and triangles (surfaces) with transparency:</p><div><img src="img/9727OS_06_07.jpg" alt="How to do it..."/></div><p>Here's the architectural model rendered using points, lines and triangles with transparency:</p><div><img src="img/9727OS_06_08.jpg" alt="How to do it..."/></div><p>The final <a id="id273" class="indexterm"/>combination <a id="id274" class="indexterm"/>with all the options enabled provides an even more intuitive visualization of the volume <a id="id275" class="indexterm"/>of the object as well as the overall 3D structure. Alternatively, we can also enable the depth test and render the solid model with no transparency:</p><div><img src="img/9727OS_06_09.jpg" alt="How to do it..."/></div><p>Instructions on<a id="id276" class="indexterm"/> how to enable/disable these options at runtime are documented in the source code.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec113"/>How it works...</h2></div></div></div><p>By combining<a id="id277" class="indexterm"/> the Assimp library and OpenGL, we can now dynamically load 3D models on the screen and create visually appealing 3D effects through an OpenGL-based interactive visualization tool.</p><p>In <code class="literal">ObjLoader.cpp</code>, the <code class="literal">loadVertices</code> function converts the scene into a single vertex buffer array to <a id="id278" class="indexterm"/>reduce the complexity of memory management. In particular, this approach reduces the number of OpenGL memory copies and the number of memory buffers on the rendering side (that is, <code class="literal">glBufferData</code> and <code class="literal">glGenBuffers</code>). In addition, the loading function handles the scaling and centering of vertices based on the bounding box. This step is critical as most 3D formats do not normalize their coordinate system.</p><p>Next, the <code class="literal">draw</code> function in <code class="literal">ObjLoader.cpp</code> traverses the <code class="literal">aiScene</code> object and draws each part of the scene with the vertex buffer. In the case of point-based rendering, we can skip this step and directly draw the entire array using <code class="literal">glDrawArray</code> because there is no dependency among the neighboring vertices.</p><p>The vertex shader (<code class="literal">pointcloud.vert</code>) contains the implementation of the heat map color generator. The <code class="literal">heatmap</code> function takes in three parameters: the input value (that is, the depth or <em>z</em> value), the minimum value, and maximum value. It returns the heat map color representation in the RGBA format.</p><p>Inside the <a id="id279" class="indexterm"/>drawing loop, the <code class="literal">computeStereoViewProjectionMatrices</code> function constructs the view and projection matrices. The details are explained in the next section.</p><p>Finally, we can <a id="id280" class="indexterm"/>mix and match various rendering techniques; for example, by enabling both points and lines only for skeleton-based rendering. Various depth visual cues, such <a id="id281" class="indexterm"/>as occlusion and motion parallax, can be easily added by supporting rotation or translation of the object. To further improve the result, other rendering techniques such as lighting or shading can be added based on the application requirements.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec114"/>See also</h2></div></div></div><p>The Assimp library also supports many file formats in addition to <code class="literal">.obj</code> files. For example, we can load <code class="literal">.stl</code> files into our system without changing the source code at all.</p><p>To download more <a id="id282" class="indexterm"/>3D models, visit various 3D model-sharing websites such as <em>Makerbot ThingiVerse</em> (<a class="ulink" href="http://www.thingiverse.com/">http://www.thingiverse.com/</a>) or <em>Turbosquid</em> (<a class="ulink" href="http://www.turbosquid.com/">http://www.turbosquid.com/</a>):</p><div><img src="img/9727OS_06_10.jpg" alt="See also"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec43"/>Stereoscopic 3D rendering</h1></div></div></div><p>3D television <a id="id283" class="indexterm"/>and 3D glasses are becoming much more prevalent with the latest trends in consumer electronics and technological advances in wearable computing. In the market, there are currently many hardware options that allow us to visualize information with stereoscopic 3D technology. One common format is side-by-side 3D, which is supported by many 3D glasses as each eye sees an image of the same scene from a different perspective. In OpenGL, creating side-by-side 3D rendering requires asymmetric adjustment as well as viewport adjustment (that is, the area to be rendered) – asymmetric frustum parallel projection or equivalently to lens-shift in photography. This technique introduces no vertical parallax and widely adopted in the stereoscopic rendering. To illustrate this concept, the following diagram shows the geometry of the scene that a user sees from the right eye:</p><div><img src="img/9727OS_06_11.jpg" alt="Stereoscopic 3D rendering"/></div><p>The <strong>intraocular distance</strong> (<strong>IOD</strong>) <a id="id284" class="indexterm"/>is the distance between two eyes. As we can see from the diagram, the <strong>Frustum Shift</strong><a id="id285" class="indexterm"/> represents the amount of skew/shift for asymmetric frustrum adjustment. Similarly, for the left eye image, we perform the transformation with a mirrored setting. The implementation of this setup is described in the next section.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec115"/>How to do it...</h2></div></div></div><p>The following <a id="id286" class="indexterm"/>code illustrates the steps to construct the projection and view matrices for stereoscopic 3D visualization. The code uses the intraocular distance, the distance of the image plane, and the distance of the near clipping plane to compute the appropriate frustum shifts value. In the source file, <code class="literal">common/controls.cpp</code>, we add the implementation for the stereo 3D matrix setup:</p><div><pre class="programlisting">void computeStereoViewProjectionMatrices(GLFWwindow* window, float IOD, float depthZ, bool left_eye){
  int width, height;
  glfwGetWindowSize(window, &amp;width, &amp;height);
  //up vector
  glm::vec3 up = glm::vec3(0,-1,0);
  glm::vec3 direction_z(0, 0, -1);
  //mirror the parameters with the right eye
  float left_right_direction = -1.0f;
  if(left_eye)
    left_right_direction = 1.0f;
  float aspect_ratio = (float)width/(float)height;
  float nearZ = 1.0f;
  float farZ = 100.0f;
  double frustumshift = (IOD/2)*nearZ/depthZ;
  float top = tan(g_initial_fov/2)*nearZ;
  float right =
aspect_ratio*top+frustumshift*left_right_direction; 
//half screen
  float left = -aspect_ratio*top+frustumshift*left_right_direction;
  float bottom = -top;
  g_projection_matrix = glm::frustum(left, right, bottom, top, nearZ, farZ);
  // update the view matrix
 g_view_matrix = 
 glm::lookAt(
   g_position-direction_z+
     glm::vec3(left_right_direction*IOD/2, 0, 0), 
     //eye position
   g_position+
     glm::vec3(left_right_direction*IOD/2, 0, 0), 
     //centre position
   up //up direction
 );
</pre></div><p>In the rendering loop in <code class="literal">main.cpp</code>, we define the viewports for each eye (<em>left</em> and <em>right</em>) and set up the<a id="id287" class="indexterm"/> projection and view matrices accordingly. For each eye, we translate our camera position by half of the intraocular distance, as illustrated in the previous figure:</p><div><pre class="programlisting">if(stereo){
  //draw the LEFT eye, left half of the screen
  glViewport(0, 0, width/2, height);
  //computes the MVP matrix from the IOD and virtual image plane distance
  computeStereoViewProjectionMatrices(g_window, IOD, depthZ, true);
  //gets the View and Model Matrix and apply to the rendering
  glm::mat4 projection_matrix = getProjectionMatrix();
  glm::mat4 view_matrix = getViewMatrix();
  glm::mat4 model_matrix = glm::mat4(1.0);
  model_matrix = glm::translate(model_matrix, glm::vec3(0.0f, 0.0f, -depthZ));
  model_matrix = glm::rotate(model_matrix, glm::pi&lt;float&gt;() * rotateY, glm::vec3(0.0f, 1.0f, 0.0f));
  model_matrix = glm::rotate(model_matrix, glm::pi&lt;float&gt;() * rotateX, glm::vec3(1.0f, 0.0f, 0.0f));
  glm::mat4 mvp = projection_matrix * view_matrix * model_matrix;
  //sends our transformation to the currently bound shader,
  //in the "MVP" uniform variable
  glUniformMatrix4fv(matrix_id, 1, GL_FALSE, &amp;mvp[0][0]);
  //render scene, with different drawing modes

  if(drawTriangles)
  obj_loader-&gt;draw(GL_TRIANGLES);

  if(drawPoints)
    obj_loader-&gt;draw(GL_POINTS);

  if(drawLines)
    obj_loader-&gt;draw(GL_LINES);
  //Draw the RIGHT eye, right half of the screen
  glViewport(width/2, 0, width/2, height);
  computeStereoViewProjectionMatrices(g_window, IOD, depthZ, false);
  projection_matrix = getProjectionMatrix();
  view_matrix = getViewMatrix();
  model_matrix = glm::mat4(1.0);
  model_matrix = glm::translate(model_matrix, glm::vec3(0.0f, 0.0f, -depthZ));
  model_matrix = glm::rotate(model_matrix, glm::pi&lt;float&gt;() * rotateY, glm::vec3(0.0f, 1.0f, 0.0f));
  model_matrix = glm::rotate(model_matrix, glm::pi&lt;float&gt;() * rotateX, glm::vec3(1.0f, 0.0f, 0.0f));
  mvp = projection_matrix * view_matrix * model_matrix;
  glUniformMatrix4fv(matrix_id, 1, GL_FALSE, &amp;mvp[0][0]);
  if(drawTriangles)
    obj_loader-&gt;draw(GL_TRIANGLES);
  if(drawPoints)
    obj_loader-&gt;draw(GL_POINTS);
  if(drawLines)
    obj_loader-&gt;draw(GL_LINES);
}</pre></div><p>The final<a id="id288" class="indexterm"/> rendering result consists of two separate images on each side of the display, and note that each image is compressed horizontally by a scaling factor of two. For some display systems, each side of the display is required to preserve the same aspect ratio depending on the specifications of the display.</p><p>Here are the final screenshots of the same models in true 3D using stereoscopic 3D rendering:</p><div><img src="img/9727OS_06_12.jpg" alt="How to do it..."/></div><p>Here's the rendering of the architectural model in stereoscopic 3D:</p><div><img src="img/9727OS_06_13.jpg" alt="How to do it..."/></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec116"/>How it works...</h2></div></div></div><p>The<a id="id289" class="indexterm"/> stereoscopic 3D rendering technique is based on the parallel axis and asymmetric frustum perspective projection principle. In simpler terms, we rendered a separate image for each eye as if the object was seen at a different eye position but viewed on the same plane. Parameters such as the intraocular distance and frustum shift can be dynamically adjusted to provide the desired 3D stereo effects.</p><p>For example, by increasing or decreasing the frustum asymmetry parameter, the object will appear to be moved in front or behind the plane of the screen. By default, the zero parallax plane is set to the middle of the view volume. That is, the object is set up so that the center position of the object is positioned at the screen level, and some parts of the object will appear in front of or behind the screen. By increasing the frustum asymmetry (that is, positive parallax), the scene will appear to be pushed behind the screen. Likewise, by decreasing the frustum asymmetry (that is, negative parallax), the scene will appear to be pulled in front of the screen.</p><p>The <code class="literal">glm::frustum</code> function sets up the projection matrix, and we implemented the asymmetric frustum projection concept illustrated in the drawing. Then, we use the <code class="literal">glm::lookAt</code> function to adjust the eye position based on the IOP value we have selected.</p><p>To project the images side by side, we use the <code class="literal">glViewport</code> function to constrain the area within which the graphics can be rendered. The function basically performs an affine transformation (that is, scale and translation) which maps the normalized device coordinate to the window coordinate. Note that the final result is a side-by-side image in which the graphic is scaled by a factor of two vertically (or compressed horizontally). Depending on the hardware configuration, we may need to adjust the aspect ratio.</p><p>The <a id="id290" class="indexterm"/>current implementation supports side-by-side 3D, which is commonly used in most wearable <a id="id291" class="indexterm"/>
<strong>Augmented Reality</strong> (<strong>AR</strong>) or <strong>Virtual Reality</strong> (<strong>VR</strong>) <a id="id292" class="indexterm"/>glasses. Fundamentally, the rendering technique, namely the asymmetric frustum perspective projection described in our chapter, is platform-independent. For example, we have successfully tested our implementation on the<a id="id293" class="indexterm"/> Meta 1 Developer Kit (<a class="ulink" href="https://www.getameta.com/products">https://www.getameta.com/products</a>) and rendered the final results on the optical see-through stereoscopic 3D display:</p><div><img src="img/9727OS_06_14.jpg" alt="How it works..."/></div><p>Here is the front view of the Meta 1 Developer Kit, showing the optical see-through stereoscopic 3D display and 3D range-sensing camera (introduced in <a class="link" href="ch05.html" title="Chapter 5. Rendering of Point Cloud Data for 3D Range-sensing Cameras">Chapter 5</a>, <em>Rendering of Point Cloud Data for 3D Range-sensing Cameras</em>):</p><div><img src="img/9727OS_06_15.jpg" alt="How it works..."/></div><p>The result is shown as follows, with the stereoscopic 3D graphics rendered onto the real world (which forms the basis of augmented reality):</p><div><img src="img/9727OS_06_16.jpg" alt="How it works..."/></div><div><img src="img/9727OS_06_17.jpg" alt="How it works..."/></div><p>In the <a id="id294" class="indexterm"/>upcoming chapters, we will transition to the increasingly powerful and ubiquitous mobile platform and introduce how to use OpenGL to visualize data in interesting ways using built-in motion sensors on mobile devices. Further details on implementing augmented reality applications will be covered in <a class="link" href="ch09.html" title="Chapter 9. Augmented Reality-based Visualization on Mobile or Wearable Platforms">Chapter 9</a>, <em>Augmented reality-based visualization on mobile or wearable platforms</em>.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec117"/>See also</h2></div></div></div><p>In addition, we can easily extend our code to support shutter glasses-based 3D monitors by utilizing the Quad Buffered OpenGL APIs (refer to the <code class="literal">GL_BACK_RIGHT</code> and <code class="literal">GL_BACK_LEFT</code> flags in the <code class="literal">glDrawBuffer</code> function). Unfortunately, such 3D formats require specific hardware synchronization and often require higher frame rate display (for example, 120Hz) as well as a professional graphics card. Further information on how to implement stereoscopic 3D in your application can be found at <a class="ulink" href="http://www.nvidia.com/content/GTC-2010/pdfs/2010_GTC2010.pdf">http://www.nvidia.com/content/GTC-2010/pdfs/2010_GTC2010.pdf</a>.</p></div></div></body></html>