- en: Big Data Architecture and Design Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data is the digital trace that gets generated in today's digital world when
    we use the internet and other digital technology. Whatever we do digitally leaves
    a massive volume of data. Interestingly, we can do far smarter analysis with those
    traces and so, therefore, make smarter decisions and much more. For example, when
    you log in to any website it shows an advertisement for a product that you searched
    or browsed earlier, even if it was on an entirely different website. So by showing
    the product that you are interested in, regardless of the specific product selling
    site, the results of big data analysis and a smart way of selling means that the
    end user might like the product and be more likely to buy it.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter intends to introduce readers to the more common big data architectural
    patterns. Some brief details on the core parts of big data, its core principles,
    and characteristics are outlined,  including analytics principles, big data workload
    patterns, and optimal decision-making patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Please be aware that this chapter is a mere introduction to the patterns. Readers
    need to refer to other materials (references sections) that are available online
    and offline.
  prefs: []
  type: TYPE_NORMAL
- en: The four V's of big data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Big data has many definitions and many different implementations across various
    sectors. However, there are four common elements of any big data definition, which
    are popularly referred to as the V''s of big data. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Velocity**: This refers to the speed of data accumulation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volume**: This refers to the scale of data or the phase that data storage
    grows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**: This refers to the diversity of the data, such as structured,
    semi-structured, unstructured, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Veracity**: This refers to collected data''s accuracy and its reflection
    of facts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest addition to the V's group is **value**. This refers to our ability
    and needs to turn accumulated data into things of value. That is not just business
    value, but it can also be any significant added value for social, medical, and
    common causes.
  prefs: []
  type: TYPE_NORMAL
- en: Big data analysis and technology concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with the technology prerequisites for big data analysis, and then
    we will cover the life cycle of big data analysis. The prerequisites are:'
  prefs: []
  type: TYPE_NORMAL
- en: Flexible architectures, that supports various data types and patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upstream use of analytics for data relevance optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced analytics and real-time visualization to accelerate actions and understandings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaborative approaches for aligning stakeholders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data analysis life cycle provides a step-by-step methodology for organizing
    the data activities and tasks related to data acquiring, processing, analyzing
    and repurposing. The following are the stages of data analysis life cycle with
    a brief overview of each of them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data discovery**: Learn the business domain, frame the business problems
    as analytics challenges, and strategize and formulate initial hypotheses to start
    learning data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data preparations**: Data **Extraction**, **Load**, and **Transform** (**ELT**)
    and data **Extraction**, **Transform**, and **Load** (**ETL**) should be used
    to become familiarized with the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model planning**: Determine and formulate techniques, workflows, and best
    practices to follow. Learn about relationships between variables and choose the
    most suitable methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model building**: Develop datasets for testing, training, and production
    deployments. Evaluate tools to run the models and suggests additional tools, workflows,
    and execution environments, if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communicate results**: Identify critical findings, quantify the business
    values of the current exercise, the success criteria, risks, and mitigations,
    and present them to stakeholders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operationalize**: Deliver proofs of concepts, final reports, and technical
    documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data analysis and data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data is the result of collecting and managing large amounts of diverse data;
    data mining is all about searching data for unrecognized patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data analysis is about breaking the mined data and assessing the impact of those
    unrecognized methods. It may even create new patterns over time and help to develop
    working applications.
  prefs: []
  type: TYPE_NORMAL
- en: Data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data science is the process of cleaning, mining, and analyzing the data to derive
    insights of value from it. Extract data insights through a combination of exploratory
    data analysis and modeling. Data science is the process of distilling insights
    from data to inform decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data science creates models that capture the underlying patterns of complex
    systems and helps those models to become working applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0d1e39f-094f-4e83-81b7-75f707a404a4.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram intends to represent the data science process followed
    by a data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: Big data platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Any software or hardware platform should support large datasets; otherwise,
    it is hard to support those large datasets with traditional database tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/219cf4d5-3588-42f2-b4a5-758c44eb3790.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts a sample big data platform with supported sample
    tools, servers, hardware, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Big data engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Big data engineering gets the most value out of the vast amount of disparate
    data, data staging, profiling, and data cleansing in any big data platform. Also,
    it represents optimal ways of migrating the data from back office systems to the
    front office to help data analysts and data scientists:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e90ffc5-e2f5-421e-a54a-9f9d1a459364.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram accounts for a sample ecosystem of a big data engineering
    landscape. One can find numerous tools in each stage of the big data landscape.
    The following are some examples of those tools: Hadoop, Oozie, Flume, Hive, HBase,
    Apache Pig, Apache Spark, MapReduce, YARN, Sqoop, ZooKeeper, text analytics, and
    so on. However, we are not going to discuss all those tools here as it is out
    side of the scope of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Big data governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Any big data enterprise would need to develop and enhance broader enterprise
    information governance by bringing rules or policies for optimization and privacy
    and also find avenues for monetizing (value) at the same time as ensuring regulatory
    compliance and facilitating prudent risk management:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c4de7b7-c471-4d44-8414-63ccfd796e02.png)'
  prefs: []
  type: TYPE_IMG
- en: Big data architecture landscape and layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should be able to extract valuable, meaningful information (**insights**)
    from the enormous volumes of data to improve an organization''s decisions that
    involve various challenges, such as data regulations, faster decisions, interactions
    with customers, dealing with legacy systems, disparate data sources, and so on.
    So, to address all those challenges efficiently, researchers came up with a unified
    architecture consisting of layers at different levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9fb2ccd3-3281-41b6-8c76-92e6c7159802.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding pyramid depicts the significant attributes of big data layers
    and the problems that are addressed in each layer. As we have mentioned earlier, big
    data is not a single technology or a framework solving just a set of use cases;
    it is a set of tools, processes, technologies, and a system infrastructure that
    helps businesses to make much smarter analysis and take smarter decisions based
    on the massive volume of data traces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unified big data architecture consists of various layers. It provides a way
    to organize different components to address problems and it represents unique
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Big data sources**: Data coming from several channels, such as handheld devices,
    software applications, sensors, legacy databases, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data messaging and storage**: Acquires data from the data sources, data compliance,
    and storage formatting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data analysis**: Data model management, analytics engines**,** and access
    to data message stores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consumption**: Dashboards, insights, reporting, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b39a5a89-b0e5-4863-b4c2-a44cc530ff9f.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts different levels and layers of the big data landscape.
    These layers perhaps may be considered as a summary of our earlier introductions
    of big data concepts and the realization of values in each layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we look at patterns, let''s summarize the big data architecture principle
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Decoupled data bus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right tool usage for the job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data structure, latency, throughput, access patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lambda architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Immutable logs, batch/speed/serving layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud-based infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System maintenance with low or no admin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost-effective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data architecture patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will take you through big data design patterns, based on
    the following big data architectural patterns, and give a brief overview of the
    big data architectural patterns.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MapReduce is a software framework implementation that processes and generates
    big datasets by applying parallel and distributed algorithms on a cluster infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary methods of MapReduce are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Map**: Responsible for filtering and sorting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduce**: Responsible for operations (for example, counting the number of
    records)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lambda architecture pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To address big data challenges (described earlier in this chapter), there needs
    to be a data processing architecture to handle massive quantities of data to process
    rapidly with batch processing and stream processing methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some fundamental characteristics of the Lambda architecture are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It is dependent on underlying data principles of append-only, immutable, and
    atomic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It thrives on balancing latency, throughput, and fault-tolerance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It correlates with the growth of big data and real-time analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It helps to mitigate the latencies of MapReduce
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/e3933109-37af-40bb-966e-0e2ea821b541.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts the Lambda architecture with three primary layers
    called the batch processing layer, the speed or real-time processing layer, and
    serving layers for responding to queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three primary layers are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch layer**: This precomputes results, using a distributed processing system
    output to the read-only data store, and updates views by replacing the existing
    precomputed views. Data accuracy in the views is high with batch jobs (accuracy
    over latency).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed**/**Real-time layer**: This processes data streams in real time and
    the views are almost instantaneous, but maybe with less data accuracy (latency
    over accuracy). However, those views can be updated later by batch methods (accuracy
    over latency).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serving layer**: This stores outputs from the batch and speed layers to respond
    to ad-hoc queries either by precomputed views or new views from the processed
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data lake architecture pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In established enterprises, the most common business case is to make use of
    existing data infrastructure along with big data implementations. The data lake
    architecture pattern provides efficient ways to achieve reusing most of the data
    infrastructure and, at the same time, get the benefits of big data paradigm shifts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data lakes have the following essential characteristics to address:'
  prefs: []
  type: TYPE_NORMAL
- en: Manage abundant unprocessed data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retain data as long as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to manage the data transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support dynamic schema
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram depicts a data lake pattern implementation. It is getting
    raw data into data storage from different data sources. Also, the received data
    needs to be retained as long as possible in the data warehouse. Conditioning is
    conducted only after a data source has been identified for immediate use in the
    mainline analytics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0dd5344-41ee-4370-8237-50b606e33ad2.png)'
  prefs: []
  type: TYPE_IMG
- en: Data lakes provide a mechanism for capturing and exploring potentially useful
    data without incurring additional transactional systems storage costs, or any
    conditioning effort to bring data sources into those transactional systems.
  prefs: []
  type: TYPE_NORMAL
- en: Data lake implementation includes HDFS, AWS S3, distributed file systems, and
    so on. Microsoft, Amazon, EMC, Teradata, and Hortonworks are prominent vendors
    with data lake implementation among their products and they sell these technologies.
    Data lakes can also be a cloud **Infrastructure as a Service **(**IaaS**).
  prefs: []
  type: TYPE_NORMAL
- en: Big data design patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section covers most prominent big data design patterns by various data
    layers such as data sources and ingestion layer, data storage layer and data access
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Data sources and ingestion layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Enterprise big data systems face a variety of data sources with non-relevant
    information (noise) alongside relevant (signal) data. Noise ratio is very high
    compared to signals, and so filtering the noise from the pertinent information,
    handling high volumes, and the velocity of data is significant. This is the responsibility
    of the ingestion layer. The common challenges in the ingestion layers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple data source load and prioritization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingested data indexing and tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data validation and cleansing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformation and compression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5b98d83a-71c5-4fae-bbbf-8b4f8945c931.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts the building blocks of the ingestion layer and
    its various components. We need patterns to address the challenges of **data sources**
    to ingestion layer communication that takes care of performance, scalability,
    and availability requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will discuss the following ingestion and streaming patterns
    and how they help to address the challenges in ingestion layers. We will also
    touch upon some common workload patterns as well, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Multisource extractor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multidestination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protocol converter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Just-in-time** (**JIT**) transformation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time streaming pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multisource extractor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An approach to ingesting multiple data types from multiple data sources efficiently
    is termed a *Multisource extractor*. Efficiency represents many factors, such
    as data velocity, data size, data frequency, and managing various data formats
    over an unreliable network, mixed network bandwidth, different technologies, and
    systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a8b3607-a5af-43df-883b-a87b32846c76.png)'
  prefs: []
  type: TYPE_IMG
- en: The multisource extractor system ensures high availability and distribution.
    It also confirms that the vast volume of data gets segregated into multiple batches
    across different nodes. The single node implementation is still helpful for lower
    volumes from a handful of clients, and of course, for a significant amount of
    data from multiple clients processed in batches. Partitioning into small volumes
    in clusters produces excellent results.
  prefs: []
  type: TYPE_NORMAL
- en: Data enrichers help to do initial data aggregation and data cleansing. Enrichers
    ensure file transfer reliability, validations, noise reduction, compression, and
    transformation from native formats to standard formats. Collection agent nodes
    represent intermediary cluster systems, which helps final data processing and
    data loading to the destination systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the benefits of the multisource extractor:'
  prefs: []
  type: TYPE_NORMAL
- en: Provides reasonable speed for storing and consuming the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better data prioritization and processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drives improved business decisions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoupled and independent from data production to data consumption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data semantics and detection of changed data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaleable and fault tolerance system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the impacts of the multisource extractor:'
  prefs: []
  type: TYPE_NORMAL
- en: Difficult or impossible to achieve near real-time data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Need to maintain multiple copies in enrichers and collection agents, leading
    to data redundancy and mammoth data volume in each node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High availability trade-off with high costs to manage system capacity growth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure and configuration complexity increases to maintain batch processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multidestination pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In multisourcing, we saw the raw data ingestion to HDFS, but in most common
    cases the enterprise needs to ingest raw data not only to new HDFS systems but
    also to their existing traditional data storage, such as Informatica or other
    analytics platforms. In such cases, the additional number of data streams leads
    to many challenges, such as storage overflow, data errors (also known as data
    regret), an increase in time to transfer and process data, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The multidestination pattern is considered as a better approach to overcome
    all of the challenges mentioned previously. This pattern is very similar to multisourcing
    until it is ready to integrate with multiple destinations (refer to the following
    diagram). The router publishes the improved data and then broadcasts it to the
    subscriber destinations (already registered with a publishing agent on the router).
    Enrichers can act as publishers as well as subscribers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/712d489f-f2d2-45a0-aac2-f7448b4fa5a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Deploying routers in the cluster environment is also recommended for high volumes
    and a large number of subscribers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the benefits of the multidestination pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: Highly scalable, flexible, fast, resilient to data failure, and cost-effective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organization can start to ingest data into multiple data stores, including its
    existing RDBMS as well as NoSQL data stores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows you to use simple query language, such as Hive and Pig, along with traditional
    analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides the ability to partition the data for flexible access and decentralized processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possibility of decentralized computation in the data nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to replication on HDFS nodes, there are no data regrets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-reliant data nodes can add more nodes without any delay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the impacts of the multidestination pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: Needs complex or additional infrastructure to manage distributed nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Needs to manage distributed data in secured networks to ensure data security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Needs enforcement, governance, and stringent practices to manage the integrity
    and consistency of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protocol converter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a mediatory approach to provide an abstraction for the incoming data
    of various systems. The protocol converter pattern provides an efficient way to
    ingest a variety of unstructured data from multiple data sources and different
    protocols.
  prefs: []
  type: TYPE_NORMAL
- en: 'The message exchanger handles synchronous and asynchronous messages from various
    protocol and handlers as represented in the following diagram. It performs various
    mediator functions, such as file handling, web services message handling, stream
    handling, serialization, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f546fb4c-3683-48df-9e61-9319ceb237a7.png)'
  prefs: []
  type: TYPE_IMG
- en: In the protocol converter pattern, the ingestion layer holds responsibilities
    such as identifying the various channels of incoming events, determining incoming
    data structures, providing mediated service for multiple protocols into suitable
    sinks, providing one standard way of representing incoming messages, providing
    handlers to manage various request types, and providing abstraction from the incoming
    protocol layers.
  prefs: []
  type: TYPE_NORMAL
- en: Just-In-Time (JIT) transformation pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The JIT transformation pattern is the best fit in situations where raw data
    needs to be preloaded in the data stores before the transformation and processing
    can happen. In this kind of business case, this pattern runs independent preprocessing
    batch jobs that clean, validate, corelate, and transform, and then store the transformed
    information into the same data store (HDFS/NoSQL); that is, it can coexist with
    the raw data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a73c69f8-f253-4aaf-a429-6cacab8bc841.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts the datastore with raw data storage along with
    transformed datasets. Please note that the data enricher of the multi-data source
    pattern is absent in this pattern and more than one batch job can run in parallel
    to transform the data as required in the big data storage, such as HDFS, Mongo
    DB, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time streaming pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most modern businesses need continuous and real-time processing of unstructured
    data for their enterprise big data applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Real-time streaming implementations need to have the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimize latency by using large in-memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event processors are atomic and independent of each other and so are easily
    scalable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide API for parsing the real-time information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Independent deployable script for any node and no centralized master node implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The real-time streaming pattern suggests introducing an optimum number of event
    processing nodes to consume different input data from the various data sources
    and introducing listeners to process the generated events (from event processing
    nodes) in the event processing engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/820dee03-e1d2-4fb3-9689-a2e597c8e019.png)'
  prefs: []
  type: TYPE_IMG
- en: Event processing engines (event processors) have a sizeable in-memory capacity,
    and the event processors get triggered by a specific event. The trigger or alert
    is responsible for publishing the results of the in-memory big data analytics
    to the enterprise business process engines and, in turn, get redirected to various
    publishing channels (mobile, CIO dashboards, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Big data workload patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Workload patterns help to address data workload challenges associated with different
    domains and business cases efficiently. The big data design pattern manifests
    itself in the solution construct, and so the workload challenges can be mapped
    with the right architectural constructs and thus service the workload.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts a snapshot of the most common workload patterns
    and their associated architectural constructs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e00eebef-de3c-4519-8a86-436753437962.png)'
  prefs: []
  type: TYPE_IMG
- en: Workload design patterns help to simplify and decompose the business use cases
    into workloads. Then those workloads can be methodically mapped to the various
    building blocks of the big data solution architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data storage layer is responsible for acquiring all the data that are gathered
    from various data sources and it is also liable for converting (if needed) the
    collected data to a format that can be analyzed. The following sections discuss
    more on data storage layer patterns.
  prefs: []
  type: TYPE_NORMAL
- en: ACID versus BASE versus CAP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional RDBMS follows **atomicity**, **consistency**, **isolation**, **and
    durability** (**ACID**) to provide reliability for any user of the database. However,
    searching high volumes of big data and retrieving data from those volumes consumes
    an enormous amount of time if the storage enforces ACID rules. So, big data follows
    **basically available**, **soft state**, **eventually consistent** (**BASE**),
    a phenomenon for undertaking any search in big data space.
  prefs: []
  type: TYPE_NORMAL
- en: Database theory suggests that the NoSQL big database may predominantly satisfy
    two properties and relax standards on the third, and those properties are **consistency**,
    **availability**, **and partition tolerance** (**CAP**).
  prefs: []
  type: TYPE_NORMAL
- en: 'With the ACID, BASE, and CAP paradigms, the big data storage design patterns
    have gained momentum and purpose. We will look at those patterns in some detail
    in this section. The patterns are:'
  prefs: []
  type: TYPE_NORMAL
- en: Façade pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NoSQL pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polyglot pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Façade pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This pattern provides a way to use existing or **traditional existing data warehouses** along
    with big data storage (such as Hadoop). It can act as a façade for the enterprise
    data warehouses and business intelligence tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the façade pattern, the data from the different data sources get aggregated
    into **HDFS** before any transformation, or even before loading to the **traditional
    existing data warehouses**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b003b1dd-2ba3-4c27-9ab9-42f843c49d1a.png)'
  prefs: []
  type: TYPE_IMG
- en: The façade pattern allows structured data storage even after being ingested
    to HDFS in the form of structured storage in an **RDBMS**, or in NoSQL databases,
    or in a memory cache. The façade pattern ensures reduced data size, as only the
    necessary data resides in the structured storage, as well as faster access from
    the storage.
  prefs: []
  type: TYPE_NORMAL
- en: NoSQL pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This pattern entails getting NoSQL alternatives in place of traditional RDBMS
    to facilitate the rapid access and querying of big data. The NoSQL database stores
    data in a columnar, non-relational style. It can store data on local disks as
    well as in HDFS, as it is HDFS aware. Thus, data can be distributed across data
    nodes and fetched very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at four types of NoSQL databases in brief:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Column-oriented DBMS**: Simply called a columnar store or big table data
    store, it has a massive number of columns for each tuple. Each column has a column
    key. Column family qualifiers represent related columns so that the columns and
    the qualifiers are retrievable, as each column has a column key as well. These
    data stores are suitable for fast writes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/73b124c4-a9df-4803-8b42-1c013882d495.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Key-value pair database**: A key-value database is a data store that, when
    presented with a simple string (key), returns an arbitrarily large data (value).
    The key is bound to the value until it gets a new value assigned into or from
    a database. The key-value data store does not need to have a query language. It
    provides a way to add and remove key-value pairs. A key-value store is a dictionary
    kind of data store, where it has a list of words and each word represents one
    or more definitions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph database**: This is a representation of a system that contains a sequence
    of nodes and relationships that creates a graph when combined. A graph represents
    three data fields: nodes, relationships, and properties. Some types of graph store
    are referred to as triple stores because of their node-relationship-node structure.
    You may be familiar with applications that provide evaluations of similar or likely
    characteristics as part of the search (for example, a user bought this item also
    bought... is a good illustration of graph store implementations).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/aa3ed6f0-9794-4b41-bebe-6daf15b720fd.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Document database**: We can represent a graph data store as a tree structure.
    Document trees have a single root element or sometimes even multiple root elements
    as well. Note that there is a sequence of branches, sub-branches, and values beneath
    the root element. Each branch can have an expression or relative path to determine
    the traversal path from the origin node (root) and to any given branch, sub-branch,
    or value. Each branch may have a value associated with that branch. Sometimes
    the existence of a branch of the tree has a specific meaning, and sometimes a
    branch must have a given value to be interpreted correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00750a0b-f843-45f4-942e-7faa2c83fb8b.png)'
  prefs: []
  type: TYPE_IMG
- en: The following table summarizes some of the NoSQL use cases, providers, tools
    and scenarios that might need NoSQL pattern considerations. Most of this pattern
    implementation is already part of various vendor implementations, and they come
    as out-of-the-box implementations and as plug and play so that any enterprise
    can start leveraging the same quickly.
  prefs: []
  type: TYPE_NORMAL
- en: '| **NoSQL DB to Use** | **Scenario** | **Vendor / Application / Tools** |'
  prefs: []
  type: TYPE_TB
- en: '| Columnar database | Application that needs to fetch entire related columnar
    family based on a given string: for example, search engines | SAP HANA / IBM DB2
    BLU / ExtremeDB / EXASOL / IBM Informix / MS SQL Server / MonetDB |'
  prefs: []
  type: TYPE_TB
- en: '| Key Value Pair database | Needle in haystack applications (refer to the *Big
    data workload patterns* given in this section) | Redis / Oracle NoSQL DB / Linux
    DBM / Dynamo / Cassandra |'
  prefs: []
  type: TYPE_TB
- en: '| Graph database | Recommendation engine: application that provides evaluation
    of *Similar to / Like*: for example, *User that bought this item also bought*
    | ArangoDB / Cayley / DataStax / Neo4j / Oracle Spatial and Graph / Apache Orient
    DB / Teradata Aster |'
  prefs: []
  type: TYPE_TB
- en: '| Document database | Applications that evaluate churn management of social
    media data or non-enterprise data | Couch DB / Apache Elastic Search / Informix
    / Jackrabbit / Mongo DB / Apache SOLR |'
  prefs: []
  type: TYPE_TB
- en: Polyglot pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional (RDBMS) and multiple storage types (files, CMS, and so on) coexist
    with big data types (NoSQL/HDFS) to solve business problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most modern business cases need the coexistence of legacy databases. At the
    same time, they would need to adopt the latest big data techniques as well. Replacing
    the entire system is not viable and is also impractical. The polyglot pattern
    provides an efficient way to combine and use multiple types of storage mechanisms,
    such as Hadoop, and RDBMS. Big data appliances coexist in a storage solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1319b88d-5143-4afa-b94a-c25853e98785.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram represents the polyglot pattern way of storing data in
    different storage types, such as RDBMS, key-value stores, NoSQL database, CMS
    systems, and so on. Unlike the traditional way of storing all the information
    in one single data source, polyglot facilitates any data coming from all applications
    across multiple sources (RDBMS, CMS, Hadoop, and so on) into different storage
    mechanisms, such as in-memory, RDBMS, HDFS, CMS, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Data access layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data access in traditional databases involves JDBC connections and HTTP access
    for documents. However, in big data, the data access with conventional method
    does take too much time to fetch even with cache implementations, as the volume
    of the data is so high.
  prefs: []
  type: TYPE_NORMAL
- en: So we need a mechanism to fetch the data efficiently and quickly, with a reduced
    development life cycle, lower maintenance cost, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data access patterns mainly focus on accessing big data resources of two primary
    types:'
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end user-driven API (access through simple queries)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developer API (access provision through API methods)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we will discuss the following data access patterns that held
    efficient data access, improved performance, reduced development life cycles,
    and low maintenance costs for broader data access:'
  prefs: []
  type: TYPE_NORMAL
- en: Connector pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lightweight stateless pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service locator pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Near real-time pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stage transform pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9c91c144-6ee1-4389-b8fe-a0bb68524e77.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram represents the big data architecture layouts where the
    big data access patterns help data access. We discuss the whole of that mechanism
    in detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Connector pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The developer API approach entails fast data transfer and data access services
    through APIs. It creates optimized data sets for efficient loading and analysis.
    Some of the big data appliances abstract data in NoSQL DBs even though the underlying
    data is in HDFS, or a custom implementation of a filesystem so that the data access
    is very efficient and fast.
  prefs: []
  type: TYPE_NORMAL
- en: The connector pattern entails providing developer API and SQL like query language
    to access the data and so gain significantly reduced development time. As we saw
    in the earlier diagram, big data appliances come with connector pattern implementation.
    The big data appliance itself is a complete big data ecosystem and supports virtualization,
    redundancy, **replication using protocols** (**RAID**), and some appliances host
    NoSQL databases as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2b3223c-4262-4a25-beac-edf8ff85b1b7.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows a sample connector implementation for Oracle big
    data appliances. The data connector can connect to Hadoop and the big data appliance
    as well. It is an example of a custom implementations that we described earlier
    to facilitate faster data access with less development time.
  prefs: []
  type: TYPE_NORMAL
- en: Lightweight stateless pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This pattern entails providing data access through web services, and so it
    is independent of platform or language implementations. The data is fetched through
    restful HTTP calls, making this pattern the most sought after in cloud deployments.
    WebHDFS and HttpFS are examples of lightweight stateless pattern implementation
    for HDFS HTTP access. It uses the HTTP REST protocol. The HDFS system exposes
    the REST API (web services) for consumers who analyze big data. This pattern reduces
    the cost of ownership (pay-as-you-go) for the enterprise, as the implementations
    can be part of an **integration Platform as a Service** (**iPaaS**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b23b296-cfd1-4802-92bd-cec481f41d73.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts a sample implementation for HDFS storage that
    exposes HTTP access through the HTTP web interface.
  prefs: []
  type: TYPE_NORMAL
- en: Service locator pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a big data storage landscape, there are different types of data format (polyglot
    persistence), and if one needs to select and analyze a specific storage type from
    the list of stored data, then the service locator pattern comes in handy. It provides
    the flexibility to manipulate, filter, select, and co-relate services from the
    service catalog when storage access is with a SaaS model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f6b5f03-ee8f-4dee-9d2c-702a2657415f.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows a sample implementation of a service locator pattern.
    Observed data from various sources get aggregated and exposed through a service
    catalog and is available for visualization, or perhaps for further analysis. Service
    aggregators can aggregate services within or outside of enterprises. Different
    visualization tools can mix and match these services to show enterprise data alongside
    social media which is a different format than the other data source formats.
  prefs: []
  type: TYPE_NORMAL
- en: Near real-time pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For any enterprise to implement real-time data access or near real-time data
    access, the key challenges to be addressed are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rapid determination of data**: Ensure rapid determination of data and make
    swift decisions (within a few seconds, not in minutes) before the data becomes
    meaningless'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rapid analysis**: Ability to analyze the data in real time and spot anomalies
    and relate them to business events, provide visualization, and generate alerts
    at the moment that the data arrived'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some examples of systems that would need real-time data analysis are:'
  prefs: []
  type: TYPE_NORMAL
- en: Radar systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer services applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ATMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social media platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intrusion detection systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Storm and in-memory applications such as Oracle Coherence, Hazelcast IMDG,
    SAP HANA, TIBCO, Software AG (Terracotta), VMware, and Pivotal GemFire XD are
    some of the in-memory computing vendor/technology platforms that can implement
    near real-time data access pattern applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ce82b71-635a-4f3a-9898-c544a26e3b26.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram, with multi-cache implementation at the ingestion
    phase, and with filtered, sorted data in multiple storage destinations (here one
    of the destinations is a cache), one can achieve near real-time access. The cache
    can be of a NoSQL database, or it can be any in-memory implementations tool, as
    mentioned earlier. The preceding diagram depicts a typical implementation of a
    log search with SOLR as a search engine.
  prefs: []
  type: TYPE_NORMAL
- en: Stage transform pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the big data world, a massive volume of data can get into the data store.
    However, all of the data is not required or meaningful in every business case.
    The stage transform pattern provides a mechanism for reducing the data scanned
    and fetches only relevant data.
  prefs: []
  type: TYPE_NORMAL
- en: 'HDFS has raw data and business-specific data in a NoSQL database that can provide
    application-oriented structures and fetch only the relevant data in the required
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b15c6261-0f0d-46bc-87f9-257807dd1945.png)'
  prefs: []
  type: TYPE_IMG
- en: Combining the stage transform pattern and the NoSQL pattern is the recommended
    approach in cases where a reduced data scan is the primary requirement. The preceding
    diagram depicts one such case for a recommendation engine where we need a significant
    reduction in the amount of data scanned for an improved customer experience.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of the virtualization of data from HDFS to a NoSQL database,
    integrated with a big data appliance, is a highly recommended mechanism for rapid
    or accelerated data fetch. We have already seen that in the near real-time implementation
    shown earlier in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Rapid data analysis pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For faster data processing and access, the enterprise can choose any of the
    following tools in its data landscape. Each implementation has its own merits
    and purpose; we suggest reading each implementation in detail from the references
    that we have provided and choose the best for your enterprise needs:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bash Reduce
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disco (Nokia Research)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph Lab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Storm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Big Query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data discovery and analysis layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data discovery and analysis in big data is different from the traditional analysis
    of structured RDBMS data from limited sets. Big data analysis needs a more sophisticated
    mechanism, as it involves natural language processing, unstructured texts, videos
    and images, RFID data, and so on. This section touches upon some data discovery
    and analysis patterns and mentions the tools that are supporting these patterns.
    Readers are encouraged to read other referenced materials to get a more profound
    understanding of each pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ece67b31-700a-4344-bf5f-73ad7dc1b3ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Data queuing pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is a most common situation that a system needs to handle spikes while analyzing
    data. This pattern introduces a workflow or process to queue additional chunks
    of data and then route them to available nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef55dc1c-a7b0-45d5-8771-b7c964733aa5.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts a sample implementation of a data queue and processors
    for additional workflows and routes to available nodes (of multiple nodes).
  prefs: []
  type: TYPE_NORMAL
- en: Using cloud IaaS is the best option to handle the spikes dynamically and yield
    better cost savings. It spins additional virtual machines as needed, with more
    when there is a spike, and fewer when traffic is slow or average).
  prefs: []
  type: TYPE_NORMAL
- en: Index-based insight pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This pattern defines indexes (keys) based on the inputs from the users who interact
    with customers. Iteratively, finding a range of indexes is the mechanism suggested
    by the index-based insight pattern. It sets the analysis mechanism or pattern
    to index a variable and to provide insight into common behaviors such as parents
    buying toys, and all children aged above 13 in a neighborhood. This pattern helps
    to find a crucial efficient lookup for rapid scanning but keeps related columns
    together.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This pattern helps to find a pattern of data inputs generated from heterogeneous
    devices, such as RFID devices, energy meters, signal devices, weather-related
    devices, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding data generated by automated systems, or devices without manual
    intervention, is a challenging task, and one needs to rely on algorithms and statistical
    methods. Fortunately, there are excellent algorithms that help to analyze this
    data, and some of the conventional algorithms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes classifier algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K Means clustering algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machine algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apriori algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypothesis testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ANOVA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks / artificial neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nearest neighbors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conjoint analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use one or more combinations of these algorithms as needed. Readers are
    encouraged to refer to other materials to get an insight into each algorithm,
    as covering them is not in the scope of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Converge(r) pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most business cases, as we have seen earlier, enterprises need to deal with
    traditional (structured) data and at the same time make use of big data to get
    enterprise-wide insights. The converge(r) pattern provides an efficient way to
    merge unstructured data with structured data and get insights and make decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In some business cases, enterprises may need to understand the sentiments (views
    and opinions) of their product from social media. The converge(r) pattern, combining
    external data formats with internal enterprise data formats, is one of the best
    options. This pattern entails combining those views and opinions from social media
    with internal data analysis to get combined data insights.
  prefs: []
  type: TYPE_NORMAL
- en: The data convergence needs to happen before the enterprise data is analyzed.
    So we can use the façade pattern (refer to the *Data storage layer* section in
    this chapter), and also use machine learning patterns to use the grouped data
    from the social media (for impacts, revenues, brand images, churn rates, and so
    on).
  prefs: []
  type: TYPE_NORMAL
- en: Tools such as DrivenData, TianChi, Crowd Analytics, InfoChimps, Kaggle, and
    TopCoder provide out-of-the-box converge(r) implementation, and we can use those
    tools along with ETL tools for data transformation, cleansing, and enrichment,
    and get insights by combining the data.
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data visualization's primary responsibility is to provide more insights from
    the massive volume of data by using visual representations, such as statistical
    reports, charts, and so on. Visualization of insights is the most visible portion
    to the stakeholders and sponsors; it is the most impactful part of the whole big
    data paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: As visualization is most impactful and considering the vastness of the visualization,
    this section aims to provide only a brief introduction to a few of the common
    visualization patterns. However, we encourage readers to explore the exclusive
    visualization materials that we have provided in the reference sections.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10614bfc-e33e-473e-82f9-3f2b84e52a0f.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts data visualization patterns in a sample big data
    landscape. Visualization patterns need to support high-level views and also granular
    level details as visual representations. Moreover, visualization patterns can
    be used in conjunction with data access patterns to leverage the rapid access
    of data and its presentation.
  prefs: []
  type: TYPE_NORMAL
- en: First glimpse pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name suggests, this is an approach that provides primary or minimalistic
    visualization data and pulls detailed information only on demand.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern entails fetching only the most critical and essential data (which
    may be decided by machine learning patterns, rankings, scores, and so on) as a
    first glimpse and fetches drill-down data on demand. An example could be a search
    application displaying search results as only one page (the first page) and providing
    more data when the user needs it on subsequent pages.
  prefs: []
  type: TYPE_NORMAL
- en: Portal pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With most common cases where the enterprise already has reporting applications
    and intends to reuse the same for the visualization of big data, then this pattern
    entails enhancing the web application (portal) with scripting frameworks to enhance
    the legacy visualization, thus saving the enterprise the cost of having a new
    visualization tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lists some of the scripting frameworks one may want to include
    and enhance with enterprise portal and realize the portal pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: D3.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chart.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HighChart.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChartList.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raphel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pixi.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Webix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AnyChart
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pykcharts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cytoscape.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mashup view pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mashup view creates an aggregated mashup view from heterogeneous data stores
    such as Hadoop, cache, and RDBMS, thereby reducing the analysis time by aggregating
    the results of the queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'It helps to achieve higher performance for the queries by storing an aggregated
    mashups view in the HIVE layer, similar to the traditional data warehouse. The
    updates to the data warehouse are made as offline batch jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Some mashup view supported (vendor) tools** | **Some data integration mashup
    tools** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: IBM Netezza
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cassandra
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertica, Cloudera Impala
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hortonworks Stinger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Damia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yahoo Pipes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MS Popfly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Mashup Editor** (**GME**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exhibit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apatar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MashMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 12.2: Mashup view supported tools and data integration tools'
  prefs: []
  type: TYPE_NORMAL
- en: Some drawbacks with mashups that you may need to be aware of are text/data mismatch,
    object identifiers, schema mismatches, abstract level mismatches, and lower data
    quality or accuracy (due to data integration from independent sources).
  prefs: []
  type: TYPE_NORMAL
- en: Compression pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compression is one of the data reduction methods of big data analysis, as reduced
    data size is computationally less expensive.
  prefs: []
  type: TYPE_NORMAL
- en: The compression pattern provides a mechanism in situations where the enterprise
    needs to access data without aggregation or mashups. The compression pattern can
    help with faster data access from data storage by having standardized formats
    (with the need to transform to a standardized format regardless of data sources).
    The advantage of having formats is to ensure data correctness and consistency.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular compression data analysis platform is R, and one can explore
    in-memory compression with ReRams as well.
  prefs: []
  type: TYPE_NORMAL
- en: Exploder pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a pattern to help data analysts to look at different datasets, finding
    a relation between different datasets, and also providing different perspectives.
    The exploder pattern is a useful pattern in cases where an enterprise need various
    views (visuals) for the data and there are no restrictions with the same kind
    of visual patterns.
  prefs: []
  type: TYPE_NORMAL
- en: It also allows one to drill down from one view to a different chart type or
    visualization pattern with a click.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the development field of data analytics is not new, it has become more
    critical than ever as it experiences prodigious quantities of data generated by
    businesses, sensors, applications, and so on. Once the generated data gets stored,
    it can give extraordinary insights and helps not only business enterprises but
    also government and non-government enterprises, social communities, the economy,
    and much more.
  prefs: []
  type: TYPE_NORMAL
- en: In current technology trends, big data has been involved in many evolutions,
    from just buzzwords to crunching data from machine learning algorithms. With the
    exponential explosion of high velocity, high volume, high variety, and the veracity
    of data sources and streams (the four V's), big data has become the inevitable
    representative of the architectures, tools, and technologies that handle enterprises
    increasingly demanding requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we have gone through a brief introduction of the four V''s
    of big data, data analysis technology, and concepts. We also touched upon the
    big data life cycle and how it helps different stakeholders to achieve and realize
    their data insights. A brief section covered big data landscapes, and the data
    layers, as well as most of the architectural patterns associated with big data,
    involving data pipelines: that is an ordered combination of data acquisition,
    integration, ingestion, fast processing, storage, rapid access, and analytics
    stages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most crucial theme of this book is architectural patterns, and this chapter
    reflects it in its big data architecture, and design patterns section, in a sequence
    of architecture patterns, such as MapReduce, Lambda, and data lake. Then we have
    covered most common big data (application) design patterns by layers: that is
    patterns in various big data architectural layers, such as data sources and the
    ingestion layer, the data storage layer, the data access layer, the data discovery
    and analysis layer, and the data visualization layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Covering big data architectural patterns in one chapter has been very challenging
    for us, and we have tried our best by providing samples of big data concepts and
    the most common patterns that help data architects and other data technology stakeholders.
    We hope this chapter provides them with a head start on their big data journey.
    As mentioned in many places across this chapter, we strongly encourage readers
    to refer to the citations section should they need to get exclusive patterns and
    details of implementations.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Citations and reference materials:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Big data**: *Application Architecture Q and A, A Problem-Solution Approach*
    by Nitin Sawant and Himanshu Shah (Apress 2013)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Big data governance**: *An Emerging Imperative* by Sunil Soares, (MC Press,
    October 2012)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://assured-cloud-computing.illinois.edu/files/2015/02/Cristina_Abad.pdf](http://assured-cloud-computing.illinois.edu/files/2015/02/Cristina_Abad.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://bigr.io/architecture/](http://bigr.io/architecture/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://blog.flutura.com//2012/08/11-core-big-data-workload-design.html](http://blog.flutura.com//2012/08/11-core-big-data-workload-design.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://ercoppa.github.io/HadoopInternals/HadoopArchitectureOverview.html](http://ercoppa.github.io/HadoopInternals/HadoopArchitectureOverview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://insightdatascience.com](http://insightdatascience.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.bcs.org/upload/pdf/enterprise-architecture-patterns-201016.pdf](http://www.bcs.org/upload/pdf/enterprise-architecture-patterns-201016.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.bigdatapatterns.org/design_patterns/automated_dataset_execution](http://www.bigdatapatterns.org/design_patterns/automated_dataset_execution)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.bigdatapatterns.org/overview](http://www.bigdatapatterns.org/overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.bigdatascienceschool.com/selfstudy](http://www.bigdatascienceschool.com/selfstudy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.infoworld.com/article/2616959/big-data/7-top-tools-for-taming-big-data.html](http://www.infoworld.com/article/2616959/big-data/7-top-tools-for-taming-big-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.pentaho.com/sites/default/files/uploads/resources/forrester_patterns_in_big_data.pdf](http://www.pentaho.com/sites/default/files/uploads/resources/forrester_patterns_in_big_data.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.refcodes.org/resources/Big%20data%20processing%20the%20lean%20way%20-%20a%20case%20study%20-%20v1.7.pdf](http://www.refcodes.org/resources/Big%20data%20processing%20the%20lean%20way%20-%20a%20case%20study%20-%20v1.7.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.yottastor.com/design-principles-big-data](http://www.yottastor.com/design-principles-big-data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/ftp/arxiv/papers/1201/1201.4479.pdf](https://arxiv.org/ftp/arxiv/papers/1201/1201.4479.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://bigdatawg.nist.gov/_uploadfiles/M0060_v1_8912129783.pdf](https://bigdatawg.nist.gov/_uploadfiles/M0060_v1_8912129783.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://blogs.msmvps.com/abu/2010/10/16/data-architecture-patterns-design-patterns-and-solution-patterns/](https://blogs.msmvps.com/abu/2010/10/16/data-architecture-patterns-design-patterns-and-solution-patterns/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://conferences.oreilly.com/strata/big-data-conference-ca-2015/public/schedule/detail/38774](https://conferences.oreilly.com/strata/big-data-conference-ca-2015/public/schedule/detail/38774)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://conferences.oreilly.com/strata/strataeu2014/public/schedule/detail/37305](https://conferences.oreilly.com/strata/strataeu2014/public/schedule/detail/37305)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://hackernoon.com/ingestion-and-processing-of-data-for-big-data-and-iot-solutions-659431e37b52](https://hackernoon.com/ingestion-and-processing-of-data-for-big-data-and-iot-solutions-659431e37b52)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://iwringer.wordpress.com/2015/08/03/patterns-for-streaming-realtime-analytics/](https://iwringer.wordpress.com/2015/08/03/patterns-for-streaming-realtime-analytics/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://link.springer.com/book/10.1007%2F978-1-4302-6293-0](https://link.springer.com/book/10.1007%2F978-1-4302-6293-0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://static1.squarespace.com/static/55007c24e4b001deff386756/t/564a2b7de4b0c1a8406915fb/1447701373291/Maniyam%2C+Sujee.pdf](https://static1.squarespace.com/static/55007c24e4b001deff386756/t/564a2b7de4b0c1a8406915fb/1447701373291/Maniyam%2C+Sujee.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://vision.cloudera.com/the-six-principles-of-modern-data-architecture/](https://vision.cloudera.com/the-six-principles-of-modern-data-architecture/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.datameer.com/wp-content/uploads/pdf/white_paper/Data-Preparation-Modern-BI-Common-Design-Patterns.pdf](https://www.datameer.com/wp-content/uploads/pdf/white_paper/Data-Preparation-Modern-BI-Common-Design-Patterns.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.dezyre.com/article/types-of-analytics-descriptive-predictive-prescriptive-analytics/209](https://www.dezyre.com/article/types-of-analytics-descriptive-predictive-prescriptive-analytics/209)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.ibm.com/developerworks/library/bd-archpatterns1/index.html](https://www.ibm.com/developerworks/library/bd-archpatterns1/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.import.io/post/best-big-data-tools-use/](https://www.import.io/post/best-big-data-tools-use/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.linkedin.com/pulse/top-10-guiding-principles-big-data-architecture-ram-narasimhan](https://www.linkedin.com/pulse/top-10-guiding-principles-big-data-architecture-ram-narasimhan)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.researchgate.net/publication/296634867_Device_Data_Ingestion_for_Industrial_Big_Data_Platforms_with_a_Case_Study](https://www.researchgate.net/publication/296634867_Device_Data_Ingestion_for_Industrial_Big_Data_Platforms_with_a_Case_Study)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.slideshare.net/AmazonWebServices/big-data-architectural-patterns-and-best-practices](https://www.slideshare.net/AmazonWebServices/big-data-architectural-patterns-and-best-practices)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.slideshare.net/AsterData/sas-ny-big-analytics-conference](https://www.slideshare.net/AsterData/sas-ny-big-analytics-conference)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.slideshare.net/cscyphers/big-data-platforms-an-overview](https://www.slideshare.net/cscyphers/big-data-platforms-an-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.slideshare.net/ZachGemignani/7-design-principles-44395597](https://www.slideshare.net/ZachGemignani/7-design-principles-44395597)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
