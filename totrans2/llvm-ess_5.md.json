["```cpp\nvoid fun() {\n  int iter, a, b;\n\n  for (iter = 0; iter < 10; iter++) {\n    a = 5;\n    if (iter == a)\n      b = 2;\n    else\n      b = 5;\n  }\n}\n```", "```cpp\n$ cat licm.ll\ndefine void @func(i32 %i) {\nEntry:\n br label %Loop\nLoop:\n %j = phi i32 [ 0, %Entry ], [ %Val, %Loop ]\n %loopinvar = mul i32 %i, 17\n %Val = add i32 %j, %loopinvar\n %cond = icmp eq i32 %Val, 0\n br i1 %cond, label %Exit, label %Loop\nExit: \n ret void\n}\n\n```", "```cpp\n$ opt -licm licm.ll -o licm.bc\n$ llvm-dis licm.bc -o licm_opt.ll\n$ cat licm_opt.ll\n; ModuleID = 'licm.bc'\n\ndefine void @func(i32 %i) {\nEntry:\n %loopinvar = mul i32 %i, 17\n br label %Loop\n\nLoop: \n; preds = %Loop, %Entry\n %j = phi i32 [ 0, %Entry ], [ %Val, %Loop ]\n %Val = add i32 %j, %loopinvar\n %cond = icmp eq i32 %Val, 0\n br i1 %cond, label %Exit, label %Loop\n\nExit: \n; preds = %Loop\n ret void\n}\n\n```", "```cpp\n$ cat scalevl.ll\ndefine void @fun() {\nentry:\n br label %header\nheader:\n %i = phi i32 [ 1, %entry ], [ %i.next, %body ]\n %cond = icmp eq i32 %i, 10\n br i1 %cond, label %exit, label %body\nbody:\n %a = mul i32 %i, 5\n %b = or i32 %a, 1\n %i.next = add i32 %i, 1\n br label %header\nexit: \n ret void\n}\n\n```", "```cpp\n$ opt -analyze -scalar-evolution scalevl.ll\nPrinting analysis 'Scalar Evolution Analysis' for function 'fun':\nClassifying expressions for: @fun\n %i = phi i32 [ 1, %entry ], [ %i.next, %body ]\n -->  {1,+,1}<%header> U: [1,11) S: [1,11)    Exits: 10\n %a = mul i32 %i, 5\n -->  {5,+,5}<%header> U: [5,51) S: [5,51)    Exits: 50\n %b = or i32 %a, 1\n -->  %b U: [1,0) S: full-set                 Exits: 51\n %i.next = add i32 %i, 1\n -->  {2,+,1}<%header> U: [2,12) S: [2,12)    Exits: 11\nDetermining loop execution counts for: @fun\nLoop %header: backedge-taken count is 9\nLoop %header: max backedge-taken count is 9\n\n```", "```cpp\n$ cat intrinsic.cpp\nint func()\n{\n int a[5];\n\n for (int i = 0; i != 5; ++i)\n a[i] = 0;\n\n return a[0];\n}\n\n```", "```cpp\n$ clang -emit-llvm -S intrinsic.cpp\n\n```", "```cpp\n$ opt -O1 intrinsic.ll -S -o -\n; ModuleID = 'intrinsic.ll'\ntarget datalayout = \"e-m:e-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-unknown-linux-gnu\"\n\n; Function Attrs: nounwind readnone uwtable\ndefine i32 @_Z4funcv() #0 {\n %a = alloca [5 x i32], align 16\n %a2 = bitcast [5 x i32]* %a to i8*\n call void @llvm.memset.p0i8.i64(i8* %a2, i8 0, i64 20, i32 16, i1 false)\n %1 = getelementptr inbounds [5 x i32], [5 x i32]* %a, i64 0, i64 0\n %2 = load i32, i32* %1, align 16\n ret i32 %2\n}\n\n; Function Attrs: nounwind argmemonly\ndeclare void @llvm.memset.p0i8.i64(i8* nocapture, i8, i64, i32, i1) #1\n\n```", "```cpp\nint a[4], b[4], c[4];\n\nvoid addsub() {\na[0] = b[0] + c[0];\na[1] = b[1] + c[1];\na[2] = b[2] + c[2];\na[3] = b[3] + c[3];\n}\n```", "```cpp\n; ModuleID = 'addsub.c'\n\n@a = global [4 x i32] zeroinitializer, align 4\n@b = global [4 x i32] zeroinitializer, align 4\n@c = global [4 x i32] zeroinitializer, align 4\n\n; Function Attrs: nounwind\ndefine void @addsub() {\nentry:\n  %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0)\n  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0)\n  %add = add nsw i32 %1, %0\n  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0)\n  %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1)\n  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1)\n  %add1 = add nsw i32 %3, %2\n  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1)\n  %4 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2)\n  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2)\n  %add2 = add nsw i32 %5, %4\n  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2)\n  %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3)\n  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)\n  %add3 = add nsw i32 %7, %6\n  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)\n  ret void\n}\n```", "```cpp\n// Use the bottom up slp vectorizer to construct chains that start\n// with store instructions.\n BoUpSLP R(&F, SE, TTI, TLI, AA, LI, DT, AC);\n```", "```cpp\n    // Scan the blocks in the function in post order.\n    for (auto BB : post_order(&F.getEntryBlock())) {\n      // Vectorize trees that end at stores.\n      if (unsigned count = collectStores(BB, R)) {\n        (void)count;\n        DEBUG(dbgs() << \"SLP: Found \" << count << \" stores to vectorize.\\n\");\n        Changed |= vectorizeStoreChains(R);\n      }\n      // Vectorize trees that end at reductions.\n      Changed |= vectorizeChainsInBlock(BB, R);\n    }\n```", "```cpp\nunsigned SLPVectorizer::collectStores(BasicBlock *BB, BoUpSLP &R) {\n  unsigned count = 0;\n  StoreRefs.clear();\n  const DataLayout &DL = BB->getModule()->getDataLayout();\n  for (Instruction &I : *BB) {\n    StoreInst *SI = dyn_cast<StoreInst>(&I);\n    if (!SI)\n      continue;\n\n    // Don't touch volatile stores.\n    if (!SI->isSimple())\n      continue;\n\n    // Check that the pointer points to scalars.\n    Type *Ty = SI->getValueOperand()->getType();\n    if (!isValidElementType(Ty))\n      continue;\n\n    // Find the base pointer.\n    Value *Ptr = GetUnderlyingObject(SI->getPointerOperand(), DL);\n\n    // Save the store locations.\n    StoreRefs[Ptr].push_back(SI);\n    count++;\n  }\n  return count;\n}\n```", "```cpp\nbool SLPVectorizer::vectorizeStoreChain(ArrayRef<Value *> Chain,\n                                        int CostThreshold, BoUpSLP &R,\n                                        unsigned VecRegSize) {\n   … \n   …\n    R.buildTree(Operands);\n\n    int Cost = R.getTreeCost();\n\n    DEBUG(dbgs() << \"SLP: Found cost=\" << Cost << \" for VF=\" << VF << \"\\n\");\n    if (Cost < CostThreshold) {\n      DEBUG(dbgs() << \"SLP: Decided to vectorize cost=\" << Cost << \"\\n\");\n      R.vectorizeTree();\n… \n…\n}\n```", "```cpp\nvoid BoUpSLP::buildTree(ArrayRef<Value *> Roots,\n                        ArrayRef<Value *> UserIgnoreLst) {\n    … \n    …\n    buildTree_rec(Roots, 0);\n    …\n    … \n}\n```", "```cpp\nvoid BoUpSLP::buildTree_rec(ArrayRef<Value *> VL, unsigned Depth) {\n…\n…\ncase Instruction::Add:\nnewTreeEntry(VL, true);\n    DEBUG(dbgs() << \"SLP: added a vector of bin op.\\n\");\n\n    // Sort operands of the instructions so that each side is more \n    // likely to have the sam opcode \n    if (isa<BinaryOperator>(VL0) && VL0->isCommutative()) {\n      ValueList Left, Right;\n      reorderInputsAccordingToOpcode(VL, Left, Right);\n      buildTree_rec(Left, Depth + 1);\n      buildTree_rec(Right, Depth + 1);\n      return;\n    }\n…\n…\n}\n```", "```cpp\ncase Instruction::Load: {\n    // Check that a vectorized load would load the same memory as a // scalar load.\n    // For example we don't want vectorize loads that are smaller than 8 bit.\n    // Even though we have a packed struct {<i2, i2, i2, i2>} LLVM treats\n    // loading/storing it as an i8 struct. If we vectorize loads/stores from\n    // such a struct we read/write packed bits disagreeing with the\n    // unvectorized version.\n    const DataLayout &DL = F->getParent()->getDataLayout();\n    Type *ScalarTy = VL[0]->getType();\n\n    if (DL.getTypeSizeInBits(ScalarTy) != DL.getTypeAllocSizeInBits(ScalarTy)) {\n      BS.cancelScheduling(VL);\n      newTreeEntry(VL, false);\n      DEBUG(dbgs() << \"SLP: Gathering loads of non-packed type.\\n\");\n      return;\n    }\n    // Check if the loads are consecutive or of we need to swizzle them.\n    for (unsigned i = 0, e = VL.size() - 1; i < e; ++i) {\n      LoadInst *L = cast<LoadInst>(VL[i]);\n      if (!L->isSimple()) {\n        BS.cancelScheduling(VL);\n        newTreeEntry(VL, false);\n        DEBUG(dbgs() << \"SLP: Gathering non-simple loads.\\n\");\n        return;\n      }\n\n      if (!isConsecutiveAccess(VL[i], VL[i + 1], DL)) {\n        if (VL.size() == 2 && isConsecutiveAccess(VL[1], VL[0], DL)) {\n          ++NumLoadsWantToChangeOrder;\n        }\n        BS.cancelScheduling(VL);\n        newTreeEntry(VL, false);\n        DEBUG(dbgs() << \"SLP: Gathering non-consecutive loads.\\n\");\n        return;\n      }\n    }\n    ++NumLoadsWantToKeepOrder;\n    newTreeEntry(VL, true);\n    DEBUG(dbgs() << \"SLP: added a vector of loads.\\n\");\n    return;\n  }\n```", "```cpp\nint BoUpSLP::getTreeCost() {\n  int Cost = 0;\n  DEBUG(dbgs() << \"SLP: Calculating cost for tree of size \"\n               << VectorizableTree.size() << \".\\n\");\n\n  // We only vectorize tiny trees if it is fully vectorizable.\n  if (VectorizableTree.size() < 3 && !isFullyVectorizableTinyTree()) {\n    if (VectorizableTree.empty()) {\n      assert(!ExternalUses.size() && \"We should not have any external users\");\n    }\n    return INT_MAX;\n  }\n\n  unsigned BundleWidth = VectorizableTree[0].Scalars.size();\n\n  for (unsigned i = 0, e = VectorizableTree.size(); i != e; ++i) {\n    int C = getEntryCost(&VectorizableTree[i]);\n    DEBUG(dbgs() << \"SLP: Adding cost \" << C << \" for bundle that starts with \" << *VectorizableTree[i].Scalars [0] << \" . \\n\" );\n    Cost += C;\n  }\n\n  SmallSet<Value *, 16> ExtractCostCalculated;\n  int ExtractCost = 0;\n  for (UserList::iterator I = ExternalUses.begin(), E = ExternalUses.end();\n       I != E; ++I) {\n    // We only add extract cost once for the same scalar.\n    if (!ExtractCostCalculated.insert(I->Scalar).second)\n      continue;\n\n    // Uses by ephemeral values are free (because the ephemeral value will be\n    // removed prior to code generation, and so the extraction will be\n    // removed as well).\n    if (EphValues.count(I->User))\n      continue;\n\n    VectorType *VecTy = VectorType::get(I->Scalar->getType(), BundleWidth);\n    ExtractCost +=\n        TTI->getVectorInstrCost(Instruction::ExtractElement, VecTy, I->Lane);\n  }\n\n  Cost += getSpillCost();\n\n  DEBUG(dbgs() << \"SLP: Total Cost \" << Cost + ExtractCost << \".\\n\");\n  return Cost + ExtractCost;\n}\n```", "```cpp\nint BoUpSLP::getEntryCost(TreeEntry *E) {\n…\n…\ncase Instruction::Store: {\n    // We know that we can merge the stores. Calculate the cost.\n    int ScalarStCost = VecTy->getNumElements() *\n                       TTI->getMemoryOpCost(Instruction::Store, ScalarTy, 1, 0);\n    int VecStCost = TTI->getMemoryOpCost(Instruction::Store, VecTy, 1, 0);\n    return VecStCost - ScalarStCost;\n  }\n…\n…\n}\n```", "```cpp\ncase Instruction::Add:  {\n// Calculate the cost of this instruction.\n    int ScalarCost = 0;\n    int VecCost = 0;\n    if (Opcode == Instruction::FCmp || Opcode == Instruction::ICmp ||\n        Opcode == Instruction::Select) {\n      VectorType *MaskTy = VectorType::get(Builder.getInt1Ty(), VL.size());\n      ScalarCost =\n          VecTy->getNumElements() *\n          TTI->getCmpSelInstrCost(Opcode, ScalarTy, Builder.getInt1Ty());\n      VecCost = TTI->getCmpSelInstrCost(Opcode, VecTy, MaskTy);\n    } else {\n      // Certain instructions can be cheaper to vectorize if they have\n      // a constant second vector operand.\n      TargetTransformInfo::OperandValueKind Op1VK =\n          TargetTransformInfo::OK_AnyValue;\n      TargetTransformInfo::OperandValueKind Op2VK =\n          TargetTransformInfo::OK_UniformConstantValue;\n      TargetTransformInfo::OperandValueProperties Op1VP =\n          TargetTransformInfo::OP_None;\n      TargetTransformInfo::OperandValueProperties Op2VP =\n          TargetTransformInfo::OP_None;\n\n      // If all operands are exactly the same ConstantInt then set the\n      // operand kind to OK_UniformConstantValue.\n      // If instead not all operands are constants, then set the operand kind\n      // to OK_AnyValue. If all operands are constants but not the \n     // same, then set the operand kind to OK_NonUniformConstantValue.\n      ConstantInt *CInt = nullptr;\n      for (unsigned i = 0; i < VL.size(); ++i) {\n        const Instruction *I = cast<Instruction>(VL[i]);\n        if (!isa<ConstantInt>(I->getOperand(1))) {\n          Op2VK = TargetTransformInfo::OK_AnyValue;\n          break;\n        }\n        if (i == 0) {\n          CInt = cast<ConstantInt>(I->getOperand(1));\n          continue;\n        }\n        if (Op2VK == TargetTransformInfo::OK_UniformConstantValue &&\n            CInt != cast<ConstantInt>(I->getOperand(1)))\n          Op2VK = TargetTransformInfo::OK_NonUniformConstantValue;\n      }\n      // FIXME: Currently cost of model modification for division by\n      // power of 2 is handled only for X86\\. Add support for other \n      // targets.\n      if (Op2VK == TargetTransformInfo::OK_UniformConstantValue && CInt &&\n          CInt->getValue().isPowerOf2())\n        Op2VP = TargetTransformInfo::OP_PowerOf2;\n\n      ScalarCost = VecTy->getNumElements() *\n TTI->getArithmeticInstrCost(Opcode, ScalarTy, Op1VK, Op2VK, Op1VP, Op2VP);\n VecCost = TTI->getArithmeticInstrCost(Opcode, VecTy, Op1VK, Op2VK, Op1VP, Op2VP);\n    }\n    return VecCost - ScalarCost;\n  }\n```", "```cpp\nValue *BoUpSLP::vectorizeTree() {\n  …\n  …\n  vectorizeTree(&VectorizableTree[0]);\n  …\n  …\n}\n```", "```cpp\n$ opt -S -basicaa -slp-vectorizer -mtriple=aarch64-unknown-linuxgnu -mcpu=cortex-a57 addsub.ll –debug\n\nFeatures:\nCPU:cortex-a57\n\nSLP: Analyzing blocks in addsub.\nSLP: Found 4 stores to vectorize.\nSLP: Analyzing a store chain of length 4.\nSLP: Analyzing a store chain of length 4\nSLP: Analyzing 4 stores at offset 0\nSLP:  bundle:   store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0)\nSLP:  initialize schedule region to   store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0)\nSLP:  extend schedule region end to   store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1)\nSLP:  extend schedule region end to   store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2)\nSLP:  extend schedule region end to   store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)\nSLP: try schedule bundle [  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0);  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1);  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2);  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)] in block entry\nSLP:       update deps of [  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0);  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1);  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2);  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)]\nSLP:       update deps of /   store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1)\nSLP:       update deps of /   store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2)\nSLP:       update deps of /   store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)\nSLP:     gets ready on update:   store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0)\nSLP: We are able to schedule this bundle.\nSLP: added a vector of stores.\nSLP:  bundle:   %add = add nsw i32 %1, %0\nSLP:  extend schedule region start to   %add = add nsw i32 %1, %0\nSLP: try schedule bundle [  %add = add nsw i32 %1, %0;  %add1 = add nsw i32 %3, %2;  %add2 = add nsw i32 %5, %4;  %add3 = add nsw i32 %7, %6] in block entry\nSLP:       update deps of [  %add = add nsw i32 %1, %0;  %add1 = add nsw i32 %3, %2;  %add2 = add nsw i32 %5, %4;  %add3 = add nsw i32 %7, %6]\nSLP:       update deps of /   %add1 = add nsw i32 %3, %2\nSLP:       update deps of /   %add2 = add nsw i32 %5, %4\nSLP:       update deps of /   %add3 = add nsw i32 %7, %6\nSLP:   schedule [  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0);  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1);  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2);  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)]\nSLP:    gets ready (def): [  %add = add nsw i32 %1, %0;  %add1 = add nsw i32 %3, %2;  %add2 = add nsw i32 %5, %4;  %add3 = add nsw i32 %7, %6]\nSLP: We are able to schedule this bundle.\nSLP: added a vector of bin op.\nSLP:  bundle:   %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0)\nSLP:  extend schedule region start to   %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0)\nSLP: try schedule bundle [  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0);  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1);  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2);  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)] in block entry\nSLP:       update deps of [  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0);  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1);  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2);  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)]\nSLP:       update deps of /   %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1)\nSLP:       update deps of /   %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2)\nSLP:       update deps of /   %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)\nSLP:   schedule [  %add = add nsw i32 %1, %0;  %add1 = add nsw i32 %3, %2;  %add2 = add nsw i32 %5, %4;  %add3 = add nsw i32 %7, %6]\nSLP:    gets ready (def): [  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0);  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1);  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2);  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)]\nSLP: We are able to schedule this bundle.\nSLP: added a vector of loads.\nSLP:  bundle:   %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0)\nSLP:  extend schedule region start to   %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0)\nSLP: try schedule bundle [  %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0);  %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1);  %4 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2);  %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3)] in block entry\nSLP:       update deps of [  %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0);  %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1);  %4 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2);  %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3)]\nSLP:       update deps of /   %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1)\nSLP:       update deps of /   %4 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2)\nSLP:       update deps of /   %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3)\nSLP:     gets ready on update:   %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0)\nSLP: We are able to schedule this bundle.\nSLP: added a vector of loads.\nSLP: Checking user:  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0).\nSLP:   Internal user will be removed:  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0).\nSLP: Checking user:  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1).\nSLP:   Internal user will be removed:  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1).\nSLP: Checking user:  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2).\nSLP:   Internal user will be removed:  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2).\nSLP: Checking user:  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3).\nSLP:   Internal user will be removed:  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3).\nSLP: Checking user:  %add = add nsw i32 %1, %0.\nSLP:   Internal user will be removed:  %add = add nsw i32 %1, %0.\nSLP: Checking user:  %add1 = add nsw i32 %3, %2.\nSLP:   Internal user will be removed:  %add1 = add nsw i32 %3, %2.\nSLP: Checking user:  %add2 = add nsw i32 %5, %4.\nSLP:   Internal user will be removed:  %add2 = add nsw i32 %5, %4.\nSLP: Checking user:  %add3 = add nsw i32 %7, %6.\nSLP:   Internal user will be removed:  %add3 = add nsw i32 %7, %6.\nSLP: Checking user:  %add = add nsw i32 %1, %0.\nSLP:   Internal user will be removed:  %add = add nsw i32 %1, %0.\nSLP: Checking user:  %add1 = add nsw i32 %3, %2.\nSLP:   Internal user will be removed:  %add1 = add nsw i32 %3, %2.\nSLP: Checking user:  %add2 = add nsw i32 %5, %4.\nSLP:   Internal user will be removed:  %add2 = add nsw i32 %5, %4.\nSLP: Checking user:  %add3 = add nsw i32 %7, %6.\nSLP:   Internal user will be removed:  %add3 = add nsw i32 %7, %6.\nSLP: Calculating cost for tree of size 4.\nSLP: Adding cost -3 for bundle that starts with   store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0) .\nSLP: Adding cost -3 for bundle that starts with   %add = add nsw i32 %1, %0 .\nSLP: Adding cost -3 for bundle that starts with   %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0) .\nSLP: Adding cost -3 for bundle that starts with   %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0) .\nSLP: #LV: 0, Looking at   %add = add nsw i32 %1, %0\nSLP: #LV: 1 add, Looking at   %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0)\nSLP: #LV: 2  , Looking at   %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0)\nSLP: SpillCost=0\nSLP: Total Cost -12.\nSLP: Found cost=-12 for VF=4\nSLP: Decided to vectorize cost=-12\nSLP: schedule block entry\nSLP:    initially in ready list:   store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0)\nSLP:   schedule [  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0);  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1);  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2);  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3)]\nSLP:    gets ready (def): [  %add = add nsw i32 %1, %0;  %add1 = add nsw i32 %3, %2;  %add2 = add nsw i32 %5, %4;  %add3 = add nsw i32 %7, %6]\nSLP:   schedule [  %add = add nsw i32 %1, %0;  %add1 = add nsw i32 %3, %2;  %add2 = add nsw i32 %5, %4;  %add3 = add nsw i32 %7, %6]\nSLP:    gets ready (def): [  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0);  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1);  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2);  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)]\nSLP:    gets ready (def): [  %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0);  %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1);  %4 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2);  %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3)]\nSLP:   schedule [  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0);  %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1);  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2);  %4 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3)]\nSLP:   schedule [  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0);  %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1);  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2);  %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3)]\nSLP: Extracting 0 values .\nSLP:   Erasing scalar:  store i32 %add, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 0).\nSLP:   Erasing scalar:  store i32 %add1, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 1).\nSLP:   Erasing scalar:  store i32 %add2, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 2).\nSLP:   Erasing scalar:  store i32 %add3, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @a, i32 0, i32 3).\nSLP:   Erasing scalar:  %add = add nsw i32 %8, %3.\nSLP:   Erasing scalar:  %add1 = add nsw i32 %7, %2.\nSLP:   Erasing scalar:  %add2 = add nsw i32 %6, %1.\nSLP:   Erasing scalar:  %add3 = add nsw i32 %5, %0.\nSLP:   Erasing scalar:  %8 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 0).\nSLP:   Erasing scalar:  %7 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 1).\nSLP:   Erasing scalar:  %6 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 2).\nSLP:   Erasing scalar:  %5 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @c, i32 0, i32 3).\nSLP:   Erasing scalar:  %3 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 0).\nSLP:   Erasing scalar:  %2 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 1).\nSLP:   Erasing scalar:  %1 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 2).\nSLP:   Erasing scalar:  %0 = load i32, i32* getelementptr inbounds ([4 x i32], [4 x i32]* @b, i32 0, i32 3).\nSLP: Optimizing 0 gather sequences instructions.\nSLP: vectorized \"addsub\"\n\n```", "```cpp\n; ModuleID = 'addsub.ll'\ntarget triple = \"aarch64-unknown-linuxgnu\"\n\n@a = global [4 x i32] zeroinitializer, align 4\n@b = global [4 x i32] zeroinitializer, align 4\n@c = global [4 x i32] zeroinitializer, align 4\n\ndefine void @addsub()  {\nentry:\n %0 = load <4 x i32>, <4 x i32>* bitcast ([4 x i32]* @b to <4 x i32>*), align 4\n %1 = load <4 x i32>, <4 x i32>* bitcast ([4 x i32]* @c to <4 x i32>*), align 4\n %2 = add nsw <4 x i32> %1, %0\n store <4 x i32> %2, <4 x i32>* bitcast ([4 x i32]* @a to <4 x i32>*), align 4\n ret void\n}\n\n```"]