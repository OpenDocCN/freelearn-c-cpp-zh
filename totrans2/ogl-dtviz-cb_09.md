# Chapter 9. Augmented Reality-based Visualization on Mobile or Wearable Platforms

In this chapter, we will cover the following topics:

*   Getting started I: Setting up OpenCV on Android
*   Getting started II: Accessing the camera live feed using OpenCV
*   Displaying real-time video processing with texture mapping
*   Augmented reality-based data visualization over real-world scenes

# Introduction

The field of digital graphics has traditionally been living within its own virtual world since computers were invented. Often, computer-generated content has no awareness of the user and how the information is relevant to the user in the real world. The application is always simply waiting for a user command such as the mouse or keyboard input. One major limiting factor in the early design of computer applications is that computers are typically sitting on a desk in an office or in a home environment. The lack of mobility and the inability to interact with its environment or user ultimately limited the development of real-world interactive visualization applications.

Today, with the evolution of mobile computing, we have redefined many of our daily interactions with the world—for example, through applications that enable navigation with GPS using a mobile phone. However, instead of enabling users to seamlessly interact with the world, mobile devices still draw users away from the real world. In particular, as in previous generations of desktop computing, users are still required to look away from the real world into a virtual world (in many cases, just a tiny mobile screen).

The notion of **Augmented Reality** (**AR**) is a step towards reconnecting the user with the real world through the fusion of the virtual world (generated by the computer) with the real world. This is distinctly different from virtual reality, in which the user is immersed into the virtual world and detached from the real world. For example, a typical embodiment of AR involves the use of a video see-through display in which virtual content (such as a computer-generated map) is combined with a real-world scene (captured continuously with a built-in camera). Now, the user is engaged with the real world—a step closer to a truly human-centric application.

Ultimately, the emergence of AR-enabled wearable computing devices (such as Meta's AR eyeglasses, which features the world's first holographic interface with 3D gesture detection and 3D stereoscopic display) will create a new era of computing that will greatly revolutionize the way humans interact with computers. Developers interested in data visualization now have another set of tools that are significantly more human-centric and intuitive. Such a design, needless to say, truly connects human, machine, and the real world together. Having information directly overlaid onto the real world (for example, by overlaying a virtual guidance map for navigation) is so much more powerful and meaningful.

This final chapter introduces the fundamental building blocks for creating your first AR-based application on a commodity Android-based mobile device: OpenCV for computer vision, OpenGL for graphics rendering, as well as Android's sensor framework for interaction. With these tools, the graphics rendering capability that used to only exist in Hollywood movie production can now be made available at everyone's fingertips. While we will only focus on the use of an Android-based mobile device in this chapter, the conceptual framework for AR-based data visualization introduced in this chapter can be similarly extended to state-of-the-art wearable computing platforms, such as Meta's AR eyeglasses.

# Getting started I: Setting up OpenCV on Android

In this section, we will outline the steps to set up the OpenCV library on the Android platform, which is needed to enable access to the live camera stream central to any Augmented Reality applications.

## Getting ready

We assume that the Android SDK and NDK are configured exactly as discussed in [Chapter 7](ch07.html "Chapter 7. An Introduction to Real-time Graphics Rendering on a Mobile Platform using OpenGL ES 3.0"), *An Introduction to Real-time Graphics Rendering on a Mobile Platform Using OpenGL ES 3.0*. Here, we add in the support of OpenCV for Android. We will import and integrate the OpenCV library into our existing code structure from the previous chapter.

## How to do it...

Here, we describe the major steps for setting up the OpenCV library, mainly path setup and pre-configuration of the Java SDK project setup:

1.  Download the OpenCV for Android SDK package, Version 3.0.0 (`OpenCV-3.0.0-android-sdk-1.zip`) at [http://sourceforge.net/projects/opencvlibrary/files/opencv-android/3.0.0/OpenCV-3.0.0-android-sdk-1.zip](http://sourceforge.net/projects/opencvlibrary/files/opencv-android/3.0.0/OpenCV-3.0.0-android-sdk-1.zip).
2.  Move the package (`OpenCV-3.0.0-android-sdk-1.zip`) to the `3rd_party/android` folder created in [Chapter 7](ch07.html "Chapter 7. An Introduction to Real-time Graphics Rendering on a Mobile Platform using OpenGL ES 3.0"), *An Introduction to Real-time Graphics Rendering on a Mobile Platform Using OpenGL ES 3.0*.
3.  Unzip the package with the following commands

    [PRE0]

4.  Then in the project folder (for example `ch9/code/opencv_demo_1`), run the following script to initialize the project for Android. Note that the `3rd_party` folder is assumed to be in the same top-level directory as in previous chapters:

    [PRE1]

5.  Finally, include the OpenCV path in the build script `jni/Android.mk`.

    [PRE2]

Now, the project is linked to the OpenCV library, both from the Java side as well as from the native side.

Next we must install the OpenCV Manager on the mobile phone. The OpenCV Manager allows us to create applications without statically linking all the required libraries, and it is recommended. To install the package, we can execute the following `adb` command from the same project folder (`ch9/code/opencv_demo_1`). Again, note the relative location of the `3rd_party` folder. You can also execute this command within the Android SDK folder and modify the relative path of the `3rd_party` folder accordingly.

[PRE3]

After we have successfully completed the setup, we are ready to create our first OpenCV Android application on the phone.

## See also

Windows users should consult the following tutorials on Android development with OpenCV for setup instructions: [http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/android_dev_intro.html](http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/android_dev_intro.html) and [http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/dev_with_OCV_on_Android.html#native-c](http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/dev_with_OCV_on_Android.html#native-c).

For further information on using OpenCV in an Android application, consult the online documentation at [http://opencv.org/platforms/android.html](http://opencv.org/platforms/android.html).

# Getting started II: Accessing the camera live feed using OpenCV

Next we need to demonstrate how to integrate OpenCV into our Android-based development framework. The following block diagram illustrates the core functions and relationship among the classes that will be implemented in this chapter (only the functions or classes relevant to the introduction of OpenCV will be discussed in this section):

![Getting started II: Accessing the camera live feed using OpenCV](img/9727OS_09_01.jpg)

In particular, we will demonstrate how to extract an image frame from the camera video stream for further image processing steps. The OpenCV library provides camera support for accessing the live camera feed (the raw data buffer of the video data stream) as well as controlling the camera parameters. This feature allows us to get the raw frame data from the live preview camera with optimal resolution, frame rate, and image format.

## Getting ready

The demos in this chapter build upon the basic structure introduced in the sample code of [Chapter 8](ch08.html "Chapter 8. Interactive Real-time Data Visualization on Mobile Devices"), *Interactive Real-time Data Visualization on Mobile Devices* which utilizes the multi-touch interface and motion sensor inputs to enable interactive real-time data visualization on mobile devices. The major changes that are made to support OpenCV will be highlighted. For the complete code, download the code package from the Packt Publishing website.

## How to do it...

First, we will highlight the changes to the Java source files required to enable the use of OpenCV and the OpenCV camera module. Rename `GL3JNIActivity.java` (`src/com/android/gl3jni/`) as `GL3OpenCVDemo.java` and modify the code as follows:

1.  Include the packages for the OpenCV library:

    [PRE4]

2.  Add the `CvCameraViewListener2` interface to the `GL3OpenCVDemo` class:

    [PRE5]

3.  Create the variables to handle the camera view:

    [PRE6]

4.  Implement the `BaseLoaderCallback` function for `OpenCVLoader`:

    [PRE7]

5.  Implement the OpenCV camera callback functions and pass the image data to the JNI C/C++ side for processing and rendering:

    [PRE8]

6.  Initialize the camera in the `onCreate` function, upon starting the application:

    [PRE9]

7.  Load the OpenCV library using the asynchronized initialization function called `initAsync` from the `OpenCVLoader` class. This event is captured by the `BaseLoaderCallback mLoaderCallback` function defined earlier:

    [PRE10]

8.  Finally, handle the `onPause` event, which pauses the camera preview when the application is no longer running in the foreground:

    [PRE11]

9.  Now inside `GL3JNILib.java` (`src/com/android/gl3jni/`), add the native `setImage` function to pass the camera raw data. The entire source file is shown here, given its simplicity:

    [PRE12]

10.  Finally, the source code inside `GL3JNIView.java` is virtually identical except that we offer the option to reset the rotation data and call the `setZOrderOnTop` function to ensure that the OpenGL layer is on top of the Java layer:

    [PRE13]

11.  Finally, define the JNI prototypes to interface with the Java side in the `main.cpp` file that connects all components.

    [PRE14]

12.  To access the device camera, the following elements must be declared in the `AndroidManifest.xml` file to ensure we have the permission to control the camera. In our current example, we request access to the front and back cameras with autofocus support.

    [PRE15]

At this point, we have developed a full demo application that supports OpenCV and real-time camera feed. In the next section, we will connect the camera raw data stream to the OpenGL layer and perform real-time feature extraction with OpenCV in C/C++.

## How it works...

On the Java side, we have integrated the OpenCV Manager (installed previously) to handle the dynamic loading of all libraries at runtime. Upon starting the application, we must call the `OpenCVLoader.initAsync` function; all OpenCV-related JNI libraries must only be called after the OpenCV libraries are successfully loaded. To synchronize these actions in our case, the `callback` function (`BaseLoaderCallback`) checks the status of the initialization of OpenCV, and we proceed with the `System.loadLibrary` function to initialize OpenGL and other components only if the OpenCV loader returns success (`LoaderCallbackInterface.SUCCESS`). For simplicity, we did not include the implementation to handle library loading exceptions in this demo.

On the sensor side, we have also changed the implementation for the `SensorManager` function to return the rotation matrix instead of the Euler angles to avoid the issue of Gimbal lock (refer to [http://en.wikipedia.org/wiki/Gimbal_lock](http://en.wikipedia.org/wiki/Gimbal_lock)). We also remapped the coordinates (from device orientation to OpenGL camera orientation) using the `SensorManager.remapCoordinateSystem` function. Then the rotation matrix is directed to the OpenGL side with the native calls `GL3JNILib.setRotMatrix`. Also, we can allow the user to reset the default orientation by touching the screen. This is achieved by calling the `GL3JNILib.resetRotDataOffset` function, which resets the rotation matrix with the touch event.

Additionally, we have added the `OpenCV CvCameraViewListener2` interface and `CameraBridgeViewBase` class to enable native camera access. The `CameraBridgeViewBase` class is a basic class that handles the interaction with the Android Camera class and OpenCV library. It is responsible for controlling the camera, such as resolution, and processing the frame, such as changing the image format. The client implements `CvCameraViewListener` to receive callback events. In the current implementation, we manually set the resolution as 1280 x 720\. However, we can increase or decrease the resolution based on the application needs. Finally, the color frame buffers are returned in RGBA format, and the data stream will be transferred to the JNI C/C++ side and rendered using texture mapping.

# Displaying real-time video using texture mapping

Today, most mobile phones are equipped with cameras that are capable of capturing high-quality photos as well as videos. For example, the Samsung Galaxy Note 4 is equipped with a 16MP back-facing camera as well as a 3.7MP front-facing camera for video conferencing applications. With these built-in cameras, we can record high-definition videos with exceptional image quality in both outdoor and indoor environments. The ubiquity of these imaging sensors, as well as the increasing computational capability of mobile processors, now enable us to develop much more interactive applications such as real-time tracking of objects or faces.

By combining OpenGL with the OpenCV library, we can create interactive applications that perform real-time video processing of the real world to register and augment 3D virtual information onto real-world objects. Since both libraries are hardware-accelerated (GPU and CPU optimized), it is important that we explore the use of these libraries to obtain real-time performance.

In the previous section, we introduced the framework that provides access to the live camera feed. Here, we will create a full demo that displays real-time video using OpenGL-based texture mapping techniques (similar to those introduced in [Chapter 4](ch04.html "Chapter 4. Rendering 2D Images and Videos with Texture Mapping"), *Rendering 2D Images and Videos with Texture Mapping* to [Chapter 6](ch06.html "Chapter 6. Rendering Stereoscopic 3D Models using OpenGL"), *Rendering Stereoscopic 3D Models using OpenGL*, except we will deploy OpenGL ES for mobile platforms), and processes the video stream to perform corner detection using OpenCV. To help readers understand the additional code needed to finalize the demo, here is an overview diagram of the implementation:

![Displaying real-time video using texture mapping](img/9727OS_09_02.jpg)

## Getting ready

This demo requires the completion of all the *Getting ready* steps to enable the capture of the real-time video stream using OpenCV on an Android device. The implementation of the shader program and texture mapping code is based on the demos from [Chapter 8](ch08.html "Chapter 8. Interactive Real-time Data Visualization on Mobile Devices"), *Interactive Real-time Data Visualization on Mobile Devices*.

## How to do it...

On the native code side, create two new files called `VideoRenderer.hpp` and `VideoRenderer.cpp`. These files contain the implementation to render the video using texture mapping. Also, we will import the `Texture.cpp` and `Texture.hpp` files from the previous chapter to handle texture creation.

Inside the `VideoRenderer.hpp` file, define the `VideoRenderer` class as follows (the details of each function will be discussed next):

[PRE16]

Inside the `VideoRenderer.cpp` file, we implement each of the three key member functions (`setup`, `initTexture`, and `render`). Here is the complete implementation:

1.  Include the `VideoRenderer.hpp` header file, define functions to print debug messages, and define the constructor and destructor:

    [PRE17]

2.  Define the vertex and fragment shaders as well as associated configuration steps (similar to [Chapter 8](ch08.html "Chapter 8. Interactive Real-time Data Visualization on Mobile Devices"), *Interactive Real-time Data Visualization on Mobile Devices*):

    [PRE18]

3.  Initialize and bind the texture:

    [PRE19]

4.  Render the camera feed on the screen with texture mapping:

    [PRE20]

To further enhance the readability of the code, we encapsulate the handling of the shader program and texture mapping inside `Shader.hpp` (`Shader.cpp`) and `Texture.hpp` (`Texture.cpp`), respectively. We will only show the header files here for completeness and refer readers to the code package on the Packt Publishing website for the detailed implementation of each function.

Here is the `Shader.hpp` file:

[PRE21]

The `Texture.hpp` file should read:

[PRE22]

Finally, we integrate everything inside the `main.cpp` file with the following steps:

1.  Include all headers. In particular, include `pthread.h` to handle synchronization and OpenCV libraries for image processing.

    [PRE23]

2.  Define the `VideoRenderer` and `Shader` objects, as well as the `pthread_mutex_t` lock variable to handle synchronization for data copying using a mutex lock.

    [PRE24]

3.  Set up the `VideoRenderer` object in the `setupGraphics` function and initialize the texture.

    [PRE25]

4.  Create a `processFrame` helper function to handle feature extraction with the OpenCV `goodFeaturesToTrack` function. The function also draws the result directly on the frame for visualization.

    [PRE26]

5.  Implement frame copying with mutex lock synchronization (to avoid frame corruption due to shared memory and race condition) in the `renderFrame` function. Process the frame with the OpenCV library and render the result using OpenGL texture-mapping techniques.

    [PRE27]

6.  Define the JNI prototypes and implement the `setImage` function, which receives the raw camera image data from the Java side using a mutex lock to ensure data copying is protected. Also, implement the `toggleFeatures` function to turn feature tracking on and off upon touching the screen.

    [PRE28]

    ![How to do it...](img/9727OS_09_03.jpg)

The resulting image is a post-processed frame from OpenCV. In addition to displaying the raw video frame, we demonstrate that our implementation can easily be extended to support real-time video processing with OpenCV. The `processFrame` function uses the OpenCV `goodFeaturesToTrack` corner detection function and we overlay all corners extracted from the scene on the image.

Image features are the fundamental elements for many tracking algorithms such as **Simultaneous localization and Mapping** (**SLAM**) as well as recognition algorithms such as image-based matching. For example, with the SLAM algorithm, we can construct a map of the environment and, at the same time, keep track of the position of the device in space. Such techniques are particularly useful in AR applications as we always need to align the virtual world with the real world. Next, we can see a feature extraction algorithm (corner detection) running in real-time on a mobile phone.

![How to do it...](img/9727OS_09_04.jpg)

## How it works...

The `VideoRenderer` class has two primary functions:

*   Creating the shader program that handles texture mapping (`Shader.cpp` and `Texture.cpp`).
*   Updating the texture memory with the OpenCV raw camera frame. Each time a new frame is retrieved from OpenCV, we call the render function, which updates the texture memory and also draws the frame on the screen.

The `main.cpp` file connects all the components of the implementation, and encapsulates all the logics for the interaction. It interfaces with the Java side (for example, `setImage`) and we offload all computationally intensive tasks to the C++ native side. For example, the `processFrame` function handles the OpenCV video processing pipeline, and we can efficiently handle memory I/O and parallelization. On the other hand, the `VideoRenderer` class accelerates rendering with OpenGL for real-time performance on the mobile platform.

One may notice that the implementations of OpenGL and OpenCV on Android are mostly identical to the desktop version. That's the key reason why we employ such cross-platform languages as we can easily extend our code to any future platform with minimal effort.

## See also

On a mobile platform, computational resources are particularly limited and thus it is important to optimize the use of all available hardware resources. With OpenGL-based hardware acceleration, we can reduce most of our overhead in rendering graphics in 2D and 3D on the graphics processor. In the near future, especially with the emergence of mobile processors supporting GPGPU (for example, Nvidia's K1 mobile processor), we will enable more parallelized processing for computer vision algorithms and offer real-time performance for many applications on a mobile device. For example, Nvidia now officially supports CUDA for all its upcoming mobile processors, so we will see many more real-time image processing, machine learning (such as deep learning algorithms), and high-performance graphics emerging on the mobile platform. See the following website for more information: [https://developer.nvidia.com/embedded-computing](https://developer.nvidia.com/embedded-computing).

# Augmented reality-based data visualization over real-world scenes

In our ultimate demo, we will introduce the basic framework for AR-based data visualization by overlaying 3D data on real-world objects and scenes. We apply the same GPU-accelerated simulation model and register it to the world with a sensor-based tracking approach. The following diagram illustrates the final architecture of the implementation in this chapter:

![Augmented reality-based data visualization over real-world scenes](img/9727OS_09_08.jpg)

## Getting ready

This final demo integrates together all the concepts previously introduced in this chapter and requires the capture (and possibly processing) of a real-time video stream using OpenCV on an Android-based phone. To reduce the complexity of the code, we have created the Augmented Reality layer (`AROverlayRenderer`) and we can improve the registration, alignment, and calibration of the layer with more advanced algorithms in the future.

## How to do it...

Let's define a new class called `AROverlayRenderer` inside the `AROverlayRenderer.hpp` file:

[PRE29]

Now implement the `AROverlayRenderer` member functions inside the `AROverlayRenderer.cpp` file:

1.  Include the `AROverlayRenderer.hpp` header file and define functions to print messages as well as the constructor and destructor:

    [PRE30]

2.  Initialize the grid pattern for the simulation:

    [PRE31]

3.  Set up the shader program to overlay graphics:

    [PRE32]

4.  Create helper functions to set the scale, screen size, and rotation variables from the touch interface:

    [PRE33]

5.  Compute the projection and view matrices based on the camera parameters:

    [PRE34]

6.  Render the graphics on the screen:

    [PRE35]

7.  Finally, we only need to make minor modifications to the `main.cpp` file used in the previous demo to enable the AR overlay on top of the real-time video stream (real-world scene). Only the relevant code snippets that highlight the required modifications are shown here (download the complete code from the Packt Publishing website):

    [PRE36]

With this framework, one can overlay virtually any dataset on different real-world objects or surfaces and enable truly interactive applications, using the built-in sensors and gesture interface on mobile devices and emerging state-of-the-art wearable AR eyeglasses. Following are the results demonstrating a real-time, interactive, AR-based visualization of a 3-D dataset (in this case, a Gaussian distribution) overlaid on real-world scenes:

![How to do it...](img/9727OS_09_05.jpg)

## How it works...

The key element for enabling an AR application is the ability is overlay information onto the real world. The `AROverlayRenderer` class implements the core functions essential to all AR applications. First, we create a virtual camera that matches the parameters of the actual camera on the mobile phone. Parameters such as the **field of view** (**FOV**) and aspect ratio of the camera are currently hard-coded, but we can easily modify them in the `computeProjectionMatrices` function. Then, to perform the registration between the real world and virtual world, we control the orientation of the virtual camera based on the orientation of the device. The orientation values are fed through the rotation matrix passed from the Java side (the `setRotMatrix` function) and we apply this directly to the OpenGL camera view matrix (`view_matrix`). Also, we use the multi-touch interface of the mobile phone to reset the default orientation of the rotation matrix. This is achieved by storing the rotational matrix value upon the touch event (the `resetRotDataOffset` function) and we apply the inverse to the rotational matrix to the view matrix (this is equivalent to rotating the camera in the opposite direction).

In terms of user interaction, we have enabled the pinch and drag option to support dynamic interaction with the virtual object. Upon the pinch event, we take the scale factor and we position the rendered object at a farther distance by applying the `glm::translate` function on the `model_matrix` variable. In addition, we rotate the virtual object by capturing the dragging action from the Java side (the `setDxDy` function). The user can control the orientation of the virtual object by dragging a finger across the screen. Together, these multi-touch gestures enable a highly interactive application interface that allows users to change the perspective of the rendered object intuitively.

Due to the underlying complexity of the calibration process, we will not cover these details here. However, advanced users may consult the following website for a more in-depth discussion: [http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html](http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html).

Also, the current registration process is purely based on the IMU, and it does not support translation (that is, the virtual object does not move exactly with the real world). To address this, we can apply various image-processing techniques such as mean shift tracking, feature-based tracking, and marker-based tracking to recover the full 6 DOF (degree of freedom) model of the camera. SLAM, for example, is a great candidate to recover the 6 DOF camera model, but its detailed implementation is beyond the scope of this chapter.

## See also

Indeed, in this chapter, we have only covered the fundamentals of AR. The field of AR is becoming an increasingly hot topic in both academia and industry. If you are interested in implementing AR data visualization applications on the latest wearable computing platforms (such as the one provided by Meta that features 3D gesture input and 3D stereoscopic output), visit the following websites:

*   [https://www.getameta.com/](https://www.getameta.com/)
*   [http://www.eyetap.org/publications/](http://www.eyetap.org/publications/)

For further technical details on AR eyeglasses, please consult the following publications:

*   Raymond Lo, Alexander Chen, Valmiki Rampersad, Jason Huang, Han Wu, Steve Mann (2013). "Augmediated reality system based on 3D camera selfgesture sensing," IEEE International Symposium on Technology and Society (ISTAS) 2013, pp. 20-31.
*   Raymond Lo, Valmiki Rampersad, Jason Huang, Steve Mann (2013). "Three Dimensional High Dynamic Range Veillance for 3D Range-Sensing Cameras," IEEE International Symposium on Technology and Society (ISTAS) 2013, pp. 255-265.
*   Raymond Chun Hing Lo, Steve Mann, Jason Huang, Valmiki Rampersad, and Tao Ai. 2012\. "High Dynamic Range (HDR) Video Image Processing For Digital Glass." In Proceedings of the 20th ACM international conference on Multimedia (MM '12). ACM, New York, NY, USA, pp. 1477-1480.
*   Steve Mann, Raymond Lo, Jason Huang, Valmiki Rampersad, Ryan Janzen, Tao Ai (2012). "HDRchitecture: Real-Time stereoscopic HDR Imaging for Extreme Dynamic Range," In ACM SIGGRAPH 2012 Emerging Technologies (SIGGRAPH '12).