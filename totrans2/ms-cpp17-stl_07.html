<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">Concurrency</h1>
                
            
            
                
<p class="calibre2">In the previous chapter, we discussed how <kbd class="calibre12">std::shared_ptr&lt;T&gt;</kbd> implements reference-counting memory management, so that an object's lifetime can be cooperatively controlled by stakeholders who might be otherwise unaware of each other--for example, the stakeholders might live in different threads. In C++ before C++11, this would have posed a stumbling block right away: if one stakeholder decrements the reference count while, simultaneously, a stakeholder in another thread is in the process of decrementing the reference count, then don't we have a <em class="calibre22">data race</em> and therefore undefined behavior?</p>
<p class="calibre2">In C++ before C++11, the answer was generally "yes." (In fact, C++ before C++11 didn't have a standard concept of "threads," so another reasonable answer might have been that the question itself was irrelevant.) In C++ as of 2011, though, we have a standard memory model that accounts for concepts such as "threading" and "thread-safety," and so the question is meaningful and the answer is categorically "No!" Accesses to the reference count of <kbd class="calibre12">std::shared_ptr</kbd> are guaranteed not to race with each other; and in this chapter we'll show you how you can implement similarly thread-safe constructions using the tools the standard library provides.</p>
<p class="calibre2">In this chapter we'll cover the following topics:</p>
<ul class="calibre14">
<li class="calibre15">The difference between <kbd class="calibre12">volatile T</kbd> and <kbd class="calibre12">std::atomic&lt;T&gt;</kbd></li>
<li class="calibre15"><kbd class="calibre12">std::mutex</kbd>, <kbd class="calibre12">std::lock_guard&lt;M&gt;</kbd>, and <kbd class="calibre12">std::unique_lock&lt;M&gt;</kbd></li>
<li class="calibre15"><kbd class="calibre12">std::recursive_mutex</kbd> and <kbd class="calibre12">std::shared_mutex</kbd></li>
<li class="calibre15"><kbd class="calibre12">std::condition_variable</kbd> and <kbd class="calibre12">std::condition_variable_any</kbd></li>
<li class="calibre15"><kbd class="calibre12">std::promise&lt;T&gt;</kbd> and <kbd class="calibre12">std::future&lt;T&gt;</kbd></li>
<li class="calibre15"><kbd class="calibre12">std::thread</kbd> and <kbd class="calibre12">std::async</kbd></li>
<li class="calibre15">The dangers of <kbd class="calibre12">std::async</kbd>, and how to build a thread pool to replace it</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">The problem with volatile</h1>
                
            
            
                
<p class="calibre2">If you've been living under a rock for the past ten years--or if you're coming from old-style C--you might ask, "What's wrong with the <kbd class="calibre12">volatile</kbd> keyword? When I want to make sure some access really hits memory, I make sure it's done <kbd class="calibre12">volatile</kbd>."</p>
<p class="calibre2">The official semantics of <kbd class="calibre12">volatile</kbd> are that volatile accesses are evaluated strictly according to the rules of the abstract machine, which means, more or less, that the compiler is not allowed to reorder them or combine multiple accesses into one. For example, the compiler cannot assume that the value of <kbd class="calibre12">x</kbd> remains the same between these two loads; it must generate machine code that performs two loads, one on either side of the store to <kbd class="calibre12">y</kbd>:</p>
<pre class="calibre23">    volatile int&amp; x = memory_mapped_register_x();<br class="title-page-name"/>    volatile bool&amp; y = memory_mapped_register_y();<br class="title-page-name"/>    int stack;<br class="title-page-name"/><br class="title-page-name"/>    stack = x; // load<br class="title-page-name"/>    y = true; // store <br class="title-page-name"/>    stack += x; // load</pre>
<p class="calibre2">If <kbd class="calibre12">x</kbd> were not volatile, then the compiler would be perfectly within its rights to reorder the code like this:</p>
<pre class="calibre23">    stack = 2*x; // load<br class="title-page-name"/>    y = true; // store</pre>
<p class="calibre2">The compiler could do this (if <kbd class="calibre12">x</kbd> weren't volatile) because the write to a <kbd class="calibre12">bool</kbd> variable <kbd class="calibre12">y</kbd> cannot possibly affect the value of the <kbd class="calibre12">int</kbd> variable <kbd class="calibre12">x</kbd>. However, since <kbd class="calibre12">x</kbd> is volatile, this reordering optimization is not allowed.</p>
<pre>x</kbd> is a view onto some hardware buffer, and the store to memory location <kbd class="calibre12">y</kbd> is the signal for the hardware to load the next four bytes of data into the <kbd class="calibre12">x</kbd> register. It might help to view the situation as an operator overloading, but in hardware. And if "operator overloading, but in hardware" sounds crazy to you, then you probably have zero reason to use <kbd class="calibre12">volatile</kbd> in your programs!</pre>
<p class="calibre2">So that's what <kbd class="calibre12">volatile</kbd> does. But why can't we use <kbd class="calibre12">volatile</kbd> to make our programs thread-safe? In essence, the problem with <kbd class="calibre12">volatile</kbd> is that it's too old. The keyword has been in C++ ever since we split off from C, and it was in C since the original standard in 1989. Back then, there was very little concern about multithreading, and compilers were simpler, which meant that some potentially problematic optimizations had not yet been dreamt of. By the late 1990s and early 2000s, when C++'s lack of a thread-aware memory model started to become a real concern, it was too late to make <kbd class="calibre12">volatile</kbd> do everything that was required for thread-safe memory access, because every vendor had already implemented <kbd class="calibre12">volatile</kbd> and documented exactly what it did. Changing the rules at that point would have broken a lot of people's code--and the code that would have broken would have been low-level hardware interface code, the kind of code you really don't want bugs in.</p>
<p class="calibre2">Here are a couple of examples of the kind of guarantee we need in order to get thread-safe memory accesses:</p>
<pre class="calibre23">    // Global variables:<br class="title-page-name"/>    int64_t x = 0;<br class="title-page-name"/>    bool y = false;<br class="title-page-name"/><br class="title-page-name"/>    void thread_A() {<br class="title-page-name"/>      x = 0x42'00000042;<br class="title-page-name"/>      y = true;<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    void thread_B() {<br class="title-page-name"/>      if (x) {<br class="title-page-name"/>        assert(x == 0x42'00000042);<br class="title-page-name"/>      }<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    void thread_C() {<br class="title-page-name"/>      if (y) {<br class="title-page-name"/>        assert(x == 0x42'00000042);<br class="title-page-name"/>      }<br class="title-page-name"/>    }</pre>
<p class="calibre2">Suppose <kbd class="calibre12">thread_A</kbd>, <kbd class="calibre12">thread_B</kbd>, and <kbd class="calibre12">thread_C</kbd> are all running concurrently in different threads. How could this code go wrong? Well, <kbd class="calibre12">thread_B</kbd> is checking that <kbd class="calibre12">x</kbd> always holds exactly either zero or <kbd class="calibre12">0x42'00000042</kbd>. On a 32-bit computer, however, it may not be possible to make that guarantee; the compiler might have to implement the assignment in <kbd class="calibre12">thread_A</kbd> as a pair of assignments "set the upper half of <kbd class="calibre12">x</kbd> to 42; set the lower half of <kbd class="calibre12">x</kbd> to 42." If the test in <kbd class="calibre12">thread_B</kbd> happens to run at the right (wrong) time, it could end up seeing <kbd class="calibre12">x</kbd> as <kbd class="calibre12">0x42'00000000</kbd>. Making <kbd class="calibre12">x</kbd> volatile will not help with this one; in fact, nothing will, because our 32-bit hardware simply doesn't support this operation! It would be nice for the compiler to detect that we're trying to get an atomic 64-bit assignment, and give us a compile-time error if it knows our goal is impossible. In other words, <kbd class="calibre12">volatile</kbd> accesses are not guaranteed to be <em class="calibre22">atomic</em>. In practice, they often are atomic--and so are non-volatile accesses, but they aren't <em class="calibre22">guaranteed</em> to be, and sometimes you have to go down to the machine code level to figure out whether you're getting the code you expected. We'd like a way to guarantee that an access will be atomic (or if that's impossible, we'd like a compiler error).</p>
<p class="calibre2">Now consider <kbd class="calibre12">thread_C</kbd>. It's checking that <em class="calibre22">if</em> the value of <kbd class="calibre12">y</kbd> is visibly true, <em class="calibre22">then</em> the value of <kbd class="calibre12">x</kbd> must already be set to its final value. In other words, it's checking that the write to <kbd class="calibre12">x</kbd> "happened before" the write to <kbd class="calibre12">y</kbd>. This is definitely true from the point of view of <kbd class="calibre12">thread_A</kbd>, at least if <kbd class="calibre12">x</kbd> and <kbd class="calibre12">y</kbd> are both volatile, because we have seen that the compiler is not allowed to reorder volatile accesses. However, the same is not necessarily true from the point of view of <kbd class="calibre12">thread_C</kbd>! If <kbd class="calibre12">thread_C</kbd> is running on a different physical CPU, with its own data cache, then it may become aware of the updated values of <kbd class="calibre12">x</kbd> and <kbd class="calibre12">y</kbd> at different times, depending on when it refreshes their respective cache lines. We would like a way to say that when the compiler loads from <kbd class="calibre12">y</kbd>, it must also ensure that its entire cache is up-to-date--that it will never read a "stale" value for <kbd class="calibre12">x</kbd>. However, on some processor architectures, that requires special instructions, or additional memory-barrier logic. The compiler doesn't generate those instructions for "old-style" volatile accesses, because threading wasn't a concern when <kbd class="calibre12">volatile</kbd> was invented; and the compiler can't be <em class="calibre22">made</em> to generate those instructions for volatile accesses, because that would unnecessarily slow down or maybe even break, all the existing low-level code that uses old-style <kbd class="calibre12">volatile</kbd> for its old-style meaning. So we're left with the problem that even though volatile accesses happen in sequential order from the point of view of their own thread, they may well appear in a different order from the point of view of another thread. In other words, <kbd class="calibre12">volatile</kbd> accesses are not guaranteed to be <em class="calibre22">sequentially consistent</em>. We'd like a way to guarantee that an access will be sequentially consistent with respect to other accesses.</p>
<p class="calibre2">The solution to both of our problems was added to C++ in 2011. That solution is <kbd class="calibre12">std::atomic</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Using std::atomic&lt;T&gt; for thread-safe accesses</h1>
                
            
            
                
<p class="calibre2">In C++11 and later, the <kbd class="calibre12">&lt;atomic&gt;</kbd> header contains the definition of class template <kbd class="calibre12">std::atomic&lt;T&gt;</kbd>. There are two different ways you can think about <kbd class="calibre12">std::atomic</kbd>: you can think of it as a class template just like <kbd class="calibre12">std::vector</kbd>, with overloaded operators that just happen to implement thread-safe operations; or you can think of it as a magical built-in family of types whose names just happen to contain angle brackets. The latter way of thinking about it is actually pretty useful, because it suggests--correctly--that <kbd class="calibre12">std::atomic</kbd> is partly built into the compiler, and so the compiler will usually generate optimal code for atomic operations. The latter also suggests a way in which <kbd class="calibre12">atomic</kbd> is different from <kbd class="calibre12">vector</kbd>: with <kbd class="calibre12">std::vector&lt;T&gt;</kbd>, the <kbd class="calibre12">T</kbd> can be pretty much anything you like. With <kbd class="calibre12">std::atomic&lt;T&gt;</kbd>, the <kbd class="calibre12">T</kbd> is <em class="calibre22">can</em> be anything you like, but in practice it is a bad idea to use any <kbd class="calibre12">T</kbd> that doesn't belong to a small set of <em class="calibre22">atomic-friendly</em> types. More on this topic in a moment.</p>
<p class="calibre2">The <em class="calibre22">atomic-friendly</em> types are the integral types (at least, those no bigger than a machine register) and the pointer types. Generally speaking, on common platforms, you'll find that operations on <kbd class="calibre12">std::atomic</kbd> objects of these types will do exactly what you want:</p>
<pre class="calibre23">    // Global variables:<br class="title-page-name"/>    std::atomic&lt;int64_t&gt; x = 0;<br class="title-page-name"/>    std::atomic&lt;bool&gt; y = false;<br class="title-page-name"/><br class="title-page-name"/>    void thread_A() {<br class="title-page-name"/>      x = 0x42'00000042; // atomic!<br class="title-page-name"/>      y = true; // atomic!<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    void thread_B() {<br class="title-page-name"/>      if (x) {<br class="title-page-name"/>        // The assignment to x happens atomically.<br class="title-page-name"/>        assert(x == 0x42'00000042);<br class="title-page-name"/>      }<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    void thread_C() {<br class="title-page-name"/>      if (y) {<br class="title-page-name"/>        // The assignment to x "happens before" the<br class="title-page-name"/>        // assignment to y, even from another thread's<br class="title-page-name"/>        // point of view.<br class="title-page-name"/>        assert(x == 0x42'00000042);<br class="title-page-name"/>      }<br class="title-page-name"/>    }</pre>
<p class="calibre2"><kbd class="calibre12">std::atomic&lt;T&gt;</kbd> overloads its assignment operator to perform atomic, thread-safe assignment; and likewise its <kbd class="calibre12">++</kbd>, <kbd class="calibre12">--</kbd>, <kbd class="calibre12">+=</kbd>, and <kbd class="calibre12">-=</kbd> operators; and for integral types, also the <kbd class="calibre12">&amp;=</kbd>, <kbd class="calibre12">|=</kbd>, and <kbd class="calibre12">^=</kbd> operators.</p>
<p class="calibre2">It's important to bear in mind the difference between <em class="calibre22">objects</em> of type <kbd class="calibre12">std::atomic&lt;T&gt;</kbd> (which conceptually live "out there" in memory) and short-lived <em class="calibre22">values</em> of type <kbd class="calibre12">T</kbd> (which conceptually live "right here," close at hand; for example, in CPU registers). So, for example, there is no copy-assignment operator for <kbd class="calibre12">std::atomic&lt;int&gt;</kbd>:</p>
<pre class="calibre23">    std::atomic&lt;int&gt; a, b;<br class="title-page-name"/>    a = b; // DOES NOT COMPILE!</pre>
<p class="calibre2">There's no copy-assignment operator (nor move-assignment operator) because it wouldn't have a clear meaning: Does the programmer mean that the computer should load the value of <kbd class="calibre12">b</kbd> into a register and then store the value of that register into <kbd class="calibre12">a</kbd>? That sounds like two different atomic operations, not one operation! Or the programmer might mean that the computer should copy the value from <kbd class="calibre12">b</kbd> to <kbd class="calibre12">a</kbd> in a single atomic operation; but that involves touching two different memory locations in a single atomic operation, which is not within the capabilities of most computer hardware. So instead, C++ requires that you write out explicitly what you mean: a single atomic load from object <kbd class="calibre12">b</kbd> into a register (represented in C++ by a non-atomic stack variable), and then a single atomic store into object <kbd class="calibre12">a</kbd>:</p>
<pre class="calibre23">    int shortlived = b; // atomic load<br class="title-page-name"/>    a = shortlived; // atomic store</pre>
<p class="calibre2"><kbd class="calibre12">std::atomic&lt;T&gt;</kbd> provides the member functions <kbd class="calibre12">.load()</kbd> and <kbd class="calibre12">.store(v)</kbd> for the benefit of programmers who like to see what they're doing at every step. Using them is optional:</p>
<pre class="calibre23">    int shortlived = b.load(); // atomic load<br class="title-page-name"/>    a.store(shortlived); // atomic store</pre>
<p class="calibre2">In fact, by using these member functions, you <em class="calibre22">could</em> write the assignment in a single line of code as <kbd class="calibre12">b.store(a.load())</kbd>; but I advise strongly against doing that. Writing both function calls on one line of code does <em class="calibre22">not</em> mean that they'll happen "closer together" in time, and <em class="calibre22">certainly</em> doesn't mean they'll happen "atomically" (as we've just seen, that's impossible on most hardware), but writing both function calls on one line of code might very well <em class="calibre22">deceive you into thinking</em> that the calls happen "together."</p>
<p class="calibre2">Dealing with threaded code is hard enough when you're doing only one thing at a time. If you start getting clever and doing several things at once, in a single line of code, the potential for bugs skyrockets. Stick to a single atomic operation per source line; you'll find that it clarifies your thinking process and incidentally makes your code easier to read.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Doing complicated operations atomically</h1>
                
            
            
                
<p class="calibre2">You may have noticed that the operators <kbd class="calibre12">*=</kbd>, <kbd class="calibre12">/=</kbd>, <kbd class="calibre12">%=</kbd>, <kbd class="calibre12">&lt;&lt;=</kbd>, and <kbd class="calibre12">&gt;&gt;=</kbd> were omitted from the list of overloaded operators in the preceding section. These operators were deleted by <kbd class="calibre12">std::atomic&lt;int&gt;</kbd> and all the rest of the integral atomic types because they were perceived as being difficult to provide efficiently on any real hardware. However, even among the operations that were included in <kbd class="calibre12">std::atomic&lt;int&gt;</kbd>, most of them require a slightly expensive implementation trick.</p>
<p class="calibre2">Let's suppose that our hardware doesn't have an "atomic multiply" instruction, but we'd still like to implement <kbd class="calibre12">operator*=</kbd>). How would we do it? The trick is to use a primitive atomic operation known as "compare and swap," or in C++ "compare-exchange."</p>
<pre class="calibre23">    std::atomic&lt;int&gt; a = 6;<br class="title-page-name"/><br class="title-page-name"/>    a *= 9; // This isn't allowed.<br class="title-page-name"/><br class="title-page-name"/>    // But this is:<br class="title-page-name"/><br class="title-page-name"/>    int expected, desired;<br class="title-page-name"/>    do {<br class="title-page-name"/>      expected = a.load();<br class="title-page-name"/>      desired = expected * 9;<br class="title-page-name"/>    } while (!a.compare_exchange_weak(expected, desired));<br class="title-page-name"/><br class="title-page-name"/>    // At the end of this loop, a's value will<br class="title-page-name"/>    // have been "atomically" multiplied by 9.</pre>
<p class="calibre2">The meaning of <kbd class="calibre12">a.compare_exchange_weak(expected, desired)</kbd> is that the processor should look at <kbd class="calibre12">a</kbd>; and <em class="calibre22">if</em> its value is currently <kbd class="calibre12">expected</kbd>, then set its value to <kbd class="calibre12">desired</kbd>; otherwise don't. The function call returns <kbd class="calibre12">true</kbd> if <kbd class="calibre12">a</kbd> was set to <kbd class="calibre12">desired</kbd> and <kbd class="calibre12">false</kbd> otherwise.</p>
<p class="calibre2">But there's one more thing it does, too. Notice that every time through the preceding loop, we're loading the value of <kbd class="calibre12">a</kbd> into <kbd class="calibre12">expected</kbd>; but the compare-exchange function is also loading the value of <kbd class="calibre12">a</kbd> in order to compare it with <kbd class="calibre12">expected</kbd>. The second time we go through the loop, we'd prefer not to load <kbd class="calibre12">a</kbd> a second time; we'd prefer simply to set <kbd class="calibre12">expected</kbd> to the value that the compare-exchange function saw. Fortunately, <kbd class="calibre12">a.compare_exchange_weak(expected, desired)</kbd> anticipates this desire of ours, and preemptively--if it would return <kbd class="calibre12">false</kbd>--updates <kbd class="calibre12">expected</kbd> to the value it saw. That is, whenever we use <kbd class="calibre12">compare_exchange_weak</kbd>, we must provide a modifiable value for <kbd class="calibre12">expected</kbd> because the function takes it by reference.</p>
<p class="calibre2">Therefore, we should really write our example like this:</p>
<pre class="calibre23">    int expected = a.load();<br class="title-page-name"/>    while (!a.compare_exchange_weak(expected, expected * 9)) {<br class="title-page-name"/>      // continue looping<br class="title-page-name"/>    }</pre>
<p class="calibre2">The <kbd class="calibre12">desired</kbd> variable isn't really necessary except if it helps to clarify the code.</p>
<p class="calibre2">The dirty little secret of <kbd class="calibre12">std::atomic</kbd> is that most of the compound assignment operations are implemented as compare-exchange loops just like this. On RISC processors, this is practically always the case. On x86 processors, this is the case only if you want to use the return value of the operator, as in <kbd class="calibre12">x = (a += b)</kbd>.</p>
<p class="calibre2">When the atomic variable <kbd class="calibre12">a</kbd> isn't being modified very frequently by other threads, there's no harm in doing a compare-exchange loop. But when <kbd class="calibre12">a</kbd> is being frequently modified--when it is highly <em class="calibre22">contended</em>--then we might see the loop being taken several times before it succeeds. In an absolutely pathological case, we might even see starvation of the looping thread; it might just keep looping forever, until the contention died down. However, notice that every time our compare-exchange returns <kbd class="calibre12">false</kbd> and we loop around again, it is because the value of <kbd class="calibre12">a</kbd> in memory has changed; which means that some other thread must have made a little bit of progress. Compare-exchange loops by themselves will never cause the program to enter a state where <em class="calibre22">nobody</em> is making progress (a state known technically as "livelock").</p>
<p class="calibre2">The previous paragraph probably sounds scarier than it ought to. There's generally no need to worry about this pathological behavior, since it manifests itself only under really high contention and even then doesn't really cause any terrible problem. The real takeaway you should take from this section is how you can use a compare-exchange loop to implement complicated, non-built-in "atomic" operations on <kbd class="calibre12">atomic&lt;T&gt;</kbd> objects. Just remember the order of the parameters to <kbd class="calibre12">a.compare_exchange_weak(expected, desired)</kbd> by remembering what it does to <kbd class="calibre12">a</kbd>: "if <kbd class="calibre12">a</kbd> has the expected value, give it the desired value."</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Big atomics</h1>
                
            
            
                
<p class="calibre2">The compiler will recognize and generate optimal code for <kbd class="calibre12">std::atomic&lt;T&gt;</kbd> when <kbd class="calibre12">T</kbd> is an integral type (including <kbd class="calibre12">bool</kbd>), or when <kbd class="calibre12">T</kbd> is a pointer type such as <kbd class="calibre12">void *</kbd>. But what if <kbd class="calibre12">T</kbd> is a bigger type, such as <kbd class="calibre12">int[100]</kbd>? In that case, the compiler will generally call out to a routine in the C++ runtime library which will perform the assignment under a <em class="calibre22">mutex</em>. (We'll look at mutexes in a moment.) Since the assignment is being performed out in a library which doesn't know how to copy arbitrary user-defined types, the C++17 standard restricts <kbd class="calibre12">std::atomic&lt;T&gt;</kbd> to work only with types that are <em class="calibre22">trivially copyable</em>, which is to say they can be copied safely using <kbd class="calibre12">memcpy</kbd>. So, if you wanted <kbd class="calibre12">std::atomic&lt;std::string&gt;</kbd>, tough luck--you'll have to write that one yourself.</p>
<p class="calibre2">The other catch when using big (trivially copyable) types with <kbd class="calibre12">std::atomic</kbd> is that the relevant C++ runtime routines often live in a different place from the rest of the C++ standard library. On some platforms, you'd be required to add <kbd class="calibre12">-latomic</kbd> to your linker command line. But this is only a problem if you actually do use big types with <kbd class="calibre12">std::atomic</kbd>, and as you really shouldn't, there's generally no reason to worry.</p>
<p class="calibre2">Now let's look at how you'd write that atomic string class!</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Taking turns with std::mutex</h1>
                
            
            
                
<p class="calibre2">Suppose we want to write a class type that behaves basically like <kbd class="calibre12">std::atomic&lt;std::string&gt;</kbd> would, if it existed. That is, we'd like to make it support atomic, thread-safe loads and stores, so that if two threads are accessing the <kbd class="calibre12">std::string</kbd> concurrently, neither one will ever observe it in a "halfway assigned" state, the way we observed a "halfway assigned" <kbd class="calibre12">int64_t</kbd> in the code sample in the previous section "The problem with volatile."</p>
<p class="calibre2">The best way to write this class is to use a standard library type called <kbd class="calibre12">std::mutex</kbd>. The name "mutex" is so common in technical circles that these days it basically just stands for itself, but originally its name is derived from "<em class="calibre22">mut</em>ual <em class="calibre22">ex</em>clusion." This is because a mutex acts as a way to ensure that only one thread is allowed into a particular section of code (or set of sections of code) at once--that is, to ensure that the possibilities "thread A is executing this code" and "thread B is executing this code" are <em class="calibre22">mutually exclusive</em> possibilities.</p>
<p class="calibre2">At the start of such a critical section, to indicate that we don't want to be disturbed by any other thread, we <em class="calibre22">take a lock</em> on the associated mutex. When we leave the critical section, we <em class="calibre22">release the lock</em>. The library takes care of making sure that no two threads can hold locks on the same mutex at the same time. Specifically, this means that if thread B comes in while thread A is already holding the lock, thread B must <em class="calibre22">wait</em> until thread A leaves the critical section and releases the lock. As long as thread A holds the lock, thread B's progress is <em class="calibre22">blocked</em>; therefore this phenomenon is referred to as either <em class="calibre22">waiting</em> or <em class="calibre22">blocking</em>, interchangeably.</p>
<p class="calibre2">"Taking a lock on a mutex" is often shortened to "locking the mutex," and "releasing the lock" shortened to "unlocking the mutex."</p>
<p class="calibre2">Sometimes (albeit rarely) it can be useful to test whether a mutex is currently locked. For this purpose <kbd class="calibre12">std::mutex</kbd> exposes not only the member functions <kbd class="calibre12">.lock()</kbd> and <kbd class="calibre12">.unlock()</kbd> but also the member function <kbd class="calibre12">.try_lock()</kbd>, which returns <kbd class="calibre12">true</kbd> if it was able to acquire a lock on the mutex (in which case the mutex will be locked) and <kbd class="calibre12">false</kbd> if the mutex was already locked by some thread.</p>
<p class="calibre2">In some languages, like Java, each object carries with it its own mutex; this is how Java implements its <kbd class="calibre12">synchronized</kbd> blocks, for example. In C++, a mutex is its own object type; when you want to use a mutex to control a section of code, you need to think about the lifetime semantics of the mutex object itself. Where can you put the mutex so that there will be just a single mutex object that is visible to everyone who wants to use it? Sometimes, if there is just one critical section that needs protection, you can put the mutex in a function-scoped static variable:</p>
<pre class="calibre23">    void log(const char *message)<br class="title-page-name"/>    {<br class="title-page-name"/>      static std::mutex m;<br class="title-page-name"/>      m.lock(); // avoid interleaving messages on stdout<br class="title-page-name"/>      puts(message);<br class="title-page-name"/>      m.unlock();<br class="title-page-name"/>    }</pre>
<p class="calibre2">The <kbd class="calibre12">static</kbd> keyword here is very important! If we had omitted it, then <kbd class="calibre12">m</kbd> would have been a plain old stack variable, and each thread that entered <kbd class="calibre12">log</kbd> would have received its own distinct copy of <kbd class="calibre12">m</kbd>. That wouldn't have helped us with our goal, because the library merely ensures that no two threads have a lock on the <em class="calibre22">same</em> mutex object at once. If each thread is locking and unlocking its own distinct mutex object, then the library has nothing to do; none of the mutexes are being <em class="calibre22">contended</em>.</p>
<p class="calibre2">If we want to make sure that two different functions are mutually exclusive with each other, such that only one thread is allowed in either <kbd class="calibre12">log1</kbd> or <kbd class="calibre12">log2</kbd> at any given time, we must put the mutex object somewhere that can be seen by both critical sections:</p>
<pre class="calibre23">    static std::mutex m;<br class="title-page-name"/><br class="title-page-name"/>    void log1(const char *message) {<br class="title-page-name"/>      m.lock();<br class="title-page-name"/>      printf("LOG1: %s\n", message);<br class="title-page-name"/>      m.unlock();<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    void log2(const char *message) {<br class="title-page-name"/>      m.lock();<br class="title-page-name"/>      printf("LOG2: %s\n", message);<br class="title-page-name"/>      m.unlock();<br class="title-page-name"/>    }</pre>
<p class="calibre2">Generally, if you find yourself needing to do this, you should try to eliminate the global variable by creating a class type and making the mutex object a member variable of that class, like this:</p>
<pre class="calibre23">    struct Logger {<br class="title-page-name"/>      std::mutex m_mtx;<br class="title-page-name"/><br class="title-page-name"/>      void log1(const char *message) {<br class="title-page-name"/>        m_mtx.lock();<br class="title-page-name"/>        printf("LOG1: %s\n", message);<br class="title-page-name"/>        m_mtx.unlock();<br class="title-page-name"/>      }<br class="title-page-name"/><br class="title-page-name"/>      void log2(const char *message) {<br class="title-page-name"/>        m_mtx.lock();<br class="title-page-name"/>        printf("LOG2: %s\n", message);<br class="title-page-name"/>        m_mtx.unlock();<br class="title-page-name"/>      }<br class="title-page-name"/>    };</pre>
<p class="calibre2">Now messages printed by one <kbd class="calibre12">Logger</kbd> may interleave with messages printed by another <kbd class="calibre12">Logger</kbd>, but concurrent accesses to the same <kbd class="calibre12">Logger</kbd> object will take locks on the same <kbd class="calibre12">m_mtx</kbd>, which means they will block each other and nicely take turns entering the critical functions <kbd class="calibre12">log1</kbd> and <kbd class="calibre12">log2</kbd>, one at a time.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">"Taking locks" the right way</h1>
                
            
            
                
<p class="calibre2">Recall from <a href="part0093.html#2OM4A0-2fdac365b8984feebddfbb9250eaf20d" class="calibre4">Chapter 6</a>, <em class="calibre22">Smart Pointers</em>, that one of the major problems of programs written in C and "old-style" C++ is the presence of pointer bugs--memory leaks, double-frees, and heap corruption--and that the way we eliminate those bugs from "new-style" C++ programs is via the use of RAII types such as <kbd class="calibre12">std::unique_ptr&lt;T&gt;</kbd>. Multi-threaded programming with raw mutexes have failure modes that are analogous to the failure modes of heap programming with raw pointers:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="calibre1">Lock leaks</strong><em class="calibre22">:</em> You might take a lock on a particular mutex, and accidentally forget to write the code that frees it.</li>
<li class="calibre15"><strong class="calibre1">Lock leaks</strong>: You might have written that code, but due to an early return or an exception being thrown, the code never runs and the mutex remains locked!</li>
<li class="calibre15"><strong class="calibre1">Use-outside-of-lock</strong>: Because a raw mutex is just another variable, it is physically disassociated from the variables it "guards." You might accidentally access one of those variables without taking the lock first.</li>
<li class="calibre15"><strong class="calibre1">Deadlock</strong>: Suppose thread A takes a lock on mutex 1 and thread B takes a lock on mutex 2. Then, thread A attempts to acquire a lock on mutex 2 (and blocks); and while thread A is still blocked, thread B attempts to acquire a lock on mutex 1 (and blocks). Now both threads are blocked, and will never make progress again.</li>
</ul>
<p class="calibre2">This is not an exhaustive list of concurrency pitfalls; for example, we've already briefly mentioned "livelock" in connection with <kbd class="calibre12">std::atomic&lt;T&gt;</kbd>. For a thorough treatment of concurrency bugs and how to avoid them, consult a book on multithreaded or concurrent programming.</p>
<p class="calibre2">The C++ standard library has some tools that help us eliminate these bugs from our multithreaded programs. Unlike the situation with memory management, the standard library's solutions in this case are not 100 percent guaranteed to fix your issues--multithreading is much harder than single-threaded programming, and in fact a good rule of thumb is <em class="calibre22">not to do it</em> if you can help it. But if you must do concurrent programming, the standard library can help somewhat.</p>
<p class="calibre2">Just as in <a href="part0093.html#2OM4A0-2fdac365b8984feebddfbb9250eaf20d" class="calibre4">Chapter 6</a>, <em class="calibre22">Smart Pointers,</em> we can eliminate bugs related to "lock leaks" by the conscientious use of RAII. You might have noticed that I have been consistently using the phrase "take a lock on the mutex" instead of "lock the mutex"; now we'll see why. In the phrase "lock the mutex," "lock" is a <em class="calibre22">verb</em>; this phrasing corresponds exactly to the C++ code <kbd class="calibre12">mtx.lock()</kbd>. But in the phrase "take a lock <em class="calibre22">on</em> the mutex," "lock" is a <em class="calibre22">noun</em>. Let's invent a type that reifies the idea of "lock"; that is, that turns it into a noun (an RAII class) instead of a verb (a method on a non-RAII class):</p>
<pre class="calibre23">    template&lt;typename M&gt;<br class="title-page-name"/>    class unique_lock {<br class="title-page-name"/>      M *m_mtx = nullptr;<br class="title-page-name"/>      bool m_locked = false;<br class="title-page-name"/>    public:<br class="title-page-name"/>      constexpr unique_lock() noexcept = default;<br class="title-page-name"/>      constexpr unique_lock(M *p) noexcept : m_mtx(p) {}<br class="title-page-name"/><br class="title-page-name"/>      M *mutex() const noexcept { return m_mtx; }<br class="title-page-name"/>      bool owns_lock() const noexcept { return m_locked; }<br class="title-page-name"/><br class="title-page-name"/>      void lock() { m_mtx-&gt;lock(); m_locked = true; }<br class="title-page-name"/>      void unlock() { m_mtx-&gt;unlock(); m_locked = false; }<br class="title-page-name"/><br class="title-page-name"/>      unique_lock(unique_lock&amp;&amp; rhs) noexcept {<br class="title-page-name"/>        m_mtx = std::exchange(rhs.m_mtx, nullptr);<br class="title-page-name"/>        m_locked = std::exchange(rhs.m_locked, false);<br class="title-page-name"/>      }<br class="title-page-name"/><br class="title-page-name"/>      unique_lock&amp; operator=(unique_lock&amp;&amp; rhs) {<br class="title-page-name"/>        if (m_locked) {<br class="title-page-name"/>            unlock();<br class="title-page-name"/>        }<br class="title-page-name"/>        m_mtx = std::exchange(rhs.m_mtx, nullptr);<br class="title-page-name"/>        m_locked = std::exchange(rhs.m_locked, false);<br class="title-page-name"/>        return *this;<br class="title-page-name"/>      }<br class="title-page-name"/><br class="title-page-name"/>      ~unique_lock() {<br class="title-page-name"/>        if (m_locked) {<br class="title-page-name"/>            unlock();<br class="title-page-name"/>        }<br class="title-page-name"/>      }<br class="title-page-name"/>    };</pre>
<p class="calibre2">As suggested by the name, <kbd class="calibre12">std::unique_lock&lt;M&gt;</kbd> is a "unique ownership" RAII class, similar in spirit to <kbd class="calibre12">std::unique_ptr&lt;T&gt;</kbd>. If you stick to using the noun <kbd class="calibre12">unique_ptr</kbd> instead of the verbs <kbd class="calibre12">new</kbd> and <kbd class="calibre12">delete</kbd>, you'll never forget to free a pointer; and if you stick to using the noun <kbd class="calibre12">unique_lock</kbd> instead of the verbs <kbd class="calibre12">lock</kbd> and <kbd class="calibre12">unlock</kbd>, you'll never forget to release a mutex lock.</p>
<p class="calibre2"><kbd class="calibre12">std::unique_lock&lt;M&gt;</kbd> does expose the member functions <kbd class="calibre12">.lock()</kbd> and <kbd class="calibre12">.unlock()</kbd>, but generally you will not need to use those. They can be useful if you need to acquire or release a lock in the middle of a block of code, far away from the natural point of destruction of the <kbd class="calibre12">unique_lock</kbd> object. We will also see in the next section a function that takes as a parameter a locked <kbd class="calibre12">unique_lock</kbd>, which the function unlocks and re-locks as part of its functionality.</p>
<p class="calibre2">Notice that because <kbd class="calibre12">unique_lock</kbd> is movable, it must have a "null" or "empty" state, just like <kbd class="calibre12">unique_ptr</kbd>. In most cases, you won't need to move your locks around; you'll just unconditionally take the lock at the start of some scope, and unconditionally release it at the end of the scope. For this use-case, there's <kbd class="calibre12">std::lock_guard&lt;M&gt;</kbd>. <kbd class="calibre12">lock_guard</kbd> is much like <kbd class="calibre12">unique_lock</kbd>, but it is not movable, nor does it have the <kbd class="calibre12">.lock()</kbd> and <kbd class="calibre12">.unlock()</kbd> member functions. Therefore, it doesn't need to carry around an <kbd class="calibre12">m_locked</kbd> member, and its destructor can unconditionally unlock the mutex the object has been guarding, without any extra tests.</p>
<p class="calibre2">In both cases (<kbd class="calibre12">unique_lock</kbd> and <kbd class="calibre12">lock_guard</kbd>), the class template is parameterized on the kind of mutex being locked. (We'll look at a couple more kinds of mutexes in a minute, but almost invariably, you'll want to use <kbd class="calibre12">std::mutex</kbd>.) C++17 has a new language feature called <em class="calibre22">class template argument deduction</em> that, in most cases, allows you to elide the template parameter: to write simply <kbd class="calibre12">std::unique_lock</kbd> instead of <kbd class="calibre12">std::unique_lock&lt;std::mutex&gt;</kbd>, for example. This is one of the very few cases where I would personally recommend relying on class template argument deduction, because writing out the parameter type <kbd class="calibre12">std::mutex</kbd> really adds so little information for your reader.</p>
<p class="calibre2">Let's see some examples of <kbd class="calibre12">std::lock_guard</kbd>, with and without class template argument deduction:</p>
<pre class="calibre23">    struct Lockbox {<br class="title-page-name"/>      std::mutex m_mtx;<br class="title-page-name"/>      int m_value = 0;<br class="title-page-name"/><br class="title-page-name"/>      void locked_increment() {<br class="title-page-name"/>        std::lock_guard&lt;std::mutex&gt; lk(m_mtx);<br class="title-page-name"/>        m_value += 1;<br class="title-page-name"/>      }<br class="title-page-name"/><br class="title-page-name"/>      void locked_decrement() {<br class="title-page-name"/>        std::lock_guard lk(m_mtx); // C++17 only<br class="title-page-name"/>        m_value -= 1;<br class="title-page-name"/>      }<br class="title-page-name"/>    };</pre>
<p class="calibre2">Before we can see similarly practical examples of <kbd class="calibre12">std::unique_lock</kbd>, we'll have to explain a good reason to use <kbd class="calibre12">std::unique_lock</kbd> in the first place.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Always associate a mutex with its controlled data</h1>
                
            
            
                
<p class="calibre2">Consider the following sketch of a thread-safe <kbd class="calibre12">StreamingAverage</kbd> class. There is a bug here; can you find it?</p>
<pre class="calibre23">    class StreamingAverage {<br class="title-page-name"/>      double m_sum = 0;<br class="title-page-name"/>      int m_count = 0;<br class="title-page-name"/>      double m_last_average = 0;<br class="title-page-name"/>      std::mutex m_mtx;<br class="title-page-name"/>    public:<br class="title-page-name"/>      // Called from the single producer thread<br class="title-page-name"/>      void add_value(double x) {<br class="title-page-name"/>        std::lock_guard lk(m_mtx);<br class="title-page-name"/>        m_sum += x;<br class="title-page-name"/>        m_count += 1; // A<br class="title-page-name"/>      }<br class="title-page-name"/><br class="title-page-name"/>      // Called from the single consumer thread<br class="title-page-name"/>      double get_current_average() {<br class="title-page-name"/>        std::lock_guard lk(m_mtx);<br class="title-page-name"/>        m_last_average = m_sum / m_count; // B<br class="title-page-name"/>        return m_last_average;<br class="title-page-name"/>      }<br class="title-page-name"/><br class="title-page-name"/>      // Called from the single consumer thread<br class="title-page-name"/>      double get_last_average() const {<br class="title-page-name"/>        return m_last_average; // C<br class="title-page-name"/>      }<br class="title-page-name"/><br class="title-page-name"/>      // Called from the single consumer thread<br class="title-page-name"/>      double get_current_count() const {<br class="title-page-name"/>        return m_count; // D<br class="title-page-name"/>      }<br class="title-page-name"/>    };</pre>
<p class="calibre2">The bug is the line <kbd class="calibre12">A</kbd>, which writes to <kbd class="calibre12">this-&gt;m_count</kbd> in the producer thread, races with line <kbd class="calibre12">D</kbd>, which reads from <kbd class="calibre12">this-&gt;m_count</kbd> in the consumer thread. Line <kbd class="calibre12">A</kbd> correctly takes a lock on <kbd class="calibre12">this-&gt;m_mtx</kbd> before writing, but line <kbd class="calibre12">D</kbd> fails to take a similar lock, which means that it will happily barge in and attempt to read <kbd class="calibre12">m_count</kbd> even while line <kbd class="calibre12">A</kbd> is writing to it.</p>
<p class="calibre2">Lines <kbd class="calibre12">B</kbd> and <kbd class="calibre12">C</kbd> look superficially similar, which is probably how the bug originally crept in. Line <kbd class="calibre12">C</kbd> doesn't need to take a lock; why should line <kbd class="calibre12">D</kbd> have to? Well, line <kbd class="calibre12">C</kbd> is called only from the consumer thread, which is the same thread that writes to <kbd class="calibre12">m_last_average</kbd> on line <kbd class="calibre12">B</kbd>. Since lines <kbd class="calibre12">B</kbd> and <kbd class="calibre12">C</kbd> are executed only by the single consumer thread, they can't both be executed simultaneously--at least as long as the rest of the program conforms to the comments! (Let's assume the code comments are correct. This is often sadly untrue in practice, but for the sake of this example let's assume it.)</p>
<p class="calibre2">We have a recipe for confusion here: Locking <kbd class="calibre12">m_mtx</kbd> is required when touching <kbd class="calibre12">m_sum</kbd> or <kbd class="calibre12">m_count</kbd>, but it is not required when touching <kbd class="calibre12">m_last_average</kbd>. If this class becomes even more complicated, it might even have several mutexes involved (although at that point, it would clearly be violating the Single Responsibility Principle and would probably benefit from refactoring into smaller components). Therefore, a very good practice when dealing with mutexes is to place the mutex in the tightest possible relationship to the variables it "guards." One way to do this is simply via careful naming:</p>
<pre class="calibre23">    class StreamingAverage {<br class="title-page-name"/>      double m_sum = 0;<br class="title-page-name"/>      int m_count = 0;<br class="title-page-name"/>      double m_last_average = 0;<br class="title-page-name"/>      std::mutex m_sum_count_mtx;<br class="title-page-name"/><br class="title-page-name"/>      // ...<br class="title-page-name"/>    };</pre>
<p class="calibre2">A better way is via a nested struct definition:</p>
<pre class="calibre23">    class StreamingAverage {<br class="title-page-name"/>      struct {<br class="title-page-name"/>        double sum = 0;<br class="title-page-name"/>        int count = 0;<br class="title-page-name"/>        std::mutex mtx;<br class="title-page-name"/>      } m_guarded_sc;<br class="title-page-name"/>      double m_last_average = 0;<br class="title-page-name"/><br class="title-page-name"/>      // ...<br class="title-page-name"/>    };</pre>
<p class="calibre2">The hope above is that when the programmer is forced to write <kbd class="calibre12">this-&gt;m_guarded_sc.sum</kbd>, it reminds him to make sure he's already gotten a lock on <kbd class="calibre12">this-&gt;m_guarded_sc.mtx</kbd>. We could use the GNU extension of "anonymous struct members" to avoid retyping <kbd class="calibre12">m_guarded_sc</kbd> all over our code; but this would defeat the purpose of this approach, which is to make sure that every place the data is accessed <em class="calibre22">must</em> use the word "guarded," reminding the programmer to take that lock on <kbd class="calibre12">this-&gt;m_guarded_sc.mtx</kbd>.</p>
<p class="calibre2">An even more bulletproof, but somewhat inflexible, approach is to place the mutex in a class that allows access to its private members only when the mutex is locked, by returning an RAII handle. The handle-returning class would look more or less like this:</p>
<pre class="calibre23">    template&lt;class Data&gt;<br class="title-page-name"/>    class Guarded {<br class="title-page-name"/>      std::mutex m_mtx;<br class="title-page-name"/>      Data m_data;<br class="title-page-name"/><br class="title-page-name"/>      class Handle {<br class="title-page-name"/>        std::unique_lock&lt;std::mutex&gt; m_lk;<br class="title-page-name"/>        Data *m_ptr;<br class="title-page-name"/>      public:<br class="title-page-name"/>        Handle(std::unique_lock&lt;std::mutex&gt; lk, Data *p) :<br class="title-page-name"/>          m_lk(std::move(lk)), m_ptr(p) {}<br class="title-page-name"/>        auto operator-&gt;() const { return m_ptr; }<br class="title-page-name"/>      };<br class="title-page-name"/>    public:<br class="title-page-name"/>      Handle lock() {<br class="title-page-name"/>        std::unique_lock lk(m_mtx);<br class="title-page-name"/>        return Handle{std::move(lk), &amp;m_data};<br class="title-page-name"/>      }<br class="title-page-name"/>    };</pre>
<p class="calibre2">And our <kbd class="calibre12">StreamingAverage</kbd> class could use it like this:</p>
<pre class="calibre23">    class StreamingAverage {<br class="title-page-name"/>      struct Guts {<br class="title-page-name"/>        double m_sum = 0;<br class="title-page-name"/>        int m_count = 0;<br class="title-page-name"/>      };<br class="title-page-name"/>      Guarded&lt;Guts&gt; m_sc;<br class="title-page-name"/>      double m_last_average = 0;<br class="title-page-name"/><br class="title-page-name"/>      // ...<br class="title-page-name"/><br class="title-page-name"/>      double get_current_average() {<br class="title-page-name"/>        auto h = m_sc.lock();<br class="title-page-name"/>        m_last_average = h-&gt;m_sum / h-&gt;m_count;<br class="title-page-name"/>        return m_last_average;<br class="title-page-name"/>      }<br class="title-page-name"/>    };
impossible</em> for any member function of <kbd class="calibre12">StreamingAverage</kbd> to access <kbd class="calibre12">m_sum</kbd> without owning a lock on <kbd class="calibre12">m_mtx</kbd>; access to the guarded <kbd class="calibre12">m_sum</kbd> is possible only via the RAII <kbd class="calibre12">Handle</kbd> type.</pre>
<p>This pattern is included in Facebook's Folly library under the name <kbd class="calibre25">folly::Synchronized&lt;T&gt;</kbd>, and many more variations on it are available in Ansel Sermersheim and Barbara Geller's "libGuarded" template library.</p>
<p class="calibre2">Notice the use of <kbd class="calibre12">std::unique_lock&lt;std::mutex&gt;</kbd> in the <kbd class="calibre12">Handle</kbd> class! We're using <kbd class="calibre12">unique_lock</kbd> here, not <kbd class="calibre12">lock_guard</kbd>, because we want the ability to pass this lock around, return it from functions, and so on--so it needs to be movable. This is the main reason you'd reach into your toolbox for <kbd class="calibre12">unique_lock</kbd>.</p>
<p class="calibre2">Do be aware that this pattern does not solve all lock-related bugs--it solves only the simplest "forget to lock the mutex" cases--and it might encourage programming patterns that lead to more concurrency bugs of other types. For example, consider the following rewrite of <kbd class="calibre12">StreamingAverage::get_current_average</kbd>:</p>
<pre class="calibre23">    double get_sum() {<br class="title-page-name"/>      return m_sc.lock()-&gt;m_sum; <br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    int get_count() {<br class="title-page-name"/>      return m_sc.lock()-&gt;m_count;<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    double get_current_average() {<br class="title-page-name"/>      return get_sum() / get_count();<br class="title-page-name"/>    }</pre>
<p class="calibre2">Because of the two calls to <kbd class="calibre12">m_sc.lock()</kbd>, there is a gap between the read of <kbd class="calibre12">m_sum</kbd> and the read of <kbd class="calibre12">m_count</kbd>. If the producer thread calls <kbd class="calibre12">add_value</kbd> during this gap, we will compute an incorrect average (too low by a factor of <kbd class="calibre12">1 / m_count</kbd>). And if we try to "fix" this bug by taking a lock around the entire computation, we'll find ourselves in deadlock:</p>
<pre class="calibre23">    double get_sum() {<br class="title-page-name"/>      return m_sc.lock()-&gt;m_sum; // LOCK 2<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    int get_count() {<br class="title-page-name"/>      return m_sc.lock()-&gt;m_count;<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    double get_current_average() {<br class="title-page-name"/>      auto h = m_sc.lock(); // LOCK 1<br class="title-page-name"/>      return get_sum() / get_count();<br class="title-page-name"/>    }</pre>
<p class="calibre2">The line marked <kbd class="calibre12">LOCK 1</kbd> causes the mutex to become locked; then, on the line marked <kbd class="calibre12">LOCK 2</kbd>, we try to lock the mutex again. The general rule with mutexes is, if you're trying to lock a mutex and it's already locked, then you must <em class="calibre22">block</em> and wait for it to become unlocked. So our thread blocks and waits for the mutex to unlock--which will never happen, since the lock is being held by our own thread!</p>
<p class="calibre2">This problem (deadlock with oneself) should generally be dealt with by careful programming--that is, you should try not to take locks you already hold! But if taking locks this way is unavoidably part of your design, then the standard library has your back, so let's talk about <kbd class="calibre12">recursive_mutex</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Special-purpose mutex types</h1>
                
            
            
                
<p class="calibre2">Recall that <kbd class="calibre12">std::lock_guard&lt;M&gt;</kbd> and <kbd class="calibre12">std::unique_lock&lt;M&gt;</kbd> are parameterized on the mutex type. So far we've seen only <kbd class="calibre12">std::mutex</kbd>. However, the standard library does contain a few other mutex types which can be useful in special circumstances.</p>
<p class="calibre2"><kbd class="calibre12">std::recursive_mutex</kbd> is like <kbd class="calibre12">std::mutex</kbd>, but remembers <em class="calibre22">which</em> thread has locked it. If that particular thread tries to lock it a second time, the recursive mutex will merely increment an internal reference count of "how many times I've been locked." If some other thread tries to lock the recursive mutex, that thread will block until the original thread has unlocked the mutex the appropriate number of times.</p>
<p class="calibre2"><kbd class="calibre12">std::timed_mutex</kbd> is like <kbd class="calibre12">std::mutex</kbd>, but is aware of the passage of time. It has as member functions not only the usual <kbd class="calibre12">.try_lock()</kbd>, but also <kbd class="calibre12">.try_lock_for()</kbd> and <kbd class="calibre12">.try_lock_until()</kbd>, which interact with the standard <kbd class="calibre12">&lt;chrono&gt;</kbd> library. Here's an example of <kbd class="calibre12">try_lock_for</kbd>:</p>
<pre class="calibre23">    std::timed_mutex m;<br class="title-page-name"/>    std::atomic&lt;bool&gt; ready = false;<br class="title-page-name"/><br class="title-page-name"/>    std::thread thread_b([&amp;]() {<br class="title-page-name"/>      std::lock_guard lk(m);<br class="title-page-name"/>      puts("Thread B got the lock.");<br class="title-page-name"/>      ready = true;<br class="title-page-name"/>      std::this_thread::sleep_for(100ms);<br class="title-page-name"/>    });<br class="title-page-name"/><br class="title-page-name"/>    while (!ready) {<br class="title-page-name"/>      puts("Thread A is waiting for thread B to launch.");<br class="title-page-name"/>      std::this_thread::sleep_for(10ms);<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    while (!m.try_lock_for(10ms)) {<br class="title-page-name"/>      puts("Thread A spent 10ms trying to get the lock and failed.");<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    puts("Thread A finally got the lock!");<br class="title-page-name"/>    m.unlock();</pre>
<p class="calibre2">And here's an example of <kbd class="calibre12">try_lock_until</kbd>:</p>
<pre class="calibre23">    std::timed_mutex m1, m2;<br class="title-page-name"/>    std::atomic&lt;bool&gt; ready = false;<br class="title-page-name"/><br class="title-page-name"/>    std::thread thread_b([&amp;]() {<br class="title-page-name"/>      std::unique_lock lk1(m1);<br class="title-page-name"/>      std::unique_lock lk2(m2);<br class="title-page-name"/>      puts("Thread B got the locks.");<br class="title-page-name"/>      ready = true;<br class="title-page-name"/>      std::this_thread::sleep_for(50ms);<br class="title-page-name"/>      lk1.unlock();<br class="title-page-name"/>      std::this_thread::sleep_for(50ms);<br class="title-page-name"/>    });<br class="title-page-name"/><br class="title-page-name"/>    while (!ready) {<br class="title-page-name"/>      std::this_thread::sleep_for(10ms); <br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    auto start_time = std::chrono::system_clock::now();<br class="title-page-name"/>    auto deadline = start_time + 100ms;<br class="title-page-name"/><br class="title-page-name"/>    bool got_m1 = m1.try_lock_until(deadline);<br class="title-page-name"/>    auto elapsed_m1 = std::chrono::system_clock::now() - start_time;<br class="title-page-name"/><br class="title-page-name"/>    bool got_m2 = m2.try_lock_until(deadline);<br class="title-page-name"/>    auto elapsed_m2 = std::chrono::system_clock::now() - start_time;<br class="title-page-name"/><br class="title-page-name"/>    if (got_m1) {<br class="title-page-name"/>      printf("Thread A got the first lock after %dms.\n",<br class="title-page-name"/>      count_ms(elapsed_m1));<br class="title-page-name"/>      m1.unlock();<br class="title-page-name"/>    }<br class="title-page-name"/>    if (got_m2) {<br class="title-page-name"/>      printf("Thread A got the second lock after %dms.\n",<br class="title-page-name"/>      count_ms(elapsed_m2));<br class="title-page-name"/>      m2.unlock();<br class="title-page-name"/>    }  </pre>
<p class="calibre2">Incidentally, the <kbd class="calibre12">count_ms</kbd> function being used here is just a little lambda that factors out some of the usual <kbd class="calibre12">&lt;chrono&gt;</kbd> boilerplate:</p>
<pre class="calibre23">    auto count_ms = [](auto&amp;&amp; d) -&gt; int {<br class="title-page-name"/>      using namespace std::chrono;<br class="title-page-name"/>      return duration_cast&lt;milliseconds&gt;(d).count();<br class="title-page-name"/>    };</pre>
<p class="calibre2">In both of the preceding examples, pay attention to our use of <kbd class="calibre12">std::atomic&lt;bool&gt;</kbd> to synchronize threads <kbd class="calibre12">A</kbd> and <kbd class="calibre12">B</kbd>. We simply initialize the atomic variable to <kbd class="calibre12">false</kbd>, and then loop until it becomes <kbd class="calibre12">true</kbd>. The body of the polling loop is a call to <kbd class="calibre12">std::this_thread::sleep_for</kbd>, which is a sufficient hint to the compiler that the value of the atomic variable might change. Be careful never to write a polling loop that does not contain a sleep, because in that case the compiler is within its rights to collapse all the consecutive loads of <kbd class="calibre12">ready</kbd> down into a single load and a (necessarily infinite) loop.</p>
<p class="calibre2"><kbd class="calibre12">std::recursive_timed_mutex</kbd> is like you took <kbd class="calibre12">recursive_mutex</kbd> and <kbd class="calibre12">timed_mutex</kbd> and smushed them together; it provides the "counting" semantics of <kbd class="calibre12">recursive_mutex</kbd>, <em class="calibre22">plus</em> the <kbd class="calibre12">try_lock_for</kbd> and <kbd class="calibre12">try_lock_until</kbd> methods of <kbd class="calibre12">timed_mutex</kbd>.</p>
<p class="calibre2"><kbd class="calibre12">std::shared_mutex</kbd> is perhaps poorly named. It implements behavior that in most concurrency textbooks would be called a <em class="calibre22">read-write lock</em> (also known as a <em class="calibre22">rwlock</em> or <em class="calibre22">readers-writer lock</em>). The defining characteristic of a read-write lock, or <kbd class="calibre12">shared_mutex</kbd>, is that it can be "locked" in two different ways. You can take a normal exclusive ("write") lock by calling <kbd class="calibre12">sm.lock()</kbd>, or you can take a non-exclusive ("read") lock by calling <kbd class="calibre12">sm.lock_shared()</kbd>. Many different threads are allowed to take read locks at the same time; but if <em class="calibre22">anybody</em> is reading, then <em class="calibre22">nobody</em> can be writing; and if <em class="calibre22">anybody</em> is writing, then <em class="calibre22">nobody</em> can be doing anything else (neither reading nor writing). These happen to be fundamentally the same rules that define "race conditions" in the C++ memory model: two threads reading from the same object simultaneously is fine, as long as no thread is writing to it at the same time. What <kbd class="calibre12">std::shared_mutex</kbd> adds to the mix is safety: it ensures that if anyone <em class="calibre22">does</em> try to write (at least if they play nice and take a write lock on the <kbd class="calibre12">std::shared_mutex</kbd> first), they'll block until all the readers have exited and it's safe to write.</p>
<p class="calibre2"><kbd class="calibre12">std::unique_lock&lt;std::shared_mutex&gt;</kbd> is the noun corresponding to an exclusive ("write") lock on a <kbd class="calibre12">std::shared_mutex</kbd>. As you might expect, the standard library also provides <kbd class="calibre12">std::shared_lock&lt;std::shared_mutex&gt;</kbd> to reify the idea of a non-exclusive ("read") lock on a <kbd class="calibre12">std::shared_mutex</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Upgrading a read-write lock</h1>
                
            
            
                
<p class="calibre2">Suppose you have a read lock on a <kbd class="calibre12">shared_mutex</kbd> (that is to say, you have a <kbd class="calibre12">std::shared_lock&lt;std::shared_mutex&gt; lk</kbd> such that <kbd class="calibre12">lk.owns_lock()</kbd>), and you want to get a write lock. Can you "upgrade" your lock?</p>
<p class="calibre2">No, you can't. Consider what would happen if threads <kbd class="calibre12">A</kbd> and <kbd class="calibre12">B</kbd> both hold read locks, and simultaneously attempt to upgrade to write locks without first releasing their read locks. Neither one would be able to acquire a write lock, and so they'd deadlock with each other.</p>
<p class="calibre2">There <em class="calibre22">are</em> third-party libraries that attempt to solve this problem, such as <kbd class="calibre12">boost::thread::upgrade_lock</kbd>, which works with <kbd class="calibre12">boost::thread::shared_mutex</kbd>; but they are outside the scope of this book. The standard solution is that if you hold a read lock and want a write lock, you must release your read lock and then go stand in line for a write lock with everyone else:</p>
<pre class="calibre23">    template&lt;class M&gt; <br class="title-page-name"/>    std::unique_lock&lt;M&gt; upgrade(std::shared_lock&lt;M&gt; lk)<br class="title-page-name"/>    {<br class="title-page-name"/>      lk.unlock();<br class="title-page-name"/>      // Some other writer might sneak in here.<br class="title-page-name"/>      return std::unique_lock&lt;M&gt;(*lk.mutex());<br class="title-page-name"/>    }</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Downgrading a read-write lock</h1>
                
            
            
                
<p class="calibre2">Suppose you have an exclusive write lock on a <kbd class="calibre12">shared_mutex</kbd> and you want to get a read lock. Can you "downgrade" your lock?</p>
<p class="calibre2">In principle the answer is yes, it should be possible to downgrade a write lock to a read lock; but in standard C++17 the answer is no, you can't do it directly. As in the upgrade case, you can use <kbd class="calibre12">boost::thread::shared_mutex</kbd>. The standard solution is that if you hold a write lock and want a read lock, you must release your write lock and then go stand in line for a read lock with everyone else:</p>
<pre class="calibre23">    template&lt;class M&gt;<br class="title-page-name"/>    std::shared_lock&lt;M&gt; downgrade(std::unique_lock&lt;M&gt; lk)<br class="title-page-name"/>    {<br class="title-page-name"/>      lk.unlock();<br class="title-page-name"/>      // Some other writer might sneak in here.<br class="title-page-name"/>      return std::shared_lock&lt;M&gt;(*lk.mutex()); <br class="title-page-name"/>    }</pre>
<p class="calibre2">As you can see from these examples, C++17's <kbd class="calibre12">std::shared_mutex</kbd> is a bit half-baked at the moment. If your architectural design calls for a read-write lock, I strongly recommend using something like <kbd class="calibre12">boost::thread::shared_mutex</kbd>, which comes "batteries included."</p>
<p class="calibre2">You may have noticed that since new readers can come in while a read lock is held, but new writers cannot, it is conceivable and even likely for a prospective writer thread to be "starved" by a steady stream of prospective readers, unless the implementation goes out of its way to provide a strong "no starvation" guarantee. <kbd class="calibre12">boost::thread::shared_mutex</kbd> provides such a guarantee (at least, it avoids starvation if the underlying operating system's scheduler does). The standard wording for <kbd class="calibre12">std::shared_mutex</kbd> provides no such guarantee, although any implementation that allowed starvation in practice would be considered a pretty poor one. In practice you'll find that your standard library vendor's implementation of <kbd class="calibre12">shared_mutex</kbd> is pretty close to the Boost one, except for the missing upgrade/downgrade functionality.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Waiting for a condition</h1>
                
            
            
                
<p class="calibre2">In the section titled "Special-purpose mutex types," we launched a task in a separate thread and then needed to wait until a certain bit of initialization was done before continuing. We used a polling loop around a <kbd class="calibre12">std::atomic&lt;bool&gt;</kbd> in that case. But there are better ways to wait!</p>
<p class="calibre2">The problem with our 50-millisecond polling loop is that it <em class="calibre22">never</em> spends the right amount of time asleep. Sometimes our thread will wake up, but the condition it's waiting for hasn't been satisfied, so it'll go back to sleep--that means we didn't sleep long enough the first time. Sometimes our thread will wake up and see that the condition it's waiting for <em class="calibre22">has</em> been satisfied, sometime in the past 50 milliseconds, but we don't know how long ago--that means we've <em class="calibre22">overslept</em> by about 25 milliseconds on average. Whatever happens, the chance that we slept <em class="calibre22">just precisely the right amount of time</em> is slim to none.</p>
<p class="calibre2">So, if we don't want to waste time, the right thing to do is to avoid polling loops. The standard library provides a way to wait just the <em class="calibre22">right</em> amount of time; it's called <kbd class="calibre12">std::condition_variable</kbd>.</p>
<p class="calibre2">Given a variable <kbd class="calibre12">cv</kbd> of type <kbd class="calibre12">std::condition_variable</kbd>, our thread can "wait on" <kbd class="calibre12">cv</kbd> by calling <kbd class="calibre12">cv.wait(lk)</kbd>; that puts our thread to sleep. Calling <kbd class="calibre12">cv.notify_one()</kbd> or <kbd class="calibre12">cv.notify_all()</kbd> wakes up one, or all of, the threads currently waiting on <kbd class="calibre12">cv</kbd>. However, this is not the only way that those threads might wake up! It's possible that an interrupt from outside (such as a POSIX signal) might jar your thread awake without anybody's having called <kbd class="calibre12">notify_one</kbd>. This phenomenon is called a <em class="calibre22">spurious wakeup</em>. The usual way to guard against spurious wakeups is to check your condition when you wake up. For example, if you're waiting for some input to arrive in a buffer <kbd class="calibre12">b</kbd>, then when you wake up, you ought to check <kbd class="calibre12">b.empty()</kbd> and, if it's empty, go back to waiting.</p>
<p class="calibre2">By definition, some other thread is going to be putting that data into <kbd class="calibre12">b</kbd>; so when you read <kbd class="calibre12">b.empty()</kbd>, you'd better do it under some kind of mutex. Which means the first thing you'll do when you wake up is take a lock on that mutex, and the last thing you'll do when you go back to sleep is release your lock on that mutex. (In fact, you need to release your lock on that mutex atomically with the going-to-sleep operation, so that nobody can slip in, modify <kbd class="calibre12">b</kbd>, and call <kbd class="calibre12">cv.notify_one()</kbd> before you've managed to get to sleep.) This chain of logic leads us to the reason that <kbd class="calibre12">cv.wait(lk)</kbd> takes that parameter <kbd class="calibre12">lk</kbd>--it's a <kbd class="calibre12">std::unique_lock&lt;std::mutex&gt;</kbd> that will be released upon going to sleep and regained upon awaking!</p>
<p class="calibre2">Here's an example of waiting for some condition to be satisfied. First the simple but wasteful polling loop on a <kbd class="calibre12">std::atomic</kbd> variable:</p>
<pre class="calibre23">    std::atomic&lt;bool&gt; ready = false;<br class="title-page-name"/><br class="title-page-name"/>    std::thread thread_b([&amp;]() {<br class="title-page-name"/>      prep_work();<br class="title-page-name"/>      ready = true;<br class="title-page-name"/>      main_work();<br class="title-page-name"/>    });<br class="title-page-name"/><br class="title-page-name"/>    // Wait for thread B to be ready.<br class="title-page-name"/>    while (!ready) {<br class="title-page-name"/>      std::this_thread::sleep_for(10ms); <br class="title-page-name"/>    }<br class="title-page-name"/>    // Now thread B has completed its prep work.</pre>
<p class="calibre2">And now the preferable and more efficient <kbd class="calibre12">condition_variable</kbd> implementation:</p>
<pre class="calibre23">    bool ready = false; // not atomic!<br class="title-page-name"/>    std::mutex ready_mutex;<br class="title-page-name"/>    std::condition_variable cv;<br class="title-page-name"/><br class="title-page-name"/>    std::thread thread_b([&amp;]() {<br class="title-page-name"/>      prep_work();<br class="title-page-name"/>      {<br class="title-page-name"/>        std::lock_guard lk(ready_mutex);<br class="title-page-name"/>        ready = true;<br class="title-page-name"/>      }<br class="title-page-name"/>      cv.notify_one();<br class="title-page-name"/>      main_work();<br class="title-page-name"/>    });<br class="title-page-name"/><br class="title-page-name"/>    // Wait for thread B to be ready.<br class="title-page-name"/>    {<br class="title-page-name"/>      std::unique_lock lk(ready_mutex);<br class="title-page-name"/>      while (!ready) {<br class="title-page-name"/>        cv.wait(lk);<br class="title-page-name"/>      }<br class="title-page-name"/>    }<br class="title-page-name"/>    // Now thread B has completed its prep work.</pre>
<p class="calibre2">If we're waiting to read from a structure protected by a read-write lock (that is, a <kbd class="calibre12">std::shared_mutex</kbd>), then we don't want to pass in a <kbd class="calibre12">std::unique_lock&lt;std::mutex&gt;</kbd>; we want to pass in a <kbd class="calibre12">std::shared_lock&lt;std::shared_mutex&gt;</kbd>. We can do this, if (and sadly only if) we plan ahead and define our condition variable to be of type <kbd class="calibre12">std::condition_variable_any</kbd> instead of <kbd class="calibre12">std::condition_variable</kbd>. In practice, there is unlikely to be any performance difference between <kbd class="calibre12">std::condition_variable_any</kbd> and <kbd class="calibre12">std::condition_variable</kbd>, which means you should choose between them based on your program's needs, or, if either one would serve, then based on the clarity of the resulting code. Generally this means saving four characters and using <kbd class="calibre12">std::condition_variable</kbd>. However, notice that because of the layer of insulating abstraction provided by <kbd class="calibre12">std::shared_lock</kbd>, the actual code for waiting on <kbd class="calibre12">cv</kbd> under a read-write lock is almost identical to the code for waiting on <kbd class="calibre12">cv</kbd> under a plain old mutex. Here is the read-write lock version:</p>
<pre class="calibre23">    bool ready = false;<br class="title-page-name"/>    std::<strong class="calibre1">shared_</strong>mutex ready_rwlock;<br class="title-page-name"/>    std::condition_variable<strong class="calibre1">_any</strong> cv;<br class="title-page-name"/>    std::thread thread_b([&amp;]() {<br class="title-page-name"/>      prep_work();<br class="title-page-name"/>      {<br class="title-page-name"/>        std::lock_guard lk(ready_rwlock);<br class="title-page-name"/>        ready = true;<br class="title-page-name"/>      }<br class="title-page-name"/>      cv.notify_one();<br class="title-page-name"/>      main_work();<br class="title-page-name"/>    });<br class="title-page-name"/><br class="title-page-name"/>    // Wait for thread B to be ready.<br class="title-page-name"/>    {<br class="title-page-name"/>      std::<strong class="calibre1">shared_</strong>lock lk(ready_rwlock);<br class="title-page-name"/>      while (!ready) {<br class="title-page-name"/>        cv.wait(lk);<br class="title-page-name"/>      }<br class="title-page-name"/>    }<br class="title-page-name"/>    // Now thread B has completed its prep work.</pre>
<p class="calibre2">This is perfectly correct code, and as efficient as it can be. However, manually fiddling with mutex locks and condition variables is almost as dangerous to one's health as fiddling with raw mutexes or raw pointers. We can do better! The better solution is the subject of our next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Promises about futures</h1>
                
            
            
                
<p class="calibre2">If you haven't encountered concurrent programming topics before, the last few sections probably got progressively more and more challenging. Mutexes are pretty simple to understand because they model a familiar idea from daily life: getting exclusive access to some resource by putting a lock on it. Read-write locks (<kbd class="calibre12">shared_mutex</kbd>) aren't much harder to understand. However, we then took a significant jump upward in esotericism with condition variables--which are hard to grasp partly because they seem to model not a noun (like "padlock") but a sort of prepositional verb phrase: "sleep until, but also, wake." Their opaque name doesn't help much either.</p>
<p class="calibre2">Now we continue our journey into concurrent programming with a topic that may be unfamiliar even if you've taken an undergraduate course in concurrent programming, but is well worth the learning: <em class="calibre22">promises</em> and <em class="calibre22">futures</em>.</p>
<p class="calibre2">In C++11, the types <kbd class="calibre12">std::promise&lt;T&gt;</kbd> and <kbd class="calibre12">std::future&lt;T&gt;</kbd> always appear in pairs. Someone coming from the Go language might think of a promise-future pair as a sort of <em class="calibre22">channel</em>, in that if one thread shoves a value (of type <kbd class="calibre12">T</kbd>) into the "promise" side of the pair, that value will eventually emerge at the "future" side (which is typically in a different thread by then). However, promise-future pairs are also like unstable wormholes: as soon as you've shoved a single value through the wormhole, it collapses.</p>
<p class="calibre2">We might say that a promise-future pair is like a directed, portable, one-shot wormhole. It's "directed" because you're allowed to shove data into only the "promise" side and retrieve data only via the "future" side. It's "portable" because if you own one end of the wormhole, you can move that end around and even move it between threads; you won't break the tunnel between the two ends. And it's "one-shot" because once you've shoved one piece of data into the "promise" end, you can't shove any more.</p>
<p class="calibre2">Another metaphor for the pair is suggested by their names: A <kbd class="calibre12">std::future&lt;T&gt;</kbd> is not actually a value of type <kbd class="calibre12">T</kbd>, but it is in some sense a <em class="calibre22">future</em> value--it will, at some point in the future, give you access to a <kbd class="calibre12">T</kbd>, but "not yet." (In this way, it is also something like a thread-safe <kbd class="calibre12">optional&lt;T&gt;</kbd>.) Meanwhile, a <kbd class="calibre12">std::promise&lt;T&gt;</kbd> object is like an unfulfilled promise, or an I-O-U. The holder of the promise object <em class="calibre22">promises</em> to put a value of type <kbd class="calibre12">T</kbd> into it at some point; if he doesn't ever put in a value, then he's "broken his promise."</p>
<p class="calibre2">Generally speaking, you use a promise-future pair by first creating a <kbd class="calibre12">std::promise&lt;T&gt;</kbd>, where <kbd class="calibre12">T</kbd> is the type of data you're planning to send through it; then creating the wormhole's "future" end by calling <kbd class="calibre12">p.get_future()</kbd>. When you're ready to fulfill the promise, you call <kbd class="calibre12">p.set_value(v)</kbd>. Meanwhile, in some other thread, when you're ready to retrieve the value, you call <kbd class="calibre12">f.get()</kbd>. If a thread calls <kbd class="calibre12">f.get()</kbd> before the promise has been fulfilled, that thread will block until the promise is fulfilled and the value is ready to retrieve. On the other hand, when the promise-holding thread calls <kbd class="calibre12">p.set_value(v)</kbd>, if nobody's waiting, that's fine; <kbd class="calibre12">set_value</kbd> will just record the value <kbd class="calibre12">v</kbd> in memory so that it's ready and waiting whenever anyone <em class="calibre22">does</em> ask for it via <kbd class="calibre12">f.get()</kbd>.</p>
<p class="calibre2">Let's see <kbd class="calibre12">promise</kbd> and <kbd class="calibre12">future</kbd> in action!</p>
<pre class="calibre23">    std::promise&lt;int&gt; p1, p2;<br class="title-page-name"/>    std::future&lt;int&gt; f1 = p1.get_future();<br class="title-page-name"/>    std::future&lt;int&gt; f2 = p2.get_future();<br class="title-page-name"/><br class="title-page-name"/>      // If the promise is satisfied first,<br class="title-page-name"/>      // then f.get() will not block.<br class="title-page-name"/>    p1.set_value(42);<br class="title-page-name"/>    assert(f1.get() == 42);<br class="title-page-name"/><br class="title-page-name"/>      // If f.get() is called first, then it<br class="title-page-name"/>      // will block until set_value() is called<br class="title-page-name"/>      // from some other thread.<br class="title-page-name"/>    std::thread t([&amp;](){<br class="title-page-name"/>      std::this_thread::sleep_for(100ms);<br class="title-page-name"/>      p2.set_value(43);<br class="title-page-name"/>    });<br class="title-page-name"/>    auto start_time = std::chrono::system_clock::now();<br class="title-page-name"/>    assert(f2.get() == 43);<br class="title-page-name"/>    auto elapsed = std::chrono::system_clock::now() - start_time;<br class="title-page-name"/>    printf("f2.get() took %dms.\n", count_ms(elapsed));<br class="title-page-name"/>    t.join();</pre>
<p class="calibre2">(For the definition of <kbd class="calibre12">count_ms</kbd>, see the previous section, <em class="calibre22">Special-purpose mutex types</em>.)</p>
<p class="calibre2">One nice detail about the standard library's <kbd class="calibre12">std::promise</kbd> is that it has a specialization for <kbd class="calibre12">void</kbd>. The idea of <kbd class="calibre12">std::future&lt;void&gt;</kbd> might seem a little silly at first--what good is a wormhole if the only data type you can shove through it is a type with no values? But in fact <kbd class="calibre12">future&lt;void&gt;</kbd> is extremely useful, whenever we don't care so much about the <em class="calibre22">value</em> that was received as about the fact that some signal was received at all. For example, we can use <kbd class="calibre12">std::future&lt;void&gt;</kbd> to implement yet a third version of our "wait for thread B to launch" code:</p>
<pre class="calibre23">    std::promise&lt;void&gt; ready_p;<br class="title-page-name"/>    std::future&lt;void&gt; ready_f = ready_p.get_future();<br class="title-page-name"/><br class="title-page-name"/>    std::thread thread_b([&amp;]() {<br class="title-page-name"/>      prep_work();<br class="title-page-name"/>      ready_p.set_value();<br class="title-page-name"/>      main_work();<br class="title-page-name"/>    });<br class="title-page-name"/><br class="title-page-name"/>      // Wait for thread B to be ready.<br class="title-page-name"/>    ready_f.wait();<br class="title-page-name"/>      // Now thread B has completed its prep work.</pre>
<p class="calibre2">Compare this version to the code samples from the section titled "Waiting for a condition." This version is much cleaner! There's practically no cruft, no boilerplate at all. The "signal B's readiness" and "wait for B's readiness" operations both take only a single line of code. So this is definitely the preferred way to signal between a single pair of threads, as far as syntactic cleanliness is concerned. For yet a fourth way to signal from one thread to a group of threads, see this chapter's subsection titled "Identifying individual threads and the current thread."</p>
<p class="calibre2">There <em class="calibre22">is</em> a price to pay for <kbd class="calibre12">std::future</kbd>, though. The price is dynamic memory allocation. You see, <kbd class="calibre12">promise</kbd> and <kbd class="calibre12">future</kbd> both need access to a shared storage location, so that when you store <kbd class="calibre12">42</kbd> in the promise side, you'll be able to pull it out from the future side. (That shared storage location also holds the mutex and condition variable required for synchronizing between the threads. The mutex and condition variable haven't disappeared from our code; they've just moved down a layer of abstraction so that we don't have to worry about them.) So, <kbd class="calibre12">promise</kbd> and <kbd class="calibre12">future</kbd> both act as a sort of "handle" to this shared state; but they're both movable types, so neither of them can actually hold the shared state as a member. They need to allocate the shared state on the heap, and hold pointers to it; and since the shared state isn't supposed to be freed until <em class="calibre22">both</em> handles are destroyed, we're talking about shared ownership via something like <kbd class="calibre12">shared_ptr</kbd> (see <a target="_blank" href="part0093.html#2OM4A0-2fdac365b8984feebddfbb9250eaf20d" class="calibre4">Chapter 6</a>, <em class="calibre22">Smart Pointers</em>). Schematically, <kbd class="calibre12">promise</kbd> and <kbd class="calibre12">future</kbd> look like this:</p>
<div><img src="img/00023.jpeg" class="calibre45"/></div>
<p class="calibre2">The shared state in this diagram will be allocated with <kbd class="calibre12">operator new</kbd>, unless you use a special "allocator-aware" version of the constructor <kbd class="calibre12">std::promise</kbd>. To use <kbd class="calibre12">std::promise</kbd> and <kbd class="calibre12">std::future</kbd> with an allocator of your choice, you'd write the following:</p>
<pre class="calibre23">    MyAllocator myalloc{};<br class="title-page-name"/>    std::promise&lt;int&gt; p(std::allocator_arg, myalloc);<br class="title-page-name"/>    std::future&lt;int&gt; f = p.get_future();</pre>
<p class="calibre2"><kbd class="calibre12">std::allocator_arg</kbd> is defined in the <kbd class="calibre12">&lt;memory&gt;</kbd> header. See <a target="_blank" href="part0129.html#3R0OI0-2fdac365b8984feebddfbb9250eaf20d" class="calibre4">Chapter 8</a>, <em class="calibre22">Allocators</em>, for the details of <kbd class="calibre12">MyAllocator</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Packaging up tasks for later</h1>
                
            
            
                
<p class="calibre2">Another thing to notice about the preceding diagram is that the shared state doesn't just contain an <kbd class="calibre12">optional&lt;T&gt;</kbd>; it actually contains a <kbd class="calibre12">variant&lt;T, exception_ptr&gt;</kbd> (for <kbd class="calibre12">variant</kbd> and <kbd class="calibre12">optional</kbd>, see <a target="_blank" href="part0074.html#26I9K0-2fdac365b8984feebddfbb9250eaf20d" class="calibre4">Chapter 5</a>, <em class="calibre22">Vocabulary Types</em>). This implies that not only can you shove data of type <kbd class="calibre12">T</kbd> through the wormhole; you can also shove <em class="calibre22">exceptions</em> through. This is particularly convenient and symmetrical because it allows <kbd class="calibre12">std::future&lt;T&gt;</kbd> to represent all the possible outcomes of calling a function with the signature <kbd class="calibre12">T()</kbd>. Maybe it returns a <kbd class="calibre12">T</kbd>; maybe it throws an exception; and of course maybe it never returns at all. Similarly, a call to <kbd class="calibre12">f.get()</kbd> may return a <kbd class="calibre12">T</kbd>; or throw an exception; or (if the promise-holding thread loops forever) might never return at all. In order to shove an exception through the wormhole, you'd use the method <kbd class="calibre12">p.set_exception(ex)</kbd>, where <kbd class="calibre12">ex</kbd> is an object of type <kbd class="calibre12">std::exception_ptr</kbd> such as might be returned from <kbd class="calibre12">std::current_exception()</kbd> inside a catch handler.</p>
<p class="calibre2">Let's take a function of signature <kbd class="calibre12">T()</kbd> and package it up in a future of type <kbd class="calibre12">std::future&lt;T&gt;</kbd>:</p>
<pre class="calibre23">    template&lt;class T&gt;<br class="title-page-name"/>    class simple_packaged_task {<br class="title-page-name"/>      std::function&lt;T()&gt; m_func;<br class="title-page-name"/>      std::promise&lt;T&gt; m_promise;<br class="title-page-name"/>    public:<br class="title-page-name"/>      template&lt;class F&gt;<br class="title-page-name"/>      simple_packaged_task(const F&amp; f) : m_func(f) {}<br class="title-page-name"/><br class="title-page-name"/>      auto get_future() { return m_promise.get_future(); }<br class="title-page-name"/><br class="title-page-name"/>      void operator()() {<br class="title-page-name"/>        try {<br class="title-page-name"/>          T result = m_func();<br class="title-page-name"/>          m_promise.set_value(result);<br class="title-page-name"/>        } catch (...) {<br class="title-page-name"/>          m_promise.set_exception(std::current_exception());<br class="title-page-name"/>        }<br class="title-page-name"/>      }<br class="title-page-name"/>    };</pre>
<p class="calibre2">This class superficially resembles the standard library type <kbd class="calibre12">std::packaged_task&lt;R(A...)&gt;</kbd>; the difference is that the standard library type takes arguments, and uses an extra layer of indirection to make sure that it can hold even move-only functor types. Back in <a target="_blank" href="part0074.html#26I9K0-2fdac365b8984feebddfbb9250eaf20d" class="calibre4">Chapter 5</a>, <em class="calibre22">Vocabulary Types</em>, we showed you some workarounds for the fact that <kbd class="calibre12">std::function</kbd> can't hold move-only function types; fortunately those workarounds are not needed when dealing with <kbd class="calibre12">std::packaged_task</kbd>. On the other hand, you'll probably never have to deal with <kbd class="calibre12">std::packaged_task</kbd> in your life. It's interesting mainly as an example of how to compose promises, futures, and functions together into user-friendly class types with externally very simple interfaces. Consider for a moment: The <kbd class="calibre12">simple_packaged_task</kbd> class above uses type-erasure in <kbd class="calibre12">std::function</kbd>, and then has the <kbd class="calibre12">std::promise</kbd> member, which is implemented in terms of <kbd class="calibre12">std::shared_ptr</kbd>, which does reference counting; and the shared state pointed to by that reference-counted pointer holds a mutex and a condition variable. That's quite a lot of ideas and techniques packed into a very small volume! And yet the interface to <kbd class="calibre12">simple_packaged_task</kbd> is indeed simple: construct it with a function or lambda of some kind, then call <kbd class="calibre12">pt.get_future()</kbd> to get a future that you can <kbd class="calibre12">f.get()</kbd>; and meanwhile call <kbd class="calibre12">pt()</kbd> (probably from some other thread) to actually execute the stored function and shove the result through the wormhole into <kbd class="calibre12">f.get()</kbd>.</p>
<p class="calibre2">If the stored function throws an exception, then <kbd class="calibre12">packaged_task</kbd> will catch that exception (in the promise-holding thread) and shove it into the wormhole. Then, whenever the other thread calls <kbd class="calibre12">f.get()</kbd> (or maybe it already called it and it's blocked inside <kbd class="calibre12">f.get()</kbd> right now), <kbd class="calibre12">f.get()</kbd> will throw that exception out into the future-holding thread. In other words, by using promises and futures, we can actually "teleport" exceptions across threads. The exact mechanism of this teleportation, <kbd class="calibre12">std::exception_ptr</kbd>, is unfortunately outside the scope of this book. If you do library programming in a codebase that uses a lot of exceptions, it is definitely worth becoming familiar with <kbd class="calibre12">std::exception_ptr</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">The future of futures</h1>
                
            
            
                
<p class="calibre2">As with <kbd class="calibre12">std::shared_mutex</kbd>, the standard library's own version of <kbd class="calibre12">std::future</kbd> is only half-baked. A much more complete and useful version of <kbd class="calibre12">future</kbd> is coming, perhaps in C++20, and there are very many third-party libraries that incorporate the best features of the upcoming version. The best of these libraries include <kbd class="calibre12">boost::future</kbd> and Facebook's <kbd class="calibre12">folly::Future</kbd>.</p>
<p class="calibre2">The major problem with <kbd class="calibre12">std::future</kbd> is that it requires "touching down" in a thread after each step of a potentially multi-step computation. Consider this pathological usage of <kbd class="calibre12">std::future</kbd>:</p>
<pre class="calibre23">    template&lt;class T&gt;<br class="title-page-name"/>    auto pf() {<br class="title-page-name"/>      std::promise&lt;T&gt; p;<br class="title-page-name"/>      std::future&lt;T&gt; f = p.get_future();<br class="title-page-name"/>      return std::make_pair(std::move(p), std::move(f));<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    void test() {<br class="title-page-name"/>      auto [p1, f1] = pf&lt;Connection&gt;();<br class="title-page-name"/>      auto [p2, f2] = pf&lt;Data&gt;();<br class="title-page-name"/>      auto [p3, f3] = pf&lt;Data&gt;();<br class="title-page-name"/><br class="title-page-name"/>      auto t1 = std::thread([p1 = std::move(p1)]() mutable {<br class="title-page-name"/>        Connection conn = slowly_open_connection();<br class="title-page-name"/>        p1.set_value(conn);<br class="title-page-name"/>        // DANGER: what if slowly_open_connection throws?<br class="title-page-name"/>      });<br class="title-page-name"/>      auto t2 = std::thread([p2 = std::move(p2)]() mutable {<br class="title-page-name"/>        Data data = slowly_get_data_from_disk();<br class="title-page-name"/>        p2.set_value(data);<br class="title-page-name"/>      });<br class="title-page-name"/>      auto t3 = std::thread(<br class="title-page-name"/>      [p3 = std::move(p3), f1 = std::move(f1)]() mutable {<br class="title-page-name"/>        Data data = slowly_get_data_from_connection(f1.get());<br class="title-page-name"/>        p3.set_value(data);<br class="title-page-name"/>      });<br class="title-page-name"/>      bool success = (f2.get() == f3.get());<br class="title-page-name"/><br class="title-page-name"/>      assert(success);<br class="title-page-name"/>    }</pre>
<p class="calibre2">Notice the line marked <kbd class="calibre12">DANGER</kbd>: each of the three thread bodies has the same bug, which is that they fail to catch and <kbd class="calibre12">.set_exception()</kbd> when an exception is thrown. The solution is a <kbd class="calibre12">try...catch</kbd> block, just like we used in our <kbd class="calibre12">simple_packaged_task</kbd> in the preceding section; but since that would get tedious to write out every time, the standard library provides a neat wrapper function called <kbd class="calibre12">std::async()</kbd>, which takes care of creating a promise-future pair and spawning a new thread. Using <kbd class="calibre12">std::async()</kbd>, we have this much cleaner-looking code:</p>
<pre class="calibre23">    void test() {<br class="title-page-name"/>      auto f1 = std::async(slowly_open_connection);<br class="title-page-name"/>      auto f2 = std::async(slowly_get_data_from_disk);<br class="title-page-name"/>      auto f3 = std::async([f1 = std::move(f1)]() mutable {<br class="title-page-name"/>        return slowly_get_data_from_connection(f1.get());<br class="title-page-name"/>        // No more danger.<br class="title-page-name"/>      });<br class="title-page-name"/>      bool success = (f2.get() == f3.get());<br class="title-page-name"/><br class="title-page-name"/>      assert(success);<br class="title-page-name"/>    }</pre>
<p class="calibre2">However, this code is cleaner only in its aesthetics; it's equally horrifically bad for the performance and robustness of your codebase. This is <em class="calibre22">bad</em> code!</p>
<p class="calibre2">Every time you see a <kbd class="calibre12">.get()</kbd> in that code, you should think, "What a waste of a context switch!" And every time you see a thread being spawned (whether explicitly or via <kbd class="calibre12">async</kbd>), you should think, "What a possibility for the operating system to run out of kernel threads and for my program to start throwing unexpected exceptions from the constructor of <kbd class="calibre12">std::thread</kbd>!" Instead of either of the preceding codes, we'd prefer to write something like this, in a style that might look familiar to JavaScript programmers:</p>
<pre class="calibre23">    void test() {<br class="title-page-name"/>      auto f1 = my::async(slowly_open_connection);<br class="title-page-name"/>      auto f2 = my::async(slowly_get_data_from_disk);<br class="title-page-name"/>      auto f3 = f1.then([](Connection conn) {<br class="title-page-name"/>        return slowly_get_data_from_connection(conn);<br class="title-page-name"/>      });<br class="title-page-name"/>      bool success = f2.get() == f3.get();<br class="title-page-name"/> <br class="title-page-name"/>      assert(success);<br class="title-page-name"/>    }</pre>
<p class="calibre2">Here, there are no calls to <kbd class="calibre12">.get()</kbd> except at the very end, when we have nothing to do but wait for the final answer; and there is one fewer thread spawned. Instead, before <kbd class="calibre12">f1</kbd> finishes its task, we attach a "continuation" to it, so that when <kbd class="calibre12">f1</kbd> does finish, the promise-holding thread can immediately segue right into working on the continuation task (if original task of <kbd class="calibre12">f1</kbd> threw an exception, we won't enter this continuation at all. The library should provide a symmetrical method, <kbd class="calibre12">f1.on_error(continuation)</kbd>, to deal with the exceptional codepath).</p>
<p class="calibre2">Something like this is already available in Boost; and Facebook's Folly library contains a particularly robust and fully featured implementation even better than Boost's. While we wait for C++20 to improve the situation, my advice is to use Folly if you can afford the cognitive overhead of integrating it into your build system. The single advantage of <kbd class="calibre12">std::future</kbd> is that it's standard; you'll be able to use it on just about any platform without needing to worry about downloads, include paths, or licensing terms.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Speaking of threads...</h1>
                
            
            
                
<p class="calibre2">Throughout this entire chapter, we've been using the word "thread" without ever defining exactly what we mean by it; and you've probably noticed that many of our multithreaded code examples have used the class type <kbd class="calibre12">std::thread</kbd> and the namespace <kbd class="calibre12">std::this_thread</kbd> without much explanation. We've been focusing on <em class="calibre22">how</em> to synchronize behavior between different threads of execution, but so far we have glossed over exactly <em class="calibre22">who</em> is doing the executing!</p>
<p class="calibre2">To put it another way: When execution reaches the expression <kbd class="calibre12">mtx.lock()</kbd>, where <kbd class="calibre12">mtx</kbd> is a locked mutex, the semantics of <kbd class="calibre12">std::mutex</kbd> say that the current thread of execution should block and wait. While that thread is blocked, what is happening? Our C++ program is still "in charge" of what's going on, but clearly <em class="calibre22">this particular C++ code</em> is no longer executing; so who <em class="calibre22">is</em> executing? The answer is: another thread. We specify the existence of other threads, and the code we want them to execute, by using the standard library class <kbd class="calibre12">std::thread</kbd>, defined in the <kbd class="calibre12">&lt;thread&gt;</kbd> header.</p>
<p class="calibre2">To spawn a new thread of execution, simply construct an object of type <kbd class="calibre12">std::thread</kbd>, and pass a single argument to the constructor: a lambda or function that tells you what code you want to run in the new thread. (Technically, you are allowed to pass multiple arguments; all arguments after the first will be passed along to the first argument as <em class="calibre22">its</em> function parameters, after undergoing <kbd class="calibre12">reference_wrapper</kbd> decay as described in <a target="_blank" href="part0074.html#26I9K0-2fdac365b8984feebddfbb9250eaf20d" class="calibre4">Chapter 5</a>, <em class="calibre22">Vocabulary Types</em>. As of C++11, lambdas have made the extra arguments to the <kbd class="calibre12">thread</kbd> constructor unnecessary and even error-prone; I recommend avoiding them.)</p>
<p class="calibre2">The new thread will immediately start running; if you want it to "start up paused," you'll have to build that functionality yourself using one of the synchronization tricks shown in the section titled "Waiting for a condition," or the alternative trick shown in "Identifying individual threads and the current thread."</p>
<p class="calibre2">The new thread will run through the code it's given, and when it gets to the end of the lambda or function you provided to it, it will "become joinable." This idea is very similar to what happens with <kbd class="calibre12">std::future</kbd> when it "becomes ready": the thread has completed its computation and is ready to deliver the result of that computation to you. Just as with <kbd class="calibre12">std::future&lt;void&gt;</kbd>, the result of that computation is "valueless"; but the very fact that the computation <em class="calibre22">has finished</em> is valuable nonetheless--no pun intended!</p>
<p class="calibre2">Unlike <kbd class="calibre12">std::future&lt;void&gt;</kbd>, though, it is not permitted to destroy a <kbd class="calibre12">std::thread</kbd> object without fetching that valueless result. By default, if you destroy any new thread without dealing with its result, the destructor will call <kbd class="calibre12">std::terminate</kbd>, which is to say, it will bluntly kill your program. The way to avoid this fate is to indicate to the thread that you see and acknowledge its completion--"Good job, thread, well done!"--by calling the member function <kbd class="calibre12">t.join()</kbd>. Alternatively, if you do not expect the thread to finish (for example if it is a background thread running an infinite loop) or don't care about its result (for example if it represents some short-lived "fire and forget" task), you can dismiss it to the background--"Go away, thread, I don't want to hear from you again!"--via <kbd class="calibre12">t.detach()</kbd>.</p>
<p class="calibre2">Here are some complete examples of how to use <kbd class="calibre12">std::thread</kbd>:</p>
<pre class="calibre23">    using namespace std::literals; // for "ms"<br class="title-page-name"/><br class="title-page-name"/>    std::thread a([](){<br class="title-page-name"/>      puts("Thread A says hello ~0ms");<br class="title-page-name"/>      std::this_thread::sleep_for(10ms);<br class="title-page-name"/>      puts("Thread A says goodbye ~10ms");<br class="title-page-name"/>    });<br class="title-page-name"/><br class="title-page-name"/>    std::thread b([](){<br class="title-page-name"/>      puts("Thread B says hello ~0ms");<br class="title-page-name"/>      std::this_thread::sleep_for(20ms);<br class="title-page-name"/>      puts("Thread B says goodbye ~20ms");<br class="title-page-name"/>    });<br class="title-page-name"/><br class="title-page-name"/>    puts("The main thread says hello ~0ms");<br class="title-page-name"/>    a.join(); // waits for thread A<br class="title-page-name"/>    b.detach(); // doesn't wait for thread B<br class="title-page-name"/>    puts("The main thread says goodbye ~10ms");</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Identifying individual threads and the current thread</h1>
                
            
            
                
<p class="calibre2">Objects of type <kbd class="calibre12">std::thread</kbd>, like every other type described in this chapter, do not support <kbd class="calibre12">operator==</kbd>. You can't directly ask "Are these two thread objects the same?" This also means that you can't use <kbd class="calibre12">std::thread</kbd> objects as the keys in an associative container such as <kbd class="calibre12">std::map</kbd> or <kbd class="calibre12">std::unordered_map</kbd>. However, you <em class="calibre22">can</em> ask about equality indirectly, via a feature called <em class="calibre22">thread-ids</em>.</p>
<p class="calibre2">The member function <kbd class="calibre12">t.get_id()</kbd> returns a unique identifier of type <kbd class="calibre12">std::thread::id</kbd>, which, although it is technically a class type, behaves an awful lot like an integer type. You can compare thread-ids using operators <kbd class="calibre12">&lt;</kbd> and <kbd class="calibre12">==</kbd>; and you can use thread-ids as keys in associative containers. Another valuable feature of thread-id objects is that they can be <em class="calibre22">copied</em>, unlike <kbd class="calibre12">std::thread</kbd> objects themselves, which are move-only. Remember, each <kbd class="calibre12">std::thread</kbd> object represents an actual thread of execution; if you could copy <kbd class="calibre12">thread</kbd> objects, you would be "copying" threads of execution, which doesn't make a whole lot of sense--and would certainly lead to some interesting bugs!</p>
<p class="calibre2">The third valuable feature of <kbd class="calibre12">std::thread::id</kbd> is that it is possible to get the thread-id of the <em class="calibre22">current</em> thread, or even of the main thread. From within a thread, there is no way to say "Please give me the <kbd class="calibre12">std::thread</kbd> object that manages this thread." (This would be a trick analogous to <kbd class="calibre12">std::enable_shared_from_this&lt;T&gt;</kbd> from <a target="_blank" href="part0093.html#2OM4A0-2fdac365b8984feebddfbb9250eaf20d" class="calibre4">Chapter 6</a>, <em class="calibre22">Smart Pointers</em>; but as we've seen, such a trick requires support from the part of the library that creates managed resources--which in this case would be the constructor of <kbd class="calibre12">std::thread</kbd>.) And the main thread, the thread in which <kbd class="calibre12">main</kbd> begins execution, doesn't have a corresponding <kbd class="calibre12">std::thread</kbd> object at all. But it still has a thread-id!</p>
<p class="calibre2">Finally, thread-ids are convertible in some implementation-defined manner to a string representation, which is guaranteed to be unique--that is, <kbd class="calibre12">to_string(id1) == to_string(id2)</kbd> if and only if <kbd class="calibre12">id1 == id2</kbd>. Unfortunately this string representation is exposed only via the stream operator (see <a target="_blank" href="part0144.html#49AH00-2fdac365b8984feebddfbb9250eaf20d" class="calibre4">Chapter 9</a>, <em class="calibre22">Iostreams</em>); if you want to use the syntax <kbd class="calibre12">to_string(id1)</kbd> you need to write a simple wrapper function:</p>
<pre class="calibre23">    std::string to_string(std::thread::id id)<br class="title-page-name"/>    {<br class="title-page-name"/>      std::ostringstream o;<br class="title-page-name"/>      o &lt;&lt; id;<br class="title-page-name"/>      return o.str();<br class="title-page-name"/>    }</pre>
<p class="calibre2">You can get the thread-id of the current thread (including of the main thread, if that happens to be your current thread) by calling the free function <kbd class="calibre12">std::this_thread::get_id()</kbd>. Look carefully at the syntax! <kbd class="calibre12">std::thread</kbd> is the name of a class, but <kbd class="calibre12">std::this_thread</kbd> is the name of a <em class="calibre22">namespace</em>. In this namespace live some free functions (unassociated with any C++ class instance) that manipulate the current thread. <kbd class="calibre12">get_id()</kbd> is one of those functions. Its name was chosen to be reminiscent of <kbd class="calibre12">std::thread::get_id()</kbd>, but in fact it is a completely different function: <kbd class="calibre12">thread::get_id()</kbd> is a member function and <kbd class="calibre12">this_thread::get_id()</kbd> is a free function.</p>
<p class="calibre2">Using two thread-ids, you can find out, for example, which of an existing list of threads represents your current thread:</p>
<pre class="calibre23">    std::mutex ready;<br class="title-page-name"/>    std::unique_lock lk(ready);<br class="title-page-name"/>    std::vector&lt;std::thread&gt; threads;<br class="title-page-name"/><br class="title-page-name"/>    auto task = [&amp;](){<br class="title-page-name"/>        // Block here until the main thread is ready.<br class="title-page-name"/>      (void)std::lock_guard(ready);<br class="title-page-name"/>        // Now go. Find my thread-id in the vector.<br class="title-page-name"/>      auto my_id = std::this_thread::get_id();<br class="title-page-name"/>      auto iter = std::find_if(<br class="title-page-name"/>        threads.begin(), threads.end(),<br class="title-page-name"/>        [=](const std::thread&amp; t) {<br class="title-page-name"/>          return t.get_id() == my_id;<br class="title-page-name"/>         }<br class="title-page-name"/>      );<br class="title-page-name"/>      printf("Thread %s %s in the list.\n",<br class="title-page-name"/>        to_string(my_id).c_str(),<br class="title-page-name"/>        iter != threads.end() ? "is" : "is not");<br class="title-page-name"/>    };<br class="title-page-name"/><br class="title-page-name"/>    std::vector&lt;std::thread&gt; others;<br class="title-page-name"/>    for (int i = 0; i &lt; 10; ++i) {<br class="title-page-name"/>      std::thread t(task);<br class="title-page-name"/>      if (i % 2) {<br class="title-page-name"/>        threads.push_back(std::move(t));<br class="title-page-name"/>      } else {<br class="title-page-name"/>        others.push_back(std::move(t));<br class="title-page-name"/>      }<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>      // Let all the threads run.<br class="title-page-name"/>    ready.unlock();<br class="title-page-name"/><br class="title-page-name"/>      // Join all the threads.<br class="title-page-name"/>    for (std::thread&amp; t : threads) t.join();<br class="title-page-name"/>    for (std::thread&amp; t : others) t.join();</pre>
<p class="calibre2">What you cannot do, ever, is go the other direction; you cannot reconstruct the <kbd class="calibre12">std::thread</kbd> object corresponding to a given <kbd class="calibre12">std::thread::id</kbd>. Because if you could, you'd have two different objects in your program representing that thread of execution: the original <kbd class="calibre12">std::thread</kbd> wherever it is, and the one you just reconstructed from its thread-id. And you can never have two <kbd class="calibre12">std::thread</kbd> objects controlling the same thread.</p>
<p class="calibre2">The two other free functions in the <kbd class="calibre12">std::this_thread</kbd> namespace are <kbd class="calibre12">std::this_thread::sleep_for(duration)</kbd>, which you've seen me use extensively in this chapter, and <kbd class="calibre12">std::this_thread::yield()</kbd>, which is basically the same thing as <kbd class="calibre12">sleep_for(0ms)</kbd>: it tells the runtime that it would be a good idea to context-switch to a different thread right now, but doesn't connote any <em class="calibre22">particular</em> time delay on the current thread.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Thread exhaustion and std::async</h1>
                
            
            
                
<p class="calibre2">In this chapter's section <em class="calibre22">The future of futures</em>, we introduced <kbd class="calibre12">std::async</kbd>, which is a simple wrapper around a thread constructor with the result captured into a <kbd class="calibre12">std::future</kbd>. Its implementation looks more or less like this:</p>
<pre class="calibre23">    template&lt;class F&gt;<br class="title-page-name"/>    auto async(F&amp;&amp; func) {<br class="title-page-name"/>      using ResultType = std::invoke_result_t&lt;std::decay_t&lt;F&gt;&gt;;<br class="title-page-name"/>      using PromiseType = std::promise&lt;ResultType&gt;;<br class="title-page-name"/>      using FutureType = std::future&lt;ResultType&gt;;<br class="title-page-name"/><br class="title-page-name"/>      PromiseType promise;<br class="title-page-name"/>      FutureType future = promise.get_future();<br class="title-page-name"/>      auto t = std::thread([<br class="title-page-name"/>        func = std::forward&lt;F&gt;(func),<br class="title-page-name"/>        promise = std::move(promise)<br class="title-page-name"/>      ]() mutable {<br class="title-page-name"/>        try {<br class="title-page-name"/>          ResultType result = func();<br class="title-page-name"/>           promise.set_value(result);<br class="title-page-name"/>        } catch (...) {<br class="title-page-name"/>          promise.set_exception(std::current_exception());<br class="title-page-name"/>        }<br class="title-page-name"/>      });<br class="title-page-name"/>      // This special behavior is not implementable<br class="title-page-name"/>      // outside of the library, but async does do it.<br class="title-page-name"/>      // future.on_destruction([t = std::move(t)]() {<br class="title-page-name"/>      //  t.join();<br class="title-page-name"/>      // });<br class="title-page-name"/>      return future;<br class="title-page-name"/>    }</pre>
<p class="calibre2">Notice the commented-out lines indicating a special behavior "on destruction" of the <kbd class="calibre12">std::future</kbd> returned from <kbd class="calibre12">std::async</kbd>. This is a strange and awkward behavior of the standard library's <kbd class="calibre12">std::async</kbd> implementation, and a good reason to avoid or reimplement <kbd class="calibre12">std::async</kbd> in your own code: The futures returned from <kbd class="calibre12">std::async</kbd> have destructors that call <kbd class="calibre12">.join()</kbd> on their underlying threads! This means that their destructors can block, and that the task certainly will not be "executing in the background" as you might naturally expect. If you call <kbd class="calibre12">std::async</kbd> and don't assign the returned future to a variable, the return value will be destroyed right then and there, which means ironically that a line containing nothing but a call to <kbd class="calibre12">std::async</kbd> will actually execute the specified function <em class="calibre22">synchronously:</em></p>
<pre class="calibre23">    template&lt;class F&gt;<br class="title-page-name"/>    void fire_and_forget_wrong(const F&amp; f) {<br class="title-page-name"/>      // WRONG! Runs f in another thread, but blocks anyway.<br class="title-page-name"/>      std::async(f);<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    template&lt;class F&gt;<br class="title-page-name"/>    void fire_and_forget_better(const F&amp; f) {<br class="title-page-name"/>      // BETTER! Launches f in another thread without blocking.<br class="title-page-name"/>      std::thread(f).detach();<br class="title-page-name"/>    }</pre>
<p class="calibre2">The original reason for this limitation seems to have been a concern that if <kbd class="calibre12">std::async</kbd> launched background threads in the usual way, it would lead to people overusing <kbd class="calibre12">std::async</kbd> and possibly introducing dangling-reference bugs, as in this example:</p>
<pre class="calibre23">    int test() {<br class="title-page-name"/>      int i = 0;<br class="title-page-name"/>      auto future = std::async([&amp;]() {<br class="title-page-name"/>        i += 1;<br class="title-page-name"/>      });<br class="title-page-name"/>      // suppose we do not call f.wait() here<br class="title-page-name"/>      return i;<br class="title-page-name"/>    }</pre>
<p class="calibre2">If we didn't wait for the result of this future, the function <kbd class="calibre12">test()</kbd> might return to its caller before the new thread got a chance to run; then, when the new thread did finally run and attempt to increment <kbd class="calibre12">i</kbd>, it would be accessing a stack variable that no longer existed. So, rather than run the risk of people writing such buggy code, the Standards Committee decided that <kbd class="calibre12">std::async</kbd> should return futures with special, "magic" destructors that join their threads automatically.</p>
<p class="calibre2">Anyway, overuse of <kbd class="calibre12">std::async</kbd> is problematic for other reasons as well. The biggest reason is that on all popular operating systems, <kbd class="calibre12">std::thread</kbd> represents a <em class="calibre22">kernel thread</em>--a thread whose scheduling is under the control of the OS kernel. Because the OS has only finite resources to track these threads, the number of threads available to any one process is fairly limited: often only a few tens of thousands. If you're using <kbd class="calibre12">std::async</kbd> as your thread manager, spawning a new <kbd class="calibre12">std::thread</kbd> every time you have another task that might benefit from concurrency, you'll quickly find yourself running out of kernel threads. When this happens, the constructor of <kbd class="calibre12">std::thread</kbd> will start throwing exceptions of type <kbd class="calibre12">std::system_error</kbd>, often with the text <kbd class="calibre12">Resource temporarily unavailable</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Building your own thread pool</h1>
                
            
            
                
<p class="calibre2">If you use <kbd class="calibre12">std::async</kbd> to spawn a thread every time you have a new task, you risk exhausting the kernel's number of available threads for your process. A better way to run tasks concurrently is to use a <em class="calibre22">thread pool</em>--a small number of "worker threads" whose sole job is to run tasks as they are provided by the programmer. If there are more tasks than workers, the excess tasks are placed in a <em class="calibre22">work queue</em>. Whenever a worker finishes a task, it checks the work queue for new tasks.</p>
<p class="calibre2">This is a well-known idea, but has not yet been taken up into the standard library as of C++17. However, you can combine the ideas shown in this chapter to create your own production-quality thread pool. I'll walk through a simple one here; it's not "production quality" in terms of performance, but it <em class="calibre22">is</em> properly thread-safe and correct in all its functionality. Some performance tweaks will be discussed at the end of the walkthrough.</p>
<p class="calibre2">We'll start with the member data. Notice that we are using the rule that all the data controlled by a mutex should be located together under a single visual namespace; in this case, a nested struct definition. We're also going to use <kbd class="calibre12">std::packaged_task&lt;void()&gt;</kbd> as our move-only function type; if your codebase already has a move-only function type, you'll probably want to use that instead. If you don't already have a move-only function type, consider adopting Folly's <kbd class="calibre12">folly::Function</kbd> or Denis Blank's <kbd class="calibre12">fu2::unique_function</kbd>:</p>
<pre class="calibre23">    class ThreadPool {<br class="title-page-name"/>      using UniqueFunction = std::packaged_task&lt;void()&gt;;<br class="title-page-name"/>      struct {<br class="title-page-name"/>        std::mutex mtx;<br class="title-page-name"/>        std::queue&lt;UniqueFunction&gt; work_queue;<br class="title-page-name"/>        bool aborting = false;<br class="title-page-name"/>      } m_state;<br class="title-page-name"/>      std::vector&lt;std::thread&gt; m_workers;<br class="title-page-name"/>      std::condition_variable m_cv;</pre>
<p class="calibre2">The <kbd class="calibre12">work_queue</kbd> variable will hold tasks as they come in to us. The member variable <kbd class="calibre12">m_state.aborting</kbd> will be set to <kbd class="calibre12">true</kbd> when it's time for all the workers to stop working and "come home to rest." <kbd class="calibre12">m_workers</kbd> holds the worker threads themselves; and <kbd class="calibre12">m_state.mtx</kbd> and <kbd class="calibre12">m_cv</kbd> are just for synchronization. (The workers will spend much of their time asleep when there's no work to do. When a new task comes in and we need to wake up some worker, we'll notify <kbd class="calibre12">m_cv</kbd>.)</p>
<p class="calibre2">The constructor of <kbd class="calibre12">ThreadPool</kbd> spawns worker threads and populates the <kbd class="calibre12">m_workers</kbd> vector. Each worker thread will be running the member function <kbd class="calibre12">this-&gt;worker_loop()</kbd>, which we'll see in a minute:</p>
<pre class="calibre23">    public:<br class="title-page-name"/>      ThreadPool(int size) {<br class="title-page-name"/>        for (int i=0; i &lt; size; ++i) {<br class="title-page-name"/>          m_workers.emplace_back([this]() { worker_loop(); });<br class="title-page-name"/>        }<br class="title-page-name"/>      }</pre>
<p class="calibre2">As promised, the destructor sets <kbd class="calibre12">m_state.aborting</kbd> to <kbd class="calibre12">true</kbd> and then waits for all of the worker threads to notice the change and terminate. Notice that when we touch <kbd class="calibre12">m_state.aborting</kbd>, it's only under a lock on <kbd class="calibre12">m_state.mtx</kbd>; we are following good hygiene in order to avoid bugs!</p>
<pre class="calibre23">      ~ThreadPool() {<br class="title-page-name"/>        if (std::lock_guard lk(m_state.mtx); true) {<br class="title-page-name"/>          m_state.aborting = true;<br class="title-page-name"/>        }<br class="title-page-name"/>        m_cv.notify_all();<br class="title-page-name"/>        for (std::thread&amp; t : m_workers) {<br class="title-page-name"/>          t.join();<br class="title-page-name"/>        }<br class="title-page-name"/>      }</pre>
<p class="calibre2">Now let's see how we enqueue tasks into the work queue. (We have not yet seen how workers grab tasks out; we'll see that happening in the <kbd class="calibre12">worker_loop</kbd> member function.) It's very straightforward; we just have to make sure that we access <kbd class="calibre12">m_state</kbd> only under the mutex lock, and that once we have enqueued the task, we call <kbd class="calibre12">m_cv.notify_one()</kbd> so that some worker will wake up to handle the task:</p>
<pre class="calibre23">      void enqueue_task(UniqueFunction task) {<br class="title-page-name"/>        if (std::lock_guard lk(m_state.mtx); true) {<br class="title-page-name"/>          m_state.work_queue.push(std::move(task));<br class="title-page-name"/>        }<br class="title-page-name"/>        m_cv.notify_one();<br class="title-page-name"/>      }</pre>
<p class="calibre2">At last, here is the worker loop. This is the member function that each worker runs:</p>
<pre class="calibre23">    private:<br class="title-page-name"/>      void worker_loop() {<br class="title-page-name"/>        while (true) {<br class="title-page-name"/>          std::unique_lock lk(m_state.mtx);<br class="title-page-name"/>          while (m_state.work_queue.empty() &amp;&amp; !m_state.aborting) {<br class="title-page-name"/>            m_cv.wait(lk);<br class="title-page-name"/>          }<br class="title-page-name"/>          if (m_state.aborting) break;<br class="title-page-name"/>          // Pop the next task, while still under the lock.<br class="title-page-name"/>          assert(!m_state.work_queue.empty());<br class="title-page-name"/>          UniqueFunction task = std::move(m_state.work_queue.front());<br class="title-page-name"/>          m_state.work_queue.pop();<br class="title-page-name"/><br class="title-page-name"/>          lk.unlock();<br class="title-page-name"/>          // Actually run the task. This might take a while.<br class="title-page-name"/>          task();<br class="title-page-name"/>          // When we're done with this task, go get another.<br class="title-page-name"/>        }<br class="title-page-name"/>      }</pre>
<p class="calibre2">Notice the inevitable loop around <kbd class="calibre12">m_cv.wait(lk)</kbd>, and notice that we hygienically access <kbd class="calibre12">m_state</kbd> only under the mutex lock. Also notice that when we actually call out to perform <kbd class="calibre12">task</kbd>, we release the mutex lock first; this ensures that we are not holding the lock for a very long time while the user's task executes. If we <em class="calibre22">were</em> to hold the lock for a long time, then no other worker would be able to get in and grab its next task--we'd effectively reduce the concurrency of our pool. Also, if we were to hold the lock during <kbd class="calibre12">task</kbd>, and if <kbd class="calibre12">task</kbd> itself tried to enqueue a new task on this pool (which requires taking the lock itself), then <kbd class="calibre12">task</kbd> would deadlock and our whole program would freeze up. This is a special case of the more general rule never to call a user-provided callback while holding a mutex lock: that's generally a recipe for deadlock.</p>
<p class="calibre2">Finally, let's round out our <kbd class="calibre12">ThreadPool</kbd> class by implementing a safe version of <kbd class="calibre12">async</kbd>. Our version will allow calling <kbd class="calibre12">tp.async(f)</kbd> for any <kbd class="calibre12">f</kbd> that is callable without arguments, and just like <kbd class="calibre12">std::async</kbd>, we'll return a <kbd class="calibre12">std::future</kbd> via which our caller can retrieve the result of <kbd class="calibre12">f</kbd> once it's ready. Unlike the futures returned from <kbd class="calibre12">std::async</kbd>, our futures will be safe to drop on the floor: If the caller decides that he doesn't want to wait for the result after all, the task will remain enqueued and will eventually be executed, and the result will simply be ignored:</p>
<pre class="calibre23">    public:<br class="title-page-name"/>      template&lt;class F&gt;<br class="title-page-name"/>      auto async(F&amp;&amp; func) {<br class="title-page-name"/>        using ResultType = std::invoke_result_t&lt;std::decay_t&lt;F&gt;&gt;;<br class="title-page-name"/><br class="title-page-name"/>        std::packaged_task&lt;ResultType()&gt; pt(std::forward&lt;F&gt;(func));<br class="title-page-name"/>        std::future&lt;ResultType&gt; future = pt.get_future();<br class="title-page-name"/><br class="title-page-name"/>        UniqueFunction task(<br class="title-page-name"/>           [pt = std::move(pt)]() mutable { pt(); }<br class="title-page-name"/>        );<br class="title-page-name"/><br class="title-page-name"/>        enqueue_task(std::move(task));<br class="title-page-name"/><br class="title-page-name"/>        // Give the user a future for retrieving the result.<br class="title-page-name"/>        return future;<br class="title-page-name"/>      }<br class="title-page-name"/>    }; // class ThreadPool</pre>
<p class="calibre2">We can use our <kbd class="calibre12">ThreadPool</kbd> class to write code like this function, which creates 60,000 tasks:</p>
<pre class="calibre23">    void test() {<br class="title-page-name"/>      std::atomic&lt;int&gt; sum(0);<br class="title-page-name"/>      ThreadPool tp(4);<br class="title-page-name"/>      std::vector&lt;std::future&lt;int&gt;&gt; futures;<br class="title-page-name"/>      for (int i=0; i &lt; 60000; ++i) {<br class="title-page-name"/>        auto f = tp.async([i, &amp;sum](){<br class="title-page-name"/>          sum += i;<br class="title-page-name"/>          return i;<br class="title-page-name"/>        });<br class="title-page-name"/>        futures.push_back(std::move(f));<br class="title-page-name"/>      }<br class="title-page-name"/>      assert(futures[42].get() == 42);<br class="title-page-name"/>      assert(903 &lt;= sum &amp;&amp; sum &lt;= 1799970000);<br class="title-page-name"/>    }</pre>
<p class="calibre2">We could try to do the same with <kbd class="calibre12">std::async</kbd>, but we'd likely run into thread exhaustion when we tried to create 60,000 kernel threads. The preceding example uses only four kernel threads, as indicated by the parameter to the <kbd class="calibre12">ThreadPool</kbd> constructor.</p>
<p class="calibre2">When you run this code, you'll see at least the numbers 0 through 42 printed to standard output, in some order. We know that 42 must be printed because the function definitely waits for <kbd class="calibre12">futures[42]</kbd> to be ready before it exits, and all the previous numbers must be printed because their tasks were placed in the work queue ahead of task number 42. The numbers 43 through 59,999 might or might not be printed, depending on the scheduler; because as soon as task 42 is completed, we exit <kbd class="calibre12">test</kbd> and thus destroy the thread pool. The thread pool's destructor, as we've seen, notifies all of its workers to stop working and come home after they complete their current tasks. So it is likely that we'll see a few more numbers printed, but then all the workers will come home and the remaining tasks will be dropped on the floor.</p>
<p class="calibre2">Of course if you wanted the destructor of <kbd class="calibre12">ThreadPool</kbd> to block until all enqueued tasks were completed, you could do that, by changing the code of the destructor. However, typically when you're destroying a thread pool, it's because your program (such as a web server) is exiting, and that's because you've received a signal such as the user pressing <em class="calibre22">Ctrl</em> + <em class="calibre22">C</em>. In that situation, you <em class="calibre22">probably</em> want to exit as soon as you can, as opposed to trying to clear the queue. Personally, I'd prefer to add a member function <kbd class="calibre12">tp.wait_for_all_enqueued_tasks()</kbd>, so that the user of the thread pool could decide whether they want to block or just drop everything on the floor.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Improving our thread pool's performance</h1>
                
            
            
                
<p class="calibre2">The biggest performance bottleneck in our <kbd class="calibre12">ThreadPool</kbd> is that every worker thread is vying for the same mutex, <kbd class="calibre12">this-&gt;m_state.mtx</kbd>. The reason they're all contending that mutex is because that is the mutex that guards <kbd class="calibre12">this-&gt;m_state.work_queue</kbd>, and every worker needs to touch that queue in order to find out its next job. So one way to reduce contention and speed up our program is to find a way of distributing work to our workers that doesn't involve a single central work queue.</p>
<p class="calibre2">The simplest solution is to give each worker its own "to-do list"; that is, to replace our single <kbd class="calibre12">std::queue&lt;Task&gt;</kbd> with a whole <kbd class="calibre12">std::vector&lt;std::queue&lt;Task&gt;&gt;</kbd>, with one entry for each worker thread. Of course then we'd also need a <kbd class="calibre12">std::vector&lt;std::mutex&gt;</kbd> so that we had one mutex for each work queue. The <kbd class="calibre12">enqueue_task</kbd> function distributes tasks to the work queues in a round-robin fashion (using atomic increments of a <kbd class="calibre12">std::atomic&lt;int&gt;</kbd> counter to deal with simultaneous enqueues).</p>
<p class="calibre2">You could alternatively use a <kbd class="calibre12">thread_local</kbd> counter per enqueuing thread, if you are fortunate enough to work on a platform that supports C++11's <kbd class="calibre12">thread_local</kbd> keyword. On x86-64 POSIX platforms, access to a <kbd class="calibre12">thread_local</kbd> variable is approximately as fast as access to a plain old global variable; all the complication of setting up thread-local variables happens under the hood and only when you spawn a new thread. However, because that complication <em class="calibre22">does</em> exist and needs runtime support, many platforms do not yet support the <kbd class="calibre12">thread_local</kbd> storage class specifier. (On those that do, <kbd class="calibre12">thread_local int x</kbd> is basically the same thing as <kbd class="calibre12">static int x</kbd>, except that when your code accesses <kbd class="calibre12">x</kbd> by name, the actual memory address of <kbd class="calibre12">x</kbd> will vary depending on <kbd class="calibre12">std::this_thread::get_id()</kbd>. In principle, there is a whole array of <kbd class="calibre12">x</kbd> somewhere behind the scenes, indexed by thread-id and populated by the C++ runtime as threads are created and destroyed.)</p>
<p class="calibre2">The next significant performance improvement to our <kbd class="calibre12">ThreadPool</kbd> would be "work-stealing": now that each worker has its own to-do list, it might happen by chance or malice that one worker becomes overworked while all the other workers lie idle. In this case, we want the idle workers to scan the queues of the busy workers and "steal" tasks if possible. This re-introduces lock contention among the workers, but only when an inequitable assignment of tasks has already produced inefficiency--inefficiency which we are hoping to <em class="calibre22">correct</em> via work-stealing.</p>
<p class="calibre2">Implementing separate work queues and work-stealing is left as an exercise for the reader; but I hope that after seeing how simple the basic <kbd class="calibre12">ThreadPool</kbd> turned out, you won't be too daunted by the idea of modifying it to include those extra features.</p>
<p class="calibre2">Of course, there also exists professionally written thread-pool classes. Boost.Asio contains one, for example, and Asio is on track to be brought into the standard perhaps in C++20. Using Boost.Asio, our <kbd class="calibre12">ThreadPool</kbd> class would look like this:</p>
<pre class="calibre23">    class ThreadPool {<br class="title-page-name"/>      boost::thread_group m_workers;<br class="title-page-name"/>      boost::asio::io_service m_io;<br class="title-page-name"/>      boost::asio::io_service::work m_work;<br class="title-page-name"/>    public:<br class="title-page-name"/>      ThreadPool(int size) : m_work(m_io) {<br class="title-page-name"/>        for (int i=0; i &lt; size; ++i) {<br class="title-page-name"/>          m_workers.create_thread([&amp;](){ m_io.run(); });<br class="title-page-name"/>        }<br class="title-page-name"/>      }<br class="title-page-name"/><br class="title-page-name"/>      template&lt;class F&gt;<br class="title-page-name"/>      void enqueue_task(F&amp;&amp; func) {<br class="title-page-name"/>        m_io.post(std::forward&lt;F&gt;(func));<br class="title-page-name"/>      }<br class="title-page-name"/><br class="title-page-name"/>      ~ThreadPool() {<br class="title-page-name"/>        m_io.stop();<br class="title-page-name"/>        m_workers.join_all();<br class="title-page-name"/>      }<br class="title-page-name"/>    };</pre>
<p class="calibre2">An explanation of Boost.Asio is, of course, far outside the scope of this book.</p>
<p class="calibre2">Any time you use a thread pool, be careful that the tasks you enqueue never block indefinitely on conditions controlled by other tasks in the same thread pool. A classic example would be a task A that waits on a condition variable, expecting that some later task B will notify the condition variable. If you make a <kbd class="calibre12">ThreadPool</kbd> of size 4 and enqueue four copies of task A followed by four copies of task B, you'll find that task B never runs--the four worker threads in your pool are all occupied by the four copies of task A, which are all asleep waiting for a signal that will never come! "Handling" this scenario is tantamount to writing your own user-space threading library; if you don't want to get into that business, then the only sane answer is to be careful that the scenario cannot arise in the first place.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="calibre2">Multithreading is a difficult and subtle subject, with many pitfalls that are obvious only in hindsight. In this chapter we have learned:</p>
<p class="calibre2"><kbd class="calibre12">volatile</kbd>, while useful for dealing directly with hardware, is insufficient for thread-safety. <kbd class="calibre12">std::atomic&lt;T&gt;</kbd> for scalar <kbd class="calibre12">T</kbd> (up to the size of a machine register) is the right way to access shared data without races and without locks. The most important primitive atomic operation is compare-and-swap, which in C++ is spelled <kbd class="calibre12">compare_exchange_weak</kbd>.</p>
<p class="calibre2">To force threads to take turns accessing shared non-atomic data, we use <kbd class="calibre12">std::mutex</kbd>. Always lock mutexes via an RAII class such as <kbd class="calibre12">std::unique_lock&lt;M&gt;</kbd>. Remember that although C++17 class template argument deduction allows us to omit the <kbd class="calibre12">&lt;M&gt;</kbd> from these templates' names, that is just a syntactic convenience; they remain template classes.</p>
<p class="calibre2">Always clearly indicate which data is controlled by each mutex in your program. One good way to do this is with a nested struct definition.</p>
<p class="calibre2"><kbd class="calibre12">std::condition_variable</kbd> allows us to "sleep until" some condition is satisfied. If the condition can be satisfied only once, such as a thread becoming "ready," then you probably want to use a promise-future pair instead of a condition variable. If the condition can be satisfied over and over again, consider whether your problem can be rephrased in terms of the <em class="calibre22">work queue</em> pattern.</p>
<p class="calibre2"><kbd class="calibre12">std::thread</kbd> reifies the idea of a thread of execution. The "current thread" is not directly manipulable as a <kbd class="calibre12">std::thread</kbd> object, but a limited set of operations are available as free functions in the <kbd class="calibre12">std::this_thread</kbd> namespace. The most important of these operations are <kbd class="calibre12">sleep_for</kbd> and <kbd class="calibre12">get_id</kbd>. Each <kbd class="calibre12">std::thread</kbd> must always be joined or detached before it can be destroyed. Detaching is useful only for background threads that you will never need to shut down cleanly.</p>
<p class="calibre2">The standard function <kbd class="calibre12">std::async</kbd> takes a function or lambda for execution on some other thread, and returns a <kbd class="calibre12">std::future</kbd> that becomes ready when the function is done executing. While <kbd class="calibre12">std::async</kbd> itself is fatally flawed (destructors that <kbd class="calibre12">join</kbd>; kernel thread exhaustion) and thus should not be used in production code, the general idea of dealing with concurrency via futures is a good one. Prefer to use an implementation of promises and futures that supports the <kbd class="calibre12">.then</kbd> method. Folly's implementation is the best.</p>
<p class="calibre2"><em class="calibre22">Multithreading is a difficult and subtle subject, with many pitfalls that are</em> <em class="calibre22">obvious only in hindsight.</em></p>


            

            
        
    </body></html>