- en: Parallel Programming Patterns in CUDA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA中的并行编程模式
- en: In this chapter, we will cover parallel programming algorithms that will help
    you understand how to parallelize different algorithms and optimize CUDA. The
    techniques we will cover in this chapter can be applied to a variety of problems,
    for example, the parallel reduction problem we looked at in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA
    Thread Programming*, which can be used to design an efficient softmax layer in
    neural network operations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖并行编程算法，这将帮助您了解如何并行化不同的算法并优化CUDA。本章中我们将涵盖的技术可以应用于各种问题，例如我们在[第3章](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml)中看到的并行减少问题，*CUDA线程编程*，它可以用于设计神经网络操作中的高效softmax层。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Matrix multiplication optimization
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵乘法优化
- en: Image convolution
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像卷积
- en: Prefix sum
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前缀和
- en: Pack and split
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打包和拆分
- en: N-body operation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N体操作
- en: QuickSort in CUDA using dynamic parallelism
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CUDA中使用动态并行性进行快速排序
- en: Radix sort
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基数排序
- en: Histogram calculation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直方图计算
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To complete this chapter, it is recommended that you use an NVIDIA GPU card
    later than the Pascal architecture. In other words, your GPU's compute capability
    should be equal to or greater than 60\. If you are unsure of your GPU's architecture,
    please visit NVIDIA GPU's site ([https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus))
    and confirm your GPU's compute capability.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成本章，建议您使用Pascal架构之后的NVIDIA GPU卡。换句话说，您的GPU的计算能力应等于或大于60。如果您不确定您的GPU的架构，请访问NVIDIA
    GPU的网站（[https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)）并确认您的GPU的计算能力。
- en: The same codes in this chapter have been developed and tested with CUDA version
    10.1\. In general, it is recommended to use the latest CUDA version if applicable.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的相同代码已经使用CUDA版本10.1进行开发和测试。一般来说，如果适用的话，建议使用最新的CUDA版本。
- en: Matrix multiplication optimization
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵乘法优化
- en: Although we have used matrix multiplication code in many examples, we didn't
    investigate whether the operation was optimized. Now, let's review its operation
    and how we can find an opportunity for optimization.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在许多示例中使用了矩阵乘法代码，但我们并没有调查操作是否被优化。现在，让我们回顾其操作以及如何找到优化的机会。
- en: 'Matrix multiplication is a group of dot product operations from two matrices.
    We can simply parallelize the operations that are done by all the CUDA threads
    to generate a dot product of elements. However, this operation is inefficient
    in terms of memory usage because the data that''s loaded from memory isn''t reused. To
    confirm our analogy, let''s measure the performance limiter. The following chart shows
    the GPU utilization for a Tesla V100 card using Nsight Compute:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法是从两个矩阵进行的一组点积运算。我们可以简单地并行化所有CUDA线程执行的操作，以生成元素的点积。然而，从内存使用的角度来看，这种操作效率低，因为从内存加载的数据没有被重复使用。为了确认我们的类比，让我们测量性能限制器。以下图表显示了使用NVIDIA
    Nsight Compute的Tesla V100卡的GPU利用率：
- en: '![](img/82f700d0-9e74-4188-b49c-dc58c1cd2baf.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82f700d0-9e74-4188-b49c-dc58c1cd2baf.png)'
- en: 'Based on our performance limiter analysis, this utilization ratio can be categorized
    as memory bounded. Therefore, we should review the memory utilization to mitigate
    utilization. The following screenshot shows the memory workload analysis section:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的性能限制器分析，这种利用率可以归类为内存受限。因此，我们应该审查内存利用率以减少利用率。以下截图显示了内存工作负载分析部分：
- en: '![](img/2b579069-63f5-4a3c-bb71-e5ce6de69cb5.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b579069-63f5-4a3c-bb71-e5ce6de69cb5.png)'
- en: From this analysis, we can see that the L2 cache hit rate is low and that the
    max bandwidth is low. We can presume that this is because the original matrix
    multiplication operation does not reuse loaded data, as we mentioned earlier.
    This can be resolved by using shared memory, that is, reusing the loaded data
    and mitigating global memory usage. Now, let's review matrix multiplication and
    how we can optimize this to use shared memory that has a small memory space.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个分析，我们可以看到L2缓存命中率低，最大带宽也低。我们可以推测这是因为原始矩阵乘法操作没有重复使用加载的数据，正如我们之前提到的。这可以通过使用共享内存来解决，即重复使用加载的数据并减少全局内存使用。现在，让我们回顾矩阵乘法以及如何优化使用具有小内存空间的共享内存。
- en: 'Matrix multiplication is a group of dot product operations with some small
    size matrices and a cumulation of output. The small matrices are called tiles
    and they map to the matrices along the output matrix. Each tile will compute its
    own output in parallel. This operation can be implemented in the following steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法是一组点积运算，使用一些小尺寸矩阵和输出的累积。小矩阵称为瓦片，它们映射到输出矩阵上。每个瓦片将并行计算自己的输出。这个操作可以按以下步骤实现：
- en: Determine the tile size for two input and output matrices.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定两个输入和输出矩阵的瓦片大小。
- en: Traverse the input tiles, along with their direction (matrix A goes to the right,
    and matrix B goes down).
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历输入瓦片，以及它们的方向（矩阵A向右移动，矩阵B向下移动）。
- en: Compute matrix multiplication within the tile.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在瓦片内计算矩阵乘法。
- en: Continue the second step until the tile reaches the end.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续第二步，直到瓦片达到末尾。
- en: Flush the output.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 刷新输出。
- en: 'The following diagram shows the concept of tiled matrix multiplication:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了瓦片矩阵乘法的概念：
- en: '![](img/320ce4a9-f221-4986-9909-716c76e4f6ad.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/320ce4a9-f221-4986-9909-716c76e4f6ad.png)'
- en: In the preceding diagram, we compute a matrix multiplication, *C = AB*. We compute
    a smaller matrix multiplication as a tile, in green, from matrix A and matrix
    B. Then, we traverse the input tile position, respectively. The operation result
    is accumulated to the previous output to generate the matrix multiplication's
    output.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们计算矩阵乘法，*C = AB*。我们从矩阵A和矩阵B中计算一个较小的矩阵乘法作为瓦片（绿色）。然后，我们分别遍历输入瓦片位置。操作结果累积到先前的输出，以生成矩阵乘法的输出。
- en: This operation provides an optimization opportunity because we can break down
    the large matrix operation with the small problems and place it in the small memory
    space. In CUDA programming, we place the small matrices in shared memory and mitigate
    global memory access. In our implementation, we will match the tile with the CUDA
    thread block. The tile's position will be determined by its block index, which
    is done with the `tid_*` variable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作提供了一个优化机会，因为我们可以将大矩阵操作分解为小问题，并将其放置在小内存空间中。在CUDA编程中，我们将小矩阵放置在共享内存中，并减少全局内存访问。在我们的实现中，我们将瓦片与CUDA线程块匹配。瓦片的位置将由其块索引确定，这是通过`tid_*`变量完成的。
- en: Implementation of the tiling approach
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现平铺方法
- en: 'Now, let''s implement an optimized matrix multiplication using the tiled approach. We
    will reuse the previous matrix multiplication sample code that we used in [Chapter
    3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA Thread Programming*. After
    optimization, we will look at how performance can be enhanced. Follow these steps
    to get started:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用平铺方法实现优化的矩阵乘法。我们将重用之前在[第3章](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml)中使用的矩阵乘法示例代码，即CUDA线程编程。优化后，我们将看看如何提高性能。按照以下步骤开始：
- en: 'Let''s create a kernel function that will be our optimized version of matrix
    multiplication. We will name the kernel function `v2` in the `sgemm` operation.
    This kernel function will compute ![](img/7697ab42-f52b-40c7-971b-aaa358bf19e7.png),
    so we should provide the related parameters, respectively. We will also pass the
    matrix size information with `M`, `N`, and `K`:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个核函数，这将是我们优化版本的矩阵乘法。我们将在`sgemm`操作中命名核函数为`v2`。这个核函数将计算![](img/7697ab42-f52b-40c7-971b-aaa358bf19e7.png)，因此我们应该分别提供相关参数。我们还将使用`M`、`N`和`K`传递矩阵大小信息：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For this operation, we will use the block index and the thread index separately.
    As we discussed earlier, we need to use the block index separately to designate
    the tile position. We will use the thread index for the tile-level matrix multiplication.
    Therefore, we need to create the CUDA index parameter, as follows:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个操作，我们将分别使用块索引和线程索引。正如我们之前讨论的，我们需要单独使用块索引来指定瓦片位置。我们将使用线程索引进行瓦片级矩阵乘法。因此，我们需要创建CUDA索引参数，如下所示：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After that, we will use shared memory as tiles and use a local register to
    save the output value:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们将使用共享内存作为瓦片，并使用本地寄存器保存输出值：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we will write a loop that controls the tiles'' position. Here is the
    for loop code that controls a loop based on its block size. Be aware that the
    loop size is determined by `K` considering how many times blocks should be traversed:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将编写一个控制瓦片位置的循环。以下是基于其块大小控制循环的for循环代码。请注意，循环大小由`K`决定，考虑到块应该遍历多少次：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we will write code that feeds data in the second loop. As we discussed
    earlier, each tile has its own moving direction, along with the matrices; tile
    `A` traverses the column of matrix `A` and tile `B` traverses the row of matrix
    `B`. We place them according to the diagram shown in the *Matrix multiplication
    optimization* section. After that, we should place `__syncthreads()` after copying
    data from global memory to shared memory to avoid un-updated data from the previous
    iteration:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将编写代码，将数据输入第二个循环。正如我们之前讨论的，每个瓦片都有自己的移动方向，以及矩阵；瓦片`A`遍历矩阵`A`的列，瓦片`B`遍历矩阵`B`的行。我们根据*矩阵乘法优化*部分中显示的图表来放置它们。之后，我们应该在从全局内存复制数据到共享内存后放置`__syncthreads()`，以避免来自上一次迭代的未更新数据：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we can write matrix multiplication code from the tiles. The local variable
    known as `element_c` will cumulate the result:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以从瓦片中编写矩阵乘法代码。名为`element_c`的本地变量将累积结果：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will write the result into global memory. The following operation should
    be placed after the second loop finishes:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将结果写入全局内存。以下操作应该放置在第二个循环完成后：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s review how this tiling approach benefits the matrix multiplication
    operation. By using shared memory in our tiled matrix multiplication, we can expect
    that we will reduce the global memory traffic by using the input data and thus
    enhancing performance. We can confirm this with the profile result easily:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下这种平铺方法如何有利于矩阵乘法操作。通过在我们的平铺矩阵乘法中使用共享内存，我们可以期望通过使用输入数据减少全局内存流量，从而增强性能。我们可以轻松地通过配置文件结果来确认这一点：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Since we designed the kernel to reuse input data, the increased block size
    may help with performance. For instance, a 32 x 32 block size can be optimal considering
    the warp size and the number of shared memory banks to avoid bank conflicts. We
    can easily obtain its experiment result using the profile:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们设计了核心以重用输入数据，增加的块大小可能有助于性能。例如，考虑到warp大小和共享内存银行的数量，32 x 32的块大小可能是最佳的，以避免银行冲突。我们可以轻松地使用配置文件获得其实验结果：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, the increased tile size benefits the matrix multiplication operation's
    performance. Now, let's analyze its performance.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，增加的瓦片大小有利于矩阵乘法操作的性能。现在，让我们分析其性能。
- en: Performance analysis of the tiling approach
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平铺方法的性能分析
- en: Previously, we looked at the tiling approach and how it can achieve good performance. Let's
    review what the tiling approach resolves and look at what steps we can take next.
    Covering this part is optional in general because NVIDIA provides the cuBLAS and
    CUTLASS libraries for the `GEMM` (short for **General Matrix Multiply**) operation
    to provide optimized performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们看过了平铺方法以及它如何能够实现良好的性能。让我们回顾一下平铺方法解决了什么问题，并看看接下来我们可以采取哪些步骤。总的来说，覆盖这部分是可选的，因为NVIDIA提供了cuBLAS和CUTLASS库，用于提供优化性能的GEMM（**通用矩阵乘法**）操作。
- en: 'The following chart shows the updated GPU utilization report from Nsight Compute.
    The updated utilization output from the lower profile is a result of the upper
    profile:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了来自NVIDIA Nsight Compute的更新后的GPU利用率报告。较低配置文件的更新利用率输出是上配置文件的结果：
- en: '![](img/c864bc30-a77c-4fcf-8261-00d776982d81.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c864bc30-a77c-4fcf-8261-00d776982d81.png)'
- en: 'As both resources scored high utilization, we should review each one''s resource
    usage. First of all, let''s review the memory workload. The following screenshot
    shows the updated result:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两个资源都利用率很高，我们应该审查每个资源的资源使用情况。首先，让我们来审查内存工作量。以下截图显示了更新后的结果：
- en: '![](img/113b5a48-87e6-47cf-b505-348afeaffec6.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/113b5a48-87e6-47cf-b505-348afeaffec6.png)'
- en: From this result, we can see that global memory access is optimized from maximized
    memory bandwidth and reduced memory throughput. Also, the L2 cache hit rate is
    enhanced. So, our tiling approach transforms matrix multiplication from global
    memory into the on-chip-level operation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个结果可以看出，全局内存访问从最大化内存带宽和减少内存吞吐量进行了优化。此外，L2缓存命中率也得到了提高。因此，我们的平铺方法将矩阵乘法从全局内存转换为芯片级操作。
- en: 'However, this does not mean that we achieved the most optimized performance.
    From the memory workload analysis, we can see that the memory pipes are too busy.
    This is due to our element-wise multiplication from shared memory. To resolve
    this issue, we need to remap the data in shared memory. We will not cover that
    in this book, but you can learn about it in this article: [https://github.com/NervanaSystems/maxas/wiki/SGEMM](https://github.com/NervanaSystems/maxas/wiki/SGEMM).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不意味着我们已经实现了最优化的性能。从内存工作量分析中，我们可以看到内存管道太忙了。这是由于我们从共享内存进行逐元素乘法。为了解决这个问题，我们需要重新映射共享内存中的数据。我们不会在这本书中涉及到这个问题，但你可以在这篇文章中了解到：[https://github.com/NervanaSystems/maxas/wiki/SGEMM](https://github.com/NervanaSystems/maxas/wiki/SGEMM)。
- en: As we discussed earlier, the cuBLAS library shows much faster performance. We
    will cover its usage in the *cuBLAS* section in [Chapter 8](a4f84b40-7530-4ad1-83be-d4de09b071bf.xhtml),
    *Programming with Libraries and Other Languages*. However, understanding the tiling
    approach at this stage is useful so that we can understand how GPUs can begin
    optimization.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，cuBLAS库显示出更快的性能。我们将在[第8章](a4f84b40-7530-4ad1-83be-d4de09b071bf.xhtml)的*cuBLAS*部分中介绍其用法。然而，在这个阶段理解平铺方法是有用的，这样我们就可以理解GPU如何开始优化。
- en: Convolution
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积
- en: The convolutional operation (or filtering) is another common operation in many
    applications, especially in image and signal processing, as well as deep learning.
    Although this operation is based on the product of sequential data from the input
    and filter, we have a different approach for matrix multiplication.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作（或滤波）是许多应用中常见的操作，特别是在图像和信号处理以及深度学习中。虽然这个操作是基于输入和滤波器的顺序数据的乘积，但我们对矩阵乘法有不同的方法。
- en: Convolution operation in CUDA
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA中的卷积操作
- en: 'The convolutional operation consists of source data and a filter. The filter
    is also known as a kernel. By applying the filter against the input data, we can
    obtain the modified result. A two-dimensional convolution is shown in the following
    diagram:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作包括源数据和滤波器。滤波器也被称为核。通过将滤波器应用于输入数据，我们可以获得修改后的结果。下图显示了二维卷积的示意图：
- en: '![](img/3992c61c-8920-487f-8b8a-f38031e6cd8e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3992c61c-8920-487f-8b8a-f38031e6cd8e.png)'
- en: We need to consider a couple of concepts when we implement convolution operation,
    that is, kernel and padding. The kernel is a set of coefficients that we want
    to apply to the source data. This is also known as a filter. The padding is extra
    virtual space around the source data so that we can apply kernel functions to
    the edge. When the padding size is 0, we don't allow the filter to move beyond
    the source space. However, in general, the padding size is half the size of the
    filter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们实现卷积操作时，我们需要考虑一些概念，即核和填充。核是一组我们想要应用到源数据的系数。这也被称为滤波器。填充是源数据周围的额外虚拟空间，以便我们可以将核函数应用到边缘。当填充大小为0时，我们不允许滤波器移动超出源空间。然而，一般来说，填充大小是滤波器大小的一半。
- en: 'To start easily, we can design the kernel function with the following in mind:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了轻松开始，我们可以考虑以下几点来设计核函数：
- en: Each CUDA thread generates one filtered output.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个CUDA线程生成一个滤波输出。
- en: Each CUDA thread applies the filter's coefficients to the data.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个CUDA线程将滤波器的系数应用于数据。
- en: The filter shape is a box filter.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器的形状是盒状滤波器。
- en: 'Following these conditions, we can have a simple convolutional operation filter
    like this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在满足这些条件的情况下，我们可以有一个简单的卷积操作滤波器，如下所示：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This kernel function fetches input data and a filter for the very operation
    and does not reuse all the data. Considering the performance impact from memory
    inefficiency, we need to design our kernel code so that we can reuse the loaded
    data. Now, let's write the optimized version of convolution.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个核函数获取输入数据和滤波器进行操作，并没有重用所有数据。考虑到内存效率带来的性能影响，我们需要设计我们的核心代码，以便可以重用加载的数据。现在，让我们编写卷积的优化版本。
- en: Optimization strategy
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化策略
- en: First of all, the convolution filter is a read-only matrix and is used by all
    the CUDA threads. In this case, we can use CUDA's constant memory to utilize its
    cache operation with the broadcasting operation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，卷积滤波器是一个只读矩阵，并且被所有CUDA线程使用。在这种情况下，我们可以使用CUDA的常量内存来利用其缓存操作和广播操作。
- en: In convolution implementation design, we use the tiling approach, and each tile
    will generate the filtered output to the mapped position. Our tile design has
    extra space to consider the convolution filter size, which provides the required
    data for the convolution operation. This extra space is called **padding**. The
    following diagram shows an example of a thread block with a 6 x 6 dimension and
    a filter that's 3 x 3 size.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积实现设计中，我们使用平铺方法，每个平铺将生成映射位置的滤波输出。我们的平铺设计有额外的空间来考虑卷积滤波器的大小，这为卷积操作提供了所需的数据。这个额外的空间被称为**填充**。下图显示了一个具有6
    x 6维度和3 x 3大小滤波器的线程块的示例。
- en: 'Then, we need to have an 8 x 8 sized tile on shared memory for each thread
    block, as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要为每个线程块在共享内存上有一个8 x 8大小的平铺，如下所示：
- en: '![](img/519d0f1f-3b3b-48dd-8606-676e66d417da.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/519d0f1f-3b3b-48dd-8606-676e66d417da.png)'
- en: 'The pad area can be input data when the source''s addresses are invalid memory
    space, or they are filled with zero (zero-padding approach). By doing this, we
    can make the tile replace the input global memory with no additional effect on
    the boundary elements. To fill the tile, we iterate over the tile with the thread
    block size, and determine which value should be filled by checking the boundary
    condition of the input data. Our implementation sets the input data as a multiple
    of the tile size so that the boundary condition matches with the pad space of
    each thread block''s tile. A brief diagram of mapping the source data to the tile
    is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当源地址无效内存空间时，或者填充为零（零填充方法）时，填充区域可以是输入数据。通过这样做，我们可以使瓷砖替换输入全局内存而不会对边界元素产生额外影响。为了填充瓷砖，我们使用线程块大小迭代瓷砖，并通过检查输入数据的边界条件来确定应该填充哪个值。我们的实现将输入数据设置为瓷砖大小的倍数，以便边界条件与每个线程块的瓷砖的填充空间匹配。将源数据映射到瓷砖的简要图示如下：
- en: '![](img/93e820c5-8149-4d4c-adab-98325e1e6512.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93e820c5-8149-4d4c-adab-98325e1e6512.png)'
- en: 'In this design, the number of iterations we need to do to fill the tile is
    four. However, this should be changed depending on the filter size. This way,
    the number of iterations to fill the tile is determined by the number of the ceiling
    of tile size, divided by the thread block size. Its implementation is simple,
    as shown in the following code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计中，我们需要做的迭代次数来填充瓷砖是四次。然而，这应该根据滤波器大小进行更改。这样，填充瓷砖的迭代次数由瓷砖大小的上限除以线程块大小确定。其实现很简单，如下面的代码所示：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, let's implement the optimized convolution operation using shared memory
    as a box filter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用共享内存作为盒式滤波器来实现优化的卷积操作。
- en: Filtering coefficients optimization using constant memory
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用常量内存优化滤波系数
- en: Firstly, we will learn how to optimize filter coefficient data usage.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将学习如何优化滤波系数数据的使用。
- en: 'We will make a modified version of `convolution_kernel()`. Let''s duplicate
    the kernel code and rename one of them as `convolution_kernel_v2()`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将制作`convolution_kernel()`的修改版本。让我们复制内核代码，并将其中一个重命名为`convolution_kernel_v2()`：
- en: 'First, we will create a constant memory space to store the filter coefficients.
    The constant memory''s size is limited and we can''t make modifications to the
    kernel code. However, we can use this constant memory since our convolutional
    filter is suitable for this condition. We can use constant memory like so:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个常量内存空间来存储滤波系数。常量内存的大小是有限的，我们不能对内核代码进行修改。然而，我们可以使用这个常量内存，因为我们的卷积滤波器适合这种条件。我们可以这样使用常量内存：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we can place our convolutional filter coefficients in constant memory
    using the `cudaMemcpyToSymbol()` function:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`cudaMemcpyToSymbol()`函数将卷积滤波系数放置在常量内存中：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s switch the filter operation so that we can use constant memory. The
    whole kernel implementation is as follows. As you can see, only one variable''s
    usage has changed:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们切换滤波操作，这样我们就可以使用常量内存。整个内核实现如下。正如你所看到的，只有一个变量的使用发生了变化：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we can confirm the performance enhancement thanks to the filter data reusing `nvprof`:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以通过重复使用`nvprof`来确认性能提升：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: From this result, we can see a reduced kernel execution time.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个结果中，我们可以看到减少的内核执行时间。
- en: Tiling input data using shared memory
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用共享内存平铺输入数据
- en: 'Now, we will optimize input data usage using shared memory. To differentiate
    our next optimization step, let''s duplicate the previous convolution kernel function
    and name it `convolution_kernel_v3()`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用共享内存来优化输入数据的使用。为了区分我们的下一个优化步骤，让我们复制之前的卷积核函数并将其命名为`convolution_kernel_v3()`：
- en: 'First, we need to preprepare the shared memory space so that it can store the
    input data. To get the benefit of the filter operation from shared memory, we
    need to have extra input data. To create sufficient memory space, we need to modify
    the kernel call, as follows:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要预先准备共享内存空间，以便它可以存储输入数据。为了从共享内存中获得滤波操作的好处，我们需要额外的输入数据。为了创建足够的内存空间，我们需要修改内核调用，如下所示：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the kernel code, we can declare the shared memory space as follows:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在内核代码中，我们可以声明共享内存空间如下：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we can copy the input data to shared memory, which will be calculated
    by the thread block. First, let''s declare some variables that help control the
    memory operation:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以将输入数据复制到由线程块计算的共享内存中。首先，让我们声明一些帮助控制内存操作的变量：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can copy the load input data to shared memory by following the tiling
    design we discussed previously:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以按照之前讨论的平铺设计将加载的输入数据复制到共享内存中：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Since the input memory has changed, our convolution code should be updated.
    We can write the convolution code as follows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于输入内存已更改，我们的卷积代码应该更新。我们可以将卷积代码编写如下：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we can measure the performance gain using `nvprof`. From the result,
    we can confirm that we have accelerated about 35% faster than the original operation:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`nvprof`来测量性能增益。从结果中，我们可以确认我们的加速速度比原始操作快了大约35%：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, we have covered how to utilize loaded data so that we can reuse it with
    other on-chip caches instead of global memory. We'll talk about this in more detail
    in the next section.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经了解如何利用加载的数据，以便我们可以重复使用它与其他片上缓存而不是全局内存。我们将在下一节中更详细地讨论这个问题。
- en: Getting more performance
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获得更多性能
- en: 'If the filter is the symmetric filter or separable filter, we can break down
    the box filter as two filters: a horizontal filter and a vertical filter. Using
    two directional filters, we can have more optimization in shared memory usage:
    memory space and memory utilization. If you want to learn more about this, have
    a look at a CUDA sample named `convolutionSeparable` in the `3_Imaging/convolutionSeparable`
    directory. Its detailed explanation is also included in the same directory as
    `doc/convolutionSeparable.pdf`.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果滤波器是对称滤波器或可分离滤波器，我们可以将盒式滤波器分解为两个滤波器：水平滤波器和垂直滤波器。使用两个方向滤波器，我们可以在共享内存使用方面进行更多优化：内存空间和内存利用率。如果您想了解更多信息，请查看名为`convolutionSeparable`的CUDA示例，该示例位于`3_Imaging/convolutionSeparable`目录中。其详细说明也包含在相同目录的`doc/convolutionSeparable.pdf`中。
- en: Prefix sum (scan)
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前缀和（扫描）
- en: 'Prefix sum (scan) is used to obtain a cumulative number array from the given
    input numbers array. For example, we can make a prefix-sum sequence as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀和（扫描）用于从给定的输入数字数组中获得累积数字数组。例如，我们可以按以下方式制作前缀和序列：
- en: '| **Input numbers ** | 1 | 2 | 3 | 4 | 5 | 6 | ... |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| **输入数字** | 1 | 2 | 3 | 4 | 5 | 6 | ... |'
- en: '| **Prefix sums** | 1 | 3 | 6 | 10 | 15 | 21 | ... |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| **前缀和** | 1 | 3 | 6 | 10 | 15 | 21 | ... |'
- en: 'It differs from parallel reduction since reduction just generates the total
    operation output from the given input data. On the other hand, scan generates
    outputs from each operation. The easiest way to solve this problem is to iterate
    all the inputs to generate the output. However, it would take a long time and
    would be inefficient in GPUs. Hence, the mild approach can parallelize the prefix-sum
    operation, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 它与并行减少不同，因为减少只是从给定的输入数据生成总操作输出。另一方面，扫描从每个操作生成输出。解决这个问题的最简单方法是迭代所有输入以生成输出。但是，在GPU中这将花费很长时间并且效率低下。因此，温和的方法可以并行化前缀和操作，如下所示：
- en: '![](img/40a0239f-03fc-48b4-a4b9-bcfe8d18b761.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40a0239f-03fc-48b4-a4b9-bcfe8d18b761.png)'
- en: 'In this approach, we can obtain the output using multiple CUDA cores. However,
    this method does not reduce the total number of iterations because the first input
    element should be added for all the outputs one by one. Also, we cannot predict
    the output result when the array is sufficiently large, so multiple thread blocks
    should be launched. This is because all the scheduled CUDA threads are not launched
    at the same time in the CUDA architecture and there would be conflicts in multiple
    CUDA threads. To avoid this, we need a double buffer approach for the array, which
    is another inefficiency. The following code shows its implementation:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们可以使用多个CUDA核心来获得输出。但是，这种方法并不会减少迭代的总次数，因为第一个输入元素应该逐个添加到所有输出中。此外，当数组足够大时，我们无法预测输出结果，因此应该启动多个线程块。这是因为在CUDA架构中，并非所有计划的CUDA线程都同时启动，并且多个CUDA线程会发生冲突。为了避免这种情况，我们需要对数组采用双缓冲区方法，这是另一种低效的方法。以下代码显示了它的实现：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'There is another optimized approach named **Blelloch scan**. This method generates
    prefix-sum outputs by increasing and decreasing the strides exponentially. This
    method''s procedure is shown in the following diagram:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种优化方法叫做**Blelloch扫描**。该方法通过指数增加和减少步长来生成前缀和输出。该方法的过程如下图所示：
- en: '![](img/037f5546-f0b2-437e-a772-a3c9cdfcaf62.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/037f5546-f0b2-437e-a772-a3c9cdfcaf62.png)'
- en: There are two steps based on the stride controls. While increasing the stride,
    it obtains the partial summations accordingly. Then, it obtains the partial summations
    while reducing the stride accordingly. Each step has a different operation pattern,
    but they can be figured out with the stride size. Now, let's cover the Blelloch
    scan's implementation and check out the updated performance.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 基于步长控制有两个步骤。在增加步长的同时，相应地获得部分总和。然后，在减小步长的同时获得部分总和。每个步骤都有不同的操作模式，但可以根据步长大小来确定。现在，让我们来看一下Blelloch扫描的实现并检查更新后的性能。
- en: Blelloch scan implementation
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Blelloch扫描实现
- en: 'The following steps will show you how to implement the optimized parallel scan
    algorithm:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将向您展示如何实现优化的并行扫描算法：
- en: 'Let''s create a kernel function that can accept input and output memories,
    along with their size:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个可以接受输入和输出内存以及它们的大小的内核函数：
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we will create a CUDA thread index and a global index to handle the input
    data:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个CUDA线程索引和一个全局索引来处理输入数据：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To speed up the iteration, we will use shared memory. This algorithm can generate
    outputs that are double the size of CUDA threads, so we will load extra block-sized
    input data into shared memory:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了加快迭代速度，我们将使用共享内存。该算法可以生成CUDA线程大小的两倍输出，因此我们将额外加载块大小的输入数据到共享内存中：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Before we start the iteration, we will declare the offset variable that counts
    the gap between the left-hand operand and the right-hand operand:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始迭代之前，我们将声明偏移变量，该变量计算左操作数和右操作数之间的差距：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we will add up the input data until the offset becomes larger than the
    input''s length:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将添加输入数据，直到偏移量大于输入的长度为止：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After that, we will iterate again while we reduce the reduction size by two:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们将通过减小减少大小来再次迭代两次：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we will store the output value in global memory using the kernel function:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用内核函数将输出值存储在全局内存中：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we can call this scan kernel function as follows:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以调用这个扫描内核函数如下：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You can also write a naîve scan version with the same function interface. Now,
    let's review how fast our new version is, and if there are any other optimization
    opportunities we can take advantage of.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用相同的函数接口编写一个朴素扫描版本。现在，让我们回顾一下我们的新版本有多快，以及我们是否可以利用其他优化机会。
- en: 'The following code shows the profiled result of the naïve scan''s and Blelloch
    scan''s performance:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码显示了朴素扫描和Blelloch扫描性能的分析结果：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As you can see, the Blolloch scan is about five times faster than the naive
    scan algorithm due to reduced overhead. We can also validate the operation result
    by comparing the output of the different implementations:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，由于减少了开销，Blolloch扫描比朴素扫描算法快了大约五倍。我们还可以通过比较不同实现的输出来验证操作结果：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Up until now, we have covered how to design and implement the optimized parallel
    prefix-sum operation on a single block size. To use the prefix-sum operation on
    the input data, which has more data than the block size, we need to build a block-level
    prefix-sum operation based on our block-level reduction code. We'll talk about
    this in more detail in the following section.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了如何设计和实现优化的单个块大小的并行前缀和操作。要在输入数据上使用前缀和操作，需要比块大小更多的数据，我们需要基于我们的块级减少代码构建一个块级前缀和操作。我们将在下一节详细讨论这个问题。
- en: Building a global size scan
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建全局大小扫描
- en: Our implemented prefix-sum operation works within a single thread block. Since
    the first step has two inputs and the maximum CUDA threads we can have in a thread
    block is 1,024, the maximum available size is 2,048\. Without considering other
    thread block operations, the thread block does up-sweeping and down-sweeping.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现的前缀和操作在单个线程块内工作。由于第一步有两个输入，而我们在一个线程块中最多可以有1,024个CUDA线程，因此最大可用大小为2,048。在不考虑其他线程块操作的情况下，线程块进行上扫描和下扫描。
- en: 'However, this operation can be enlarged if we perform a block-wise scan operation.
    To do this, you will need extra steps that collect the last prefix-sum result,
    scan them, and add each thread block''s result with each block''s block-level
    scanned value. This procedure can be implemented as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们执行一个分块扫描操作，这个操作可以被扩大。为了做到这一点，你需要额外的步骤来收集最后一个前缀和的结果，扫描它们，并将每个线程块的结果与每个块的块级扫描值相加。这个过程可以按照以下方式实现：
- en: '![](img/9080ba5d-b61c-4763-bc01-989c933a5255.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9080ba5d-b61c-4763-bc01-989c933a5255.png)'
- en: The pursuit of better performance
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 追求更好的性能
- en: Our implementation code performs the optimal operation. However, we can make
    further optimizations by reducing the shared memory's bank conflicts. In our implementation,
    the CUDA threads access the same memory banks at certain points. NVIDIA's GPU
    Gem3 introduced prefix-sum (scan) in *Chapter 39, Parallel Prefix Sum (Scan) with
    CUDA* ([https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html](https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)),
    and points out this issue in *39.2.3 Avoiding Bank Conflicts*. You can adapt the
    solution to our implementation, but you should update `NUM_BANKS` to `32` and
    `LOG_NUM_BANKS` to `5` if you do. Nowadays, the CUDA architecture has 32 shared
    memory banks.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现代码执行了最佳操作。然而，我们可以通过减少共享内存的银行冲突来进一步优化。在我们的实现中，CUDA线程在某些点上访问相同的内存银行。NVIDIA的GPU
    Gem3在*第39章，使用CUDA进行并行前缀和（扫描）*（[https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html](https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)）中介绍了前缀和（扫描），并在*39.2.3避免银行冲突*中指出了这个问题。你可以将解决方案调整到我们的实现，但如果这样做，你应该将`NUM_BANKS`更新为`32`，`LOG_NUM_BANKS`更新为`5`。现在，CUDA架构有32个共享内存银行。
- en: Other applications for the parallel prefix-sum operation
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行前缀和操作的其他应用
- en: Dr. G.E. Blelloch published an article about his prefix-sum named *Prefix Sums
    and Their Application* ([https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf](https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf))
    in 1993\. You can learn more about the parallel prefix-sum algorithm and its applications
    by reading his article. The applications are compact, split, segmented scan, quick
    sort, radix sort, and merge sort.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: G.E. Blelloch博士在1993年发表了一篇关于他的前缀和算法的文章*前缀和及其应用*（[https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf](https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf)）。通过阅读他的文章，你可以了解更多关于并行前缀和算法及其应用。这些应用包括压缩、分割、分段扫描、快速排序、基数排序和归并排序。
- en: Dr. Ahmed Sallm's video lecture, *Intro to Parallel Processing with CUDA - Lecture
    4 Part 2\3* ([https://youtu.be/y2HzWKTqo3E](https://youtu.be/y2HzWKTqo3E)), provides
    a good introduction to these. It provides conceptual introductions to how the
    prefix-sum algorithm can be used to clip graphics and build a sparse matrix. He
    also provides instructions regarding how to use the sort algorithms.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Ahmed Sallm博士的视频讲座，*使用CUDA进行并行处理简介-第4讲第2\3部分*（[https://youtu.be/y2HzWKTqo3E](https://youtu.be/y2HzWKTqo3E)），对此提供了很好的介绍。它提供了关于前缀和算法如何用于裁剪图形和构建稀疏矩阵的概念介绍。他还提供了关于如何使用排序算法的说明。
- en: Compact and split
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩和分割
- en: 'Previously, we covered how to parallelize the sequential prefix sum algorithm
    and discussed how it can be used for other applications. Now, let''s cover some
    of those applications: compact and split. The compact operation is an algorithm
    that can consolidate values that fulfill the given condition from an array. On
    the other hand, the split operation is an algorithm that distributes the values
    to the designated place. In general, these algorithms work sequentially. However,
    we will see how the parallel prefix-sum operation can improve how it functions.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们介绍了如何并行化顺序前缀和算法，并讨论了它如何用于其他应用。现在，让我们来介绍其中一些应用：压缩和分割。压缩操作是一种可以从数组中整合满足给定条件的值的算法。另一方面，分割操作是一种将值分配到指定位置的算法。一般来说，这些算法是顺序工作的。然而，我们将看到并行前缀和操作如何改进它的功能。
- en: 'The compact operation is used to collect specific data that meets a certain
    condition into an array. For example, if we want to use the compact operation
    for the positive elements in an array, then the operation is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩操作用于将满足特定条件的特定数据收集到一个数组中。例如，如果我们想要对数组中的正元素使用压缩操作，那么操作如下：
- en: '![](img/ec6509d7-6673-4967-898a-f387974abaca.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec6509d7-6673-4967-898a-f387974abaca.png)'
- en: 'In parallel programming, we have a different approach that can utilize multiple
    cores using the parallel prefix-sum operation. First, we mark the data to check
    whether it meets the condition or not (that is, predicate), and then we do the
    prefix-sum operation. The output of prefix-sum will be the index of the marked
    values, so we can obtain the gathered array by copying them. The following diagram
    shows an example of a compact operation:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行编程中，我们有一种不同的方法，可以利用并行前缀和操作使用多个核心。首先，我们标记数据以检查它是否满足条件（即谓词），然后进行前缀和操作。前缀和的输出将是标记值的索引，因此我们可以通过复制它们来获得收集的数组。下面的图表显示了压缩操作的一个示例：
- en: '![](img/16b52978-c007-437d-8b05-577c3b76efa9.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16b52978-c007-437d-8b05-577c3b76efa9.png)'
- en: Since all of these tasks can be done in parallel, we can obtain the gathered
    array in four steps.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有这些任务都可以并行完成，我们可以在四个步骤中获得收集的数组。
- en: 'On the other hand, split means to distribute the data to a number of different
    places. In general, we distribute the data from where it was initially. The following
    diagram shows an example of its operation:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，拆分意味着将数据分发到多个不同的位置。一般来说，我们会从最初的位置分发数据。下面的图表显示了它的操作示例：
- en: '![](img/517c99ed-34ed-471d-934e-7640d07b839f.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/517c99ed-34ed-471d-934e-7640d07b839f.png)'
- en: 'This example shows that the gathered array elements are distributed where they
    were from. We can also do this in parallel using prefix-sum. Firstly, we refer
    to the predicate array and do the prefix-sum. Since the outputs are each element''s
    address, we can distribute them easily. The following diagram shows how this operation
    can be done:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子显示了收集的数组元素是如何分布在它们原来的位置的。我们也可以使用前缀和并行地做到这一点。首先，我们参考谓词数组并进行前缀和操作。由于输出是每个元素的地址，我们可以很容易地分配它们。下面的图表显示了如何进行这个操作：
- en: '![](img/7af231f8-fb0d-4d46-887a-99d82c9aa08a.png)`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/7af231f8-fb0d-4d46-887a-99d82c9aa08a.png)`'
- en: Now, let's implement this and discuss their performance limiters and their application.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现这个并讨论它们的性能限制和应用。
- en: Implementing compact
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现压缩
- en: The compact operation is a sequence of predicate, scan, addressing, and gather.
    In this implementation, we will build an array of positive numbers from an array
    of randomly generated numbers. The initial version can only afford a single thread
    block operation since we will only use a single block-sized prefix-sum operation.
    However, we can learn how prefix-sum is useful for other applications and extend
    this operation to larger arrays with the extended prefix-sum operation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩操作是一个谓词、扫描、寻址和收集的序列。在这个实现中，我们将从一个随机生成的数字数组中构建一个正数数组。初始版本只能承受单个线程块操作，因为我们只会使用一个块大小的前缀和操作。然而，我们可以了解前缀和如何对其他应用有用，并将这个操作扩展到更大的数组，使用扩展的前缀和操作。
- en: 'To implement a compact operation, we will write several kernel functions that
    can do the required operation for each step and call those last:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现压缩操作，我们将编写几个核函数，可以为每个步骤执行所需的操作，并调用最后那些：
- en: 'Let''s write a kernel function that can make a predicate array by checking
    whether each element''s value is greater than zero or not:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编写一个核函数，通过检查每个元素的值是否大于零来生成一个谓词数组：
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, we have to perform a prefix-sum operation for that predicate array. We
    will reuse the previous implementation here. After that, we can write a kernel
    function that can detect the address of the scanned array and gather the target
    elements as output:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须对谓词数组执行前缀和操作。我们将在这里重用之前的实现。之后，我们可以编写一个可以检测扫描数组的地址并将目标元素收集为输出的核函数：
- en: '[PRE33]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s call them all together to make a compact operation:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们一起调用它们来进行压缩操作：
- en: '[PRE34]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, we have an array of positive numbers that were gathered from a randomly
    generated array:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们有了一个从随机生成的数组中收集到的正数数组：
- en: '[PRE35]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: By using the parallel prefix-sum operation, we can implement the compact operation
    in parallel easily. Our implementation compacts the positive values from the given
    array, but we can switch this to the other condition and apply the compact operation
    without difficulty. Now, let's cover how to distribute these compact elements
    to the original array.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用并行前缀和操作，我们可以很容易地并行实现压缩操作。我们的实现从给定数组中压缩正值，但我们可以将其切换到其他条件并且应用压缩操作而不会有困难。现在，让我们来讨论如何将这些压缩元素分发到原始数组。
- en: Implementing split
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现拆分
- en: 'The split operation is a sequence of predicate, scan, address, and split. In
    this implementation, we will reuse the address array we created in the previous
    section. Therefore, we can skip the previous steps and just implement the split
    operation from the address array:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 拆分操作是一个谓词、扫描、地址和拆分的序列。在这个实现中，我们将重用在前一节中创建的地址数组。因此，我们可以跳过之前的步骤，只需从地址数组中实现拆分操作：
- en: 'Let''s write the split kernel function, as follows:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编写拆分核函数，如下所示：
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, we can call the kernel function, as follows:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以调用核函数，如下所示：
- en: '[PRE37]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Since we''ll be using the output of the scan from the previous step, we will
    copy it to the input and clear the original array. In total, we can do a parallel
    compact and split using CUDA. Here is the output of our implementation. You can
    confirm that it operates as desired:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们将使用前一步骤的扫描输出，我们将把它复制到输入并清除原始数组。总的来说，我们可以使用CUDA进行并行压缩和拆分。这是我们实现的输出。您可以确认它按预期运行：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In our implementation, we generated a compact array and a split array for the
    positive elements. Thanks to the parallel prefix-sum, we can also do this in parallel.
    One of the major limitations of our version is that it only supports less than
    2,048 elements since our implementation is based on our previous parallel prefix-sum
    implementation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们为正值生成了一个压缩数组和一个拆分数组。由于并行前缀和，我们也可以并行地做到这一点。我们版本的一个主要限制是，它只支持少于2,048个元素，因为我们的实现是基于之前的并行前缀和实现的。
- en: N-body
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N-body
- en: Any N-body simulation is a simulation of the dynamical system that evolves under
    the influence of physical forces. Numerical approximation is done as the bodies
    continuously interact with each other. N-body simulation is done extensively in
    physics and astronomy, for example, so that scientists can understand the dynamics
    of particles in the Universe. N-body simulations are used in many other domains,
    including computational fluid dynamics in order to understand turbulent fluid
    flow simulation.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 任何N体模拟都是一个在物理力的影响下演化的动力学系统的模拟。随着物体不断相互作用，进行数值近似。N体模拟在物理学和天文学中被广泛使用，例如，科学家可以了解宇宙中粒子的动态。N体模拟也在许多其他领域中使用，包括计算流体动力学，以便理解湍流流体流动模拟。
- en: A relatively easy method for solving N-body simulation is to make use of a brute-force
    technique that has *O(N²)* complexity. This approach is embarrassingly parallel
    in nature. There are various optimizations at algorithmic scale that can reduce
    the compute complexity. Instead of applying all-pairs to the whole simulation,
    it can be used to determine forces in close-range interactions. Even in this case,
    creating a kernel for solving the forces on CUDA is very useful as it will also
    improve the performance of far-field components. Accelerating one component will
    offload work from the other components, so the entire application benefits from
    accelerating one kernel.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 解决N体模拟的一个相对简单的方法是利用*O(N²)*复杂度的蛮力技术。这种方法在本质上是尴尬地并行的。在算法规模上有各种优化可以减少计算复杂度。可以用来确定近距离相互作用中的力，而不是将所有对应用于整个模拟。即使在这种情况下，为CUDA解决力量创建一个内核也是非常有用的，因为它还将提高远场组件的性能。加速一个组件将卸载其他组件的工作，因此整个应用程序都会从加速一个内核中受益。
- en: Implementing an N-body simulation on GPU
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在GPU上实现N体模拟
- en: The algorithm is basically an all-pairs algorithm calculating force, *f[ij]*, for
    an N![](img/5b868d65-55c7-40fd-a496-8c31b23a562d.png)N grid. The total force/acceleration, *F[i]*,
    on a body, *i*, is the result of a summation of all the entries in row *i*. From
    a parallelism point of view, this is an embarrassingly parallel task of *O(N²)*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法基本上是一个计算力*f[ij]*的所有对算法，对于一个N![](img/5b868d65-55c7-40fd-a496-8c31b23a562d.png)N网格。一个物体*i*上的总力/加速度*F[i]*是该行中所有条目的总和。从并行性的角度来看，这是一个尴尬地并行的任务，复杂度为*O(N²)*。
- en: From a performance point of view, the application is memory bound and would
    be limited by memory bandwidth. The good part is that much of the data can be
    reused and stored in high bandwidth and low latency memory such as shared memory.
    Data reuse and storage in shared memory reduces the load on global memory and
    hence helps in reaching peak compute performance.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 从性能角度来看，该应用程序受到内存限制，并且会受到内存带宽的限制。好的一点是，许多数据可以被重复使用并存储在高带宽和低延迟的内存中，比如共享内存。在共享内存中重复使用和存储数据可以减少对全局内存的负载，从而有助于达到峰值计算性能。
- en: 'The following diagram shows the strategy that we will be using:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了我们将使用的策略：
- en: '![](img/b5eefd12-c31d-480d-a0b8-a473c3eed048.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5eefd12-c31d-480d-a0b8-a473c3eed048.png)'
- en: Instead of loading the memory again and again from global memory, we make use
    of tiling. We've already demonstrated the use of tiling for matrix multiplication
    and looked at its use in imaging applications in previous chapters. The preceding
    diagram shows that each row is evaluated in parallel. The tile size is defined
    by the maximum number of elements that can be stored in shared memory that don't
    affect the occupancy of the kernel. Each block loads the data into shared memory,
    followed by performing synchronization. Once the data has been loaded into shared
    memory, the force/acceleration calculation is done in every block. It is visible
    that even though a separate row is calculated in parallel, to achieve optimal
    data reuse, the interaction in each row is done sequentially.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再从全局内存中反复加载内存，而是利用平铺。我们已经在之前的章节中演示了在矩阵乘法中使用平铺，并在图像应用中使用了它。前面的图表显示了每一行都是并行评估的。平铺大小由可以存储在共享内存中而不影响内核占用率的最大元素数量定义。每个块将数据加载到共享内存中，然后执行同步。一旦数据加载到共享内存中，就在每个块中进行力/加速度计算。可以看到，即使单独的行是并行计算的，为了实现最佳的数据重用，每行中的相互作用是顺序进行的。
- en: Overview of an N-body simulation implementation
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N体模拟实现概述
- en: 'Let''s review the implementation of this in pseudocode format, followed by
    explaining its logic. In this example, we use gravitational potential to illustrate
    the basic form of computation in an all pairs N-body simulation. The implemented
    code can be found in `07_parallel_programming_pattern/05_n-body`. Follow these
    steps to get started:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以伪代码格式回顾一下这个实现，然后解释它的逻辑。在这个例子中，我们使用引力势来说明所有对N体模拟中的基本计算形式。实现的代码可以在`07_parallel_programming_pattern/05_n-body`中找到。按照以下步骤开始：
- en: 'Initialize n-space with random variables:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用随机变量初始化n空间：
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Declare and store the data in an intermediate shared memory space for efficient
    reuse. Synchronize it to guarantee that all the threads within the block see the
    updated values in shared memory:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个中间共享内存空间中声明和存储数据，以便有效地重复使用。同步以确保块内的所有线程都能看到共享内存中的更新值：
- en: '[PRE40]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Calculate the force by iterating every block:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过迭代每个块来计算力：
- en: '[PRE41]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, compile the application with the `nvcc` compiler with the following
    command:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用以下命令将应用程序编译为`nvcc`编译器：
- en: '[PRE42]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As you can see, implementing an N-body simulation is an embarrassingly parallel
    task and quite straightforward. While we have implemented the basic version of
    code here, there are various algorithmic variations that exist. You can make use
    of this version as a template that you can improve, based on changes that are
    made to the algorithm.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，实现N体模拟是一个尴尬地并行的任务，而且非常简单。虽然我们在这里实现了基本版本的代码，但存在各种算法变体。你可以利用这个版本作为一个模板，根据对算法的更改进行改进。
- en: Histogram calculation
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直方图计算
- en: In an embarrassingly parallel job, ideally, you would assign computation to
    each thread working on independent data, resulting in no data races. By now, you
    will have realized that some patterns don't fit this category. One such pattern
    is when we're calculating a histogram. The histogram pattern displays the frequency
    of a data item, for example, the number of times we used the word CUDA in each
    ch
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个尴尬的并行作业中，理想情况下，您会将计算分配给每个线程，这些线程在独立数据上工作，从而不会发生数据竞争。到目前为止，您可能已经意识到有些模式不适合这个类别。其中一种模式是当我们计算直方图时。直方图模式显示了数据项的频率，例如，我们在每个章节中使用CUDA这个词的次数
- en: 'apter, the number of times each letter occurred in this chapter, and so on.
    A histogram takes the following form:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 章节，本章中每个字母出现的次数等。直方图采用以下形式：
- en: '![](img/713c8091-8a43-4097-8ba0-192a4fe97b41.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/713c8091-8a43-4097-8ba0-192a4fe97b41.png)'
- en: In this section, we will make use of atomic operations to serialize access to
    data in order to get the correct results.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将利用原子操作来串行访问数据，以便获得正确的结果。
- en: Compile and execution steps
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译和执行步骤
- en: 'The histogram provides important features about the datasets at hand, as well
    as useful insights about the same. For example, out of the whole image, there
    are only a few regions where regions of interest may lie. Creating a histogram
    is sometimes used to figure out where in the image the region of interest may
    be. In this example, we will be making use of calculating a histogram on an image
    where the whole image is divided into chunks. Let''s get started:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图提供了关于手头数据集的重要特征，以及有用的见解。例如，在整个图像中，只有少数区域可能存在感兴趣的区域。有时创建直方图用于找出图像中可能存在感兴趣区域的位置。在这个例子中，我们将使用在整个图像中将图像分成块来计算直方图。让我们开始吧：
- en: Prepare your GPU application. This code can be found at `07_parallel_programming_pattern/08_histogram`.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备您的GPU应用程序。此代码可以在`07_parallel_programming_pattern/08_histogram`中找到。
- en: 'Compile your application with the `nvcc` compiler with the following command:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将您的应用程序编译为`nvcc`编译器：
- en: '[PRE43]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The `scrImagePgmPpmPackage.cpp` file provides the source code that we can use
    to read and write images with `.pgm` extensions. The histogram calculation code
    can be found in `image_histogram.cu`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`scrImagePgmPpmPackage.cpp`文件提供了我们可以用来读取和写入`.pgm`扩展名图像的源代码。直方图计算代码可以在`image_histogram.cu`中找到。'
- en: Understanding a parallel histogram
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解并行直方图
- en: 'Patterns such as the histogram demand atomic operation, which means updating
    a value at a specific address in a serialized fashion to remove contention from
    multiple threads, thereby updating the same address. This requires coordination
    among multiple threads. In this seven-step process, you might have observed that
    we made use of privatization. Privatization is a technique that makes use of low
    latency memory such as shared memory to reduce throughput and decrease latency,
    as shown in the following diagram:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如直方图之类的模式需要原子操作，这意味着以串行方式更新特定地址的值，以消除多个线程之间的争用，从而更新相同的地址。这需要多个线程之间的协调。在这个七步过程中，您可能已经注意到我们使用了私有化。私有化是一种利用低延迟内存（如共享内存）来减少吞吐量和降低延迟的技术，如下图所示：
- en: '![](img/02786dd6-3c15-4c9f-9d7a-770d31b640b4.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02786dd6-3c15-4c9f-9d7a-770d31b640b4.png)'
- en: Basically, instead of making use of atomic operations on global memory, we make
    use of atomics on shared memory. The reason should be quite obvious to you by
    now. Atomic operations on global memory are more costly compared to doing the
    same on shared memory/an L1 cache. From the Maxwell architecture onward, atomic
    operations are hardware supported. The privatized shared memory implementation
    should ideally give you 2x performance from the Maxwell architecture onward. However,
    please note that atomic operations are limited to specific functions and data
    sizes.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们不是在全局内存上使用原子操作，而是在共享内存上使用原子操作。原因现在应该对您来说是相当明显的。与在共享内存/ L1缓存上执行相同操作相比，在全局内存上执行原子操作的成本更高。从Maxwell架构开始，原子操作得到了硬件支持。私有化的共享内存实现应该从Maxwell架构开始为您提供2倍的性能。但是，请注意，原子操作仅限于特定的函数和数据大小。
- en: Calculating a histogram with CUDA atomic functions
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA原子函数计算直方图
- en: 'Primarily, we are going to make use of the `atomicAdd()` operation on shared
    memory to calculate a histogram for each block in shared memory. Follow these
    steps to calculate the histogram in a kernel:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 主要地，我们将利用共享内存上的`atomicAdd()`操作来计算共享内存中每个块的直方图。按照以下步骤在内核中计算直方图：
- en: 'Allocate shared memory per block equal to the size of the histogram per block.
    Since it is a char image, we expect the elements to be in the range of 0-255:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个块分配与每个块的直方图大小相等的共享内存。由于这是一个char图像，我们期望元素在0-255的范围内：
- en: '[PRE44]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Initialize the shared memory array to `0` per block:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个块的共享内存数组初始化为`0`：
- en: '[PRE45]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Synchronize this to make sure all the threads within a block see initialized
    array:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同步这一点，以确保块内的所有线程看到初始化的数组：
- en: '[PRE46]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Read the data of the image from the global/texture memory:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从全局/纹理内存中读取图像的数据：
- en: '[PRE47]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Do an `atomicAdd()` operation on shared memory:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在共享内存上进行`atomicAdd()`操作：
- en: '[PRE48]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Synchronize across the block before writing to global memory:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在写入全局内存之前，在块之间进行同步：
- en: '[PRE49]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Write the histogram per block to global memory:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个块的直方图写入全局内存：
- en: '[PRE50]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Now, we have finished implementing the histogram calculation on the GPU.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经完成了在GPU上实现直方图计算。
- en: To summarize, histograms are easy to implement with shared atomic memory. This
    approach can attain high performance on Maxwell onward cards due to its native
    support for shared atomic memory in hardware.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，使用共享原子内存很容易实现直方图。由于硬件对共享原子内存的本机支持，这种方法可以在Maxwell架构之后的显卡上获得高性能。
- en: Quicksort in CUDA using dynamic parallelism
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用动态并行性在CUDA中进行快速排序
- en: 'One of the key algorithms that''s a fundamental building block for any application
    is sorting. There are many sorting algorithms available that have been studied
    extensively. The worst time complexity, best time complexity, input data characteristics
    (is the data almost sorted or random? Is it a key-value pair? Is it an integer
    or a float?), in-place or out of place memory requirements, and so on define which
    algorithm is suitable for which application. Some of the sorting algorithms fall
    into the category of divide and conquer algorithms. These algorithms are suitable
    for parallelism and suit architectures such as GPU where data to be sorted can
    be divided for sorting. One such algorithm is Quicksort. As we stated earlier,
    Quicksort falls into the category of divide and conquer. It is a three-step approach,
    as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 作为任何应用程序的基本构建块的关键算法之一是排序。有许多可用的排序算法已经得到了广泛的研究。最坏时间复杂度、最佳时间复杂度、输入数据特征（数据几乎排序好还是随机的？是键值对吗？是整数还是浮点数？）、原地或非原地内存需求等等，这些都定义了哪种算法适用于哪种应用。一些排序算法属于分治算法的范畴。这些算法适合并行处理，并适用于GPU等可以将要排序的数据分割进行排序的架构。其中一个这样的算法是快速排序。正如我们之前所述，快速排序属于分治范畴。它是一个三步方法，如下：
- en: Pick an element from an array that needs to be sorted. This element acts as
    a pivot element.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从需要排序的数组中选择一个元素。这个元素作为枢轴元素。
- en: The second step is partitioning where all the elements go. All the elements
    that are less than the pivot are shifted to the left and all the elements greater
    than or equal to the pivot are shifted to the right of the pivot element. This
    step is also known as partitioning.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步是分区，确定所有元素的位置。所有小于枢轴的元素都移到左边，所有大于或等于枢轴的元素都移到枢轴元素的右边。这一步也被称为分区。
- en: Recursively do steps 1 and 2 until all the sub-arrays have been sorted.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归地执行步骤1和2，直到所有子数组都被排序。
- en: Quicksort worst-case complexity is O(![](img/79e46e31-d8d8-4612-9c6d-9e08bf0e40a0.png)),
    which may not seem ideal compared to other sorting processes whose worst-case
    complexity is O(![](img/3859e9ea-10c4-44b8-9ddc-61e8a07878fe.png)), such as merge
    sort and heap sort). However, Quicksort is seen to be effective in practice. The
    choice of the pivot element can be chosen with consideration and sometimes randomly
    so that worst-case complexity hardly occurs. Also, Quicksort is seen to have less
    memory load and requirements compared to other sorting algorithms, such as merge
    sort, which requires extra storage. More practical implementations of Quicksort
    use a randomized version. The randomized version has the expected time complexity
    of O(![](img/52cec39e-957c-4bfc-be02-07aa00f96b82.png)). Worst-case complexity
    is also possible in the randomized version, but it doesn't occur for a particular
    pattern (such as a sorted array) and randomized Quicksort works well in practice.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 快速排序的最坏情况复杂度是O(n^2)，这与其他排序过程的最坏情况复杂度为O(nlogn)相比可能不太理想（例如归并排序和堆排序）。然而，实际上快速排序被认为是有效的。枢轴元素的选择可以经过考虑，有时也可以随机选择，以使最坏情况复杂度几乎不会发生。此外，与其他排序算法相比，快速排序的内存负载和需求较少，例如归并排序需要额外的存储空间。更实际的快速排序实现使用随机化版本。随机化版本的期望时间复杂度为O(nlogn)。最坏情况复杂度在随机化版本中也是可能的，但它不会发生在特定模式（例如排序好的数组）上，随机化快速排序在实践中表现良好。
- en: While we can write a whole chapter on the characteristics of the sorting algorithm,
    we plan to cover only the features of CUDA that will help you to implement Quicksort
    efficiently on GPU. In this section, we will be making use of dynamic parallelism,
    which was introduced from CUDA 6.0 and GPUs with a 3.5 architecture onwards.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以写一整章关于排序算法的特性，但我们计划只覆盖CUDA的特性，这将帮助您在GPU上高效实现快速排序。在本节中，我们将使用从CUDA 6.0和GPU架构3.5开始引入的动态并行性。
- en: Now, let's review how dynamic parallelism contributes to the sorting algorithm.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下动态并行性是如何对排序算法做出贡献的。
- en: Quicksort and CUDA dynamic parallelism
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速排序和CUDA动态并行性
- en: The Quicksort algorithm demands launching kernels recursively. So far, the algorithms
    we have seen call the kernel once via the CPU. After the kernel has finished executing,
    we return to the CPU thread and then relaunch it. Doing this results in giving
    back control to the CPU, and may also result in data transfer between CPU and
    GPU, which is a costly operation. It used to be very difficult to efficiently
    implement algorithms such as Quicksort on GPUs that demand features such as recursion.
    With the GPU architecture 3.5 and CUDA 5.0 onwards, a new feature was introduced
    called dynamic parallelism.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 快速排序算法要求递归地启动内核。到目前为止，我们所见过的算法是通过CPU一次调用内核。内核执行完毕后，我们返回到CPU线程，然后重新启动它。这样做会导致将控制权交还给CPU，并且可能导致CPU和GPU之间的数据传输，这是一项昂贵的操作。以前在GPU上高效实现需要递归等特性的算法（如快速排序）曾经非常困难。从GPU架构3.5和CUDA
    5.0开始，引入了一个名为动态并行性的新特性。
- en: 'Dynamic parallelism allows the threads within a kernel to launch new kernels
    from the GPU without returning control back to the CPU. The word dynamic comes
    from the fact that it is dynamically based on the runtime data. Multiple kernels
    can be launched by threads at once. The following diagram simplifies this explanation:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 动态并行性允许内核内的线程在不将控制权返回给CPU的情况下从GPU上启动新的内核。动态一词来自于它基于运行时数据的动态性。多个内核可以同时由线程启动。以下图表简化了这个解释：
- en: '![](img/7e923707-6635-4f2b-8816-5c34e27ab0f9.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e923707-6635-4f2b-8816-5c34e27ab0f9.png)'
- en: 'If we were to translate this concept to how Quicksort is executed, it would
    look something like this:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个概念转化为快速排序的执行方式，它会看起来像这样：
- en: '![](img/82e06370-f447-437d-a8e3-15356096543e.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82e06370-f447-437d-a8e3-15356096543e.jpg)'
- en: 'Depth 0 is the call from the CPU. For each subarray, we launch two kernels:
    one for the left array and one for the right array. Recursion stops after the
    max depth of the kernel has been reached or the number of elements is less than
    32, which is the warp size. For the kernel''s launch to be in a non-zero stream
    and asynchronous so that the subarray kernel gets launched independently, we need
    to create a stream before every kernel launch:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 深度0是来自CPU的调用。对于每个子数组，我们启动两个内核：一个用于左数组，一个用于右数组。递归在达到内核的最大深度或元素数量小于32（即warp大小）后停止。为了使内核的启动在非零流中是异步的，以便子数组内核可以独立启动，我们需要在每次内核启动之前创建一个流：
- en: '[PRE51]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This is a really important step because, otherwise, the kernel's launch may
    get serialized. For more details on streams, please refer to the multi-GPU kernel.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常重要的步骤，否则内核的启动可能会被序列化。有关流的更多细节，请参考多GPU内核。
- en: Quicksort with CUDA
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA的Quicksort
- en: 'For our Quicksort implementation, we are going to make use of dynamic parallelism
    to launch the GPU kernel recursively. The major steps involved in implementing
    Quicksort are as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的Quicksort实现，我们将利用动态并行性来递归启动GPU内核。实现Quicksort的主要步骤如下：
- en: '**The CPU launches the first kernel**: The kernel is launched with one block
    and one thread. The left element is the start of the array, while the right is
    the last element of the array (basically the whole array):'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**CPU启动第一个内核**：内核以一个块和一个线程启动。左元素是数组的开始，右元素是数组的最后一个元素（基本上是整个数组）：'
- en: '[PRE52]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '**Limit check**: Check the two criteria before launching the kernel from inside
    a kernel. First, check if we have reached the max allowed limit of depth by the
    hardware. Second, we need to check whether the number of elements to be sorted
    in a sub-array is less than the warp size (32). If one of them is true, then we
    have to do a selection sort sequentially rather than launch a new kernel:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**限制检查**：在从内核内部启动内核之前检查两个条件。首先，检查我们是否已经达到硬件允许的最大深度限制。其次，我们需要检查子数组中要排序的元素数量是否小于warp大小（32）。如果其中一个条件为真，那么我们必须按顺序执行选择排序，而不是启动一个新的内核：'
- en: '[PRE53]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**Partitioning**: If the preceding conditions are met, then partition the array
    into two sub-arrays and launch two new kernels, one for the left array and another
    for the right array. If you look closely at the following code, you''ll see we
    are launching a kernel from inside the kernel:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分区**：如果满足前面的条件，那么将数组分成两个子数组，并启动两个新的内核，一个用于左数组，另一个用于右数组。如果你仔细看下面的代码，你会发现我们是从内核内部启动内核的：'
- en: '[PRE54]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '**Executing the code**: The implemented code can be found at `07_parallel_programming_pattern/06_quicksort`. Compile
    your application with the `nvcc` compiler with the following command:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行代码**：实现的代码可以在`07_parallel_programming_pattern/06_quicksort`中找到。使用以下命令使用`nvcc`编译您的应用程序：'
- en: '[PRE55]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'As you can see, we have added two flags to the compilation:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们在编译中添加了两个标志：
- en: '`-- gpu-architecture=sm_70`: This flag tells the `nvcc` to compile and generate
    the binary/`ptx` for the Volta GPU. If you specifically do not add this flag,
    the compiler tries to compile the code compatible from `sm_20`, that is, Fermi
    generation cards, until the new architecture, which is `sm_70`, that is, Volta.
    The compilation will fail since dynamic parallelism is not supported by older
    generation cards.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-- gpu-architecture=sm_70`：这个标志告诉`nvcc`为Volta GPU编译和生成二进制/`ptx`。如果你没有特别添加这个标志，编译器会尝试从`sm_20`（即Fermi代）兼容的代码编译，直到新架构`sm_70`（即Volta）。由于旧一代的卡不支持动态并行性，编译将失败。'
- en: '`-rdc=true`: This is a key argument that enables dynamic parallelism on the
    GPU.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-rdc=true`：这是一个关键参数，它在GPU上启用动态并行性。'
- en: Dynamic parallelism guidelines and constraints
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态并行性指南和约束
- en: Though dynamic parallelism provides us with an opportunity to port algorithms
    such as Quicksort on GPU, there are some fundamental rules and guidelines that
    need to be followed.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然动态并行性为我们提供了在GPU上移植Quicksort等算法的机会，但需要遵循一些基本规则和指南。
- en: '**Programming model rules**: Basically, all the CUDA programming model rule
    apply:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**编程模型规则**：基本上，所有CUDA编程模型规则都适用：'
- en: The kernel launches are asynchronous per thread.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核启动是每个线程异步的。
- en: Synchronization is only allowed per block.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同步只允许在块内进行。
- en: Streams that are created are shared within a block.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建的流在一个块内共享。
- en: Events can be used to create inter-stream dependencies.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件可用于创建流间依赖关系。
- en: '**Memory consistency rules**:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '**内存一致性规则**：'
- en: The child kernel sees the parent kernel's state at the time of the launch.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子内核在启动时看到父内核的状态。
- en: The parent kernel can see the changes that have been made by the child kernel,
    but only after the synchronization.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 父内核只能在同步后看到子内核所做的更改。
- en: Local and shared memory is, as usual, private, and cannot be passed or accessed
    by the parent kernel.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地和共享内存通常是私有的，父内核无法传递或访问。
- en: '**Guidelines**:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**指南**：'
- en: It is also important to understand that there is latency that gets added per
    kernel launch. The latency of launching a kernel from inside another kernel has
    gradually reduced over time with the new architecture.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要的是要理解，每次内核启动都会增加延迟。从另一个内核内部启动内核的延迟随着新架构的推出逐渐减少。
- en: While launch throughput is an order of magnitude higher than from the host,
    limits can be placed on the maximum depth. The max depth that's allowed is 24
    for the latest generation cards.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然启动吞吐量比主机高一个数量级，但最大深度可以设置限制。最新一代卡允许的最大深度是24。
- en: Performing `cudaDeviceSynchronize()` from inside the kernel is a very costly
    operation and should be avoided as much as possible.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从内核内部执行`cudaDeviceSynchronize()`是一个非常昂贵的操作，应尽量避免。
- en: There is additional memory preallocated on global memory so that we can store
    kernels before they are launched.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在全局内存上预先分配了额外的内存，以便在启动之前存储内核。
- en: If the kernel fails, the error is only visible from the host. Hence, you are
    advised to make use of the `-lineinfo` flag along with `cuda-memcheck` to locate
    the error's location.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果内核失败，错误只能从主机上看到。因此，建议您使用`-lineinfo`标志以及`cuda-memcheck`来定位错误的位置。
- en: Radix sort
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基数排序
- en: 'Another very popular sorting algorithm is radix sort as it is really fast on
    sequential machines. The fundamental policy of radix sort is that each element
    is sorted digit by digit. Let''s look at a simple example to explain the steps
    involved in radix sort:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常流行的排序算法是基数排序，因为它在顺序机器上非常快。基数排序的基本策略是每个元素都按位排序。让我们看一个简单的例子来解释基数排序涉及的步骤：
- en: 'Suppose the elements to be sorted are as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 假设要排序的元素如下：
- en: '| Value | 7 | 14 | 4 | 1 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 值 | 7 | 14 | 4 | 1 |'
- en: 'The equivalent binary values of these numbers are as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字的等效二进制值如下：
- en: '| Bits | 0111 | 1110 | 0100 | 0001 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 位 | 0111 | 1110 | 0100 | 0001 |'
- en: 'The first step is to sort based on bit 0\. Bit 0 for the numbers are as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是根据第0位进行排序。这些数字的第0位如下：
- en: '| 0^(th) Bit | 1 | 0 | 0 | 1 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 0位 | 1 | 0 | 0 | 1 |'
- en: 'To sort based on the *o^(th)* bit basically means that all the zeroes are on
    the left. All the ones are on the right while preserving the order of elements:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 根据第o位排序基本上意味着所有的零都在左边。所有的1都在右边，同时保持元素的顺序：
- en: '| Sorted value on 0^(th) bit | 14 | 4 | 7 | 1 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 第0位上的排序值 | 14 | 4 | 7 | 1 |'
- en: '| Sorted bits based on 0^(th) bit | 1110 | 0100 | 0111 | 0001 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 根据第0位排序的位 | 1110 | 0100 | 0111 | 0001 |'
- en: 'After the 0^(th) bit is done, we move on to the first bit. The result after
    sorting based on the first bit is as follows:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 第0位完成后，我们继续到第一位。根据第一位排序后的结果如下：
- en: '| Sorted value on the first bit | 4 | 14 | 7 | 1 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 第一位上的排序值 | 4 | 14 | 7 | 1 |'
- en: '| Sorted bits based on the first bit | 0100 | 1110 | 0111 | 0001 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 根据第一位排序的位 | 0100 | 1110 | 0111 | 0001 |'
- en: 'Then, we move on to the next higher bit until all the bits are over. The final
    result is as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续到下一个更高的位，直到所有的位都结束。最终结果如下：
- en: '| Sorted value on all bits | 1 | 4 | 7 | 1 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 所有位上的排序值 | 1 | 4 | 7 | 1 |'
- en: '| Sorted bits based on all bits | 0001 | 0100 | 0111 | 1110 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 根据所有位排序的位 | 0001 | 0100 | 0111 | 1110 |'
- en: As you can see, the upper limit that we set in this example was 4 bits. For
    larger numbers, such as integers, this will continue until 32 bits as integers
    are 32-bit.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，在这个例子中我们设置的上限是4位。对于更大的数字，比如整数，这将持续到32位，因为整数是32位的。
- en: Now that we have understood this algorithm, let's look at how this can be implemented
    in the GPU. Compared to the other sections in this chapter, we will take two approaches
    to showcase the CUDA ecosystem so that we can implement/use radix sort.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了这个算法，让我们看看如何在GPU中实现它。与本章中的其他部分相比，我们将采取两种方法来展示CUDA生态系统，以便我们可以实现/使用基数排序。
- en: '**Option 1**: We are going to make use of a warp level to do radix sort on
    just 32 elements. The reason for this is that we want to make use of radix sort
    to introduce you to warp-level primitives.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '**选项1**：我们将使用翘曲级别来对32个元素进行基数排序。这样做的原因是我们希望利用基数排序来向您介绍翘曲级别原语。'
- en: '**Option 2**: We will be making use of the Thrust library, which is part of
    the CUDA Toolkit. It implements a generic radix sort. The best implementation
    is reuse. Since Thrust already provides one of the best implementations of radix
    sort, we will use that.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**选项2**：我们将使用CUDA工具包的一部分Thrust库。它实现了通用基数排序。最好的实现是重用。由于Thrust已经提供了最好的基数排序实现之一，我们将使用它。'
- en: Two approaches
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两种方法
- en: To ease your understanding, let's begin with the example code. In this example,
    we will be making use of warp-level primitives and the Thrust library to implement/use
    the radix sort. The example code can be found at `07_parallel_programming_pattern/07_radixsort`.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便您的理解，让我们从示例代码开始。在这个例子中，我们将使用翘曲级别原语和Thrust库来实现/使用基数排序。示例代码可以在`07_parallel_programming_pattern/07_radixsort`中找到。
- en: 'Compile your application with the `nvcc` compiler with the following command:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令使用`nvcc`编译器编译您的应用程序：
- en: 'Warp-level primitive version:'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翘曲级别原语版本：
- en: '[PRE56]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Thrust library version:'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thrust库版本：
- en: '[PRE57]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: These two examples show the sorted output that's given by the GPU. Now, let's
    review how these operations are implemented in detail.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个例子展示了GPU给出的排序输出。现在，让我们详细了解这些操作是如何实现的。
- en: Approach 1 – warp-level primitives
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法1 - 翘曲级别原语
- en: 'Let''s look at how CUDA warp-level primitives are used to implement our algorithm
    in the code:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看CUDA翘曲级别原语是如何在代码中实现我们的算法的：
- en: 'First, load the data from global memory to shared memory:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，将数据从全局内存加载到共享内存中：
- en: '[PRE58]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The size of the memory is equal to the warp size, `*2`, so that it can implement
    the ping pong buffer.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 内存的大小等于翘曲大小，*2，以便它可以实现乒乓缓冲区。
- en: 'Loop through from the lower bit to the upper bit:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从低位到高位循环：
- en: '[PRE59]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Get the current bitmask:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取当前的掩码：
- en: '[PRE60]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Get the number of ones and zeroes (histogram):'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取1和0的数量（直方图）：
- en: '[PRE61]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Get the positions of the threads that have zero (0) in the current bit (Prefix
    Sum).
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取当前位数为零（0）的线程的位置（前缀和）。
- en: 'Get the positions of the threads that have one (1) in the current bit (Prefix
    Sum):'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取当前位数为一（1）的线程的位置（前缀和）：
- en: '[PRE62]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Store the data in the ping pong shared buffer memory:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据存储在乒乓共享缓冲区内存中：
- en: '[PRE63]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Repeat steps 2-6 until the upper bit has been reached.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2-6，直到达到上限位。
- en: 'Store the final result in global memory from shared memory:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从共享内存中将最终结果存储到全局内存中：
- en: '[PRE64]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: It may not be clear to you where the histogram and prefix sum appeared all of
    a sudden. Let's talk about this implementation in detail so that we can understand how
    we use warp-level primitives to implement the same.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 也许对于您来说，直方图和前缀和突然出现可能不太清楚。让我们详细讨论这个实现，以便我们可以理解如何使用翘曲级别原语来实现相同的功能。
- en: At the beginning of this section, we described how we sort using an example.
    What we did not cover, however, was how to find out the position of the element
    that needs to be swapped. Radix sort can be implemented using fundamental primitives
    such as histogram and prefix-sum and, hence, can easily be implemented in the
    GPU.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的开头，我们描述了如何使用示例进行排序。然而，我们没有涵盖的是如何找出需要交换的元素的位置。基数排序可以使用基本原语（如直方图和前缀和）来实现，因此可以很容易地在GPU上实现。
- en: 'Let''s revisit the example we looked at and gather its details, included the
    steps of the histogram and prefix sum. The following table shows various calculations
    that were done at each bit iteratively:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视我们看过的示例，并收集其细节，包括直方图和前缀和的步骤。以下表格显示了在每个位上迭代进行的各种计算：
- en: '| Value | 7 | 14 | 4 | 1 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 值 | 7 | 14 | 4 | 1 |'
- en: '| Binary | 0111 | 1110 | 0100 | 0001 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 二进制 | 0111 | 1110 | 0100 | 0001 |'
- en: '| Bit 0 | 1 | 0 | 0 | 1 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 位 0 | 1 | 0 | 0 | 1 |'
- en: '| Histogram prefix sum | 2 | 0 | 2 | 2 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 直方图前缀和 | 2 | 0 | 2 | 2 |'
- en: '| Offset | 0 | 0 | 1 | 1 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 偏移 | 0 | 0 | 1 | 1 |'
- en: '| New index (prefix Sum and Offset) | 2 | 0 | 1 | 3 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 新索引（前缀和和偏移） | 2 | 0 | 1 | 3 |'
- en: 'Let''s explain each and every calculation shown in the preceding table, as
    follows:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释前面表格中显示的每一项计算，如下所示：
- en: 'First, we build a histogram for the number of elements with 0 in the 0th-bit
    position and the number of elements with 1 in the 0th-bit position:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们为第0位位置的元素构建直方图，包括具有0和1的元素的数量：
- en: '*Histogram: zero-bits (2 values), one-bits (2 values)*'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '*直方图：零位（2个值），一位（2个值）*'
- en: Then, we perform an exclusive prefix sum on these values. The prefix sum can
    be defined as the sum of all the previous values. In our case, we do this separately
    for both 0^(th)-bits and 1^(st)-bits.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对这些值进行排他性前缀和。前缀和可以定义为所有先前值的总和。在我们的情况下，我们分别对0位和1位进行这样的操作。
- en: Finally, we move the elements based on the prefix sum values.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们根据前缀和的值移动元素。
- en: The warp-level primitives we used to find the histogram and prefix sum were `__ballot_sync()` and `__popc()`,
    respectively.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来找到直方图和前缀和的warp级原语分别是`__ballot_sync()`和`__popc()`。
- en: The `__ballot_sync()` API evaluates the predicates for all the active threads
    of the warp and returns an integer whose Nth bit is set if, and only if, the predicate
    evaluates to non-zero for the Nth thread of the warp. `__popc()`, which counts
    the number of integers, was set to one.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '`__ballot_sync()` API评估warp的所有活动线程的谓词，并返回一个整数，其第N位设置为1，当且仅当谓词对于warp的第N个线程求值为非零时。`__popc()`用于计算整数的数量，被设置为1。'
- en: In the CUDA programming model, we have seen that the minimum execution unit
    is a warp (32 threads). CUDA provides various warp-level primitives with fine-grained
    control that, in many applications, can result in better performance. We covered
    one such primitive, `__bllot__sync()`, in the previous section. Other important
    warp-level primitives include `shuffle` instructions, which are used for doing
    warp-level reduction in particular. `shuffle` instructions have already been covered
    in this book. If you have reached ninja programmer level proficiency in CUDA,
    then we recommend that you look at the CUDA API guide to understand more of these
    warp-level primitives.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA编程模型中，我们已经看到最小的执行单元是一个warp（32个线程）。CUDA提供了各种warp级原语，可以进行细粒度控制，在许多应用中可以实现更好的性能。我们在上一节介绍了一个这样的原语`__ballot_sync()`。其他重要的warp级原语包括`shuffle`指令，用于特定的warp级归约。`shuffle`指令已经在本书中介绍过。如果您已经达到了CUDA的忍者程序员水平，那么我们建议您查看CUDA
    API指南，以了解更多这些warp级原语。
- en: This completes describing radix sort using warp-level primitives. Now, let's
    look at the Thrust-based library implementation.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了使用warp级原语描述基数排序。现在，让我们看看基于Thrust库的实现。
- en: Approach 2 – Thrust-based radix sort
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法2 - 基于Thrust的基数排序
- en: Thrust-based radix sort is a generic implementation of radix sort and works
    pretty well for different types of data, such as integer, float, or key-value
    pairs. We would like to reemphasize the fact that sorting is a heavily studied
    algorithm and so has its parallel implementation. Therefore, we recommend that
    you reuse existing libraries before implementing one on your own.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Thrust的基数排序是基数排序的通用实现，对于不同类型的数据（如整数、浮点数或键值对）都能很好地工作。我们想再次强调排序是一个经过深入研究的算法，因此有其并行实现。因此，我们建议在自己实现之前重用现有的库。
- en: 'The steps involved in making use of Thrust for radix sort are as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Thrust进行基数排序的步骤如下：
- en: 'Import the relevant header files (Thrust is a header-only library, similar
    to STL):'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的头文件（Thrust是一个仅包含头文件的库，类似于STL）：
- en: '[PRE65]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Declare and initialize a device vector:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明并初始化设备向量：
- en: '[PRE66]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Perform sorting on the initialized device vector:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对初始化的设备向量进行排序：
- en: '[PRE67]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Using this library provides an easier and robust approach. Thrust provides different
    types of sorting methods, including radix sort for integers and floats. Alternatively,
    you can create a custom comparator to do customized sortings, such as sorting
    all the event numbers followed by odd numbers, sorting in descending order, and
    so on. You are advised to look at sample examples that have been provided by CUDA
    if you want to learn more about Thrust-based sorting examples.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个库提供了一种更简单和更健壮的方法。Thrust提供了不同类型的排序方法，包括整数和浮点数的基数排序。或者，您可以创建一个自定义比较器来进行自定义排序，例如按照偶数后面是奇数的顺序排序，按降序排序等等。如果您想了解更多关于基于Thrust的排序示例，建议您查看CUDA提供的示例示例。
- en: Now, we have looked at both approaches to implementing radix sort on GPUs.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经看过了在GPU上实现基数排序的两种方法。
- en: Summary
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at the implementation of commonly used algorithms
    and patterns in CUDA. These algorithms and patterns are commonly available. We
    covered basic optimization techniques in matrix multiplication and convolution
    filtering. Then, we expanded our discussion on how to parallelize the problem
    by using prefix sum, N-body, histogram, and sorting. To do this, we have used
    dedicated GPU knowledge, libraries, and lower-level primitives.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看了CUDA中常用算法和模式的实现。这些算法和模式是常见的。我们涵盖了矩阵乘法和卷积滤波中的基本优化技术。然后，我们扩展了讨论，介绍了如何通过使用前缀和、N体、直方图和排序来并行化问题。为此，我们使用了专门的GPU知识、库和较低级别的原语。
- en: 'Many of the algorithms we have covered are implemented in CUDA libraries. For
    example, matrix multiplication is in the cuBLAS library, while convolution is
    in the CUDNN library. In addition, we have covered two approaches in the radix
    sort implementation: using the Thrust library or warp-level primitives for histogram
    computation.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所涵盖的许多算法都是在CUDA库中实现的。例如，矩阵乘法在cuBLAS库中，而卷积在CUDNN库中。此外，我们还涵盖了基数排序实现中的两种方法：使用Thrust库或warp级原语进行直方图计算。
- en: Now that you've seen how these patterns can be implemented in commonly used
    libraries, the next logical step is to see how we can use these libraries. This
    is what we will be doing in the next chapter.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了这些模式如何在常用库中实现，下一个合乎逻辑的步骤是看看我们如何可以使用这些库。这就是我们将在下一章中要做的事情。
