- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Parallel Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行算法
- en: The previous chapters have focused on how to introduce concurrency and asynchrony
    in our programs by using threads and coroutines. This chapter focuses on parallel
    execution of independent tasks, which is related to but distinct from concurrency.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的章节着重介绍了如何通过使用线程和协程在我们的程序中引入并发和异步性。本章侧重于独立任务的并行执行，这与并发相关但又不同。
- en: In earlier chapters, I stressed that I prefer standard library algorithms over
    handcrafted `for`-loops. In this chapter, you will see some great advantages of
    using standard library algorithms with the execution policies introduced with
    C++17.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我强调我更喜欢使用标准库算法而不是手工制作的`for`循环。在本章中，您将看到使用C++17引入的执行策略与标准库算法的一些巨大优势。
- en: This chapter is not going to go in depth into theories of parallelizing algorithms
    or parallel programming in general, as these subjects are far too complex to cover
    in a single chapter. Also, there are a multitude of books on this subject. Instead,
    this chapter is going to take a more practical approach and demonstrate how to
    extend a current C++ code base to utilize parallelism while preserving the readability
    of the code base. In other words, we do not want the parallelism to get in the
    way of readability; rather, we want the parallelism to be abstracted away so that
    parallelizing the code is only a matter of changing a parameter to an algorithm.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不会深入探讨并行算法或并行编程的理论，因为这些主题太复杂，无法在一章中涵盖。此外，关于这个主题有很多书籍。相反，本章将采取更实际的方法，演示如何扩展当前的C++代码库以利用并行性，同时保持代码库的可读性。换句话说，我们不希望并行性影响可读性；相反，我们希望将并行性抽象出来，使得并行化代码只是改变算法的一个参数。
- en: 'In this chapter, you will learn:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学到：
- en: Various techniques for implementing parallel algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现并行算法的各种技术
- en: How to evaluate the performance of parallel algorithms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何评估并行算法的性能
- en: How to adapt a code base to use the parallel extensions of the standard library
    algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何调整代码库以使用标准库算法的并行扩展
- en: Parallel programming is a complicated topic, so before starting, you need to
    understand the motivation for introducing parallelism in the first place.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 并行编程是一个复杂的话题，因此在开始之前，您需要了解引入并行性的动机。
- en: The importance of parallelism
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行性的重要性
- en: From a programmer's perspective, it would be very convenient if the computer
    hardware of today was a 100 GHz single core CPU rather than a 3 GHz multi-core
    CPU; we wouldn't need to care about parallelism. Unfortunately, making single-core
    CPUs faster and faster has hit a physical limit. So, as the evolution of computer
    hardware is going in the direction of multi-core CPUs and programmable GPUs, programmers
    have to use efficient parallel patterns in order to make the most of the hardware.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从程序员的角度来看，如果今天的计算机硬件是一个100 GHz的单核CPU而不是一个3 GHz的多核CPU，那将非常方便；我们就不需要关心并行性。不幸的是，使单核CPU变得越来越快已经达到了物理极限。因此，随着计算机硬件的发展朝着多核CPU和可编程GPU的方向发展，程序员必须使用高效的并行模式来充分利用硬件。
- en: Parallel algorithms allow us to optimize our programs by executing multiple
    individual tasks or subtasks at the exact same time on a multi-core CPU or GPU.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 并行算法允许我们通过在多核CPU或GPU上同时执行多个单独的任务或子任务来优化我们的程序。
- en: Parallel algorithms
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行算法
- en: As mentioned in *Chapter 11*, *Concurrency*, the terms *concurrency* and *parallelism*
    can be a little hard to distinguish from each other. As a reminder, a program
    is said to run concurrently if it has multiple individual control flows running
    during overlapping time periods. On the other hand, a parallel program executes
    multiple tasks or subtasks simultaneously (at the exact same time), which requires
    hardware with multiple cores. We use parallel algorithms to optimize latency or
    throughput. It makes no sense to parallelize algorithms if we don't have hardware
    that can execute multiple tasks simultaneously to achieve better performance.
    A few simple formulas will now follow to help you understand what factors need
    to be considered when evaluating parallel algorithms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在*第11章*，*并发*中提到的，*并发*和*并行*这两个术语有时很难区分。作为提醒，如果程序在重叠的时间段内具有多个单独的控制流，则称该程序在并发运行。另一方面，并行程序同时执行多个任务或子任务（在完全相同的时间），这需要具有多个核心的硬件。我们使用并行算法来优化延迟或吞吐量。如果没有硬件可以同时执行多个任务以实现更好的性能，那么并行化算法就毫无意义。现在将介绍一些简单的公式，以帮助您了解在评估并行算法时需要考虑哪些因素。
- en: Evaluating parallel algorithms
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估并行算法
- en: 'In this chapter, **speedup** is defined as the ratio between a sequential and
    a parallel version of an algorithm, as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，**加速比**被定义为顺序算法和并行算法之间的比率，如下所示：
- en: '![](img/B15619_14_001.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_14_001.png)'
- en: '*T*[1] is the time it takes to solve a problem using a sequential algorithm
    executing at one core, and *T*[n] is the time it takes to solve the same problem
    using *n* cores. *Time* refers to wall-clock time (not CPU time).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*T*[1]是使用顺序算法在一个核心上执行解决问题所需的时间，*T*[n]是使用*n*个核心解决相同问题所需的时间。*时间*指的是挂钟时间（而不是CPU时间）。'
- en: A parallel algorithm is usually more complicated and requires more computational
    resources (CPU time, for example) compared to its sequential equivalent. The benefits
    of the parallel version come from the ability to spread the algorithm onto several
    processing units.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与其顺序等效物相比，并行算法通常更复杂，需要更多的计算资源（例如CPU时间）。并行版本的好处来自于能够将算法分布到多个处理单元上。
- en: 'With that in mind, it''s also notable that not all algorithms gain the same
    performance boost when run in parallel. The **efficiency** of a parallel algorithm
    can be computed by the following formula:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，值得注意的是，并非所有算法在并行运行时都能获得相同的性能提升。并行算法的**效率**可以通过以下公式计算：
- en: '![](img/B15619_14_002.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_14_002.png)'
- en: In this formula, *n* is the number of cores executing the algorithm. Since *T*[1]/*T*[n]
    denote the speedup, the efficiency can also be expressed as *Speedup*/*n*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，*n*是执行算法的核心数。由于*T*[1]/*T*[n]表示加速比，效率也可以表示为*加速比*/*n*。
- en: If the efficiency is *1.0*, the algorithm parallelizes perfectly. For example,
    it means that we achieve an 8x speedup when executing a parallel algorithm on
    a computer with eight cores. In practice, though, there are a multitude of parameters
    that limit parallel execution, such as creating threads, memory bandwidth, and
    context switches, as mentioned in *Chapter 11*, *Concurrency.* So, typically,
    the efficiency is well below 1.0.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果效率为*1.0*，则算法并行化完美。例如，这意味着在具有八个核心的计算机上执行并行算法时，我们可以实现8倍的加速。但在实践中，有许多参数限制了并行执行，比如创建线程、内存带宽和上下文切换，正如*第11章*，*并发*中所述。因此，通常效率远低于1.0。
- en: The efficiency of a parallel algorithm depends on how independently each chunk
    of work can be processed. For example, `std::transform()` is trivial to parallelize
    in the sense that each element is processed completely independently of every
    other. This will be demonstrated later in this chapter.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 并行算法的效率取决于每个工作块的独立处理程度。例如，`std::transform()`在某种意义上是非常容易并行化的，因为每个元素的处理完全独立于其他元素。这将在本章后面进行演示。
- en: The efficiency also depends on the problem size and the number of cores. For
    example, a parallel algorithm may perform very poorly on small data sets due to
    the overhead incurred by the added complexity of a parallel algorithm. Likewise,
    executing a program on a great many cores might hit other bottlenecks in the computer
    such as memory bandwidth. We say that a parallel algorithm scales if the efficiency
    stays constant when we change the number of cores and/or the size of the input.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 效率还取决于问题的规模和核心数量。例如，由于并行算法的复杂性增加而导致的开销，一个并行算法在小数据集上可能表现非常糟糕。同样，在计算机上执行程序时，可能会遇到其他瓶颈，比如内存带宽，这可能会导致在大量核心上执行程序时性能下降。我们说一个并行算法是可扩展的，如果效率在改变核心数量和/或输入规模时保持不变。
- en: It's also important to keep in mind that not all parts of a program can be parallelized.
    This fact limits the theoretical maximum speedup of a program even if we had an
    unlimited number of cores. We can compute the maximum possible speedup by using
    **Amdahl's law**, which was introduced in *Chapter 3*, *Analyzing and Measuring
    Performance*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是要记住，并非程序的所有部分都可以并行化。即使我们有无限数量的核心，这个事实也限制了程序的理论最大加速。我们可以使用**阿姆达尔定律**来计算最大可能的加速，这是在*第3章*，*分析和测量性能*中介绍的。
- en: Amdahl's law revisited
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阿姆达尔定律的再审视
- en: 'Here, we will apply Amdahl''s law to parallel programs. It works like this:
    the total running time of a program can be split into two distinct parts or *fractions*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将阿姆达尔定律应用于并行程序。它的工作原理是：程序的总运行时间可以分为两个不同的部分或*比例*：
- en: '*F*[seq] is the fraction of the program that can only be executed *sequentially*'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F*[seq]是程序中只能按顺序执行的部分的比例'
- en: '*F*[par] is the fraction of the program that can be executed in *parallel*'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F*[par]是程序中可以并行执行的部分的比例'
- en: 'Since these two fractions together make up the entire program, it means that
    *F*[seq] = 1 - *F*[par]. Now, Amdahl''s law tells us that the **maximum speedup**
    of a program executing on *n* cores is:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个比例加起来构成了整个程序，这意味着*F*[seq] = 1 - *F*[par]。现在，阿姆达尔定律告诉我们，程序在*n*个核心上执行的**最大加速**是：
- en: '![](img/B15619_14_003.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_14_003.png)'
- en: 'To visualize the effect of this law, the following image shows the execution
    time of a program with the sequential fraction at the bottom and the parallel
    fraction on the top. Increasing the number of cores only affects the parallel
    fraction, which sets a limit on the maximum speedup:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这个定律的效果，下面的图像显示了一个程序的执行时间，其中底部是顺序部分，顶部是并行部分。增加核心数量只会影响并行部分，这限制了最大加速比。
- en: '![](img/B15619_14_01.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_14_01.png)'
- en: 'Figure 14.1: Amdahl''s law defines the maximum speedup; in this case it is
    2x'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：阿姆达尔定律定义了最大加速；在这种情况下是2倍
- en: In the figure above, the sequential part accounts for 50% of the execution time
    when running on a single CPU. Therefore, the maximum speedup we can achieve by
    adding more cores when executing such a program is 2x.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，当在单个CPU上运行时，顺序部分占执行时间的50%。因此，当执行这样的程序时，通过增加更多核心，我们可以实现的最大加速是2倍。
- en: To give you some idea of how parallel algorithms can be implemented, we will
    now go through a few examples. We will begin with `std::transform()` because it
    is relatively easy to split into multiple independent parts.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您了解并行算法是如何实现的，我们现在将通过一些示例来说明。我们将从`std::transform()`开始，因为它相对容易分成多个独立的部分。
- en: Implementing parallel std::transform()
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现并行的std::transform()
- en: Although algorithmically `std::transform()` is easy to implement, in practice,
    implementing even a rudimentary parallel version is more complex than it might
    appear at first sight.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从算法上来说`std::transform()`很容易实现，但在实践中，实现一个初级的并行版本比看上去更复杂。
- en: 'The algorithm `std::transform()` calls a function for each element in a sequence,
    and stores the result in another sequence. A possible implementation of a sequential
    version of `std::transform()` may look something like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 算法`std::transform()`为序列中的每个元素调用一个函数，并将结果存储在另一个序列中。`std::transform()`的顺序版本的一个可能实现可能看起来像这样：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The standard library version also returns the `dst` iterator, but we will ignore
    that in our examples. To understand the challenges with a parallel version of
    `std::transform()`, let's begin with a naive approach.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 标准库版本也返回`dst`迭代器，但在我们的示例中将忽略它。为了理解`std::transform()`并行版本的挑战，让我们从一个天真的方法开始。
- en: Naive implementation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 天真的实现
- en: 'A naive parallel implementation of `std::transform()` would probably look something
    like this:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::transform()`的一个天真的并行实现可能看起来像这样：'
- en: Divide the elements into chunks corresponding to the number of cores in the
    computer
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将元素分成与计算机核心数相对应的块
- en: Process each chunk in a separate task
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单独的任务中处理每个块
- en: Wait for all tasks to finish
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等待所有任务完成
- en: 'Using `std::thread::hardware_concurrency()` to determine the number of supported
    hardware threads, a possible implementation could look like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `std::thread::hardware_concurrency()` 来确定支持的硬件线程数量，可能的实现如下：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that `hardware_concurrency()` might return `0` if it, for some reason,
    is undetermined, and therefore is clamped to be at least one.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果 `hardware_concurrency()` 由于某种原因无法确定，可能会返回 `0`，因此会被夹制为至少为 1。
- en: A subtle difference between `std::transform()` and our parallel version is that
    they put different requirements on the iterators. `std::transform()` can operate
    on input and output iterators such as `std::istream_iterator<>` bound to `std::cin`.
    This is not possible with `par_transform_naive()` since the iterators are copied
    and used from multiple tasks. As you will see, there are no parallel algorithms
    presented in this chapter that can operate on input and output iterators. Instead,
    the parallel algorithms at least require forward iterators that allow multi-pass
    traversal.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::transform()` 和我们的并行版本之间的一个细微差别是它们对迭代器有不同的要求。`std::transform()` 可以操作输入和输出迭代器，比如绑定到
    `std::cin` 的 `std::istream_iterator<>`。这对于 `par_transform_naive()` 是不可能的，因为迭代器被复制并且从多个任务中使用。正如你将看到的，本章中没有呈现可以操作输入和输出迭代器的并行算法。相反，并行算法至少需要允许多次遍历的前向迭代器。'
- en: Performance evaluation
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能评估
- en: Continuing the naive implementation, let's measure its performance with a simple
    performance evaluation compared to the sequential version of `std::transform()`
    executing at a single CPU core.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用朴素实现，让我们通过与在单个 CPU 核心上执行的顺序版本 `std::transform()` 的简单性能评估来测量其性能。
- en: In this test we will measure the time (clock on the wall) and the total time
    spent on the CPUs when varying the input size of the data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个测试中，我们将测量数据的输入大小变化时的时间（挂钟时间）和在 CPU 上花费的总时间。
- en: 'We will set up this benchmark using Google Benchmark, which was introduced
    in *Chapter 3*, *Analyzing and Measuring Performance*. To avoid duplicating code,
    we''ll implement a function that will set up a test fixture for our benchmark.
    The fixture needs a source range with some example values, a destination range
    for the result, and a transform function:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在 *第 3 章* *分析和测量性能* 中介绍的 Google Benchmark 来设置这个基准。为了避免重复代码，我们将实现一个函数来为我们的基准设置一个测试夹具。夹具需要一个包含一些示例值的源范围，一个用于结果的目标范围，以及一个转换函数：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we have our fixture set up, it''s time to implement the actual benchmark.
    There will be two versions: one for the sequential `std::transform()` and one
    for our parallel version, `par_transform_naive()`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了我们的夹具，是时候实现实际的基准了。将会有两个版本：一个用于顺序的 `std::transform()`，一个用于我们的并行版本 `par_transform_naive()`：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Only the code within the `for`-loops will be measured. By using `state.range(0)`
    for input size, we can generate different values by appending a range of values
    to each benchmark. In fact, we need to specify a couple of arguments for each
    benchmark, so we create a helper function that applies all the settings we need:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 只有 `for`-循环内的代码将被测量。通过使用 `state.range(0)` 作为输入大小，我们可以通过将一系列值附加到每个基准来生成不同的值。实际上，我们需要为每个基准指定一些参数，因此我们创建一个帮助函数，应用我们需要的所有设置：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A few things to note about the custom arguments:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自定义参数的一些注意事项：
- en: We pass the values 50, 10,000, and 1,000,000 as arguments to the benchmark.
    They are used as the input size when creating the vectors in the `setup_fixture()`
    function. These values are accessed using `state.range(0)` in the test functions.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将值 50、10,000 和 1,000,000 作为基准的参数传递。它们在创建 `setup_fixture()` 函数中的向量时用作输入大小。在测试函数中使用
    `state.range(0)` 访问这些值。
- en: By default, Google Benchmark only measures CPU time on the main thread. But
    since we are interested in the total amount of CPU time on all threads, we use
    `MeasureProcessCPUTime()`.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，Google Benchmark 只在主线程上测量 CPU 时间。但由于我们对所有线程上的 CPU 时间总量感兴趣，我们使用 `MeasureProcessCPUTime()`。
- en: Google Benchmark decides how many times each test needs to be repeated until
    a statistically stable result has been achieved. We want the library to use the
    clock-on-the-wall time for this rather than CPU time, and therefore we apply the
    setting `UseRealTime()`.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Benchmark 决定每个测试需要重复多少次，直到达到统计上稳定的结果。我们希望库在这方面使用挂钟时间而不是 CPU 时间，因此我们应用设置
    `UseRealTime()`。
- en: 'That''s almost it. Finally, register the benchmarks and call main:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎就是了。最后，注册基准并调用 main：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After compiling this code with optimizations turned on (using gcc with -O3),
    I executed this benchmark on a laptop using eight cores. The following table shows
    the results when using 50 elements:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用优化后的代码（使用 gcc 和 -O3）编译后，我在一台具有八个核心的笔记本电脑上执行了这个基准。以下表格显示了使用 50 个元素时的结果：
- en: '| Algorithm | CPU | Time | Speedup |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: 算法 | CPU | 时间 | 加速比 |
- en: '| `std::transform()` | 0.02 ms | 0.02 ms | 0.25x |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '`std::transform()` | 0.02 毫秒 | 0.02 毫秒 | 0.25x |'
- en: '| `par_transform_naive()` | 0.17 ms | 0.08 ms |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '`par_transform_naive()` | 0.17 毫秒 | 0.08 毫秒 |'
- en: '*CPU* is the total time spent on the CPU. *Time* is the wall-clock time, which
    is what we are most interested in. *Speedup* is the relative speedup when comparing
    the elapsed time of the sequential version with the parallel version (0.02/0.08
    in this case).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*CPU* 是在 CPU 上花费的总时间。*时间* 是挂钟时间，这是我们最感兴趣的。*加速比* 是比较顺序版本的经过时间和并行版本的相对加速比（在这种情况下为
    0.02/0.08）。'
- en: 'Clearly, the sequential version outperforms the parallel algorithm for this
    small data set with only 50 elements. With 10,000 elements we really start to
    see the benefits of parallelization though:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，对于只有 50 个元素的小数据集，顺序版本的性能优于并行算法。但是，当有 10,000 个元素时，我们真的开始看到并行化的好处：
- en: '| Algorithm | CPU | Time | Speedup |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: 算法 | CPU | 时间 | 加速比 |
- en: '| `std::transform()` | 0.89 ms | 0.89 ms | 4.5x |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '`std::transform()` | 0.89 毫秒 | 0.89 毫秒 | 4.5x |'
- en: '| `par_transform_naive()` | 1.95 ms | 0.20 ms |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '`par_transform_naive()` | 1.95 毫秒 | 0.20 毫秒 |'
- en: 'Finally, using 1,000,000 elements gives us even higher efficiency, as can be
    seen in the following table:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用 1,000,000 个元素给我们带来了更高的效率，如下表所示：
- en: '| Algorithm | CPU | Time | Speedup |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | CPU | 时间 | 加速比 |'
- en: '| `std::transform()` | 9071 ms | 9092 ms | 7.3x |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `std::transform()` | 9071 ms | 9092 ms | 7.3x |'
- en: '| `par_transform_naive()` | 9782 ms | 1245 ms |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `par_transform_naive()` | 9782 ms | 1245 ms |'
- en: 'The efficiency of the parallel algorithm in this last run is really high. It
    was executed on eight cores, so the efficiency is 7.3x/8 = 0.925\. The results
    presented here (both the absolute execution time and the relative speedup) should
    not be relied upon too much. Among other things, the results depend on the computer
    architecture, the OS scheduler, and how much other work is currently running on
    the machine when performing the test. Nevertheless, the benchmarking results confirm
    a few important points discussed earlier:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次运行中，并行算法的效率非常高。它在八个核心上执行，因此效率为7.3x/8 = 0.925。这里呈现的结果（绝对执行时间和相对加速比）不应过分依赖。结果取决于计算机架构、操作系统调度程序以及在执行测试时当前机器上运行的其他工作量。尽管如此，基准测试结果证实了前面讨论的一些重要观点：
- en: For small data sets, the sequential version `std::transform()` is much faster
    than the parallel version because of the overhead incurred by creating threads
    etc.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于小数据集，由于创建线程等产生的开销，顺序版本`std::transform()`比并行版本快得多。
- en: The parallel version always uses more computational resources (CPU time) compared
    to `std::transform()`.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与`std::transform()`相比，并行版本总是使用更多的计算资源（CPU时间）。
- en: For large data sets, the parallel version outperforms the sequential version
    when measuring wall-clock time. The speedup is over 7x when running on a machine
    with eight cores.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大数据集，当测量挂钟时间时，并行版本的性能优于顺序版本。在具有八个核心的机器上运行时，加速比超过7倍。
- en: One reason for the high efficiency of our algorithm (at least on large data
    sets) is that the computational cost is evenly distributed, and each subtask is
    highly independent. This is not always the case, though.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们算法效率高的一个原因（至少对于大数据集来说）是计算成本均匀分布，每个子任务高度独立。然而，并非总是如此。
- en: Shortcomings of the naive implementation
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 朴素实现的缺点
- en: The naive implementation might do a good job if each chunk of work has the same
    computational cost and the algorithm executes in an environment where no other
    application utilizes the hardware. However, this is rarely the case; rather, we
    want a good general-purpose parallel implementation that is both efficient and
    scalable.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个工作块的计算成本相同，并且算法在没有其他应用程序利用硬件的环境中执行，那么朴素实现可能会做得很好。然而，这种情况很少发生；相反，我们希望有一个既高效又可扩展的通用并行实现。
- en: 'The following illustrations show the problems we want to avoid. If the computational
    cost is not equivalent for each chunk, the implementation is limited to the chunk
    that takes the most time:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下插图显示了我们要避免的问题。如果每个块的计算成本不相等，实现将受限于花费最长时间的块：
- en: '![](img/B15619_14_02.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_14_02.png)'
- en: 'Figure 14.2: Possible scenarios where computation time is not proportional
    to chunk size'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2：计算时间与块大小不成比例的可能场景
- en: 'If the application and/or the operating system has other processes to handle,
    the operation will not process all chunks in parallel:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序和/或操作系统有其他进程需要处理，操作将无法并行处理所有块：
- en: '![](img/B15619_14_03.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_14_03.png)'
- en: 'Figure 14.3: Possible scenarios where computation time is proportional to chunk
    size'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：计算时间与块大小成比例的可能场景
- en: As you can see in *Figure 14.3*, splitting the operation into smaller chunks
    makes the parallelization adjust to the current condition, avoiding single tasks
    that stall the whole operation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在*图14.3*中所见，将操作分割成更小的块使并行化适应当前条件，避免了使整个操作停滞的单个任务。
- en: Also note that the naive implementation was unsuccessful for small data sets.
    There are many ways to adjust the naive implementation to perform better. For
    instance, we could create more tasks and smaller tasks by multiplying the number
    of cores by some factor greater than 1\. Or, to avoid significant overhead on
    small data sets, we could let the chunk size decide the number of tasks to create
    etc.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，对于小数据集，朴素实现是不成功的。有许多方法可以调整朴素实现以获得更好的性能。例如，我们可以通过将核心数乘以大于1的某个因子来创建更多任务和更小的任务。或者，为了避免在小数据集上产生显着的开销，我们可以让块大小决定要创建的任务数量等。
- en: You now have the knowledge of how to implement and evaluate a simple parallel
    algorithm. We will not do any fine-tuning of the naive implementation; instead,
    I will show a different useful technique to use when implementing parallel algorithms.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经知道如何实现和评估简单的并行算法。我们不会对朴素实现进行任何微调；相反，我将展示在实现并行算法时使用的另一种有用技术。
- en: Divide and conquer
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分而治之
- en: 'An algorithm technique for dividing a problem into smaller subproblems is called
    **divide and conquer**. We will here implement another version of a parallel transform
    algorithm using divide and conquer. It works as follows: if the input range is
    smaller than a specified threshold, the range is processed; otherwise, the range
    is split into two parts:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将问题分解为较小子问题的算法技术称为**分而治之**。我们将在这里实现另一个使用分而治之的并行转换算法版本。它的工作原理如下：如果输入范围小于指定的阈值，则处理该范围；否则，将范围分成两部分：
- en: The first part is processed on a newly branched task
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一部分在新分支的任务上处理
- en: The other part is recursively processed at the calling thread
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一部分在调用线程中进行递归处理
- en: 'The following illustration shows how the divide and conquer algorithm would
    recursively transform a range using the following data and parameters:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下插图显示了如何使用以下数据和参数递归地转换范围的分治算法：
- en: 'Range size: 16'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 范围大小：16
- en: Source range contains floats from 1.0 to 16.0
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源范围包含从1.0到16.0的浮点数
- en: 'Chunk size: 4'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块大小：4
- en: 'Transformation function: `[](auto x) { return x*x; }`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换函数：`[](auto x) { return x*x; }`
- en: '![](img/B15619_14_04.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_14_04.png)'
- en: 'Figure 14.4: A range is divided recursively for parallel processing. The source
    array contains float values from 1.0 to 8.0\. The destination array contains the
    transformed values.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4：一个范围被递归地分割以进行并行处理。源数组包含从1.0到8.0的浮点值。目标数组包含转换后的值。
- en: In *Figure 14.4*, you can see that the main task spawns two asynchronous tasks
    (**task 1** and **task 2**) and finally transforms the last chunk in the range.
    **Task 1** spawns **task 3** and then transforms the remaining elements containing
    values 5.0, 6.0, 7.0, and 8.0\. Let's head over to the implementation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图14.4*中，您可以看到主任务生成了两个异步任务（**任务1**和**任务2**），最后转换了范围中的最后一个块。**任务1**生成了**任务3**，然后转换了包含值5.0、6.0、7.0和8.0的剩余元素。让我们来看看实现。
- en: Implementation
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实施
- en: 'Implementation-wise, it''s quite a small bit of code. The incoming range is
    recursively split into two chunks; the first chunk is invoked as a new task, and
    the second chunk is recursively processed on the same task:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施方面，这是一小段代码。输入范围被递归地分成两个块；第一个块被调用为一个新任务，第二个块在同一个任务上被递归处理：
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Combining recursion with multithreading like this can take a while to get your
    head around. In the following examples, you will see that this pattern can be
    used when implementing more complicated algorithms as well. But first, let's see
    how it performs.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 将递归与多线程结合起来可能需要一段时间才能理解。在以下示例中，您将看到这种模式在实现更复杂的算法时可以使用。但首先，让我们看看它的性能如何。
- en: Performance evaluation
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能评估
- en: 'To evaluate our new version, we will modify the benchmark fixture by updating
    the transform function with a version that takes more time depending on the input
    value. The range of input values will be increased by filling the range using
    `std::iota()`. Doing this means the algorithms need to process jobs of different
    sizes. Here is the new `setup_fixture()` function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的新版本，我们将通过更新transform函数来修改基准测试装置，使其根据输入值的不同需要更长的时间。通过使用`std::iota()`填充范围来增加输入值的范围。这样做意味着算法需要处理不同大小的作业。以下是新的`setup_fixture()`函数：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can now try to find an optimal chunk size to be used by the divide-and-conquer
    algorithm by using an increasing parameter for the chunk size. It would also be
    interesting to see how our divide-and-conquer algorithm performs compared to the
    naive version on this new fixture, which needs to process jobs of different sizes.
    Here is the full code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以尝试通过使用递增的参数来找到分而治之算法的最佳块大小。看看我们的分而治之算法在这个新的装置上与朴素版本相比的表现，这需要处理不同大小的作业。以下是完整的代码：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following diagram reveals the results I achieved when running the tests
    on macOS using an Intel Core i7 CPU with eight cores:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了我在macOS上运行测试时所获得的结果，使用了一个拥有八个核心的英特尔Core i7 CPU：
- en: '![](img/B15619_14_05.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_14_05.png)'
- en: 'Figure 14.5: Comparison between our naive algorithm and the divide-and-conquer
    algorithm using different chunk sizes'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5：比较我们的朴素算法和使用不同块大小的分而治之算法
- en: The best efficiency was achieved when using chunk sizes of around 10,000 elements,
    which creates 1,000 tasks. With larger chunks, the performance is bottlenecked
    in the time it takes to process the final chunks, whereas too small chunks result
    in too much overhead in creating and invoking tasks compared to the computation.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用大约10,000个元素的块大小时，可以实现最佳效率，这将创建1,000个任务。使用更大的块时，性能会在处理最终块所需的时间上受到瓶颈，而使用太小的块会导致在创建和调用任务方面产生过多的开销，与计算相比。
- en: A takeaway from this example is that the performance penalty of scheduling 1,000
    smaller tasks rather than a few big ones isn't a problem here. It would be possible
    to restrict the number of threads using a thread pool, but `std::async()` seems
    to work fairly well in this scenario. A generic implementation would opt for using
    a fairly large number of tasks rather than trying to match the exact number of
    cores.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中可以得出的结论是，调度1,000个较小的任务而不是几个大任务所带来的性能惩罚在这里并不是一个问题。可以通过使用线程池来限制线程的数量，但在这种情况下`std::async()`似乎运行得相当好。通用的实现会选择使用相当大数量的任务，而不是试图匹配确切的核心数量。
- en: Finding optimal values for chunk size and the number of tasks is a real problem
    when implementing parallel algorithms. As you can see, it depends on many variables
    and also whether you optimize for latency or throughput. The best way to gain
    insights is to measure in the environment that your algorithms are supposed to
    run.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现并行算法时，找到最佳的块大小和任务数量是一个真正的问题。如您所见，这取决于许多变量，也取决于您是优化延迟还是吞吐量。获得洞察力的最佳方法是在您的算法应该运行的环境中进行测量。
- en: Now that you have learned how to implement a parallel transform algorithm using
    divide and conquer, let's see how the same technique can be applied to other problems.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经学会了如何使用分而治之来实现并行转换算法，让我们看看相同的技术如何应用到其他问题上。
- en: Implementing parallel std::count_if()
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现并行std::count_if()
- en: 'A nice thing with divide and conquer is that it can be applied to many problems.
    We can easily use the same technique to implement a parallel version of `std::count_if()`,
    with the difference being that we need to accumulate the returned value, like
    this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 分而治之的好处是它可以应用到许多问题上。我们可以很容易地使用相同的技术来实现`std::count_if()`的并行版本，唯一的区别是我们需要在函数末尾累加返回的值，就像这样：
- en: '[PRE9]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As you can see, the only difference here is that we need to sum the result
    at the end of the function. If you want to have the chunk size depend on the number
    of cores, you can easily wrap the `par_count_if()` in an outer function:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这里唯一的区别是我们需要在函数末尾对结果进行求和。如果您希望块大小取决于核心数量，您可以很容易地将`par_count_if()`包装在一个外部函数中：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The magic number 32 here is a somewhat arbitrary factor that will give us more
    chunks and smaller chunks if we are given a large input range. As usual, we would
    need to measure the performance to come up with a good constant here. Let's now
    move on and try to tackle a more complicated parallel algorithm.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的神奇数字32是一个相当任意的因子，如果我们有一个大的输入范围，它将给我们更多的块和更小的块。通常情况下，我们需要测量性能来得出一个好的常数。现在让我们继续尝试解决一个更复杂的并行算法。
- en: Implementing parallel std::copy_if()
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现并行std::copy_if()
- en: We've had a look at `std::transform()` and `std::count_if()`, which are quite
    easy to implement both sequentially and in parallel. If we take another algorithm
    that is easily implemented sequentially, `std::copy_if()`, things get a lot harder
    to perform in parallel.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了`std::transform()`和`std::count_if()`，它们在顺序和并行实现上都相当容易。如果我们再考虑另一个在顺序中容易实现的算法`std::copy_if()`，在并行中执行起来就会变得更加困难。
- en: 'Sequentially, implementing `std::copy_if()` is as easy as this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序地，实现`std::copy_if()`就像这样简单：
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To demonstrate how it can be used, consider the following example where we
    have a range that contains a sequence of integers and we want to copy only the
    odd integers into another range:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示如何使用它，考虑以下示例，其中我们有一个包含整数序列的范围，我们想要将奇数整数复制到另一个范围中：
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, if we want to make a parallel version of `copy_if()`, we immediately run
    into problems as we cannot write to the destination iterator concurrently. Here
    is a failed attempt with undefined behavior, since both tasks will write to the
    same position in the destination range:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想要制作`copy_if()`的并行版本，我们立即遇到问题，因为我们不能同时向目标迭代器写入。这是一个失败的尝试，具有未定义的行为，因为两个任务将同时写入目标范围中的相同位置：
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We now have two simple approaches: either we synchronize the index we write
    to (by using an atomic/lock-free variable), or we split the algorithm into two
    parts. We will explore both approaches next.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两种简单的方法：要么我们同步我们写入的索引（使用原子/无锁变量），要么我们将算法分成两部分。接下来我们将探索这两种方法。
- en: 'Approach 1: Use a synchronized write position'
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方法1：使用同步的写入位置
- en: The first approach we might consider is to synchronize the write position by
    using an atomic `size_t` and the `fetch_add()` member function, as you learned
    about in *Chapter 11*, *Concurrency*. Whenever a thread tries to write a new element,
    it fetches the current index and adds one atomically; thus, each value is written
    to a unique index.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能考虑的第一种方法是使用原子`size_t`和`fetch_add()`成员函数来同步写入位置，就像你在*第11章* *并发*中学到的那样。每当一个线程尝试写入一个新元素时，它原子地获取当前索引并添加一个；因此，每个值都被写入到一个唯一的索引。
- en: 'In our code, we will split the algorithm into two functions: an inner function
    and an outer function. The atomic write index will be defined in the outer function,
    whereas the main part of the algorithm will be implemented in the inner function.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码中，我们将算法分成两个函数：一个内部函数和一个外部函数。原子写入索引将在外部函数中定义，而算法的主要部分将在内部函数中实现。
- en: Inner function
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内部函数
- en: 'The inner function requires an atomic `size_t` that synchronizes the write
    positions. As the algorithm is recursive, it cannot store the atomic `size_t`
    itself; it requires an outer function to invoke the algorithm:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 内部函数需要一个同步写入位置的原子`size_t`。由于算法是递归的，它不能自己存储原子`size_t`；它需要一个外部函数来调用算法：
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is still a divide-and-conquer algorithm and hopefully you will start to
    see the pattern we are using. The atomic update of the write index `dst_idx` ensures
    that multiple threads never write to the same index in the destination sequence.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然是一个分而治之的算法，希望你现在开始看到我们正在使用的模式。写入索引`dst_idx`的原子更新确保多个线程永远不会写入相同的目标序列中的索引。
- en: Outer function
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 外部函数
- en: 'The outer function, called from the client code, is simply a placeholder for
    the atomic `size_t`, which is initialized to zero. The function then initializes
    the inner function, which parallelizes the code further:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 从客户端代码调用的外部函数只是原子`size_t`的占位符，它被初始化为零。然后函数初始化内部函数，进一步并行化代码：
- en: '[PRE15]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Once the inner function returns, we can use `dst_write_idx` to compute the end
    iterator of the destination range. Let's now have a look at the other approach
    to solve the same problem.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 内部函数返回后，我们可以使用`dst_write_idx`来计算目标范围的结束迭代器。现在让我们来看看解决相同问题的另一种方法。
- en: 'Approach 2: Split the algorithm into two parts'
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方法2：将算法分成两部分
- en: The second approach is to split the algorithm into two parts. First, the conditional
    copying is performed in parallel chunks, and then the resulting sparse range is
    squeezed to a continuous range.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是将算法分成两部分。首先，在并行块中执行条件复制，然后将结果稀疏范围压缩为连续范围。
- en: Part one – Copy elements in parallel into the destination range
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一部分 - 并行复制元素到目标范围
- en: 'The first part copies the elements in chunks, resulting in the sparse destination
    array illustrated in *Figure 14.6*. Each chunk is conditionally copied in parallel,
    and the resulting range iterators are stored in `std::future` objects for later
    retrieval:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分将元素分块复制，得到了在*图14.6*中说明的稀疏目标数组。每个块都是以并行方式有条件地复制的，结果范围迭代器存储在`std::future`对象中以供以后检索：
- en: '![](img/B15619_14_06.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_14_06.png)'
- en: 'Figure 14.6: The sparse destination range after the first step of the conditional
    copying'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：第一步条件复制后的稀疏目标范围
- en: 'The following code implements the first half of the algorithm:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实现了算法的前半部分：
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We have now copied the elements (that should be copied) into the sparse destination
    range. It's time to fill the gaps by moving the elements to the left in the range.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将（应该被复制的）元素复制到了稀疏的目标范围中。现在是时候通过将元素向左移动到范围中来填补空白了。
- en: Part two – Move the sparse range sequentially into a continuous range
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第二部分 - 将稀疏范围顺序地移动到连续范围
- en: 'When the sparse range is created, it is merged using the resulting value from
    each `std::future`. The merge is performed sequentially as the parts overlap:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建稀疏范围时，它使用每个`std::future`的结果值进行合并。合并是顺序执行的，因为部分重叠：
- en: '[PRE17]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This second part of the algorithm that moves all the subranges to the beginning
    of the range is illustrated in the following image:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有子范围移动到范围开始的算法的第二部分如下图所示：
- en: '![](img/B15619_14_07.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_14_07.png)'
- en: 'Figure 14.7: Merging a sparse range into a continuous range'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7：将稀疏范围合并到连续范围中
- en: With two algorithms solving the same problem, it's time to see how they measure
    up.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个解决同一个问题的算法，现在是时候看看它们的表现如何了。
- en: Performance evaluation
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能评估
- en: 'The performance boost from using this parallelized version of `copy_if()` is
    heavily dependent on how expensive the predicate is. Therefore, we use two different
    predicates in our benchmark with different computational costs. Here is the *inexpensive*
    predicate:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个并行化版本的`copy_if()`的性能提升严重依赖于谓词的昂贵程度。因此，在我们的基准测试中，我们使用了两个不同计算成本的谓词。这是*廉价*的谓词：
- en: '[PRE18]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The more *expensive* predicate checks whether its argument is a prime number:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 更*昂贵*的谓词检查其参数是否为质数：
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note, this is not a particularly optimal way to implement `is_prime()`, and
    is only used here for the purposes of the benchmark.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这不是实现`is_prime()`的特别优化的方式，仅仅是为了基准测试的目的而使用。
- en: 'The benchmarking code is not spelled out here but is included in the accompanying
    source code. Three algorithms are compared: `std::copy_if()`, `par_copy_if_split()`,
    and `par_copy_if_sync()`. The following graph shows the results as measured using
    an Intel Core i7 CPU. The parallel algorithms use a chunk size of 100,000 in this
    benchmark.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试代码没有在这里详细说明，但包含在附带的源代码中。比较了三个算法：`std::copy_if()`、`par_copy_if_split()`和`par_copy_if_sync()`。下图显示了在使用英特尔Core
    i7 CPU进行测量时的结果。这个基准测试中，并行算法使用了一个大小为100,000的块。
- en: '![](img/B15619_14_08.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_14_08.png)'
- en: 'Figure 14.8: Conditional copy strategies versus computation time'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8：条件复制策略与计算时间
- en: The most obvious observation when measuring the performance is how ridiculously
    slow the synchronized version `par_copy_if_sync()` is when using the inexpensive
    `is_odd()` predicate. The disastrous performance is actually not due to the atomic
    write index; rather, it is because the cache mechanism of the hardware is trashed
    due to several threads writing to the same cache line (as you learned about in
    *Chapter 7*, *Memory Management*).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在测量性能时最明显的观察是，当使用廉价的`is_odd()`谓词时，同步版本`par_copy_if_sync()`的性能是多么慢。灾难性的性能实际上并不是由于原子写入索引，而是因为硬件的缓存机制由于多个线程写入同一缓存行而被破坏（正如你在*第7章*，*内存管理*中学到的）。
- en: So, with this knowledge, we understand now why `par_copy_if_split()` performs
    better. On the inexpensive predicate, `is_odd()`, `par_copy_if_split()` is about
    2x faster than `std::copy_if()`, but with the expensive `is_prime()`, the efficiency
    increases to almost 5x. The increased efficiency is a result of spending most
    of the computations in the first part of the algorithm, which executes in parallel.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有了这个知识，我们现在明白了为什么`par_copy_if_split()`的性能更好。在廉价的谓词`is_odd()`上，`par_copy_if_split()`比`std::copy_if()`快大约2倍，但在昂贵的`is_prime()`上，效率增加到了近5倍。增加的效率是由于大部分计算在算法的第一部分中并行执行。
- en: You should now have a grasp of some techniques that can be used for parallelizing
    an algorithm. These new insights will help you understand the requirements and
    expectations when using parallel algorithms from the standard library.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该掌握了一些用于并行化算法的技术。这些新的见解将帮助你理解使用标准库中的并行算法时的要求和期望。
- en: Parallel standard library algorithms
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行标准库算法
- en: As of C++17, the standard library has been extended with parallel versions of
    most, but not all, algorithms. Changing your algorithms to allow for parallel
    execution is simply a matter of adding a parameter that tells the algorithm which
    parallel execution policy to use.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 从C++17开始，标准库已经扩展了大多数算法的并行版本，但并非所有算法都有。将你的算法更改为允许并行执行只是添加一个参数，告诉算法使用哪个并行执行策略。
- en: 'As stressed earlier in this book, if your code base is based upon standard
    library algorithms, or at least if you have the habit of writing C++ by using
    algorithms, you will get an instant performance boost almost for free by adding
    an execution policy where suitable:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本书早些时候强调过，如果你的代码基于标准库算法，或者至少习惯于使用算法编写C++，那么通过在适当的地方添加执行策略，你几乎可以免费获得即时的性能提升：
- en: '[PRE20]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Once you specify an execution policy, you are in the realm of parallel algorithms,
    which have some notable differences compared to their original sequential versions.
    Firstly, the minimum iterator category requirements change from input iterators
    to forward iterators. Secondly, exceptions thrown by your code (from copy constructors
    or function objects passed to the algorithm) never reach you. Instead, the algorithm
    is required to call `std::terminate()`. Thirdly, the complexity guarantees (both
    time and memory) of the algorithm might be relaxed because of the added complexity
    of parallel implementations.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦指定了执行策略，你就进入了并行算法的领域，这些算法与它们原始的顺序版本有一些显著的区别。首先，最小的迭代器类别要求从输入迭代器变为前向迭代器。其次，你的代码抛出的异常（从复制构造函数或传递给算法的函数对象）永远不会到达你。相反，算法要求调用`std::terminate()`。第三，由于并行实现的复杂性增加，算法的复杂性保证（时间和内存）可能会放宽。
- en: When using a parallel version of the standard library algorithms, you specify
    an execution policy that states how an algorithm is allowed to parallelize the
    execution. An implementation may decide to execute the algorithm sequentially,
    though. If you compare the efficiency and scalability of parallel algorithms in
    different standard library implementations, you can expect to see big differences.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用标准库算法的并行版本时，你需要指定一个执行策略，该策略规定了算法允许并行执行的方式。但是，实现可能会决定按顺序执行算法。如果你比较不同标准库实现中并行算法的效率和可伸缩性，你会发现巨大的差异。
- en: Execution policies
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行策略
- en: An **execution policy** informs an algorithm if and how the execution can be
    parallelized. There are four default execution policies included in the parallel
    extensions of the standard library. Compilers and third-party libraries can extend
    these policies for certain hardware and conditions. For example, it's already
    possible to use the parallel power of the modern graphics card from standard library
    algorithms using vendor-specific policies.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**执行策略**通知算法执行是否可以并行化以及如何并行化。标准库的并行扩展中包括四种默认执行策略。编译器和第三方库可以为特定的硬件和条件扩展这些策略。例如，已经可以使用特定供应商的策略从标准库算法中使用现代图形卡的并行能力。'
- en: The execution policies are defined in the header `<execution>` and reside in
    the namespace `std::execution`. There are currently four distinct tag types, one
    for each execution policy. The types cannot be instantiated by you; instead, there
    is one predefined object per type. For instance, the parallel execution policy
    has a type called `std::execution::parallel_policy` and the predefined instance
    of this type is named `std::execution::par`. The reason there is one *type* per
    policy (rather than one type with multiple predefined instances) is so that the
    policies you provide can be distinguished at compile time by the library.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 执行策略在头文件`<execution>`中定义，并驻留在命名空间`std::execution`中。目前有四种不同的标签类型，每种执行策略对应一种。这些类型不能由您实例化；相反，每种类型有一个预定义对象。例如，并行执行策略有一个名为`std::execution::parallel_policy`的类型，该类型的预定义实例名为`std::execution::par`。每种策略有一个*类型*（而不是具有多个预定义实例的一个类型）的原因是，您提供的策略可以在库中在编译时区分。
- en: Sequenced policy
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 顺序策略
- en: The sequenced execution policy, `std::execution::seq`, makes the algorithm execute
    sequentially with no parallelism, similar to how the algorithms without the extra
    execution policy argument would run. However, whenever you specify an execution
    policy, it means that you are using a version of the algorithm with relaxed complexity
    guarantees and stricter iterator requirements; it also assumes that the code you
    provide doesn't throw exceptions, or the algorithm will call `std::terminate()`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序执行策略`std::execution::seq`使算法以顺序方式执行，没有并行性，类似于没有额外执行策略参数的算法将运行的方式。然而，每当您指定执行策略时，这意味着您正在使用具有放宽的复杂性保证和更严格的迭代器要求的算法的版本；它还假定您提供的代码不会抛出异常，否则算法将调用`std::terminate()`。
- en: Parallel policy
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并行策略
- en: 'The parallel execution policy, `std::execution::par`, can be considered the
    standard execution policy for parallel algorithms. The code you provide to the
    algorithm needs to be thread safe. A way to understand this requirement is to
    think about the loop body in the sequential version of the algorithm you are about
    to use. For example, think about the sequential version of `copy_if()`, which
    we spelled out like this earlier in the chapter:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 并行执行策略`std::execution::par`可以被认为是并行算法的标准执行策略。您提供给算法的代码需要是线程安全的。理解这一要求的一种方法是考虑您将要使用的算法的顺序版本中的循环主体。例如，考虑我们在本章前面这样拼写出来的`copy_if()`的顺序版本：
- en: '[PRE21]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In this algorithm, the code inside the loop body will call the predicate you
    provided and invoke the copy assignment operator on the elements in the range.
    If you pass `std::execution::par` to `copy_if()`, it is your responsibility to
    guarantee that these parts are thread safe and can safely be executed in parallel.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中，循环主体内的代码将调用您提供的谓词，并在范围内的元素上调用复制赋值运算符。如果您将`std::execution::par`传递给`copy_if()`，则您有责任保证这些部分是线程安全的，并且可以安全地并行执行。
- en: 'Let''s look at an example where we provide unsafe code and then see what we
    can do about it. Assume we have a vector of strings:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，我们提供不安全的代码，然后看看我们能做些什么。假设我们有一个字符串向量：
- en: '[PRE22]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If we want to compute the total size of all strings in the vector using a parallel
    algorithm, an inadequate way to do this would be to use `std::for_each()`, like
    this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要使用并行算法计算向量中所有字符串的总大小，一个不足的方法是使用`std::for_each()`，就像这样：
- en: '[PRE23]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Since the body of the function object is not thread safe (as it updates a shared
    variable from multiple threads), this code exhibits undefined behavior. We could,
    of course, protect the `tot_size` variable with a `std::mutex`, but that would
    defeat the whole purpose of executing this code in parallel, since the mutex would
    only allow one thread at a time to enter the body. Using an `std::atomic` data
    type would be another option, but that could also degrade the efficiency.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于函数对象的主体不是线程安全的（因为它从多个线程更新共享变量），这段代码表现出未定义的行为。当然，我们可以使用`std::mutex`保护`tot_size`变量，但这将破坏以并行方式执行此代码的整个目的，因为互斥锁只允许一个线程一次进入主体。使用`std::atomic`数据类型是另一种选择，但这也可能降低效率。
- en: 'The solution here is to *not* use `std::for_each()` for this problem at all.
    Instead, we can use `std::transform_reduce()` or `std::reduce()`, which are tailor-made
    for this kind of job. Here is how you would do it using `std::reduce()`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的解决方案是*不*使用`std::for_each()`来解决这个问题。相反，我们可以使用`std::transform_reduce()`或`std::reduce()`，这些都是专门为这种工作量身定做的。以下是使用`std::reduce()`的方法：
- en: '[PRE24]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: By getting rid of the mutable reference inside the lambda, the body of the lambda
    is now thread safe. The `const` reference to the `std::string` objects is fine,
    because it never mutates any string objects and therefore doesn't introduce any
    data races.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通过消除lambda内部的可变引用，lambda的主体现在是线程安全的。对`std::string`对象的`const`引用是可以的，因为它从不改变任何字符串对象，因此不会引入任何数据竞争。
- en: Normally, the code you pass to an algorithm is thread safe unless your function
    objects capture objects by reference or have other side effects such as writing
    to files.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您传递给算法的代码是线程安全的，除非您的函数对象通过引用捕获对象或具有其他诸如写入文件的副作用。
- en: Unsequenced policy
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非顺序策略
- en: The unsequenced policy was added in C++20\. It tells the algorithm that the
    loop is allowed to be vectorized using, for example, SIMD instructions. In practice,
    this means that you cannot use any synchronization primitives in the code you
    pass to the algorithm, since this could result in deadlocks.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 无序策略是在C++20中添加的。它告诉算法，循环允许使用例如SIMD指令进行矢量化。实际上，这意味着您不能在传递给算法的代码中使用任何同步原语，因为这可能导致死锁。
- en: 'To understand how a deadlock can occur, we will get back to the previous inadequate
    example when counting the total size of all strings in a vector. Assume that,
    instead of using `std::reduce()`, we protect the `tot_size` variable by adding
    a mutex, like this:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解死锁是如何发生的，我们将回到之前不足的例子，当计算向量中所有字符串的总大小时。假设，我们不是使用`std::reduce()`，而是通过添加互斥锁来保护`tot_size`变量，像这样：
- en: '[PRE25]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This code is now safe to execute using `std::execution::par`, but it is very
    inefficient. If we were to change the execution policy to `std::execution::unseq`,
    the result would not only be an inefficient program but also a program that runs
    the risk of deadlocking!
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用`std::execution::par`执行此代码是安全的，但效率很低。如果我们将执行策略更改为`std::execution::unseq`，结果不仅是一个低效的程序，还是一个有死锁风险的程序！
- en: The unsequenced execution policy tells the algorithm that it may reorder the
    instruction of our code in a way that is normally not allowed by the optimizing
    compiler.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 无序执行策略告诉算法，它可以重新排序我们的代码的指令，这通常是优化编译器不允许的。
- en: 'For the algorithm to benefit from vectorization, it needs to read multiple
    values from the input range, and then apply SIMD instructions to multiple values
    at once. Let''s analyze what two iterations in the loop of `for_each()` could
    look like, with and without reorderings. Here are two loop iterations without
    any reorderings:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使算法受益于矢量化，它需要从输入范围中读取多个值，然后一次应用SIMD指令于多个值。让我们分析一下`for_each()`循环中的两次迭代可能是什么样子，有无重新排序。以下是两次迭代没有任何重新排序的情况：
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The algorithm is allowed to merge these two iterations in the following way:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 算法允许以以下方式合并这两次迭代：
- en: '[PRE27]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Trying to execute this code on the same thread will deadlock because we are
    trying to lock the very same mutex twice consecutively. In other words, when using
    the `std::execution::unseq` policy, you must make sure that the code you provide
    to the algorithm doesn't acquire any locks.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在同一线程上执行此代码将导致死锁，因为我们试图连续两次锁定同一个互斥锁。换句话说，当使用`std::execution::unseq`策略时，您必须确保您提供给算法的代码不会获取任何锁。
- en: Note that the optimizing compiler is free to vectorize your code anytime. However,
    in those cases, it's up to the compiler to guarantee that the vectorization doesn't
    change the meaning of the program, just like any other optimizations that the
    compiler and hardware are allowed to perform. The difference here, when explicitly
    providing the `std::execute::unseq` policy to an algorithm, is that *you* guarantee
    that the code you provide is safe to vectorize.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，优化编译器随时可以对您的代码进行矢量化。然而，在这些情况下，由编译器来保证矢量化不会改变程序的含义，就像编译器和硬件允许执行的任何其他优化一样。在这里，当显式地为算法提供`std::execute::unseq`策略时，*您*保证您提供的代码是安全的可矢量化的。
- en: Parallel unsequenced policy
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并行无序策略
- en: The parallel unsequenced policy, `std::execution::par_unseq`, executes the algorithm
    in parallel like the parallel policy, with the addition that it may also vectorize
    the loop.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 并行无序策略`std::execution::par_unseq`像并行策略一样并行执行算法，另外还可以对循环进行矢量化。
- en: Apart from the four standard execution policies, standard library vendors can
    provide you with additional policies with custom behavior and put other constraints
    on the input. For example, the Intel Parallel STL library defines four custom
    execution policies that only accept random access iterators.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 除了四种标准执行策略之外，标准库供应商可以为您提供具有自定义行为的其他策略，并对输入施加其他约束。例如，英特尔并行STL库定义了四种只接受随机访问迭代器的自定义执行策略。
- en: Exception handling
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常处理
- en: 'If you provide one of the four standard execution policies to an algorithm,
    your code must not throw exceptions, or the algorithm will call `std::terminate()`.
    This is a big difference from the normal single-threaded algorithms, which always
    propagate exceptions back to the caller:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您为算法提供了四种标准执行策略中的一种，您的代码不能抛出异常，否则算法将调用`std::terminate()`。这与正常的单线程算法有很大的不同，后者总是将异常传播回调用者：
- en: '[PRE28]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Running the same code with an execution policy results in a call to `std::terminate()`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用执行策略运行相同的代码会导致调用`std::terminate()`：
- en: '[PRE29]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You might think that this means the parallel algorithms are declared `noexcept`,
    but that's not the case. Many parallel algorithms need to allocate memory, and
    therefore the standard parallel algorithms themselves are allowed to throw `std::bad_alloc`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能认为这意味着并行算法声明为`noexcept`，但事实并非如此。许多并行算法需要分配内存，因此标准并行算法本身可以抛出`std::bad_alloc`。
- en: It should also be said that execution policies provided by other libraries may
    handle exceptions in a different way.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 还应该说，其他库提供的执行策略可能以不同的方式处理异常。
- en: Now, we will move on to discuss some of the algorithms that were added and modified
    when the parallel algorithms were first introduced in C++17.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续讨论在C++17中首次引入并行算法时添加和修改的一些算法。
- en: Additions and changes to parallel algorithms
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行算法的添加和更改
- en: Most algorithms in the standard library are available as parallel versions straight
    out the box. However, there are some noteworthy exceptions, including `std::accumulate()`
    and `std::for_each()`, as their original specifications required in-order execution.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 标准库中的大多数算法都可以直接作为并行版本使用。但是，也有一些值得注意的例外，包括`std::accumulate()`和`std::for_each()`，因为它们的原始规范要求按顺序执行。
- en: std::accumulate() and std::reduce()
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: std::accumulate()和std::reduce()
- en: The `std::accumulate()` algorithm cannot be parallelized as it must be executed
    in the order of the elements, which is not possible to parallelize. Instead, a
    new algorithm called `std::reduce()` has been added, which works just like `std::accumulate()`
    with the exception that it is executed unordered.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::accumulate()`算法不能并行化，因为它必须按元素的顺序执行，这是不可能并行化的。相反，已添加了一个新算法叫做`std::reduce()`，它的工作方式与`std::accumulate()`相同，只是它是无序执行的。'
- en: 'With commutative operations, their results are the same, as the order of accumulation
    doesn''t matter. In other words, given a range of integers:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可交换的操作，它们的结果是相同的，因为累积的顺序无关紧要。换句话说，给定一个整数范围：
- en: '[PRE30]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'accumulating them by addition or multiplication:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通过加法或乘法累积它们：
- en: '[PRE31]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'would yield the same result as invoking `std::reduce()` instead of `std::accumulate()`,
    as both the addition and multiplication of integers are commutative. For example:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 将产生与调用`std::reduce()`而不是`std::accumulate()`相同的结果，因为整数的加法和乘法都是可交换的。例如：
- en: '![](img/B15619_14_004.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_14_004.png)'
- en: 'But, if the operation is not commutative, the result is *non-deterministic*
    since it depends on the order of the arguments. For example, if we were to accumulate
    a list of strings as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果操作不是可交换的，结果是*不确定的*，因为它取决于参数的顺序。例如，如果我们要按如下方式累积字符串列表：
- en: '[PRE32]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'this code will always produce the string `"ABC"`. But, by using `std::reduce()`,
    the characters in the resulting string could be in any order because string concatenation
    is not commutative. In other words, the string `"A" + "B"` is not equal to `"B"
    + "A"`. Therefore, the following code using `std::reduce()` might produce different
    results:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将始终产生字符串`"ABC"`。但是，通过使用`std::reduce()`，结果字符串中的字符可能以任何顺序出现，因为字符串连接不是可交换的。换句话说，字符串`"A"
    + "B"`不等于`"B" + "A"`。因此，使用`std::reduce()`的以下代码可能产生不同的结果：
- en: '[PRE33]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: An interesting point related to performance is that floating-point math is not
    commutative. By using `std::reduce()` on floating-point values, the results may
    vary, but it also means that `std::reduce()` is potentially much faster than `std::accumulate()`.
    This is because `std::reduce()` is allowed to reorder operations and utilize SIMD
    instructions in a way that `std::accumulate()` isn't allowed to do when using
    strict floating-point math.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 与性能相关的一个有趣点是浮点数运算不是可交换的。通过在浮点值上使用`std::reduce()`，结果可能会有所不同，但这也意味着`std::reduce()`可能比`std::accumulate()`快得多。这是因为`std::reduce()`允许重新排序操作并利用SIMD指令，而在使用严格的浮点数运算时，`std::accumulate()`是不允许这样做的。
- en: std::transform_reduce()
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: std::transform_reduce()
- en: 'As an addition to the standard library algorithms, `std::transform_reduce()`
    has also been added to the `<numeric>` header. It does exactly what it says: it
    transforms a range of elements as `std::transform()` and then applies a function
    object. This accumulates them out of order, like `std::reduce()`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 作为标准库算法的补充，`std::transform_reduce()`也已添加到`<numeric>`头文件中。它确切地做了它所说的：它将一个元素范围转换为`std::transform()`，然后应用一个函数对象。这样累积它们是无序的，就像`std::reduce()`一样：
- en: '[PRE34]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Both `std::reduce()` and `std::transform_reduce()` were added to C++17 when
    parallel algorithms were introduced. Another necessary change was to adjust the
    return type of `std::for_each()`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当并行算法被引入时，`std::reduce()`和`std::transform_reduce()`也被添加到C++17中。另一个必要的更改是调整`std::for_each()`的返回类型。
- en: std::for_each()
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`std::for_each()`'
- en: 'A somewhat rarely used property of `std::for_each()` is that it returns the
    function object passed into it. This makes it possible to use `std::for_each()`
    to accumulate values inside a stateful function object. The following examples
    demonstrate a possible use case:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::for_each()`的一个相对不常用的特性是它返回传递给它的函数对象。这使得可以使用`std::for_each()`在有状态的函数对象内累积值。以下示例演示了可能的用例：'
- en: '[PRE35]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This usage is similar to what we can achieve using `std::accumulate()` and
    therefore also exhibits the same problem when trying to parallelize it: executing
    the function object out of order would yield non-deterministic results as the
    invocation order is undefined. Consequently, the parallel version of `std::for_each()`
    simply returns `void`.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这种用法类似于使用`std::accumulate()`可以实现的用法，因此在尝试并行化时也会出现相同的问题：无序执行函数对象将产生不确定的结果，因为调用顺序是未定义的。因此，`std::for_each()`的并行版本简单地返回`void`。
- en: Parallelizing an index-based for-loop
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于索引的for循环的并行化
- en: Even though I recommend using algorithms, sometimes a raw, index-based `for`-loop
    is required for a specific task. The standard library algorithms provide an equivalent
    of a range-based `for`-loop by including the algorithm `std::for_each()` in the
    library.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我建议使用算法，但有时特定任务需要原始的基于索引的`for`循环。标准库算法通过在库中包含算法`std::for_each()`提供了等效于基于范围的`for`循环。
- en: 'However, there is no algorithm equivalent of an index-based `for`-loop. In
    other words, we cannot easily parallelize code like this by simply adding a parallel
    policy to it:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并没有基于索引的`for`循环的等效算法。换句话说，我们不能简单地通过向其添加并行策略来轻松并行化这样的代码：
- en: '[PRE36]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: But let's see how we can build one by combining algorithms. As you will have
    already concluded, implementing parallel algorithms is complicated. But in this
    case, we will build a `parallel_for()` algorithm using `std::for_each()` as a
    building block, thus leaving the complex parallelism to `std::for_each()`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们看看如何通过组合算法来构建一个。正如您已经得出的结论，实现并行算法是复杂的。但在这种情况下，我们将使用`std::for_each()`作为构建块构建一个`parallel_for()`算法，从而将复杂的并行性留给`std::for_each()`。
- en: Combining std::for_each() with std::views::iota()
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合std::for_each()和std::views::iota()
- en: 'An index-based `for`-loop based on a standard library algorithm can be created
    by combining `std::for_each()` with `std::views::iota()` from the ranges library
    (see *Chapter 6*, *Ranges and Views*). This is how it would look:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 基于标准库算法的基于索引的`for`循环可以通过将标准库算法`std::for_each()`与范围库中的`std::views::iota()`结合使用来创建（见*第6章*，*范围和视图*）。它看起来是这样的：
- en: '[PRE37]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This can then be further parallelized by using the parallel execution policy:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以通过使用并行执行策略进一步并行化：
- en: '[PRE38]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: As stated earlier, we have to be very careful when passing references to a lambda
    that will be invoked from multiple threads like this. By only accessing vector
    elements via the unique index `i`, we avoid introducing data races when mutating
    the strings in the vector.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所述，我们在像这样从多个线程调用的lambda中传递引用时必须非常小心。通过仅通过唯一索引`i`访问向量元素，我们避免了在向量中突变字符串时引入数据竞争。
- en: Simplifying construction via a wrapper
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过包装简化构造
- en: 'In order to iterate the indices with a neat syntax, the previous code is wrapped
    into a utility function named `parallel_for()`, as shown here:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以简洁的语法迭代索引，先前的代码被封装到一个名为`parallel_for()`的实用函数中，如下所示：
- en: '[PRE39]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The `parallel_for()` function template can then be used directly like this:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以直接使用`parallel_for()`函数模板，如下所示：
- en: '[PRE40]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As the `parallel_for()` is built upon `std::for_each()`, it accepts any policy
    that `std::for_each()` accepts.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`parallel_for()`是建立在`std::for_each()`之上的，它接受`std::for_each()`接受的任何策略。
- en: We will wrap this chapter up with a short introductory overview of GPUs and
    how they can be used for parallel programming, now and in the future.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用一个简短的介绍性概述来总结本章，介绍GPU以及它们如何在现在和将来用于并行编程。
- en: Executing algorithms on the GPU
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在GPU上执行算法
- en: '**Graphics** **processing units** (**GPUs**) were originally designed and used
    for processing points and pixels for computer graphics rendering. Briefly, what
    the GPUs did was retrieve buffers of pixel data or vertex data, perform a simple
    operation on each buffer individually, and store the result in a new buffer (to
    eventually be displayed).'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**图形** **处理单元**（**GPU**）最初是为了处理计算机图形渲染中的点和像素而设计和使用的。简而言之，GPU所做的是检索像素数据或顶点数据的缓冲区，对每个缓冲区进行简单操作，并将结果存储在新的缓冲区中（最终将被显示）。'
- en: 'Here are some examples of simple, independent operations that could be executed
    on the GPU at an early stage:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可以在早期阶段在GPU上执行的简单独立操作的示例：
- en: Transform a point from world coordinates to screen coordinates
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将点从世界坐标转换为屏幕坐标
- en: Perform a lighting calculation at a specific point (by lighting calculation,
    I am referring to calculating the color of a specific pixel in an image)
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特定点执行光照计算（通过光照计算，我指的是计算图像中特定像素的颜色）
- en: As these operations could be performed in parallel, the GPUs were designed for
    executing small operations in parallel. Later on, these graphics operations became
    programmable, although the programs were written in terms of computer graphics
    (that is, the memory reads were done in terms of reading colors from a texture,
    and the result was always written as a color to a texture). These programs are
    called **shaders**.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些操作可以并行执行，GPU被设计用于并行执行小操作。后来，这些图形操作变得可编程，尽管程序是以计算机图形的术语编写的（也就是说，内存读取是以从纹理中读取颜色的形式进行的，结果总是以颜色写入纹理）。这些程序被称为**着色器**。
- en: Over time, more shader-type programs were introduced, and shaders gained more
    and more low-level options, such as reading and writing raw values from buffers
    instead of color values from textures.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，引入了更多的着色器类型程序，着色器获得了越来越多的低级选项，例如从缓冲区中读取和写入原始值，而不是从纹理中读取颜色值。
- en: Technically, a CPU commonly consists of a few general-purpose cached cores,
    whereas a GPU consists of a huge number of highly specialized cores. This means
    that parallel algorithms that scale well are highly suitable to execute on a GPU.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，CPU通常由几个通用缓存核心组成，而GPU由大量高度专门化的核心组成。这意味着良好扩展的并行算法非常适合在GPU上执行。
- en: GPUs have their own memory and before an algorithm can execute on the GPU, the
    CPU needs to allocate memory in the GPU memory and copy data from main memory
    to GPU memory. The next thing that happens is that the CPU launches a routine
    (also called a kernel) on the GPU. Finally, the CPU copies data back from the
    GPU memory into the main memory, making it accessible for "normal" code executing
    on the CPU. The overhead incurred by copying data back and forth between the CPU
    and the GPU is one of the reasons why GPUs are more suitable for batch processing
    tasks where throughput is more important than latency.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: GPU有自己的内存，在算法可以在GPU上执行之前，CPU需要在GPU内存中分配内存，并将数据从主内存复制到GPU内存。接下来发生的事情是CPU在GPU上启动例程（也称为内核）。最后，CPU将数据从GPU内存复制回主内存，使其可以被在CPU上执行的“正常”代码访问。在CPU和GPU之间来回复制数据所产生的开销是GPU更适合批处理任务的原因之一，其中吞吐量比延迟更重要。
- en: There are several libraries and abstraction layers available today that make
    GPU programming accessible from C++, but standard C++ offers nearly nothing in
    this area. However, the parallel execution policies `std::execution::par` and
    `std::execution::par_unseq` allow compilers to move the execution of standard
    algorithms from the CPU to the GPU. One example of this is NVC++, the NVIDIA HPC
    compiler. It can be configured to compile standard C++ algorithms for execution
    on NVIDIA GPUs.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 今天有几个库和抽象层可用，使得从C++进行GPU编程变得容易。然而，标准C++在这方面几乎没有提供任何东西。但是，并行执行策略`std::execution::par`和`std::execution::par_unseq`允许编译器将标准算法的执行从CPU移动到GPU。其中一个例子是NVC++，NVIDIA
    HPC编译器。它可以配置为将标准C++算法编译为在NVIDIA GPU上执行。
- en: If you want to learn more about the current status of C++ and GPU programming,
    I highly recommend the talk *GPU Programming with modern C++* by Michael Wong
    ([https://accu.org/video/spring-2019-day-3/wong/](https://accu.org/video/spring-2019-day-3/wong/))
    from the ACCU 2019 conference.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解C++和GPU编程的当前状态，我强烈推荐Michael Wong在ACCU 2019年会议上的演讲*使用现代C++进行GPU编程*（[https://accu.org/video/spring-2019-day-3/wong/](https://accu.org/video/spring-2019-day-3/wong/)）。
- en: Summary
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have learned about the complexity of handcrafting an algorithm
    to execute in parallel. You also now know how to analyze, measure, and tune the
    efficiency of parallel algorithms. The insights you gained while learning about
    parallel algorithms will have deepened your understanding of the requirements
    and the behaviors of the parallel algorithms found in the C++ standard library.
    C++ comes with four standard execution policies, which can be extended by compiler
    vendors. This opens up the door for utilizing the GPU for standard algorithms.
    The next C++ standard, C++23, will most likely add increased support for parallel
    programming on the GPU.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经了解了手工编写并行算法的复杂性。您现在也知道如何分析、测量和调整并行算法的效率。在学习并行算法时获得的见解将加深您对C++标准库中并行算法的要求和行为的理解。C++带有四种标准执行策略，可以由编译器供应商进行扩展。这为利用GPU执行标准算法打开了大门。下一个C++标准，C++23，很可能会增加对GPU并行编程的支持。
- en: You have now reached the end of the book. Congratulations! Performance is an
    important aspect of code quality. But too often, performance comes at the expense
    of other quality aspects, such as readability, maintainability, and correctness.
    Mastering the art of writing efficient and clean code requires practical training.
    My hope is that you have learned things from this book that you can incorporate
    into your day-to-day life while creating stunning software.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经到达了本书的结尾。恭喜！性能是代码质量的重要方面。但往往性能是以牺牲其他质量方面（如可读性、可维护性和正确性）为代价的。掌握编写高效和干净代码的艺术需要实际训练。我希望您从本书中学到了一些东西，可以将其融入到您的日常生活中，创造出令人惊叹的软件。
- en: Solving performance problems usually comes down to a willingness to investigate
    things further. More often than not, it requires understanding the hardware and
    underlying OS well enough to be able to draw conclusions from measurement data.
    This book has scratched the surface in these areas when I've felt it necessary.
    After writing about C++20 features in this second edition, I'm now looking forward
    to starting to use these features in my profession as a software developer. As
    I've mentioned previously, a lot of the code presented in this book is only partially
    supported by the compilers today. I will keep updating the GitHub repository and
    adding information about compiler support. Best of luck!
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 解决性能问题通常需要愿意进一步调查事情。往往需要足够了解硬件和底层操作系统，以便能够从测量数据中得出结论。在这本书中，我在这些领域只是浅尝辄止。在第二版中写了关于C++20特性之后，我现在期待着开始在我的职业作为软件开发人员中使用这些特性。正如我之前提到的，这本书中呈现的许多代码今天只有部分得到编译器的支持。我将继续更新GitHub存储库，并添加有关编译器支持的信息。祝你好运！
- en: '**Share your experience**'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**分享您的经验**'
- en: Thank you for taking the time to read this book. If you enjoyed this book, help
    others to find it. Leave a review at [https://www.amazon.com/dp/1839216549](https://www.amazon.com/dp/1839216549).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您抽出时间阅读本书。如果您喜欢这本书，帮助其他人找到它。在[https://www.amazon.com/dp/1839216549](https://www.amazon.com/dp/1839216549)留下评论。
