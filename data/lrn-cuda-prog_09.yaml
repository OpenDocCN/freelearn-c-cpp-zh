- en: GPU Programming Using OpenACC
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenACC进行GPU编程
- en: Every processor architecture provides different approaches to writing code to
    run on the processor. CUDA is no exception; it also provides different approaches
    to coding. One such approach, which has become very popular in recent years, is
    making use of OpenACC, which fundamentally is directive-based programming.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 每个处理器架构都提供了不同的编写代码以在处理器上运行的方法。CUDA也不例外；它也提供了不同的编码方法。近年来变得非常流行的一种方法是使用OpenACC，它基本上是基于指令的编程。
- en: OpenACC is basically a standard which exposes heterogeneous computing as a first-class
    citizen. The standard fundamentally dictates that there are two kinds of processor,
    that is, a host and a device/accelerator, which is very similar to the concepts
    that the CUDA programming model states.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: OpenACC基本上是一个将异构计算作为一等公民的标准。该标准基本上规定了有两种处理器，即主机和设备/加速器，这与CUDA编程模型所述的概念非常相似。
- en: CUDA programming, using languages such as C, C++, Fortran, and Python, is the
    preferred way to express parallelism for programmers who want to get the best
    performance. Programming languages require a programmer to recreate their sequential
    program from scratch, while maintaining both serial and parallel versions of their
    key operations. Programmers can micromanage everything about their program and
    often use device-specific features, which are too specific for higher-level approaches,
    to achieve the best performance. Parallel programs created in a parallel programming
    language tend to only work on a very small number of platforms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于希望获得最佳性能的程序员来说，使用诸如C、C++、Fortran和Python等语言的CUDA编程是表达并行性的首选方式。编程语言要求程序员从头开始重新创建他们的顺序程序，同时保持他们的关键操作的串行和并行版本。并行编程语言中创建的并行程序往往只适用于非常少数的平台。
- en: Compiler directives blend the flexibility of programming languages with the
    easy use of libraries. The programmer annotates the code with high-level instructions
    that a compiler can use to parallelize the code, or can safely ignore. This means
    that code with compiler directives can be compiled for many different parallel
    platforms, and there’s no need to maintain separate serial and parallel versions
    of the code. Also, there is sometimes a need to quickly test and prototype an
    application to run on a GPU. One such example is converting code bases such as
    weather code, which has millions of lines of code, to run on a GPU; doing this
    using popular languages will take a lot of effort. In such a scenario, OpenACC
    becomes a logical choice. In OpenACC the developers provide hints to the compiler
    in the form of directives. The compiler takes these hints and generates an architecture-specific
    accelerator code.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器指令将编程语言的灵活性与库的易用性相结合。程序员使用高级指令对代码进行注释，编译器可以使用这些指令来并行化代码，或者可以安全地忽略。这意味着带有编译器指令的代码可以编译为许多不同的并行平台，并且无需维护代码的串行和并行版本。此外，有时需要快速测试和原型化应用程序以在GPU上运行。一个这样的例子是将天气代码等代码库（拥有数百万行代码）转换为在GPU上运行；使用流行的语言将需要大量的工作。在这种情况下，OpenACC成为一个合乎逻辑的选择。在OpenACC中，开发人员以指令的形式向编译器提供提示。编译器接受这些提示并生成特定于架构的加速器代码。
- en: The OpenACC standard also provides vendor neutrality to the developers of code.
    Single-source code with OpenACC directives can be recompiled for different devices.
    For example, the PGI compiler currently supports OpenACC backends such as Intel
    CPU multi-core, NVIDIA GPU, Intel Xeon Phi, and **F****ield-Programmable Gate
    Array** (**FPGA**) / **Application Specific Integrated Circuit** (**ASIC**) architectures.
    This is a really attractive proposition to developers who want to write vendor-neutral
    code. Key applications in **high-processing computing** (**HPC**) such as **Vienna
    Ab-initio Simulation Package** (**VASP**) (molecular dynamics/quantum chemistry), **Weather
    Research and Forecasting** (**WRF**), and ANSYS Fluent **Computational Fluid Dynamics**
    (**CFD**) make use of the OpenACC programming model to target the NVIDIA GPU.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: OpenACC标准还为代码的开发人员提供了供应商中立性。带有OpenACC指令的单一源代码可以重新编译为不同的设备。例如，PGI编译器目前支持OpenACC后端，如Intel
    CPU多核、NVIDIA GPU、Intel Xeon Phi和FPGA/ASIC架构。这对于希望编写供应商中立代码的开发人员来说是一个非常有吸引力的提议。高性能计算中的关键应用程序，如Vienna
    Ab-initio Simulation Package（VASP）（分子动力学/量子化学）、Weather Research and Forecasting（WRF）和ANSYS
    Fluent（CFD）利用OpenACC编程模型来针对NVIDIA GPU。
- en: 'To summarize the key takeaways for OpenACC:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 总结OpenACC的关键要点：
- en: The OpenACC standard was developed when heterogeneous computing was considered
    to be the new programming model.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当异构计算被视为新的编程模型时，OpenACC标准得以发展。
- en: OpenACC provides performance portability across various accelerators.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenACC在各种加速器上提供性能可移植性。
- en: OpenACC is not an alternative to the CUDA programming language. When the targeted
    processor is chosen as NVIDIA, the OpenACC compilers generate CUDA code behind
    the scenes.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenACC并不是CUDA编程语言的替代品。当选择的处理器是NVIDIA时，OpenACC编译器在后台生成CUDA代码。
- en: In recent years, the OpenMP standard has also started incorporating heterogeneous
    computing APIs. But to date, there is no compiler that supports different processor
    architectures, and so we have chosen with stick to OpenACC in this book.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，OpenMP标准也开始纳入异构计算API。但迄今为止，还没有支持不同处理器架构的编译器，因此我们选择在本书中坚持使用OpenACC。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: OpenACC directives
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenACC指令
- en: Asynchronous programming in OpenACC
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenACC中的异步编程
- en: Additional important directives and clauses
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外重要的指令和子句
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux/Windows PC with a modern NVIDIA GPU (Pascal architecture onward) is
    required for this chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要一台带有现代NVIDIA GPU（Pascal架构或更高）的Linux/Windows PC。
- en: As mentioned in the introduction, OpenACC is a standard and this standard is
    implemented by different compilers such as the GCC, PGI, and CRAY compilers. The
    compiler that we will be using for this chapter is PGI. The PGI compiler has been
    really popular in the Fortran community and has always been ahead of the curve
    in implementing the OpenACC latest specifications, and it provides a community
    edition, which can be downloaded from the PGI website for free. The good part
    is that fundamentally there is no change in functionality between the community
    edition and a paid-for version of the PGI compiler. For this chapter, you will
    be required to download the PGI community edition.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如介绍中所述，OpenACC是一个标准，这个标准由不同的编译器实现，如GCC、PGI和CRAY编译器。我们将在本章中使用的编译器是PGI。PGI编译器在Fortran社区中非常受欢迎，并且一直在实现OpenACC最新规范方面处于领先地位，并且提供了一个可以从PGI网站免费下载的社区版。好处是在社区版和付费版本的PGI编译器之间在功能上基本没有变化。在本章中，您需要下载PGI社区版。
- en: 'This chapter''s code is also available on GitHub at: [https://github.com/PacktPublishing/Learn-CUDA-Programming](https://github.com/PacktPublishing/Learn-CUDA-Programming).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码也可以在GitHub上找到：[https://github.com/PacktPublishing/Learn-CUDA-Programming](https://github.com/PacktPublishing/Learn-CUDA-Programming)。
- en: Sample code examples are developed and tested with version 19.4 of the PGI community
    edition. But it is recommended you use the latest PGI version.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 示例代码示例是使用PGI社区版的19.4版本开发和测试的。但建议您使用最新的PGI版本。
- en: Image merging on a GPU using OpenACC
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenACC在GPU上合并图像
- en: 'In order to understand the OpenACC concept, we have chosen a simple computer
    vision algorithm for merging two images. Fundamentally in this code, we are trying
    to merge two images, shown as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解OpenACC概念，我们选择了一个简单的计算机视觉算法来合并两个图像。在这段代码中，我们试图合并两个图像，如下所示：
- en: '![](img/18bb51b7-333b-4c18-8f05-de66809037a2.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18bb51b7-333b-4c18-8f05-de66809037a2.png)'
- en: The preceding image demonstrates a computer vision algorithm merging two images.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图像演示了一个计算机视觉算法，用于合并两个图像。
- en: 'We will talk more about the code structure later in the chapter. To start, configure
    the environment according to the following steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面更多地讨论代码结构。首先，根据以下步骤配置环境：
- en: Prepare your GPU application. As an example, we will use a kernel algorithm
    for merging two images. This code can be found at `09_openacc/`.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备您的GPU应用程序。例如，我们将使用一个用于合并两个图像的核算法。此代码可以在`09_openacc/`中找到。
- en: 'Compile your application with the `pgc++` compiler:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pgc++`编译器编译您的应用程序：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding commands will create a binary named `blurring.out`. As you might
    have observed we are using the `pgc++` compiler to compile our code. Also, we
    pass a few arguments to our code. Let''s understand them in more detail:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令将创建一个名为`blurring.out`的二进制文件。正如您可能已经观察到的，我们正在使用`pgc++`编译器来编译我们的代码。此外，我们向我们的代码传递了一些参数。让我们更详细地了解它们：
- en: '`-acc`: This flag tells the compiler to parse the OpenACC directives provided
    in the code.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-acc`：此标志告诉编译器解析代码中提供的OpenACC指令。'
- en: '`-ta`: Stands for the target architecture that the device code should be generated
    for. Note that `-ta=tesla` means we are targeting a NVIDIA GPU. Some examples
    of other targets include `-ta=multi-core`, which targets multi-core as the device,
    `-ta=radeaon`, which targets AMD GPUs, and a few others. Additionally, we can
    add device-specific flags; for example, we added a pinned flag to a GPU that allocates
    all CPU memory as pinned (non-pageable).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-ta`：代表应该为设备代码生成的目标架构。请注意，`-ta=tesla`表示我们的目标是NVIDIA GPU。其他目标的一些示例包括`-ta=multi-core`，它将多核作为设备目标，`-ta=radeaon`，它将AMD
    GPU作为目标，还有一些其他目标。此外，我们可以添加特定于设备的标志；例如，我们为分配所有CPU内存作为固定（不可分页）的GPU添加了一个固定标志。'
- en: '`-Minfo`: This option tells the compiler to provide us with more information
    about steps taken by the compiler to make our code parallel. By saying `-Minfo-accel`,
    we are asking the compiler to provide us with more information related to the
    accelerator region only. We can change the flag to `-Minfo=all` to provide details
    of the non-accelerator region also. The following output shows part of the output
    of adding a `Minfo` flag to our code:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-Minfo`：此选项告诉编译器为我们提供有关编译器采取的步骤的更多信息，使我们的代码并行化。通过说`-Minfo-accel`，我们要求编译器为我们提供与加速器区域相关的更多信息。我们可以将标志更改为`-Minfo=all`，以提供非加速器区域的详细信息。以下输出显示了向我们的代码添加`Minfo`标志的部分输出：'
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To understand this compilation output, we need to understand OpenACC pragmas,
    which we will do in the next section. We will revisit this compilation output
    later. Further details on other available flags can be found using `pgc++ --help`*.*
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这个编译输出，我们需要了解OpenACC pragma，我们将在下一节中进行。稍后我们将重新访问这个编译输出。可以使用`pgc++ --help`找到其他可用标志的更多详细信息。
- en: 'The sample output after running the binary is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 运行二进制文件后的示例输出如下：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding output shows that we are reading an image of size 1536*2048\.
    The code has one serial implementation and three parallel implementations using
    OpenACC pragmas. The timings of each of the implementations are shown in the preceding
    output. The last implementation with the pipeline approach shows the best timing:
    `0.0008 seconds`. We will take an incremental approach and go into the details
    of each implementation in the next sections.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出显示我们正在读取一个大小为1536*2048的图像。代码有一个串行实现和三个使用OpenACC pragma的并行实现。每个实现的时间在前面的输出中显示。最后一个使用pipeline方法的实现显示了最佳时间：`0.0008秒`。我们将采取增量方法，并在接下来的部分详细介绍每个实现。
- en: 'The serial implementation of this algorithm is very simple, and shown in the
    following code snippet:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的串行实现非常简单，如下面的代码片段所示：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There is nothing fancy about the code; basically, it takes two input image data
    (`in1` and `in2`), performs the average operation to merge both inputs, and finally
    stores the output. The key thing for us, with respect to parallelism, is that
    the loop is embarrassingly parallel and suitable for architecture such as GPUs.
    As shown in the preceding code output, serial implementation took `0.0028` seconds.
    Please note that the timings may vary slightly, based on the system that you are
    running the code on.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 代码没有什么特别之处；基本上，它接受两个输入图像数据（`in1`和`in2`），执行平均操作以合并两个输入，最后存储输出。对于我们来说，关键的是循环是尴尬并行的，适合于GPU等架构。如上面的代码输出所示，串行实现花费了`0.0028`秒。请注意，计时可能会因运行代码的系统而略有不同。
- en: In the next section, we will introduce you to the OpenACC directives necessary
    to convert the sample code to run on a GPU.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将向您介绍OpenACC指令，以便将示例代码转换为在GPU上运行所需的指令。
- en: OpenACC directives
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenACC指令
- en: 'In this section, we will try to understand the syntax of OpenACC pragmas, and
    implement basic parallel and data directives for the merge operation. The basic
    syntax of the OpenACC pragma is as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试理解OpenACC pragma的语法，并为合并操作实现基本的并行和数据指令。OpenACC pragma的基本语法如下：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding command is explained as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令解释如下：
- en: '`#pragma` in C/C++ is what''s known as a "compiler hint." These are very similar
    to programmer comments; however, the compiler will actually read our pragmas.
    If the compiler does not understand the pragma, it can ignore it, rather than
    throw a syntax error.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在C/C++中的`#pragma`被称为“编译器提示”。这些与程序员注释非常相似；但是，编译器实际上会读取我们的pragma。如果编译器不理解pragma，它可以忽略它，而不是抛出语法错误。
- en: '`acc` is an addition to our pragma. It specifies that this is an OpenACC pragma.
    Any non-OpenACC compiler will ignore this pragma.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acc`是我们pragma的一个补充。它指定这是一个OpenACC pragma。任何非OpenACC编译器都会忽略此pragma。'
- en: '`directive` is a command in OpenACC that will tell the compiler to perform
    some operation. For now, we will only use directives that allow the compiler to
    parallelize our code.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`指令`是OpenACC中的一个命令，它告诉编译器执行某些操作。目前，我们只会使用允许编译器并行化我们的代码的指令。'
- en: '`clauses` are additions/alterations to our directives. These include, but are
    not limited to, optimizations.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`子句`是对我们的指令的补充/修改。这些包括但不限于优化。'
- en: 'There are three directives we will cover in this section: *parallel*, *loop,*
    and *data*. We will showcase each of them and finally apply them to our merge
    algorithm.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍三个指令：*parallel*，*loop*和*data*。我们将展示它们各自的用法，并最终将它们应用到我们的合并算法中。
- en: Parallel and loop directives
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行和循环指令
- en: 'The parallel directive is the most straightforward of the directives. It will
    mark a region of the code for parallelization (this usually only involves parallelizing
    a single `for` loop), as shown in the following code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 并行指令是最直接的指令。它将标记代码的一个区域进行并行化（通常只涉及并行化一个`for`循环），如下面的代码所示：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We may also define a parallel region. The parallel region may have multiple
    loops (though this is often not recommended!). The parallel region is everything
    contained within the outer most curly braces, as shown in the following code snippet:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以定义一个并行区域。并行区域可以有多个循环（尽管这通常不推荐！）。并行区域是指最外层花括号内的所有内容，如下面的代码片段所示：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It is extremely important to include the loop; otherwise, you will not be parallelizing
    the loop properly. The parallel directive tells the compiler to parallelize the
    code redundantly, shown as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 包含循环非常重要；否则，您将无法正确地并行化循环。并行指令告诉编译器冗余地并行化代码，如下所示：
- en: '![](img/1ed25634-1b03-4d94-b6e8-f63049177ff5.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ed25634-1b03-4d94-b6e8-f63049177ff5.png)'
- en: 'The loop directive specifically tells the compiler that we want the loop parallelized,
    as shown in the following screenshot:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 循环指令明确告诉编译器我们希望并行化循环，如下面的屏幕截图所示：
- en: '![](img/03886faa-19f3-4ad8-9401-a3ebd22606d7.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03886faa-19f3-4ad8-9401-a3ebd22606d7.png)'
- en: 'The loop directive has two major uses:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 循环指令有两个主要用途：
- en: To mark a single loop for parallelization
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记单个循环进行并行化
- en: To allow us to explicitly define optimizations/alterations for the loop
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许我们明确定义循环的优化/修改
- en: 'We will cover loop optimizations later on in the chapter, along with gang and
    vector; for now, we will focus on the parallelization aspect. For the loop directive
    to work properly, it must be contained within the parallel directive:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面讨论循环优化，以及gang和vector；目前，我们将专注于并行化方面。循环指令要正常工作，必须包含在并行指令内：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When using the parallel directive, you must include the loop directive for
    the code to function properly. We may also use the loop directive to parallelize
    multidimensional loop nests. In the following code snippet, we see a nested loop
    and we mention the loop clause explicitly for the second loop:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用并行指令时，必须包含循环指令才能使代码正常运行。我们还可以使用循环指令来并行化多维循环嵌套。在下面的代码片段中，我们看到了一个嵌套循环，并且我们明确为第二个循环提到了循环子句：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that, in the preceding code snippet, we do not put the parallel clause
    again in the inner loop as we have already mentioned it in the scope that starts
    from the outer loop.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上面的代码片段中，我们没有在内部循环中再次放置并行子句，因为我们已经在从外部循环开始的范围中提到了它。
- en: Data directive
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据指令
- en: The OpenACC parallel model states that we have a host, which runs our sequential
    code (mostly it would be a CPU). Then we have our device, which is some sort of
    parallel hardware. The host and device usually (though not always) have separate
    memories, and the programmer can use OpenACC to move data between the two memories.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: OpenACC并行模型规定我们有一个主机，运行我们的顺序代码（通常是CPU）。然后我们有我们的设备，这是某种并行硬件。主机和设备通常（虽然并非总是）有单独的内存，程序员可以使用OpenACC在两个内存之间移动数据。
- en: As discussed in the first chapter, the GPU and CPU architectures are fundamentally
    different. The GPU, being a throughput-based architecture, has a high number of
    computational units along with high-speed memory bandwidth. The CPU, on the other
    hand, is a latency-reducing architecture, has a large cache hierarchy, and also
    provides a large main memory. Any data that needs to be operated on needs to be
    first copied to the GPU memory. (Note that even in the case of unified memory
    the data gets copied behind the scenes in the form of pages by the driver.)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第一章中讨论的，GPU和CPU架构在根本上是不同的。GPU作为吞吐量架构，具有大量计算单元和高速内存带宽。另一方面，CPU是一种减少延迟的架构，具有大型缓存层次结构，并且提供大容量的主存储器。需要操作的任何数据都需要首先复制到GPU内存。（请注意，即使在统一内存的情况下，数据也会在后台以页面的形式由驱动程序复制。）
- en: 'As illustrated in the following diagram, the data transfer between the two
    architectures (CPU and GPU) happens via an I/O bus:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，两种架构（CPU和GPU）之间的数据传输通过I/O总线进行：
- en: '![](img/ac932965-3f84-440d-be0a-1f8428eb7e48.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac932965-3f84-440d-be0a-1f8428eb7e48.png)'
- en: Our goal when using a GPU as the target architecture in OpenACC is to only use
    it to offload our parallel code, and the sequential code will continue to run
    on our CPU. The OpenACC standard allows the programmer to explicitly define data
    management by using the OpenACC **data directive and data clauses**. Data clauses
    allow the programmer to specify data transfers between the host and device (or,
    in our case, the CPU and the GPU).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenACC中使用GPU作为目标架构的目标是仅将并行代码卸载到GPU上，而顺序代码将继续在CPU上运行。OpenACC标准允许程序员通过使用OpenACC
    **数据指令和数据子句** 显式定义数据管理。数据子句允许程序员在主机和设备（或在我们的情况下，CPU和GPU）之间指定数据传输。
- en: '**I****mplicit data management**: We can leave the transfer of data to the
    compiler as shown in the following example:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐式数据管理：**我们可以将数据传输留给编译器，如下例所示：'
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding code, the compiler will understand that the `A` vector needs
    to be copied from the GPU, and generate an implicit transfer for the developer.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，编译器将理解需要从GPU复制`A`向量，并为开发人员生成隐式传输。
- en: '**Explicit data management**: It is good practice to make use of explicit data
    transfers to gain more control over the transfers, as shown in the following code
    where we are using the copy data clause:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**显式数据管理：**最好使用显式数据传输来获得对传输更多控制，如下面的代码中使用复制数据子句所示：'
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the preceding code snippet we make use of the copy data clause. The following
    diagram explains the steps executed when runtime reached the copy data directive:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了复制数据子句。下图解释了运行时到达复制数据指令时执行的步骤：
- en: '![](img/52965e3f-256c-4efd-925f-94fd52a84cd5.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52965e3f-256c-4efd-925f-94fd52a84cd5.png)'
- en: We will go into the details of these steps with the help of the merge code where
    we will be applying the data clauses.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过合并代码的详细步骤来解释这些步骤，其中我们将应用数据子句。
- en: 'Other available data clauses are as listed as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可用的数据子句如下所列：
- en: '| **Data clause** | **Description** | **Key usage** |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **数据子句** | **描述** | **关键用法** |'
- en: '| `copy(list)` |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `copy(list)` |'
- en: Allocates memory on the device
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在设备上分配内存
- en: Copies data from the host to the device when entering the region
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进入区域时，从主机复制数据到设备
- en: Copies data to the host when exiting the region
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在退出区域时，将数据复制到主机
- en: '| This is the default for input data structures that are modified and then
    returned from function |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 这是默认的输入数据结构，被修改后从函数返回 |'
- en: '| `copyin(list)` |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| `copyin(list)` |'
- en: Allocates memory on the device
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在设备上分配内存
- en: Copies data from the host to the device when entering the region
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进入区域时，从主机复制数据到设备
- en: '| Vectors that are just input to a subroutine |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 作为子例程的输入的向量 |'
- en: '| `copyout(list)` |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `copyout(list)` |'
- en: Allocates memory on the device
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在设备上分配内存
- en: Copies data to the host when exiting the region
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在退出区域时，将数据复制到主机
- en: '| A result that doesn''t overwrite the input data structure |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 不覆盖输入数据结构的结果 |'
- en: '| `create(list)` |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `create(list)` |'
- en: Only allocates memory on the device
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅在设备上分配内存
- en: No copy is made
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不进行复制
- en: '| Temporary arrays |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 临时数组 |'
- en: To maximize performance, the programmer should avoid all unnecessary data transfers,
    and hence explicit memory management is preferred over implicit data management.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化性能，程序员应避免所有不必要的数据传输，因此显式内存管理优于隐式数据管理。
- en: '**Array shaping: **Array shaping is how you specify the size of the array.
    If you do not specify a shape, then the compiler will try to assume the size.
    This works well in Fortran, since Fortran tracks the size of the array; however,
    it will most likely not work in C/C++. Array shaping is also the only way to copy
    a portion of data from the array (for example, if you only need to copy half of
    the array, this can be a performance boost, cutting out unnecessary copies), as
    shown in the following code snippet:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**数组形状：**数组形状是指定数组大小的方式。如果不指定形状，编译器将尝试假定大小。这在Fortran中效果很好，因为Fortran跟踪数组的大小；然而，在C/C++中可能不起作用。数组形状也是从数组复制数据的唯一方式（例如，如果只需要复制数组的一半，这可能提高性能，减少不必要的复制），如下面的代码片段所示：'
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This would copy all of the elements of `A` except for the first and last elements.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这将复制`A`的所有元素，除了第一个和最后一个元素。
- en: Applying the parallel, loop, and data directive to merge image code
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将并行、循环和数据指令应用于合并图像代码
- en: 'Let''s now try to apply the parallel, loop, and data directive to the merge
    sequential code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试将并行、循环和数据指令应用于合并顺序代码：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We have made both the loops (height: `y` and width: `x`) parallel using the
    parallel loop directive. Also, we have explicitly added data clauses to copy the
    data. Note that, since the `in1` and `in2` vectors are input only, they are copied
    using the `copyin()` data clause. The `out` vector is the output and is copied
    using the `copyout()` data clause. Let''s try to understand the compiler output
    for this function:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用并行循环指令并行化了两个循环（高度：`y`和宽度：`x`）。此外，我们还明确地添加了数据子句来复制数据。请注意，由于`in1`和`in2`向量只是输入，它们是使用`copyin()`数据子句进行复制的。`out`向量是输出，使用`copyout()`数据子句进行复制。让我们试着理解这个函数的编译器输出：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding compiler output shows that for the `merge_parallel_pragma` function
    the following actions have been generated by the compiler:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的编译器输出显示，对于`merge_parallel_pragma`函数，编译器生成了以下操作：
- en: At line 30, `copyin` was generated for the `in1` and `in2 `variables. The array
    size copied to the GPU before the kernel launch will be `[0:w*h]`.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第30行，为`in1`和`in2`变量生成了`copyin`。在内核启动前将被复制到GPU的数组大小将是`[0:w*h]`。
- en: At line 30, `copyout` was generated for the `out` variable. The array size that
    will be copied after the GPU kernel launch will be `[0:w*h]`.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第30行，为`out`变量生成了`copyout`。在GPU内核启动后将被复制的数组大小将是`[0:w*h]`。
- en: 'At lines 30 and 32, Tesla kernel code was generated:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第30和32行，生成了Tesla内核代码：
- en: At line 30, the outer loop was parallelized with gang-level parallelism.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第30行，外部循环使用了gang级并行化。
- en: At line 32, the inner loop was parallelized with vector-level parallelism
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第32行，内部循环使用了矢量级并行化
- en: 'When the code is run on V100, the time taken by this whole kernel is `0.0010s`.
    This is basically approximately twice as fast as the serial code. This may not
    sound impressive. The reason for that is that most time is spent on data transfers
    rather than kernel computation. In order to confirm this, let''s make use of `nvprof`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当代码在V100上运行时，整个内核所花费的时间为`0.0010s`。这基本上是串行代码的两倍快。这可能听起来并不令人印象深刻。原因是大部分时间花在了数据传输上，而不是内核计算。为了确认这一点，让我们使用`nvprof`：
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As you can observe in the preceding profiling output, 94% of the time is spent
    on data transfers while only 5% of the time (45 microseconds) is spent on kernel
    execution. The query you might have is: How do I know which kernel this is? If
    you look closely at the name of the GPU kernel, `merge_parallel_pragma_30_gpu`,
    the PGI compiler generated a CUDA kernel in the `merge_parallel_pragma` function at
    line 30, and that is how we can relate it back to the pragmas that put in a function
    at that line number.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的分析输出中所观察到的，94%的时间花在了数据传输上，而只有5%的时间（45微秒）花在了内核执行上。您可能会问：我怎么知道这是哪个内核？如果您仔细看GPU内核的名称，`merge_parallel_pragma_30_gpu`，PGI编译器在`merge_parallel_pragma`函数的第30行生成了一个CUDA内核，这就是我们如何将其与在该行号放置的编译指示联系起来的方式。
- en: So we know the problem, but what about the solution? The optimization technique
    that we will use to hide this latency is blocking. We will cover more about the
    blocking technique, and using the asynchronous clause to overlap this transfer,
    in upcoming sections.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们知道问题在哪里，但解决方案呢？我们将使用的优化技术是blocking来隐藏这种延迟。我们将在接下来的章节中更多地介绍blocking技术，并使用异步子句来重叠这个传输。
- en: Asynchronous programming in OpenACC
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenACC中的异步编程
- en: 'In order to achieve better performance for merging parallel code, we will make
    use of a concept called blocking. Blocking basically means that, rather than transferring
    the whole input and output arrays in one shot, we can create blocks of the array
    which can be transferred and operated in parallel. The following diagram demonstrates
    creating blocks and overlapping data transfers with the kernel execution:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现合并并行代码的更好性能，我们将利用一个叫做blocking的概念。Blocking基本上意味着，我们可以创建数组的块，而不是一次性传输整个输入和输出数组，这些块可以并行传输和操作。以下图表演示了创建块并在内核执行时重叠数据传输：
- en: '![](img/5521327f-6b8b-4e44-b04c-abbd69131c84.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5521327f-6b8b-4e44-b04c-abbd69131c84.png)'
- en: 'The preceding diagram shows that different blocks are transferred and the kernel
    execution of these blocks can be independent of each block. In order for this
    to happen, we need the data transfer commands and kernel calls to be fired and
    executed asynchronously. In order to achieve blocking, we will be introducing
    more directives/clauses in this section: the structured/unstructured data directive
    and `async` clause. We will showcase each of them and finally apply them to our
    basic OpenACC merge parallel code.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了不同的块被传输，这些块的内核执行可以独立于每个块。为了实现这一点，我们需要数据传输命令和内核调用被异步地触发和执行。为了实现blocking，我们将在本节中引入更多的指令/子句：结构化/非结构化数据指令和`async`子句。我们将展示它们的每一个，并最终将它们应用到我们的基本OpenACC合并并行代码中。
- en: Structured data directive
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化数据指令
- en: 'The OpenACC data directives allow the programmer to explicitly manage the data
    on the device (in our case, the GPU). The following code snippet shows an example
    of marking a structured data region:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: OpenACC数据指令允许程序员显式地管理设备上的数据（在我们的例子中是GPU）。以下代码片段显示了标记结构化数据区域的示例：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Device memory allocation happens at the beginning of the region, and device
    memory deallocation happens at the end of the region. Additionally, any data movement
    from the host to the device (CPU to GPU) happens at the beginning of the region,
    and any data movement from the device to the host (GPU to CPU) happens at the
    end of the region. Memory allocation/deallocation and data movement are defined
    by which clauses the programmer includes.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 设备内存分配发生在区域的开始，设备内存释放发生在区域的结束。此外，从主机到设备（CPU到GPU）的任何数据移动发生在区域的开始，从设备到主机（GPU到CPU）的任何数据移动发生在区域的结束。内存分配/释放和数据移动是由程序员包含的子句定义的。
- en: '**Encompassing multiple compute regions: **A single data region can contain
    any number of parallel/kernels regions, as shown in the following example:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**包含多个计算区域：**一个数据区域可以包含任意数量的并行/内核区域，如下例所示：'
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Unstructured data directive
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非结构化数据指令
- en: 'There are two unstructured data directives:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个非结构化数据指令：
- en: '**enter data**: Handles device memory allocation, and copies from the host
    to the device. The two clauses that you may use with enter data are:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进入数据**：处理设备内存分配，并从主机复制到设备。您可以在进入数据中使用的两个子句是：'
- en: '`create`: This will only perform device memory allocation.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create`：这将只执行设备内存分配。'
- en: '`copyin`: This will perform allocation along with a memory copy to the device.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`copyin`：这将执行分配以及从设备到设备的内存复制。'
- en: '**exit data**: Handles device memory deallocation, and copies from the device
    to the host. The two clauses that you may use with exit data are:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**退出数据**：处理设备内存释放，并从设备复制到主机。您可以在退出数据中使用的两个子句是：'
- en: '`delete`: This will perform only device memory deallocation.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delete`：这将仅执行设备内存释放。'
- en: '`copyout`: This will first do a memory copy from the device to the host, followed
    by device memory deallocation.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`copyout`：这将首先从设备复制内存到主机，然后执行设备内存释放。'
- en: 'Unstructured data directives do not mark a data region as you are able to have
    multiple enter data and exit data directives in your code. It is better to think
    of them purely as memory allocation and deallocation. The largest advantage of
    using unstructured data directives is their ability to branch across multiple
    functions. You may allocate your data in one function, and deallocate it in another.
    We can look at a simple example of that:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据指令不会将数据区域标记为您可以在代码中有多个进入数据和退出数据指令。最好将它们纯粹视为内存分配和释放。使用非结构化数据指令的最大优势是它们能够跨多个函数进行分支。您可以在一个函数中分配数据，并在另一个函数中释放它。我们可以看一个简单的例子：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The preceding code snippet shows that allocation happens in the separate `allocate()`
    function, and deletion happens in `deallocate()`. You can link the same concept
    to `enter data create` as part of the constructor and `exit data delete` as part
    of the destructor in C++.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码片段显示了分配发生在单独的`allocate()`函数中，删除发生在`deallocate()`中。您可以将相同的概念链接到C++中构造函数的一部分`enter
    data create`和析构函数的一部分`exit data delete`。
- en: Asynchronous programming in OpenACC
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenACC中的异步编程
- en: 'By default, all OpenACC calls are synchronous in nature. Which means that,
    after every call to a data transfer or every kernel call to the GPU, a synchronization
    gets added implicitly. The CPU will wait till the OpenACC call has finished and
    then start executing the next instruction. To make the call asynchronous, we can
    make use of the `async` clause along with the data and parallel directive, as
    shown in the following code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有OpenACC调用都是同步的。这意味着，在每次数据传输或每次对GPU的内核调用之后，都会隐式添加同步。CPU将等待直到OpenACC调用完成，然后开始执行下一条指令。为了使调用异步，我们可以在数据和并行指令中使用`async`子句，如下面的代码所示：
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The primary benefits of using `async` can be summarized as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`async`的主要好处可以总结如下：
- en: If we want to execute host and device code simultaneously, we can launch our
    device code with `async`, and while that executes we can go back to the host to
    continue unrelated (non-device dependent) code.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们想要同时执行主机和设备代码，我们可以使用`async`启动我们的设备代码，而在执行时我们可以返回到主机继续不相关（非设备相关）的代码。
- en: We can *queue up* multiple device kernel launches so that they execute back
    to back, which in some cases can reduce the overhead associated with launching
    device kernels.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以*排队*多个设备内核启动，以便它们连续执行，这在某些情况下可以减少启动设备内核的开销。
- en: We can perform device computation at the same time as data movement between
    host and device**.** This is the optimization we will be applying to our code,
    and is the most general use case of `async`.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在主机和设备之间同时执行数据移动和设备计算**。**这是我们将应用于我们的代码的优化，并且是`async`的最常见用例。
- en: Under the hood, whenever we use the `async` clause, we are adding some *work* to
    a queue. Work that is submitted to different queues can execute *asynchronously*,
    and work that is in the same queue will execute *sequentially* (one after the
    other). When we use `async`, we are able to specify a queue number. If no queue
    number is specified, then a default will automatically be used.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，每当我们使用`async`子句时，我们都会向*队列*添加一些*工作*。提交给不同队列的工作可以*异步*执行，而在同一队列中的工作将*顺序*执行（一个接着一个）。当我们使用`async`时，我们可以指定队列号。如果未指定队列号，则将自动使用默认值。
- en: Applying the unstructured data and async directives to merge image code
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将非结构化数据和异步指令应用于合并图像代码
- en: 'Let''s now try to apply data directives along with the `async` clause to merge
    parallel code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试将数据指令与`async`子句一起应用于合并并行代码：
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We have made use of data directives and also an `async` clause to implement
    the blocking concept. Let''s break down the overall implementation, which will
    make it simpler to understand:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了数据指令和`async`子句来实现阻塞概念。让我们分解整体实现，这将使其更容易理解：
- en: '**Enter data region**: The `enter data create` clause allocates memory for
    the `in1` and `in2` variables and `out` in the GPU.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**进入数据区域**：`enter data create`子句在GPU中为`in1`和`in2`变量以及`out`分配内存。'
- en: '**Creates blocks**: We decided that we will split the image into eight blocks.
    The blocks are split across rows. The outer `for` loop for the block gets added
    for this reason.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建块**：我们决定将图像分成八个块。这些块分布在行中。外部的`for`循环用于此目的添加了这个原因。'
- en: '**Transfer data from host to device asynchronously**: `acc update device` basically
    copies data from the host to the device asynchronously as we have added an `async`
    clause to the same.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**异步从主机传输数据到设备**：`acc update device`基本上将数据从主机异步复制到设备，因为我们已经在其中添加了一个`async`子句。'
- en: '**Launch parallel loop** **asynchronously**: The `async` clause is added to
    the parallel clause to launch the GPU kernel asynchronously.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**异步启动并行循环**：`async`子句被添加到并行子句中，以异步启动GPU内核。'
- en: '**Transfer data from device to host asynchronously**: `acc update self` basically
    copies the data from the device to the host asynchronously as we have added an
    `async` clause to the same.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**异步从设备传输数据到主机**：`acc update self`基本上是将数据从设备异步地复制到主机，因为我们已经在同一个地方添加了一个`async`子句。'
- en: '**Wait**: `acc wait` will make sure the CPU waits till all the OpenACC launches
    have finished, prior to moving forward in all the queues.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**等待**：`acc wait`将确保CPU等待，直到所有OpenACC启动都完成，然后在所有队列中继续前进。'
- en: '**Exit data region**: `acc exit data delete` will delete the `in1` and `in2`
    vectors and `out`, which were allocated in the `enter data` clause.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**退出数据区域**：`acc exit data delete`将删除在`enter data`子句中分配的`in1`和`in2`向量以及`out`。'
- en: 'Let''s try to understand the compiler output of the `merge_async_pipelined `function:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着理解`merge_async_pipelined`函数的编译器输出：
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding compiler output shows that, for the `merge_async_pipelined` function,
    the following actions have been generated by the compiler:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的编译器输出显示，对于`merge_async_pipelined`函数，编译器生成了以下操作：
- en: At line `67` , the `data create` region has been generated for the `in1`, `in2`
    and `out` variables.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第67行，为`in1`、`in2`和`out`变量生成了`data create`区域。
- en: 'At line `74` , `update device` is called for `in1` and `in2`, and the transfer
    of data to the device is restricted to block the upper and lower bounds: `in1[w*lower:w*(upper-lower)],in2[w*lower:w*(upper-lower)]`.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第74行，为`in1`和`in2`调用了`update device`，并且数据传输到设备被限制在上下界之间：`in1[w*lower:w*(upper-lower)],in2[w*lower:w*(upper-lower)]`。
- en: At lines `74` and `76` , the Tesla kernel code has been generated.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第74和76行，Tesla内核代码已经生成。
- en: 'At line `81`, `update self` is called for the `out` variable, and the transfer
    of data from the device is restricted to block the upper and lower bounds: `out[w*lower:w*(upper-lower)]`.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第81行，为`out`变量调用了`update self`，并且数据从设备传输被限制在上下界之间：`out[w*lower:w*(upper-lower)]`。
- en: At line `84`, the data region ends, and `delete` is called to free up the memory
    allocated on the GPU.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第84行，数据区域结束，并调用`delete`来释放在GPU上分配的内存。
- en: 'When the code is run on V100, the time taken by this whole kernel is 0.0008
    seconds. To understand this in more detail, let''s go back to the profiler. This
    time we will visualize the output by making use of the NVIDIA Visual Profiler:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当代码在V100上运行时，整个内核所花费的时间为0.0008秒。为了更详细地了解这一点，让我们回到分析器。这次我们将利用NVIDIA Visual Profiler来可视化输出：
- en: '![](img/2d3b6f80-eaf4-4292-ac0f-e1321f6907bf.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d3b6f80-eaf4-4292-ac0f-e1321f6907bf.png)'
- en: Output by using NVIDIA Visual Profiler
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NVIDIA Visual Profiler输出
- en: 'The preceding screenshot showstheVisual Profiler output after using `async`
    and blocking. The key message from the profiler window is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的屏幕截图显示了使用`async`和阻塞后的Visual Profiler输出。来自分析器窗口的关键消息如下：
- en: We see three streams being created and used. This is because our code uses `async(block%2)`,
    which means that we have requested max `2` queues. The third queue is the default
    queue and is not used during the pipeline execution.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们看到有三个流被创建和使用。这是因为我们的代码使用了`async(block%2)`，这意味着我们请求了最大2个队列。第三个队列是默认队列，在管道执行期间不被使用。
- en: We see that the host-to-device and device-to-host transfer also overlaps as
    the GPU has two **Direct Memory Access** (**DMA**) engines, and hence the data
    transfer in the opposite direction can be overlapped.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们看到主机到设备和设备到主机的传输也重叠了，因为GPU有两个**直接内存访问**（**DMA**）引擎，因此反向的数据传输也可以重叠。
- en: We also see that our kernel execution overlaps with the data transfer.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还看到我们的内核执行与数据传输重叠。
- en: So far we have seen key directives that helped us to make sequential code for image
    merging to run on a GPU. In the next section, we will introduce you to more clauses
    which will help you to optimize your OpenACC code further.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了帮助我们将顺序代码转换为在GPU上运行的图像合并的关键指令。在下一节中，我们将向您介绍更多的子句，这些子句将帮助您进一步优化您的OpenACC代码。
- en: Additional important directives and clauses
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他重要的指令和子句
- en: In this section, we will cover other important, widely used directives that
    we can apply to our merge algorithm.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍其他重要的广泛使用的指令，可以应用到我们的合并算法中。
- en: Gang/vector/worker
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gang/vector/worker
- en: 'Gang/worker/vector defines the various levels of parallelism we can achieve
    with OpenACC. This parallelism is most useful when parallelizing multi-dimensional
    loop nests. OpenACC allows us to define a generic gang/worker/vector model that
    will be applicable to a variety of hardware, but we will focus more on a GPU-specific
    implementation. The following diagram shows an OpenACC parallel programming model:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Gang/worker/vector定义了我们可以在OpenACC中实现的各种并行级别。这种并行在并行化多维循环嵌套时非常有用。OpenACC允许我们定义一个通用的gang/worker/vector模型，适用于各种硬件，但我们将更多地专注于GPU特定的实现。下图显示了OpenACC并行编程模型：
- en: '![](img/cd25f916-e4c0-4563-8add-eda78ea46a13.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd25f916-e4c0-4563-8add-eda78ea46a13.png)'
- en: This preceding diagram represents a single gang. When parallelizing our `for`
    loops, the loop iterations will be broken up evenly among a number of gangs. Each
    gang will contain a number of threads. These threads are organized into blocks.
    A worker is a row of threads.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个前面的图表代表了一个单一的gang。当我们并行化我们的`for`循环时，循环迭代将会被均匀地分配给多个gang。每个gang将包含一定数量的线程。这些线程被组织成块。一个worker是一行线程。
- en: In the preceding figure, there are three workers, which means that there are
    three rows of threads. The vector refers to how long each row is. So in the preceding
    graphic, the vector is eight, because each row is eight threads long. By default,
    when programming for a GPU, gang and vector parallelism is automatically applied.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，有三个worker，这意味着有三行线程。向量指的是每行有多长。所以在前面的图中，向量是八，因为每行有八个线程。在为GPU编程时，默认情况下会自动应用gang和vector并行。
- en: 'As OpenACC is an open standard and targets multiple hardware; it provides generic
    constructs. But how does this construct get mapped to a particular target device?
    The answer is simple; it depends on the architecture and compiler, and hence providing
    performance portability. If we were to map how the current PGI compiler maps this
    concept to CUDA (NVIDIA GPU), it would be as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于OpenACC是一个开放标准并且面向多种硬件，它提供了通用构造。但是这个构造如何映射到特定的目标设备呢？答案很简单；这取决于架构和编译器，因此提供了性能可移植性。如果我们要映射当前PGI编译器如何将这个概念映射到CUDA（NVIDIA
    GPU），那么它将如下所示：
- en: The OpenACC gang maps to a CUDA block.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenACC gang映射到CUDA块。
- en: The worker essentially maps to a CUDA warp.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: worker本质上映射到CUDA线程束。
- en: The OpenACC vector maps to `threadIdx.x` and (X dimension).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenACC向量映射到`threadIdx.x`和（X维度）。
- en: The OpenACC worker maps to `threadIdx.y` (Y dimension).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenACC worker映射到`threadIdx.y`（Y维度）。
- en: 'Again it is important to reiterate that this is how the PGI compiler maps the
    OpenACC constructs. Other compilers might map this differently. Specifically for
    NVIDIA GPUs, the gang worker vector will define the organization of our GPU threads.
    By adding the following clauses, the developer can tell the compiler which levels
    of parallelism to use on given loops:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这是PGI编译器如何映射OpenACC构造的方式。其他编译器可能会以不同的方式进行映射。特别是对于NVIDIA GPU，gang worker
    vector将定义我们的GPU线程的组织。通过添加以下子句，开发人员可以告诉编译器在给定的循环上使用哪些并行级别：
- en: '`gang`: Marks the loop for gang parallelism.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gang`: 标记用于gang并行的循环。'
- en: '`worker`: Marks the loop for worker parallelism.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`worker`: 标记用于工作并行的循环。'
- en: '`vector`: Marks the loop for vector parallelism.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vector`: 标记用于向量并行的循环。'
- en: 'The following code snippet has three loops, and each loop parallelism has been
    explicitly defined: the outer loop as `gang`, the middle loop as the `worker`
    loop, and the innermost loop as the `vector` loop:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段有三个循环，并且每个循环的并行性都已经明确定义：外循环为`gang`，中间循环为`worker`循环，最内层循环为`vector`循环：
- en: '[PRE21]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Adjusting gangs, workers, and vectors: **The compiler will choose a number
    of gangs and workers and a vector length for you, but you can change it with the
    following clauses:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**调整gangs、workers和vectors：**编译器将为您选择一定数量的gangs和workers以及向量长度，但您可以使用以下子句进行更改：'
- en: '`num_gangs(N)`: Generates the `N` gangs for the parallel region'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_gangs(N)`: 为并行区域生成`N`个gangs'
- en: '`num_workers(M)`: Generates `M` workers for the parallel region.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers(M)`: 为并行区域生成`M`个workers。'
- en: '`vector_length(Q)`: Uses a vector length of `Q` for the parallel region'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vector_length(Q)`: 为并行区域使用向量长度`Q`'
- en: 'For an example in the following code snippet we have set the number of gangs
    to `2`, the number of workers to `2` and the vector length to `32`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段的示例中，我们将gangs的数量设置为`2`，workers的数量设置为`2`，向量长度设置为`32`：
- en: '[PRE22]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It is rarely a good idea to set the number of gangs in your code—let the compiler
    decide. Most of the time you can effectively tune a loop nest by adjusting only
    the vector length. Also, it is rare to use a worker loop for the GPU.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中设置gangs的数量很少是一个好主意——让编译器决定。大多数情况下，您可以通过调整向量长度有效地调整循环嵌套。此外，在GPU上很少使用worker循环。
- en: Managed memory
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 托管内存
- en: OpenACC provides an option to allow the compiler to handle memory management.
    We will be able to achieve better performance by managing memory ourselves; however,
    allowing the compiler to use the managed memory is very simple. We do not need
    to make any changes to our code to get the managed memory working.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: OpenACC提供了一个选项，允许编译器处理内存管理。通过自己管理内存，我们将能够获得更好的性能；但是，允许编译器使用托管内存非常简单。我们不需要对我们的代码进行任何更改，就可以让托管内存正常工作。
- en: 'In order to make use of the managed memory, we can pass the managed flag to
    the `pgc++` compiler like this:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用托管内存，我们可以像这样将托管标志传递给`pgc++`编译器：
- en: '[PRE23]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: After adding the managed clause, the compiler will basically ignore the data
    clauses, and the managed memory is used to transfer data between the CPU and GPU.
    Note that the managed memory is only for heap data and not stack/static data.
    The unified memory concept that we covered in the previous chapter will remain
    the same.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了托管子句后，编译器基本上会忽略数据子句，并且托管内存用于在CPU和GPU之间传输数据。请注意，托管内存仅用于堆数据，而不是栈/静态数据。我们在上一章介绍的统一内存概念将保持不变。
- en: Kernel directive
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核指令
- en: 'The kernel directive allows the programmer to step back and rely solely on
    the compiler. Some sample code using a kernel directive is as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 内核指令允许程序员退一步，完全依赖编译器。使用内核指令的一些示例代码如下：
- en: '[PRE24]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Just like in the parallel directive example, we are parallelizing a single loop.
    Recall that, when using the parallel directive, it must always be paired with
    the loop directive; otherwise, the code will be improperly parallelized. The kernel
    directive does not follow the same rule; in some compilers, adding the loop directive
    may limit the compiler's ability to optimize the code.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 就像并行指令示例中一样，我们正在并行化一个循环。请记住，使用并行指令时，必须始终与循环指令配对；否则，代码将无法正确并行化。内核指令不遵循相同的规则；在一些编译器中，添加循环指令可能会限制编译器优化代码的能力。
- en: The kernel directive is the exact opposite of the parallel directive. This means
    that the compiler is making a lot of assumptions, and may even override the programmer's
    decision to parallelize the code. Also, by default, the compiler will attempt
    to optimize the loop. The compiler is generally pretty good at optimizing loops,
    and sometimes may be able to optimize the loop in a way that the programmer cannot
    describe. However, usually programmers will be able to achieve better performance
    by optimizing the loop themselves.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 内核指令是并行指令的完全相反。这意味着编译器做出了很多假设，甚至可能覆盖程序员并行化代码的决定。此外，默认情况下，编译器将尝试优化循环。编译器通常很擅长优化循环，并且有时甚至可以以程序员无法描述的方式优化循环。然而，通常程序员可以通过自己优化循环来获得更好的性能。
- en: 'If you run into a situation where the compiler refuses to parallelize a loop,
    you may override the compiler''s decision. (However, keep in mind that by overriding
    the compiler''s decision, you are taking responsibility for any mistakes that
    occur from parallelizing the code!) In this code segment, we are using the independent
    clause to assure the compiler that we think the loop is parallelizable:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遇到编译器拒绝并行化循环的情况，您可以覆盖编译器的决定。（但请记住，通过覆盖编译器的决定，您要对并行化代码造成的任何错误负责！）在这段代码中，我们使用独立子句来向编译器保证我们认为该循环是可以并行化的：
- en: '[PRE25]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'One of the most telling advantages of the kernel directive is its ability to
    parallelize many loops at once. For example, in the following code segment, we
    are able to effectively parallelize two loops at once by utilizing a kernel region:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Kernel指令最明显的优势之一是它能够同时并行化许多循环。例如，在下面的代码段中，我们能够通过利用内核区域同时有效地并行化两个循环：
- en: '[PRE26]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Collapse clause
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Collapse子句
- en: 'The **collapse clause** allows us to transform multi-dimensional loop nests
    into a single-dimensional loop. This process is helpful for increasing the overall
    length (which usually increases parallelism) of our loops, and will often help
    with memory locality. Let''s look at the syntax:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**collapse子句**允许我们将多维循环嵌套转换为单一维度循环。这个过程对于增加循环的整体长度（通常增加并行性）和通常有助于内存局部性。让我们看一下语法：'
- en: '[PRE27]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The code will combine the three-dimensional loop nest into a single one-dimensional
    loop.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码将三维循环嵌套合并为单一维度循环。
- en: Tile clause
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tile子句
- en: 'The **tile clause** allows us to break up a multi-dimensional loop into *tiles*,
    or *blocks*. This is often useful for increasing memory locality in some code.
    Let''s look at the syntax:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**tile子句**允许我们将多维循环分解为*瓦片*或*块*。这通常对于增加某些代码的内存局部性很有用。让我们看一下语法：'
- en: '[PRE28]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The preceding code will break our loop iterations up into 32 x 32 tiles (or
    blocks), and then execute those blocks in parallel.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将我们的循环迭代分成32 x 32个瓦片（或块），然后并行执行这些块。
- en: CUDA interoperability
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA互操作性
- en: As mentioned earlier in the chapter, OpenACC is not an alternative to CUDA languages;
    in fact, developers can start making use of OpenACC to port hotspots to a GPU.
    They can start integrating CUDA kernels for the most critical function only. There
    are several ways to turn an OpenACC/CUDA into interoperable code. We will look
    at some of them in this section.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章前面提到的，OpenACC并不是CUDA语言的替代品；事实上，开发人员可以开始利用OpenACC将热点部分移植到GPU上。他们可以开始仅集成CUDA内核以用于最关键的功能。有几种方法可以将OpenACC/CUDA转换为可互操作的代码。我们将在本节中介绍其中一些。
- en: DevicePtr clause
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DevicePtr子句
- en: 'This clause can be used to map the CUDA device pointer allocated using `cudaMalloc`
    and pass it to OpenACC. The following code snippet shows the use of the `deviceptr` clause:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这个子句可以用来映射使用`cudaMalloc`分配的CUDA设备指针，并将其传递给OpenACC。以下代码片段展示了`deviceptr`子句的使用：
- en: '[PRE29]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Normally, the OpenACC runtime expects to be given a host pointer, which will
    then be translated to some associated device pointer. The `deviceptr` clause is
    a way to tell the OpenACC runtime that a given pointer should not be translated
    since it is already a device pointer.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，OpenACC运行时期望得到一个主机指针，然后将其转换为一些相关的设备指针。`deviceptr`子句是一种告诉OpenACC运行时一个给定指针不应该被转换，因为它已经是一个设备指针的方法。
- en: Routine directive
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Routine指令
- en: 'The last topic to discuss is using CUDA device functions within OpenACC parallel
    and kernel regions. These are functions that are compiled to be called by the
    accelerator within a GPU kernel or OpenACC region. To use CUDA `__device__` functions
    within our OpenACC loops, we can also use the routine directive:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要讨论的话题是在OpenACC并行和内核区域内使用CUDA设备函数。这些函数是编译为由GPU内核或OpenACC区域调用的。为了在我们的OpenACC循环中使用CUDA
    `__device__`函数，我们还可以使用routine指令：
- en: '[PRE30]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Please note that this chapter provides a practical approach to making use of
    OpenACC and does not cover the whole standard API. For extensive API information,
    see [https://www.openacc.org/.](https://www.openacc.org/)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本章提供了一种实际利用OpenACC的方法，不涵盖整个标准API。有关广泛的API信息，请参阅[https://www.openacc.org/.](https://www.openacc.org/)
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we provided you with an alternative approach to making use
    of a GPU. This directive-based programming approach using OpenACC is really popular
    for legacy applications, and also for new applications it provides a very easy
    and portable approach. Using this approach, you can see how compilers have become
    more advanced. User feedback on directives has been used by making use of directives
    can generate optimal parallel code for different architectures.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们为您提供了一种利用GPU的替代方法。使用OpenACC的基于指令的编程方法对于传统应用程序非常受欢迎，对于新应用程序也提供了一种非常简单和可移植的方法。使用这种方法，您可以看到编译器变得更加先进。用户对指令的反馈已经被使用，通过利用指令可以为不同的架构生成最佳的并行代码。
- en: We covered parallel directives that provide an instruction/hint to the compiler
    about which part in the code to make parallel. We also made use of data directives
    to take control of the data transfer instead of relying on managed memory. With
    the use of an asynchronous clause, we also tried optimizing our application by
    overlapping kernels and data transfers. We explored mapping OpenACC constructs
    to the CUDA hierarchy, and also how OpenACC and CUDA C/C++ code can interoperate.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了提供指示/提示给编译器的并行指令。我们还利用数据指令来控制数据传输，而不是依赖于托管内存。通过使用异步子句，我们还尝试通过重叠内核和数据传输来优化我们的应用程序。我们探讨了将OpenACC构造映射到CUDA层次结构，以及OpenACC和CUDA
    C/C++代码之间的互操作性。
- en: In the next chapter, we will start applying our knowledge of CUDA to deep learning.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始将我们对CUDA的知识应用于深度学习。
