- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Concurrency
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发
- en: After covering lazy evaluation and proxy objects in the last chapter, we will
    now explore how to write concurrent programs in C++ using threads with shared
    memory. We will look at ways to make concurrent programs correct by writing programs
    that are free from data races and deadlocks. This chapter will also contain advice
    on how to make concurrent programs run with low latency and high throughput.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中涵盖了惰性求值和代理对象之后，我们现在将探讨如何使用共享内存在C++中编写并发程序。我们将探讨如何通过编写没有数据竞争和死锁的程序来使并发程序正确运行。本章还将包含关于如何使并发程序以低延迟和高吞吐量运行的建议。
- en: Before we go any further, you should know that this chapter is not a complete
    introduction to concurrent programming, nor will it cover all the details of concurrency
    in C++. Instead, this chapter is an introduction to the core building blocks of
    writing concurrent programs in C++, mixed with some performance-related guidelines.
    If you haven't written concurrent programs before, it is wise to go through some
    introductory material to cover the theoretical aspects of concurrent programming.
    Concepts such as deadlocks, critical sections, condition variables, and mutexes
    will be very briefly discussed, but this will serve more as a refresher than a
    thorough introduction to the concepts.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，你应该知道本章不是并发编程的完整介绍，也不会涵盖C++中所有并发的细节。相反，本章是C++中编写并发程序的核心构建块的介绍，结合了一些与性能相关的指导方针。如果你以前没有编写过并发程序，最好通过一些入门材料来了解并发编程的理论方面。死锁、临界区、条件变量和互斥锁等概念将会被简要讨论，但这将更像是一个复习而不是对概念的彻底介绍。
- en: 'The chapter covers the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下内容：
- en: The fundamentals of concurrent programming, including parallel execution, shared
    memory, data races, and deadlocks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并发编程的基础知识，包括并行执行、共享内存、数据竞争和死锁
- en: An introduction to the C++ thread support library, the atomic library, and the
    C++ memory model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C++线程支持库、原子库和C++内存模型的介绍
- en: A short example of lock-free programming
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无锁编程的简短示例
- en: Performance guidelines
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能指南
- en: Understanding the basics of concurrency
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解并发编程的基础知识
- en: 'A concurrent program can execute multiple tasks at the same time. Concurrent
    programming is, in general, a lot harder than sequential programming, but there
    are several reasons why a program may benefit from being concurrent:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 并发程序可以同时执行多个任务。并发编程一般比顺序编程更难，但有几个原因可以使程序从并发中受益：
- en: '**Efficiency**: The smartphones and desktop computers of today have multiple
    CPU cores that can execute multiple tasks in parallel. If you manage to split
    a big task into subtasks that can be run in parallel, it is theoretically possible
    to divide the running time of the big task by the number of CPU cores. For programs
    that run on machines with one single core, there can still be a gain in performance
    if a task is I/O bound. While one subtask is waiting for I/O, other subtasks can
    still perform useful work on the CPU.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率**：今天的智能手机和台式电脑都有多个CPU核心，可以并行执行多个任务。如果你成功地将一个大任务分割成可以并行运行的子任务，理论上可以将大任务的运行时间除以CPU核心数。对于在单核机器上运行的程序，如果一个任务是I/O绑定的，仍然可以获得性能上的提升。当一个子任务在等待I/O时，其他子任务仍然可以在CPU上执行有用的工作。'
- en: '**Responsiveness and low latency contexts**: For applications with a graphical
    user interface, it is important to never block the UI so that the application
    becomes unresponsive. To prevent unresponsiveness, it is common to let long-running
    tasks (like loading a file from disk or fetching some data from the network) execute
    in separate background threads so that the thread responsible for the UI is never
    blocked by long-running tasks. Another example where low latency matters is real-time
    audio. The function responsible for producing buffers of audio data is executed
    in a separate high-priority thread, while the rest of the program can run in lower-priority
    threads to handle the UI and so on.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**响应性和低延迟环境**：对于具有图形用户界面的应用程序，重要的是永远不要阻塞UI，以免应用程序变得无响应。为了防止无响应，通常会让长时间运行的任务（如从磁盘加载文件或从网络获取数据）在单独的后台线程中执行，以便负责UI的线程永远不会被长时间运行的任务阻塞。低延迟很重要的另一个例子是实时音频。负责生成音频数据缓冲区的函数在单独的高优先级线程中执行，而程序的其余部分可以在低优先级线程中运行，以处理UI等。'
- en: '**Simulation**: Concurrency can make it easier to simulate systems that are
    concurrent in the real world. After all, most things around us happen concurrently,
    and sometimes it is very hard to model concurrent flows with a sequential programming
    model. We will not focus on simulation in this book, but will instead focus on
    performance-related aspects of concurrency.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模拟**：并发可以使模拟现实世界中并发系统变得更容易。毕竟，我们周围的大多数事情都是同时发生的，有时很难用顺序编程模型来建模并发流。本书不会专注于模拟，而是专注于并发的性能相关方面。'
- en: Concurrency solves many problems for us, but introduces new ones, as we will
    discuss next.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 并发为我们解决了许多问题，但也引入了新问题，接下来我们将讨论这些问题。
- en: What makes concurrent programming hard?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发编程为何如此困难？
- en: 'There are a number of reasons why concurrent programming is hard, and, if you
    have written concurrent programs before, you have most likely already encountered
    the ones listed here:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多原因使并发编程变得困难，如果你以前编写过并发程序，你很可能已经遇到了以下列出的原因：
- en: Sharing state between multiple threads in a safe manner is hard. Whenever we
    have data that can be read and written to at the same time, we need some way of
    protecting that data from data races. You will see many examples of this later
    on.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以安全的方式在多个线程之间共享状态是困难的。每当我们有可以同时读写的数据时，我们需要一些方法来保护这些数据免受数据竞争的影响。稍后你将看到许多这样的例子。
- en: Concurrent programs are usually more complicated to reason about because of
    the multiple parallel execution flows.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于多个并行执行流，并发程序通常更难推理。
- en: Concurrency complicates debugging. Bugs that occur because of data races can
    be very hard to debug since they are dependent on how threads are scheduled. These
    kinds of bugs can be hard to reproduce and, in the worst-case scenario, they may
    even cease to exist when running the program using a debugger. Sometimes an innocent
    debug trace to the console can change the way a multithreaded program behaves
    and make the bug temporarily disappear. You have been warned!
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并发使调试变得复杂。由于数据竞争而导致的错误可能非常难以调试，因为它们依赖于线程的调度方式。这类错误很难复现，并且在最坏的情况下，甚至在使用调试器运行程序时可能会消失。有时，对控制台的无辜调试跟踪可能会改变多线程程序的行为方式，并使错误暂时消失。你已经被警告了！
- en: Before we start looking at concurrent programming using C++, a few general concepts
    related to concurrent and parallel programming will be introduced.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用C++进行并发编程之前，将介绍一些与并发和并行编程相关的一般概念。
- en: Concurrency and parallelism
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发和并行
- en: '**Concurrency** and **parallelism** are two terms that are sometimes used interchangeably.
    However, they are not the same and it is important to understand the differences
    between them. A program is said to run concurrently if it has multiple individual
    control flows running during overlapping time periods. In C++, each individual
    control flow is represented by a thread. The threads may or may not execute at
    the exact same time, though. If they do, they are said to execute in parallel.
    For a concurrent program to run in parallel, it needs to be executed on a machine
    that has support for parallel execution of instructions; that is, a machine with
    multiple CPU cores.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**并发**和**并行**是有时可以互换使用的两个术语。然而，它们并不相同，重要的是要理解它们之间的区别。如果程序在重叠的时间段内具有多个单独的控制流运行，则称其并发运行。在C++中，每个单独的控制流由一个线程表示。这些线程可能会或可能不会同时执行。如果它们同时执行，就称为并行执行。要使并发程序并行运行，需要在支持指令并行执行的机器上执行它；也就是说，具有多个CPU核心的机器。'
- en: At first glance, it might seem obvious that we always want concurrent programs
    to run in parallel if possible, for efficiency reasons. However, that is not necessarily
    always true. A lot of synchronization primitives (such as mutex locks) covered
    in this chapter are required only to support the parallel execution of threads.
    Concurrent tasks that are not run in parallel do not require the same locking
    mechanisms and can be a lot easier to reason about.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，似乎很明显我们总是希望并发程序尽可能并行运行，出于效率原因。然而，这并不一定总是正确的。本章涵盖的许多同步原语（如互斥锁）仅需要支持线程的并行执行。不在并行运行的并发任务不需要相同的锁定机制，可能更容易推理。
- en: Time slicing
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间片
- en: 'You might ask, "How are concurrent threads executed on machines with only a
    single CPU core?" The answer is **time slicing**. It is the same mechanism that
    is used by the operating system to support the concurrent execution of processes.
    In order to understand time slicing, let''s assume we have two separate sequences
    of instructions that should be executed concurrently, as shown in the following
    figure:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，“在只有一个CPU核心的机器上如何执行并发线程？”答案是**时间片**。这是操作系统用来支持进程并发执行的相同机制。为了理解时间片，让我们假设我们有两个应该同时执行的独立指令序列，如下图所示：
- en: '![](img/B15619_11_01.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/B15619_11_01.png)
- en: 'Figure 11.1: Two separate sequences of instructions'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：两个独立的指令序列
- en: The numbered boxes represent the instructions. Each sequence of instructions
    is executed in a separate thread, labeled **T1** and **T2**. The operating system
    will schedule each thread to have some limited time on the CPU and then perform
    a context switch. The context switch will store the current state of the running
    thread and load the state of the thread that should be executed. This is done
    often enough so that it appears as if the threads are running at the same time.
    A context switch is time-consuming, though, and most likely will generate a lot
    of cache misses each time a new thread gets to execute on a CPU core. Therefore,
    we don't want context switches to happen too often.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 编号的方框表示指令。每个指令序列在一个单独的线程中执行，标记为**T1**和**T2**。操作系统将安排每个线程在CPU上有一定的时间，然后执行上下文切换。上下文切换将存储正在运行的线程的当前状态，并加载应该执行的线程的状态。这样做的频率足够高，以至于看起来好像线程在同时运行。然而，上下文切换是耗时的，并且每次新线程在CPU核心上执行时很可能会产生大量的缓存未命中。因此，我们不希望上下文切换发生得太频繁。
- en: 'The following figure shows a possible execution sequence of two threads that
    are being scheduled on a single CPU:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了两个线程在单个CPU上调度的可能执行顺序：
- en: '![](img/B15619_11_02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/B15619_11_02.png)
- en: 'Figure 11.2: A possible execution of two threads. The dots indicate context
    switches'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：两个线程的可能执行。点表示上下文切换
- en: The first instruction of thread T1 starts, and is then followed by a context
    switch to let thread T2 execute the first two instructions. As programmers, we
    must make sure that the program can run as expected, regardless of how the operating
    system scheduler is scheduling the tasks. If a sequence, for some reason, is invalid,
    there are ways to control the order in which the instructions get executed by
    using locks, which will be covered later on.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: T1线程的第一条指令开始执行，然后进行上下文切换，让T2线程执行前两条指令。作为程序员，我们必须确保程序可以按预期运行，无论操作系统调度程序如何调度任务。如果某个序列因某种原因无效，有方法可以通过使用锁来控制指令执行的顺序，这将在后面介绍。
- en: If a machine has multiple CPU cores, it is possible to execute the two threads
    in parallel. However, there is no guarantee (it's even unlikely) that the two
    threads will execute on one core each throughout the lifetime of the program.
    The entire system shares time on the CPU, so the scheduler will let other processes
    execute as well. This is one of the reasons why the threads are not scheduled
    on dedicated cores.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一台机器有多个CPU核心，就有可能并行执行两个线程。然而，并没有保证（甚至是不太可能）这两个线程在程序的整个生命周期中都会在各自的核心上执行。整个系统共享CPU的时间，所以调度程序也会让其他进程执行。这就是为什么线程不会被调度到专用核心上的原因之一。
- en: '*Figure 11.3* shows the execution of the same two threads, but now they are
    running on a machine with two CPU cores. As you can see, the second and third
    instructions of the first thread (white boxes) are executing at the exact same
    time as the other thread is executing — the two threads are executing in parallel:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.3*显示了相同的两个线程的执行情况，但现在它们在一个有两个CPU核心的机器上运行。正如你所看到的，第一个线程的第二和第三条指令（白色框）与另一个线程同时执行
    - 两个线程在并行执行：'
- en: '![](img/B15619_11_03.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_03.png)'
- en: 'Figure 11.3: Two threads executing on a multicore machine. This makes it possible
    to execute the two threads in parallel.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：两个线程在多核机器上执行。这使得两个线程可以并行执行。
- en: Let's discuss shared memory next.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们讨论共享内存。
- en: Shared memory
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享内存
- en: Threads created in the same process share the same virtual memory. This means
    that a thread can access any data that is addressable within the process. The
    operating system, which protects memory between processes using virtual memory,
    does nothing to protect us from accidentally accessing memory inside a process
    that was not intended to be shared among different threads. Virtual memory only
    protects us from accessing memory allocated in different processes to our own.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一进程中创建的线程共享相同的虚拟内存。这意味着一个线程可以访问进程内可寻址的任何数据。操作系统使用虚拟内存在进程之间保护内存，但对于意外访问进程内未打算在不同线程之间共享的内存，操作系统不会提供保护。虚拟内存只保护我们免受访问分配给我们自己的不同进程中的内存的影响。
- en: Sharing memory between multiple threads can be a very efficient way to handle
    communication between threads. However, sharing memory in a safe way between threads
    is one of the major challenges when writing concurrent programs in C++. We should
    always strive to minimize the number of shared resources between threads.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个线程之间共享内存可以是处理线程间通信的一种非常有效的方式。然而，在C++中编写并发程序时，以安全的方式在线程之间共享内存是一个主要挑战之一。我们应该始终努力将线程之间共享的资源数量最小化。
- en: Fortunately, not all memory is shared by default. Each thread has its own stack
    for storing local variables and other data necessary for handling function calls.
    Unless a thread passes references or pointers to local variables to other threads,
    no other thread will be able to access the stack from that thread. This is one
    more reason to use the stack as much as possible (if you are not already convinced
    that the stack is a good place for your data after reading *Chapter 7*, *Memory
    Management*).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，并非所有内存默认都是共享的。每个线程都有自己的堆栈，用于存储本地变量和处理函数调用所需的其他数据。除非一个线程将本地变量的引用或指针传递给其他线程，否则其他线程将无法访问该线程的堆栈。这是尽可能使用堆栈的另一个原因（如果你在阅读*第7章*，*内存管理*后还不相信堆栈是一个好地方存储数据）。
- en: There is also **thread local storage**, sometimes abbreviated to **TLS**, which
    can be used to store variables that are global in the context of a thread but
    which are not shared between threads. A thread local variable can be thought of
    as a global variable where each thread has its own copy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 还有**线程本地存储**，有时缩写为**TLS**，它可以用来存储在线程上下文中是全局的，但在不同线程之间不共享的变量。线程本地变量可以被视为每个线程都有自己副本的全局变量。
- en: Everything else is shared by default; that is, dynamic memory allocated on the
    heap, global variables, and static local variables. Whenever you have shared data
    that is mutated by some thread, you need to ensure that no other thread is accessing
    that data at the same time or you will have a data race.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其他所有内容默认情况下都是共享的；即堆上分配的动态内存、全局变量和静态局部变量。每当你有被某个线程改变的共享数据时，你需要确保没有其他线程同时访问该数据，否则就会出现数据竞争。
- en: 'Remember the figure from the *Process memory* section of *Chapter 7*, *Memory
    Management*, which illustrated the virtual address space of a process? Here it
    is again, but modified to show how it looks when a process contains multiple threads.
    As you can see in the following figure, each thread has its own stack memory,
    but there is only one heap for all threads:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得*第7章*，*内存管理*中*进程内存*部分的图示吗？这里再次展示，但修改后显示了当一个进程包含多个线程时的情况。如下图所示，每个线程都有自己的堆栈内存，但所有线程只有一个堆：
- en: '![](img/B15619_11_04.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_04.png)'
- en: 'Figure 11.4: A possible layout of the virtual address space for a process'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：进程的虚拟地址空间的可能布局
- en: The process contains three threads in this example. The heap memory is by default
    shared by all threads.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，进程包含三个线程。堆内存默认情况下被所有线程共享。
- en: Data races
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据竞争
- en: A **data race** happens when two threads are accessing the same memory at the
    same time and at least one of the threads is mutating the data. If your program
    has a data race, it means that your program has undefined behavior. The compiler
    and optimizer will *assume* that there are no data races in your code and optimize
    it under that assumption. This may result in crashes or other completely surprising
    behavior. In other words, you can under no circumstances allow data races in your
    program. The compiler usually doesn't warn you about data races since they are
    hard to detect at compile time.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据竞争**发生在两个线程同时访问同一内存且至少一个线程正在改变数据时。如果你的程序有数据竞争，这意味着你的程序有未定义的行为。编译器和优化器会*假设*你的代码中没有数据竞争，并在这个假设下对其进行优化。这可能导致崩溃或其他完全令人惊讶的行为。换句话说，你绝对不能允许程序中出现数据竞争。编译器通常不会在编译时警告你有数据竞争，因为它们很难在编译时检测到。'
- en: Debugging data races can be a real challenge and sometimes requires tools such
    as **ThreadSanitizer** (from Clang) or **Concurrency Visualizer** (a Visual Studio
    extension). These tools typically instrument the code so that a runtime library
    can detect, warn about, or visualize potential data races while running the program
    you are debugging.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 调试数据竞争可能是一个真正的挑战，有时需要像**ThreadSanitizer**（来自Clang）或**Concurrency Visualizer**（Visual
    Studio扩展）这样的工具。这些工具通常会对代码进行插装，以便运行时库可以在调试程序运行时检测、警告或可视化潜在的数据竞争。
- en: 'Example: A data race'
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 例子：数据竞争
- en: '*Figure 11.5* shows two threads that are going to update an integer called
    `counter`. Imagine that these threads are both incrementing a global counter variable
    with the instruction `++counter`. It turns out that incrementing an `int` might
    involve multiple CPU instructions. This can be done in different ways on different
    CPUs, but let''s pretend that `++counter` generates the following made-up machine
    instructions:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.5*显示了两个线程要更新一个名为`counter`的整数。想象一下，这些线程都在使用指令`++counter`来增加一个全局计数器变量。事实证明，增加一个`int`可能涉及多个CPU指令。这可以在不同的CPU上以不同的方式完成，但假设`++counter`生成以下虚构的机器指令：'
- en: '**R**: Read counter from memory'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R**：从内存中读取counter'
- en: '**+1**: Increment counter'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**+1**：增加counter'
- en: '**W**: Write new counter value to memory'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W**：将新的counter值写入内存'
- en: Now, if we have two threads that are going to update the `counter` value that
    initially is 42, we would expect it to become 44 after both threads have run.
    However, as you can see in the following figure, there is no guarantee that the
    instructions will be executed sequentially to guarantee a correct increment of
    the `counter` variable.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们有两个线程要更新`counter`的值，初始值为42，我们期望在这两个线程运行后它变成44。然而，如下图所示，没有保证指令会按顺序执行以确保`counter`变量的正确增加。
- en: '![](img/B15619_11_05.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_05.png)'
- en: 'Figure 11.5: The two threads are both incrementing the same shared variable'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：两个线程都在增加相同的共享变量
- en: Without a data race, the counter would have reached the value 44, but instead,
    it only reaches 43.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 没有数据竞争，counter本应该达到值44，但实际上只达到了43。
- en: In this example, both threads read the value 42 and increment that value to
    43\. Then, they both write the new value, 43, which means that we never reach
    the correct answer of 44\. Had the first thread been able to write the value 43
    before the next thread started to read, we would have ended up with 44 instead.
    Note also that this would have been possible even if there was only one CPU core.
    The scheduler could have scheduled the two threads in a similar way so that both
    read instructions were executed before any writes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，两个线程都读取值42并将该值增加到43。然后，它们都写入新值43，这意味着我们永远不会达到正确的答案44。如果第一个线程能够在下一个线程开始读取之前写入值43，我们最终会得到44。还要注意，即使只有一个CPU核心，这也是可能的。调度程序可以以类似的方式安排这两个线程，以便在任何写入之前执行两个读取指令。
- en: Again, this is one possible scenario, but the important thing is that the behavior
    is undefined. Anything could happen when your program has a data race. One such
    example is **tearing**, which is the common term for **torn reads** and **torn
    writes**. This happens when a thread writes parts of a value to memory while another
    thread reads the value at the same time and therefore ends up with a corrupt value.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这只是一种可能的情况，但重要的是行为是未定义的。当程序存在数据竞争时，任何事情都可能发生。其中一个例子是**tearing**，这是**torn
    reads**和**torn writes**的常用术语。当一个线程在另一个线程同时读取值时向内存写入值的部分，因此最终得到一个损坏的值时，就会发生这种情况。
- en: Avoiding data races
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 避免数据竞争
- en: 'How can we avoid data races? There are two main options:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何避免数据竞争？有两个主要选项：
- en: Use an atomic data type instead of the `int`. This will tell the compiler to
    execute the read, increment, and write atomically. We will spend more time discussing
    atomic data types later in this chapter.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用原子数据类型而不是`int`。这将告诉编译器以原子方式执行读取、增加和写入。我们将在本章后面花更多时间讨论原子数据类型。
- en: Use a mutually exclusive lock (mutex) that guarantees that multiple threads
    never execute a critical section at the same time. A **critical section** is a
    place in the code that must not be executed simultaneously since it updates or
    reads shared memory that potentially could generate data races.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用互斥锁（mutex）来保证多个线程永远不会同时执行关键部分。**关键部分**是代码中不得同时执行的地方，因为它更新或读取可能会产生数据竞争的共享内存。
- en: It is also worth emphasizing that immutable data structures — data structures
    that are never changed — can be accessed by multiple threads without any risk
    of data races. Minimizing the use of mutable objects is good for many reasons,
    but it becomes even more important when writing concurrent programs. A common
    pattern is to always create new immutable objects instead of mutating existing
    objects. When the new object is fully constructed and represents the new state,
    it can be swapped with the old object. In that way, we can minimize the critical
    sections of our code. Only the swap is a critical section, and hence needs to
    be protected by an atomic operation or a mutex.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 值得强调的是，不可变数据结构——永远不会改变的数据结构——可以被多个线程访问而不会有任何数据竞争的风险。减少可变对象的使用有很多好处，但在编写并发程序时变得更加重要。一个常见的模式是总是创建新的不可变对象，而不是改变现有对象。当新对象完全构建并表示新状态时，它可以与旧对象交换。这样，我们可以最小化代码的关键部分。只有交换是一个关键部分，因此需要通过原子操作或互斥体来保护。
- en: Mutex
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 互斥体
- en: A **mutex**, short for **mutual exclusion lock**, is a synchronization primitive
    for avoiding data races. A thread that needs to enter a critical section first
    needs to lock the mutex (locking is sometimes also called acquiring a mutex lock).
    This means that no other thread can lock the same mutex until the first thread
    that holds the lock has unlocked the mutex. In that way, the mutex guarantees
    that only one thread at a time is inside a critical section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**互斥锁**，简称**互斥锁**，是用于避免数据竞争的同步原语。需要进入临界区的线程首先需要锁定互斥锁（有时锁定也称为获取互斥锁）。这意味着在持有锁的第一个线程解锁互斥锁之前，没有其他线程可以锁定相同的互斥锁。这样，互斥锁保证一次只有一个线程在临界区内部。'
- en: In *Figure 11.6*, you can see how the race condition demonstrated in the section
    *A data race example* can be avoided by using a mutex. The instruction labeled
    **L** is a lock instruction and the instruction labeled **U** is an unlock instruction.
    The first thread executing on Core 0 reaches the critical section first and locks
    the mutex before reading the value of the counter. It then adds 1 to the counter
    and writes it back to memory. After that, it releases the lock.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图11.6*中，您可以看到在*数据竞争示例*部分演示的竞争条件是如何通过使用互斥锁来避免的。标记为**L**的指令是锁定指令，标记为**U**的指令是解锁指令。在核心0上执行的第一个线程首先到达临界区并在读取计数器的值之前锁定互斥锁。然后，它将1添加到计数器并将其写回内存。之后，它释放锁。
- en: 'The second thread, executing on Core 1, reaches the critical section just after
    the first thread has acquired the mutex lock. Since the mutex is already locked,
    the thread is blocked until the first thread has updated the counter undisturbed
    and released the mutex:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个线程，在核心1上执行，在第一个线程获取互斥锁后立即到达临界区。由于互斥锁已经被锁定，线程被阻塞，直到第一个线程无干扰地更新计数器并释放互斥锁：
- en: '![](img/B15619_11_06.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_06.png)'
- en: 'Figure 11.6: The mutex lock is protecting the critical section and avoids data
    races on the counter variable'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：互斥锁保护临界区，避免计数器变量的数据竞争
- en: The net result is that the two threads can update the mutable shared variable
    in a safe and correct way. However, it also means that the two threads can no
    longer be run in parallel. If most of the work a thread does cannot be done without
    serializing the work, there is, from a performance perspective, no point in using
    threads.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，两个线程可以以安全和正确的方式更新可变的共享变量。然而，这也意味着这两个线程不能再并行运行。如果一个线程大部分工作都不能在不串行化的情况下完成，从性能的角度来看，使用线程就没有意义了。
- en: The state where the second thread is blocked by the first thread is called **contention**.
    This is something we strive to minimize, because it hurts the scalability of a
    concurrent program. Adding more CPU cores will not improve performance if the
    degree of contention is high.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个线程被第一个线程阻塞的状态称为**争用**。这是我们努力最小化的东西，因为它会影响并发程序的可伸缩性。如果争用程度很高，增加CPU核心数量将不会提高性能。
- en: Deadlock
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 死锁
- en: When using mutex locks to protect shared resources, there is a risk of getting
    stuck in a state called **deadlock**. A deadlock can happen when two threads are
    waiting for each other to release their locks. Neither of the threads can proceed
    and they are stuck in a deadlock state. One condition that needs to be fulfilled
    for a deadlock to occur is that one thread that already holds a lock tries to
    acquire an additional lock. When a system grows and gets larger, it becomes more
    and more difficult to track all locks that might be used by all threads running
    in a system. This is one reason for always trying to minimize the use of shared
    resources, and this demonstrates the need for exclusive locking.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用互斥锁保护共享资源时，存在陷入**死锁**状态的风险。当两个线程互相等待对方释放锁时，就会发生死锁。两个线程都无法继续进行，它们陷入了死锁状态。死锁发生的一个条件是，已经持有一个锁的线程尝试获取另一个锁。当系统增长并变得更大时，跟踪系统中所有线程可能使用的所有锁变得越来越困难。这是始终努力最小化使用共享资源的一个原因，也说明了对独占锁的需求。
- en: '*Figure 11.7* shows two threads in a waiting state, trying to acquire the lock
    held by the other thread:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.7*显示了两个线程处于等待状态，试图获取另一个线程持有的锁：'
- en: '![](img/B15619_11_07.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_07.png)'
- en: 'Figure 11.7: An example of a deadlock state'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：死锁状态的示例
- en: Let's discuss synchronous and asynchronous tasks next.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们讨论同步和异步任务。
- en: Synchronous and asynchronous tasks
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同步和异步任务
- en: I will refer to **synchronous tasks** and **asynchronous tasks** in this chapter.
    Synchronous tasks are like ordinary C++ functions. When a synchronous task is
    finished doing whatever it is supposed to do, it will return the control to the
    caller of the task. The caller of the task is waiting or blocked until the synchronous
    task has finished.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将提到**同步任务**和**异步任务**。同步任务就像普通的C++函数。当同步任务完成其任务后，它将控制权返回给任务的调用者。任务的调用者在等待或被阻塞，直到同步任务完成。
- en: An asynchronous task, on the other hand, will return the control back to the
    caller immediately and instead perform its work concurrently.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，异步任务将立即将控制权返回给调用者，并同时执行其工作。
- en: 'The sequence in *Figure 11.8* shows the difference between calling a synchronous
    and asynchronous task, respectively:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.8*中的序列显示了分别调用同步和异步任务之间的区别：'
- en: '![](img/B15619_11_08.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_08.png)'
- en: 'Figure 11.8: Synchronous versus asynchronous calls. The asynchronous task returns
    immediately but continues to work after the caller has regained control.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8：同步与异步调用。异步任务立即返回，但在调用者重新获得控制权后继续工作。
- en: If you haven't seen asynchronous tasks before, they might look strange at first,
    since ordinary functions in C++ always stop executing when they encounter a return
    statement or reach the end of the function body. Asynchronous APIs are getting
    more and more common, though, and it is likely that you have encountered them
    before, for example, when working with asynchronous JavaScript.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您以前没有见过异步任务，它们可能一开始看起来很奇怪，因为在C++中，普通函数遇到返回语句或到达函数体末尾时总是停止执行。然而，异步API变得越来越常见，很可能您以前已经遇到过，例如在使用异步JavaScript时。
- en: Sometimes, we use the term **blocking** for operations that block the caller;
    that is, make the caller wait until the operation has finished.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们使用术语**阻塞**来表示阻塞调用者的操作；也就是说，使调用者等待操作完成。
- en: With a general introduction to concurrency behind us, it's time to explore the
    support for threaded programming in C++.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在对并发性进行了一般介绍之后，现在是时候探索C++中的线程编程支持了。
- en: Concurrent programming in C++
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: C++中的并发编程
- en: The concurrency support in C++ makes it possible for a program to execute multiple
    tasks concurrently. As mentioned earlier, writing a correct concurrent C++ program
    is, in general, a lot harder than writing a program that executes all tasks sequentially
    in one thread. This section will also demonstrate some common pitfalls to make
    you aware of all the difficulties involved in writing concurrent programs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: C++中的并发支持使程序能够同时执行多个任务。正如前面提到的，编写正确的并发C++程序通常比在一个线程中依次执行所有任务的程序要困难得多。本节还将演示一些常见的陷阱，以使您了解编写并发程序所涉及的所有困难。
- en: Concurrency support was first introduced in C++11 and has since been extended
    into C++14, C++17, and C++20\. Before concurrency was part of the language, it
    was implemented with native concurrency support from the operating system, **POSIX
    Threads** (**pthreads**), or some other library.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 并发支持首次出现在C++11中，并在C++14、C++17和C++20中得到扩展。在并发成为语言的一部分之前，它是通过操作系统的本机并发支持、**POSIX线程**（**pthreads**）或其他一些库来实现的。
- en: With concurrency support directly in the C++ language, we can write cross-platform
    concurrent programs, which is great! Sometimes, however, you have to reach for
    platform-specific functionality when dealing with concurrency on your platform.
    For example, there is no support in the C++ standard library for setting thread
    priorities, configuring CPU affinity (CPU pinning), or setting the stack size
    of new threads.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有了C++语言中的并发支持，我们可以编写跨平台的并发程序，这很棒！然而，当处理平台上的并发时，有时必须使用特定于平台的功能。例如，在C++标准库中没有支持设置线程优先级、配置CPU亲和性（CPU绑定）或设置新线程的堆栈大小。
- en: It should also be said that the thread support library has been extended quite
    a bit with the release of C++20, and more features are likely to be added in future
    versions of the language. The need for good concurrency support is increasing
    because of the way hardware is being developed, and there is a lot yet to be discovered
    when it comes to the efficiency, scalability, and correctness of highly concurrent
    programs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 还应该说一下，随着C++20的发布，线程支持库得到了相当大的扩展，未来版本的语言很可能会添加更多功能。由于硬件的发展方式，对良好的并发支持的需求正在增加，而在高度并发程序的效率、可伸缩性和正确性方面还有很多待发现的地方。
- en: The thread support library
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程支持库
- en: We will now take a tour through the C++ thread support library and cover its most important
    components.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将通过C++线程支持库进行一次介绍，并涵盖其最重要的组件。
- en: Threads
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线程
- en: 'A running program contains at least one thread. When your main function is
    called, it is executed on a thread usually referred to as the **main thread**.
    Each thread has an identifier, which can be useful when debugging a concurrent
    program. The following program prints the thread identifier of the main thread:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 运行中的程序至少包含一个线程。当调用主函数时，它会在通常被称为**主线程**的线程上执行。每个线程都有一个标识符，在调试并发程序时可能会有用。以下程序打印主线程的线程标识符：
- en: '[PRE0]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Running the preceding program might produce something like this:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述程序可能会产生类似以下的输出：
- en: '[PRE1]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It is possible to make a thread sleep. Sleep is rarely used in production code
    but can be very useful during debugging. For example, if you have a data race
    that only occurs under rare circumstances, adding sleep to your code might make
    it appear more often. This is how to make the currently running thread sleep for
    a second:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 线程可以休眠。在生产代码中很少使用休眠，但在调试过程中可能非常有用。例如，如果您有一个只在罕见情况下发生的数据竞争，向代码中添加休眠可能会使其更频繁地出现。以下是使当前运行的线程休眠一秒钟的方法：
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Your program should never expose any data races after inserting random sleeps
    in your code. Your program may not work satisfactorily after adding sleeps; buffers
    may become full, the UI may lag, and so on, but it should always behave in a predictable
    and defined way. We don't have control over the scheduling of the threads, and
    random sleeps simulate unlikely but possible scheduling scenarios.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的程序中插入随机休眠后，程序不应该暴露任何数据竞争。在添加休眠后，您的程序可能无法正常工作；缓冲区可能变满，UI可能会出现延迟等，但它应该始终以可预测和定义的方式行为。我们无法控制线程的调度，随机休眠模拟了不太可能但可能发生的调度场景。
- en: 'Now, let''s create an additional thread using the `std::thread` class from
    the `<thread>` header. It represents a single thread of execution and is usually
    a wrapper around an operating system thread. The `print()` function will be invoked
    from a thread created by us explicitly:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`<thread>`头文件中的`std::thread`类创建一个额外的线程。它表示一个执行线程，并且通常是操作系统线程的包装器。`print()`函数将从我们显式创建的线程中调用：
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When creating the thread, we pass in a callable object (a function, lambda,
    or a function object) that the thread will begin to execute whenever it gets scheduled
    time on the CPU. I have added a call to sleep to make it obvious why we need to
    call `join()` on the thread. When a `std::thread` object is destructed, it must
    have been *joined* or *detached* or it will cause the program to call `std::terminate()`,
    which by default will call `std::abort()` if we haven't installed a custom `std::terminate_handler`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建线程时，我们传递一个可调用对象（函数、lambda或函数对象），线程将在CPU上获得调度时间时开始执行。我添加了一个调用sleep，以明显地说明为什么我们需要在线程上调用`join()`。当`std::thread`对象被销毁时，它必须已经*加入*或*分离*，否则将导致程序调用`std::terminate()`，默认情况下将调用`std::abort()`，如果我们没有安装自定义的`std::terminate_handler`。
- en: 'In the preceding example, the `join()` function is blocking — it waits until
    the thread has finished running. So, in the preceding example, the `main()` function
    will not return until thread `t1` has finished running. Consider the following
    line:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，`join()`函数是阻塞的——它会等待线程运行结束。因此，在前面的例子中，`main()`函数将在线程`t1`运行结束之前不会返回。考虑以下一行：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Suppose we detach the thread `t1` by replacing the preceding line with the
    following line:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们通过以下一行替换前面的行来分离线程`t1`：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In such a case, our main function will end before thread `t1` wakes up to print
    the message, and, as a result, the program will (most likely) only output the
    thread ID of the main thread. Remember, we have no control of the scheduling of
    the threads and it is possible, but very unlikely, that the main thread will output
    its message *after* the `print()` function has had time to sleep, wake up, and
    print its thread ID.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的主函数将在线程`t1`唤醒打印消息之前结束，因此程序将（很可能）只输出主线程的线程ID。请记住，我们无法控制线程的调度，可能但非常不太可能，主线程将在`print()`函数有时间休眠、唤醒并打印其线程ID之后输出其消息。
- en: 'Using `detach()` instead of `join()` in this example also introduces another
    problem. We are using `std::cout` from both threads without any synchronization,
    and since `main()` is no longer waiting for thread `t1` to finish, they both could
    theoretically use `std::cout` in parallel. Fortunately, `std::cout` is thread-safe
    and can be used from multiple threads without introducing data races, so there
    is no undefined behavior. However, it is still possible that the output generated
    by the threads is interleaved, resulting in something like the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，使用`detach()`而不是`join()`也引入了另一个问题。我们在两个线程中都使用了`std::cout`，而没有任何同步，而且由于`main()`不再等待线程`t1`完成，它们两者理论上都可以并行使用`std::cout`。幸运的是，`std::cout`是线程安全的，可以从多个线程中使用而不会引入数据竞争，因此没有未定义的行为。但是，仍然有可能线程生成的输出是交错的，导致类似以下的结果：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If we want to avoid the interleaved output, we need to treat the outputting
    of characters as a critical section and synchronize access to `std::cout`. We
    will talk more about critical sections and race conditions in a while, but first,
    let's cover some details about `std::thread`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想避免交错输出，我们需要将字符的输出视为临界区，并同步访问`std::cout`。我们将在稍后更多地讨论临界区和竞争条件，但首先，让我们先了解一些关于`std::thread`的细节。
- en: Thread states
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线程状态
- en: Before we go any further, you should have a good understanding of what a `std::thread`
    object really represents and in what states it can be. We haven't yet talked about
    what sort of threads there normally are in a system executing a C++ program.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，您应该对`std::thread`对象的真正表示以及它可能处于的状态有一个很好的理解。我们还没有讨论在执行C++程序的系统中通常有哪些类型的线程。
- en: In the following figure, you can see a snapshot of a hypothetical running system.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，您可以看到一个假设运行中系统的快照。
- en: '![](img/B15619_11_09.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_09.png)'
- en: 'Figure 11.9: Snapshot of a hypothetical running system'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9：假设运行中系统的快照
- en: 'Starting from the bottom, the figure shows the CPU and its **hardware threads**.
    Those are the execution units on the CPU. In this example, the CPU provides four
    hardware threads. Usually that means it has four cores, but it could be some other
    configuration; for example, some cores can execute two hardware threads. This
    is usually called **hyperthreading**. The total number of hardware threads can
    be printed at runtime with this:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从底部开始，图中显示了CPU及其**硬件线程**。这些是CPU上的执行单元。在这个例子中，CPU提供了四个硬件线程。通常这意味着它有四个核心，但也可能是其他配置；例如，一些核心可以执行两个硬件线程。这通常被称为**超线程**。硬件线程的总数可以在运行时打印出来：
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding code might also output `0` if the number of hardware threads cannot
    be determined on the running platform.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行平台上无法确定硬件线程的数量时，前面的代码也可能输出`0`。
- en: The layer above the hardware threads contains the **operating system threads**.
    These are the actual software threads. The operating system scheduler determines
    when and for how long an operating system thread is executed by a hardware thread.
    In *Figure 11.9*, there are currently three out of six software threads executing.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件线程上面的一层包含了**操作系统线程**。这些是实际的软件线程。操作系统调度程序确定操作系统线程由硬件线程执行的时间和持续时间。在*图11.9*中，目前有六个软件线程中的三个正在执行。
- en: 'The topmost layer in the figure contains the `std::thread` objects. A `std::thread`
    object is nothing more than an ordinary C++ object that may or may not be associated
    with an underlying operating system thread. Two instances of `std::thread` cannot
    be associated with the same underlying thread. In the figure, you can see that
    the program currently has three instances of `std::thread`; two are associated
    with threads and one is not. It''s possible to use the `std::thread::joinable`
    property to find out what state a `std::thread` object is in. A thread is *not*
    joinable if it has been:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图中最上层包含了`std::thread`对象。`std::thread`对象只是一个普通的C++对象，可能与底层操作系统线程相关联，也可能不相关联。两个`std::thread`实例不能与同一个底层线程相关联。在图中，您可以看到程序当前有三个`std::thread`实例；两个与线程相关联，一个没有。可以使用`std::thread::joinable`属性来查找`std::thread`对象的状态。如果它已经：
- en: Default constructed; that is, if it has nothing to execute
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认构造；也就是说，如果它没有任何要执行的内容
- en: Moved from (its associated running thread has been transferred to another `std::thread`
    object)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从中移动（其关联的运行线程已被转移到另一个`std::thread`对象）
- en: Detached by a call to `detach()`
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用`detach()`分离
- en: Already joined by a call to `join()`
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用`join()`已连接
- en: Otherwise, the `std::thread` object is in the joinable state. Remember, when
    a `std::thread` object is destructed, it must no longer be in the joinable state
    or the program will terminate.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，`std::thread`对象处于可连接状态。请记住，当`std::thread`对象被销毁时，它不能再处于可连接状态，否则程序将终止。
- en: Joinable thread
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可连接的线程
- en: 'C++20 introduced a new thread class named `std::jthread`. It is very similar
    to `std::thread`, but with a couple of important additions:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: C++20引入了一个名为`std::jthread`的新线程类。它与`std::thread`非常相似，但有一些重要的补充：
- en: '`std::jthread` has support for stopping a thread using a stop token. This is
    something that we had to implement manually before C++20 when using `std::thread`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`std::jthread`支持使用停止令牌停止线程。在C++20之前，使用`std::thread`时，我们必须手动实现这一点。'
- en: Instead of terminating the app when it is being destructed in a non-joinable
    state, the destructor of `std::jthread` will send a stop request and join the
    thread on destruction.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在非可连接状态下销毁应用程序时，`std::jthread`的析构函数将发送一个停止请求并在销毁时加入线程。
- en: 'I will illustrate the latter point next. First, we will use the `print()` function,
    which is defined like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我将说明后一点。首先，我们将使用如下定义的`print()`函数：
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It sleeps for a second, and then prints the current thread identifier:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 它休眠一秒，然后打印当前线程标识符：
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following output was produced when running the code on my machine:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的机器上运行代码时，产生了以下输出：
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let''s change our `print()` function so that it output messages continuously
    in a loop. We then need some way to communicate to the `print()` function when
    to stop. The `std::jthread` (as opposed to `std::thread`) has built-in support
    for this by using a stop token. When `std::jthread` invokes the `print()` function,
    it can pass an instance of a `std::stop_token` if the `print()` function accepts
    such an argument. Here is an example of how we could implement this new `print()`
    function using a stop token:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们改变我们的`print()`函数，使其在循环中连续输出消息。然后我们需要一些方法来通知`print()`函数何时停止。`std::jthread`（而不是`std::thread`）通过使用停止令牌内置支持这一点。当`std::jthread`调用`print()`函数时，如果`print()`函数接受这样的参数，它可以传递一个`std::stop_token`的实例。以下是我们如何使用停止令牌来实现这个新的`print()`函数的示例：
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `while`-loop checks at each iteration whether the function has been requested
    to stop by calling `stop_requested()`. From our `main()` function, it''s now possible
    to request a stop by calling `request_stop()` on our `std::jthread` instance:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`while`循环在每次迭代时检查函数是否已被调用`stop_requested()`请求停止。现在，从我们的`main()`函数中，可以通过在我们的`std::jthread`实例上调用`request_stop()`来请求停止：'
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When I run this program, it generates the following output:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行这个程序时，它生成了以下输出：
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this example, we could have omitted the explicit call to `request_stop()`
    because `jthread` will call `request_stop()` automatically on destruction.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们本可以省略对`request_stop()`的显式调用，因为`jthread`在销毁时会自动调用`request_stop()`。
- en: The new `jthread` class is a welcome addition to the C++ thread library and
    it should be the first choice when reaching for a thread class in C++.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 新的`jthread`类是C++线程库中的一个受欢迎的补充，当在C++中寻找线程类时，它应该是第一选择。
- en: Protecting critical sections
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保护关键部分
- en: As I already mentioned, our code must not contain any data races. Unfortunately,
    writing code with data races is very easy. Finding the critical sections and protecting
    them with locks is something we constantly need to think about when writing concurrent
    programs in this style using threads.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，我们的代码不能包含任何数据竞争。不幸的是，编写带有数据竞争的代码非常容易。在使用线程编写并发程序时，找到关键部分并用锁保护它们是我们不断需要考虑的事情。
- en: C++ provides us with a `std::mutex` class that can be used for protecting critical
    sections and avoiding data races. I will demonstrate how to use a mutex with a
    classic example using a shared mutable counter variable updated by multiple threads.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: C++为我们提供了一个`std::mutex`类，可以用于保护关键部分并避免数据竞争。我将演示如何使用互斥锁来处理一个经典的例子，其中多个线程更新了一个共享的可变计数器变量。
- en: 'First, we define a global mutable variable and the function incrementing the
    counter:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个全局可变变量和一个增加计数器的函数：
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `main()` function that follows creates two threads that will both execute
    the `increment_counter()` function. Note also in this example how we can pass
    arguments to the function invoked by the thread. We can pass an arbitrary number
    of arguments to the thread constructor in order to match the parameters in the
    signature of the function to be called. Finally, we assert that the counter has
    the value we would expect it to have if the program was free from race conditions:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的`main()`函数创建了两个线程，它们都将执行`increment_counter()`函数。在这个例子中还可以看到如何向线程调用的函数传递参数。我们可以向线程构造函数传递任意数量的参数，以匹配要调用的函数签名中的参数。最后，我们断言如果程序没有数据竞争，计数器的值将符合我们的预期：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This program will most likely fail. The `assert()` function doesn't hold since
    the program currently contains a race condition. When I repeatedly run the program,
    I end up with different values of the counter. Instead of reaching the value `200000000`,
    I once ended up with no more than `137182234`. This example is very similar to
    the data race example that was illustrated earlier in this chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序很可能会失败。`assert()`函数不起作用，因为程序当前包含竞争条件。当我反复运行程序时，计数器的值会不同。我最终得到的不是达到值`200000000`，而是最多只有`137182234`。这个例子与本章前面所举的数据竞争例子非常相似。
- en: The line with the expression `++counter` is a critical section — it uses a shared
    mutable variable and is executed by multiple threads. In order to protect the
    critical section, we will now use the `std::mutex` included in the `<mutex>` header.
    Later on, you will see how we can avoid data races in this example by using atomics,
    but, for now, we will use a lock.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 带有表达式`++counter`的那一行是一个关键部分——它使用了一个共享的可变变量，并由多个线程执行。为了保护这个关键部分，我们现在将使用`<mutex>`头文件中包含的`std::mutex`。稍后，您将看到我们如何通过使用原子操作来避免这个例子中的数据竞争，但现在我们将使用锁。
- en: 'First, we add the global `std::mutex` object next to the `counter`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在`counter`旁边添加全局`std::mutex`对象：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: But isn't the `std::mutex` object itself a mutable shared variable that can
    generate data races if used by multiple threads? Yes, it is a mutable shared variable,
    but no, it will not generate data races. The synchronization primitives from the
    C++ thread library, such as `std::mutex`, are designed for this particular purpose.
    In that respect, they are very special and use hardware instructions, or whatever
    is necessary on our platform, to guarantee that they don't generate data races
    themselves.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，`std::mutex`对象本身不是一个可变的共享变量吗？如果被多个线程使用，它不会产生数据竞争吗？是的，它是一个可变的共享变量，但不会产生数据竞争。C++线程库中的同步原语，如`std::mutex`，是为了这个特定目的而设计的。在这方面，它们非常特殊，并使用硬件指令或者平台上必要的任何东西来保证它们自己不会产生数据竞争。
- en: 'Now we need to use the mutex in our critical section that reads and updates
    the counter variable. We could use the `lock()` and `unlock()` member functions
    on the `counter_mutex`, but the preferred and safer method is to always use RAII
    for handling the mutex. Think of the mutex as a resource that always needs to
    be unlocked when we have finished using it. The thread library provides us with
    some useful RAII class templates for handling locking. Here, we will use the `std::scoped_lock<Mutex>`
    template to ensure that we release the mutex safely. Below is the updated `increment_counter()`
    function, which is now protected with a mutex lock:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要在读取和更新计数器变量的关键部分使用互斥锁。我们可以在`counter_mutex`上使用`lock()`和`unlock()`成员函数，但更倾向于更安全的方法是始终使用RAII来处理互斥锁。把互斥锁看作一个资源，当我们使用完毕时总是需要解锁。线程库为我们提供了一些有用的RAII类模板来处理锁定。在这里，我们将使用`std::scoped_lock<Mutex>`模板来确保我们安全地释放互斥锁。下面是更新后的`increment_counter()`函数，现在受到互斥锁的保护：
- en: '[PRE17]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The program is now free from data races and works as expected. If we run it
    again, the condition in the `assert()` function will now hold true.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在摆脱了数据竞争，并且按预期工作。如果我们再次运行它，`assert()`函数中的条件现在将成立。
- en: Avoiding deadlocks
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 避免死锁
- en: As long as a thread never acquires more than one lock at a time, there is no
    risk of deadlocks. Sometimes, though, it is necessary to acquire another lock
    while already holding onto a previously acquired lock. The risk of deadlocks in
    these situations can be avoided by grabbing both locks at the exact same time.
    C++ has a way to do this by using the `std::lock()` function, which takes an arbitrary
    number of locks and blocks until all locks have been acquired.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 只要一个线程一次只获取一个锁，就不会有死锁的风险。然而，有时需要在已经持有先前获取的锁的情况下获取另一个锁。在这种情况下，通过同时抓住两个锁来避免死锁的风险。C++有一种方法可以通过使用`std::lock()`函数来做到这一点，该函数获取任意数量的锁，并在所有锁都被获取之前阻塞。
- en: 'The following is an example of transferring money between accounts. Both accounts
    need to be protected during the transaction, and therefore we need to acquire
    two locks at the same time. Here is how it works:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个在账户之间转账的示例。在交易期间需要保护两个账户，因此我们需要同时获取两个锁。操作如下：
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We again use a RAII class template to ensure that we release the lock whenever
    this function returns. In this case, we use `std::unique_lock`, which provides
    us with the possibility to defer the locking of the mutex. Then, we explicitly
    lock both mutexes at the same time by using the `std::lock()` function.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用RAII类模板来确保每当这个函数返回时我们都释放锁。在这种情况下，我们使用`std::unique_lock`，它为我们提供了推迟锁定互斥锁的可能性。然后，我们通过使用`std::lock()`函数同时显式锁定两个互斥锁。
- en: Condition variables
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条件变量
- en: A **condition variable** makes it possible for threads to wait until some specific
    condition has been met. Threads can also use a condition variable to signal to
    other threads that the condition has changed.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**条件变量**使线程能够等待直到某个特定条件得到满足。线程还可以使用条件变量向其他线程发出条件已经改变的信号。'
- en: A common pattern in a concurrent program is to have one or many threads that
    are waiting for data to be consumed somehow. These threads are usually called
    **consumers**. Another group of threads is then responsible for producing data
    that is ready to be consumed. These threads producing data are called **producers**,
    or a **producer** if it is only one thread.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 并发程序中的一个常见模式是有一个或多个线程在等待数据以某种方式被消耗。这些线程通常被称为**消费者**。另一组线程负责生成准备好被消耗的数据。这些生成数据的线程被称为**生产者**，如果只有一个线程，则称为**生产者**。
- en: 'The producer and consumer pattern can be implemented using a condition variable.
    We can use a combination of `std::condition_variable` and `std::unique_lock` for
    this purpose. Let''s have a look at an example of a producer and consumer to make
    them less abstract:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者和消费者模式可以使用条件变量来实现。我们可以结合使用`std::condition_variable`和`std::unique_lock`来实现这个目的。让我们看一个生产者和消费者的示例，使它们不那么抽象：
- en: '[PRE19]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We are creating two threads: one `consumer` thread and one `producer` thread.
    The `producer` thread generates a sequence of integers and pushes them to a global
    `std::queue<int>` once every second. Whenever an element is added to the queue,
    the producer signals that the condition has changed using `notify_one()`.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了两个线程：一个`consumer`线程和一个`producer`线程。`producer`线程生成一系列整数，并在每秒钟将它们推送到全局`std::queue<int>`中。每当向队列添加元素时，生产者都会使用`notify_one()`来发出条件已经改变的信号。
- en: The program checks whether there is data in the queue that is available for
    consumption by the consumer thread. Note also that it is not required to hold
    the lock while notifying the condition variable.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 程序检查队列中是否有数据可供消费者线程使用。还要注意的是，在通知条件变量时不需要持有锁。
- en: The consumer thread is responsible for printing the data (that is, the integers)
    to the console. It uses the condition variable to wait for the empty queue to
    change. When the consumer calls `cv.wait(lock)`, the thread goes to sleep and
    leaves the CPU for other threads to execute. It is important to understand why
    we need to pass the variable `lock` when calling `wait()`. Apart from putting
    the thread to sleep, `wait()` also unlocks the mutex while sleeping and then acquires
    the mutex before it returns. If `wait()` didn't release the mutex, the producer
    would not be able to add elements to the queue.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者线程负责将数据（即整数）打印到控制台。它使用条件变量等待空队列发生变化。当消费者调用`cv.wait(lock)`时，线程会进入睡眠状态，让出CPU给其他线程执行。重要的是要理解为什么在调用`wait()`时需要传递变量`lock`。除了让线程进入睡眠状态，`wait()`在睡眠时也会释放互斥锁，然后在返回之前重新获取互斥锁。如果`wait()`没有释放互斥锁，生产者将无法向队列中添加元素。
- en: Why is the consumer waiting on the condition variable with a `while`-loop around
    it and not an `if` statement? This is a common pattern, and sometimes we need
    to do that since there might be other consumers that were also woken up and emptied
    the queue before us. In our program, we only have one consumer thread, though,
    so that cannot happen. However, it is possible for the consumer to be awoken from
    its wait even though the producer thread did not signal. This phenomenon is called
    **spurious wakeup**, and the reasons that this can happen are beyond the scope
    of this book.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么消费者在条件变量上等待时使用`while`循环而不是`if`语句？这是一个常见的模式，有时我们需要这样做，因为可能有其他消费者在我们之前被唤醒并清空了队列。在我们的程序中，我们只有一个消费者线程，所以这种情况不会发生。但是，消费者可能会在等待时被唤醒，即使生产者线程没有发出信号。这种现象称为**虚假唤醒**，导致这种情况发生的原因超出了本书的范围。
- en: 'As an alternative to using a `while`-loop, we can use an overloaded version
    of `wait()` that accepts a predicate. This version of `wait()` check if the predicate
    is satisfied and will do the looping for us. In our example it would look like
    this:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 作为使用`while`循环的替代方案，我们可以使用`wait()`的重载版本，该版本接受一个谓词。这个`wait()`版本检查谓词是否满足，并为我们执行循环。在我们的示例中，它看起来像这样：
- en: '[PRE20]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You can find more information about spurious wakeups in *C++ Concurrency in
    Action*, *Second Edition*, by Anthony Williams. You now at least know how to handle
    situations where spurious wakeups can happen: always check the condition in a
    while loop or use the overloaded version of `wait()` that accepts a predicate*.*'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Anthony Williams的*C++ Concurrency in Action*，*Second Edition*中找到有关虚假唤醒的更多信息。您现在至少知道如何处理可能发生虚假唤醒的情况：始终在while循环中检查条件，或者使用接受谓词的`wait()`的重载版本。
- en: Condition variables and mutexes are synchronization primitives that have been
    available in C++ since the introduction of threads in C++. C++20 comes with additional
    useful class templates for synchronizing threads, namely `std::counting_semaphore`,
    `std::barrier`, and `std::latch`. We will cover these new primitives later on.
    First we are going to spend some time on return values and error handling.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 条件变量和互斥锁是自从C++引入线程以来就可用的同步原语。C++20还提供了额外的有用的类模板，用于同步线程，即`std::counting_semaphore`、`std::barrier`和`std::latch`。我们将在后面介绍这些新的原语。首先，我们将花一些时间讨论返回值和错误处理。
- en: Returning data and handling errors
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 返回数据和处理错误
- en: The examples presented so far in this chapter have used shared variables to
    communicate state between threads. We have used mutex locks to ensure that we
    avoid data races. Using shared data with mutexes, as we have been doing, can be
    very hard to do correctly when the size of a program increases. There is also
    a lot of work in maintaining code that uses explicit locking spread out over a
    code base. Keeping track of shared memory and explicit locking moves us further
    away from what we really want to accomplish and spend time on when writing a program.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中所呈现的示例都使用了共享变量来在线程之间通信状态。我们使用互斥锁来确保避免数据竞争。在程序规模增大时，使用互斥锁的共享数据可能会非常难以正确实现。在代码库中分散使用显式锁定也需要大量工作。跟踪共享内存和显式锁定使我们远离我们编写程序时真正想要实现和花时间的目标。
- en: In addition, we haven't dealt with error handling at all yet. What if a thread
    needs to report an error to some other thread? How do we do that using exceptions,
    as we are used to doing when a function needs to report a runtime error?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还没有处理错误处理。如果一个线程需要向另一个线程报告错误怎么办？当函数需要报告运行时错误时，我们通常使用异常，那么我们如何使用异常来做到这一点呢？
- en: In the standard library `<future>` header, we can find some class templates
    that help us with writing concurrent code without global variables and locks,
    and, in addition, can communicate exceptions between threads for handling errors.
    I will now present **futures** and **promises**, which represent two sides of
    a value. The future is the receiving side of the value and the promise is the
    returning side of the value.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准库的`<future>`头文件中，我们可以找到一些类模板，可以帮助我们编写并发代码，而无需全局变量和锁，并且可以在线程之间传递异常以处理错误。我现在将介绍**future**和**promise**，它们代表值的两个方面。future是值的接收方，promise是值的返回方。
- en: 'The following is an example of using `std::promise` to return the result to
    the caller:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用`std::promise`将结果返回给调用者的示例：
- en: '[PRE21]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The caller (the `main()` function) creates the `std::promise` object and passes
    it to the `divide()` function. We need to use `std::ref` from `<functional>` so
    that a reference can be correctly forwarded through the `std::thread` to `compute()`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 调用者（`main()`函数）创建`std::promise`对象并将其传递给`divide()`函数。我们需要使用`<functional>`中的`std::ref`，以便引用可以通过`std::thread`正确地转发到`compute()`。
- en: When the `divide()` function has computed the result, it passes the return value
    through the promise by calling the `set_value()` function. If an error occurs
    in the `divide()` function, it calls the `set_exception()` function on the promise
    instead.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当`divide()`函数计算出结果时，通过调用`set_value()`函数通过promise传递返回值。如果`divide()`函数发生错误，则在promise上调用`set_exception()`函数。
- en: The future represents the value of the computation that may or may not be computed
    yet. Since the future is an ordinary object, we can, for example, pass it around
    to other objects that need the computed value. Finally, when the value is needed
    by some client, it calls `get()` to get hold of the actual value. If it is not
    computed at that point in time, the call to `get()` will block until it is finished.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: future代表可能已经计算或尚未计算的计算值。由于future是一个普通对象，我们可以将其传递给需要计算值的其他对象。最后，当某个客户端需要该值时，它调用`get()`来获取实际值。如果在那时没有计算，调用`get()`将阻塞，直到完成。
- en: Note also how we managed to pass data back and forth with proper error handling,
    without using any shared global data and with no explicit locking. The promise
    takes care of that for us, and we can focus on implementing the essential logic
    of the program instead.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意的是，我们成功地进行了适当的错误处理来回传递数据，而没有使用任何共享全局数据，并且没有显式锁定。promise为我们处理了这一切，我们可以专注于实现程序的基本逻辑。
- en: Tasks
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务
- en: With futures and promises, we managed to get away from explicit locks and shared
    global data. Our code will benefit from using higher-level abstractions when possible,
    especially when the code base grows. Here, we will go further and explore classes
    that automatically set up the futures and promises for us. You will also see how
    we can get rid of the manual administration of threads and leave that to the library.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过future和promise，我们成功摆脱了显式锁定和共享全局数据。在可能的情况下，我们的代码将受益于使用更高级的抽象。在这里，我们将进一步探索自动为我们设置未来和承诺的类。您还将看到我们如何摆脱手动管理线程，并将其留给库。
- en: In many cases, we don't have any need for managing threads; instead, what we
    really need is to be able to execute a **task** asynchronously and have that task
    execute on its own concurrently with the rest of the program, and then eventually
    get the result or error communicated to the parts of the program that need it.
    The task should be carried out in isolation to minimize contention and the risk
    of data races.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们并不需要管理线程；相反，我们真正需要的是能够异步执行任务，并使该任务与程序的其余部分同时执行，然后最终将结果或错误传达给需要它的程序部分。任务应该在隔离环境中执行，以最小化争用和数据竞争的风险。
- en: 'We will begin by rewriting our previous example that divided two numbers. This
    time, we will use the `std::packaged_task` from `<future>`, which makes all the
    work of setting up the promise correct for us:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从重写我们之前的例子开始，该例子将两个数字相除。这一次，我们将使用`<future>`中的`std::packaged_task`，它为我们设置promise的所有工作都是正确的：
- en: '[PRE22]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`std::packaged_task` is itself a callable object that can be moved to the `std::thread`
    object we are creating. As you can see, `std::packaged_task` now does most of
    the work for us: we don''t have to create the promise ourselves. But, more importantly,
    we can write our `divide()` function just like a normal function, without the
    need for explicitly returning values or exceptions through the promise; the `std::packaged_task`
    will do that for us.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::packaged_task`本身是一个可调用对象，可以移动到我们正在创建的`std::thread`对象中。正如你所看到的，`std::packaged_task`现在为我们做了大部分工作：我们不必自己创建promise。但更重要的是，我们可以像编写普通函数一样编写我们的`divide()`函数，而不需要通过promise显式返回值或异常；`std::packaged_task`会为我们做这些。'
- en: 'As a last step in this section, we would also like to get rid of the manual
    thread management. Creating threads is not free, and you will see later on that
    the number of threads in a program can affect performance. It seems like the question
    of whether we should create a new thread for our `divide()` function is not necessarily
    up to the caller of `divide()`. The library again helps us here by providing another
    useful function template called `std::async()`. The only thing we need to do in
    our `divide()` example is replace the code creating the `std::packaged_task` and
    the `std::thread` object with a simple call to `std::async()`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的最后一步，我们还希望摆脱手动线程管理。创建线程并不是免费的，您将在后面看到，程序中的线程数量会影响性能。似乎是否为我们的`divide()`函数创建一个新线程并不一定由`divide()`的调用者决定。库再次通过提供另一个有用的函数模板`std::async()`来帮助我们。在我们的`divide()`示例中，我们唯一需要做的是用一个简单的调用`std::async()`替换创建`std::packaged_task`和`std::thread`对象的代码：
- en: '[PRE23]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We have now switched from a thread-based programming model to a task-based
    model. The complete task-based example now looks like this:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经从基于线程的编程模型切换到了基于任务的模型。完整的基于任务的示例现在看起来是这样的：
- en: '[PRE24]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: There is really a minimal amount of code left here for handling concurrency.
    The recommended way to call functions asynchronously is to use `std::async()`.
    For a deeper discussion about why and when `std::async()` is preferred, I highly
    recommend the *Concurrency* chapter in *Effective Modern C++* by Scott Meyers.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这里真的只剩下很少的代码来处理并发。异步调用函数的推荐方式是使用`std::async()`。关于为什么以及何时首选`std::async()`的更深入讨论，我强烈推荐Scott
    Meyers的*Effective Modern C++*中的*并发*章节。
- en: Additional synchronization primitives in C++20
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C++20中的额外同步原语
- en: C++20 comes with a few additional synchronization primitives, namely `std::latch`,
    `std::barrier`, and `std::counting_semaphore` (and the template specialization
    `std::binary_semaphore`). This section will be an overview of these new types
    and some typical scenarios where they can be useful. We'll begin with `std::latch`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: C++20带来了一些额外的同步原语，即`std::latch`、`std::barrier`和`std::counting_semaphore`（以及模板特化`std::binary_semaphore`）。本节将概述这些新类型以及它们可以有用的一些典型场景。我们将从`std::latch`开始。
- en: Using latches
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用门闩
- en: A latch is a synchronization primitive that can be used for synchronizing multiple
    threads. It creates a synchronization point where all threads must arrive at.
    You can think of a latch as a decrementing counter. Typically, all threads decrement
    the counter once and then wait for the latch to reach zero before moving on.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 门闩是一种同步原语，可用于同步多个线程。它创建一个同步点，所有线程都必须到达。您可以将门闩视为递减计数器。通常，所有线程都会递减计数器一次，然后等待门闩达到零，然后再继续。
- en: 'A latch is constructed by passing an initial value of the internal counter:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 门闩是通过传递内部计数器的初始值来构造的：
- en: '[PRE25]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Threads can then decrement the counter using `count_down()`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后线程可以使用`count_down()`递减计数器：
- en: '[PRE26]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'A thread can wait on the latch to reach zero:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 线程可以等待在门闩上达到零：
- en: '[PRE27]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'It''s also possible to check (without blocking) to see whether the counter
    has reached zero:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以检查（不阻塞）计数器是否已经达到零：
- en: '[PRE28]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'It''s common to wait for the latch to reach zero right after decrementing the
    counter, as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在递减计数器后立即等待门闩达到零，如下所示：
- en: '[PRE29]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In fact, this use case is common enough to deserve a tailor-made member function;
    `arrive_and_wait()` decrements the latch and then waits for the latch to reach
    zero:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这种用法很常见，值得一个定制的成员函数；`arrive_and_wait()`递减门闩，然后等待门闩达到零：
- en: '[PRE30]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Joining a set of forked tasks is a common scenario when working with concurrency.
    If the tasks only need to be joined at the end, we can use an array of future
    objects (to wait on) or just wait for all the threads to complete. But in other
    cases, we want a set of asynchronous tasks to arrive at a common synchronization
    point, and then have the tasks continue running. These situations typically occur
    when some sort of initialization is needed before multiple worker threads start
    their actual work.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在并发工作时，加入一组分叉任务是一种常见情况。如果任务只需要在最后加入，我们可以使用一个未来对象数组（等待）或者只等待所有线程完成。但在其他情况下，我们希望一组异步任务到达一个共同的同步点，然后让任务继续运行。这些情况通常发生在多个工作线程开始实际工作之前需要某种初始化的情况下。
- en: 'Example: Initializing threads using std::latch'
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：使用std::latch初始化线程
- en: The following example demonstrates how `std::latch` can be used when multiple
    worker threads need to run some initialization code before they start working.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了当多个工作线程需要在开始工作之前运行一些初始化代码时，如何使用`std::latch`。
- en: 'When a thread is created, a contiguous block of memory is allocated for the
    stack. Typically, this memory does not yet reside in physical memory when it is
    first allocated in the virtual address space. Instead, when the stack is being
    used, *page faults* will be generated in order to map the virtual memory to physical
    memory. The operating system handles the mapping for us, and it is an efficient
    way to lazily map memory when needed. Usually, this is just what we want: we pay
    for the cost of mapping memory as late as possible and only if needed. However,
    in circumstances where low latency is important, for example in real-time code,
    it might be necessary to completely avoid page faults. The stack memory is unlikely
    to be paged out by the operating system, so it is usually enough to run some code
    that will generate page faults and thereby map the virtual stack memory to physical
    memory. This process is called **prefaulting**.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个线程时，会为堆栈分配一块连续的内存。通常，当首次在虚拟地址空间中分配内存时，这块内存尚未驻留在物理内存中。相反，当堆栈被使用时，将生成*页错误*，以便将虚拟内存映射到物理内存。操作系统会为我们处理映射，这是一种在需要时懒惰地映射内存的有效方式。通常，这正是我们想要的：我们尽可能晚地支付映射内存的成本，只有在需要时才会支付。然而，在低延迟很重要的情况下，例如在实时代码中，可能需要完全避免页错误。堆栈内存不太可能被操作系统分页出去，因此通常只需运行一些代码，生成页错误，从而将虚拟堆栈内存映射到物理内存。这个过程称为**预缓存**。
- en: 'There is no portable way to set or get the stack size of a C++ thread, so here
    we will just assume that the stack is at least 500 KB. The following code is an
    attempt to prefault the first 500 KB of the stack:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一种可移植的方法来设置或获取C++线程的堆栈大小，所以这里我们只是假设堆栈至少为500 KB。以下代码尝试预先分配堆栈的前500 KB：
- en: '[PRE31]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The idea here is to allocate an array on the stack that will occupy a significant
    chunk of stack memory. Then, in order to generate page faults, we write to every
    element in the array using `std::fill()`. The volatile keyword was not mentioned
    earlier and is a somewhat confusing keyword in C++. It has nothing to do with
    concurrency; it's only added here to prevent the compiler from optimizing away
    this code. By declaring the `mem` array `volatile`, the compiler is not allowed
    to ignore the writes to the array.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是在堆栈上分配一个数组，它将占用大量的堆栈内存。然后，为了生成页面错误，我们使用`std::fill()`写入数组中的每个元素。之前没有提到`volatile`关键字，它是C++中一个有些令人困惑的关键字。它与并发无关；它只是在这里添加以防止编译器优化掉这段代码。通过声明`mem`数组为`volatile`，编译器不允许忽略对数组的写入。
- en: 'Now, let''s focus on the actual `std::latch`. Let''s say we want to create
    a number of worker threads that should only start their work once all thread stacks
    have been prefaulted. We can achieve this synchronization using a `std::latch`,
    as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们专注于实际的`std::latch`。假设我们想要创建一些工作线程，只有在所有线程堆栈都被预分配后才能开始它们的工作。我们可以使用`std::latch`来实现这种同步，如下所示：
- en: '[PRE32]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: After all threads have arrived, the main thread can start to submit work to
    the worker threads. In this example, all threads are waiting for the other threads
    to arrive by calling `arrive_and_wait()` on the latch. Once the latch has reached
    zero, it can no longer be reused. There is no function for resetting the latch.
    If we have a scenario that requires multiple synchronization points, we can instead
    use a `std::barrier`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 所有线程到达后，主线程可以开始向工作线程提交工作。在这个例子中，所有线程都在等待其他线程到达，通过在屏障上调用`arrive_and_wait()`来实现。一旦屏障达到零，就不能再重用它了。没有重置屏障的函数。如果我们有一个需要多个同步点的场景，我们可以使用`std::barrier`来代替。
- en: Using barriers
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用屏障
- en: 'Barriers are similar to latches but with two major additions: a barrier can
    be *reused*, and it can run a *completion function* whenever all threads have
    reached the barrier.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 屏障类似于latch，但有两个主要的附加功能：屏障可以被*重用*，并且当所有线程到达屏障时可以运行*完成函数*。
- en: 'A barrier is constructed by passing an initial value of the internal counter
    and a completion function:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递内部计数器的初始值和完成函数来构造屏障：
- en: '[PRE33]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Threads can arrive and wait in the same way we use a latch:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 线程可以以与使用latch相同的方式到达并等待：
- en: '[PRE34]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Whenever all threads have arrived (that is, when the internal counter of the
    barrier reaches zero) two things happens:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 每当所有线程都到达（也就是说，当屏障的内部计数器达到零时）时，会发生两件事：
- en: The completion function provided to the constructor is called by the barrier.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供给构造函数的完成函数由屏障调用。
- en: The internal counter is reset to its initial value after the completion function
    has returned.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完成函数返回后，内部计数器将被重置为其初始值。
- en: Barriers are useful in parallel programming algorithms that are based on the
    **fork-join model**. Typically, an iterative algorithm contains a part that can
    be run in parallel and another part that needs to run sequentially. Multiple tasks
    are forked and run in parallel. Then, when all tasks have finished and joined,
    some single-threaded code is executed to determine whether the algorithm should
    continue or finish.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 屏障在基于**fork-join模型**的并行编程算法中非常有用。通常，迭代算法包含一个可以并行运行的部分和一个需要顺序运行的部分。多个任务被分叉并并行运行。然后，当所有任务都完成并加入时，会执行一些单线程代码来确定算法是否应该继续还是结束。
- en: '![](img/B15619_11_10.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_10.png)'
- en: 'Figure 11.10: An example of the fork-join model'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10：fork-join模型的示例
- en: Concurrent algorithms that follow the fork-join model will benefit from using
    barriers and can avoid other explicit locking mechanisms in an elegant and efficient
    way. Let's see how we can use a barrier but with two major for a simple problem.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循fork-join模型的并发算法将受益于使用屏障，并可以以一种优雅和高效的方式避免其他显式的锁定机制。让我们看看如何在一个简单的问题中使用屏障但有两个主要的问题。
- en: 'Example: Fork-join using std::barrier'
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：使用std::barrier进行fork-join
- en: Our next example is a toy problem that will demonstrate the fork-join model.
    We will create a small program that will simulate a set of dice being rolled and
    count the number of rolls it takes before getting all 6s. Rolling a set of dice
    is something we can do concurrently (forked). The join step, executing in a single
    thread, checks the result and determines whether to roll the dice again or to
    finish.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个示例是一个玩具问题，将演示fork-join模型。我们将创建一个小程序，模拟一组骰子被掷出，并计算在获得所有6之前需要掷出的次数。掷一组骰子是我们可以并发执行的（分叉）操作。在单个线程中执行的加入步骤检查结果，并确定是重新掷骰子还是结束。
- en: 'First, we need to implement the code for rolling a dice with six faces. For
    generating a number between 1 and 6 we can use a combination of classes found
    in the `<random>` header, as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要实现掷骰子的代码，有六个面。为了生成1到6之间的数字，我们可以使用`<random>`头文件中的类的组合，如下所示：
- en: '[PRE35]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here the `std::random_device` is responsible for generating a seed to the engine
    that will produce pseudo-random numbers. To pick an integer between 1 and 6 with
    equal probability, we are using `std::uniform_int_distribution`. The variable
    `result` is the result of rolling a dice.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`std::random_device`负责生成一个种子，用于产生伪随机数的引擎。为了以相等的概率选择1到6之间的整数，我们使用`std::uniform_int_distribution`。变量`result`是掷骰子的结果。
- en: Now we want to encapsulate this code into a function that will generate a random
    integer. Generating the seed and creating the engine is typically slow and something
    we want to avoid doing at every call. A common way to do this is to declare the
    random engine with `static` duration so that it lives during the entire lifetime
    of the program. However, the classes in `<random>` are not thread-safe so we need
    to protect the `static` engine somehow. Instead of synchronizing access with a
    mutex, which would make the random number generator run sequentially, I will take
    the opportunity to demonstrate how to use thread-local storage.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想将此代码封装到一个函数中，该函数将生成一个随机整数。生成种子并创建引擎通常很慢，我们希望避免在每次调用时都这样做。通常的做法是使用`static`持续时间声明随机引擎，以便它在整个程序的生命周期内存在。但是，`<random>`中的类不是线程安全的，因此我们需要以某种方式保护`static`引擎。我将利用这个机会演示如何使用线程本地存储，而不是使用互斥锁同步访问，这将使随机数生成器按顺序运行。
- en: 'Here is how to declare the engine as a `static thread_local` object:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何将引擎声明为`static thread_local`对象的方法：
- en: '[PRE36]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'A static variable with `thread_local` storage duration will be created once
    per thread; it''s therefore safe to call `random_int()` from multiple threads
    concurrently without using any synchronization primitives. With this small helper
    function in place, we can move on to implement the rest of our program using a
    `std::barrier`:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 具有`thread_local`存储期的静态变量将在每个线程中创建一次；因此，可以在不使用任何同步原语的情况下同时从多个线程调用`random_int()`。有了这个小的辅助函数，我们可以继续使用`std::barrier`实现程序的其余部分：
- en: '[PRE37]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The lambda `check_result()` is the completion function that will be called every
    time all the threads have arrived at the barrier. The completion function checks
    the values of each dice and determines whether a new round should be played or
    if we are done.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: lambda`check_result()`是完成函数，每次所有线程都到达屏障时都会调用它。完成函数检查每个骰子的值，并确定是否应该玩新一轮，或者我们已经完成。
- en: The lambda passed to the `std::thread` objects captures the index `i` by value
    so that all threads have a unique index. The other variables, `done`, `dice`,
    and `bar`, are captured by reference.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给`std::thread`对象的lambda通过值捕获索引`i`，以便所有线程都具有唯一的索引。其他变量`done`、`dice`和`bar`通过引用捕获。
- en: Note also how we can mutate and read the variables captured by reference from
    different threads without introducing any data races thanks to the coordination
    performed by the barrier.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，我们可以在不引入任何数据竞争的情况下从不同线程中对引用捕获的变量进行突变和读取，这要归功于屏障执行的协调。
- en: Signalling and resource counting using semaphores
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用信号量进行信号传递和资源计数
- en: The word **semaphore** means something that can be used for signaling, such
    as a flag or a light. In the example that follows, you will see how we can use
    semaphores for signaling different states that other threads can be waiting for.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**信号量**一词表示可以用于信号传递的东西，例如旗帜或灯。在接下来的示例中，您将看到我们如何使用信号量来传递其他线程可能正在等待的不同状态。'
- en: 'A semaphore can also be used to control access to a resource, similarly to
    how a `std::mutex` restricts access to a critical section:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 信号量还可以用于控制对资源的访问，类似于`std::mutex`限制对临界区的访问：
- en: '[PRE38]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In this case, the semaphore is initialized with a value of `4`, which means
    that at most four concurrent requests can be handled at the same time. Instead
    of mutually exclusive access to a section in the code, multiple threads can have
    access to the same section but with restrictions concerning the number of threads
    currently in that section.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，信号量的初始值为`4`，这意味着最多可以同时处理四个并发请求。与代码中的某个部分相互排斥的访问不同，多个线程可以访问相同的部分，但受限于当前在该部分的线程数量。
- en: The member function `acquire()` decrements the semaphore if the semaphore is
    greater than zero. Otherwise `acquire()` blocks until the semaphore allows it
    to decrement and enter the restricted section. `release()` increments the counter
    without blocking. If the semaphore was zero before it was incremented by `release()`,
    waiting threads will be signaled.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 成员函数`acquire()`在信号量大于零时减少信号量。否则，`acquire()`将阻塞，直到信号量允许其减少并进入受限制的部分。`release()`在不阻塞的情况下增加计数器。如果在`release()`增加计数器之前信号量为零，则会发出信号通知等待的线程。
- en: In addition to the `acquire()` function, it's also possible to try to decrement
    the counter *without blocking* using the `try_acquire()` function. It returns
    `true` if it managed to decrement the counter, or `false` otherwise. The functions
    `try_acquire_for()` and `try_acquire_until()` can be used in a similar way. But
    instead of immediately returning `false` when the counter is already zero, they
    automatically try to decrement the counter within a specified time before returning
    to the caller.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`acquire()`函数之外，还可以使用`try_acquire()`函数*无阻塞*地尝试减少计数器。如果成功减少计数器，则返回`true`，否则返回`false`。函数`try_acquire_for()`和`try_acquire_until()`可以类似地使用。但是，它们在计数器已经为零时不会立即返回`false`，而是在指定时间内自动尝试减少计数器，然后再返回给调用者。
- en: This trio of functions follows the same pattern as other types in the standard
    library, for example, `std::timed_mutex` and its `try_lock()`, `try_lock_for()`,
    and `try_lock_until()` member functions.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个函数的模式与标准库中的其他类型相同，例如`std::timed_mutex`及其`try_lock()`、`try_lock_for()`和`try_lock_until()`成员函数。
- en: The `std::counting_semaphore` is a template with one template parameter accepting
    the maximum value of the semaphore. It is considered a programming error to increment
    (release) a semaphore above its maximum value.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::counting_semaphore`是一个模板，具有一个模板参数，接受信号量的最大值。在增加（释放）信号量超过其最大值时被认为是编程错误。'
- en: 'A `std::counting_semaphore` with a maximum size of 1 is called a **binary semaphore**.
    The `<semaphore>` header includes an alias-declaration for binary semaphores:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最大大小为1的`std::counting_semaphore`称为**二进制信号量**。`<semaphore>`头文件包括二进制信号量的别名声明：
- en: '[PRE39]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: A binary semaphore is guaranteed to be implemented more efficiently than a counting
    semaphore with a higher maximum value.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制信号量的实现效率比具有更高最大值的计数信号量更高。
- en: Another important property of semaphores is that the thread that releases a
    semaphore may not be the thread that acquired it. This is in contrast with `std::mutex`,
    which requires that the thread that acquired the mutex is also the thread that
    must release it. However, with semaphores it's common to have one type of task to
    do the waiting (acquire) and another type of task to do the signaling (release).
    This will be demonstrated in our next example.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 信号量的另一个重要属性是释放信号量的线程可能不是获取它的线程。这与`std::mutex`相反，后者要求获取互斥锁的线程也必须释放它。然而，使用信号量时，通常有一种类型的任务负责等待（获取），另一种类型的任务负责信号（释放）。这将在我们的下一个示例中演示。
- en: 'Example: A bounded buffer using semaphores'
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：使用信号量的有界缓冲区
- en: The following example demonstrates a bounded buffer. It's a fixed-size buffer
    that can have multiple threads reading and writing from it. Again, this example
    demonstrates the kind of producer-consumer pattern that you have already seen
    using condition variables. The producer threads are the ones writing to the buffer
    and the reader threads are the ones reading (and popping elements) from the buffer.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了一个有界缓冲区。这是一个固定大小的缓冲区，可以有多个线程从中读取和写入。同样，这个示例演示了你已经使用条件变量看到的生产者-消费者模式。生产者线程是写入缓冲区的线程，而读取线程是从缓冲区中读取（和弹出元素）的线程。
- en: 'The following figure shows the buffer (a fixed-size array) and the two variables
    that keep track of the read and write positions:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了缓冲区（一个固定大小的数组）和跟踪读取和写入位置的两个变量：
- en: '![](img/B15619_11_11.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_11.png)'
- en: 'Figure 11.11: A bounded buffer has a fixed size'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11：有界缓冲区具有固定大小
- en: 'We will take one step at a time and start with a version that focuses on the
    internal logic of the bounded buffer. The signaling using semaphores will be added
    in the next version. Here, the initial attempt demonstrates how the read and write
    positions are used:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一步一步地开始，从一个专注于有界缓冲区内部逻辑的版本开始。使用信号量进行信号传递将在下一个版本中添加。在这里，初始尝试演示了读取和写入位置的使用方式：
- en: '[PRE40]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This first attempt contains the fixed-sized buffer, the read and write positions,
    and a mutex for protecting the data members from data races. This implementation
    should be able to have an arbitrary number of threads calling `push()` and `pop()`
    concurrently.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第一次尝试包含了固定大小的缓冲区，读取和写入位置，以及一个互斥锁，用于保护数据成员免受数据竞争的影响。这个实现应该能够让任意数量的线程同时调用`push()`和`pop()`。
- en: The `push()` function overloads on `const T&` and `T&&`. This is an optimization
    technique used by the standard library containers. The `T&&` version avoids copying
    the argument when the caller passes an rvalue.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '`push()`函数重载了`const T&`和`T&&`。这是标准库容器使用的一种优化技术。`T&&`版本在调用者传递一个右值时避免了参数的复制。'
- en: To avoid duplicating the logic of the push operation, a helper function, `do_push()`,
    contains the actual logic. By using a forwarding reference (`auto&& item`) together
    with `std::forward`, the `item` parameter will be move assigned or copy assigned,
    depending on whether the client called `push()` with an rvalue or lvalue.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免重复推送操作的逻辑，一个辅助函数`do_push()`包含了实际的逻辑。通过使用转发引用（`auto&& item`）以及`std::forward`，`item`参数将根据客户端使用右值还是左值调用`push()`而进行移动分配或复制分配。
- en: This version of the bounded buffer is not complete, though, because it doesn't
    protect us from having the `write_pos` point at (or beyond) the `read_pos`. Similarly,
    the `read_pos` must never point at the `write_pos` (or beyond). What we want is
    a buffer where producer threads block when the buffer is full and consumer threads
    block when the buffer is empty.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这个有界缓冲区的版本并不完整，因为它没有保护我们免受`write_pos`指向（或超出）`read_pos`的影响。同样，`read_pos`绝不能指向`write_pos`（或超出）。我们想要的是一个缓冲区，在缓冲区满时生产者线程被阻塞，而在缓冲区为空时消费者线程被阻塞。
- en: This is a perfect application for using counting semaphores. A semaphore *blocks*
    a thread that tries to decrease the semaphore when it is already zero. A semaphore
    *signals* the blocked threads whenever a semaphore that has the value zero increments.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用计数信号量的完美应用。信号量*阻塞*试图将信号量减少到已经为零的线程。信号量*信号*被阻塞的线程，每当一个值为零的信号量增加时。
- en: 'For the bounded buffer we need two semaphores:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有界缓冲区，我们需要两个信号量：
- en: The first semaphore, `n_empty_slots`, keeps track of the number of empty slots in
    the buffer. It will start with a value of the size of the buffer.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个信号量`n_empty_slots`跟踪缓冲区中空槽的数量。它将以缓冲区大小的值开始。
- en: The second semaphore, `n_full_slots`, keeps track of the number of full slots in
    the buffer.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个信号量`n_full_slots`跟踪缓冲区中满槽的数量。
- en: 'Make sure you understand why two counting semaphores are needed (rather than
    one). The reason is that there are two distinct *states* that need to be signaled:
    when the buffer is *full* and when the buffer is *empty*.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你理解为什么需要两个计数信号量（而不是一个）。原因是有两个不同的*状态*需要被信号：当缓冲区*满*时和当缓冲区*空*时。
- en: 'After adding signal handling using two counting semaphores, the bounded buffer
    now looks like this (lines added in this version are marked with "new"):'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加了使用两个计数信号量进行信号处理后，有界缓冲区现在看起来像这样（在此版本中添加的行用“new”标记）：
- en: '[PRE41]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This version supports multiple producers and consumers. The use of both semaphores
    guarantees that neither of the semaphores will reach a value greater than the
    maximum number of elements in the buffer. For example, there is no way a producer
    thread can add a value and increment the `n_full_slots` semaphore without first
    checking that there is at least one empty slot.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本支持多个生产者和消费者。两个信号量的使用保证了两者都不会达到缓冲区中元素的最大数量。例如，生产者线程无法在首先检查是否有至少一个空槽之前添加值并增加`n_full_slots`信号量。
- en: Note also that `acquire()` and `release()` are called from different threads.
    For example, the consumer threads are waiting (`acquire()`) on the `n_full_slots`
    semaphore and the producer threads are signaling (`release()`) on the very same
    semaphore.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，`acquire()`和`release()`是从不同的线程调用的。例如，消费者线程正在等待（`acquire()`）`n_full_slots`信号量，而生产者线程正在对同一个信号量进行信号（`release()`）。
- en: The new synchronization primitives added to C++20 are well known constructs
    that are commonly found in threading libraries. They offer convenient and often
    more efficient alternatives to synchronize access to shared resources compared
    to `std::mutex` and `std::condition_variable`.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: C++20中添加的新同步原语是常见的线程库中常见的构造。与`std::mutex`和`std::condition_variable`相比，它们提供了方便且通常更有效的替代方案来同步对共享资源的访问。
- en: Atomic support in C++
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C++中的原子支持
- en: The standard library contains support for **atomic variables**, sometimes called
    **atomics**. An atomic variable is a variable that can safely be used and mutated
    from multiple threads without introducing data races.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 标准库包含对**原子变量**的支持，有时被称为**原子**。原子变量是一种可以安全地从多个线程使用和变异而不引入数据竞争的变量。
- en: 'Do you remember the data race example we looked at earlier where two threads
    updated a global counter? We solved it by adding a mutex lock together with the
    counter. Instead of using an explicit lock, we could have used a `std::atomic<int>`
    instead:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 您还记得我们之前看过的两个线程更新全局计数器的数据竞争示例吗？我们通过添加互斥锁和计数器来解决了这个问题。我们可以使用 `std::atomic<int>`
    来代替显式锁：
- en: '[PRE42]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The `++counter` is a convenient way of saying `counter.fetch_add(1)`. All member
    functions that can be invoked on an atomic are safe to call from multiple threads
    concurrently.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`++counter` 是一种方便的方式，相当于 `counter.fetch_add(1)`。可以从多个线程同时调用的所有成员函数都是安全的。'
- en: 'The atomic types are from the `<atomic>` header. There are typedefs for all
    the scalar data types named on the `std::atomic_int` form. This is identical to
    saying `std::atomic<int>`. It is possible to wrap a custom type in a `std::atomic`
    template, as long as the custom type is trivially copyable. Basically, this means
    that an object of a class is fully described by the bits of its data members.
    In that way, an object can be copied with, for example, `std::memcpy()`, by only
    copying the raw bytes. So, if a class contains virtual functions, pointers to
    dynamic memory, and so on, it''s no longer possible to just copy the raw bits
    of the object and expect it to work, and hence it is not trivially copyable. This
    can be checked at compile time, so you will get a compilation error if you try
    to create an atomic of a type that is not trivially copyable:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 原子类型来自`<atomic>`头文件。对于所有标量数据类型，都有命名为`std::atomic_int`的typedef。这与`std::atomic<int>`相同。只要自定义类型是平凡可复制的，就可以将自定义类型包装在`std::atomic`模板中。基本上，这意味着类的对象完全由其数据成员的位描述。这样，对象可以通过例如`std::memcpy()`仅复制原始字节来复制。因此，如果一个类包含虚函数、指向动态内存的指针等，就不再可能仅仅复制对象的原始位并期望它能够工作，因此它不是平凡可复制的。这可以在编译时检查，因此如果尝试创建一个不是平凡可复制的类型的原子，将会得到编译错误：
- en: '[PRE43]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: It's also possible to create atomic pointers. This makes the pointer itself
    atomic, but not the object it points at. We will talk more about atomic pointers
    and references in a while.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以创建原子指针。这使得指针本身是原子的，但指向的对象不是。我们将在稍后更多地讨论原子指针和引用。
- en: The lock-free property
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无锁属性
- en: A reason for using atomics rather than protecting access to a variable with
    a mutex is to avoid the performance overhead introduced by using `std::mutex`.
    Also, the fact that a mutex can block the thread for a non-deterministic duration
    of time and introduce priority inversion (see the section *Thread priorities*)
    rules out mutexes in low latency contexts. In other words, there might be parts
    of your code with latency requirements that completely forbid the use of mutexes.
    In those cases, it's important to know whether an atomic variable is using a mutex.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用原子而不是用互斥锁保护变量的原因是避免使用`std::mutex`引入的性能开销。此外，互斥锁可能会阻塞线程一段非确定性的时间，并引入优先级反转（参见*线程优先级*部分），这排除了在低延迟环境中使用互斥锁。换句话说，您的代码中可能有延迟要求的部分完全禁止使用互斥锁。在这些情况下，了解原子变量是否使用互斥锁是很重要的。
- en: 'An atomic variable may or may not use a lock to protect the data; this depends
    on the type of the variable and the platform. If the atomic does not use a lock,
    it is said to be **lock-free**. You can query the variable in runtime if it''s
    lock-free:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 原子变量可能会或可能不会使用锁来保护数据；这取决于变量的类型和平台。如果原子变量不使用锁，则称为**无锁**。您可以在运行时查询变量是否无锁：
- en: '[PRE44]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This is good, because now we at least assert when running the program that using
    the `variable` object is lock-free. Typically, all atomic objects of the same
    type will be either lock-free or not, but on some exotic platforms there is a
    possibility that two atomic objects might generate different answers.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，因为现在至少在运行程序时我们可以断言使用 `variable` 对象是无锁的。通常，同一类型的所有原子对象都将是无锁或有锁的，但在一些奇异的平台上，有可能两个原子对象会生成不同的答案。
- en: 'It''s generally more interesting to know whether an atomic type (`std::atomic<T>`)
    is guaranteed to be lock-free on a certain platform, and preferably we would like
    to know that at compile time rather than runtime. Since C++17, it''s also possible
    to verify that an atomic specialization is lock-free at compile time by using
    `is_always_lock_free()`, like this:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 通常更有趣的是知道在特定平台上是否保证了原子类型（`std::atomic<T>`）是无锁的，最好是在编译时而不是运行时知道。自C++17以来，还可以使用`is_always_lock_free()`在编译时验证原子特化是否是无锁的，就像这样：
- en: '[PRE45]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This code will generate a compilation error if `atomic<int>` is not lock-free
    on the platform we are targeting. Now, if we compile a program that assumes that
    `std::atomic<int>` doesn't use locks, it will fail to compile, which is exactly
    want we want.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的目标平台上 `atomic<int>` 不是无锁的，这段代码将生成编译错误。现在，如果我们编译一个假设 `std::atomic<int>`
    不使用锁的程序，它将无法编译，这正是我们想要的。
- en: On modern platforms, any `std::atomic<T>` where `T` fits into the native word
    size will typically be *always lock-free*. And on modern x64 chips, you even get
    double that amount. For example, on libc++ compiled on a modern Intel CPU, `std::atomic<std::complex<double>>`
    is always lock-free.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代平台上，任何`std::atomic<T>`，其中`T`适合本机字大小，通常都是*始终无锁*的。在现代x64芯片上，甚至可以获得双倍的数量。例如，在现代英特尔CPU上编译的libc++上，`std::atomic<std::complex<double>>`始终是无锁的。
- en: Atomic flags
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原子标志
- en: An atomic type that is guaranteed to always be lock-free is `std::atomic_flag`
    (regardless of the target platform). As a consequence, `std::atomic_flag` does
    not provide us with the `is_always_lock_free()`/`is_lock_free()` functions since
    they would always return `true`.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 保证始终是无锁的原子类型是`std::atomic_flag`（无论目标平台如何）。因此，`std::atomic_flag`不提供`is_always_lock_free()`/`is_lock_free()`函数，因为它们总是返回`true`。
- en: 'Atomic flags can be used to protect critical sections as an alternative to
    using `std::mutex`. Since a lock is conceptually easy to understand, I will use
    that as an example here. It should be noted, though, that the implementations
    of locks that I demonstrate in this book are not production-ready code, but rather
    conceptual implementation. The following example demonstrates how to conceptually
    implement a simple spinlock:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 原子标志可以用来保护临界区，作为使用`std::mutex`的替代方案。由于锁的概念容易理解，我将在这里以此为例。但需要注意的是，我在本书中演示的锁的实现并不是生产就绪的代码，而是概念上的实现。以下示例演示了如何概念上实现一个简单的自旋锁：
- en: '[PRE46]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The `lock()` function calls `test_and_set()` to set the flag and at the same
    time obtain the previous value of the flag. If `test_and_set()` returns `false`,
    it means that the caller managed to acquire the lock (setting the flag when it
    was previously cleared). Otherwise, the inner `while`-loop will constantly poll
    the state of the flag using `test()` in a spinning loop. The reason we use `test()`
    in an extra inner loop is performance: `test()` doesn''t invalidate the cache
    line, whereas `test_and_set()` does. This locking protocol is called **test and
    test-and-set**.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`lock()`函数调用`test_and_set()`来设置标志并同时获取标志的先前值。如果`test_and_set()`返回`false`，意味着调用者成功获取了锁（在先前清除标志时设置标志）。否则，内部的`while`循环将不断使用`test()`在一个自旋循环中轮询标志的状态。我们在额外的内部循环中使用`test()`的原因是性能：`test()`不会使缓存行失效，而`test_and_set()`会。这种锁定协议称为**测试和测试并设置**。'
- en: This spinlock works but is not very resource-friendly; when the thread is executing,
    it constantly uses the CPU to check the same condition over and over again. We
    could add a short sleep with an exponential backoff in each iteration, but finetuning
    this for various platforms and scenarios is hard.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这个自旋锁可以工作，但不太节约资源；当线程执行时，它不断使用CPU来一遍又一遍地检查相同的条件。我们可以在每次迭代中添加一个短暂的休眠和指数退避，但是为各种平台和场景微调这一点是很困难的。
- en: Fortunately, C++20 added a wait and notify API to `std::atomic`, which makes
    it possible for threads to wait (in a resource-friendly manner) on an atomic variable
    to change its value.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，C++20为`std::atomic`添加了等待和通知API，使线程可以等待（以一种节约资源的方式）原子变量改变其值。
- en: Atomic wait and notify
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原子等待和通知
- en: Since C++20, `std::atomic` and `std::atomic_flag` provide the functionality
    for waiting and notifying. The function `wait()` blocks the current thread until
    the value of the atomic variable changes and some other thread notifies the waiting
    thread about it. A thread can notify that a change has occurred by calling either
    `notify_one()` or `notify_all()`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 自C++20以来，`std::atomic`和`std::atomic_flag`提供了等待和通知的功能。`wait()`函数阻塞当前线程，直到原子变量的值发生变化，并且其他线程通知等待线程。线程可以通过调用`notify_one()`或`notify_all()`来通知发生了变化。
- en: With this new functionality, we can avoid continuously polling the state of
    the atomic and instead wait in a more resource-friendly way until the value changes;
    this is similar to how a `std::condition_variable` allows us to wait and notify
    state changes.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个新功能，我们可以避免不断轮询原子的状态，而是以更节约资源的方式等待值的改变；这类似于`std::condition_variable`允许我们等待和通知状态改变的方式。
- en: 'By using wait and notify, the `SimpleMutex` implemented in the previous section
    can be rewritten like this:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用等待和通知，前一节中实现的`SimpleMutex`可以重写如下：
- en: '[PRE47]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We pass the old value (`true`) to `wait()`. By the time `wait()` returns, the
    atomic variable is guaranteed to have changed so that it is no longer `true`.
    However, there is no guarantee that we will catch *all* the changes to the variable.
    The variable might have changed from state A to state B and then back to state
    A without notifying the waiting thread. This is a phenomenon in lock-free programming
    called the **ABA problem**.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将旧值（`true`）传递给`wait()`。在`wait()`返回时，可以保证原子变量已经改变，不再是`true`。但不能保证我们会捕捉到*所有*变量的改变。变量可能已经从状态A改变到状态B，然后再回到状态A，而没有通知等待的线程。这是无锁编程中的一种现象，称为**ABA问题**。
- en: This example demonstrated the wait and notify functions using `std::atomic_flag`.
    The same wait and notify API is also available on the `std::atomic` class template.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例演示了使用`std::atomic_flag`的等待和通知功能。相同的等待和通知API也适用于`std::atomic`类模板。
- en: Please note that the spinlocks presented in this chapter are not production-ready
    code. Implementing a highly efficient lock typically involves the correct use
    of memory orderings (discussed later) and non-portable code for yielding, which
    is beyond the scope of this book. A detailed discussion can be found at [https://timur.audio/using-locks-in-real-time-audio-processing-safely](https://timur.audio/using-locks-in-real-time-audio-processing-safely).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本章中介绍的自旋锁不是生产就绪的代码。实现高效的锁通常涉及正确使用内存顺序（稍后讨论）和用于让出的非可移植代码，这超出了本书的范围。详细讨论可在[https://timur.audio/using-locks-in-real-time-audio-processing-safely](https://timur.audio/using-locks-in-real-time-audio-processing-safely)找到。
- en: Now, we will continue talking about atomic pointers and atomic references.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续讨论原子指针和原子引用。
- en: Using shared_ptr in a multithreaded environment
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在多线程环境中使用shared_ptr
- en: What about the `std::shared_ptr`? Can it be used in a multithreaded environment,
    and how is reference counting handled when multiple threads are accessing an object
    referenced by multiple shared pointers?
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::shared_ptr`怎么样？它能在多线程环境中使用吗？当多个线程访问由多个共享指针引用的对象时，引用计数是如何处理的？'
- en: 'To understand shared pointers and thread safety, we need to recall how `std::shared_ptr`
    is typically implemented (see also *Chapter 7*, *Memory Management*). Consider
    the following code:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解共享指针和线程安全，我们需要回顾`std::shared_ptr`通常是如何实现的（也可以参见*第7章*，*内存管理*）。考虑以下代码：
- en: '[PRE48]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The code creates an `int` on the heap and a reference-counted smart pointer
    pointing at the `int` object. When creating the shared pointer with `std::make_shared()`,
    a `control block` is created next to the `int`. The control block contains, among
    other things, a variable for the reference count, which is incremented whenever
    a new pointer to the `int` is created and decremented whenever a pointer to the
    `int` is destroyed. To summarize, when the preceding code line is executed, three
    separate entities are created:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 代码在堆上创建了一个`int`和一个指向`int`对象的引用计数智能指针。使用`std::make_shared()`创建共享指针时，会在`int`旁边创建一个`控制块`。控制块包含引用计数等内容，每当创建指向`int`的新指针时，引用计数就会增加，每当销毁指向`int`的指针时，引用计数就会减少。总之，当执行上述代码行时，会创建三个单独的实体：
- en: The actual `std::shared_ptr` object `p1` (local variable on the stack)
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际的`std::shared_ptr`对象`p1`（堆栈上的局部变量）
- en: A control block (heap object)
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个控制块（堆对象）
- en: An `int` (heap object)
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`int`（堆对象）
- en: 'The following figure shows the three objects:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了三个对象：
- en: '![](img/B15619_11_12.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_12.png)'
- en: 'Figure 11.12: A shared_ptr instance, p1, that points to the integer object
    and a control block that contains the reference counting. In this case, there
    is only one shared pointer using the int, and hence the ref count is 1.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12：一个指向整数对象的shared_ptr实例p1和包含引用计数的控制块。在这种情况下，只有一个共享指针使用int，因此引用计数为1。
- en: 'Now, consider what would happen if the following code was executed by a second
    thread:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑如果以下代码被第二个线程执行会发生什么？
- en: '[PRE49]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We are creating a new pointer pointing at the `int` (and the control block).
    When creating the `p2` pointer, we read `p1`, but we also need to mutate the control
    block when updating the reference counter. The control block lives on the heap
    and is shared among the two threads, so it needs synchronization to avoid data
    races. Since the control block is an implementation detail hidden behind the `std::shared_ptr`
    interface, there is no way for us to know how to protect it, and it turns out
    that it has already been taken care of by the implementation.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在创建一个新的指针指向`int`（和控制块）。创建`p2`指针时，我们读取`p1`，但在更新引用计数时也需要改变控制块。控制块位于堆上，并且在两个线程之间共享，因此需要同步以避免数据竞争。由于控制块是隐藏在`std::shared_ptr`接口后面的实现细节，我们无法知道如何保护它，结果发现它已经被实现照顾了。
- en: Typically, it would use a mutable atomic counter. In other words, the ref counter
    update is thread-safe so that we can use multiple shared pointers from different
    threads without worrying about synchronizing the ref counter. This is a good practice
    and something to think about when designing classes. If you are mutating variables
    in methods that appear to be semantically read-only (`const`) from the client's
    perspective, you should make the mutating variables thread-safe. On the other
    hand, everything that can be detected by the client as mutating functions should
    be left to the client of the class to synchronize.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，它会使用可变的原子计数器。换句话说，引用计数更新是线程安全的，因此我们可以在不担心同步引用计数的情况下，从不同线程使用多个共享指针。这是一个良好的实践，也是在设计类时需要考虑的事情。如果在客户端视角下，对变量进行了语义上只读（`const`）的方法中进行了变异，那么应该使变异变量线程安全。另一方面，客户端可以检测到的一切作为变异函数的东西应该留给类的客户端来同步。
- en: 'The following figure shows two `std::shared_ptrs`, `p1` and `p2`, that have
    access to the same object. The `int` is the shared object and the control block
    is an internally shared object between the `std::shared_ptr` instances. The control
    block is thread-safe by default:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了两个`std::shared_ptr`，`p1`和`p2`，它们都可以访问相同的对象。`int`是共享对象，控制块是`std::shared_ptr`实例之间内部共享的对象。控制块默认是线程安全的：
- en: '![](img/B15619_11_13.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_13.png)'
- en: 'Figure 11.13: Two shared_ptrs accessing the same object'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13：两个共享指针访问相同的对象
- en: 'To summarize:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：
- en: The shared object, the `int` in this example, is not thread-safe and needs explicit
    locking if it is accessed from multiple threads.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个例子中，共享对象，即`int`，不是线程安全的，如果从多个线程访问，需要显式加锁。
- en: The control block is already thread-safe, so the reference counting mechanism
    works in multi-threaded environments.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制块已经是线程安全的，因此引用计数机制在多线程环境中可以工作。
- en: Let's move on to protecting the `shared_ptr` instance.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续保护`shared_ptr`实例。
- en: Protecting the shared_ptr instance
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 保护shared_ptr实例
- en: 'Now there is only one part remaining: what about the actual `std::shared_ptr`
    objects, `p1` and `p2`, in the previous example? To understand this, let''s turn
    to an example using only one global `std::shared_ptr` object called `p`:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在只剩下一个部分：在前面的例子中，实际的`std::shared_ptr`对象`p1`和`p2`怎么样？为了理解这一点，让我们来看一个只使用一个名为`p`的全局`std::shared_ptr`对象的例子：
- en: '[PRE50]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'How can we mutate `p` from multiple threads without introducing a data race?
    One option is to protect `p` with an explicit mutex whenever we use `p`. Or, we
    could use a template specialization of `std::atomic` for `std::shared_ptr` (introduced
    in C++20). In other words, it''s possible to declare `p` as an atomic shared pointer
    like this:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在多个线程中改变`p`而不引入数据竞争？一种选择是在使用`p`时用显式互斥锁保护`p`。或者，我们可以使用`std::atomic`的模板特化来处理`std::shared_ptr`（在C++20中引入）。换句话说，可以这样声明`p`为原子共享指针：
- en: '[PRE51]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This template specialization may or may not be lock-free. You can verify this
    with the `is_lock_free()` member function. Another thing to note is that the specialization
    `std::atomic<std::shared_ptr<T>>` is an exception to the rule that `std::atomic`
    can only be specialized with types that are trivially copyable. Regardless, we
    are glad to finally have this useful type in the standard library.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模板特化可能是锁定的，也可能不是。您可以使用 `is_lock_free()` 成员函数来验证这一点。另一个需要注意的是，特化 `std::atomic<std::shared_ptr<T>>`
    是一个例外，它违反了 `std::atomic` 只能用可以平凡复制的类型进行特化的规则。不管怎样，我们很高兴最终在标准库中拥有了这个有用的类型。
- en: 'The following example demonstrates how to load and store a shared pointer object
    atomically from multiple threads:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了如何从多个线程原子地加载和存储共享指针对象：
- en: '[PRE52]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: In the preceding example, we assume that there are two threads, `T1` and `T2`,
    that call functions `f1()` and `f2()`, respectively. New heap-allocated `int`
    objects are created from the thread `T1` with the call to `std::make_shared<int>()`.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们假设有两个线程 `T1` 和 `T2`，分别调用函数 `f1()` 和 `f2()`。从线程 `T1` 中使用 `std::make_shared<int>()`
    调用创建了新的堆分配的 `int` 对象。
- en: 'There is one subtle detail to consider in this example: in which thread is
    the heap-allocated `int` deleted? When `local_p` goes out of scope in the `f2()`
    function, it might be the last reference to the `int` (the reference count reaches
    zero). In that case, the deletion of the heap-allocated `int` will happen from
    thread `T2`. Otherwise, the deletion will happen from thread `T1` when `std::atomic_store()`
    is called. So, the answer is that the deletion of the `int` can happen from both
    threads.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中有一个微妙的细节需要考虑：堆分配的 `int` 在哪个线程中被删除？当 `f2()` 函数中的 `local_p` 超出范围时，它可能是对
    `int` 的最后一个引用（引用计数达到零）。在这种情况下，堆分配的 `int` 将从线程 `T2` 中删除。否则，当调用 `std::atomic_store()`
    时，删除将从线程 `T1` 中进行。因此，答案是 `int` 的删除可以从两个线程中进行。
- en: Atomic references
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原子引用
- en: So far you have seen `std::atomc_flag` and `std::atomic<>` with numerous useful
    specializations. `std::atomic` can be specialized with pointers such as `std::atomic<T*>`,
    but you haven't seen how to use atomics with reference types. It's not possible
    to write `std::atomic<T&>`; instead, the standard library provides us with a template
    called `std::atomic_ref`.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到了 `std::atomc_flag` 和 `std::atomic<>` 以及许多有用的特殊化。`std::atomic` 可以用指针进行特殊化，比如
    `std::atomic<T*>`，但您还没有看到如何使用引用类型的原子操作。不可能编写 `std::atomic<T&>`；相反，标准库为我们提供了一个名为
    `std::atomic_ref` 的模板。
- en: The template `std::atomic_ref` was introduced in C++20\. Its interface is identical
    to `std::atomic` and the reason for having a separate name is to avoid the risk
    of impacting existing generic code that uses `std::atomic<T>`.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::atomic_ref` 模板在 C++20 中引入。它的接口与 `std::atomic` 相同，之所以有一个单独的名称是为了避免影响使用
    `std::atomic<T>` 的现有通用代码的风险。'
- en: An atomic reference allows us to perform atomic operations on a non-atomic object
    that we have a reference to. This can be convenient when we reference objects
    provided by a client or some third-party code that doesn't provide internally
    synchronized objects. We will look at an example to demonstrate the usefulness
    of atomic references.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 原子引用允许我们对我们拥有引用的非原子对象执行原子操作。当我们引用由客户端或一些不提供内部同步对象的第三方代码提供的对象时，这可能很方便。我们将看一个例子来演示原子引用的有用性。
- en: 'Example: Using atomic references'
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：使用原子引用
- en: 'Assume that we are writing a function that flips a coin a specified number
    of times:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在编写一个函数，该函数会将硬币翻转指定次数：
- en: '[PRE53]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The outcomes are accumulated in the `outcomes` object of type `Stats`, which
    looks like this:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 结果累积在类型为 `Stats` 的 `outcomes` 对象中，它看起来像这样：
- en: '[PRE54]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'A client can call `flip_coins()` multiple times using the same `Stats` instance,
    and the outcomes of the flipping are added to the `Stats`:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端可以多次调用 `flip_coins()`，使用相同的 `Stats` 实例，翻转的结果将被添加到 `Stats` 中：
- en: '[PRE55]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Let''s say we want to parallelize the implementation of `flip_coin()` and have
    multiple threads mutate the `Stats` object. In addition, we can assume the following:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要并行化 `flip_coin()` 的实现，并让多个线程改变 `Stats` 对象。此外，我们可以假设以下情况：
- en: The `Stats` struct cannot be changed (maybe it's from a third-party library).
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Stats` 结构体无法更改（可能来自第三方库）。'
- en: We want the client to be unaware of the fact that our utility function `flip_coin()`
    is concurrent; that is, the concurrency of the `flip_coin()` function should be
    completely *transparent to the caller*.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望客户端不知道我们的实用函数 `flip_coin()` 是并发的；也就是说，`flip_coin()` 函数的并发应该对调用者完全透明。
- en: 'For this example, we will reuse our previously defined function for generating
    random numbers:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将重用我们之前定义的用于生成随机数的函数。
- en: '[PRE56]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now we are ready to define our `flip_coin()` function, which will use two threads
    to flip a coin `n` number of times:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备定义我们的 `flip_coin()` 函数，它将使用两个线程来翻转硬币 `n` 次：
- en: '[PRE57]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Both threads will update the non-atomic outcome object whenever they have tossed
    a coin. Instead of using a `std::mutex`, we will create two `std::atomic_ref<int>`
    variables that atomically update the members of the outcome object. It is important
    to remember that in order to protect the heads and tails counters from data races,
    all concurrent accesses to the counters need to be protected using `std::atomic_ref`.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 两个线程都会在抛硬币后更新非原子结果对象。我们将创建两个 `std::atomic_ref<int>` 变量，用于原子更新结果对象的成员，而不是使用 `std::mutex`。重要的是要记住，为了保护头和尾计数器免受数据竞争的影响，所有对计数器的并发访问都需要使用
    `std::atomic_ref` 进行保护。
- en: 'The following small program demonstrates that the `flip_coin()` function can
    be called without any knowledge about the concurrent implementation of `flip_coin()`:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小程序演示了 `flip_coin()` 函数可以在不了解 `flip_coin()` 的并发实现的情况下被调用：
- en: '[PRE58]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Running this program on my machine produced the following output:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的机器上运行此程序产生了以下输出：
- en: '[PRE59]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This example concludes our section about the various atomic class templates
    in C++. Atomics have been part of the standard library since C++11 and have continued
    to evolve. C++20 introduced:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子结束了我们关于C++中各种原子类模板的部分。原子操作自C++11以来就已经成为标准库的一部分，并且不断发展。C++20引入了：
- en: The specialization `std::atomic<std::shared_ptr<T>>`
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特化`std::atomic<std::shared_ptr<T>>`
- en: Atomic references; that is, the `std::atomic_ref<T>` template
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原子引用；即`std::atomic_ref<T>`模板
- en: The wait and notify API, which is a lightweight alternative to using condition
    variables
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等待和通知API，这是使用条件变量的轻量级替代方案
- en: We will now move on to discuss the C++ memory model and how it relates to atomics
    and concurrent programming.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将继续讨论C++内存模型以及它与原子操作和并发编程的关系。
- en: The C++ memory model
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C++内存模型
- en: Why are we talking about the memory model of C++ in a chapter about concurrency?
    The memory model is closely related to concurrency since it defines how the reads
    and writes to the memory should be visible among threads. This is a rather complicated
    subject that touches on both compiler optimizations and multicore computer architecture.
    The good news, though, is that if your program is free from data races and you
    use the memory order that the atomics library provides by default, your concurrent
    program will behave according to an intuitive memory model that is easy to understand.
    Still, it is important to at least have an understanding of what the memory model
    is and what the default memory order guarantees.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么在并发章节中我们要谈论C++的内存模型？内存模型与并发密切相关，因为它定义了内存读写在线程之间如何可见。这是一个相当复杂的主题，涉及编译器优化和多核计算机架构。不过好消息是，如果你的程序没有数据竞争，并且使用原子库默认提供的内存顺序，你的并发程序将遵循一个直观易懂的内存模型。但是，至少了解内存模型是什么以及默认内存顺序保证是很重要的。
- en: 'The concepts covered in this section are thoroughly explained by Herb Sutter
    in his talks *Atomic Weapons: The C++ Memory Model and Modern Hardware 1 & 2*.
    The talks are freely available at [https://herbsutter.com/2013/02/11/atomic-weapons-the-c-memory-model-and-modern-hardware/](https://herbsutter.com/2013/02/11/atomic-weapons-the-c-memory-model-and-modern-hardware/)
    and are highly recommended if you need more depth on this subject.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分涵盖的概念由Herb Sutter在他的演讲*原子武器：C++内存模型和现代硬件1和2*中得到了深入解释。这些演讲可以在[https://herbsutter.com/2013/02/11/atomic-weapons-the-c-memory-model-and-modern-hardware/](https://herbsutter.com/2013/02/11/atomic-weapons-the-c-memory-model-and-modern-hardware/)上免费获取，并且强烈推荐如果你需要更深入地了解这个主题。
- en: Instruction reordering
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指令重新排序
- en: To understand the importance of the memory model, you first need some background
    about how the programs we write are actually executed.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 理解内存模型的重要性，首先需要了解我们编写的程序实际上是如何执行的一些背景知识。
- en: 'When we write and run a program, it would be reasonable to assume that the
    instructions in the source code will be executed in the same order as they appear
    in the source code. This is not true. The code we write will be optimized in multiple
    stages before it is finally executed. Both the compiler and the hardware will
    reorder instructions with the goal of executing the program more efficiently.
    This is not new technology: compilers have done this for a long time, and this
    is one reason why an optimized build runs faster than a non-optimized build. The
    compiler (and hardware) are free to reorder instructions as long as the reordering
    is not observable when running the program. The program runs *as if* everything
    happens in program order.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们编写和运行程序时，合理地假设源代码中的指令将按照它们在源代码中出现的顺序执行。这是不正确的。我们编写的代码将在最终执行之前经过多个阶段的优化。编译器和硬件都会重新排序指令，以更有效地执行程序。这并不是新技术：编译器长期以来一直在做这个，这也是为什么优化构建比非优化构建运行得更快的原因之一。编译器（和硬件）可以自由地重新排序指令，只要在运行程序时不可观察到重新排序。程序运行时*好像*一切都按照程序顺序发生。
- en: 'Let''s look at an example code snippet:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个代码片段的例子：
- en: '[PRE60]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Here, it is obvious that line number two and line number three could be swapped
    without introducing any observable effect:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，很明显第二行和第三行可以交换而不会引入任何可观察的效果：
- en: '[PRE61]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Here is another example, which is similar, but not identical, to the example
    from *Chapter 4*, *Data Structures*, where the compiler can optimize a cache-unfriendly
    version when iterating over a two-dimensional matrix:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个例子，类似但不完全相同于*第4章*，*数据结构*中的例子，编译器可以在遍历二维矩阵时优化一个不友好的缓存版本：
- en: '[PRE62]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'You saw in *Chapter 4*, *Data Structures*, that code similar to this produces
    a lot of cache misses, which hurts performance. A compiler is free to optimize
    this by reordering the `for` statements, like this:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 你在*第4章*，*数据结构*中看到，类似这样的代码会产生大量的缓存未命中，从而影响性能。编译器可以通过重新排序`for`语句来优化这个问题，就像这样：
- en: '[PRE63]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: There is no way to observe the difference between the two versions when executing
    the program, but the latter will run faster.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行程序时，无法观察到这两个版本之间的差异，但后者将运行得更快。
- en: Optimizations performed by the compiler and the hardware (including instruction
    pipelining, branch prediction, and cache hierarchies) are very complicated and
    constantly evolving technologies. Fortunately, all these transformations of the
    original program can be seen as re-orderings of reads and writes in the source
    code. This also means that it doesn't matter whether it is the compiler or some
    part of the hardware that performs the transformations. The important thing for
    C++ programmers to know is that the instructions can be re-ordered but without
    any observable effect.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器和硬件执行的优化（包括指令流水线、分支预测和缓存层次结构）是非常复杂且不断发展的技术。幸运的是，原始程序的所有这些转换都可以看作是源代码中读写的重新排序。这也意味着无论是编译器还是硬件的某个部分执行转换都无关紧要。对于C++程序员来说，重要的是知道指令可以被重新排序，但没有任何可观察的效果。
- en: If you have been trying to debug an optimized build of your program, you have
    probably noticed that it can be hard to step through it because of the re-orderings.
    So, by using a debugger, the re-orderings are in some sense observable, but they
    are not observable when running the program in a normal way.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您一直在尝试调试程序的优化版本，您可能已经注意到很难逐步执行，因为重新排序。因此，通过使用调试器，重新排序在某种意义上是可观察的，但在正常运行程序时是不可观察的。
- en: Atomics and memory orders
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原子操作和内存顺序
- en: When writing single-threaded programs in C++, there is no risk of data races
    occurring. We can write our programs happily without being aware of instruction
    re-orderings. However, when it comes to shared variables in multi-threaded programs,
    it is a completely different story. The compiler (and hardware) does all its optimizations
    based on what is true and observable for *one* thread only. The compiler cannot
    know what other threads are able to observe through shared variables, so it is
    our job as programmers to inform the compiler of what re-orderings are allowed.
    In fact, that is exactly what we are doing when we are using an atomic variable
    or a mutex to protect us from data races.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在C++中编写单线程程序时，不会发生数据竞争的风险。我们可以快乐地编写程序，而不必关心指令重新排序。然而，在多线程程序中涉及共享变量时，情况完全不同。编译器（和硬件）基于仅对*一个*线程为真和可观察的内容进行所有优化。编译器无法知道其他线程通过共享变量能观察到什么，因此我们作为程序员的工作就是告知编译器允许进行哪些重新排序。事实上，当我们使用原子变量或互斥锁保护我们免受数据竞争时，这正是我们所做的。
- en: When protecting a critical section with a mutex, it is guaranteed that only
    the thread that currently owns the lock can execute the critical section. But,
    the mutex is also creating memory fences around the critical section to inform
    the system that certain re-orderings are not allowed at the critical section boundaries.
    When acquiring the lock, an `acquire` fence is added, and when releasing the lock,
    a `release` fence is added.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 当用互斥锁保护临界区时，可以保证只有当前拥有锁的线程才能执行临界区。但是，互斥锁还在临界区周围创建内存栅栏，以通知系统在临界区边界不允许某些重新排序。在获取锁时，会添加一个“获取”栅栏，在释放锁时，会添加一个“释放”栅栏。
- en: 'I will demonstrate this with an example. Imagine that we have four instructions:
    **i1**, **i2**, **i3**, and **i4**. There is no dependency between each one, so
    the system could reorder the instructions arbitrarily without any observable effect.
    The instructions i2 and i3 are using shared data and are, therefore, critical
    sections that needs to be protected by a mutex. After adding the `acquire` and
    `release` of the mutex lock, there are now some re-orderings that are no longer
    valid. Obviously, we cannot move the instructions that are part of the critical
    section outside of the critical section, or they will no longer be protected by
    the mutex. The one-way fences ensure that no instructions can be moved out from
    the critical section. The i1 instruction could be moved inside the critical section
    by passing the acquire fence, but not beyond the release fence. The i4 instruction
    could also be moved inside the critical section by passing the release fence,
    but not beyond the acquire fence.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 我将用一个例子来证明这一点。假设我们有四条指令：**i1**，**i2**，**i3**和**i4**。它们之间没有依赖关系，因此系统可以任意重新排序指令而不会产生可观察的影响。指令i2和i3使用共享数据，因此它们是需要通过互斥锁保护的临界区。在添加互斥锁的“获取”和“释放”后，现在有一些重新排序不再有效。显然，我们不能将临界区的指令移出临界区，否则它们将不再受互斥锁的保护。单向栅栏确保没有指令可以从临界区移出。i1指令可以通过获取栅栏移入临界区，但不能超过释放栅栏。i4指令也可以通过释放栅栏移入临界区，但不能超过获取栅栏。
- en: 'The following figure shows how one-way fences limit the reordering of instructions.
    No read or write instructions can pass above the acquire fence, and nothing can
    pass below the release fence:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了单向栅栏如何限制指令的重新排序。没有读取或写入指令可以通过获取栅栏上方，也没有任何指令可以通过释放栅栏下方：
- en: '![](img/B15619_11_14.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_14.png)'
- en: 'Figure 11.14: One-way fences limit the reordering of the instructions'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14：单向栅栏限制指令的重新排序
- en: When acquiring a mutex, we are creating an acquire memory fence. It tells the
    system that no memory accesses (reads or writes) can be moved above the line where
    the acquire fence is located. It is possible for the system to move the i4 instruction
    above the release fence beyond the i3 and i2 instructions, but no further than
    that because of the acquire fence.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取互斥锁时，我们创建了一个获取内存栅栏。它告诉系统不能将内存访问（读取或写入）移动到获取栅栏所在的线以上。系统可以将i4指令移动到释放栅栏之上，超过i3和i2指令，但不能超过获取栅栏。
- en: 'Now, let''s have a look at atomic variables instead of mutexes. When we use
    a shared atomic variable in our program, it gives us two things:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看原子变量而不是互斥锁。当我们在程序中使用共享原子变量时，它给我们两件事：
- en: '**Protection against torn writes**: The atomic variable is always updated atomically
    so there is no way a reader can read a partially written value.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止写入时出现撕裂**：原子变量始终以原子方式更新，因此读取者无法读取部分写入的值。'
- en: '**Synchronization of memory by adding sufficient memory fences**: This prevents
    certain instruction re-orderings to guarantee a certain memory order specified
    by the atomic operations.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过添加足够的内存栅栏同步内存**：这可以防止某些指令重新排序，以保证原子操作指定的特定内存顺序。'
- en: The C++ memory model guarantees **sequential consistency** if our program is
    free from data races and we use the default memory order when using atomics. So,
    what is sequential consistency? Sequential consistency guarantees that the result
    of the execution is the same as if the operations were executed in the order specified
    by the original program. The interleaving of instructions among threads is arbitrary;
    that is, we have no control over the scheduling of the threads. This may sound
    complicated at first, but it is probably the way you already think about how a
    concurrent program is executed.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的程序没有数据竞争，并且在使用原子操作时使用默认的内存顺序，C++内存模型会保证**顺序一致性**。那么，什么是顺序一致性？顺序一致性保证执行的结果与按照原始程序指定的顺序执行操作时的结果相同。线程之间指令的交错是任意的；也就是说，我们无法控制线程的调度。这一开始可能听起来很复杂，但这可能是你已经对并发程序的执行方式有所了解的方式。
- en: The downside with sequential consistency is that it can hurt performance. It
    is, therefore, possible to use atomics with a relaxed memory model instead. This
    means that you only get the protection against torn writes, but not the memory
    order guarantees provided by sequential consistency.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序一致性的缺点是可能会影响性能。因此，可以使用松散的内存模型来代替原子操作。这意味着你只能获得对撕裂写入的保护，而无法获得顺序一致性提供的内存顺序保证。
- en: I strongly advise you against using anything else except the default sequential
    consistency memory order, unless you have a very thorough understanding of the
    effects a weaker memory model can introduce.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议你除了默认的顺序一致性内存顺序之外，不要使用其他任何东西，除非你非常了解更弱的内存模型可能引入的影响。
- en: We will not discuss relaxed memory order any further here because it is beyond
    the scope of this book. But as a side note, you may be interested to know that
    the reference counter in a `std::shared_ptr` uses a relaxed model when incrementing
    the counter (but not when decrementing the counter). This is the reason why the
    `std::shared_ptr` member function `use_count()` only reports the approximate number
    of actual references when it is used in a multi-threaded environment.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里进一步讨论松散的内存顺序，因为这超出了本书的范围。但值得一提的是，你可能会对知道`std::shared_ptr`中的引用计数器在增加计数时使用了松散模型（但在减少计数时没有使用）。这就是为什么在多线程环境中使用`std::shared_ptr`成员函数`use_count()`时，它只会报告大约的实际引用数量。
- en: One area where the memory model and atomics are highly relevant is lock-free
    programming. The following section will give you a taste of what lock-free programming
    is and some of its applications.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 内存模型和原子操作非常相关的一个领域是无锁编程。接下来的部分将让你对无锁编程有所了解，并介绍一些应用场景。
- en: Lock-free programming
  id: totrans-415
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无锁编程
- en: Lock-free programming is hard. We will not spend a lot of time discussing lock-free
    programming in this book, but instead I will provide you with an example of how
    a very simple lock-free data structure could be implemented. There is a great
    wealth of resources — on the web and in books (such as the Anthony Williams book
    mentioned earlier) — dedicated to lock-free programming that will explain the
    concepts you need to understand before writing your own lock-free data structures.
    Some concepts you might have heard of, such as **compare-and-swap** (**CAS**)
    and the ABA problem, will not be further discussed in this book.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 无锁编程很难。我们不会在本书中花费很多时间讨论无锁编程，而是会为你提供一个非常简单的无锁数据结构的示例。有很多资源（网上和书籍中，比如之前提到的Anthony
    Williams的书）专门讨论无锁编程，这些资源会解释在编写自己的无锁数据结构之前需要理解的概念。一些你可能听说过的概念，比如**比较和交换**（**CAS**）和ABA问题，在本书中不会进一步讨论。
- en: 'Example: A lock-free queue'
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：无锁队列
- en: Here, you are going to see an example of a lock-free queue, which is a relatively
    simple but useful lock-free data structure. Lock-free queues can be used for one-way
    communication with threads that cannot use locks to synchronize access to shared
    data.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你将看到一个无锁队列的示例，这是一个相对简单但有用的无锁数据结构。无锁队列可用于与无法使用锁来同步对共享数据的访问的线程进行单向通信。
- en: 'Its implementation is straightforward because of the limited requirements:
    it only supports *one reader* thread and *one writer* thread. The capacity of
    the queue is also fixed and cannot change during runtime.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对队列的要求有限，它只支持*一个读取*线程和*一个写入*线程。队列的容量也是固定的，在运行时无法更改。
- en: A lock-free queue is an example of a component that might be used in environments
    where exceptions are typically abandoned. The queue that follows is therefore
    designed without exceptions, which makes the API differ from other examples in
    this book.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 无锁队列是一个可能在通常放弃异常的环境中使用的组件的示例。因此，后面的队列设计中没有异常，这使得API与本书中其他示例不同。
- en: 'The class template `LockFreeQueue<T>` has the following public interface:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 类模板`LockFreeQueue<T>`具有以下公共接口：
- en: '`push()`: Adds an element to the queue and returns `true` on success. This
    function must only be called by the (one and only) *writer thread*. To avoid unnecessary
    copying when the client provides an rvalue, `push()` overloads on `const T&` and
    `T&&.`This technique was also used in the `BoundedBuffer` class presented earlier
    in this chapter.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push()`: 将一个元素添加到队列中，并在成功时返回`true`。这个函数只能被（唯一的）*写入线程*调用。为了避免在客户端提供右值时进行不必要的复制，`push()`重载了`const
    T&`和`T&&`。这种技术也在本章前面介绍的`BoundedBuffer`类中使用过。'
- en: '`pop()`: Returns an `std::optional<T>` with the front element of the queue
    unless the queue is empty. This function must only be called by the (one and only)
    *reader thread*.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pop()`: 返回一个`std::optional<T>`，其中包含队列的第一个元素，除非队列为空。这个函数只能被（唯一的）*读取线程*调用。'
- en: '`size()`: Returns the current size of the queue. This function can be called
    by *both threads* concurrently.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size()`: 返回队列的当前大小。这个函数可以被*两个线程*同时调用。'
- en: 'The following is the complete implementation of the queue:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是队列的完整实现：
- en: '[PRE64]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The only data member that needs atomic access is the `size_` variable. The `read_pos_`
    member is only used by the reader thread, and the `write_pos_` is only used by
    the writer thread. So what about the buffer of type `std::array`? It is mutable
    and accessed by both threads? Doesn't that require synchronization? Since the
    algorithm ensures that the two threads are never accessing the same element in
    the array concurrently, C++ guarantees that individual elements in an array can
    be accessed without data races. It doesn't matter how small the elements are;
    even a `char` array holds this guarantee.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一需要原子访问的数据成员是`size_`变量。`read_pos_`成员仅由读取线程使用，`write_pos_`仅由写入线程使用。那么`std::array`类型的缓冲区呢？它是可变的，并且被两个线程访问？这不需要同步吗？由于算法确保两个线程永远不会同时访问数组中的相同元素，C++保证可以在没有数据竞争的情况下访问数组中的单个元素。元素有多小都没关系；即使是`char`数组也具有这一保证。
- en: When can a non-blocking queue like this be useful? One example is in audio programming,
    when there is a UI running on the main thread that needs to send or receive data
    from a real-time audio thread, which cannot block under any circumstances. The
    real-time thread cannot use mutex locks, allocate/free memory, or do anything
    else that may cause the thread to wait on threads with lower priority. Lock-free
    data structures are required for scenarios like these.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这种非阻塞队列何时会有用？一个例子是在音频编程中，当主线程上运行着一个UI需要与实时音频线程发送或接收数据时，实时线程在任何情况下都不能阻塞。实时线程不能使用互斥锁，分配/释放内存，或执行任何可能导致线程等待低优先级线程的操作。这些情况下需要无锁数据结构。
- en: 'Both the reader and the writer are lock-free in `LockFreeQueue`, so we could
    have two instances of the queue to communicate in both directions between the
    main thread and the audio thread, as the following figure demonstrates:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在`LockFreeQueue`中，读取器和写入器都是无锁的，因此我们可以有两个队列实例在主线程和音频线程之间双向通信，如下图所示：
- en: '![](img/B15619_11_15.png)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_11_15.png)'
- en: 'Figure 11.15: Using two lock-free queues to pass state between the main thread
    and a real-time audio thread'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15：使用两个无锁队列在主线程和实时音频线程之间传递状态
- en: As already mentioned, this book only scratches the surface of lock-free programming.
    It's time to end this chapter now with a few guidelines on performance when writing
    concurrent programs.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，本书只是浅尝辄止无锁编程的表面。现在是时候用一些关于编写并发程序时性能的指南来结束本章了。
- en: Performance guidelines
  id: totrans-433
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能指南
- en: I cannot stress enough the importance of having a concurrent program running
    *correctly* before trying to improve the performance. Also, before applying any
    of these guidelines related to performance, you first need to set up a reliable
    way of measuring what you are trying to improve.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 我无法强调在尝试提高性能之前，正确运行并发程序的重要性。此外，在应用与性能相关的任何指南之前，您首先需要建立一种可靠的方式来衡量您要改进的内容。
- en: Avoid contention
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免争用
- en: Whenever multiple threads are using shared data, there will be contention. Contention
    hurts performance and sometimes the overhead caused by contention can make a parallel
    algorithm work slower than a single-threaded alternative.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 每当多个线程使用共享数据时，就会发生争用。争用会影响性能，有时由争用引起的开销会使并行算法的工作速度比单线程替代方案更慢。
- en: Using a lock that causes a wait and a context switch is an obvious performance
    penalty, but what is not equally obvious is that both locks and atomics disable
    optimizations in the code generated by the compiler, and they do so at runtime
    when the CPU executes the code. This is necessary in order to guarantee sequential
    consistency. But remember, the solution to such problems is never to ignore synchronization
    and therefore introduce data races. Data races mean undefined behavior, and having
    a fast but incorrect program makes nobody happy.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 使用导致等待和上下文切换的锁是明显的性能惩罚，但同样不明显的是，锁和原子操作都会禁用编译器生成的代码中的优化，并且在CPU执行代码时会在运行时这样做。这是为了保证顺序一致性。但请记住，这类问题的解决方案绝不是忽略同步，从而引入数据竞争。数据竞争意味着未定义行为，拥有快速但不正确的程序不会让任何人满意。
- en: Instead, we need to minimize the time spent in critical sections. We can do
    that by entering a critical section less often, and by minimizing the critical
    section itself so that once we are in it, we leave it as soon as possible.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们需要尽量减少在关键部分的时间。我们可以通过更少地进入关键部分，并通过尽量减少关键部分本身来做到这一点，以便一旦进入关键部分，我们就尽快离开它。
- en: Avoid blocking operations
  id: totrans-439
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免阻塞操作
- en: To write a modern responsive UI application that always runs smoothly, it is
    absolutely necessary to never block the main thread for more than a few milliseconds.
    A smoothly running app updates its interface 60 times per second. This means that
    if you are doing something that blocks the UI thread for more than 16 ms, the
    FPS will drop.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 要编写一个现代响应式UI应用程序，始终保持流畅运行，绝对不能阻塞主线程超过几毫秒。一个流畅运行的应用程序每秒更新其界面60次。这意味着如果您正在做一些阻塞UI线程超过16毫秒的事情，FPS将会下降。
- en: You can design your internal APIs in an application with this in mind. Whenever
    you write a function that performs I/O or something else that might take more
    than a few milliseconds, it needs to be implemented as an asynchronous function.
    This pattern has become very common in iOS and Windows, where, for example, all
    network APIs have become asynchronous.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在设计应用程序的内部API时考虑这一点。每当编写执行I/O或可能需要超过几毫秒的其他操作的函数时，它需要被实现为异步函数。这种模式在iOS和Windows中变得非常普遍，例如，所有网络API都已变成异步。
- en: Number of threads/CPU cores
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程数/CPU核心数
- en: The more CPU cores a machine has, the more active running threads you can have.
    If you manage to split a sequential CPU-bound task into a parallel version, you
    can gain performance by having multiple cores working on the task in parallel.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 机器的CPU核心越多，您可以运行的活动线程就越多。如果您设法将顺序的CPU绑定任务拆分为并行版本，您可以通过多个核心并行处理任务来提高性能。
- en: Going from a single-threaded algorithm to an algorithm that can be run by two
    threads can, in the best-case scenario, double the performance. But, after adding
    more and more threads, you will eventually reach a limit when there is no more
    performance gain. Adding more threads beyond that limit will actually degrade
    performance since the overhead caused by context switching becomes more significant
    the more threads you add.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 从单线程算法转变为可以由两个线程运行的算法，在最佳情况下可能会使性能翻倍。但是，添加越来越多的线程后，最终会达到一个极限，此时不会再有性能增益。超过该极限添加更多线程实际上会降低性能，因为上下文切换引起的开销会随着添加的线程数量增加而变得更加显著。
- en: I/O-intensive tasks, for example, a web crawler that will spend a lot of time
    waiting for network data, require a lot of threads before reaching the limit where
    the CPU is oversubscribed. A thread that is waiting for I/O will most likely be
    switched out from the CPU to make room for other threads that are ready to execute.
    For CPU-bound tasks, there is usually no point in using more threads than there
    are cores on the machine.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，I/O密集型任务，例如等待网络数据的网络爬虫，在达到CPU过度订阅的极限之前需要大量线程。等待I/O的线程很可能会从CPU中切换出来，以为其他准备执行的线程腾出空间。对于CPU密集型任务，通常没有必要使用超过机器上核心数量的线程。
- en: Controlling the total number of threads in a big program can be hard. A good
    way of controlling the number of threads is to use a thread pool that can be sized
    to match the current hardware.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 控制大型程序中的线程总数可能很困难。控制线程数量的一个好方法是使用可以根据当前硬件大小调整大小的线程池。
- en: In *Chapter 14*, *Parallel Algorithms*, you will see examples of how to parallelize
    algorithms and how to tweak the amount of concurrency based on the number of CPU
    cores.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第14章*，*并行算法*中，您将看到如何并行化算法的示例，以及如何根据CPU核心数量调整并发量。
- en: Thread priorities
  id: totrans-448
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程优先级
- en: The priority of a thread affects how the thread is scheduled. A thread with
    high priority is likely to be scheduled more often than threads with lower priorities.
    Thread priorities are important for lowering the latency of tasks.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 线程的优先级会影响线程的调度。具有高优先级的线程可能比具有较低优先级的线程更频繁地被调度。线程优先级对降低任务的延迟很重要。
- en: Threads provided by the operating system usually have priorities. There is currently
    no way of setting the priority on a thread with the current C++ thread APIs. However,
    by using `std::thread::native_handle`, you can get a handle to the underlying
    operating system thread and use native APIs to set priorities.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统提供的线程通常具有优先级。目前，使用当前的C++线程API无法设置线程的优先级。但是，通过使用`std::thread::native_handle`，您可以获取到底层操作系统线程的句柄，并使用本机API来设置优先级。
- en: One phenomenon related to thread priorities that can hurt the performance, and
    should be avoided, is called **priority inversion**. It happens when a thread
    with high priority is waiting to acquire a lock that is currently held by a low-priority
    thread. Such dependencies hurt the high-priority thread, which is blocked until
    the next time the low-priority thread gets scheduled so that it can release the
    lock.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 与线程优先级相关的一种可能会影响性能并且应该避免的现象称为**优先级反转**。当一个具有高优先级的线程正在等待获取当前由低优先级线程持有的锁时，就会发生这种情况。这种依赖关系会影响高优先级线程，因为它被阻塞，直到下一次低优先级线程被调度以释放锁。
- en: For real-time applications, this is a big problem. In practice, it means that
    you cannot use locks to protect any shared resources that need to be accessed
    by real-time threads. A thread that produces real-time audio, for example, runs
    with the highest possible priority, and in order to avoid priority inversion,
    it is not possible for the audio thread to call any functions (including `std::malloc()`)
    that might block and cause a context switch.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实时应用程序来说，这是一个大问题。实际上，这意味着您不能使用锁来保护需要实时线程访问的任何共享资源。例如，生成实时音频的线程以最高可能的优先级运行，为了避免优先级反转，不可能让音频线程调用任何可能阻塞并引起上下文切换的函数（包括`std::malloc()`）。
- en: Thread affinity
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程亲和性
- en: Thread affinity makes it possible to give the scheduler hints about which threads
    could benefit from sharing the same CPU caches. In other words, this is a request
    to the scheduler that some threads should be executed on a particular core if
    possible, to minimize cache misses.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 线程亲和性使得调度程序可以提示哪些线程可以受益于共享相同的CPU缓存。换句话说，这是对调度程序的请求，如果可能的话，一些线程应该在特定的核心上执行，以最小化缓存未命中。
- en: Why would you want one thread to be executed on a particular core? The answer
    is (again) caching. Threads that operate on the same memory could benefit from
    running on the same core, and hence take advantage of warm caches. For the scheduler,
    this is just one of many parameters to take into account when assigning a thread
    to a core, so this is hardly any guarantee, but again, the behavior is very different
    among operating systems. Thread priorities, and even utilization of all cores
    (to avoid overheating), are one of the requirements that need to be taken into
    account by a modern scheduler.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要让一个线程在特定的核心上执行？答案是（再次）缓存。在相同内存上操作的线程可能会受益于在同一核心上运行，从而利用热缓存。对于调度程序来说，这只是分配线程到核心时需要考虑的众多参数之一，因此这几乎不是任何保证，但是，操作系统之间的行为差异非常大。线程优先级，甚至利用所有核心（以避免过热）是现代调度程序需要考虑的要求之一。
- en: 'It is not possible to set thread affinity in a portable way with the current
    C++ APIs, but most platforms support some way of setting an affinity mask on a
    thread. In order to access platform-specific functionality, you need to get a
    handle on the native thread. The example that follows demonstrates how to set
    the thread affinity mask on Linux:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 使用当前的C++ API无法以便携的方式设置线程亲和性，但大多数平台支持在线程上设置亲和性掩码的某种方式。为了访问特定于平台的功能，您需要获取本机线程的句柄。接下来的示例演示了如何在Linux上设置线程亲和性掩码：
- en: '[PRE65]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Note, this is not portable C++, but it is likely that you need to do some non-portable
    configuration of threads if you are doing performance-critical concurrency programming.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这不是便携式的C++，但如果您正在进行性能关键的并发编程，很可能需要对线程进行一些不便携式的配置。
- en: False sharing
  id: totrans-459
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚假共享
- en: '**False sharing**, or destructive interference, can degrade performance very
    significantly. It occurs when two threads use some data (that is not logically
    shared between the threads) but happen to be located in the same cache line. Imagine
    what will happen if the two threads are executing on different cores and constantly
    updating the variable residing on the shared cache line. The threads will invalidate
    the cache line for each other, although there is no true sharing of data between
    the threads.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '**虚假共享**，或者破坏性干扰，可能会严重降低性能。当两个线程使用一些数据（这些数据在逻辑上不共享）但碰巧位于同一个缓存行时，就会发生虚假共享。想象一下，如果两个线程在不同的核心上执行，并且不断更新位于共享缓存行上的变量，会发生什么。尽管线程之间没有真正共享数据，但它们会相互使缓存行失效。'
- en: False sharing will most likely occur when using global data or dynamically allocated
    data that is shared between threads. An example where false sharing is likely
    to occur is when allocating an array that is shared between threads, but each
    thread is only using a single element of the array.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 虚假共享很可能发生在使用全局数据或动态分配的数据在线程之间共享时。一个可能发生虚假共享的例子是分配一个数组，该数组在线程之间共享，但每个线程只使用数组的一个元素。
- en: 'The solution to this problem is to pad each element in the array so that two
    adjacent elements cannot reside on the same cache line. Since C++17, there is
    a portable way of doing this using the `std::hardware_destructive_interference_size`
    constant defined in `<new>` in combination with the `alignas` specifier. The following
    example demonstrates how to create an element that prevents false sharing:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是对数组中的每个元素进行填充，以便相邻的两个元素不能位于同一个缓存行上。自C++17以来，有一种便携式的方法可以使用`<new>`中定义的`std::hardware_destructive_interference_size`常量和`alignas`说明符来实现这一点。以下示例演示了如何创建一个元素来防止虚假共享：
- en: '[PRE66]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: The elements in the vector are now guaranteed to reside on separate cache lines.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，向量中的元素被保证位于不同的缓存行上。
- en: Summary
  id: totrans-465
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you have seen how to create programs that can execute multiple
    threads concurrently. We also covered how to avoid data races by protecting critical
    sections with locks or by using atomics. You learned that C++20 comes with some
    useful synchronization primitives: latches, barriers, and semaphores. We then
    looked into execution order and the C++ memory model, which becomes important
    to understand when writing lock-free programs. You also discovered that immutable
    data structures are thread-safe. The chapter ended with some guidelines for improving
    performance in concurrent applications.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经学会了如何创建可以同时执行多个线程的程序。我们还介绍了如何通过使用锁或原子操作来保护关键部分，以避免数据竞争。您了解到C++20带来了一些有用的同步原语：屏障、障碍和信号量。然后我们研究了执行顺序和C++内存模型，在编写无锁程序时理解这些内容变得很重要。您还发现了不可变数据结构是线程安全的。本章最后介绍了一些改进并发应用程序性能的指南。
- en: The next two chapters are dedicated to a completely new C++20 feature called
    coroutines, which allows us to write asynchronous code in a sequential style.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两章专门介绍了一个全新的C++20特性，称为协程，它允许我们以顺序方式编写异步代码。
