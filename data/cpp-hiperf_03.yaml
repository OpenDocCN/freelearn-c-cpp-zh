- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Analyzing and Measuring Performance
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析和测量性能
- en: Since this is a book about writing C++ code that runs efficiently, we need to
    cover some basics regarding how to measure software performance and estimate algorithmic
    efficiency. Most of the topics in this chapter are not specific to C++ and can
    be used whenever you are facing a problem where performance is an issue.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一本关于编写高效运行的C++代码的书，我们需要涵盖一些关于如何衡量软件性能和估算算法效率的基础知识。本章大部分主题并不特定于C++，在面对性能问题时都可以使用。
- en: You will learn how to estimate algorithmic efficiency using big O notation.
    This is essential knowledge when choosing algorithms and data structures from
    the C++ standard library. If you are new to big O notation, this part might take
    some time to digest. But don't give up! This is a very important topic to grasp
    in order to understand the rest of the book, and, more importantly, to become
    a performance-aware programmer. If you want a more formal or more practical introduction
    to these concepts, there are plenty of books and online resources dedicated to
    this topic. On the other hand, if you have already mastered big O notation and
    know what amortized time complexity is, you can skim the next section and go to
    the later parts of this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 您将学习如何使用大O符号估算算法效率。在选择C++标准库中的算法和数据结构时，这是必不可少的知识。如果您对大O符号不熟悉，这部分可能需要一些时间来消化。但不要放弃！这是一个非常重要的主题，以便理解本书的其余部分，更重要的是，成为一个注重性能的程序员。如果您想要更正式或更实用的介绍这些概念，有很多专门讨论这个主题的书籍和在线资源。另一方面，如果您已经掌握了大O符号并知道摊销时间复杂度是什么，您可以略过下一节，转到本章的后面部分。
- en: 'This chapter includes sections on:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括以下部分：
- en: Estimating algorithmic efficiency using big O notation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用大O符号估算算法效率
- en: A suggested workflow when optimizing code so that you don't spend time fine-tuning
    code without good reason
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化代码的建议工作流程，这样您不会在没有充分理由的情况下花费时间微调代码
- en: CPU profilers—what they are and why you should use them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU性能分析器——它们是什么以及为什么你应该使用它们
- en: Microbenchmarking
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微基准测试
- en: Let's begin by taking a look at how to estimate algorithmic efficiency using
    big O notation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一下如何使用大O符号来估算算法效率。
- en: Asymptotic complexity and big O notation
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 渐近复杂度和大O符号
- en: There is usually more than one way to solve a problem, and if efficiency is
    a concern, you should first focus on high-level optimizations by choosing the
    right algorithms and data structures. A useful way of evaluating and comparing
    algorithms is by analyzing their asymptotic computational complexity—that is,
    analyzing how the running time or memory consumption grows when the size of the
    input increases. In addition, the C++ standard library specifies the asymptotic
    complexity for all containers and algorithms, which means that a basic understanding
    of this topic is a must if you are using this library. If you already have a good
    understanding of algorithm complexity and the big O notation, you can safely skip
    this section.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通常解决问题的方法不止一种，如果效率是一个问题，您应该首先专注于通过选择正确的算法和数据结构进行高级优化。评估和比较算法的一个有用方法是分析它们的渐近计算复杂性——也就是分析输入大小增加时运行时间或内存消耗的增长情况。此外，C++标准库为所有容器和算法指定了渐近复杂度，这意味着如果您使用这个库，对这个主题的基本理解是必须的。如果您已经对算法复杂度和大O符号有很好的理解，可以安全地跳过本节。
- en: 'Let''s start off with an example. Suppose we want to write an algorithm that
    returns `true` if it finds a specific key in an array, or `false` otherwise. In
    order to find out how our algorithm behaves when passed different sizes of the
    array, we would like to analyze the running time of this algorithm as a function
    of its input size:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个例子开始。假设我们想编写一个算法，如果在数组中找到特定的键，则返回`true`，否则返回`false`。为了找出我们的算法在不同大小的数组上的行为，我们希望分析这个算法的运行时间作为其输入大小的函数：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The algorithm is straightforward. It iterates over the elements in the array
    and compares each element with the key. If we are lucky, we find the key at the
    beginning of the array and it returns immediately, but we might loop through the
    entire array without finding the key at all. This would be the worst case for
    the algorithm, and in general, that is the case we want to analyze.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法很简单。它遍历数组中的元素，并将每个元素与键进行比较。如果我们幸运的话，在数组的开头找到键并立即返回，但我们也可能在整个数组中循环而根本找不到键。这将是算法的最坏情况，通常情况下，这是我们想要分析的情况。
- en: 'But what happens with the running time when we increase the input size? Say
    we double the size of the array. Well, in the worst case, we need to compare all
    elements in the array that would double the running time. There seems to be a linear
    relationship between the input size and the running time. We call this a linear
    growth rate:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当我们增加输入大小时，运行时间会发生什么变化？假设我们将数组的大小加倍。嗯，在最坏的情况下，我们需要比较数组中的所有元素，这将使运行时间加倍。输入大小和运行时间之间似乎存在线性关系。我们称这为线性增长率。
- en: '![](img/B15619_03_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_01.png)'
- en: 'Figure 3.1: Linear growth rate'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：线性增长率
- en: 'Now consider the following algorithm:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑以下算法：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We are comparing points instead of integers and we are using an index with the
    subscript operator to access each element. How is the running time affected by
    these changes? The absolute running time is probably higher compared to the first
    algorithm since we are doing more work—for example, the comparison of points involves
    two integers instead of just one for each element in the array. However, at this
    stage, we are interested in the growth rate the algorithm exhibits, and if we
    plot the running time against the input size, we will still end up with a straight
    line, as shown in the preceding figure.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较的是点而不是整数，并且我们使用下标运算符的索引来访问每个元素。这些变化如何影响运行时间？绝对运行时间可能比第一个算法高，因为我们做了更多的工作——例如，比较点涉及两个整数，而不是数组中每个元素的一个整数。然而，在这个阶段，我们对算法表现的增长率感兴趣，如果我们将运行时间绘制成输入大小的函数，我们仍然会得到一条直线，如前图所示。
- en: 'As the last example of searching for integers, let''s see whether we can find
    a better algorithm if we assume that the elements in the array are sorted. Our
    first algorithm would work regardless of the order of the elements, but if we
    know that they are sorted, we can use a binary search. It works by looking at
    the element in the middle to determine whether it should continue searching in
    the first or second half of the array. For simplicity, the indexes `high`, `low`,
    and `mid` are of type `int` and requires a `static_cast`. A better option would
    be to use iterators that will be covered in succeeding chapters. Here follows
    the algorithm:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作为搜索整数的最后一个例子，让我们看看是否可以找到更好的算法，如果我们假设数组中的元素是排序的。我们的第一个算法将在元素的顺序无关紧要的情况下工作，但是如果我们知道它们是排序的，我们可以使用二分搜索。它通过查看中间的元素来确定它是否应该继续在数组的第一半或第二半中搜索。为简单起见，索引`high`，`low`和`mid`的类型为`int`，需要`static_cast`。更好的选择是使用迭代器，这将在后续章节中介绍。以下是算法：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, this algorithm is harder to get correct than a simple linear
    scan. It looks for the specified key by *guessing* that it's in the middle of
    the array. If it's not, it compares the key with the element in the middle to
    decide which half of the array it should keep looking for the key in. So, in each
    iteration, it cuts the array in half.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这个算法比简单的线性扫描更难正确实现。它通过*猜测*数组中间的元素来寻找指定的键。如果不是，它将比较键和中间的元素，以决定应该在数组的哪一半中继续寻找键。因此，在每次迭代中，它将数组减半。
- en: 'Assume we called `binary_search()` with an array containing 64 elements. In
    the first iteration we reject 32 elements, in the next iteration we reject 16
    elements, in the next iteration we reject 8 elements, and so on, until there are
    no more elements to compare or until we find the key. For an input size of 64,
    there will be, at most, 7 loop iterations. What if we *double the input size*
    to 128? Since we halve the size in each iteration, it means that we only need
    *one more loop iteration*. Clearly, the growth rate is no longer linear—it''s
    actually logarithmic. If we measure the running time of `binary_search()`, we
    will see that the growth rate looks similar to the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用包含64个元素的数组调用`binary_search()`。在第一次迭代中，我们拒绝32个元素，在下一次迭代中，我们拒绝16个元素，在下一次迭代中，我们拒绝8个元素，依此类推，直到没有更多元素可以比较，或者直到我们找到键。对于输入大小为64，最多将有7次循环迭代。如果我们将输入大小*加倍*到128呢？由于我们在每次迭代中将大小减半，这意味着我们只需要*再进行一次循环迭代*。显然，增长率不再是线性的——实际上是对数的。如果我们测量`binary_search()`的运行时间，我们将看到增长率看起来类似于以下内容：
- en: '![](img/B15619_03_02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_02.png)'
- en: 'Figure 3.2: Logarithmic growth rate'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：对数增长率
- en: 'On my machine, a quick timing of the three algorithms repeatedly called 10,000
    times with various input sizes (*n*) produced the results shown in the following
    table:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的机器上，对三种算法进行快速计时，每次调用10,000次，不同输入大小（*n*）产生了以下表中显示的结果：
- en: '| Algorithm | n = 10 | n = 1,000 | n = 100,000 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | n = 10 | n = 1,000 | n = 100,000 |'
- en: '| Linear search with `int` | 0.04 ms | 4.7 ms | 458 ms |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 使用`int`的线性搜索 | 0.04毫秒 | 4.7毫秒 | 458毫秒 |'
- en: '| Linear search with `Point` | 0.07 ms | 6.7 ms | 725 ms |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 使用`Point`的线性搜索 | 0.07毫秒 | 6.7毫秒 | 725毫秒 |'
- en: '| Binary search with `int` | 0.03 ms | 0.08 ms | 0.16 ms |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 使用`int`的二分搜索 | 0.03毫秒 | 0.08毫秒 | 0.16毫秒 |'
- en: 'Table 3.1: Comparison of different versions of search algorithms'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1：不同版本搜索算法的比较
- en: Comparing algorithms 1 and 2, we can see that comparing points instead of integers
    takes more time, but they are still in the same order of magnitude even when the
    input size increases. However, if we compare all three algorithms when the input
    size increases, what really matters is the growth rate the algorithm exhibits.
    By exploiting the fact that the array was sorted, we could implement the search
    function with very few loop iterations. For large arrays, a binary search is practically
    free compared to linearly scanning the array.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 比较算法1和2，我们可以看到，比较点而不是整数需要更多时间，但即使输入大小增加，它们仍然处于相同数量级。然而，当输入大小增加时，比较所有三种算法时，真正重要的是算法表现出的增长率。通过利用数组已排序的事实，我们可以用很少的循环迭代来实现搜索功能。对于大数组，与线性扫描数组相比，二分搜索实际上是免费的。
- en: It's usually not a good idea to spend time tuning your code before you are certain
    that you have chosen the correct algorithms and data structures for your problem.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定选择正确的算法和数据结构之前，花时间调整代码通常不是一个好主意。
- en: Wouldn't it be nice if we could express the growth rate of algorithms in a way
    that would help us decide which algorithm to use? Here is where the big O notation
    comes in handy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能以一种有助于我们决定使用哪种算法的方式来表达算法的增长率，那不是很好吗？这就是大O符号表示法派上用场的地方。
- en: 'Here follows an informal definition:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个非正式的定义：
- en: If *f(n)* is a function that specifies the running time of an algorithm with
    input size *n*, we say that *f(n)* is *O(g(n))* if there is a constant *k* such
    that ![](img/B15619_03_001.png).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*f(n)*是一个指定算法在输入大小*n*的运行时间的函数，我们说*f(n)*是*O(g(n))*，如果存在一个常数*k*，使得![](img/B15619_03_001.png)。
- en: This means that we could say that the time complexity of `linear_search()` is
    *O(n)*, for both versions (the one that operates with integers and the one that
    operates with points), whereas the time complexity of `binary_search()` is *O(log
    n)* or big O of *log n*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以说`linear_search()`的时间复杂度是*O(n)*，对于两个版本（一个操作整数，一个操作点），而`binary_search()`的时间复杂度是*O(log
    n)*或者*O(log n)*的大O。
- en: In practice, when we want to find the big O of a function, we can do that by
    eliminating all terms except the one with the largest growth rate and then remove
    any constant factors. For example, if we have an algorithm with a time complexity
    described by *f(n) = 4n*² *+ 30n + 100*, we pick out the term with the highest
    growth rate, 4*n*². Next, we remove the constant factor of 4 and end up with *n*²,
    which means that we can say that our algorithm runs in *O(n*²*)*. Finding the
    time complexity of an algorithm can be hard, but the more you start thinking of
    it while writing code, the easier it will get. For the most part, it's enough
    to keep track of loops and recursive functions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当我们想要找到一个函数的大O时，我们可以通过消除除了具有最大增长率的项之外的所有项，然后去掉任何常数因子来做到这一点。例如，如果我们有一个时间复杂度由*f(n)
    = 4n*² *+ 30n + 100*描述的算法，我们挑出具有最高增长率的项，4*n*²。接下来，我们去掉常数因子4，最终得到*n*²，这意味着我们可以说我们的算法运行在*O(n*²*)*。找到算法的时间复杂度可能很难，但是当你在编写代码时开始思考它时，它会变得更容易。在大多数情况下，跟踪循环和递归函数就足够了。
- en: 'Let''s try to find the time complexity of the following sorting algorithm:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着找出以下排序算法的时间复杂度：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The input size is the size of the array. The running time could be estimated
    approximately by looking at the loops that iterate over all elements. First, there
    is an outer loop iterating over *n - 1* elements. The inner loop is different:
    the first time we reach the `while`-loop, `j` is 1 and the loop only runs one
    iteration. On the next iteration, `j` starts at 2 and decreases to 0\. For each
    iteration in the outer `for`-loop, the inner loop needs to do more and more work.
    Finally, `j` starts at *n - 1*, which means that we have, in the worst case, executed
    `swap()` *1 + 2 + 3 + ... + (n - 1)* times. We can express this in terms of *n*
    by noting that this is an arithmetic series. The sum of the series is:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小是数组的大小。通过查看迭代所有元素的循环，可以大致估计运行时间。首先，有一个迭代*n - 1*个元素的外部循环。内部循环不同：第一次到达`while`循环时，`j`为1，循环只运行一次。在下一次迭代中，`j`从2开始减少到0。对于外部`for`循环的每次迭代，内部循环需要做更多的工作。最后，`j`从*n
    - 1*开始，这意味着在最坏的情况下，我们执行了`swap()`*1 + 2 + 3 + ... + (n - 1)*次。我们可以通过注意到这是一个等差数列来用*n*来表示这一点。数列的和是：
- en: '![](img/B15619_03_002.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_002.png)'
- en: 'So, if we set *k = (n - 1)*, the time complexity of the sorting algorithm is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们设*k = (n - 1)*，排序算法的时间复杂度是：
- en: '![](img/B15619_03_003.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_003.png)'
- en: We can now find the big O of this function by first eliminating all terms except
    the one with the largest growth rate, which leaves us with *(1/2)n*². After that,
    we remove the constant *1/2* and conclude that the running time of the sorting
    algorithm is *O(n*²*)*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过首先消除除了具有最大增长率的项之外的所有项来找到这个函数的大O，这让我们得到了*(1/2)n*²。之后，我们去掉常数*1/2*，得出排序算法的运行时间是*O(n*²*)*。
- en: Growth rates
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增长率
- en: 'As stated previously, the first step in finding the big O of a complexity function
    is to remove all terms except the one with the highest growth rate. To be able
    to do that, we must know the growth rate of some common functions. In the following
    figure, I have plotted some of the most common functions:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，找到复杂度函数的大O的第一步是消除除了具有最高增长率的项之外的所有项。为了能够做到这一点，我们必须知道一些常见函数的增长率。在下图中，我画出了一些最常见的函数：
- en: '![](img/B15619_03_03.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_03.png)'
- en: 'Figure 3.3: Comparison of growth rate functions'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：增长率函数的比较
- en: 'The growth rates are independent of machine or coding style and so on. When
    the growth rates differ between two algorithms, the one with the slowest growth
    rate will always win when the input size gets sufficiently large. Let''s see what
    happens with the running time for different growth rates if we assume that it
    takes 1 ms to perform 1 unit of work. The following table lists the growth function,
    its common name, and different input sizes, *n*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 增长率与机器或编码风格等无关。当两个算法之间的增长率不同时，当输入大小足够大时，增长率最慢的算法将始终获胜。让我们看看不同增长率的运行时间会发生什么，假设执行1单位的工作需要1毫秒。下表列出了增长函数、其常见名称和不同的输入大小*n*：
- en: '| Big O | Name | n = 10 | n = 50 | n = 1000 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 大O | 名称 | n = 10 | n = 50 | n = 1000 |'
- en: '| *O(1)* | Constant | 0.001 sec | 0.001 sec | 0.001 sec |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| *O(1)* | 常数 | 0.001秒 | 0.001秒 | 0.001秒 |'
- en: '| *O(log n)* | Logarithmic | 0.003 sec | 0.006 sec | 0.01 sec |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| *O(log n)* | 对数 | 0.003秒 | 0.006秒 | 0.01秒 |'
- en: '| *O(n)* | Linear | 0.01 sec | 0.05 sec | 1 sec |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| *O(n)* | 线性 | 0.01秒 | 0.05秒 | 1秒 |'
- en: '| *O(n log n)* | Linearithmic or *n log n* | 0.03 sec | 0.3 sec | 10 sec |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| *O(n log n)* | 线性对数或*n log n* | 0.03秒 | 0.3秒 | 10秒 |'
- en: '| *O(n*²*)* | Quadratic | 0.1 sec | 2.5 sec | 16.7 minutes |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| *O(n*²*)* | 二次方 | 0.1秒 | 2.5秒 | 16.7分钟 |'
- en: '| *O(2*^n*)* | Exponential | 1 sec | 35,700 years | 3.4 * 10^(290) years |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| *O(2*^n*)* | 指数 | 1秒 | 35,700年 | 3.4 * 10^(290)年 |'
- en: 'Table 3.2: Absolute running times for different growth rates and various values
    of input size'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2：不同增长率和各种输入大小的绝对运行时间
- en: Note that the number in the bottom-right cell is a 291-digit number! Compare
    this with the age of the universe, 13.7 * 10⁹ years, which is only an 11-digit
    number.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意右下角的数字是一个291位数！将其与宇宙的年龄13.7 * 10⁹年相比较，后者只是一个11位数。
- en: Next, I will introduce amortized time complexity, which is frequently used in
    the C++ standard library.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将介绍摊销时间复杂度，这在C++标准库中经常使用。
- en: Amortized time complexity
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摊销时间复杂度
- en: Usually, an algorithm behaves differently with different inputs. Going back
    to our algorithm that linearly searched for an element in an array, we were analyzing
    a case where the key was not in the array at all. For that algorithm, that was
    the worst case—that is, it used the *most* resources the algorithm will need.
    The best case refers to the *least* amount of resources the algorithm will need,
    whereas the average case specifies the amount of resources the algorithm will
    use on average with different inputs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，算法在不同的输入下表现不同。回到我们线性搜索数组中元素的算法，我们分析了一个关键字根本不在数组中的情况。对于该算法，这是最坏情况，即算法将需要*最多*的资源。最佳情况是指算法将需要*最少*的资源，而平均情况指定了算法在不同输入下平均使用的资源量。
- en: The standard library usually refers to the *amortized running time* of functions
    that operate on containers. If an algorithm runs in constant amortized time, it
    means that it will run in *O(1)* in almost all cases, except very few where it
    will perform worse. At first sight, amortized running time can be confused with
    average time, but as you will see, they are not the same.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 标准库通常指的是对容器进行操作的函数的*摊销运行时间*。如果算法以恒定的摊销时间运行，这意味着它在几乎所有情况下都将以*O(1)*运行，只有极少数情况下会表现得更差。乍一看，摊销运行时间可能会与平均时间混淆，但正如您将看到的那样，它们并不相同。
- en: 'To understand amortized time complexity, we will spend some time thinking about
    `std::vector::push_back()`. Let''s assume that the vector internally has a fixed-size
    array to store all its elements. If there is room for more elements in the fixed-size
    array when calling `push_back()`, the operation will run in constant time, *O(1)*—that
    is, it''s not dependent on how many elements are already in the vector as long
    as the internal array has room for one more:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解摊销时间复杂度，我们将花一些时间思考`std::vector::push_back()`。假设向量在内部具有固定大小的数组来存储所有元素。当调用`push_back()`时，如果固定大小数组中还有空间可以存放更多元素，则该操作将在常数时间*O(1)*内运行，即不依赖于向量中已有多少元素，只要内部数组还有空间可以存放一个以上的元素：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: But what happens when the internal array is full? One way to handle the growing
    vector is to create a new empty internal array with a bigger size and then move
    all the elements from the old array to the new one. This is obviously not constant
    time anymore since we need one move per element in the array—that is, *O(n)*.
    If we considered this the worst case, it would mean that `push_back()` is *O(n)*.
    However, if we call `push_back()` many times, we know that the expensive `push_back()`
    can't happen very often, and so it would be pessimistic, and not very useful,
    to say that `push_back()` is *O(n)* if we know that `push_back()` is called many
    times in a row.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当内部数组已满时会发生什么？处理增长向量的一种方法是创建一个新的空内部数组，大小更大，然后将所有元素从旧数组移动到新数组。这显然不再是常数时间，因为我们需要对数组中的每个元素进行一次移动，即*O(n)*。如果我们认为这是最坏情况，那么这意味着`push_back()`是*O(n)*。然而，如果我们多次调用`push_back()`，我们知道昂贵的`push_back()`不会经常发生，因此如果我们知道`push_back()`连续调用多次，那么说`push_back()`是*O(n)*是悲观且不太有用的。
- en: 'Amortized running time is used for analyzing a *sequence* of operations rather
    than a single one. We are still analyzing the worst case, but for a sequence of
    operations. The amortized running time can be computed by first analyzing the
    running time of the entire sequence and then dividing that by the length of the
    sequence. Suppose we are performing a sequence of *m* operations with the total
    running time *T(m)*:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 摊销运行时间用于分析一系列操作，而不是单个操作。我们仍然在分析最坏情况，但是针对一系列操作。摊销运行时间可以通过首先分析整个序列的运行时间，然后将其除以序列的长度来计算。假设我们执行一系列*m*个操作，总运行时间为*T(m)*：
- en: '![](img/B15619_03_004.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_004.png)'
- en: 'where *t*[0] *= 1*, *t*[1] *= n*, *t*[2] *= 1*, *t*[3] *= n*, and so on. In
    other words, half of the operations run in constant time and the other half run
    in linear time. The total time *T* for all *m* operations can be expressed as
    follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*t*[0] *= 1*, *t*[1] *= n*, *t*[2] *= 1*, *t*[3] *= n*，依此类推。换句话说，一半的操作在常数时间内运行，另一半在线性时间内运行。所有*m*个操作的总时间*T*可以表示如下：
- en: '![](img/B15619_03_005.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_005.png)'
- en: 'The amortized complexity for each operation is the total time divided by the
    number of operations, which turns out to be *O(n)*:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每个操作的摊销复杂度是总时间除以操作数，结果为*O(n)*：
- en: '![](img/B15619_03_006.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_006.png)'
- en: However, if we can guarantee that the number of expensive operations differs
    by orders of magnitude compared to the number of constant time operations, we
    will achieve lower amortized running costs. For example, if we can guarantee that
    an expensive operation only occurs once in a sequence *T(n) + T(1) + T(1) + ...*,
    then the amortized running time is *O(1)*. So, depending on the frequency of the
    expensive operations, the amortized running time changes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们可以保证昂贵操作的次数与常数时间操作的次数相比相差很大，我们将实现更低的摊销运行成本。例如，如果我们可以保证昂贵操作仅在序列*T(n) +
    T(1) + T(1) + ...*中发生一次，那么摊销运行时间为*O(1)*。因此，根据昂贵操作的频率，摊销运行时间会发生变化。
- en: Now, back to `std::vector`. The C++ standard states that `push_back()` needs
    to run in amortized constant time, *O(1)*. How do the library vendors achieve
    this? If the capacity is increased by a fixed number of elements each time the
    vector becomes full, we will have a case similar to the preceding one where we
    had a running time of *O(n)*. Even if we use a large constant, the capacity changes
    would still occur at fixed intervals. The key insight is that the vector needs
    to grow exponentially in order to get the expensive operations to occur rarely
    enough. Internally, the vector uses a growth factor such that the capacity of
    the new array is the current size times the growth factor.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到`std::vector`。C++标准规定`push_back()`需要在摊销常数时间内运行，*O(1)*。库供应商是如何实现这一点的呢？如果每次向量变满时容量增加固定数量的元素，我们将会有一个类似于前面的情况，其中运行时间为*O(n)*。即使使用一个大常数，容量变化仍然会以固定间隔发生。关键的见解是向量需要呈指数增长，以便使昂贵的操作发生得足够少。在内部，向量使用增长因子，使得新数组的容量是当前大小乘以增长因子。
- en: 'A big growth factor would potentially waste more memory but would make the
    expensive operation occur less frequently. To simplify the math, let''s use a
    common strategy, namely by doubling the capacity each time the vector needs to
    grow. We can now estimate how often the expensive calls occur. For a vector of
    size *n*, we would need to grow the internal array *log*[2]*(n)* times since we
    are doubling the size all the time. Each time we grow the array, we need to move
    all the elements that are currently in the array. The *i*^(th) time we grow the
    array there will be *2*^i elements to move. So if we perform *m* number of `push_back()`
    operations, the total running time of the grow operations will be:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一个大的增长因子可能会浪费更多的内存，但会使昂贵的操作发生得更少。为了简化数学计算，让我们使用一个常见的策略，即每次向量需要增长时都加倍容量。现在我们可以估计昂贵调用发生的频率。对于大小为*n*的向量，我们需要增长内部数组*log*[2]*(n)*次，因为我们一直在加倍大小。每次增长数组时，我们需要移动当前数组中的所有元素。当我们增长数组的第*i*次时，将有*2*^i个元素需要移动。因此，如果我们执行*m*次`push_back()`操作，增长操作的总运行时间将是：
- en: '![](img/B15619_03_007.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_007.png)'
- en: 'This is a geometric series and can also be expressed as:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个等比数列，也可以表示为：
- en: '![](img/B15619_03_008.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_008.png)'
- en: Dividing this by the length of the sequence, *m*, we end up with the amortized
    running time *O(1)*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个除以序列的长度*m*，我们最终得到摊销运行时间*O(1)*。
- en: 'As I have already said, amortized time complexity is used a lot in the standard
    library, so it''s good to understand the analysis. Thinking about how `push_back()`
    could be implemented in amortized constant time has helped me remember the simplified
    version of amortized constant time: It will run in *O(1)* in almost all cases,
    except very few where it will perform worse.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我已经说过的，摊销时间复杂度在标准库中被广泛使用，因此了解这种分析是很有帮助的。思考`push_back()`如何在摊销常数时间内实现已经帮助我记住了摊销常数时间的简化版本：它几乎在所有情况下都是*O(1)*，只有极少数情况下会表现得更差。
- en: That is all we are going to cover regarding asymptotic complexity. Now we will
    move on to how you can tackle a performance problem and work effectively by optimizing
    your code.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们将要涵盖的关于渐近复杂度的全部内容。现在我们将继续讨论如何解决性能问题，并通过优化代码来有效地工作。
- en: What to measure and how?
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要测量什么以及如何测量？
- en: Optimizations almost always add complexity to your code. High-level optimizations,
    such as choosing algorithms and data structures, can make the intention of the
    code clearer, but for the most part, optimizations will make the code harder to
    read and maintain. We therefore want to be absolutely sure that the optimizations
    we add have an actual impact on what we are trying to achieve in terms of performance.
    Do we really need to make the code faster? In what way? Does the code really use
    too much memory? To understand what optimizations are possible, we need to have
    a good understanding of the requirements, such as latency, throughput, and memory usage.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 优化几乎总是会给你的代码增加复杂性。高级优化，比如选择算法和数据结构，可以使代码的意图更清晰，但在大多数情况下，优化会使代码更难阅读和维护。因此，我们要确信我们添加的优化对我们在性能方面试图实现的目标有实际影响。我们真的需要让代码更快吗？以何种方式？代码真的使用了太多内存吗？为了了解可能的优化，我们需要对要求有一个很好的理解，比如延迟、吞吐量和内存使用。
- en: 'Optimizing code is fun, but it''s also very easy to get lost without any measurable
    gains. We will start this section with a suggested workflow to follow when tuning
    your code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 优化代码是有趣的，但也很容易在没有可衡量的收益的情况下迷失方向。我们将从建议的工作流程开始，以便在调整代码时进行优化：
- en: '**Define a goal**: It''s easier to know how to optimize and when to stop optimizing
    if you have a well-defined, quantitative goal. For some applications, it''s clear
    from the start what the requirements are, but in many cases it tends to be fuzzier.
    Even though it might be obvious that the code is running too slow, it''s important
    to know what would be good enough. Each domain has its own limits, so make sure
    you understand the ones that are relevant to your application. Here are some examples
    to make it more concrete:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义一个目标**：如果有一个明确定义的定量目标，那么知道如何优化以及何时停止优化会更容易。对于一些应用程序，从一开始就很明确要求是什么，但在许多情况下，要求往往更加模糊。即使代码运行太慢可能是显而易见的，但知道什么是足够好是很重要的。每个领域都有自己的限制，所以确保你了解与你的应用程序相关的限制。以下是一些例子，以使其更具体：'
- en: A response time for user-interactive applications of 100 ms; refer to [https://www.nngroup.com/articles/response-times-3-important-limits](https://www.nngroup.com/articles/response-times-3-important-limits).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户交互应用的响应时间为100毫秒；参考[https://www.nngroup.com/articles/response-times-3-important-limits](https://www.nngroup.com/articles/response-times-3-important-limits)。
- en: Graphics with 60 Frames Per Second (FPS) give you 16 ms per frame.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每秒60帧（FPS）的图形给你每帧16毫秒。
- en: Real-time audio with a 128 sample buffer at a 44.1 kHz sample rate means slightly
    less than 3 ms.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以44.1 kHz的采样率和128个样本缓冲的实时音频意味着略低于3毫秒。
- en: '**Measure**: Once we know what to measure and what the limits are, we proceed
    by measuring how the application is performing right now. From *step 1*, it should
    be obvious if we are interested in average times, peaks, load, and so on. In this
    step, we are only concerned with measuring the goal we have set up. Depending
    on the application, measuring can be anything from using a stopwatch to using
    highly sophisticated performance analysis tools.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测量**：一旦我们知道要测量什么和限制是什么，我们就可以通过测量应用程序当前的性能来继续。从*步骤1*开始，如果我们对平均时间、峰值、负载等感兴趣，那么很明显。在这一步中，我们只关心测量我们设定的目标。根据应用程序的不同，测量可以是从使用秒表到使用高度复杂的性能分析工具的任何事情。'
- en: '**Find the bottlenecks**: Next, we need to find the application''s bottlenecks—the
    parts that are too slow and make the application useless. Don''t trust your gut
    feeling at this point! Maybe you gained some insights by measuring the code at
    various points in *step 2*—that''s fine, but you usually need to profile your
    code further in order to find the hot spots that matter most.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**找出瓶颈**：接下来，我们需要找出应用程序的瓶颈——那些太慢的部分，使应用程序变得无用。此时不要相信你的直觉！也许在*步骤2*的不同点测量代码时你获得了一些见解——这很好，但通常你需要进一步对代码进行分析，以找到最重要的热点。'
- en: '**Make an educated guess**: Come up with a hypothesis for how to improve the
    performance. Can a lookup table be used? Can we cache data to gain the overall
    throughput? Can we change the code so that the compiler can vectorize it? Can
    we decrease the number of allocations in the critical sections by reusing memory?
    Coming up with ideas is usually not that hard if you know that they are just educated
    guesses. It''s okay to be wrong—you will find out later whether they had an impact
    or not.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**做出合理猜测**：提出一个如何提高性能的假设。可以使用查找表吗？我们可以缓存数据以获得整体吞吐量吗？我们可以改变代码以便编译器可以对其进行矢量化吗？我们可以通过重用内存来减少关键部分的分配次数吗？如果你知道这些只是合理的猜测，提出想法通常并不那么困难。错了也没关系——你以后会发现它们是否产生了影响。'
- en: '**Optimize**: Let''s implement the hypothesis we sketched in *step 4*. Don''t
    spend too much time on this step making it perfect before you know that it actually
    has an effect. Be prepared to reject this optimization. It might not have the
    desired effect.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**优化**：让我们实现我们在*步骤4*中勾画的假设。在知道它是否真的产生效果之前，不要在这一步上花费太多时间使其完美。准备拒绝这种优化。它可能没有预期的效果。'
- en: '**Evaluate**: Measure again. Do the exact same test as in *step 2* and compare
    the results. What did we gain? If we didn''t gain anything, reject the code and
    go back to *step 4*. If the optimization actually had a positive effect, you need
    to ask yourself whether it''s good enough to spend more time on. How complicated
    is the optimization? Is it worth the effort? Is this a general performance gain
    or is it highly specific to a certain case/platform? Is it maintainable? Can we
    encapsulate it, or does it spread out all over the code base? If you can''t motivate
    the optimization, go back to *step 4*, otherwise continue to the final step.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估**：再次测量。做与*步骤2*中完全相同的测试，并比较结果。我们得到了什么？如果我们没有得到任何东西，拒绝这段代码并返回*步骤4*。如果优化实际上产生了积极的效果，你需要问自己是否值得再花更多时间。这种优化有多复杂？是否值得努力？这是一般性能提升还是高度特定于某种情况/平台？它是否可维护？我们能封装它吗，还是它散布在整个代码库中？如果你无法证明这种优化，返回*步骤4*，否则继续进行最后一步。'
- en: '**Refactor**: If you followed the instructions in *step 5* and didn''t spend
    too much time writing perfect code in the first place, it''s time to refactor
    the optimization to make it cleaner. Optimizations almost always need some comments
    to explain why we are doing things in an unusual way.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重构**：如果你遵循了*步骤5*中的指示，并且在一开始没有花太多时间编写完美的代码，那么现在是时候重构优化以使其更清晰了。优化几乎总是需要一些注释来解释为什么我们以一种不寻常的方式做事情。'
- en: Following this process will ensure that you stay on the right track and don't
    end up with complicated optimizations that aren't motivated. The importance of
    spending time on defining concrete goals and measuring cannot be overestimated.
    In order to be successful in this area, you need to understand what performance
    properties are relevant for your application.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循这个过程将确保你保持在正确的轨道上，不会最终得到没有动机的复杂优化。花时间定义具体目标和测量的重要性不可低估。为了在这个领域取得成功，你需要了解哪些性能特性对你的应用程序是相关的。
- en: Performance properties
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能特性
- en: 'Before you start measuring, you must know which performance properties are
    important for the application you are writing. In this section, I will explain
    some frequently used terms when measuring performance. Depending on the application
    you are writing, some properties are more relevant than others. For example, throughput
    might be a more important property than latency if you are writing an online image
    converter service, whereas latency is key when writing interactive applications
    with real-time requirements. Below are some valuable terms and concepts that are
    worth becoming familiar with during performance measurement:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始测量之前，你必须知道对你正在编写的应用程序来说哪些性能特性是重要的。在本节中，我将解释一些在测量性能时经常使用的术语。根据你正在编写的应用程序，有些特性比其他特性更相关。例如，如果你正在编写在线图像转换服务，吞吐量可能比延迟更重要，而在编写具有实时要求的交互式应用程序时，延迟就很关键。以下是一些在性能测量过程中值得熟悉的有价值的术语和概念：
- en: '**Latency/response time**: Depending on the domain, latency and response time
    might have very precise and different meanings. However, in this book, I mean
    the time between the request and the response of an operation—for example, the
    time it takes for an image conversion service to process one image.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟/响应时间**：根据领域的不同，延迟和响应时间可能有非常精确和不同的含义。然而，在本书中，我指的是请求和操作响应之间的时间——例如，图像转换服务处理一个图像所需的时间。'
- en: '**Throughput**: This refers to the number of transactions (operations, requests,
    and so on) processed per time unit—for example, the number of images that an image
    conversion service can process per second.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**吞吐量**：这指的是每个时间单位处理的交易（操作，请求等）的数量，例如，图像转换服务每秒可以处理的图像数量。'
- en: '**I/O bound or CPU bound**: A task usually spends the majority of its time
    computing things on the CPU or waiting for I/O (hard drives, networks, and so
    on). A task is said to be CPU bound if it would run faster if the CPU were faster.
    It''s said to be I/O bound if it would run faster by making the I/O faster. Sometimes
    you hear about memory-bound tasks too, which means that the amount or speed of
    the main memory is the current bottleneck.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**I/O绑定或CPU绑定**：任务通常在CPU上计算大部分时间或等待I/O（硬盘，网络等）。如果CPU速度更快，任务通常会更快，就称为CPU绑定。如果通过加快I/O速度，任务通常会更快，就称为I/O绑定。有时你也会听到内存绑定任务，这意味着主内存的数量或速度是当前的瓶颈。'
- en: '**Power consumption**: This is a very important consideration for code that
    executes on mobile devices with batteries. In order to decrease the power usage,
    the application needs to use the hardware more efficiently, just as if we are
    optimizing for CPU usage, network efficiency, and so on. Other than that, high-frequency
    polling should be avoided since it prevents the CPU from going to sleep.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**功耗**：这对于在带电池的移动设备上执行的代码来说非常重要。为了减少功耗，应用程序需要更有效地使用硬件，就像我们在优化CPU使用率，网络效率等一样。除此之外，应该避免高频率轮询，因为它会阻止CPU进入睡眠状态。'
- en: '**Data aggregation**: It''s usually necessary to aggregate the data when collecting
    a lot of samples during performance measurement. Sometimes *mean values* are a
    good enough indicator of how the program performs, but more often the *median*
    tells you more about the actual performance since it''s more robust against outliers.
    If you are interested in outliers, you can always measure *min* and *max* values
    (or the 10th percentile, for example).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据聚合**：在进行性能测量时，当收集大量样本时通常需要对数据进行聚合。有时*平均值*足以成为程序性能的良好指标，但更常见的是*中位数*，因为它对异常值更具鲁棒性，可以更多地告诉你实际性能。如果你对异常值感兴趣，你可以测量*最小*和*最大*值（或者例如第10百分位数）。'
- en: This list is by no means exhaustive, but it's a good start. The important thing
    to remember here is that there are established terms and concepts that we can
    use when measuring performance. Spending some time on defining what we really
    mean by optimizing code helps us reach our goals faster.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表并不是详尽无遗的，但这是一个很好的开始。在这里要记住的重要事情是，在测量性能时，我们可以使用已经确立的术语和概念。花一些时间来定义我们所说的优化代码实际意味着帮助我们更快地达到我们的目标。
- en: Speedup of execution time
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行时间的加速
- en: 'When we compare the relative performance between two versions of a program
    or function, it''s customary to talk about **speedup**. Here I will give you a
    definition of speedup when comparing execution time (or latency). Assume we have
    measured the execution times of two versions of some code: an old slower version,
    and a new faster version. The speedup of execution time can then be computed accordingly:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们比较程序或函数的两个版本之间的相对性能时，通常习惯谈论**加速**。在这里我将给出一个比较执行时间（或延迟）时的加速定义。假设我们已经测量了某段代码的两个版本的执行时间：一个旧的较慢版本和一个新的较快版本。执行时间的加速可以相应地计算如下：
- en: '![](img/B15619_03_009.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_009.png)'
- en: Where *T*[old] is the execution time of the initial version of the code, and
    *T*[new] is the execution time of the optimized version. This definition of speedup
    implies that a speedup of 1 means no speedup at all.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*T*[old]是代码初始版本的执行时间，*T*[new]是优化版本的执行时间。这个加速的定义意味着加速比为1表示根本没有加速。
- en: 'Let''s make sure that you know how to measure the relative execution time with
    an example. Assume that we have a function that executes in 10 ms (*T*[old] =
    10 ms) and we manage to make it run in 4 ms after some optimization (*T*[new]
    = 4 ms). We can then compute the speedup as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来确保你知道如何测量相对执行时间。假设我们有一个函数，执行时间为10毫秒（*T*[old] = 10毫秒），经过一些优化后我们设法让它在4毫秒内运行（*T*[new]
    = 4毫秒）。然后我们可以计算加速比如下：
- en: '![](img/B15619_03_010.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_010.png)'
- en: 'In other words, our new optimized version provided a 2.5x speedup. If we want
    to express this improvement as a percentage, we can use the following formula
    to convert speedup to percentage improvement:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们的新优化版本提供了2.5倍的加速。如果我们想将这种改进表示为百分比，我们可以使用以下公式将加速转换为百分比改进：
- en: '![](img/B15619_03_011.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_011.png)'
- en: We can then say that the new version of the code runs 60% faster than the old
    one and that this corresponds to a speedup of 2.5x. In this book, I will consistently
    use speedup, and not percentage improvement, when comparing execution time.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以说新版本的代码比旧版本快60%，这对应着2.5倍的加速。在本书中，当比较执行时间时，我将一贯使用加速，而不是百分比改进。
- en: In the end, we are usually interested in execution time, but time is not always
    the best thing to measure. By inspecting other values on the hardware, the hardware
    might give us some other useful guidance toward optimizing our code.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们通常对执行时间感兴趣，但时间并不总是最好的衡量标准。通过检查硬件上的其他值，硬件可能会给我们一些其他有用的指导，帮助我们优化我们的代码。
- en: Performance counters
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能计数器
- en: Apart from the obvious properties, such as execution time and memory usage,
    it can sometimes be beneficial to measure other things. Either because they are
    more reliable or because they can give us better insights into what is causing
    our code to run slow.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了显而易见的属性，比如执行时间和内存使用，有时候测量其他东西可能会更有益。要么是因为它们更可靠，要么是因为它们可以更好地帮助我们了解导致代码运行缓慢的原因。
- en: Many CPUs are equipped with hardware performance counters that can provide us
    with metrics such as the number of instructions, CPU cycles, branch mispredictions,
    and cache misses. I haven't introduced these hardware aspects yet in this book,
    and we will not explore performance counters in depth. However, it's good to know
    that they exist and that there are ready-made tools and libraries (accessible
    through APIs) for all the major operating systems to collect **Performance Monitoring
    Counters** (**PMC**) while running a program.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 许多CPU配备了硬件性能计数器，可以为我们提供诸如指令数、CPU周期、分支错误预测和缓存未命中等指标。我在本书中尚未介绍这些硬件方面，我们也不会深入探讨性能计数器。但是，知道它们的存在以及所有主要操作系统都有现成的工具和库（通过API可访问）来收集运行程序时的**性能监视计数器**（**PMC**）是很有好处的。
- en: The support for performance counters varies depending on the CPU and operating
    system. Intel provides a powerful tool called VTune, which can be used for monitoring
    performance counters. FreeBSD offers `pmcstat`. macOS comes with DTrace and Xcode
    Instruments. Microsoft Visual Studio provides support for collecting CPU counters
    on Windows.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 性能计数器的支持因CPU和操作系统而异。英特尔提供了一个强大的工具称为VTune，可用于监视性能计数器。FreeBSD提供了`pmcstat`。macOS自带DTrace和Xcode
    Instruments。微软Visual Studio在Windows上提供了收集CPU计数器的支持。
- en: 'Another popular tool is `perf`, which is available on GNU/Linux systems. Running
    the command:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的工具是`perf`，它在GNU/Linux系统上可用。运行命令：
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'will reveal a lot of interesting events, such as the number of context switches,
    page faults, mispredicted branches, and so on. Here is an example of what it output
    when running a small program:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将显示许多有趣的事件，例如上下文切换的次数，页面错误，错误的预测分支等。以下是运行小程序时输出的示例：
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We will now move on to highlight some best practices when testing and evaluating
    performance.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将重点介绍一些测试和评估性能的最佳实践。
- en: Performance testing — best practices
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能测试-最佳实践
- en: For some reason, it's more common to see regression tests covering functional
    requirements than performance requirements or other non-functional requirements
    covered in tests. Performance testing is usually carried out more sporadically
    and, more often than not, way too late in the development process. My recommendation
    is to measure early and detect regression as soon as possible by adding performance
    tests to your nightly builds.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于某种原因，更常见的是回归测试涵盖功能要求，而不是性能要求或其他非功能要求在测试中得到覆盖。性能测试通常更加零星地进行，而且往往太晚了。我的建议是通过将性能测试添加到每晚的构建中，尽早测量并尽快检测到回归。
- en: Choose algorithms and data structures wisely if they are to handle large inputs,
    but don't fine-tune code without good reason. It's also important to test your
    application with realistic test data early on. Ask questions about data sizes
    early in the project. How many table rows is the application supposed to handle
    and still be able to scroll smoothly? Don't just try it with 100 elements and
    hope that your code will scale—test it!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要处理大量输入，则明智地选择算法和数据结构，但不要没有充分理由就对代码进行微调。早期使用真实测试数据测试应用程序也很重要。在项目早期就询问数据大小的问题。应用程序应该处理多少表行并且仍然能够平稳滚动？不要只尝试100个元素并希望您的代码能够扩展-进行测试！
- en: Plotting your data is a very effective way of understanding the data you have
    collected. There are so many good and easy-to-use plotting tools available today,
    so there is really no excuse for not plotting. Both RStudio and Octave provide
    powerful plotting capabilities. Other examples include gnuplot and Matplotlib
    (Python), which can be used on various platforms and require a minimal amount
    of scripting to produce useful plots after collecting your data. A plot does not
    have to look pretty in order to be useful. Once you plot your data, you are going
    to see the outliers and patterns that are usually hard to find in a table full
    of numbers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制数据是了解收集到的数据的一种非常有效的方式。今天有很多好用的绘图工具，所以没有理由不绘图。RStudio和Octave都提供强大的绘图功能。其他例子包括gnuplot和Matplotlib（Python），它们可以在各种平台上使用，并且在收集数据后需要最少的脚本编写来生成有用的图表。图表不一定要看起来漂亮才有用。一旦绘制了数据，您将能够看到通常在充满数字的表中很难找到的异常值和模式。
- en: This concludes our *What to measure and how*? section. Next, we'll now move
    on to exploring ways to find the critical parts of your code that waste too many
    resources.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们的*要测量和如何测量*？部分。接下来，我们将探索找到代码中浪费太多资源的关键部分的方法。
- en: Knowing your code and hot spots
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解您的代码和热点
- en: The Pareto principle, or the 80/20 rule, has been applied in various fields
    since it was first observed by the Italian economist Vilfredo Pareto more than
    100 years ago. He was able to show that 20% of the Italian population owned 80%
    of the land. In computer science, it has been widely used, perhaps even overused.
    In software optimization, it suggests that 20% of the code is responsible for
    80% of the resources that a program uses.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 帕累托原则，或80/20法则，自100多年前意大利经济学家维尔弗雷多·帕累托首次观察到以来，已经在各个领域得到应用。他能够证明意大利人口的20%拥有80%的土地。在计算机科学中，它已被广泛使用，甚至可能被过度使用。在软件优化中，它表明代码的20%负责程序使用的80%资源。
- en: This is, of course, only a rule of thumb and shouldn't be taken too literally.
    Nevertheless, for code that has not been optimized, it's common to find some relatively
    small hot spots that spend the vast majority of the total resources. As a programmer,
    this is actually good news, because it means that we can write most of our code
    without tweaking it for performance reasons and instead focus on keeping the code
    clean. It also means that when doing optimizations, we need to know *where* to
    do them; otherwise, there is a good chance we will optimize code that will not
    have an impact on the overall performance. In this section, we will look at methods
    and tools for finding the 20% of your code that might be worth optimizing.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只是一个经验法则，不应该被过于字面理解。尽管如此，对于尚未优化的代码，通常会发现一些相对较小的热点，它们消耗了绝大部分的资源。作为程序员，这实际上是个好消息，因为这意味着我们可以大部分时间编写代码而不需要为了性能而对其进行调整，而是专注于保持代码的清晰。这也意味着在进行优化时，我们需要知道*在哪里*进行优化；否则，我们很可能会优化对整体性能没有影响的代码。在本节中，我们将探讨寻找可能值得优化的代码中的20%的方法和工具。
- en: Using a profiler is usually the most efficient way of identifying hot spots
    in a program. Profilers analyze the execution of a program and output a statistical
    summary, a profile, of how often the functions or instructions in the program
    are being called.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用性能分析器通常是识别程序中热点的最有效方法。性能分析器分析程序的执行并输出函数或指令被调用的统计摘要，即性能分析结果。
- en: 'In addition, profilers usually also output a call graph that shows the relationship
    between function calls, that is, the callers and callees for each function that
    was called during the profiling. In the following figure, you can see that the
    `sort()` function was called from `main()` (the caller) and that `sort()` called
    the function `swap()` (the callee):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，性能分析器通常还会输出一个调用图，显示函数调用之间的关系，即每个在分析期间被调用的函数的调用者和被调用者。在下图中，您可以看到`sort()`函数是从`main()`（调用者）调用的，而`sort()`又调用了`swap()`函数（被调用者）：
- en: '![](img/New_B15619_03_04.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/New_B15619_03_04.png)'
- en: 'Figure 3.4: Example of a call graph. The function sort() is called once and
    calls swap() 50 times.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：调用图的示例。函数`sort()`被调用一次，并调用`swap()` 50次。
- en: 'There are two main categories of profilers: sampling profilers and instrumentation
    profilers. The approaches can also be mixed to create a hybrid of sampling and
    instrumentation. `gprof`, the Unix performance analysis tool, is an example of
    this. The sections that follow focus on instrumentation profilers and sampling
    profilers.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析器主要分为两类：采样性能分析器和插装性能分析器。这两种方法也可以混合使用，创建采样和插装的混合性能分析器。Unix性能分析工具`gprof`就是一个例子。接下来的部分将重点介绍插装性能分析器和采样性能分析器。
- en: Instrumentation profilers
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 插装性能分析器
- en: By instrumentation, I mean inserting code into a program to be analyzed in order
    to gather information about how frequently each function is being executed. Typically,
    the inserted instrumentation code records each entry and exit point. You can write
    your own primitive instrumentation profiler by inserting the code manually yourself,
    or you can use a tool that automatically inserts the necessary code as a step
    in the build process.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通过插装，我指的是向程序中插入代码以便分析，以收集关于每个函数被执行频率的信息。通常，插入的插装代码记录每个入口和出口点。您可以通过手动插入代码来编写自己的原始插装性能分析器，或者您可以使用一个工具，在构建过程中自动插入必要的代码。
- en: A simple implementation might be good enough for your purposes, but be aware
    of the impact that the added code can have on performance, which can make the
    profile misleading. Another problem with a naive implementation like this is that
    it might prevent compiler optimizations or run the risk of being optimized away.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的实现可能对您的目的足够了，但要注意添加的代码对性能的影响，这可能会使性能分析结果产生误导。像这样的天真实现的另一个问题是，它可能会阻止编译器优化或者有被优化掉的风险。
- en: 'Just to give you an example of an instrumentation profiler, here is a simplified
    version of a timer class I have used in previous projects:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅举一个插装性能分析器的例子，这里是一个我在以前项目中使用过的计时器类的简化版本：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `ScopedTimer` class will measure the time from when it was created to the
    time it went out of scope, that is, destructed. We are using the class `std::chrono::steady_clock`,
    available since C++11, which was designed for measuring time intervals. `steady_clock`
    is monotonic, which means that it will never decrease between two consecutive
    calls to `clock_type::now()`. This is not the case for the system clock, for example,
    which can be adjusted at any time.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`ScopedTimer`类将测量从创建到超出作用域（即析构）的时间。我们使用自C++11以来可用的`std::chrono::steady_clock`类，它专门用于测量时间间隔。`steady_clock`是单调的，这意味着在两次连续调用`clock_type::now()`之间它永远不会减少。这对于系统时钟来说并非如此，例如，系统时钟可以随时调整。'
- en: 'We can now use our timer class by measuring each function in a program by creating
    a `ScopedTimer` instance at the beginning of each function:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过在每个函数的开头创建一个`ScopedTimer`实例来使用我们的计时器类：
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Even though we don''t recommend the use of preprocessor macros in general,
    this might be a case for using one:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们通常不建议使用预处理宏，但这可能是使用预处理宏的一个案例：
- en: '[PRE9]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We are using the only predefined function-local `__func__` variable available
    since C++11 to get the name of the function. C++20 also introduced the handy `std::source_location`
    class, which provides us with the functions `function_name()`, `file_name()`,
    `line()`, and `column()`. If `std::source_location` is not supported by your compiler
    yet, there are other nonstandard predefined macros that are widely supported and
    can be really useful for debugging purposes, for example, `__FUNCTION__`, `__FILE__`,
    and `__LINE__`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用自C++11以来可用的唯一预定义的函数局部`__func__`变量来获取函数的名称。C++20还引入了方便的`std::source_location`类，它为我们提供了`function_name()`、`file_name()`、`line()`和`column()`等函数。如果您的编译器尚不支持`std::source_location`，还有其他非标准的预定义宏被广泛支持，对于调试目的非常有用，例如`__FUNCTION__`、`__FILE__`和`__LINE__`。
- en: 'Now, our `ScopedTimer` class can be used like this:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的`ScopedTimer`类可以像这样使用：
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Assuming that we have defined `USE_TIMER` when compiling our timer, it will
    produce the following output each time `some_function()` returns:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在编译计时器时定义了`USE_TIMER`，那么每次`some_function()`返回时，它将产生以下输出：
- en: '[PRE11]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: I have demonstrated how we can manually instrument our code by inserting code
    that prints the elapsed time between two points in the code. Although this is
    a handy tool for some scenarios, please be aware of the misleading results a simple
    tool like this can produce. In the next section, I will introduce a profiling
    method that doesn't require any modifications of the executing code.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经演示了如何通过在代码中插入打印两个代码点之间经过的时间的代码来手动检测我们的代码。虽然这对于某些情况来说是一个方便的工具，请注意这样一个简单工具可能产生误导性的结果。在下一节中，我将介绍一种不需要对执行代码进行任何修改的性能分析方法。
- en: Sampling profilers
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采样分析器
- en: Sampling profilers create a profile by looking at the running program's state
    at even intervals—typically, every 10 ms. Sampling profilers usually have a minimum
    impact on the program's actual performance, and it's also possible to build the
    program in release mode with all optimizations turned on. A drawback of sampling
    profilers is their inaccuracy and statistical approach, which is usually not a
    problem as long as you are aware of it.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 采样分析器通过在均匀间隔（通常为每10毫秒）查看运行程序的状态来创建概要。采样分析器通常对程序的实际性能影响很小，并且还可以在启用所有优化的发布模式下构建程序。采样分析器的缺点是它们的不准确性和统计方法，通常只要你意识到这一点，这通常不是问题。
- en: 'The following figure shows a sampling session of a running program with five
    functions: `main()`, `f1()`, `f2()`, `f3()`, and `f4()`. The **t**[1] - **t**[10]
    labels indicate when each sample was taken. The boxes indicate the entry and exit
    point of each executing function:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个运行程序的采样会话，其中包含五个函数：`main()`、`f1()`、`f2()`、`f3()`和`f4()`。标签**t**[1] -
    **t**[10]表示每个样本的取样时间。方框表示每个执行函数的入口和出口点：
- en: '![](img/B15619_03_05.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_05.png)'
- en: 'Figure 3.5: Example of a sampling profiler session'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：采样分析器会话的示例
- en: 'The profile is summarized in the following table:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 概要显示在下表中：
- en: '| Function | Total | Self |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 总数 | 自身 |'
- en: '| `main()` | 100% | 10% |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `main()` | 100% | 10% |'
- en: '| `f1()` | 80% | 10% |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `f1()` | 80% | 10% |'
- en: '| `f2()` | 70% | 30% |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `f2()` | 70% | 30% |'
- en: '| `f3()` | 50% | 50% |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| `f3()` | 50% | 50% |'
- en: 'Table 3.3: For each function, the profile shows the total percentage of call
    stacks that it appeared in (Total) and the percentage of call stacks where it
    occurred on top of the stack (Self).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.3：对于每个函数，概要显示了它出现在调用堆栈中的总百分比（Total）以及它出现在堆栈顶部的百分比（Self）。
- en: The **Total** column in the preceding table shows the percentage of call stacks
    that contained a certain function. In our example, the main function was present
    in all 10 out of 10 call stacks (100%), whereas the `f2()` function was only detected
    in 7 call stacks, which corresponds to 70% of all call stacks.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 前表中的**Total**列显示了包含某个函数的调用堆栈的百分比。在我们的示例中，主函数在10个调用堆栈中都出现（100%），而`f2()`函数只在7个调用堆栈中被检测到，占所有调用堆栈的70%。
- en: The **Self** column shows, for each function, how many times it occurred on
    top of the call stack. The `main()` function was detected once on top of the call
    stack at the fifth sample, **t**[5], whereas the `f2()` function was on top of
    the call stack at samples **t**[6], **t**[8], and **t**[9], which corresponds
    to 3/10 = 30%.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**Self**列显示了每个函数在调用堆栈顶部出现的次数。`main()`函数在第五个样本**t**[5]中被检测到在调用堆栈顶部出现一次，而`f2()`函数在样本**t**[6]、**t**[8]和**t**[9]中出现在调用堆栈顶部，对应3/10
    = 30%。'
- en: The `f3()` function had the highest **Self** value (5/10) and was on top of
    the call stack whenever it was detected.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`f3()`函数具有最高的**Self**值（5/10），每当检测到它时，它都位于调用堆栈的顶部。'
- en: Conceptually, a sampling profiler stores samples of call stacks at even time
    intervals. It detects what is currently running on the CPU. Pure sampling profilers
    usually only detect functions that are currently being executed in a thread that
    is in a running state, since sleeping threads do not get scheduled on the CPU.
    This means that if a function is waiting for a lock that causes the thread to
    sleep, that time will not show up in the time profile. This is important because
    your bottlenecks might be caused by thread synchronization, which might be invisible
    to the sampling profiler.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念上，采样分析器以均匀的时间间隔存储调用堆栈的样本。它检测当前在CPU上运行的内容。纯采样分析器通常只检测当前在运行状态的线程中执行的函数，因为休眠线程不会被调度到CPU上。这意味着如果一个函数正在等待导致线程休眠的锁，那么这段时间不会显示在时间概要中。这很重要，因为您的瓶颈可能是由线程同步引起的，这可能对采样分析器是不可见的。
- en: What happened to the `f4()` function? According to the graph, it was called
    by the `f2()` function between samples two and three, but it never showed up in
    our statistical profile since it was never registered in any of the call stacks.
    This is an important property of sampling profilers. If the time between each
    sample is too long or the total sampling session is too short, then short and
    infrequently called functions will not show up in the profile. This is usually
    not a problem since these functions are rarely the functions you need to tune.
    You may note that the `f3()` function was also missed between **t**[5] and **t**[6],
    but since `f3()` was called very frequently, it had a big impact on the profile,
    anyway.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`f4()`函数发生了什么？根据图表，它在样本二和三之间被`f2()`函数调用，但它从未出现在我们的统计概要中，因为它从未在任何调用堆栈中注册过。这是采样分析器的一个重要特性。如果每个样本之间的时间太长或总采样会话时间太短，那么短且不经常调用的函数将不会出现在概要中。这通常不是问题，因为这些函数很少是您需要调整的函数。您可能注意到`f3()`函数也在**t**[5]和**t**[6]之间被错过了，但由于`f3()`被频繁调用，它对概要产生了很大的影响。'
- en: Make sure you understand what your time profiler actually registers. Be aware
    of its limitations and strengths in order to use it as effectively as possible.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您了解您的时间分析器实际上记录了什么。要充分利用它，要意识到它的局限性和优势。
- en: Microbenchmarking
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微基准测试
- en: 'Profiling can help us find the bottlenecks in our code. If these bottlenecks
    are caused by inefficient data structures (see *Chapter 4*, *Data Structures*),
    the wrong choice of algorithm (see *Chapter 5*, *Algorithms*), or unnecessary
    contention (see *Chapter 11*, *Concurrency*), these bigger issues should be addressed
    first. But sometimes we find a small function or a small block of code that we
    need to optimize, and in those cases, we can use a method called **microbenchmarking**.
    With this process we create a microbenchmark—a program that runs a small piece
    of code in isolation from the rest of the program. The process of microbenchmarking
    consists of the following steps:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 分析可以帮助我们找到代码中的瓶颈。如果这些瓶颈是由低效的数据结构（见*第4章*，*数据结构*）、算法选择错误（见*第5章*，*算法*）或不必要的争用（见*第11章*，*并发*）引起的，那么应该首先解决这些更大的问题。但有时我们会发现需要优化的小函数或小代码块，在这种情况下，我们可以使用一种称为**微基准测试**的方法。通过这个过程，我们创建一个微基准测试——一个在程序的其余部分中孤立运行小代码片段的程序。微基准测试的过程包括以下步骤：
- en: Find a hot spot that needs tuning, preferably using a profiler.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到需要调整的热点，最好使用分析器。
- en: Separate it from the rest of the code and create an isolated microbenchmark.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其与其余代码分离并创建一个孤立的微基准测试。
- en: Optimize the microbenchmark. Use a benchmarking framework to test and evaluate
    the code during optimization.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化微基准测试。使用基准测试框架在优化过程中测试和评估代码。
- en: Integrate the newly optimized code into the program and *measure again* to see if
    the optimizations are relevant when the code runs in a bigger context with more
    relevant input.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新优化的代码集成到程序中，然后*重新测量*，看看当代码在更大的上下文中运行时，优化是否相关。
- en: 'The four steps of the process are illustrated in the following figure:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的四个步骤如下图所示：
- en: '![](img/B15619_03_06.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_06.png)'
- en: 'Figure 3.6: The microbenchmarking process'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：微基准测试过程
- en: 'Microbenchmarking is fun. However, before diving into the process of trying
    to make a particular function faster, we should first make sure that:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 微基准测试很有趣。然而，在着手尝试加快特定函数之前，我们应该首先确保：
- en: The time spent inside the function when running the program significantly affects
    the overall performance of the program we want to speed up. Profiling and Amdahl's
    law will help us understand this. Amdahl's law will be explained below.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行程序时在函数内部花费的时间显着影响我们想要加速的程序的整体性能。分析和阿姆达尔定律将帮助我们理解这一点。下面将解释阿姆达尔定律。
- en: We cannot easily decrease the number of times the function is being called.
    Eliminating calls to expensive functions is usually the most effective way of optimizing
    the overall performance of the program.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们无法轻易减少函数被调用的次数。消除对昂贵函数的调用通常是优化程序整体性能最有效的方法。
- en: Optimizing code using microbenchmarking should usually be seen as the last resort.
    The expected overall performance gains are usually small. However, sometimes we
    cannot avoid the fact that we need to make a relatively small piece of code run
    faster by tuning its implementation, and in those cases microbenchmarks can be
    very effective.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微基准测试来优化代码通常应该被视为最后的手段。预期的整体性能提升通常很小。然而，有时我们无法避免需要通过调整实现来加快相对较小的代码片段的运行速度，而在这些情况下，微基准测试可以非常有效。
- en: Next, you will learn how the speedup of a microbenchmark can affect the overall
    speedup of a program.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将了解微基准测试的加速比如何影响程序的整体加速比。
- en: Amdahl's law
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阿姆达尔定律
- en: When working with microbenchmarks, it's essential to keep in mind how big (or
    small) an impact the optimization of the isolated code will have on the complete
    program. It's our experience that it is easy to get a little too excited sometimes
    when improving a microbenchmark, just to realize that the overall effect was nearly
    negligible. This risk of going nowhere is partly addressed by using a sound profiling
    technique, but also by keeping the overall impact of an optimization in mind.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用微基准测试时，要牢记孤立代码的优化对整个程序的影响有多大（或多小）是至关重要的。我们的经验是，有时在改进微基准测试时很容易有点过于兴奋，只是意识到整体效果几乎可以忽略不计。使用健全的分析技术部分地解决了这种无法前进的风险，同时也要牢记优化的整体影响。
- en: 'Say that we are optimizing an isolated part of a program in a microbenchmark.
    The upper limit of the overall speedup of the entire program can then be computed
    using Amdahl''s law. We need to know two values in order to compute the overall
    speedup:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在优化程序中的一个孤立部分的微基准测试。然后可以使用阿姆达尔定律计算整个程序的整体加速比的上限。为了计算整体加速比，我们需要知道两个值：
- en: First, we need to know how much execution time the isolated part accounts for
    in proportion to the overall execution time. We denote this value of *proportional
    execution* time with the letter *p*.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们需要知道孤立部分的执行时间在整体执行时间中所占的比例。我们用字母*p*来表示这个*比例执行*时间的值。
- en: Secondly, we need to know the speedup of the part we are optimizing—the microbenchmark
    that is. We denote this value of *local speedup* with the letter *s*.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，我们需要知道我们正在优化的部分的加速比——即微基准测试的。我们用字母*s*来表示这个*本地加速比*的值。
- en: 'Using *p* and *s*, we can now use Amdahl''s law to compute the overall speedup:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*p*和*s*，我们现在可以使用阿姆达尔定律来计算整体加速比：
- en: '![](img/B15619_03_012.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_012.png)'
- en: 'Hopefully this doesn''t look too complicated, because it''s very intuitive
    when put in to use. To get an intuition for Amdahl''s law, you can see what the
    overall speedup becomes when using various extreme values of *p* and *s*:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这看起来不会太复杂，因为当投入使用时，这是非常直观的。为了直观理解阿姆达尔定律，可以看看在使用各种极端*p*和*s*值时整体加速比会变成什么样：
- en: Setting *p = 0* and *s = 5x* means that the part we optimize has no impact on
    the overall execution time. Therefore, the overall speedup will always be 1x regardless
    of the value of *s*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置*p = 0*和*s = 5x*意味着我们优化的部分对整体执行时间没有影响。因此，无论*s*的值如何，整体加速比始终为1x。
- en: Setting *p = 1*, *s = 5x* means that we optimize a part that accounts for the
    entire execution time of a program, and in that case, the overall speedup will
    always be equal to the speedup we achieve in the part we optimize—5x in this case.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置*p = 1*，*s = 5x*意味着我们优化了整个程序执行时间的一部分，在这种情况下，整体加速将始终等于我们在优化部分所实现的加速——在这种情况下是5倍。
- en: Setting *p = 0.5* and *s = ∞* means that we completely remove the part of the
    program that accounted for half of the execution time. The overall speedup will
    then be 2x.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置*p = 0.5*和*s = ∞*意味着我们完全删除了程序执行时间的一半。整体加速将是2倍。
- en: 'The results are summarized in the following table:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 结果总结在下表中：
- en: '| p | s | Overall speedup |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| p | s | 整体加速 |'
- en: '| 0 | 5x | 1x |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 5x | 1x |'
- en: '| 1 | 5x | 5x |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 5x | 5x |'
- en: '| 0.5 | ∞ | 2x |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | ∞ | 2x |'
- en: 'Table 3.4: Extreme values of p and s and the achieved overall speedup'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.4：p和s的极端值及实现的整体加速
- en: 'A complete example will demonstrate how we can use Amdahl''s law in practice.
    Say that you are optimizing a function so that the optimized version is 2 times
    faster than the original version, that is, a speedup of *2x (s = 2)*. Further,
    let''s assume that this function is only responsible for 1% of the overall execution
    time of a program (*p = 0.01*), then the overall speedup of the entire program
    can be computed as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完整的例子将演示我们如何在实践中使用阿姆达尔定律。假设你正在优化一个函数，使得优化版本比原始版本快2倍，即*2x (s = 2)*的加速。此外，让我们假设这个函数只占程序整体执行时间的1%（*p
    = 0.01*），那么整个程序的整体加速可以计算如下：
- en: '![](img/B15619_03_013.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15619_03_013.png)'
- en: So, even if we managed to make our isolated code 2 times faster, the overall
    speedup is only a factor of 1.005—not saying that this speedup is necessarily
    negligible, but we constantly need to go back and look at our gains in proportion
    to the bigger picture.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，即使我们设法使我们的孤立代码快2倍，整体加速只有1.005倍的因素——并不是说这种加速必然是可以忽略的，但我们不断需要回过头来看我们的收益与整体情况的比例。
- en: Pitfalls of microbenchmarking
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微基准测试的陷阱
- en: 'There are plenty of hidden difficulties when measuring software performance
    in general and microbenchmarks in particular. Here, I will present a list of things
    to be aware of when working with microbenchmarks:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般情况下测量软件性能和特别是微基准测试时，有很多隐藏的困难。在这里，我将列出在处理微基准测试时需要注意的事项：
- en: Results are sometimes overgeneralized and are treated as universal truths.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时结果被过度概括，并被视为普遍真理。
- en: The compiler might optimize isolated code differently compared to how it is
    optimized in the full program. For example, a function might be inlined in the
    microbenchmark but not when compiled in the full program. Or, the compiler might
    be able to precompute parts of the microbenchmark.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译器可能会以不同于在完整程序中优化的方式来优化孤立的代码。例如，在微基准测试中可能会内联一个函数，但在完整程序中编译时可能不会内联。或者，编译器可能能够预先计算微基准测试的部分。
- en: Unused returned values in a benchmark might make the compiler remove the function
    we are trying to measure.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在基准测试中未使用的返回值可能会使编译器删除我们试图测量的函数。
- en: Static test data provided in the microbenchmark might give the compiler an unrealistic
    advantage when optimizing the code. For example, if we hardcode how many times
    a loop will be executed, and the compiler knows that this hardcoded value happens
    to be a multiple of 8, it can vectorize the loop differently, skipping the prologue
    and epilogue of parts that might not be aligned with the SIMD register size. And
    then in the real code, where this hardcoded compile-time constant is replaced
    with a runtime value, that optimization doesn't happen.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在微基准测试中提供的静态测试数据可能会使编译器在优化代码时获得不切实际的优势。例如，如果我们硬编码循环将执行的次数，并且编译器知道这个硬编码的值恰好是8的倍数，它可以以不同的方式对循环进行矢量化，跳过可能与SIMD寄存器大小不对齐的部分的序言和尾声。然后在真实代码中，这个硬编码的编译时常量被替换为运行时值，这种优化就不会发生。
- en: Unrealistic test data can have an impact on branch prediction when running the
    benchmark.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不切实际的测试数据可能会影响运行基准测试时的分支预测。
- en: Results between multiple measurements might vary because of factors such as
    frequency scaling, cache pollution, and the scheduling of other processes.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多次测量之间的结果可能会有所不同，因为频率缩放、缓存污染和其他进程的调度等因素。
- en: The limiting factor of a code's performance could be due to cache misses, not
    the time it actually takes to execute instructions. Therefore, in many scenarios,
    an important rule of microbenchmarking is that you have to thrash the cache before
    you measure, otherwise you're not really measuring anything.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码性能的限制因素可能是缓存未命中，而不是实际执行指令所需的时间。因此，在许多情况下，微基准测试的一个重要规则是，在测量之前必须清除缓存，否则你实际上并没有在测量任何东西。
- en: I wish I had a simple formula for avoiding all the pitfalls listed above, but
    unfortunately, I don't. However, in the next section, we will have a look at a concrete
    example to see how some of those pitfalls can be addressed by using a microbenchmarking
    support library.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望有一个简单的公式来避免上面列出的所有陷阱，但不幸的是，我没有。然而，在下一节中，我们将通过使用微基准测试支持库来看一个具体的例子，看看如何通过使用微基准测试支持库来解决其中一些陷阱。
- en: A microbenchmark example
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个微基准测试的例子
- en: We will wrap this chapter up by going back to our initial examples with linear
    search and binary search from this chapter and demonstrate how they can be benchmarked
    using a benchmarking framework.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过回到本章的线性搜索和二分搜索的初始例子，并演示如何使用基准测试框架对它们进行基准测试来结束这一章。
- en: 'We began this chapter by comparing two ways of searching for an integer in
    a `std::vector`. If we knew that the vector was already sorted, we could use a
    binary search, which outperformed the simple linear search algorithm. I will not
    repeat the definition of the functions here, but the declaration looked like this:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始这一章节，比较了在`std::vector`中搜索整数的两种方法。如果我们知道向量已经排序，我们可以使用二分搜索，这比简单的线性搜索算法效果更好。我不会在这里重复函数的定义，但声明看起来是这样的：
- en: '[PRE12]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The difference in the execution time of these functions is very obvious once
    the input is sufficiently large, but it will serve as a good enough example for
    our purpose. We will begin by only measuring `linear_search()`. Then, when we
    have a working benchmark in place, we will add `binary_search()` and compare the
    two versions.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦输入足够大，这些函数的执行时间差异是非常明显的，但它将作为我们目的的一个足够好的例子。我们将首先只测量`linear_search()`。然后，当我们有一个可用的基准测试时，我们将添加`binary_search()`并比较这两个版本。
- en: 'In order to make a testing program, we first need a way to generate a sorted
    vector of integers. A simple implementation, as follows, will be sufficient for
    our needs:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了制作一个测试程序，我们首先需要一种方法来生成一个排序的整数向量。以下是一个简单的实现，对我们的需求来说足够了：
- en: '[PRE13]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The vector that is returned will contain all integers between 0 and *n - 1*.
    Once we have that in place, we can create a naive test program like this:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的向量将包含0到*n-1*之间的所有整数。一旦我们有了这个，我们就可以创建一个像这样的简单测试程序：
- en: '[PRE14]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We are searching for the value `n`, which we know isn''t in the vector, so
    the algorithm will exhibit its worst-case performance using this test data. That''s
    the good part of this test. Other than that, it is afflicted with many flaws that
    will make this benchmark useless:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在搜索值`n`，我们知道它不在向量中，所以算法将展示其在这个测试数据中的最坏情况性能。这是这个测试的好部分。除此之外，它还有许多缺陷，这将使得这个基准测试无用：
- en: Compiling this code using optimizations will most likely completely remove the
    code because the compiler can see that the results from the functions are not
    being used.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用优化编译这段代码很可能会完全删除代码，因为编译器可以看到函数的结果没有被使用。
- en: We don't want to measure the time it takes to create and fill the `std::vector`.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不想测量创建和填充`std::vector`所需的时间。
- en: By only running the `linear_search()` function once, we will not achieve a statistically
    stable result.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只运行一次`linear_search()`函数，我们将无法获得统计上稳定的结果。
- en: It's cumbersome to test for different input sizes.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试不同的输入大小是很麻烦的。
- en: Let's see how these problems can be addressed by using a microbenchmarking support
    library. There are various tools/libraries for benchmarking, but we will use **Google
    Benchmark**, [https://github.com/google/benchmark](https://github.com/google/benchmark),
    because of its widespread use, and as a bonus, it can also be easily tested online
    on the page [http://quick-bench.com](http://quick-bench.com) without the need
    for any installation.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何通过使用微基准支持库来解决这些问题。有各种各样的用于基准测试的工具/库，但我们将使用**Google Benchmark**，[https://github.com/google/benchmark](https://github.com/google/benchmark)，因为它被广泛使用，而且作为一个奖励，它也可以在[http://quick-bench.com](http://quick-bench.com)页面上轻松在线测试，而无需任何安装。
- en: 'Here is how a simple microbenchmark of `linear_search()` might look when using
    Google Benchmark:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用Google Benchmark时`linear_search()`的一个简单微基准测试的样子：
- en: '[PRE15]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'That''s it! The only thing we haven''t addressed yet is the fact that the input
    size is hardcoded to 1024\. We will fix that in a while. Compiling and running
    this program will generate something like this:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们还没有解决的唯一问题是输入大小被硬编码为1024。我们稍后会解决这个问题。编译和运行这个程序将生成类似这样的东西：
- en: '[PRE16]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The number of iterations reported in the rightmost column reports the number
    of times the loop needed to execute before a statistically stable result was achieved.
    The `state` object passed to our benchmarking function determines when to stop.
    The average time per iteration is reported in two columns: **Time** is the wall-clock
    time and **CPU** is the time spent on the CPU by the main thread. In this case
    they were the same, but if `linear_search()` had been blocked waiting for I/O
    (for example), the CPU time would have been lower than the wall-clock time.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧列中报告的迭代次数报告了循环需要执行多少次才能获得统计上稳定的结果。传递给我们基准测试函数的`state`对象确定了何时停止。每次迭代的平均时间在两列中报告：**时间**是挂钟时间，**CPU**是主线程在CPU上花费的时间。在这种情况下，它们是相同的，但如果`linear_search()`被阻塞等待I/O（例如），CPU时间将低于挂钟时间。
- en: 'Another important thing to note is that the code that generates the vector
    is not included in the reported time. The only code that is being measured is
    the code inside this loop:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的事情要注意的是生成向量的代码不包括在报告的时间内。唯一被测量的代码是这个循环内的代码：
- en: '[PRE17]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The boolean value returned from our search functions is wrapped inside `benchmark::DoNotOptimize()`.
    This is the mechanism used to ensure that the returned value is not optimized
    away, which could make the entire call to `linear_search()` disappear.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的搜索函数返回的布尔值被包裹在`benchmark::DoNotOptimize()`中。这是用来确保返回值不被优化掉的机制，这可能会使对`linear_search()`的整个调用消失。
- en: 'Now let''s make this benchmark a little more interesting by varying the input
    size. We can do that by passing arguments to our benchmarking function using the
    `state` object. Here is how to do it:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过改变输入大小使这个基准测试更有趣。我们可以通过使用`state`对象向我们的基准测试函数传递参数来做到这一点。以下是如何做到的：
- en: '[PRE18]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will start with an input size of 64 and double the size until it reaches
    256\. On my machine, the test generated the following output:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从输入大小为64开始，每次加倍大小，直到达到256。在我的机器上，测试生成了以下输出：
- en: '[PRE19]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As a final example, we will benchmark both the `linear_search()` and the `binary_search()`
    functions using a variable input size and also try to let the framework estimate
    the time complexity of our functions. This can be done by providing the input
    size to the `state` object using the `SetComplexityN()` function. The complete
    microbenchmark example looks like this:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用可变输入大小对`linear_search()`和`binary_search()`函数进行基准测试，并尝试让框架估计我们函数的时间复杂度。这可以通过使用`SetComplexityN()`函数向`state`对象提供输入大小来实现。完整的微基准测试示例如下：
- en: '[PRE20]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When running the benchmark, we will get the following results printed to the
    console:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 运行基准测试时，将在控制台上打印以下结果：
- en: '[PRE21]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The output is aligned with our initial results in this chapter, where we concluded
    that the algorithms exhibit linear runtime and logarithmic runtime, respectively.
    If we plot the values in a table, we can clearly see the linear and logarithmic
    growth rates of the functions.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7：绘制不同输入大小的执行时间，显示了搜索函数的增长率
- en: 'The following figure was generated using Python with Matplotlib:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 输出与本章初步结果一致，我们得出结论，这些算法分别表现出线性运行时间和对数运行时间。如果我们将数值绘制在表中，我们可以清楚地看到函数的线性和对数增长率。
- en: '![](img/B15619_03_07.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: 输出结果如下：
- en: 'Figure 3.7: Plotting the execution time for various input sizes reveals the
    growth rates of the search functions'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 总结
- en: 'You now have a lot of tools and insights for finding and improving the performance
    of your code. I cannot stress enough the importance of measuring and setting goals
    when working with performance. A quote from Andrei Alexandrescu will conclude
    this section:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图是使用Python和Matplotlib生成的：
- en: '"Measuring gives you a leg up on experts who don''t need to measure."'
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “测量让您领先于不需要测量的专家。”
- en: '- Andrei Alexandrescu, 2015, Writing Fast Code I, code::dive conference 2015,
    https://codedive.pl/2015/writing-fast-code-part-1.'
  id: totrans-252
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您现在拥有了许多工具和见解，可以找到并改进代码的性能。在处理性能时，我再次强调测量和设定目标的重要性。Andrei Alexandrescu的一句话将结束本节：
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本章中，您学会了如何使用大O符号比较算法的效率。您现在知道C++标准库为算法和数据结构提供了复杂性保证。所有标准库算法都指定它们的最坏情况或平均情况性能保证，而容器和迭代器指定摊销或精确复杂度。
- en: In this chapter, you learned how to compare the efficiency of algorithms by
    using big O notation. You now know that the C++ standard library provides complexity
    guarantees for algorithms and data structures. All standard library algorithms
    specify their worst-case or average-case performance guarantees, whereas containers
    and iterators specify amortized or exact complexity.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: -Andrei Alexandrescu，2015年，编写快速代码I，code::dive conference 2015，https://codedive.pl/2015/writing-fast-code-part-1。
- en: You also discovered how to quantify software performance by measuring latency
    and throughput.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 您还了解了如何通过测量延迟和吞吐量来量化软件性能。
- en: Lastly, you learned how to detect hot spots in your code by using CPU profilers
    and how to perform microbenchmarkings to improve isolated parts of your program.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您学会了如何使用CPU分析器检测代码中的热点，并如何执行微基准测试来改进程序的孤立部分。
- en: In the next chapter, you will find out how to use data structures provided by
    the C++ standard library efficiently.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将了解如何有效使用C++标准库提供的数据结构。
