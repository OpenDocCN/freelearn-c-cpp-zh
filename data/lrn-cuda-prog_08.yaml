- en: Programming with Libraries and Other Languages
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用库和其他语言进行编程
- en: This chapter covers other GPU programming methods—programming with GPU accelerated
    libraries and other languages. Programming using the GPU accelerated libraries
    enables us to develop applications with the optimized kernels. Also, we can develop
    the CUDA software using other programming languages, which are aware of CUDA acceleration.
    Both ways improve programmability and productivity. Also, we don't have to spend
    our time optimizing the common operations, which are already optimized.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还涵盖了其他GPU编程方法——使用GPU加速库和其他语言进行编程。使用GPU加速库进行编程使我们能够开发具有优化内核的应用程序。此外，我们可以使用其他编程语言开发了解CUDA加速的CUDA软件。这两种方式都提高了可编程性和生产力。此外，我们不必花时间优化已经优化的常见操作。
- en: The CUDA Toolkit provides many GPU accelerated libraries in linear algebra,
    image and signal processing, and random processing. They are cuBLAS (Basic Linear
    Algebra Subroutines), cuFFT (Fast Fourier Transform), cuRAND (Random Number Generation), NPP
    (image and signal processing), cuSPARSE (Sparse Linear Algebra), nvGRAPH (Graph
    Analysis), cuSolver (LAPACK in GPU), Thrust (STL in CUDA), and so on. We also
    can write GPU accelerated programs with the OpenCV library. We will cover some
    of these libraries in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA工具包提供了许多线性代数、图像和信号处理以及随机处理的GPU加速库。它们包括cuBLAS（基本线性代数子程序）、cuFFT（快速傅里叶变换）、cuRAND（随机数生成）、NPP（图像和信号处理）、cuSPARSE（稀疏线性代数）、nvGRAPH（图分析）、cuSolver（GPU中的LAPACK）、Thrust（CUDA中的STL）等。我们还可以使用OpenCV库编写GPU加速程序。本章将涵盖其中一些库。
- en: We also can use GPU accelerations using R, MATLAB, Octave, and Python. Nowadays,
    Python integration is popular and powerful, as GPU can accelerate many machine
    learning and data science tasks. We also cover these languages as an entry-level.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在R、MATLAB、Octave和Python中使用GPU加速。如今，Python集成很受欢迎且功能强大，因为GPU可以加速许多机器学习和数据科学任务。我们也将这些语言作为入门级内容进行介绍。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Linear algebra operation using cuBLAS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用cuBLAS进行线性代数运算
- en: Mixed-precision operation using cuBLAS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用cuBLAS进行混合精度运算
- en: cuRAND for parallel random number generation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于并行随机数生成的cuRAND
- en: cuFFT for Fast Fourier Transformation in GPU
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于GPU的快速傅里叶变换的cuFFT
- en: NPP for image and signal processing with GPU
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GPU进行图像和信号处理的NPP
- en: Writing GPU accelerated code in OpenCV
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在OpenCV中编写GPU加速代码
- en: Writing Python code that works with CUDA
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写与CUDA配合使用的Python代码
- en: NVBLAS for zero coding acceleration in Octave and R
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Octave和R中的零编码加速NVBLAS
- en: CUDA acceleration in MATLAB
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MATLAB中的CUDA加速
- en: Linear algebra operation using cuBLAS
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuBLAS进行线性代数运算
- en: The cuBLAS library is a GPU-optimized, standard implementation of **Basic Linear
    Algebra Subroutines** (**BLAS**). Using its APIs, the programmers can write GPU-optimized,
    compute-intensive code to a single GPU or multiple GPUs. There are three levels
    in cuBLAS. Level-1 performs the vector-vector operation, level-2 does the matrix-vector
    operation, and level-3 does the matrix-matrix operation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: cuBLAS库是GPU优化的**基本线性代数子程序**（**BLAS**）的标准实现。使用其API，程序员可以将计算密集型代码优化为单个GPU或多个GPU。cuBLAS有三个级别。级别1执行矢量-矢量运算，级别2执行矩阵-矢量运算，级别3执行矩阵-矩阵运算。
- en: Covering each level is out of the scope of this book. We are just focusing on
    how to use cuBLAS APIs and extend its performance for multiple GPUs. To be specific,
    this receipt will cover a **Single Precision Floating Matrix Multiplication** (**SGEMM**)
    operation—a level-3 operation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 涵盖每个级别超出了本书的范围。我们只关注如何使用cuBLAS API并将其性能扩展到多个GPU。具体来说，本文将涵盖**单精度浮点矩阵乘法**（**SGEMM**）运算——一个三级运算。
- en: 'The cuBLAS library is a part of CUDA Toolkit, so you can use cuBLAS without
    extra installation. Also, you can use the `cc` or `cpp` file extensions, rather
    than `.cu`, because you do not need to use CUDA-specific built-in keywords such
    as `__global__` or `threadIdx`. This following code snippet shows the basic application
    of the cuBLAS function (`cubalsSgemm`):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: cuBLAS库是CUDA工具包的一部分，因此您可以在不进行额外安装的情况下使用cuBLAS。此外，您可以使用`cc`或`cpp`文件扩展名，而不是`.cu`，因为您不需要使用CUDA特定的内置关键字，如`__global__`或`threadIdx`。以下代码片段显示了cuBLAS函数（`cubalsSgemm`）的基本应用：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see, cuBLAS APIs work with the `cublasHandle_t` type handle.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，cuBLAS API使用`cublasHandle_t`类型的句柄。
- en: cuBLAS SGEMM operation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cuBLAS SGEMM运算
- en: 'The GEMM operation can be denoted by the following equation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: GEMM运算可以用以下方程表示：
- en: '![](img/e837544a-a0b8-4716-9c9d-0943d86de399.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e837544a-a0b8-4716-9c9d-0943d86de399.png)'
- en: 'Where *alpha* and *beta* are scalas and *A*, *B*, and *C *are matrices in column-major
    format. This matches with the cuBLAS function interface in the following box:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*alpha*和*beta*是标量，*A*、*B*和*C*是以列为主的矩阵。这与以下框中cuBLAS函数接口相匹配：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Before using this GEMM function, let''s look at the details of the parameters:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这个GEMM函数之前，让我们看一下参数的细节：
- en: '`transa` and `transb`: Instruction to the cuBLAS functions whether the matrices
    *A* and *B* should be transposed or not for the operation.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transa`和`transb`：cuBLAS函数的指令，用于确定是否应该对矩阵*A*和*B*进行转置操作。'
- en: '`m`, `n`, and `k`: The dimensional size of the matrices.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`m`，`n`和`k`：矩阵的维度大小。'
- en: '`alpha` and `beta`: Parameters that determine how to configure the output value
    from the source.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`和`beta`：确定如何配置源的输出值的参数。'
- en: '`*A`, `*B`, and `*C`: Linear buffer for the matrix data.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*A`，`*B`和`*C`：矩阵数据的线性缓冲区。'
- en: '`lda`: Leading column dimension of matrix *A*. cuBLAS aligns the matrix elements
    with this value.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lda`：矩阵*A*的主列维度。cuBLAS将矩阵元素与此值对齐。'
- en: '`ldb`: Leading column dimension of matrix *B*. cuBLAS aligns the matrix elements
    with this value.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ldb`：矩阵*B*的主列维度。cuBLAS将矩阵元素与此值对齐。'
- en: To transfer the data between the host and the device, you can use cuBLAS's `cublasSetMatrix()`
    and `cublasGetMatrix()` helper functions. They are wrapper functions of `cudaMemcpy()`,
    but have the matrices' dimensional information, so that they help to enhance the
    code readability; you can simply use `cudaMemcpy()` instead, of course.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要在主机和设备之间传输数据，您可以使用cuBLAS的`cublasSetMatrix()`和`cublasGetMatrix()`辅助函数。它们是`cudaMemcpy()`的包装函数，但具有矩阵的维度信息，因此它们有助于增强代码的可读性；当然，您也可以简单地使用`cudaMemcpy()`。
- en: 'Let''s just implement an application that has the GEMM operation using cuBLAS
    SGEMM function. We will include `cublas_v2.h` to use the updated cuBLAS API. For
    convenience, we will use the `getMatrix()` function to get a randomly generated
    matrix from the given dimension, and the `printMatrix()` function to print the
    matrix elements. The codes are implemented in the given example codes. In the
    main function, we will initialize three matrices—`A`, `B`, and `C`—from the given
    `M`, `N`, and `K`. Then we will compute `cublasSgemm()` as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个应用程序，使用cuBLAS SGEMM函数进行GEMM操作。我们将包括`cublas_v2.h`以使用更新的cuBLAS API。为了方便起见，我们将使用`getMatrix()`函数从给定的维度获取一个随机生成的矩阵，并使用`printMatrix()`函数打印矩阵元素。代码已在给定的示例代码中实现。在主函数中，我们将从给定的`M`、`N`和`K`初始化三个矩阵—`A`、`B`和`C`。然后我们将计算`cublasSgemm()`如下：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Compile the code with `nvcc` by linking the cuBLAS library:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过链接cuBLAS库使用`nvcc`编译代码：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following code snippet shows the output of the execution:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段显示了执行的输出：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As described in the `cublasSgemm()` function call, matrices *A* and *B* are
    transposed matrices. We passed the original leading column size to the `cublasSgemm()`
    function as `lda`, `ldb`, and `ldc`, and we could see that the operation works
    as expected.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如`cublasSgemm()`函数调用中所述，矩阵*A*和*B*是转置矩阵。我们将原始的主列大小传递给`cublasSgemm()`函数作为`lda`、`ldb`和`ldc`，我们可以看到操作按预期工作。
- en: Multi-GPU operation
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多GPU操作
- en: 'The cuBLAS library''s cuBLAS-XT API provides cuBLAS''s level-3 operation when
    it is working on multiple GPUs. With this API, your application can use multi-GPU computing
    operation. This snippet shows the basic operation for using cuBLAS-XT:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: cuBLAS库的cuBLAS-XT API在多个GPU上运行时提供了cuBLAS的3级操作。使用此API，您的应用程序可以使用多GPU计算操作。此片段显示了使用cuBLAS-XT的基本操作：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `cublasXtSgemm()` interface is the same as the `cublasSgemm()` function,
    so we can use multiple GPU''s computing performances at ease. For example, we
    can obtain the following result using the sample code in the repository with two
    GPUs. This performance can vary depending on your GPU and system configuration:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`cublasXtSgemm()`接口与`cublasSgemm()`函数相同，因此我们可以轻松使用多个GPU的计算性能。例如，我们可以使用存储库中的示例代码在两个GPU上获得以下结果。这种性能可能会因您的GPU和系统配置而有所不同：'
- en: '![](img/d2ce6a4a-c2c3-4fe3-a141-2416f6ce45de.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2ce6a4a-c2c3-4fe3-a141-2416f6ce45de.png)'
- en: 'The cuBLAS library provides a lot of versatile linear algebra operations. So
    you should check how your necessary function is provided in the library. Also,
    you will need an example of how to use that function. The following items are
    the links to the document and examples. So, it is recommended that you check both
    documents frequently when you need to implement an application based on cuBLAS:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: cuBLAS库提供了许多多功能的线性代数操作。因此，您应该检查库中提供的必要功能。此外，您将需要一个如何使用该功能的示例。以下项目是文档和示例的链接。因此，建议您在需要基于cuBLAS实现应用程序时经常检查这两个文档：
- en: NVIDIA's cuBLAS programming guide—A reference guide: [https://docs.nvidia.com/cuda/cublas/index.html](https://docs.nvidia.com/cuda/cublas/index.html)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英伟达的cuBLAS编程指南-参考指南：[https://docs.nvidia.com/cuda/cublas/index.html](https://docs.nvidia.com/cuda/cublas/index.html)
- en: <q>Matrix computations on the GPU</q>*: CUBLAS, CUSOLVER, and MAGMA by example*
    by Andrzej Chrzȩszczyk and Jacob Anders: [https://developer.nvidia.com/sites/default/files/akamai/cuda/files/Misc/mygpu.pdf](https://developer.nvidia.com/sites/default/files/akamai/cuda/files/Misc/mygpu.pdf)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <q>GPU上的矩阵计算</q>*：CUBLAS、CUSOLVER和MAGMA示例* by Andrzej Chrzȩszczyk and Jacob
    Anders: [https://developer.nvidia.com/sites/default/files/akamai/cuda/files/Misc/mygpu.pdf](https://developer.nvidia.com/sites/default/files/akamai/cuda/files/Misc/mygpu.pdf)
- en: Mixed-precision operation using cuBLAS
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuBLAS进行混合精度操作
- en: The cuBLAS library supports mixed-precision computation. This computation means
    an operation that operates with different precisions, for instance, computation
    with single and half-precision variables, or with single and characters (`INT8`).
    This technique is useful when we need to achieve a higher performance using lowered
    precision, while also obtaining a higher accuracy in the result.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: cuBLAS库支持混合精度计算。这种计算意味着使用不同精度进行操作，例如，使用单精度和半精度变量进行计算，或者使用单精度和字符(`INT8`)进行计算。当我们需要以降低精度获得更高性能，同时又获得更高准确性时，这种技术是有用的。
- en: 'The cuBLAS library provides `cublasGemmEx()` and `cublas{S/C}gemmEx()` to support
    GEMM operation for the mixed-precision operations. They are extensions of `cublas<t>gemm()`,
    which accepts specified data types for each *A*, *B*, and *C* matrices. The following
    table shows the precision support matrix for `cublasGemmEx()`, and other replaceable
    APIs in cuBLAS library:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: cuBLAS库提供了`cublasGemmEx()`和`cublas{S/C}gemmEx()`来支持混合精度操作的GEMM运算。它们是`cublas<t>gemm()`的扩展，接受每个*A*、*B*和*C*矩阵的指定数据类型。以下表格显示了`cublasGemmEx()`的精度支持矩阵，以及cuBLAS库中其他可替换的API：
- en: '| Compute type | A type / B type | C type | Replaceable APIs |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 计算类型 | A类型 / B类型 | C类型 | 可替换的API |'
- en: '| `CUDA_R_16F` | `CUDA_R_16F` | `CUDA_R_16F` | `cublasHgemm()` |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| `CUDA_R_16F` | `CUDA_R_16F` | `CUDA_R_16F` | `cublasHgemm()` |'
- en: '| `CUDA_R_32I` | `CUDA_R_8I` | `CUDA_R_32I` | N/A |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| `CUDA_R_32I` | `CUDA_R_8I` | `CUDA_R_32I` | N/A |'
- en: '| `CUDA_R_32F` | `CUDA_R_16F` | `CUDA_R_16F` | `cublasSgemmEx()` |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `CUDA_R_32F` | `CUDA_R_16F` | `CUDA_R_16F` | `cublasSgemmEx()` |'
- en: '| `CUDA_R_8I` | `CUDA_R_32F` |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `CUDA_R_8I` | `CUDA_R_32F` |'
- en: '| `CUDA_R_16F` | `CUDA_R_32F` |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `CUDA_R_16F` | `CUDA_R_32F` |'
- en: '| `CUDA_R_32F` | `CUDA_R_32F` |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `CUDA_R_32F` | `CUDA_R_32F` |'
- en: '| `CUDA_R_64F` | `CUDA_R_64F` | `CUDA_R_64F` | `cublasDgemm()` |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `CUDA_R_64F` | `CUDA_R_64F` | `CUDA_R_64F` | `cublasDgemm()` |'
- en: '| `CUDA_C_32F` | `CUDA_C_8I` | `CUDA_C_32F` | `cublasCgemmEx()` |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `CUDA_C_32F` | `CUDA_C_8I` | `CUDA_C_32F` | `cublasCgemmEx()` |'
- en: '| `CUDA_C_32F` | `CUDA_C_32F` |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `CUDA_C_32F` | `CUDA_C_32F` |'
- en: '| `CUDA_C_64F` | `CUDA_C_64F` | `CUDA_C_64F` | `cublasZgemm()` |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `CUDA_C_64F` | `CUDA_C_64F` | `CUDA_C_64F` | `cublasZgemm()` |'
- en: You can see that `cublasGemmEx()` can cover the `cublas{S/C}gemmEx()` function's
    operation. Therefore, we will cover `cublasGemmEx()` in this section.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到`cublasGemmEx()`可以覆盖`cublas{S/C}gemmEx()`函数的操作。因此，我们将在本节中介绍`cublasGemmEx()`。
- en: The last parameter of the `cublasGemmEx()` function, `cublasGemmAlgo_t`, specifies
    the algorithm for matrix-matrix multiplication. With this parameter, we can choose
    whether to use TensorCore or not. `CUBLAS_GEMM_DEFAULT` selects the GEMM algorithm
    and runs on CUDA cores. On the other hand, `CUBLAS_GEMM_DEFAULT_TENSOR_OP` selects
    algorithms that use tensor cores. If TensorCore is unavailable for the given condition,
    cuBLAS selects an algorithm that uses CUDA cores. This condition can be for the
    GPUs having no tensor cores or matrix size, which does not fit with how the TensorCore
    operates—in multiples of four (** 4*).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`cublasGemmEx()`函数的最后一个参数`cublasGemmAlgo_t`指定了矩阵乘法的算法。有了这个参数，我们可以选择是否使用TensorCore。`CUBLAS_GEMM_DEFAULT`选择GEMM算法并在CUDA核心上运行。另一方面，`CUBLAS_GEMM_DEFAULT_TENSOR_OP`选择使用张量核心的算法。如果给定条件下TensorCore不可用，cuBLAS会选择使用CUDA核心的算法。这种情况可能是没有张量核心的GPU或矩阵大小不符合张量核心操作的方式，即以四的倍数（**
    4*）。'
- en: GEMM with mixed precision
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合精度的GEMM
- en: 'Now, let''s try the mixed-precision using the cuBLAS GEMM operation. After
    the implementation, we will cover how the matrix size can affect the operations.
    The fully implemented version is in `02_sgemm_mixed_precision/cublasGemmEx.cu`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用cuBLAS GEMM操作进行混合精度。实现后，我们将介绍矩阵大小如何影响操作。完全实现的版本在`02_sgemm_mixed_precision/cublasGemmEx.cu`中：
- en: 'This code uses a custom memory managing class, `CBuffer`, to ease the handling
    of mixed precisions and copy, but it can use unified memory instead. For the cuBLAS
    operation, we should include `cublas_v2.h` in our code:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此代码使用自定义内存管理类`CBuffer`来简化混合精度和复制的处理，但也可以使用统一内存。对于cuBLAS操作，我们应该在代码中包含`cublas_v2.h`：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s implement the `main()` function. First, we will create and initialize
    `A`, `B`, and `C` matrices. The following snippet shows how to use the `CBuffer` class
    and initialize the matrices:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们实现`main()`函数。首先，我们将创建和初始化`A`、`B`和`C`矩阵。以下代码片段显示了如何使用`CBuffer`类并初始化矩阵：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To specify the precision types of `A`, `B`, and `C`, and to test the various
    precisions together, we need to specify some CUDA data type parameters:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定`A`、`B`和`C`的精度类型，并测试各种精度，我们需要指定一些CUDA数据类型参数：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For the cuBLAS operation, we should initialize `cublas_handle`, `alpha`, and
    `beta`:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于cuBLAS操作，我们应该初始化`cublas_handle`、`alpha`和`beta`：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we copy the data to the GPU:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将数据复制到GPU：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We then call `cublasGemmEx()` function as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们调用`cublasGemmEx()`函数如下：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To review the matrix values, we can use `printMatrix()`, which is defined in `helper.h`:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看矩阵的值，我们可以使用`helper.h`中定义的`printMatrix()`：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `printMatrix()` is defined using the function overriding method to allow
    the printing of half-precision values with the same format in other data types.
    Part of the definitions is as follows:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`printMatrix()`使用函数重载方法定义，以允许以其他数据类型相同格式打印半精度值。部分定义如下：'
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, the code will have a GEMM operation to the given `A`, `B`, and `C` matrices.
    The following shows an example of the output when `M` is `4`, `N` is `5`, and
    `M` is `6`:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，代码将对给定的`A`、`B`和`C`矩阵进行GEMM操作。以下是当`M`为`4`，`N`为`5`，`M`为`6`时的输出示例：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let''s try with the other data types and see how `cublasGemmEx()` operates
    to the given matrices. The provided example also outputs the operation''s execution
    time to measure the performance:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试其他数据类型，并查看`cublasGemmEx()`如何对给定的矩阵进行操作。提供的示例还输出了操作的执行时间以衡量性能：
- en: What should we modify if matrix *A* or matrix *B* is the transposed matrix?
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果矩阵*A*或矩阵*B*是转置矩阵，我们应该修改什么？
- en: Is there any preferable matrix size to the operation? Compare the execution
    time by changing the size.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有任何更适合的矩阵大小进行操作？通过改变大小来比较执行时间。
- en: Is there any preferable matrix size for each data type? If you try the `INT8`
    precision, you will see errors. How this can be fixed? Change the size and see
    how the `INT8` operation can be supported in `cublasGemmEx()`.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每种数据类型是否有任何更适合的矩阵大小？如果尝试`INT8`精度，会出现错误。如何解决这个问题？改变大小并查看`cublasGemmEx()`如何支持`INT8`操作。
- en: GEMM with TensorCore
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorCore的GEMM
- en: TensorCore provides the accelerated performance of tensor's dot operations.
    It supports FP16 in the Volta architecture, and `INT8` and `INT4` in the Turing
    architecture. Therefore, we should use reduced precision or mixed precision to
    use TensorCore.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: TensorCore提供了张量点运算的加速性能。它支持Volta架构中的FP16，Turing架构中的`INT8`和`INT4`。因此，我们应该使用降低的精度或混合精度来使用TensorCore。
- en: Previously we used `CUBLAS_GEMM_DEFAULT` as the cuBLAS GEMM algorithm, which
    uses CUDA cores in their operation. To use TensorCore, we should use `CUBLAS_GEMM_DEFAULT_TENSOR_OP`.
    To utilize the TensorCore, each dimension of your operand matrices should be a
    multiple of 4\. That is the unit size of TensorCore's **WMMA** (short for, **Warp
    Matrix Multiply Accumulate**) operation optimization internally. For instance,
    matrix-matrix multiplication with *A* (8,192 × 8,192) and *B* (8,192 × 8,192)
    shows a much higher performance against the operation with *A* (8,192 × 8,192)
    and *B* (8,192 × 8,190). You can also confirm this operation via a profile.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们使用`CUBLAS_GEMM_DEFAULT`作为cuBLAS GEMM算法，它在操作中使用CUDA核心。要使用TensorCore，我们应该使用`CUBLAS_GEMM_DEFAULT_TENSOR_OP`。为了利用TensorCore，您的操作数矩阵的每个维度都应该是4的倍数。这是TensorCore的**WMMA**（**Warp
    Matrix Multiply Accumulate**的缩写）操作优化的单位大小。例如，矩阵*A*（8,192×8,192）和*B*（8,192×8,192）的矩阵乘法与*A*（8,192×8,192）和*B*（8,192×8,190）的操作相比，性能要高得多。您也可以通过性能分析来确认这个操作。
- en: 'The following timeline is a result of a matrix multiplication using matrix
    *A* (8,192 × 8,192) and matrix *B* (8,192 × 8,190):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下时间线是使用矩阵*A*（8,192×8,192）和矩阵*B*（8,192×8,190）进行矩阵乘法的结果：
- en: '![](img/ceedb286-f967-4a78-b6d1-31340f23ae90.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ceedb286-f967-4a78-b6d1-31340f23ae90.png)'
- en: 'Furthermore, this timeline image is a result of a matrix multiplication from
    matrix *A* (8,192 × 8,192) and matrix *B* (8,192 × 8,192):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这个时间线图像是矩阵*A*(8,192×8,192)和矩阵*B*(8,192×8,192)的矩阵乘法的结果：
- en: '![](img/3430e8a9-267e-4233-9876-ee4deb8c06af.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3430e8a9-267e-4233-9876-ee4deb8c06af.png)'
- en: Both tests use `CUBLAS_GEMM_DEFAULT_TENSOR_OP` in CUDA C/C++, but the GEMM operation
    with TensorCore is 6.7x faster than with CUDA cores. As TensorCore is available
    based on the matrix size, `nvcc` compiles the code with the special kernel functions,
    starting with `volta_s884g`. In conclusion, pad your matrices to align with 4,
    if you want to get benefits of TensorCore. This can be an overhead, but performance
    gain from TensorCore may overwhelm the overhead.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 两个测试都在CUDA C/C++中使用`CUBLAS_GEMM_DEFAULT_TENSOR_OP`，但是使用TensorCore的GEMM操作比使用CUDA核心快6.7倍。由于TensorCore基于矩阵大小可用，`nvcc`将使用特殊的内核函数编译代码，从`volta_s884g`开始。总之，如果要获得TensorCore的好处，可以将矩阵填充到4的倍数。这可能会增加开销，但是TensorCore的性能收益可能会超过开销。
- en: NVIDIA provides how-to programming TensorCores using the cuBLAS library in their
    development blog site ([https://devblogs.nvidia.com/programming-tensor-cores-cuda-9](https://devblogs.nvidia.com/programming-tensor-cores-cuda-9)).
    This document also introduces other available methods. But, using the cuBLAS library
    provides the fastest performance for you, as proven following a paper from Oak
    Ridge National Laboratory—*NVIDIA Tensor Core Programmability, **Performance and
    Precision* ([https://arxiv.org/pdf/1803.04014.pdf](https://arxiv.org/pdf/1803.04014.pdf)).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA在其开发博客网站提供了如何使用cuBLAS库编程TensorCores的方法([https://devblogs.nvidia.com/programming-tensor-cores-cuda-9](https://devblogs.nvidia.com/programming-tensor-cores-cuda-9))。本文档还介绍了其他可用的方法。但是，使用cuBLAS库可以为您提供最快的性能，正如来自奥克岭国家实验室的一篇论文所证明的那样——*NVIDIA
    Tensor Core可编程性、性能和精度* ([https://arxiv.org/pdf/1803.04014.pdf](https://arxiv.org/pdf/1803.04014.pdf))。
- en: cuRAND for parallel random number generation
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于并行随机数生成的cuRAND
- en: Many applications use the pseudo-random number for their simulation or probabilistic
    analysis. In spite of its conventional usages, a large number of random number
    generation procedure took much time. One solution is to generate random numbers
    in parallel, but each multiple thread should have different random seeds in order
    to generate random numbers independently.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用程序在模拟或概率分析中使用伪随机数。尽管它的常规用途，但大量的随机数生成过程需要很长时间。一个解决方案是并行生成随机数，但每个多线程应该有不同的随机种子，以便独立生成随机数。
- en: The cuRAND library enables GPU to generate a number of random numbers from GPU.
    This library is available from the host or from the device code. The host API
    enables the generation of random numbers only using the host code. Therefore,
    you can use the generated data directly for other kernel functions. The device
    API enables the generation of random numbers in kernel code, so you can make CUDA
    threads that have their own randomly generated numbers during the execution.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: cuRAND库使GPU能够从GPU生成大量随机数。此库可从主机代码或设备代码中使用。主机API仅允许使用主机代码生成随机数。因此，您可以直接将生成的数据用于其他内核函数。设备API允许在内核代码中生成随机数，因此您可以在执行过程中创建具有自己随机生成数的CUDA线程。
- en: cuRAND host API
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cuRAND主机API
- en: 'First, you need to create a new generator of the desired type, using `curandGenerator()`.
    Then, set the generator options of the desired seed and order. For example, you
    can generate the pseudo-random generator using `curandSetPseudoRandomGeneratorSeed()`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要使用`curandGenerator()`创建所需类型的新生成器。然后，设置所需种子和顺序的生成器选项。例如，您可以使用`curandSetPseudoRandomGeneratorSeed()`生成伪随机生成器：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, you can generate random numbers using `curandGenerate()`. There are nine
    different generation functions. For example, you can generate uniformly distributed
    floating-point values using `curandGenerateUnifrom()`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用`curandGenerate()`生成随机数。有九种不同的生成函数。例如，您可以使用`curandGenerateUnifrom()`生成均匀分布的浮点值：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The cuRAND programming guide provides descriptions of various kinds of generation
    functions: [https://docs.nvidia.com/cuda/curand/host-api-overview.html#generation-functions](https://docs.nvidia.com/cuda/curand/host-api-overview.html#generation-functions).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: cuRAND编程指南提供了各种生成函数的描述：[https://docs.nvidia.com/cuda/curand/host-api-overview.html#generation-functions](https://docs.nvidia.com/cuda/curand/host-api-overview.html#generation-functions)。
- en: 'After the use of random number generation, you can terminate the cuRAND generator
    with the destroyer function:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用随机数生成后，您可以使用销毁函数终止cuRAND生成器：
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, let's implement an application that generates random numbers using several
    cuRAND APIs. The fully implemented version is `03_curand/curand_host.cpp`. So,
    you can modify the code and test other functions as you need.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个使用几个cuRAND API生成随机数的应用程序。完全实现的版本是`03_curand/curand_host.cpp`。因此，您可以根据需要修改代码并测试其他函数。
- en: 'At first, we should include `curand.h` for the cuRAND host APIs and other CPP-related
    header files as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该包括`curand.h`用于cuRAND主机API和其他与CPP相关的头文件，如下所示：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s assume that we will create a matrix that is initialized with random
    numbers. We need to implement the `printMatrix()` function in order to review
    the generated random numbers as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将创建一个用随机数初始化的矩阵。我们需要实现`printMatrix()`函数，以便查看生成的随机数，如下所示：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we will allocate the required memory space as follows. Now, we will implement
    the `main()` function that initializes random numbers and prints the result using
    `printMatrix()`. First, we will initialize the cuRAND handle for the operation
    as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将按以下方式分配所需的内存空间。现在，我们将实现`main()`函数，该函数初始化随机数并使用`printMatrix()`打印结果。首先，我们将为操作初始化cuRAND句柄，如下所示：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You can change the random seed as you want. The next thing is to allocate memory
    space. To ease the evaluation of the operation, we will use a unified memory,
    because cuRAND functions will generate random numbers on GPU:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以根据需要更改随机种子。接下来要做的是分配内存空间。为了简化操作的评估，我们将使用统一内存，因为cuRAND函数将在GPU上生成随机数：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we will generate random numbers to the given memory space. We will use
    the integer memory space (`np_random`) for random number generation, and floating
    memory space (`fp_random`) for uniformly distributed random numbers, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在给定的内存空间中生成随机数。我们将使用整数内存空间（`np_random`）进行随机数生成，使用浮点内存空间（`fp_random`）进行均匀分布的随机数生成，如下所示：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Because we are using a unified memory, we can allow the GPU and the host to
    share the same memory address, and we can review the output values by synchronizing
    them. Finally, we can terminate the cuRAND handle and memories as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们使用统一内存，我们可以允许GPU和主机共享相同的内存地址，并且可以通过同步它们来查看输出值。最后，我们可以终止cuRAND句柄和内存，如下所示：
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, its time to compile and run the code. Compiling the code using cuRAND
    APIs should provide `-lcurand` for the `nvcc` compiler. When `M = 3` and `N =
    5`, the outputs are as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候编译和运行代码了。使用cuRAND API编译代码应该为`nvcc`编译器提供`-lcurand`。当`M = 3`和`N = 5`时，输出如下：
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We have covered how CUDA can generate random numbers using the host API, but,
    in some cases, it is better to design the CUDA kernels to generate random numbers.
    We call this the device API, and we can obtain random numbers from each CUDA thread.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了CUDA如何使用主机API生成随机数，但在某些情况下，最好设计CUDA内核来生成随机数。我们称之为设备API，我们可以从每个CUDA线程获取随机数。
- en: cuRAND device API
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cuRAND设备API
- en: Using the device API, we can set the generator seed and generate random numbers
    on your CUDA device.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用设备API，我们可以在CUDA设备上设置生成器种子并生成随机数。
- en: 'Firstly, we need to prepare a device memory space of `curandState_t`, in order
    to store generator seeds to provide the random seed to the CUDA threads in parallel.
    This can be done like a normal device memory allocation code as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要准备一个`curandState_t`的设备内存空间，以便并行地为CUDA线程提供随机种子。这可以像正常的设备内存分配代码一样完成，如下所示：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In your kernel code, we need to initialize random seeds using `curand_init()`.
    This function requires seed, sequence number, and offset. Then, this function
    sets up the state. For the same seed, cuFFT always generates the same state. To
    generate random values, use the `curand()` function. Like the host''s generation
    function, the device API has various generation functions. For example, uniformly
    distributed random number generation can be done like this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的内核代码中，我们需要使用`curand_init()`初始化随机种子。这个函数需要种子、序列号和偏移量。然后，这个函数设置状态。对于相同的种子，cuFFT总是生成相同的状态。要生成随机值，使用`curand()`函数。与主机的生成函数一样，设备API有各种生成函数。例如，均匀分布的随机数生成可以这样做：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The cuRAND library provides various generation functions for the various data
    types and stochastic distributions. To find your desired generation function,
    check the cuRAND developer guide''s device API overview. After the random number
    generation, the device states buffer should be terminated like normal memory,
    as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: cuRAND库提供了各种数据类型和随机分布的生成函数。要找到您需要的生成函数，请查看cuRAND开发人员指南的设备API概述。在随机数生成之后，设备状态缓冲区应该像正常内存一样终止，如下所示：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, we will create an application that uses cuRAND device APIs. The fully
    implemented codes are `curand_device.cu`, so you can modify and test the code
    too. Firstly, we should include the `curand_kernel.h` file with other C++ required
    header files as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个使用cuRAND设备API的应用程序。完全实现的代码是`curand_device.cu`，所以您也可以修改和测试代码。首先，我们应该包括`curand_kernel.h`文件和其他C++所需的头文件，如下所示：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will write `setup_kernel()` that initializes a random seed for each CUDA
    thread as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编写`setup_kernel()`，为每个CUDA线程初始化一个随机种子，如下所示：
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Write two random number generation functions: `generate_kernel()` and `generate_uniform_kernel()`.
    We will generate a 32-bit integer and a single floating point with uniformly distributed
    random numbers:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 编写两个随机数生成函数：`generate_kernel()`和`generate_uniform_kernel()`。我们将生成一个32位整数和一个均匀分布的单精度浮点数随机数：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we will implement the `main()` function and initialize the device states
    buffer:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现`main()`函数并初始化设备状态缓冲区：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, generate random numbers using `generate_kernel()`. For convenience, we
    will use unified memory for space and validate the output from the host. After
    that, we will print out the result as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用`generate_kernel()`生成随机数。为了方便起见，我们将使用统一内存空间，并验证来自主机的输出。之后，我们将打印结果，如下所示：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the same way, we will create uniformly distributed random numbers using
    `generate_uniform_kernel()` as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们将使用`generate_uniform_kernel()`创建均匀分布的随机数，如下所示：
- en: '[PRE33]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Because we are using unified memory, we can allow the GPU and the host to share
    the same memory address, and we can review the output values by synchronizing
    them. Finally, we can terminate the cuRAND handle and memories as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们使用统一内存，我们可以允许GPU和主机共享相同的内存地址，并且可以通过同步它们来查看输出值。最后，我们可以终止cuRAND句柄和内存，如下所示：
- en: '[PRE34]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, its time to compile and run the code. In order to compile the code using
    cuRAND, APIs should provide `-lcurand` for the `nvcc` compiler. When `M` equals
    `3` and `N` equals `5`, the outputs are as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候编译和运行代码了。为了使用cuRAND编译代码，API应该为`nvcc`编译器提供`-lcurand`。当`M`等于`3`，`N`等于`5`时，输出如下：
- en: '[PRE35]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: When you compare the output numbers from the host API and the device API, the
    generated random numbers are the same, whereas the uniform random numbers are
    not. This can be resolved if you reset the random seed ahead of the second random
    number generation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当您比较主机API和设备API的输出数字时，生成的随机数是相同的，而均匀随机数不是。如果在第二次随机数生成之前重置随机种子，这个问题可以得到解决。
- en: cuRAND with mixed precision cuBLAS GEMM
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cuRAND与混合精度cuBLAS GEMM
- en: 'Previously, we have used the C++ random number generator to initialize matrices
    for a GEMM operation. This function is handy when we want to generate random numbers
    in general. However, you may find that this function took a long time to generate
    large random numbers in the last section. In this section, we will cover how cuRAND
    API can work with the cuBLAS GEMM operations. The fully implemented version is
    the `gemm_with_curand_host.cpp` file. Let''s see how this was implemented:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们使用C++随机数生成器来初始化GEMM操作的矩阵。当我们想要生成随机数时，这个函数很方便。然而，您可能会发现，在上一节中生成大量随机数需要很长时间。在本节中，我们将介绍cuRAND
    API如何与cuBLAS GEMM操作配合使用。完全实现的版本是`gemm_with_curand_host.cpp`文件。让我们看看这是如何实现的：
- en: 'Currently, we don''t have a low-precision random number generator in the cuRAND
    library. Also, we need to convert the half-precision numbers to float in order
    to evaluate the output. For these reasons, we need to create type conversion functions
    on GPU as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前，cuRAND库中没有低精度的随机数生成器。此外，我们需要将半精度数转换为浮点数以评估输出。因此，我们需要在GPU上创建类型转换函数，如下所示：
- en: '[PRE36]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, we will write a random number generation function that uses the cuRAND
    host API. As we discussed before, we should convert the generated random numbers
    from float to half, when we need to use half-precision data. This function can
    be implemented as follows:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将编写一个使用cuRAND主机API的随机数生成函数。正如我们之前讨论的，当我们需要使用半精度数据时，我们应该将生成的随机数从浮点数转换为半精度。这个函数可以实现如下：
- en: '[PRE37]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Define some local variables that control GEMM operations in the `main()` function:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`main()`函数中定义一些控制GEMM操作的本地变量：
- en: '[PRE38]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In this code, we determine the GEMM operation size, data type, and operation
    type.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们确定了GEMM操作的大小、数据类型和操作类型。
- en: 'Now, let''s create input buffer arrays, and set parameters, along with the
    operation precision:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建输入缓冲区数组，并设置参数，以及操作精度：
- en: '[PRE39]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Create cuRAND and cuBLAS handles as follows:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建cuRAND和cuBLAS句柄如下：
- en: '[PRE40]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Then, we should determine the operation type in order to use TensorCores:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们应该确定操作类型以使用TensorCores：
- en: '[PRE41]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then, we can call the `cublasGemmEx()` function that affords FP32 and FP16
    operations as follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以调用`cublasGemmEx()`函数，提供FP32和FP16操作，如下所示：
- en: '[PRE42]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The GEMM operation should show a similar performance when compared to the previous
    version. But, you may find that the whole application speed is enhanced, since
    the parallel random number generation on the GPU is much faster than the generation
    from the host.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前的版本相比，GEMM操作应该表现出类似的性能。但是，您可能会发现整个应用程序的速度得到了提高，因为GPU上的并行随机数生成比主机上的生成要快得多。
- en: The cuRAND developer guide will help you to find other random number generators,
    options, and distributions. This document is located at [https://docs.nvidia.com/pdf/CURAND_Library.pdf](https://docs.nvidia.com/pdf/CURAND_Library.pdf).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: cuRAND开发人员指南将帮助您找到其他随机数生成器、选项和分布。该文档位于[https://docs.nvidia.com/pdf/CURAND_Library.pdf](https://docs.nvidia.com/pdf/CURAND_Library.pdf)。
- en: cuFFT for Fast Fourier Transformation in GPU
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于GPU中的快速傅里叶变换的cuFFT
- en: The cuFFT library provides GPU accelerated operations for the **FFT** (short
    for, **Fast Fourier Transform**) algorithm. The programmers can transform real
    or complex data using GPU computing power, and apply GPU kernel operations for
    the transformed signal. Also, the supported functions are matched with the FFTW
    library, so we can migrate the host project to the GPU.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: cuFFT库为**FFT**（快速傅里叶变换）算法提供了GPU加速操作。程序员可以利用GPU计算能力转换实数或复数数据，并对转换后的信号应用GPU核操作。此外，支持的函数与FFTW库匹配，因此我们可以将主机项目迁移到GPU。
- en: To handle the FFT sample's dimensional information, cuFFT is required to create
    a plan handle using `cufftPlan1D()`, `cufftPlan2D()`, or `cufftPlan3D()`, accordingly.
    If sample data has a batched and stride layout, we should use `cufftPlanMany()`.
    If the sample size is greater than 4 GB, we should use `64` as a suffix to the
    plan functions to support that size. For example, `cufftPlanMany64()` supports
    larger samples on top of the `cufftPlanMany()` function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理FFT样本的维度信息，需要使用cuFFT来使用`cufftPlan1D()`、`cufftPlan2D()`或`cufftPlan3D()`来创建一个计划句柄。如果样本数据具有批处理和步幅布局，我们应该使用`cufftPlanMany()`。如果样本大小大于4GB，我们应该使用`64`作为计划函数的后缀来支持该大小。例如，`cufftPlanMany64()`支持`cufftPlanMany()`函数之上的更大样本。
- en: 'The cuFFT library supports multi-GPU operations. First, you need to create
    an empty plan using `cufftCreate()`. Then, we can specify the list of GPUs that
    will carry out the operation using `cufftXtSetGPUs()`. After that, we can generate
    a plan using normal plan generation functions, which we have previously covered.
    The following table shows the plan generation function categories:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: cuFFT库支持多GPU操作。首先，您需要使用`cufftCreate()`创建一个空计划。然后，我们可以使用`cufftXtSetGPUs()`指定将执行操作的GPU列表。之后，我们可以使用先前介绍过的普通计划生成函数生成一个计划。以下表格显示了计划生成函数的类别：
- en: '|  | Basic plans | Multi-GPU plans |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: 基本计划 | 多GPU计划
- en: '| Simple plan | `cufftPlan{1d,2d,3d}()` | `cufftMakePlan{1d,2d,3d}()` |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: 简单计划 | `cufftPlan{1d,2d,3d}()` | `cufftMakePlan{1d,2d,3d}()`
- en: '| Advanced data layout | `cufftPlanMany()` | `cufftMakePlanMany()` |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: 高级数据布局 | `cufftPlanMany()` | `cufftMakePlanMany()`
- en: '| FP16 operation |  `cufftXtMakePlanMany()` |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: FP16操作 | `cufftXtMakePlanMany()`
- en: 'Then you can forward (FFT) and inverse (IFFT) to your sample data using the
    `cufftExec()` function. The cuFFT library provides three kinds of data transformation:
    complex-to-complex, real-to-complex, and complex-to-real. Its operation data type
    can be a float or a double:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用`cufftExec()`函数对样本数据进行前向（FFT）和反向（IFFT）变换。cuFFT库提供三种数据转换：复杂到复杂、实到复杂和复杂到实。其操作数据类型可以是浮点数或双精度数。
- en: '| Transform direction | Float | Double |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: 变换方向 | 浮点数 | 双精度
- en: '| Complex-to-complex | `cufftExecC2C()` `cufftXtExecDescriptorC2C()` | `cufftExecZ2Z()`
    `cufftXtExecDescriptorZ2Z()` |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: 复杂到复杂 | `cufftExecC2C()` `cufftXtExecDescriptorC2C()` | `cufftExecZ2Z()` `cufftXtExecDescriptorZ2Z()`
- en: '| Real-to-complex | `cufftDExecR2C()` `cufftXtExecDescriptorR2C()` | `cufftExecD2Z()`
    `cufftXtExecDescriptorD2Z()` |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 实到复 | `cufftDExecR2C()` `cufftXtExecDescriptorR2C()` | `cufftExecD2Z()` `cufftXtExecDescriptorD2Z()`
    |'
- en: '| Complex-to-real | `cufftExecC2R()` `cufftXtExecDescriptorC2R()` | `cufftExecZ2D()`
    `cufftXtExecDesciptorZ2D()` |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 复到实 | `cufftExecC2R()` `cufftXtExecDescriptorC2R()` | `cufftExecZ2D()` `cufftXtExecDesciptorZ2D()`
    |'
- en: '| All | `cufftXtExec()` / `cufftXtExecDesciptor()` |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 所有 | `cufftXtExec()` / `cufftXtExecDesciptor()` |'
- en: The cuFFT operation is either *forward* or *inverse*, and the operation should
    be paired with the other direction.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: cuFFT操作要么是*正向*要么是*反向*，操作应该与另一个方向配对。
- en: The functions that transform between the real data and the complex data, such
    as `R2C` and `C2R`, have implicit directional information in their function name.
    This feature helps you to avoid having to have an additional operation in order
    to convert your data in the real domain to a complex data type. Meanwhile, you
    have to create an additional plan since each plan has transformation direction
    information.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数名称中，将实数据和复杂数据之间进行转换的函数，例如`R2C`和`C2R`，具有隐含的方向信息。这个特性可以帮助您避免必须进行额外的操作来将您的数据从实域转换为复杂数据类型。同时，由于每个计划都有变换方向信息，因此您必须创建一个额外的计划。
- en: On the other hand, you have to provide the transform direction information for
    the complex-to-complex transformation, such as `C2C` and `Z2Z`. For the inversion
    operation, you don't have to create another cuFFT handle, because the plan should
    be the same data type operation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，您必须为复到复变换提供变换方向信息，例如`C2C`和`Z2Z`。对于反演操作，您不必创建另一个cuFFT句柄，因为计划应该是相同数据类型的操作。
- en: The `cufftXtExec()` and `cufftXtExecDescriptor()` functions can perform transformation
    on any given data type, since every input data should be provided with their data
    type information when you create a cuFFT plan.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`cufftXtExec()`和`cufftXtExecDescriptor()`函数可以对任何给定的数据类型执行变换，因为在创建cuFFT计划时应该提供每个输入数据的数据类型信息。'
- en: Basic usage of cuFFT
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cuFFT的基本用法
- en: 'Now let''s try to use cuFFT. The fully implemented version is the `04_cufft/cufft.1d.cpp`
    file. Let''s discuss how this is implemented:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试使用cuFFT。完全实现的版本是`04_cufft/cufft.1d.cpp`文件。让我们讨论一下它是如何实现的：
- en: 'Firstly, start with some header files: C++, CUDA, cuRAND, and cuFFT:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从一些头文件开始：C++，CUDA，cuRAND和cuFFT：
- en: '[PRE43]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In this FFT operation, we will have both real-to-complex and complex-to-real
    transformations. Therefore, let''s declare some custom data types, `Real` and
    `Complex`, in order to simplify the code. This can be done as follows:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个FFT操作中，我们将进行实到复和复到实的转换。因此，让我们声明一些自定义数据类型，`Real`和`Complex`，以简化代码。可以这样做：
- en: '[PRE44]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, let''s begin with the `main()` function. For the input sample data, we
    will use unified memory in order to ease the data transfer between the host and
    the GPU. The transformed data may only be used on the GPU. Therefore, memory space
    can be allocated as follows:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们从`main()`函数开始。对于输入的样本数据，我们将使用统一内存以便简化主机和GPU之间的数据传输。转换后的数据只能在GPU上使用。因此，可以分配内存空间如下：
- en: '[PRE45]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, we will use the cuRAND host API to initialize the input data as follows:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用cuRAND主机API来初始化输入数据如下：
- en: '[PRE46]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'And, we should initialize the cuFFT plan for forward and inverse transforms.
    Since they have different data types we should create two plans, respectively,
    for real-to-complex and complex-to-real transformations:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 而且，我们应该初始化cuFFT计划以进行正向和反向变换。由于它们具有不同的数据类型，我们应该分别为实到复和复到实的转换创建两个计划：
- en: '[PRE47]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, we can have forward or inverse transforms, with the given cuFFT plans.
    In order to measure the execution time, we can embrace these operations using
    CUDA events:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用给定的cuFFT计划进行正向或反向变换。为了测量执行时间，我们可以使用CUDA事件来包含这些操作：
- en: '[PRE48]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Then, we can compile the code with the following commands:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下命令编译代码：
- en: '[PRE49]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The `cufft.1d` command will report its transform time for each step as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`cufft.1d`命令将报告每个步骤的变换时间如下：'
- en: '[PRE50]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: cuFFT with mixed precision
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合精度的cuFFT
- en: The cuFFT library provides extended CUDA computing features, such as the FP16
    FFT operation. The full version is the `cufft.half.cpp` file. Let's discuss its
    implementation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: cuFFT库提供了扩展的CUDA计算功能，例如FP16 FFT操作。完整版本是`cufft.half.cpp`文件。让我们讨论它的实现。
- en: 'In this code, we should use `cufftXtMakePlanMany()` for the plan creation and
    the `cufftXtExec()` function for the transformation. `cufftXtMakePlanMany()` allows
    the passing of input and output data types if they are FP16 or FP32\. Also, we
    should create two plans for forward and inverse transformation, in order to cover
    real-to-complex and complex-to-real transformations. To an empty cuFFT plan, `cufftXtMakePlanMany()` can
    specify the sample size, the input data format and type, the batch size, and so
    on. For example, plan creations can be implemented as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们应该使用`cufftXtMakePlanMany()`来创建计划，使用`cufftXtExec()`函数进行变换。`cufftXtMakePlanMany()`允许传递输入和输出数据类型，如果它们是FP16或FP32。此外，我们应该为正向和反向变换创建两个计划，以涵盖实到复和复到实的转换。对于空的cuFFT计划，`cufftXtMakePlanMany()`可以指定样本大小、输入数据格式和类型、批处理大小等。例如，计划的创建可以实现如下：
- en: '[PRE51]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In this implementation, we also have to consider whether to provide the input
    data in half-precision. You may use the host random function and convert them
    into half-precision data, but, this code shows you how the cuRAND host API can
    be used for this purpose, as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，我们还必须考虑是否提供半精度的输入数据。您可以使用主机随机函数并将它们转换为半精度数据，但是，这段代码向您展示了如何使用cuRAND主机API来实现此目的，如下所示：
- en: '[PRE52]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'So, we can provide half-precision on uniformly distributed random numbers for
    FFT, and we can use `cufftXtExec()` for the forward and inverse transformations.
    Transformation performance is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以为FFT提供半精度的均匀分布随机数，并且我们可以使用`cufftXtExec()`进行正向和反向变换。变换性能如下：
- en: '[PRE53]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: cuFFT for multi-GPU
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多GPU的cuFFT
- en: 'Another usage of cuFFT is having a large FFT operation using multiple GPUs.
    To do this, we have to create an empty cuFFT plan using `cufftCreate()`, and provide
    GPU numbers using `cufftXtSetGPUs()`. For example, this can be done as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: cuFFT的另一个用途是使用多个GPU进行大型FFT操作。为了做到这一点，我们必须使用`cufftCreate()`创建一个空的cuFFT计划，并使用`cufftXtSetGPUs()`提供GPU数量。例如，可以这样做：
- en: '[PRE54]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The total number of GPUs can vary depending on the system. Now, we can generate
    the cuFFT plan using `cufftXtMakePlanMany()` to specify the sample information.
    For instance, `cufftXtMakePlanMany()` can be called like this:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: GPU的总数可能会根据系统而有所不同。现在，我们可以使用`cufftXtMakePlanMany()`生成cuFFT计划，以指定样本信息。例如，可以这样调用`cufftXtMakePlanMany()`：
- en: '[PRE55]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The cuFFT library provides `cufftXtMalloc()`, which prepares the GPU memory
    space for the target GPUs. Then, we can copy our data to the allocated memory
    using the `cufftXtMemcpy()` function. For example, this can be implemented as
    follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: cuFFT库提供了`cufftXtMalloc()`，它为目标GPU准备GPU内存空间。然后，我们可以使用`cufftXtMemcpy()`函数将我们的数据复制到分配的内存中。例如，可以这样实现：
- en: '[PRE56]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Then, we can execute FFT on multi-GPUs with the `cufftXtExecDesciptor()` function:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`cufftXtExecDesciptor()`函数在多个GPU上执行FFT。
- en: '[PRE57]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Using `nvidia-smi`, we can monitor the distributed memory allocation and execution
    across the GPUs. The elapsed time can be different, depending on your GPUs and
    system configuration.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`nvidia-smi`，我们可以监视分布式内存分配和跨GPU的执行。经过的时间可能会有所不同，这取决于你的GPU和系统配置。
- en: If you want to learn more about the cuFFT library and its functions, the cuFFT
    library user guide ([https://docs.nvidia.com/cuda/cufft/index.html](https://docs.nvidia.com/cuda/cufft/index.html))
    is a good reference for you.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于cuFFT库及其函数的信息，cuFFT库用户指南([https://docs.nvidia.com/cuda/cufft/index.html](https://docs.nvidia.com/cuda/cufft/index.html))是一个很好的参考资料。
- en: CUDA sample code is another good reference for learning how to use cuFFT functions.
    The sample codes are placed in the `NVIDIA_CUDA-10.x_Samples/7_CUDALibraries/CUFFT*`
    directory. You can learn how to apply filter operations using CUDA kernel code,
    and by working with cuFFT's forward/backward transformations.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA示例代码是学习如何使用cuFFT函数的另一个很好的参考。示例代码放在`NVIDIA_CUDA-10.x_Samples/7_CUDALibraries/CUFFT*`目录中。你可以学习如何使用CUDA核心代码应用滤波操作，以及通过cuFFT的正向/反向变换来实现。
- en: NPP for image and signal processing with GPU
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NPP用于GPU图像和信号处理
- en: The **NPP** (short for, **NVIDIA Performance Primitive**) library is a default
    CUDA library with a set of GPU accelerated processing functions that focus on
    imaging and video processing. While it enables flexible development in these fields,
    the developers can save their application development time.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**NPP**（NVIDIA性能基元）库是一个默认的CUDA库，其中包含一组GPU加速处理函数，专注于图像和视频处理。虽然它能够在这些领域灵活开发，但开发人员可以节省应用程序开发时间。'
- en: 'The NPP library has two functional parts: imaging-processing APIs, and signal-processing
    APIs. The image-processing APIs include tools relating to image filtering, compression/decompression,
    color transformation, resizing, color conversion, statistical operations, and
    so on. The signal-processing APIs are filtering, conversion, and so on. You can
    visit the NPP''s document ([https://docs.nvidia.com/cuda/npp](https://docs.nvidia.com/cuda/npp)),
    and see its configurations and the full list of functionalities.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: NPP库有两个功能部分：图像处理API和信号处理API。图像处理API包括与图像滤波、压缩/解压缩、颜色转换、调整大小、颜色转换、统计操作等相关的工具。信号处理API包括滤波、转换等。你可以访问NPP的文档([https://docs.nvidia.com/cuda/npp](https://docs.nvidia.com/cuda/npp))，查看它的配置和完整的功能列表。
- en: CUDA provides many NPP-based samples. In this section, we will cover the basic
    use of the NPP library and discuss its application.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA提供了许多基于NPP的示例。在本节中，我们将介绍NPP库的基本用法并讨论其应用。
- en: Image processing with NPP
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NPP进行图像处理
- en: 'First, we will cover how the NPP library can ease an image-processing task.
    Before doing this, we should install the FreeImage library in order to be able
    to load and write a JPEG compressed image file easily. There are three options
    that can be used to prepare the library:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍NPP库如何简化图像处理任务。在这之前，我们应该安装FreeImage库，以便能够轻松地加载和写入JPEG压缩图像文件。有三种选项可以用来准备库：
- en: 'Installation from the Ubuntu archive:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Ubuntu存档中安装：
- en: '[PRE58]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Build from the source code and install:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从源代码构建和安装：
- en: '[PRE59]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Use the library that has already been installed with the CUDA Toolkit. An NPP
    sample code, `7_CUDALibraries/freeImageInteropNPP`, in CUDA sample code uses the
    FreeImage library. For this sample, NPP header files and library files are installed
    at `7_CUDALibrires/common/FreeImage` in the CUDA sample directory. You may use
    this if you prefer not to install other binaries into your machine.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用已经安装了CUDA Toolkit的库。CUDA示例代码中的NPP示例代码`7_CUDALibraries/freeImageInteropNPP`使用了FreeImage库。对于这个示例，NPP头文件和库文件安装在CUDA示例目录中的`7_CUDALibrires/common/FreeImage`。如果你不想在你的机器上安装其他二进制文件，你可以使用这个。
- en: 'Now, let''s implement the NPP-based image-processing application. The fully
    implemented code is `05_npp/imageFilter.cpp`. This file begins with the header
    files:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现基于NPP的图像处理应用程序。完全实现的代码是`05_npp/imageFilter.cpp`。这个文件以头文件开始：
- en: '[PRE60]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'In this application, it has the `ImageInfo_t` structure to easily manage image
    information and data:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个应用程序中，它有`ImageInfo_t`结构来方便地管理图像信息和数据：
- en: '[PRE61]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Write the `LoadImage()` function in order to load a JPEG image. The `FreeImage`
    library supports any other image format, so you can try other images as you want.
    Then, we will fill the source image information managing structure with the loaded
    image data. The `loadImage()` function is implemented as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 编写`LoadImage()`函数以加载JPEG图像。`FreeImage`库支持任何其他图像格式，所以你可以根据需要尝试其他图像。然后，我们将用加载的图像数据填充源图像信息管理结构。`loadImage()`函数的实现如下：
- en: '[PRE62]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Then, write some NPPI helper functions that provide the NPPI image size and
    the NPPI ROI size data from the image structure as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，编写一些NPPI辅助函数，从图像结构中提供NPPI图像大小和NPPI ROI大小数据，如下所示：
- en: '[PRE63]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Then, let''s implement the NPPI-based image resizing function as follows. In
    this function, we will use `nppiResize_8u_C3R()`, which was discussed at the beginning. NPP
    APIs have naming convention rules to explicitly clarify their operation. Depending
    on their functional categories, their naming starts with `nppi` for the image
    processing, and `npps`for the signal processing. For instance, an NPP image-processing
    function, `nppiResize_8u_C3R()`, begins with the `nppi` prefix, and it resizes
    input data with an unsigned char data type in three channels to the given ROI
    (you can learn more detail about this convention in the document):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们实现基于NPPI的图像调整大小函数。在此函数中，我们将使用一开始讨论的`nppiResize_8u_C3R()`。NPP API有命名约定规则，以明确说明它们的操作。根据其功能类别，它们的命名以`nppi`开头用于图像处理，以`npps`开头用于信号处理。例如，一个NPP图像处理函数`nppiResize_8u_C3R()`以`nppi`前缀开头，它将具有三个通道的无符号字符数据类型的输入数据调整大小到给定的ROI（您可以在文档中了解更多关于这种约定的细节）：
- en: '[PRE64]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'To compare the performance with the CPU, we will use a FreeImage''s function,
    as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与CPU的性能进行比较，我们将使用FreeImage的一个函数，如下所示：
- en: '[PRE65]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now, let''s implement the `main()` function. At first, we should initialize
    the FreeImage library and load an image:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现`main()`函数。首先，我们应该初始化FreeImage库并加载一个图像：
- en: '[PRE66]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Then, we will initialize the GPU memory space for the input image, as follows.
    In this procedure, we initialize the global memory space with an NPPI function
    and transfer the loaded image into the global memory using `cudaMemcpy2D()`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将初始化输入图像的GPU内存空间，如下所示。在此过程中，我们将使用NPPI函数初始化全局内存空间，并使用`cudaMemcpy2D()`将加载的图像传输到全局内存中：
- en: '[PRE67]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'After that, we will initialize the output memory space with the resized image
    size information as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将初始化输出内存空间，并提供调整后的图像大小信息，如下所示：
- en: '[PRE68]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Then, we call the `ResizeGPU()` and `ResizeCPU()` functions, which we have
    implemented already. For each operation, we will use `cudaEvent` to measure the execution
    time on the GPU:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们调用已经实现的`ResizeGPU()`和`ResizeCPU()`函数。对于每个操作，我们将使用`cudaEvent`来测量GPU上的执行时间：
- en: '[PRE69]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'For verification, we will save the result to the file. To do this, we should
    create a FreeImage bitmap, and copy the resized image into the memory space. Then,
    we can save an output image, as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证，我们将结果保存到文件中。为此，我们应该创建一个FreeImage位图，并将调整大小后的图像复制到内存空间中。然后，我们可以保存输出图像，如下所示：
- en: '[PRE70]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'After that, we can finally terminate the related resources:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们最终可以终止相关资源：
- en: '[PRE71]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Compile the code using `nvcc` with the linked NPP and FreeImage library:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 使用链接的NPP和FreeImage库使用`nvcc`编译代码：
- en: '[PRE72]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'As a result, when the scale factor is 0.5 f, the image size is reduced like
    this:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当比例因子为0.5 f时，图像大小会减小如下：
- en: '[PRE73]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The measured elapsed time is `0.04576 ms` using V100\. Its time can vary depending
    on the GPU:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 使用V100测得的经过时间为`0.04576 ms`。它的时间可能会因GPU而异：
- en: '[PRE74]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'For more detail on the use of NPP for image processing, visit and see the linked
    document: [http://on-demand.gputechconf.com/gtc/2014/presentations/HANDS-ON-LAB-S4793-image-processing-using-npp.pdf](http://on-demand.gputechconf.com/gtc/2014/presentations/HANDS-ON-LAB-S4793-image-processing-using-npp.pdf).'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 有关NPP用于图像处理的更多详细信息，请访问并查看链接的文档：[http://on-demand.gputechconf.com/gtc/2014/presentations/HANDS-ON-LAB-S4793-image-processing-using-npp.pdf](http://on-demand.gputechconf.com/gtc/2014/presentations/HANDS-ON-LAB-S4793-image-processing-using-npp.pdf)。
- en: Signal processing with NPP
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NPP进行信号处理
- en: NPP also provides signal-processing features. The main difference to image-processing
    APIs is that they do not require image-shape-related information. As we continue
    to cover the basic usage of NPP functions, we will find out how we can obtain
    the sum, min/max, mean, and L2 normalized distribution value from the given arrays.
    The fully written code is `05_npp/statisticsNPP.cpp`.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: NPP还提供了信号处理功能。与图像处理API的主要区别在于，它们不需要图像形状相关的信息。随着我们继续介绍NPP函数的基本用法，我们将了解如何从给定的数组中获取总和、最小/最大值、均值和L2归一化分布值。完整的代码是`05_npp/statisticsNPP.cpp`。
- en: 'First, let''s begin with the required header file:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从所需的头文件开始：
- en: '[PRE75]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'As an input data, we will use randomly generated numbers:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用随机生成的数字作为输入数据：
- en: '[PRE76]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Before we call the statistical operation functions, we need a temporary memory
    space for their operations. We can obtain the required size using other NPP functions
    that are related to the operations, and we can create a common workspace memory
    space:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用统计操作函数之前，我们需要一个临时内存空间进行操作。我们可以使用与操作相关的其他NPP函数来获取所需的大小，并创建一个公共工作空间内存空间：
- en: '[PRE77]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let''s begin with the `main()` function. At first, we will begin with input
    data preparation, and getting to know the required workspace memory space. We
    will prepare two input data types, and compare their differences using NPP:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`main()`函数开始。首先，我们将开始准备输入数据，并了解所需的工作空间内存空间。我们将准备两种输入数据类型，并使用NPP比较它们的差异：
- en: '[PRE78]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'After that, we will allocate GPU memory space for the input/output and workspace.
    We will also transfer the input data as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将为输入/输出和工作空间分配GPU内存空间。我们还将按以下方式传输输入数据：
- en: '[PRE79]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now, let''s do some simple statistical operations, using NPP functions:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用NPP函数进行一些简单的统计操作：
- en: '[PRE80]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'NPP also provides functions that report the differences between the two inputs
    as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: NPP还提供了报告两个输入之间差异的函数，如下所示：
- en: '[PRE81]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Then, we terminate the used memories. After that, let''s compile the code with
    the following command:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们终止使用的内存。之后，让我们用以下命令编译代码：
- en: '[PRE82]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'As a result, we obtain the result as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到的结果如下：
- en: '[PRE83]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Applications of NPP
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NPP的应用
- en: In this section, we have covered the filtering in image processing, and statistical
    operations in signal processing. Although we have tried simple applications, we
    may find that NPP programming is much easier than kernel implementation. For this
    reason, NPP is applied to many media transcoding filters, bath image-processing
    applications, pre-processing of images in computer vision or deep learning, and
    so on.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经涵盖了图像处理中的滤波和信号处理中的统计操作。尽管我们尝试了简单的应用程序，但我们可能会发现NPP编程比内核实现要容易得多。因此，NPP被应用于许多媒体转码滤镜、浴图像处理应用程序、计算机视觉或深度学习中的图像预处理等领域。
- en: Writing GPU accelerated code in OpenCV
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在OpenCV中编写GPU加速的代码
- en: The OpenCV library is quite a popular library in computer vision. It supports
    GPU programming in order to benefit performance at higher resolutions in the computer
    vision area. In this section, we will cover how to use a GPU with OpenGL.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV库在计算机视觉中非常受欢迎。它支持GPU编程，以便在计算机视觉领域的更高分辨率下获得更好的性能。在本节中，我们将介绍如何在OpenGL中使用GPU。
- en: CUDA-enabled OpenCV installation
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA-enabled OpenCV installation
- en: 'To start OpenCV programming with CUDA, you need to compile the OpenCV library
    with the CUDA feature enabled. Follow this to enable OpenCV in Ubuntu:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用CUDA进行OpenCV编程，您需要使用启用CUDA功能的OpenCV库进行编译。按照以下步骤在Ubuntu中启用OpenCV：
- en: '[PRE84]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'If your system can use X window (not a server), install other packages to enable
    the GTK dialog:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的系统可以使用X窗口（而不是服务器），请安装其他软件包以启用GTK对话框：
- en: '[PRE85]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Download the source code and untar them using the following commands. This
    was tested with OpenCV, which were the latest OpenCV versions at the time of writing:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 下载源代码并使用以下命令解压它们。这是在撰写时测试的OpenCV，这是最新的OpenCV版本：
- en: '[PRE86]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Now, let''s compile the downloaded source code using the following commands. You
    can put other options if you want. Its compilation takes a while:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令编译下载的源代码。如果需要，您可以添加其他选项。它的编译需要一些时间：
- en: '[PRE87]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'To confirm the installation use the following command:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 要确认安装，请使用以下命令：
- en: '[PRE88]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'In OpenCV 4, CUDA-related functions and classes are defined in CUDA namespaces.
    For instance, you can create a CUDA global memory space using this command:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV 4中，CUDA相关的函数和类在CUDA命名空间中定义。例如，您可以使用此命令创建CUDA全局内存空间：
- en: '[PRE89]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Then, the device `cuda_mem` memory space can be handled like a normal CPU memory
    type (`cv::Mat`).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，设备`cuda_mem`内存空间可以像正常的CPU内存类型（`cv::Mat`）一样处理。
- en: Implementing a CUDA-enabled blur filter
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现CUDA-enabled模糊滤镜
- en: 'Now, we will implement a tiny GPU-enabled OpenCV application and compare its
    performance. Let''s begin by including the required header files:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现一个小型的GPU-enabled OpenCV应用程序并比较其性能。让我们首先包括所需的头文件：
- en: '[PRE90]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Here is the host blur filter implementation using OpenCV:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用OpenCV的主机模糊滤镜实现：
- en: '[PRE91]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'And this is the CUDA-enabled blur filter implementation:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这是CUDA-enabled模糊滤镜的实现：
- en: '[PRE92]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'This receipt code shows how the `bilateralFilter()` operation matches with
    the host, and how CUDA matches with the CUDA namespace. For the CUDA memory manipulation, `cv::cuda::GpuMat` is
    used for the device memory, and the device memory provides `upload()` and `download()` member
    functions, such as `cudaMemcpy()`. To measure the elapsed time, `cv::TickMeter` was
    used. Then, `main()` calls both implementations, as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码显示了`bilateralFilter()`操作如何与主机匹配，以及CUDA如何与CUDA命名空间匹配。对于CUDA内存操作，`cv::cuda::GpuMat`用于设备内存，并且设备内存提供`upload()`和`download()`成员函数，例如`cudaMemcpy()`。为了测量经过的时间，使用了`cv::TickMeter`。然后，`main()`调用了两种实现，如下所示：
- en: '[PRE93]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Now, let''s compile the code. We should include the OpenCV header files and
    libraries using ``pkg-config --cflag opencv`` in your compilation option. For
    example, the compilation option can be written like this:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编译代码。我们应该在编译选项中使用``pkg-config --cflag opencv``包含OpenCV头文件和库。例如，编译选项可以写成这样：
- en: '[PRE94]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Then, the output result is as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，输出结果如下：
- en: '[PRE95]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: The execution time can be different depending on your system and the GPU.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 执行时间可能会因系统和GPU而异。
- en: Enabling multi-stream processing
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启用多流处理
- en: 'In OpenCV, the CUDA stream is managed with `cv::cuda::Stream`. Using this,
    we can have multi-stream-based pipelining GPU operations:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中，CUDA流使用`cv::cuda::Stream`进行管理。使用这个，我们可以进行基于多流的管道化GPU操作：
- en: 'As we know, the host memory should be a pinned memory in order to have asynchronous
    data transfer:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们知道，主机内存应该是固定的内存，以便进行异步数据传输：
- en: '[PRE96]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Then, we will create multiple streams, as follows:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建多个流，如下所示：
- en: '[PRE97]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'And, we load the source image and initialize the GPU memory based on the loaded
    image information as follows:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们加载源图像并根据加载的图像信息初始化GPU内存，如下所示：
- en: '[PRE98]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Now, we will transfer the image to the GPU, blur the image, and transfer it
    back to the host with each stream:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用每个流将图像传输到GPU，对图像进行模糊处理，然后将其传输回主机：
- en: '[PRE99]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Then, we have to synchronize the host and the GPU. To do this, we will use
    the `cv::Stream.waitForCompletion()` function that can synchronize for each stream
    after they finish the data transfer to the host:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须同步主机和GPU。为此，我们将使用`cv::Stream.waitForCompletion()`函数，在每个流完成数据传输到主机后进行同步：
- en: '[PRE100]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'To compare the performance with the CPU, we also call `cv::bilateralFilter()`
    as follows:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了与CPU性能进行比较，我们也调用`cv::bilateralFilter()`如下：
- en: '[PRE101]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Its execution time is as follows. The GPU execution time is the average of
    the measured time from the multi-stream execution loop to synchronization:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 其执行时间如下。GPU执行时间是从多流执行循环到同步的测量时间的平均值：
- en: '[PRE102]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'In order to confirm the multi-stream operation, we can profile the operation.
    The following screenshot shows this:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确认多流操作，我们可以对操作进行分析。以下截图显示了这一点：
- en: '![](img/aea5b6e4-ce11-4d84-9057-0c5466df0094.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aea5b6e4-ce11-4d84-9057-0c5466df0094.png)'
- en: Profiling the operation
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 对操作进行分析
- en: The first operation on the default stream is warp-up execution, and a four-multi-stream
    operation follows. Here, we can see that the GPU operations are overlapped. For
    this reason, the average execution time is shorter than for a one-stream execution.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 默认流上的第一个操作是预热执行，接下来是四个多流操作。在这里，我们可以看到GPU操作是重叠的。因此，平均执行时间比单流执行时间短。
- en: We have only covered bilateral filtering in OpenCV. However, many OpenCV features
    support CUDA acceleration, so that you can get the benefits of GPU computing.
    Its interface is consistent with the CPU version, so you can easily migrate your
    CPU version to the GPU.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只介绍了OpenCV中的双边滤波。然而，许多OpenCV功能支持CUDA加速，因此您可以获得GPU计算的好处。其接口与CPU版本一致，因此您可以轻松将CPU版本迁移到GPU。
- en: 'As an introductory level, there are some useful materials from GTC:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 作为入门级别，有一些来自GTC的有用材料：
- en: '[http://on-demand.gputechconf.com/gtc/2013/webinar/opencv-gtc-express-shalini-gupta.pdf](http://on-demand.gputechconf.com/gtc/2013/webinar/opencv-gtc-express-shalini-gupta.pdf)'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://on-demand.gputechconf.com/gtc/2013/webinar/opencv-gtc-express-shalini-gupta.pdf](http://on-demand.gputechconf.com/gtc/2013/webinar/opencv-gtc-express-shalini-gupta.pdf)'
- en: '[http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-itseez-opencv-webinar.pdf](http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-itseez-opencv-webinar.pdf)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-itseez-opencv-webinar.pdf](http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-itseez-opencv-webinar.pdf)'
- en: '[http://developer.download.nvidia.com/GTC/PDF/1085_Fung.pdf](http://developer.download.nvidia.com/GTC/PDF/1085_Fung.pdf)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://developer.download.nvidia.com/GTC/PDF/1085_Fung.pdf](http://developer.download.nvidia.com/GTC/PDF/1085_Fung.pdf)'
- en: It is recommended that you start with OpenCV's reference guide: [https://docs.opencv.org/4.1.1/d2/dbc/cuda_intro.html](https://docs.opencv.org/4.1.1/d2/dbc/cuda_intro.html).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 建议您从OpenCV的参考指南开始：[https://docs.opencv.org/4.1.1/d2/dbc/cuda_intro.html](https://docs.opencv.org/4.1.1/d2/dbc/cuda_intro.html)。
- en: Writing Python code that works with CUDA
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写与CUDA兼容的Python代码
- en: Nowadays, many people use CUDA with Python. It works not only as a glue of binaries,
    but it also enables to us write GPU accelerated code directly. As a glue language,
    Python can call the APIs from the CUDA C/C++ libraries, using `pybind11` ([https://github.com/pybind/pybind11](https://github.com/pybind/pybind11))
    or SWIG ([http://swig.org/](http://swig.org/)). However, we have to write CUDA
    C/C++ codes and integrate them into the Python application.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，许多人使用Python与CUDA。它不仅作为二进制文件的粘合剂，还使我们能够直接编写GPU加速的代码。作为粘合语言，Python可以调用来自CUDA
    C/C++库的API，使用`pybind11` ([https://github.com/pybind/pybind11](https://github.com/pybind/pybind11))
    或 SWIG ([http://swig.org/](http://swig.org/))。但是，我们必须编写CUDA C/C++代码并将其集成到Python应用程序中。
- en: However, there are Python packages—Numba, CuPy, and PyCUDA—that enable GPU programming
    with Python. They provide native accelerated APIs and wrappers for CUDA kernels.
    In other words, we don't have to write C/C++ code and spend our time performing
    integration. Numba provides a vectorization and CUDA **just-in-time** (**jit**)
    compiler to accelerate its operation. It is compatible with NumPy, so you can
    accelerate your numerical computing code based on NumPy. You can also write flexible
    CUDA code in Python thanks to the jit compiler. CuPy is also NumPy compatible
    and accelerates linear algebra algorithms. It provides Pythonic programmability
    and transparent custom kernel programming, such as Numba. PyCUDA provides a CUDA
    C/C++ interface, so that you can write and use the CUDA kernel function in your
    Python code.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，有一些Python软件包—Numba、CuPy和PyCUDA—可以使用Python进行GPU编程。它们为CUDA内核提供了本机加速的API和包装器。换句话说，我们不必编写C/C++代码并花费时间进行集成。Numba提供了矢量化和CUDA即时（jit）编译器来加速其操作。它与NumPy兼容，因此您可以加速基于NumPy的数值计算代码。您还可以通过jit编译器在Python中编写灵活的CUDA代码。CuPy也与NumPy兼容，并加速线性代数算法。它提供了Pythonic的可编程性和透明的自定义内核编程，如Numba。PyCUDA提供了CUDA
    C/C++接口，因此您可以在Python代码中编写和使用CUDA内核函数。
- en: Numba – a high-performance Python compiler
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Numba – 高性能Python编译器
- en: Numba ([https://numba.pydata.org/](https://numba.pydata.org/)) translates Python
    functions for execution on the GPU without any C/C++ programming.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: Numba ([https://numba.pydata.org/](https://numba.pydata.org/)) 可以将Python函数翻译成在GPU上执行，而无需进行任何C/C++编程。
- en: 'In Numba, you can easily write vectorized functions by applying the Numba decorator
    to the target function:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在Numba中，您可以通过将Numba装饰器应用于目标函数轻松编写矢量化函数：
- en: '[PRE103]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'As you can see, the decorator specifies the parameters and return data types,
    and the target specifies which architecture that code will operate. There are
    three kinds of targets:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，装饰器指定了参数和返回数据类型，目标指定了代码将在哪种架构上运行。有三种目标：
- en: '| Target | Description | Recommended data size and operation |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 描述 | 推荐的数据大小和操作 |'
- en: '| `cuda` | Targeting NVIDIA GPU | Larger than 1 MB, compute-intensive operation
    |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| `cuda` | 针对NVIDIA GPU | 大于1MB，计算密集型操作 |'
- en: '| `parallel` | Optimized for multi-core CPU | Less than 1 MB, normal operation
    |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| `parallel` | 优化多核CPU | 小于1MB，正常操作 |'
- en: '| `cpu` | Optimized for single thread operation | Less than 1 KB, low compute-intensive
    operation |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| `cpu` | 优化单线程操作 | 小于1KB，低计算密集型操作 |'
- en: If your function does not return a value, use `@guvectorize`, and specify the
    parameter as the vectors.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的函数不返回值，请使用`@guvectorize`，并将参数指定为向量。
- en: 'Another use of Numba is with the `@cuda.jit` decorator. This enables you to
    write CUDA-specific operations like the following:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: Numba的另一个用途是使用`@cuda.jit`装饰器。这使您能够编写类似以下的CUDA特定操作：
- en: '[PRE104]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'The `cuda.grid()` keyword provides the CUDA threads index in grid-level, so
    that you can write the kernel code, such as CUDA C/C++ code, in the Python way.
    Calling the CUDA kernel function can be done as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuda.grid()` 关键字提供了CUDA线程在网格级别的索引，因此您可以以Python的方式编写内核代码，例如CUDA C/C++代码。调用CUDA内核函数可以按以下方式完成：'
- en: '[PRE105]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Now, let's install this package and try some examples.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们安装这个软件包并尝试一些示例。
- en: Installing Numba
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Numba
- en: 'To use Numba in your Python code, you need to install the package, and configure
    the environment variables:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python代码中使用Numba，您需要安装该软件包，并配置环境变量：
- en: '[PRE106]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'You would need to put the environment variable settings at the end of `.bashrc`
    or `.zshrc`, for the ease of future use. If they are not set, Python will return
    this message:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要将环境变量设置放在`.bashrc`或`.zshrc`的末尾，以便将来使用时更加方便。如果它们没有设置，Python将返回此消息：
- en: '[PRE107]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: Using Numba with the @vectorize decorator
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用带有@vectorize装饰器的Numba
- en: 'We will test the `@vectorize` decorator with a simple `saxpy` operation. This
    converts a specific function to work in parallel:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`@vectorize`装饰器测试一个简单的`saxpy`操作。这将把一个特定的函数转换为并行工作：
- en: Create `numba_saxpy.py`.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`numba_saxpy.py`。
- en: 'Import `numba`, `numpy`, and any other required packages:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numba`、`numpy`和其他所需的软件包：
- en: '[PRE108]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Write a `saxpy` code with the `@vectorize` decorator and target with `''cuda''`
    in order to work on a CUDA device:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`@vectorize`装饰器编写一个带有目标`'cuda'`的`saxpy`代码，以便在CUDA设备上工作：
- en: '[PRE109]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Write a saxpy code with the `@vecotrize` decorator and target with `''parallel''`
    to work on a multi-core processor (host):'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用带有`@vecotrize`装饰器和目标为`'parallel'`的Numba编写saxpy代码，以在多核处理器（主机）上工作：
- en: '[PRE110]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Write an operation code to call the functions with some NumPy-generated input
    data:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个操作代码，调用一些NumPy生成的输入数据的函数：
- en: '[PRE111]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'This code reports the elapsed time with the various operand sizes:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码报告了各种操作数大小的耗时：
- en: '[PRE112]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: In this situation, CUDA shows slower performance than the CPU, because the operation
    is simple, but data transfer overhead is heavy.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，CUDA的性能比CPU慢，因为操作很简单，但数据传输开销很大。
- en: Using Numba with the @cuda.jit decorator
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用带有@cuda.jit装饰器的Numba
- en: 'We also can write sophisticated operations to work on the GPU with Numba using
    the `@cuda.jit` decorator:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`@cuda.jit`装饰器编写复杂的操作，以在GPU上使用Numba：
- en: Create `numba_matmul.py`.
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`numba_matmul.py`。
- en: 'Import `numpy`, `numba`, and any other required packages:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`、`numba`和其他所需的软件包：
- en: '[PRE113]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Write a matrix multiplication code with the `@cuda.jit` decorator:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`@cuda.jit`装饰器编写矩阵乘法代码：
- en: '[PRE114]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: In this code, we use `cuda.grid(dimension_size)` to specify the CUDA thread
    index among the grid, so, we can specify the index of the CUDA threads in Python.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们使用`cuda.grid(dimension_size)`来指定网格中的CUDA线程索引，因此，我们可以在Python中指定CUDA线程的索引。
- en: 'Create the `a` and `b` matrices as a NumPy matrix:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`a`和`b`矩阵作为NumPy矩阵：
- en: '[PRE115]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Copy the NumP- generated data to the device:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将NumPy生成的数据复制到设备：
- en: '[PRE116]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Create the `c` matrix that will be placed in the CUDA device memory:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建将放置在CUDA设备内存中的`c`矩阵：
- en: '[PRE117]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Call the matrix multiplication kernel function:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用矩阵乘法内核函数：
- en: '[PRE118]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Copy the output to the host:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出复制到主机：
- en: '[PRE119]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Compare the CUDA operation with the host:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将CUDA操作与主机进行比较：
- en: '[PRE120]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'With a `@cuda.jit` decorator and the built-in `cuda.grid()` keywords, this
    sample code shows how simple it is to implement Numba into the matrix multiplication
    in Python. This code reports the operation-elapsed time on both the device and
    the host:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`@cuda.jit`装饰器和内置的`cuda.grid()`关键字，这个示例代码展示了在Python中实现Numba到矩阵乘法是多么简单。这段代码报告了设备和主机上的操作耗时：
- en: '[PRE121]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Now, let's cover the CuPy that enables more Pythonic programming in CUDA programming.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来介绍CuPy，它可以在CUDA编程中实现更多的Python编程。
- en: CuPy – GPU accelerated Python matrix library
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CuPy- GPU加速Python矩阵库
- en: CuPy ([https://cupy.chainer.org](https://cupy.chainer.org)) enables linear algebra
    accelerations using Python and fully utilizes GPUs by using CUDA libraries. It
    is NumPy compatible and provides enjoyable Pythonic programmability.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: CuPy ([https://cupy.chainer.org](https://cupy.chainer.org)) 使用Python实现线性代数加速，并通过使用CUDA库充分利用GPU。它与NumPy兼容，并提供了愉快的Python编程体验。
- en: Let's cover its installation, basic usage, and manual kernel developments.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来介绍它的安装、基本用法和手动内核开发。
- en: Installing CuPy
  id: totrans-407
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装CuPy
- en: 'We can use `pip` to install CuPy using the following command. Then it also
    installs the `cupy` package and the CUDA dependencies:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令使用`pip`安装CuPy。然后它还会安装`cupy`包和CUDA依赖项：
- en: '[PRE122]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: Now, let's cover the basic usage of CuPy.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来介绍CuPy的基本用法。
- en: Basic usage of CuPy
  id: totrans-411
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CuPy的基本用法
- en: 'We can write a saxpy operation as follows:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以编写一个saxpy操作，如下所示：
- en: '[PRE123]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'We also can use the `matmul()` function for the matrix multiplication as follows:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`matmul()`函数进行矩阵乘法：
- en: '[PRE124]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'As we discussed earlier, CuPy is compatible with NumPy. Basically, the previous
    CuPy''s object is CuPy''s array type:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，CuPy与NumPy兼容。基本上，之前的CuPy对象是CuPy的数组类型：
- en: '[PRE125]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'However, we can convert that using the `cupy.asnumpy()` function to convert
    to the NumPy array as follows:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们可以使用`cupy.asnumpy()`函数将其转换为NumPy数组，如下所示：
- en: '[PRE126]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'The reverse is also available using `cupy.ascupy()` function. Therefore, we
    can do the following operation based on this compatibility:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用`cupy.asnumpy()`函数进行反向操作。因此，我们可以基于这种兼容性进行以下操作：
- en: '[PRE127]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: As you can see, we can easily switch the target computing process, and we can
    benefit from each platform's advantages. Now, let's cover the custom kernel implementation
    using CuPy.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们可以轻松切换目标计算过程，并且可以从每个平台的优势中受益。现在，让我们来介绍使用CuPy进行自定义内核实现。
- en: Implementing custom kernel functions
  id: totrans-423
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现自定义内核函数
- en: 'CuPy provides three types of custom kernel function: elementwise, reduction
    and raw kernels. The elementwise kernel helps with the automatic indexing for
    each element. Therefore, we can just write an element''s operation. The reduction
    kernel carries out the reduction operation, while also performing the user-defined
    operation. The raw kernel enables direct CUDA C/C++ kernel programming on Python
    codes, so that we can define any operation on it. In this section, we will not
    cover all of them. However, you can learn more from the relevant documentation—[https://docs-cupy.chainer.org/en/stable/tutorial/kernel.html](https://docs-cupy.chainer.org/en/stable/tutorial/kernel.html).'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: CuPy提供了三种自定义内核函数：elementwise、reduction和raw kernels。elementwise内核有助于为每个元素进行自动索引。因此，我们只需编写一个元素的操作。reduction内核执行减少操作，同时执行用户定义的操作。raw
    kernel可以在Python代码上直接进行CUDA C/C++内核编程，因此我们可以对其进行任何操作。在本节中，我们不会涵盖所有内容。但是，您可以从相关文档中了解更多信息-[https://docs-cupy.chainer.org/en/stable/tutorial/kernel.html](https://docs-cupy.chainer.org/en/stable/tutorial/kernel.html)。
- en: 'Let''s discuss the user-defined elementwise kernel implementation. Here is
    an example of elementwise operation:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论用户定义的逐元素内核实现。这是一个逐元素操作的示例：
- en: '[PRE128]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Then, we can do the elementwise operation without the explicit indexing operation:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以执行逐元素操作，而无需显式的索引操作：
- en: '[PRE129]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: As you can see, CuPy provides a highly Pythonic interface and is easy to learn.
    There are lots of internal routines, which are also compatible with NumPy—[https://docs-cupy.chainer.org/en/stable/reference/routines.html](https://docs-cupy.chainer.org/en/stable/reference/routines.html).
    In other words, we can consider using CuPy when we need accelerated computations
    in NumPy.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在此代码中所看到的，CuPy提供了一个高度Pythonic的接口，并且易于学习。还有许多内部例程，这些例程也与NumPy兼容—[https://docs-cupy.chainer.org/en/stable/reference/routines.html](https://docs-cupy.chainer.org/en/stable/reference/routines.html)。换句话说，当我们需要在NumPy中进行加速计算时，可以考虑使用CuPy。
- en: Now, we will cover PyCUDA, which provides direct kernel programming and implicit
    memory management wrappers.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将介绍PyCUDA，它提供了直接的内核编程和隐式内存管理包装器。
- en: PyCUDA – Pythonic access to CUDA API
  id: totrans-431
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyCUDA – Pythonic access to CUDA API
- en: PyCUDA ([https://documen.tician.de/pycuda/](https://documen.tician.de/pycuda/))
    enables us to write CUDA C/C++ codes in Python codes, and execute them without
    compilation. In this way, you can write CUDA C/C++ codes that are CUDA-specific
    operations. But, you have to optimize this code yourself, since PyCUDA doesn't
    optimize your kernel functions.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: PyCUDA（[https://documen.tician.de/pycuda/](https://documen.tician.de/pycuda/)）使我们能够在Python代码中编写CUDA
    C/C++代码，并在不编译的情况下执行它们。通过这种方式，您可以编写CUDA C/C++代码，这些代码是特定于CUDA的操作。但是，由于PyCUDA不会优化您的内核函数，因此您必须自行优化此代码。
- en: 'This is a snippet of code that was produced using PyCUDA:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用PyCUDA生成的代码片段：
- en: '[PRE130]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'As you can see in this code, we can write the kernel code using the same Python
    code. We also can retain signs of required data transfer using `driver.In()` and
    `driver.Out()`. These indicate that PyCUDA should transfer the data before invoking
    the kernel. The data transfers automatically, and we also can transfer the data
    as follows:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在此代码中所看到的，我们可以使用相同的Python代码编写内核代码。我们还可以使用`driver.In()`和`driver.Out()`保留所需数据传输的标志。这表明PyCUDA在调用内核之前应传输数据。数据传输是自动的，我们也可以按以下方式传输数据：
- en: '[PRE131]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: Now, let's install PyCUDA and try some simple examples.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们安装PyCUDA并尝试一些简单的示例。
- en: Installing PyCUDA
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装PyCUDA
- en: To use PyCUDA, you also need to install the package. Download the PyCUDA source
    file from the website ([https://pypi.org/project/pycuda/](https://pypi.org/project/pycuda/)).
    At the moment, version 2019.1.1 is in use.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用PyCUDA，还需要安装该软件包。从网站（[https://pypi.org/project/pycuda/](https://pypi.org/project/pycuda/)）下载PyCUDA源文件。目前正在使用2019.1.1版本。
- en: 'Then install the dependencies as follows:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按照以下方式安装依赖项：
- en: '[PRE132]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: If you want to use Python 2, skip using Python 3 for the `configure.py` command.
    The configuration command can be different depending on your Python version.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要使用Python 2，请跳过使用Python 3进行`configure.py`命令。配置命令可能会因您的Python版本而异。
- en: Matrix multiplication using PyCUDA
  id: totrans-443
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyCUDA进行矩阵乘法
- en: 'We can perform matrix multiplication using PyCUDA in the following manner:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用PyCUDA以以下方式执行矩阵乘法：
- en: Create a `pycuda_matmul.py` file.
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`pycuda_matmul.py`文件。
- en: 'Import the required packages as follows:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的软件包如下：
- en: '[PRE133]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'Write a CUDA kernel function code:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写CUDA内核函数代码：
- en: '[PRE134]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'Generate input/output matrices using NumPy:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NumPy生成输入/输出矩阵：
- en: '[PRE135]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'Compile the kernel code:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译内核代码：
- en: '[PRE136]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Get the kernel function from the compiled module:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从编译模块中获取内核函数：
- en: '[PRE137]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'Create device memories with the input data that is generated from the host:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用从主机生成的输入数据创建设备内存：
- en: '[PRE138]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'Configure the grid and block dimensions:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置网格和块维度：
- en: '[PRE139]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'Prepare to get the GPU events:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备获取GPU事件：
- en: '[PRE140]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'Call the kernel function:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用内核函数：
- en: '[PRE141]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'Launch the matrix multiplication from the host, and compare this with the result
    from the device:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从主机启动矩阵乘法，并将其与设备上的结果进行比较：
- en: '[PRE142]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'This code also reports an estimated time on the device and the host:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码还报告了设备和主机上的估计时间：
- en: '[PRE143]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: While PyCUDA exposes the CUDA C/C++ kernel code, this result gives a hint that
    manual kernel optimization is required, due to the lack of performance against
    the operation that was carried out by Numba.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然PyCUDA公开了CUDA C/C++内核代码，但由于性能不及Numba执行的操作，这表明需要手动优化内核。
- en: NVBLAS for zero coding acceleration in Octave and R
  id: totrans-469
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Octave和R中的零编码加速NVBLAS
- en: NVBLAS is a CUDA library for the BLAS operation for other packages, such as
    Octave and R. By replacing the operations carried out OpenBLAS, the Octave or
    developers and data scientists can easily enjoy GPU performance. In this chapter,
    we will cover how to accelerate Octave and R using NVBLAS.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: NVBLAS是用于其他软件包（如Octave和R）的BLAS操作的CUDA库。通过替换OpenBLAS执行的操作，Octave或开发人员和数据科学家可以轻松享受GPU性能。在本章中，我们将介绍如何使用NVBLAS加速Octave和R。
- en: NVBLAS is a dynamic library on top of the cuBLAS operation. The cuBLAS library
    is a GPU implementation of linear algebra operations. It replaces BLAS libraries,
    so that we can easily accelerate any application with zero coding effort. Let's
    see how this can be done from GEMM example codes.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: NVBLAS是cuBLAS操作的动态库。cuBLAS库是线性代数操作的GPU实现。它替换了BLAS库，因此我们可以轻松加速任何应用程序，而无需编码。让我们看看如何从GEMM示例代码中实现这一点。
- en: Configuration
  id: totrans-472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置
- en: 'To use NVBLAS in Octave and R, we need to provide some working environment
    variables to NVBLAS. To do this, let''s create an `nvblas.conf` file, where the
    directory, which we will work with Octave and R code examples, can be found. The
    `nvblas.conf` file can be written as follows:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Octave和R中使用NVBLAS，我们需要为NVBLAS提供一些工作环境变量。为此，让我们创建一个`nvblas.conf`文件，其中可以找到我们将使用Octave和R代码示例的目录。`nvblas.conf`文件可以编写如下：
- en: '[PRE144]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'In this file, we can see that NVBLAS needs to be aware of the CPU side''s BLAS
    library. We will use OpenBLAS in this session, so we need to install it with the
    following command in Ubuntu:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个文件中，我们可以看到NVBLAS需要了解CPU端的BLAS库。在本次会话中，我们将使用OpenBLAS，因此需要使用以下命令在Ubuntu中安装它：
- en: '[PRE145]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: Also, we can get a multi-GPU performance by providing multiple GPU IDs for `NVBLAS_GPU_LIST`.
    This book provides results from a GPU execution result, but try to provide multiple
    IDs if you have multiple GPUs.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以通过为`NVBLAS_GPU_LIST`提供多个GPU ID来获得多GPU性能。本书提供了GPU执行结果，但如果您有多个GPU，可以尝试提供多个ID。
- en: 'To use NVBLAS in Octave and R, we should set an environment—`LD_PRELOAD=libnvblas.so`—with
    your application execution:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Octave和R中使用NVBLAS，我们应该设置一个环境——`LD_PRELOAD=libnvblas.so`——与您的应用程序执行：
- en: 'For Octave code, execute your code as follows:'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Octave代码，执行以下代码：
- en: '[PRE146]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'For the R script, execute your script as follows:'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于R脚本，执行以下命令：
- en: '[PRE147]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: Of course, the `libnvblas.so` file should be accessible from the working directory.
    It is located in `/usr/local/cuda/lib64/`.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，`libnvblas.so`文件应该可以从工作目录访问。它位于`/usr/local/cuda/lib64/`中。
- en: 'NVBLAS is compatible with the archived packages. Therefore, using Octave- and
    R-installed ones with the following commands works well with our test:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: NVBLAS与存档软件包兼容。因此，使用以下命令安装的Octave和R软件包可以很好地与我们的测试配合使用：
- en: '[PRE148]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: Now, let's try using NVBLAS using the Octave and R languages.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，让我们尝试使用Octave和R语言来使用NVBLAS。 '
- en: Accelerating Octave's computation
  id: totrans-487
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速Octave的计算
- en: 'First, we will try NVBLAS using Octave. The fully implemented code is `08_nvblas/sgemm.m`.
    This is implemented as follows:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将尝试使用Octave来使用NVBLAS。完全实现的代码是`08_nvblas/sgemm.m`。实现如下：
- en: '[PRE149]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: 'For the GPU operation, execute the Octave script using the following command,
    and compare the performance with GPU, with the NVBLAS environment library and
    CPU by default:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPU操作，使用以下命令执行Octave脚本，并将性能与默认情况下的GPU、NVBLAS环境库和CPU进行比较：
- en: '[PRE150]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'Then, we can launch this with the `octave sgemm.m` command. The output results
    are as follows:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`octave sgemm.m`命令启动。输出结果如下：
- en: '| CPU | GPU V100 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| CPU | GPU V100 |'
- en: '|'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '`Elapsed Time [1024]: 0.011 ms, 188.909 GFlops`'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [1024]: 0.011 ms, 188.909 GFlops`'
- en: '`Elapsed Time [2048]: 0.075 ms, 228.169 GFlops`'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [2048]: 0.075 ms, 228.169 GFlops`'
- en: '`Elapsed Time [4096]: 0.212 ms, 647.022 GFlops`'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [4096]: 0.212 ms, 647.022 GFlops`'
- en: '`Elapsed Time [8192]: 1.158 ms, 949.763 GFlops`'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [8192]: 1.158 ms, 949.763 GFlops`'
- en: '`Elapsed Time [16384]: 7.292 ms, 1206.241 GFlops`'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [16384]: 7.292 ms, 1206.241 GFlops`'
- en: '|'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '`Elapsed Time [1024]: 0.010 ms, 208.346 GFlops`'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [1024]: 0.010 ms, 208.346 GFlops`'
- en: '`Elapsed Time [2048]: 0.024 ms, 721.731 GFlops`'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [2048]: 0.024 ms, 721.731 GFlops`'
- en: '`Elapsed Time [4096]: 0.094 ms, 1465.538 GFlops`'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [4096]: 0.094 ms, 1465.538 GFlops`'
- en: '`Elapsed Time [8192]: 0.582 ms, 1889.193 GFlops`'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [8192]: 0.582 ms, 1889.193 GFlops`'
- en: '`Elapsed Time [16384]: 4.472 ms, 1967.037 GFlops`'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [16384]: 4.472 ms, 1967.037 GFlops`'
- en: '|'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: As you can see, GPU shows higher computational throughput as the matrices' size
    get larger.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，随着矩阵大小的增加，GPU显示出更高的计算吞吐量。
- en: Accelerating R's compuation
  id: totrans-508
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速R的计算
- en: 'Now, we will try NVBLAS for the R language, with the help of the following
    steps:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将尝试使用NVBLAS来进行R语言的测试，以下是帮助步骤：
- en: 'First, let''s write a `sgemm.R` file which carries out a dot operation:'
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们编写一个`sgemm.R`文件，执行点操作：
- en: '[PRE151]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: 'Execute the R script using the following command and compare the performance:'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令执行R脚本，并比较性能：
- en: '[PRE152]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'The sample code operates several times, while increasing the data size. The
    following table shows the outputs of the previous commands:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 样本代码多次操作，同时增加数据大小。以下表格显示了先前命令的输出：
- en: '| CPU | GPU V100 |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| CPU | GPU V100 |'
- en: '|'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '`Elapsed Time [1024]: 0.029 ms, 74.051 GFlops`'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [1024]: 0.029 ms, 74.051 GFlops`'
- en: '`Elapsed Time [2048]: 0.110 ms, 156.181 GFlops`'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [2048]: 0.110 ms, 156.181 GFlops`'
- en: '`Elapsed Time [4096]: 0.471 ms, 291.802 GFlops`'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [4096]: 0.471 ms, 291.802 GFlops`'
- en: '`Elapsed Time [8192]: 2.733 ms, 402.309 GFlops`'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [8192]: 2.733 ms, 402.309 GFlops`'
- en: '`Elapsed Time [16384]: 18.291 ms, 480.897 GFlops`'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [16384]: 18.291 ms, 480.897 GFlops`'
- en: '|'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '`Elapsed Time [1024]: 0.034 ms, 63.161 GFlops`'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [1024]: 0.034 ms, 63.161 GFlops`'
- en: '`Elapsed Time [2048]: 0.063 ms, 272.696 GFlops`'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [2048]: 0.063 ms, 272.696 GFlops`'
- en: '`Elapsed Time [4096]: 0.286 ms, 480.556 GFlops`'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [4096]: 0.286 ms, 480.556 GFlops`'
- en: '`Elapsed Time [8192]: 1.527 ms, 720.047 GFlops`'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [8192]: 1.527 ms, 720.047 GFlops`'
- en: '`Elapsed Time [16384]: 9.864 ms, 891.737 GFlops`'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Elapsed Time [16384]: 9.864 ms, 891.737 GFlops`'
- en: '|'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: From the results, we can see the performance gap between the CPU and GPU. Also,
    we are able to identify that the performance gain of GPU increases when we increase
    the sample size.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，我们可以看到CPU和GPU之间的性能差距。此外，我们可以确定当样本大小增加时，GPU的性能增益也会增加。
- en: 'If you are interested in R acceleration with GPU, please visit an NVIDIA development
    blog: [https://devblogs.nvidia.com/accelerate-r-applications-cuda/](https://devblogs.nvidia.com/accelerate-r-applications-cuda/)'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对使用GPU加速的R感兴趣，请访问NVIDIA开发博客：[https://devblogs.nvidia.com/accelerate-r-applications-cuda/](https://devblogs.nvidia.com/accelerate-r-applications-cuda/)
- en: CUDA acceleration in MATLAB
  id: totrans-531
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MATLAB中的CUDA加速
- en: MATLAB is a productive, high-level numerical analysis tool with various tools
    and functions. This tool supports CUDA from the early stages with their **Parallel
    Computing Toolbox**. This section will show us how to generate CUDA code using
    this tool.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: MATLAB是一个高效的、高级的数值分析工具，具有各种工具和函数。该工具从早期阶段就支持CUDA，配备了**并行计算工具箱**。本节将向我们展示如何使用该工具生成CUDA代码。
- en: To enable GPU acceleration, we need to install MATLAB with the Parallel Computing
    Toolbox. If you already have MATLAB, check if your license covers the Parallel
    Computing Toolbox. If you don't, you can try the MATLAB evaluation code. From
    MATLAB's evaluation site, you may download any kind of package, except the control
    systems. Most packages contain the Parallel Computing Toolbox, so you may try
    this. But if you are not considering using MATLAB, you may skip this section.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用GPU加速，我们需要安装带有并行计算工具箱的MATLAB。如果您已经有MATLAB，请检查您的许可证是否包括并行计算工具箱。如果没有，您可以尝试MATLAB评估代码。从MATLAB的评估网站上，您可以下载任何类型的软件包，除了控制系统。大多数软件包都包含并行计算工具箱，所以您可以尝试这个。但是如果您不考虑使用MATLAB，可以跳过这一部分。
- en: 'When we use MATLAB code to work on GPU, you need to create a device memory
    using gpuArray. In the same way that *Numba* and *PyCUDA* send their host data
    to the device, MATLAB''s `gpuArray()` creates a device memory and transfers the
    given host data to the device:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用MATLAB代码在GPU上运行时，您需要使用gpuArray来创建设备内存。与*Numba*和*PyCUDA*将它们的主机数据发送到设备的方式相同，MATLAB的`gpuArray()`创建设备内存并将给定的主机数据传输到设备上：
- en: '[PRE153]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: 'This session will assume that you have already installed MATLAB and the Parallel
    Computing Toolbox. In this section, we will focus on implementing the sample code,
    and compare the performances of the host and GPU:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 本节假设您已经安装了MATLAB和并行计算工具箱。在本节中，我们将重点介绍实现示例代码，并比较主机和GPU的性能：
- en: 'Let''s write a `host.m` file, which can work on the CPU. The code is as follows:'
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编写一个在CPU上运行的`host.m`文件。代码如下：
- en: '[PRE154]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: 'Now, let''s execute both implementations with the following commands. This
    is the command to MATLAB, and its output:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令执行这两种实现。这是针对MATLAB的命令及其输出：
- en: '[PRE155]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: 'Then, let''s write a `cuda.m` file, which works on the GPU. We just apply `gpuArray()` to
    the input matrices as follows:'
  id: totrans-541
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们编写一个在GPU上运行的`cuda.m`文件。我们只需将`gpuArray()`应用于输入矩阵，如下所示：
- en: '[PRE156]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: 'This is the GPU version execution code, and the execution result:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 这是GPU版本的执行代码和执行结果：
- en: '[PRE157]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: As we can see, GPU shows a higher performance against the CPU.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，GPU相对于CPU表现出更高的性能。
- en: 'MathWorks provides plenty of examples of GPU computing with MATLAB. Please
    visit their site if you want to learn more: [https://www.mathworks.com/examples/parallel-computing/category/gpu-computing](https://www.mathworks.com/examples/parallel-computing/category/gpu-computing).'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: MathWorks提供了许多关于MATLAB GPU计算的示例。如果您想了解更多，请访问他们的网站：[https://www.mathworks.com/examples/parallel-computing/category/gpu-computing](https://www.mathworks.com/examples/parallel-computing/category/gpu-computing)。
- en: Summary
  id: totrans-547
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have covered the CUDA programming methods using CUDA libraries,
    and other compatible languages. We have also covered the basic use of cuBLAS and
    its mixed-precision operation feature. Also, we explored the cuRAND, cuFFT, NPP,
    and OpenCV libraries. Thanks to these libraries, we could implement GPU applications
    with little effort, as discussed at the beginning of the chapter.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们已经介绍了使用CUDA库和其他兼容语言的CUDA编程方法。我们还介绍了cuBLAS的基本用法及其混合精度操作特性。此外，我们还探讨了cuRAND、cuFFT、NPP和OpenCV库。由于这些库，我们可以轻松实现GPU应用程序，正如本章开头所讨论的那样。
- en: We have implemented some GPU applications using other languages that are compatible
    with CUDA. Firstly, we covered several Python packages, which enable Python and
    CUDA interops. They provide Pythonic programmabilities and compatibilities with
    other Python features. Then, we covered CUDA accelerations in other scientific
    computing languages, such as Octave, R, and MATLAB.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用其他与CUDA兼容的语言实现了一些GPU应用程序。首先，我们介绍了几个Python包，这些包使Python和CUDA可以互操作。它们提供了Python式的可编程性，并与其他Python特性兼容。然后，我们介绍了其他科学计算语言中的CUDA加速，例如Octave、R和MATLAB。
- en: Now, we have one more GPU programming method to cover—OpenACC. With this we
    can covert the original C/C++ and Fortran host codes to work on GPUs using directives
    such as `#pragma acc kernels`. We will cover this in the next chapter.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们还有一种GPU编程方法要介绍——OpenACC。通过这种方法，我们可以使用诸如`#pragma acc kernels`之类的指令将原始的C/C++和Fortran主机代码转换为在GPU上运行。我们将在下一章中介绍这一点。
