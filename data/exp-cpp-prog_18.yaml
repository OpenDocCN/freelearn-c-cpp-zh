- en: Multithreading with Distributed Computing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式计算中的多线程
- en: Distributed computing was one of the original applications of multithreaded
    programming. Back when every personal computer just contained a single processor
    with a single core, government and research institutions, as well as some companies
    would have multi-processor systems, often in the form of clusters. These would
    be capable of multithreaded processing; by splitting tasks across processors,
    they could speed up various tasks, including simulations, rendering of CGI movies,
    and the like.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算是多线程编程的最初应用之一。在每台个人电脑只包含单个处理器和单个核心的时代，政府和研究机构，以及一些公司会拥有多处理器系统，通常以集群的形式存在。这些系统可以进行多线程处理；通过将任务分配到处理器上，它们可以加速各种任务，包括模拟、CGI电影的渲染等。
- en: Nowadays virtually every desktop-level or better system has more than a single
    processor core, and assembling a number of systems together into a cluster is
    very easy, using cheap Ethernet wiring. Combined with frameworks such as OpenMP
    and Open MPI, it's quite easy to expand a C++ based (multithreaded) application
    to run on a distributed system.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，几乎每台桌面级或更高级别的系统都有多个处理器核心，并且使用廉价的以太网布线非常容易将多台系统组装成集群。结合OpenMP和Open MPI等框架，很容易将基于C++（多线程）的应用程序扩展到分布式系统上。
- en: 'Topics in this chapter include:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主题包括：
- en: Integrating OpenMP and MPI in a multithreaded C++ application
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多线程C++应用程序中集成OpenMP和MPI
- en: Implementing a distributed, multithreaded application
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现分布式多线程应用程序
- en: Common applications and issues with distributed, multithreaded programming
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式多线程编程的常见应用和问题
- en: Distributed computing, in a nutshell
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式计算简介
- en: When it comes to processing large datasets in parallel, it would be ideal if
    one could take the data, chop it up into lots of small parts, and push it to a
    lot of threads, thus significantly shortening the total time spent processing
    the said data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及并行处理大型数据集时，如果能够将数据分割成许多小部分，并将其推送到许多线程中，从而显著缩短处理所述数据的总时间，那将是理想的。
- en: 'The idea behind distributed computing is exactly this: on each node in a distributed
    system one or more instances of our application run, whereby this application
    can either be single or multithreaded. Due to the overhead of inter-process communication,
    it''s generally more efficient to use a multithreaded application, as well as
    due to other possible optimizations--courtesy of resource sharing.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算的理念正是这样：在分布式系统的每个节点上运行我们的应用程序的一个或多个实例，这个应用程序可以是单线程或多线程。由于进程间通信的开销，使用多线程应用程序通常更有效，还有其他可能的优化--由于资源共享。
- en: If one already has a multithreaded application ready to use, then one can move
    straight to using MPI to make it work on a distributed system. Otherwise, OpenMP
    is a compiler extension (for C/C++ and Fortran) which can make it relatively painless
    to make an application multithreaded without refactoring.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果已经有一个准备好使用的多线程应用程序，那么可以直接使用MPI使其在分布式系统上运行。否则，OpenMP是一个编译器扩展（用于C/C++和Fortran），可以相对轻松地使应用程序多线程化而无需重构。
- en: 'To do this, OpenMP allows one to mark a common code segment, to be executed
    on all slave threads. A master thread creates a number of slave threads which
    will concurrently process that same code segment. A basic *Hello World* OpenMP
    application looks like this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，OpenMP允许用户标记一个常见的代码段，以便在所有从属线程上执行。主线程创建了许多从属线程，这些线程将同时处理相同的代码段。一个基本的*Hello
    World* OpenMP应用程序看起来像这样：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: What one can easily tell from this basic sample is that OpenMP provides a C
    based API through the `<omp.h>` header. We can also see the section that will
    be executed by each thread, as marked by a `#pragma omp` preprocessor macro.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个基本示例中很容易看出，OpenMP通过`<omp.h>`头文件提供了一个基于C的API。我们还可以看到每个线程将执行的部分，由`#pragma omp`预处理器宏标记。
- en: The advantage of OpenMP over the examples of multithreaded code which we saw
    in the preceding chapters, is the ease with which a section of code can be marked
    as being multithreaded without having to make any actual code changes. The obvious
    limitation that comes with this is that every thread instance will execute the
    exact same code and further optimization options are limited.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP相对于我们在前面章节中看到的多线程代码的优势在于，可以轻松地将代码段标记为多线程，而无需进行任何实际的代码更改。这带来的明显限制是，每个线程实例将执行完全相同的代码，并且进一步的优化选项有限。
- en: MPI
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MPI
- en: In order to schedule the execution of code on specific nodes, **MPI** (**Message
    Passing Interface**) is commonly used. Open MPI is a free library implementation
    of this, and used by many high-ranking supercomputers. MPICH is another popular
    implementation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安排在特定节点上执行代码，**MPI**（**消息传递接口**）通常被使用。Open MPI是这方面的一个免费库实现，被许多高级超级计算机使用。MPICH是另一个流行的实现。
- en: MPI itself is defined as a communication protocol for the programming of parallel
    computers. It is currently at its third revision (MPI-3).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MPI本身被定义为并行计算编程的通信协议。它目前处于第三个修订版（MPI-3）。
- en: 'In summary, MPI offers the following basic concepts:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，MPI提供了以下基本概念：
- en: '**Communicators**: A communicator object connects a group of processes within
    an MPI session. It both assigns unique identifiers to processes and arranges processes
    within an ordered topology.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通信器**：通信器对象连接了MPI会话中的一组进程。它为进程分配唯一标识符，并在有序拓扑中安排进程。'
- en: '**Point-to-point operations**: This type of operation allows for direct communication
    between specific processes.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点对点操作**：这种操作允许特定进程之间的直接通信。'
- en: '**Collective functions**: These functions involve broadcasting communications
    within a process group. They can also be used in the reverse manner, which would
    take the results from all processes in a group and, for example, sum them on a
    single node. A more selective version would ensure that a specific data item is
    sent to a specific node.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集体函数**：这些函数涉及在进程组内进行广播通信。它们也可以以相反的方式使用，从进程组中获取所有进程的结果，例如在单个节点上对它们进行求和。更具选择性的版本可以确保特定的数据项被发送到特定的节点。'
- en: '**Derived datatype**: Since not every node in an MPI cluster is guaranteed
    to have the same definition, byte order, and interpretation of data types, MPI
    requires that it is specified what type each data segment is, so that MPI can
    do data conversion.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**派生数据类型**：由于MPI集群中的每个节点都不能保证具有相同的定义、字节顺序和数据类型的解释，MPI要求指定每个数据段的类型，以便MPI进行数据转换。'
- en: '**One-sided communications**: These are operations which allow one to write
    or read to or from remote memory, or perform a reduction operation across a number
    of tasks without having to synchronize between tasks. This can be useful for certain
    types of algorithms, such as those involving distributed matrix multiplication.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单边通信**：这些操作允许在远程内存中写入或读取数据，或者在多个任务之间执行归约操作，而无需在任务之间进行同步。这对于某些类型的算法非常有用，比如涉及分布式矩阵乘法的算法。'
- en: '**Dynamic process management**: This is a feature which allows MPI processes
    to create new MPI processes, or establish communication with a newly created MPI
    process.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态进程管理**：这是一个允许MPI进程创建新的MPI进程，或者与新创建的MPI进程建立通信的功能。'
- en: '**Parallel I/O**: Also called MPI-IO, this is an abstraction for I/O management
    on distributed systems, including file access, for easy use with MPI.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行I/O**：也称为MPI-IO，这是分布式系统上I/O管理的抽象，包括文件访问，方便与MPI一起使用。'
- en: Of these, MPI-IO, dynamic process management, and one-sided communication are
    MPI-2 features. Migration from MPI-1 based code and the incompatibility of dynamic
    process management with some setups, along with many applications not requiring
    MPI-2 features, means that uptake of MPI-2 has been relatively slow.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，MPI-IO、动态进程管理和单边通信是MPI-2的特性。由于从基于MPI-1的代码迁移和动态进程管理与某些设置不兼容，以及许多应用程序不需要MPI-2的特性，MPI-2的采用速度相对较慢。
- en: Implementations
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: The initial implementation of MPI was **MPICH**, by **Argonne National Laboratory**
    (**ANL**) and Mississippi State University. It is currently one of the most popular
    implementations, used as the foundation for MPI implementations, including those
    by IBM (Blue Gene), Intel, QLogic, Cray, Myricom, Microsoft, Ohio State University
    (MVAPICH), and others.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: MPI的最初实现是由阿贡国家实验室（ANL）和密西西比州立大学开发的MPICH。它目前是最受欢迎的实现之一，被用作MPI实现的基础，包括IBM（蓝色基因）、英特尔、QLogic、Cray、Myricom、微软、俄亥俄州立大学（MVAPICH）等公司的实现。
- en: 'Another very common implementation is Open MPI, which was formed out of the
    merger of three MPI implementations:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常常见的实现是Open MPI，它是由三个MPI实现合并而成的：
- en: FT-MPI (University of Tennessee)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FT-MPI（田纳西大学）
- en: LA-MPI (Los Alamos National Laboratory)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 洛斯阿拉莫斯国家实验室（LA-MPI）
- en: LAM/MPI (Indiana University)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LAM/MPI（印第安纳大学）
- en: These, along with the PACX-MPI team at the University of Stuttgart, are the
    founding members of the Open MPI team. One of the primary goals of Open MPI is
    to create a high-quality, open source MPI-3 implementation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些术语，以及斯图加特大学的PACX-MPI团队，是Open MPI团队的创始成员。Open MPI的主要目标之一是创建一个高质量的开源MPI-3实现。
- en: MPI implementations are mandated to support C and Fortran. C/C++ and Fortran
    along with assembly support is very common, along with bindings for other languages.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: MPI实现必须支持C和Fortran。C/C++和Fortran以及汇编支持非常普遍，还有其他语言的绑定。
- en: Using MPI
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MPI
- en: Regardless of the implementation chosen, the resulting API will always match
    the official MPI standard, differing only by the MPI version that the library
    one has picked supports. All MPI-1 (revision 1.3) features should be supported
    by any MPI implementation, however.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择哪种实现，结果的API都将始终符合官方MPI标准，只有所选择的库支持的MPI版本不同。任何MPI实现都应该支持所有MPI-1（修订版1.3）的特性。
- en: 'This means that the canonical Hello World (as, for example, found on the MPI
    Tutorial site: [http://mpitutorial.com/tutorials/mpi-hello-world/](http://mpitutorial.com/tutorials/mpi-hello-world/))
    for MPI should work regardless of which library one picks:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着无论选择哪个库，MPI的典型Hello World（例如，在MPI教程网站上找到的）应该都能工作：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When reading through this basic example of an MPI-based application, it''s
    important to be familiar with the terms used with MPI, in particular:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这个基于MPI的应用程序的基本示例时，熟悉MPI使用的术语是很重要的，特别是：
- en: '**World**: The registered MPI processes for this job'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**世界**：这个作业的注册MPI进程'
- en: '**Communicator**: The object which connects all MPI processes within a session'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通信器**：连接会话中所有MPI进程的对象'
- en: '**Rank**: The identifier for a process within a communicator'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**秩**：通信器内的进程的标识符'
- en: '**Processor**: A physical CPU, a singular core of a multi-core CPU, or the
    hostname of the system'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理器**：物理CPU，多核CPU的单个核心，或系统的主机名'
- en: In this Hello World example, we can see that we include the `<mpi.h>` header.
    This MPI header will always be the same, regardless of the implementation we use.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Hello World的例子中，我们可以看到我们包含了`<mpi.h>`头文件。无论我们使用哪种实现，这个MPI头文件都是一样的。
- en: Initializing the MPI environment requires a single call to `MPI_Init()`, which
    can take two parameters, both of which are optional at this point.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化MPI环境只需要调用一次`MPI_Init()`，此时可以传入两个参数，这两个参数都是可选的。
- en: Getting the size of the world (meaning, number of processes available) is the
    next step. This is done using `MPI_Comm_size()`, which takes the `MPI_COMM_WORLD`
    global variable (defined by MPI for our use) and updates the second parameter
    with the number of processes in that world.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 获取世界的大小（即可用进程数）是下一步。这是使用`MPI_Comm_size()`完成的，它接受`MPI_COMM_WORLD`全局变量（由MPI为我们定义）并使用第二个参数更新该世界中的进程数。
- en: The rank we then obtain is essentially the unique ID assigned to this process
    by MPI. Obtaining this UID is performed with `MPI_Comm_rank()`. Again, this takes
    the `MPI_COMM_WORLD` variable as the first parameter and returns our numeric rank
    as the second parameter. This rank is useful for self-identification and communication
    between processes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们获得的排名基本上是MPI为该进程分配的唯一ID。使用`MPI_Comm_rank()`执行此UID。同样，这需要`MPI_COMM_WORLD`变量作为第一个参数，并将我们的数字排名作为第二个参数返回。此排名对于自我识别和进程之间的通信很有用。
- en: Obtaining the name of the specific piece of hardware on which one is running
    can also be useful, particularly for diagnostic purposes. For this we can call
    `MPI_Get_processor_name()`. The returned string will be of a globally defined
    maximum length and will identify the hardware in some manner. The exact format
    of this string is implementation defined.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 获取正在运行的特定硬件的名称也可能很有用，特别是用于诊断目的。为此，我们可以调用`MPI_Get_processor_name()`。返回的字符串将具有全局定义的最大长度，并且将以某种方式标识硬件。此字符串的确切格式由实现定义。
- en: Finally, we print out the information we gathered and clean up the MPI environment
    before terminating the application.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打印出我们收集的信息，并在终止应用程序之前清理MPI环境。
- en: Compiling MPI applications
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译MPI应用程序
- en: In order to compile an MPI application, the `mpicc` compiler wrapper is used.
    This executable should be part of whichever MPI implementation has been installed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编译MPI应用程序，使用`mpicc`编译器包装器。这个可执行文件应该是已安装的任何MPI实现的一部分。
- en: 'Using it is, however, identical to how one would use, for example, GCC:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用它与使用例如GCC是相同的：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This can be compared to:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以与：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This would compile and link our Hello World example into a binary, ready to
    be executed. Executing this binary is, however, not done by starting it directly,
    but instead a launcher is used, like this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这将编译和链接我们的Hello World示例为一个二进制文件，准备执行。然而，执行此二进制文件不是直接启动它，而是使用启动器，如下所示：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding output is from Open MPI running inside a Bash shell on a Windows
    system. As we can see, we launch four processes in total (4 ranks). The processor
    name is reported as the hostname for each process ("PC").
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出来自在Windows系统上运行的Bash shell中的Open MPI。正如我们所看到的，我们总共启动了四个进程（4个排名）。处理器名称报告为每个进程的主机名（“PC”）。
- en: The binary to launch MPI applications with is called mpiexec or mpirun, or orterun.
    These are synonyms for the same binary, though not all implementations will have
    all synonyms. For Open MPI, all three are present and one can use any of these.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 用于启动MPI应用程序的二进制文件称为mpiexec或mpirun，或orterun。这些是相同二进制文件的同义词，尽管并非所有实现都具有所有同义词。对于Open
    MPI，所有三者都存在，可以使用其中任何一个。
- en: The cluster hardware
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群硬件
- en: The systems an MPI based or similar application will run on consist of multiple
    independent systems (nodes), each of which is connected to the others using some
    kind of network interface. For high-end applications, these tend to be custom
    nodes with high-speed, low-latency interconnects. At the other end of the spectrum
    are so-called Beowulf and similar type clusters, made out of standard (desktop)
    computers and usually connected using regular Ethernet.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: MPI基于或类似应用程序将运行的系统由多个独立系统（节点）组成，每个系统都使用某种网络接口连接到其他系统。对于高端应用程序，这些往往是具有高速、低延迟互连的定制节点。在光谱的另一端是所谓的Beowulf和类似类型的集群，由标准（台式）计算机组成，通常使用常规以太网连接。
- en: At the time of writing, the fastest supercomputer (according to the TOP500 listing)
    is the Sunway TaihuLight supercomputer at the National Supercomputing Center in
    Wuxi, China. It uses a total of 40,960 Chinese-designed SW26010 manycore RISC
    architecture-based CPUs, with 256 cores per CPU (divided in 4 64-core groups),
    along with four management cores. The term *manycore* refers to a specialized
    CPU design which focuses more on explicit parallelism as opposed to the single-thread
    and general-purpose focus of most CPU cores. This type of CPU is similar to a
    GPU architecture and vector processors in general.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，根据TOP500榜单，最快的超级计算机是中国无锡国家超级计算中心的Sunway TaihuLight超级计算机。它使用了总共40,960个中国设计的SW26010多核RISC架构CPU，每个CPU有256个核心（分为4个64核心组），以及四个管理核心。术语“多核”是指一种专门的CPU设计，它更注重显式并行性，而不是大多数CPU核心的单线程和通用重点。这种类型的CPU类似于GPU架构和矢量处理器。
- en: 'Each of these nodes contains a single SW26010 along with 32 GB of DDR3 memory.
    They are connected via a PCIe 3.0-based network, itself consisting of a three-level
    hierarchy: the central switching network (for supernodes), the supernode network
    (connecting all 256 nodes in a supernode), and the resource network, which provides
    access to I/O and other resource services. The bandwidth for this network between
    individual nodes is 12 GB/second, with a latency of about 1 microsecond.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点都包含一个SW26010和32GB的DDR3内存。它们通过基于PCIe 3.0的网络连接，本身由三级层次结构组成：中央交换网络（用于超级节点），超级节点网络（连接超级节点中的所有256个节点）和资源网络，提供对I/O和其他资源服务的访问。节点之间的网络带宽为12GB/秒，延迟约为1微秒。
- en: 'The following graphic (from "The Sunway TaihuLight Supercomputer: System and
    Applications", DOI: 10.1007/s11432-016-5588-7) provides a visual overview of this
    system:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表（来自“Sunway TaihuLight超级计算机：系统和应用”，DOI：10.1007/s11432-016-5588-7）提供了对该系统的视觉概述：
- en: '![](img/916608cf-a01d-43b6-86c2-af46982d0d28.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/916608cf-a01d-43b6-86c2-af46982d0d28.png)'
- en: For situations where the budget does not allow for such an elaborate and highly
    customized system, or where the specific tasks do not warrant such an approach,
    there always remains the "Beowulf" approach. A Beowulf cluster is a term used
    to refer to a distributed computing system constructed out of common computer
    systems. These can be Intel or AMD-based x86 systems, with ARM-based processors
    now becoming popular.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在预算不允许这样复杂和高度定制的系统，或者特定任务不需要这样的方法的情况下，总是可以采用“Beowulf”方法。Beowulf集群是指由普通计算机系统构建的分布式计算系统。这些可以是基于Intel或AMD的x86系统，现在也流行起了基于ARM处理器的系统。
- en: It's generally helpful to have each node in a cluster to be roughly identical
    to the other nodes. Although it's possible to have an asymmetric cluster, management
    and job scheduling becomes much easier when one can make broad assumptions about
    each node.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通常希望集群中的每个节点大致相同。虽然可以有不对称的集群，但是当可以对每个节点做出广泛的假设时，管理和作业调度变得更加容易。
- en: At the very least, one would want to match the processor architecture, with
    a base level of CPU extensions, such as SSE2/3 and perhaps AVX and kin, common
    across all nodes. Doing this would allow one to use the same compiled binary across
    the nodes, along with the same algorithms, massively simplifying the deployment
    of jobs and the maintenance of the code base.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，希望匹配处理器架构，具有基本的CPU扩展，如SSE2/3，也许还有AVX等，所有节点上都通用。这样做可以让您在节点上使用相同的编译二进制文件，以及相同的算法，大大简化作业的部署和代码库的维护。
- en: 'For the network between the nodes, Ethernet is a very popular option, delivering
    communication times measured in tens to hundreds of microseconds, while costing
    only a fraction of faster options. Usually each node would be connected to a single
    Ethernet network, as in this graphic:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于节点之间的网络，以太网是一个非常受欢迎的选项，传输时间以十到百微秒计，成本只是更快选项的一小部分。通常，每个节点都会连接到一个以太网网络，如图所示：
- en: '![](img/28e8fc11-68fd-4692-9f6e-93250348f221.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28e8fc11-68fd-4692-9f6e-93250348f221.png)'
- en: 'There is also the option to add a second or even third Ethernet link to each
    or specific nodes to give them access to files, I/O, and other resources, without
    having to compete with bandwidth on the primary network layer. For very large
    clusters, one could consider an approach such as that used with the Sunway TaihuLight
    and many other supercomputers: splitting nodes up into supernodes, each with their
    own inter-node network. This would allow one to optimize traffic on the network
    by limiting it to only associated nodes.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个选项，可以为每个或特定节点添加第二甚至第三个以太网链接，使它们可以访问文件、I/O和其他资源，而无需在主要网络层上竞争带宽。对于非常大的集群，可以考虑一种类似于Sunway
    TaihuLight和许多其他超级计算机使用的方法：将节点分割成超级节点，每个节点都有自己的节点间网络。这将允许通过限制只与相关节点通信来优化网络流量。
- en: 'An example of such an optimized Beowulf cluster would look like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这样优化的Beowulf集群的示例将如下所示：
- en: '![](img/8a4a767b-6ab9-4f17-bc91-63a04a9452cf.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a4a767b-6ab9-4f17-bc91-63a04a9452cf.png)'
- en: Clearly there is a wide range of possible configurations with MPI-based clusters,
    utilizing custom, off-the-shelf, or a combination of both types of hardware. The
    intended purpose of the cluster often determines the most optimal layout for a
    specific cluster, such as running simulations, or the processing of large datasets.
    Each type of job presents its own set of limitations and requirements, which is
    also reflected in the software implementation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，基于MPI的集群有各种可能的配置，利用定制的、现成的或两种硬件类型的组合。集群的预期用途通常决定了特定集群的最佳布局，例如运行模拟或处理大型数据集。每种类型的作业都有自己的一套限制和要求，这也反映在软件实现中。
- en: Installing Open MPI
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Open MPI
- en: For the remainder of this chapter, we will focus on Open MPI. In order to get
    a working development environment for Open MPI, one will have to install its headers
    and library files, along with its supporting tools and binaries.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将专注于Open MPI。为了获得Open MPI的工作开发环境，需要安装其头文件和库文件，以及支持工具和二进制文件。
- en: Linux and BSDs
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Linux和BSD
- en: 'On Linux and BSD distributions with a package management system, it''s quite
    easy: simply install the Open MPI package and everything should be set up and
    configured, ready to be used. Consult the manual for one''s specific distribution,
    to see how to search for and install specific packages.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有软件包管理系统的Linux和BSD发行版上，这很容易：只需安装Open MPI软件包，一切都应该设置和配置好，准备好使用。查阅特定发行版的手册，了解如何搜索和安装特定软件包。
- en: 'On Debian-based distributions, one would use:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于Debian的发行版上，可以使用：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding command would install the Open MPI binaries, documentation, and
    development headers. The last two packages can be omitted on compute nodes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将安装Open MPI二进制文件、文档和开发头文件。计算节点上可以省略最后两个软件包。
- en: Windows
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Windows
- en: On Windows things get slightly complex, mostly because of the dominating presence
    of Visual C++ and the accompanying compiler toolchain. If one wishes to use the
    same development environment as on Linux or BSD, using MinGW, one has to take
    some additional steps.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows上，情况变得稍微复杂，主要是因为Visual C++和相应的编译器工具链的主导地位。如果希望在Linux或BSD上使用与之相同的开发环境，使用MinGW，就需要采取一些额外的步骤。
- en: This chapter assumes the use of either GCC or MinGW. If one wishes to develop
    MPI applications using the Visual Studio environment, please consult the relevant
    documentation for this.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设使用GCC或MinGW。如果希望使用Visual Studio环境开发MPI应用程序，请查阅相关文档。
- en: The easiest to use and most up to date MinGW environment is MSYS2, which provides
    a Bash shell along with most of the tools one would be familiar with under Linux
    and BSD. It also features the Pacman package manager, as known from the Linux
    Arch distribution. Using this, it's easy to install the requisite packages for
    Open MPI development.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最易于使用且最新的MinGW环境是MSYS2，它提供了一个Bash shell，以及大多数在Linux和BSD下熟悉的工具。它还具有Pacman软件包管理器，正如Linux
    Arch发行版所知。使用这个软件包管理器，可以轻松安装Open MPI开发所需的软件包。
- en: 'After installing the MSYS2 environment from [https://msys2.github.io/](https://msys2.github.io/),
    install the MinGW toolchain:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从[https://msys2.github.io/](https://msys2.github.io/)安装MSYS2环境后，安装MinGW工具链：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This assumes that the 64-bit version of MSYS2 was installed. For the 32-bit
    version, select i686 instead of x86_64\. After installing these packages, we will
    have both MinGW and the basic development tools installed. In order to use them,
    start a new shell using the MinGW 64-bit postfix in the name, either via the shortcut
    in the start menu, or by using the executable file in the MSYS2 `install` folder.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这假设安装了64位版本的MSYS2。对于32位版本，请选择i686而不是x86_64。安装了这些软件包后，我们将同时安装MinGW和基本开发工具。为了使用它们，使用MinGW
    64位后缀的名称启动一个新的shell，可以通过开始菜单中的快捷方式，或者通过MSYS2 `install`文件夹中的可执行文件来实现。
- en: With MinGW ready, it's time to install MS-MPI version 7.x. This is Microsoft's
    implementation of MPI and the easiest way to use MPI on Windows. It's an implementation
    of the MPI-2 specification and mostly compatible with the MPICH2 reference implementation.
    Since MS-MPI libraries are not compatible between versions, we use this specific
    version.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好MinGW后，现在是安装MS-MPI版本7.x的时候了。这是微软在Windows上使用MPI的最简单的方法。它是MPI-2规范的实现，与MPICH2参考实现大部分兼容。由于MS-MPI库在不同版本之间不兼容，我们使用这个特定的版本。
- en: Though version 7 of MS-MPI has been archived, it can still be downloaded via
    the Microsoft Download Center at [https://www.microsoft.com/en-us/download/details.aspx?id=49926](https://www.microsoft.com/en-us/download/details.aspx?id=49926).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MS-MPI的第7版已经存档，但仍然可以通过Microsoft下载中心下载，网址为[https://www.microsoft.com/en-us/download/details.aspx?id=49926](https://www.microsoft.com/en-us/download/details.aspx?id=49926)。
- en: 'MS-MPI version 7 comes with two installers, `msmpisdk.msi` and `MSMpiSetup.exe`.
    Both need to be installed. Afterwards, we should be able to open a new MSYS2 shell
    and find the following environment variable set up:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: MS-MPI版本7带有两个安装程序，`msmpisdk.msi`和`MSMpiSetup.exe`。都需要安装。之后，我们应该能够打开一个新的MSYS2
    shell，并找到以下环境变量设置：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This output for the printenv command shows that the MS-MPI SDK and runtime
    was properly installed. Next, we need to convert the static library from the Visual
    C++ LIB format to the MinGW A format:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: printenv命令的输出显示MS-MPI SDK和运行时已经正确安装。接下来，我们需要将Visual C++ LIB格式的静态库转换为MinGW A格式：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We first copy the original LIB file into a new temporary folder in our home
    folder, along with the runtime DLL. Next, we use the gendef tool on the DLL in
    order to create the definitions which we will need in order to convert it to a
    new format.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将原始LIB文件复制到我们的主文件夹中的一个新临时文件夹中，以及运行时DLL。接下来，我们使用gendef工具处理DLL，以创建我们需要的定义，以便将其转换为新格式。
- en: This last step is done with dlltool, which takes the definitions file along
    with the DLL and outputs a static library file which is compatible with MinGW.
    This file we then copy to a location where MinGW can find it later when linking.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用dlltool，它需要使用定义文件和DLL，输出一个与MinGW兼容的静态库文件。然后我们将该文件复制到MinGW在链接时可以找到的位置。
- en: 'Next, we need to copy the MPI header:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要复制MPI头文件：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After copying this header file, we must open it and locate the section that
    starts with:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 复制这个头文件后，我们必须打开它并找到以下部分：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Immediately above that line, we need to add the following line:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在该行的正上方，我们需要添加以下行：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This include adds the definition for `__int64`, which we will need for the code
    to compile correctly.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个包含添加了`__int64`的定义，这是我们编译代码所需要的。
- en: 'Finally, copy the header file to the MinGW `include` folder:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将头文件复制到MinGW的`include`文件夹中：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With this we have the libraries and headers all in place for MPI development
    with MinGW. allowing us to compile and run the earlier Hello World example, and
    continue with the rest of this chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就可以在MinGW下进行MPI开发所需的库和头文件，从而可以编译和运行之前的Hello World示例，并继续进行本章的其余部分。
- en: Distributing jobs across nodes
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨节点分发作业
- en: In order to distribute MPI jobs across the nodes in a cluster, one has to either
    specify these nodes as a parameter to the `mpirun`/`mpiexec` command or make use
    of a host file. This host file contains the names of the nodes on the network
    which will be available for a run, along with the number of available slots on
    the host.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在集群中的节点之间分发MPI作业，必须将这些节点作为`mpirun`/`mpiexec`命令的参数指定，或者使用主机文件。这个主机文件包含网络上将用于运行的节点的名称，以及主机上可用插槽的数量。
- en: A prerequisite for running MPI applications on a remote node is that the MPI
    runtime is installed on that node, and that password-less access has been configured
    for that node. This means that so long as the master node has the SSH keys installed,
    it can log into each of these nodes in order to launch the MPI application on
    it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在远程节点上运行MPI应用程序的先决条件是在该节点上安装了MPI运行时，并且已为该节点配置了无密码访问。这意味着只要主节点安装了SSH密钥，它就可以登录到每个节点，以便在其上启动MPI应用程序。
- en: Setting up an MPI node
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置MPI节点
- en: After installing MPI on a node, the next step is to set up password-less SSH
    access for the master node. This requires the SSH server to be installed on the
    node (part of the *ssh* package on Debian-based distributions). After this we
    need to generate and install the SSH key.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在节点上安装MPI后，下一步是为主节点设置无密码SSH访问。这需要在节点上安装SSH服务器（在基于Debian的发行版中属于*ssh*软件包的一部分）。之后，我们需要生成并安装SSH密钥。
- en: One way to easily do this is by having a common user on the master node and
    other nodes, and using an NFS network share or similar to mount the user folder
    on the master node on the compute nodes. This way all nodes would have the same
    SSH key and known hosts file. One disadvantage of this approach is the lack of
    security. For an internet-connected cluster, this would not be a very good approach.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的方法是在主节点和其他节点上有一个公共用户，并使用NFS网络共享或类似的方式在计算节点上挂载主节点上的用户文件夹。这样所有节点都将拥有相同的SSH密钥和已知主机文件。这种方法的一个缺点是缺乏安全性。对于连接到互联网的集群来说，这不是一个很好的方法。
- en: 'It is, however, a definitely good idea to run the job on each node as the same
    user to prevent any possible permission issues, especially when using files and
    other resources. With the common user account created on each node, and with the
    SSH key generated, we can transfer the public key to the node using the following
    command:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，以相同用户在每个节点上运行作业绝对是一个好主意，以防止任何可能的权限问题，特别是在使用文件和其他资源时。通过在每个节点上创建一个公共用户帐户，并生成SSH密钥，我们可以使用以下命令将公钥传输到节点：
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Alternatively, we can copy the public key into the `authorized_keys` file on
    the node system while we are setting it up. If creating and configuring a large
    number of nodes, it would make sense to use an image to copy onto each node's
    system drive, use a setup script, or possibly boot from an image through PXE boot.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，在设置节点系统时，我们可以将公钥复制到节点系统的`authorized_keys`文件中。如果要创建和配置大量节点，最好使用镜像复制到每个节点的系统驱动器上，使用设置脚本，或者可能通过PXE引导从镜像启动。
- en: With this step completed, the master node can now log into each compute node
    in order to run jobs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了这一步，主节点现在可以登录到每个计算节点以运行作业。
- en: Creating the MPI host file
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建MPI主机文件
- en: As mentioned earlier, in order to run a job on other nodes, we need to specify
    these nodes. The easiest way to do this is to create a file containing the names
    of the compute nodes we wish to use, along with optional parameters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，为了在其他节点上运行作业，我们需要指定这些节点。最简单的方法是创建一个文件，其中包含我们希望使用的计算节点的名称，以及可选参数。
- en: 'To allow us to use names for the nodes instead of IP addresses, we have to
    modify the operating system''s host file first: for example, `/etc/hosts` on Linux:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们能够使用节点的名称而不是IP地址，我们首先需要修改操作系统的主机文件：例如，在Linux上是`/etc/hosts`。
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next we create a new file which will be the host file for use with MPI:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个新文件，这将是用于MPI的主机文件：
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With this configuration, a job would be executed on both compute nodes, as well
    as the master node. We can take the master node out of this file to prevent this.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个配置，作业将在两个计算节点以及主节点上执行。我们可以从这个文件中删除主节点，以防止这种情况发生。
- en: 'Without any optional parameter provided, the MPI runtime will use all available
    processors on the node. If it is desirable, we can limit this number:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有提供任何可选参数，MPI运行时将使用节点上的所有可用处理器。如果需要，我们可以限制这个数字：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Assuming that both nodes are quad-core CPUs, this would mean that only half
    the cores on node0 would be used, and all of them on node1.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 假设两个节点都是四核CPU，这意味着只有node0上的一半核心会被使用，而node1上的所有核心都会被使用。
- en: Running the job
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行作业
- en: 'Running an MPI job across multiple MPI nodes is basically the same as executing
    it only locally, as in the example earlier in this chapter:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个MPI节点上运行MPI作业基本上与仅在本地执行相同，就像本章前面的示例一样：
- en: '[PRE17]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This command would tell the MPI launcher to use a host file called `my_hostfile`
    and run a copy of the specified MPI application on each processor of each node
    found in that host file.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令会告诉MPI启动器使用一个名为`my_hostfile`的主机文件，并在该主机文件中找到的每个节点的每个处理器上运行指定的MPI应用程序的副本。
- en: Using a cluster scheduler
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集群调度程序
- en: In addition to using a manual command and host files to create and start jobs
    on specific nodes, there are also cluster scheduler applications. These generally
    involve the running of a daemon process on each node as well as the master node.
    Using the provided tools, one can then manage resources and jobs, scheduling allocation
    and keeping track of job status.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用手动命令和主机文件在特定节点上创建和启动作业之外，还有集群调度程序应用程序。这些通常涉及在每个节点以及主节点上运行一个守护进程。使用提供的工具，我们可以管理资源和作业，安排分配并跟踪作业状态。
- en: 'One of the most popular cluster management scheduler''s is SLURM, which short
    for Simple Linux Utility for Resource management (though now renamed to Slurm
    Workload Manager with the website at [https://slurm.schedmd.com/](https://slurm.schedmd.com/)).
    It is commonly used by supercomputers as well as many computer clusters. Its primary
    functions consist out of:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的集群管理调度程序之一是SLURM，它是Simple Linux Utility for Resource management的缩写（尽管现在更名为Slurm
    Workload Manager，网站为[https://slurm.schedmd.com/](https://slurm.schedmd.com/)）。它通常被超级计算机以及许多计算机集群所使用。其主要功能包括：
- en: Allocating exclusive or non-exclusive access to resources (nodes) to specific
    users using time slots
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用时间段为特定用户分配对资源（节点）的独占或非独占访问权限
- en: The starting and monitoring of jobs such as MPI-based applications on a set
    of nodes
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一组节点上启动和监视诸如基于MPI的应用程序之类的作业
- en: Managing a queue of pending jobs to arbitrate contention for shared resources
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理待处理作业队列，以调解共享资源的争用
- en: The setting up of a cluster scheduler is not required for a basic cluster operation,
    but can be very useful for larger clusters, when running multiple jobs simultaneously,
    or when having multiple users of the cluster wishing to run their own job.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 设置集群调度程序对于基本的集群操作并不是必需的，但在运行多个作业同时或者有多个集群用户希望运行自己的作业时，它可能非常有用。
- en: MPI communication
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MPI通信
- en: At this point, we have a functional MPI cluster, which can be used to execute
    MPI-based applications (and others, as well) in a parallel fashion. While for
    some tasks it might be okay to just send dozens or hundreds of processes on their
    merry way and wait for them to finish, very often it is crucial that these parallel
    processes are able to communicate with each other.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们有一个功能齐全的MPI集群，可以用于以并行方式执行基于MPI的应用程序（以及其他应用程序）。虽然对于某些任务，只需将几十个或几百个进程发送出去并等待它们完成可能是可以的，但很多时候，这些并行进程能够相互通信是至关重要的。
- en: This is where the true meaning of MPI (being "Message Passing Interface") comes
    into play. Within the hierarchy created by an MPI job, processes can communicate
    and share data in a variety of ways. Most fundamentally, they can share and receive
    messages.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是MPI（“消息传递接口”）的真正含义所在。在MPI作业创建的层次结构中，进程可以以各种方式进行通信和共享数据。最基本的是，它们可以共享和接收消息。
- en: 'An MPI message has the following properties:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: MPI消息具有以下属性：
- en: A sender
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发送方
- en: A receiver
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接收方
- en: A message tag (ID)
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息标签（ID）
- en: A count of the elements in the message
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息中元素的计数
- en: An MPI datatype
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个MPI数据类型
- en: The sender and receiver should be fairly obvious. The message tag is a numeric
    ID which the sender can set and which the receiver can use to filter messages,
    to, for example, allow for the prioritizing of specific messages. The data type
    determines the type of information contained in the message.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 发送方和接收方应该是相当明显的。消息标签是发送方可以设置的数字ID，接收方可以使用它来过滤消息，例如，允许对特定消息进行优先排序。数据类型确定消息中包含的信息类型。
- en: 'The send and receive functions look like this:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 发送和接收函数如下所示：
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: An interesting thing to note here is that the count parameter in the send function
    indicates the number of elements that the function will be sending, whereas the
    same parameter in the receive function indicates the maximum number of elements
    that this thread will accept.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一个有趣的事情是，发送函数中的计数参数表示函数将发送的元素数，而接收函数中的相同参数表示此线程将接受的最大元素数。
- en: The communicator refers to the MPI communicator instance being used, and the
    receive function contains a final parameter which can be used to check the status
    of the MPI message.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通信器指的是正在使用的MPI通信器实例，接收函数包含一个最终参数，可用于检查MPI消息的状态。
- en: MPI data types
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MPI数据类型
- en: 'MPI defines a number of basic types, which one can use directly:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: MPI定义了许多基本类型，可以直接使用：
- en: '| **MPI datatype** | **C equivalent** |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **MPI数据类型** | **C等效** |'
- en: '| --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `MPI_SHORT` | short int |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_SHORT` | short int |'
- en: '| `MPI_INT` | int |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_INT` | int |'
- en: '| `MPI_LONG` | long int |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_LONG` | long int |'
- en: '| `MPI_LONG_LONG` | long long int |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_LONG_LONG` | long long int |'
- en: '| `MPI_UNSIGNED_CHAR` | unsigned char |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_UNSIGNED_CHAR` | unsigned char |'
- en: '| `MPI_UNSIGNED_SHORT` | unsigned short int |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_UNSIGNED_SHORT` | unsigned short int |'
- en: '| `MPI_UNSIGNED` | unsigned int |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_UNSIGNED` | unsigned int |'
- en: '| `MPI_UNSIGNED_LONG` | unsigned long int |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_UNSIGNED_LONG` | unsigned long int |'
- en: '| `MPI_UNSIGNED_LONG_LONG` | unsigned long long int |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_UNSIGNED_LONG_LONG` | unsigned long long int |'
- en: '| `MPI_FLOAT` | float |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_FLOAT` | float |'
- en: '| `MPI_DOUBLE` | double |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_DOUBLE` | double |'
- en: '| `MPI_LONG_DOUBLE` | long double |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_LONG_DOUBLE` | long double |'
- en: '| `MPI_BYTE` | char |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| `MPI_BYTE` | char |'
- en: MPI guarantees that when using these types, the receiving side will always get
    the message data in the format it expects, regardless of endianness and other
    platform-related issues.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: MPI保证使用这些类型时，接收方将始终以其期望的格式获取消息数据，而不受字节顺序和其他与平台相关的问题的影响。
- en: Custom types
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义类型
- en: 'In addition to these basic formats, one can also create new MPI data types.
    These use a number of MPI functions, including `MPI_Type_create_struct`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基本格式之外，还可以创建新的MPI数据类型。这些使用了许多MPI函数，包括`MPI_Type_create_struct`：
- en: '[PRE19]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With this function, one can create an MPI type that contains a struct, to be
    passed just like a basic MPI data type:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此函数，可以创建一个包含结构的MPI类型，就像使用基本的MPI数据类型一样：
- en: '[PRE20]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here we see how a new MPI data type called `mpi_car_type` is defined and used
    to message between two processes. To create a struct type like this, we need to
    define the number of items in the struct, the number of elements in each block,
    their byte displacement, and their basic MPI types.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到了一个名为`mpi_car_type`的新MPI数据类型是如何定义和用于在两个进程之间传递消息的。要创建这样的结构类型，我们需要定义结构中的项目数，每个块中的元素数，它们的字节位移以及它们的基本MPI类型。
- en: Basic communication
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本通信
- en: A simple example of MPI communication is the sending of a single value from
    one process to another. In order to do this, one needs to use the following listed
    code and run the compiled binary to start at least two processes. It does not
    matter whether these processes run locally or on two compute nodes.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: MPI通信的一个简单示例是从一个进程向另一个进程发送单个值。为了做到这一点，需要使用以下列出的代码，并运行编译后的二进制文件以启动至少两个进程。这些进程是在本地运行还是在两个计算节点上运行并不重要。
- en: 'The following code was gratefully borrowed from [http://mpitutorial.com/tutorials/mpi-hello-world/](http://mpitutorial.com/tutorials/mpi-hello-world/):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码感激地借鉴自[http://mpitutorial.com/tutorials/mpi-hello-world/](http://mpitutorial.com/tutorials/mpi-hello-world/)：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There isn't a lot to this code. We work through the usual MPI initialization,
    followed by a check to ensure that our world size is at least two processes large.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码并不复杂。我们通过通常的MPI初始化，然后检查我们的世界大小是否至少有两个进程。
- en: The process with rank 0 will then send an MPI message of data type `MPI_INT`
    and value `-1`. The process with rank `1` will wait to receive this message. The
    receiving process specifies for `MPI_Status MPI_STATUS_IGNORE` to indicate that
    the process will not be checking the status of the message. This is a useful optimization
    technique.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 具有等级0的进程将发送一个数据类型为`MPI_INT`且值为`-1`的MPI消息。等级为`1`的进程将等待接收此消息。接收进程指定`MPI_Status
    MPI_STATUS_IGNORE`以指示该进程不会检查消息的状态。这是一种有用的优化技术。
- en: 'Finally, the expected output is the following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，预期的输出如下：
- en: '[PRE22]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here we start the compiled demo code with a total of two processes. The output
    shows that the second process received the MPI message from the first process,
    with the correct value.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们启动了一个总共有两个进程的编译后的演示代码。输出显示第二个进程从第一个进程接收了MPI消息，并且值是正确的。
- en: Advanced communication
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级通信
- en: For advanced MPI communication, one would use the `MPI_Status` field to obtain
    more information about a message. One can use `MPI_Probe` to discover a message's
    size before accepting it with `MPI_Recv`. This can be useful for situations where
    it is not known beforehand what the size of a message will be.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高级MPI通信，可以使用`MPI_Status`字段来获取有关消息的更多信息。可以使用`MPI_Probe`在接受消息之前发现消息的大小，然后使用`MPI_Recv`接受消息。这在不事先知道消息大小的情况下非常有用。
- en: Broadcasting
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广播
- en: 'Broadcasting a message means that all processes in the world will receive it.
    This simplifies the broadcast function relative to the send function:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 广播消息意味着世界上的所有进程都会收到它。这简化了广播函数相对于发送函数：
- en: '[PRE23]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The receiving processes would simply use a normal `MPI_Recv` function. All that
    the broadcast function does is optimize the sending of many messages using an
    algorithm that uses multiple network links simultaneously, instead of just one.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接收进程将简单地使用普通的`MPI_Recv`函数。广播函数所做的就是优化使用一种算法同时使用多个网络链接发送多条消息，而不是只使用一个。
- en: Scattering and gathering
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 散射和聚集
- en: 'Scattering is very similar to broadcasting a message, with one very important
    distinction: instead of sending the same data in each message, instead it sends
    a different part of an array to each recipient. Its function definition looks
    as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 散射非常类似于广播消息，但有一个非常重要的区别：它不是在每条消息中发送相同的数据，而是将数组的不同部分发送给每个接收者。其功能定义如下：
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Each receiving process will get the same data type, but we can specify how many
    items will be sent to each process (`send_count`). This function is used on both
    the sending and receiving side, with the latter only having to define the last
    set of parameters relating to receiving data, with the world rank of the root
    process and the relevant communicator being provided.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 每个接收进程将获得相同的数据类型，但我们可以指定将发送到每个进程的项目数（`send_count`）。这个函数在发送和接收方都使用，后者只需要定义与接收数据相关的最后一组参数，提供根进程的世界等级和相关的通信器。
- en: 'Gathering is the inverse of scattering. Here multiple processes will send data
    that ends up at a single process, with this data sorted by the rank of the process
    which sent it. Its function definition looks as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 聚集是散射的逆过程。在这里，多个进程将发送的数据最终到达单个进程，这些数据按发送它的进程的等级进行排序。其功能定义如下：
- en: '[PRE25]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: One may notice that this function looks very similar to the scatter function.
    This is because it works basically the same way, only this time around the sending
    nodes have to all fill in the parameters related to sending the data, while the
    receiving process has to fill in the parameters related to receiving data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能会注意到这个函数看起来与散射函数非常相似。这是因为它基本上是以相同的方式工作，只是这一次发送节点必须填写与发送数据相关的参数，而接收进程必须填写与接收数据相关的参数。
- en: It is important to note here that the `recv_count` parameter relates to the
    amount of data received from each sending process, not the size in total.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的是`recv_count`参数与从每个发送进程接收的数据量有关，而不是总大小。
- en: There exist further specializations of these two basic functions, but these
    will not be covered here.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个基本功能还有进一步的专业化，但这里不会涉及。
- en: MPI versus threads
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MPI与线程
- en: One might think that it would be easiest to use MPI to allocate one instance
    of the MPI application to a single CPU core on each cluster node, and this would
    be true. It would, however, not be the fastest solution.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能认为最容易的方法是使用MPI将MPI应用程序的一个实例分配给每个集群节点上的单个CPU核心，这是正确的。然而，这并不是最快的解决方案。
- en: Although for communication between processes across a network MPI is likely
    the best choice in this context, within a single system (single or multi-CPU system)
    using multithreading makes a lot of sense.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在网络上的进程间通信方面，MPI可能是最佳选择，但在单个系统（单CPU或多CPU系统）中，使用多线程是非常有意义的。
- en: The main reason for this is simply that communication between threads is significantly
    faster than inter-process communication, especially when using a generalized communication
    layer such as MPI.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这主要是因为线程之间的通信比进程间通信要快得多，特别是在使用诸如MPI这样的通用通信层时。
- en: 'One could write an application that uses MPI to communicate across the cluster''s
    network, whereby one allocates one instance of the application to each MPI node.
    The application itself would detect the number of CPU cores on that system, and
    create one thread for each core. Hybrid MPI, as it''s often called, is therefore
    commonly used, for the advantages it provides:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 可以编写一个使用MPI在集群网络上进行通信的应用程序，其中为每个MPI节点分配一个应用程序实例。应用程序本身将检测该系统上的CPU核心数量，并为每个核心创建一个线程。因此，混合MPI，通常被称为，因为它提供了以下优势，因此通常被使用：
- en: '**Faster communication** – using fast inter-thread communication.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更快的通信** - 使用快速的线程间通信。'
- en: '**Fewer MPI messages** – fewer messages means a reduction in bandwidth and
    latency.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更少的MPI消息** - 更少的消息意味着带宽和延迟的减少。'
- en: '**Avoiding data duplication** – data can be shared between threads instead
    of sending the same message to a range of processes.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免数据重复** - 数据可以在线程之间共享，而不是向一系列进程发送相同的消息。'
- en: Implementing this can be done the way we have seen in previous chapters, by
    using the multithreading features found in C++11 and successive versions. The
    other option is to use OpenMP, as we saw at the very beginning of this chapter.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用在前几章中看到的C++11和后续版本中找到的多线程功能来实现这一点。另一种选择是使用OpenMP，就像我们在本章的开头看到的那样。
- en: The obvious advantage of using OpenMP is that it takes very little effort from
    the developer's side. If all that one needs is to get more instances of the same
    routine running, all it takes is are the small modifications to mark the code
    to be used for the worker threads.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenMP的明显优势是开发者几乎不需要付出什么努力。如果需要的只是运行相同例程的更多实例，只需要对代码进行少量修改，标记代码用于工作线程即可。
- en: 'For example:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE26]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The above code combines an OpenMP application with MPI. To compile it we would
    run for example:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将OpenMP应用程序与MPI结合起来。例如，要编译它，我们将运行：
- en: '[PRE27]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, to run the application, we would use mpirun or equivalent:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，要运行应用程序，我们将使用mpirun或等效命令：
- en: '[PRE28]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The mpirun command would run two MPI processes using the hellohybrid binary,
    passing the environment variable we exported with the -x flag to each new process.
    The value contained in that variable will then be used by the OpenMP runtime to
    create that number of threads.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: mpirun命令将使用hellohybrid二进制文件运行两个MPI进程，并将我们使用-x标志导出的环境变量传递给每个新进程。然后，该变量中包含的值将由OpenMP运行时用于创建相应数量的线程。
- en: Assuming we have at least two MPI nodes in our MPI host file, we would end up
    with two MPI processes across two nodes, each of which running eight threads,
    which would fit a quad-core CPU with Hyper-Threading or an octo-core CPU.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的MPI主机文件中至少有两个MPI节点，我们将在两个节点上运行两个MPI进程，每个进程运行八个线程，这将适合具有超线程的四核CPU或八核CPU。
- en: Potential issues
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在问题
- en: When writing MPI-based applications and executing them on either a multi-core
    CPU or cluster, the issues one may encounter are very much the same as those we
    already came across with the multithreaded code in the preceding chapters.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写基于MPI的应用程序并在多核CPU或集群上执行时，可能会遇到的问题与我们在前面章节中已经遇到的多线程代码问题非常相似。
- en: However, an additional worry with MPI is that one relies on the availability
    of network resources. Since a send buffer used for an `MPI_Send` call cannot be
    reclaimed until the network stack can process the buffer, and this call is a blocking
    type, sending lots of small messages can lead to one process waiting for another,
    which in turn is waiting for a call to complete.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用MPI的一个额外担忧是依赖网络资源的可用性。由于用于`MPI_Send`调用的发送缓冲区在网络堆栈处理缓冲区之前无法回收，并且此调用是阻塞类型，发送大量小消息可能导致一个进程等待另一个进程，而另一个进程又在等待调用完成。
- en: This type of deadlock should be kept in mind when designing the messaging structure
    of an MPI application. One can, for example, ensure that there are no send calls
    building up on one side, which would lead to such a scenario. Providing feedback
    messages on, queue depth and similar could be used to the ease pressure.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计MPI应用程序的消息传递结构时，应该牢记这种死锁。例如，可以确保一侧没有发送调用积累，这将导致这种情况。提供有关队列深度等的反馈消息可以用于减轻压力。
- en: MPI also contains a synchronization mechanism using a so-called barrier. This
    is meant to be used between MPI processes to allow them to synchronize on for
    example a task. Using an MPI barrier (`MPI_Barrier`) call is similarly problematic
    as a mutex in that if an MPI process does not manage to get synchronized, everything
    will hang at this point.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: MPI还包含使用所谓的屏障的同步机制。这是用于允许MPI进程在例如一个任务上进行同步的。使用MPI屏障（`MPI_Barrier`）调用与互斥锁类似，如果MPI进程无法实现同步，一切都将在此时挂起。
- en: Summary
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked in some detail at the MPI standard, along with a
    number of its implementations, specifically Open MPI, and we looked at how to
    set up a cluster. We also saw how to use OpenMP to easily add multithreading to
    existing codes.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细研究了MPI标准，以及其中一些实现，特别是Open MPI，并了解了如何设置集群。我们还看到如何使用OpenMP轻松地为现有代码添加多线程。
- en: At this point, the reader should be capable of setting up a basic Beowulf or
    similar cluster, configuring it for MPI, and running basic MPI applications on
    it. How to communicate between MPI processes and how to define custom data types
    should be known. In addition, the reader will be aware of the potential pitfalls
    when programming for MPI.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，读者应该能够建立一个基本的贝奥武夫或类似的集群，为MPI进行配置，并在其上运行基本的MPI应用程序。应该知道如何在MPI进程之间进行通信以及如何定义自定义数据类型。此外，读者将意识到在为MPI编程时可能遇到的潜在问题。
- en: In the next chapter, we will take all our knowledge of the preceding chapters
    and see how we can combine it in the final chapter, as we look at general-purpose
    computing on videocards (GPGPU).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将汇总前面章节的所有知识，并看看如何在最后一章中将它们结合起来，以便研究通用计算机上的视频卡（GPGPU）。
