- en: Introduction to CUDA Programming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA编程简介
- en: 'Since its first release in 2007, **Compute Unified Device Architecture** (**CUDA**)
    has grown to become the de facto standard when it comes to using **Graphic Computing
    Units** (**GPUs**) for general-purpose computation, that is, non-graphics applications.
    So, what exactly is CUDA? Someone might ask the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自2007年首次发布以来，**统一计算设备架构**（**CUDA**）已经成长为使用**图形计算单元**（**GPU**）进行通用计算的事实标准，即非图形应用程序。那么，CUDA到底是什么？有人可能会问以下问题：
- en: Is it a programming language?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一种编程语言吗？
- en: Is it a compiler?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个编译器吗？
- en: Is it a new computing paradigm?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一种新的计算范式吗？
- en: In this chapter, we will demystify some of the myths around GPU and CUDA. This
    chapter lays the foundation for heterogeneous computing by providing a simplified
    view of **High-Performance Computing** (**HPC**) history and substantiating it
    with laws such as Moore's Law and Dennard Scaling, which were—and still are—driving
    the semiconductor industry and hence the processor architecture itself. You will
    also be introduced to the CUDA programming model and get to know the fundamental
    difference between CPU and GPU architecture. By the end of this chapter, you will
    be able to write and understand `Hello World!` programs using CUDA programming
    constructs in the C language.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将揭开有关GPU和CUDA的一些神秘之谜。本章通过提供对**高性能计算**（**HPC**）历史的简化视图，并用摩尔定律和丹纳德缩放等法则加以证实，为异构计算奠定基础，这些法则一直在推动半导体行业，因此也推动了处理器架构本身。您还将了解CUDA编程模型，并了解CPU和GPU架构之间的根本区别。通过本章的学习，您将能够使用C语言中的CUDA编程构造编写和理解`Hello
    World!`程序。
- en: While this chapter primarily uses C to demonstrate CUDA constructs, we will
    be covering other programming languages such as Python, Fortran, and OpenACC in
    other chapters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章主要使用C语言来演示CUDA构造，但我们将在其他章节中涵盖其他编程语言，如Python、Fortran和OpenACC。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The history of high-performance computing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高性能计算的历史
- en: Hello World from CUDA
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自CUDA的Hello World
- en: Vector addition using CUDA
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CUDA进行矢量加法
- en: Error reporting with CUDA
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA的错误报告
- en: Data type support in CUDA
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA中的数据类型支持
- en: The history of high-performance computing
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高性能计算的历史
- en: HPC has always pushed the limits in order to deliver scientific discoveries.
    The fundamental shift in processor architecture and design has helped to cross
    FLOP barriers, starting from **Mega-Floating Point Operations** (**MFLOPs**) to
    now being able to do PetaFLOP calculation in a second.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 高性能计算一直在不断突破极限，以实现科学发现。处理器架构和设计的根本转变有助于跨越FLOP障碍，从**百万浮点运算**（**MFLOPs**）开始，现在能够在一秒内进行PetaFLOP计算。
- en: '**Floating-Point Operations** (**FLOPs**) per second is the fundamental unit
    for measuring the theoretical peak of any compute processor. MegaFLOP stands for
    10 to the 6^(th) power of FLOPS. PetaFLOP stands for 10 to the 15^(th) power of
    FLOPS.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 每秒**浮点运算**（**FLOPs**）是衡量任何计算处理器理论峰值的基本单位。MegaFLOP代表FLOPS的10的6次方。PetaFLOP代表FLOPS的10的15次方。
- en: '**Instruction-Level Parallelism** (**ILP**) is a concept wherein code-independent
    instructions can execute at the same time. For the instructions to execute in
    parallel, they need to be independent of each other. All modern CPU architecture
    (even GPU architecture) provides five to 15+ stages to allow for faster clock
    rates:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**指令级并行性**（**ILP**）是一个概念，其中独立于代码的指令可以同时执行。为了使指令并行执行，它们需要彼此独立。所有现代CPU架构（甚至GPU架构）都提供了五到15个以上的阶段，以实现更快的时钟频率：'
- en: '`Instr 1: add = inp1 + inp2`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`Instr 1: add = inp1 + inp2`'
- en: '`Instr 2: mult = inp1 * inp2`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`Instr 2: mult = inp1 * inp2`'
- en: '`Instr 3: final_result = mult / add`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`Instr 3: final_result = mult / add`'
- en: Operations for calculating the `mult` and `add` variables do not depend on each
    other, so they can be calculated simultaneously while calculating `final_result`,
    which depends on the results of the `Instr 1` and `Instr 2` operations. Therefore,
    it cannot be calculated until `add` and `mult` have been calculated.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 用于计算`mult`和`add`变量的操作不相互依赖，因此可以在计算`final_result`时同时计算，而`final_result`依赖于`Instr
    1`和`Instr 2`操作的结果。因此，在计算`add`和`mult`之前无法计算它。
- en: 'When we look at the history of HPC in terms of technology changes, which resulted
    in a fundamental shift in designing new processors and its impact on the scientific
    community, there are three primary ones that stand out and can be referred to
    as epochs:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从技术变革的角度看高性能计算的历史，这些变革导致了新处理器设计的根本转变，以及对科学界的影响，有三个主要的变革可以被称为时代：
- en: '**Epoch 1**: The history of the supercomputer goes back to CRAY-1, which was
    basically a single vector CPU architecture providing peak 160 MegaFLOP/MFLOP compute
    power.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时代1**：超级计算机的历史可以追溯到CRAY-1，它基本上是一个提供峰值160 MegaFLOP/MFLOP计算能力的单一矢量CPU架构。'
- en: '**Epoch 2**: The MegaFLOP barrier was crossed by moving from single-core design
    to multi-core design in CRAY-2, which was a 4 Core Vector CPU that gave 2 GigaFLOPs
    of peak performance.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时代2**：通过从单核设计转向CRAY-2的多核设计，跨越了MegaFLOP障碍，CRAY-2是一个4核矢量CPU，提供了2 GigaFLOPs的峰值性能。'
- en: '**Epoch 3**: Crossing GigaFLOP compute performance was a fundamental shift
    and required compute nodes to work with each other and communicate by a network
    to deliver higher performance. Cray T3D was one of the first machines that delivered
    1 TeraFLOP of compute performance. The network was 3D Torus and provided a bandwidth
    of 300 MB/s. It was the first significant implementation of a rich *shell* around
    a standard microprocessor.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时代3**：跨越GigaFLOP计算性能是一个根本性的转变，需要计算节点相互协作，并通过网络进行通信，以提供更高的性能。 Cray T3D是第一批提供1
    TeraFLOP计算性能的机器之一。网络是3D Torus，提供300 MB/s的带宽。这是标准微处理器周围丰富*shell*的第一个重要实现。'
- en: 'After this, for almost 20 years, there were no fundamental innovations. Technological
    innovations were primarily focused on three architectural innovations:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，将近20年没有根本性的创新。技术创新主要集中在三个架构创新上：
- en: Moving from an 8-bit to a 16-bit to a 32-bit and now a 64-bit instruction set
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从8位到16位再到32位，现在是64位指令集
- en: Increasing ILP
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加ILP
- en: Increasing the number of cores
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加核心数量
- en: This was supported by increasing the clock rate, which currently stands at 4
    GHz. It was possible to deliver this because of the fundamental laws that drove
    the semiconductor industry.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这得到了时钟频率的增加，目前为4 GHz。由于驱动半导体行业的基本定律，这是可能的。
- en: Moore's Law: This law observes the number of transistors in a dense integrated
    circuit double every two years.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 摩尔定律：这个定律观察到密集集成电路中的晶体管数量每两年翻一番。
- en: Moore's prediction proved accurate for several decades and still does. Moore's
    Law is an observation and projection of a historical trend.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 摩尔的预测在几十年来一直准确无误。摩尔定律是对历史趋势的观察和预测。
- en: '**Dennard scaling:** This a scaling law that keeps Moore''s Law alive. Dennard
    made an observation with respect to the relationship between transistor size and
    power density and summarized it in the following formula:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Dennard缩放：这是一个使摩尔定律保持活力的缩放定律。Dennard观察到晶体管尺寸和功率密度之间的关系，并用以下公式总结了这一观察：
- en: '*P = QfCV² + V I[leakage]*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*P = QfCV² + V I[leakage]*'
- en: In this equation, *Q* is the number of transistors, *f* is the operating frequency,
    *C* is the capacitance, *V* is the operating voltage, and *I[leakage]* is the
    leakage current.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*Q*是晶体管数量，*f*是操作频率，*C*是电容，*V*是操作电压，*I[leakage]*是泄漏电流。
- en: Dennard scaling and Moore's Law are related to each other as it's inferred that
    reducing the size of transistors can lead to more and more transistors per chip
    in terms of cost-effectiveness.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Dennard缩放和摩尔定律彼此相关，因为可以推断出，减小晶体管的尺寸可以在成本效益方面导致芯片上的晶体管数量越来越多。
- en: With Dennard scaling rules, the total chip power for a given size stayed the
    same for many processor generations. Transistor count doubled while size kept
    shrinking (*1/S* rate) and increased in frequency by 40% every two years. This
    stopped after the feature size reached below 65 nm as these rules could no longer
    be sustained due to the leakage current growing exponentially. To reduce the effect
    of leakage current, new innovations were enforced on the switching process. However,
    these breakthroughs still were not sufficient to revive how voltage was scaled.
    The voltage remained constant at 1 V for many processor designs. It was no longer
    possible to keep the power envelope constant. This is also popularly known as
    Powerwall.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Dennard缩放规则，对于给定尺寸的芯片，多个处理器世代的总芯片功率保持不变。晶体管数量翻倍，而尺寸不断缩小（*1/S*速率），并且每两年以40%的速度增加频率。当特征尺寸达到65纳米以下时，这种情况停止了，因为泄漏电流呈指数增长，这些规则不再能够持续。为了减少泄漏电流的影响，新的创新被强制执行在开关过程中。然而，这些突破仍然不足以恢复电压的缩放。电压在许多处理器设计中保持在1V恒定。不再可能保持功率包络恒定。这也被称为Powerwall。
- en: Dennard scaling held its own from 1977 until 1997 and then began to fade. Due
    to this, from 2007 to 2017, processors went from 45 nm to 16 nm but resulted in
    a threefold increase in energy/chip size.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Dennard缩放从1977年一直持续到1997年，然后开始衰退。因此，从2007年到2017年，处理器从45纳米变为16纳米，但导致每芯片能耗增加了三倍。
- en: At the same time, the pipeline stages went from five stages to 15+ in the latest
    architecture. To keep the instruction pipeline full, advance techniques such as
    speculation were used. The speculation unit involves predicting the program's
    behavior, such as predicting branches and memory addresses. If a prediction is
    accurate, it can proceed; otherwise, it undoes the work it did and restarts. Deep
    pipeline stages and the way legacy software is written resulted in unused transistors
    and wasted clock cycles, which means that there was no improvement in terms of
    performance for the application.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，管线阶段从五个阶段发展到了最新架构的15+个阶段。为了保持指令管线的充分，使用了先进的技术，比如推测。推测单元涉及预测程序的行为，比如预测分支和内存地址。如果预测准确，它可以继续；否则，它会撤销已经完成的工作并重新开始。深层管线阶段和传统软件的编写方式导致了未使用的晶体管和浪费的时钟周期，这意味着应用性能没有改善。
- en: Then came GPU, which was primarily used for graphics processing. A researcher
    named Mark Harris made use of GPU for non-graphics tasks for the first time, and
    the new term **General Purpose Computation using GPU** (**GPGPU**) was coined.
    GPU was proven to be efficient when it came to certain tasks that fell into the
    category of data parallelism. Unsurprisingly, most of the compute-intensive tasks
    in many HPC applications are data-parallel in nature. They were mostly matrix
    to matrix multiplications, which is a routine in the **Basic Linear Algebra Specification** (**BLAS**)
    and used extensively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后出现了GPU，最初主要用于图形处理。研究人员马克·哈里斯首次利用GPU进行了非图形任务，并创造了新术语**使用GPU进行通用计算**（GPGPU）。GPU在某些数据并行类任务方面被证明是有效的。毫不奇怪，许多HPC应用程序中的大部分计算密集型任务在性质上都是数据并行的。它们主要是矩阵乘法，这在**基本线性代数规范**（BLAS）中是常规且广泛使用的。
- en: The only problem for users when it came to adapting and using GPU was that they
    had to understand the graphics pipeline to make use of GPU. The only interface
    that was provided for any computation work on GPU centered around shader execution.
    There was a need to provide a more general interface that was known to developers
    who were working in the HPC community. This was solved by the introduction of
    CUDA in 2007.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 用户在适应和使用GPU时唯一的问题是他们必须了解图形管线以利用GPU。提供给GPU上任何计算工作的唯一接口围绕着着色器执行。需要提供一个更通用的接口，让在HPC社区工作的开发人员熟悉。这个问题在2007年引入CUDA时得到解决。
- en: While the GPU architecture is also bound by the same laws (Moore's Law and Dennard
    scaling), the design of processors takes a different approach and dedicates transistors
    for different usage and achieves higher performance than traditional homogeneous
    architectures.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GPU架构也受到相同的定律约束（摩尔定律和Dennard缩放），但处理器的设计采用了不同的方法，为不同的用途专门分配晶体管，并实现了比传统的同质架构更高的性能。
- en: 'The following diagram shows the evolution of computer architecture from sequential processing
    to distributed memory and its impact on programming models:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了计算机体系结构从顺序处理到分布式内存的演变及其对编程模型的影响：
- en: '![](img/88656735-ceac-44e5-87e3-3a376bd123a1.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88656735-ceac-44e5-87e3-3a376bd123a1.png)'
- en: With GPU being added to existing servers, there are two types of processors
    (CPU and GPU) on which the application runs, which brings in a notion of heterogeneity.
    This is what we will introduce in the next section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随着GPU被添加到现有服务器上，应用程序在两种处理器（CPU和GPU）上运行，引入了异构的概念。这是我们将在下一节介绍的内容。
- en: Heterogeneous computing
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异构计算
- en: The common misconception around GPU is that it is an alternative to CPU. GPUs
    are used to accelerate the parts of the code that are parallel in nature. **Accelerator**
    is a common term that's used for GPUs because they accelerate an application by
    running the parallel part of the code faster, while CPUs run the other part of
    the code, which is latency bound. Hence, a highly efficient CPU coupled with a
    high throughput GPU results in improved performance for the application.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕GPU的一个常见误解是它是CPU的替代品。GPU用于加速代码中并行的部分。**加速器**是一个常用术语，用于描述GPU，因为它们通过更快地运行代码的并行部分来加速应用程序，而CPU运行另一部分代码，即延迟绑定的部分。因此，高效的CPU与高吞吐量的GPU相结合，可以提高应用程序的性能。
- en: 'The following diagram represents an application running on multiple processor
    types:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表代表了在多种处理器类型上运行的应用程序：
- en: '![](img/3d540d44-63ee-44d7-b988-654c173377a4.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d540d44-63ee-44d7-b988-654c173377a4.png)'
- en: This concept can be very well defined with the help of Amdahl's law. Amdahl's
    law is used to define the maximum speedup that can be achieved when only a fraction
    of the application is parallelized. To demonstrate this, the preceding diagram
    shows two parts of the code. One part is latency bound, while the other is throughput
    bound. We will cover what these two terms mean in the next section, which differentiates
    between the CPU and GPU architecture.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念可以很好地用阿姆达尔定律来定义。阿姆达尔定律用于定义当应用程序的一部分被并行化时可以实现的最大加速。为了演示这一点，前面的图表显示了代码的两个部分。一个部分是延迟绑定的，而另一个是吞吐量绑定的。我们将在下一节中介绍这两个术语，区分CPU和GPU体系结构。
- en: The key point is that CPU is good for a certain fraction of code that is latency
    bound, while GPU is good at running the **Single Instruction Multiple Data** (**SIMD**)
    part of the code in parallel. If only one of them, that is, CPU code or GPU code,
    runs faster after optimization, this won't necessarily result in good speedup
    for the overall application. It is required that both of the processors, when
    used optimally, give maximum benefit in terms of performance. This approach of
    essentially *offloading* certain types of operations from the processor onto a
    GPU is called **heterogeneous computing**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点是，CPU对于某些延迟绑定的代码部分很好，而GPU擅长并行运行代码的**单指令多数据**（**SIMD**）部分。如果在优化后只有其中一个，即CPU代码或GPU代码，运行速度更快，这不一定会导致整体应用程序的速度提升。需要的是，当两个处理器都得到最佳利用时，性能方面才能获得最大的好处。这种从处理器上**卸载**某些类型的操作到GPU的方法被称为**异构计算**。
- en: 'The following diagram depicts the two types of sections that all applications
    have, that is, latency bound and throughput bound:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了所有应用程序具有的两种部分，即延迟绑定和吞吐量绑定：
- en: '![](img/9762f9d6-53f0-4e5e-9faf-ad4bde50a008.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9762f9d6-53f0-4e5e-9faf-ad4bde50a008.png)'
- en: Here, the importance of improving both sections is demonstrated using Amdahl's
    law.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，使用阿姆达尔定律演示了改进两个部分的重要性。
- en: Programming paradigm
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编程范式
- en: The classification of computer architecture was done using Flynn's taxonomy,
    which describes four classes of architecture. One of Flynn's classification SIMDs
    is used to describe GPU architecture. However, there is a subtle difference between
    the two. SIMD is used to describe an architecture where the same instruction is
    applied in parallel to multiple data points. This description is suitable for
    processors that have the capability of doing vectorization. In contrast, in **Single
    Instruction Multiple Threads** (**SIMTs**), rather than a single thread issuing
    the instructions, multiple threads issue the same instruction to different data.
    The GPU architecture is more suitable in terms of the SIMT category compared to
    SIMD.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机体系结构的分类是使用弗林分类法进行的，描述了四类体系结构。弗林的分类之一SIMD用于描述GPU体系结构。然而，两者之间存在微妙的差异。SIMD用于描述同一指令并行应用于多个数据点的体系结构。这种描述适用于具有矢量化能力的处理器。相比之下，在**单指令多线程**（**SIMT**）中，不是单个线程发出指令，而是多个线程向不同的数据发出相同的指令。与SIMD相比，GPU体系结构更适合SIMT类别。
- en: 'Let''s look at an example of adding two arrays and storing data in a third
    array. The dataset for this operation consists of the arrays *A*, *B*, and *C*.
    The same operations that are used for addition are used on each element of the
    array:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，将两个数组相加并将数据存储在第三个数组中。这个操作的数据集包括数组*A*、*B*和*C*。用于加法的相同操作被用于数组的每个元素：
- en: '*Cx = Ax + Bx*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*Cx = Ax + Bx*'
- en: It is obvious that each task is independent of each other, but the same operation
    is being applied by all of the threads.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，每个任务都是独立的，但所有线程都在应用相同的操作。
- en: 'The following screenshot shows vector addition, depicting an example of this paradigm:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了矢量加法，展示了这种范例的一个例子：
- en: '![](img/135a15fb-cafc-4775-89b5-25054ea73460.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/135a15fb-cafc-4775-89b5-25054ea73460.png)'
- en: Low latency versus higher throughput
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低延迟与高吞吐量
- en: As we mentioned in the previous section, CPU architecture is optimized for low
    latency access while GPU architecture is optimized for data parallel throughput
    computation. As shown in the following screenshot, the CPU architecture has a
    large amount of cache compared to GPU and has many types. The higher we go, that
    is, L3 to L1, the lower the amount of cache is present, but less latency. The
    CPU architecture is designed for low latency access to cached datasets. A large
    number of transistors are used to implement the speculative execution and out
    of order execution. Since CPUs run at a very high clock speed, it becomes necessary
    to hide the latency of fetching the data by frequently storing used data in caches
    and predicting the next instruction to execute. Applications that can explore
    this temporal locality can optimally make use of a CPU cache. Also, applications
    where it is easy to fill the instruction pipeline, for example, an application
    with no `if` and `else` statements in its code, can benefit from this by hiding
    the latency of fetching the instruction. Hence, the CPU architecture is a latency
    reducing architecture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中提到的，CPU架构被优化用于低延迟访问，而GPU架构被优化用于数据并行吞吐量计算。如下截图所示，与GPU相比，CPU架构具有大量缓存并且具有许多类型。我们越高，即从L3到L1，缓存的数量就越少，但延迟就越低。CPU架构旨在实现对缓存数据集的低延迟访问。大量晶体管用于实现推测执行和乱序执行。由于CPU以非常高的时钟速度运行，因此有必要通过频繁地将使用的数据存储在缓存中并预测下一条要执行的指令来隐藏获取数据的延迟。可以最佳地利用CPU缓存的应用程序可以探索这种时间局部性。此外，可以利用填充指令管线的应用程序，例如代码中没有`if`和`else`语句的应用程序，通过隐藏获取指令的延迟来受益。因此，CPU架构是一种减少延迟的架构。
- en: 'The following screenshot shows how the **CPU** and **GPU** architecture dedicate
    the chip die area for different memory and compute units. While **GPU** uses a
    lot of transistors for computing **ALUs**, **CPU** uses it to reduce latency:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了CPU和GPU架构如何为不同的内存和计算单元分配芯片芯片区域。GPU使用大量晶体管进行计算ALUs，而CPU使用它来减少延迟。
- en: '![](img/e651f520-b274-4564-b578-eec763952aa8.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e651f520-b274-4564-b578-eec763952aa8.png)'
- en: The GPU architecture, on the other hand, is called a **latency reducing** or
    **high throughput architecture**. The GPU architecture hides latency with computations
    from other threads. When one thread is waiting for the data to be available for
    computation, the other threads can start execution and hence not waste any clock
    cycles. If you are familiar with CUDA, then you might know about the concept of
    warps. We will cover the concept of warps in the upcoming chapters. (In CUDA,
    the execution unit is a warp and not a thread. Due to this, context switching
    happens between warps and not threads).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，GPU架构被称为“减少延迟”或“高吞吐量架构”。GPU架构通过来自其他线程的计算来隐藏延迟。当一个线程在等待数据可用进行计算时，其他线程可以开始执行，因此不会浪费任何时钟周期。如果您熟悉CUDA，那么您可能已经了解到warp的概念。我们将在接下来的章节中介绍warp的概念。（在CUDA中，执行单元是warp而不是线程。因此，上下文切换发生在warp而不是线程之间）。
- en: Some of you might be already wondering why we can't create these threads in
    the CPU and do the same thing to hide latency. The reason for this is that GPUs
    have lots of registers, and all of the thread context switching information is
    already present in them. This is the fastest memory that's available. However,
    in CPU, there are limited sets of registers and hence thread-related information
    is usually stored in a lower memory hierarchy such as a cache. For example, Volta
    contains 20 MB of register storage. Due to this, the context switching time between
    threads in CPU, compared to GPU, is much higher.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人可能已经在想为什么我们不能在CPU中创建这些线程并做同样的事情来隐藏延迟。原因是GPU有大量的寄存器，并且所有线程上下文切换信息已经存在于其中。这是最快的内存。然而，在CPU中，寄存器集是有限的，因此线程相关的信息通常存储在较低的内存层次结构中，比如缓存。例如，Volta包含20MB的寄存器存储。因此，与GPU相比，CPU中线程之间的上下文切换时间要长得多。
- en: Now, let's take a look at the different approaches when it comes to programming
    on GPU.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看在GPU编程方面的不同方法。
- en: Programming approaches to GPU
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU的编程方法
- en: Let's go back to our original question, that is, what is CUDA? CUDA is a parallel
    computing platform and programming model architecture developed by NVIDIA that
    exposes general-purpose computations on GPU as first-class capabilities. Like
    any other processor, the GPU architecture can be coded using various methods.
    The easiest method, which provides drop-in acceleration, is making use of existing
    libraries. Alternatively, developers can choose to make use of **OpenACC** directives
    for quick acceleration results and portability. Another option is to choose to
    dive into CUDA by making use of language constructs in C, C++, Fortran, Python,
    and more for the highest performance and flexibility. We will be covering all
    of these methods in detail in the subsequent chapters.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到我们最初的问题，即CUDA是什么？CUDA是由NVIDIA开发的并行计算平台和编程模型架构，它将GPU上的通用计算作为一流能力进行暴露。与任何其他处理器一样，GPU架构可以使用各种方法进行编码。提供快速加速的最简单方法是利用现有库。另外，开发人员可以选择使用OpenACC指令以获得快速加速结果和可移植性。另一种选择是选择通过使用C、C++、Fortran、Python等语言构造来深入研究CUDA，以获得最高的性能和灵活性。我们将在接下来的章节中详细介绍所有这些方法。
- en: 'The following screenshot represents the various ways we can perform GPU programming:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图表示了我们可以进行GPU编程的各种方式。
- en: '![](img/f38685df-fd3b-4df8-a3ab-bbe34b0ea63b.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f38685df-fd3b-4df8-a3ab-bbe34b0ea63b.png)'
- en: In this section, we provided you with a perspective of how processors and high-performance
    computing have evolved over time. We provided you with an overview of why the
    heterogeneous programming model is key to getting the best performance from an
    application, followed by approaches to GPU programming. In the next section, we
    will start writing a Hello World program on a GPU.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们为您提供了处理器和高性能计算随时间演变的视角。我们为您提供了异构编程模型对于从应用程序中获得最佳性能的关键性概述，以及GPU编程的方法。在下一节中，我们将开始在GPU上编写一个Hello
    World程序。
- en: Technical requirements
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux/Windows PC with a modern NVIDIA GPU (Pascal architecture onwards) is
    required for this chapter, along with all of the necessary GPU drivers and the
    CUDA Toolkit (10.0 onward) installed. If you're unsure of your GPU's architecture,
    please visit NVIDIA's GPU site ([https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus))
    and confirm your GPU's architecture. This chapter's code is also available on
    GitHub at [https://github.com/PacktPublishing/Learn-CUDA-Programming](https://github.com/PacktPublishing/Learn-CUDA-Programming).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要一台安装了现代NVIDIA GPU（Pascal架构及以上）的Linux/Windows PC，以及所有必要的GPU驱动程序和安装了CUDA Toolkit（10.0及以上版本）。如果您不确定您的GPU架构，请访问NVIDIA的GPU网站（[https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)）并确认您的GPU架构。本章的代码也可以在GitHub上找到：[https://github.com/PacktPublishing/Learn-CUDA-Programming](https://github.com/PacktPublishing/Learn-CUDA-Programming)。
- en: The code examples in this chapter have been developed and tested with version
    10.1 of CUDA Toolkit, but it is recommended to use the latest CUDA version, if
    possible.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码示例是使用CUDA Toolkit的10.1版本开发和测试的，但建议尽可能使用最新的CUDA版本。
- en: Hello World from CUDA
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来自CUDA的Hello World
- en: CUDA is a heterogeneous programming model that includes provisions for both
    CPU and GPU. The CUDA C/C++ programming interface consists of C language extensions
    so that you can target portions of source code for parallel execution on the device
    (GPU). It is based on industry-standard C/C++ and provides a library of C functions
    that can be executed on the host (CPU) so that it can interact with the device.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA是一个包括CPU和GPU在内的异构编程模型。CUDA C/C++编程接口由C语言扩展组成，以便您可以将源代码的部分目标定为在设备（GPU）上并行执行。它基于行业标准的C/C++，并提供了一系列C函数库，可以在主机（CPU）上执行，以便它可以与设备进行交互。
- en: In CUDA, there are two processors that work with each other. The host is usually
    referred to as the CPU, while the device is usually referred to as the GPU. The
    host is responsible for calling the device functions. As we've already mentioned,
    part of the code that runs on the GPU is called **device code**, while the serial
    code that runs on the CPU is called **host code**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，有两个相互配合的处理器。主机通常被称为CPU，而设备通常被称为GPU。主机负责调用设备函数。正如我们已经提到的，运行在GPU上的代码的一部分被称为**设备代码**，而在CPU上运行的串行代码被称为**主机代码**。
- en: Let's start by writing our first CUDA code in C. The intention is to take a
    systematic step-wise approach, start with some sequential code, and convert it
    into CUDA-aware code by adding some additional keywords. As we mentioned earlier,
    there is no necessity to learn a new language—all we need to do is add some keywords
    to the existing language so that we can run it in a heterogeneous environment
    with CPU and GPU.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从在C中编写我们的第一个CUDA代码开始。我们的意图是采取一个系统化的逐步方法，从一些顺序代码开始，通过添加一些额外的关键字将其转换为CUDA感知代码。正如我们之前提到的，没有必要学习一门新语言，我们只需要在现有语言中添加一些关键字，以便在CPU和GPU的异构环境中运行它。
- en: 'Let''s take a look at our first piece of code. All this code does is print
    Hello World! from both the host and device:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们的第一段代码。这段代码的作用只是从主机和设备上打印Hello World!
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s try to compile and run the preceding snippet:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试编译和运行前面的代码片段：
- en: '**Compile the code**: Place the preceding code into a file called `hello_world.cu`
    and compile it using the **NVIDIA C Compiler** (**nvcc**). Note that the extension
    of the file is `.cu`, which tells the compiler that this file has GPU code inside
    it:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编译代码**：将前面的代码放入一个名为`hello_world.cu`的文件中，并使用**NVIDIA C Compiler**（**nvcc**）进行编译。请注意，文件的扩展名是`.cu`，这告诉编译器这个文件里面有GPU代码：'
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Execute the GPU code**: We should receive the following output after executing
    the GPU code:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行GPU代码**：在执行GPU代码后，我们应该收到以下输出：'
- en: '![](img/2cfcd6a2-b158-4c73-8cc4-55e05d6df128.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cfcd6a2-b158-4c73-8cc4-55e05d6df128.png)'
- en: By now, you might have already observed that the CUDA C code isn't used very
    differently and only requires that we learn some additional constructs to tell
    the compiler which function is GPU code and how to call a GPU function. It isn't
    like we need to learn a new language altogether.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您可能已经注意到CUDA C代码的使用方式并没有太大不同，只需要学习一些额外的构造来告诉编译器哪个函数是GPU代码，以及如何调用GPU函数。这并不像我们需要完全学习一门新语言。
- en: 'In the preceding code, we added a few constructs and keywords, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们添加了一些构造和关键字，如下：
- en: '`__global__`: This keyword, when added before the function, tells the compiler
    that this is a function that will run on the device and not on the host. However,
    note that it is called by the host. Another important thing to note here is that
    the return type of the device function is always "void". Data-parallel portions
    of an algorithm are executed on the device as kernels.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__global__`：在函数之前添加此关键字，告诉编译器这是一个将在设备上而不是主机上运行的函数。但请注意，它是由主机调用的。这里另一个重要的事情是设备函数的返回类型始终是"void"。算法的数据并行部分在设备上作为内核执行。'
- en: '`<<<,>>>`: This keyword tells the compiler that this is a call to the device
    function and not the host function. Additionally, the `1,1` parameter basically
    dictates the number of threads to launch in the kernel. We will cover the parameters
    inside angle brackets later. For now, the `1,1` parameter basically means we are
    launching the kernel with only one thread, that is, sequential code with a thread
    since we are not doing anything important in the code apart from printing.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<<<,>>>`: 这个关键字告诉编译器这是对设备函数的调用，而不是对主机函数的调用。此外，`1,1`参数基本上决定了在内核中启动的线程数。我们将稍后介绍尖括号内的参数。目前，`1,1`参数基本上意味着我们只启动一个线程的内核，也就是说，除了打印之外，我们在代码中没有做任何重要的事情。'
- en: '`threadIdx.x`*,* `blockIdx.x`: This is a unique ID that''s given to all threads.
    We will cover this topic more in the next section.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threadIdx.x`*,* `blockIdx.x`: 这是给所有线程的唯一ID。我们将在下一节更详细地介绍这个主题。'
- en: '`cudaDeviceSynchronize()`: All of the kernel calls in CUDA are asynchronous
    in nature. The host becomes free after calling the kernel and starts executing
    the next instruction afterward. This should come as no big surprise since this
    is a heterogeneous environment and hence both the host and device can run in parallel
    to make use of the types of processors that are available. In case the host needs
    to wait for the device to finish, APIs have been provided as part of CUDA programming
    that make the host code wait for the device function to finish. One such API is
    `cudaDeviceSynchronize`, which waits until all of the previous calls to the device
    have finished.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cudaDeviceSynchronize()`: CUDA中的所有内核调用都是异步的。在调用内核后，主机变得空闲，并在之后开始执行下一条指令。这应该不足为奇，因为这是一个异构环境，因此主机和设备都可以并行运行，以利用可用的处理器类型。如果主机需要等待设备完成，CUDA编程提供了API使主机代码等待设备函数完成。其中一个API是`cudaDeviceSynchronize`，它会等待所有先前对设备的调用完成。'
- en: Try removing the `cudaDeviceSynchronize()` call and see whether the device output
    is visible or not. Alternatively, try putting this call before printing it on
    the host code.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试删除`cudaDeviceSynchronize()`调用，看看设备输出是否可见。或者，尝试在打印主机代码之前放置这个调用。
- en: Thread hierarchy
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程层次结构
- en: Now, let's start playing around with the two parameters, that is, `threadIdx.x`
    and `blockIdx.x`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始玩弄两个参数，即`threadIdx.x`和`blockIdx.x`。
- en: '**Experiment 1**: First, change the parameter from `<<<1,1>>>` to `<<<2,1>>`
    and view the output. The output of running multiple thread-single blocks of Hello
    World code should be as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验1**：首先，将参数从`<<<1,1>>>`更改为`<<<2,1>>>`并查看输出。运行多个线程-单个块的Hello World代码的输出应该如下：'
- en: '![](img/71a3108d-a11b-4cf8-8df6-2ef02f1c53c5.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71a3108d-a11b-4cf8-8df6-2ef02f1c53c5.png)'
- en: As we can see, instead of one thread, we now have two threads printing the value.
    Note that their unique IDs are different.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，现在我们不是一个线程，而是两个线程打印值。请注意，它们的唯一ID是不同的。
- en: '**Experiment 2**: Now, instead of changing the first parameter, let''s change
    the second, that is, change `<<<1,1>>>` to `<<<1,2>>>` and observe the output
    of running multiple single-thread blocks of Hello World code, as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验2**：现在，不要更改第一个参数，而是更改第二个参数，即将`<<<1,1>>>`更改为`<<<1,2>>>`，并观察运行多个单线程块的Hello
    World代码的输出，如下所示：'
- en: '![](img/451492da-208c-4cc7-98ff-9d943decd7f0.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/451492da-208c-4cc7-98ff-9d943decd7f0.png)'
- en: As you can see, the total number of threads that were launched into the kernel
    is two, just like before—the only difference is that their IDs are different. So,
    what are these thread and block concepts? To combat this, let's dive into the
    GPU architecture some more.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，被启动到内核中的线程总数是两个，就像以前一样——唯一的区别是它们的ID不同。那么，这些线程和块的概念是什么？为了解决这个问题，让我们更深入地了解GPU架构。
- en: GPU architecture
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU架构
- en: One of the key reasons why CUDA became so popular is because the hardware and
    software have been designed and tightly bound to get the best performance out
    of the application. Due to this, it becomes necessary to show the relationship
    between the software CUDA programming concepts and the hardware design itself.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA变得如此受欢迎的一个关键原因是因为硬件和软件被设计和紧密绑定，以获得应用程序的最佳性能。因此，有必要展示软件CUDA编程概念与硬件设计本身之间的关系。
- en: 'The following screenshot shows the two sides of CUDA:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了CUDA的两个方面：
- en: '![](img/dfdc4e4b-dce4-4f73-a781-8e21a84db518.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfdc4e4b-dce4-4f73-a781-8e21a84db518.png)'
- en: We can see that the CUDA software has been mapped to the GPU hardware.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，CUDA软件已经映射到了GPU硬件。
- en: 'The following table, in accordance with the preceding screenshot, explains
    software and hardware mapping in terms of the CUDA programming model:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的截图，以下表解释了CUDA编程模型的软件和硬件映射：
- en: '| **Software** | **Executes on/as** | **Hardware** |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **软件** | **执行于/作为** | **硬件** |'
- en: '| CUDA thread | CUDA Core/SIMD code |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| CUDA线程 | CUDA核心/SIMD代码 |'
- en: '| CUDA block | Streaming multiprocessor |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| CUDA块 | 流多处理器 |'
- en: '| GRID/kernel | GPU device |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 网格/内核 | GPU设备 |'
- en: 'Let''s take a look at the preceding table''s components in detail:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看一下前表的组件：
- en: '**CUDA Threads**: CUDA threads execute on a CUDA core. CUDA threads are different
    from CPU threads. CUDA threads are extremely lightweight and provide fast context
    switching. The reason for fast context switching is due to the availability of
    a large register size in a GPU and hardware-based scheduler. The thread context
    is present in registers compared to CPU, where the thread handle resides in a
    lower memory hierarchy such as a cache. Hence, when one thread is idle/waiting,
    another thread that is ready can start executing with almost no delay. Each CUDA
    thread must execute the same kernel and work independently on different data (SIMT).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CUDA线程**：CUDA线程在CUDA核心上执行。CUDA线程不同于CPU线程。CUDA线程非常轻量级，并提供快速的上下文切换。快速上下文切换的原因是由于GPU中有大量的寄存器和硬件调度器。线程上下文存在于寄存器中，而不是像CPU中那样在较低的内存层次结构中，比如缓存中。因此，当一个线程处于空闲/等待状态时，另一个准备好的线程几乎可以立即开始执行。每个CUDA线程必须执行相同的内核，并且独立地处理不同的数据（SIMT）。'
- en: '**CUDA blocks**: CUDA threads are grouped together into a logical entity called
    a CUDA block. CUDA blocks execute on a single **Streaming Multiprocessor** (**SM**).
    One block runs on a single SM, that is, all of the threads within one block can
    only execute on cores in one SM and do not execute on the cores of other SMs.
    Each GPU may have one or more SM and hence to effectively make use of the whole
    GPU; the user needs to divide the parallel computation into blocks and threads.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CUDA块**：CUDA线程被组合成一个称为CUDA块的逻辑实体。CUDA块在单个**流多处理器**（**SM**）上执行。一个块在一个SM上运行，也就是说，一个块内的所有线程只能在一个SM的核心上执行，不会在其他SM的核心上执行。每个GPU可能有一个或多个SM，因此为了有效地利用整个GPU，用户需要将并行计算划分为块和线程。'
- en: '**GRID/kernel**: CUDA blocks are grouped together into a logical entity called
    a CUDA GRID. A CUDA GRID is then executed on the device.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GRID/核心**：CUDA块被组合成一个称为CUDA GRID的逻辑实体。然后在设备上执行CUDA GRID。'
- en: This may sound somewhat complicated at first glance. In this next section, we'll
    take a look at an example of vector addition to explain this. Hopefully, things
    will become much clearer.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这可能听起来有些复杂。在接下来的部分，我们将以向量加法的示例来解释这个问题。希望事情会变得更清晰。
- en: Vector addition using CUDA
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA进行向量加法
- en: 'The problem that we are trying to solve is vector addition. As we are aware, **vector**
    addition is a data parallel operation. Our dataset consists of three arrays: *A*,
    *B*, and *C*. The same operation is performed on each element:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要解决的问题是向量加法。正如我们所知，**向量**加法是一种数据并行操作。我们的数据集包括三个数组：*A*、*B*和*C*。每个元素执行相同的操作：
- en: '*Cx = Ax + Bx*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*Cx = Ax + Bx*'
- en: 'Each addition is independent of each other, but the same operation is applied
    by all CUDA threads. To get started, configure your environment according to the
    following steps:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 每个加法是相互独立的，但所有CUDA线程都执行相同的操作。要开始，根据以下步骤配置您的环境：
- en: Prepare your GPU application. This code will be placed in `01_cuda_introduction/01_vector_addition`.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备你的GPU应用程序。这段代码将放在`01_cuda_introduction/01_vector_addition`中。
- en: 'Compile your application with the `nvcc` compiler with the following command:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nvcc`编译器编译您的应用程序，命令如下：
- en: '[PRE2]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code is sequential code. We will convert this code so that it
    can run on a GPU using a step-by-step approach, as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码是顺序代码。我们将按照以下步骤逐步将此代码转换为可以在GPU上运行的代码：
- en: '[PRE3]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Before converting the sequential code, let''s take a look at the fundamental
    changes or steps that are taken between the CUDA and sequential code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换顺序代码之前，让我们看一下CUDA代码和顺序代码之间所采取的基本变化或步骤：
- en: '| **Sequential code** | **CUDA code** |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **顺序代码** | **CUDA代码** |'
- en: '| Step 1 | Allocate memory on the CPU, that is, `malloc new`. | Step 1 | Allocate
    memory on the CPU, that is, `malloc new`. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 步骤1 | 在CPU上分配内存，即`malloc new`。 | 步骤1 | 在CPU上分配内存，即`malloc new`。 |'
- en: '| Step 2 | Populate/initialize the CPU data. | Step 2 | Allocate memory on
    the GPU, that is, `cudaMalloc`. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 步骤2 | 填充/初始化CPU数据。 | 步骤2 | 在GPU上分配内存，即`cudaMalloc`。 |'
- en: '| Step 3 | Call the CPU function that has the crunching of data. The actual
    algorithm is vector addition in this case. | Step 3 | Populate/initialize the
    CPU data. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 步骤3 | 调用处理数据的CPU函数。在这种情况下，实际算法是向量加法。 | 步骤3 | 填充/初始化CPU数据。 |'
- en: '| Step 4 | Consume the crunched data, which is printed in this case. | Step
    4 | Transfer the data from the host to the device with `cudaMemcpy`. |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 步骤4 | 处理数据，这里是打印出来的。 | 步骤4 | 用`cudaMemcpy`将数据从主机传输到设备。 |'
- en: '| Step 5 | Call the GPU function with `<<<,>>>` brackets. |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 步骤5 | 用`<<<,>>>`括号调用GPU函数。 |'
- en: '| Step 6 | Synchronize the device and host with `cudaDeviceSynchronize`. |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 步骤6 | 用`cudaDeviceSynchronize`同步设备和主机。 |'
- en: '| Step 7 | Transfer data from the device to the host with `cudaMemcpy`. |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 步骤7 | 用`cudaMemcpy`将数据从设备传输到主机。 |'
- en: '| Step 8 | Consume the crunched data, which is printed in this case. |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 步骤8 | 处理数据，这里是打印出来的。 |'
- en: This book is not a replacement for the CUDA API guide and does not cover all
    CUDA APIs. For extensive use of the API, please refer to the CUDA API guide.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不是CUDA API指南的替代品，也不涵盖所有CUDA API。如需广泛使用API，请参考CUDA API指南。
- en: 'As we can see, the CUDA processing flow has some additional steps that need
    to be added to the sequential code. These are as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，CUDA处理流程有一些额外的步骤需要添加到顺序代码中。具体如下：
- en: '**Memory allocation on GPU:** CPU memory and GPU memory are physically separate
    memory. `malloc` allocates memory on the CPU''s RAM. The GPU kernel/device function
    can only access memory that''s allocated/pointing to the device memory. To allocate
    memory on the GPU, we need to use the `cudaMalloc` API. Unlike the `malloc` command, `cudaMalloc` does
    not return a pointer to allocated memory; instead, it takes a pointer reference
    as a parameter and updates the same with the allocated memory.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在GPU上分配内存：**CPU内存和GPU内存是物理上分开的内存。`malloc`在CPU的RAM上分配内存。GPU核心/设备函数只能访问已分配/指向设备内存的内存。要在GPU上分配内存，我们需要使用`cudaMalloc`
    API。与`malloc`命令不同，`cudaMalloc`不会返回指向已分配内存的指针；相反，它以指针引用作为参数，并更新相同的已分配内存。'
- en: '**Transfer data from host memory to device memory:** The host data is then
    copied to the device''s memory, which was allocated using the `cudaMalloc` command
    used in the previous step. The API that''s used to copy the data between the host
    and device and vice versa is `cudaMemcpy`. Like other `memcopy` commands, this
    API requires the destination pointer, source pointer, and size. One additional
    parameter it takes is the direction of copy, that is, whether we are copying from
    the host to the device or from the device to the host. In the latest version of
    CUDA, this is optional since the driver is capable of understanding whether the
    pointer points to the host memory or device memory. Note that there is an asynchronous
    alternative to `cudaMemcpy`. This will be covered in more detail in other chapters.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将数据从主机内存传输到设备内存：**主机数据然后被复制到使用前一步中使用的`cudaMalloc`命令分配的设备内存。用于在主机和设备之间复制数据的API是`cudaMemcpy`。与其他`memcopy`命令一样，此API需要目标指针、源指针和大小。它额外需要一个参数，即复制的方向，也就是说，我们是从主机到设备复制，还是从设备到主机复制。在CUDA的最新版本中，这是可选的，因为驱动程序能够理解指针是指向主机内存还是设备内存。请注意，`cudaMemcpy`有一个异步的替代方案。这将在其他章节中更详细地介绍。'
- en: '**Call and execute a CUDA function:** As shown in the Hello World CUDA program,
    we call a kernel by using `<<<,>>>` brackets, which provide parameters for the
    block and thread size, respectively. We will cover this in more detail after all
    of the steps are complete.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调用和执行CUDA函数：**如同在Hello World CUDA程序中所示，我们通过`<<<,>>>`括号调用一个核函数，它提供了块和线程大小的参数。在完成所有步骤后，我们将更详细地介绍这一点。'
- en: '**Synchronize:** As we mentioned in the Hello World program, kernel calls are
    asynchronous in nature. In order for the host to make sure that kernel execution
    has finished, the host calls the `cudaDeviceSynchronize` function. This makes
    sure that all of the previously launched device calls have finished.'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**同步：**正如我们在Hello World程序中提到的，核函数调用是异步的。为了确保主机确保核执行已完成，主机调用`cudaDeviceSynchronize`函数。这确保了之前启动的所有设备调用都已完成。'
- en: '**Transfer data from host memory to device memory:** Use the same `cudaMemcpy`
    API to copy the data back from the device to the host for post-processing or validation
    duties such as printing. The only change here, compared to the first step, is
    that we reverse the direction of the copy, that is, the destination pointer points
    to the host while the source pointer points to the device allocated in memory.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将数据从主机内存传输到设备内存：**使用相同的`cudaMemcpy` API将数据从设备复制回主机，用于后处理或验证任务，如打印。与第一步相比，唯一的变化是我们颠倒了复制的方向，也就是说，目标指针指向主机，而源指针指向在内存中分配的设备。'
- en: '**Free the allocated GPU memory:** Finally, free the allocated GPU memory using the `cudaFree` API.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**释放分配的GPU内存：**最后，使用`cudaFree` API释放分配的GPU内存。'
- en: 'Change the sequential vector addition code''s `main` function to reflect these
    new steps. The `main` function will look like this:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 更改顺序向量加法代码的`main`函数以反映这些新步骤。`main`函数将如下所示：
- en: '[PRE4]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, let's look at how kernel code is written and manage the thread and block
    sizes. For this, we will be conducting multiple experiments.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下如何编写核心代码并管理线程和块大小。为此，我们将进行多个实验。
- en: Experiment 1 – creating multiple blocks
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验1 - 创建多个块
- en: 'In this section, we will make use of CUDA blocks to run the vector addition
    code in parallel on the GPU. Additional keywords will be exposed that are related
    to how we can index CUDA blocks. Change the call to the `device_add` function,
    as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将利用CUDA块在GPU上并行运行向量加法代码。将会暴露与我们如何索引CUDA块相关的附加关键字。更改对`device_add`函数的调用如下：
- en: '[PRE5]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will execute the `device_add` function `N` times in parallel instead of
    once. Each parallel invocation of the `device_add` function is referred to as
    a block. Now, let''s add a `__global__` device function, as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使`device_add`函数并行执行`N`次，而不是一次。`device_add`函数的每个并行调用称为一个块。现在，让我们添加一个`__global__`设备函数，如下所示：
- en: '[PRE6]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'By using `blockIdx.x` to index the array, each block handles a different element
    of the array. On the device, each block can execute in parallel. Let''s take a
    look at the following screenshot:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`blockIdx.x`来索引数组，每个块处理数组的不同元素。在设备上，每个块可以并行执行。让我们看一下以下的屏幕截图：
- en: '![](img/ad588e52-b0fc-4c3a-9ab8-d3035f49ccdc.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad588e52-b0fc-4c3a-9ab8-d3035f49ccdc.png)'
- en: The preceding screenshot represents the vector addition GPU code in which every
    block shows indexing for multiple single-thread blocks.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的屏幕截图代表了向量加法GPU代码，其中每个块显示了多个单线程块的索引。
- en: Experiment 2 – creating multiple threads
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验2 - 创建多个线程
- en: In this section, we will make use of CUDA threads to run the vector addition
    code in parallel on GPU. Additional keywords will be exposed that are related
    to how we can index CUDA threads.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将利用CUDA线程在GPU上并行运行向量加法代码。将会暴露与我们如何索引CUDA线程相关的附加关键字。
- en: 'A block can be split into multiple threads. Change the call to the `device_add`
    function, as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一个块可以分成多个线程。更改对`device_add`函数的调用如下：
- en: '[PRE7]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will execute the `device_add` function `N` times in parallel instead of
    once. Each parallel invocation of the `device_add` function is referred to as
    a thread. Change the device routine to reflect the kernel, as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这将并行执行`device_add`函数`N`次，而不是一次。`device_add`函数的每个并行调用被称为一个线程。更改设备例程以反映内核，如下所示：
- en: '[PRE8]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'One notable difference is that, instead of `blockIdx.x`, we make use of `threadIdx.x`,
    as shown in the following screenshot:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显著的区别是，我们使用`threadIdx.x`而不是`blockIdx.x`，如下面的截图所示：
- en: '![](img/5534f36e-92c5-434e-a5c6-a4a5d2ac36d9.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5534f36e-92c5-434e-a5c6-a4a5d2ac36d9.png)'
- en: The preceding screenshot represents vector addition GPU code in which every
    block shows indexing for a single block-multiple threads.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图代表了向量加法GPU代码，其中每个块显示了单个块-多个线程的索引。
- en: Experiment 3 – combining blocks and threads
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验3 - 结合块和线程
- en: So far, we've looked at parallel vector addition through the use of several
    blocks with one thread in the *Experiment 1 – creating multiple blocks* section
    and one block with several threads in the *Experiment 2 – creating multiple threads* section.
    In this experiment, we'll use multiple blocks as well as separate blocks containing
    multiple threads. This becomes more challenging in terms of how to find the index
    because we need to combine both `threadIdx` and `blockIdx` to generate a unique
    ID.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经通过在*实验1 - 创建多个块*部分使用多个块和一个线程，以及在*实验2 - 创建多个线程*部分使用一个块和多个线程来查看并行向量加法。在这个实验中，我们将使用多个块以及包含多个线程的单独块。在如何找到索引方面，这变得更具挑战性，因为我们需要结合`threadIdx`和`blockIdx`来生成一个唯一的ID。
- en: 'Let''s take a look at two scenarios that depict different combinations that
    the developer can choose from:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下两种不同组合的场景，开发人员可以从中选择：
- en: '**Scenario 1:** Let''s consider that the total number of vector elements is
    32\. Each block contains eight threads and a total of four blocks.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**场景1：** 假设向量元素的总数是32。每个块包含八个线程，总共有四个块。'
- en: '**Scenario 2:** Let''s consider that the total number of vector elements is
    32\. Each block contains four threads and a total of eight blocks.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**场景2：** 假设向量元素的总数是32。每个块包含四个线程，总共有八个块。'
- en: In both scenarios, the number of parallel executions is 32, where all 32 elements
    get populated in parallel. The developer makes the choice between the threads
    within a block and the number of blocks based on the problem's size and restriction
    by each piece of hardware. We will be covering details about the right choice
    of sizing based on the architecture in another chapter.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，并行执行的数量都是32，所有32个元素都会并行填充。开发人员根据问题的大小和每个硬件的限制选择块内的线程和块的数量。我们将在另一章节中详细介绍基于架构的正确尺寸选择的细节。
- en: 'The following screenshot shows the vector addition GPU indexing code for different
    block and thread configurations:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了不同块和线程配置的向量加法GPU索引代码：
- en: '![](img/a4c34dbe-c938-4624-83d5-a3ed095c03f6.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4c34dbe-c938-4624-83d5-a3ed095c03f6.jpg)'
- en: 'Now, let''s look at how the kernel code can be changed to combine both threads
    and blocks to calculate a global index:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何改变内核代码以结合线程和块来计算全局索引：
- en: '[PRE9]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'While calling the kernel from the `main()` function, the developer chooses
    the block and thread configuration, as depicted in the following code, for the
    two scenarios we mentioned previously:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在从`main()`函数中调用内核时，开发人员选择了块和线程的配置，如前面提到的两种情况所示的代码：
- en: '**Scenario 1:** Following is the code for vector addition GPU grid and block
    size calculation for eight threads per block:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**场景1：** 以下是用于计算每个块八个线程的向量加法GPU网格和块大小的代码：'
- en: '[PRE10]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Scenario 2:** Following is the code for vector addition GPU grid and block
    size calculation for four threads per block:'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**场景2：** 以下是用于计算每个块四个线程的向量加法GPU网格和块大小的代码：'
- en: '[PRE11]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'With a combination of threads and blocks, the unique ID of a thread can be
    calculated. As shown in the preceding code, another variable is given to all threads.
    This is called `blockDim`. This variable consists of the block''s dimensions,
    that is, the number of threads per block. Let''s take a look at the following
    screenshot:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 通过线程和块的组合，可以计算出线程的唯一ID。如前面的代码所示，所有线程都被赋予另一个变量。这被称为`blockDim`。这个变量包含了块的维度，也就是每个块的线程数。让我们看一下下面的截图：
- en: '![](img/ffe81d23-3db4-4439-847b-4733a7386a23.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ffe81d23-3db4-4439-847b-4733a7386a23.png)'
- en: Here, we can see a vector addition GPU indexing calculation for scenario 1.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到场景1的向量加法GPU索引计算。
- en: Why bother with threads and blocks?
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要费心处理线程和块？
- en: It might not be obvious why we need this additional hierarchy of threads and
    blocks. They add a level of complexity where the developer needs to find out the
    right block and grid size. Also, global indexing becomes a challenge. The reason
    for this is because of the restrictions that the CUDA programming model put it
    place.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 可能不明显为什么我们需要这种额外的线程和块的层次结构。它增加了开发人员需要找到正确的块和网格大小的复杂性。全局索引也变得具有挑战性。这是因为CUDA编程模型设置了限制。
- en: Unlike parallel blocks, threads have mechanisms to communicate and synchronize
    efficiently. Real-world applications require threads to communicate with each
    other and may want to wait for certain data to be interchanged before proceeding
    further. This kind of operation requires threads to communicate, and the CUDA
    programming model allows this communication for threads within the same block.
    Threads belonging to different blocks cannot communicate/synchronize with each
    other during the execution of the kernel. This restriction allows the scheduler
    to schedule the blocks on the SM independently of each other. The result of this
    is that, if new hardware is released with more SMs and if the code has enough
    parallelism, the code can be scaled linearly. In other words, this allows the
    hardware to scale the number of blocks running in parallel based on the GPU's
    capability.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与并行块不同，线程有有效的通信和同步机制。现实世界的应用程序需要线程之间进行通信，并且可能希望在继续之前等待某些数据进行交换。这种操作需要线程进行通信，CUDA编程模型允许同一块内的线程进行通信。属于不同块的线程在内核执行期间无法进行通信/同步。这种限制允许调度程序独立地在SM上调度块。其结果是，如果发布了具有更多SM的新硬件，并且代码具有足够的并行性，则代码可以线性扩展。换句话说，这允许硬件根据GPU的能力并行运行块的数量。
- en: 'The threads communicate with each other using a special memory known as shared
    memory. We will cover shared memory extensively in [Chapter 2](95b5fe3b-0f6a-4c4f-bc26-31156ce536e3.xhtml), *CUDA
    Memory Management*, where we will expose other memory hierarchies in the GPU and
    their optimal usage. The following screenshot demonstrates scaling blocks across
    different GPUs consisting of different amounts of SMs:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 线程之间使用一种称为共享内存的特殊内存进行通信。我们将在[第2章](95b5fe3b-0f6a-4c4f-bc26-31156ce536e3.xhtml)中广泛介绍共享内存，即*CUDA内存管理*，在那里我们将介绍GPU中的其他内存层次结构及其最佳使用方法。以下屏幕截图演示了在不同GPU上扩展块，这些GPU包含不同数量的SM：
- en: '![](img/8aefa6e3-b6e7-433c-8115-040a7f7a55d6.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8aefa6e3-b6e7-433c-8115-040a7f7a55d6.png)'
- en: Now, let's find out more about launching kernels in multiple dimensions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更多地了解在多个维度中启动内核。
- en: Launching kernels in multiple dimensions
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在多个维度中启动内核
- en: So far, we have been launching threads and blocks in one dimension. This means
    we have only using indexes for one dimension; for example, we've been using `threadIdx.x`,
    where `x` represents that we are using only an `x` dimension thread index. Similarly,
    we've been using `blockIdx.x`, where `x` represents that we are using only an `x`
    dimension block index. We can launch threads and blocks in one, two, or three
    dimensions. One example of launching threads and blocks in two dimensions is when
    we use parallel operations on an image, for example, to blur the image using a
    filter. The developer has the choice of launching threads and blocks in two dimensions,
    which is a more natural choice given that images are two-dimensional in nature.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在一维中启动线程和块。这意味着我们只使用一个维度的索引；例如，我们一直在使用`threadIdx.x`，其中`x`表示我们只使用一个`x`维度的线程索引。同样，我们一直在使用`blockIdx.x`，其中`x`表示我们只使用一个`x`维度的块索引。我们可以在一、二或三个维度中启动线程和块。在二维中启动线程和块的一个例子是当我们在图像上进行并行操作，例如使用滤波器模糊图像。开发人员可以选择在二维中启动线程和块，这是一个更自然的选择，因为图像在本质上是二维的。
- en: It is important to understand that every GPU architecture also puts a restriction
    on the dimensions of threads and blocks. For example, the NVIDIA Pascal card allows
    a maximum of 1,024 threads per thread block in the `x` and `y` dimensions, while
    in the `z` dimension, you can only launch 64 threads. Similarly, the maximum blocks
    in a grid are restricted to 65,535 in the `y` and `z` dimensions in the Pascal
    architecture and `2^31 -1` in the `x` dimension. If the developer launches a kernel
    with an unsupported dimension, the application throws a runtime error.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要了解每个GPU架构也对线程和块的维度施加了限制。例如，NVIDIA Pascal卡允许在`x`和`y`维度中每个线程块最多有1,024个线程，而在`z`维度中，您只能启动64个线程。同样，在Pascal架构中，网格中的最大块数限制为`y`和`z`维度中的65,535个，`x`维度中为`2^31
    -1`。如果开发人员使用不受支持的维度启动内核，应用程序会抛出运行时错误。
- en: So far, we have been assuming that the code we have written is error-free. But
    in the real world, every programmer writes code that has bugs in it, and it is
    necessary to catch those errors. In this next section, we'll take a look at how
    error reporting in CUDA works.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直假设我们编写的代码是没有错误的。但在现实世界中，每个程序员都会写有错误的代码，必须捕捉这些错误。在下一节中，我们将看看CUDA中的错误报告是如何工作的。
- en: Error reporting in CUDA
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA中的错误报告
- en: 'In CUDA, the host code manages errors. Most CUDA functions call `cudaError_t`,
    which is basically an enumeration type. `cudaSuccess` (value 0) indicates a `0`
    error. The user can also make use of the `cudaGetErrorString()` function, which
    returns a string describing the error condition, as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，主机代码管理错误。大多数CUDA函数调用`cudaError_t`，它基本上是一个枚举类型。`cudaSuccess`（值0）表示`0`错误。用户还可以使用`cudaGetErrorString()`函数，该函数返回描述错误条件的字符串。
- en: '[PRE12]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Kernel launches have no return value. We can make use of a function such as `cudaGetLastError()` here,
    which returns the error code for the last CUDA function (including kernel launches).
    In the case of multiple errors, only the last one is reported:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 内核启动没有返回值。我们可以在这里使用`cudaGetLastError()`这样的函数，它返回最后一个CUDA函数（包括内核启动）的错误代码。在多个错误的情况下，只报告最后一个错误：
- en: '[PRE13]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When it comes to production code, it is advised to make use of error checking
    code at logical checkpoints as the CPU code will continue with normal execution
    even if the GPU kernel has crashed, resulting in incorrect results.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产代码中，建议在逻辑检查点处使用错误检查代码，因为即使GPU内核崩溃，CPU代码也会继续正常执行，导致结果不正确。
- en: In the next section, we will introduce you to the data types that are supported
    in the CUDA programming model.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将向您介绍CUDA编程模型中支持的数据类型。
- en: Data type support in CUDA
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA中的数据类型支持
- en: Like any processor architecture, a GPU also has different types of memories,
    each meant for a different purpose. We will cover them in more detail in [Chapter
    2](95b5fe3b-0f6a-4c4f-bc26-31156ce536e3.xhtml), *CUDA Memory Management*. However,
    it is important to understand the different data types that are supported and
    their implications on performance and accuracy. CUDA programming supports all
    of the standard data types that developers are familiar with in terms of their
    respective languages. Along with standard data types with different sizes (`char` is
    1 byte, `float` is 4 bytes, `double` is 8 bytes, and so on), it also supports
    vector types such as `float2` and `float4`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何处理器架构一样，GPU也有不同类型的内存，每种用于不同的目的。我们将在[第2章](95b5fe3b-0f6a-4c4f-bc26-31156ce536e3.xhtml)中更详细地介绍它们，*CUDA内存管理*。然而，重要的是要理解支持的不同数据类型及其对性能和精度的影响。CUDA编程支持开发人员在其各自语言中熟悉的所有标准数据类型。除了不同大小的标准数据类型（`char`为1字节，`float`为4字节，`double`为8字节等）之外，它还支持矢量类型，如`float2`和`float4`。
- en: It is recommended that the data types are naturally aligned since aligned data
    access for data types that are 1, 2, 4, 8, or 16 bytes in size ensure that the
    GPU calls a single memory instruction. If they are not aligned, the compiler generates
    multiple instructions, which are interleaved, resulting in inefficient utilization
    of the memory and instruction bus. Due to this, the recommendation is to use types
    that are naturally aligned for data residing in GPU memory. The alignment requirement
    is automatically fulfilled for the built-in types of `char`, `short`, `int`, `long`,
    `long long`, `float`, and `double` such as `float2` and `float4`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 建议数据类型自然对齐，因为对于大小为1、2、4、8或16字节的数据类型的对齐数据访问，可以确保GPU调用单个内存指令。如果它们没有对齐，编译器将生成多个交错的指令，导致内存和指令总线的低效利用。因此，建议在GPU内存中使用自然对齐的类型。对于`char`、`short`、`int`、`long`、`long
    long`、`float`和`double`等内置类型，如`float2`和`float4`，对齐要求会自动满足。
- en: 'Also, CUDA programming supports complex data structures such as structures
    and classes (in the context of C and C++). For complex data structures, the developer
    can make use of alignment specifiers to the compiler to enforce the alignment
    requirements, as shown in the following code:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，CUDA编程还支持复杂的数据结构，如结构和类（在C和C++的上下文中）。对于复杂的数据结构，开发人员可以利用对齐说明符来强制编译器满足对齐要求，如下面的代码所示：
- en: '[PRE14]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Every GPU has a limited set of cores and so the FLOPS are different. For example,
    a Tesla V100 card with the Volta architecture has 2,560 FP64 cores (double precision),
    while it has double the number of 32-bit single precision cores. It is quite evident
    that using the right data types based on the precision requirements of the algorithm
    is essential. Mixed precision algorithms are now being developed to make use of
    different types of cores where some part of the algorithm runs with higher precision
    while some parts run with lower precision. We will cover more on this topic in
    the upcoming chapters as well. For now, it is important to understand that the
    GPU memory hierarchy is different and, hence, using the right data type matters.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 每个GPU都有一组有限的核心，因此FLOPS是不同的。例如，具有Volta架构的Tesla V100卡具有2560个FP64核心（双精度），而具有两倍数量的32位单精度核心。很明显，根据算法的精度要求使用正确的数据类型是至关重要的。现在正在开发混合精度算法，以利用不同类型的核心，其中算法的某些部分以更高精度运行，而某些部分以较低精度运行。我们将在即将到来的章节中更多地涵盖这个主题。目前，重要的是要理解GPU内存层次结构是不同的，因此使用正确的数据类型很重要。
- en: While this was a general introduction to the data types that are supported in
    GPU, more details about all of the supported data types can be found at [https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-vector-types](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-vector-types).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是对GPU支持的数据类型的一般介绍，但有关所有支持的数据类型的更多详细信息可以在[https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-vector-types](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-vector-types)找到。
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we provided you with a perspective on heterogeneous computing
    with the help of history and high-performance computing. We went into detail about
    how the two processors, that is, CPU and GPU, are different. We also wrote a Hello
    World and vector addition CUDA program on a GPU. Finally, we looked at how to
    detect errors in CUDA since the majority of calls that are made to a CUDA API
    are asynchronous in nature.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过历史和高性能计算为您提供了异构计算的视角。我们详细介绍了两个处理器，即CPU和GPU的不同之处。我们还在GPU上编写了一个Hello
    World和矢量加法CUDA程序。最后，我们看了如何检测CUDA中的错误，因为对CUDA API的大多数调用都是异步的。
- en: In the next chapter, we will look at the different types of GPU memory that
    are available and how to utilize them optimally.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看一下不同类型的GPU内存以及如何最优地利用它们。
