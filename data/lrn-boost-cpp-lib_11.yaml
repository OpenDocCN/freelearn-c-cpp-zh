- en: Chapter 11. Network Programming Using Boost Asio
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。使用Boost Asio进行网络编程
- en: In today's networked world, Internet servers handling thousands of requests
    per second have a tough mandate to fulfill—of maintaining responsiveness and not
    slowing down even with increasing volumes of requests. Building reliable processes
    that efficiently handle network I/O and scale with the number of connections is
    challenging because it often requires the application programmer to understand
    the underlying protocol stack and exploit it in ingenious ways. What adds to the
    challenge is the variance in the programming interfaces and models for network
    programming across platforms, and the inherent difficulties of using low-level
    APIs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的网络世界中，处理每秒数千个请求的互联网服务器有一个艰巨的任务要完成——保持响应性，并且即使请求量增加也不会减慢。构建可靠的进程，有效地处理网络I/O并随着连接数量的增加而扩展，是具有挑战性的，因为它通常需要应用程序员理解底层协议栈并以巧妙的方式利用它。增加挑战的是跨平台的网络编程接口和模型的差异，以及使用低级API的固有困难。
- en: 'Boost Asio (pronounced ay-see-oh) is a portable library for performing efficient
    network I/O using a consistent programming model. The emphasis is on performing
    asynchronous I/O (hence the name Asio), where the program initiates I/O operations
    and gets on with its other jobs, without blocking for the OS to return with the
    results of the operation. When the operation is complete in the underlying OS,
    the program is notified by the Asio library and takes an appropriate action. The
    problems Asio helps solve and the consistent, portable interfaces it uses to do
    so, make Asio compellingly useful. But the asynchronous nature of interactions
    also makes it more complex and less straightforward to reason about. This is the
    reason we will study Asio in two parts: to first understand its interaction model
    and then use it to perform network I/O:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Boost Asio（发音为ay-see-oh）是一个可移植的库，用于使用一致的编程模型执行高效的网络I/O。重点是执行异步I/O（因此称为Asio），其中程序启动I/O操作并继续执行其他任务，而不会阻塞等待操作系统返回操作结果。当底层操作系统完成操作时，Asio库会通知程序并采取适当的操作。Asio帮助解决的问题以及它使用的一致、可移植接口使其非常有用。但是，交互的异步性质也使其更加复杂和不那么直观。这就是为什么我们将分两部分学习Asio的原因：首先理解其交互模型，然后使用它执行网络I/O：
- en: Task execution with Asio
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Asio进行任务执行
- en: Network programming using Asio
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Asio进行网络编程
- en: Asio provides a toolkit for performing and managing arbitrary tasks, and the
    focus of the first part of this chapter is to understand this toolkit. We apply
    this understanding in the second part of this chapter, when we look specifically
    at how Asio helps write programs that communicate with other programs over the
    network, using protocols from the Internet Protocol (IP) suite.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Asio提供了一个工具包，用于执行和管理任意任务，本章的第一部分重点是理解这个工具包。我们在本章的第二部分应用这种理解，当我们具体看一下Asio如何帮助编写使用互联网协议（IP）套件的程序与其他程序进行网络通信时。
- en: Task execution with Asio
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Asio进行任务执行
- en: At its core, Boost Asio provides a task execution framework that you can use
    to perform operations of any kind. You create your tasks as function objects and
    post them to a task queue maintained by Boost Asio. You enlist one or more threads
    to pick these tasks (function objects) and invoke them. The threads keep picking
    up tasks, one after the other till the task queues are empty at which point the
    threads do not block but exit.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，Boost Asio提供了一个任务执行框架，您可以使用它来执行任何类型的操作。您将您的任务创建为函数对象，并将它们发布到Boost Asio维护的任务队列中。您可以注册一个或多个线程来选择这些任务（函数对象）并调用它们。线程不断地选择任务，直到任务队列为空，此时线程不会阻塞，而是退出。
- en: IO Service, queues, and handlers
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IO服务、队列和处理程序
- en: At the heart of Asio is the type `boost::asio::io_service`. A program uses the
    `io_service` interface to perform network I/O and manage tasks. Any program that
    wants to use the Asio library creates at least one instance of `io_service` and
    sometimes more than one. In this section, we will explore the task management
    capabilities of `io_service`, and defer the discussion of network I/O to the latter
    half of the chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Asio的核心是类型`boost::asio::io_service`。程序使用`io_service`接口执行网络I/O和管理任务。任何想要使用Asio库的程序都会创建至少一个`io_service`实例，有时甚至会创建多个。在本节中，我们将探索`io_service`的任务管理能力，并将网络I/O的讨论推迟到本章的后半部分。
- en: 'Here is the IO Service in action using the obligatory "hello world" example:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是IO服务在使用强制性的“hello world”示例：
- en: '**Listing 11.1: Asio Hello World**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.1：Asio Hello World**'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We include the convenience header `boost/asio.hpp`, which includes most of the
    Asio library that we need for the examples in this chapter (line 1). All parts
    of the Asio library are under the namespace `boost::asio`, so we use a shorter
    alias for this (line 3). The program itself just prints `Hello, world!` on the
    console but it does so through a task.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们包括方便的头文件`boost/asio.hpp`，其中包括本章示例中需要的大部分Asio库（第1行）。Asio库的所有部分都在命名空间`boost::asio`下，因此我们为此使用一个更短的别名（第3行）。程序本身只是在控制台上打印`Hello,
    world!`，但是通过一个任务来实现。
- en: The program first creates an instance of `io_service` (line 6) and *posts* a
    function object to it, using the `post` member function of `io_service`. The function
    object, in this case defined using a lambda expression, is referred to as a **handler**.
    The call to `post` adds the handler to a queue inside `io_service`; some thread
    (including that which posted the handler) must *dispatch* them, that is, remove
    them off the queue and call them. The call to the `run` member function of `io_service`
    (line 14) does precisely this. It loops through the handlers in the queue inside
    `io_service`, removing and calling each handler. In fact, we can post more handlers
    to the `io_service` before calling `run`, and it would call all the posted handlers.
    If we did not call `run`, none of the handlers would be dispatched. The `run`
    function blocks until all the handlers in the queue have been dispatched and returns
    only when the queue is empty. By itself, a handler may be thought of as an independent,
    packaged task, and Boost Asio provides a great mechanism for dispatching arbitrary
    tasks as handlers. Note that handlers must be nullary function objects, that is,
    they should take no arguments.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 程序首先创建了一个`io_service`的实例（第6行），并使用`io_service`的`post`成员函数将一个函数对象*发布*到其中。在这种情况下，使用lambda表达式定义的函数对象被称为**处理程序**。对`post`的调用将处理程序添加到`io_service`内部的队列中；一些线程（包括发布处理程序的线程）必须*分派*它们，即，将它们从队列中移除并调用它们。对`io_service`的`run`成员函数的调用（第14行）正是这样做的。它循环遍历`io_service`内部的处理程序，移除并调用每个处理程序。实际上，我们可以在调用`run`之前向`io_service`发布更多的处理程序，并且它会调用所有发布的处理程序。如果我们没有调用`run`，则不会分派任何处理程序。`run`函数会阻塞，直到队列中的所有处理程序都被分派，并且只有在队列为空时才会返回。单独地，处理程序可以被视为独立的、打包的任务，并且Boost
    Asio提供了一个很好的机制来分派任意任务作为处理程序。请注意，处理程序必须是无参数的函数对象，也就是说，它们不应该带有参数。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Asio is a header-only library by default, but programs using Asio need to link
    at least with `boost_system`. On Linux, we can use the following command line
    to build this example:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Asio是一个仅包含头文件的库，但使用Asio的程序需要至少链接`boost_system`。在Linux上，我们可以使用以下命令行构建这个示例：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Most examples in this chapter would require you to link to additional libraries.
    You can use the following command line to build all the examples in this chapter:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的大多数示例都需要您链接到其他库。您可以使用以下命令行构建本章中的所有示例：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you did not install Boost from a native package, and for installation on
    Windows, refer to [Chapter 1](ch01.html "Chapter 1. Introducing Boost"), *Introducing
    Boost*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有从本机包安装Boost，并且需要在Windows上安装，请参考[第1章](ch01.html "第1章。介绍Boost")*介绍Boost*。
- en: 'Running this program prints the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此程序会打印以下内容：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that `Greetings:` is printed from the main function (line 13) before the
    call to `run` (line 14). The call to `run` ends up dispatching the sole handler
    in the queue, which prints `Hello, World!`. It is also possible for multiple threads
    to call `run` on the same I/O object and dispatch handlers concurrently. We will
    see how this can be useful in the next section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在调用`run`（第14行）之前，`Greetings：`是从主函数（第13行）打印出来的。调用`run`最终会分派队列中的唯一处理程序，打印出`Hello,
    World!`。多个线程也可以调用相同的I/O对象上的`run`并发地分派处理程序。我们将在下一节中看到这如何有用。
- en: Handler states – run_one, poll, and poll_one
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理程序状态 - run_one、poll和poll_one
- en: While the `run` function blocks until there are no more handlers in the queue,
    there are other member functions of `io_service` that let you process handlers
    with greater flexibility. But before we look at this function, we need to distinguish
    between pending and ready handlers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`run`函数会阻塞，直到队列中没有更多的处理程序，但`io_service`还有其他成员函数，让您以更大的灵活性处理处理程序。但在我们查看这个函数之前，我们需要区分挂起和准备好的处理程序。
- en: The handlers we posted to the `io_service` were all ready to run immediately
    and were invoked as soon as their turn came on the queue. In general, handlers
    are associated with background tasks that run in the underlying OS, for example,
    network I/O tasks. Such handlers are meant to be invoked only once the associated
    task is completed, which is why in such contexts, they are called **completion
    handlers**. These handlers are said to be **pending** until the associated task
    is awaiting completion, and once the associated task completes, they are said
    to be **ready**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发布到`io_service`的处理程序都准备立即运行，并在它们在队列上轮到时立即被调用。一般来说，处理程序与在底层操作系统中运行的后台任务相关联，例如网络I/O任务。这样的处理程序只有在关联的任务完成后才会被调用，这就是为什么在这种情况下，它们被称为**完成处理程序**。这些处理程序被称为**挂起**，直到关联的任务等待完成，一旦关联的任务完成，它们就被称为**准备**。
- en: The `poll` member function, unlike `run`, dispatches all the ready handlers
    but does not wait for any pending handler to become ready. Thus, it returns immediately
    if there are no ready handlers, even if there are pending handlers. The `poll_one`
    member function dispatches exactly one ready handler if there be one, but does
    not block waiting for pending handlers to get ready.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与`run`不同，`poll`成员函数会分派所有准备好的处理程序，但不会等待任何挂起的处理程序准备就绪。因此，如果没有准备好的处理程序，它会立即返回，即使有挂起的处理程序。`poll_one`成员函数如果有一个准备好的处理程序，会分派一个，但不会阻塞等待挂起的处理程序准备就绪。
- en: The `run_one` member function blocks on a nonempty queue waiting for a handler
    to become ready. It returns when called on an empty queue, and otherwise, as soon
    as it finds and dispatches a ready handler.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`run_one`成员函数会在非空队列上阻塞，等待处理程序准备就绪。如果在空队列上调用它，它会返回，并且在找到并分派一个准备好的处理程序后立即返回。'
- en: post versus dispatch
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发布与分派
- en: 'A call to the `post` member function adds a handler to the task queue and returns
    immediately. A later call to `run` is responsible for dispatching the handler.
    There is another member function called `dispatch` that can be used to request
    the `io_service` to invoke a handler immediately if possible. If `dispatch` is
    invoked in a thread that has already called one of `run`, `poll`, `run_one`, or
    `poll_one`, then the handler will be invoked immediately. If no such thread is
    available, `dispatch` adds the handler to the queue and returns just like `post`
    would. In the following example, we invoke `dispatch` from the `main` function
    and from within another handler:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对`post`成员函数的调用会将处理程序添加到任务队列并立即返回。稍后对`run`的调用负责调度处理程序。还有另一个名为`dispatch`的成员函数，可以用来请求`io_service`立即调用处理程序。如果在已经调用了`run`、`poll`、`run_one`或`poll_one`的线程中调用了`dispatch`，那么处理程序将立即被调用。如果没有这样的线程可用，`dispatch`会将处理程序添加到队列并像`post`一样立即返回。在以下示例中，我们从`main`函数和另一个处理程序中调用`dispatch`：
- en: '**Listing 11.2: post versus dispatch**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.2：post与dispatch**'
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Running this code produces the following output:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码会产生以下输出：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first call to `dispatch` (line 8) adds a handler to the queue without invoking
    it because `run` was yet to be called on `io_service`. We call this the Hello
    Handler, as it prints `Hello`. This is followed by the two calls to `post` (lines
    10, 18), which add two more handlers. The first of these two handlers prints `Hello,
    world!` (line 12), and in turn, calls `dispatch` (line 13) to add another handler
    that prints the Spanish greeting, `Hola, mundo!` (line 14). The second of these
    handlers prints the German greeting, `Hallo, Welt` (line 18). For our convenience,
    let''s just call them the English, Spanish, and German handlers. This creates
    the following entries in the queue:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对`dispatch`的第一次调用（第8行）将处理程序添加到队列中而不调用它，因为`io_service`上尚未调用`run`。我们称这个为Hello处理程序，因为它打印`Hello`。然后是两次对`post`的调用（第10行，第18行），它们分别添加了两个处理程序。这两个处理程序中的第一个打印`Hello,
    world!`（第12行），然后调用`dispatch`（第13行）添加另一个打印西班牙问候语`Hola, mundo!`（第14行）的处理程序。这两个处理程序中的第二个打印德国问候语`Hallo,
    Welt`（第18行）。为了方便起见，让我们称它们为英文、西班牙文和德文处理程序。这在队列中创建了以下条目：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, when we call `run` on the `io_service` (line 19), the Hello Handler is
    dispatched first and prints `Hello`. This is followed by the English Handler,
    which prints `Hello, World!` and calls `dispatch` on the `io_service`, passing
    the Spanish Handler. Since this executes in the context of a thread that has already
    called `run`, the call to `dispatch` invokes the Spanish Handler, which prints
    `Hola, mundo!`. Following this, the German Handler is dispatched printing `Hallo,
    Welt!` before `run` returns.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们在`io_service`上调用`run`（第19行）时，首先调度Hello处理程序并打印`Hello`。然后是英文处理程序，它打印`Hello,
    World!`并在`io_service`上调用`dispatch`，传递西班牙处理程序。由于这在已经调用`run`的线程的上下文中执行，对`dispatch`的调用会调用西班牙处理程序，打印`Hola,
    mundo!`。随后，德国处理程序被调度打印`Hallo, Welt!`，在`run`返回之前。
- en: 'What if the English Handler called `post` instead of `dispatch` (line 13)?
    In that case, the Spanish Handler would not be invoked immediately but would queue
    up after the German Handler. The German greeting `Hallo, Welt!` would precede
    the Spanish greeting `Hola, mundo!`. The output would look like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果英文处理程序调用`post`而不是`dispatch`（第13行），那么西班牙处理程序将不会立即被调用，而是在德国处理程序之后排队。德国问候语`Hallo,
    Welt!`将在西班牙问候语`Hola, mundo!`之前出现。输出将如下所示：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Concurrent execution via thread pools
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过线程池并发执行
- en: 'The `io_service` object is thread-safe and multiple threads can call `run`
    on it concurrently. If there are multiple handlers in the queue, they can be processed
    concurrently by such threads. In effect, the set of threads that call `run` on
    a given `io_service` form a **thread pool**. Successive handlers can be processed
    by different threads in the pool. Which thread dispatches a given handler is indeterminate,
    so the handler code should not make any such assumptions. In the following example,
    we post a bunch of handlers to the `io_service` and then start four threads, which
    all call `run` on it:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`io_service`对象是线程安全的，多个线程可以同时在其上调用`run`。如果队列中有多个处理程序，它们可以被这些线程同时处理。实际上，调用`run`的一组线程在给定的`io_service`上形成一个**线程池**。后续的处理程序可以由池中的不同线程处理。哪个线程调度给定的处理程序是不确定的，因此处理程序代码不应该做出任何这样的假设。在以下示例中，我们将一堆处理程序发布到`io_service`，然后启动四个线程，它们都在其上调用`run`：'
- en: '**Listing 11.3: Simple thread pools**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.3：简单线程池**'
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We post twenty handlers in a loop (line 18). Each handler prints its identifier
    (line 19), and then sleeps for a second (lines 19-20). To run the handlers, we
    create a group of four threads, each of which calls run on the `io_service` (line
    21) and wait for all the threads to finish (line 24). We define the macro `PRINT_ARGS`
    which writes output to the console in a thread-safe way, tagged with the current
    thread ID (line 7-10). We will use this macro in other examples too.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在循环中发布了20个处理程序（第18行）。每个处理程序打印其标识符（第19行），然后休眠一秒钟（第19-20行）。为了运行处理程序，我们创建了一个包含四个线程的组，每个线程在`io_service`上调用run（第21行），并等待所有线程完成（第24行）。我们定义了宏`PRINT_ARGS`，它以线程安全的方式将输出写入控制台，并标记当前线程ID（第7-10行）。我们以后也会在其他示例中使用这个宏。
- en: 'To build this example, you must also link against `libboost_thread`, `libboost_date_time`,
    and in Posix environments, with `libpthread` too:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建此示例，您还必须链接`libboost_thread`、`libboost_date_time`，在Posix环境中还必须链接`libpthread`。
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'One particular run of this program on my laptop produced the following output
    (with some lines snipped):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的笔记本电脑上运行此程序的一个特定运行产生了以下输出（有些行被剪掉）：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can see that the different handlers are executed by different threads (each
    thread ID marked differently).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到不同的处理程序由不同的线程执行（每个线程ID标记不同）。
- en: Tip
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If any of the handlers threw an exception, it would be propagated across the
    call to the `run` function on the thread that was executing the handler.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何处理程序抛出异常，它将传播到执行处理程序的线程上对`run`函数的调用。
- en: io_service::work
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: io_service::work
- en: 'Sometimes, it is useful to keep the thread pool started, even when there are
    no handlers to dispatch. Neither `run` nor `run_one` blocks on an empty queue.
    So in order for them to block waiting for a task, we have to indicate, in some
    way, that there is outstanding work to be performed. We do this by creating an
    instance of `io_service::work`, as shown in the following example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，即使没有处理程序要调度，保持线程池启动也是有用的。`run`和`run_one`都不会在空队列上阻塞。因此，为了让它们阻塞等待任务，我们必须以某种方式指示有未完成的工作要执行。我们通过创建`io_service::work`的实例来实现这一点，如下例所示：
- en: '**Listing 11.4: Using io_service::work to keep threads engaged**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**11.4节：使用io_service::work保持线程忙碌**'
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this example, we create an object of `io_service::work` wrapped in a `unique_ptr`
    (line 18). We associate it with an `io_service` object by passing to the `work`
    constructor a reference to the `io_service` object. Note that, unlike listing
    11.3, we create the worker threads first (lines 24-27) and then post the handlers
    (lines 33-39). However, the worker threads stay put waiting for the handlers because
    of the calls to `run` block (line 26). This happens because of the `io_service::work`
    object we created, which indicates that there is outstanding work in the `io_service`
    queue. As a result, even after all handlers are dispatched, the threads do not
    exit. By calling `reset` on the `unique_ptr,` wrapping the `work` object, its
    destructor is called, which notifies the `io_service` that all outstanding work
    is complete (line 42). The calls to `run` in the threads return and the program
    exits once all the threads are joined (line 43). We wrapped the `work` object
    in a `unique_ptr` to destroy it in an exception-safe way at a suitable point in
    the program.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个例子中，我们创建了一个包装在`unique_ptr`中的`io_service::work`对象（第18行）。我们通过将`io_service`对象的引用传递给`work`构造函数，将其与`io_service`对象关联起来。请注意，与11.3节不同，我们首先创建了工作线程（第24-27行），然后发布了处理程序（第33-39行）。然而，由于调用`run`阻塞（第26行），工作线程会一直等待处理程序。这是因为我们创建的`io_service::work`对象指示`io_service`队列中有未完成的工作。因此，即使所有处理程序都被调度，线程也不会退出。通过在包装`work`对象的`unique_ptr`上调用`reset`，其析构函数被调用，通知`io_service`所有未完成的工作已完成（第42行）。线程中的`run`调用返回，一旦所有线程都加入，程序就会退出（第43行）。我们将`work`对象包装在`unique_ptr`中，以便在程序的适当位置以异常安全的方式销毁它。 '
- en: We omitted the definition of `PRINT_ARGS` here, refer to listing 11.3.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里省略了`PRINT_ARGS`的定义，请参考11.3节。
- en: Serialized and ordered execution via strands
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过strands进行序列化和有序执行
- en: Thread pools allow handlers to be run concurrently. This means that handlers
    that access shared resources need to synchronize access to these resources. We
    already saw examples of this in listings 11.3 and 11.4, when we synchronized access
    to `std::cout`, which is a global object. As an alternative to writing synchronization
    code in handlers, which can make the handler code more complex, we can use **strands**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 线程池允许处理程序并发运行。这意味着访问共享资源的处理程序需要同步访问这些资源。我们在11.3和11.4节中已经看到了这方面的例子，当我们同步访问全局对象`std::cout`时。作为在处理程序中编写同步代码的替代方案，我们可以使用**strands**。
- en: Tip
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Think of a strand as a subsequence of the task queue with the constraint that
    no two handlers from the same strand ever run concurrently.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将strand视为任务队列的子序列，其中没有两个来自同一strand的处理程序会同时运行。
- en: 'The scheduling of other handlers in the queue, which are not in the strand,
    is not affected by the strand in any way. Let us look at an example of using strands:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 队列中的其他处理程序的调度不受strand的影响。让我们看一个使用strands的例子：
- en: '**Listing 11.5: Using strands**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**11.5节：使用strands**'
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this example, we create two handler functions: `workFuncStrand` (line 21)
    and `workFunc` (line 28). The lambda `workFuncStrand` captures a counter `on_strand`,
    increments it, and prints a message `Hello, from strand!`, prefixed with the value
    of the counter. The function `workFunc` captures another counter `regular`, increments
    it, and prints `Hello, World!`, prefixed with the counter. Both pause for 2 seconds
    before returning.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们创建了两个处理程序函数：`workFuncStrand`（第21行）和`workFunc`（第28行）。lambda `workFuncStrand`捕获一个计数器`on_strand`，递增它，并打印一个带有计数器值前缀的消息`Hello,
    from strand!`。函数`workFunc`捕获另一个计数器`regular`，递增它，并打印带有计数器前缀的消息`Hello, World!`。两者在返回前暂停2秒。
- en: 'To define and use a strand, we first create an object of `io_service::strand`
    associated with the `io_service` instance (line 17). Thereafter, we post all handlers
    that we want to be part of that strand by wrapping them using the `wrap` member
    function of the `strand` (line 36). Alternatively, we can post the handlers to
    the strand directly by using either the `post` or the `dispatch` member function
    of the strand, as shown in the following snippet:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义和使用strand，我们首先创建一个与`io_service`实例关联的`io_service::strand`对象（第17行）。然后，我们通过使用`strand`的`wrap`成员函数（第36行）将所有要成为该strand一部分的处理程序发布。或者，我们可以直接使用strand的`post`或`dispatch`成员函数发布处理程序到strand，如下面的代码片段所示：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `wrap` member function of strand returns a function object, which in turn
    calls `dispatch` on the strand to invoke the original handler. Initially, it is
    this function object rather than our original handler that is added to the queue.
    When duly dispatched, this invokes the original handler. There are no constraints
    on the order in which these wrapper handlers are dispatched, and therefore, the
    actual order in which the original handlers are invoked can be different from
    the order in which they were wrapped and posted.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: strand的`wrap`成员函数返回一个函数对象，该函数对象调用strand上的`dispatch`来调用原始处理程序。最初，添加到队列中的是这个函数对象，而不是我们的原始处理程序。当得到适当的调度时，这将调用原始处理程序。对于这些包装处理程序的调度顺序没有约束，因此，原始处理程序被调用的实际顺序可能与它们被包装和发布的顺序不同。
- en: On the other hand, calling `post` or `dispatch` directly on the strand avoids
    an intermediate handler. Directly posting to a strand also guarantees that the
    handlers will be dispatched in the same order that they were posted, achieving
    a deterministic ordering of the handlers in the strand. The `dispatch` member
    of `strand` blocks until the handler is dispatched. The `post` member simply adds
    it to the strand and returns.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在线程上直接调用`post`或`dispatch`可以避免中间处理程序。直接向线程发布也可以保证处理程序将按照发布的顺序进行分发，实现线程中处理程序的确定性排序。`strand`的`dispatch`成员会阻塞，直到处理程序被分发。`post`成员只是将其添加到线程并返回。
- en: Note that `workFuncStrand` increments `on_strand` without synchronization (line
    22), while `workFunc` increments the counter `regular` within the `PRINT_ARGS`
    macro (line 29), which ensures that the increment happens in a critical section.
    The `workFuncStrand` handlers are posted to a strand and therefore are guaranteed
    to be serialized; hence no need for explicit synchronization. On the flip side,
    entire functions are serialized via strands and synchronizing smaller blocks of
    code is not possible. There is no serialization between the handlers running on
    the strand and other handlers; therefore, the access to global objects, like `std::cout`,
    must still be synchronized.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`workFuncStrand`在没有同步的情况下递增`on_strand`（第22行），而`workFunc`在`PRINT_ARGS`宏（第29行）中递增计数器`regular`，这确保递增发生在临界区内。`workFuncStrand`处理程序被发布到一个线程中，因此可以保证被序列化；因此不需要显式同步。另一方面，整个函数通过线程串行化，无法同步较小的代码块。在线程上运行的处理程序和其他处理程序之间没有串行化；因此，对全局对象的访问，如`std::cout`，仍然必须同步。
- en: 'The following is a sample output of running the preceding code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码的示例输出如下：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'There were three distinct threads in the pool and the handlers from the strand
    were picked up by two of these three threads: initially, by thread ID `b73b6b40`,
    and later on, by thread ID `b63b4b40`. This also dispels a frequent misunderstanding
    that all handlers in a strand are dispatched by the same thread, which is clearly
    not the case.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 线程池中有三个不同的线程，并且线程中的处理程序由这三个线程中的两个选择：最初由线程ID`b73b6b40`选择，后来由线程ID`b63b4b40`选择。这也消除了一个常见的误解，即所有线程中的处理程序都由同一个线程分发，这显然不是这样。
- en: Tip
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Different handlers in the same strand may be dispatched by different threads
    but will never run concurrently.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 同一线程中的不同处理程序可能由不同的线程分发，但永远不会同时运行。
- en: Network I/O using Asio
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Asio进行网络I/O
- en: We want to use Asio to build scalable network services that perform I/O over
    the network. Such services receive requests from clients running on remote machines
    and send them information over the network. The data transfer between processes
    across machine boundaries, happening over the wire, is done using certain protocols
    of network communication. The most ubiquitous of these protocols is IP or the
    **Internet Protocol** and a **suite of protocols** layered above it. Boost Asio
    supports TCP, UDP, and ICMP, the three popular protocols in the IP protocol suite.
    We do not cover ICMP in this book.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用Asio构建可扩展的网络服务，执行网络I/O。这些服务接收来自远程机器上运行的客户端的请求，并通过网络向它们发送信息。跨机器边界的进程之间的数据传输，通过网络进行，使用某些网络通信协议。其中最普遍的协议是IP或**Internet
    Protocol**及其上层的**一套协议**。Boost Asio支持TCP、UDP和ICMP，这三种流行的IP协议套件中的协议。本书不涵盖ICMP。
- en: UDP and TCP
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UDP和TCP
- en: '**User Datagram Protocol** or UDP is used to transmit **datagrams** or message
    units from one host to another over an IP network. UDP is a very basic protocol
    built over IP and is stateless in the sense that no context is maintained across
    multiple network I/O operations. The reliability of data transfer using UDP depends
    on the reliability of the underlying network, and UDP transfers have the following
    caveats:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**用户数据报协议**或UDP用于在IP网络上从一个主机向另一个主机传输**数据报**或消息单元。UDP是一个基于IP的非常基本的协议，它是无状态的，即在多个网络I/O操作之间不维护任何上下文。使用UDP进行数据传输的可靠性取决于底层网络的可靠性，UDP传输具有以下注意事项：'
- en: A UDP datagram may not be delivered at all
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UDP数据报可能根本不会被传递
- en: A given datagram may be delivered more than once
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定的数据报可能会被传递多次
- en: Two datagrams may not be delivered to the destination in the order in which
    they were dispatched from the source
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个数据报可能不会按照从源发送到目的地的顺序被传递
- en: UDP will detect any data corruption in the datagrams and drop such messages
    without any means of recovery
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UDP将检测数据报的任何数据损坏，并丢弃这样的消息，没有任何恢复的手段
- en: For these reasons, UDP is considered to be an unreliable protocol.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，UDP被认为是一种不可靠的协议。
- en: 'If an application requires stronger guarantees from the protocol, we choose
    **Transmission Control Protocol** or TCP. TCP deals in terms of byte streams rather
    than messages. It uses a handshake mechanism between two endpoints of the network
    communication to establish a durable **connection** between the two points and
    maintains state during the life of the connection. All communications between
    the two endpoints happen over such a connection. At the cost of a somewhat higher
    latency than UDP, TCP guarantees the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序需要协议提供更强的保证，我们选择**传输控制协议**或TCP。TCP使用字节流而不是消息进行处理。它在网络通信的两个端点之间使用握手机制建立持久的**连接**，并在连接的生命周期内维护状态。两个端点之间的所有通信都发生在这样的连接上。以比UDP略高的延迟为代价，TCP提供以下保证：
- en: On a given connection, the receiving application receives the stream of bytes
    sent by the sender in the order they were sent
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定的连接上，接收应用程序按照发送顺序接收发送方发送的字节流
- en: Any data lost or corrupted on the wire can be retransmitted, greatly improving
    the reliability of deliveries
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在传输过程中丢失或损坏的数据可以重新传输，大大提高了交付的可靠性
- en: Real-time applications that can handle unreliability and data loss for themselves
    often use UDP. In addition, a lot of higher level protocols are run on top of
    UDP. TCP is more frequently used, where correctness concerns supersede real-time
    performance, for example, e-mail and file transfer protocols, HTTP, and so on.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可以自行处理不可靠性和数据丢失的实时应用通常使用UDP。此外，许多高级协议都是在UDP之上运行的。TCP更常用，其中正确性关注超过实时性能，例如电子邮件和文件传输协议，HTTP等。
- en: IP addresses
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IP地址
- en: IP addresses are numeric identifiers used to uniquely identify interfaces connected
    to an IP network. The older IPv4 protocol uses 32-bit IP addresses in an address
    space of 4 billion (2^(32)) addresses. The emergent IPv6 protocol uses 128-bit
    IP addresses in an address space of 3.4 × 10^(38) (2^(128)) unique addresses,
    which is practically inexhaustible. You can represent IP addresses of both types
    using the class `boost::asio::ip::address`, while version-specific addresses can
    be represented using `boost::asio::ip::address_v4` and `boost::asio::ip::address_v6`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: IP地址是用于唯一标识连接到IP网络的接口的数字标识符。较旧的IPv4协议在4十亿（2^(32)）地址的地址空间中使用32位IP地址。新兴的IPv6协议在3.4
    × 10^(38)（2^(128)）个唯一地址的地址空间中使用128位IP地址，这几乎是不可枯竭的。您可以使用类`boost::asio::ip::address`表示两种类型的IP地址，而特定版本的地址可以使用`boost::asio::ip::address_v4`和`boost::asio::ip::address_v6`表示。
- en: IPv4 addresses
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IPv4地址
- en: The familiar IPv4 addresses, such as 212.54.84.93, are 32-bit unsigned integers
    expressed in the *dotted-quad notation*; four 8-bit unsigned integers or *octets*
    representing the four bytes in the address, the most significant on the left to
    the least significant on the right, separated by dots (period signs). Each octet
    can range from 0 through 255\. IP addresses are normally interpreted in network
    byte order, that is, Big-endian.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉的IPv4地址，例如212.54.84.93，是以*点分四进制表示法*表示的32位无符号整数；四个8位无符号整数或*八位字节*表示地址中的四个字节，从左到右依次是最重要的，以点（句号）分隔。每个八位字节的范围是从0到255。IP地址通常以网络字节顺序解释，即大端序。
- en: Subnets
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 子网
- en: Larger computer networks are often divided into logical parts called **subnets**.
    A subnet consists of a set of nodes that can communicate with each other using
    broadcast messages. A subnet has an associated pool of IP addresses that have
    a common prefix, usually, called the *routing prefix* or *network address*. The
    remaining part of the IP address field is called the *host part*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的计算机网络通常被划分为称为**子网**的逻辑部分。子网由一组可以使用广播消息相互通信的节点组成。子网有一个关联的IP地址池，具有一个共同的前缀，通常称为*路由前缀*或*网络地址*。IP地址字段的剩余部分称为*主机部分*。
- en: Given an IP address *and* the length of the prefix, we can compute the prefix
    using the **netmask**. The netmask of a subnet is a 4-byte bitmask, whose bitwise-AND
    with an IP address in the subnet yields the routing prefix. For a subnet with
    a routing prefix of length N, the netmask has the most significant N bits set
    and the remaining 32-N bits unset. The netmask is often expressed in a dotted-quad
    notation. For example, if the address 172.31.198.12 has a routing prefix that
    is 16 bits long, then its netmask would be 255.255.0.0 and the routing prefix
    would be 172.31.0.0.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 给定IP地址*和*前缀长度，我们可以使用**子网掩码**计算前缀。子网的子网掩码是一个4字节的位掩码，与子网中的IP地址进行按位与运算得到路由前缀。对于具有长度为N的路由前缀的子网，子网掩码的最高有效N位设置，剩余的32-N位未设置。子网掩码通常以点分四进制表示法表示。例如，如果地址172.31.198.12具有长度为16位的路由前缀，则其子网掩码将为255.255.0.0，路由前缀将为172.31.0.0。
- en: In general, the length of the routing prefix must be explicitly specified. The
    **Classless Interdomain Routing** (**CIDR) notation** augments the dotted-quad
    notation with a trailing slash and a number between 0 and 32 that represents the
    prefix length. Thus, 10.209.72.221/22 represents an IP address with a prefix length
    of 22\. An older scheme of classification, referred to as the *classful scheme*,
    divided the IPv4 address space into ranges and assigned a *class* to each range
    (see the following table). Addresses belonging to each range were said to be of
    the corresponding class, and the length of the routing prefix was determined based
    on the class, without being specified as, with the CIDR notation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，路由前缀的长度必须明确指定。**无类域间路由**（**CIDR）表示法**使用点分四进制表示法，并在末尾加上一个斜杠和一个介于0和32之间的数字，表示前缀长度。因此，10.209.72.221/22表示具有前缀长度为22的IP地址。一个旧的分类方案，称为*有类方案*，将IPv4地址空间划分为范围，并为每个范围分配一个*类*（见下表）。属于每个范围的地址被认为是相应类的地址，并且路由前缀的长度是基于类确定的，而不是使用CIDR表示法指定。
- en: '| Class | Address range | Prefix length | Netmask | Remarks |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 类 | 地址范围 | 前缀长度 | 子网掩码 | 备注 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Class A | 0.0.0.0 – 127.255.255.255 | 8 | 255.0.0.0 |   |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 类A | 0.0.0.0 – 127.255.255.255 | 8 | 255.0.0.0 |   |'
- en: '| Class B | 128.0.0.0 – 191.255.255.255 | 16 | 255.255.0.0 |   |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 类B | 128.0.0.0 – 191.255.255.255 | 16 | 255.255.0.0 |   |'
- en: '| Class C | 192.0.0.0 – 223.255.255.255 | 24 | 255.255.255.0 |   |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 类C | 192.0.0.0 – 223.255.255.255 | 24 | 255.255.255.0 |   |'
- en: '| Class D | 224.0.0.0 – 239.255.255.255 | Not specified | Not specified | Multicast
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 类D | 224.0.0.0 – 239.255.255.255 | 未指定 | 未指定 | 多播 |'
- en: '| Class E | 240.0.0.0 – 255.255.255.255 | Not specified | Not specified | Reserved
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 类E | 240.0.0.0 – 255.255.255.255 | 未指定 | 未指定 | 保留 |'
- en: Special addresses
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特殊地址
- en: Some IPv4 addresses have special meanings. For example, an IP address with all
    bits set in the host part is known as the **broadcast address** for the subnet
    and is used to broadcast messages to all hosts in the subnet. For example, the
    broadcast address in the network 172.31.0.0/16 is 172.31.255.255.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一些IPv4地址具有特殊含义。例如，主机部分中所有位设置的IP地址被称为子网的**广播地址**，用于向子网中的所有主机广播消息。例如，网络172.31.0.0/16中的广播地址为172.31.255.255。
- en: Applications listening for incoming requests use the **unspecified address**
    0.0.0.0 (`INADDR_ANY`) to listen on all available network interfaces, without
    the need to know addresses plumbed on the system.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 监听传入请求的应用程序使用**未指定地址** 0.0.0.0（`INADDR_ANY`）来监听所有可用的网络接口，而无需知道系统上的地址。
- en: The **loopback address** 127.0.0.1 is commonly associated with a virtual network
    interface that is not associated with any hardware and does not require the host
    to be connected to a network. Data sent over the loopback interface immediately
    shows up as received data on the sender host itself. Often used for testing networked
    applications within a box, you can configure additional loopback interfaces and
    associate loopback addresses from the range 127.0.0.0 through 127.255.255.255.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**回环地址** 127.0.0.1通常与一个虚拟网络接口相关联，该接口不与任何硬件相关，并且不需要主机连接到网络。通过回环接口发送的数据立即显示为发送方主机上的接收数据。经常用于在一个盒子内测试网络应用程序，您可以配置额外的回环接口，并将回环地址从127.0.0.0到127.255.255.255的范围关联起来。'
- en: Handling IPv4 addresses with Boost
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用Boost处理IPv4地址
- en: 'Let us now look at a code example of constructing IPv4 addresses and glean
    useful information from them, using the type `boost::asio::ip::address_v4`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一个构造IPv4地址并从中获取有用信息的代码示例，使用类型`boost::asio::ip::address_v4`：
- en: '**Listing 11.6: Handling IPv4 addresses**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.6：处理IPv4地址**'
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This example highlights a few basic operations on IPv4 addresses. We create
    a vector of `boost::asio::ip::address` objects (not just `address_v4`) and push
    IPv4 addresses constructed from their string representations using the `address_v4::from_string`
    static function (line 32). We use the two-argument overload of `from_string`,
    which takes the address string, and a non-const reference to an `error_code` object
    that is set if it is unable to parse the address string. A one-argument overload
    exists, which throws if there is an error. Note that you can implicitly convert
    or assign `address_v4` instances to `address` instances. Default constructed instances
    of `address_v4` are equivalent to the unspecified address 0.0.0.0 (line 28), which
    is also returned by `address_v4::any()` (line 29).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子突出了IPv4地址的一些基本操作。我们创建了一个`boost::asio::ip::address`对象的向量（不仅仅是`address_v4`），并从它们的字符串表示中构造IPv4地址，使用`address_v4::from_string`静态函数（第32行）。我们使用`from_string`的两个参数重载，它接受地址字符串和一个非const引用到`error_code`对象，如果无法解析地址字符串，则设置该对象。存在一个单参数重载，如果有错误则抛出。请注意，您可以隐式转换或分配`address_v4`实例到`address`实例。默认构造的`address_v4`实例等同于未指定地址0.0.0.0（第28行），也可以由`address_v4::any()`（第29行）返回。
- en: To print the properties of the address, we have written the `printAddrProperties`
    function (line 9). We print IP addresses by streaming them to `std::cout` (line
    10). We check whether an address is an IPv4 or IPv6 address using the `is_v4`
    and `is_v6` member functions (lines 12, 14), print the netmask for an IPv4 address
    using the `address_v4::netmask` static function (line 13), and also check whether
    the address is an unspecified address, loopback address, or IPv4 multicast address
    (class D) using appropriate member predicates (lines 16-18). Note that the `address_v4::from_string`
    function does not recognize the CIDR format (as of Boost version 1.57), and the
    netmask is computed based on the classful scheme.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了打印地址的属性，我们编写了`printAddrProperties`函数（第9行）。我们通过将IP地址流式传输到`std::cout`（第10行）来打印IP地址。我们使用`is_v4`和`is_v6`成员函数（第12、14行）来检查地址是IPv4还是IPv6地址，使用`address_v4::netmask`静态函数（第13行）打印IPv4地址的网络掩码，并使用适当的成员谓词（第16-18行）检查地址是否为未指定地址、回环地址或IPv4多播地址（类D）。请注意，`address_v4::from_string`函数不识别CIDR格式（截至Boost版本1.57），并且网络掩码是基于类别的方案计算的。
- en: In the next section, following a brief overview of IPv6 addresses, we will augment
    the `printAddrProperties` (line 14) function to print IPv6 specific properties
    as well.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将在简要概述IPv6地址之后，增强`printAddrProperties`（第14行）函数，以打印IPv6特定属性。
- en: IPv6 addresses
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IPv6地址
- en: 'In its most general form, an IPv6 address is represented as a sequence of eight
    2-byte unsigned hexadecimal integers, separated by colons. Digits `a` through
    `f` in the hexadecimal integers are written in lowercase by convention and leading
    zeros in each 16-bit number are omitted. Here is an example of an IPv6 address
    in this notation:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最一般的形式中，IPv6地址被表示为由冒号分隔的八个2字节无符号十六进制整数序列。按照惯例，十六进制整数中的数字`a`到`f`以小写字母写入，并且每个16位数字中的前导零被省略。以下是以这种表示法的IPv6地址的一个例子：
- en: 2001:0c2f:003a:01e0:0000:0000:0000:002a
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 2001:0c2f:003a:01e0:0000:0000:0000:002a
- en: One sequence of two or more zero terms can be collapsed completely. Thus, the
    preceding address can be written as 2001:c2f:3a:1e0::2a. All leading zeros have
    been removed and the contiguous zero terms between bytes 16 and 63 have been collapsed,
    leaving the colon pair (::). If there be multiple zero-term sequences, the longest
    one is collapsed, and if there is a tie, the one that is leftmost is collapsed.
    Thus, we can abbreviate this 2001:0000:0000:01e0:0000:0000:001a:002a to this 2001::1e0:0:0:1a:2a.
    Note that the leftmost sequence of two zero-terms is collapsed, while the other
    between bits 32 and 63 are not collapsed.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 两个或多个零项的序列可以完全折叠。因此，前面的地址可以写成2001:c2f:3a:1e0::2a。所有前导零已被移除，并且在字节16和63之间的连续零项已被折叠，留下了冒号对(::)。如果有多个零项序列，则折叠最长的序列，如果有平局，则折叠最左边的序列。因此，我们可以将2001:0000:0000:01e0:0000:0000:001a:002a缩写为2001::1e0:0:0:1a:2a。请注意，最左边的两个零项序列被折叠，而32到63位之间的其他零项未被折叠。
- en: In environments transitioning from IPv4 to IPv6, software frequently supports
    both IPv4 and IPv6\. *IPv4-mapped IPv6 addresses* are used to enable communication
    between IPv6 and IPv4 interfaces. IPv4 addresses are mapped to an IPv6 address
    with the ::ffff:0:0/96 prefix and the last 32-bits same as the IPv4 address. For
    example, 172.31.201.43 will be represented as ::ffff:172.31.201.43/96.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在从IPv4过渡到IPv6的环境中，软件通常同时支持IPv4和IPv6。*IPv4映射的IPv6地址*用于在IPv6和IPv4接口之间进行通信。IPv4地址被映射到具有::ffff:0:0/96前缀和最后32位与IPv4地址相同的IPv6地址。例如，172.31.201.43将表示为::ffff:172.31.201.43/96。
- en: Address classes, scopes, and subnets
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 地址类、范围和子网
- en: 'There are three classes of IPv6 addresses:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: IPv6地址有三类：
- en: '**Unicast addresses**: These addresses identify a single network interface'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单播地址**：这些地址标识单个网络接口'
- en: '**Multicast addresses**: These addresses identify a group of network interfaces
    and are used to send data to all the interfaces in the group'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多播地址**：这些地址标识一组网络接口，并用于向组中的所有接口发送数据'
- en: '**Anycast addresses**: These addresses identify a group of network interfaces,
    but data sent to an **anycast** address is delivered to one or more interfaces
    that are topologically closest to the sender and not to all the interfaces in
    the group'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任播地址**：这些地址标识一组网络接口，但发送到**任播**地址的数据将传递给距离发送者拓扑最近的一个或多个接口，而不是传递给组中的所有接口'
- en: In unicast and anycast addresses, the least significant 64-bits of the address
    represent the host ID. In general, the higher order 64-bits represent the network
    prefix.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在单播和任播地址中，地址的最低有效64位表示主机ID。一般来说，高阶64位表示网络前缀。
- en: 'Each IPv6 address also has a **scope**, which identifies the segment of the
    network in which it is valid:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 每个IPv6地址也有一个**范围**，用于标识其有效的网络段：
- en: '**Node-local** addresses, including loopback addresses are used for communication
    within the node.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点本地**地址，包括环回地址，用于节点内通信。'
- en: '**Global** addresses are routable addresses reachable across networks.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局**地址是可通过网络到达的可路由地址。'
- en: '**Link-local** addresses are automatically assigned to each and every IPv6-enabled
    interface and are accessible only within a network, that is, routers do not route
    traffic headed for link-local addresses. Link-local addresses are assigned to
    interfaces even when they have routable addresses. Link-local addresses have a
    prefix of fe80::/64.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链路本地**地址会自动分配给每个启用IPv6的接口，并且只能在网络内访问，也就是说，路由器不会路由到链路本地地址的流量。即使具有可路由地址，链路本地地址也会分配给接口。链路本地地址的前缀为fe80::/64。'
- en: Special addresses
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特殊地址
- en: 'The IPv6 **loopback address** analogous to 127.0.0.1 in IPv4 is ::1\. The **unspecified
    address** (all zeros) in IPv6 is written as :: (`in6addr_any`). There are no broadcast
    addresses in IPv6, and multicast addresses are used to define groups of recipient
    interfaces, a topic that is outside the scope of this book.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: IPv6的**环回地址**类似于IPv4中的127.0.0.1，为::1。在IPv6中，**未指定地址**（全零）写为::（`in6addr_any`）。IPv6中没有广播地址，多播地址用于定义接收方接口的组，这超出了本书的范围。
- en: Handling IPv6 addresses with Boost
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用Boost处理IPv6地址
- en: 'In the following example, we construct IPv6 addresses and query properties
    of these addresses using the `boost::asio::ip::address_v6` class:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，我们构造IPv6地址，并使用`boost::asio::ip::address_v6`类查询这些地址的属性：
- en: '**Listing 11.7: Handling IPv6 addresses**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表11.7：处理IPv6地址**'
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This example augments listing 11.6 with IPv6-specific checks. The function `printAddrProperties`
    (line 15) is the same as that from listing 11.6, so it is not repeated in full.
    The `printAddr6Properties` function (line 8) checks whether the address is an
    IPv4-mapped IPv6 address (line 9) and whether it is a link-local address (line
    11). Other relevant checks are already performed through version-agnostic members
    of `address` in `printAddrProperties` (see listing 11.6).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子通过IPv6特定的检查增强了列表11.6。函数`printAddrProperties`（第15行）与列表11.6中的相同，因此不再完整重复。`printAddr6Properties`函数（第8行）检查地址是否为IPv4映射的IPv6地址（第9行），以及它是否为链路本地地址（第11行）。其他相关检查已经通过`printAddrProperties`中的与版本无关的`address`成员执行（参见列表11.6）。
- en: We create a vector of `boost::asio::ip::address` objects (not just `address_v6`)
    and push IPv6 addresses constructed from their string representations, using the
    `address_v6::from_string` static function (line 24), which returns `address_v6`
    objects, which are implicitly convertible to `address`. Notice that we have the
    loopback address, the unspecified address, IPv4-mapped address, a regular IPv6
    unicast address, and a link-local address (lines 20-21).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个`boost::asio::ip::address`对象的向量（不仅仅是`address_v6`），并推送由它们的字符串表示构造的IPv6地址，使用`address_v6::from_string`静态函数（第24行），它返回`address_v6`对象，可以隐式转换为`address`。请注意，我们有环回地址、未指定地址、IPv4映射地址、常规IPv6单播地址和链路本地地址（第20-21行）。
- en: Endpoints, sockets, and name resolution
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端点、套接字和名称解析
- en: Applications **bind** to IP addresses when providing network services, and multiple
    applications initiate outbound communication with other applications, starting
    from an IP address. Multiple applications can bind to the same IP address using
    different **ports**. A port is an unsigned 16-bit integer which, along with the
    IP address and protocol (TCP, UDP, etc.), uniquely identifies a communication
    **endpoint**. Data communication happens between two such endpoints. Boost Asio
    provides distinct endpoint types for UDP and TCP, namely, `boost::asio::ip::udp::endpoint`
    and `boost::asio::ip::tcp::endpoint`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序在提供网络服务时绑定到IP地址，多个应用程序从IP地址开始发起对其他应用程序的出站通信。多个应用程序可以使用不同的**端口**绑定到同一个IP地址。端口是一个无符号的16位整数，与IP地址和协议（TCP、UDP等）一起，唯一标识一个通信**端点**。数据通信发生在两个这样的端点之间。Boost
    Asio为UDP和TCP提供了不同的端点类型，即`boost::asio::ip::udp::endpoint`和`boost::asio::ip::tcp::endpoint`。
- en: Ports
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端口
- en: Many standard and widely used network services use fixed, well-known ports.
    Ports 0 through 1023 are assigned to well-known system services, including the
    likes of FTP, SSH, telnet, SMTP, DNS, HTTP, and HTTPS. Widely used applications
    may register standard ports between 1024 and 49151 with the **Internet Assigned
    Numbers Authority** (**IANA**). Ports above 49151 can be used by any application,
    without the need for registration. The mapping of well-known ports to services
    is often maintained on a disk file, such as `/etc/services` on POSIX systems and
    `%SYSTEMROOT%\system32\drivers\etc\services` on Windows.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 许多标准和广泛使用的网络服务使用固定的众所周知的端口。端口0到1023分配给众所周知的系统服务，包括FTP、SSH、telnet、SMTP、DNS、HTTP和HTTPS等。广泛使用的应用程序可以在1024到49151之间注册标准端口，由**互联网编号分配机构**（**IANA**）负责。49151以上的端口可以被任何应用程序使用，无需注册。通常将众所周知的端口映射到服务的映射通常保存在磁盘文件中，例如在POSIX系统上是`/etc/services`，在Windows上是`%SYSTEMROOT%\system32\drivers\etc\services`。
- en: Sockets
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 套接字
- en: A **socket** represents an endpoint in use for network communication. It represents
    one end of a communication channel and provides the interface for performing all
    data communication. Boost Asio provides distinct socket types for UDP and TCP,
    namely, `boost::asio::ip::udp::socket` and `boost::asio::ip::tcp::socket`. Sockets
    are always associated with a corresponding local endpoint object. The native network
    programming interfaces on all modern operating systems use some derivative of
    the Berkeley Sockets API, which is a C API for performing network communications.
    The Boost Asio library provides type-safe abstractions built around this core
    API.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**套接字**表示用于网络通信的端点。它表示通信通道的一端，并提供执行所有数据通信的接口。Boost Asio为UDP和TCP提供了不同的套接字类型，即`boost::asio::ip::udp::socket`和`boost::asio::ip::tcp::socket`。套接字始终与相应的本地端点对象相关联。所有现代操作系统上的本机网络编程接口都使用某种伯克利套接字API的衍生版本，这是用于执行网络通信的C
    API。Boost Asio库提供了围绕这个核心API构建的类型安全抽象。'
- en: Sockets are an example of **I/O objects**. In Asio, I/O objects are the class
    of objects that are used to initiate I/O operations. The operations are dispatched
    to the underlying operating system by an **I/O service** object, which is an instance
    of `boost::asio::io_service`. Earlier in this chapter, we saw the I/O service
    objects in action as task managers. But their primary role is as an interface
    for operations on the underlying operating system. Each I/O object is constructed
    with an associated I/O service instance. In this way, high-level I/O operations
    are initiated on the I/O object, but the interactions between the I/O object and
    the I/O service remain encapsulated. In the following sections, we will see examples
    of using UDP and TCP sockets for network communication.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 套接字是**I/O对象**的一个例子。在Asio中，I/O对象是用于启动I/O操作的对象类。这些操作由底层操作系统的**I/O服务**对象分派，该对象是`boost::asio::io_service`的实例。在本章的前面，我们看到了I/O服务对象作为任务管理器的实例。但是它们的主要作用是作为底层操作系统上操作的接口。每个I/O对象都是使用关联的I/O服务实例构造的。通过这种方式，高级I/O操作在I/O对象上启动，但是I/O对象和I/O服务之间的交互保持封装。在接下来的章节中，我们将看到使用UDP和TCP套接字进行网络通信的示例。
- en: Hostnames and domain names
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主机名和域名
- en: Identifying hosts in a network by names rather than numeric addresses is often
    more convenient. The Domain Name System (DNS) provides a hierarchical naming system
    in which hosts in a network are each identified by a hostname qualified with a
    unique name identifying the network, known as the **fully-qualified domain name**
    or simply **domain name**. For example, the imaginary domain name `elan.taliesyn.org`
    could be mapped to the IP address 140.82.168.29\. Here, `elan` would identify
    the specific host and `taliesyn.org` would identify the domain that the host is
    part of. It is quite possible for different groups of machines in a single network
    to report to different domains and even for a given machine to be part of multiple
    domains.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通过名称而不是数字地址来识别网络中的主机通常更方便。域名系统（DNS）提供了一个分层命名系统，其中网络中的主机通过带有唯一名称的主机名来标识，该名称标识了网络，称为**完全限定域名**或简称**域名**。例如，假想的域名`elan.taliesyn.org`可以映射到IP地址140.82.168.29。在这里，`elan`将标识特定主机，`taliesyn.org`将标识主机所属的域。在同一网络中，不同组的计算机可能报告给不同的域，甚至某台计算机可能属于多个域。
- en: Name resolution
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 名称解析
- en: 'A hierarchy of DNS servers across the world, and within private networks, maintain
    name-to-address mappings. Applications ask a configured DNS server to resolve
    a fully-qualified domain name to an address. The DNS server either resolves the
    request to an IP address or forwards it to another DNS server higher up in the
    hierarchy if there is one. The resolution fails if none of the DNS servers, all
    the way up to the root of the hierarchy, has an answer. A specialized program
    or a library that initiates such name resolution requests is called a **resolver**.
    Boost Asio provides protocol-specific resolvers: `boost::asio::ip::tcp::resolver`
    and `boost::asio::ip::udp::resolver` for performing such name resolutions. We
    query for services on hostnames and obtain one or more endpoints for that service.
    The following example shows how to do this, given a hostname, and optionally,
    a service name or port:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 全球范围内的DNS服务器层次结构以及私人网络内的DNS服务器维护名称到地址的映射。应用程序询问配置的DNS服务器以解析完全限定域名到地址。如果有的话，DNS服务器将请求解析为IP地址，否则将其转发到层次结构更高的另一个DNS服务器。如果直到层次结构的根部都没有答案，解析将失败。发起这种名称解析请求的专门程序或库称为**解析器**。Boost
    Asio提供了特定协议的解析器：`boost::asio::ip::tcp::resolver`和`boost::asio::ip::udp::resolver`用于执行此类名称解析。我们查询主机名上的服务，并获取该服务的一个或多个端点。以下示例显示了如何做到这一点，给定一个主机名，以及可选的服务名或端口：
- en: '**Listing 11.8: Looking up IP addresses of hosts**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.8：查找主机的IP地址**'
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You run this program by passing it a hostname and an optional service name
    on the command line. This program resolves these to an IP address and a port,
    and prints them to the standard output (lines 21-22). The program creates an instance
    of `io_service` (line 14), which would be the conduit for operations on the underlying
    operating system, and an instance of `boost::asio::ip::tcp::resolver` (line 15)
    that provides the interface for requesting name resolution. We create a name lookup
    request in terms of the hostname and service name, encapsulated in a `query` object
    (line 16), and call the `resolve` member function of the `resolver`, passing the
    `query` object as an argument (line 18). The `resolve` function returns an **endpoint
    iterator** to a sequence of `endpoint` objects resolved by the query. We iterate
    through this sequence, printing the address and port number for each endpoint.
    This would print IPv4 as well as IPv6 addresses if any. If we wanted IP addresses,
    specific to one version of IP, we would need to use the three-argument constructor
    for `query` and specify the protocol in the first argument. For example, to look
    up only IPv6 addresses, we can use this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在命令行上传递主机名和可选的服务名来运行此程序。该程序将这些解析为IP地址和端口，并将它们打印到标准输出（第21-22行）。程序创建了一个`io_service`实例（第14行），它将成为底层操作系统操作的通道，以及一个`boost::asio::ip::tcp::resolver`实例（第15行），它提供了请求名称解析的接口。我们根据主机名和服务名创建一个名称查找请求，封装在一个`query`对象中（第16行），并调用`resolver`的`resolve`成员函数，将`query`对象作为参数传递（第18行）。`resolve`函数返回一个**endpoint
    iterator**，指向查询解析的一系列`endpoint`对象。我们遍历这个序列，打印每个端点的地址和端口号。如果有的话，这将打印IPv4和IPv6地址。如果我们想要特定于IP版本的IP地址，我们需要使用`query`的三参数构造函数，并在第一个参数中指定协议。例如，要仅查找IPv6地址，我们可以使用这个：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'On lookup failure, the `resolve` function throws an exception unless we use
    the two-argument version that takes a non-const reference to `error_code`, as
    a second argument and sets it on error. In the following example, we perform the
    reverse lookup. Given an IP address and a port, we look up the associated hostname
    and service name:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在查找失败时，`resolve`函数会抛出异常，除非我们使用接受非const引用`error_code`的两参数版本，并在错误时设置它。在下面的例子中，我们执行反向查找。给定一个IP地址和一个端口，我们查找关联的主机名和服务名：
- en: '**Listing 11.9: Looking up hosts and service names**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.9：查找主机和服务名称**'
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We pass the IP address and the port number to the program from the command line,
    and using them, we construct the `endpoint` (lines 16-17). We then pass the `endpoint`
    to the `resolve` member function of the `resolver` (line 19), and iterate through
    the results. The iterator in this case points to `boost::asio::ip::tcp::query`
    objects, and we print the host and service name for each, using the appropriate
    member functions (lines 22-23).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从命令行传递IP地址和端口号给程序，然后使用它们构造`endpoint`（第16-17行）。然后我们将`endpoint`传递给`resolver`的`resolve`成员函数（第19行），并遍历结果。在这种情况下，迭代器指向`boost::asio::ip::tcp::query`对象，我们使用适当的成员函数打印每个对象的主机和服务名称（第22-23行）。
- en: Buffers
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓冲区
- en: 'Data is sent or received over the network as a byte stream. A contiguous byte
    stream can be represented using a pair of values: the starting address of the
    sequence and the number of bytes in the sequence. Boost Asio provides two abstractions
    for such sequences, `boost::asio::const_buffer` and `boost::asio::mutable_buffer`.
    The `const_buffer` type represents a read-only sequence that is typically used
    as a data source when sending data over the network. The `mutable_buffer` represents
    a read-write sequence that is used when you need to add or update data in your
    buffer, for example, when you receive data from a remote host:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 数据作为字节流通过网络发送或接收。一个连续的字节流可以用一对值来表示：序列的起始地址和序列中的字节数。Boost Asio提供了两种用于这种序列的抽象，`boost::asio::const_buffer`和`boost::asio::mutable_buffer`。`const_buffer`类型表示一个只读序列，通常用作发送数据时的数据源。`mutable_buffer`表示一个读写序列，当您需要在缓冲区中添加或更新数据时使用，例如当您从远程主机接收数据时：
- en: '**Listing 11.10: Using const_buffer and mutable_buffer**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.10：使用const_buffer和mutable_buffer**'
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In this example, we show how a char array is wrapped in a `mutable_buffer` and
    a `const_buffer` (lines 8-9). While constructing a buffer, you specify the starting
    address of the memory region and the length of the region in number of bytes.
    A `const char` array can only be wrapped in a `const_buffer`, not in a `mutable_buffer`.
    These buffer wrappers *do not* allocate storage, manage any heap-allocated memory,
    or perform any data copying.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们展示了如何将char数组包装在`mutable_buffer`和`const_buffer`中（第8-9行）。在构造缓冲区时，您需要指定内存区域的起始地址和区域的字节数。`const
    char`数组只能被包装在`const_buffer`中，而不能被包装在`mutable_buffer`中。这些缓冲区包装器*不*分配存储空间，不管理任何堆分配的内存，也不执行任何数据复制。
- en: The function `boost::asio::buffer_size` returns the length of the buffer in
    bytes (lines 11-12). This is the length you passed while constructing the buffer,
    and it is not dependent on the data present in the buffer. Default-initialized
    buffers have zero length.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`boost::asio::buffer_size`返回缓冲区的字节长度（第11-12行）。这是您在构造缓冲区时传递的长度，它不依赖于缓冲区中的数据。默认初始化的缓冲区长度为零。
- en: 'The function template `boost::asio::buffer_cast<>` is used to obtain a pointer
    to the underlying byte array of a buffer (lines 14-15). Note that we get a compilation
    error if we try to use `buffer_cast` to get a mutable array from a `const_buffer`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 函数模板`boost::asio::buffer_cast<>`用于获取缓冲区的基础字节数组的指针（第14-15行）。请注意，如果我们尝试使用`buffer_cast`从`const_buffer`获取可变数组，将会得到编译错误：
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Finally, you can create a buffer from an offset into another buffer, using the
    `operator+` (line 19). The length of the resultant buffer would be less than that
    of the original buffer by the length of the offset (line 22).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以使用`operator+`从另一个缓冲区的偏移量创建一个缓冲区（第19行）。结果缓冲区的长度将比原始缓冲区的长度少偏移量的长度（第22行）。
- en: Buffer sequences for vectored I/O
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量I/O的缓冲区序列
- en: 'Sometimes, it is convenient to send data from a series of buffers or split
    the received data across a series of buffers. Calling network I/O functions once
    per sequence would be inefficient, because these calls ultimately translate to
    system calls and there is an overhead in making each such call. An alternative
    is to use network I/O functions that can process a **sequence of buffers** passed
    to it as an argument. This is often called **vectored I/O** or **gather-scatter
    I/O**. All of Boost Asio''s I/O functions deal in buffer sequences, and so they
    must be passed buffer sequences rather than single buffers. A valid buffer sequence
    for use with Asio I/O functions satisfies the following conditions:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，从一系列缓冲区发送数据或将接收到的数据分割到一系列缓冲区中是很方便的。每个序列调用一次网络I/O函数会很低效，因为这些调用最终会转换为系统调用，并且每次调用都会有开销。另一种选择是使用可以处理作为参数传递给它的缓冲区序列的网络I/O函数。这通常被称为**向量I/O**或**聚集-分散I/O**。Boost
    Asio的所有I/O函数都处理缓冲区序列，因此必须传递缓冲区序列而不是单个缓冲区。用于Asio I/O函数的有效缓冲区序列满足以下条件：
- en: Has a member function `begin` that returns a bidirectional iterator, which points
    to a `mutable_buffer` or `const_buffer`
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个返回双向迭代器的成员函数`begin`，该迭代器指向`mutable_buffer`或`const_buffer`
- en: Has a member function `end` that returns an iterator pointing to the end of
    the sequence
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个返回指向序列末尾的迭代器的成员函数`end`
- en: Is copyable
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可复制
- en: 'For a buffer sequence to be useful, it must either be a sequence of `const_buffer`s
    or a sequence of `mutable_buffer`s. Formally, these requirements are summarized
    in the **ConstBufferSequence** and **MutableBufferSequence** concepts. This is
    a slightly simplified set of conditions, but is good enough for our purposes.
    We can make such sequences using Standard Library containers, such as `std::vector`,
    `std::list`, and so on, as well as Boost containers. However, since we frequently
    deal with only a single buffer, Boost provides the `boost::asio::buffer` function
    that makes it easy to adapt a single buffer as a buffer sequence of length one.
    Here is a short example illustrating these ideas:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 要使缓冲区序列有用，它必须是`const_buffer`序列或`mutable_buffer`序列。形式上，这些要求总结在**ConstBufferSequence**和**MutableBufferSequence**概念中。这是一组稍微简化的条件，但对我们的目的来说已经足够了。我们可以使用标准库容器（如`std::vector`、`std::list`等）以及Boost容器来创建这样的序列。然而，由于我们经常只处理单个缓冲区，Boost提供了`boost::asio::buffer`函数，可以轻松地将单个缓冲区适配为长度为1的缓冲区序列。以下是一个简短的示例，说明了这些想法：
- en: '**Listing 11.11: Using buffers**'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.11：使用缓冲区**'
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In this example, we create a mutable buffer sequence as a `vector` of two `mutable_buffer`s
    (line 14). The two mutable buffers wrap a `vector` of `char`s (lines 16-17) and
    an array of `char`s (line 18). Using the `buffers_begin` (line 20) and `buffers_end`
    functions (line 21), we determine the entire range of bytes encapsulated by the
    buffer sequence `bufseq` and iterate through it, setting each byte to a random
    character (line 22). As these get written to the underlying vector or array, we
    construct strings using the underlying vector or array and print their contents
    (lines 27-28).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们创建一个`vector`的两个`mutable_buffer`的可变缓冲区序列（第14行）。这两个可变缓冲区包装了一个`char`的`vector`（第16-17行）和一个`char`的数组（第18行）。使用`buffers_begin`函数（第20行）和`buffers_end`函数（第21行），我们确定了缓冲区序列`bufseq`所封装的字节的整个范围，并遍历它，将每个字节设置为随机字符（第22行）。当这些写入底层的vector或数组时，我们使用底层的vector或数组构造字符串并打印它们的内容（第27-28行）。
- en: Synchronous and asynchronous communications
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同步和异步通信
- en: In the following sections, we put together our understanding of IP addresses,
    endpoints, sockets, buffers, and other Asio infrastructure we learned so far to
    write network client and server programs. Our examples use the **client-server
    model** of interaction, in which a **server** program services incoming requests,
    and a **client** program initiates such requests. Such clients are referred to
    as the **active endpoints**, while such servers are referred to as **passive endpoints**.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将整合我们迄今为止学到的IP地址、端点、套接字、缓冲区和其他Asio基础设施的理解，来编写网络客户端和服务器程序。我们的示例使用**客户端-服务器模型**进行交互，其中**服务器**程序服务于传入的请求，而**客户端**程序发起这些请求。这样的客户端被称为**主动端点**，而这样的服务器被称为**被动端点**。
- en: Clients and servers may communicate **synchronously**, blocking on each network
    I/O operation until the request has been handled by the underlying OS, and only
    then proceeding to the next step. Alternatively, they can use **asynchronous I/O**,
    initiating network I/O without waiting for them to complete, and being notified
    later upon their completion. With asynchronous I/O, unlike the synchronous case,
    programs do not wait idly if there are I/O operations to perform. Thus, asynchronous
    I/O scales better with larger numbers of peers and higher volumes of data. We
    will look at both synchronous and asynchronous models of communication. While
    the programming model for asynchronous interactions is event-driven and more complex,
    the use of Boost Asio coroutines can keep it very manageable. Before we write
    UDP and TCP servers, we will take a look at the Asio deadline timer to understand
    how we write synchronous and asynchronous logic using Asio.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端和服务器可以进行**同步**通信，即在每个网络I/O操作上阻塞，直到请求被底层操作系统处理，然后才继续下一步。或者，它们可以使用**异步I/O**，在不等待完成的情况下启动网络I/O，并在稍后被通知其完成。与同步情况不同，使用异步I/O时，程序不会在需要执行I/O操作时空闲等待。因此，异步I/O在具有更多对等方和更大数据量时具有更好的扩展性。我们将研究通信的同步和异步模型。虽然异步交互的编程模型是事件驱动的且更复杂，但使用Boost
    Asio协程可以使其非常易于管理。在编写UDP和TCP服务器之前，我们将看一下Asio截止时间定时器，以了解如何使用Asio编写同步和异步逻辑。
- en: Asio deadline timer
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Asio截止时间定时器
- en: 'Asio provides the `basic_deadline_timer` template, using which you can wait
    for a specific duration to elapse or for an absolute time point. The specialization
    `deadline_timer` is defined as:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Asio提供了`basic_deadline_timer`模板，使用它可以等待特定持续时间的过去或绝对时间点。特化的`deadline_timer`定义如下：
- en: '[PRE23]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It uses `boost::posix_time::ptime` and `boost::posix_time::time_duration` as
    the time point and duration type respectively. The following example illustrates
    how an application can use `deadline_timer` to wait for a duration to elapse:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用`boost::posix_time::ptime`和`boost::posix_time::time_duration`作为时间点和持续时间类型。下面的例子演示了一个应用程序如何使用`deadline_timer`等待一段时间：
- en: '**Listing 11.12: Waiting synchronously**'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.12：同步等待**'
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We create an object of `io_service` (line 6), which acts as the conduit for
    operations on the underlying OS. We create an instance of `deadline_timer` associated
    with the `io_service` (line 7). We specify a 5 second duration to wait for using
    the member function `expires_from_now` of `deadline_timer` (line 12). We then
    call the `wait` member function to block until the duration elapses. Notice that
    we do not need to call `run` on the `io_service` instance. We can instead use
    the `expires_at` member function to wait until a specific time point, as shown
    here:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个`io_service`对象（第6行），它作为底层操作的通道。我们创建了一个与`io_service`相关联的`deadline_timer`实例（第7行）。我们使用`deadline_timer`的`expires_from_now`成员函数指定了一个5秒的等待时间（第12行）。然后我们调用`wait`成员函数来阻塞直到时间到期。注意我们不需要在`io_service`实例上调用`run`。我们可以使用`expires_at`成员函数来等待到特定的时间点，就像这样：
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Sometimes, programs do not want to block waiting for the timer to go off, or
    in general, for any future event it is interested in. In the meantime, it can
    finish off other valuable work and therefore be more responsive than if it were
    to block, waiting on the event. Instead of blocking on an event, we just want
    to tell the timer to notify us when it goes off, and proceed to do other work
    meanwhile. For this purpose, we call the `async_wait` member function and pass
    it a *completion handler*. A completion handler is a function object we register
    using `async_wait` to be called once the timer expires:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，程序不想阻塞等待定时器触发，或者一般来说，不想阻塞等待它感兴趣的任何未来事件。与此同时，它可以完成其他有价值的工作，因此比起阻塞等待事件，它可以更加响应。我们不想在事件上阻塞，只是想告诉定时器在触发时通知我们，并且同时进行其他工作。为此，我们调用`async_wait`成员函数，并传递一个*完成处理程序*。完成处理程序是我们使用`async_wait`注册的函数对象，一旦定时器到期就会被调用：
- en: '**Listing 11.13: Waiting asynchronously**'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.13：异步等待**'
- en: '[PRE26]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'There are two essential changes in listing 11.13 compared to listing 11.12\.
    We call the `async_wait` member function of `deadline_timer` instead of `wait`,
    passing it a pointer to the completion handler function `on_timer_expiry`. We
    then call `run` on the `io_service` object. When we run this program, it prints
    the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 与清单11.12相比，清单11.13有两个关键的变化。我们调用`deadline_timer`的`async_wait`成员函数而不是`wait`，并传递一个指向完成处理程序函数`on_timer_expiry`的指针。然后在`io_service`对象上调用`run`。当我们运行这个程序时，它会打印以下内容：
- en: '[PRE27]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The call to `async_wait` does not block (line 24) and therefore the first two
    lines are printed in quick succession. Following this, the call to `run` (line
    27) blocks until the timer expires, and the completion handler for the timer is
    dispatched. Unless some error occurred, the completion handler prints `Timer expired`.
    Thus, there is a time lag between the appearance of the first two messages and
    the third message, which is from the completion handler.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`async_wait`不会阻塞（第24行），因此前两行消息会快速连续打印出来。随后，调用`run`（第27行）会阻塞直到定时器到期，并且定时器的完成处理程序被调度。除非发生了错误，否则完成处理程序会打印`Timer
    expired`。因此，第一和第二条消息出现与第三条消息之间存在时间差，第三条消息是完成处理程序的输出。
- en: Asynchronous logic using Asio coroutines
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Asio协程的异步逻辑
- en: 'The `async_wait` member function of `deadline_timer` initiates an asynchronous
    operation. Such a function returns before the operation it initiates is completed.
    It registers a completion handler, and the completion of the asynchronous event
    is notified to the program through a call to this handler. If we have to run such
    asynchronous operations in a sequence, the control flow becomes complex. For example,
    let us suppose we want to wait for 5 seconds, print `Hello`, then wait for 10
    more seconds, and finally, print `world`. Using synchronous `wait`, it is as easy
    as shown in the following snippet:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`deadline_timer`的`async_wait`成员函数启动了一个异步操作。这样的函数在启动的操作完成之前就返回了。它注册了一个完成处理程序，并且通过调用这个处理程序来通知程序异步事件的完成。如果我们需要按顺序运行这样的异步操作，控制流就会变得复杂。例如，假设我们想等待5秒，打印`Hello`，然后再等待10秒，最后打印`world`。使用同步的`wait`，就像下面的代码片段一样简单：'
- en: '[PRE28]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In many real-life scenarios, especially with network I/O, blocking on synchronous
    operations is just not an option. In such cases, the code becomes considerably
    more complex. Using `async_wait` as a model asynchronous operation, the following
    example illustrates the complexity of asynchronous code:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多现实场景中，特别是在网络I/O中，阻塞同步操作根本不是一个选择。在这种情况下，代码变得更加复杂。使用`async_wait`作为模型异步操作，下面的例子演示了异步代码的复杂性：
- en: '**Listing 11.14: Asynchronous operations**'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.14：异步操作**'
- en: '[PRE29]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The move from synchronous to asynchronous logic for the same functionality incurs
    more than double the lines of code and a complex control flow. We register the
    function `print_hello` (line 10) as the completion handler for the first 5-second
    wait (lines 22, 24). `print_hello` in turn starts a 10-second wait using the same
    timer, and registers the function `print_world` (line 6), as the completion handler
    for this wait (lines 14-15).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 将相同功能从同步逻辑转换为异步逻辑，代码行数超过两倍，控制流也变得复杂。我们将函数`print_hello`（第10行）注册为第一个5秒等待的完成处理程序（第22、24行）。`print_hello`又使用同一个定时器开始了一个10秒的等待，并将函数`print_world`（第6行）注册为这个等待的完成处理程序（第14-15行）。
- en: Notice that we use `boost::bind` to generate the completion handler for the
    first 5-second wait, passing the `timer` from the `main` function to the `print_hello`
    function. The `print_hello` function thus uses the same timer. Why did we need
    to do it this way? First of all, `print_hello` needs to use the same `io_service`
    instance to initiate the 10-second wait operation and the earlier 5-second wait.
    The `timer` instance refers to this `io_service` instance and is used by both
    completion handlers. Moreover, creating a local `deadline_timer` instance in `print_hello`
    would be problematic because `print_hello` would return before the timer would
    go off, and the local timer object would be destroyed, so it would never go off.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用`boost::bind`为第一个5秒等待生成完成处理程序，将`timer`从`main`函数传递给`print_hello`函数。因此，`print_hello`函数使用相同的计时器。为什么我们需要这样做呢？首先，`print_hello`需要使用相同的`io_service`实例来启动10秒等待操作和之前的5秒等待。`timer`实例引用了这个`io_service`实例，并且被两个完成处理程序使用。此外，在`print_hello`中创建一个本地的`deadline_timer`实例会有问题，因为`print_hello`会在计时器响起之前返回，并且本地计时器对象会被销毁，所以它永远不会响起。
- en: Example 11.14 illustrates the problem of *inversion of control flow*, which
    is a source of significant complexity in asynchronous programming models. We can
    no longer string together a sequence of statements, and assume that each initiates
    an operation only once the operation initiated by the preceding statement is completed—a
    safe assumption for the synchronous model. Instead, we depend on notifications
    from `io_service` to determine the right time to run the next operation. The logic
    is fragmented across functions, and any data that needs to be shared across these
    functions requires more effort to manage.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 示例11.14说明了*控制流反转*的问题，在异步编程模型中是一个重要的复杂性来源。我们不能再将一系列语句串在一起，并假设每个语句只有在前面的操作完成后才会启动一个操作——这对于同步模型是一个安全的假设。相反，我们依赖于`io_service`的通知来确定运行下一个操作的正确时间。逻辑在函数之间分散，需要更多的努力来管理需要在这些函数之间共享的任何数据。
- en: Asio simplifies asynchronous programming using a thin wrapper around the Boost
    Coroutine library. Like with Boost Coroutine, it is possible to use stackful as
    well as stackless coroutines. In this book, we only look at stackful coroutines.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Asio使用Boost Coroutine库的薄包装简化了异步编程。与Boost Coroutine一样，可以使用有栈和无栈协程。在本书中，我们只研究有栈协程。
- en: 'Using the `boost::asio::spawn` function template, we can launch tasks as coroutines.
    If a coroutine is dispatched and it calls an asynchronous function, the coroutine
    is suspended. Meanwhile, the `io_service` dispatches other tasks, including other
    coroutines. Once an asynchronous operation is completed, the coroutine that initiated
    it is resumed, and it proceeds to the next step. In the following listing, we
    rewrite listing 11.14 using coroutines:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`boost::asio::spawn`函数模板，我们可以启动任务作为协程。如果一个协程被调度并调用了一个异步函数，那么协程会被暂停。与此同时，`io_service`会调度其他任务，包括其他协程。一旦异步操作完成，启动它的协程会恢复，并继续下一步。在下面的清单中，我们使用协程重写清单11.14：
- en: '**Listing 11.15: Asynchronous programming using coroutines**'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.15：使用协程进行异步编程**'
- en: '[PRE30]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `wait_and_print` function is the coroutine and takes two arguments: an
    object of type `boost::asio::yield_context` and a reference to an `io_service`
    instance (line 7). `yield_context` is a thin wrapper around Boost Coroutine. We
    must use `boost::asio::spawn` to dispatch a coroutine, and such a coroutine must
    have the signature `void (boost::asio::yield_context)`. Thus, we adapt the `wait_and_print`
    function using `boost::bind` to make it compatible with the coroutine signature
    expected by `spawn`. We bind the second argument to a reference to the `io_service`
    instance (lines 24-26).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`wait_and_print`函数是协程，接受两个参数：一个`boost::asio::yield_context`类型的对象和一个`io_service`实例的引用（第7行）。`yield_context`是Boost
    Coroutine的薄包装。我们必须使用`boost::asio::spawn`来调度一个协程，这样一个协程必须具有`void (boost::asio::yield_context)`的签名。因此，我们使用`boost::bind`来使`wait_and_print`函数与`spawn`期望的协程签名兼容。我们将第二个参数绑定到`io_service`实例的引用（第24-26行）。'
- en: The `wait_and_print` coroutine creates a `deadline_timer` instance on the stack,
    and starts a 5-second asynchronous wait, passing its `yield_context` to the `async_wait`
    function in place of a completion handler. This suspends the `wait_and_print`
    coroutine, and it is resumed only once the wait is completed. In the meantime,
    other tasks if any can be processed from the `io_service` queue. Once the wait
    is over and `wait_and_print` is resumed, it prints `Hello` and starts a 10-second
    wait. Once again, the coroutine suspends, and it resumes only after the 10 seconds
    elapse, thereafter printing `world`. Coroutines make the asynchronous logic as
    simple and readable as the synchronous one, with very little overhead. In the
    following sections, we will use coroutines to write TCP and UDP servers.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`wait_and_print`协程在堆栈上创建一个`deadline_timer`实例，并开始一个5秒的异步等待，将其`yield_context`传递给`async_wait`函数，而不是完成处理程序。这会暂停`wait_and_print`协程，只有在等待完成后才会恢复。与此同时，如果有其他任务，可以从`io_service`队列中处理。等待结束并且`wait_and_print`恢复后，它打印`Hello`并开始等待10秒。协程再次暂停，只有在10秒后才会恢复，然后打印`world`。协程使异步逻辑与同步逻辑一样简单易读，开销很小。在接下来的章节中，我们将使用协程来编写TCP和UDP服务器。'
- en: UDP
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UDP
- en: The UDP I/O model is relatively simple and the distinction between client and
    server is blurred. For network I/O using UDP, we create a UDP socket, and use
    the `send_to` and `receive_from` functions to send datagrams to specific endpoints.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: UDP I/O模型相对简单，客户端和服务器之间的区别模糊不清。对于使用UDP的网络I/O，我们创建一个UDP套接字，并使用`send_to`和`receive_from`函数将数据报发送到特定的端点。
- en: Synchronous UDP client and server
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 同步UDP客户端和服务器
- en: In this section, we write a UDP client (listing 11.16) and a synchronous UDP
    server (listing 11.17). The UDP client tries to send some data to a UDP server
    on a given endpoint. The UDP server blocks waiting to receive data from one or
    more UDP clients. After sending data, the UDP client blocks waiting to receive
    a response from the server. The server, after receiving the data, sends some response
    back before proceeding to handle more incoming messages.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们编写了一个UDP客户端（清单11.16）和一个同步UDP服务器（清单11.17）。UDP客户端尝试向给定端点的UDP服务器发送一些数据。UDP服务器阻塞等待从一个或多个UDP客户端接收数据。发送数据后，UDP客户端阻塞等待从服务器接收响应。服务器在接收数据后，在继续处理更多传入消息之前发送一些响应。
- en: '**Listing 11.16: Synchronous UDP client**'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.16：同步UDP客户端**'
- en: '[PRE31]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We run the client by passing it the server hostname and the service (or port)
    to connect to on the command line. It resolves them to an endpoint (IP address
    and port number) for UDP (lines 13-17), creates a UDP socket for IPv4 (line 18),
    and calls the `send_to` member function on it. We pass to `send_to`, a `const_buffer`
    containing the data to be sent and the destination endpoint (line 23).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过命令行向客户端传递服务器主机名和要连接的服务（或端口）。它们会解析为UDP的端点（IP地址和端口号）（第13-17行），为IPv4创建一个UDP套接字（第18行），并在其上调用`send_to`成员函数。我们传递给`send_to`一个包含要发送的数据和目标端点的`const_buffer`（第23行）。
- en: Each and every program that performs network I/O using Asio uses an *I/O service*,
    which is an instance of the type `boost::asio::io_service`. We have already seen
    `io_service` in action as a task manager. But the primary role of the I/O service
    is that of an interface for operations on the underlying operating system. Asio
    programs use *I/O objects* that are responsible for initiating I/O operations.
    Sockets, for example, are I/O objects.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 每个使用Asio执行网络I/O的程序都使用*I/O服务*，它是类型`boost::asio::io_service`的实例。我们已经看到`io_service`作为任务管理器的作用。但是I/O服务的主要作用是作为底层操作的接口。Asio程序使用负责启动I/O操作的*I/O对象*。例如，套接字就是I/O对象。
- en: We call the `send_to` member function on the UDP socket to send a predefined
    message string to the server (line 23). Note that we wrap the message array in
    a buffer sequence of length one constructed using the `boost::asio::buffer` function,
    as shown earlier in this chapter, in the section on buffers. Once `send_to` completes,
    the client calls `recv_from` on the same socket, passing a mutable buffer sequence
    constructed out of a writable character array using `boost::asio::buffer` (lines
    25-26). The second argument to `receive_from` is a non-const reference to a `boost::asio::ip::udp::endpoint`
    object. When `receive_from` returns, this object contains the address and port
    number of the remote endpoint, which sent the message (lines 28-29).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用UDP套接字的`send_to`成员函数，向服务器发送预定义的消息字符串（第23行）。请注意，我们将消息数组包装在长度为1的缓冲区序列中，该序列使用`boost::asio::buffer`函数构造，如本章前面所示，在缓冲区部分。一旦`send_to`完成，客户端在同一套接字上调用`recv_from`，传递一个可变的缓冲区序列，该序列由可写字符数组使用`boost::asio::buffer`构造（第25-26行）。`receive_from`的第二个参数是对`boost::asio::ip::udp::endpoint`对象的非const引用。当`receive_from`返回时，该对象包含发送消息的远程端点的地址和端口号（第28-29行）。
- en: The calls to `send_to` and `receive_from` are **blocking calls**. The call to
    `send_to` does not return until the buffer passed to it has been written to the
    underlying UDP buffer in the system. Dispatching the UDP buffer over the wire
    to the server may happen later. The call to `receive_from` does not return until
    some data has been received.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`send_to`和`receive_from`的是**阻塞调用**。调用`send_to`不会返回，直到传递给它的缓冲区已经被写入系统中的底层UDP缓冲区。将UDP缓冲区通过网络发送到服务器可能会在稍后发生。调用`receive_from`不会返回，直到接收到一些数据为止。
- en: 'We can use a single UDP socket to send data to multiple other endpoints, and
    we can receive data from multiple other endpoints on a single socket. Thus, each
    call to `send_to` takes the destination endpoint as input. Likewise, each call
    to `receive_from` takes a non-const reference to an endpoint, and on return, sets
    it to the sender''s endpoint. We will now write the corresponding UDP server using
    Asio:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用单个UDP套接字向多个其他端点发送数据，并且可以在单个套接字上从多个其他端点接收数据。因此，每次调用`send_to`都将目标端点作为输入。同样，每次调用`receive_from`都会使用非const引用传递一个端点，并在返回时将其设置为发送方的端点。现在我们将使用Asio编写相应的UDP服务器：
- en: '**Listing 11.17: Synchronous UDP server**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.17：同步UDP服务器**'
- en: '[PRE32]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The synchronous UDP server creates a single UDP endpoint of type `boost::asio::ip::udp::endpoint`
    on the port 55000, keeping the address unspecified (line 14). Notice that we use
    a two-argument `endpoint` constructor, which takes *the protocol* and port as
    arguments. The server creates a single UDP socket of type `boost::asio::ip::udp::socket`
    for this endpoint (line 15), and spins in a loop, calling `receive_from` on the
    socket per iteration, waiting until a client sends some data. The data is received
    in a `char` array called `msg`, which is passed to `receive_from` wrapped in a
    mutable buffer sequence of length one. The call to `receive_from` returns the
    number of bytes received, which is used to add a terminating null character in
    `msg` so that it can be used like a C-style string (line 22). In general, UDP
    presents the incoming data as a message containing a sequence of bytes and its
    interpretation is left to the application. Each time the server receives data
    from a client, it echoes back the data sent, preceded by a fixed greeting string.
    It does so by calling the `send_to` member function on the socket twice, passing
    the buffer to send, and the endpoint of the recipient (lines 26-27, 28).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 同步UDP服务器在端口55000上创建一个`boost::asio::ip::udp::endpoint`类型的单个UDP端点，保持地址未指定（第14行）。请注意，我们使用了一个两参数的`endpoint`构造函数，它将*协议*和端口作为参数。服务器为此端点创建一个`boost::asio::ip::udp::socket`类型的单个UDP套接字（第15行），并在循环中旋转，每次迭代调用套接字上的`receive_from`，等待直到客户端发送一些数据。数据以一个名为`msg`的`char`数组接收，该数组被包装在长度为一的可变缓冲序列中传递给`receive_from`。`receive_from`的调用返回接收到的字节数，用于在`msg`中添加一个终止空字符，以便它可以像C风格的字符串一样使用（第22行）。一般来说，UDP将传入的数据呈现为包含一系列字节的消息，其解释留给应用程序。每当服务器从客户端接收数据时，它会将发送的数据回显，先前由一个固定的问候字符串。它通过在套接字上两次调用`send_to`成员函数来实现，传递要发送的缓冲区和接收方的端点（第26-27行，28行）。
- en: The calls to `send_to` and `receive_from` are synchronous and return only once
    the data is passed completely to the OS (`send_to`) or received completely by
    the application (`receive_from`). If many instances of the client send messages
    to the server at the same time, the server can still only process one message
    at a time, and therefore the clients queue up waiting for a response. Of course,
    if the clients did not wait for a response, they could all have sent messages
    and exited but the messages would still be received by the server serially.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对`send_to`和`receive_from`的调用是同步的，只有当数据完全传递给操作系统（`send_to`）或应用程序完全接收到数据（`receive_from`）时才会返回。如果许多客户端实例同时向服务器发送消息，服务器仍然只能一次处理一条消息，因此客户端排队等待响应。当然，如果客户端不等待响应，它们可以全部发送消息并退出，但消息仍然会按顺序被服务器接收。
- en: Asynchronous UDP server
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异步UDP服务器
- en: An asynchronous version of the UDP server can significantly improve the responsiveness
    of the server. A traditional asynchronous model can entail a more complex programming
    model, but coroutines can significantly improve the situation.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: UDP服务器的异步版本可以显著提高服务器的响应性。传统的异步模型可能涉及更复杂的编程模型，但协程可以显著改善情况。
- en: Asynchronous UDP server using completion handler chains
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用完成处理程序链的异步UDP服务器
- en: 'For asynchronous communication, we use the `async_receive_from` and `async_send_to`
    member functions of `socket`. These functions do not wait for the I/O request
    to be handled by the operating system but return immediately. They are passed
    a function object, which is to be called when the underlying operation is completed.
    This function object is queued in the task queue of the `io_service` and is dispatched
    when the actual operation on the operating system returns:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于异步通信，我们使用`socket`的`async_receive_from`和`async_send_to`成员函数。这些函数不会等待I/O请求被操作系统处理，而是立即返回。它们被传递一个函数对象，当底层操作完成时将被调用。这个函数对象被排队在`io_service`的任务队列中，在操作系统上的实际操作返回时被调度。
- en: '[PRE33]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The signature for both the read handler passed to `async_receive_from` and
    the write handler passed to `async_send_to` is as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给`async_receive_from`的读处理程序和传递给`async_send_to`的写处理程序的签名如下：
- en: '[PRE34]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The handlers expect to be passed a non-const reference to an `error_code` object,
    indicating the status of the completed operation and the number of bytes read
    or written. The handlers can call other asynchronous I/O operations and register
    other handlers. Thus, the entire I/O operation is defined in terms of a chain
    of handlers. We now look at a program for an asynchronous UDP server:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 处理程序期望传递一个非const引用给`error_code`对象，指示已完成操作的状态和读取或写入的字节数。处理程序可以调用其他异步I/O操作并注册其他处理程序。因此，整个I/O操作是以一系列处理程序的链条来定义的。现在我们来看一个异步UDP服务器的程序：
- en: '**Listing 11.18: Asynchronous UDP server**'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.18：异步UDP服务器**'
- en: '[PRE35]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The UDP server is encapsulated in the class `UDPAsyncServer` (line 8). To start
    the server, we first create the obligatory `io_service` object (line 42), followed
    by an instance of `UDPAsyncServer` (line 43) that is passed the `io_service` instance
    and the port number it should use. Finally, a call to the `run` member function
    of `io_service` starts the processing of incoming requests (line 44). So how does
    `UDPAsyncServer` work?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: UDP服务器封装在`UDPAsyncServer`类中（第8行）。要启动服务器，我们首先创建必需的`io_service`对象（第42行），然后创建一个`UDPAsyncServer`实例（第43行），该实例传递了`io_service`实例和应该使用的端口号。最后，调用`io_service`的`run`成员函数开始处理传入的请求（第44行）。那么`UDPAsyncServer`是如何工作的呢？
- en: The constructor of `UDPAsyncServer` initializes the member UDP `socket` with
    a local endpoint (lines 12-13). It then calls the member function `waitForReceive`
    (line 14), which in turn calls `async_receive_from` on the socket (line 18), to
    start waiting for any incoming messages. We call `async_receive_from,` passing
    a mutable buffer made from the `buffer` member variable (line 17), a non-const
    reference to the `remote_peer` member variable (line 18), and a lambda expression
    that defines a completion handler for the receive operation (lines 19-31). `async_receive_from`
    initiates an I/O operation, adds the handler to the task queue in `io_service`,
    and returns. The call to `run` on the `io_service` (line 43) blocks as long as
    there are I/O tasks in the queue. When a UDP message comes along, the data is
    received by the OS, and it invokes the handler to take further action. To understand
    how the UDP server keeps handling more and more messages ad infinitum, we need
    to understand what the handlers do.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`UDPAsyncServer`的构造函数使用本地端点初始化了UDP `socket`成员（第12-13行）。然后调用成员函数`waitForReceive`（第14行），该函数又在套接字上调用`async_receive_from`（第18行），开始等待任何传入的消息。我们调用`async_receive_from`，传递了从`buffer`成员变量制作的可变缓冲区（第17行），对`remote_peer`成员变量的非const引用（第18行），以及一个定义接收操作完成处理程序的lambda表达式（第19-31行）。`async_receive_from`启动了一个I/O操作，将处理程序添加到`io_service`的任务队列中，然后返回。对`io_service`的`run`调用（第43行）会阻塞，直到队列中有I/O任务。当UDP消息到来时，数据被操作系统接收，并调用处理程序来采取进一步的操作。要理解UDP服务器如何无限处理更多消息，我们需要了解处理程序的作用。'
- en: The *receive handler* is invoked when the server receives a message. It prints
    the message received and the details of the remote sender (lines 22-23), and then
    issues a call to `waitForReceive`, thus restarting the receive operation. It then
    sends a message `hello from server` (line 21) back to the sender identified by
    the `remote_peer` member variable. It does so by calling the `async_send_to` member
    function of the UDP `socket`, passing the message buffer (line 27), the destination
    endpoint (line 28), and another handler in the form of a lambda (lines 29-32),
    which does nothing.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接收处理程序在服务器接收到消息时被调用。它打印接收到的消息和远程发送者的详细信息（第22-23行），然后发出对`waitForReceive`的调用，从而重新启动接收操作。然后它发送一条消息`hello
    from server`（第21行）回到由`remote_peer`成员变量标识的发送者。它通过调用UDP `socket`的`async_send_to`成员函数来实现这一点，传递消息缓冲区（第27行），目标端点（第28行），以及另一个以lambda形式的处理程序（第29-32行），该处理程序什么也不做。
- en: Note that we capture the `this` pointer in the lambdas to be able to access
    the member variables from the surrounding scope (line 20, 29). Also, neither handler
    does error checking using the `error_code` argument, which is a must in real-world
    software.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在lambda中捕获了`this`指针，以便能够从周围范围访问成员变量（第20行，29行）。另外，处理程序都没有使用`error_code`参数进行错误检查，这在现实世界的软件中是必须的。
- en: Asynchronous UDP server using coroutines
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用协程的异步UDP服务器
- en: 'Handler chaining fragments the logic across a set of handlers and sharing state
    across handlers becomes particularly complex. It is the price for better performance,
    but it is a price we can avoid, as we saw earlier using Asio coroutines to handle
    asynchronous waits on `boost::asio::deadline_timer` in listing 11.15\. We will
    now use Asio coroutines to write an asynchronous UDP server:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 处理程序链接将逻辑分散到一组处理程序中，并且在处理程序之间共享状态变得特别复杂。这是为了更好的性能，但我们可以避免这个代价，就像我们在列表11.15中使用Asio协程处理`boost::asio::deadline_timer`上的异步等待一样。现在我们将使用Asio协程来编写一个异步UDP服务器：
- en: '**Listing 11.19: Asynchronous UDP server using Asio coroutines**'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表11.19：使用Asio协程的异步UDP服务器**'
- en: '[PRE36]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'With the use of coroutines, the structure of the asynchronous UDP server changes
    considerably from listing 11.18 and is closer to the synchronous model of listing
    11.17\. The function `udp_server` contains the core logic for the UDP server (line
    23). It is meant to be used as a coroutine; hence, one of its arguments is of
    type `boost::asio::yield_context` (line 23). It takes two additional arguments:
    a reference to an `io_service` instance (line 24) and the UDP server port (line
    25).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用协程，异步UDP服务器的结构与列表11.18有了相当大的变化，并且更接近列表11.17的同步模型。函数`udp_server`包含了UDP服务器的核心逻辑（第23行）。它被设计为协程使用，因此它的一个参数是`boost::asio::yield_context`类型（第23行）。它还接受两个额外的参数：对`io_service`实例的引用（第24行）和UDP服务器端口（第25行）。
- en: In the main function, we create an instance of `io_service` (line 48), and then
    add a task to run `udp_server` as a coroutine, using the `boost::asio::spawn`
    function template (lines 49-50). We bind the service and port arguments of `udp_server`
    appropriately. We then call `run` on the `io_service` instance to start processing
    I/O operations. The call to `run` dispatches the `udp_server` coroutine (line
    51).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在主函数中，我们创建了一个`io_service`实例（第48行），然后添加一个任务以将`udp_server`作为协程运行，使用`boost::asio::spawn`函数模板（第49-50行）。我们适当地绑定了`udp_server`的服务和端口参数。然后我们调用`io_service`实例上的`run`来开始处理I/O操作。对`run`的调用会派发`udp_server`协程（第51行）。
- en: The `udp_server` coroutine creates a UDP socket associated with the unspecified
    IPv4 address (0.0.0.0) and the specific port passed as an argument (lines 27-29).
    The socket is wrapped in a `shared_ptr`, the reasons for which will become clear
    in a bit. There are additional variables on the coroutine stack to hold the data
    received from clients (line 31) and to identify the client endpoint (line 32).
    The `udp_server` function then spins in a loop calling `async_receive_from` on
    the socket, passing the `yield_context` for the receive handler (lines 36-37).
    This suspends the execution of the `udp_server` coroutine until `async_receive_from`
    completes. In the meantime, the call to `run` resumes and processes other tasks
    if any. Once a call to `async_receive_from` function completes, the `udp_server`
    coroutine resumes execution and proceeds to the next iteration of its loop.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`udp_server` 协程创建一个与未指定的 IPv4 地址（0.0.0.0）和作为参数传递的特定端口相关联的 UDP 套接字（第 27-29 行）。
    套接字被包装在 `shared_ptr` 中，稍后将清楚其原因。 协程堆栈上有额外的变量来保存从客户端接收的数据（第 31 行）并标识客户端端点（第 32
    行）。 `udp_server` 函数然后在循环中旋转，调用套接字上的 `async_receive_from`，传递接收处理程序的 `yield_context`（第
    36-37 行）。 这会暂停 `udp_server` 协程的执行，直到 `async_receive_from` 完成。 与此同时，对 `run` 的调用会恢复并处理其他任务（如果有）。
    一旦调用 `async_receive_from` 函数完成，`udp_server` 协程将恢复执行并继续进行其循环的下一次迭代。'
- en: For each completed receive operation, `udp_server` sends a fixed greeting string
    ("Hello from server") in response to the client. The task to send this greeting
    is also encapsulated in a coroutine, `udp_send_to` (line 14), which the `udp_server`
    coroutine adds to the task queue using `spawn` (line 40). We pass the UDP socket
    and the endpoint identifying the client as arguments to this coroutine. Notice
    that the local variable called `remote_peer` is passed by value to the `udp_send_to`
    coroutine (line 42). This is used inside `udp_send_to`, as an argument to `async_send_to`,
    to specify the recipient of the response (lines 19-20). We pass a copy rather
    than a reference to `remote_peer` because when the call to `async_send_to` is
    issued, another call to `async_receive_from` can be active and can overwrite the
    `remote_peer` object, before it is used by `async_send_to`. We also pass the socket
    wrapped in a `shared_ptr`. Sockets are not copyable unlike endpoints. If the socket
    object was on automatic storage in the `udp_server` function, and `udp_server`
    exited while there were still a pending `udp_send_to` task, the reference to the
    socket inside `udp_send_to` would be invalid and possibly lead to crashes. For
    this reason, the `shared_ptr` wrapper is the correct choice.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个完成的接收操作，`udp_server` 都会发送一个固定的问候字符串（“来自服务器的问候”）作为对客户端的响应。 发送这个问候的任务也封装在一个协程中，即
    `udp_send_to`（第 14 行），`udp_server` 协程使用 `spawn`（第 40 行）将其添加到任务队列中。 我们将 UDP 套接字和标识客户端的端点作为参数传递给这个协程。
    请注意，称为 `remote_peer` 的局部变量被按值传递给 `udp_send_to` 协程（第 42 行）。 这在 `udp_send_to` 中被使用，作为
    `async_send_to` 的参数，用于指定响应的接收者（第 19-20 行）。 我们传递副本而不是引用给 `remote_peer`，因为当发出对 `async_send_to`
    的调用时，另一个对 `async_receive_from` 的调用可能是活动的，并且可能在 `async_send_to` 使用之前覆盖 `remote_peer`
    对象。 我们还传递了包装在 `shared_ptr` 中的套接字。 套接字不可复制，不像端点。 如果套接字对象在 `udp_server` 函数中的自动存储中，并且在仍有待处理的
    `udp_send_to` 任务时 `udp_server` 退出，那么 `udp_send_to` 中的套接字引用将无效，并可能导致崩溃。 出于这个原因，`shared_ptr`
    包装器是正确的选择。
- en: If you noticed, the handler to `async_receive_from` is written as `yield[ec]`
    (line 37). The `yield_context` class has an overloaded subscript operator using
    which we can specify a mutable reference to a variable of type `error_code`. When
    the asynchronous operation completes, the variable passed as the argument of the
    subscript operator is set to the error code if any.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您注意到，对 `async_receive_from` 的处理程序写为 `yield[ec]`（第 37 行）。 `yield_context` 类具有重载的下标运算符，我们可以使用它来指定对
    `error_code` 类型的变量的可变引用。 当异步操作完成时，作为下标运算符参数传递的变量将设置为错误代码（如果有）。
- en: Tip
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Prefer using coroutines over handler-chaining, when writing asynchronous servers.
    Coroutines enable simpler code and a more intuitive control flow.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写异步服务器时，更倾向于使用协程而不是处理程序链。 协程使代码更简单，控制流更直观。
- en: Performance and concurrency
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能和并发
- en: We claimed that the asynchronous mode of communication improves responsiveness
    of the server. Let us understand exactly what factors contribute to this improvement.
    In the synchronous model of listing 11.17, a call to `receive_from` could not
    be issued unless the `send_to` function returned. In the asynchronous code of
    listing 11.18, `waitForReceive` is called as soon as a message is received and
    consumed (lines 23-25), and it does not wait for the `async_send_to` to complete.
    Likewise, in listing 11.19 which illustrates the use of coroutines in asynchronous
    models, coroutines help suspend a function waiting for an asynchronous I/O operation
    to complete, and to continue processing other tasks in the queue meanwhile. This
    is the principal source of improvement in the responsiveness of the asynchronous
    servers.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声称异步通信模式提高了服务器的响应性。 让我们确切地了解哪些因素导致了这种改进。 在列表 11.17 的同步模型中，除非 `send_to` 函数返回，否则无法发出对
    `receive_from` 的调用。 在列表 11.18 的异步代码中，一旦接收并消耗了消息，就会立即调用 `waitForReceive`（第 23-25
    行），它不会等待 `async_send_to` 完成。 同样，在列表 11.19 中，它展示了在异步模型中使用协程，协程帮助暂停等待异步 I/O 操作完成的函数，并同时继续处理队列中的其他任务。
    这是异步服务器响应性改进的主要来源。
- en: It is worth noting that in listing 11.18, all I/O happens on a single thread.
    This means that at any given point in time, our program handles only one incoming
    UDP message. This allows us to reuse the `buffer` and `remote_peer` member variables,
    without worrying about synchronization. We must still ensure that we print the
    received buffer (lines 22-23) before calling `waitForReceive` again (line 24).
    If we inverted that order, the buffer could potentially be overwritten by a new
    incoming message before it could be printed.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在列表11.18中，所有I/O都在单个线程上进行。这意味着在任何给定时间点，我们的程序只处理一个传入的UDP消息。这使我们能够重用`buffer`和`remote_peer`成员变量，而不必担心同步。我们仍然必须确保在再次调用`waitForReceive`之前打印接收到的缓冲区（第22-23行）。如果我们颠倒了顺序，缓冲区可能会在打印之前被新的传入消息覆盖。
- en: 'Consider what would have happened if we called `waitForReceive` inside the
    receive handler rather than the send handler like this:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下，如果我们像这样在接收处理程序中调用`waitForReceive`而不是发送处理程序中：
- en: '[PRE37]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In this case, the receive would be started only after the send completed; so
    even with asynchronous calls it would be no better than the synchronous example
    in listing 11.17.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，接收将仅在发送完成后开始；因此，即使使用异步调用，它也不会比列表11.17中的同步示例更好。
- en: 'In listing 11.18, we do not need the buffer received from the remote peer while
    sending content back, so we do not need to hold on to that buffer till the send
    is complete. This allows us to start the asynchronous receive (line 24) without
    waiting for the send to complete. The receive can complete first and overwrite
    the buffer, but as long as the send operation does not use the buffer, everything
    is fine. Too often in the real world, this is not the case, so let us see how
    to fix this without delaying the receive till after the send. Here is a modified
    implementation of the handlers:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表11.18中，我们在发送内容回来时不需要来自远程对等方的缓冲区，因此我们不需要在发送完成之前保留该缓冲区。这使我们能够在不等待发送完成的情况下开始异步接收（第24行）。接收可能会首先完成并覆盖缓冲区，但只要发送操作不使用缓冲区，一切都没问题。在现实世界中，这种情况经常发生，因此让我们看看如何在不延迟接收直到发送之后的情况下解决这个问题。以下是处理程序的修改实现：
- en: '[PRE38]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now, instead of relying on a buffer that is a shared member variable, we allocate
    a buffer for receiving each new message (line 18). This obviates the need for
    the `buffer` member variable in listing 11.18\. We use the `boost::shared_array`
    wrapper because this buffer needs to be passed from the `waitForReceive` call
    to the receive handler and further; it should be released only when the last reference
    to it is gone. Likewise, we remove the `remote_peer` member variable that represented
    the remote endpoint, and use a `shared_ptr`-wrapped endpoint for each new request.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不再依赖于一个共享的成员变量作为缓冲区，而是为每个新消息分配一个接收缓冲区（第18行）。这消除了列表11.18中`buffer`成员变量的需要。我们使用`boost::shared_array`包装器，因为这个缓冲区需要从`waitForReceive`调用传递到接收处理程序，而且只有在最后一个引用消失时才应该释放它。同样，我们移除了代表远程端点的`remote_peer`成员变量，并为每个新请求使用了一个`shared_ptr`包装的端点。
- en: We pass the underlying array to `async_receive_from` (line 21), and make sure
    it survives long enough by capturing its `shared_array` wrapper in the completion
    handler for `async_receive_from` (line 23). For the same reason, we also capture
    the endpoint wrapper `epPtr`. The receive handler calls `waitForReceive` (line
    25), and then prints the message received from the client, prefixed with the thread
    ID of the current thread (with an eye on the future). It then calls `async_send_to`,
    passing the buffer received instead of some fixed message (line 34). Once again,
    we need to ensure that the buffer and remote endpoint survive till the send completes;
    so we capture the `shared_array` wrapper of the buffer and the `shared_ptr` wrapper
    of the remote endpoint in the send completion handler (line 36).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将底层数组传递给`async_receive_from`（第21行），并通过在`async_receive_from`的完成处理程序中捕获其`shared_array`包装器（第23行）来确保它存活足够长的时间。出于同样的原因，我们还捕获端点包装器`epPtr`。接收处理程序调用`waitForReceive`（第25行），然后打印从客户端接收到的消息，并在当前线程的线程ID前加上前缀（考虑未来）。然后它调用`async_send_to`，传递接收到的缓冲区而不是一些固定的消息（第34行）。再一次，我们需要确保缓冲区和远程端点在发送完成之前存活；因此，我们在发送完成处理程序中捕获了缓冲区的`shared_array`包装器和远程端点的`shared_ptr`包装器（第36行）。
- en: 'The changes for the coroutine-based asynchronous UDP server (listing 11.19)
    are on the same lines:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 基于协程的异步UDP服务器的更改（列表11.19）也是在同样的基础上进行的。
- en: '[PRE39]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As the data received from the client needs to be echoed back, the `udp_send_to`
    coroutine must have access to it. Thus, it takes the buffer containing the received
    data and the number of bytes read as arguments (line 17). In order to make sure
    that this data is not overwritten by a subsequent receive, we must allocate buffers
    for receiving the data in each iteration of the loop in `udp_server` (line 39).
    We pass this buffer, and also the number of bytes read as returned by `async_receive_from`
    (line 40) to `udp_send_to` (line 47). With these changes, our asynchronous UDP
    servers can now maintain the context of each incoming request until it has responded
    to that peer, without the need to delay the handling of newer requests.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要将从客户端接收的数据回显回去，`udp_send_to`协程必须访问它。因此，它将包含接收到的数据的缓冲区和读取的字节数作为参数（第17行）。为了确保这些数据不会被后续接收覆盖，我们必须在`udp_server`循环的每次迭代中为接收数据分配缓冲区（第39行）。我们将这个缓冲区，以及`async_receive_from`返回的读取的字节数（第40行），传递给`udp_send_to`（第47行）。通过这些更改，我们的异步UDP服务器现在可以在响应对等方之前保持每个传入请求的上下文，而无需延迟处理新请求的需要。
- en: 'These changes also make the handlers thread-safe because essentially, we removed
    any shared data across handlers. While the `io_service` is still shared, it is
    a thread-safe object. We can easily turn the UDP server into a multithreaded server.
    Here is how we do this:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更改还使处理程序线程安全，因为实质上，我们删除了处理程序之间的任何共享数据。虽然`io_service`仍然是共享的，但它是一个线程安全的对象。我们可以很容易地将UDP服务器转换为多线程服务器。下面是我们如何做到这一点：
- en: '[PRE40]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This would create four worker threads that handle incoming UDP messages concurrently.
    The same would work with coroutines.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建四个处理传入UDP消息的工作线程。使用协程也可以实现相同的功能。
- en: TCP
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TCP
- en: In terms of network I/O, the programming model for UDP is about as simple as
    it gets—you either send a message, or receive a message, or do both. TCP is a
    fairly complex beast in comparison and its interaction model has a few additional
    details to understand.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络I/O方面，UDP的编程模型非常简单——你要么发送消息，要么接收消息，要么两者都做。相比之下，TCP是一个相当复杂的东西，它的交互模型有一些额外的细节需要理解。
- en: In addition to reliability guarantees, TCP implements several nifty algorithms
    to ensure that an overeager sender does not swamp a relatively slow receiver with
    lots of data (**flow control**), and all senders get a fair share of the network
    bandwidth (**congestion control**). There is a fair amount of computation that
    happens at the TCP layer for all of this, and TCP needs to maintain some state
    information to perform these computations. For this TCP uses **connections** between
    endpoints.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可靠性保证外，TCP还实现了几个巧妙的算法，以确保过于热心的发送方不会用大量数据淹没相对较慢的接收方（**流量控制**），并且所有发送方都能公平地分享网络带宽（**拥塞控制**）。TCP层需要进行相当多的计算来实现这一切，并且需要维护一些状态信息来执行这些计算。为此，TCP使用端点之间的**连接**。
- en: Establishing a TCP connection
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建立TCP连接
- en: A **TCP connection** consists of a pair of TCP sockets, potentially on different
    hosts connected by an IP network and some associated state data. Relevant connection
    state information is maintained at each end of the connection. A **TCP server**
    typically starts *listening for incoming connections* and is said to constitute
    the **passive end** of the connection. A **TCP client** initiates a request to
    connect to a TCP server and is said to be the *active end* of the connection.
    A well-defined mechanism known as the **TCP 3-way handshake** is used for establishing
    TCP connections. Similar mechanisms exist for coordinated connection termination.
    Connections can also be unilaterally reset or terminated, like in case of applications
    or hosts going down for various reasons or in case of an irrecoverable error of
    some sort.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**TCP连接**由一对TCP套接字组成，可能位于不同主机上，通过IP网络连接，并带有一些相关的状态数据。相关的连接状态信息在连接的每一端都得到维护。**TCP服务器**通常开始*监听传入连接*，被称为连接的**被动端**。**TCP客户端**发起连接到TCP服务器的请求，并被称为*主动端*。一个被称为**TCP三次握手**的明确定义的机制用于建立TCP连接。类似的机制也存在于协调连接终止。连接也可以被单方面重置或终止，比如在应用程序或主机因各种原因关闭或发生不可恢复的错误的情况下。'
- en: Client- and server-side calls
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 客户端和服务器端的调用
- en: 'For a TCP connection to be set up, a server process must be listening on an
    endpoint, and a client process must actively initiate a connection to that endpoint.
    The server performs the following steps:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 要建立TCP连接，服务器进程必须在一个端点上监听，并且客户端进程必须主动发起到该端点的连接。服务器执行以下步骤：
- en: Create a TCP listener socket.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个TCP监听套接字。
- en: Create a local endpoint for listening to incoming connections and bind the TCP
    listener socket to this endpoint.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为监听传入连接创建一个本地端点，并将TCP监听套接字绑定到该端点。
- en: Start listening for incoming connections on the listener.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始在监听器上监听传入的连接。
- en: Accept any incoming connections, and open a server-side endpoint (different
    from the listener endpoint) to serve that connection.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受任何传入的连接，并打开一个服务器端点（与监听端点不同）来服务该连接。
- en: Perform communication on that connection.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在该连接上进行通信。
- en: Handle the termination of the connection.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理连接的终止。
- en: Continue to listen for other incoming connections.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续监听其他传入的连接。
- en: 'The client in turn performs the following steps:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端依次执行以下步骤：
- en: Create a TCP socket and, optionally, bind it to a local endpoint.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个TCP套接字，并可选地将其绑定到本地端点。
- en: Connect to a remote endpoint serviced by a TCP server.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到由TCP服务器提供服务的远程端点。
- en: Once connection is established, perform communication on that connection.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦连接建立，就在该连接上进行通信。
- en: Handle termination of the connection.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理连接的终止。
- en: Synchronous TCP client and server
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 同步TCP客户端和服务器
- en: 'We will now write a TCP client which connects to a TCP server on a specified
    host and port, sends some text to the server, and then receives some messages
    back from the server:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将编写一个TCP客户端，它连接到指定主机和端口上的TCP服务器，向服务器发送一些文本，然后从服务器接收一些消息：
- en: '**Listing 11.20: Synchronous TCP client**'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.20：同步TCP客户端**'
- en: '[PRE41]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The TCP client resolves the host and port (or service name) passed to it on
    the command line (lines 16-19) and creates an endpoint representing the server
    to connect to (line 21). It creates an IPv4 socket (line 23) and calls the `connect`
    member function on it to initiate a connection to the remote server (line 25).
    The `connect` call blocks until a connection is established, or throws an exception
    if the attempt to connect fails. Once the connection is successful, we use the
    `boost::asio::write` function to send the text `Hello from client` to the server
    (lines 27-28). We call the `shutdown` member function of the socket with the argument
    `shutdown_send` (line 29) to close the write channel to the server. This shows
    up as an EOF on the server-side. We then use the `read` function to receive any
    message sent by the server (lines 33-34). Both `boost::asio::write` and `boost::asio::read`
    are blocking calls. The call to `write` would throw an exception on failure, for
    example, if the connection was reset or the send timed out because of a busy server.
    We call a non-throwing overload of `read`, and on failure, it sets the non-const
    reference to the error code we pass to it.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: TCP客户端解析传递给它的主机和端口（或服务名称）（第16-19行），并创建一个表示要连接的服务器的端点（第21行）。它创建一个IPv4套接字（第23行），并调用`connect`成员函数来启动与远程服务器的连接（第25行）。`connect`调用会阻塞，直到建立连接，或者如果连接尝试失败则抛出异常。连接成功后，我们使用`boost::asio::write`函数将文本“Hello
    from client”发送到服务器（第27-28行）。我们调用套接字的`shutdown`成员函数，参数为`shutdown_send`（第29行），关闭与服务器的写通道。这在服务器端显示为EOF。然后我们使用`read`函数接收服务器发送的任何消息（第33-34行）。`boost::asio::write`和`boost::asio::read`都是阻塞调用。对于失败的`write`调用会抛出异常，例如，如果连接被重置或由于服务器繁忙而发送超时。我们调用`read`的非抛出重载，在失败时，它会将我们传递给它的错误代码设置为非const引用。
- en: The function `boost::asio::read` tries to read as many bytes as it can to fill
    the buffer passed, and blocks until either all the data has arrived, or an end-of-file
    is received. Although an end-of-file is flagged as an error condition by `read`,
    it could simply indicate that the server was done sending data, and we would be
    interested in whatever data was received. For this reason, we specifically use
    a non-throwing overload of `read`, and in case an error was set in the `error_code`
    reference, we distinguish between end-of-file and other errors (line 35). For
    the same reason, we called `shutdown` to close the write channel on this connection
    (line 29) so that the server did not wait for more input.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`boost::asio::read`尝试读取尽可能多的字节以填充传递的缓冲区，并阻塞，直到所有数据到达或接收到文件结束符。虽然文件结束符被`read`标记为错误条件，但它可能只是表示服务器已经完成发送数据，我们对接收到的任何数据感兴趣。因此，我们特别使用`read`的非抛出重载，并在`error_code`引用中设置错误时，区分文件结束符和其他错误（第35行）。出于同样的原因，我们调用`shutdown`关闭此连接的写通道（第29行），以便服务器不等待更多输入。
- en: Tip
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Unlike UDP, TCP is stream-oriented and does not define message boundaries. An
    application must define its own mechanism to identify message boundaries. Some
    strategies include prefixing the length of the message to the message, using character
    sequences as message end-markers, or using messages of a fixed length. In the
    examples in this book, we use the `shutdown` member function of `tcp::socket`,
    which causes an end-of-file to be read by the receiver, indicating that we are
    done sending messages. This keeps the examples simple, but in practice, this is
    not the most flexible strategy.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 与UDP不同，TCP是面向流的，并且不定义消息边界。应用程序必须定义自己的机制来识别消息边界。一些策略包括在消息前面加上消息的长度，使用字符序列作为消息结束标记，或者使用固定长度的消息。在本书的示例中，我们使用`tcp::socket`的`shutdown`成员函数，这会导致接收方读取文件结束符，表示我们已经完成发送消息。这使示例保持简单，但实际上，这不是最灵活的策略。
- en: 'Let us now write the TCP server, which will handle requests from this client:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写TCP服务器，它将处理来自此客户端的请求：
- en: '**Listing 11.21: Synchronous TCP server**'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.21：同步TCP服务器**'
- en: '[PRE42]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The first thing that a TCP server does is to create a listener socket and bind
    it to a local endpoint. With Boost Asio, you do this by creating an instance of
    `asio::ip::tcp::acceptor` and passing it the endpoint to bind to (line 14). We
    create an IPv4 endpoint specifying only the port and not the address so that it
    uses the unspecified address 0.0.0.0 (line 13). We bind the endpoint to the listener
    by passing it to the constructor of the `acceptor` (line 14). We then spin in
    a loop waiting for incoming connections (line 16). We create a new socket as we
    need a distinct socket to serve as the server-side endpoint for each new connection
    (line 17). We then call the `accept` member function on the acceptor (line 18),
    passing it the new socket. The call to `accept` blocks until a new connection
    is established. When `accept` returns, the socket passed to it represents the
    server-side endpoint of the connection established.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: TCP服务器的第一件事是创建一个监听套接字并将其绑定到本地端点。使用Boost Asio，您可以通过创建`asio::ip::tcp::acceptor`的实例并将其传递给要绑定的端点来实现这一点（第14行）。我们创建一个IPv4端点，只指定端口而不指定地址，以便使用未指定地址0.0.0.0（第13行）。我们通过将其传递给`acceptor`的构造函数将端点绑定到监听器（第14行）。然后我们在循环中等待传入的连接（第16行）。我们需要一个独立的套接字来作为每个新连接的服务器端点，因此我们创建一个新的套接字（第17行）。然后我们在接受器上调用`accept`成员函数（第18行），将新套接字传递给它。`accept`调用会阻塞，直到建立新连接。当`accept`返回时，传递给它的套接字表示建立的连接的服务器端点。
- en: We create a new thread to serve each new connection established (line 19). We
    generate the initial function for this thread using a lambda (line 19-44), capturing
    the `shared_ptr`-wrapped server-side `socket` for this connection (line 19). Within
    the thread, we call the `read` function to read data sent by the client (lines
    31-32), and then write data back using `write` (line 37). To show how it is done,
    we send data from a multi-buffer sequence set up from two character strings (lines
    22-26). The network I/O in this thread is done inside a try-block to make sure
    that no exception escapes the thread. Note that we call `close` on the socket
    after the call to `write` returns (line 38). This closes the connection from the
    server-side, and the client reads an end-of-file in the received stream.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个新线程来为每个建立的新连接提供服务（第19行）。我们使用lambda（第19-44行）生成此线程的初始函数，捕获此连接的`shared_ptr`包装的服务器端`socket`（第19行）。在线程内部，我们调用`read`函数来读取客户端发送的数据（第31-32行），然后使用`write`写回数据（第37行）。为了展示如何做到这一点，我们从两个字符字符串设置的多缓冲序列中发送数据（第22-26行）。此线程中的网络I/O在try块内完成，以确保没有异常逃逸出线程。请注意，在`write`返回后我们在socket上调用`close`（第38行）。这关闭了服务器端的连接，客户端在接收流中读取到文件结束符。
- en: Concurrency and performance
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 并发和性能
- en: 'The TCP server handles each connection independently. But creating a new thread
    for each new connection scales badly, and the server''s resources could be overrun
    if a large number of connections hit it over a very short interval. One way to
    handle this is to limit the number of threads. Earlier, we modified the UDP server
    example from listing 11.18 to use a thread pool and limit the total number of
    threads. We can do the same with our TCP server from listing 11.21\. Here is an
    outline for how this can be done:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: TCP服务器独立处理每个连接。但是为每个新连接创建一个新线程的扩展性很差，如果在非常短的时间内有大量连接到达服务器，服务器的资源可能会耗尽。处理这种情况的一种方法是限制线程数量。之前，我们修改了清单11.18中的UDP服务器示例，使用了线程池并限制了总线程数量。我们可以对清单11.21中的TCP服务器做同样的事情。以下是如何实现的概述：
- en: '[PRE43]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: First, we create a pool of a fixed number of threads (lines 15-20), and make
    sure they do not exit by posting a dummy work to the `io_service`'s task queue
    (lines 13-14). Instead of creating a thread for each new connection, we post a
    handler for the connection to the task queue of the `io_service` (line 28). This
    handler can be exactly the same as the initial function of the per-connection
    thread in listing 11.21\. The threads in the pool then dispatch the handlers on
    their own schedule. The number of threads represented by `max_threads` can be
    tweaked easily based on the number of processors in the system.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建了一个固定数量线程的线程池（第15-20行），并通过向`io_service`的任务队列发布一个虚拟工作（第13-14行）来确保它们不会退出。我们不是为每个新连接创建一个线程，而是将连接的处理程序发布到`io_service`的任务队列中（第28行）。这个处理程序可以与清单11.21中每个连接线程的初始函数完全相同。然后线程池中的线程按照自己的时间表分派处理程序。`max_threads`表示的线程数量可以根据系统中的处理器数量轻松调整。
- en: While using the thread pool limits the number of threads, it does little to
    improve the responsiveness of the server. In the event of a large influx of new
    connections, handlers of the newer connections would form a big backlog in the
    queue, and these clients would be kept waiting while the server services earlier
    connections. We have already addressed similar concerns in our UDP server by using
    asynchronous I/O. In the next section, we will use the same strategy to scale
    our TCP servers better.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用线程池限制了线程数量，但对于服务器的响应性几乎没有改善。在大量新连接涌入时，新连接的处理程序会在队列中形成一个大的积压，这些客户端将被保持等待，而服务器则服务于先前的连接。我们已经通过使用异步I/O在UDP服务器中解决了类似的问题。在下一节中，我们将使用相同的策略来更好地扩展我们的TCP服务器。
- en: Asynchronous TCP server
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异步TCP服务器
- en: The synchronous TCP server is inefficient mainly because the read and write
    operations on the sockets block for a finite amount of time, waiting for the operations
    to complete. During this time, even with thread pools around, the thread serving
    the connection just waits idly for an I/O operation to go through, before it can
    proceed to handle the next available connection.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 同步TCP服务器效率低下主要是因为套接字上的读写操作会阻塞一段有限的时间，等待操作完成。在此期间，即使有线程池，服务连接的线程也只是空闲地等待I/O操作完成，然后才能处理下一个可用连接。
- en: We can eliminate these idle waits using asynchronous I/O. Just as we saw with
    the asynchronous UDP server, we could either use chains of handlers or coroutines
    to write the asynchronous TCP server. While handler chains make the code complex,
    and therefore error-prone, coroutines make it far more readable and intuitive.
    We will first write an asynchronous TCP server using coroutines, and then use
    the more traditional handler-chaining, just to put the difference between the
    two approaches in perspective. You can skip the handler-chaining implementations
    on first reading.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用异步I/O来消除这些空闲等待。就像我们在异步UDP服务器中看到的那样，我们可以使用处理程序链或协程来编写异步TCP服务器。虽然处理程序链使代码复杂，因此容易出错，但协程使代码更易读和直观。我们将首先使用协程编写一个异步TCP服务器，然后再使用更传统的处理程序链，以便更好地理解这两种方法之间的差异。在第一次阅读时，您可以跳过处理程序链的实现。
- en: Asynchronous TCP server using coroutines
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用协程的异步TCP服务器
- en: 'The following is the complete code for a TCP server employing asynchronous
    I/O via coroutines:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用协程进行异步I/O的TCP服务器的完整代码：
- en: '**Listing 11.22: Asynchronous TCP server using coroutines**'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.22：使用协程的异步TCP服务器**'
- en: '[PRE44]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We use two coroutines: `accept_connections` handles incoming connection requests
    (line 42), while `handle_connection` performs I/O on each new connection (line
    14). The `main` function calls the `spawn` function template to add the `accept_connections`
    task to the `io_service` queue, to be run as a coroutine (line 63). The spawn
    function template is available through the header `boost/asio/spawn.hpp` (line
    2). The call to the `run` member function of the `io_service` invokes the `accept_connections`
    coroutine, which spins in a loop awaiting new connection requests (line 65).'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了两个协程：`accept_connections`处理传入的连接请求（第42行），而`handle_connection`在每个新连接上执行I/O（第14行）。`main`函数调用`spawn`函数模板将`accept_connections`任务添加到`io_service`队列中，以作为协程运行（第63行）。`spawn`函数模板可通过头文件`boost/asio/spawn.hpp`（第2行）获得。调用`io_service`的`run`成员函数会调用`accept_connections`协程，该协程在一个循环中等待新的连接请求（第65行）。
- en: The `accept_connections` function takes two arguments in addition to the obligatory
    `yield_context`. These are a reference to the `io_service` instance, and the port
    to listen on for new connections—values bound by the `main` function when it spawns
    this coroutine (lines 63-64). The `accept_connections` function creates an endpoint
    for the unspecified IPv4 address and the specific port it is passed (lines 46-47),
    and creates an acceptor for that endpoint (line 48). It then calls the `async_accept`
    member function of the acceptor in each iteration of the loop, passing a reference
    to a TCP socket, and the local `yield_context` as the completion handler (line
    53). This suspends the `accept_connections` coroutine until a new connection is
    accepted. Once a new connection request is received, `async_accept` accepts it,
    sets the socket reference passed to it to the server-side socket for the new connection,
    and resumes the `accept_connections` coroutine. The `accept_connections` coroutine
    adds the `handle_connection` coroutine to the `io_service` queue for handling
    the I/O on this specific connection (lines 56-57). In the next iteration of the
    loop, it again waits for new incoming connections.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`accept_connections`函数除了强制的`yield_context`之外，还接受两个参数。这些是对`io_service`实例的引用，以及用于监听新连接的端口——在`main`函数在生成此协程时绑定的值（第63-64行）。`accept_connections`函数为未指定的IPv4地址和传递的特定端口创建一个端点（第46-47行），并为该端点创建一个接受者（第48行）。然后，在循环的每次迭代中调用接受者的`async_accept`成员函数，传递一个TCP套接字的引用，并将本地的`yield_context`作为完成处理程序（第53行）。这会暂停`accept_connections`协程，直到接受到新的连接。一旦接收到新的连接请求，`async_accept`接受它，将传递给它的套接字引用设置为新连接的服务器端套接字，并恢复`accept_connections`协程。`accept_connections`协程将`handle_connection`协程添加到`io_service`队列中，用于处理此特定连接的I/O（第56-57行）。在下一次循环迭代中，它再次等待新的传入连接。'
- en: The `handle_connection` coroutine takes a TCP socket wrapped in a `shared_ptr`,
    as a parameter in addition to `yield_context`. The `accept_connections` coroutine
    creates this socket, and passes it to `handle_connection`, wrapped in the `shared_ptr`.
    The `handle_connection` function receives any data sent by the client using `async_read`
    (lines 23-24). If the receive is successful, it sends back a response string `Hello
    from server`, and then echoes back the received data, using a buffer sequence
    of length 2 (lines 28-30).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`handle_connection`协程除了`yield_context`之外，还接受一个包装在`shared_ptr`中的TCP套接字作为参数。`accept_connections`协程创建此套接字，并将其包装在`shared_ptr`中传递给`handle_connection`。`handle_connection`函数使用`async_read`接收客户端发送的任何数据（第23-24行）。如果接收成功，它会发送一个响应字符串`Hello
    from server`，然后使用长度为2的缓冲区序列回显接收到的数据（第28-30行）。'
- en: Asynchronous TCP server without coroutines
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 没有协程的异步TCP服务器
- en: We now look at how to write an asynchronous TCP server without coroutines. This
    involves a more complex handshake between handlers, and hence, we want to split
    the code into appropriate classes. We define two classes in two separate header
    files. The class `TCPAsyncServer` (listing 11.23) represents the server instance
    that listens for incoming connections. It goes in the `asyncsvr.hpp` header file.
    The class `TCPAsyncConnection` (listing 11.25) represents the processing context
    of a single connection. It goes in the `asynconn.hpp` header file.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看如何编写一个没有协程的异步TCP服务器。这涉及处理程序之间更复杂的握手，因此，我们希望将代码拆分成适当的类。我们在两个单独的头文件中定义了两个类。`TCPAsyncServer`类（清单11.23）表示监听传入连接的服务器实例。它放在`asyncsvr.hpp`头文件中。`TCPAsyncConnection`类（清单11.25）表示单个连接的处理上下文。它放在`asynconn.hpp`头文件中。
- en: '`TCPAsyncServer` creates a new instance of `TCPAsyncConnection` for each new
    incoming connection. The `TCPAsyncConnection` instance reads incoming data from
    the client and sends back messages to the client until the client closes the connection
    to the server.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`TCPAsyncServer`为每个新的传入连接创建一个新的`TCPAsyncConnection`实例。`TCPAsyncConnection`实例从客户端读取传入数据，并向客户端发送消息，直到客户端关闭与服务器的连接。'
- en: 'To start the server, you create an instance of `TCPAsyncServer`, passing the
    instance of `io_service` and a port number, and then call the `run` member function
    of the `io_service` to start processing new connections:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动服务器，您需要创建一个`TCPAsyncServer`实例，传递`io_service`实例和端口号，然后调用`io_service`的`run`成员函数来开始处理新连接：
- en: '**Listing 11.23: Asynchronous TCP server (asyncsvr.hpp)**'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.23：异步TCP服务器（asyncsvr.hpp）**'
- en: '[PRE45]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The `TCPAsyncServer` class has an acceptor member variable of type `boost::asio::ip::tcp::acceptor`,
    which is used to listen for and accept incoming connections (line 39). The constructor
    initializes the acceptor with a local TCP endpoint on the unspecified IPv4 address
    and a specific port (lines 17-19), and then calls the `waitForConnection` member
    function (line 20).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`TCPAsyncServer`类具有一个`boost::asio::ip::tcp::acceptor`类型的接受者成员变量，用于监听和接受传入连接（第39行）。构造函数使用未指定的IPv4地址和特定端口初始化接受者（第17-19行），然后调用`waitForConnection`成员函数（第20行）。'
- en: The `waitForConnection` function creates a new instance of `TCPAsyncConnection`
    wrapped in a `shared_ptr` called `connectionPtr` (lines 24-25) to handle each
    new connection from a client. We have included our own header file `asynconn.hpp`
    to access the definition of `TCPAsyncConnection` (line 7), which we will look
    at shortly. It then calls the `async_accept` member function on the acceptor to
    listen for new incoming connections and accept them (line 26-27). We pass to `async_accept`,
    a non-const reference to a `tcp::socket` object that is a member of `TCPAsyncConnection`,
    and a completion handler that is called each time a new connection is established
    (lines 27-35). It is an asynchronous call and returns immediately. But each time
    a new connection is established, the socket reference is set to the server-side
    socket for serving that connection, and the completion handler gets called.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '`waitForConnection`函数创建了一个新的`TCPAsyncConnection`实例，将其包装在名为`connectionPtr`的`shared_ptr`中（第24-25行），以处理来自客户端的每个新连接。我们已经包含了我们自己的头文件`asynconn.hpp`来访问`TCPAsyncConnection`的定义（第7行），我们很快就会看到。然后调用acceptor的`async_accept`成员函数来监听新的传入连接并接受它们（第26-27行）。我们传递给`async_accept`一个对`TCPAsyncConnection`的`tcp::socket`对象的非const引用，以及一个在每次建立新连接时调用的完成处理程序（第27-35行）。这是一个异步调用，会立即返回。但每次建立新连接时，套接字引用都会设置为用于服务该连接的服务器端套接字，并调用完成处理程序。'
- en: The completion handler for `async_accept` is written as a lambda, and it captures
    the `this` pointer pointing to the `TCPAsyncServer` instance and the `connectionPtr`
    (line 27). This allows the lambda to call member functions on both the `TCPAsyncServer`
    instance, and on the `TCPAsyncConnection` instance serving this specific connection.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '`async_accept`的完成处理程序被编写为lambda，并捕获指向`TCPAsyncServer`实例的`this`指针和`connectionPtr`（第27行）。这允许lambda在`TCPAsyncServer`实例和为该特定连接提供服务的`TCPAsyncConnection`实例上调用成员函数。'
- en: Tip
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The lambda expression generates a function object and the captured `connectionPtr`
    is copied to a member of it. Since `connectionPtr` is a `shared_ptr`, its reference
    count is bumped up in the process. The `async_accept` function pushes this function
    object into the task handler queue of `io_service`, so the underlying instance
    of `TCPAsyncConnection` survives, even after `waitForConnection` returns.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: lambda表达式生成一个函数对象，并将捕获的`connectionPtr`复制到其中的一个成员。由于`connectionPtr`是一个`shared_ptr`，在此过程中它的引用计数会增加。`async_accept`函数将此函数对象推送到`io_service`的任务处理程序队列中，因此`TCPAsyncConnection`的底层实例会在`waitForConnection`返回后继续存在。
- en: 'Upon connection establishment, when the completion handler is called, it does
    two things. If there were no errors, it initiates I/O on the new connection by
    calling the `waitForReceive` function on the `TCPAsyncConnection` object (line
    32). It then restarts the wait for the next connection by calling `waitForConnection`
    on the `TCPAsyncServer` object, via the captured `this` pointer (line 33). In
    case of an error, it prints a message (lines 29-30). The `waitForConnection` call
    is asynchronous, and we will soon find out that so is the `waitForReceive` call
    because both call asynchronous Asio functions. Once the handler returns, the server
    proceeds to handle I/O on existing connections or accepts new connections:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接建立时，当调用完成处理程序时，它会执行两件事。如果没有错误，它会通过在`TCPAsyncConnection`对象上调用`waitForReceive`函数（第32行）来启动新连接上的I/O。然后通过调用`TCPAsyncServer`对象上的`waitForConnection`（通过捕获的`this`指针）来重新等待下一个连接（第33行）。如果出现错误，它会打印一条消息（第29-30行）。`waitForConnection`调用是异步的，我们很快就会发现`waitForReceive`调用也是异步的，因为两者都调用了异步Asio函数。处理程序返回后，服务器将继续处理现有连接上的I/O或接受新连接：
- en: '**Listing 11.24: Running the async server**'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.24：运行异步服务器**'
- en: '[PRE46]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'To run the server, we simply instantiate it with the `io_service` and port
    number (line 12), and then call the `run` method on `io_service` (line 13). The
    server we are building will be thread-safe, so we can as well call `run` from
    each of the pool of threads to introduce some concurrency in the processing of
    incoming connections. We will now see how I/O on each connection is handled:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行服务器，我们只需用`io_service`和端口号实例化它（第12行），然后在`io_service`上调用`run`方法（第13行）。我们正在构建的服务器将是线程安全的，因此我们也可以从线程池中的每个线程调用`run`，以在处理传入连接时引入一些并发。现在我们将看到如何处理每个连接上的I/O：
- en: '**Listing 11.25: Per-connection I/O Handler class (asynconn.hpp)**'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.25：每个连接的I/O处理程序类（asynconn.hpp）**'
- en: '[PRE47]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We saw in listing 11.23 how an instance of `TCPAsyncConnection` gets created,
    wrapped in a `shared_ptr`, to handle each new connection, and I/O is initiated
    on it by a call to the `waitForReceive` member function. Let us now understand
    its implementation. `TCPAsyncConnection` has two public members for performing
    asynchronous I/O on the connection: `waitForReceive` to perform asynchronous receives
    (line 23) and `startSend` to perform asynchronous sends (line 40).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在11.23清单中看到了如何创建`TCPAsyncConnection`的实例，并将其包装在`shared_ptr`中，以处理每个新连接，并通过调用`waitForReceive`成员函数来启动I/O。现在让我们来了解它的实现。`TCPAsyncConnection`有两个用于在连接上执行异步I/O的公共成员：`waitForReceive`用于执行异步接收（第23行），`startSend`用于执行异步发送（第40行）。
- en: The `waitForReceive` function initiates a receive by calling the `­async­_read`
    function on the socket (line 25). The data is received into the `buf` member (line
    57). The completion handler for this call (line 26-37) is invoked when the data
    is completely received. If there were no errors, it calls `startSend`, which asynchronously
    sends a message to the client (line 28), and then calls `waitForReceive` again,
    provided an end-of-file was not encountered by the previous receive (line 32).
    Thus, as long as there was no read error, the server keeps waiting to read more
    data on the connection. If there was an error, it prints a diagnostic message
    (lines 34-35).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`waitForReceive`函数通过在套接字上调用`async_read`函数（第25行）来启动接收。数据被接收到`buf`成员中（第57行）。此调用的完成处理程序（第26-37行）在数据完全接收时被调用。如果没有错误，它调用`startSend`，它异步地向客户端发送一条消息（第28行），然后再次调用`waitForReceive`，前提是之前的接收没有遇到文件结尾（第32行）。因此，只要没有读取错误，服务器就会继续等待在连接上读取更多数据。如果出现错误，它会打印诊断消息（第34-35行）。'
- en: The `startSend` function uses the function `async_write` to send the text `Hello
    from server` to the client. Its handler does not do anything on success but prints
    a diagnostic message on failure (lines 49-50). For EOF write errors, it closes
    the socket (line 47).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '`startSend`函数使用`async_write`函数向客户端发送文本`Hello from server`。它的处理程序在成功时不执行任何操作，但在失败时打印诊断消息（第49-50行）。对于EOF写入错误，它关闭套接字（第47行）。'
- en: Lifetime of TCPAsyncConnection
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TCPAsyncConnection的生命周期
- en: Each instance of `TCPAsyncConnection` needs to survive as long as the client
    remains connected to the server. This makes it difficult to bind the scope of
    this object to any function in the server. This is the reason we create the `TCPAsyncConnection`
    object wrapped in a `shared_ptr`, and then capture it in handler lambdas. The
    `TCPAsyncConnection` member functions for performing I/O on the connection, `waitForReceive`
    and `startSend`, are both asynchronous. So they push a handler into the `io_service`'s
    task queue before returning. These handlers capture the `shared_ptr` wrapped instance
    of `TCPAsyncConnection` to keep the instance alive across calls.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`TCPAsyncConnection`实例需要在客户端保持连接到服务器的时间内存活。这使得将这个对象的范围绑定到服务器中的任何函数变得困难。这就是我们在`shared_ptr`中创建`TCPAsyncConnection`对象的原因，然后在处理程序lambda中捕获它。`TCPAsyncConnection`用于在连接上执行I/O的成员函数`waitForReceive`和`startSend`都是异步的。因此，它们在返回之前将处理程序推入`io_service`的任务队列。这些处理程序捕获了`TCPAsyncConnection`的`shared_ptr`包装实例，以保持实例在调用之间的存活状态。
- en: In order for the handlers to have access to the `shared_ptr`-wrapped instance
    of the `TCPAsyncConnection` object from within `waitForReceive` and `startSend`,
    it is required that these member functions of `TCPAsyncConnection` have access
    to the `shared_ptr` wrapped instance on which they are called. The *enable shared
    from this* idiom, which we learned in [Chapter 3](ch03.html "Chapter 3. Memory
    Management and Exception Safety"), *Memory Management and Exception Safety*, is
    tailor-made for such purposes. This is the reason we derive `TCPAsyncConnection`
    from `enable_shared_from_this<TCPAsyncConnection>`. By virtue of this, `TCPAsyncConnection`
    inherits the `shared_from_this` member function, which returns the `shared_ptr`-wrapped
    instance we need. This means that `TCPAsyncConnection` should always be allocated
    dynamically and wrapped in a `shared_ptr`, and any other way would result in undefined
    behavior.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使处理程序能够从`waitForReceive`和`startSend`中访问`TCPAsyncConnection`对象的`shared_ptr`包装实例，需要这些`TCPAsyncConnection`的成员函数能够访问它们被调用的`shared_ptr`包装实例。我们在[第3章](ch03.html
    "第3章。内存管理和异常安全")中学到的*enable shared from this*习惯用法，*内存管理和异常安全*，是为这种目的量身定制的。这就是我们将`TCPAsyncConnection`从`enable_shared_from_this<TCPAsyncConnection>`派生的原因。由于这个原因，`TCPAsyncConnection`继承了`shared_from_this`成员函数，它返回我们需要的`shared_ptr`包装实例。这意味着`TCPAsyncConnection`应该始终动态分配，并用`shared_ptr`包装，否则会导致未定义的行为。
- en: This is the reason we call `shared_from_this` in both `waitForReceive` (line
    24) and `startSend` (line 42), and it is captured by the respective handlers (lines
    26, 44). As long as the `waitForReceive` member function keeps getting called
    from the completion handler for `async_read` (line 32), the `TCPAsyncConnection`
    instance survives. If an error is encountered in receive, either because the remote
    endpoint closed the connection or for another reason, then this cycle breaks.
    The `shared_ptr` wrapping the `TCPAsyncConnection` object is no longer captured
    by any handler and is destroyed at the end of the scope, closing the connection.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在`waitForReceive`（第24行）和`startSend`（第42行）中都调用`shared_from_this`的原因，它被各自的处理程序捕获（第26行，44行）。只要`waitForReceive`成员函数从`async_read`（第32行）的完成处理程序中被调用，`TCPAsyncConnection`实例就会存活。如果在接收中遇到错误，要么是因为远程端点关闭了连接，要么是因为其他原因，那么这个循环就会中断。包装`TCPAsyncConnection`对象的`shared_ptr`不再被任何处理程序捕获，并且在作用域结束时被销毁，关闭连接。
- en: Performance and concurrency
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能和并发性
- en: Notice that both implementations of TCP asynchronous server, with and without
    coroutines, are single-threaded. However, there are no thread-safety issues in
    either implementation, so we could have as well employed a thread pool, each of
    whose threads would call `run` on the `io_service`.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，TCP异步服务器的两种实现，使用和不使用协程，都是单线程的。然而，在任何实现中都没有线程安全问题，因此我们也可以使用线程池，每个线程都会在`io_service`上调用`run`。
- en: Inversion of control flow
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 控制流的反转
- en: 'The most significant difficulty with programming asynchronous systems is the
    inversion of control flow. To write the code for a synchronous server, we know
    we have to call the operations in the following sequence:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 编写异步系统的最大困难在于控制流的反转。要编写同步服务器的代码，我们知道必须按以下顺序调用操作：
- en: Call `accept` on the acceptor.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接收器上调用`accept`。
- en: Call `read` on the socket.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在套接字上调用`read`。
- en: Call `write` on the socket.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在套接字上调用`write`。
- en: We know that `accept` returns only when the connection has been established,
    so it is safe to call `read`. Also, `read` returns only after it has read the
    number of bytes asked for, or encountered an end-of-file. So it is safe for a
    `write` call to follow. This made writing code incredibly easy compared to the
    asynchronous model, but introduced waits that affected our ability to handle other
    waiting connections, while our requests were being serviced.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道`accept`仅在连接建立后才返回，因此可以安全地调用`read`。此外，`read`仅在读取所请求的字节数或遇到文件结束后才返回。因此，可以安全地调用`write`。与异步模型相比，这使得编写代码变得非常容易，但引入了等待，影响了我们处理其他等待连接的能力，同时我们的请求正在被处理。
- en: We eliminated that wait with asynchronous I/O, but lost the simplicity of the
    model when we used handler chaining. As we cannot deterministically tell at which
    point an asynchronous I/O operation is completed, we ask the `io_service` to run
    specific handlers on completion of our requests. We still know which operation
    to perform after which, but we no longer know when. So we tell the `io_service`
    *what* to run, and it uses the appropriate notifications from the OS to know *when*
    to run them. The biggest challenge in this model is to maintain object states
    and managing object lifetimes across handlers.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过异步I/O消除了等待，但在使用处理程序链接时失去了模型的简单性。由于我们无法确定地告诉异步I/O操作何时完成，因此我们要求`io_service`在我们的请求完成时运行特定的处理程序。我们仍然知道在之后执行哪个操作，但不再知道何时。因此，我们告诉`io_service`要运行*什么*，它使用来自操作系统的适当通知来知道*何时*运行它们。这种模型的最大挑战是在处理程序之间维护对象状态和管理对象生命周期。
- en: Coroutines eliminate this *inversion of control flow* by allowing the sequence
    of asynchronous I/O operations to be written in a single coroutine, which is *suspended*
    instead of waiting for an asynchronous operation to complete, and *resumed* when
    the operation is completed. This allows for wait-free logic without the inherent
    complexities of handler chaining.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 通过允许将异步I/O操作的序列写入单个协程来消除这种*控制流反转*，该协程被*挂起*而不是等待异步操作完成，并在操作完成时*恢复*。这允许无等待逻辑，而不会引入处理程序链接的固有复杂性。
- en: Tip
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Always prefer coroutines over handler chaining when writing asynchronous servers.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写异步服务器时，始终优先使用协程而不是处理程序链接。
- en: Self-test questions
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自测问题
- en: 'For multiple choice questions, choose all options that apply:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多项选择题，选择所有适用的选项：
- en: What is the difference between `io_service::dispatch` and `io_service::post`?
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`io_service::dispatch`和`io_service::post`之间的区别是什么？'
- en: a. `dispatch` returns immediately while `post` runs the handler before returning
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: a. `dispatch`立即返回，而`post`在返回之前运行处理程序
- en: b. `post` returns immediately while `dispatch` may run the handler on the current
    thread if it can, or it behaves like post
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: b. `post`立即返回，而`dispatch`如果可以在当前线程上运行处理程序，或者它的行为类似于post
- en: c. `post` is thread-safe while `dispatch` is not
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: c. `post`是线程安全的，而`dispatch`不是
- en: d. `post` returns immediately while `dispatch` runs the handler
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: d. `post`立即返回，而`dispatch`运行处理程序
- en: What happens if a handler throws an exception when it is dispatched?
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当处理程序在分派时抛出异常会发生什么？
- en: a. It is undefined behavior
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: a. 这是未定义行为
- en: b. It terminates the program with a call to `std::terminate`
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: b. 它通过调用`std::terminate`终止程序
- en: c. The call to run, on the `io_service` that dispatched the handler, will throw
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: c. 在分派处理程序的`io_service`上调用run将抛出异常。
- en: d. The `io_service` is stopped
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: d. `io_service`被停止
- en: What is the role of the unspecified address 0.0.0.0 (IPv4) or ::/1 (IPv6)?
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 未指定地址0.0.0.0（IPv4）或::/1（IPv6）的作用是什么？
- en: a. It is used to communicate with local services on a system
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: a. 它用于与系统上的本地服务通信
- en: b. Packets sent to this address are echoed back to the sender
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: b. 发送到此地址的数据包将被回显到发送方
- en: c. It is used to broadcast to all connected hosts in the network
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: c. 它用于向网络中的所有连接的主机进行广播
- en: d. It is used to bind to all available interfaces without the need to know addresses
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: d. 它用于绑定到所有可用接口，而无需知道地址
- en: Which of the following statements about TCP are true?
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下关于TCP的哪些陈述是正确的？
- en: a. TCP is faster than UDP
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: a. TCP比UDP更快
- en: b. TCP detects data corruption but not data loss
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: b. TCP检测数据损坏但不检测数据丢失
- en: c. TCP is more reliable than UDP
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: c. TCP比UDP更可靠
- en: d. TCP retransmits lost or corrupted data
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: d. TCP重新传输丢失或损坏的数据
- en: What do we mean when we say that a particular function, for example, `async_read`,
    is asynchronous?
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们说特定函数，例如`async_read`是异步时，我们是什么意思？
- en: a. The function returns before the requested action is complete
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: a. 在请求的操作完成之前，该函数会返回
- en: b. The function starts the operation on a different thread and returns immediately
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: b. 该函数在不同的线程上启动操作，并立即返回
- en: c. The requested action is queued for processing by the same or another thread
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: c. 请求的操作被排队等待由同一线程或另一个线程处理
- en: d. The function performs the action if it immediately can, or returns an error
    if it cannot immediately perform the action
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: d. 如果可以立即执行操作，则该函数执行该操作，否则返回错误
- en: How can we ensure that an object created just before calling an asynchronous
    function would still be available in the handler?
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何确保在调用异步函数之前创建的对象仍然可以在处理程序中使用？
- en: a. Make the object global.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: a. 将对象设为全局。
- en: b. Copy/capture the object wrapped in a `shared_ptr` in the handler.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: b. 在处理程序中复制/捕获包装在`shared_ptr`中的对象。
- en: c. Allocate the object dynamically and wrap it in a `shared_ptr`.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: c. 动态分配对象并将其包装在`shared_ptr`中。
- en: d. Make the object a member of the class.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: d. 将对象设为类的成员。
- en: Summary
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Asio is a well-designed library that can be used to write fast, nimble network
    servers that utilize the most optimal mechanisms for asynchronous I/O available
    on a system. It is an evolving library and is the basis for a Technical Specification
    that proposes to add a networking library to a future revision of the C++ Standard.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: Asio是一个设计良好的库，可用于编写快速、灵活的网络服务器，利用系统上可用的最佳异步I/O机制。它是一个不断发展的库，是提议在未来的C++标准修订版中添加网络库的技术规范的基础。
- en: In this chapter, we learned how to use the Boost Asio library as a task queue
    manager and leverage Asio's TCP and UDP interfaces to write programs that communicate
    over the network. Using Boost Asio, we were able to highlight some of the general
    concerns of network programming, the challenges to scaling for a large number
    of concurrent connections, and the advantages and complexity of asynchronous I/O.
    In particular, we saw how using stackful coroutines makes writing asynchronous
    servers a breeze, compared to the older model of chaining handlers. While we did
    not cover stackless coroutines, the ICMP protocol, and serial port communications
    among other things, the topics covered in this chapter should provide you with
    a solid foundation for understanding these areas.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了如何使用Boost Asio库作为任务队列管理器，并利用Asio的TCP和UDP接口编写可以在网络上通信的程序。使用Boost Asio，我们能够突出显示网络编程的一些一般性问题，针对大量并发连接的扩展挑战，以及异步I/O的优势和复杂性。特别是，我们看到使用stackful协程相对于旧模型的处理程序链，使得编写异步服务器变得轻而易举。虽然我们没有涵盖stackless协程、ICMP协议和串口通信等内容，但本章涵盖的主题应该为您提供了理解这些领域的坚实基础。
- en: References
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '*Thinking Asynchronously in C++* (blog), *Christopher Kohlhoff*: [http://blog.think-async.com/](http://blog.think-async.com/)'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Thinking Asynchronously in C++* (博客), *Christopher Kohlhoff*: [http://blog.think-async.com/](http://blog.think-async.com/)'
- en: '*Networking Library Proposal*, *Christopher Kohlhoff*: [http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4332.html](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4332.html)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Networking Library Proposal*, *Christopher Kohlhoff*: [http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4332.html](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4332.html)'
