["```cpp\n$ nvcc -o sgemm sgemm.cu\n$ nvprof -o sgemm.nvvp ./sgemm\n$ nvprof --analysis-metrics -o sgemm-analysis.nvvp ./sgemm\n```", "```cpp\n$ ./nvvp\n```", "```cpp\n__global__ void device_add(int *a, int *b, int *c) {\n     int index = threadIdx.x + blockIdx.x * blockDim.x;\n     c[index] = a[index] + b[index];\n}\nint main (void) {\n...\n    // Alloc space for device copies of a, b, c\n    cudaMalloc((void **)&d_a, size);\n    cudaMalloc((void **)&d_b, size);\n    cudaMalloc((void **)&d_c, size);\n...\n\n   // Free space allocated for device copies\n   cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n...\n\n}\n```", "```cpp\n$ nvcc -o vec_addition ./vector_addition_gpu_thread_block.cu\n```", "```cpp\n//Data structure representing an image stored in Structure of Array Format\nstruct Coefficients_SOA {\n int r;\n int b;\n int g;\n int hue;\n int saturation;\n int maxVal;\n int minVal;\n int finalVal;\n};\n```", "```cpp\n//Data structure representing an image stored in Array of Structure Format\nstruct Coefficients_AOS {\n int* r;\n int* b;\n int* g;\n int* hue;\n int* saturation;\n int* maxVal;\n int* minVal;\n int* finalVal;\n};\n```", "```cpp\n$ nvcc -o aos_soa ./aos_soa.cu\n$ nvcc -o aos_soa_solved ./aos_soa_solved.cu\n$ nvprof --analysis-metrics --export-profile aos_soa.prof ./aos_soa\n$ nvprof --analysis-metrics --export-profile aos_soa_solved.prof ./aos_soa_solved\n```", "```cpp\n__global__ void matrix_transpose_naive(int *input, int *output) {\n     int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n     int indexY = threadIdx.y + blockIdx.y * blockDim.y;\n     int index = indexY * N + indexX;\n     int transposedIndex = indexX * N + indexY;\n     output[index] = input[transposedIndex];\n}\n```", "```cpp\n$ nvcc -o matrix_transpose ./matrix_transpose.cu\n$ nvcc -o conflict_solved ./conflict_solved.cu\n$ nvprof --analysis-metrics --export-profile matrix_transpose.prof ./matrix_transpose\n$ nvprof --analysis-metrics --export-profile conflict_solved.prof ./conflict_solved\n```", "```cpp\n__global__ void matrix_transpose_shared(int *input, int *output) {\n\n    __shared__ int sharedMemory [BLOCK_SIZE] [BLOCK_SIZE];\n\n    //global index\n     int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n     int indexY = threadIdx.y + blockIdx.y * blockDim.y;\n\n    //transposed global memory index\n     int tindexX = threadIdx.x + blockIdx.y * blockDim.x;\n     int tindexY = threadIdx.y + blockIdx.x * blockDim.y;\n\n    //local index\n     int localIndexX = threadIdx.x;\n     int localIndexY = threadIdx.y;\n     int index = indexY * N + indexX;\n     int transposedIndex = tindexY * N + tindexX;\n\n    //transposed the matrix in shared memory. \n    // Global memory is read in coalesced fashion\n     sharedMemory[localIndexX][localIndexY] = input[index];\n     __syncthreads();\n\n    //output written in global memory in coalesed fashion.\n     output[transposedIndex] = sharedMemory[localIndexY][localIndexX];\n}\n```", "```cpp\n__global__ void matrix_transpose_shared(int *input, int *output) {\n\n     __shared__ int sharedMemory [BLOCK_SIZE] [BLOCK_SIZE + 1];\n\n    //global index\n     int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n     int indexY = threadIdx.y + blockIdx.y * blockDim.y;\n\n    //transposed index\n     int tindexX = threadIdx.x + blockIdx.y * blockDim.x;\n     int tindexY = threadIdx.y + blockIdx.x * blockDim.y;\n     int localIndexX = threadIdx.x;\n     int localIndexY = threadIdx.y;\n     int index = indexY * N + indexX;\n     int transposedIndex = tindexY * N + tindexX;\n\n    //reading from global memory in coalesed manner \n    // and performing tanspose in shared memory\n     sharedMemory[localIndexX][localIndexY] = input[index];\n\n    __syncthreads();\n\n    //writing into global memory in coalesed fashion \n    // via transposed data in shared memory\n     output[transposedIndex] = sharedMemory[localIndexY][localIndexX];\n}\n```", "```cpp\n$nvcc -c scrImagePgmPpmPackage.cpp \n$nvcc -c image_scaling.cu\n$nvcc -o image_scaling image_scaling.o scrImagePgmPpmPackage.o\n```", "```cpp\ntexture<unsigned char, 2, cudaReadModeElementType> tex;\n```", "```cpp\ncudaArray* cu_array;\ncudaChannelFormatKind kind = cudaChannelFormatKindUnsigned;\ncudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(8, 0, 0, 0, kind);\n```", "```cpp\nstruct cudaTextureDesc texDesc;\nmemset(&texDesc, 0, sizeof(texDesc)); \n//set the memory to zero\ntexDesc.addressMode[0] = cudaAddressModeClamp; \n// setting the x dimension addressmode to Clamp\ntexDesc.addressMode[1] = cudaAddressModeClamp; \n//Setting y dimension addressmode to Clamp\ntexDesc.filterMode = cudaFilterModePoint; \n// Filter mode set to Point\ntexDesc.readMode = cudaReadModeElementType; \n// Reading element type and not interpolated\ntexDesc.normalizedCoords = 0;\n```", "```cpp\nimageScaledData[index] = tex2D<unsigned char>(texObj,(float)(tidX*scale_factor),(float)(tidY*scale_factor));\n```", "```cpp\ncudaDestroyTextureObject(texObj);\n```", "```cpp\n$make\n$./bandwidthTest --mode=shmoo --csv --memory=pageable > pageable.csv\n$./bandwidthTest --mode=shmoo --csv --memory=pinned >  pinned.csv\n```", "```cpp\ncudaError_t status = cudaMallocHost((void**)&h_aPinned, bytes);\nif (status != cudaSuccess)\n printf(\"Error allocating pinned host memory\\n\");\n```", "```cpp\n$nvcc -o unified_simple.out unified_memory.cu\n$nvcc -o unified_initialized.out unified_memory_initialized.cu\n$nvcc -o unified_prefetch.out unified_memory_prefetch.cu\n$nvcc -o unified_64align.out unified_memory_64align.cu\n```", "```cpp\nfloat *x, *y;\nint size = N * sizeof(float);\n...\ncudaMallocManaged(&x, size);\ncudaMallocManaged(&y, size);\n...\n\n for (int ix = 0; ix < N; ix++) {\n    x[ix] = rand()%10;\n    y[ix] = rand()%20;\n  }\n...\n\n add<<<numBlocks, blockSize>>>(x, y, N);\n```", "```cpp\n$ nvprof ./unified_simple.out\n```", "```cpp\n__global__ void init(int n, float *x, float *y) {\n int index = threadIdx.x + blockIdx.x * blockDim.x;\n int stride = blockDim.x * gridDim.x;\n for (int i = index; i < n; i += stride) {\n   x[i] = 1.0f;\n   y[i] = 2.0f;\n  }\n}\n```", "```cpp\nnvprof ./unified_initialized.out\n```", "```cpp\n$ nvprof --print-gpu-trace ./unified_initialized.out\n```", "```cpp\n#define STRIDE_64K 65536\n__global__ void init(int n, float *x, float *y) {\n  int lane_id = threadIdx.x & 31;\n  size_t warp_id = (threadIdx.x + blockIdx.x * blockDim.x) >> 5;\n  size_t warps_per_grid = (blockDim.x * gridDim.x) >> 5;\n  size_t warp_total = ((sizeof(float)*n) + STRIDE_64K-1) / STRIDE_64K;\n  for(; warp_id < warp_total; warp_id += warps_per_grid) {\n    #pragma unroll\n    for(int rep = 0; rep < STRIDE_64K/sizeof(float)/32; rep++) {\n      size_t ind = warp_id * STRIDE_64K/sizeof(float) + rep * 32 + lane_id;\n      if (ind < n) {\n        x[ind] = 1.0f;\n        y[ind] = 2.0f;\n      }\n    }\n  }\n}\n```", "```cpp\n$ nvprof --print-gpu-trace ./unified_64align.out\n```", "```cpp\n// Allocate Unified Memory -- accessible from CPU or GPU\n cudaMallocManaged(&x, N*sizeof(float));  cudaMallocManaged(&y, N*sizeof(float));\n// initialize x and y arrays on the host\n for (int i = 0; i < N; i++) {  x[i] = 1.0f;  y[i] = 2.0f;  } \n//prefetch the memory to GPU\ncudaGetDevice(&device);\ncudaMemPrefetchAsync(x, N*sizeof(float), device, NULL);\ncudaMemPrefetchAsync(y, N*sizeof(float), device, NULL); \n...\n add<<<numBlocks, blockSize>>>(N, x, y);\n//prefetch the memory to CPU\n cudaMemPrefetchAsync(y, N*sizeof(float), cudaCpuDeviceId, NULL);\n // Wait for GPU to finish before accessing on host\n cudaDeviceSynchronize();\n...\nfor (int i = 0; i < N; i++)\n maxError = fmax(maxError, fabs(y[i]-3.0f));\n\n```", "```cpp\n// Sets the data readonly for the GPU\ncudaMemAdvise(data, N, ..SetReadMostly, processorId); \nmykernel<<<..., s>>>(data, N); \n```", "```cpp\ncudaMemAdvise(input, N, ..PreferredLocation, processorId); \nmykernel<<<..., s>>>(input, N); \n```", "```cpp\ncudaMemAdvise(input, N, ..SetAccessedBy, processorId); \nmykernel<<<..., s>>>(input, N); \n```"]