["```cpp\n__global__ void index_print_kernel() {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int warp_idx = threadIdx.x / warpSize;\n    int lane_idx = threadIdx.x & (warpSize - 1);\n\n    if ((lane_idx & (warpSize/2 - 1)) == 0)\n        //thread, block, warp, lane\n        printf(\" %5d\\t%5d\\t %2d\\t%2d\\n\", idx, blockIdx.x, \n               warp_idx, lane_idx);\n}\n```", "```cpp\nint main() {\n    int gridDim = 4, blockDim = 128;\n    puts(\"thread, block, warp, lane\");\n    index_print_kernel<<< gridDim, blockDim >>>();\n    cudaDeviceSynchronize();\n}\n```", "```cpp\nnvcc -m64 -o cuda_thread_block cuda_thread_block.cu\n```", "```cpp\n$ ./cuda_thread_block.cu 4 128\nthread, block, warp, lane\n 64     0     2     0\n 80     0     2    16\n 96     0     3     0\n 112     0     3    16\n 0     0     0     0\n 16     0     0    16\n ...\n 352     2     3     0\n 368     2     3    16\n 288     2     1     0\n 304     2     1    16\n```", "```cpp\n__global__ void sgemm_gpu_kernel(const float *A, const float *B, \n        float *C, int N, int M, int K, alpha, float beta) {\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.f;\n    for (int i = 0; i < K; ++i) {\n        sum += A[row * K + i] * B[i * K + col];\n    }\n    C[row * M + col] = alpha * sum + beta * C[row * M + col];\n}\n```", "```cpp\nvoid sgemm_gpu(const float *A, const float *B, float *C,\n            int N, int M, int K, float alpha, float beta) {\n    dim3 dimBlock(BLOCK_DIM, BLOCK_DIM);\n    dim3 dimGrid(M / dimBlock.x, N / dimBlock.y);\n    sgemm_gpu_kernel<<< dimGrid, dimBlock >>>(A, B, C, N, M, K, alpha, beta);\n}\n```", "```cpp\n$ nvcc -m 64 --resource-usage \\\n -gencode arch=compute_70,code=sm_70 \\\n -I/usr/local/cuda/samples/common/inc \\\n -o sgemm ./sgemm.cu \n```", "```cpp\n$ nvcc -m64 --resource-usage \\\n      -gencode arch=compute_70,code=sm_70 \\\n      -gencode arch=compute_75,code=sm_75 \\\n      -I/usr/local/cuda/samples/common/inc \\\n      -o sgemm ./sgemm.cu\n```", "```cpp\n$ nvcc -m64 --resource-usage \\\n      -gencode arch=compute_70,code=sm_70 \\\n      -gencode arch=compute_75,code=sm_75 \\\n      -gencode arch=compute_75,code=compute_75 \\\n      -I/usr/local/cuda/samples/common/inc \\\n      -o sgemm ./sgemm.cu\n```", "```cpp\nint maxThreadPerBlock = 256;\nint minBlocksPerMultiprocessor = 2;\n__global__ void\n__launch_bound__ (maxThreadPerBlock, minBlocksPerMultiprocessor) foo_kernel() {\n    ...\n}\n```", "```cpp\n$ nvcc -m64 -I/usr/local/cuda/samples/common/inc -gencode arch=compute_70,code=sm_70 --resource-usage --maxrregcount 24 -o sgemm ./sgemm.cu\n```", "```cpp\n__global__ void naive_reduction_kernel\n     (float *data_out, float *data_in, int stride, int size) {\n     int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n     if (idx_x + stride < size)\n         data_out[idx_x] += data_in[idx_x + stride];\n}\n```", "```cpp\nvoid naive_reduction(float *d_out, float *d_in, int n_threads, int size) {\n    int n_blocks = (size + n_threads - 1) / n_threads;\n    for (int stride = 1; stride < size; stride *= 2)\n        naive_reduction_kernel<<<n_blocks, n_threads>>>(d_out, d_in, stride, size);\n}\n```", "```cpp\n__global__ void reduction_kernel(float* d_out, float* d_in, \n                                 unsigned int size) {\n    unsigned int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n\n    extern __shared__ float s_data[];\n    s_data[threadIdx.x] = (idx_x < size) ? d_in[idx_x] : 0.f;\n\n    __syncthreads();\n\n    // do reduction\n    for (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {\n        // thread synchronous reduction\n        if ( (idx_x % (stride * 2)) == 0 )\n            s_data[threadIdx.x] += s_data[threadIdx.x + stride];\n\n        __syncthreads();\n    }\n\n    if (threadIdx.x == 0)\n        d_out[blockIdx.x] = s_data[0];\n}\n```", "```cpp\nvoid reduction(float *d_out, float *d_in, int n_threads, int size)\n{\n    cudaMemcpy(d_out, d_in, size * sizeof(float), cudaMemcpyDeviceToDevice);\n    while(size > 1) {\n        int n_blocks = (size + n_threads - 1) / n_threads;\n        reduction_kernel\n            <<< n_blocks, n_threads, n_threads * sizeof(float), 0 >>>\n            (d_out, d_out, size);\n        size = n_blocks;\n    }\n}\n```", "```cpp\n// Initialize timer\nStopWatchInterface *timer;\nsdkCreateTimer(&timer);\nsdkStartTimer(&timer);\n\n... Execution code ...\n\n// Getting elapsed time\ncudaDeviceSynchronize(); // Blocks the host until GPU finishes the work\nsdkStopTimer(&timer);\n\n// Getting execution time in micro-secondes\nfloat execution_time_ms = sdkGetTimerValue(&timer)\n\n// Termination of timer\nsdkDeleteTimer(&timer);\n```", "```cpp\n# Reduction with global memory\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o reduction_global ./reduction_global.cpp reduction_global_kernel.cu\n\n# Reduction using shared memory\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o reduction_shared ./reduction_shared.cpp reduction_shared_kernel.cu\n```", "```cpp\n$ nvprof -o reduction_global.nvvp ./reduction_global \n$ nvprof --analysis-metrics -o reduction_global_metric.nvvp ./reduction_global\n```", "```cpp\n$ nvprof -o reduction_shared.nvvp ./reduction_shared \n$ nvprof --analysis-metrics -o reduction_shared_metric.nvvp ./reduction_shared\n```", "```cpp\nfor (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {\n     if ( (idx_x % (stride * 2)) == 0 )\n         s_data[threadIdx.x] += s_data[threadIdx.x + stride];\n     __syncthreads();\n }\n```", "```cpp\nfor (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {\n     if ( (idx_x & (stride * 2 - 1)) == 0 )\n         s_data[threadIdx.x] += s_data[threadIdx.x + stride];\n     __syncthreads();\n }\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o reduction_shared ./reduction_shared.cpp reduction_shared_kernel.cu\n```", "```cpp\n__global__ void reduction_kernel(float* d_out, float* d_in, unsigned int size) {\n    unsigned int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n\n    extern __shared__ float s_data[];\n    s_data[threadIdx.x] = (idx_x < size) ? d_in[idx_x] : 0.f;\n\n    __syncthreads();\n\n    // do reduction\n    for (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {\n        // thread synchronous reduction\n        if ( (idx_x % (stride * 2 - 1)) == 0 )\n            s_data[threadIdx.x] += s_data[threadIdx.x + stride];\n\n        __syncthreads();\n    }\n\n    if (threadIdx.x == 0)\n        d_out[blockIdx.x] = s_data[0];\n}\n```", "```cpp\n__global__ void\n interleaved_reduction_kernel(float* g_out, float* g_in, unsigned int size) {\n    unsigned int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n\n    extern __shared__ float s_data[];\n    s_data[threadIdx.x] = (idx_x < size) ? g_in[idx_x] : 0.f;\n    __syncthreads();\n\n    // do reduction\n    // interleaved addressing\n    for (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {\n        int index = 2 * stride * threadIdx.x;\n        if (index < blockDim.x)\n            s_data[index] += s_data[index + stride];\n        __syncthreads();\n    }\n    if (threadIdx.x == 0)\n        g_out[blockIdx.x] = s_data[0];\n}\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o reduction ./reduction.cpp ./reduction_kernel_interleaving.cu\n```", "```cpp\n__global__ void\n sequantial_reduction_kernel(float *g_out, float *g_in, \n                             unsigned int size)\n{\n    unsigned int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n\n    extern __shared__ float s_data[];\n\n    s_data[threadIdx.x] = (idx_x < size) ? g_in[idx_x] : 0.f;\n\n    __syncthreads();\n\n    // do reduction\n    // sequential addressing\n    for (unsigned int stride = blockDim.x / 2; stride > 0; \n         stride >>= 1)\n    {\n        if (threadIdx.x < stride)\n            s_data[threadIdx.x] += s_data[threadIdx.x + stride];\n\n        __syncthreads();\n    }\n\n    if (threadIdx.x == 0)\n        g_out[blockIdx.x] = s_data[0];\n}\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o reduction ./reduction.cpp ./reduction_kernel_sequential.cu\n```", "```cpp\n__global__ void reduction_kernel(float *g_out, float *g_in, \n                                 unsigned int size) {\n    unsigned int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n    extern __shared__ float s_data[];\n\n    // cumulates input with grid-stride loop \n // and save to the shared memory\n float input = 0.f;\n for (int i = idx_x; i < size; i += blockDim.x * gridDim.x)\n input += g_in[i];\n s_data[threadIdx.x] = input;\n __syncthreads();\n\n    // do reduction\n    for (unsigned int stride = blockDim.x / 2; stride > 0; \n         stride >>= 1) {\n        if (threadIdx.x < stride)\n            s_data[threadIdx.x] += s_data[threadIdx.x + stride];\n        __syncthreads();\n    }\n    if (threadIdx.x == 0)\n        g_out[blockIdx.x] = s_data[0];\n}\n\n```", "```cpp\nint reduction(float *g_outPtr, float *g_inPtr, int size, int n_threads)\n{\n    int num_sms;\n    int num_blocks_per_sm;\n    cudaDeviceGetAttribute(&num_sms, \n                           cudaDevAttrMultiProcessorCount, 0);\n    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&num_blocks_per_sm, \n           reduction_kernel, n_threads, n_threads*sizeof(float));\n    int n_blocks = min(num_blocks_per_sm * num_sms, (size \n                       + n_threads - 1) / n_threads);\n\n    reduction_kernel<<<n_blocks, n_threads, n_threads * \n                       sizeof(float), 0>>>(g_outPtr, g_inPtr, size);\n    reduction_kernel<<<1, n_threads, n_threads * sizeof(float), \n                       0>>>(g_outPtr, g_inPtr, n_blocks);\n    return 1;\n}\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o reduction ./reduction.cpp ./reduction_kernel.cu\n```", "```cpp\n#define NUM_LOAD 4\n__global__ void\n reduction_kernel(float *g_out, float *g_in, unsigned int size)\n{\n    unsigned int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n\n    extern __shared__ float s_data[];\n\n    // cumulates input with grid-stride loop \n    // and save to the shared memory\n    float input[NUM_LOAD] = {0.f};\n    for (int i = idx_x; i < size; i += blockDim.x * \n         gridDim.x * NUM_LOAD)\n    {\n        for (int step = 0; step < NUM_LOAD; step++)\n            input[step] += (i + step * blockDim.x * gridDim.x < size) ? \n                g_in[i + step * blockDim.x * gridDim.x] : 0.f;\n    }\n    for (int i = 1; i < NUM_LOAD; i++)\n        input[0] += input[i];\n    s_data[threadIdx.x] = input[0];\n\n    __syncthreads();\n\n    // do reduction\n    for (unsigned int stride = blockDim.x / 2; stride > 0; \n         stride >>= 1)\n    {\n        if (threadIdx.x < stride)\n            s_data[threadIdx.x] += s_data[threadIdx.x + stride];\n\n        __syncthreads();\n    }\n\n    if (threadIdx.x == 0) {\n        g_out[blockIdx.x] = s_data[0];\n    }\n}\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o reduction ./reduction.cpp ./reduction_kernel_opt.cu\n```", "```cpp\n__inline__ __device__ float warp_reduce_sum(float val) {\n    for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n        unsigned int mask = __activemask();\n        val += __shfl_down_sync(mask, val, offset);\n    }\n    return val;\n}\n```", "```cpp\n__inline__ __device__ float block_reduce_sum(float val) {\n    // Shared mem for 32 partial sums\n    static __shared__ float shared[32]; \n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    val = warp_reduce_sum(val); // Warp-level partial reduction\n    if (lane == 0)\n        shared[wid] = val; // Write reduced value to shared memory\n    __syncthreads(); // Wait for all partial reductions\n\n    //read from shared memory only if that warp existed\n    if (wid == 0) {\n        val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;\n        val = warp_reduce_sum(val); //Final reduce within first warp\n    }\n    return val;\n}\n```", "```cpp\n__global__ void\nreduction_kernel(float *g_out, float *g_in, unsigned int size) {\n    unsigned int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n    // cumulates input with grid-stride loop and save to share memory\n    float sum[NUM_LOAD] = { 0.f };\n    for (int i = idx_x; i < size; i += blockDim.x * gridDim.x * NUM_LOAD) {\n        for (int step = 0; step < NUM_LOAD; step++)\n            sum[step] += (i + step * blockDim.x * gridDim.x < size) ? g_in[i + step * blockDim.x * gridDim.x] : 0.f;\n    }\n    for (int i = 1; i < NUM_LOAD; i++)\n        sum[0] += sum[i];\n    // warp synchronous reduction\n    sum[0] = block_reduce_sum(sum[0]);\n\n    if (threadIdx.x == 0)\n        g_out[blockIdx.x] = sum[0];\n}\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o reduction ./reduction.cpp ./reduction_wp_kernel.cu\n```", "```cpp\nthread_block block = this_thread_block();\n```", "```cpp\ndim3 group_index();  // 3-dimensional block index within the grid\ndim3 thread_index(); // 3-dimensional thread index within the block\n```", "```cpp\nthread_block block = this_thread_block();\n```", "```cpp\n__device__ bar(thread_group block, float *x) {\n    ...\n    block.sync();\n    ...\n}\n\n__global__ foo() {\n    bar(this_thread_block(), float *x);\n}\n```", "```cpp\n__global__ void\n reduction_kernel(float* g_out, float* g_in, unsigned int size)\n{\n    unsigned int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n\n    thread_block block = this_thread_block();\n\n    extern __shared__ float s_data[];\n```", "```cpp\n    // do reduction\n    for (unsigned int stride = block.group_dim().x / 2; stride > 0; \n         stride >>= 1) {\n        if (block.thread_index().x < stride) {\n            s_data[block.thread_index().x] += \n                s_data[block.thread_index().x + stride];\n            block.sync(); // threads synchronization in a branch\n        }\n    }\n}\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o reduction_cg -rdc=true ./reduction.cpp ./reduction_cg_kernel.cu \n```", "```cpp\n// do reduction\nfor (unsigned int stride = block.group_dim().x / 2; stride > 0; \n     stride >>= 1) {\n    // scheduled threads reduce for every iteration\n    // and will be smaller than a warp size (32) eventually.\n    if (block.thread_index().x < stride) { \n        s_data[block.thread_index().x] += s_data[\n                       block.thread_index().x + stride];\n\n        // __syncthreads(); // (3) Error. Deadlock.\n        // block.sync();    // (4) Okay. Benefit of Cooperative Group\n    }\n    // __syncthreads();     // (1) Okay\n    block.sync();           // (2) Okay\n}\n```", "```cpp\ntemplate <typename group_t>\n__inline__ __device__ float\n warp_reduce_sum(group_t group, float val)\n{\n    #pragma unroll\n    for (int offset = group.size() / 2; offset > 0; offset >>= 1)\n        val += group.shfl_down(val, offset);\n    return val;\n}\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o reduction_cg -rdc=true ./reduction.cpp ./reduction_cg_kernel.cu\n```", "```cpp\n#define FULL_MASK 0xFFFFFFFF\n__inline__ __device__ float\nwarp_reduce_sum(float val) {\n#pragma unroll 5\n    for (int offset = 1; offset < 6; offset++)\n        val += __shfl_down_sync(FULL_MASK, val, warpSize >> offset);\n    return val;\n}\n```", "```cpp\nnvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o reduction_wp -rdc=true ./reduction.cpp ./reduction_wp_kernel.cu\n```", "```cpp\n__global__ void\n atomic_reduction_kernel(float *data_out, float *data_in, int size)\n {\n     int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n     atomicAdd(&data_out[0], data_in[idx_x]);\n }\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o mixed_precision_single ./mixed_precision.cu\n```", "```cpp\n__global__ void\n reduction_kernel(float* g_out, float* g_in, unsigned int size)\n{\n    unsigned int idx_x = blockIdx.x * (2 * blockDim.x) + threadIdx.x;\n\n    thread_block block = this_thread_block();\n\n    // cumulates input with grid-stride loop and save to share memory\n    float sum[NUM_LOAD] = { 0.f };\n    for (int i = idx_x; i < size; i += blockDim.x \n         * gridDim.x * NUM_LOAD)\n    {\n        for (int step = 0; step < NUM_LOAD; step++)\n            sum[step] += (i + step * blockDim.x * gridDim.x < size) ? \n                         g_in[i + step * blockDim.x * gridDim.x] : 0.f;\n    }\n    for (int i = 1; i < NUM_LOAD; i++)\n        sum[0] += sum[i];\n    // warp synchronous reduction\n    sum[0] = block_reduce_sum(block, sum[0]);\n\n    sum[0] = block_reduce_sum(sum[0]);\n\n    // Performing Atomic Add per block\n    if (block.thread_rank() == 0) {\n        atomicAdd(&g_out[0], sum);\n    }\n}\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o reduction_atomic_block ./reduction.cpp ./reduction_blk_atmc_kernel.cu\n```", "```cpp\n__global__ void hfma_kernel(half *d_x, half *d_y, float *d_z, int size)\n {\n     int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n     int stride = gridDim.x * blockDim.x;\n\n     half2 *dual_x = reinterpret_cast<half2*>(d_x);\n     half2 *dual_y = reinterpret_cast<half2*>(d_y);\n     float2 *dual_z = reinterpret_cast<float2*>(d_z);\n\n     extern __shared__ float2 s_data[];\n\n #if __CUDA_ARCH__ >= 530\n     for (int i = idx_x; i < size; i+=stride) {\n         s_data[threadIdx.x] = __half22float2(__hmul2(dual_y[i], \n                                                      dual_x[i]));\n         __syncthreads();\n         dual_z[i] = s_data[threadIdx.x];\n     }\n     #else\n     for (int i = idx_x; i < size; i+=stride) {\n         s_data[threadIdx.x] = __half22float2(dual_x[i]) * \n                               __half22float2(dual_y[i]);\n         __syncthreads();\n         dual_z[i] = s_data[threadIdx.x];\n     }\n     #endif\n }\n```", "```cpp\nint n_threads = 256;\nint num_sms;\nint num_blocks_per_sm;\ncudaDeviceGetAttribute(&num_sms, cudaDevAttrMultiProcessorCount, 0);\ncudaOccupancyMaxActiveBlocksPerMultiprocessor(&num_blocks_per_sm,   \n    hfma_kernel, n_threads, n_threads*sizeof(float2));\nint n_blocks = min(num_blocks_per_sm * num_sms, \n                   (size/2 + n_threads - 1) / n_threads);\nhfma_kernel<<< n_blocks, n_threads, n_threads * sizeof(float2) \n               >>>(X.d_ptr_, Y.d_ptr_, Z.d_ptr_, size/2);\n```", "```cpp\n__global__ void dp4a_kernel(char *d_x, char *d_y, int *d_z, int size)\n {\n     int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n     int stride = gridDim.x * blockDim.x;\n\n #if __CUDA_ARCH__ >= 610\n     char4 *quad_x = (char4 *)d_x;\n     char4 *quad_y = (char4 *)d_y;\n\n     for (int i = idx_x; i < size; i+=stride)\n         d_z[i] = __dp4a(quad_y[i], quad_x[i], 0);\n #else\n     for (int i = idx_x; i < size; i+=4*stride) {\n         int sum = 0;\n         for (int j = 0; j < 4; j++)\n             sum += d_y[4 * i + j] * d_x[4 * i + j];\n         d_z[i] = sum + 0;\n     }\n #endif\n }\n```", "```cpp\nint n_threads = 256;\nint num_sms;\nint num_blocks_per_sm;\ncudaDeviceGetAttribute(&num_sms, cudaDevAttrMultiProcessorCount, 0);\ncudaOccupancyMaxActiveBlocksPerMultiprocessor(&num_blocks_per_sm, \n    dp4a_kernel, n_threads, n_threads*sizeof(int));\nint n_blocks = min(num_blocks_per_sm * num_sms, (size/4 + n_threads \n                                                  - 1) / n_threads);\ndp4a_kernel<<< n_blocks, n_threads, n_threads * sizeof(int) >>>  \n    (X.d_ptr_, Y.d_ptr_, Z.d_ptr_, size/4);\n```", "```cpp\n# Single-precision\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o mixed_precision_single ./mixed_precision.cu\n\n# Half-precision\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o mixed_precision_half ./mixed_precision_half.cu\n\n# INT8 \n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o mixed_precision_int ./mixed_precision_int.cu\n```"]