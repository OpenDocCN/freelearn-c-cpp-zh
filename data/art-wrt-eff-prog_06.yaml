- en: '*Chapter 5*: Threads, Memory, and Concurrency'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第5章*：线程、内存和并发性'
- en: Until now, we have studied the performance of a single CPU executing one program,
    one instruction sequence. In the introduction of [*Chapter 1*](B16229_01_Epub_AM.xhtml#_idTextAnchor014),
    *Introduction to Performance and Concurrency*, we mentioned that this is not the
    world we live in anymore, and we never touched the subject again. Instead, we
    studied every aspect of the performance of a single-threaded program running on
    one CPU. We have now learned all we need to know about the performance of one
    thread and are ready to study the performance of concurrent programs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经研究了单个CPU执行一个程序，一个指令序列的性能。在[*第1章*](B16229_01_Epub_AM.xhtml#_idTextAnchor014)的介绍中，*性能和并发性简介*，我们提到这不再是我们生活的世界，然后再也没有涉及这个主题。相反，我们研究了单线程程序在单个CPU上运行的性能的每个方面。现在我们已经学会了关于单个线程性能的所有知识，并准备好研究并发程序的性能。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Overview of threads
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程概述
- en: Multi-threaded and multi-core memory accesses
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多线程和多核内存访问
- en: Data races and memory access synchronization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据竞争和内存访问同步
- en: Locks and atomic operations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 锁和原子操作
- en: Memory model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存模型
- en: Memory order and memory barriers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存顺序和内存屏障
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Again, you will need a C++ compiler and a micro-benchmarking tool, such as the
    Google Benchmark library we used in the previous chapter (found at [https://github.com/google/benchmark](https://github.com/google/benchmark)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您将需要一个C++编译器和一个微基准测试工具，比如我们在上一章中使用的Google Benchmark库（在[https://github.com/google/benchmark](https://github.com/google/benchmark)找到）。
- en: The code for the chapter can be found at [https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter05](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter05)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter05](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter05)找到。
- en: Understanding threads and concurrency
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解线程和并发性
- en: 'All high-performance computers today have multiple CPUs or multiple CPU cores
    (independent processors in a single package). Even most laptop computers have
    at least two, often four, cores. As we have said many times, in the context of
    performance, efficiency is not leaving any hardware idle; a program cannot be
    efficient or high-performing if it uses only a fraction of the computing power,
    such as one of many CPU cores. There is only one way for a program to use more
    than one processor at a time: we have to run multiple threads or processes. As
    a side note, this isn''t the only way to use multiple processors for the benefit
    of the user: very few laptops, for example, are used for high-performance computing.
    Instead, they use multiple CPUs to better run different and independent programs
    at the same time. It is a perfectly good use model, just not the one we are interested
    in in the context of high-performance computing. HPC systems usually run one program
    on each computer at any time, even one program on many computers in case of distributed
    computations. How does one program use many CPUs? Usually, the program runs multiple
    threads.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 今天所有高性能计算机都有多个CPU或多个CPU核心（单个封装中的独立处理器）。即使大多数笔记本电脑也至少有两个，通常是四个核心。正如我们多次提到的，在性能方面，效率就是不让任何硬件空闲；如果程序只使用了计算能力的一部分，比如多个CPU核心中的一个，那么它就不能高效或高性能。程序要同时使用多个处理器的唯一方法是：我们必须运行多个线程或进程。顺便说一句，这并不是利用多个处理器为用户带来好处的唯一方法：例如，很少有笔记本电脑用于高性能计算。相反，它们使用多个CPU来更好地同时运行不同和独立的程序。这是一个完全合理的使用模式，只是不是我们在高性能计算的背景下感兴趣的。HPC系统通常一次在每台计算机上运行一个程序，甚至在分布式计算的情况下，一次在许多计算机上运行一个程序。一个程序如何使用多个CPU？通常，程序运行多个线程。
- en: What is a thread?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是线程？
- en: 'A **thread** is a sequence of instructions that can be executed independently
    of other threads. Multiple threads are running concurrently within the same program.
    All threads share the same memory, so, by definition, threads of the same process
    run on the same machine. We have mentioned that an HPC program can also consist
    of multiple processes. A distributed program runs on multiple machines and utilizes
    many separate processes. The subject of distributed computing is outside the scope
    of this book: we are learning how to maximize the performance of each of these
    processes.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**线程**是一系列指令，可以独立于其他线程执行。多个线程在同一个程序中同时运行。所有线程共享同一内存，因此，根据定义，同一进程的线程在同一台机器上运行。我们已经提到HPC程序也可以由多个进程组成。分布式程序在多台机器上运行，并利用许多独立的进程。分布式计算的主题超出了本书的范围：我们正在学习如何最大化每个进程的性能。'
- en: So, what can we say about the performance of multiple threads? First of all,
    having multiple instruction sequences execute at the same time is beneficial only
    if the system has enough resources actually to execute them *at the same time*.
    Otherwise, the operating system is just switching between different threads to
    allow each one a time slice to execute.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，关于多线程的性能，我们能说些什么呢？首先，只有当系统有足够的资源来同时执行多个指令序列时，同时执行多个指令序列才是有益的。否则，操作系统只是在不同的线程之间切换，以允许每个线程执行一个时间片。
- en: 'On a single processor, a thread that is busy computing provides as much work
    as the processor can handle. This is true even if the thread is not using all
    of the computing units or is waiting on memory accesses: the processor can execute
    only one instruction sequence at a time – it has a single program counter. Now,
    if the thread is waiting on something, such as user input or network traffic,
    the CPU is idle and could execute another thread without impacting the performance
    of the first one. Again, the operating system handles the switching between the
    threads. Note that waiting on memory does not count as waiting in this sense:
    when a thread is waiting on memory, it just takes longer to execute one instruction.
    When a thread is waiting on I/O, it has to make an operating system call, then
    it''s blocked by the OS and isn''t executing anything at all until the OS wakes
    it up to process the data.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个处理器上，一个忙于计算的线程提供了处理器可以处理的工作量。即使线程没有使用所有的计算单元或者在等待内存访问，这也是真实的：处理器一次只能执行一个指令序列
    - 它只有一个程序计数器。现在，如果线程在等待某些东西，比如用户输入或网络流量，CPU是空闲的，可以在不影响第一个线程性能的情况下执行另一个线程。再次强调，操作系统处理线程之间的切换。需要注意的是，在这种情况下，等待内存不算是等待：当线程在等待内存时，执行一个指令需要更长的时间。当线程在等待I/O时，它必须进行操作系统调用，然后被操作系统阻塞，直到操作系统唤醒它来处理数据。
- en: All threads that do heavy computing require adequate resources if the goal is
    to make the program more efficient overall. Usually, when we think about resources
    for threads, we have in mind multiple processors or processor cores. But there
    are other ways to increase resource utilization through concurrency as well, as
    we are about to see.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有进行大量计算的线程都需要足够的资源，如果目标是使程序整体更加高效。通常，当我们考虑线程的资源时，我们会想到多个处理器或处理器核心。但通过并发性也有其他增加资源利用率的方法，我们将很快看到。
- en: Symmetric multi-threading
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对称多线程
- en: 'We have mentioned several times throughout the book that a processor has a
    lot of computing hardware, and most programs rarely, if ever, use all of it: the
    data dependencies in the program limit how much computation the processor can
    do at any time. If the processor has spare computing units, can''t it execute
    another thread at the same time to improve efficiency? This is the idea behind
    **Symmetric Multi-Threading** (**SMT**), also known as **hyper-threading**.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在整本书中多次提到，处理器有大量的计算硬件，大多数程序很少（如果有的话）会全部使用：程序中的数据依赖限制了处理器在任何时候可以进行多少计算。如果处理器有多余的计算单元，它不能同时执行另一个线程以提高效率吗？这就是**对称多线程**（**SMT**）的理念，也被称为**超线程**。
- en: 'An SMT-capable processor has a single set of registers and computing units,
    but two (or more) program counters and an extra copy of whatever additional hardware
    it uses to maintain the state of a running thread (the exact implementation varies
    from one processor to another). The end result is: a single processor appears
    to the operating system and the program as two (usually) or more separate processors,
    each capable of running one thread. In reality, all threads running on one CPU
    compete for the shared internal resources such as registers. The SMT can offer
    significant performance gains if each thread does not make full use of these shared
    resources. In other words, it compensates for the inefficiency of one thread by
    running several such threads.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 支持SMT的处理器有一组寄存器和计算单元，但有两个（或更多）程序计数器，以及维护运行线程状态的额外副本的任何其他硬件（具体实现因处理器而异）。最终结果是：单个处理器对操作系统和程序来说看起来像是两个（通常）或更多个独立的处理器，每个都能运行一个线程。实际上，所有在一个CPU上运行的线程都竞争共享的内部资源，比如寄存器。如果每个线程没有充分利用这些共享资源，SMT可以提供显著的性能提升。换句话说，它通过运行多个这样的线程来弥补一个线程的低效率。
- en: 'In practice, most SMT-capable processors can run two threads, and the performance
    gains vary widely. It is rare to see 100% speedup (two threads both run at full
    speed). Usually, the practical speedup is between 25% and 50% (the second thread
    is effectively running at quarter-speed to half-speed), but some programs get
    no speedup at all. For the purposes of this book, we will not treat the SMT threads
    in any special way: to the program, an SMT processor appears as two processors,
    and anything we say about the performance of two *real* threads running on separate
    cores applies equally to the performance of two threads that happen to run on
    the same core. At the end of the day, you have to measure whether running more
    threads than you have physical cores provides any speedup to your program and,
    based on that, decide how many threads you want to run.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，大多数支持SMT的处理器可以运行两个线程，性能提升的幅度差异很大。很少见到100%的加速（两个线程都以全速运行）。通常，实际的加速在25%到50%之间（第二个线程实际上以四分之一到半的速度运行），但有些程序根本没有加速。在本书中，我们不会特别对待SMT线程：对于程序来说，SMT处理器看起来就像两个处理器，我们对两个*真实*线程在不同核心上运行的性能所说的任何事情同样适用于在同一个核心上运行的两个线程的性能。最终，你必须测量运行比物理核心更多的线程是否为程序提供了任何加速，并根据这一点决定要运行多少线程。
- en: Whether we're sharing entire physical cores or the logical cores created by
    the SMT hardware, the performance of a concurrent program largely depends on how
    independently the threads can work. This is determined, first and foremost, by
    the algorithm and the partitioning of work between threads; both matters have
    hundreds of books dedicated to them but lie outside of the scope of this book.
    Instead, we now focus on the fundamental factors that affect thread interaction
    and determine the success or failure of a particular implementation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们是共享整个物理核心还是由SMT硬件创建的逻辑核心，并发程序的性能在很大程度上取决于线程能够独立工作的程度。这首先取决于算法和工作在线程之间的分配；这两个问题有数百本专门的书籍来讨论，但超出了本书的范围。相反，我们现在专注于影响线程交互并决定特定实现成功或失败的基本因素。
- en: Threads and memory
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程和内存
- en: 'Since there is no performance benefit to time-slice a CPU between multiple
    computing threads, we can assume for the rest of this chapter that we run one
    HPC thread on every processor core (or one thread on every *logical core* presented
    by an SMT processor). As long as these threads do not compete for any resources,
    they run entirely independently of each other, and we enjoy *perfect speedup*:
    two threads will do twice as much work in the same time as could be done by one
    thread. If the work can be divided perfectly between two threads in a way that
    does not require any interaction between them, two threads will solve the problem
    in half the time.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在多个计算线程之间进行时间分片对CPU没有性能优势，我们可以假设在本章的其余部分中，在每个处理器核心上运行一个HPC线程（或者在SMT处理器提供的每个*逻辑核心*上运行一个线程）。只要这些线程不竞争任何资源，它们就完全独立运行，并且我们可以享受*完美的加速*：两个线程将在相同的时间内完成两倍于一个线程所能完成的工作。如果工作可以完美地在两个线程之间分配，而不需要它们之间的任何交互，那么两个线程将在一半的时间内解决问题。
- en: 'This ideal situation does happen, but not often; more importantly, if it happens,
    you are already prepared to get the best performance from your program: you know
    how to optimize the performance of a single thread.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种理想的情况确实会发生，但并不经常；更重要的是，如果发生了，你已经准备好从你的程序中获得最佳性能：你知道如何优化单个线程的性能。
- en: The hard part of writing efficient concurrent programs begins when the work
    done by different threads is not entirely independent, and the threads start to
    compete for resources. But if each thread has full use of its CPU, what else is
    there left to compete for? What is left is the memory, which is shared between
    all threads and is, therefore, a common resource. This is why any exploration
    of the performance of multi-threaded programs focuses almost exclusively on the
    issues arising from the interaction between threads through memory.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 编写高效并发程序的难点在于当不同线程执行的工作不完全独立时，线程开始竞争资源。但如果每个线程都充分利用其CPU，那么还有什么可以竞争的呢？剩下的就是内存，它在所有线程之间共享，因此是一个共同的资源。这就是为什么对多线程程序性能的探索几乎完全集中在线程之间通过内存交互引起的问题上。
- en: There is another aspect of writing high-performance concurrent programs, and
    that is dividing work between threads and processes that together comprise the
    program. But to learn about that, you have to find a book on parallel programming.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 编写高性能并发程序的另一个方面是在组成程序的线程和进程之间分配工作。但要了解这一点，你必须找一本关于并行编程的书。
- en: It turns out that the memory, which was already the *long pole* of performance,
    is even more of a problem when we add concurrency. While the fundamental limits
    imposed by the hardware cannot be overcome, most programs aren't performing even
    close to these limits, and there is much room for a skillful programmer to improve
    the efficiency of their code; this chapter gives the reader the necessary knowledge
    and tools.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，内存，已经是性能的*长杆*，在添加并发性后更加成为问题。虽然硬件施加的基本限制是无法克服的，但大多数程序的性能远未接近这些限制，而且熟练的程序员有很大的空间来提高其代码的效率；本章为读者提供了必要的知识和工具。
- en: Let us first examine the performance of the memory system in the presence of
    threads. We do it the same way as in the last chapter, by measuring the speed
    of reading or writing into memory, only now we use several threads to read or
    write at the same time. We start with the case where each thread has its own memory
    region to access. We are not sharing any data between threads, but we are sharing
    the hardware resources, such as memory bandwidth.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先检查在存在线程的情况下内存系统的性能。我们以与上一章相同的方式进行，通过测量读取或写入内存的速度，只是现在我们使用多个线程同时读取或写入。我们从每个线程都有自己的内存区域来访问的情况开始。我们不在线程之间共享任何数据，但我们在共享硬件资源，比如内存带宽。
- en: 'The memory benchmark itself is almost the same as the one we used earlier.
    In fact, the benchmark function itself is exactly the same. For example, to benchmark
    sequential reading, we use this function:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 内存基准本身与我们之前使用的基本相同。实际上，基准函数本身完全相同。例如，要对顺序读取进行基准测试，我们使用这个函数：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that the memory is allocated inside the benchmark function. If this function
    is called from multiple threads, each thread has its own memory region to read.
    This is exactly what the Google Benchmark library does when it runs multi-threaded
    benchmarks. To run a benchmark on more than one thread, you just need to use the
    right arguments:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，内存是在基准函数内分配的。如果这个函数从多个线程中调用，每个线程都有自己的内存区域进行读取。这正是谷歌基准库在运行多线程基准测试时所做的。要在多个线程上运行基准测试，只需要使用正确的参数：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can specify as many runs as you want for different thread counts or use
    the `ThreadRange()` argument to generate a range of 1, 2, 4, 8, … threads. You
    have to decide how many threads you want to use; for an HPC benchmark, in general,
    there is no reason to go over the number of CPUs you have (accounting for SMT).
    The benchmarking of other memory access modes, such as random access, is done
    the same way; you have already seen the code in the last chapter. For writing,
    we would need something to write; any value will do:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为不同的线程计数指定尽可能多的运行次数，或者使用`ThreadRange()`参数生成1、2、4、8、...线程的范围。您必须决定要使用多少个线程；对于HPC基准测试，一般来说，没有理由超过您拥有的CPU数量（考虑SMT）。其他内存访问模式的基准测试，比如随机访问，也是以相同的方式进行的；您已经在上一章中看到了代码。对于写入，我们需要一些内容来写入；任何值都可以：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now it is time to show the results. For example, here is the memory throughput
    for sequential writes:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是展示结果的时候了。例如，这是顺序写入的内存吞吐量：
- en: '![Figure 5.1 – Memory throughput (words per nanosecond) for sequential writing
    of 64-bit integers as a function of memory range for 1 through 16 threads'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.1 - 64位整数顺序写入的内存吞吐量（每纳秒字数）作为内存范围的函数，线程数为1到16'
- en: '](img/Figure_5.1_B16229.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.1_B16229.jpg)'
- en: Figure 5.1 – Memory throughput (words per nanosecond) for sequential writing
    of 64-bit integers as a function of memory range for 1 through 16 threads
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1-64位整数的顺序写入的内存吞吐量（每纳秒字数）作为内存范围的函数，为1到16个线程
- en: 'The overall trend is already familiar to us: we see the speed jumps corresponding
    to the cache sizes. Now we focus on the differences between the curves for a different
    number of threads. We have the results for 1 through 16 threads (the machine used
    to collect these measurements does indeed have at least 16 physical CPU cores).
    Let us start from the left side of the plot. Here, the speed is limited by the
    L1 cache (up to 32 KB) then by the L2 cache (256 KB). This processor has separate
    L1 and L2 caches for each core, so, as long as the data fits into the L2 cache,
    there should not be any interaction between the threads since they don''t share
    any resources: each thread has its own cache. In reality, this is not quite true,
    there are other CPU components that are still shared even for small memory ranges,
    but it''s almost true: the throughput for 2 threads is twice as large as that
    for 1 thread, 4 threads write to memory twice as fast again, and 16 threads are
    almost 4 times faster than 4 threads.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 总体趋势对我们来说已经很熟悉：我们看到与缓存大小相对应的速度跳跃。现在我们关注不同线程数量的曲线之间的差异。我们有1到16个线程的结果（用于收集这些测量数据的机器确实至少有16个物理CPU核心）。让我们从图的左侧开始。在这里，速度受到L1缓存（最多32
    KB）的限制，然后是L2缓存（256 KB）。这个处理器为每个核心都有单独的L1和L2缓存，因此只要数据适合L2缓存，线程之间就不应该有任何交互，因为它们不共享任何资源：每个线程都有自己的缓存。实际上，这并不完全正确，即使对于小内存范围，仍然有其他共享的CPU组件，但几乎是正确的：2个线程的吞吐量是1个线程的两倍，4个线程的写入速度再次快两倍，16个线程几乎比4个线程快4倍。
- en: 'The picture changes drastically as we exceed the size of the L2 cache and cross
    into the L3 cache and then the main memory: on this system, the L3 cache is shared
    between all the CPU cores. The main memory is shared too, although different memory
    banks are *closer* to different CPUs (the non-uniform memory architecture). For
    1, 2, and even 4 threads, the throughput continues to scale with the number of
    threads: the main memory appears to have enough bandwidth for up to 4 processors
    writing into it at full speed. Then things take a turn for the worse: the throughput
    almost doesn''t increase when we go from 6 to 16 threads. We have saturated the
    memory bus: it can''t write the data any faster.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们超过L2缓存的大小并进入L3缓存，然后是主内存，图片发生了巨大的变化：在这个系统上，L3缓存是所有CPU核心共享的。主内存也是共享的，尽管不同的内存bank*更接近*不同的CPU（非均匀内存架构）。对于1、2甚至4个线程，吞吐量继续随着线程数量增加而增加：主内存似乎有足够的带宽，可以支持最多4个处理器以全速写入。然后情况变得更糟：当我们从6个线程增加到16个线程时，吞吐量几乎不再增加。我们已经饱和了内存总线：它无法更快地写入数据。
- en: 'If this wasn''t bad enough, consider that these results were obtained on the
    latest hardware at the time of writing (2020). In 2018, the same chart presented
    by the author in one of his classes looked like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这还不够糟糕，请考虑这些结果是在撰写时的最新硬件上获得的（2020年）。在2018年，作者在他的一堂课上呈现的同一张图表如下：
- en: '![Figure 5.2 – Memory throughput of an older (2018) CPU'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.2-较旧（2018年）CPU的内存吞吐量'
- en: '](img/Figure_5.2_B16229.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.2_B16229.jpg)'
- en: Figure 5.2 – Memory throughput of an older (2018) CPU
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2-较旧（2018年）CPU的内存吞吐量
- en: This system has a memory bus that can be completely saturated by just two threads.
    Let us see what the implications of this fact for the performance of a concurrent
    program are.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统有一个内存总线，只需两个线程就可以完全饱和。让我们看看这个事实对并发程序性能的影响。
- en: Memory-bound programs and concurrency
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存绑定程序和并发性
- en: 'The same results can be presented in a different way: by plotting the memory
    speed per thread versus the number of threads relative to that for one thread,
    we focus exclusively on the effect of concurrency on the memory speed:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的结果可以以不同的方式呈现：通过绘制每个线程的内存速度与相对于一个线程的线程数量的图，我们专注于并发对内存速度的影响。
- en: '![Figure 5.3 – Memory throughput, relative to the throughput for a single thread,
    vs. thread count'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.3-内存吞吐量，相对于单个线程的吞吐量，与线程计数'
- en: '](img/Figure_5.3_B16229.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.3_B16229.jpg)'
- en: Figure 5.3 – Memory throughput, relative to the throughput for a single thread,
    vs. thread count
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3-内存吞吐量，相对于单个线程的吞吐量，与线程计数
- en: With the memory speed normalized, so it's always 1 for the single thread, it
    is much easier to see that for small data sets that fit into L1 or L2 cache, the
    memory speed per thread remains almost the same even for 16 threads (each thread
    is writing at 80% of its single-threaded speed). However, as soon as we cross
    into the L3 cache or exceed its size, the speed goes down after 4 threads. Going
    from 8 to 16 threads provides only minimal improvement. There just isn't enough
    bandwidth in the system to write data to memory fast enough.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对内存速度进行归一化，使得单个线程的速度始终为1，我们更容易看到对于适合L1或L2缓存的小数据集，每个线程的内存速度几乎保持不变，即使对于16个线程（每个线程的写入速度为其单线程速度的80%）。然而，一旦我们跨入L3缓存或超过其大小，速度在4个线程后下降。从8到16个线程只提供了极小的改善。系统中没有足够的带宽来快速写入数据到内存。
- en: The results for different memory access patterns look similar, although the
    bandwidth for reading memory often scales slightly better than that for writing.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 不同内存访问模式的结果看起来很相似，尽管读取内存的带宽通常比写入内存的带宽略微好一些。
- en: We can see that if our program was memory-bound in the single-threaded case,
    so its performance was limited by the speed of moving the data to and from the
    main memory, there is a fairly hard limit on the performance improvement we can
    expect to gain from concurrency. If you think that this does not apply to you
    because you don't have an expensive 16-core processor, remember that cheaper processors
    come with a cheaper memory bus, so most 4-core systems don't have enough memory
    bandwidth for all cores either.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，如果我们的程序在单线程情况下受到内存限制，因此其性能受到将数据移动到和从主内存的速度的限制，那么我们可以期望从并发中获得的性能改进有一个相当严格的限制。如果你认为这不适用于你，因为你没有昂贵的16核处理器，那么请记住，更便宜的处理器配备了更便宜的内存总线，因此大多数4核系统也没有足够的内存带宽来满足所有核心。
- en: 'For multi-threaded programs, it is even more important to avoid becoming memory-bound.
    The implementation techniques that are useful here are splitting computations
    so more work can be done on smaller data sets that fit into L1 or L2 caches; rearranging
    the computations so more work can be done with fewer memory accesses, often at
    the expense of repeating some computations; optimizing the memory access patterns
    so the memory is accessed sequentially instead of randomly (even though you can
    saturate both access patterns, the total bandwidth of sequential accesses is much
    larger, so for the same amount of data your program may be memory-bound if you
    use random access and not limited by memory speed at all if you use sequential
    access). If the implementation techniques alone are insufficient and do not yield
    the desired performance improvements, the next step is to adapt the algorithm
    to the realities of concurrent programming: many problems have multiple algorithms
    that differ in their memory requirements. The fastest algorithm for a single-threaded
    program can often be outperformed by another algorithm that is better suited for
    concurrency: what we lose in the single-threaded execution speed, we make up for
    by brute force of scalable execution.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多线程程序来说，避免成为内存限制更加重要。在这里有用的实现技术包括分割计算，这样更多的工作可以在适合L1或L2缓存的较小数据集上完成；重新排列计算，这样更多的工作可以通过更少的内存访问完成，通常会重复一些计算；优化内存访问模式，使内存按顺序访问而不是随机访问（尽管两种访问模式都可以饱和，但顺序访问的总带宽要大得多，因此对于相同数量的数据，如果使用随机访问，程序可能会受到内存限制，而如果使用顺序访问，则根本不受内存速度限制）。如果仅靠实现技术是不够的，无法产生期望的性能改进，下一步就是调整算法以适应并发编程的现实：许多问题有多种算法，它们在内存需求上有所不同。单线程程序的最快算法通常可以被另一个更适合并发性的算法超越：虽然我们在单线程执行速度上失去了一些，但我们通过可扩展执行的蛮力来弥补。
- en: So far, we have assumed that each thread does its own work completely independently
    from all other threads. The only interaction between threads was indirect, due
    to contention for a limited resource such as memory bandwidth. This is the easiest
    kind of program to write, but most real-life programs do not allow such limitations.
    This brings with it a whole new set of performance problems, and it is time for
    us to learn about them.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设每个线程都完全独立于所有其他线程地完成自己的工作。线程之间的唯一交互是间接的，由于争夺内存带宽等有限资源。这是最容易编写的程序类型，但大多数现实生活中的程序都不允许这种限制。这带来了一整套全新的性能问题，现在是我们学习它们的时候了。
- en: Understanding the cost of memory synchronization
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解内存同步的成本
- en: The last section was all about running multiple threads on the same machine
    without any interaction between these threads. If you can split the work your
    program does between threads in a way that makes such implementation possible,
    by all means, do it. You cannot beat the performance of such an *embarrassingly
    parallel* program.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一节讨论了在同一台机器上运行多个线程而这些线程之间没有任何交互。如果你可以以一种使这种实现成为可能的方式来分割程序的工作，那么请务必这样做。你无法击败这种*尴尬并行*程序的性能。
- en: More often than not, threads must interact with each other because they are
    contributing work to a common result. Such interactions happen by means of threads
    communicating with each other through the one resource they share, the memory.
    We must now understand the performance implications of this.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 往往，线程必须相互交互，因为它们正在为一个共同的结果做出贡献。这种交互是通过线程通过它们共享的唯一资源——内存——相互通信来实现的。我们现在必须了解这种交互的性能影响。
- en: Let us start with a trivial example. Say we want to compute a sum of many values.
    We have many numbers to add, but, in the end, only one result. We have so many
    numbers to add that we want to split the work of adding them between several threads.
    But there is only one result value, so the threads have to interact with each
    other as they add to this value.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的例子开始。假设我们想要计算许多值的总和。我们有许多数字要相加，但最终只有一个结果。我们有这么多数字要相加，以至于我们想要在几个线程之间分割添加它们的工作。但只有一个结果值，所以线程必须在添加到这个值时相互交互。
- en: 'We can reproduce this problem in a micro-benchmark:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在微基准中重现这个问题：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For simplicity, we always increment the result by 1 (the cost of adding integers
    does not depend on the values, and we don't want to benchmark the generation of
    different values, just the addition itself). Since the benchmark function is called
    by each thread, any variable declared inside this function exists independently
    on the stack of each thread; these variables are not shared at all. To have a
    common result that both threads contribute to, the variable must be declared outside
    of the benchmark function, at the file scope (bad idea in general, but necessary
    and acceptable in the very limited context of the micro-benchmark).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，我们总是将结果增加1（添加整数的成本不取决于值，我们不想对生成不同值进行基准测试，只是对添加本身进行基准测试）。由于每个线程都调用基准函数，因此在此函数内声明的任何变量都独立存在于每个线程的堆栈上；这些变量根本不共享。为了有一个两个线程都能贡献的共同结果，变量必须在基准函数之外的文件范围内声明（一般来说这是个坏主意，但在微基准的非常有限的上下文中是必要且可以接受的）。
- en: 'Of course, this program has a much bigger problem than the global variable:
    the program is simply wrong, and its results are undefined. The problem is that
    we have two threads incrementing the same value. Incrementing a value is a 3-step
    process: the program reads the value from memory, increments it in the register,
    and writes the new value back into memory. It is entirely possible for both threads
    to read the same value (0) at the same time, increment it separately on each processor
    (1), and write it back. The thread that writes second simply overwrites the result
    of the first thread, and, after two increments, the result is 1 instead of 2\.
    Such *competition* of two threads for writing into the same memory location is
    called a **data race**.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个程序的问题远不止全局变量：这个程序是错误的，其结果是未定义的。问题在于我们有两个线程增加相同的值。增加一个值是一个3步过程：程序从内存中读取值，在寄存器中增加它，然后将新值写回内存。完全有可能两个线程同时读取相同的值（0），在每个处理器上分别增加它（1），然后写回。第二个写入的线程简单地覆盖了第一个线程的结果，经过两次增加，结果是1而不是2。这两个线程竞争写入同一内存位置的情况被称为数据竞争。
- en: 'Now that you understand why such unguarded concurrent accesses are a problem,
    you may as well forget it; instead, follow this general rule: any program has
    undefined results if it accesses the same memory location from multiple threads
    without synchronization and at least one of these accesses is a write. This is
    very important: it is not necessary for you to figure out exactly what sequence
    of operations must happen for the result to be incorrect. In fact, there is nothing
    to be gained in this line of reasoning at all. Any time you have two or more threads
    accessing the same memory location, you have a data race unless you can guarantee
    one of two things: either all accesses are read-only, or all accesses use the
    correct memory synchronization (which we are yet to learn about).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了为什么这样的无保护并发访问是一个问题，你可能会忘记它；相反，遵循这个一般规则：如果一个程序从多个线程访问相同的内存位置而没有同步，并且其中至少有一个访问是写入的，那么该程序的结果是未定义的。这是非常重要的：你不需要确切地弄清楚为了结果是不正确而必须发生的操作序列。事实上，在这种推理中根本没有任何收获。任何时候你有两个或更多的线程访问相同的内存位置，除非你能保证两件事中的一件：要么所有访问都是只读的，要么所有访问都使用正确的内存同步（我们还要学习）。
- en: 'Our problem of computing the sum requires that we write the answer into the
    result variable, so the access is definitely not read-only. The synchronization
    of memory accesses is, in general, provided by a mutex: every access to a variable
    shared between threads must be guarded by a mutex (it must, of course, be the
    same mutex for all threads).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算总和的问题要求我们将答案写入结果变量，因此访问肯定不是只读的。内存访问的同步通常由互斥锁提供：每次访问线程之间共享的变量都必须由互斥锁保护（当然，对于所有线程来说，必须是相同的互斥锁）。
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The lock guard locks the mutex in its constructor and unlocks it in the destructor.
    Only one thread at a time can have the lock and, thus, increment the shared result
    variable. The other threads are blocked on the lock until the first thread releases
    it. Note that *all* accesses must be locked, *both* reads and writes, as long
    as at least one thread is modifying the variable.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 锁卫在其构造函数中锁定互斥锁，并在析构函数中解锁它。一次只有一个线程可以拥有锁，因此增加共享结果变量。其他线程在锁上被阻塞，直到第一个线程释放它。请注意，只要至少有一个线程修改变量，*所有*访问都必须被锁定，*包括*读取和写入。
- en: 'Locks are the simplest way to ensure the correctness of a multi-threaded program,
    but they are not the easiest thing to study in terms of performance. They are
    fairly complex entities and often involve a system call. We will start with a
    synchronization option that is, in this particular case, easier to analyze: the
    atomic variable.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 锁是确保多线程程序正确性的最简单方法，但从性能的角度来看，它们并不是最容易研究的东西。它们是相当复杂的实体，通常涉及系统调用。我们将从一个在这种特定情况下更容易分析的同步选项开始：原子变量。
- en: 'C++ gives us an option to declare a variable to be atomic. It means that all
    supported operations on this variable are performed as single, non-interruptible,
    atomic transactions: any other thread observing this variable will see its state
    either before the atomic operation or after it, but never in the middle of the
    operation. For example, all integer atomic variables in C++ support atomic increment
    operations: if one thread is executing the operation, no other thread can access
    this variable until the first operation is complete. These operations require
    certain hardware support: for example, the atomic increment is a special hardware
    instruction that reads the old value, increments it, and writes the new value
    all as a single hardware operation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: C++给了我们一个选项，可以声明一个变量为原子变量。这意味着对这个变量的所有支持的操作都作为单个、不可中断的原子事务执行：观察这个变量的任何其他线程都会在原子操作之前或之后看到它的状态，但永远不会在操作中间。例如，在C++中，所有整数原子变量都支持原子增量操作：如果一个线程正在执行该操作，其他线程就无法访问该变量，直到第一个操作完成。这些操作需要特定的硬件支持：例如，原子增量是一个特殊的硬件指令，它读取旧值，增加它，并将新值作为单个硬件操作写入。
- en: For our example, an atomic increment is all we need. It must be stressed that,
    whatever synchronization mechanism we decided to use, all threads must use the
    same mechanism for concurrent accesses to a particular memory location. If we
    use atomic operations on one thread, it guarantees no data races as long as all
    threads use atomic operations. If another thread uses a mutex or non-atomic access,
    all guarantees are void, and the result is undefined again.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的例子，原子增量就是我们需要的。必须强调的是，无论我们决定使用什么样的同步机制，所有线程都必须使用相同的机制来并发访问特定的内存位置。如果我们在一个线程上使用原子操作，只要所有线程都使用原子操作，就不会有数据竞争。如果另一个线程使用互斥锁或非原子访问，所有的保证都将失效，结果再次是未定义的。
- en: 'Let us rewrite our benchmark to use C++ atomic operations:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重写我们的基准测试来使用C++原子操作：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The program is now correct: there are no data races here. It is not necessarily
    accurate since a single increment is a very short time interval to measure; we
    really should unroll the loop manually or with a macro and do several increments
    per loop iteration (we have done that in the last chapter, so you can see the
    macro there). Let us see how well it performs. If there were no interaction between
    the threads, two threads would compute the sum in half the time it takes one thread
    to do it:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在是正确的：这里没有数据竞争。这并不一定准确，因为单个增量是一个非常短的时间间隔来测量；我们真的应该手动展开循环或使用宏，并在每个循环迭代中进行多次增量（我们在上一章中已经这样做了，所以你可以在那里看到宏）。让我们看看它的表现如何。如果线程之间没有交互，两个线程计算总和所需的时间将是一个线程所需时间的一半：
- en: '![Figure 5.4 – Atomic increment time in a multi-threaded program'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.4 - 多线程程序中的原子增量时间'
- en: '](img/Figure_5.4_B16229.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.4_B16229.jpg)'
- en: Figure 5.4 – Atomic increment time in a multi-threaded program
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 - 多线程程序中的原子增量时间
- en: 'We have normalized the results to show the average time of a single increment,
    that is, the time to compute the sum divided by the total number of additions.
    The performance of this program is very disappointing: not only is there no improvement,
    but, in fact, it takes longer to compute the sum on two threads than on one.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经对结果进行了归一化，以显示单个增量的平均时间，也就是计算总和所需的时间除以总加法次数。这个程序的性能非常令人失望：不仅没有改进，而且事实上，在两个线程上计算总和所需的时间比一个线程上还要长。
- en: 'The results are even worse if we use the more conventional mutex:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用更传统的互斥锁，结果甚至更糟：
- en: '![Figure 5.5 – Increment time in a multi-threaded program with a mutex'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.5 - 使用互斥锁的多线程程序中的增量时间'
- en: '](img/Figure_5.5_B16229.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.5_B16229.jpg)'
- en: Figure 5.5 – Increment time in a multi-threaded program with a mutex
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 - 使用互斥锁的多线程程序中的增量时间
- en: 'First of all, as we expected, locking the mutex is a fairly expensive operation
    even on one thread: 23 nanoseconds for a mutex-guarded increment versus 7 nanoseconds
    for the atomic increment. The performance degrades much faster as the number of
    threads increases.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，正如我们预期的那样，即使在一个线程上，锁定互斥锁也是一个相当昂贵的操作：使用互斥锁的增量需要23纳秒，而原子增量只需要7纳秒。随着线程数量的增加，性能会更快地下降。
- en: There is a very important lesson to be learned from these experiments. The portion
    of the program that accesses the shared data will never scale. The best performance
    you can have for accessing the shared data is single-threaded performance. As
    soon as you have two or more threads accessing the same data at the same time,
    the performance can only get worse. Of course, if two threads are accessing the
    same data at different times, they don't really interact with each other, so you
    get the single-threaded performance both times. The performance advantage of a
    multi-threaded program comes from the computations that the threads do independently,
    without synchronization. By definition, such computations are done on data that
    is not shared (if you want your program to be correct, anyway). But why are concurrent
    accesses to the shared data so expensive? In the next section, we will learn the
    reason. We will also learn a very important lesson on carefully interpreting measurements.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些实验中可以得出一个非常重要的教训。访问共享数据的程序部分永远不会扩展。访问共享数据的最佳性能是单线程性能。一旦有两个或更多线程同时访问相同的数据，性能只会变得更糟。当然，如果两个线程在不同时间访问相同的数据，它们实际上并不相互交互，因此两次都会获得单线程性能。多线程程序的性能优势来自线程独立进行的计算，无需同步。根据定义，这样的计算是在不共享的数据上进行的（无论如何，如果你希望你的程序是正确的）。但是为什么并发访问共享数据如此昂贵？在下一节中，我们将了解原因。我们还将学到一个非常重要的关于仔细解释测量结果的教训。
- en: Why data sharing is expensive
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么数据共享如此昂贵
- en: 'As we have just seen, concurrent (simultaneous) access of the shared data is
    a real performance killer. Intuitively, it makes sense: in order to avoid a data
    race, only one thread can operate on the shared data at any given time. We can
    accomplish this with a mutex or use an atomic operation if one is available. Either
    way, when one thread is, say, incrementing the shared variable, all other threads
    have to wait. Our measurements in the last section confirm it.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚刚看到的，共享数据的并发（同时）访问真的会严重影响性能。直观上讲，这是有道理的：为了避免数据竞争，任何给定时间只有一个线程可以操作共享数据。我们可以通过互斥锁或使用原子操作来实现这一点。无论哪种方式，当一个线程，比如说，增加共享变量时，所有其他线程都必须等待。我们上一节的测量结果证实了这一点。
- en: However, before taking any action based on observations and experiments, it
    is critically important to understand precisely what we measured and what can
    be concluded with certainty.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在基于观察和实验的任何行动之前，准确理解我们测量了什么以及可以得出什么结论至关重要。
- en: 'It is easy to describe what was observed: incrementing a shared variable from
    multiple threads at the same time does not scale at all and, in fact, is slower
    than using just one thread. This is true for both atomic shared variables and
    non-atomic variables guarded by a mutex. We have not tried to measure unguarded
    access to a non-atomic variable because such an operation leads to undefined behavior
    and incorrect results. We also know that unguarded access to variables that are
    thread-specific (not shared) scales very well with the number of threads, at least
    until we saturate the aggregate memory bandwidth (which can only happen if we
    write large amounts of data; for a single variable this is not an issue). Analyzing
    your experimental results critically and without unjustified preconceptions is
    a very important skill, so let us state again what we know: guarded access to
    shared data is slow and unguarded access to non-shared data is fast. If we conclude
    from this that data sharing makes your program slow, we are making an assumption:
    **shared data** is what''s important, **guarded access** is not. This brings up
    another very important point you should remember when doing performance measurements:
    when comparing two versions of the program, try to change only one thing at a
    time and measure the result.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易描述所观察到的情况：同时从多个线程增加共享变量根本不会扩展，并且实际上比只使用一个线程更慢。这对于原子共享变量和受互斥锁保护的非原子变量都是如此。我们没有尝试测量对非原子变量的无保护访问，因为这样的操作会导致未定义的行为和不正确的结果。我们也知道对于线程特定的（非共享）变量的无保护访问会随着线程数量的增加而扩展得非常好，至少直到我们饱和了总内存带宽（只有在我们写入大量数据时才会发生；对于单个变量来说，这不是问题）。批判性地分析你的实验结果并且没有不合理的先入之见是非常重要的技能，所以让我们再次声明我们所知道的：对共享数据的受保护访问很慢，对非共享数据的无保护访问很快。如果我们从中得出结论，即数据共享使程序变慢，那么我们就是在做一个假设：共享数据是重要的，受保护访问不是。这提出了另一个非常重要的观点，当进行性能测量时，你应该记住：当比较程序的两个版本时，尽量只改变一件事，并测量结果。
- en: 'The measurement we are missing is this one: non-shared access to guarded data.
    Of course, we don''t really need to protect accesses to data that is accessed
    by only one thread, but we are trying to understand exactly what makes shared
    data access so expensive: the fact that it is shared or the fact that it is atomic
    (or protected by the lock). We have to make one change at a time, so let us keep
    the atomic access and remove data sharing. There are at least two simple ways
    to do this. The first one is to create a global array of atomic variables and
    have each thread access its own array element:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们缺少的测量是这样的：对受保护数据的非共享访问。当然，我们不需要保护只被一个线程访问的数据，但我们试图准确理解共享数据访问为何如此昂贵：是因为它是共享的还是因为它是原子的（或者受锁保护）。我们必须一次只做一个改变，所以让我们保持原子访问并移除数据共享。至少有两种简单的方法可以做到这一点。第一种方法是创建一个原子变量的全局数组，并让每个线程访问自己的数组元素：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The thread index in Google Benchmark is unique for each thread, the numbers
    start from 0 and are compact (0, 1, 2…). The other simple way is to declare the
    variable in the `benchmark` function itself as shown in the following code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Google Benchmark中的线程索引对于每个线程是唯一的，数字从0开始并且是紧凑的（0, 1, 2...）。另一种简单的方法是在`benchmark`函数本身中声明变量，如下面的代码所示：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we are incrementing the same atomic integer as we did when we collected
    the measurements for *Figure 5.4*, only it is no longer shared between threads.
    This will tell us whether it is the sharing or the atomic variable that makes
    the increment slow. Here are the results:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们正在增加与我们在收集图5.4的测量时相同的原子整数，只是它不再在线程之间共享。这将告诉我们是共享还是原子变量使增量变慢。以下是结果：
- en: '![Figure 5.6 – Atomic increment time for shared and not shared variables'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.6 - 共享和非共享变量的原子增量时间'
- en: '](img/Figure_5.6_B16229.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.6_B16229.jpg)'
- en: Figure 5.6 – Atomic increment time for shared and not shared variables
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 - 共享和非共享变量的原子增量时间
- en: 'The **Shared** curve is the one from *Figure 5.4*, while the other two are
    from the benchmarks without data sharing. The benchmark with a local variable
    on each thread is labeled **Not shared** and behaves as such: the computation
    on two threads takes half the time compared to that on one thread, going to four
    threads cuts the time by half again, and so on. Remember that this is the average
    time of one increment operation: we do, say, 1 million increments in total, measure
    the total time it takes, and divide by a million. Since the variables that we
    increment are not shared between threads, we expect two threads to run twice as
    fast as one thread, so the **Not shared** result is exactly what we expected.
    The other benchmark, the one where we use an array of atomic variables, but each
    thread uses its own array element, also has no shared data. However, it performs
    as if the data was shared between threads, at least for a small number of threads,
    so we call it **False sharing**: nothing is really shared, but the program behaves
    as if it was.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 共享曲线是来自图5.4的曲线，而另外两个来自没有数据共享的基准测试。每个线程都有一个本地变量的基准测试被标记为“非共享”，并且表现为：两个线程上的计算时间是一个线程的一半，四个线程的时间再次减半，依此类推。请记住，这是一次增量操作的平均时间：我们总共进行了，比如说，100万次增量，测量总共花费的时间，然后除以100万。由于我们增量的变量在线程之间不共享，我们期望两个线程的运行速度是一个线程的两倍，所以“非共享”结果正是我们预期的。另一个基准测试，我们使用原子变量的数组，但每个线程使用自己的数组元素，也没有共享数据。然而，它的表现就好像数据在线程之间是共享的，至少对于少量线程来说，所以我们称之为“伪共享”：实际上没有真正共享，但程序的行为却好像共享了一样。
- en: 'This result shows that the reason for the high cost of data sharing is more
    complex than what we assumed previously: in the case of false sharing, only one
    thread is operating on each array element, so it does not have to wait for any
    other thread to complete its increment. And yet, threads clearly wait for each
    other. To understand this anomaly, we have to learn more about the way caches
    work.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这一结果表明，数据共享成本高的原因比我们之前假设的更加复杂：在伪共享的情况下，每个数组元素只有一个线程在操作，因此它不必等待任何其他线程完成递增。然而，线程显然彼此等待。要理解这种异常，我们必须更多地了解缓存的工作方式。
- en: The way the data moves between the processors and the memory in a multi-core
    or a multi-processor system is illustrated in *Figure 5.7*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 多核或多处理器系统中数据在处理器和内存之间的传输方式如*图5.7*所示。
- en: '![Figure 5.7 – Data transfer between CPUs and memory in a multi-core system'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.7 - 多核系统中CPU和内存之间的数据传输'
- en: '](img/Figure_5.7_B16229.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.7_B16229.jpg)'
- en: Figure 5.7 – Data transfer between CPUs and memory in a multi-core system
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 - 多核系统中CPU和内存之间的数据传输
- en: 'The processor operates on the data in individual bytes, or in words that depend
    on the type of the variable; in our case, an `unsigned long` is an 8-byte word.
    An atomic increment reads a single word at the specified address, increments it,
    and writes it back. But reads from where? The CPU has direct access only to the
    L1 cache, so it gets the data from there. How does the data get from the main
    memory into the cache? It''s copied over the memory bus, which is much wider.
    The minimum amount of data that can be copied from memory to cache and back is
    called a **cache line**. On all x86 CPUs, one cache line is 64 bytes. When a CPU
    needs to lock a memory location for an atomic transaction, such as an atomic increment,
    it may be writing a single word, but it has to lock the entire cache line: if
    two CPUs are allowed to write the same cache line into memory at the same time,
    one of them will overwrite the other. Note that, for simplicity, we show only
    one level of cache hierarchy in *Figure 5.7*, but it makes no difference: data
    travels through all cache levels in chunks of cache line length.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器操作数据以单个字节或取决于变量类型的单词；在我们的情况下，`unsigned long`是一个8字节的单词。原子递增读取指定地址的单个单词，递增它，然后将其写回。但是从哪里读取？CPU只能直接访问L1缓存，因此它从那里获取数据。数据如何从主内存传输到缓存？它通过更宽的内存总线复制。可以从内存复制到缓存和反复制的最小数据量称为**缓存行**。在所有x86
    CPU上，一个缓存行是64字节。当CPU需要锁定内存位置进行原子事务（如原子递增）时，它可能只写入一个单词，但必须锁定整个缓存行：如果允许两个CPU同时将同一个缓存行写入内存，其中一个将覆盖另一个。请注意，为简单起见，我们在*图5.7*中只显示了一级缓存层次结构，但这并没有影响：数据以缓存行长度的块通过所有缓存级别传输。
- en: 'Now we can explain the false sharing we observed: even though the adjacent
    array elements are not really shared between threads, they do occupy the same
    cache line. When a CPU requests exclusive access to one array element for the
    duration of the atomic increment operation, it locks the entire cache line and
    prevents any other CPU from accessing any data in it. Incidentally, this explains
    why the false sharing in *Figure 5.7* appears equivalent to the true data sharing
    for up to 8 threads but becomes faster for more threads: we are writing 8-byte
    words, so 8 of them fit into the same cache line. If we have only 8 threads (or
    fewer), only one thread can increment its value at any given time, the same as
    for true sharing. But with more than 8 threads, the array occupies at least two
    cache lines, and they can be locked by two CPUs independently from each other.
    So, if we have, say, 16 threads at any time, there are two threads that can move
    forward, one for each half of the array.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以解释我们观察到的伪共享：即使相邻的数组元素实际上并不在线程之间共享，它们确实占据了同一个缓存行。当CPU请求对一个数组元素进行原子递增操作的独占访问时，它会锁定整个缓存行，并阻止任何其他CPU访问其中的任何数据。顺便说一句，这解释了为什么*图5.7*中的伪共享对于多达8个线程而言等效于真实数据共享，但对于更多的线程而言会更快：我们在写入8字节的单词，所以8个单词可以放入同一个缓存行中。如果我们只有8个线程（或更少），那么在任何给定时间只有一个线程可以递增其值，就像真正的共享一样。但是对于超过8个线程，数组至少占据两个缓存行，并且它们可以被两个CPU独立地锁定。因此，如果我们有，比如说，16个线程，那么在任何时候都有两个线程可以前进，一个用于数组的每一半。
- en: On the other hand, the real no-sharing benchmark allocates the atomic variables
    on the stack of each thread. These are completely independent memory allocations,
    separated by many cache lines. With no interaction through memory, these threads
    run completely independently of each other.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，真正的无共享基准在每个线程的堆栈上分配原子变量。这些是完全独立的内存分配，相隔多个缓存行。通过内存没有任何交互，这些线程完全独立地运行。
- en: 'Our analysis shows that the real reason for the high cost of accessing the
    shared data is the work that must be done to maintain the exclusive access to
    a cache line and to make sure all CPUs have consistent data in their caches: after
    one CPU has obtained exclusive access and updated even one bit in the cache line,
    the copy of that line in all caches of all other CPUs is out of date. Before these
    other CPUs can access any data in the same cache line, they must fetch the updated
    content from the main memory, which, as we have seen, takes a relatively long
    time.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析表明，访问共享数据的高成本的真正原因是必须进行的工作，以保持对缓存行的独占访问，并确保所有CPU的缓存中都有一致的数据：在一个CPU获得独占访问并更新缓存行中的任何一个位之后，所有其他CPU的所有缓存中该行的副本都已过时。在这些其他CPU可以访问同一个缓存行中的任何数据之前，它们必须从主内存中获取更新的内容，正如我们所看到的，这需要相对较长的时间。
- en: As we have seen, it doesn't really matter whether two threads try to access
    the same memory location or not, as long as they are competing for access to the
    same cache line. That exclusive cache line access is the origin of the high cost
    of shared variables.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，两个线程是否尝试访问相同的内存位置并不重要，只要它们竞争访问同一个缓存行。这种独占缓存行访问是共享变量高成本的根源。
- en: 'One may wonder whether the reason locks are expensive is also found in the
    shared data they contain (all locks must have some amount of shared data, that''s
    the only way one thread can let another thread know that the lock is taken). A
    mutex lock is much more expensive than single atomic access, even on one thread,
    as we have seen in *Figures 5.4* and *5.5*. We can assume, correctly, that locking
    a mutex involves more work than just modifying one atomic variable. But why does
    this work take more time when we have more than one thread? Is it because the
    data is shared and needs exclusive access to the cache line? We leave it as an
    exercise to the reader to confirm that this is indeed so. The key to this experiment
    is to set up false sharing of locks: an array of locks such that each thread operates
    on its own lock, but they compete for the same cache line (of course, such per-thread
    locks don''t actually protect anything from concurrent access, but all we want
    is the time it takes to lock and unlock them). The experiment is slightly more
    complex than you might think: the standard C++ mutex, `std::mutex`, is usually
    quite large, between 40 and 80 bytes depending on the OS. This means you can''t
    fit even two of them into the same cache line. You have to do this experiment
    with a smaller lock, such as a **spinlock** or a **futex**.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能会想知道锁昂贵的原因是否也存在于它们包含的共享数据中（所有锁必须包含一定量的共享数据，这是一个线程可以让另一个线程知道锁已被占用的唯一方法）。正如我们在*图5.4*和*5.5*中所看到的，互斥锁比单个原子访问要昂贵得多，即使在一个线程上也是如此。我们可以正确地假设，锁定互斥锁涉及的工作比只修改一个原子变量要多。但为什么当我们有多个线程时，这项工作需要更多时间呢？这是因为数据是共享的，并且需要对缓存行进行独占访问吗？我们留给读者作为练习来确认这确实是这样。这个实验的关键是设置锁的伪共享：一个锁数组，使得每个线程操作自己的锁，但它们竞争同一个缓存行（当然，这样的每线程锁实际上并不能保护任何东西免受并发访问，但我们只想知道锁定和解锁所需的时间）。这个实验比你想象的要稍微复杂一些：标准的C++互斥锁`std::mutex`通常相当大，根据操作系统的不同在40到80字节之间。这意味着你甚至不能将两个互斥锁放入同一个缓存行中。你必须使用一个更小的锁来进行这个实验，比如**自旋锁**或**futex**。
- en: 'We now understand why the cost of accessing the shared data concurrently is
    so high. This understanding gives us two important lessons. The first one is to
    avoid false data sharing when we attempt to create non-shared data. How can the
    unintended *false sharing* creep into our program? Consider the simple example
    we have studied throughout this chapter: accumulating a sum concurrently. Some
    of our approaches were slower than others, but they were all very slow (slower
    than the single-threaded program, or, at best, no faster). We understand that
    accessing shared data is expensive. So, what is less expensive? Not accessing
    the shared data, of course! Or at least not accessing it as often. There is no
    reason for us to access the shared sum value every time we want to add something
    to it: we can make all the additions locally, on the thread, and add them to the
    shared accumulator value once, at the very end. The code would look something
    like this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在明白了为什么同时访问共享数据的成本如此之高。这种理解给了我们两个重要的教训。第一个教训是在尝试创建非共享数据时要避免伪共享。伪共享如何潜入我们的程序？考虑我们在本章中一直研究的简单示例：并发累积总和。我们的一些方法比其他方法慢，但它们都非常慢（比单线程程序慢，或者最多也不会更快）。我们明白访问共享数据是昂贵的。那么什么更便宜呢？当然是不访问共享数据！或者至少不那么频繁地访问。我们没有理由每次想要向总和添加东西时都访问共享总和值：我们可以在线程上本地进行所有的加法，然后在最后一次将它们添加到共享累加器值上。代码看起来会像这样：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We have the global result, `sum`, that is shared between all threads and must
    be atomic (or protected by a lock). But each thread accesses this variable exactly
    once after all the work is done. Each thread uses another variable to hold the
    partial sum, only the values added on this thread (increments of 1 in our trivial
    case, but the performance is the same regardless of the values being added). We
    can create a large array to store these per-thread partial sums and give each
    thread a unique array element to work on. Of course, in this trivial example,
    we could just use a local variable, but in a real program, the partial results
    often need to be kept after the worker threads are done, and the final processing
    of these results is done elsewhere, perhaps by another thread. To simulate this
    kind of implementation, we use an array of per-thread variables. Note that these
    variables are just plain integers, not atomic: there is no concurrent access to
    them. Unfortunately, in the process, we fell into the trap of false sharing: the
    adjacent elements of the array are (usually) on the same cache line and, thus,
    cannot be accessed concurrently. This is reflected in the performance of our program:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有全局结果`sum`，它在所有线程之间共享，并且必须是原子的（或者由锁保护）。但是每个线程在完成所有工作后只访问这个变量一次。每个线程使用另一个变量来保存部分总和，只有在这个线程上添加的值（在我们的简单情况中是增量1，但无论添加的值如何，性能都是一样的）。我们可以创建一个大数组来存储这些每线程部分总和，并给每个线程一个唯一的数组元素来处理。当然，在这个简单的例子中，我们可以只使用一个本地变量，但在一个真实的程序中，部分结果通常需要在工作线程完成后保留，并且这些结果的最终处理是在其他地方完成的，也许是由另一个线程完成。为了模拟这种实现，我们使用一个每线程变量的数组。请注意，这些变量只是普通的整数，不是原子的：它们没有并发访问。不幸的是，在这个过程中，我们陷入了伪共享的陷阱：数组的相邻元素（通常）在同一个缓存行上，因此不能同时访问。这反映在我们程序的性能上：
- en: '![Figure 5.8 – Sum accumulation with and without false sharing'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.8 - 带有和不带有伪共享的总和累积'
- en: '](img/Figure_5.8_B16229.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.8_B16229.jpg)'
- en: Figure 5.8 – Sum accumulation with and without false sharing
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 - 带有和不带有伪共享的总和累积
- en: As you can see in *Figure 5.8*, our program scales very poorly until we get
    to a very large number of threads. On the other hand, it scales perfectly, as
    expected, if we eliminate the false sharing by making sure the per-thread partial
    sums are at least 64 bytes apart (or simply using local variables in our case).
    While both programs become faster when we use more threads, the implementation
    that is not burdened by the false sharing remains approximately twice as fast.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*图5.8*中所看到的，我们的程序在线程数量很大时扩展性非常差。另一方面，如果我们通过确保每个线程的部分和至少相隔64字节（或者在我们的情况下简单地使用本地变量）来消除虚假共享，那么它的扩展性就完美，正如预期的那样。当我们使用更多线程时，两个程序都变得更快，但没有虚假共享负担的实现大约快两倍。
- en: 'The second lesson will become more important in the later chapters: since accessing
    shared variables concurrently is, comparatively, very expensive, an algorithm
    or an implementation that uses fewer shared variables will, in general, perform
    faster.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个教训在后面的章节中将变得更加重要：由于并发访问共享变量相对来说非常昂贵，因此使用更少共享变量的算法或实现通常会执行得更快。
- en: 'This statement may be confusing at the moment: by nature of the problem, we
    have some amount of the data that must be shared. We can do optimizations like
    the one we just did and eliminate unnecessary accesses to this data. But once
    this is done, the rest is the data we need to access to produce the desired results.
    How can there be more, or fewer, shared variables, then? To understand this, we
    have to realize that there is more to writing concurrent programs than protecting
    access to all shared data.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个陈述可能在这一刻令人困惑：由于问题的性质，我们有一些必须共享的数据。我们可以进行像刚刚做的优化，并消除对这些数据的不必要访问。但一旦这样做了，剩下的就是我们需要访问以产生期望结果的数据。那么，共享变量可能会更多，或更少，吗？要理解这一点，我们必须意识到编写并发程序不仅仅是保护对所有共享数据的访问。
- en: Learning about concurrency and order
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习并发和顺序
- en: 'As the reader was reminded earlier in this chapter, any program that accesses
    any shared data without access synchronization (mutexes or atomic accesses, usually)
    has undefined behavior that is usually called a data race. This seems simple enough,
    at least in theory. But our motivational example was too simple: it had just one
    variable shared between threads. There is more to concurrency than locking shared
    variables, as we are about to see.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如读者在本章前面提醒过的，任何访问任何共享数据的程序，如果没有访问同步（通常是互斥锁或原子访问），都会产生未定义行为，通常称为数据竞争。这在理论上似乎很简单。但我们的激励性例子太简单了：它只有一个在线程之间共享的变量。并发性不仅仅是锁定共享变量，我们将很快看到。
- en: The need for order
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序的需求
- en: 'Now consider this example known as the **producer-consumer queue**. Let us
    say that we have two threads. The first thread, the producer, prepares some data
    by constructing objects. The second thread, the consumer, processes the data (does
    work on each object). For simplicity, let us say that we have a large memory buffer
    that is initially uninitialized and the producer thread constructs new objects
    in the buffer as if they were array elements:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑一下这个例子，被称为**生产者-消费者队列**。假设我们有两个线程。第一个线程，生产者，通过构造对象来准备一些数据。第二个线程，消费者，处理数据（对每个对象进行操作）。为了简单起见，假设我们有一个大的内存缓冲区，最初未初始化，生产者线程在缓冲区中构造新对象，就好像它们是数组元素一样：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In order to produce (construct) an object, the producer thread calls the constructor
    via the placement of the `new` operator on each element of the array, starting
    with `N==0`:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生产（构造）一个对象，生产者线程通过在数组的每个元素上放置`new`运算符来调用构造函数，从`N==0`开始：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now the array element `buffer[N]` is initialized and is available to the consumer
    thread. The producer signals this by advancing the counter `N` then moves on to
    initialize the next object:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数组元素`buffer[N]`已初始化，并且可以被消费者线程访问。生产者通过增加计数器`N`来发出信号，然后继续初始化下一个对象：
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The consumer thread must not access an array element `buffer[i]` until the
    counter `N` has been incremented so it is greater than `i`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者线程在计数器`N`增加到大于`i`之前，不能访问数组元素`buffer[i]`：
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For simplicity, let us ignore the problem of running out of memory and assume
    that the buffer is large enough. Also, we are not concerned with the termination
    condition (how does the consumer know when to keep consuming?) right now. At the
    moment, we are interested in the producer-consumer handshake protocol: how does
    the consumer access the data without any races?'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，让我们忽略内存耗尽的问题，并假设缓冲区足够大。此外，我们现在不关心终止条件（消费者如何知道何时继续消费？）。此刻，我们对生产者-消费者握手协议感兴趣：消费者如何在没有任何竞争的情况下访问数据？
- en: 'The general rule states that any access to the shared data must be protected.
    Obviously, the counter `N` is a shared variable, so accessing it needs more care:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一般规则规定，对共享数据的任何访问都必须受到保护。显然，计数器`N`是一个共享变量，因此访问它需要更多的注意：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'But is this enough? Look carefully: there is more shared data in our program.
    The entire array of objects `T` is shared between the two threads: each thread
    needs to access every element. But if we need to lock the entire array, we might
    as well go back to a single-threaded implementation: one of the two threads is
    always going to be locked out. From experience, every programmer who has ever
    written any multi-threaded code knows that, in this case, we do not need to lock
    the array, only the counter. In fact, it is the whole point of locking the counter
    that we don''t need to lock the array this way: any particular element of the
    array is never accessed concurrently. First, it is accessed only by the producer
    before the counter is incremented. Then, it is accessed only by the consumer after
    the counter is incremented. This is known. But the goal of this book is to teach
    you how to understand why things work the way they do, and so, why is locking
    the counter enough? What guarantees that the events really happen in the order
    we imagine?'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 但这足够吗？仔细看：我们的程序中有更多的共享数据。对象数组`T`在两个线程之间是共享的：每个线程都需要访问每个元素。但是，如果我们需要锁定整个数组，我们可能会回到单线程实现：两个线程中的一个始终会被锁定。根据经验，每个编写过任何多线程代码的程序员都知道，在这种情况下，我们不需要锁定数组，只需要锁定计数器。事实上，锁定计数器的整个目的是我们不需要以这种方式锁定数组：数组的任何特定元素都不会被同时访问。首先，它只能在生产者在计数器递增之前访问。然后，它只能在计数器递增后由消费者访问。这是已知的。但是，本书的目标是教会你如何理解事情为什么会发生，因此，为什么锁定计数器足够？是什么保证事件确实按我们想象的顺序发生？
- en: 'By the way, even this trivial example has just become not so trivial. The naïve
    way to protect the consumer''s access to the counter is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，即使这个平凡的例子也变得不那么平凡了。保护消费者对计数器`N`的访问的天真方法如下：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This is a guaranteed deadlock: once the consumer acquires the lock, it waits
    for the element `i` to be initialized before releasing the lock. The producer
    cannot make any progress because it is waiting to acquire the lock before it can
    increment the counter `N`. Both threads are now waiting forever. It is easy to
    notice that our code would be so much simpler if we just used an atomic variable
    for the counter:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个保证的死锁：一旦消费者获取锁，它就会等待元素`i`被初始化，然后才会释放锁。生产者无法取得任何进展，因为它正在等待获取锁，然后才能递增计数器`N`。两个线程现在都永远在等待。很容易注意到，如果我们只是使用原子变量来计数，我们的代码将简单得多：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now every read of the counter `N` by the consumer is atomic, but in between
    the two reads the producer is not blocked and can keep working. This approach
    to concurrency is known as `buffer[i]` concurrently?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，消费者对计数器`N`的每次读取都是原子的，但在两次读取之间，生产者没有被阻塞，可以继续工作。这种并发处理方法被称为`buffer[i]`？
- en: Memory order and memory barriers
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存顺序和内存屏障
- en: 'As we have realized, being able to access shared variables safely is not enough
    to write any non-trivial concurrent program. We also have to be able to reason
    about the order in which events happen. In our producer and consumer example,
    the entire program rests on a single assumption: that we can guarantee that the
    construction of the Nth array element, incrementing the counter to N + 1, and
    the access to the Nth element by the consumer thread happen in that order.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们意识到的那样，能够安全地访问共享变量并不足以编写任何非平凡的并发程序。我们还必须能够推断事件发生的顺序。在我们的生产者和消费者示例中，整个程序都建立在一个假设上：我们可以保证第N个数组元素的构造，将计数器递增到N
    + 1，以及消费者线程访问第N个元素的顺序。
- en: 'But the problem is really even more complex than that once we realize that
    we are dealing not just with multiple threads but with multiple processors that
    are executing these threads truly at the same time. The key concept we have to
    remember here is **visibility**. A thread is executing on one CPU and is making
    changes to the memory when the CPU assigns values to variables. In reality, the
    CPU is only changing the content of its cache; the cache and memory hardware eventually
    propagate these changes to the main memory or the shared higher-level cache, at
    which point these changes may become visible to other CPUs. We say "*may*" because
    the other CPUs have different values for the same variables in their caches, and
    we do not know when these differences are reconciled. We do know that, once a
    CPU begins an operation on an atomic variable, no other CPU can access the same
    variable until this operation is done, and that once this operation completes,
    all other CPUs will see the latest updated value of this variable (but only if
    all CPUs treat the variable as atomic). We know that the same applies to a variable
    guarded by a lock. But these guarantees are not sufficient for our producer-consumer
    program: based on what we know so far, we cannot be sure it is correct. This is
    because, until now, we were concerned with only one aspect of accessing a shared
    variable: the atomic or transactional nature of this access. We wanted to make
    sure that the entire operation, whether simple or complex, is executed as a single
    transaction without the possibility of being interrupted.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，一旦我们意识到我们不仅仅是处理多个线程，而且是处理真正同时执行这些线程的多个处理器时，问题实际上更加复杂。我们必须记住的关键概念是**可见性**。一个线程在一个CPU上执行，并且在CPU分配值给变量时对内存进行更改。实际上，CPU只是更改其缓存的内容；缓存和内存硬件最终将这些更改传播到主内存或共享的高级缓存，此时这些更改可能对其他CPU可见。我们说“*可能*”，因为其他CPU的缓存中对相同变量有不同的值，我们不知道这些差异何时会被协调。我们知道，一旦CPU开始对原子变量执行操作，其他CPU就无法访问相同的变量，直到此操作完成，并且一旦此操作完成，所有其他CPU将看到此变量的最新更新值（但仅当所有CPU将变量视为原子时）。我们知道，同样适用于由锁保护的变量。但是，这些保证对于我们的生产者-消费者程序来说是不够的：根据我们目前所知，我们无法确定它是否正确。这是因为，到目前为止，我们只关注了访问共享变量的一个方面：这种访问的原子性或事务性。我们希望确保整个操作，无论是简单还是复杂，都作为单个事务执行，而不会被中断。
- en: But there is another aspect of accessing shared data, that of **memory order**.
    Just like the atomicity of the access itself, it is a feature of the hardware
    that is activated using a particular machine instruction (often an attribute or
    a flag on the atomic instruction itself).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 但访问共享数据还有另一个方面，即**内存顺序**。就像访问本身的原子性一样，它是硬件的一个特性，使用特定的机器指令（通常是原子指令本身的属性或标志）来激活。
- en: 'There are several forms of memory order. The least restricted one is the relaxed
    memory order. When an atomic operation is executed with relaxed order, the only
    guarantee we have is that the operation itself is executed atomically. What does
    this mean? Let us first consider the CPU that is executing the atomic operation.
    It runs a thread that contains other operations, both non-atomic and atomic. Some
    of these operations modify the memory; the results of these operations can be
    seen by other CPUs. Other operations read the memory; they observe the results
    of operations executed by other CPUs. The CPU that runs our thread executes these
    operations in a certain order. It may not be the order in which they are written
    in the program: both the compiler and the hardware can reorder instructions, usually
    to improve the performance. But it is a well-defined order. Now let us look at
    it from the point of view of another CPU that is executing a different thread.
    That second CPU can see the content of the memory changing as the first CPU does
    its work. But it does not necessarily see them in the same order with respect
    to each other or to the atomic operation that we have been focused on:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 内存顺序有几种形式。最不受限制的是松散内存顺序。当使用松散顺序执行原子操作时，我们唯一的保证是操作本身是原子执行的。这是什么意思？让我们首先考虑执行原子操作的CPU。它运行包含其他操作的线程，既有非原子操作，也有原子操作。其中一些操作修改内存；这些操作的结果可以被其他CPU看到。其他操作读取内存；它们观察其他CPU执行的操作的结果。运行我们线程的CPU按照一定顺序执行这些操作。它可能不是程序中编写的顺序：编译器和硬件都可以重新排序指令，通常是为了提高性能。但这是一个明确定义的顺序。现在让我们从执行不同线程的另一个CPU的角度来看。第二个CPU可以看到内存内容随着第一个CPU的工作而改变。但它不一定以与原子操作相同的顺序看到它们，也不一定以与彼此相同的顺序看到它们：
- en: '![Figure 5.9 – Visibility of operations with relaxed memory order'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.9 - 使用松散内存顺序的操作的可见性'
- en: '](img/Figure_5.9_B16229.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.9_B16229.jpg)'
- en: Figure 5.9 – Visibility of operations with relaxed memory order
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 - 使用松散内存顺序的操作的可见性
- en: 'This is the visibility that we were talking about earlier: one CPU executes
    operations in a certain order, but their results are visible to other CPUs in
    a very different order. For brevity, we usually talk about the visibility of the
    operations and do not mention *results* every time.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们之前谈论的可见性：一个CPU按照一定顺序执行操作，但它们的结果以非常不同的顺序对其他CPU可见。为了简洁起见，我们通常谈论操作的可见性，并不是每次都提到*结果*。
- en: 'If our operations on the shared counter `N` were executed with relaxed memory
    order, we would be in deep trouble: the only way to make our program correct would
    be to lock it so only one thread, the producer or the consumer, can run at any
    time, and we get no performance improvement from concurrency.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对共享计数器`N`的操作使用松散内存顺序执行，我们将陷入深深的麻烦：使程序正确的唯一方法是锁定它，以便只有一个线程，生产者或消费者，可以同时运行，并且我们无法从并发中获得性能改进。
- en: 'Fortunately, there are other memory order guarantees we can use. The most important
    one is the acquire-release memory order. When an atomic operation is executed
    with this order, we have a guarantee that any operation that accesses the memory
    and was executed before the atomic operation becomes visible to another thread
    before that thread executes an atomic operation on the same atomic variable. Similarly,
    all operations that are executed after the atomic operation become visible only
    after an atomic operation on the same variable. Again, remember that when we talk
    about the visibility of operations, we really mean that their results become observable
    to other CPUs. This is evident in *Figure 5.10*: on the left, we have the operations
    as they are executed by **CPU0**. On the right, we have the same operations as
    they are seen by **CPU1**. Note, in particular, that the atomic operation shown
    on the right is *Atomic Write*. But **CPU1** does not execute atomic write: it
    executes an atomic read to see the results of the atomic write executed by **CPU0**.
    The same goes for all other operations: on the left, the order is as executed
    by **CPU0**. On the right, the order is as seen by **CPU1**.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以使用其他内存顺序保证。最重要的是获取-释放内存顺序。当使用此顺序执行原子操作时，我们保证任何访问内存的操作在执行原子操作之前，并在另一个线程执行相同原子变量的原子操作之前变得可见。同样，所有在原子操作之后执行的操作只有在相同变量上的原子操作之后才变得可见。再次强调，当我们谈论操作的可见性时，我们真正意味着它们的结果对其他CPU变得可观察。这在*图5.10*中是显而易见的：在左边，我们有**CPU0**执行的操作。在右边，我们有**CPU1**看到的相同操作。特别要注意的是，右边显示的原子操作是*原子写*。但**CPU1**并没有执行原子写：它执行原子读以查看**CPU0**执行的原子写的结果。其他所有操作也是如此：在左边，顺序是由**CPU0**执行的。在右边，顺序是由**CPU1**看到的。
- en: '![Figure 5.10 – Visibility of operations with acquire-release memory order'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.10 - 使用获取-释放内存顺序的操作的可见性'
- en: '](img/Figure_5.10_B16229.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.10_B16229.jpg)'
- en: Figure 5.10 – Visibility of operations with acquire-release memory order
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 - 使用获取-释放内存顺序的操作的可见性
- en: The acquire-release order guarantee is a terse statement packed with a lot of
    important information, so let us elaborate on a few distinct points. First of
    all, the order is defined relative to the operations both threads execute on the
    same atomic variable. Until two threads access the same variable atomically, their
    *clocks* remain entirely arbitrary with respect to each other, and we cannot reason
    about what happens before or after something else, there is no meaning to these
    words. It is only when one thread has observed the results of an atomic operation
    executed by another thread that we can talk about *before* and *after*. In our
    producer-consumer example, the producer atomically increments the counter `N`.
    The consumer atomically reads the same counter. If the counter has not changed,
    we don't know anything about the state of the producer. But if the consumer sees
    that the counter has changed from N to N+1 and both threads use the acquire-release
    memory order, we know that all operations executed by the producer prior to incrementing
    the counter are now visible to the consumer. These operations include all the
    work necessary to construct the object that now resides in the array element `buffer[N]`,
    and, thus, the consumer can safely access it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 获取-释放顺序保证是一个简洁的陈述，包含了许多重要信息，让我们详细阐述一些不同的观点。首先，该顺序是相对于两个线程在同一个原子变量上执行的操作而定义的。直到两个线程以原子方式访问相同的变量，它们的“时钟”相对于彼此来说是完全任意的，我们无法推断出某件事情发生在另一件事情之前或之后，这些词语是没有意义的。只有当一个线程观察到另一个线程执行的原子操作的结果时，我们才能谈论“之前”和“之后”。在我们的生产者-消费者示例中，生产者原子地增加计数器`N`。消费者原子地读取相同的计数器。如果计数器没有改变，我们对生产者的状态一无所知。但是，如果消费者看到计数器已经从N变为N+1，并且两个线程都使用获取-释放内存顺序，我们知道生产者在增加计数器之前执行的所有操作现在对消费者可见。这些操作包括构造现在驻留在数组元素`buffer[N]`中的对象所需的所有工作，因此，消费者可以安全地访问它。
- en: The second salient point is that both threads must use the acquire-release memory
    order when accessing the atomic variable. If the producer uses this order to increment
    the count, but the consumer reads it with relaxed memory order, there are no guarantees
    on the visibility of any operations.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个显著的观点是，当访问原子变量时，两个线程都必须使用获取-释放内存顺序。如果生产者使用此顺序来增加计数，但消费者以松散的内存顺序读取它，那么对任何操作的可见性就没有任何保证。
- en: 'The final point is that all order guarantees are given in terms of *before*
    and *after* the operation on the atomic variable. Again, in our producer-consumer
    example, we know that the results of the operations executed by the producer to
    construct the Nth object are all visible by the consumer when it sees the counter
    change. There are no guarantees on the order in which these operations become
    visible. You can see this in *Figure 5.10*. Of course, it should not matter to
    us: we can''t touch any part of the object until it''s constructed, and, once
    the construction is finished, we don''t care about the order in which it was done.
    The atomic operations with memory order guarantees act as barriers across which
    other operations cannot move. You can imagine such a barrier in *Figure 5.10*,
    dividing the entire program into two distinct parts: everything that happened
    before the count was incremented and everything that happened after. For that
    reason, it is often convenient to talk about such atomic operations as memory
    barriers.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点是，所有顺序保证都是以原子变量上的操作“之前”和“之后”来给出的。同样，在我们的生产者-消费者示例中，当消费者看到计数器改变时，我们知道生产者执行的操作结果构造第N个对象对消费者是可见的。对这些操作变得可见的顺序没有任何保证。你可以在*图5.10*中看到这一点。当然，这对我们来说不重要：在对象构造之前我们不能触摸任何部分，一旦构造完成，我们也不关心它是以什么顺序完成的。具有内存顺序保证的原子操作充当着其他操作无法移动的屏障。你可以想象在*图5.10*中有这样一个屏障，将整个程序分成两个不同的部分：在计数增加之前发生的一切和之后发生的一切。因此，通常方便将这样的原子操作称为内存屏障。
- en: 'Let us assume, for a moment, that in our program, all atomic operations on
    the counter `N` have acquire-release barriers. That would certainly guarantee
    that the program is correct. Note, however, that the acquire-release order is
    overkill for our needs. For the producer, it gives us the guarantee that all objects
    `buffer[0]` through `buffer[N]` that were constructed before we incremented the
    count to N+1 will be visible to the consumer when it sees the counter change from
    N to N+1\. We need that guarantee. But we also have the guarantee that none of
    the operations executed for the purpose of constructing the remaining objects,
    `buffer[N+1]` and beyond, have become visible yet. We don''t care about that:
    the consumer is not going to access these objects until it sees the next value
    of the counter. Similarly, on the consumer side, we have the guarantee that all
    the operations executed after the consumer sees the counter change to N+1 will
    have their effects (memory accesses) happen after that atomic operation. We need
    that guarantee: we do not want the CPU to reorder our consumer operations and
    execute some of the instructions that access the object `buffer[N]` before it
    is ready. But we also have the guarantee that the work done by the consumer to
    process the previous objects like `buffer[N-1]` is done and made visible to all
    threads before the consumer moves to the next object. Again, we don''t need that
    guarantee: nothing depends on it.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设一下，在我们的程序中，对计数器`N`的所有原子操作都有获取-释放屏障。这肯定会保证程序的正确性。然而，请注意，获取-释放顺序对我们的需求来说有些过度。对于生产者来说，它给了我们一个保证，即在我们将计数增加到N+1之前构造的所有对象`buffer[0]`到`buffer[N]`在消费者看到计数从N变为N+1时将对其可见。我们需要这个保证。但我们也有保证，为了构造剩余的对象`buffer[N+1]`及更多对象而执行的操作尚未变得可见。我们不关心这一点：消费者在看到下一个计数值之前不会访问这些对象。同样，在消费者方面，我们有保证，消费者看到计数变为N+1后执行的所有操作的效果（内存访问）将发生在该原子操作之后。我们需要这个保证：我们不希望CPU重新排序我们的消费者操作，并在准备好之前执行一些访问对象`buffer[N]`的指令。但我们也有保证，消费者处理之前的对象如`buffer[N-1]`的工作已经完成并对所有线程可见，然后消费者才会移动到下一个对象。同样，我们不需要这个保证：没有什么依赖它。
- en: What is the harm in having stronger guarantees than what is strictly necessary?
    In terms of correctness, none. But this is a book about writing fast programs
    (also, correct ones). Why are the ordering guarantees necessary in the first place?
    Because when left to their own devices, the compilers and the processors can reorder
    our program instructions almost arbitrarily. Why would they do that? Usually,
    to improve performance. Thus, it stands to reason that the more restrictions we
    impose on the ability to reorder the execution, the stronger the adverse impact
    on performance is. Therefore, in general, we want to use the memory order that
    is restrictive enough for the correctness of our program but no more strict than
    that.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有比严格必要的更强保证有什么害处？在正确性方面，没有。但这是一本关于编写快速程序的书（也是正确的）。为什么首先需要顺序保证？因为在自己的设备上，编译器和处理器几乎可以任意重新排序我们的程序指令。为什么他们会这样做？通常是为了提高性能。因此，可以推断出，我们对执行重新排序的能力施加的限制越多，对性能的不利影响就越大。因此，一般来说，我们希望使用足够严格以确保程序正确性的内存顺序，但不要更严格。
- en: 'The memory order that gives us exactly what we need for our producer-consumer
    program is as follows. On the producer side, we need one-half of the guarantee
    given by the acquire-release memory barrier: all operations executed before the
    atomic operation with the barrier must become visible to other threads before
    they execute the corresponding atomic operation. This is known as the release
    memory order:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的生产者-消费者程序，给我们提供了确切所需的内存顺序如下。在生产者方面，我们需要获取-释放内存屏障提供的保证的一半：在具有屏障的原子操作之前执行的所有操作必须在执行相应的原子操作之前对其他线程可见。这被称为释放内存顺序：
- en: '![Figure 5.11 – Release memory order'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.11 – 释放内存顺序'
- en: '](img/Figure_5.11_B16229.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.11_B16229.jpg)'
- en: Figure 5.11 – Release memory order
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 – 释放内存顺序
- en: 'When **CPU1** sees the result of the atomic write operation executed by **CPU0**
    with the release memory order, it is guaranteed that the state of the memory,
    as seen by **CPU1**, already reflects all operations executed by **CPU0** before
    this atomic operation. Note that we said nothing about the operations executed
    by **CPU0** after the atomic operation. As we see in *Figure 5.11*, these operations
    may become visible in any order. The memory barrier created by the atomic operation
    is effective only in one direction: any operation that is executed before the
    barrier cannot cross it and be seen after the barrier. But the barrier is permeable
    in the other direction. For this reason, the release memory barrier and the corresponding
    acquire memory barrier are sometimes called **half-barriers**.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当**CPU1**看到由**CPU0**执行的具有释放内存顺序的原子写操作的结果时，可以保证，根据**CPU1**看到的内存状态，已经反映了在这个原子操作之前由**CPU0**执行的所有操作。请注意，我们没有提到原子操作之后由**CPU0**执行的操作。正如我们在*图5.11*中看到的，这些操作可能以任何顺序变得可见。原子操作创建的内存屏障只在一个方向上有效：在屏障之前执行的任何操作都不能越过它，并在屏障之后被看到。但是屏障在另一个方向上是可渗透的。因此，释放内存屏障和相应的获取内存屏障有时被称为**半屏障**。
- en: 'The acquire memory order is what we need to use on the consumer side. It guarantees
    that all operations executed after the barrier become visible to other threads
    after the barrier, as shown in *Figure 5.12*:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 获取内存顺序是我们在消费者方面需要使用的。它保证了屏障后执行的所有操作在屏障后对其他线程可见，如*图5.12*所示：
- en: '![Figure 5.12 – Acquire memory order'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.12 – 获取内存顺序'
- en: '](img/Figure_5.12_B16229.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.12_B16229.jpg)'
- en: Figure 5.12 – Acquire memory order
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 – 获取内存顺序
- en: 'The acquire and release memory barriers are always used as a pair: if one thread
    (in our case, the producer) uses the release memory order with an atomic operation,
    the other thread (the consumer) must use the acquire memory order on the same
    atomic variable. Why do we need both barriers? On the one hand, we have the guarantee
    that everything that is done by the producer to build the new object before it
    increments the count is already visible to the consumer as soon as this increment
    is seen. But this is not enough, so, on the other hand, we have the guarantee
    that the operations executed by the consumer to process this new object cannot
    be moved backward in time, to a moment before the barrier when they could have
    seen the object in an unfinished state.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 获取和释放内存屏障总是成对使用的：如果一个线程（在我们的情况下是生产者）使用原子操作的释放内存顺序，另一个线程（消费者）必须在同一个原子变量上使用获取内存顺序。为什么我们需要两个屏障？一方面，我们保证生产者在增加计数之前构建新对象的所有操作在消费者看到这个增量时已经可见。但这还不够，另一方面，我们保证消费者执行的操作来处理这个新对象不能被移动到时间上向后，到达屏障之前的时刻，此时它们可能已经看到了一个未完成的对象。
- en: Now that we understand that it is not enough to just operate atomically on the
    shared data, you may ask whether our producer-consumer program actually works.
    As it turns out, both the lock version and the lock-free version are correct,
    even though we did not say anything explicitly about the memory order. So, how
    is the memory order controlled in C++?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们明白了仅仅在共享数据上进行原子操作是不够的，您可能会问我们的生产者-消费者程序是否实际上有效。事实证明，无论是锁版本还是无锁版本都是正确的，即使我们没有明确说明内存顺序。那么，在C++中如何控制内存顺序呢？
- en: Memory order in C++
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C++中的内存顺序
- en: 'First of all, let us think back to the lock-free version of our producer-consumer
    program, the one with the atomic counter:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回想一下我们的生产者-消费者程序的无锁版本，即具有原子计数器的版本：
- en: '[PRE16]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The counter `N` is an atomic variable, an object of a type generated by the
    template `std::atomic` with the type parameter `size_t`. All atomic types support
    atomic read and write operations, that is, they can appear in assignment operations.
    In addition, the integer atomics have the regular integer operations defined and
    implemented atomically, so `++N` is an atomic increment (not all operations are
    defined, for example, there is no operator `*=`). None of these operations explicitly
    specify the memory order, so what guarantees do we have? As it turns out, by default,
    we get the strongest possible guarantee, the bidirectional memory barrier with
    each atomic operation (the actual guarantee is even a little stricter, as you
    will see in the next section). This is why our program is correct.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 计数器`N`是一个原子变量，是由模板`std::atomic`生成的类型参数为`size_t`的对象。所有原子类型都支持原子读写操作，即它们可以出现在赋值操作中。此外，整数原子类型具有常规整数操作的定义和实现，因此`++N`是原子增量（并非所有操作都被定义，例如没有`*=`运算符）。这些操作都没有明确指定内存顺序，那么我们有什么保证呢？事实证明，默认情况下，我们获得了最强大的可能保证，即每个原子操作都具有双向内存屏障（实际保证甚至更严格，您将在下一节中看到）。这就是为什么我们的程序是正确的。
- en: 'If you think this is overkill, you can reduce the guarantees to be just the
    ones you need, but you have to be explicit about it. The atomic operations can
    also be executed by calling the member functions of the `std::atomic` type, and
    that is where you can specify the memory order. The consumer thread needs a load
    operation with the acquire barrier:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您认为这太过分了，您可以将保证减少到您需要的部分，但您必须明确说明。原子操作也可以通过调用`std::atomic`类型的成员函数来执行，并且在那里您可以指定内存顺序。消费者线程需要一个带有获取屏障的加载操作：
- en: '[PRE17]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The producer thread needs an increment operation with the release barrier (just
    like the increment operator, the member function also returns the value before
    the increment was done):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者线程需要一个带有释放屏障的增量操作（就像增量运算符一样，成员函数也返回增量之前的值）：
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Before we go any further, we must realize that we have jumped over one critically
    important step in our optimization. The right way to start the previous paragraph
    is, *If you think this is overkill, you have to prove it by performance measurements,
    and only then can you reduce the guarantees to be just the ones you need*. Concurrent
    programs are hard enough to write even when using locks; the use of lock-free
    code and especially explicit memory orders has to be justified.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们必须意识到我们在优化中跳过了一个非常重要的步骤。开始上一段的正确方式是，“如果您认为这太过分，您必须通过性能测量来证明，然后才能将保证减少到您需要的部分”。即使在使用锁时编写并发程序也很困难；使用无锁代码，尤其是显式内存顺序，必须得到证明。
- en: 'Speaking of locks, what memory order guarantees do they give? We know that
    any operation protected by the lock will be seen by any other thread that acquires
    the lock later, but what about the rest of the memory? The memory order enforced
    by the use of the lock is shown in *Figure 5.13*:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 说到锁，它们提供了什么内存顺序保证？我们知道由锁保护的任何操作将被稍后获取锁的任何其他线程看到，但其他内存呢？锁的使用强制执行的内存顺序如*图5.13*所示：
- en: '![Figure 5.13 – Memory order guarantees of a mutex'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.13 - 互斥锁的内存顺序保证'
- en: '](img/Figure_5.13_B16229.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.13_B16229.jpg)'
- en: Figure 5.13 – Memory order guarantees of a mutex
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 - 互斥锁的内存顺序保证
- en: 'The mutexes have (at least) two atomic operations inside. Locking the mutex
    is an equivalent of a read operation with the acquire memory order (which explains
    the name: this is the memory order we use when we *acquire* the lock). The operation
    creates a half-barrier any operation executed earlier can be seen after the barrier,
    but any operation executed after the lock is acquired cannot be observed earlier.
    When we unlock the mutex or *release* the lock, the release memory order is guaranteed.
    Any operation executed before this barrier will become visible before the barrier.
    You can see that the pair of barriers, acquire and release, act as borders for
    the section of the code sandwiched between them. This is known as the critical
    section: any operation executed inside the critical section, that is, executed
    while the thread was holding the lock, will become visible to any other thread
    when it enters the critical section. No operation can leave the critical section
    (become visible earlier or later), but other operations from the outside can enter
    the critical section. Crucially, no such operation can cross the critical section:
    if an outside operation enters the critical section, it cannot leave. So, anything
    that was done by **CPU0** before its critical section is guaranteed to be visible
    by **CPU1** after its critical section.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥锁内部至少有两个原子操作。锁定互斥锁相当于使用获取内存顺序的读操作（这解释了名称：这是我们在*获取*锁时使用的内存顺序）。该操作创建了一个半屏障，任何在此之前执行的操作都可以在屏障之后看到，但在获取锁之后执行的任何操作都不能被观察到。当我们解锁互斥锁或*释放*锁时，释放内存顺序是有保证的。在此屏障之前执行的任何操作将在屏障之前变得可见。您可以看到，获取和释放的一对屏障充当了它们之间代码部分的边界。这被称为临界区：在临界区内执行的任何操作，也就是在线程持有锁时执行的操作，将在其他线程进入临界区时变得可见。没有操作可以离开临界区（变得更早或更晚可见），但来自外部的其他操作可以进入临界区。至关重要的是，没有这样的操作可以穿过临界区：如果外部操作进入临界区，它就无法离开。因此，**CPU0**在其临界区之前执行的任何操作都保证在**CPU1**在其临界区之后变得可见。
- en: 'For our producer-consumer program, this translates into the following guarantee:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的生产者-消费者程序，这转化为以下保证：
- en: '[PRE19]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: All operations executed by the producer to construct the Nth object are done
    before the producer enters the critical section. They will be visible to the consumer
    before it leaves its critical section and begins consuming the Nth object. Therefore,
    the program is correct.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者执行的所有操作以构造第N个对象为例都在生产者进入临界区之前完成。它们将在消费者离开其临界区并开始消费第N个对象之前对消费者可见。因此，程序是正确的。
- en: The section you just read introduced the concept of memory order and illustrated
    it with examples. But, as you try to use this knowledge in your code, you will
    find the results wildly inconsistent. To better understand performance, what you
    should expect from the different ways you can use to synchronize your multi-threaded
    programs and avoid data races, we need to have a less hand-waving way to describe
    the memory order and related concepts.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您刚刚阅读的部分介绍了内存顺序的概念，并用示例进行了说明。但是，当您尝试在代码中使用这些知识时，您会发现结果极不一致。为了更好地理解性能，您应该从不同的方式同步多线程程序以及避免数据竞争中期望什么，我们需要以更少的含糊方式描述内存顺序和相关概念。
- en: Memory model
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存模型
- en: We need a more systematic and rigorous way to describe the interaction of threads
    through memory, their use of the shared data, and its effect on concurrent applications.
    This description is known as the memory model. The memory model describes what
    guarantees and restrictions exist when threads access the same memory location.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种更系统和严格的方式来描述线程通过内存的交互，它们对共享数据的使用以及对并发应用的影响。这种描述被称为内存模型。内存模型描述了线程访问相同内存位置时存在的保证和限制。
- en: 'Prior to the C++11 standard, the C++ language had no memory model at all (the
    word *thread* was not mentioned in the standard). Why is that a problem? Consider
    our producer-consumer example again (let us focus on the producer side):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在C++11标准之前，C++语言根本没有内存模型（标准中没有提到*线程*这个词）。为什么这是个问题？再次考虑我们的生产者-消费者示例（让我们专注于生产者方面）：
- en: '[PRE20]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `lock_guard` is just an RAII wrapper around the mutex, so we can''t forget
    to unlock it, so the code boils down to this:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`lock_guard`只是一个围绕互斥锁的RAII包装器，所以我们不会忘记解锁它，所以代码可以简化为这样：'
- en: '[PRE21]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Note that each line of this code uses either the variable `N` or the object
    `nM`, but they are never used together in one operation. From the C++ point of
    view, this code is similar to the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此代码的每一行都使用变量`N`或对象`nM`，但它们从不在一次操作中同时使用。从C++的角度来看，这段代码类似于以下代码：
- en: '[PRE22]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In this code, the order of the operations does not matter, and the compiler
    is free to reorder them as long as the observable behavior does not change (observable
    behavior is things like input and output, changing a value in memory is not an
    observable behavior). Going back to our original example, why doesn't the compiler
    reorder the operations there?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，操作的顺序并不重要，编译器可以自由地重新排序它们，只要可观察的行为不发生变化（可观察的行为是输入和输出，改变内存中的值不是可观察的行为）。回到我们最初的例子，为什么编译器不会重新排序那里的操作呢？
- en: '[PRE23]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This would be very bad, and yet, nothing in the C++ standard (until C++11) prevents
    the compiler from doing so.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是非常糟糕的，然而，在C++标准中（直到C++11之前）没有任何东西阻止编译器这样做。
- en: 'Of course, we were writing multi-threaded programs in C++ long before 2011,
    so how did they work? Obviously, the compilers did not do such *optimizations*,
    but why? The answer is found in the memory model: the compilers provided certain
    guarantees that went beyond the C++ standard and supplied a certain memory model
    even when the standard required none. The Windows-based compilers followed the
    Windows memory model, while most Unix- and Linux-based compilers provided the
    POSIX memory model and the corresponding guarantees.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，早在2011年之前，我们就已经在C++中编写了多线程程序，那么它们是如何工作的呢？显然，编译器并没有进行这样的*优化*，但是为什么呢？答案在于内存模型：编译器提供了一些超出C++标准的保证，并在标准不要求的情况下提供了某种内存模型。基于Windows的编译器遵循Windows内存模型，而大多数基于Unix和Linux的编译器提供了POSIX内存模型和相应的保证。
- en: 'The C++11 standard changed that and gave C++ its own memory model. We have
    already taken advantage of it in the previous section: the memory order guarantees
    that accompany the atomic operations, and the locks are part of this memory model.
    The C++ memory model now guarantees portability across platforms that previously
    offered a different set of guarantees, each according to its memory model. In
    addition, the C++ memory model offers some language-specific guarantees.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: C++11标准改变了这一点，并为C++提供了自己的内存模型。我们已经在前一节中利用了它：伴随原子操作的内存顺序保证，以及锁，都是这个内存模型的一部分。C++内存模型现在保证了跨平台的可移植性，以前的平台根据其内存模型提供了不同的一组保证。此外，C++内存模型提供了一些特定于语言的保证。
- en: 'We have seen these guarantees in the form of the different memory order specifications:
    relaxed, acquire, release, and acquire-release. C++ has an even stricter memory
    order called `std::memory_order_seq_cst`), which is the default order you get
    when you don''t specify one: not only there is a bidirectional memory barrier
    associated with each atomic operation that specifies this order, but the entire
    program satisfies the sequential consistency requirement. This requirement states
    that the program behaves as if all operations executed by all processors were
    executed in a single global order. Furthermore, this global order has an important
    property: consider any two operations A and B that are executed on one processor
    such that A executed before B. These two operations must appear in the global
    order with A preceding B as well. You can think of a sequentially consistent program
    like this: imagine a deck of cards for every processor, where the cards are operations.
    Then we slide these decks together without shuffling them; cards from one deck
    slide between cards from the other deck, but the order of the cards from the same
    deck never changes. The one combined deck of cards is the apparent global order
    of the operations in the program. Sequential consistency is a desirable property
    because it makes it much easier to reason about the correctness of a concurrent
    program. It does, however, often come at a cost in performance. We can demonstrate
    this cost in a very simple benchmark that compares different memory orders:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在不同的内存顺序规范中看到了这些保证：relaxed、acquire、release和acquire-release。C++还有一种更严格的内存顺序，称为`std::memory_order_seq_cst`，这是当你不指定顺序时默认的顺序：不仅每个指定此顺序的原子操作都有一个双向内存屏障，而且整个程序都满足顺序一致性要求。这个要求规定程序的行为就好像所有处理器执行的所有操作都是按照一个全局顺序执行的。此外，这个全局顺序具有一个重要的特性：考虑在一个处理器上执行的任意两个操作A和B，使得A在B之前执行。这两个操作必须以A在前、B在后的顺序出现在全局顺序中。你可以把一个顺序一致的程序想象成这样：想象每个处理器都有一副牌，牌就是操作。然后我们将这些牌堆在一起，而不混洗它们；一副牌的牌会在另一副牌的牌之间滑动，但是同一副牌的牌的顺序永远不会改变。合并后的一副牌就是程序中操作的明显全局顺序。顺序一致性是一个理想的特性，因为它使得并发程序的正确性更容易推理。然而，它通常会以性能的代价为代价。我们可以在一个非常简单的基准测试中展示这个代价，比较不同的内存顺序：
- en: '[PRE24]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can run this benchmark using different memory orders. The results will,
    of course, depend on the hardware, but the following result is not uncommon:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用不同的内存顺序来运行这个基准测试。结果当然会取决于硬件，但以下结果并不罕见：
- en: '![Figure 5.14 – Performance of acquire-release vs. sequential consistency memory
    order'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.14 - acquire-release与顺序一致性内存顺序的性能'
- en: '](img/Figure_5.14_B16229.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.14_B16229.jpg)'
- en: Figure 5.14 – Performance of acquire-release vs. sequential consistency memory
    order
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 - acquire-release与顺序一致性内存顺序的性能
- en: 'There is a lot more to the C++ memory model than just the atomic operations
    and the memory order. For example, when we studied false sharing earlier, we have
    assumed that it is safe to access adjacent elements of an array from multiple
    threads concurrently. It makes sense: these are different variables. And yet,
    it was not guaranteed by the language or even by the additional restrictions adopted
    by the compiler. On most hardware platforms, accessing adjacent elements of an
    array of integers is indeed thread-safe. But it is definitely not the case for
    data types of smaller size, for example, an array of `bool`. Many processors write
    a single byte using a *masked* integer write: they load the entire 4-byte word
    containing this byte, change the byte to the new value, and write the word back.
    Obviously, if two processors do this at the same time for two bytes that share
    the same 4-byte word, the second write will overwrite the first one. The C++11
    memory model requires that writing into any distinct variables, such as array
    elements, is thread-safe if no two threads access the same variable. Prior to
    C++11, it was easy to write a program that would demonstrate that writing into
    two adjacent `bool` or `char` variables from two threads is not thread-safe. The
    only reason we don''t have this demonstration in this book is that the compilers
    available today don''t fall back to this aspect of C++03 behavior even if you
    specify the standard level as C++03 (this is not guaranteed, and a compiler could
    use masked writes to write single bytes in C++03 mode, but most compilers use
    the same instructions as in C++11 mode).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: C++内存模型还有很多内容，不仅仅是原子操作和内存顺序。例如，当我们之前研究了伪共享时，我们假设从多个线程同时访问数组的相邻元素是安全的。这是有道理的：这些是不同的变量。然而，语言甚至编译器采用的额外限制也不能保证这一点。在大多数硬件平台上，访问整数数组的相邻元素确实是线程安全的。但对于更小的数据类型，比如`bool`数组，情况绝对不是这样。许多处理器使用*掩码*整数写入来写入单个字节：它们加载包含此字节的整个4字节字，将字节更改为新值，然后将字写回。显然，如果两个处理器同时对共享相同4字节字的两个字节执行此操作，第二个写入将覆盖第一个写入。C++11内存模型要求，如果没有两个线程访问相同的变量，那么写入任何不同的变量，比如数组元素，都是线程安全的。在C++11之前，很容易编写一个程序来证明从两个线程写入两个相邻的`bool`或`char`变量是不安全的。我们之所以在本书中没有这个演示，是因为即使您将标准级别指定为C++03（这并不是保证，编译器可能会使用掩码写入以在C++03模式下写入单个字节，但大多数编译器在C++11模式下使用与C++11模式相同的指令），今天可用的编译器也不会回退到C++03行为的这一方面。
- en: 'The last example of the importance of the C++ memory model also contains a
    valuable observation: the language and the compiler are not all that defined the
    memory model. The hardware has a memory model, the OS and the runtime environment
    have their memory models, and each component of the hardware/software system the
    program runs on has a memory model. The overall memory model, the total set of
    guarantees and restrictions available to the program, is a superposition of all
    of these memory models. Sometimes you can take advantage of that, for example,
    when writing processor-specific code. However, any portable C++ code can rely
    only on the memory model of the language itself, and, more often than not, other
    underlying memory models are a complication.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: C++内存模型的最后一个例子也包含了一个有价值的观察：语言和编译器并不是定义内存模型的全部。硬件有一个内存模型，操作系统和运行时环境有它们的内存模型，程序运行的硬件/软件系统的每个组件都有一个内存模型。整体内存模型，程序可用的所有保证和限制的总集，是所有这些内存模型的叠加。有时您可以利用这一点，比如在编写特定于处理器的代码时。然而，任何可移植的C++代码只能依赖于语言本身的内存模型，而且往往其他底层内存模型会带来复杂性。
- en: Two kinds of problems arise because of the differences in the memory model of
    the language and that of the hardware. First of all, there may be bugs in your
    program that cannot be detected on particular hardware. Consider the acquire-release
    protocol we used for our producer-consumer program. If we made a mistake and used
    the release memory order on the producer side but relaxed memory order (no barrier
    at all) on the consumer side, we would expect the program to produce incorrect
    results intermittently. However, if you run this program on an x86 CPU, it would
    appear to be correct. This is because the memory model of the x86 architecture
    is such that every store is accompanied by a release barrier, and every load has
    an implicit acquire barrier. Our program still has a bug, and it would trip us
    up if we ported it to, say, an ARM-based processor (like the one in an iPad).
    But the only way to find this bug on x86 hardware is with the help of a tool like
    the **Thread Sanitizer** (**TSAN**) available in GCC and Clang.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语言和硬件的内存模型差异，会出现两种问题。首先，您的程序可能存在无法在特定硬件上检测到的错误。考虑我们为生产者-消费者程序使用的获取-释放协议。如果我们犯了一个错误，在生产者端使用了释放内存顺序，但在消费者端使用了松散内存顺序（根本没有屏障），我们会期望程序会间歇性地产生错误结果。然而，如果您在x86
    CPU上运行此程序，它看起来是正确的。这是因为x86架构的内存模型是这样的，每个存储都伴随着一个释放屏障，每个加载都有一个隐式获取屏障。我们的程序仍然有一个错误，如果我们将其移植到比如iPad中的基于ARM的处理器上，它会让我们遇到麻烦。但是在x86硬件上找到这个bug的唯一方法是使用类似GCC和Clang中可用的**Thread
    Sanitizer**（**TSAN**）这样的工具。
- en: 'The second problem is the flip-side of the first one: reducing the restrictions
    on the memory order does not always result in better performance. As you can expect
    from what you have just learned, going from release to relaxed memory order on
    write operations does not yield any benefit on an x86 processor because the overall
    memory model still guarantees the release order (in theory, the compiler might
    do more optimizations with the relaxed memory order than with the release one,
    however, most compilers do not optimize the code across atomic operations at all).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是第一个问题的反面：降低内存顺序的限制并不总是会带来更好的性能。正如您刚刚所学到的，从释放到松散的内存顺序在x86处理器上的写操作上并不会带来任何好处，因为整体内存模型仍然保证释放顺序（理论上，编译器可能会对松散内存顺序进行更多优化，而不是释放内存顺序，但是大多数编译器根本不会跨原子操作优化代码）。
- en: The memory model provides both the scientific foundation and the common language
    for discussing how programs interact with the memory system. The memory barriers
    are the actual tools the programmer utilizes, in code, to control the memory model
    features. Often, these barriers are invoked implicitly by using locks, but they
    are always there. The optimal use of memory barriers can make a large difference
    in the efficiency of certain high-performance concurrent programs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 内存模型为讨论程序如何与内存系统交互提供了科学基础和共同语言。内存屏障是程序员在代码中实际使用的工具，用于控制内存模型的特性。通常情况下，通过使用锁隐式地调用这些屏障，但它们总是存在的。合理使用内存屏障可以极大地提高某些高性能并发程序的效率。
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned about the C++ memory model and the guarantees
    it gives to the programmer. The result is a thorough understanding of the low
    level of what happens when multiple threads interact through shared data.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了C++内存模型以及它给程序员的保证。结果是对多个线程通过共享数据进行交互时发生的低级细节有了深入的理解。
- en: In multi-threaded programs, unsynchronized and unordered access to memory leads
    to undefined behavior and must be avoided at any cost. The cost, however, is usually
    paid in performance. While we always value a correct program over an incorrect
    but fast one, when it comes to memory synchronization, it is easy to overpay for
    correctness. We have seen different ways to manage concurrent memory accesses,
    their advantages, and tradeoffs. The simplest option is to lock all accesses to
    the shared data. The most elaborate implementation, on the other hand, uses atomic
    operations and restricts memory order as little as possible.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在多线程程序中，未同步和无序的内存访问会导致未定义的行为，必须尽一切可能避免。然而，通常情况下代价是性能。虽然我们总是更看重正确的程序而不是快速但不正确的程序，但在内存同步方面，很容易为了正确性而付出过高的代价。我们已经看到了管理并发内存访问的不同方式，它们的优势和权衡。最简单的选择是锁定对共享数据的所有访问。另一方面，最复杂的实现使用原子操作，并尽可能限制内存顺序。
- en: 'The first rule of performance is in full force here: performance must be measured,
    not guessed. This is even more important for concurrent programs where clever
    optimizations can fail to yield measurable results for a multitude of reasons.
    On the other hand, the one guarantee you always have is that a simple program
    with locks is easier to write and is more likely to be correct.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 性能的第一准则在这里完全适用：性能必须被测量，而不是猜测。这对于并发程序来说更加重要，因为聪明的优化可能由于多种原因而无法产生可衡量的结果。另一方面，你始终可以保证的是，使用锁的简单程序更容易编写，而且更有可能是正确的。
- en: 'Armed with the understanding of the fundamental factors affecting the performance
    of data sharing, you can better understand the results of your measurements, as
    well as developing some sense for when it makes sense to even try to optimize
    the concurrent memory accesses: the larger the part of your code affected by the
    memory order restrictions, the more likely it is that relaxing these restrictions
    will improve the performance. Also, keep in mind that some of the restrictions
    come from the hardware itself.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握了影响数据共享性能的基本因素，你可以更好地理解测量结果，以及在何时尝试优化并发内存访问时有一些感觉：受内存顺序限制影响的代码部分越大，放宽这些限制就越有可能提高性能。另外，要记住，一些限制来自硬件本身。
- en: Overall, this is much more complex material than anything you had to deal with
    in the earlier chapters (not surprising, concurrency is hard in general). The
    next chapter shows some of the ways you can manage this complexity in your program
    without giving up the performance benefits. You will also see the practical applications
    of the knowledge you have learned here.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这比你在前几章中需要处理的任何内容都要复杂得多（这并不奇怪，总的来说，并发性本身就很难）。下一章将展示一些你可以在程序中管理这种复杂性的方法，而不放弃性能优势。你还将看到你在这里学到的知识的实际应用。
- en: Questions
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the memory model?
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是内存模型？
- en: Why is access to the shared data so important to understand?
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么理解对共享数据的访问如此重要？
- en: What determines the overall memory model for a program?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是什么决定了程序的整体内存模型？
- en: What constrains the performance gain from concurrency?
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么限制了并发性带来的性能提升？
