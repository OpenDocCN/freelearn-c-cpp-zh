- en: '*Chapter 4*: Memory Architecture and Performance'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：内存架构和性能'
- en: After the CPU, the memory is often the hardware component that is limiting the
    overall program performance. In this chapter, we begin by learning about modern
    memory architectures, their inherent weaknesses, and the ways to counter or at
    least hide these weaknesses. For many programs, the performance is entirely dependent
    on whether the programmer takes advantage of the hardware features designed to
    improve memory performance, and this chapter teaches the necessary skills.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU之后，内存通常是限制整体程序性能的硬件组件。在本章中，我们首先学习现代内存架构，它们固有的弱点以及对抗或至少隐藏这些弱点的方法。对于许多程序来说，性能完全取决于程序员是否利用了旨在提高内存性能的硬件特性，本章将教授必要的技能。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Overview of the memory subsystem
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存子系统概述
- en: Performance of memory accesses
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存访问性能
- en: Access patterns and impact on algorithms and data structure design
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问模式及其对算法和数据结构设计的影响
- en: Memory bandwidth and latency
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存带宽和延迟
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Again, you will need a C++ compiler and a micro-benchmarking tool, such as
    the Google Benchmark library we used in the previous chapter (found at [https://github.com/google/benchmark](https://github.com/google/benchmark)).
    We will also use the **LLVM Machine Code Analyzer** (**LLVM-MCA**), found at [https://llvm.org/docs/CommandGuide/llvm-mca.html](https://llvm.org/docs/CommandGuide/llvm-mca.html).
    If you want to use the MCA, your choice of compilers is more limited: you need
    an LLVM-based compiler such as Clang.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您将需要一个C++编译器和一个微基准测试工具，例如我们在上一章中使用的Google Benchmark库（位于[https://github.com/google/benchmark](https://github.com/google/benchmark)）。我们还将使用**LLVM机器码分析器**（**LLVM-MCA**），位于[https://llvm.org/docs/CommandGuide/llvm-mca.html](https://llvm.org/docs/CommandGuide/llvm-mca.html)。如果您想使用MCA，您的编译器选择将更有限：您需要一个基于LLVM的编译器，如Clang。
- en: 'The code for the chapter can be found here: [https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter04](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter04)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在此处找到：[https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter04](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter04)
- en: The performance begins with the CPU but does not end there
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能始于CPU，但并不止于此。
- en: 'In the previous chapter, we studied the CPU resources and the ways to use them
    for optimal performance. In particular, we observed that CPUs have the ability
    to do quite a lot of computation in parallel (instruction-level parallelism).
    We demonstrated it on multiple benchmarks, which show that the CPU can do many
    operations per cycle without any performance penalty: adding and subtracting two
    numbers, for example, takes just as much time as only adding them.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们研究了CPU资源以及如何将它们用于实现最佳性能。特别是，我们观察到CPU具有并行进行大量计算的能力（指令级并行性）。我们在多个基准测试中进行了演示，显示CPU可以在没有任何性能惩罚的情况下每个周期执行许多操作：例如，添加和减去两个数字所需的时间与仅添加它们所需的时间相同。
- en: 'You might have noticed, however, that these benchmarks and examples have one
    rather unusual property. Consider the following example:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您可能已经注意到，这些基准测试和示例具有一个相当不寻常的特性。考虑以下示例：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We have already used this fragment of code to demonstrate that the CPU can do
    eight operations on the two values, `p1[i]` and `p2[i]`, at almost no extra cost
    compared to just one operation. But we were always very careful to add more operations
    without adding more inputs; on several occasions, we mentioned, in passing, that
    the CPU's internal parallelism applies *as long as the values are already in the
    registers*. In the earlier example, while adding the second, third, and so on
    until the eighth operation, we were careful to stay with just two inputs. This
    results in some unusual and unrealistic code. In real life, how many things do
    you usually need to compute on a given set of inputs? Less than eight, most of
    the time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了这段代码片段来证明CPU可以对两个值`p1[i]`和`p2[i]`进行八次操作，几乎没有额外成本，与仅进行一次操作相比。但我们总是非常小心地添加更多操作而不添加更多输入；在几个场合，我们提到过，CPU的内部并行性适用于*只要值已经在寄存器中*。在之前的示例中，当添加第二个、第三个等直到第八个操作时，我们小心地保持只有两个输入。这导致了一些不寻常和不现实的代码。在现实生活中，您通常需要在给定的输入集上计算多少个事情？大多数情况下少于八个。
- en: This doesn't mean that the entire computational potential of the CPU is wasted
    unless you happen to run exotic code like the earlier example. The instruction-level
    parallelism is the computational foundation for pipelining, where we execute operations
    from different iterations of the loop simultaneously. Branchless computing is
    all about trading conditional instructions for unconditional computations and,
    therefore, relies almost entirely on the fact that we can usually get a few more
    computations for free.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着除非您运行类似之前示例的奇异代码，否则CPU的整个计算潜力都会被浪费。指令级并行性是流水线处理的计算基础，我们可以同时执行循环不同迭代的操作。无分支计算完全是为了将条件指令换成无条件计算，因此几乎完全依赖于通常情况下我们通常可以免费获得更多计算的事实。
- en: 'The question remains, however: why did we limit our CPU benchmarks in this
    manner? After all, it would have been so much easier to come up with eight different
    things to do in the earlier example if we just added more inputs:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，问题仍然存在：为什么我们要限制CPU基准测试的方式呢？毕竟，如果我们只是增加更多的输入，那么在之前的示例中想出八种不同的事情会更容易得多：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is the same code as we saw earlier, only now it operates on four different
    input values per iteration instead of two. It does inherit all the awkwardness
    of the previous example, but only because we want to change as little as possible
    when measuring the impact of some change on performance. And the impact is significant:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们之前看到的代码相同，只是现在每次迭代操作四个不同的输入值，而不是两个。它继承了之前示例的所有尴尬之处，但只是因为我们希望在测量某些性能变化的影响时尽可能少地进行更改。而且影响是显著的：
- en: '![Figure 4.1'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.1'
- en: '](img/Image86710.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Image86710.jpg)'
- en: Figure 4.1
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1
- en: The same computations done on four input values take about 36% longer. The computations
    are delayed, somehow, when we need to access more data in memory.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对四个输入值进行相同的计算大约需要多36%的时间。当我们需要在内存中访问更多数据时，计算会受到延迟。
- en: 'It should be noted that there is another reason why adding more independent
    variables, inputs, or outputs, could impact the performance: the CPU could be
    running out of registers in which to store these variables for computations. While
    this is a significant concern in many real programs, it is not the case here.
    The code isn''t complex enough to use up all the registers of a modern CPU (the
    easiest way to confirm this is by examining the machine code, unfortunately).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 应该指出，还有另一个原因会影响性能，那就是增加更多的独立变量、输入或输出可能会影响性能：CPU可能会用尽寄存器来存储这些变量进行计算。虽然这是许多实际程序中的一个重要问题，但在这里并非如此。这段代码并不复杂到足以用完现代CPU的所有寄存器（确认这一点最简单的方法是检查机器代码，不幸的是）。
- en: 'Clearly, accessing more data seems to reduce the speed of the code. But why?
    At a very high level, the reason is that the memory simply cannot keep up with
    the CPU. There are several ways to estimate the size of this *memory gap*. The
    simplest way is evident in the specs of a modern CPU. CPUs today operate at clock
    frequencies between 3 GHz and 4 GHz, which means that one cycle is about 0.3 nanoseconds.
    As we have seen, under the right circumstances, the CPU can do several operations
    per second, so executing ten operations per nanosecond is not out of the question
    (although hard to achieve in practice and is a sure sign of a very efficient program).
    On the other hand, the memories are much slower: the DDR4 memory clock, for example,
    operates at 400 MHz. You can also find the values as high as 3200 MHz; however,
    this is not the memory clock but the *data rate*, and to convert it to something
    resembling *memory speed,* you also have to take into account the **Column Access
    Strobe Latency**, usually known as **CAS Latency** or **CL**. Roughly, this is
    the number of cycles it takes for the RAM to receive a request for data, process
    it, and return the value. There is no single definition of memory speed that makes
    sense under all circumstances (later in this chapter, we will see some of the
    reasons why), but, to the first approximation, the memory speed of a DDR4 module
    with the data rate of 3.2 GHz and CAS Latency 15 is about 107 MHz or 9.4 nanoseconds
    per access.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，访问更多的数据似乎会降低代码的速度。但是为什么呢？从非常高的层面上来说，原因是内存根本跟不上CPU。有几种方法可以估计这种*内存差距*的大小。最简单的方法在现代CPU的规格中就可以看出来。如我们所见，CPU今天的时钟频率在3
    GHz到4 GHz之间，这意味着一个周期大约是0.3纳秒。在适当的情况下，CPU每秒可以执行多个操作，因此每纳秒执行十次操作并不是不可能的（尽管在实践中很难实现，并且是一个非常高效程序的明确迹象）。另一方面，内存速度要慢得多：例如，DDR4内存时钟的工作频率为400
    MHz。您还可以找到高达3200 MHz的值；但是，这不是内存时钟，而是*数据速率*，要将其转换为类似*内存速度*的东西，您还必须考虑**列访问脉冲延迟**，通常称为**CAS延迟**或**CL**。粗略地说，这是RAM接收数据请求、处理数据请求并返回值所需的周期数。没有一个单一的内存速度定义在所有情况下都是有意义的（本章后面我们将看到一些原因），但是，第一次近似地，具有3.2
    GHz数据速率和CAS延迟15的DDR4模块的内存速度约为107 MHz，或者每次访问需要9.4纳秒。
- en: 'Whichever way you look at it, the CPU can do a lot more operations per second
    than the memory can supply the input values for these operations or store the
    results. All programs need to use memory in some way, and the details of how the
    memory is accessed are going to have a significant impact on performance, sometimes
    to the point of limiting it. The details, however, are extremely important: the
    effects of the *memory gap* on performance can vary from insignificant to memory
    becoming the bottleneck of the program. We have to understand how the memory impacts
    the program performance under different conditions and why, so we can use this
    knowledge to design and implement our code for the best performance.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 无论从哪个角度来看，CPU每秒可以执行的操作比内存提供的输入值要多得多，或者存储结果。所有程序都需要以某种方式使用内存，内存访问的细节将对性能产生重大影响，有时甚至会限制性能。然而，这些细节非常重要：*内存差距*对性能的影响可以从微不足道到内存成为程序的瓶颈。我们必须了解内存在不同条件下对程序性能的影响以及原因，这样我们才能利用这些知识来设计和实现最佳性能的代码。
- en: Measuring memory access speed
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量内存访问速度
- en: We have good evidence to assume that CPUs can operate much faster on the data
    already in registers compared to the data in memory. The specifications of the
    processor and memory speeds alone suggest at least an order of magnitude difference.
    However, we have learned by now not to make any guesses or assumptions about performance
    without verifying them through direct measurements. This does not mean that any
    prior knowledge about the system architecture and any assumptions we can make
    based on that knowledge are not useful. Such assumptions can be used to guide
    the experiments and devise the right measurements. We will see in this chapter
    that the process of discovery *by accident* can take you only so far and can even
    lead you into error. The measurements can be correct in and of themselves, but
    it is often hard to determine what exactly is being measured and what conclusions
    we can derive from the results.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有充分的证据表明，与内存中的数据相比，CPU可以更快地处理寄存器中已有的数据。处理器和内存速度的规格单独就至少暗示了一个数量级的差异。然而，我们现在已经学会了不要在没有通过直接测量验证之前对性能进行任何猜测或假设。这并不意味着对系统架构的任何先前知识以及我们可以基于该知识做出的任何假设都没有用。这些假设可以用来指导实验并设计正确的测量方法。我们将在本章中看到，*偶然*发现的过程只能让你走得更远，甚至可能导致错误。测量本身可能是正确的，但往往很难确定到底在测量什么以及我们可以从结果中得出什么结论。
- en: 'It would seem that measuring memory access speed should be fairly trivial.
    All we need is some memory to read from and a way to time the reads, like so:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 测量内存访问速度似乎应该是相当琐碎的。我们只需要一些内存来读取，并且一种计时读取的方法，就像这样：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This benchmark runs and measures … something. You can expect to get the time
    of one iteration reported as 0 nanoseconds. This could be the result of an unwanted
    compiler optimization: if the compiler figures out that the whole program has
    no observable effects, it may indeed optimize it to nothing. We did take precautions
    against such an event, though: the memory we read is `volatile`, and accessing
    `volatile` memory is considered an observable effect and cannot be optimized away.
    Instead, the 0 nanoseconds result is partly a deficiency in the benchmark itself:
    it suggests that the single read is faster than 1 nanosecond. While this is not
    quite what we expected based on the memory speed, we can''t learn anything, including
    our own mistakes, from a number we do not know. To fix the measurement aspect
    of the benchmark, all we have to do is perform multiple reads in one benchmark
    iteration, like so:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此基准运行和测量……某物。您可以期望报告一个迭代的时间为0纳秒。这可能是不希望的编译器优化的结果：如果编译器发现整个程序没有可观察的效果，它可能会将其优化为无效果。尽管如此，我们已经采取了预防措施：我们读取的内存是`volatile`，访问`volatile`内存被认为是可观察的效果，不能被优化掉。相反，0纳秒的结果在某种程度上是基准本身的不足：它表明单次读取比1纳秒更快。虽然这与我们基于内存速度的预期不太一样，但我们无法从一个我们不知道的数字中学到任何东西，包括我们自己的错误。要修复基准的测量方面，我们所要做的就是在一个基准迭代中执行多次读取，如下所示：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, we perform `32` reads per iteration. While we could figure
    out the time of the individual read from the reported iteration time, it is convenient
    to make the Google Benchmark library do the calculation for us and report the
    number of reads per second; this is accomplished by setting the number of items
    processed at the end of the benchmark.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们每次迭代执行32次读取。虽然我们可以从报告的迭代时间中计算出单个读取的时间，但让Google Benchmark库为我们进行计算并报告每秒读取的次数更方便；这是通过在基准结束时设置处理的项目数量来实现的。
- en: This benchmark should report the iteration time around 5 nanoseconds on a mid-range
    CPU, confirming that a single read is 1/32 of this time and well below 1 nanosecond
    (so our guess about the reason why 0 is reported for a single read per iteration
    is validated). On the other hand, this measured value does not match our expectations
    for the memory being slow. It is possible that our earlier assumptions about what
    makes the performance bottleneck are incorrect; it would not be the first time.
    Or, we could be measuring something other than the memory speed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基准应该在中档CPU上报告迭代时间约为5纳秒，证实单次读取为这个时间的1/32，远低于1纳秒（因此我们对每次迭代单次读取为0的原因的猜测得到了验证）。另一方面，这个测得的值与我们对内存速度的期望不符。我们之前对性能瓶颈的假设可能是错误的；这并非第一次。或者，我们可能正在测量与内存速度不同的东西。
- en: Memory architecture
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存架构
- en: 'To understand how to measure memory performance correctly, we have to learn
    more about the memory architecture of a modern processor. The most important feature
    of the memory system, for our purposes, is that it is hierarchical. The CPU does
    not access the main memory directly but through a hierarchy of caches:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要正确理解如何测量内存性能，我们必须更多地了解现代处理器的内存架构。对于我们的目的来说，内存系统最重要的特性是它是分层的。CPU不直接访问主内存，而是通过一系列缓存层次结构：
- en: '![Figure 4.2 – Memory hierarchy diagram'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.2-内存层次结构图'
- en: '](img/Figure_4.2_B16229.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.2_B16229.jpg)'
- en: Figure 4.2 – Memory hierarchy diagram
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2-内存层次结构图
- en: 'The **RAM** in *Figure 4.2* is the main memory, the DRAM on the motherboard.
    When the system specifications say that the machine has so many gigabytes of memory,
    that''s the capacity of the DRAM. As you can see, the CPU does not access the
    main memory directly but instead through several levels of a hierarchy of caches.
    These caches are also memory circuits, but they are located on the CPU die itself,
    and they use different technology to store the data: they are all SRAMs of different
    speeds. The key difference between the DRAM and the SRAM, from our point of view,
    is that the SRAM is much faster to access, but it draws significantly more power
    than the DRAM. The speed of the memory access increases as we move closer to the
    CPU through the memory hierarchy: the level-1 (**L1**) cache has almost the same
    access time as the CPU registers, but it uses so much power that we can have only
    a few kilobytes of such memory, most commonly 32 KB per CPU core. The next level,
    **L2** cache, is larger but slower, the third level (**L3**) cache is even larger
    but also slower (and usually shared between multiple cores of a CPU), and the
    last level of the hierarchy is the main memory itself.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2中的RAM是主内存，主板上的DRAM。当系统规格说明机器有多少吉字节的内存时，那就是DRAM的容量。正如你所看到的，CPU并不直接访问主内存，而是通过多个层次的缓存层次结构。这些缓存也是内存电路，但它们位于CPU芯片上，并且使用不同的技术来存储数据：它们都是不同速度的SRAM。从我们的角度来看，DRAM和SRAM之间的关键区别是SRAM的访问速度要快得多，但它的功耗比DRAM要大得多。随着我们通过内存层次结构接近CPU，内存访问速度也会增加：一级（L1）缓存的访问时间几乎与CPU寄存器相同，但它使用的功率很大，我们只能有很少的这样的内存，通常每个CPU核心为32KB。下一级，L2缓存，更大但更慢，第三级（L3）缓存更大但也更慢（通常在CPU的多个核心之间共享），层次结构的最后一级是主内存本身。
- en: When the CPU reads a data value from the main memory for the first time, the
    value is propagated through all the cache levels, and a copy of it remains in
    the cache. When the CPU reads the same value again, it does not need to wait for
    the value to be fetched from the main memory because a copy of the same value
    is already available in the fast L1 cache.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当CPU第一次从主内存中读取数据值时，该值通过所有缓存级别传播，并且它的副本留在缓存中。当CPU再次读取相同的值时，它不需要等待该值从主内存中获取，因为相同值的副本已经在快速的L1缓存中可用。
- en: 'As long as the data we want to read fits into the L1 cache, that is all that
    needs to happen: all the data will be loaded into the cache the first time it''s
    accessed, after that, the CPU only ever needs to access the L1 cache. However,
    if we try to access a value that is not currently in the cache and the cache is
    already full, something has to be evicted from the cache to make room for the
    new value. This process is controlled entirely by the hardware, which has some
    heuristics to determine which value we are least likely to need again, based on
    the values we have accessed recently (to the first approximation, the data that
    wasn''t used for the longest time is probably not going to be needed again soon).
    The next-level caches are larger, but they are used in the same way: as long as
    the data is in the cache, it is accessed there (the closer to the CPU, the better).
    Otherwise, it has to be fetched from the next level cache or, for the L3 cache,
    from the main memory, and, if the cache is full, some other piece of data has
    to be evicted from the cache (that is, forgotten by the cache, since the original
    remains in the main memory).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们想要读取的数据适合L1缓存，那就是需要发生的一切：所有数据将在第一次访问时加载到缓存中，之后，CPU只需要访问L1缓存。然而，如果我们尝试访问当前不在缓存中的值，并且缓存已经满了，就必须从缓存中驱逐一些数据以为新值腾出空间。这个过程完全由硬件控制，硬件有一些启发式方法来确定我们最不可能再次需要的值，基于我们最近访问的值（第一次近似，很可能很长时间没有使用的数据可能不会很快再次需要）。下一级缓存更大，但使用方式相同：只要数据在缓存中，就在那里访问（离CPU越近越好）。否则，它必须从下一级缓存或者L3缓存中获取，如果缓存已满，就必须从缓存中驱逐一些其他数据（也就是说，被缓存遗忘，因为原始数据仍然在主内存中）。
- en: 'Now we can better understand what we measured earlier: since we were reading
    the same value over and over, tens of thousands of times, the cost of the initial
    read was completely lost, and the average read time was that of the L1 cache read.
    The L1 cache indeed appears to be quite fast, so if your entire data fits into
    the 32 KB, you do not need to worry about the memory gap. Otherwise, you have
    to learn how to measure memory performance correctly, so you can draw conclusions
    that will be applicable to your program.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以更好地理解我们之前测量的内容：因为我们一遍又一遍地读取相同的值，成千上万次，初始读取的成本完全丢失了，平均读取时间就是L1缓存读取的时间。L1缓存确实似乎非常快，所以如果你的整个数据适合32
    KB，你不需要担心内存差距。否则，你必须学会如何正确测量内存性能，这样你就可以得出适用于你的程序的结论。
- en: Measuring memory and cache speeds
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量内存和缓存速度
- en: 'Now that we understand that the memory speed is more complex than just the
    time of a single read, we can devise a more appropriate benchmark. We can expect
    the cache sizes to affect the results significantly, so we have to access data
    of different sizes, from several kilobytes (fits into the 32 KB L1 cache) to tens
    of megabytes or more (L3 cache sizes vary but are usually around 8 MB to 12 MB).
    Since, for large data volumes, the memory system will have to evict the *old*
    data from the cache, we can expect the performance to depend on how well that
    prediction works or, more generally, on the access patterns. Sequential access,
    such as copying a range of memory, may end up performing very differently than
    accessing the same range in random order. Finally, the results may depend on the
    granularity of the memory access: is accessing a 64-bit `long` value slower than
    accessing a single `char`?'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们明白了内存速度比单次读取的时间更复杂，我们可以设计一个更合适的基准测试。我们可以预期缓存大小会显著影响结果，因此我们必须访问不同大小的数据，从几千字节（适合32
    KB L1缓存）到数十兆字节或更多（L3缓存大小不同，但通常在8 MB到12 MB左右）。由于对于大数据量，内存系统将不得不从缓存中清除*旧*数据，我们可以预期性能取决于该预测的有效性，或者更一般地说，取决于访问模式。顺序访问，比如复制一系列内存，最终的性能可能会与以随机顺序访问相同范围的性能有很大不同。最后，结果可能取决于内存访问的粒度：访问64位`long`值是否比访问单个`char`更慢？
- en: 'A simple benchmark for sequentially reading a large array can look like this:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 用于顺序读取大数组的简单基准测试可以如下所示：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The benchmark for writing looks very similar, with a one-line change in the
    main loop:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 写入的基准测试看起来非常相似，在主循环中只有一行变化：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The value we write into the array should not matter; if you are concerned that
    zero is somehow *special*, you can initialize the `fill` variable with any other
    value.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们写入数组的值不应该有影响；如果你担心零有些*特殊*，你可以用任何其他值初始化`fill`变量。
- en: 'The macro `REPEAT` is used to avoid manually copying the benchmarked code many
    times. We still want to perform several memory reads per iteration: while avoiding
    the *0 nanoseconds per iteration* report is less critical once we start reporting
    the number of reads per second, the overhead of the loop itself is non-trivial
    for a very cheap iteration like ours, so it is better to unroll this loop manually.
    Our `REPEAT` macro unrolls the loop 32 times:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 宏`REPEAT`用于避免手动复制基准测试代码多次。我们仍然希望在每次迭代中执行多次内存读取：一旦我们开始报告每秒读取的次数，避免*每次迭代0纳秒*的报告就不那么关键了，但是对于像我们这样非常便宜的迭代来说，循环本身的开销是非常重要的，因此最好手动展开这个循环。我们的`REPEAT`宏将循环展开32次：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Of course, we have to make sure that the memory size we request is large enough
    for the 32 values of the `Word` type and that the total array size is divisible
    by 32; neither is a significant restriction on our benchmark code.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们必须确保我们请求的内存大小足够大，可以容纳32个`Word`类型的值，并且总数组大小可以被32整除；这两者对我们的基准测试代码都不是重大限制。
- en: 'Speaking of the `Word` type, this is the first time we used a `TEMPLATE` benchmark.
    It is used to generate the benchmarks for several types without copying the code.
    There is a slight difference in invoking such a benchmark:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 说到`Word`类型，这是我们第一次使用`TEMPLATE`基准测试。它用于生成多种类型的基准测试，而不是复制代码。调用这样的基准测试有一点不同：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If the CPU supports it, we can read and write the data in even larger chunks,
    for example, using SSE and AVX instructions to move 16 or 32 bytes at a time on
    an x86 CPU. In GCC or Clang, there are library headers for these larger types:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果CPU支持，我们可以使用SSE和AVX指令以更大的块读取和写入数据，例如在x86 CPU上一次移动16或32字节。在GCC或Clang中，有这些更大类型的库头文件：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The types `__m128i` and `__m256i` are not built into the language (at least
    not C/C++), but C++ lets us declare new types easily: these are value-type classes
    (classes that represent a single value), and they have a set of arithmetic operations
    defined for them, such as addition and multiplication, which the compiler implements
    using the appropriate SIMD instructions.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 类型`__m128i`和`__m256i`不是内置语言（至少不是C/C++），但C++让我们很容易地声明新类型：这些是值类型类（表示单个值的类），并且为它们定义了一组算术运算，例如加法和乘法，编译器使用适当的SIMD指令实现这些运算。
- en: The preceding benchmark accesses the memory range sequentially, from the beginning
    to the end, in order, one word at a time. The size of the memory varies, as specified
    by the benchmark arguments (in the example, from 1 KB to 1 GB, doubling every
    time). After the memory range is copied, the benchmark does it again, from the
    beginning, until enough measurements are accumulated.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的基准测试按顺序访问内存范围，从开始到结束，依次，每次一个字。内存的大小会变化，由基准参数指定（在本例中，从1 KB到1 GB，每次加倍）。复制完内存范围后，基准测试会再次进行，从开始，直到积累足够的测量。
- en: 'More care must be taken when measuring the speed of accessing the memory in
    random order. The *naïve* implementation would see us benchmarking the code that
    looks something like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在以随机顺序访问内存速度时，必须更加小心。*天真*的实现会导致我们测量类似这样的代码：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Unfortunately, this benchmark measures the time it takes to call the `rand()`
    function: it is so much more computationally expensive than reading a single integer
    that you''ll never notice the cost of the latter. Even the modulo operator `%`
    is significantly more expensive than a single read or write. The only way to get
    something remotely accurate is to precompute the random indices and store them
    in another array. Of course, we have to contend with the fact that we''re now
    reading both the index values and the indexed data, so the measured cost is that
    of two reads (or a read and a write).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个基准测试测量了调用`rand()`函数所需的时间：它的计算成本比读取一个整数要高得多，你永远不会注意到后者的成本。甚至取模运算符`%`的成本也比单个读取或写入要高得多。获得一些近似准确的方法是预先计算随机索引并将它们存储在另一个数组中。当然，我们必须面对这样一个事实，即我们现在既读取索引值又读取索引数据，因此测量成本是两次读取（或一次读取和一次写入）。
- en: 'The additional code for writing memory in random order can be as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 按随机顺序写入内存的附加代码可以如下所示：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here we use the STL algorithm `random_shuffle` to generate a random order of
    indices (we could have used random numbers instead; it''s not exactly the same
    since some indices would have appeared more than once and others never, but it
    should not affect the results much). The value we write should not really matter:
    writing any number takes the same time, but the compiler can sometimes do special
    optimizations if it can figure out that the code is writing a lot of zeroes, so
    it''s best to avoid that and write something else. Note also that the longer AVX
    types cannot be initialized with an integer, so we write an arbitrary bit pattern
    into the writing value using `memset()`.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用STL算法`random_shuffle`生成索引的随机顺序（我们也可以使用随机数；虽然有些索引可能出现多次，而其他索引可能从未出现，但这不应该对结果产生太大影响）。我们写入的值实际上并不重要：写入任何数字都需要相同的时间，但是如果编译器能够确定代码正在写入大量零，它有时可以进行特殊优化，因此最好避免这样做并写入其他内容。还要注意，更长的AVX类型不能用整数初始化，因此我们使用`memset()`将任意位模式写入写入值。
- en: 'The benchmark for reading is, of course, very similar, just the inner loop
    has to change:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 读取的基准测试当然非常相似，只是内部循环必须改变：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We have the benchmarking code that measures mostly the cost of the memory access.
    The arithmetic operations necessary to advance the indices are unavoidable, but
    the additions take a single cycle at most, and we have already seen that the CPU
    can do several at once, so the math is not going to be the bottleneck (and, in
    any case, any program that accesses memory in an array would have to do the same
    computations, so this is the access speed that matters in practice). Now let us
    see the results of our efforts.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有测量主要测量内存访问成本的基准代码。推进索引所需的算术运算是不可避免的，但是加法最多需要一个周期，并且我们已经看到CPU可以同时执行多个加法，因此数学不会成为瓶颈（而且无论如何，任何访问数组中的内存的程序都必须执行相同的计算，因此实际上重要的是访问速度）。现在让我们看看我们的努力的结果。
- en: 'The speed of memory: the numbers'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存速度：数字
- en: Now that we have our benchmarking code to measure the speed of reading and writing
    into memory, we can collect the results and see how we can get the best performance
    when accessing data in memory. We begin with random access, where the location
    of each value we read or write is unpredictable.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了测量读取和写入内存速度的基准测试代码，我们可以收集结果并看看在访问内存中的数据时如何获得最佳性能。我们首先从随机访问开始，其中我们读取或写入的每个值的位置是不可预测的。
- en: The speed of random memory access
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机内存访问的速度
- en: 'The measurements are likely to be fairly noisy unless you run this benchmark
    many times and average the results (the benchmark library can do that for you).
    For a *reasonable* run time (minutes), you will likely see the results that look
    something like this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 测量结果可能会相当嘈杂，除非你多次运行这个基准测试并对结果取平均值（基准库可以为你做到这一点）。对于一个*合理*的运行时间（几分钟），你可能会看到类似这样的结果：
- en: '![Figure 4.3 – Random read speed as a function of memory size'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.3 - 内存大小的随机读取速度'
- en: '](img/Figure_4.3_B16229.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.3_B16229.jpg)'
- en: Figure 4.3 – Random read speed as a function of memory size
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 - 内存大小的随机读取速度
- en: 'The benchmark results in *Figure 4.3* show the number of words read from memory
    per second (in billions, on any reasonable PC or workstation you can find today),
    where the *word* is a 64-bit integer or a 265-bit integer (`long` or `__m256i`,
    respectively). The same measurements can be alternatively presented as the time
    it takes to read a single word of the chosen size:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.3*中的基准结果显示了每秒从内存中读取的字数（以十亿计，在任何合理的PC或工作站上都可以找到），其中*字*是64位整数或256位整数（`long`或`__m256i`，分别）。相同的测量结果也可以用所选大小的单个字的读取时间来呈现。'
- en: '![Figure 4.4 – Read time for one array element versus array size'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.4 - 读取一个数组元素的时间与数组大小'
- en: '](img/Figure_4.4_B16229.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.4_B16229.jpg)'
- en: Figure 4.4 – Read time for one array element versus array size
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 - 读取一个数组元素的时间与数组大小
- en: 'The graphs have several interesting features we can observe at once. First
    of all, as we expected, there is no single memory speed. The time it takes to
    read a single 64-bit integer varies from 0.3 nanoseconds to 7 nanoseconds on the
    machine I have used. Reading small amounts of data is significantly faster, per
    value, than reading large amounts of data. We can see the cache sizes in these
    graphs: the L1 cache of 32 KB is fast, and the read speed does not depend on the
    data volume as long as it all fits into the L1 cache. As soon as we exceed 32
    KB of data, the read speed starts to drop. The data now fits into the L2 cache,
    which is larger (256 KB) but slower. The larger the array, the smaller is the
    portion of it that fits into the fast L1 cache at any time, and the slower is
    the access.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图表有几个有趣的特点，我们可以一次观察到。首先，正如我们预期的那样，没有单一的内存速度。从我使用的机器上读取一个64位整数的时间从0.3纳秒到7纳秒不等。读取少量数据的速度，每个值而言，比读取大量数据要快得多。我们可以从这些图表中看到缓存的大小：32
    KB的L1缓存速度快，只要所有数据都适合L1缓存，读取速度就不依赖于数据量。一旦我们超过32 KB的数据，读取速度就开始下降。数据现在适合于L2缓存，它更大（256
    KB）但速度较慢。数组越大，适合快速L1缓存的部分就越小，访问速度就越慢。
- en: 'The read time increases even more if the data spills out of the L2 cache, and
    we have to use the L3 cache, which is even slower. The L3 cache is much larger,
    though, so nothing happens until the data size exceeds 8 MB. Only at that point
    do we actually start reading from the main memory: until now, the data was moved
    from the memory into caches the first time we touched it, and all subsequent read
    operations used the caches only. But if we need to access more than 8 MB of data
    at once, some of it will have to be read from the main memory (on this machine,
    anyway—cache sizes vary between CPU models). We don''t lose the benefit of caches
    right away, of course: as long as most data fits in the cache, it is at least
    somewhat effective. But once the volume of data exceeds the cache size by several
    times, the read time is almost completely determined by the time it takes to retrieve
    the data from the memory.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据溢出L2缓存，读取时间会进一步增加，我们必须使用更慢的L3缓存。L3缓存更大，但速度更慢。然而，直到数据大小超过8MB，才会发生任何事情。只有在那时，我们才会实际从主存储器中读取数据：直到现在，数据是在我们第一次接触它时从内存中移动到缓存中的，所有后续的读取操作都只使用缓存。但是，如果我们需要一次访问超过8MB的数据，其中一些数据将不得不从主存储器中读取（在这台机器上，缓存大小会因CPU型号而异）。当然，我们不会立即失去缓存的好处：只要大部分数据适合缓存，它至少在某种程度上是有效的。但是一旦数据量超过缓存大小几倍，读取时间几乎完全取决于从内存中检索数据所需的时间。
- en: Whenever we need to read or write some variable, and we find it in a cache,
    we call it a *cache hit*. However, if it's not found, then we register a *cache
    miss*. Of course, an L1 cache miss can be an L2 hit. An L3 cache miss means that
    we have to go all the way to the main memory.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们需要读取或写入某个变量，并且在缓存中找到它时，我们称之为*缓存命中*。然而，如果没有找到，那么我们就会注册*缓存未命中*。当然，L1缓存未命中可能会成为L2命中。L3缓存未命中意味着我们必须一直到主存储器。
- en: 'The second property of note is the value itself: 7 nanoseconds to read a single
    integer from memory. By processor standards, this is a very long time: in the
    previous chapter, we have seen that the same CPU can do several operations per
    nanosecond. Let this sink in: the CPU can do about 50 arithmetic operations in
    the time it takes to read a single integer value from memory unless the value
    happens to be in the cache already. Very few programs need to do 50 operations
    on each value, which means that the CPU will likely be underutilized unless we
    can figure out something to speed up memory access.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 值本身的第二个值得注意的属性是：从内存中读取一个整数需要7纳秒。按照处理器的标准，这是一个非常长的时间：在前一章中，我们已经看到相同的CPU可以在每纳秒做几个操作。让这个事实深入人心：CPU可以在读取单个整数值的时间内做大约50个算术运算，除非该值已经在缓存中。很少有程序需要对每个值进行50次操作，这意味着除非我们能找出一些方法来加速内存访问，否则CPU可能会被低效利用。
- en: 'Finally, we see that the read speed in words per second does not depend on
    the size of the word. From a practical point of view, the most relevant implication
    is that we can read four times as much data if we use 256-bit instructions to
    read the memory. Of course, it''s not that simple: SSE and AVX load instructions
    read values into different registers than the regular loads, so we also have to
    use the SSE or AVX SIMD instructions to do the computations. One simpler case
    is when we just need to copy a large amount of data from one location in memory
    to another; our measurements suggest that copying 256-bit words does the job four
    times faster than using 64-bit words. Of course, there is already a library function
    that copies memory, `memcpy()` or `std::memcpy()`, and it is optimized for best
    efficiency.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看到每秒的读取速度不取决于字的大小。从实际角度来看，最相关的含义是，如果我们使用256位指令来读取内存，我们可以读取四倍的数据。当然，事情并不那么简单：SSE和AVX加载指令将值读入不同的寄存器，而不是常规加载，因此我们还必须使用SSE或AVX
    SIMD指令来进行计算。一个更简单的情况是当我们只需要从内存的一个位置复制大量数据到另一个位置；我们的测量表明，复制256位字的速度比使用64位字快四倍。当然，已经有一个复制内存的库函数`memcpy()`或`std::memcpy()`，它经过了最佳效率的优化。
- en: 'There is another implication from the fact that the speed does not depend on
    the word size: it implies that the read speed is limited by latency and not by
    bandwidth. Latency is the delay between the time the request for data is issued
    and the time the data is retrieved. Bandwidth is the total amount of data the
    memory bus can transmit in a given time. Going from a 64-bit word to a 256-bit
    word transmits four times as much data in the same time; this implies that we
    haven''t hit the bandwidth limit yet. While this may seem like a purely theoretical
    distinction, it does have important consequences for writing efficient programs
    that we will learn about later in this chapter.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个暗示是，速度不依赖于字长的事实：这意味着读取速度受延迟而不是带宽限制。延迟是发出数据请求和检索数据之间的延迟时间。带宽是内存总线在给定时间内可以传输的数据总量。从64位字到256位字传输的数据量是相同时间内的四倍；这意味着我们还没有达到带宽限制。虽然这可能看起来是一个纯理论上的区别，但它对编写高效程序有重要的影响，我们将在本章后面学习到。
- en: 'Finally, we can measure the speed of writing the memory:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以测量写入内存的速度：
- en: '![Figure 4.5 – Write time for one array element versus array size'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.5 - 一个数组元素的写入时间与数组大小的关系'
- en: '](img/Figure_4.5_B16229.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.5_B16229.jpg)'
- en: Figure 4.5 – Write time for one array element versus array size
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 - 一个数组元素的写入时间与数组大小的关系
- en: 'In our case, the random reads and writes have very similar performance, but
    this can vary for different hardware: sometimes reads are faster. Everything we
    observed earlier about the speed of reading memory also applies to writing: we
    see the cache size effects in *Figure 4.5*, the overall wait time for a write
    is very long if the main memory is involved, and writing large words is more efficient.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，随机读写的性能非常相似，但这在不同的硬件上可能会有所不同：有时读取速度更快。我们之前观察到的有关读取内存速度的一切也适用于写入：我们在图4.5中看到了缓存大小的影响，如果主内存参与其中，写入一个元素的总等待时间非常长，而写入大字更有效。
- en: 'What can we conclude about the impact of memory access on performance? On the
    one hand, if we need to access a small amount of data (less than 32 KB) repeatedly,
    we don''t have to worry much about it. Of course, *repeatedly* is the key here:
    the first access to any memory location will have to touch the main memory regardless
    of how much memory we plan to access (the computer doesn''t know that your array
    is small until you read the entire array and go back to the beginning—reading
    the first element of a small array for the first time looks exactly the same as
    reading the first element of a large array). On the other hand, if we have to
    access large amounts of data, the memory speed is likely to become our first concern:
    at 7 nanoseconds per number, you can''t get very far.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 关于内存访问对性能的影响，我们可以得出什么结论？一方面，如果我们需要重复访问少量数据（小于32KB），我们不必太担心。当然，“重复”是关键：对任何内存位置的第一次访问将不得不触及主内存，无论我们计划访问多少内存（计算机不知道你的数组很小，直到你读取整个数组并回到开头——第一次读取小数组的第一个元素看起来与读取大数组的第一个元素完全相同）。另一方面，如果我们需要访问大量数据，内存速度很可能成为我们的首要关注点：每个数字需要7纳秒，你走不了太远。
- en: There are several techniques for improving memory performance that we will see
    throughout this chapter. Before we study how to improve our code, let us see what
    help we can get from the hardware itself.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将看到几种提高内存性能的技术。在我们研究如何改进我们的代码之前，让我们看看我们可以从硬件本身得到什么帮助。
- en: The speed of sequential memory access
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序内存访问的速度
- en: So far, we have measured the speed of accessing memory at random locations.
    When we do this, every memory access is effectively new. The entire array we are
    reading is loaded into the smallest cache it can fit into, and then our reads
    and writes randomly access different locations in that cache. If the array does
    not fit into any cache, then we randomly access different locations in memory
    and incur the 7 nanoseconds latency on every access (for the hardware we use).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经测量了在随机位置访问内存的速度。当我们这样做时，每次内存访问实际上都是新的。我们正在读取的整个数组被加载到它可以容纳的最小缓存中，然后我们的读写随机访问该缓存中的不同位置。如果数组无法适应任何缓存，那么我们将随机访问内存中的不同位置，并在每次访问时产生7纳秒的延迟（对于我们使用的硬件）。
- en: 'Random memory accesses happen quite often in our programs, but just as often,
    we have a large array that we need to process from the first element to the last.
    It is important to point out that *random* and *sequential* access here is determined
    by the order of memory addresses. There is a potential for misunderstanding: a
    list is a data structure that does not support random access (meaning you cannot
    jump into the middle of the list) and must be accessed sequentially, starting
    from the head element. However, traversing the list sequentially is likely to
    access the memory in random order if each list element was allocated separately
    and at different times. An array, on the other hand, is a random access data structure
    (meaning you can access any element without accessing the ones before it). However,
    reading the array from the beginning to the end accesses memory sequentially,
    in order of monotonically increasing addresses. In this entire chapter, unless
    otherwise stated, we are concerned with the order of accessing memory addresses
    when we talk about sequential or random access.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随机内存访问在我们的程序中经常发生，但同样频繁的是，我们有一个需要从第一个元素到最后一个元素处理的大数组。重要的是要指出，这里的“随机”和“顺序”访问是由内存地址的顺序决定的。有可能会产生误解：列表是一种不支持随机访问的数据结构（意味着你不能跳到列表的中间），必须按顺序访问，从头元素开始。然而，如果每个列表元素是分别分配并在不同时间分配的，那么按顺序遍历列表很可能以随机顺序访问内存。另一方面，数组是一种随机访问数据结构（意味着你可以访问任何元素而不必访问它之前的元素）。然而，从头到尾读取数组是按顺序访问内存，按照单调递增的地址顺序。在本章中，除非另有说明，我们在谈论顺序或随机访问时都关注访问内存地址的顺序。
- en: 'The performance of sequential memory accesses is quite different. Here are
    the results for sequential writes:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序内存访问的性能是完全不同的。以下是顺序写入的结果：
- en: '![Figure 4.6 – Write time for one array element versus array size, sequential
    access'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.6 - 一个数组元素的写入时间与数组大小的关系，顺序访问'
- en: '](img/Figure_4.6_B16229.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.6_B16229.jpg)'
- en: Figure 4.6 – Write time for one array element versus array size, sequential
    access
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 - 一个数组元素的写入时间与数组大小的关系，顺序访问
- en: 'The overall shape of the graphs is the same as before, but the differences
    are just as important as the similarities. The first difference we should note
    is the scale of the vertical axis: the time values are much smaller than the ones
    we saw in *Figure 4.5*. It takes only 2.5 nanoseconds to write a 256-bit value
    and just 0.8 nanoseconds for the 64-bit integer.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图的整体形状与之前相同，但差异和相似之处同样重要。我们应该注意的第一个差异是垂直轴的刻度：时间值比我们在*图4.5*中看到的要小得多。写入256位值只需要2.5纳秒，而64位整数只需要0.8纳秒。
- en: 'The second difference is that the curves for different word sizes are no longer
    the same. An important caveat here is that this result is highly hardware-dependent:
    on many systems, you will see the results more similar to the ones from the previous
    section. On the hardware I used, sequential write times for different word sizes
    are the same for the L1 cache but different for other caches and the main memory.
    Looking at the main memory values, we can observe that the time to write a 64-bit
    integer is not quite twice the time it takes to write a 32-bit integer, and for
    the larger sizes, the write times double every time the word size doubles. This
    means that the limit is not how many words per second we can write, but how many
    bytes per second: the speed in bytes per second will be the same for all word
    sizes (except the smallest one). This implies that the speed is now limited not
    by latency but by bandwidth: we''re pushing the bits into memory as fast as the
    bus can transmit them, and it doesn''t matter whether we group them into 64-bit
    chunks or 256-bit chunks that we call *words*, we''ve hit the bandwidth limit
    of the memory. Again, this outcome is much more hardware-dependent than any other
    observation we make in this chapter: on many machines, the memory is fast enough,
    and a single CPU cannot saturate its bandwidth.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个不同之处是不同字大小的曲线不再相同。这里有一个重要的警告：这个结果高度依赖于硬件：在许多系统上，你会看到与上一节类似的结果。在我使用的硬件上，不同字大小的顺序写入时间对于L1缓存是相同的，但对于其他缓存和主内存是不同的。观察主内存的数值，我们可以看到写入64位整数的时间并不是写入32位整数所需时间的两倍，对于更大的大小，写入时间每当字大小加倍时就会加倍。这意味着限制不是我们每秒可以写入多少个字，而是每秒可以写入多少个字节：所有字大小的速度（除了最小的那个）每秒的速度将是相同的。这意味着速度现在不再受延迟的限制，而是受带宽的限制：我们正在以总线能够传输的速度将位推入内存，无论我们是将它们分组成64位块还是256位块，我们称之为*字*，我们已经达到了内存的带宽限制。再次强调，这个结果比我们在本章中做出的任何其他观察都更依赖于硬件：在许多机器上，内存足够快，单个CPU无法饱和其带宽。
- en: The last observation we can make is that while the steps in the curves corresponding
    to the cache sizes are still visible, they are much less pronounced and not nearly
    as steep. We have the results, we have the observations. What does this all mean?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出的最后一个观察是，虽然与缓存大小对应的曲线上的步骤仍然可见，但它们不那么明显，也没有那么陡峭。我们有了结果，也有了观察。这一切意味着什么呢？
- en: Memory performance optimizations in hardware
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件中的内存性能优化
- en: 'The three observations, combined, point to some sort of latency-hiding technique
    employed by the hardware itself (other than changing the memory access order,
    we have not done anything to improve the performance of our code, so the gains
    are all thanks to the hardware doing something different). When accessing the
    main memory randomly, each access takes 7 nanoseconds on our machine. That''s
    how long it takes from the time the data at a particular address is requested
    until it''s delivered into a CPU register, and this delay is entirely determined
    by latency (it doesn''t matter how many bytes we requested, we have to wait for
    7 nanoseconds to get anything). When accessing memory sequentially, the hardware
    can begin transferring the next element of the array right away: the very first
    element still takes 7 nanoseconds to access, but after that, the hardware can
    start streaming the entire array from or to memory as fast as the CPU and the
    memory bus can handle it. The transfer of the second and the later elements of
    the array begins even before the CPU has issued the request for the data. Thus,
    the latency is no longer the limiting factor, the bandwidth is.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个观察结果合在一起，指向硬件本身采用了某种延迟隐藏技术（除了改变内存访问顺序，我们没有做任何事情来改善我们代码的性能，所以所有的收益都归功于硬件做了一些不同的事情）。在随机访问主内存时，每次访问在我们的机器上需要7纳秒。这是从请求特定地址的数据到它被传送到CPU寄存器所需的时间，这种延迟完全由延迟决定（无论我们请求了多少字节，我们都必须等待7纳秒才能得到任何东西）。在顺序访问内存时，硬件可以立即开始传输数组的下一个元素：第一个元素仍然需要7纳秒才能访问，但之后，硬件可以开始以CPU和内存总线可以处理的速度从内存中流式传输整个数组。数组的第二个和之后的元素的传输甚至在CPU发出数据请求之前就开始了。因此，延迟不再是限制因素，带宽是。
- en: Of course, this assumes that the hardware knows that we want to access the entire
    array sequentially and how large the array is. In reality, the hardware knows
    nothing of the sort, but, just like it did with the conditional instructions we
    studied in the last chapter, there are learning circuits in the memory system
    that make educated guesses. In our case, we have encountered the hardware technique
    known as the **prefetch**. Once the memory controller notices that the CPU has
    accessed several addresses sequentially, it makes the assumption that the pattern
    will continue and prepares for the access of the next memory location by transferring
    the data into the L1 cache (for reads) or vacating space in the L1 cache (for
    writes). Ideally, the prefetch technique would allow the CPU always to access
    memory at the L1 cache speeds because, by the time the CPU needs each array element,
    it is already in the L1 cache. Whether the reality matches this ideal case or
    not depends on how much work the CPU needs to do between accessing the adjacent
    elements. In our benchmark, the CPU does almost no work at all, and the prefetch
    falls behind. Even anticipating the linear sequential access, there is no way
    it can transfer the data between the main memory and the L1 cache fast enough.
    However, the prefetch is very effective at hiding the latency of memory access.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这假设硬件知道我们要顺序访问整个数组以及数组的大小。实际上，硬件并不知道这些，但就像我们在上一章中学习的条件指令一样，内存系统中有学习电路来做出合理的猜测。在我们的情况下，我们遇到了被称为**预取**的硬件技术。一旦内存控制器注意到CPU连续访问了几个地址，它就假设模式将继续，并准备访问下一个内存位置，将数据传输到L1缓存（对于读取）或为写入在L1缓存中腾出空间。理想情况下，预取技术将允许CPU始终以L1缓存速度访问内存，因为在CPU需要每个数组元素时，它已经在L1缓存中。现实是否符合这种理想情况取决于CPU在访问相邻元素之间需要多少工作。在我们的基准测试中，CPU几乎没有做任何工作，预取落后了。即使预期线性顺序访问，它也无法以足够快的速度在主内存和L1缓存之间传输数据。然而，预取非常有效地隐藏了内存访问的延迟。
- en: The prefetch is not based on any prescience or prior knowledge about how the
    memory is going to be accessed (there are some platform-specific system calls
    that allow the program to notify the hardware that a range of memory is about
    to be accessed sequentially, but they are not portable and, in practice, rarely
    useful). Instead, the prefetch tries to detect a pattern in accessing memory.
    The effectiveness of the prefetch is, thus, determined by how effectively it can
    determine the pattern and guess the location of the next access.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 预取不是基于对内存访问将如何进行的预见或先验知识（有一些特定于平台的系统调用允许程序通知硬件即将按顺序访问一段内存，但它们不具有可移植性，在实践中很少有用）。相反，预取试图检测内存访问中的模式。因此，预取的有效性取决于它能够多么有效地确定模式并猜测下一个访问的位置。
- en: There is a lot of information, much of it is outdated, about what the limitations
    of the prefetch pattern detection are. For example, in the older literature, you
    can read that accessing memory in the *forward* order (for an array `a`, going
    from `a[0]` to `a[N-1]`) is more efficient than going *backward*. This is no longer
    true for any modern CPU and hasn't been true for years. This book risks falling
    into the same trap if I start describing exactly which patterns are and aren't
    efficient in terms of prefetch. Ultimately, if your algorithm requires a particular
    memory access pattern and you want to find out whether your prefetch can handle
    it, the most reliable way is to measure it using the benchmark code similar to
    what we used in this chapter for random memory access.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多信息，其中很多是过时的，关于预取模式检测的限制。例如，在旧的文献中，你可以读到，按*正向*顺序访问内存（对于数组`a`，从`a[0]`到`a[N-1]`）比*反向*访问更有效。这对于任何现代CPU来说都不再成立，也已经多年如此。如果我开始准确描述哪些模式在预取方面是有效的，哪些不是有效的，这本书可能会陷入同样的陷阱。最终，如果你的算法需要特定的内存访问模式，并且你想找出你的预取是否能够处理它，最可靠的方法是使用类似我们在本章中用于随机内存访问的基准代码来进行测量。
- en: In general terms, I can tell you that the prefetch is equally effective for
    accessing memory in increasing and decreasing orders. However, reversing the direction
    will incur some penalty until the prefetch adjusts to the new pattern. Accessing
    memory with stride, such as accessing every fourth element in an array, will be
    detected and predicted just as efficiently as a dense sequential access. The prefetch
    can detect multiple concurrent strides (that is, accessing every third and every
    seventh element), but here we're getting into the territory where you have to
    gather your own data as the hardware capabilities change from one processor to
    another.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我可以告诉你，预取对于按递增和递减顺序访问内存同样有效。然而，改变方向会导致一些惩罚，直到预取适应新的模式。使用步长访问内存，比如在数组中访问每四个元素，将被检测和预测，就像密集的顺序访问一样有效。预取可以检测多个并发步长（即访问每三个和每七个元素），但在这里，我们进入了一个领域，你必须收集自己的数据，因为硬件能力从一个处理器到另一个处理器会发生变化。
- en: 'Another performance optimization technique that the hardware employs very successfully
    is the familiar one: **pipelining** or **hardware loop unrolling**. We have already
    seen it in the last chapter, where it was used to hide the delay caused by the
    conditional instructions. Similarly, pipelining is used to hide the latency of
    memory accesses. Consider this loop:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件采用的另一种性能优化技术是非常成功的**流水线**或**硬件循环展开**。我们已经在上一章中看到了它的应用，用于隐藏条件指令造成的延迟。同样，流水线也用于隐藏内存访问的延迟。考虑这个循环：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'On every iteration, we read a value `a[i]` from the array, do some computations,
    and store the result, `b[i]`, in another array. Since both reading and writing
    takes time, we can expect the timeline of the execution of the loop to look like
    this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们从数组中读取值`a[i]`，进行一些计算，并将结果`b[i]`存储在另一个数组中。由于读取和写入都需要时间，我们可以期望循环执行的时间线看起来像这样：
- en: '![Figure 4.7 – Timeline of a non-pipelined loop'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.7 - 非流水线循环的时间线'
- en: '](img/Figure_4.7_B16229.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.7_B16229.jpg)'
- en: Figure 4.7 – Timeline of a non-pipelined loop
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 - 非流水线循环的时间线
- en: 'This sequence of operations would leave the CPU waiting for memory operations
    to complete most of the time. Instead, the hardware will read ahead into the instruction
    stream and overlay the instruction sequences that do not depend on each other:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列操作会让CPU大部分时间都在等待内存操作完成。相反，硬件将预先读取指令流，并叠加不相互依赖的指令序列：
- en: '![Figure 4.8 – Timeline of a pipelined (unrolled) loop'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.8 - 流水线（展开）循环的时间线'
- en: '](img/Figure_4.8_B16229.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.8_B16229.jpg)'
- en: Figure 4.8 – Timeline of a pipelined (unrolled) loop
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 - 流水线（展开）循环的时间线
- en: The load of the second array element can start as soon as the first one is read,
    assuming there are enough registers. For simplicity, we are assuming that the
    CPU cannot load two values at a time; most real CPUs can do more than one memory
    access at the same time, which just means that the pipeline can be even wider,
    but it doesn't change the main idea. The second set of computations begin as soon
    as the input value is available. After the first few steps, the pipeline is loaded,
    and the CPU spends most of the time computing (if the computing steps from different
    iterations overlap, the CPU may even be executing several iterations at once,
    provided it has enough compute units to do so).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个数组元素的加载可以在第一个元素被读取后立即开始，假设有足够的寄存器。为简单起见，我们假设CPU一次只能加载两个值；大多数真实的CPU可以同时进行多次内存访问，这意味着流水线可以更宽，但这并不改变主要思想。第二组计算在输入值可用后立即开始。经过前几步后，流水线被加载，CPU大部分时间都在计算（如果不同迭代的计算步骤重叠，CPU甚至可以同时执行多个迭代，前提是它有足够的计算单元来这样做）。
- en: The pipelining can hide the latency of memory accesses, but, obviously, there
    is a limit. If it takes 7 nanoseconds to read one value and we need to read a
    million of them, it is going to take 7 milliseconds at best, there is no getting
    around that (again, assuming the CPU can read only one value at a time). The pipelining
    can help us by overlaying the computations with the memory operations, so, in
    the ideal case, all the computing is done during these 7 milliseconds. The prefetch
    can start reading the next value before we need it and thus cut down the average
    read time, but only if it guesses correctly what that value is. Either way, the
    measurements done in this chapter show the best-case scenarios for accessing memory
    in different ways.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线可以隐藏内存访问的延迟，但显然是有限制的。如果读取一个值需要7纳秒，而我们需要读取一百万个值，那么最好情况下需要7毫秒，这是无法避免的（再次假设CPU一次只能读取一个值）。流水线可以通过将计算与内存操作叠加在一起来帮助我们，在理想情况下，所有计算都在这7毫秒内完成。预取可以在我们需要之前开始读取下一个值，从而缩短平均读取时间，但前提是它能正确猜测出该值。无论如何，本章中进行的测量展示了以不同方式访问内存的最佳情况。
- en: 'In terms of measuring memory speed and presenting the results, we have covered
    the basics and learned about the general properties of the memory system. Any
    more detailed or specific measurements are left as an exercise for the reader,
    and you should be well-equipped to gather the data you need to make informed decisions
    about the performance of your particular applications. We now turn our attention
    to the next step: we know how the memory works and what performance we can expect
    from it, but what can we do to improve the performance of a concrete program?'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在测量内存速度和呈现结果方面，我们已经涵盖了基础知识，并了解了内存系统的一般特性。任何更详细或具体的测量都留给读者自行练习，你应该有足够的能力收集所需的数据，以便对你特定应用程序的性能做出明智的决策。现在我们转向下一步：我们知道内存是如何工作的，以及我们可以期望从中获得的性能，但我们可以做些什么来改善具体程序的性能呢？
- en: Optimizing memory performance
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化内存性能
- en: 'The first reaction many programmers have when they learn the material from
    the previous section is often this: *"Thanks, I understand now why my program
    is slow, but I have to process the amount of data I have, not the ideal 32 KB,
    and the algorithm is what it is, including the complex data access pattern, so
    there is nothing I can do about it."* This chapter would not be worth much if
    we didn''t learn how to get better memory performance for the problems we need
    to solve. In this section, we will learn the techniques that can be used to improve
    memory performance.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当许多程序员学习了上一节的材料后，他们通常的第一反应是：“谢谢，我现在明白为什么我的程序慢了，但我必须处理我拥有的数据量，而不是理想的32KB，算法也是固定的，包括复杂的数据访问模式，所以我无能为力。”如果我们不学会如何为我们需要解决的问题获得更好的内存性能，那么本章就没有多大价值。在本节中，我们将学习可以用来改善内存性能的技术。
- en: Memory-efficient data structures
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存高效的数据结构
- en: 'The choice of data structures, or, more generally, data organization, is usually
    the most important decision the programmer makes as far as memory performance
    is concerned. It is important to understand what you can and cannot do: the memory
    performance shown in *Figure 4.5* and *Figure 4.6* is really all there is, and
    you can''t get around it (strictly speaking, this is only 99% true; there are
    some exotic memory access techniques that, rarely, can exceed the limits shown
    in these figures). But, you can choose where on these graphs is the point corresponding
    to your program. Let us consider first a simple example: we have 1 M 64-bit integers
    that we need to store and process in order. We can store these values in an array;
    the size of the array will be 8 MB, and, according to our measurements, the access
    time is about 0.6 nanoseconds per value, as shown in *Figure 4.6*.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 数据结构的选择，或者更一般地说，数据组织，通常是程序员在内存性能方面做出的最重要决定。重要的是要了解你能做什么，不能做什么：*图4.5*和*图4.6*中显示的内存性能确实就是全部，你无法绕过它（严格来说，这只有99%的真实性；有一些少见的异类内存访问技术可以超出这些图表所显示的限制）。但是，你可以选择在这些图表上的哪个点对应于你的程序。首先让我们考虑一个简单的例子：我们有1百万个64位整数，我们需要按顺序存储和处理。我们可以将这些值存储在一个数组中；数组的大小将为8
    MB，并且根据我们的测量，访问时间约为0.6纳秒/值，如*图4.6*所示。
- en: '![Figure 4.9 – Write time for one array (A) versus list (L) element'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.9 - 一个数组（A）与列表（L）元素的写入时间'
- en: '](img/Figure_4.9_B16229.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.9_B16229.jpg)'
- en: Figure 4.9 – Write time for one array (A) versus list (L) element
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 - 一个数组（A）与列表（L）元素的写入时间
- en: Alternatively, we could use a list to store the same numbers. The `std::list`
    is a collection of nodes, and each node has the value and two pointers to the
    next and the previous node. The entire list, therefore, uses 24 MB of memory.
    Furthermore, each node is allocated through a separate call to `operator new`,
    so different nodes are likely to be at very different addresses, especially if
    the program is doing other memory allocations and deallocations at the same time.
    There isn't going to be any pattern in the addresses we need to access when traversing
    the list, so to find the performance of the list, all we need to do is find the
    point corresponding to the 24 MB memory range on the curve for random memory accesses.
    This gives us just over 5 nanoseconds per value or almost an order of magnitude
    slower than accessing the same data in an array.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用列表来存储相同的数字。`std::list`是一个节点集合，每个节点都有值和指向下一个和上一个节点的两个指针。因此，整个列表使用了24
    MB的内存。此外，每个节点都是通过单独调用`operator new`来分配的，因此不同的节点可能位于非常不同的地址，特别是如果程序同时进行其他内存分配和释放。在遍历列表时，我们需要访问的地址不会有任何模式，因此要找到列表的性能，我们只需要在曲线上找到对应于24
    MB内存范围的点，这给出了每个值超过5纳秒，几乎比在数组中访问相同数据慢一个数量级。
- en: 'Those of you who, at this point, demanded proof, have learned something valuable
    from the previous chapter. We can easily construct a micro-benchmark to compare
    writing data into a list and a vector of the same size. Here is the benchmark
    for the vector:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上要求证明的人，从上一章中学到了宝贵的东西。我们可以轻松地构建一个微基准测试，比较将数据写入列表和相同大小的向量。这是向量的基准测试：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Change `std::vector` to `std::list` to create a list benchmark. Note that the
    meaning of the size has changed, compared to the earlier benchmarks: now it is
    the number of the elements in the container, so the memory size will depend on
    the element type and the container itself, just as was shown in *Figure 4.6*.
    The results, for 1 M elements, are exactly as promised:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 将`std::vector`更改为`std::list`以创建一个列表基准测试。请注意，与先前的基准测试相比，大小的含义已经改变：现在它是容器中元素的数量，因此内存大小将取决于元素类型和容器本身，就像*图4.6*中所示的那样。对于1百万个元素，结果正如所承诺的那样：
- en: '![Figure 4.10 – List versus vector benchmark'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.10 - 列表与向量基准测试'
- en: '](img/Figure_4.10_B16229.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.10_B16229.jpg)'
- en: Figure 4.10 – List versus vector benchmark
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 - 列表与向量基准测试
- en: 'Why would anyone choose the list over the array (or `std::vector`)? The most
    common reason is that at the time of creation, we did not know how much data we
    were going to have, and growing a vector is extremely inefficient because of the
    copying involved. There are several ways around this problem. Sometimes it is
    possible to precompute the final size of the data relatively inexpensively. For
    example, it may cost us a single scan of the input data to determine how much
    space to allocate for the results. If the inputs are efficiently organized, it
    may be worth it to do two passes over the inputs: first, to count, and second,
    to process.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么有人会选择链表而不是数组（或`std::vector`）？最常见的原因是，在创建时，我们不知道将要有多少数据，而且由于涉及到复制，增长向量是非常低效的。有几种解决这个问题的方法。有时可以相对廉价地预先计算数据的最终大小。例如，我们可能需要扫描一次输入数据来确定为结果分配多少空间。如果输入数据组织得很有效，可能值得对输入进行两次遍历：首先是计数，其次是处理。
- en: 'If it is not possible to know the final data size in advance, we may need a
    smarter data structure that combines the memory efficiency of a vector with the
    resizing efficiency of a list. This can be achieved using a block-allocated array:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不可能预先知道最终数据大小，我们可能需要一个更智能的数据结构，它结合了向量的内存效率和列表的调整效率。这可以通过使用块分配的数组来实现：
- en: '![Figure 4.11 – A block-allocated array (deque) can be grown in place'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.11 - 块分配的数组（deque）可以就地增长'
- en: '](img/Figure_4.11_B16229.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.11_B16229.jpg)'
- en: Figure 4.11 – A block-allocated array (deque) can be grown in place
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 - 块分配的数组（deque）可以就地增长
- en: 'This data structure allocates memory in blocks of a fixed amount, usually small
    enough that they fit into the L1 cache (anywhere between 2 KB and 16 KB is commonly
    used). Each block is used as an array, so, within each block, the elements are
    accessed sequentially. The blocks themselves are organized in a list. If we need
    to grow this data structure, we just allocate another block and add it to the
    list. Accessing the first element of each block is likely to incur a cache miss,
    but the rest of the elements in the block can be accessed efficiently once the
    prefetch detects the pattern of sequential access. Amortized over the number of
    elements in each block, the cost of the random access can be made very small,
    and the resulting data structure can perform almost identically to the array or
    vector. In STL, we have such a data structure: `std::deque` (unfortunately, the
    implementation in most STL versions is not particularly efficient, and sequential
    accesses to the deque are usually somewhat slower than to the vector of the same
    size).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据结构以固定数量的块分配内存，通常足够小，可以适应L1缓存（通常使用2 KB到16 KB之间）。每个块都被用作数组，因此在每个块内，元素是按顺序访问的。块本身是以列表的形式组织的。如果需要扩展这种数据结构，只需分配另一个块并将其添加到列表中。访问每个块的第一个元素可能会导致缓存未命中，但一旦预取检测到顺序访问的模式，块中的其余元素可以被高效地访问。在每个块中的元素数量上摊销，随机访问的成本可以变得非常小，由此产生的数据结构几乎可以表现得与数组或向量相同。在STL中，我们有这样的数据结构：`std::deque`（不幸的是，大多数STL版本中的实现并不特别高效，对deque的顺序访问通常比相同大小的向量要慢一些）。
- en: Yet another reason to prefer a list over an array, monolithic or block-allocated,
    is that the list allows fast insertions at any point, not just at the ends. If
    you need this, then you have to use a list or another node-allocated container.
    In such cases, often the best solution is to not attempt to select a single data
    structure that works for all requirements but to migrate the data from one data
    structure to another. For example, if we want to use the list to store data elements,
    one at a time while maintaining sorted order, one question to ask is, do we need
    the order to be sorted at all times, only after all elements are inserted, or
    a few times in the middle of the construction process but not all the time?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个偏好列表而不是数组（单块或分配的）的原因是列表允许在任何位置快速插入，而不仅仅是在末尾。如果需要这样做，那么必须使用列表或另一个节点分配的容器。在这种情况下，通常最好的解决方案是不要尝试选择适用于所有要求的单个数据结构，而是将数据从一个数据结构迁移到另一个数据结构。例如，如果我们想使用列表存储数据元素，一次一个，同时保持排序顺序，一个问题要问的是，我们是否需要顺序始终保持排序，只在插入所有元素后，或者在构建过程中的某些时候但不是一直？
- en: If there is a point in the algorithm where the data access patterns change,
    it is often advantageous to change the data structure at that point, even at the
    cost of some copying of memory. For example, we may construct a list and, after
    the last element is added, copy it into an array for faster sequential access
    (assuming we won't need to add any more elements). If we can be sure that some
    part of the data is complete, we may convert that part to an array, perhaps one
    or more blocks in a block-allocated array, and leave the still mutable data in
    a list or a tree data structure. On the other hand, if we rarely need to process
    the data in the sorted order, or need to process it in multiple orders, then separating
    the order from the storage is often the best solution. The data is stored in a
    vector or a deque, and the order is imposed on top of it by an array of pointers
    sorted in the desired order. Since all ordered data accesses are now indirect
    (through an intermediate pointer), this is efficient only if such accesses are
    rare, and most of the time, we can process the data in the order in which it's
    stored in the array.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果算法中存在数据访问模式变化的点，通常有利于在该点更改数据结构，即使需要复制一些内存。例如，我们可以构建一个列表，并在添加最后一个元素后，将其复制到数组中以实现更快的顺序访问（假设我们不需要再添加任何元素）。如果我们可以确定某部分数据是完整的，我们可以将该部分转换为数组，可能是块分配数组中的一个或多个块，并将仍然可变的数据留在列表或树数据结构中。另一方面，如果我们很少需要按排序顺序处理数据，或者需要以多种顺序处理数据，那么将顺序与存储分离通常是最佳解决方案。数据存储在向量或双端队列中，并且顺序是通过按所需顺序排序的指针数组施加的。由于所有有序数据访问现在是间接的（通过中间指针），只有在这种访问很少的情况下才是有效的，大部分时间，我们可以按照数组中存储的顺序处理数据。
- en: 'The bottom line is, if we access some data a lot, we should choose a data structure
    that makes that particular access pattern optimal. If the access pattern changes
    in time, the data structure should change as well. On the other hand, if we don''t
    spend much time accessing the data, the overhead of converting from one arrangement
    of the data to another likely cannot be justified. However, in this case, inefficient
    data access should not be a problem in the first place. This brings us to the
    next question: how do we figure out which data is accessed inefficiently and,
    more generally, which data is expensive to access?'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是，如果我们经常访问某些数据，我们应该选择使该特定访问模式最佳的数据结构。如果访问模式随时间变化，数据结构也应该随之变化。另一方面，如果我们不花太多时间访问数据，那么从一种数据排列转换到另一种排列的开销可能无法证明是合理的。然而，在这种情况下，低效的数据访问本来就不应该是一个问题。这带我们来到下一个问题：我们如何找出哪些数据访问效率低，更一般地说，哪些数据访问成本高？
- en: Profiling memory performance
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能分析内存
- en: 'Often, the efficiency of a particular data structure or data organization is
    fairly obvious. For example, if we have a class containing an array or a vector,
    and the interface of this class allows only one mode of access to the data, with
    sequential iteration from the beginning to the end (forward iterator, in the STL
    language), then we can be quite certain that the data is accessed as efficiently
    as possible, at the memory level anyway. We can''t be sure about the efficiency
    of the algorithm: for example, a linear search of a particular element in an array
    is very inefficient (each memory read is efficient, of course, but there are a
    lot of them; we know better ways to organize data for searching).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，特定数据结构或数据组织的效率是相当明显的。例如，如果我们有一个包含数组或向量的类，并且这个类的接口只允许一种数据访问方式，即从开始到结束的顺序迭代（在STL语言中为前向迭代器），那么我们可以相当肯定地说，数据在内存级别上被访问得尽可能高效。我们无法确定算法的效率：例如，在数组中进行特定元素的线性搜索是非常低效的（每次内存读取当然是高效的，但读取次数很多；我们知道更好的数据组织方式来进行搜索）。
- en: 'Simply knowing which data structures are memory-efficient is not enough: we
    also need to know how much time the program spends working on a particular set
    of data. Sometimes, this is self-evident, especially with good encapsulation.
    If we have a function that, according to the profile or the timing report, takes
    a lot of time, and the code inside the function is not particularly heavy on computations
    but moves a lot of data, the odds are good that making access to this data more
    efficient will improve the overall performance.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅知道哪些数据结构在内存上是高效的是不够的：我们还需要知道程序在特定数据集上花费了多少时间。有时，这是不言自明的，尤其是在良好的封装下。如果我们有一个函数，在概况或时间报告中花费了很多时间，而函数内的代码并不特别繁重，但移动了大量数据，那么提高对这些数据的访问效率很可能会改善整体性能。
- en: 'Unfortunately, that is the easy case, and so it gets optimized first. Then
    we get to the point where no single function or code fragment stands out in terms
    of execution time, but the program is still inefficient. When you have no *hot*
    code, very often you have *hot* data: one or more data structures that are accessed
    throughout the program; the cumulative time spent on this data is large, but it''s
    not localized to any function or loop. Conventional profiling does not help us:
    it will show that the runtime is spread evenly across the entire program, and
    optimizing any one fragment of code will yield very little improvement. What we
    need is a way to find the data that is accessed inefficiently throughout the program,
    and it adds up.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这是比较容易的情况，因此首先进行了优化。然后我们到了一个没有单个函数或代码片段在执行时间上突出的程度，但程序仍然效率低下的地步。当你没有*热点*代码时，很多时候你有*热点*数据：一个或多个数据结构在整个程序中被访问；在这些数据上花费的累计时间很长，但没有局限在任何函数或循环中。传统的分析无法帮助我们：它会显示运行时间均匀分布在整个程序中，并且优化任何一个代码片段都会带来很少的改进。我们需要的是一种方法来找到整个程序中访问效率低下的数据，并将其累积起来。
- en: 'It is very hard to collect this information with just time-measuring tools.
    However, it can be collected fairly easily using a profiler that utilizes the
    hardware event counters. Most CPUs can count memory accesses and, more specifically,
    cache hits and misses. In this chapter, we again use the `perf` profiler; with
    it, we can measure how effective the L1 cache is used with this command:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅使用时间测量工具很难收集这些信息。然而，使用硬件事件计数器的分析器可以相当容易地收集这些信息。大多数CPU可以计算内存访问，更具体地说是缓存命中和未命中。在本章中，我们再次使用`perf`分析器；通过它，我们可以使用以下命令来测量L1缓存的使用效果：
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The cache measurement counters are not part of the default counter set and must
    be specified explicitly. The exact set of available counters varies from one CPU
    to another but can always be viewed by running the `perf list` command. In our
    example, we measure L1 cache misses when reading data. The term **dcache** stands
    for **data cache** (pronounced *dee-cache*); the CPUs also have a separate **instruction
    cache** or **icache** (pronounced *ay-cache*) that is used to load the instructions
    from memory.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存测量计数器不是默认计数器集的一部分，必须显式指定。可用计数器的确切集合因CPU而异，但始终可以通过运行`perf list`命令查看。在我们的示例中，我们在读取数据时测量L1缓存未命中。术语**dcache**代表**数据缓存**（发音为*dee-cache*）；CPU还有一个单独的**指令缓存**或**icache**（发音为*ay-cache*），用于从内存中加载指令。
- en: 'We can use this command line to profile our memory benchmarks for reading the
    memory at random addresses. When the memory range is small, say, 16 KB, the entire
    array fits into the L1 cache, and there are almost no cache misses:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个命令行来对我们的内存基准进行分析，以便随机地址读取内存。当内存范围较小，比如16KB时，整个数组可以适应L1缓存，几乎没有缓存未命中：
- en: '![Figure 4.12 – Profile of a program with good use of the L1 cache'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.12 - 使用L1缓存良好的程序概况'
- en: '](img/Image86899.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Image86899.jpg)'
- en: Figure 4.12 – Profile of a program with good use of the L1 cache
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 - 使用L1缓存良好的程序概况
- en: 'Increasing the memory size to 128 MB means that cache misses are very frequent:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 将内存大小增加到128MB意味着缓存未命中非常频繁：
- en: '![Figure 4.13 – Profile of a program with poor use of the L1 cache'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.13 - 使用L1缓存不佳的程序概况'
- en: '](img/Image86907.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Image86907.jpg)'
- en: Figure 4.13 – Profile of a program with poor use of the L1 cache
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 - 使用L1缓存不佳的程序概况
- en: 'Note that `perf stat` collects the overall values for the entire program, where
    some memory accesses are cache-efficient, and others aren''t. Once we know that
    somebody, somewhere, is handling memory accesses badly, we can get a detailed
    profile using `perf record` and `perf report`, as was shown in [*Chapter 2*](B16229_02_Epub_AM.xhtml#_idTextAnchor026),
    *Performance Measurements* (we used a different counter there, but the process
    is the same for any counter we choose to collect). Of course, if our original
    timing profile failed to detect any hot code, the cache profile will show the
    same. There will be many locations in the code where the fraction of cache misses
    is large. Each location contributes only a small amount of time to the overall
    execution time, but it adds up. It is now up to us to notice that many of these
    code locations have one thing in common: the memory they operate on. For example,
    if we see that there are dozens of different functions that, between them all,
    account for the 15% cache miss rate, but they all operate on the same list, then
    the list is the problematic data structure, and we have to organize our data in
    some other way.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`perf stat`收集整个程序的总体值，其中一些内存访问是高效的，而另一些则不是。一旦我们知道某个地方的某人处理内存访问不当，我们就可以使用`perf
    record`和`perf report`来获取详细的概要，就像[*第2章*](B16229_02_Epub_AM.xhtml#_idTextAnchor026)中所展示的那样，*性能测量*（我们在那里使用了不同的计数器，但对于我们选择收集的任何计数器来说，过程都是相同的）。当然，如果我们最初的时间概要未能检测到任何热点代码，那么缓存概要也将显示相同的情况。代码中将有许多位置，其中缓存未命中的比例很高。每个位置对总体执行时间只有很小的贡献，但它们会累积起来。现在轮到我们注意到这些代码位置中的许多位置有一个共同点：它们操作的内存。例如，如果我们看到有几十个不同的函数，它们共同占据了15%的缓存未命中率，但它们都操作同一个列表，那么列表就是有问题的数据结构，我们必须以其他方式组织我们的数据。
- en: 'We have now learned how to detect and identify the data structures whose inefficient
    memory access patterns negatively impact performance and what are some of the
    alternatives. Unfortunately, the alternative data structures usually don''t have
    the same features or performance: a list cannot be replaced with a vector if elements
    must be inserted at arbitrary locations throughout the life of the data structure.
    Often, it is not the data structure but the algorithm itself that calls for inefficient
    memory accesses. In such cases, we may have to change the algorithms.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经学会了如何检测和识别那些低效的内存访问模式对性能产生负面影响的数据结构，以及一些替代方案。不幸的是，替代的数据结构通常没有相同的特性或性能：如果元素必须在数据结构的生命周期中的任意位置插入，那么列表就不能用向量来替换。通常情况下，不是数据结构本身，而是算法本身需要低效的内存访问。在这种情况下，我们可能需要改变算法。
- en: Optimizing algorithms for memory performance
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化内存性能的算法
- en: 'The memory performance of algorithms is an often overlooked subject. Algorithms
    are most commonly chosen for their **algorithmic performance** or the number of
    operations or steps they perform. Memory optimizations often call for a counter-intuitive
    choice: do more work, even do unnecessary work, to improve memory performance.
    The game here is to trade some computing for faster memory operations. The memory
    operations are slow, so our *budget* for extra work is quite large.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的内存性能经常被忽视。算法通常是根据它们的**算法性能**或执行的操作或步骤数量来选择的。内存优化通常需要做出违反直觉的选择：做更多的工作，甚至做一些不必要的工作，以改善内存性能。这里的关键是要用一些计算来换取更快的内存操作。内存操作很慢，所以我们用于额外工作的*预算*相当大。
- en: One way to use memory faster is to use less of it. This approach often results
    in recomputing some values that could have been stored and retrieved from memory.
    In the worst-case scenario, if this retrieval results in random accesses, reading
    each value will take several nanoseconds (7 nanoseconds in our measurements).
    If recomputing the value takes less than that, and 7 nanoseconds is a fairly long
    time when converted to the number of operations the CPU can do, then we are better
    off not storing the values. This is the conventional tradeoff of space versus
    memory.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 更快地使用内存的一种方法是使用更少的内存。这种方法通常会导致重新计算一些本来可以存储和从内存中检索的值。在最坏的情况下，如果这种检索导致随机访问，那么读取每个值将需要几个纳秒（在我们的测量中为7纳秒）。如果重新计算该值所需的时间少于这个时间，而且当转换为CPU可以执行的操作数量时，7纳秒是相当长的时间，那么我们最好不要存储这些值。这是空间与内存的传统权衡。
- en: 'There is an interesting variant of this optimization: instead of simply using
    less memory, we try to use less memory at any given time. The idea here is to
    try to fit the current working data set into one of the caches, say, the L2 cache,
    and do as much work on it as possible before moving to the next section of the
    data. Loading a new data set into the cache incurs a cache miss on every memory
    address, by definition. But it is better to accept that cache miss once and then
    operate on the data efficiently for some time, rather than process all the data
    at once and risk a cache miss every time we need this data element.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优化的一个有趣变体是：我们不仅仅是使用更少的内存，而是尝试在任何给定时间使用更少的内存。这里的想法是尝试将当前的工作数据集适应到其中一个缓存中，比如L2缓存，并在移动到数据的下一部分之前尽可能多地对其进行操作。将新的数据集加载到缓存中会导致每个内存地址都发生缓存未命中，根据定义。但是最好是接受那一次缓存未命中，然后在一段时间内有效地操作数据，而不是一次处理所有数据，然后冒险每次需要这个数据元素时都发生缓存未命中。
- en: 'In this chapter, I will show you a more interesting technique, where we do
    more memory accesses to save a few other memory accesses. The tradeoff here is
    different: we want to reduce the number of slow, random accesses, but we pay for
    it in an increased number of fast, sequential accesses. Since sequential memory
    streaming is about an order of magnitude faster than random access, we again have
    a sizeable *budget* to pay for the extra work we have to do to reduce the slow
    memory accesses.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将向您展示一种更有趣的技术，我们通过更多的内存访问来节省一些其他内存访问。这里的权衡是不同的：我们希望减少慢速的随机访问，但我们要付出的代价是增加快速的顺序访问。由于顺序内存流大约比随机访问快一个数量级，我们再次有一个可观的*预算*来支付我们必须做的额外工作，以减少慢速内存访问。
- en: 'The demonstration requires a more elaborate example. Let us say that we have
    a collection of data records, such as strings, and the program needs to apply
    a set of changes to some of these records. Then we get another set of changes,
    and so on. Each set will have changes to some records, while other records remain
    unchanged. The changes do, in general, change the size of the record as well as
    its content. The subset of records that are changed in each set is completely
    random and unpredictable. Here is a diagram showing just that:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 演示需要一个更复杂的例子。假设我们有一组数据记录，比如字符串，程序需要对其中一些记录应用一组变更。然后我们得到另一组变更，依此类推。每个集合都会对一些记录进行更改，而其他记录保持不变。这些变更通常会改变记录的大小以及内容。每个集合中被更改的记录子集是完全随机和不可预测的。下面是一个显示这一点的图表：
- en: '![Figure 4.14 – The record editing problem. In each change set, the records
    marked by * are edited,'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.14 - 记录编辑问题。在每个变更集中，用*标记的记录被编辑，其余保持不变'
- en: the rest remains unchanged
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其余保持不变
- en: '](img/Figure_4.14_B16229.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.14_B16229.jpg)'
- en: Figure 4.14 – The record editing problem. In each change set, the records marked
    by * are edited, the rest remains unchanged
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 - 记录编辑问题。在每个变更集中，用*标记的记录被编辑，其余保持不变
- en: 'The most straightforward way to solve this problem is to store records in their
    own memory allocation and organize them in some data structure that allows each
    record to be replaced by a new one (the old record is deallocated, since the new
    one is, generally, of a different size). The data structure can be a tree (set
    in C++) or a list. To make the example more concrete, let us use strings for records.
    We also must be more specific about the way the change set is specified. Let us
    say that it does not point to a specific record that needs to be changed; instead,
    for any record, we can say whether it needs to be changed or not. The simplest
    example of such a change set for strings is a set of find and replace patterns.
    Now we can sketch our implementation:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题最直接的方法是将记录存储在它们自己的内存分配中，并将它们组织在一些数据结构中，允许每个记录被新记录替换（旧记录被释放，因为新记录通常大小不同）。数据结构可以是树（在C++中设置）或列表。为了使示例更具体，让我们使用字符串作为记录。我们还必须更具体地说明变更集的指定方式。让我们说它不指向需要更改的特定记录；相反，对于任何记录，我们可以说它是否需要更改。这样的字符串变更集的最简单示例是一组查找和替换模式。现在我们可以勾画出我们的实现：
- en: '[PRE15]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In every change set, we iterate over the entire collection of records, determine
    if the record needs to be changed, and, if needed, do so (the change set is hidden
    inside the functions `must_change()` and `change()`). The code shows just one
    change set, so we run this loop as many times as needed.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个变更集中，我们遍历整个记录集合，确定记录是否需要更改，如果需要，就这样做（变更集隐藏在函数`must_change()`和`change()`中）。代码只显示了一个变更集，所以我们会根据需要运行这个循环多次。
- en: The weakness of this algorithm is that we are using a list, and, to make it
    even worse, we keep moving the strings in memory. Every access to a new string
    is a cache miss. Now, if the strings are very long, then the initial cache miss
    doesn't matter, and the rest of the string is read using fast sequential access.
    The result is similar to the block-allocated array we saw earlier, and the memory
    performance is fine. But if the strings are short, the entire string may well
    be read in a single load operation, and every load is done at a random address.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这种算法的弱点在于我们使用了一个列表，更糟糕的是，我们不断地在内存中移动字符串。对新字符串的每次访问都会导致缓存未命中。现在，如果字符串非常长，那么初始的缓存未命中并不重要，剩下的字符串可以使用快速的顺序访问来读取。结果类似于我们之前看到的块分配数组，内存性能良好。但是如果字符串很短，整个字符串可能会在单个加载操作中被读取，而每次加载都是在随机地址上进行的。
- en: 'Our entire algorithm does nothing but loads and stores at random addresses.
    As we have seen, this is pretty much the worst way to access memory. But what
    else can we do? We can''t store the strings in one huge array: if one string in
    the middle of the array needs to grow, where would the memory come from? Right
    after that string is the next string, so there is no room to grow.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的整个算法只是在随机地址上进行加载和存储。正如我们所见，这几乎是访问内存的最糟糕方式。但是我们还能做什么呢？我们不能将字符串存储在一个巨大的数组中：如果数组中间的一个字符串需要增长，那么内存从哪里来呢？就在那个字符串之后是下一个字符串，所以没有空间可以增长。
- en: 'Coming up with an alternative requires a paradigm shift. The algorithm that
    performs the required operations literally as specified also imposes restrictions
    on the memory organization: changing records requires moving them in memory, and,
    as long as we want to be able to change any one record without affecting anything
    else, we cannot avoid random distribution of records in memory. We have to come
    at the problem sideways and start with the restrictions. We really want to access
    all records sequentially. What can we do under this constraint? We can read all
    records very fast. We can decide whether the record must be changed; this step
    is the same as before. But what do we do if the record must grow? We have to move
    it somewhere else, there is no room to grow in place. But we agreed that the records
    would remain allocated in sequence, one after the other. Then the previous record
    and the next record have to be moved too, so they remain stored before and after
    our new record. This is the key to the alternative algorithm: all records are
    moved with each change set, whether they are changed or not. Now we can store
    all records in one huge contiguous buffer (assuming we know the upper limit on
    the total record size):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 提出替代方案需要进行范式转变。执行所需操作的算法按照指定的方式也对内存组织施加了限制：更改记录需要在内存中移动它们，只要我们希望能够更改任何一条记录而不影响其他任何内容，我们就无法避免记录在内存中的随机分布。我们必须侧面解决问题，并从限制开始。我们真的希望按顺序访问所有记录。在这种约束下，我们能做些什么？我们可以非常快速地读取所有记录。我们可以决定记录是否必须更改；这一步与以前相同。但是如果记录必须增长，我们该怎么办？我们必须将其移动到其他地方，没有足够的空间来增长。但我们同意记录将保持按顺序分配，一个接一个。然后前一条记录和下一条记录也必须移动，以便它们仍然存储在我们新记录的前后。这是替代算法的关键：所有记录在每个更改集中都会移动，无论它们是否被更改。现在我们可以将所有记录存储在一个巨大的连续缓冲区中（假设我们知道总记录大小的上限）：
- en: '![Figure 4.15 – Processing all records sequentially'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.15 – 顺序处理所有记录'
- en: '](img/Figure_4.15_B16229.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.15_B16229.jpg)'
- en: Figure 4.15 – Processing all records sequentially
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 – 顺序处理所有记录
- en: 'The algorithm requires allocating the second buffer of equal size during the
    copying, so the peak memory consumption is twice the size of the data:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在复制过程中，算法需要分配相同大小的第二个缓冲区，因此峰值内存消耗是数据大小的两倍：
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In every change set, we copy every string (record) from the old buffer to the
    new one. If the record needs to be changed, the new version is written into the
    new buffer. Otherwise, the original is simply copied. With every new change set,
    we will create a new buffer and, at the end of the operation, release the old
    one (a practical implementation would avoid repeated calls to allocate and deallocate
    memory and simply swap two buffers).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个更改集中，我们将每个字符串（记录）从旧缓冲区复制到新缓冲区。如果记录需要更改，新版本将被写入新缓冲区。否则，原始记录将被简单复制。随着每个新的更改集，我们将创建一个新的缓冲区，并在操作结束时释放旧缓冲区（实际实现将避免重复调用分配和释放内存，并简单地交换两个缓冲区）。
- en: 'The obvious downside to this implementation is the use of the huge buffer:
    we have to be pessimistic in choosing its size to allocate enough memory for the
    largest possible records we might encounter. Doubling of the peak memory size
    is also concerning. We can solve this problem by combining this approach with
    the **growable array** data structure we saw earlier. Instead of allocating one
    contiguous buffer, we can store the records in a series of fixed-size blocks:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实现的明显缺点是使用了巨大的缓冲区：我们必须在选择其大小时持悲观态度，以便为可能遇到的最大记录分配足够的内存。峰值内存大小的翻倍也令人担忧。我们可以通过将这种方法与我们之前看到的**可增长数组**数据结构相结合来解决这个问题。我们可以将记录存储在一系列固定大小的块中，而不是分配一个连续的缓冲区：
- en: '![Figure 4.16 – Using a block buffer to edit records'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.16 – 使用块缓冲区编辑记录'
- en: '](img/Figure_4.16_B16229.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.16_B16229.jpg)'
- en: Figure 4.16 – Using a block buffer to edit records
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 – 使用块缓冲区编辑记录
- en: 'To simplify the diagram, we draw all records of the same size, but this restriction
    is not necessary: the records can span multiple blocks (we treat the blocks as
    a contiguous sequence of bytes, nothing more). When editing a record, we need
    to allocate a new block for the edited record. As soon as the editing is done,
    the block (or blocks) that contained the old record can be released; we don''t
    have to wait for the entire buffer to be read. But we can do even better than
    that: instead of returning the recently freed block to the operating system, we
    can put it on the list of empty blocks. We are about to edit the next record,
    and we will need an empty new block for the result. We just happened to have one:
    it''s the block that used to contain the last record we edited; it is at the head
    of our list of recently released blocks, and, best of all, that block is the last
    memory we accessed, so it is probably still in the cache!'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化图表，我们绘制了相同大小的所有记录，但这个限制并非必要：记录可以跨越多个块（我们将块视为连续的字节序列，仅此而已）。在编辑记录时，我们需要为编辑后的记录分配一个新的块。一旦编辑完成，包含旧记录的块（或块）就可以被释放；我们不必等待整个缓冲区被读取。但我们甚至可以做得更好：我们可以将最近释放的块放回空块列表，而不是将其返回给操作系统。我们即将编辑下一条记录，我们将需要一个空的新块来存放结果。我们碰巧有一个：它就是曾经包含我们上次编辑的最后一条记录的块；它位于我们最近释放的块列表的开头，并且最重要的是，该块是我们最后访问的内存，因此它很可能仍然在缓存中！
- en: 'At first glance, this algorithm seems a really bad idea: we are going to copy
    all the records every time. But let us analyze the two algorithms more carefully.
    First of all, the amount of reading is the same: both algorithms must read each
    string to determine whether it must be changed. The second algorithm is already
    ahead in performance: it reads all data in a single sequential sweep, while the
    first algorithm jumps around the memory. If the string is edited, then both algorithms
    must write a new one into a new area of memory. The second algorithm again comes
    out ahead due to its sequential memory access pattern (also, it does not need
    to do a memory allocation for every string). The tradeoff comes when the string
    is not edited. The first algorithm does nothing; the second one makes a copy.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这个算法似乎是一个非常糟糕的主意：我们每次都要复制所有记录。但让我们仔细分析这两种算法。首先，阅读的数量是相同的：两种算法都必须读取每个字符串以确定是否必须更改。第二种算法在性能上已经领先：它在单个顺序扫描中读取所有数据，而第一种算法则在内存中跳来跳去。如果字符串被编辑，那么两种算法都必须将新字符串写入新的内存区域。第二种算法再次领先，因为它的内存访问模式是顺序的（而且它不需要为每个字符串进行内存分配）。权衡出现在字符串未被编辑时。第一种算法什么都不做；第二种算法进行复制。
- en: From this analysis, we can define the good and bad cases for each algorithm.
    The sequential access algorithm wins if the strings are short, and a large fraction
    of them are changed in every change set. The random access algorithm wins if the
    strings are long or if very few of them are changed. However, the only way to
    determine what is *long* and how many are *a large fraction* is to measure.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种分析，我们可以定义每种算法的优劣情况。如果字符串很短，并且每次更改集中有大部分字符串被更改，顺序访问算法会获胜。如果字符串很长，或者很少有字符串被更改，随机访问算法会获胜。然而，确定什么是*长*以及有多少是*大部分*的唯一方法是进行测量。
- en: The fact that we must measure the performance does not necessarily mean that
    you have to always write two versions of the complete program. Very often, we
    can simulate the particular aspect of the behavior in a small *mock* program that
    operates on the simplified data. We just need to know the approximate size of
    the records, how many are changed, and we need the code that does the change to
    a single record so we can measure the impact of the memory accesses on performance
    (if each change is very computationally expensive, it won't matter how long it
    takes to read or write the record). With such mock or prototype implementation,
    we can do the approximate measurements and make the right design decision.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须测量性能并不一定意味着您必须始终编写完整程序的两个版本。我们经常可以在操作简化数据的小*模拟*程序中模拟行为的特定方面。我们只需要知道记录的大致大小，更改了多少个记录，以及更改单个记录的代码，以便我们可以测量内存访问对性能的影响（如果每次更改都非常耗时，那么读取或写入记录需要多长时间就无关紧要了）。有了这样的模拟或原型实现，我们可以进行近似测量并做出正确的设计决策。
- en: 'So, in real life, is the sequential string-copying algorithm ever worth it?
    We have done the tests for editing medium-length strings (128 bytes) using a regular
    expression pattern. If 99% of all strings are edited in each change set, the sequential
    algorithm is approximately four times faster than random (the results will be
    somewhat specific to a machine, so the measurements must be done on the hardware
    similar to what you expect to use). If 50% of all records are edited, the sequential
    access is still faster, but only by about 12% (this is likely to be within the
    variation between different models of CPU and types of memory, so let''s call
    it a tie). The more surprising result is that if only 1% of all records are changed,
    the two algorithms are almost tied for speed: the time saved by not doing a random
    read pays for the cost of the almost entirely unnecessary copying.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在现实生活中，顺序字符串复制算法是否值得呢？我们已经对编辑中等长度字符串（128字节）使用正则表达式模式进行了测试。如果每个更改集中有99%的字符串都被编辑，那么顺序算法大约比随机算法快四倍（结果可能会与机器有关，因此必须在与您期望使用的硬件类似的硬件上进行测量）。如果50%的记录都被编辑，顺序访问仍然更快，但只快约12%（这可能在不同型号的CPU和内存类型之间的差异范围内，因此我们称之为平局）。更令人惊讶的结果是，如果只有1%的记录被更改，那么两种算法的速度几乎相当：不进行随机读取所节省的时间可以弥补几乎完全不必要的复制的成本。
- en: 'For longer strings, the random-access algorithm wins handily if few strings
    are changed, and for very long strings, it''s a tie even if all strings are changed:
    both algorithms read and write all strings sequentially (the random access to
    the beginning of a long string adds negligible time).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较长的字符串，如果很少更改字符串，随机访问算法会轻松获胜，对于非常长的字符串，即使所有字符串都更改，它也是平局：两种算法都按顺序读取和写入所有字符串（对长字符串的随机访问增加的时间可以忽略不计）。
- en: 'We now have everything we need to determine the better algorithm for our application.
    This is the way the design for performance often goes: we identify the root of
    the performance problem, we come up with a way to eliminate the issue, at the
    cost of doing something else, and then we have to hack together a prototype that
    lets us measure whether the clever trick actually pays off.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了确定我们的应用程序更好算法的一切所需。这通常是性能设计的方式：我们确定了性能问题的根源，想出了消除问题的方法，以代价做其他事情，然后我们必须拼凑出一个原型，让我们能够测量聪明的技巧是否真的值得。
- en: Before we end this chapter, I would like to show you an entirely different "use"
    of the performance improvements provided by the caches and other hardware.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，我想向您展示缓存和其他硬件提供的性能改进的完全不同的“用法”。
- en: The ghost in the machine
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器中的幽灵
- en: 'In the last two chapters, we have learned how complex the path from the initial
    data to the final result can be on a modern computer. Sometimes the machine does
    precisely what the code prescribes: read the data from memory, do the computations
    as written, save the results back to memory. More often than not, however, it
    goes through some strange intermediate states we don''t even know about. *Read
    from memory* does not always read from memory: instead of executing instructions
    as written, the CPU may decide to execute something else, speculatively, because
    it thinks you will need it, and so on. We have tried to confirm by direct performance
    measurements that all of those things really do exist. By necessity, these measurements
    are always indirect: the hardware optimizations and transformations of the code
    are designed to deliver the correct result, after all, only faster.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两章中，我们已经了解到，在现代计算机上，从初始数据到最终结果的路径有多么复杂。有时，机器确实按照代码规定的方式执行：从内存中读取数据，按照指令进行计算，将结果保存回内存。然而，更常见的情况是，它经历了一些我们甚至不知道的奇怪中间状态。*从内存中读取*并不总是从内存中读取：CPU可能决定执行其他东西，因为它认为你会需要它，等等。我们已经尝试通过直接性能测量来确认所有这些事情确实存在。出于必要，这些测量总是间接的：硬件优化和代码转换旨在提供正确的结果，毕竟只是更快。
- en: 'In this section, we show yet more observable evidence of the hardware operations
    that were supposed to remain hidden. This is a big one: its discovery in 2018
    triggered a brief cybersecurity panic and a flood of patches from hardware and
    software vendors. We are talking about the Spectre and Meltdown family of security
    vulnerabilities, of course.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了更多本来应该隐藏的硬件操作的可观察证据。这是一个重大发现：2018年的发现引发了一场短暂的网络安全恐慌，并导致硬件和软件供应商发布了大量补丁。当然，我们谈论的是Spectre和Meltdown安全漏洞家族。
- en: What is Spectre?
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是Spectre？
- en: 'In this section, we will demonstrate, in detail, the early version of the Spectre
    attack, known as Spectre version 1\. This isn''t a book on cybersecurity; however,
    the Spectre attack is carried out by carefully measuring the performance of the
    program, and it relies on the two performance-enhancing hardware techniques we
    have studied in this book: speculative execution and memory caching. This makes
    the attack educational in a work dedicated to software performance as well.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细演示Spectre攻击的早期版本，即Spectre版本1。这不是一本关于网络安全的书；然而，Spectre攻击是通过仔细测量程序的性能来执行的，并且依赖于我们在本书中学习的两种性能增强硬件技术：推测执行和内存缓存。这使得攻击在致力于软件性能的工作中具有教育意义。
- en: 'The idea behind Spectre is as follows. We have learned earlier that when a
    CPU encounters a conditional jump instruction, it attempts to predict the result
    and proceeds to execute the instructions in the assumption that the prediction
    is correct. This is known as speculative execution, and without it, we would not
    have pipelining in any practically useful code. The tricky part of the speculative
    execution is the error handling: errors frequently occur in the speculatively
    executed code, but until the prediction is proven correct, these errors must remain
    invisible. The most obvious example is the null pointer dereference: if the processor
    predicts that a pointer is not null and executes the corresponding branch, a fatal
    error will occur every time the branch is mispredicted, and the pointer is, in
    fact, null. Since the code is written correctly to avoid dereferencing the null
    pointer, it must execute correctly as well: the potential error must remain potential.
    Another common speculative error is array boundary read or write:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Spectre背后的想法是这样的。我们早些时候已经了解到，当CPU遇到条件跳转指令时，它会尝试预测结果，并继续执行假设预测是正确的指令。这被称为推测执行，如果没有它，我们在任何实际有用的代码中都不会有流水线。推测执行的棘手部分是错误处理：在推测执行的代码中经常发生错误，但在预测被证明正确之前，这些错误必须保持不可见。最明显的例子是空指针解引用：如果处理器预测指针不为空并执行相应的分支，那么每次分支被错误预测并且指针实际上为空时，都会发生致命错误。由于代码被正确编写以避免对空指针进行解引用，它也必须正确执行：潜在错误必须保持潜在。另一个常见的推测性错误是数组边界读取或写入：
- en: '[PRE17]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If the index `i` is usually less than the array size `N`, then that will become
    the prediction, and the read from `a[i]` will be executed, speculatively, every
    time. What happens if the prediction is wrong? The result is discarded, so no
    harm was done, right? Not so fast: the memory location `a[i]` is not in the original
    array. It doesn''t even have to be the element right after the array. The index
    could be arbitrarily large, so the indexed memory location could belong to a different
    program or even to the operating system. We do not have the access privileges
    to read this memory. The OS does enforce access control, so normally trying to
    read some memory from another program would trigger an error. But this time, we
    do not know for sure that the error is real: the execution is still in the speculative
    phase, and the branch prediction could have been wrong. The error remains a *speculative
    error* until we know whether the prediction was correct. There is nothing new
    here so far; we have seen it all earlier.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果索引`i`通常小于数组大小`N`，那么这将成为预测，并且每次都会执行对`a[i]`的读取，具有推测性。如果预测错误会发生什么？结果被丢弃，所以没有造成伤害，对吧？不要那么快：内存位置`a[i]`不在原始数组中。它甚至不必是数组右后面的元素。索引可以任意大，因此索引的内存位置可能属于不同的程序，甚至属于操作系统。我们没有访问权限来读取这个内存。操作系统确实执行访问控制，因此通常尝试从另一个程序读取一些内存会触发错误。但这次，我们并不确定错误是否真实：执行仍处于推测阶段，分支预测可能是错误的。在我们知道预测是否正确之前，错误仍然是*推测性错误*。到目前为止，这里没有什么新鲜的；我们早就见过这一切。
- en: 'However, there is a subtle side effect to the potentially illegal read operation:
    the value `a[i]` is loaded into the cache. The next time we try to read from the
    same location, the read will be faster. This is true whether the read is real
    or speculative: memory operations during speculative execution work just like
    the *real* ones. Reading from the main memory takes longer while reading from
    the cache is faster. The speed of memory load is something we can observe and
    measure. It is not the intended result of the program but a measurable side effect
    nonetheless. In effect, the program has an additional output mechanism through
    means other than its intended output; this is called a **side-channel**.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于潜在的非法读取操作存在一个微妙的副作用：值`a[i]`被加载到缓存中。下次我们尝试从相同位置读取时，读取速度会更快。无论读取是真实的还是推测的，内存操作在推测执行期间的工作方式与*真实*操作一样。从主内存读取需要更长的时间，而从缓存读取则更快。内存加载的速度是我们可以观察和测量的。这不是程序的预期结果，但仍然是一个可测量的副作用。实际上，程序通过意外的方式具有了额外的输出机制；这被称为**侧信道**。
- en: 'The Spectre attack exploits this side-channel:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Spectre攻击利用了这个侧信道：
- en: '![Figure 4.17 – Setting up the Spectre attack'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.17 – 设置Spectre攻击'
- en: '](img/Figure_4.17_B16229.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.17_B16229.jpg)'
- en: Figure 4.17 – Setting up the Spectre attack
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17 – 设置Spectre攻击
- en: 'It uses the value at the location `a[i]`, obtained during the speculative execution,
    to index into another array, `t`. After this is done, one array element, `t[a[i]]`,
    will be loaded into the cache. The rest of the array `t` was never accessed and
    is still in memory. Note that, unlike the element `a[i]`, which is not really
    an element of the array `a` but some value at the memory location we can''t get
    to by any legitimate means, the array `t` is entirely within our control. It is
    crucial for the success of the attack that the branch remains unpredicted long
    enough while we read the value `a[i]` and then the value `t[a[i]]`. Otherwise,
    the speculative execution will end as soon as the CPU detects that the branch
    is mispredicted and none of these memory accesses are, in fact, needed. After
    the speculative execution is carried out, the misprediction is eventually detected,
    and all the consequences of the speculative operations are rolled back, including
    the would-be memory access error. All consequences but one, that is: the value
    of the array `t[a[i]]` is still in the cache. There is nothing wrong with it,
    per se: accessing this value is legal, we can do it any time, and, in any case,
    the hardware moves data from and to the caches all the time; it never changes
    the result or lets you access any memory you weren''t supposed to.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用在推测执行期间获得的位置`a[i]`的值来索引另一个数组`t`。完成后，一个数组元素`t[a[i]]`将被加载到缓存中。数组`t`的其余部分从未被访问过，仍然在内存中。请注意，与元素`a[i]`不同，后者实际上不是数组`a`的元素，而是我们无法通过任何合法手段到达的内存位置的某个值，数组`t`完全在我们的控制范围内。攻击的成功与否取决于分支保持足够长时间的不可预测性，同时我们读取值`a[i]`和值`t[a[i]]`。否则，一旦CPU检测到分支被错误预测，并且实际上不需要任何这些内存访问，推测执行将立即结束。推测执行完成后，最终会检测到错误预测，并且将回滚推测操作的所有后果，包括将要发生的内存访问错误。除了一个后果：数组`t[a[i]]`的值仍然在缓存中。这本身并没有问题：访问这个值是合法的，我们可以随时这样做，而且无论如何，硬件一直在缓存之间移动数据；它从不改变结果，也不会让你访问任何你不应该访问的内存。
- en: 'There is, however, an observable after-effect of this entire series of events:
    one element of the array `t` is much faster to access than the rest of them:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这整个系列事件的一个可观察的后果是：数组`t`的一个元素比其余元素访问速度要快得多：
- en: '![Figure 4.18 – Memory and cache state after the Spectre attack'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.18 – Spectre攻击后的内存和缓存状态'
- en: '](img/Figure_4.18_B16229.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.18_B16229.jpg)'
- en: Figure 4.18 – Memory and cache state after the Spectre attack
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18 – Spectre攻击后的内存和缓存状态
- en: If we can measure how long it takes to read each element of the array `t`, we
    can find out the one that was indexed by the value `a[i]`; that is the secret
    value we were not supposed to know!
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以测量读取数组`t`的每个元素所需的时间，我们就可以找出由值`a[i]`索引的那个元素；这就是我们本不应该知道的秘密值！
- en: Spectre by example
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spectre by example
- en: The Spectre attack takes several pieces to put together; we will go through
    them one by one since, overall, it is quite a large coding example for a book
    (this particular implementation is a variation on the example given by Chandler
    Carruth at CPPCon in 2018).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Spectre攻击需要几个部分来组合；我们将逐一介绍它们，因为总的来说，这对于一本书来说是一个相当大的编码示例（这个特定的实现是根据2018年CPPCon上Chandler
    Carruth给出的示例进行的变体）。
- en: 'One component we will need is an accurate timer. We can try to use the C++
    high-resolution timer:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的一个组件是一个准确的计时器。我们可以尝试使用C++高分辨率计时器：
- en: '[PRE18]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The overhead and the resolution of this timer depend on the implementation;
    the standard does not require any particular performance guarantees. On the x86
    CPU, we can try to use the **time-stamp counter** (**TSC**), which is a hardware
    counter that counts the number of cycles since some point in the past. Using the
    cycle count as a timer typically results in noisier measurements, but the timer
    itself is faster, which is important here, considering that we are going to try
    to measure how long it takes to load a single value from memory. GCC, Clang, and
    many other compilers have a built-in function for accessing this counter:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计时器的开销和分辨率取决于实现；标准不要求任何特定的性能保证。在x86 CPU上，我们可以尝试使用**时间戳计数器**（**TSC**），它是一个硬件计数器，计算自过去某个时间点以来的周期数。使用循环计数作为计时器通常会导致测量结果更加嘈杂，但计时器本身更快，这在这里很重要，因为我们将尝试测量从内存中加载单个值需要多长时间。GCC、Clang和许多其他编译器都有一个内置函数来访问这个计数器：
- en: '[PRE19]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Either way, we now have a fast timer. The next step is the timing array. In
    practice, it''s not quite as simple as an array of integers, which we implied
    in our figures: integers are too close to each other in memory; loading one into
    the cache affects the time it takes to access its neighbors. We need to space
    the values far apart:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们现在有了一个快速的计时器。下一步是计时数组。实际上，它并不像我们在图中暗示的那样简单，只是一个整数数组：整数在内存中太靠近了；将一个加载到缓存中会影响访问其邻居所需的时间。我们需要将值远远地分开：
- en: '[PRE20]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here we are going to use only the first byte of the `timing_element`; the rest
    are there to enforce the distance in memory. There is nothing magical about the
    distance of 1024 bytes; it just has to be *large enough*, but what that is for
    you is something that you have to determine experimentally: the attack becomes
    unreliable if the distance is too small. There are 256 elements in the timing
    array. This is because we are going to read the *secret memory* one byte at a
    time. So, in our earlier example, the array `a[i]` will be an array of characters
    (even if the real data type is not `char`, we can still read it byte by byte).
    Initializing the timing array is not, strictly speaking, necessary; nothing depends
    on the content of this array.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将只使用`timing_element`的第一个字节；其余的是为了在内存中强制距离。1024字节的距离并没有什么神奇之处；它只是*足够大*，但对于你来说，这是需要通过实验来确定的：如果距离太小，攻击就会变得不可靠。计时数组中有256个元素。这是因为我们将逐字节读取*秘密内存*。因此，在我们之前的例子中，数组`a[i]`将是一个字符数组（即使实际的数据类型不是`char`，我们仍然可以逐字节读取它）。初始化计时数组在严格意义上来说并不是必要的；没有任何东西依赖于这个数组的内容。
- en: 'We are now ready to see the *heart* of the code. What follows is a simplified
    implementation: it is missing a few necessary twists that we are going to add
    later, but it''s easier to explain the code by focusing on the key parts first.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备看代码的*核心*。接下来是一个简化的实现：它缺少一些我们将在后面添加的必要的细节，但通过首先关注关键部分来解释代码会更容易一些。
- en: 'We need the array that we are going to read out-of-bounds:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的数组是我们将要越界读取的。
- en: '[PRE21]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here `size` is the real size of the `data`, and `evil_index` is larger than
    `size`: it is the index of the secret value outside of the proper data array.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`size`是`data`的真实大小，`evil_index`大于`size`：它是数据数组之外的秘密值的索引。
- en: 'Next, we are going to *train* the branch predictor: we need it to learn that
    the more likely branch is the one that does access the array. To that end, we
    generate a valid index that always points into the array (we will see exactly
    how in a moment). This is our `ok_index`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将*训练*分支预测器：我们需要它学会更有可能的分支是访问数组的分支。为此，我们生成一个始终指向数组的有效索引（我们马上就会看到确切的方法）。这就是我们的`ok_index`：
- en: '[PRE22]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then we read the memory at the location `timing_array + data[i]`, where `i`
    is either the `ok` index or the `evil` index, but the former happens much more
    often than the latter (we try to read the secret data only once out of 16 attempts,
    to keep the branch predictor trained for a successful read). Note that the actual
    memory access is guarded by a valid bounds check; this is of utmost importance:
    we never actually read the memory we are not supposed to read; this code is 100%
    correct.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们读取位置`timing_array + data[i]`处的内存，其中`i`要么是`ok`索引，要么是`evil`索引，但前者发生的频率要比后者高得多（我们尝试读取秘密数据只有16次中的一次，以保持分支预测器对成功读取的训练）。请注意，实际的内存访问受到有效的边界检查的保护；这是至关重要的：我们从未真正读取我们不应该读取的内存；这段代码是100%正确的。
- en: 'The function to access memory is, conceptually, just a memory read. In practice,
    we have to contend with the clever optimizing compiler, which will try to eliminate
    redundant or unnecessary memory operations. Here is one way, it uses intrinsic
    assembly (the read instruction is actually generated by the compiler because the
    location `*p` is flagged as input):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 访问内存的函数，在概念上只是一个内存读取。实际上，我们必须应对聪明的优化编译器，它会尝试消除多余或不必要的内存操作。这是一种方法，它使用内在的汇编语言（读取指令实际上是由编译器生成的，因为位置`*p`被标记为输入）：
- en: '[PRE23]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We run the prediction-misprediction loop a number of times (`100`, in our example).
    Now we expect one element of the `timing_array` to be in the cache, so we just
    have to measure how long it takes to access each element. The one caveat here
    is that sequentially accessing the whole array will not work: the prefetch will
    quickly kick in and move the element we are about to access into the cache. Very
    effective most of the time, but not what we need now. We have to access the elements
    of the array in random order instead and store the time it took to access each
    element in the array of memory access latencies:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行预测-误判循环多次（在我们的例子中是`100`次）。现在我们期望`timing_array`中的一个元素在缓存中，所以我们只需要测量访问每个元素所需的时间。这里的一个注意事项是，顺序访问整个数组是行不通的：预取会迅速启动并将我们即将访问的元素移入缓存。大多数情况下非常有效，但不是我们现在需要的。相反，我们必须以随机顺序访问数组的元素，并将访问每个元素所需的时间存储在内存访问延迟数组中：
- en: '[PRE24]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You may wonder, why not simply look for the one fast access? Two reasons: first,
    we don''t know what *fast* really means for any particular hardware; we just know
    that it''s faster than *normal*. So we have to measure what is *normal* too. Second,
    any individual measurement is not going to be 100% reliable: sometimes, the computation
    is interrupted by another process or the OS; the exact timing of the whole sequence
    of operations depends on what else the CPU is doing at the time, and so on. It''s
    only very likely that this process will reveal the value at the secret memory
    location, but not 100% guaranteed, so we have to try several times and average
    the results.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么不简单地寻找一个快速访问？有两个原因：首先，我们不知道对于任何特定的硬件来说*快速*到底意味着什么；我们只知道它比*正常*更快。因此，我们也必须测量什么是*正常*。其次，任何单独的测量都不会是100%可靠的：有时，计算会被另一个进程或操作系统中断；整个操作序列的确切时间取决于CPU在此时正在做什么，等等。这个过程只是很有可能会揭示秘密内存位置的值，但并不是100%保证的，所以我们必须尝试多次并平均结果。
- en: 'Before we do that, there are several omissions in the code we saw. First of
    all, it assumes that the timing array values are not in the cache already. Even
    if it was true when we started, it wouldn''t be after we successfully peek at
    the first secret byte. We have to purge the timing array from the cache every
    time before we start the attack on the next byte we want to read:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行此操作之前，我们看到代码中有几个遗漏。首先，它假设定时数组值尚未在缓存中。即使在我们开始时是真的，但在成功窥视第一个秘密字节之后，它也不会是真的。我们必须在攻击下一个要读取的字节之前每次都从缓存中清除定时数组：
- en: '[PRE25]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Again, we use a GCC/Clang built-in function; most compilers have something similar,
    but the function name could vary.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们使用GCC/Clang内置函数；大多数编译器都有类似的东西，但函数名称可能会有所不同。
- en: 'Second, the attack works only if the speculative execution lasts long enough
    for the two memory accesses (data and timing array) to happen before the CPU figures
    out which branch it was supposed to take. In practice, the code as written does
    not spend enough time in the speculative execution context, so we have to make
    it harder to compute what the correct branch is. There is more than one way to
    do it; here, we make the branch condition dependent on reading some value from
    memory. We will copy the array size into another variable that is slow to access:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，攻击只有在推测执行持续时间足够长，以便在CPU弄清楚应该采取哪个分支之前发生两次内存访问（数据和定时数组）时才能生效。实际上，按照现有的代码，推测执行上下文中的时间不够长，因此我们必须使得计算正确分支更加困难。有多种方法可以做到这一点；在这里，我们使分支条件依赖于从内存中读取某个值。我们将数组大小复制到另一个访问速度较慢的变量中：
- en: '[PRE26]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we have to make sure this value is evicted from the cache before we need
    to read it and use the array size value stored in `*data_size` instead of the
    original `size` value:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须确保在我们需要读取它之前将该值从缓存中清除，并使用存储在`*data_size`中的数组大小值，而不是原始的`size`值：
- en: '[PRE27]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'There is also a magical *delay* in the preceding code, some useless computation
    that separates the cache flush from the access to the data size (it defeats the
    possible instruction reordering that would let the CPU access the array size faster).
    Now the condition `i < *data_size` takes some time to compute: the CPU needs to
    read the value from memory before it knows the result. The branch is predicted
    according to the more likely outcome, which is a valid index, so the array is
    accessed speculatively.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中还有一个神奇的*延迟*，一些无用的计算将缓存刷新与数据大小的访问分开（它击败了可能的指令重排序，让CPU更快地访问数组大小）。现在条件`i
    < *data_size`需要一些时间来计算：CPU需要在知道结果之前从内存中读取值。分支根据更可能的结果进行预测，即有效索引，因此数组被进行了推测性访问。
- en: Spectre, unleashed
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幽灵，释放
- en: The final step is to put it all together and run the procedure many times to
    accumulate statistically reliable measurements (timing measurements of a single
    instruction are very noisy given that the timer itself takes about as long as
    what we are trying to measure).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将所有内容汇总并多次运行该过程，以积累统计上可靠的测量数据（鉴于单个指令的定时测量非常嘈杂，因为计时器本身所需的时间大约与我们试图测量的时间一样长）。
- en: 'The following function attacks a single byte outside of the data array:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数攻击数据数组之外的单个字节：
- en: '[PRE28]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For each element of the timing array, we will compute a score, which is the
    number of times this element was the fastest one to access. We also track the
    second-fastest element, which should be just one of the regular, slow to access,
    array elements. We keep doing it for many iterations: ideally, until we get the
    result, but, in practice, we have to give up at some point.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定时数组的每个元素，我们将计算一个分数，即该元素成为最快访问的次数。我们还跟踪第二快的元素，它应该只是常规的、访问速度较慢的数组元素之一。我们会进行多次迭代：理想情况下，直到获得结果，但实际上，我们必须在某个时候放弃。
- en: Once a large enough gap opens between the best score and the second-best score,
    we know that we have reliably detected the *fast* element of the timing array,
    which is the one indexed by the value of the *secret* byte (if we reach the maximum
    number of iterations without getting a reliable answer, the attack has failed,
    although we can try to use the best guess we have so far).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦最佳分数和次佳分数之间出现足够大的差距，我们就知道我们已经可靠地检测到了定时数组的*快*元素，即由*secret*字节的值索引的元素（如果我们在达到最大迭代次数之前没有得到可靠的答案，攻击就失败了，尽管我们可以尝试使用到目前为止最好的猜测）。
- en: 'We have two utility functions to compute the average scores for the latencies
    and find the two best scores; these can be implemented any way you want as long
    as they give the correct results. The first function computes the average latency
    and increments the scores for the timing elements that have latencies somewhat
    below average (the threshold for *somewhat* has to be adjusted experimentally
    but is not very sensitive). Note that we expect one array element to be significantly
    faster to access, so we can skip it when computing the average latency (ideally,
    that one element would have much lower latency than the rest, and the rest would
    all be the same):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个实用函数来计算延迟的平均分数并找到两个最佳分数；只要它们能给出正确的结果，可以按任何方式实现。第一个函数计算平均延迟并增加具有略低于平均延迟的时间元素的分数（*略低*的阈值必须经过实验调整，但不太敏感）。请注意，我们希望一个数组元素的访问速度明显更快，因此在计算平均延迟时可以跳过它（理想情况下，该元素的延迟应比其余元素低得多，其余元素的延迟应该都相同）：
- en: '[PRE29]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The second function simply finds the two best scores in the array:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数只是在数组中找到两个最佳分数：
- en: '[PRE30]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we have a function that returns the value of a single byte outside of the
    specified array without ever reading this byte directly. We are ready to use it
    to get access to some secret data! For demonstration, we are going to allocate
    a very large array but designate most of it *off-limits* by specifying a small
    value as the array size. This is, in practice, the only way you can demonstrate
    this attack today: since its discovery, most computers have been patched against
    the Spectre vulnerability, so, unless you have a machine that was hidden in a
    cave and not updated for a few years, the attack will not work against any memory
    that you are really not allowed to access. The patches do not prevent you from
    using Spectre against any data that you are allowed to access, but you have to
    examine the code and prove that it really does return the values without accessing
    the memory directly. That is what we are going to do: our `spectre_attack` function
    does not read any memory outside of the data array of the specified size, so we
    can create an array that is twice as large as specified and hide a secret message
    in the upper half:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个函数，它返回指定数组之外的单个字节的值，而不是直接读取这个字节。我们准备使用它来访问一些秘密数据！为了演示，我们将分配一个非常大的数组，但通过指定一个小值作为数组大小，大部分数组都被*禁止访问*。实际上，这是你今天可以演示这种攻击的唯一方式：自发现以来，大多数计算机已经修补了Spectre漏洞，因此，除非你有一台隐藏在山洞中并且几年没有更新的机器，否则这种攻击不会对你真正不允许访问的任何内存起作用。这些补丁并不会阻止你使用Spectre攻击你被允许访问的任何数据，但你必须检查代码并证明它确实返回值而不是直接访问内存。这就是我们要做的：我们的`spectre_attack`函数不会读取指定大小的数据数组之外的任何内存，因此我们可以创建一个大小是指定大小两倍的数组，并将秘密消息隐藏在上半部分。
- en: '[PRE31]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Examine again the values we give to the `spectre_attack` function: the array
    *size* is just the length of the string stored in the array; no other memory is
    accessed by the code except in the speculative execution context. All memory accesses
    are guarded by the correct bound checks. And yet, byte by byte, this program reveals
    the content of the second string, the one that is never read directly.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 再次检查我们给`spectre_attack`函数的值：数组的*大小*只是存储在数组中的字符串的长度；代码除了在推测执行上下文中以外，不会访问任何其他内存。所有内存访问都受到正确的边界检查的保护。然而，这个程序逐字节地揭示了第二个字符串的内容，而这个字符串从未被直接读取。
- en: 'To conclude, we used the speculative execution context to peek at the memory
    that we are not allowed to access. Because the branch condition for accessing
    this memory is correct, the invalid access error remains a *potential error*;
    it never actually happens. All the results of the mispredicted branch are undone,
    except one: the accessed value remains in the cache, so the next access to the
    same value is faster. By measuring the memory access times carefully, we can figure
    out what that value was! Why did we do this when we are interested in performance,
    not hacking? Mostly to confirm that the processor and the memory really behave
    the way we described: the speculative execution really happens, and the caches
    really work and make data accesses faster.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们利用了推测执行上下文来窥视我们不允许访问的内存。因为访问该内存的分支条件是正确的，所以无效访问错误仍然是一个*潜在错误*；它实际上从未发生过。所有错误预测分支的结果都被撤消，除了一个：被访问的值仍然留在缓存中，因此对相同值的下一次访问会更快。通过仔细测量内存访问时间，我们可以弄清楚那个值是什么！为什么我们这样做，当我们关心的是性能，而不是黑客行为？主要是为了确认处理器和内存确实按照我们描述的方式运行：推测执行确实发生，缓存确实起作用并使数据访问更快。
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we have learned how the memory system works: in a word, slowly.
    The difference in the performance of the CPUs and the memory creates the memory
    gap, where the fast CPU is held back by the low performance of the memory. But
    the memory gap also contains within it the seeds of the potential solution: we
    can trade many CPU operations for one memory access.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了内存系统的工作原理：简而言之，缓慢。CPU和内存性能的差异造成了内存差距，快速的CPU受到内存性能低下的限制。但内存差距中也蕴含着潜在解决方案的种子：我们可以用多个CPU操作来交换一个内存访问。
- en: 'We have further learned that the memory system is very complex and hierarchical
    and that it does not have a single speed. This can hurt your program''s performance
    really badly if you end up in the worst-case scenario. But again, the trick is
    to look at it as an opportunity rather than a burden: the gains from optimizing
    memory accesses can be so large that they more than pay for the overhead.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还进一步了解到，内存系统非常复杂和分层，并且它没有单一的速度。如果最终陷入最坏情况，这可能会严重影响程序的性能。但是，再次强调，关键是将其视为一种机会而不是负担：优化内存访问所带来的收益可能会远远超过开销。
- en: As we have seen, the hardware itself provides several tools to improve memory
    performance. Beyond that, we have to choose memory-efficient data structures and,
    if that alone does not suffice, memory-efficient algorithms to improve performance.
    As usual, all performance decisions must be guided and backed up by measurements.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，硬件本身提供了几种工具来改善内存性能。除此之外，我们必须选择内存高效的数据结构，如果仅靠这一点还不够，还要选择内存高效的算法来提高性能。和往常一样，所有性能决策都必须受到测量的指导和支持。
- en: 'So far, everything we have done and measured used a single CPU. In fact, since
    the first few pages in the introduction, we hardly even mentioned that almost
    every computer you can find today has multiple CPU cores and often multiple physical
    processors. The reason for this is very simple: we have to learn to use the single
    CPU efficiently before we can move on to the more complex multi-CPU problems.
    Starting with the next chapter, we turn our attention to the problems of concurrency
    and using large multi-core and multi-processor systems efficiently.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所做的和测量的一切都是使用单个CPU。实际上，自介绍的前几页以来，我们几乎没有提到今天几乎每台计算机都有多个CPU核心，通常还有多个物理处理器。这样做的原因非常简单：我们必须学会有效地使用单个CPU，然后才能转向更复杂的多CPU问题。从下一章开始，我们将把注意力转向并发问题，以及如何有效地使用大型多核和多处理器系统。
- en: Questions
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the memory gap?
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是内存差距？
- en: What factors affect the observed memory speed?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪些因素影响了观察到的内存速度？
- en: How can we find the places in the program where accessing memory is the main
    cause of poor performance?
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何找到程序中访问内存是性能不佳的主要原因的地方？
- en: What are the main ways to optimize the program for better memory performance?
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪些主要的优化程序以获得更好的内存性能的方法？
