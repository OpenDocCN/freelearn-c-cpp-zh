- en: Deep Learning Acceleration with CUDA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA加速深度学习
- en: Deep learning is a machine learning method that can interpret data based on
    artificial neural networks. Specifically, we provide data that a machine can understand
    and build neural network models that learn representations from data. We can use
    this technique to build models that recognize speech, classify objects from images,
    understand text, translate languages, transform data domains, and so on. Basic
    neural networks include the **fully connected layer** (**FCL**), the **convolutional
    neural network** (**CNN**), and the **recurrent neural network** (**RNN**). These
    architectures show strong accuracy in data classification, regional understandings,
    and sequential relationships.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一种可以根据人工神经网络解释数据的机器学习方法。具体来说，我们提供机器可以理解的数据，并构建学习数据表示的神经网络模型。我们可以使用这种技术构建识别语音、从图像中分类对象、理解文本、翻译语言、转换数据域等模型。基本的神经网络包括全连接层（FCL）、卷积神经网络（CNN）和循环神经网络（RNN）。这些架构在数据分类、区域理解和顺序关系方面显示出强大的准确性。
- en: Deep learning requires large computations so that it can be widely used. However,
    this issue was resolved because we can reduce the training time significantly
    by using GPU computing power. This is because the basic architecture of neural
    networks is based on matrix operations and GPU is a hardware platform that's been
    optimized for this. Specifically, the innovations of deep learning were tackled
    with NVIDIA CUDA accelerations as many algorithms in deep learning can be accelerated.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习需要大量计算，以便广泛应用。然而，通过使用GPU计算能力，我们可以显著减少训练时间，从而解决了这个问题。这是因为神经网络的基本架构是基于矩阵运算的，而GPU是一个针对此进行了优化的硬件平台。具体来说，深度学习的创新是通过NVIDIA
    CUDA加速来解决的，因为深度学习中的许多算法可以加速。
- en: In this chapter, we will review the neural network operations briefly and discuss
    how these can be accelerated on GPUs. As practice, we will implement a convolutional
    network using the cuDNN and cuBLAS CUDA libraries. The cuDNN library is NVIDIA's
    CUDA library that optimizes deep learning operations specifically. We will cover
    its implementation across three sections. We will also cover how GPUs can optimize
    the required operations. Then, we will cover how using the cuDNN library is effective
    by comparing the performance of the **long short-term memory** (**LSTM**) network.
    Then, we will cover profiling methods in deep learning using the **NVIDIA Tools
    Extension** (**NVTX**). This measures network operations on the GPUs so that we
    can analyze the operations in the timeline and understand their performance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将简要回顾神经网络操作，并讨论如何在GPU上加速这些操作。作为实践，我们将使用cuDNN和cuBLAS CUDA库实现一个卷积网络。cuDNN库是NVIDIA的CUDA库，专门优化了深度学习操作。我们将在三个部分中介绍其实现。我们还将介绍GPU如何优化所需的操作。然后，我们将通过比较**长短期记忆**（LSTM）网络的性能来介绍使用cuDNN库的有效性。然后，我们将介绍使用**NVIDIA工具扩展**（NVTX）进行深度学习的性能分析。这可以测量GPU上的网络操作，以便我们可以分析时间线上的操作并了解其性能。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Fully connected layer acceleration with CUBLAS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CUBLAS加速全连接层
- en: Element-wise layers with cuDNN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用cuDNN的逐元素层
- en: Softmax and loss functions in cuDNN/CUDA
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cuDNN/CUDA中的Softmax和损失函数
- en: Convolutional neural networks with cuDNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用cuDNN的卷积神经网络
- en: Recurrent neural networks with CUDA
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CUDA的循环神经网络
- en: Profiling deep learning frameworks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习框架的性能分析
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires the cuDNN library and CUDA Toolkit to be installed. We
    also need CUDA-enabled GPUs. This chapter will cover the fundamentals of deep
    learning and its performance, and so will not require new GPU features. In other
    words, if you covered most of the content in the previous chapters, you will have
    a proper GPU to work with.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要安装cuDNN库和CUDA工具包。我们还需要CUDA启用的GPU。本章将介绍深度学习的基础知识和性能，因此不需要新的GPU功能。换句话说，如果您已经涵盖了前几章的大部分内容，您将拥有一个适当的GPU来使用。
- en: To install the cuDNN library, you need to download the package from [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn).
    You need to log in to the NVIDIA developer site to access the download page. You
    will need to register for an NVIDIA developer account if you don't have an account
    already. Make sure that cuDNN is compiled with the CUDA version you have installed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装cuDNN库，您需要从[https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)下载软件包。您需要登录NVIDIA开发者网站才能访问下载页面。如果您还没有帐户，您需要注册一个NVIDIA开发者帐户。确保cuDNN与您安装的CUDA版本编译一致。
- en: Fully connected layer acceleration with cuBLAS
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuBLAS加速全连接层
- en: The fully connected layer is the basic architecture of deep learning. Let's
    review its operations and see how CUDA accelerates neural networks in terms of
    the forward and back-propagation procedures. Then, we will apply them to the GPU.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层是深度学习的基本架构。让我们回顾一下它的操作，并看看CUDA如何加速神经网络的前向和反向传播过程。然后，我们将把它们应用到GPU上。
- en: Neural network operations
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络操作
- en: 'A neural network''s basic operation is to perform dot operation between the
    input data and parameters. We call this perception. In deep learning, the neural
    network connects multiple perceptions in a layered manner. We call these feed-forward
    neural networks. The following diagram shows a perceptron and the basic neural
    network:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的基本操作是在输入数据和参数之间执行点操作。我们称之为感知。在深度学习中，神经网络以分层方式连接多个感知。我们称这些为前馈神经网络。以下图表显示了一个感知和基本神经网络：
- en: '![](img/451a22fa-7568-4602-a522-e2dd3826e53e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/451a22fa-7568-4602-a522-e2dd3826e53e.png)'
- en: The perceptron's basic operation is to create a dot product with the input data
    and appropriate weights. Then, it performs a non-linear operation with an activation
    function such as a sigmoid or **rectifier linear unit** (**ReLU**). In feed-forward
    neural networks, the operation is just an affine transformation followed by the
    application of an activation function. A vector will be fed to the neural network
    as input and multiplies it with weight parameters between each node in the two
    layers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器的基本操作是使用输入数据和适当的权重创建点积。然后，它使用激活函数进行非线性操作，例如sigmoid或**整流线性单元**（**ReLU**）。在前馈神经网络中，操作只是一个仿射变换，然后是激活函数的应用。一个向量将被馈送到神经网络作为输入，并与两层中每个节点之间的权重参数相乘。
- en: To train the neural networks, we perform forward propagation, loss calculation,
    and gradient back-propagation, and then use the update parameter. Let's cover
    them briefly. Then, we will match each step using cuBLAS and other CUDA operations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练神经网络，我们进行前向传播、损失计算和梯度反向传播，然后使用更新参数。让我们简要介绍一下它们。然后，我们将使用cuBLAS和其他CUDA操作来匹配每个步骤。
- en: 'The forward operation can be denoted by the following equation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 前向操作可以用以下方程表示：
- en: '![](img/3bbf27ea-c1c4-4091-bfca-a7ef3640f55d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3bbf27ea-c1c4-4091-bfca-a7ef3640f55d.png)'
- en: Here, ![](img/f58e3de0-2d4a-43ab-888b-3ffe2c39d537.png) is a prediction result
    from the given input vector, ![](img/3b5c5481-dfe4-49e6-9bfd-16777e251f7f.png), ![](img/bf47e85d-b819-4362-ba4b-2c84785e42ce.png)
    is the weight parameter matrix, and ![](img/7fc375a7-123f-463e-8357-d6907f6095b5.png)
    is the activation function. As we can see, the basic operations in the fully connected
    layer are matrix operations. Therefore, we need to implement the matrix multiplication
    operation to the inputs and activation function. Because we take the classification
    task, we use a softmax function to normalize the output and obtain a probabilistic
    distributed result in the next layer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/f58e3de0-2d4a-43ab-888b-3ffe2c39d537.png) 是给定输入向量的预测结果，![](img/3b5c5481-dfe4-49e6-9bfd-16777e251f7f.png)
    是权重参数矩阵，![](img/bf47e85d-b819-4362-ba4b-2c84785e42ce.png) 是激活函数。正如我们所看到的，全连接层中的基本操作是矩阵运算。因此，我们需要对输入和激活函数实现矩阵乘法运算。因为我们进行分类任务，所以我们使用softmax函数来规范化输出，并在下一层获得概率分布结果。
- en: 'To obtain the loss between the true value, we apply one-hot encodings on the
    label and get cross-entropy loss by obtaining the entropies from each element,
    like so:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得真实值之间的损失，我们对标签应用one-hot编码，并通过从每个元素获得熵来获得交叉熵损失，如下所示：
- en: '![](img/3ff26108-c02a-4604-9eaf-e1a5d04df24f.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ff26108-c02a-4604-9eaf-e1a5d04df24f.png)'
- en: 'We can obtain the total loss value by means of the sum of each cross-entropy
    loss. Then, we can obtain the gradient from the preceding equation. This looks
    like a complicated operation, but it can be simplified, as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过每个交叉熵损失的总和来获得总损失值。然后，我们可以从前述方程中获得梯度。这看起来像一个复杂的操作，但可以简化如下：
- en: '![](img/fd0cf659-df61-472c-8fae-26facd67cc53.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd0cf659-df61-472c-8fae-26facd67cc53.png)'
- en: 'Now, we will propagate the gradients to the previous layer, which is called
    back-propagation. In this task, we use the chain rule to obtain the gradients
    for each weight and bias parameter. Then, we can update the weight parameter''s
    set and bias. For example, we can obtain the gradients of the weights and biases
    with the following equation:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将梯度传播到前一层，这被称为反向传播。在这个任务中，我们使用链式法则来获得每个权重和偏差参数的梯度。然后，我们可以更新权重参数集和偏差。例如，我们可以通过以下方程获得权重和偏差的梯度：
- en: '![](img/78c891c9-a052-46b4-ba22-11229094a2dc.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78c891c9-a052-46b4-ba22-11229094a2dc.png)'
- en: 'We can obtain the gradients to propagate to the previous layer with the following
    equation:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方程获得梯度传播到前一层：
- en: '![](img/29f600b2-c4fd-4055-8b00-063de51c91a9.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29f600b2-c4fd-4055-8b00-063de51c91a9.png)'
- en: 'Here, ![](img/d6c48113-6009-476e-b53a-61484c12014e.png) is the gradient of
    the activation function. Therefore, we need to obtain ![](img/9daa0c33-638e-4a0f-a64c-f6ad7d3b4b60.png) from
    the second layer for the first layer. Then, the first layer''s gradients of weight
    and biases can be obtained with the following equations:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/d6c48113-6009-476e-b53a-61484c12014e.png) 是激活函数的梯度。因此，我们需要从第二层获得![](img/9daa0c33-638e-4a0f-a64c-f6ad7d3b4b60.png)
    用于第一层。然后，可以通过以下方程获得第一层的权重和偏差的梯度：
- en: '![](img/10ef2652-3473-4250-ab5a-08b258b45757.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10ef2652-3473-4250-ab5a-08b258b45757.png)'
- en: 'Now, we can update the weights and biases based on the gradient descendant
    rule, as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以根据梯度下降规则更新权重和偏差，如下所示：
- en: '![](img/c829793e-26f4-4c44-8a50-d03e1c87a45b.png), ![](img/db73f6e5-a09f-4f6b-b850-9f78b7719aaa.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c829793e-26f4-4c44-8a50-d03e1c87a45b.png), ![](img/db73f6e5-a09f-4f6b-b850-9f78b7719aaa.png)'
- en: Here, ![](img/1da8983e-1bb9-4ea6-9d8f-39426d0661f3.png) is the iteration step.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/1da8983e-1bb9-4ea6-9d8f-39426d0661f3.png) 是迭代步骤。
- en: 'The gradient of the activation function ![](img/2eb7280e-6e41-45df-889f-049a2ed74da8.png)
    can be different, as well as its type. The implementation of this activation layer
    will be covered in the next section. The derivation of the activation functions
    can be denoted by the following equation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数![](img/2eb7280e-6e41-45df-889f-049a2ed74da8.png)的梯度可能不同，其类型也可能不同。这个激活层的实现将在下一节中介绍。激活函数的导数可以用以下方程表示：
- en: '![](img/3d8453c6-78df-4735-85c0-6b13d98729a1.png),     ![](img/da9de664-20a0-4af2-9b7a-95841c4fd8e6.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d8453c6-78df-4735-85c0-6b13d98729a1.png), ![](img/da9de664-20a0-4af2-9b7a-95841c4fd8e6.png)'
- en: As a result, the neural network operations are a set of linear algebra operations
    and can be covered with the cuBLAS library. The implemented code can be found
    in `01_ann`. We will cover these implementation details in *Implementing a fully
    connected layer,* *Implementing layer operation*, and *Implementing the softmax
    layer* sections.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络操作是一组线性代数操作，并且可以使用cuBLAS库进行覆盖。实现的代码可以在`01_ann`中找到。我们将在*实现全连接层*、*实现层操作*和*实现softmax层*部分介绍这些实现细节。
- en: Design of a neural network layer
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络层的设计
- en: 'Before we write our code, let''s cover how we can package the operations into
    a layer configuration:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写代码之前，让我们来看看如何将操作打包成一个层配置：
- en: First, we perform forward operation.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们执行前向操作。
- en: Then, we perform backward operation.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们执行反向操作。
- en: Then we get a weight update from the gradient.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们从梯度中得到一个权重更新。
- en: Finally, the output layer will obtain the loss.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，输出层将获得损失。
- en: 'In this manner, the layer can be configured as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，层可以配置如下：
- en: '![](img/b6bf914f-00bd-414e-b88f-bc36d05bb945.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6bf914f-00bd-414e-b88f-bc36d05bb945.png)'
- en: It has standardized inputs and outputs and two types of input, depending on
    the workflow. The left-hand data path will be named with the input while the right-hand
    side will be named with the output. The data is fed in two phases (forward and
    backward). We will use blobs to manage the parameters and input/output data. The
    blob is a wrapper of data that's processed across layers and helps manage memory
    space. We will use this design every layer to simplify the network's configuration.
    Every layer will have each blob's descriptors and forward/backward processing
    operations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有标准化的输入和输出，以及两种类型的输入，取决于工作流程。左侧数据路径将被命名为输入，而右侧将被命名为输出。数据分为两个阶段（前向和后向）。我们将使用blob来管理参数和输入/输出数据。blob是跨层处理的数据的包装器，并帮助管理内存空间。我们将使用这种设计来简化网络的配置。每个层都将有每个blob的描述符和前向/后向处理操作。
- en: 'Now, let''s create a layer class that will be the base class of all the layers.
    The following code shows how the `class` public function stacks. And, you can
    find its implementation in `layer.h` and `layer.cu` in `01_ann/src/ directory`.
    This has not only forward and backward operations but also weight update controls
    and loss calculations:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个层类，它将是所有层的基类。以下代码显示了`class`公共函数的堆叠。而且，你可以在`01_ann/src/ directory`的`layer.h`和`layer.cu`中找到它的实现。这不仅有前向和后向操作，还有权重更新控制和损失计算：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To support these operations, the layer class maintains several cuDNN descriptors,
    blob pointers, and weight update controllers. The detail implementations will
    be covered when we cover the network implementations:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持这些操作，层类维护了几个cuDNN描述符、blob指针和权重更新控制器。当我们涵盖网络实现时，详细的实现将会被涵盖：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This layer class will be used across deep learning network implementation in
    other sections. For this reason, it has `cudnnTensorDescriptor_t` variables for
    cuDNN operations, as well as the `get_loss()` and `get_accuracy()` functions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层类将在其他部分的深度学习网络实现中使用。因此，它具有用于cuDNN操作的`cudnnTensorDescriptor_t`变量，以及`get_loss()`和`get_accuracy()`函数。
- en: Tensor and parameter containers
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量和参数容器
- en: In our implementation, we will use a data container named `Blob`. Its name was
    borrowed from Caffe. This allows us to store tensors or network parameters with its
    dimensional size information and memory points. We will connect each layer using
    this. This helps each layer initialize its weights based on the input tensor's
    size information. Also, each layer can validate its result based on the information
    of the `Blob`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们将使用一个名为`Blob`的数据容器。它的名称是从Caffe借来的。这使我们能够存储张量或网络参数以及其维度大小信息和内存点。我们将使用这个来连接每一层。这有助于每一层根据输入张量的大小信息初始化其权重。此外，每一层都可以根据`Blob`的信息验证其结果。
- en: 'This blob will require the dimensional size information in the neural network,
    as shown in the following line of code. Then, its constructor will create a host-side
    buffer following the size information:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个blob将需要神经网络中的维度大小信息，如下一行代码所示。然后，它的构造函数将根据大小信息创建一个主机端缓冲区：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Blob` can also handle memories in the host and device and can help us access
    those memories. `Blob` has the following memory access helper functions:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`Blob`还可以处理主机和设备上的内存，并帮助我们访问这些内存。`Blob`具有以下内存访问辅助函数：'
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we discussed earlier, `Blob` can store tensors, we also need to provide
    tensor shape information as a descriptors required by cuDNN APIs. Therefore, `Blob`
    can create and set the tensor descriptor using the following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，`Blob`可以存储张量，我们还需要提供张量形状信息，作为cuDNN API所需的描述符。因此，`Blob`可以使用以下代码创建和设置张量描述符：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, let's implement a fully connected layer using `Blob`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`Blob`来实现一个全连接层。
- en: Implementing a fully connected layer
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个全连接层
- en: 'In this section, we will write a fully connected network using cuBLAS. For
    this layer, we will create a `Dense` class derived from the `Layer` class. The
    class constructor will receive the default layer configuration information, as
    follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用cuBLAS编写一个全连接网络。对于这个层，我们将创建一个从`Layer`类派生出来的`Dense`类。类构造函数将接收默认的层配置信息，如下所示：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: But this is not enough to configure the whole layer. The missing information
    will be provided from the input because the input size will be determined by the
    previous layer. Now, let's cover forward propagation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 但这还不足以配置整个层。缺失的信息将从输入中提供，因为输入大小将由前一层确定。现在，让我们来看看前向传播。
- en: Implementing forward propagation
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现前向传播
- en: 'In forward propagation, we can break the forward process into two steps, as
    follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播中，我们可以将前向过程分为两个步骤，如下所示：
- en: '![](img/95d7cd68-8fed-4f8a-bd7c-0f1d9e9a2afb.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95d7cd68-8fed-4f8a-bd7c-0f1d9e9a2afb.png)'
- en: 'Since the weight size does not have to be affected by the batch size, we only consider
    the number of input weights and output weights. On the other hand, data feeding
    blobs, such as input and output, are affected by the batch size. So, our GEMM
    operation with the filter and input data can be designed as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于权重大小不必受批量大小的影响，我们只考虑输入权重和输出权重的数量。另一方面，数据馈送blob，如输入和输出，受批量大小的影响。因此，我们的GEMM操作与过滤器和输入数据可以设计如下：
- en: '![](img/7a96d4c2-0e5a-44ce-8251-e09d28e5e49a.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a96d4c2-0e5a-44ce-8251-e09d28e5e49a.png)'
- en: 'The hidden output will be added with the bias values. The input data is not
    limited to the data from the data loader. As we stack the layers, the output of
    the previous layer will be the current layer''s input data. The forward operation
    can be implemented as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏的输出将与偏置值相加。输入数据不仅限于数据加载器中的数据。当我们堆叠层时，上一层的输出将成为当前层的输入数据。前向操作可以实现如下：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'At the first iteration, each layer needs to initialize its weight and bias.
    For example, this `Dense` layer can initialize its weights, biases, and output
    tensor elements. We can separate this initialization task into two phases. The
    first is for the weights and biases, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代中，每个层都需要初始化其权重和偏置。例如，这个`Dense`层可以初始化其权重、偏置和输出张量元素。我们可以将这个初始化任务分为两个阶段。第一个是权重和偏置，如下所示：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The next phases is about updating the input information and initializing the
    output blob. When it''s new or needs to be reconfigured, we need to do the following.
    In this task, we also need to create a vector filled with our batch size. This
    will be used in biases addition:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的阶段是关于更新输入信息和初始化输出blob。当它是新的或需要重新配置时，我们需要做以下工作。在这个任务中，我们还需要创建一个填满我们批量大小的向量。这将用于偏置的添加：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This initialization task triggered not only the first iteration but also batch
    size changes. Checking the batch size is not required in the training phase, but
    it will be useful in the testing phase. This is because the batch sizes in training
    and inference are different. In this case, we need to create an output blob following
    the new batch size. The output tensor''s size is determined as the channel size.
    The output blob''s creation code, as follows, creates a blob of size (`batch_size_`,
    `output_size_`, `1`, `1`):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个初始化任务不仅触发了第一次迭代，还触发了批量大小的变化。在训练阶段不需要检查批量大小，但在测试阶段会很有用。这是因为训练和推断阶段的批量大小是不同的。在这种情况下，我们需要根据新的批量大小创建一个输出blob。输出张量的大小是由通道大小确定的。以下代码创建了一个大小为（`batch_size_`，`output_size_`，`1`，`1`）的blob：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This creates flattened tensors. Then, we feed these tensors, which requires
    them to be aligned in channels. This alignment is specifically required in the
    softmax layer. We will cover this in the softmax layer's implementation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建扁平化张量。然后，我们将馈送这些张量，这要求它们在通道中对齐。这种对齐在softmax层中是特别需要的。我们将在softmax层的实现中进行讨论。
- en: 'Another important task in this phase is to initialize weights and biases. In
    our implementation, we will use the ReLU as an activator. We will use the normal
    initializer ([https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852))
    technique to make the network trainable. Following the guidelines in the preceding
    paper, the required weight values can be generated with the following equation:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段的另一个重要任务是初始化权重和偏置。在我们的实现中，我们将使用ReLU作为激活函数。我们将使用正常的初始化器（[https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)）技术使网络可训练。根据前述论文的指导，所需的权重值可以用以下方程生成：
- en: '![](img/85eca19f-7734-471a-b5a7-8191808d66f5.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85eca19f-7734-471a-b5a7-8191808d66f5.png)'
- en: '![](img/0ed79072-61db-46e8-a6bc-66049715f0ff.png) is the number of inputs from
    the previous layer. For this reason, we can initialize the parameters after we
    update the input tensor information. Also, the bias values will be initialized
    as `0`. The following code shows the implementation of this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/0ed79072-61db-46e8-a6bc-66049715f0ff.png)是来自上一层的输入数量。因此，我们可以在更新输入张量信息后初始化参数。此外，偏置值将被初始化为`0`。以下代码显示了这一实现：'
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, let's cover backward propagation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来讨论反向传播。
- en: Implementing backward propagation
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现反向传播
- en: 'As we discussed earlier, the gradient from the next layer is propagated to
    this layer. Based on propagated gradients, we need to obtain three gradients for
    the weights, biases, and data (gradient of input). We need to create the blobs
    that can store them. Their size does not depend on the batch size, so we just
    need to make sure that we create them. The following code shows how we can create
    blobs for this purpose:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，来自下一层的梯度被传播到这一层。基于传播的梯度，我们需要获得权重、偏置和数据（输入梯度）的三个梯度。我们需要创建可以存储它们的blob。它们的大小不取决于批量大小，所以我们只需要确保创建它们。以下代码显示了我们如何为此目的创建blob：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code, `grad_output_` means the gradients of the output data
    that is propagated from the next layer, and `grad_input_` means the gradients
    of the input data that will be propagated to the previous layer. Therefore, we
    don't need to create a `grad_output_` blob. If you find these naming conventions
    confusing, it may be easier if you regard `grad_input_` as ![](img/84d797c7-3cb7-40e4-8b12-bf9e5be2921e.png) and
    `grad_input_` as ![](img/5dbe0798-7e12-4625-adcb-b2a5044495d2.png).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`grad_output_`表示从下一层传播的输出数据的梯度，`grad_input_`表示将传播到上一层的输入数据的梯度。因此，我们不需要创建`grad_output_`
    blob。如果您觉得这些命名约定令人困惑，也许更容易理解`grad_input_`为![](img/84d797c7-3cb7-40e4-8b12-bf9e5be2921e.png)，`grad_input_`为![](img/5dbe0798-7e12-4625-adcb-b2a5044495d2.png)。
- en: 'The following code shows how we can implement this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了我们如何实现这一点：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can also skip computing the gradients of the input data if this layer is
    the first layer in the model since we don't have to do anything with it.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这一层是模型中的第一层，我们也可以跳过计算输入数据的梯度，因为我们不需要对其进行任何操作。
- en: 'The weight and bias updates will be done when we want to update the weights.
    In this section, we will use **Stochastic Gradient Descent** (**SGD**) for this.
    This operation can be used in other layers as well. Here, we will place this function
    in the `Layer` class. The weight updates can also be done with `cublas` functions,
    as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要更新权重时，将会更新权重和偏置值。在本节中，我们将使用**随机梯度下降**（**SGD**）来实现这一点。这个操作也可以在其他层中使用。在这里，我们将把这个函数放在`Layer`类中。权重更新也可以使用`cublas`函数来完成，如下所示：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, we can update the weights and bias with the learning rate. Of
    course, you can change the `eps` operation to apply other optimization algorithms
    as well.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们可以使用学习率更新权重和偏差。当然，你也可以改变`eps`操作以应用其他优化算法。
- en: Layer termination
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层终止
- en: 'In C/C++ programming, the programmers should cover how to return the used resource
    when it terminates class instances. Following our design, the layer will create
    six blobs at most if they have weights parameters and can update them from the
    gradients. The following code shows the layer termination code, which terminates
    blobs that are created internally:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在C/C++编程中，程序员应该覆盖如何在终止类实例时返回所使用的资源。根据我们的设计，如果层具有权重参数并且可以从梯度中更新它们，该层最多会创建六个blob。以下代码显示了终止blob的层终止代码，这些blob是在内部创建的：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The input blob or tensor descriptors will be handled by other layers or blob
    terminations. The layer class is a base class to the other layers. Therefore,
    we can focus on terminating custom-created resources, because this termination
    code will be called together when we terminate any derived layers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输入blob或张量描述符将由其他层或blob终止处理。层类是其他层的基类。因此，我们可以专注于终止自定义创建的资源，因为当我们终止任何派生层时，这个终止代码将一起被调用。
- en: Even though we have architected the network and the layers, we should develop
    some additional layers to complete the network. For example, we didn't implement
    the activation, softmax, and loss calculation layers. We will cover these layers
    in the upcoming sections.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经设计了网络和层，但我们还应该开发一些额外的层来完成网络。例如，我们没有实现激活、softmax和损失计算层。我们将在接下来的部分中介绍这些层。
- en: Activation layer with cuDNN
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuDNN的激活层
- en: 'There are many element-wise operations in neural network layers. The activation
    function is one of these operations. The cuDNN library provides six activation
    functions: sigmoid, ReLU, tanh, clipped ReLU, ELU, and identity. In the cuDNN
    library, `cudnnActivationForward()` does forward operation and `cudnnActivationBackward()`
    does backward operation.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络层中有许多逐元素操作。激活函数是这些操作之一。cuDNN库提供了六种激活函数：sigmoid、ReLU、tanh、clipped ReLU、ELU和identity。在cuDNN库中，`cudnnActivationForward()`执行前向操作，`cudnnActivationBackward()`执行后向操作。
- en: 'Let''s look at the `cuddnnActivationForward()` function''s interface, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下`cuddnnActivationForward()`函数的接口，如下所示：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Using `cudnnActivationDescriptor_t`, we can determine the types of the activation
    function. Alpha and beta are scalar values that determine the rate of input to
    be added. `xDesc` and `yDesc` hold the tensor's shape information. They can be
    created using `cudnnCreateTensorDescriptor()`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`cudnnActivationDescriptor_t`，我们可以确定激活函数的类型。Alpha和beta是标量值，用于确定要添加的输入速率。`xDesc`和`yDesc`保存张量的形状信息。它们可以使用`cudnnCreateTensorDescriptor()`创建。
- en: 'When you look at the `cudnnActivationBackward()` function, `dy` is the gradient
    input from the next layer and `dx` is the gradient output to the previous layer.
    In this case, `y` becomes the input. In this manner, `dyDesc` provides the gradient
    input shape information while `dxDesc` provides the gradient output shape information:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看`cudnnActivationBackward()`函数时，`dy`是来自下一层的梯度输入，`dx`是输出到上一层的梯度。在这种情况下，`y`变成了输入。这样，`dyDesc`提供了梯度输入形状信息，而`dxDesc`提供了梯度输出形状信息：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In general, we can expect the tensor shape between layers to not change. Due
    to this, we can use the same tensor descriptor for `x` and `dx`. It is the same
    as using `y` and `dy`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们可以期望层之间的张量形状不会改变。因此，我们可以对`x`和`dx`使用相同的张量描述符。这与使用`y`和`dy`是一样的。
- en: Now, let's implement the cuDNN-enabled activation function using the cuDNN API.
    To use the cuDNN API, we need to provide a tensor descriptor to specify the input
    and an output tensor dimension to the cuDNN functions. We also need to specify
    the activation operation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用cuDNN API实现启用cuDNN的激活函数。要使用cuDNN API，我们需要提供一个张量描述符来指定输入和输出张量的维度给cuDNN函数。我们还需要指定激活操作。
- en: Layer configuration and initialization
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层配置和初始化
- en: 'While our example implementation does not use the layer interface, we need
    to integrate our example into the layer interface. In our layer design, the activation
    layer can be implemented like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的示例实现没有使用层接口，但我们需要将我们的示例集成到层接口中。在我们的层设计中，激活层可以这样实现：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'At the initialization step, we need to create several tensor descriptors and
    an activation descriptor. The cuDNN library requires the developers to provide
    a tensor size or any other operational handles corresponding to the API:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化步骤中，我们需要创建几个张量描述符和一个激活描述符。cuDNN库要求开发人员提供与API对应的张量大小或任何其他操作句柄：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In cuDNN, we specify the activation function operation using an activation
    descriptor. We do this with the `cudnnSetActivationDescriptor()` function. Then,
    it can determine the `cudnnActivationForward/Backward()` function''s operation.
    We will cover this in the next section. Before we do that, however, we need to
    implement the class destructor so that it destroys the activation descriptor,
    like so:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在cuDNN中，我们使用激活描述符来指定激活函数操作。我们使用`cudnnSetActivationDescriptor()`函数来实现这一点。然后，它可以确定`cudnnActivationForward/Backward()`函数的操作。我们将在下一节中介绍这一点。然而，在这之前，我们需要实现类析构函数，以便它销毁激活描述符，如下所示：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now, let's cover the activation layer's forward and backward operations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们介绍激活层的前向和后向操作。
- en: Implementing layer operation
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现层操作
- en: This is also known as the caution operation. This layer does not require that
    we handle weights and biases, and so it is simpler to implement than the dense
    layer.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为警告操作。这个层不需要我们处理权重和偏差，因此比密集层更容易实现。
- en: Implementing forward propagation
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现前向传播
- en: 'At the first iteration, we need to initialize the input descriptors, output
    descriptors, and output blob. We will update the output blob when the batch size
    is changed. However, we don''t have to initialize the weights and bias because
    it doesn''t have those. The following code shows its implementation:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代中，我们需要初始化输入描述符、输出描述符和输出blob。当批处理大小改变时，我们将更新输出blob。然而，我们不需要初始化权重和偏差，因为它们没有。以下代码显示了它的实现：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After initialization, we use the `cudnnActivationForward()` function in cuDNN
    for the activation process, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化后，我们使用cuDNN中的`cudnnActivationForward()`函数进行激活过程，如下所示：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This activation function's operation is determined when we initialize this layer,
    as we discussed earlier.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个激活函数的操作是在我们初始化这个层时确定的，正如我们之前讨论的。
- en: Implementing backward propagation
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现反向传播
- en: 'The next step is to implement backward propagation. We will reuse the input/output
    tensor descriptors we already have. Now, we have to initialize the gradients we
    wish to back-propagate:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是实现反向传播。我们将重用我们已经拥有的输入/输出张量描述符。现在，我们必须初始化我们希望反向传播的梯度：
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After the initialization, we can call the `cudnnActivationBackward()` function,
    as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化后，我们可以调用`cudnnActivationBackward()`函数，如下所示：
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that we reuse the input tensor descriptor and output tensor descriptor
    that we created in the forward pass. We can do this because the activation operation
    does not change the tensor's size. We could simplify our implementation by using
    the cuDNN API in activating backward propagation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们重用了在前向传递中创建的输入张量描述符和输出张量描述符。我们之所以能够这样做，是因为激活操作不会改变张量的大小。我们可以通过在激活反向传播中使用cuDNN
    API来简化我们的实现。
- en: The output of the `cudnnActivationBackward()` function is `d_grad_input`. As
    we described in the previous section, this gradient will be passed to the lower
    layer.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudnnActivationBackward()`函数的输出是`d_grad_input`。正如我们在前一节中描述的，这个梯度将传递给下一层。'
- en: Now, we will implement the softmax layer and integrate our layer implementations
    as a network. Then, we will discuss the fully connected layer's accuracy in the
    image classification task.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现softmax层，并将我们的层实现集成为一个网络。然后，我们将讨论图像分类任务中全连接层的准确性。
- en: Softmax and loss functions in cuDNN/CUDA
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cuDNN/CUDA中的softmax和损失函数
- en: 'For the MNIST dataset classification, we will use the softmax classifier. The
    softmax function normalizes the inputs and generates the probability distribution
    of ![](img/6a23c4d0-c4cb-4caa-b483-128315b59c21.png) probabilities. The softmax
    operation can be denoted as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MNIST数据集分类，我们将使用softmax分类器。softmax函数对输入进行归一化，并生成![](img/6a23c4d0-c4cb-4caa-b483-128315b59c21.png)概率的概率分布。softmax操作可以表示如下：
- en: '![](img/f6b6d132-a44b-4e74-ade8-f2dd5d043ea5.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6b6d132-a44b-4e74-ade8-f2dd5d043ea5.png)'
- en: cuDNN's softmax forward function supports this operation, along with the channels
    and all the instances. Previously, we aligned the dense layer's output with the
    channels. Therefore, we will apply the softmax operation along with the channels.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: cuDNN的softmax前向函数支持此操作，以及通道和所有实例。之前，我们将密集层的输出与通道对齐。因此，我们将沿着通道应用softmax操作。
- en: 'To confirm that our training is done effectively, we need to calculate the
    loss function. The softmax loss function is called cross-entropy loss since its
    loss function is used to obtain loss across ![](img/0adcd213-509d-4129-8488-317b8bc434e8.png)
    probabilities. The loss function is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认我们的训练有效完成，我们需要计算损失函数。由于softmax损失函数用于获取跨![](img/0adcd213-509d-4129-8488-317b8bc434e8.png)概率的损失，所以softmax损失函数被称为交叉熵损失。损失函数如下：
- en: '![](img/28d7a607-4b3b-400a-9382-74a6107c6d57.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28d7a607-4b3b-400a-9382-74a6107c6d57.png)'
- en: 'We need to obtain the gradient of this softmax loss to update the neural networks.
    Fortunately, the gradient of softmax loss is simple after the derivation, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要获得这个softmax损失的梯度以更新神经网络。幸运的是，softmax损失的梯度在求导后很简单，如下所示：
- en: '![](img/07eee5af-eac3-444a-b542-9acdf05f231e.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07eee5af-eac3-444a-b542-9acdf05f231e.png)'
- en: For the forward operation, we will use the cuDNN function to get the softmax's
    output. To obtain gradients, having a custom operation is more intuitive and simple.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前向操作，我们将使用cuDNN函数来获取softmax的输出。为了获得梯度，拥有自定义操作更直观和简单。
- en: Implementing the softmax layer
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现softmax层
- en: Now, let's see how the softmax layer can be implemented using cuDNN and CUDA
    code.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用cuDNN和CUDA代码来实现softmax层。
- en: Implementing forward propagation
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现前向传播
- en: 'We can obtain the softmax cost function''s outputs using `cudnnSoftmaxForward()`
    from the cuDNN library:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用cuDNN库中的`cudnnSoftmaxForward()`来获得softmax成本函数的输出：
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: One of the most important parameter settings to use in this situation is `CUDNN_SOFTMAX_MODE_CHANNEL`.
    This option enables channel-level softmax operations following the input tensor
    descriptor information. By doing this, we can provide tensors that have been aligned
    by channels from mini-batch inputs from the dense layer.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下使用的最重要的参数设置之一是`CUDNN_SOFTMAX_MODE_CHANNEL`。此选项使得在输入张量描述符信息后面进行通道级别的softmax操作。通过这样做，我们可以提供已经通过密集层的小批量输入按通道对齐的张量。
- en: Implementing backward propagation
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现反向传播
- en: 'The backward pass in the softmax layer is different from other layer implementation.
    This operation takes the labels of the input data as input and obtains the appropriate
    gradients. As we discussed earlier, the gradients of the softmax loss can be obtained
    using the following equation:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: softmax层的反向传递与其他层的实现不同。这个操作将输入数据的标签作为输入，并获得适当的梯度。正如我们之前讨论的，softmax损失的梯度可以使用以下方程获得：
- en: '![](img/4e9ecd44-3c23-4f2a-a26b-2e69c5e10894.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e9ecd44-3c23-4f2a-a26b-2e69c5e10894.png)'
- en: 'We can implement this operation using `cublasSaxpy()`, as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`cublasSaxpy()`来实现这个操作，如下所示：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In the preceding code, the target blob contains one-hot-encoded target vectors,
    so adding negative target vectors to the predicted values produces the appropriate
    gradients. After that, we need to normalize batch gradients ahead of propagation
    to the previous layer, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，目标blob包含了one-hot编码的目标向量，因此将负目标向量添加到预测值中会产生适当的梯度。之后，我们需要在传播到前一层之前对批次梯度进行归一化，如下所示：
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Since this introduces the mean of the weighted sum, we can expect that the gradients
    of each batch are normalized.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这引入了加权和的均值，我们可以期望每个批次的梯度被归一化。
- en: Implementing the loss function
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现损失函数
- en: Calculating the loss value of softmax is optional. This means its value is not
    accounted for in training and inference. However, we can use this as an indicator
    of the training.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 计算softmax的损失值是可选的。这意味着它的值在训练和推断中不被考虑。然而，我们可以将其用作训练的指标。
- en: 'The softmax loss function should implement the following equation, as we discussed
    earlier:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前讨论的，softmax损失函数应该实现以下方程：
- en: '![](img/9e2f16bd-b541-4709-9d49-02e21c9a5aed.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e2f16bd-b541-4709-9d49-02e21c9a5aed.png)'
- en: 'We can obtain the loss from each sample''s output and cumulate them using a
    kernel function, as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个核函数从每个样本的输出中获得损失并累积它们，如下所示：
- en: '[PRE27]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This operation uses parallel reduction, which we covered in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml),
    *CUDA Thread Programming*, to obtain the cumulated loss value in a batch. Since
    we will just use this reduced loss value to confirm the training, we will simply
    monitor its output rather than taking its average.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作使用并行归约，在[第3章](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml) *CUDA线程编程*中介绍过，用于获取一个批次中的累积损失值。由于我们只会使用这个减少的损失值来确认训练，所以我们只会监视它的输出而不是取平均值。
- en: Now, let's integrate all the layers we have implemented with an MNIST dataset
    loader.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将我们实现的所有层与MNIST数据集加载器集成在一起。
- en: MNIST dataloader
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST数据加载器
- en: One of the important parts of this entire process is having a dataloader for
    a specific dataset. In this lab, we will use the MNIST dataset, which contains
    60,000 samples. When it comes to initialization, we tell the data loader whether
    it should load either the train or test set. After that, the data loader will
    load some magic numbers in the dataset, along with all the samples and their labels.
    The loaded data will be stored in vectors and shuffled with the same random seed.
    Since the data loader builds and shuffles the sample vector, the training loop
    or test loop may get randomized input data for each iteration. The fully implemented
    code can be found in the `src/mnist.cpp` file in this book's GitHub repository.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程中一个重要的部分是为特定数据集创建一个数据加载器。在这个实验室中，我们将使用包含60,000个样本的MNIST数据集。在初始化时，我们告诉数据加载器它应该加载训练集还是测试集。之后，数据加载器将加载数据集中的一些魔术数字，以及所有样本和它们的标签。加载的数据将被存储在向量中，并使用相同的随机种子进行洗牌。由于数据加载器构建和洗牌样本向量，训练循环或测试循环可能会在每次迭代时获得随机化的输入数据。完整的实现代码可以在本书的GitHub存储库中的`src/mnist.cpp`文件中找到。
- en: Managing and creating a model
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理和创建模型
- en: 'When we have multiple layers, we need an object that can manage those layers
    with neural network operations, that is, forward/backward propagation and weight
    updates. In this lab, we will have an array of layers and iterate the array for
    forward processing. For example, the forward operation can be performed with the
    following code:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有多个层时，我们需要一个可以管理这些层的对象，进行神经网络操作，即前向/后向传播和权重更新。在这个实验室中，我们将有一个层的数组，并迭代数组进行前向处理。例如，前向操作可以用以下代码执行：
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Backward propagation can also be done by iterating over the array in reverse
    order:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播也可以通过以相反顺序迭代数组来完成：
- en: '[PRE29]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As you can see, we manage the layers in the vector and have each layer''s operations.
    Adding a new layer into the network is even simpler, as shown in the following
    code:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们在向量中管理层，并具有每个层的操作。将新层添加到网络中甚至更简单，如下面的代码所示：
- en: '[PRE30]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'By using the `Network` class, we can use various model management functions,
    such as parameter updates, layer registration, layers initialization, and so on.
    Also, we can build a neural network like a modern deep learning framework. For
    example, we can create a model as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`Network`类，我们可以使用各种模型管理函数，如参数更新，层注册，层初始化等。此外，我们可以构建一个像现代深度学习框架一样的神经网络。例如，我们可以创建一个模型如下：
- en: '[PRE31]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can also have the following training loop:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以有以下训练循环：
- en: '[PRE32]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'For the testing phase, we create another dataset loader for the test dataset
    and only iterate with the forward pass. The following code shows its implementation:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试阶段，我们为测试数据集创建另一个数据集加载器，并只进行前向传播的迭代。以下代码显示了它的实现：
- en: '[PRE33]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the testing phase, we will obtain the accuracy after we finish testing all
    the samples in the testing dataset. Now, we need to obtain the accuracy after
    the testing loop.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试阶段，我们将在完成对测试数据集中所有样本的测试后获得准确率。现在，我们需要在测试循环之后获得准确率。
- en: Network training with the MNIST dataset
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MNIST数据集进行网络训练
- en: 'Now, let''s run our implemented code and see its result. For the training phase,
    we will iterate 2,400 steps with a batch size of 256\. The MNIST dataset has 60,000
    samples in the training set. 2,400 steps means that we will take the iteration
    of about 10 epochs. The sample code can be compiled with the following command:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行我们实现的代码并查看其结果。对于训练阶段，我们将迭代2,400步，批量大小为256。MNIST数据集在训练集中有60,000个样本。2,400步意味着我们将进行大约10个epochs的迭代。样本代码可以用以下命令编译：
- en: '[PRE34]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following screenshot shows the training and test output of our implementation:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了我们实现的训练和测试输出：
- en: '![](img/1308649b-62e6-4f67-a98f-876d835049b2.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1308649b-62e6-4f67-a98f-876d835049b2.png)'
- en: In the training iteration, the network achieved 92 percent accuracy from the
    training dataset. However, the testing accuracy is only 77 percent, which is a
    relatively low score against the training result. There can be many reasons why
    inferencing shows a large gap in accuracy between training and inference. One
    possible reason is that the fully connected layer does not consider the regional
    information that's shown in the preceding screenshot. In deep learning, we use
    a convolutional layer to make the network learn about the spacial information.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练迭代中，网络从训练数据集中获得了92%的准确率。然而，测试准确率只有77%，这与训练结果相比是一个相对较低的分数。推断显示训练和推断之间的准确率差距很大可能有很多原因。一个可能的原因是全连接层没有考虑到前面截图中显示的区域信息。在深度学习中，我们使用卷积层来使网络学习空间信息。
- en: Now, let's implement the convolutional layer with cuDNN, add this to the network,
    and compare the model's performance.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用cuDNN实现卷积层，将其添加到网络中，并比较模型的性能。
- en: Convolutional neural networks with cuDNN
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuDNN的卷积神经网络
- en: The cuDNN library provides optimized performance for convolutional operations.
    By creating a convolutional layer, we will cover the API's configuration for the
    forward and backward operations.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: cuDNN库为卷积操作提供了优化的性能。通过创建一个卷积层，我们将覆盖API的配置，用于前向和后向操作。
- en: The convolutional network layer performs convolution to the input data with
    its weights. This network architecture is useful when you want to build a neural
    network that's aware of regional information. Recall from the convolution implementation
    in [Chapter 7](71d77c43-0064-491e-9b43-307a05bd6555.xhtml), *Parallel Programming
    Patterns in CUDA*, that it needs considerable memory bandwidth and requires further
    optimization to get optimal performance. However, using the cuDNN library, we
    can obtain the best performance as well since we don't have to reinvent the wheel.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络层对输入数据进行卷积处理。当你想要构建一个了解区域信息的神经网络时，这种网络架构是很有用的。回想一下，在[第7章](71d77c43-0064-491e-9b43-307a05bd6555.xhtml)中的卷积实现，*CUDA中的并行编程模式*，它需要相当大的内存带宽，并需要进一步优化以获得最佳性能。然而，使用cuDNN库，我们也可以获得最佳性能，因为我们不必重新发明轮子。
- en: 'The implementation of a convolutional layer is similar to the fully connected
    layer implementation. There are two differences, however, thanks to the cuDNN
    library: we don''t have to fully implement as much detail as we did previously
    and we need to allocate a workspace size for the operation. For each convolution
    operation – forward, backward for the filter, and backward for the input – extra
    memory space is needed, depending on their algorithm. The algorithm can vary following
    the given input/output/filter tensor dimensions. The detailed API call will be
    handled later.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的实现与全连接层的实现类似。然而，由于cuDNN库的存在，有两个不同之处：我们不必像以前那样完全实现那么多细节，我们需要为操作分配一个工作空间大小。对于每个卷积操作——前向、反向滤波器和反向输入——都需要额外的内存空间，取决于它们的算法。算法可以根据给定的输入/输出/滤波器张量维度而变化。详细的API调用将在稍后处理。
- en: 'Like other layers, it has three work phases. For the inference phases, we will
    call `cudnnConvolutionForward()` and `cudnnAddTensor()`. For the backward phase,
    we will call `cudnnConvolutionBackwardData()`, `cudnnConvolutionBackwardFilter()`,
    and `cudnnConvolutionBackwardBias()`. Finally, for the update phase we can reuse
    the code from the fully connected layers. An overview of the layer''s configuration
    is as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他层一样，它有三个工作阶段。对于推理阶段，我们将调用`cudnnConvolutionForward()`和`cudnnAddTensor()`。对于反向阶段，我们将调用`cudnnConvolutionBackwardData()`、`cudnnConvolutionBackwardFilter()`和`cudnnConvolutionBackwardBias()`。最后，对于更新阶段，我们可以重用全连接层的代码。该层的配置概述如下：
- en: '![](img/dfbcecb1-0c13-4d9e-a0a3-ac60623461a2.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: 实现前向传播
- en: 'In deep learning neural networks, it is common to use a pooling layer along
    with the convolutional network. Pooling layers simply select input data to output
    following a simple rule. The following diagram shows examples of max-pooling:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习神经网络中，通常会与卷积网络一起使用池化层。池化层只是根据简单的规则选择输入数据进行输出。以下图示显示了最大池化的例子：
- en: '![](img/e821b3b9-c7ff-4bdc-817a-1b7c5fb97325.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e821b3b9-c7ff-4bdc-817a-1b7c5fb97325.png)'
- en: Using the cuDNN library, we will implement these two convolution operations.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 使用cuDNN库，我们将实现这两个卷积操作。
- en: The convolution layer
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: 'Like a fully connected layer, this convolution layer has weights and biases
    parameters. In the fully connected layer, we used cuBLAS and it does not require
    cuDNN-related descriptors. However, we will be using cuDNN convolution functions,
    and so we need to use a filter descriptor and convolution operation descriptor.
    The following code shows what resources we should initialize while the layer is
    being constructed:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 与全连接层类似，这个卷积层有权重和偏置参数。在全连接层中，我们使用了cuBLAS，它不需要cuDNN相关的描述符。然而，我们将使用cuDNN卷积函数，因此需要使用滤波器描述符和卷积操作描述符。以下代码显示了在构建层时应该初始化的资源：
- en: '[PRE35]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Since we provide convolution operation information when the model is constructed,
    we can specify the convolution descriptor. However, the filter's operation can
    be specified at inference time since we can learn the input tensor's size at that
    time. Now, let's implement the forward pass in the convolution layer.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在模型构建时提供了卷积操作信息，我们可以指定卷积描述符。然而，滤波器的操作可以在推断时指定，因为我们可以在那时学习输入张量的大小。现在，让我们实现卷积层的前向传递。
- en: Implementing forward propagation
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '![](img/dfbcecb1-0c13-4d9e-a0a3-ac60623461a2.png)'
- en: 'As we discussed earlier, we can initialize the convolution layer with the input
    tensor size. This input tensor size makes an impact on the output tensor''s size.
    The following code shows the parameter initialization step in the forward pass:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，我们可以用输入张量大小初始化卷积层。这个输入张量大小会影响输出张量的大小。以下代码显示了前向传递中的参数初始化步骤：
- en: '[PRE36]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we need to update the input resources, initialize the output blob, create
    a cuDNN workspace, and initialize the weights parameters, as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要更新输入资源，初始化输出blob，创建cuDNN工作空间，并初始化权重参数，如下所示：
- en: '[PRE37]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: To obtain the output tensor size, we use the `cudnnGetConvolution2dForwardOutputDim()`
    function. This function outputs dimensional size information based on the input
    tensor size, the convolution operation, and the filter size. Then, we reuse the
    same parameter initialization code that we used in the fully connected layer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得输出张量大小，我们使用`cudnnGetConvolution2dForwardOutputDim()`函数。该函数根据输入张量大小、卷积操作和滤波器大小输出维度大小信息。然后，我们重用了在全连接层中使用的相同参数初始化代码。
- en: To call cuDNN's convolution APIs, we need to provide its working algorithm and
    workspace memory. We do this because cuDNN selects the optimal convolution algorithm
    based on the convolution size, and its measurement needs to be done immediately.
    When the algorithm is determined, cuDNN can determine the workspace size. The
    convolutional layer needs to have the convolution operation for the forward pass,
    gradients of input data, and gradients of weights. We need to handle each algorithm
    individually, but we can allocate just one workspace because the workspace is
    used for each convolution operation exclusively.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 要调用cuDNN的卷积API，我们需要提供其工作算法和工作空间内存。我们这样做是因为cuDNN根据卷积大小选择最佳卷积算法，并且需要立即进行测量。确定算法后，cuDNN可以确定工作空间大小。卷积层需要进行前向传播的卷积操作、输入数据的梯度和权重的梯度。我们需要分别处理每个算法，但我们可以分配一个工作空间，因为工作空间专门用于每个卷积操作。
- en: 'So, we create the workspace with the maximum size among each convolution algorithm
    workspace size that''s required. The following code shows how we can use them
    and manage the workspace:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们创建的工作空间需要具有每个卷积算法所需的最大大小。以下代码显示了我们如何使用它们并管理工作空间：
- en: '[PRE38]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Each convolution algorithm is specified with individual types, that is, `cudnnConvolutionFwdAlgo_t`, `cudnnConvolutionBwdDataAlgo_t`,
    and `cudnnConvolutionBwdFilterAlgo_t`. We can use them by declaring them as class
    member variables, that is,  `conv_fwd_algo_`, `conv_bwd_data_algo_`, and `conv_bwd_filter_algo_`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷积算法都使用单独的类型进行指定，即`cudnnConvolutionFwdAlgo_t`、`cudnnConvolutionBwdDataAlgo_t`和`cudnnConvolutionBwdFilterAlgo_t`。我们可以通过将它们声明为类成员变量来使用它们，即`conv_fwd_algo_`、`conv_bwd_data_algo_`和`conv_bwd_filter_algo_`。
- en: 'Now, we write the forward processing code after initialization. We do convolution
    with the filter and add a bias. The following code shows the cuDNN convolution
    forward implementation:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在初始化后，我们编写前向处理代码。我们使用滤波器进行卷积并添加偏差。以下代码显示了cuDNN卷积前向实现：
- en: '[PRE39]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The result of convolution will be passed to the next layer using the output
    blob.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的结果将使用输出blob传递到下一层。
- en: Implementing backward propagation
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现反向传播
- en: 'In back-propagation, we should compute the gradients of the bias, the gradients
    of weights, and the gradients of the input data. To do this, we need to create
    blobs at the first iteration so that we can store them. Their size does not depend
    on the batch size, so we just need to make sure they are created. The initialization
    step can be implemented as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中，我们应该计算偏差的梯度、权重的梯度和输入数据的梯度。为此，我们需要在第一次迭代中创建blob以便我们可以存储它们。它们的大小不取决于批处理大小，所以我们只需要确保它们被创建。初始化步骤可以实现如下：
- en: '[PRE40]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Then, we call the cuDNN backward convolution APIs, as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们调用cuDNN反向卷积API，如下所示：
- en: '[PRE41]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Then, we pass the gradients of the input data to the previous layer to propagate
    the gradients. We will update the gradients of the weights and biases at the update
    step by using the base class' gradients update code. We covered this when we implemented
    backward propagation in the fully connected layer. We also can skip computing
    the gradients of the input data if this is the first layer.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将输入数据的梯度传递给前一层以传播梯度。在更新步骤中，我们将使用基类的梯度更新代码来更新权重和偏差的梯度。在全连接层中实现反向传播时，我们已经涵盖了这一点。如果这是第一层，则我们也可以跳过计算输入数据的梯度。
- en: Pooling layer with cuDNN
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuDNN的池化层
- en: The pooling layer has two features. First, its output size is different compared
    to the convolution layer and cuDNN provides the corresponding API for this. Second,
    it does not have any internal weights.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层有两个特点。首先，它的输出大小与卷积层不同，cuDNN为此提供了相应的API。其次，它没有任何内部权重。
- en: 'To specify the pooling operation, we can use cuDNN''s `cudnnPoolingDescriptor_t` function
    and create and specify the cuDNN''s pooling descriptor in the class constructor,
    as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指定池化操作，我们可以使用cuDNN的`cudnnPoolingDescriptor_t`函数，并在类构造函数中创建和指定cuDNN的池化描述符，如下所示：
- en: '[PRE42]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now, let's implement the forward and backward operation of the pooling layer.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现池化层的前向和反向操作。
- en: Implementing forward propagation
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现前向传播
- en: 'The pooling layer contributes to reducing the tensor''s size. Due to this,
    we need to compute the output size. We can compute the size using the `cudnnGetPooling2dForwardOutputDim()`
    function, like we did in the convolution layer implementation. Also, the tensor
    size depends on the batch size. This means we need to update the tensor size if
    the batch size is changed. The following code shows how we can initialize the
    input and output blobs:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层有助于减小张量的大小。因此，我们需要计算输出大小。我们可以使用`cudnnGetPooling2dForwardOutputDim()`函数来计算大小，就像我们在卷积层实现中所做的那样。此外，张量大小取决于批处理大小。这意味着如果批处理大小发生变化，我们需要更新张量大小。以下代码显示了我们如何初始化输入和输出blob：
- en: '[PRE43]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'For the forward pass, we call the `cudnnPoolingForward()` function, as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前向传播，我们调用`cudnnPoolingForward()`函数，如下所示：
- en: '[PRE44]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Implementing backward propagation
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现反向传播
- en: 'For the back-propagation step, we call the `cudnnPoolingBackward()` function,
    as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对于反向传播步骤，我们调用`cudnnPoolingBackward()`函数，如下所示：
- en: '[PRE45]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The pooling layer's tensor shape of inputs and gradients of inputs are same
    and the shape of outputs and gradients of outputs are same. Therefore, we can
    reuse the tensor descriptors respectively of inputs and outputs.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层的张量形状的输入和梯度的输入是相同的，输出和梯度的输出的形状也是相同的。因此，我们可以分别重用输入和输出的张量描述符。
- en: Now, let's integrate these into a single convolutional layer implementation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这些集成到单个卷积层实现中。
- en: Network configuration
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络配置
- en: 'Now, we will update our previous network, LeNet. The network code can be written
    as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将更新我们之前的网络LeNet。网络代码可以编写如下：
- en: '[PRE46]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, we can start the training and inference stages since we have configured
    our layers so that they''re connected to each other. Let''s compile the code with
    the following command:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始训练和推断阶段，因为我们已经配置了我们的层，使它们彼此连接。让我们使用以下命令编译代码：
- en: '[PRE47]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then, we can see the training and test result as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以看到训练和测试结果如下：
- en: '![](img/7bddab01-eac8-4ff6-9222-861e7e99c72a.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bddab01-eac8-4ff6-9222-861e7e99c72a.png)'
- en: 'As you can see, the network achieved higher training accuracy and inferencing
    than when it used the fully connected network only. We also can confirm its operation
    by looking at the NVIDIA profile, as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，该网络的训练准确度和推断准确度都比仅使用全连接网络时要高。我们还可以通过查看NVIDIA配置文件来确认其操作，如下所示：
- en: '![](img/60c3e92b-b87f-49e5-8a22-012f0da4be45.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60c3e92b-b87f-49e5-8a22-012f0da4be45.png)'
- en: Mixed precision operations
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合精度操作
- en: The latest NVIDIA GPUs support mixed precision operation deep learning. We will
    not cover this in this book as it is outside of our scope. However, you can access
    the example that's provided by NVIDIA at `/usr/src/cudnn_samples_v7/conv_sample` if
    you wish to learn more. To access this example, you need to download the sample
    from the cuDNN web page. This example code shows how to use mixed precision operations
    using the cuDNN library.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的NVIDIA GPU支持深度学习的混合精度操作。我们不会在本书中涵盖这一点，因为它超出了我们的范围。但是，如果您希望了解更多，可以访问NVIDIA提供的示例，位于`/usr/src/cudnn_samples_v7/conv_sample`。要访问此示例，您需要从cuDNN网页下载示例。此示例代码显示了如何使用cuDNN库进行混合精度操作。
- en: 'To have the cuDNN APIs work with the tensor cores, we need to set the math
    type, as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使cuDNN API与张量核心一起工作，我们需要设置数学类型，如下所示：
- en: '[PRE48]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Then, we need to initialize the tensor descriptors of the input and output tensors
    using `cudnnSetTensorNdDescriptor()`. This provides padding for the tensors so
    that we get optimized tensor core performance.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要使用`cudnnSetTensorNdDescriptor()`初始化输入和输出张量的张量描述符。这为张量提供填充，以便我们获得优化的张量核心性能。
- en: One good cuDNN-based implementation is `cudnn-training`: [https://github.com/tbennun/cudnn-training](https://github.com/tbennun/cudnn-training).
    It implements LeNet as a sequence of cuDNN functions. You can follow each line
    to see how the CUDNN functions work.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的基于cuDNN的实现是`cudnn-training`：[https://github.com/tbennun/cudnn-training](https://github.com/tbennun/cudnn-training)。它将LeNet实现为一系列cuDNN函数。您可以跟踪每一行，看看CUDNN函数是如何工作的。
- en: If you are interested in deploying your network using cuDNN, please check out
    the following video about GTC-CNN inference with cuDNN ([https://developer.nvidia.com/gtc/2019/video/S9644/video](https://developer.nvidia.com/gtc/2019/video/S9644/video)). This
    talk introduces useful performance optimization tricks on CNN inference using
    cuDNN.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣使用cuDNN部署您的网络，请查看以下关于GTC-CNN推断与cuDNN的视频（[https://developer.nvidia.com/gtc/2019/video/S9644/video](https://developer.nvidia.com/gtc/2019/video/S9644/video)）。这个讲座介绍了使用cuDNN进行CNN推断的有用性能优化技巧。
- en: Using half-precision in deep learning training requires more than FP16 operations
    utilization. We need to compute tensors in FP16 while we maintain the weights
    in FP32\. Also, some operations require FP32\. We call this the mixed precision.
    The cuDNN library provides a mixed precision inference example named mnistCUDNN.
    This example shows the conversion of input and layer data types. If you want to
    learn more about the mixed precision operation in deep learning and training,
    please read the following article: [https://devblogs.nvidia.com/video-mixed-precision-techniques-tensor-cores-deep-learning/](https://devblogs.nvidia.com/video-mixed-precision-techniques-tensor-cores-deep-learning/).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习训练中使用半精度需要超过FP16操作的利用率。我们需要在FP16中计算张量，同时将权重保持在FP32中。此外，一些操作需要FP32。我们称之为混合精度。cuDNN库提供了一个名为mnistCUDNN的混合精度推断示例。该示例显示了输入和层数据类型的转换。如果您想了解更多关于深度学习和训练中混合精度操作的信息，请阅读以下文章：[https://devblogs.nvidia.com/video-mixed-precision-techniques-tensor-cores-deep-learning/](https://devblogs.nvidia.com/video-mixed-precision-techniques-tensor-cores-deep-learning/)。
- en: Now, we will cover other GPU use considerations in deep learning in terms of
    performance.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将从性能方面讨论深度学习中的其他GPU使用注意事项。
- en: Recurrent neural network optimization
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络优化
- en: RRNs allow you to analyze sequential data in deep learning. Although this network
    has sequential dependencies, there's plenty of room for optimization. In this
    section, we will cover its algorithm and how cuDNN provides optimized performance.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: RRN允许您在深度学习中分析顺序数据。尽管该网络具有顺序依赖性，但仍有大量的优化空间。在本节中，我们将介绍其算法以及cuDNN如何提供优化性能。
- en: 'There are many kinds of RNNs, but cuDNN only supports four, that is, RNN with
    ReLU, RNN with tanh, LSTM, and GRU. They have two inputs: the hidden parameters
    from the previous network and the input from the source. Depending on their types,
    they have different operations. In this lab, we will cover the LSTM operation. The
    following diagram shows the forward operation of the LSTM:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多种类型的RNN，但cuDNN只支持四种，即带有ReLU的RNN，带有tanh的RNN，LSTM和GRU。它们有两个输入：来自先前网络的隐藏参数和来自源的输入。根据它们的类型，它们有不同的操作。在本实验室中，我们将介绍LSTM操作。下图显示了LSTM的前向操作：
- en: '![](img/674b1b76-8aa5-44b7-a7cd-dada656f74b0.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/674b1b76-8aa5-44b7-a7cd-dada656f74b0.png)'
- en: From a computing perspective, there are eight matrix-matrix multiplications
    and many element-wise operations. From this estimation, we can expect that LSTM
    could be memory-bounded since each operation is memory-bounded. On the other hand,
    CUDNN provides the `cudnnRNNForwardInference()` and `cudnnRNNFowardTraining()` RNN
    functions. We will cover the benefits of using this function by measuring the
    performance of this function and simulated LSTM. To do this, we will implement
    a virtual LSTM layer and compare its performance to the cuDNN LSTM function.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算的角度来看，有八个矩阵-矩阵乘法和许多逐元素操作。根据这个估计，我们可以期望LSTM可能是内存受限的，因为每个操作都是内存受限的。另一方面，CUDNN提供了`cudnnRNNForwardInference()`和`cudnnRNNFowardTraining()`RNN函数。我们将通过测量这个函数的性能和模拟LSTM的性能来介绍使用这个函数的好处。为了做到这一点，我们将实现一个虚拟的LSTM层，并将其性能与cuDNN
    LSTM函数进行比较。
- en: 'For test purposes, we will set the hyperparameter like so:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试目的，我们将设置超参数如下：
- en: '[PRE49]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The sequence length or hidden size can vary, depending on the problem. In this
    test, we will use `512` as the length, which is used a lot in sequence research.
    The CUDNN API requires more options to work, such as dropout rate, bidirectional
    or unidirectional, and persistent RNNs. We will only test the vanilla LSTM in
    this section.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 序列长度或隐藏大小可能会有所不同，这取决于问题。在这个测试中，我们将使用`512`作为长度，在序列研究中经常使用。CUDNN API需要更多的选项才能工作，比如dropout率、双向或单向以及持久RNN。在本节中，我们只测试vanilla
    LSTM。
- en: Using the CUDNN LSTM operation
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDNN LSTM操作
- en: 'Let''s write some code that executes the `cudnnRNNForwardTraining()` function
    as an LSTM layer:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一些执行`cudnnRNNForwardTraining()`函数作为LSTM层的代码：
- en: 'We need to initialize the input and output memory space. To execute cuDNN''s
    RNN API, we need to use the following variables:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要初始化输入和输出内存空间。为了执行cuDNN的RNN API，我们需要使用以下变量：
- en: '[PRE50]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'These variables are the inputs and outputs of LSTM. To provide the inputs and
    get the outputs, we need to allocate the appropriate memory space. Following the
    LSTM definition, we need to consider the length of the input, output, and hidden
    layers. These sizes can be determined as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变量是LSTM的输入和输出。为了提供输入和获取输出，我们需要分配适当的内存空间。根据LSTM的定义，我们需要考虑输入、输出和隐藏层的长度。这些大小可以确定如下：
- en: '[PRE51]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Then, we can allocate memory for each item.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以为每个项目分配内存。
- en: 'Now, we need to set up the tensor descriptors for the cuDNN RNN API. The following
    code shows the required tensor descriptors that we should set up:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要为cuDNN RNN API设置张量描述符。以下代码显示了我们应该设置的所需张量描述符：
- en: '[PRE52]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: For the input and output descriptors, we need to initialize each element, that
    is, the batch size and its input size. The other hidden tensor descriptors are
    initialized with the number of layers, the batch size, and the hidden size. This
    section will not cover how to write the initialization code. However, you can
    check out the code in the `10_deep_learning/03_rnn` file if you want to find out
    more.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入和输出描述符，我们需要初始化每个元素，即批量大小和其输入大小。其他隐藏的张量描述符是用层数、批量大小和隐藏大小进行初始化的。本节不涵盖如何编写初始化代码。但是，如果您想了解更多信息，可以查看`10_deep_learning/03_rnn`文件中的代码。
- en: 'We also have to provide a workspace for the RNN operation, just like we did
    for the convolution operation:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要为RNN操作提供一个工作空间，就像我们为卷积操作做的那样：
- en: '[PRE53]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Then, we can set the filter descriptor based on the workspace''s size, as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以根据工作空间的大小设置滤波器描述符，如下所示：
- en: '[PRE54]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can measure their performance using `cudaEvnetRecoard()` and flops computation.
    For example, the forward operation can be configured with the following equation:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`cudaEvnetRecoard()`和flops计算来衡量它们的性能。例如，前向操作可以配置为以下方程：
- en: '![](img/b385c8ec-d143-4f0e-9cc6-d31523159f38.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b385c8ec-d143-4f0e-9cc6-d31523159f38.png)'
- en: Then, we will test our implementation by changing the batch size from 32 to
    256 by increasing the size with 32\. The applicable test range can be different,
    as well as the GPU's memory size.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过将批量大小从32增加到256来测试我们的实现，每次增加32。适用的测试范围可能会有所不同，以及GPU的内存大小。
- en: In this section, we implemented the LSTM-based simulation and `cudnnRNNForwardTraining()` call.
    Our partially simulated version only has GEMM operations, which are the most compute-intensive.
    Now, let's compare the performance of these implementations.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实现了基于LSTM的模拟和`cudnnRNNForwardTraining()`调用。我们部分模拟的版本只有GEMM操作，这是最计算密集的操作。现在，让我们比较这些实现的性能。
- en: Implementing a virtual LSTM operation
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现虚拟LSTM操作
- en: In our implementation, we will focus on simulating LSTM's major operations rather
    than fully implementing it.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们将专注于模拟LSTM的主要操作，而不是完全实现它。
- en: 'Let''s determine the hyperparameters of the LSTM network. In general, the input
    sequence length ranges from 512 to 2,048\. The number of layers varies. However,
    it cannot be large due to *tanh* operations. For the input size, we will use 512\.
    Usually, the batch size is between 32 and 256 in terms of RNN usage. CUDNN requires
    more inputs about the dropout rate, bidirectional or unidirectional, and whether
    we''re using a persistent RNN or not. We''re just not using them right now. Our
    LSTM configuration information is as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确定LSTM网络的超参数。一般来说，输入序列长度范围从512到2,048。层数的数量是不确定的。但是，由于*tanh*操作，它不能太大。对于输入大小，我们将使用512。通常情况下，批量大小在RNN使用方面在32到256之间。CUDNN需要更多关于dropout率、双向或单向以及是否使用持久RNN的输入。我们现在不使用它们。我们的LSTM配置信息如下：
- en: '![](img/652a450a-efec-4f75-9b83-4048e9751bc0.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/652a450a-efec-4f75-9b83-4048e9751bc0.png)'
- en: 'Now, we will have a partially implemented operation of LSTM to measure the
    compute intensity. As we discussed earlier, LSTM has two matrix-matrix multiplications
    that we need to compute. The LSTM operation will compute that for each element
    of the input sequence, as well as for each layer. Then, the operation can be configured
    as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将部分实现LSTM操作以测量计算强度。正如我们之前讨论的，LSTM有两个矩阵-矩阵乘法需要计算。LSTM操作将为输入序列的每个元素以及每个层计算。然后，操作可以配置如下：
- en: '[PRE55]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We can use more element-wise operations, but it will just approximate the compute
    intensity, so we will omit them for now.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用更多的逐元素操作，但这只是近似计算强度，所以我们暂时不考虑它们。
- en: Comparing the performance between CUDNN and SGEMM LSTM
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较CUDNN和SGEMM LSTM的性能
- en: 'Let’s compare their there performance along with the different batch sizes
    as following codes implemented in `main()` function:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较它们的性能以及不同的批处理大小，如下所示的代码实现在`main()`函数中：
- en: '[PRE56]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'And, we can compile and execute the example source code with the following
    command:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下命令编译和执行示例源代码：
- en: '[PRE57]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The following graph shows the measured performance of cuBLAS and cuDNN from
    a Tesla V100 card:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了来自Tesla V100卡的cuBLAS和cuDNN的性能：
- en: '![](img/cecc6413-86bd-45aa-ab52-03bb687493b1.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cecc6413-86bd-45aa-ab52-03bb687493b1.png)'
- en: In the preceding graph, the two implementations show a huge difference in performance.
    The LSTM's performance of cuDNN is much better than the simulated LSTM using cuBLAS.
    Also, the LSTM operation's performance follows the roofline of the Tesla V100
    GPU. On the other hand, the two SGEMM operations don't show this performance since
    the matrix size isn't large enough to get full performance. To obtain 10 TFlops
    from the Tesla V100, the matrix size should be similar to or larger than the square
    of 1,024\. However, as we can see, our matrix size is around the square of 512.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，两种实现在性能上有很大差异。cuDNN的LSTM性能比使用cuBLAS模拟的LSTM要好得多。此外，LSTM操作的性能遵循Tesla V100
    GPU的屋顶线。另一方面，两个SGEMM操作并没有显示出这种性能，因为矩阵大小不够大以获得完整的性能。要从Tesla V100获得10 TFlops，矩阵大小应与1,024的平方相似或更大。然而，正如我们所看到的，我们的矩阵大小大约是512的平方。
- en: LSTM optimization is explained in the following NVIDIA article: [https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5](https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5).
    It combines matrix-matrix multiplications, fusing element-wise operations, multiple
    streams, and multi-layer parallelization.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM优化在以下NVIDIA文章中有解释：[https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5](https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5)。它结合了矩阵-矩阵乘法，融合逐元素操作，多个流和多层并行化。
- en: One of the optimization versions of the RNN is the persistent RNN ([https://svail.github.io/persistent_rnns](https://svail.github.io/persistent_rnns)),
    which was introduced by Greg Diamos. Although his implementation does not include
    LSTM and GRU, you can learn how the RNN can be optimized.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的优化版本之一是持久RNN（[https://svail.github.io/persistent_rnns](https://svail.github.io/persistent_rnns)），由Greg
    Diamos介绍。尽管他的实现不包括LSTM和GRU，但您可以了解RNN如何进行优化。
- en: Profiling deep learning frameworks
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习框架的性能分析
- en: In general, we develop and research neural networks using deep learning frameworks
    such as TensorFlow, PyTorch, and MxNet. Thanks to these frameworks, we can develop
    sophisticated models effectively. However, when it comes to performance engineering,
    understanding the GPU operation underneath the framework is a steep learning curve
    because of the profiling tool's capabilities. For example, profiling with chrome
    tracing is useful when the model is simple, but isn't when the model is complicated.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们使用TensorFlow、PyTorch和MxNet等深度学习框架开发和研究神经网络。由于这些框架，我们可以有效地开发复杂的模型。然而，当涉及性能工程时，由于性能分析工具的能力，理解框架下GPU操作是一个陡峭的学习曲线。例如，使用Chrome跟踪进行性能分析在模型简单时很有用，但在模型复杂时就不那么有用。
- en: In [Chapter 5](ea24897f-252a-4e76-81e3-b5d5ff645bb6.xhtml), *CUDA Application
    Profiling and Debugging*, we covered the **NVIDIA Tools Extension** (**NVTX**),
    which allows us to have custom annotations in the GPU applications and review
    the timeline using NVIDIA Nsight Systems. For complicated applications, it is
    useful for programmers to analyze their performance and find bottlenecks.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ea24897f-252a-4e76-81e3-b5d5ff645bb6.xhtml)中，*CUDA应用程序性能分析和调试*，我们介绍了**NVIDIA工具扩展**（**NVTX**），它允许我们在GPU应用程序中进行自定义注释，并使用NVIDIA
    Nsight Systems查看时间轴。对于复杂的应用程序，程序员分析其性能并找到瓶颈非常有用。
- en: In this section, we will cover how to use NVTX in PyTorch and TensorFlow by
    modifying the ResNet-50 sample code. The example code can be found in the `10_deep_learining/05_framework_profile` folder
    in this book's GitHub repository. You can obtain the original source code from
    [https://github.com/nvidia/DeepLearningExamples](https://github.com/nvidia/DeepLearningExamples).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何通过修改ResNet-50示例代码在PyTorch和TensorFlow中使用NVTX。示例代码可以在本书的GitHub存储库的`10_deep_learining/05_framework_profile`文件夹中找到。您可以从[https://github.com/nvidia/DeepLearningExamples](https://github.com/nvidia/DeepLearningExamples)获取原始源代码。
- en: To make an easy working environment configuration, we will use **NVIDIA GPU
    Cloud** (**NGC**) deep learning containers for PyTorch and TensorFlow. If you
    need to learn about NGC or basic usage of the container, please visit the appendix
    for NGC in this book.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化工作环境配置，我们将使用**NVIDIA GPU云**（**NGC**）深度学习容器用于PyTorch和TensorFlow。如果您需要了解NGC或容器的基本用法，请访问本书附录中的NGC。
- en: Now, let's begin with PyTorch first.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们先从PyTorch开始。
- en: Profiling the PyTorch model
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对PyTorch模型进行性能分析
- en: 'In PyTorch, we can place a custom tag using `torch.cuda.nvtx.range_push("foo")`
    and `torch.cuda.nvtx.range_pop()`. This maintains the original CUDA NVTX APIs,
    that is, `nvtxRangePush()` and `nvtxRangePop()`. Let''s see how NVTX annotations
    can help us understand deep learning operations in the timeline. In the following
    steps, we will use the ResNet-50 example code in the `05_framework_profile/pytorch/RN50v1.5` file:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，我们可以使用`torch.cuda.nvtx.range_push("foo")`和`torch.cuda.nvtx.range_pop()`来放置自定义标签。这保持了原始的CUDA
    NVTX API，即`nvtxRangePush()`和`nvtxRangePop()`。让我们看看NVTX注释如何帮助我们在时间轴上理解深度学习操作。在接下来的步骤中，我们将使用`05_framework_profile/pytorch/RN50v1.5`文件中的ResNet-50示例代码：
- en: 'We will place NVTX annotations in the training loop in the `train()` function
    to annotate the `step` value. This function can be found in the `image_classificaiton/training.py` file.
    The following screenshot shows the training loop and the NVTX annotations at line
    234 and line 260, respectively:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在`train()`函数中的训练循环中放置NVTX注释以注释`step`值。该函数可以在`image_classificaiton/training.py`文件中找到。以下截图显示了训练循环和分别在第234行和第260行的NVTX注释：
- en: '![](img/a9bbdf88-5cc0-421f-b7ad-00e02655c02c.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9bbdf88-5cc0-421f-b7ad-00e02655c02c.png)'
- en: In the preceding code, the training operations are implemented in the `step`
    function, which is defined by the `get_train_step()` function. Therefore, we need
    to place NVTX annotations in that function to learn more about it.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，训练操作是在`step`函数中实现的，该函数由`get_train_step()`函数定义。因此，我们需要在该函数中放置NVTX注释以了解更多信息。
- en: 'Let''s add some NVTX annotations to the `get_train_step()` function at line
    164\. This function returns the `_step()` function, which includes the training
    operations. Therefore, we will place NVTX annotations in this function. The training
    procedures are forward and backward propagation, all-reduce, and optimization
    (update weights). The following screenshot shows the annotations of forward propagation
    at lines 166 and 171:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在第164行的`get_train_step()`函数中添加一些NVTX注释。该函数返回`_step()`函数，其中包括训练操作。因此，我们将在该函数中放置NVTX注释。训练过程包括前向和反向传播、全局归约和优化（更新权重）。以下截图显示了在第166行和第171行的前向传播的注释：
- en: '![](img/0c8cae13-132c-4220-afe8-224397161611.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c8cae13-132c-4220-afe8-224397161611.png)'
- en: This way, we can place other annotations on the remaining operations.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以在其余操作上放置其他注释。
- en: 'We can also have NVTX annotations for the model layers. In this example, the
    ResNet-50 model is implemented in the `image_classification/resnet.py` file. The
    following screenshot shows the example annotations of the network:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以为模型层添加NVTX注释。在这个例子中，ResNet-50模型是在`image_classification/resnet.py`文件中实现的。以下截图显示了网络的示例注释：
- en: '![](img/c3ac1744-4518-41a9-bfea-8c7cbaf8be42.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3ac1744-4518-41a9-bfea-8c7cbaf8be42.png)'
- en: As we can see, we can place NVTX annotations following the ResNet architecture.
    We can get more information if we place annotations in each building block.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们可以按照ResNet架构放置NVTX注释。如果我们在每个构建块中放置注释，我们可以获得更多信息。
- en: 'Now, let''s profile the model. As we discussed earlier, we will use the NGC
    deep learning container known as PyTorch. The `imagenet` dataset is located in
    the `/raid/datasets/imagenet/raw-data` folder. To limit the profiling time range,
    we will use the delay option (`-y`) and duration option (`-d`). The following
    code shows a bash shell script that executes the container and profiles the network:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们对模型进行分析。正如我们之前讨论的，我们将使用NGC深度学习容器，即PyTorch。`imagenet`数据集位于`/raid/datasets/imagenet/raw-data`文件夹中。为了限制分析时间范围，我们将使用延迟选项（`-y`）和持续时间选项（`-d`）。以下代码显示了一个执行容器并对网络进行分析的bash
    shell脚本：
- en: '[PRE58]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: After execution, the preceding code generates the profiled result in the RN50v1.5
    directory, that is, `resnet50_pyt.qdrep`.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 执行后，上述代码将在RN50v1.5目录中生成profiled结果，即`resnet50_pyt.qdrep`。
- en: 'Finally, open the profiled output, `resnet50_pyt.qdrep`, using NVIDIA Nsight
    Systems and review the operations. The following screenshot shows a measured step
    with NVTX annotations:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用NVIDIA Nsight Systems打开profiled输出`resnet50_pyt.qdrep`，并查看操作。以下截图显示了带有NVTX注释的测量步骤：
- en: '![](img/2e2912c2-781b-46c0-9e0f-b8c5f97a3f29.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e2912c2-781b-46c0-9e0f-b8c5f97a3f29.png)'
- en: 'Here, we can see that the backward operations take twice as long as the forwarding
    operations. Also, PyTorch separates the host threads for the training loop and
    backward propagation. Looking at the kernel profiling, the most time-consuming
    points are element-wise kernel executions. Let''s enlarge the forwarding pass
    to review the layers'' execution time, as shown in the following screenshot:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到反向操作所花费的时间是前向操作的两倍。此外，PyTorch将主机线程分开用于训练循环和反向传播。从内核分析来看，耗时最长的点是逐元素的内核执行。让我们扩大前向传递以查看层的执行时间，如下截图所示：
- en: '![](img/013b8dd6-8fa4-4942-91b5-0519a77b7f4f.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/013b8dd6-8fa4-4942-91b5-0519a77b7f4f.png)'
- en: 'Here, we can see that the second convolution block takes the longest time to
    complete. If there are inefficient points in this layer, we can dig further. We
    can also analyze a specific kernel function using NVIDIA Nsight Compute if that
    operation is determined as a bottleneck and needs to be optimized. Comparing the
    host API tracing and the GPUs, we can see that the time durations are different.
    This is because the host and the GPU operations are asynchronous. So, we need
    to be cautious when we measure the GPU execution time from the host. Now, let''s
    take a look at the optimization step, as shown in the following screenshot:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到第二个卷积块需要最长的时间来完成。如果这一层存在效率低下的点，我们可以进一步挖掘。如果某个操作被确定为瓶颈并需要优化，我们还可以使用NVIDIA
    Nsight Compute来分析特定的内核函数。比较主机API跟踪和GPU，我们可以看到时间持续时间是不同的。这是因为主机和GPU操作是异步的。因此，当我们从主机测量GPU执行时间时，我们需要谨慎。现在，让我们看一下优化步骤，如下截图所示：
- en: '![](img/79d227f1-a5aa-4222-88a3-d62710299655.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79d227f1-a5aa-4222-88a3-d62710299655.png)'
- en: We can see that the huge difference was in the measured execution time from
    the host and the GPU. The host's measured execution time was 25.367 ms, whereas
    the GPU's time was 4.048 ms. Its operations are mainly element-wise operations
    and its execution is delayed until backward propagation has finished. We can find
    the asynchronous execution too. After that, we can see the `cudaDeviceSynchronize()`
    function, which prevents the current step from being updated by the next step.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，从主机和GPU的测量执行时间中存在巨大差异。主机的测量执行时间为25.367毫秒，而GPU的时间为4.048毫秒。其操作主要是逐元素操作，其执行被延迟直到反向传播完成。我们还可以找到异步执行。之后，我们可以看到`cudaDeviceSynchronize()`函数，该函数防止当前步骤被下一步骤更新。
- en: We also can disable these asynchronous operations by setting an environment,
    that is, `CUDA_LAUNCH_BLOCKING=1`. We can pass this to the Nsight System's profile
    option using the environment option (`-e`). Then, we can analyze the application's
    `align` operation with the host and kernel functions.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过设置环境来禁用这些异步操作，即`CUDA_LAUNCH_BLOCKING=1`。我们可以使用环境选项（`-e`）将其传递给Nsight System的配置选项。然后，我们可以分析应用程序的`align`操作与主机和内核函数。
- en: PyTorch has several NVTX featured APIs in their CUDA objects. PyTorch documentation
    can be found at [https://pytorch.org/docs/stable/_modules/torch/cuda/nvtx.html](https://pytorch.org/docs/stable/_modules/torch/cuda/nvtx.html).
    By calling the NVTX APIs in PyTorch directly, the CUDA NVTX APIs are called. This
    means we could obtain custom-tagged NVTX marks in the profiled timeline.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch在其CUDA对象中具有几个具有NVTX特色的API。 PyTorch文档可以在[https://pytorch.org/docs/stable/_modules/torch/cuda/nvtx.html](https://pytorch.org/docs/stable/_modules/torch/cuda/nvtx.html)找到。通过直接在PyTorch中调用NVTX
    API，将调用CUDA NVTX API。这意味着我们可以在分析时间线中获得自定义标记的NVTX标记。
- en: Profiling a TensorFlow model
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对TensorFlow模型进行分析
- en: 'Profiling TensorFlow graphs requires that we have an NVTX plugin that enables
    NVTX annotations. To use NVTX annotations in TensorFlow, we need to install the `nvtx-plugins-tf` Python
    plugin using the following command:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 对TensorFlow图进行分析需要使用启用NVTX注释的NVTX插件。要在TensorFlow中使用NVTX注释，我们需要使用以下命令安装`nvtx-plugins-tf`
    Python插件：
- en: '[PRE59]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: However, we don't have to do this if we use an NGC TensorFlow container later
    than version 19.o8.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们使用的是版本晚于19.08的NGC TensorFlow容器，则无需执行此操作。
- en: 'TensorFlow graph APIs are symbolic APIs, so they require specific programming
    methods. The NVTX plugin provides two options for this: a decorator and a Python
    function.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow图形API是符号API，因此它们需要特定的编程方法。 NVTX插件为此提供了两个选项：装饰器和Python函数。
- en: 'Here is an example of an NVTX decorator:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是NVTX装饰器的示例：
- en: '[PRE60]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The following is an example of an NVTX Python function:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是NVTX Python函数的示例：
- en: '[PRE61]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The NVTX plugin provides NVTXHook, which allows us to profile the TF estimator
    and session. For example, we can use the hook as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: NVTX插件提供了NVTXHook，它允许我们对TF估算器和会话进行分析。例如，我们可以按以下方式使用该钩子：
- en: '[PRE62]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Then, we can apply this to either option using the following code:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下代码将其应用于任一选项：
- en: '[PRE63]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Alternatively, we can use the following code:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用以下代码：
- en: '[PRE64]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, let''s apply this to the sample ResNet-50 code and review the operation.
    The example code can be found in the `05_framework_profile/tensorflow/RN50v1.5`
    folder:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将其应用到示例ResNet-50代码中并进行操作审查。示例代码可以在`05_framework_profile/tensorflow/RN50v1.5`文件夹中找到：
- en: 'Let''s begin by applying `NVTXHook` to the estimator. The training graph''s
    definition can be found in the `runtime/runner.py` file on line 312\. Ahead of
    building the graph, we will append `NVTXHook` to the list of hooks, as shown in
    the following block of code:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先将`NVTXHook`应用于估算器。训练图的定义可以在`runtime/runner.py`文件的第312行找到。在构建图之前，我们将`NVTXHook`附加到钩子列表中，如下面的代码块所示：
- en: '![](img/e773aace-88c2-445b-9701-5506ba0bf8d1.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e773aace-88c2-445b-9701-5506ba0bf8d1.png)'
- en: 'Then, we will apply the NVTX annotation to the model-building function. The `model_build()`
    function can be found in the `ResnetModel` class in the `model/resnet_v1_5.py` file.
    The following code shows an example of placing an NVTX annotation by using a Python
    function on the `conv1` layer in the `model_build()` function:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将NVTX注释应用于模型构建函数。`model_build()`函数可以在`model/resnet_v1_5.py`文件的`ResnetModel`类中找到。以下代码显示了如何在`model_build()`函数中的`conv1`层上使用Python函数放置NVTX注释的示例：
- en: '![](img/52fc56c0-b159-4e56-8899-67ab37fb9f4c.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52fc56c0-b159-4e56-8899-67ab37fb9f4c.png)'
- en: In the preceding code, we need to be cautious when to use proper inputs and
    outputs when using the `nvtx_tf.ops.start()` and `nvtx_tf.ops.end()` functions.
    Only place NVTX annotations in the other layers. Be sure that the final fully
    connected layer's output is the output of the network.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，当使用`nvtx_tf.ops.start()`和`nvtx_tf.ops.end()`函数时，我们需要谨慎选择适当的输入和输出。只在其他层中放置NVTX注释。确保最终的全连接层输出是网络的输出。
- en: 'We also have to disable the code to check the number of trainable variables
    it has. If NVTX''s `trainable` parameter''s value is `True`, the size changes.
    At line 174 in the `resnet_v1_5.py` file, there''s a block of assertion code that
    checks the number of that variable. Simply comment it out, as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须禁用用于检查可训练变量数量的代码。如果NVTX的`trainable`参数值为`True`，则大小会发生变化。在`resnet_v1_5.py`文件的第174行，有一段断言代码，用于检查该变量的数量。只需将其注释掉，如下所示：
- en: '![](img/e4654a73-e106-4d11-a41b-c66ccd5d104f.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4654a73-e106-4d11-a41b-c66ccd5d104f.png)'
- en: 'We also use NVTX decorators for the ResNet building blocks. In the `model/blocks`
    directory, we can find the `conv2d` and ResNet bottleneck block implementations
    in `conv2d_blocks.py` and `resnet_bottleneck_block.py`. In the `conv2d_blocks.py`
    file, we can decorate `conv2d_block()` function to annotate NVTX profiling, as
    follows:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还使用NVTX装饰器来构建ResNet模块。在`model/blocks`目录中，我们可以在`conv2d_blocks.py`和`resnet_bottleneck_block.py`中找到`conv2d`和ResNet瓶颈块的实现。在`conv2d_blocks.py`文件中，我们可以装饰`conv2d_block()`函数以注释NVTX分析，如下所示：
- en: '![](img/abed69dc-571e-4970-af59-ba04f3dd2ac6.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/abed69dc-571e-4970-af59-ba04f3dd2ac6.png)'
- en: 'In the same way, we can do the same to the `resnet_bottleneck_block.py` file:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们也可以对`resnet_bottleneck_block.py`文件执行相同操作：
- en: '![](img/1edce41d-4992-46c0-b7d6-532b4bfcb157.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1edce41d-4992-46c0-b7d6-532b4bfcb157.png)'
- en: 'Now, let''s profile the model. Like we did with the PyTorch container, we will
    use TensorFlow''s NGC container. We will assume that the `imagenet` dataset''s
    `tfrecord` files are located in the `/raid/datasets/imagenet/tfrecord` directory. The
    following code shows a bash shell script that executes the container and profiles
    the network:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们对模型进行性能分析。就像我们使用PyTorch容器一样，我们将使用TensorFlow的NGC容器。我们假设`imagenet`数据集的`tfrecord`文件位于`/raid/datasets/imagenet/tfrecord`目录中。以下代码显示了一个执行容器并对网络进行性能分析的bash
    shell脚本：
- en: '[PRE65]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: When we execute this function, we will get the `resnet50_tf.qdrep` file in the
    `RN50v1.5` directory.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行这个函数时，我们将在`RN50v1.5`目录中得到`resnet50_tf.qdrep`文件。
- en: 'Finally, let''s review the profiled output using the NVIDIA Nsight System:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们使用NVIDIA Nsight System审查分析输出：
- en: '![](img/f5e3118b-c0bc-4cee-83c2-69947821d1f9.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5e3118b-c0bc-4cee-83c2-69947821d1f9.png)'
- en: Here, we can confirm that backward propagation takes twice as long as the forward
    pass. This example code isn't synchronized with the CPU and the GPU. Due to this,
    we can see a larger time difference between the host and the GPU. As we place
    additional annotations in the building blocks, we will be able to see the sub-block
    annotations in the layers.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以确认反向传播所花费的时间是前向传播的两倍。这个示例代码与CPU和GPU不同步。因此，我们可以看到主机和GPU之间的时间差异更大。当我们在构建块中放置额外的注释时，我们将能够在层中看到子块的注释。
- en: 'Profiling with NVIDIA Nsight Systems provides additional benefits when it comes
    to monitoring all-reduce''s execution time in multi-GPU training. The following
    screenshot shows the profiling result of a GPU that was training with two GPUs:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NVIDIA Nsight Systems进行性能分析在多GPU训练中监视所有归约操作的执行时间时提供了额外的好处。以下截图显示了一个使用两个GPU进行训练的GPU的性能分析结果：
- en: '![](img/e05ad54d-f8a5-482a-935c-f25ed3479b98.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e05ad54d-f8a5-482a-935c-f25ed3479b98.png)'
- en: 'In the highlighted row, we can see the `ncclAllRecude()` function, which calls
    the backward propagation simultaneously. By doing this, we don''t get the delay
    of all-reduce operation. This example code uses Horovod to train multiple GPUs.
    If you want to learn more about this, visit Horovod''s GitHub page: [https://github.com/horovod/horovod](https://github.com/horovod/horovod).
    You can get the document and example code from here.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在突出显示的行中，我们可以看到`ncclAllRecude()`函数，它同时调用了反向传播。通过这样做，我们不会延迟所有归约操作。这个示例代码使用Horovod来训练多个GPU。如果你想了解更多，请访问Horovod的GitHub页面：[https://github.com/horovod/horovod](https://github.com/horovod/horovod)。你可以从这里获取文档和示例代码。
- en: Summary
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned how to use CUDA libraries for deep learning
    and performance benefits. While we reviewed their uses, we matched them with the
    deep learning mechanisms for each step. Thanks to the deep learning libraries
    that are available to us, we can implement a simple CNN without implementing the
    algorithms too. Then, we profiled the ResNet-50 model in PyTorch and TensorFlow
    using NVTX annotations.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用CUDA库进行深度学习和性能优势。在回顾它们的用途时，我们将它们与每个步骤的深度学习机制进行匹配。由于我们可以使用的深度学习库，我们可以实现一个简单的CNN，而不必实现算法。然后，我们使用NVTX注释在PyTorch和TensorFlow中对ResNet-50模型进行了性能分析。
- en: Implementing the base algorithms may be impractical for some deep learning engineers
    and researchers. However, understanding the performance factors and the base operations
    can help you build efficient and effective deep learning-based products. Nowadays,
    we see many productized deep learning-based services. Engineers spend a lot of
    resources productizing their trained model, as well as training their model so
    that they get the lowest error rate possible. Hopefully, you managed to gain some
    insight into how you can use NVTX profiling on your deep learning applications.
    Using this knowledge, you can get more from your GPUs. Good luck!
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些深度学习工程师和研究人员来说，实现基本算法可能是不切实际的。然而，了解性能因素和基本操作可以帮助您构建高效和有效的基于深度学习的产品。如今，我们看到许多产品化的基于深度学习的服务。工程师们花费大量资源将他们训练好的模型产品化，以及训练他们的模型，以便获得尽可能低的错误率。希望您能够了解如何在深度学习应用中使用NVTX性能分析。利用这些知识，您可以更好地利用您的GPU。祝你好运！
