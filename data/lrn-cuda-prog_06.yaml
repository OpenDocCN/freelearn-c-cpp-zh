- en: Scalable Multi-GPU Programming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可扩展的多GPU编程
- en: So far, we have concentrated on getting optimal performance on a single GPU.
    Dense nodes with multiple GPUs have become a pressing need for upcoming supercomputers,
    especially since the ExaFLOP (a quintillion operations per sec) system is becoming
    a reality. GPU architecture is energy-efficient and hence, in recent years, systems
    with GPUs have taken the majority of the top spots in the Green500 list ([https://www.top500.org/green500](https://www.top500.org/green500)). 
    In Green500's November 2018 list, seven out of the top 10 systems were based on
    the NVIDIA GPU.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直致力于在单个GPU上获得最佳性能。密集节点与多个GPU已成为即将到来的超级计算机的迫切需求，特别是自从ExaFLOP（每秒千亿次操作）系统成为现实以来。
    GPU架构具有高能效，因此近年来，具有GPU的系统在Green500榜单（[https://www.top500.org/green500](https://www.top500.org/green500)）中占据了大多数前十名。在2018年11月的Green500榜单中，前十名中有七个基于NVIDIA
    GPU。
- en: The DGX system from NVIDIA now has 16 V100 32 GB in one server. With the help
    of unified memory and interconnect technologies such as NVLink and NvSwitch, developers
    can see all GPUs as one big GPU with 512 GB memory (16 GPU * 32 GB each). In this
    chapter, we will go into the details of writing CUDA code and make use of CUDA-aware
    libraries to efficiently get scalability in a multi-GPU environment within and
    across nodes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA的DGX系统现在在一个服务器中有16个V100 32GB。借助统一内存和诸如NVLink和NvSwitch之类的互连技术，开发人员可以将所有GPU视为一个具有512GB内存的大型GPU（16个GPU
    *每个32GB）。在本章中，我们将深入讨论编写CUDA代码的细节，并利用CUDA-aware库在多GPU环境中实现节点内和节点间的可伸缩性。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Solving a linear equation using Gaussian elimination
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高斯消元法解线性方程
- en: GPUDirect peer to peer
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPUDirect点对点
- en: A brief introduction to MPI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MPI简介
- en: GPUDirect RDMA
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPUDirect RDMA
- en: CUDA streams
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA流
- en: Additional tricks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外的技巧
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux PC with a modern NVIDIA GPU (Pascal architecture onward) is required
    for this chapter, with all necessary GPU drivers and the CUDA Toolkit (10.0 onward)
    installed. If you are unsure of your GPU's architecture, please visit NVIDIA GPU's
    site ([https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus))
    and confirm your GPU's architecture. This chapter's code is also available on
    GitHub at [https://github.com/PacktPublishing/Learn-CUDA-Programming](https://github.com/PacktPublishing/Learn-CUDA-Programming).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要一台带有现代NVIDIA GPU（Pascal架构或更高版本）的Linux PC，并安装了所有必要的GPU驱动程序和CUDA Toolkit（10.0或更高版本）。如果您不确定您的GPU架构，请访问NVIDIA
    GPU网站（[https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)）并确认您的GPU架构。本章的代码也可以在GitHub上找到：[https://github.com/PacktPublishing/Learn-CUDA-Programming](https://github.com/PacktPublishing/Learn-CUDA-Programming)。
- en: The sample code examples in this chapter have been developed and tested with
    CUDA version 10.1\. However, it is recommended you use the latest version (CUDA)
    or a higher one.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的示例代码是使用CUDA版本10.1开发和测试的。但是，建议您使用最新版本（CUDA）或更高版本。
- en: Since this chapter needs to showcase multi-GPU interactions, we will need a
    minimum of two GPUs of the same type and architecture. Also, note that some features,
    such as GPUDirect RDMA and NVLink, are only supported on Tesla cards of NVIDIA.
    If you don't have a Tesla card such as the Tesla P100 or Tesla V100, don't be
    disheartened. You can safely ignore some of these features. There will be a change
    in performance numbers compared to what we show here, but the same code will work
    as-is.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章需要展示多GPU的交互，我们需要至少两个相同类型和架构的GPU。还要注意，一些功能，如GPUDirect RDMA和NVLink，仅支持NVIDIA的Tesla卡。如果您没有像Tesla
    P100或Tesla V100这样的Tesla卡，不要灰心。您可以安全地忽略其中一些功能。与我们在这里展示的情况相比，性能数字将会有所变化，但相同的代码将仍然有效。
- en: In the next section, we will look at an example of the popular Gaussian algorithm
    to solve a series of linear equations to demonstrate how to write multi-GPUs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看一个示例，使用流行的高斯算法解决一系列线性方程，以演示如何编写多GPU。
- en: Solving a linear equation using Gaussian elimination
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用高斯消元法解线性方程
- en: 'To demonstrate the usage of multiple GPUs within and across nodes, we will
    start with some sequential code and then convert it into multiple GPUs within
    and across nodes. We will be solving a linear system of equations containing *M*
    equations and *N* unknowns. The equation can be represented as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示在节点内和节点间使用多个GPU，我们将从一些顺序代码开始，然后将其转换为节点内和节点间的多个GPU。我们将解决一个包含*M*个方程和*N*个未知数的线性方程组。该方程可以表示如下：
- en: '***A × x = b***'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '***A × x = b***'
- en: Here, *A* is a matrix with *M* rows and *N* columns, *x* is a column vector
    (also referred to as a solution vector) with *N* rows, and *b* is also a column
    vector with *M* rows. Finding a solution vector involves computing vector *x*
    when *A* and *b* are given. One of the standard methods for solving a linear system
    of equations is Gaussian elimination. In Gaussian elimination, first matrix *A*
    is reduced to either the upper or lower triangular matrix by performing elementary
    row transformations. Then, the resulting triangular system of equations is solved
    by using the back substitution step.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*A*是一个具有*M*行和*N*列的矩阵，*x*是一个列向量（也称为解向量），具有*N*行，*b*也是一个具有*M*行的列向量。找到解向量涉及在给定*A*和*b*时计算向量*x*。解线性方程组的标准方法之一是高斯消元法。在高斯消元法中，首先通过执行初等行变换将矩阵*A*减少为上三角矩阵或下三角矩阵。然后，通过使用回代步骤解决得到的三角形方程组。
- en: 'The following pseudocode explains the steps that are involved in solving the
    linear equation:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下伪代码解释了解线性方程所涉及的步骤：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s take a look at an example in order to understand the algorithm. Let''s
    say the system of equations is as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个示例，以便理解算法。假设方程组如下：
- en: '![](img/2ce99d49-d0fa-45b2-9185-fd05c2242e1e.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ce99d49-d0fa-45b2-9185-fd05c2242e1e.png)'
- en: 'First, we will try to set the baseline system, as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将尝试设置基线系统，如下所示：
- en: Prepare your GPU application. This code can be found in the `06_multigpu/gaussian` folder
    in this book's GitHub repository.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备您的GPU应用程序。此代码可以在本书的GitHub存储库中的`06_multigpu/gaussian`文件夹中找到。
- en: 'Compile your application with the `nvcc` compiler, as follows:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nvcc`编译器编译您的应用程序，如下所示：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding steps compile and run two versions of the code that are present
    in this chapter:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤编译并运行了本章中存在的两个版本的代码：
- en: The CPU code, which runs sequentially
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序运行的CPU代码
- en: The CUDA code, which runs on a single GPU
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单个GPU上运行的CUDA代码
- en: Now, let's take a look at the hotspots in the single GPU implementation of Gaussian
    elimination.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看高斯消元的单GPU实现中的热点。
- en: Single GPU hotspot analysis of Gaussian elimination
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯消元的单GPU热点分析
- en: Let's try to understand and profile sequential and single GPU code to set a
    baseline. Over this baseline, we will enhance and add support for running on multi-GPUs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解和分析顺序和单GPU代码以建立基线。在此基础上，我们将增强并添加对多GPU运行的支持。
- en: '**Sequential CPU code**: The following code shows the extracted code of sequential
    implementation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**顺序CPU代码**：以下代码显示了顺序实现的提取代码：'
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Visually, the operation that takes place is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，发生的操作如下：
- en: '![](img/b938a13f-62d4-416d-a911-ad1be7c4db3d.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b938a13f-62d4-416d-a911-ad1be7c4db3d.png)'
- en: Here, in this Gaussian elimination, the number of rows is equal to the number
    of equations and the number of columns is equal to the number of unknowns. The
    **pr** row shown in the preceding diagram is the pivot row and will be used to
    reduce other rows using the pivot element.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，高斯消元中的行数等于方程的数量，列数等于未知数的数量。在前面的图表中显示的**pr**行是主元行，将用于使用主元素减少其他行。
- en: 'The first observation that we can make is that we are operating on an augmented
    matrix to merge the *A* matrix with the *b* vector. Hence, the size of unknowns
    is *N+1* as an augmented matrix has the last column as the *b* vector. Creating
    an augmented matrix helps us work on just one data structure, that is, a matrix.
    You can profile this code using the following command. The profiling results will
    show you that the `guassian_elimination_cpu()` function takes the most time to
    complete:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做出的第一个观察是，我们正在对增广矩阵进行操作，将*A*矩阵与*b*向量合并。因此，未知数的大小为*N+1*，因为增广矩阵的最后一列是*b*向量。创建增广矩阵有助于我们只处理一个数据结构，即矩阵。您可以使用以下命令对此代码进行分析。分析结果将显示`guassian_elimination_cpu()`函数完成所需的时间最长：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**CUDA single GPU code**: After going through the previous chapters, we expect
    you to have familiarized yourself with how to write optimal GPU code and hence
    we will not go into the details of the single GPU implementation. The following
    extract shows that, in a single GPU implementation, the three steps are known
    as three kernels for finding *N* unknowns:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**CUDA单GPU代码**：通过前几章的学习，我们期望您已经熟悉了如何编写最佳的GPU代码，因此我们不会详细介绍单个GPU实现。以下摘录显示，在单个GPU实现中，三个步骤被称为三个用于找到*N*未知数的核心：'
- en: '`findPivotRowAndMultipliers<<<...>>>`: The kernel finds the pivot row and multiplier,
    which should be used for row elimination.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`findPivotRowAndMultipliers<<<...>>>`：该核心查找主元行和乘数，应用于行消除。'
- en: '`extractPivotRow<<<>>>`: The kernel extracts the pivot row, which is then used
    to perform row elimination.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extractPivotRow<<<>>>`：该核心提取主元行，然后用于执行行消除。'
- en: '`rowElimination<<<>>>`: This is the final kernel call, and does the row elimination
    in parallel on the GPU.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rowElimination<<<>>>`：这是最终的核心调用，在GPU上并行进行行消除。'
- en: 'The following code snippet shows the three kernels called iteratively after
    the data has been copied to the GPU:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段显示了数据在复制到GPU后迭代调用的三个核心：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The focus of this chapter is on how this single GPU implementation can be enhanced
    to support multiple GPUs. However, to fill in the missing pieces from the GPU
    implementation, we need to make some optimization changes to the single GPU implementation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是如何增强此单个GPU实现以支持多个GPU。但是，为了填补GPU实现中的缺失部分，我们需要对单个GPU实现进行一些优化更改：
- en: 'The performance of the Gaussian elimination algorithm is heavily influenced
    by the memory access pattern. Basically, it depends on how the AB matrix is stored:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯消元算法的性能受内存访问模式的影响很大。基本上，它取决于AB矩阵的存储方式：
- en: Finding the pivot row prefers the column-major format as it provides coalesced
    access if the matrix is stored in a column major format.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到主元行更喜欢列主格式，因为如果矩阵以列主格式存储，则提供了合并访问。
- en: On the other hand, extracting a pivot row prefers the row-major format.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，提取主元行更喜欢行主格式。
- en: No matter how we store the *AB* matrix, one coalesced and one strided/non-coalesced
    access to memory is unavoidable.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论我们如何存储*AB*矩阵，内存访问中都无法避免一个合并和一个跨步/非合并的访问。
- en: The column major format is also beneficial for row elimination kernels and hence,
    for our Gaussian elimination kernel, we decided to store the transpose of the
    AB matrix instead of AB. The AB matrix gets transposed once, at the beginning
    of the code in the `transposeMatrixAB()` function.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列主格式对于行消除核心也是有益的，因此对于我们的高斯消元核心，我们决定存储AB矩阵的转置而不是AB。AB矩阵在代码开始时通过`transposeMatrixAB()`函数转置一次。
- en: In the next section, we will enable multi-GPU P2P access and split the work
    among multiple GPUs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将启用多GPU P2P访问并将工作分配给多个GPU。
- en: GPUDirect peer to peer
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU直接点对点
- en: 'The GPUDirect technology was created to allow high-bandwidth, low-latency communication
    between GPUs within and across different nodes. This technology was introduced
    to eliminate CPU overheads when one GPU needs to communicate with another. GPUDirect
    can be classified into the following major categories:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: GPUDirect技术是为了允许GPU在节点内部和跨不同节点之间进行高带宽、低延迟的通信而创建的。该技术旨在消除一个GPU需要与另一个GPU通信时的CPU开销。GPUDirect可以分为以下几个主要类别：
- en: '**Peer to peer (P2P) transfer between GPU**: AllowsCUDA programs to use high-speed
    **Direct Memory Transfer** (**DMA**) to copy data between two GPUs in the same
    system. It also allows optimized access to the memory of other GPUs within the
    same system.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU之间的点对点（P2P）传输**：允许CUDA程序在同一系统中的两个GPU之间使用高速**直接内存传输**（**DMA**）来复制数据。它还允许对同一系统中其他GPU的内存进行优化访问。'
- en: '**Accelerated communication between network and storage**: This technology
    helps with direct access to CUDA memory from third-party devices such as InfiniBand
    network adapters or storage. It eliminates unnecessary memory copies and CPU overhead
    and hence reduces the latency of transfer and access. This feature is supported
    from CUDA 3.1 onward.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络和存储之间的加速通信**：这项技术有助于从第三方设备（如InfiniBand网络适配器或存储）直接访问CUDA内存。它消除了不必要的内存复制和CPU开销，从而减少了传输和访问的延迟。此功能从CUDA
    3.1开始支持。'
- en: '**GPUDirect for video**: This technology optimizes pipelines for frame-based
    video devices. It allows low-latency communication with OpenGL, DirectX, or CUDA
    and is supported from CUDA 4.2 onward.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频的GPUDirect**：这项技术优化了基于帧的视频设备的流水线。它允许与OpenGL、DirectX或CUDA进行低延迟通信，并且从CUDA
    4.2开始支持。'
- en: '**Remote Direct Memory Access (RDMA)**: This feature allows direct communication
    between GPUs across a cluster. This feature is supported from CUDA 5.0 and later.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**远程直接内存访问（RDMA）**：此功能允许集群中的GPU之间进行直接通信。此功能从CUDA 5.0及更高版本开始支持。'
- en: In this section, we will be converting our sequential code to make use of the
    P2P feature of GPUDirect so that it can run on multiple GPUs within the same system.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把我们的顺序代码转换为使用GPUDirect的P2P功能，以便在同一系统中的多个GPU上运行。
- en: 'The GPUDirect P2P feature allows the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: GPUDirect P2P功能允许以下操作：
- en: '**GPUDirect transfers**: `cudaMemcpy()` initiates a DMA copy from GPU 1''s
    memory to GPU 2''s memory.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPUDirect传输**：`cudaMemcpy()`启动了从GPU 1的内存到GPU 2的内存的DMA复制。'
- en: '**Direct access**: GPU 1 can read or write GPU 2''s memory (load/store).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接访问**：GPU 1可以读取或写入GPU 2的内存（加载/存储）。'
- en: 'The following diagram demonstrates these features:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了这些功能：
- en: '![](img/937e7aae-d492-4615-a17f-8e2a3e2f48bc.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/937e7aae-d492-4615-a17f-8e2a3e2f48bc.png)'
- en: 'To understand the advantage of P2P, it is necessary to understand the PCIe
    bus specification. This was created with the intention of optimally communicating
    through interconnects such as InfiniBand to other nodes. This is different when
    we want to optimally send and receive data from individual GPUs. The following
    is a sample PCIe topology where eight GPUs are being connected to various CPUs
    and NIC/InfiniBand cards:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解P2P的优势，有必要了解PCIe总线规范。这是为了通过InfiniBand等互连优化与其他节点进行通信而创建的。当我们想要从单个GPU优化地发送和接收数据时，情况就不同了。以下是一个样本PCIe拓扑，其中八个GPU连接到各种CPU和NIC/InfiniBand卡：
- en: '![](img/33fbb94b-f6e0-4951-8285-c0e03c6c8bf4.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33fbb94b-f6e0-4951-8285-c0e03c6c8bf4.png)'
- en: In the preceding diagram, P2P transfer is allowed between GPU0 and GPU1 as they
    are both situated in the same PCIe switch. However, GPU0 and GPU4 cannot perform
    a P2P transfer as PCIe P2P communication is not supported between two **I/O Hubs** (**IOHs**). The
    IOH does not support non-contiguous bytes from PCI Express for remote peer-to-peer
    MMIO transactions. The nature of the QPI link connecting the two CPUs ensures
    that a direct P2P copy between GPU memory is not possible if the GPUs reside on
    different PCIe domains. Thus, a copy from the memory of GPU0 to the memory of
    GPU4 requires copying over the PCIe link to the memory attached to CPU0, then
    transferring it over the QPI link to CPU1 and over the PCIe again to GPU4\. As
    you can imagine, this process adds a significant amount of overhead in terms of
    both latency and bandwidth.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，GPU0和GPU1之间允许P2P传输，因为它们都位于同一个PCIe交换机中。然而，GPU0和GPU4不能执行P2P传输，因为两个**I/O
    Hub**（IOHs）之间不支持PCIe P2P通信。IOH不支持来自PCI Express的非连续字节进行远程对等MMIO事务。连接两个CPU的QPI链路的性质确保了如果GPU位于不同的PCIe域上，则不可能在GPU内存之间进行直接P2P复制。因此，从GPU0的内存到GPU4的内存的复制需要通过PCIe链路复制到连接到CPU0的内存，然后通过QPI链路传输到CPU1，并再次通过PCIe传输到GPU4。正如你所想象的那样，这个过程增加了大量的开销，无论是延迟还是带宽方面。
- en: 'The following diagram shows another system where GPUs are connected to each
    other via an NVLink interconnect that supports P2P transfers:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了另一个系统，其中GPU通过支持P2P传输的NVLink互连相互连接：
- en: '![](img/88e4972a-9fab-4b7c-9643-8b4b16c550fb.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88e4972a-9fab-4b7c-9643-8b4b16c550fb.png)'
- en: The preceding diagram shows a sample NVLink topology resulting in an eight-cube
    mesh where each GPU is connected to another GPU with a max 1 hop.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了一个样本NVLink拓扑，形成了一个八立方网格，其中每个GPU与另一个GPU最多相连1跳。
- en: 'The more important query would be, *How can we figure out this topology and
    which GPUs support P2P transfer?* Fortunately, there are tools for this. `nvidia-smi`
    is one such tool that gets installed as part of the NVIDIA driver''s installation.
    The following screenshot shows the output of running `nvidia-smi` on the NVIDIA
    DGX server whose network topology is shown in the preceding diagram:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的问题是，*我们如何找出这个拓扑结构以及哪些GPU支持P2P传输？*幸运的是，有工具可以做到这一点。`nvidia-smi`就是其中之一，它作为NVIDIA驱动程序安装的一部分被安装。以下屏幕截图显示了在前面图表中显示的NVIDIA
    DGX服务器上运行`nvidia-smi`的输出：
- en: '![](img/806e629d-8aa4-4dc4-a453-5f803bb512e5.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/806e629d-8aa4-4dc4-a453-5f803bb512e5.jpg)'
- en: The preceding screenshot represents the result of the `nvidia-smi topo -m` command
    being run on the DGX system, which has 8 GPUs. As you can see, any GPU that is
    connected to another GPU via SMP interconnect (`QPI`/`UPI`) cannot perform P2P
    transfer. For example, `GPU0` will not be able to do P2P with `GPU5`, `GPU6`,
    and `GPU7`. Another way is to figure out this transfer via CUDA APIs, which we
    will be using to convert our code in the next section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的屏幕截图代表了在具有8个GPU的DGX系统上运行`nvidia-smi topo -m`命令的结果。如您所见，通过SMP互连（`QPI`/`UPI`）连接到另一个GPU的任何GPU都无法执行P2P传输。例如，`GPU0`将无法与`GPU5`、`GPU6`和`GPU7`进行P2P传输。另一种方法是通过CUDA
    API来找出这种传输，我们将在下一节中使用它来转换我们的代码。
- en: Now that we have understood the system topology, we can start converting our
    application into multiple GPUs in a single node/server.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了系统拓扑，我们可以开始将我们的应用程序转换为单个节点/服务器上的多个GPU。
- en: Single node – multi-GPU Gaussian elimination
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单节点-多GPU高斯消元
- en: 'Prepare your multi-GPU application. This code can be found at `06_multigpu/gaussian` in
    this book''s GitHub repository. Compile your application with the `nvcc` compiler,
    as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 准备您的多GPU应用程序。此代码可以在本书的GitHub存储库中的`06_multigpu/gaussian`中找到。使用`nvcc`编译器编译您的应用程序，如下所示：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Going from a single-to multi-GPU implementation, the three kernels we defined
    in the previous subsection will be used as-is. However, the linear system is split
    into a number of parts equal to the number of GPUs. These parts are distributed
    one part per GPU. Each GPU is responsible for performing the operation on the
    part that''s been assigned to that GPU. The matrix is split column-wise. This
    means each GPU gets an equal number of consecutive columns from all the rows.
    The kernel for finding the pivot is launched on the GPU that holds the column
    containing the pivot element. The row index of the pivot element is broadcasted
    to other GPUs. The extracted pivot row and row elimination kernels are launched
    on all the GPUs, with each GPU working on its own part of the matrix. The following
    diagram shows the rows being split among multiple GPUs and how the pivot row needs
    to be broadcasted to the rest of the processes:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从单GPU实现转换为多GPU实现，我们在上一小节中定义的三个内核将被原样使用。但是，线性系统被分成与GPU数量相等的部分。这些部分分配给每个GPU一个部分。每个GPU负责对分配给该GPU的部分执行操作。矩阵是按列分割的。这意味着每个GPU从所有行中获得相等数量的连续列。用于找到主元的内核在包含主元素的列上启动。主元元素的行索引被广播到其他GPU。提取的主元行和行消除内核在所有GPU上启动，每个GPU都在矩阵的自己的部分上工作。以下图显示了行在多个GPU之间的分割以及主元行需要广播到其他进程的情况：
- en: '![](img/97345168-60cd-4663-9836-2f391027e26d.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97345168-60cd-4663-9836-2f391027e26d.png)'
- en: The preceding diagram represents the division of work across multiple GPUs.
    Currently, the pivot row belongs to **GPU1** and is responsible for broadcasting
    the pivot row to other GPUs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表示了在多个GPU上的工作分配。目前，主元行属于**GPU1**，负责将主元行广播到其他GPU。
- en: 'Let''s try to understand these code changes, as well as the CUDA API that was
    used to enable the P2P feature:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着理解这些代码更改，以及用于启用P2P功能的CUDA API：
- en: 'Enable P2P access between the supported GPUs. The following code shows the
    first step in this:s enabling P2P access between GPUs:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在支持的GPU之间启用P2P访问。以下代码显示了这个步骤的第一步：启用GPU之间的P2P访问：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The key APIs that were used in the preceding code are as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中使用的关键API如下：
- en: '`cudaDeviceCanAccessPeer()`: Checks if the current GPU can do P2P access to
    the GPU whose ID is sent as a parameter'
  id: totrans-85
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cudaDeviceCanAccessPeer()`: 检查当前GPU是否可以对传递的GPU ID进行P2P访问'
- en: '`cudaDeviceEnablePeerAccess()`: If `cudaDeviceCanAccessPeer()` returns `True`,
    enable P2P access'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cudaDeviceEnablePeerAccess()`: 如果`cudaDeviceCanAccessPeer()`返回`True`，则启用P2P访问'
- en: 'Split and transfer the content to the respective GPUs:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拆分并将内容传输到各自的GPU：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The key API that was used in the preceding code is `cudaSetDevice()`. This sets
    the current context to the GPU ID that was passed as an argument.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中使用的关键API是`cudaSetDevice()`。这将当前上下文设置为作为参数传递的GPU ID。
- en: 'Find the pivot row and broadcast it via P2P:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到主元行并通过P2P进行广播：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The API that's used to broadcast the transfer to GPUs is `cudaMemcpyPeer()`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将传输广播到GPU的API是`cudaMemcpyPeer()`。
- en: 'Extract the pivot row and perform row elimination:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取主元行并执行行消除：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see, we are still reusing the same kernels. The only difference is
    that we use the `cudaSetDevice()` API to tell the CUDA runtime which GPU the kernel
    should be launched on. Note that `cudaSetDevice()` is a costly call, especially
    on older generation GPUs. Therefore, it is advised that you call the for loop
    for `nGPUs` in parallel on the CPU by making use of `OpenMP`/`OpenACC` or any
    other threading mechanism on the CPU.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们仍在重用相同的内核。唯一的区别是我们使用`cudaSetDevice()` API告诉CUDA运行时内核应该在哪个GPU上启动。请注意，`cudaSetDevice()`是一个昂贵的调用，特别是在旧一代的GPU上。因此，建议您通过在CPU上并行调用`nGPUs`的for循环，利用`OpenMP`/`OpenACC`或CPU上的任何其他线程机制来调用。
- en: 'Copy the data back from the respective CPU:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从各自的CPU中复制数据回来：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: These five steps complete the exercise of converting a single GPU implementation
    into a multiple GPU on a single node.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这五个步骤完成了将单个GPU实现转换为单个节点上的多个GPU的练习。
- en: CUDA samples that get shipped as part of CUDA's installation include some sample
    code that tests P2P bandwidth performance. It can be found in the `samples/1_Utilities/p2pBandwidthLatencyTest`
    folder. It is advised that you run this application on your system so that you
    understand the P2P bandwidth and latency of your system.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 作为CUDA安装的一部分提供的CUDA示例包括一些测试P2P带宽性能的示例代码。它可以在`samples/1_Utilities/p2pBandwidthLatencyTest`文件夹中找到。建议您在系统上运行此应用程序，以便了解系统的P2P带宽和延迟。
- en: Now that we've achieved multi-GPU implementation on a single node, we'll change
    gear and run this code on multiple GPUs. But before converting our code into multiple
    GPUs, we will provide a short primer on MPI programming, which is primarily used
    for internode communication.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在单个节点上实现了多GPU，我们将改变方向并在多个GPU上运行此代码。但在将我们的代码转换为多个GPU之前，我们将提供一个关于MPI编程的简短介绍，这主要用于节点间通信。
- en: Brief introduction to MPI
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MPI的简要介绍
- en: The **Message Passing Interface** (**MPI**) standard is a message-passing library
    standard and has become the industry standard for writing message-passing programs
    on HPC platforms. Basically, MPI is used for message passing across multiple MPI
    processes. MPI processes that communicate with each other may reside on the same
    node or across multiple nodes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**消息传递接口**（**MPI**）标准是一种消息传递库标准，已成为在HPC平台上编写消息传递程序的行业标准。基本上，MPI用于在多个MPI进程之间进行消息传递。相互通信的MPI进程可以驻留在同一节点上，也可以跨多个节点。'
- en: 'The following is an example Hello World MPI program:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个Hello World MPI程序的示例：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you can see, the general steps that are involved in the MPI program are
    as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，MPI程序涉及的一般步骤如下：
- en: We include the header file, `mpi.h`, which includes the declaration of all MPI
    API calls.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们包括头文件`mpi.h`，其中包括所有MPI API调用的声明。
- en: We initialize the MPI environment by calling `MPI_Init` and passing the executable
    arguments to it. After this statement, multiple MPI ranks are created and start
    executing in parallel.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过调用`MPI_Init`并将可执行参数传递给它来初始化MPI环境。在这个语句之后，多个MPI等级被创建并开始并行执行。
- en: All MPI processes work in parallel and communicate with each other using message-passing
    APIs such as `MPI_Send()`, `MPI_Recv()`, and so on.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有MPI进程并行工作，并使用诸如`MPI_Send()`、`MPI_Recv()`等消息传递API进行通信。
- en: Finally, we terminate the MPI environment by calling `MPI_Finalize()`.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过调用`MPI_Finalize()`终止MPI环境。
- en: 'We can compile this code using different MPI implementation libraries such
    as OpenMPI, MVPICH, Intel MPI, and so on:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用不同的MPI实现库（如OpenMPI、MVPICH、Intel MPI等）来编译此代码：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We are making use of the `mpicc` compiler to compile our code. `mpicc` is basically
    is a wrapper script that internally expands the compilation instructions to include
    the paths to the relevant libraries and header files. Also, running an MPI executable
    requires it to be passed as an argument to `mpirun`. `mpirun` is a wrapper that
    helps set up the environment across multiple nodes where the application is supposed
    to be executed. The `-n 4` argument says that we want to run four processes and
    that these processes will run on nodes with the hostname stored in the file hosts
    list.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`mpicc`编译器来编译我们的代码。`mpicc`基本上是一个包装脚本，它在内部扩展编译指令，以包括相关库和头文件的路径。此外，运行MPI可执行文件需要将其作为参数传递给`mpirun`。`mpirun`是一个包装器，它帮助在应用程序应该执行的多个节点上设置环境。`-n
    4`参数表示我们要运行四个进程，并且这些进程将在主机名存储在文件主机列表中的节点上运行。
- en: In this chapter, our goal is to integrate GPU kernels with MPI to make it run
    across multiple MPI processes. However, we will not be covering the details of
    MPI programming. Those of you who are not familiar with MPI programming should
    take a look at [https://computing.llnl.gov/tutorials/mpi/](https://computing.llnl.gov/tutorials/mpi/)
    to understand distributed parallel programming before moving on to the next section.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的目标是将GPU内核与MPI集成，使其在多个MPI进程中运行。但我们不会涵盖MPI编程的细节。那些不熟悉MPI编程的人应该先查看[https://computing.llnl.gov/tutorials/mpi/](https://computing.llnl.gov/tutorials/mpi/)，了解分布式并行编程，然后再进入下一节。
- en: GPUDirect RDMA
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPUDirect RDMA
- en: In a cluster environment, we would like to make use of GPUs across multiple
    nodes. We will allow our parallel solver to integrate CUDA code with MPI to utilize
    multi-level parallelism on multi-node, multi-GPU systems. A CUDA-aware MPI is
    used to leverage GPUDirect RDMA for optimized inter-node communication.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群环境中，我们希望在多个节点上利用GPU。我们将允许我们的并行求解器将CUDA代码与MPI集成，以利用多节点、多GPU系统上的多级并行性。使用CUDA-aware
    MPI来利用GPUDirect RDMA进行优化的节点间通信。
- en: 'GPUDirect RDMA allows direct communication between GPUs across a cluster. It
    was first supported by CUDA 5.0 with the Kepler GPU card. In the following diagram,
    we can see the GPUDirect RDMA, that is, **GPU 2** in **Server 1** communicating
    directly with **GPU 1** in **Server 2**:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: GPUDirect RDMA允许在集群中的GPU之间进行直接通信。它首先由CUDA 5.0与Kepler GPU卡支持。在下图中，我们可以看到GPUDirect
    RDMA，即**Server 1**中的**GPU 2**直接与**Server 2**中的**GPU 1**通信：
- en: '![](img/38bb750c-b2e8-44c6-9293-69582300c70e.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38bb750c-b2e8-44c6-9293-69582300c70e.png)'
- en: 'The only theoretical requirement for GPUDirect RDMA to work is that the **Network
    Card** and **GPU** share the same root complex. The path between the GPU and a
    third-party device such as a network adapter decides whether RDMA is supported
    or not. Let''s revisit the output of the `nvidia-smi topo -m` command on the DGX
    system that we ran in the previous section:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: GPUDirect RDMA工作的唯一理论要求是**网络卡**和**GPU**共享相同的根复杂性。 GPU和网络适配器之间的路径决定了是否支持RDMA。让我们重新访问我们在上一节中运行的DGX系统上`nvidia-smi
    topo -m`命令的输出：
- en: '![](img/5cca0b09-e6e3-48c2-8d73-d4603bfc5ba7.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5cca0b09-e6e3-48c2-8d73-d4603bfc5ba7.png)'
- en: If we look at the `GPU4` row, it shows that the `GPU4` to `mlx5_2` connection
    type is `PIX` (traversal via PCIe switch). We can also see that the `GPU4` to
    `mlx_5_0` connection type is `SYS` (traversal via `QPI`). This means that `GPU4`
    can perform RDMA transfers via Mellanox InfiniBand Adapter `mlx_5_2` but not if
    the transfer needs to happen from `mlx_5_0` as `QPI` does not allow RDMA protocols.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下`GPU4`行，它显示`GPU4`到`mlx5_2`连接类型为`PIX`（通过PCIe交换机遍历）。我们还可以看到`GPU4`到`mlx_5_0`连接类型为`SYS`（通过`QPI`遍历）。这意味着`GPU4`可以通过Mellanox
    InfiniBand适配器`mlx_5_2`执行RDMA传输，但如果需要从`mlx_5_0`进行传输，则无法进行RDMA协议，因为`QPI`不允许。
- en: CUDA-aware MPI
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA-aware MPI
- en: 'All the latest versions of the MPI libraries support the GPUDirect feature. MPI
    libraries that support for NVIDIA GPUDirect and **Unified Virtual Addressing**
    (**UVA**) enable the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 所有最新版本的MPI库都支持GPUDirect功能。支持NVIDIA GPUDirect和**统一虚拟寻址**（**UVA**）的MPI库使以下功能可用：
- en: MPI can transfer the API to copy data directly to/from GPU memory (RDMA).
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MPI可以将API传输直接复制到/从GPU内存（RDMA）。
- en: The MPI library can also differentiate between device memory and host memory
    without any hints from the user and hence it becomes transparent to the MPI programmer.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MPI库还可以区分设备内存和主机内存，无需用户提示，因此对MPI程序员透明。
- en: The programmer's productivity increases as less application code needs to be
    changed for data transfers across multiple MPI ranks.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程序员的生产率提高了，因为少量应用代码需要更改以在多个MPI秩之间传输数据。
- en: 'As we mentioned earlier, CPU memory and GPU memory are different. Without a
    CUDA-aware MPI, the developer can only pass pointers pointing to CPU/host memory
    to MPI calls. The following code is an example of using non-CUDA-aware MPI calls:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，CPU内存和GPU内存是不同的。没有CUDA-aware MPI，开发人员只能将指向CPU/主机内存的指针传递给MPI调用。以下代码是使用非CUDA-aware
    MPI调用的示例：
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With a CUDA-aware MPI library, this is not necessary; the GPU buffers can be
    directly passed to MPI, as shown in the following code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有了CUDA-aware MPI库，这是不必要的；GPU缓冲区可以直接传递给MPI，如下所示：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For example, for Open MPI, CUDA-aware support exists in the Open MPI 1.7 series
    and later. To enable this feature, the Open MPI library needs to be configured
    with CUDA support at the time of compilation, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于Open MPI，CUDA-aware支持存在于Open MPI 1.7系列及更高版本中。要启用此功能，需要在编译时配置Open MPI库以支持CUDA，如下所示：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Having a CUDA-aware MPI does not mean that the GPUDirect RDMA is always used.
    The GPUDirect feature is used if the data transfer happens between the network
    card and the GPU that share the same root complex. Nonetheless, even if RDMA support
    is not enabled, having a CUDA-aware MPI makes the application more efficient by
    making use of features such as message transfers, which can be pipelined as shown
    in the following diagram:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有CUDA-aware MPI并不意味着总是使用GPUDirect RDMA。如果数据传输发生在网络卡和GPU之间共享相同的根复杂，则使用GPUDirect功能。尽管如此，即使未启用RDMA支持，拥有CUDA-aware
    MPI也可以通过利用诸如消息传输之类的功能使应用程序更有效，如下图所示可以进行流水线处理：
- en: '![](img/d0de7272-eae9-4914-aee2-400f73f125bf.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/d0de7272-eae9-4914-aee2-400f73f125bf.png)
- en: The preceding diagram shows the CUDA-aware MPI with GPUDirect versus the CUDA-aware
    MPI without GPUDirect. Both calls are from the CUDA-aware MPI, but the left-hand
    side is with GPUDirect transfer and the right-hand side is without GPUDirect transfer.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了具有GPUDirect的CUDA-aware MPI与不具有GPUDirect的CUDA-aware MPI。两个调用都来自CUDA-aware
    MPI，但左侧是GPUDirect传输，右侧是没有GPUDirect传输。
- en: 'Non-GPUDirect transfer has the following stages:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 非GPUDirect传输有以下阶段：
- en: 'Node 1: Transfer from GPU1 to host memory'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点1：从GPU1传输到主机内存
- en: 'Node 1: Transfer from host memory to the network adaptor staging area'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点1：从主机内存传输到网络适配器暂存区
- en: 'Network: Transfer over the network'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络：通过网络传输
- en: 'Node 2: Transfer from the network staging area to host memory'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点2：从网络暂存区传输到主机内存
- en: 'Node 2: Transfer from host memory to GPU memory'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点2：从主机内存传输到GPU内存
- en: If GPUDirect RDMA is supported, the transfer from the GPU happens directly over
    the network and the additional copies involving host memory are all removed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果支持GPUDirect RDMA，则从GPU传输直接通过网络进行，涉及主机内存的额外副本都被删除。
- en: Now that we have grasped this concept, let's start converting the code to enable
    Multi-GPU support using CUDA-aware MPI programming.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了这个概念，让我们开始将代码转换为使用CUDA-aware MPI编程启用多GPU支持。
- en: Multinode – multi-GPU Gaussian elimination
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多节点-多GPU高斯消元
- en: 'Prepare your GPU application. This code can be found at `06_multigpu/gaussian` in
    this book''s GitHub repository. Compile and run your application with the `nvcc`
    compiler, as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 准备您的GPU应用程序。此代码可以在本书的GitHub存储库中的`06_multigpu/gaussian`中找到。使用`nvcc`编译器编译和运行应用程序，如下所示：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We are using `mpicc` instead of `nvcc` to compile the MPI program. We run the
    executable using the `mpirun` command instead of running the compiled executable
    directly. The results that you will see in this section are the output of running
    on the DGX system with 8 V100 on the same system. We make use of the 8 max MPI
    process as we map 1 MPI process per GPU. To understand how to map multiple MPI
    processes onto the same GPU, please read the *MPS* subsection later in the chapter. For
    this exercise, we have used Open MPI 1.10, which has been compiled to support
    CUDA as described in the previous section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`mpicc`而不是`nvcc`来编译MPI程序。我们使用`mpirun`命令运行可执行文件，而不是直接运行已编译的可执行文件。本节中您将看到的结果是在同一系统上具有8个V100的DGX系统上运行的输出。我们利用8个最大MPI进程，将每个GPU映射为1个MPI进程。要了解如何将多个MPI进程映射到同一GPU，请阅读本章后面的*MPS*子节。在本练习中，我们使用了已编译为支持CUDA的Open
    MPI 1.10，如前一节所述。
- en: 'The steps that are involved in the multi-GPU implementation are as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 多GPU实现涉及的步骤如下：
- en: Rank 0 of the MPI process generates data for the linear system (matrices A,
    B).
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MPI进程的秩0生成线性系统（矩阵A，B）的数据。
- en: The transposed augmented matrix (AB^T) is split row-wise by the root among the
    MPI processes using `MPI_Scatterv()`.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转置增广矩阵（AB^T）由根节点在MPI进程之间使用`MPI_Scatterv()`按行分割。
- en: 'Each MPI process computes on its part of the input in parallel:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个MPI进程并行计算其部分输入：
- en: Processing the three kernels happens on the GPU.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个内核的处理发生在GPU上。
- en: The consensus of the pivot is achieved after the `findPivot` operation using
    `MPI_Send()`/`Recv()`.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`findPivot`操作后，通过`MPI_Send()`/`Recv()`实现了枢轴的共识。
- en: The reduced **transposed augmented matrix** (**ABT**) is gathered on the root
    using `MPI_Gatherv()`.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少的**转置增广矩阵**（**ABT**）使用`MPI_Gatherv()`在根节点上收集。
- en: The root performs back substitution to compute solution X.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根节点执行回代以计算解X。
- en: 'The extracted sample Gaussian code which showcases the preceding code is as
    follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 展示前面代码的提取样本高斯代码如下：
- en: '[PRE17]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let''s add multi-GPU support:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们添加多GPU支持：
- en: '**Set the CUDA device per MPI rank**:In Open MPI, you can get the local rank
    of the MPI process by making use of `MPI_COMM_TYPE_SHARED` as a parameter to `MPI_Comm_split_type`, as
    shown in the following code:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置每个MPI等级的CUDA设备**：在Open MPI中，您可以通过使用`MPI_COMM_TYPE_SHARED`作为`MPI_Comm_split_type`的参数来获得MPI进程的本地等级，如下面的代码所示：'
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now that we have the local rank, each MPI process uses it to set the current
    GPU by using `cudaSetDevice()`, as shown in the following diagram:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了本地等级，每个MPI进程都使用它来通过`cudaSetDevice()`设置当前GPU，如下图所示：
- en: '![](img/316a4f63-8ca7-4540-a2c2-47b2dcc0e122.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/316a4f63-8ca7-4540-a2c2-47b2dcc0e122.png)'
- en: 'Split and distribute the inputs to different MPI processes using `MPI_Scatter`:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`MPI_Scatter`将输入拆分并分发到不同的MPI进程：
- en: '[PRE19]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Perform Gaussian elimination on the GPU:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GPU上执行高斯消元：
- en: '[PRE20]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Before performing any operation, the current GPU is set based on local rank.
    Then, the pivot row is extracted by the process which is responsible for that
    row, followed by the pivot row being broadcasted to all the other MPI ranks, which
    we use for elimination.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行任何操作之前，基于本地等级设置当前GPU。然后，由负责该行的进程提取枢轴行，然后将枢轴行广播到所有其他MPI等级，我们用于消除。
- en: The overall performance of the transfer time can be improved by making use of
    asynchronous MPI calls instead of using broadcast APIs such as `MPI_Bcast`. In
    fact, the use of a broadcast API is discouraged; it should be replaced with `MPI_Isend`
    and `MPI_Irecv`, which are asynchronous versions that can achieve the same functionality.
    Please note that making the calls asynchronous adds complexity to other aspects
    such as debugging. Due to this, the user needs to write additional code to send
    and receive data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用异步MPI调用而不是使用广播API（如`MPI_Bcast`），可以提高传输时间的整体性能。实际上，不建议使用广播API；它应该被替换为可以实现相同功能的`MPI_Isend`和`MPI_Irecv`，这些是异步版本。请注意，使调用异步会增加其他方面（如调试）的复杂性。因此，用户需要编写额外的代码来发送和接收数据。
- en: This chapter provides the best coding practices when it comes to adding GPU
    support to an existing MPI program and should not be considered an expert guide
    on the best programming practices for MPI programming.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了在向现有MPI程序添加GPU支持时的最佳编码实践，并不应被视为MPI编程的最佳编程实践的专家指南。
- en: CUDA streams
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA流
- en: Streams act in a FIFO manner, where the sequence of operations is executed in
    the order of when they were issued. Requests that are made from the host code
    are put into First-In-First-Out queues. Queues are read and processed asynchronously
    by the driver, and the device driver ensures that the commands in a queue are
    processed in sequence. For example, memory copies end before kernel launch, and
    so on.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 流以FIFO方式工作，其中操作的顺序按照它们发出的顺序执行。从主机代码发出的请求被放入先进先出队列中。队列由驱动程序异步读取和处理，并且设备驱动程序确保队列中的命令按顺序处理。例如，内存复制在内核启动之前结束，依此类推。
- en: The general idea of using multiple streams is that CUDA operations that are
    fired in different streams may run concurrently. This can result in multiple kernels
    overlapping or overlapping memory copies within the kernel execution.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个流的一般想法是，在不同流中触发的CUDA操作可能会并发运行。这可能导致多个内核重叠或内核执行中的内存复制重叠。
- en: To understand CUDA streams, we will be looking at two applications. The first
    application is a simple vector addition code with added streams so that it can
    overlap data transfers with kernel execution. The second application is of an
    image merging application, which will also be used in [Chapter 9](9335adec-9dd0-4f6f-8eea-9ce4ca8912e5.xhtml),
    *GPU Programming Using OpenACC*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解CUDA流，我们将看两个应用程序。第一个应用程序是一个简单的矢量加法代码，添加了流，以便它可以重叠数据传输和内核执行。第二个应用程序是一个图像合并应用程序，也将在[第9章](9335adec-9dd0-4f6f-8eea-9ce4ca8912e5.xhtml)中使用，*使用OpenACC进行GPU编程*。
- en: 'To start, configure your environment according to the following steps:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，根据以下步骤配置您的环境：
- en: Prepare your GPU application. As an example, we will be merging two images.
    This code can be found in the `06_multi-gpu/streams` folder in this book's GitHub
    repository.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备您的GPU应用程序。例如，我们将合并两个图像。此代码可以在本书的GitHub存储库的`06_multi-gpu/streams`文件夹中找到。
- en: 'Compile your application with the `nvcc` compiler as follows:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nvcc`编译器编译您的应用程序如下：
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The preceding commands will create two binaries named `vector_addition` and
    `merging_multi_gpu`. As you might have observed, we are using additional arguments
    in our code. Let''s understand them in more detail:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将创建两个名为`vector_addition`和`merging_multi_gpu`的二进制文件。正如您可能已经注意到的，我们在我们的代码中使用了额外的参数。让我们更详细地了解它们：
- en: '`--default-stream per-thread`: This flag tells the compiler to parse the OpenACC
    directives provided in the code.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--default-stream per-thread`：此标志告诉编译器解析代码中提供的OpenACC指令。'
- en: '`-Xcompiler -fopenmp -lgomp`: This flag tells `nvcc` to pass these additional
    flags to the CPU compiler underneath to compile the CPU part of the code. In this
    case, we are asking the compiler to add OpenMP-related libraries to our application.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-Xcompiler -fopenmp -lgomp`：此标志告诉`nvcc`将这些附加标志传递给CPU编译器，以编译代码的CPU部分。在这种情况下，我们要求编译器向我们的应用程序添加与OpenMP相关的库。'
- en: We will divide this section into two parts. Application 1 and application 2
    demonstrate using streams in single and multiple GPUs, respectively.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这一部分分为两部分。应用程序1和应用程序2分别演示了在单个和多个GPU中使用流。
- en: Application 1 – using multiple streams to overlap data transfers with kernel
    execution
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序1-使用多个流来重叠数据传输和内核执行
- en: 'The steps that we need to follow to overlap data transfers with kernel execution
    or to launch multiple kernels concurrently are as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要遵循的步骤来重叠数据传输和内核执行，或者同时启动多个内核如下：
- en: 'Declare the host memory to be pinned, as shown in the following code snippet:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明要固定的主机内存，如下面的代码片段所示：
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, we are making use of the `cudaMallocHost()` API to allocate vectors as
    pinned memory.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`cudaMallocHost()` API来分配固定内存的向量。
- en: 'Create a `Stream` object, as shown in the following code snippet:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`Stream`对象，如下面的代码片段所示：
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here, we make use of the `cudaStreamCreateWithFlags()` API, passing `cudaStreamNonBlocking`
    as the flag to make this stream non-blocking.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`cudaStreamCreateWithFlags()` API，传递`cudaStreamNonBlocking`作为标志，使此流非阻塞。
- en: 'Call the CUDA kernel and memory copies with the `stream` flag, as shown in
    the following code snippet:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用CUDA内核和内存复制时使用`stream`标志，如下面的代码片段所示：
- en: '[PRE24]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As we can see, instead of performing the vector addition in one shot by copying
    the whole array once, instead we chunk the array into segments and copy the segments
    asynchronously. Kernel execution is also done asynchronously in the respective
    streams.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们不是通过一次复制整个数组来执行矢量加法，而是将数组分成段，并异步复制这些段。内核执行也是在各自的流中异步进行的。
- en: 'When we run this code through Visual Profiler, we can see the following characteristics:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过Visual Profiler运行这段代码时，我们可以看到以下特点：
- en: '![](img/ccb79f18-189a-4f8c-84dc-6af4ada0e01f.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccb79f18-189a-4f8c-84dc-6af4ada0e01f.png)'
- en: The preceding profiler screenshot shows that the blue bars (which are basically
    `vector_addition` kernels) overlap the memory copies. Since we created four streams
    in our code, there are four streams in the profiler as well.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的分析器截图显示，蓝色条（基本上是`vector_addition`内核）重叠了内存复制。由于我们在代码中创建了四个流，分析器中也有四个流。
- en: 'Every GPU has two memory copy engines. One is responsible for the host to device
    transfer while the other is responsible for the device to host transfer. Hence,
    the two memory copies, which happen in opposite directions, can be overlapped.
    Also, the memory copies can be overlapped with the compute kernels. This can result
    in *n*-way concurrency, as shown in the following diagram:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 每个GPU都有两个内存复制引擎。一个负责主机到设备的传输，另一个负责设备到主机的传输。因此，发生在相反方向的两个内存复制可以重叠。此外，内存复制可以与计算内核重叠。这可以导致*n*路并发，如下图所示：
- en: '![](img/c4eedad6-44a2-4498-8647-246244ec6999.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4eedad6-44a2-4498-8647-246244ec6999.png)'
- en: 'Every GPU architecture comes with certain constraints and rules based on which
    we will see these overlaps at execution time. In general, the following are some
    guidelines:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 每个GPU架构都有一定的约束和规则，根据这些规则，我们将在执行时看到这些重叠。一般来说，以下是一些指导方针：
- en: CUDA operations must be in different, non-0 streams.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA操作必须在不同的非0流中。
- en: '`cudaMemcpyAsync` with the host should be pinned using `cudaMallocHost()` or
    `cudaHostAlloc()`.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`cudaMemcpyAsync`时，主机应该使用`cudaMallocHost()`或`cudaHostAlloc()`进行固定。
- en: Sufficient resources must be available
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须有足够的资源可用。
- en: '`cudaMemcpyAsyncs` in different directions'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同方向的`cudaMemcpyAsyncs`
- en: Device resources (SMEM, registers, blocks, and so on) to launch multiple concurrent
    kernels
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备资源（SMEM、寄存器、块等）以启动多个并发内核
- en: Application 2 – using multiple streams to run kernels on multiple devices
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序2 - 使用多个流在多个设备上运行内核
- en: 'To run kernels and overlap memory transfers across multiple devices, the steps
    that we followed previously remain the same, except for one additional step: setting
    the CUDA device to create the stream. Let''s have a look at the following steps:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在多个设备上运行内核并重叠内存传输，我们之前遵循的步骤保持不变，除了一个额外的步骤：设置CUDA设备以创建流。让我们看看以下步骤：
- en: 'Create streams equal to the number of CUDA devices present in the system, as
    shown in the following code snippet:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建与系统中CUDA设备数量相等的流，如下面的代码片段所示：
- en: '[PRE25]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We make use of the `cudaGetDeviceCount()` API to get the number of CUDA devices.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`cudaGetDeviceCount()` API来获取CUDA设备的数量。
- en: 'Create the stream in the respective device, as shown in the following code
    snippet:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在各自的设备中创建流，如下面的代码片段所示：
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We are launching OpenMP threads equal to the number of CUDA devices so that
    each CPU thread can create its own CUDA stream for its respective devices. Each
    CPU thread executes `cudaSetDevice()` to set the current GPU based on its ID and
    then creates the stream for that device.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们启动与CUDA设备数量相等的OpenMP线程，以便每个CPU线程可以为其各自的设备创建自己的CUDA流。每个CPU线程执行`cudaSetDevice()`来根据其ID设置当前GPU，然后为该设备创建流。
- en: 'Launch the kernel and memory copies in that stream, as follows:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在该流中启动内核和内存复制，如下所示：
- en: '[PRE27]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output after running the code in the profiler can be seen in the following
    screenshot, which represents the Visual Profiler''s timeline view. This shows
    an overlapping memory copy for one GPU with the kernel execution of the other
    GPU:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析器中运行代码后的输出可以在下面的截图中看到，这代表了Visual Profiler的时间轴视图。这显示了一个GPU的内存复制与另一个GPU的内核执行重叠：
- en: '![](img/dfdd5f05-a117-4032-8736-123f1f2e9cc3.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfdd5f05-a117-4032-8736-123f1f2e9cc3.png)'
- en: As you can see, we ran this code on the multi-GPU system with four V100s. The
    memory copies and kernels in the different GPUs overlap each other. In this code,
    we demonstrated making use of OpenMP to call CUDA kernels in parallel on different
    devices. This can also be done by making use of MPI to launch multiple processes
    that utilize different GPUs.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们在拥有四个V100的多GPU系统上运行了这段代码。不同GPU中的内存复制和内核重叠。在这段代码中，我们演示了利用OpenMP在不同设备上并行调用CUDA内核。这也可以通过利用MPI来启动利用不同GPU的多个进程来实现。
- en: In the next section, we will take a look at some additional topics that can
    improve the performance of multi-GPU applications and help developers profile
    and debug their code.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看一些额外的主题，这些主题可以提高多GPU应用程序的性能，并帮助开发人员分析和调试他们的代码。
- en: Additional tricks
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外的技巧
- en: In this section, we will cover some additional topics that will help us understand
    the additional characteristics of the multi-GPU system.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖一些额外的主题，这些主题将帮助我们了解多GPU系统的额外特性。
- en: Benchmarking an existing system with an InfiniBand network card
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用InfiniBand网络卡对现有系统进行基准测试
- en: 'Different benchmarks are available for testing the RDMA feature. One such benchmark
    for the InfiniBand adapter can be found at [https://www.openfabrics.org/](https://www.openfabrics.org/).
    You can test your bandwidth by executing the following code:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的基准可用于测试RDMA功能。InfiniBand适配器的一个这样的基准可以在[https://www.openfabrics.org/](https://www.openfabrics.org/)找到。您可以通过执行以下代码来测试您的带宽：
- en: '[PRE28]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, you can run the following commands to test the bandwidth:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以运行以下命令来测试带宽：
- en: '[PRE29]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: NVIDIA Collective Communication Library (NCCL)
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NVIDIA集体通信库（NCCL）
- en: 'The NCCL provides an implementation of communication primitives that are commonly
    used in domains such as deep learning. NCCL 1.0 started with the implementation
    of communication primitives across multiple GPUs within the same node and evolved
    to support multiple GPUs in multiple nodes. Some key features of the NCCL library
    include the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: NCCL提供了常用于深度学习等领域的通信原语的实现。NCCL 1.0从同一节点内多个GPU之间的通信原语实现开始，并发展到支持多个节点上的多个GPU。NCCL库的一些关键特性包括以下内容：
- en: Supports calls from multiple threads and multiple processes
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持来自多个线程和多个进程的调用
- en: Supports the multiple ring and tree topology for better bus utilization within
    and across nodes
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持多个环和树拓扑，以更好地利用节点内和节点间的总线
- en: Supports InfiniBand inter-node communication
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持InfiniBand节点间通信
- en: The source package is free to download from GitHub ([https://github.com/nvidia/nccl](https://github.com/nvidia/nccl))
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源代码包可以从GitHub（[https://github.com/nvidia/nccl](https://github.com/nvidia/nccl)）免费下载
- en: 'NCCL can be scaled up to 24,000 GPUs, well below the 300 microsecond latency.
    Note that NCCL has proven to be a really useful and handy library for deep learning
    frameworks but has its limitation when used for HPC applications as it does not
    support point-to-point communication. NCCL supports collective operations, which
    are used in deep learning applications such as the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: NCCL可以扩展到24,000个GPU，延迟低于300微秒。请注意，尽管NCCL已被证明是深度学习框架中非常有用和方便的库，但在用于HPC应用时存在局限，因为它不支持点对点通信。NCCL支持集体操作，这在深度学习应用中被使用，例如以下内容：
- en: '`AllReduce`'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AllReduce`'
- en: '`AllGather`'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AllGather`'
- en: '`ReduceScatter`'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReduceScatter`'
- en: '`Reduce`'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Reduce`'
- en: '`Broadcast`'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Broadcast`'
- en: 'All NCCL calls run as CUDA kernels for faster access to GPU memory. It makes
    use of fewer threads that are implemented as one block. This ends up running only
    on one GPU SM and hence does not affect the utilization of other GPUs. Let''s
    have a look at the following code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 所有NCCL调用都作为CUDA内核运行，以更快地访问GPU内存。它使用较少的线程，实现为一个块。这最终只在一个GPU SM上运行，因此不会影响其他GPU的利用率。让我们看一下以下代码：
- en: '[PRE30]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As we can see, NCCL calls are simple and can be called with ease.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，NCCL调用简单，易于调用。
- en: Collective communication acceleration using NCCL
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NCCL加速集体通信
- en: The **NVIDIA Collective Communication Library **(**NCCL**) provides a performance-optimized
    collective of communication primitives for multiple NVIDIA GPUs. In this section,
    we will see how this library works and how we can benefit from using it.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**NVIDIA集体通信库（NCCL）**提供了为多个NVIDIA GPU优化的性能集体通信原语。在本节中，我们将看到这个库是如何工作的，以及我们如何从中受益。'
- en: 'It isn''t difficult to find deep learning models that use multiple GPUs to
    train the network. Since two GPUs compute the neural network in parallel, we can
    easily imagine that this technique will increase training performance along with
    the GPU numbers. Unfortunately, the world is not that simple. The gradients should
    be shared across multiple GPUs and the weight update procedure in one GPU should
    wait for the others'' gradients to update its weights. This is the general procedure
    of deep learning training with multiple GPUs, and is shown in the following diagram:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 并不难找到使用多个GPU来训练网络的深度学习模型。由于两个GPU并行计算神经网络，我们很容易想象这种技术将随着GPU数量的增加而提高训练性能。不幸的是，世界并不那么简单。梯度应该在多个GPU之间共享，并且一个GPU中的权重更新过程应该等待其他GPU的梯度来更新其权重。这是使用多个GPU进行深度学习训练的一般过程，并在以下图表中显示：
- en: '![](img/914037ea-4291-4208-a867-6281cff0b742.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/914037ea-4291-4208-a867-6281cff0b742.png)'
- en: 'Collective communication has many types: all-reduce, broadcast, reduce, all
    gather, reduce scatter, and so on. In deep learning, each GPU collects another
    GPU''s data while it transfers its own to the other GPUs. Therefore, we can determine
    that deep learning needs all types of reducing style communication in their communication.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 集体通信有许多类型：全局归约、广播、归约、全局收集、归约散射等。在深度学习中，每个GPU在传输自己的数据的同时收集另一个GPU的数据。因此，我们可以确定深度学习在通信中需要所有类型的归约样式通信。
- en: In the HPC community, collective communication, including all-reduce, is quite
    a common topic. Communication between the processor from inter- and intra-nodes
    was a challenging but crucial issue because it's directly related to scalability.
    As we mentioned in [Chapter 6](ba3092b0-9a57-4137-8ec9-229253c98552.xhtml), *Scalable
    Multi-GPU Programming*, in the *Multiple GPU programming* section, it requires
    a lot of consideration to communicate with each GPU. The developer should design
    and implement collective communication in GPUs, even though MPI already supports
    such communication patterns.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在HPC社区中，包括全局归约在内的集体通信是一个常见的话题。节点内和节点间处理器之间的通信是一个具有挑战性但至关重要的问题，因为它直接关系到可扩展性。正如我们在[第6章](ba3092b0-9a57-4137-8ec9-229253c98552.xhtml)中提到的，*可扩展的多GPU编程*，在*多GPU编程*部分，需要仔细考虑与每个GPU的通信。开发人员应该设计和实现GPU中的集体通信，即使MPI已经支持这样的通信模式。
- en: NCCL provides such a collective that's aware of the GPU topology configuration.
    By using a variety of grouping and communication commands, you can apply the required
    communication task.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: NCCL提供了一种集体通信，它了解GPU拓扑配置。通过使用各种分组和通信命令，您可以应用所需的通信任务。
- en: One prerequisite is that your system needs to have more than one GPU because
    NCCL is a communication library that works with multiple GPUs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一个前提是您的系统需要有多个GPU，因为NCCL是一个与多个GPU一起工作的通信库。
- en: 'The following steps cover how to call `ncclAllReduce()` as a test and measure
    the system''s GPU network bandwidth. The sample code is implemented in `04_nccl`:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤涵盖了如何调用`ncclAllReduce()`来测试和测量系统的GPU网络带宽。示例代码实现在`04_nccl`中：
- en: 'Let''s define a type that will contain, send, and receive, a buffer and `cudaStream` for
    each GPU device, as follows:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一个类型，它将包含、发送和接收每个GPU设备的缓冲区和`cudaStream`，如下所示：
- en: '[PRE31]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'At the beginning of the application, we need to prepare some handles so that
    we can control multiple GPUs:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在应用程序开始时，我们需要准备一些句柄，以便我们可以控制多个GPU：
- en: '[PRE32]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, we will create a buffer, assuming that we have data. For each device,
    we will initialize each device''s items, as follows:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个缓冲区，假设我们有数据。对于每个设备，我们将初始化每个设备的项目，如下所示：
- en: '[PRE33]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Before starting the NCCL communication, we need to initialize the GPU devices
    so that they are aware of their rank across the GPU group. Since we will be testing
    the bandwidth with a single process, we are safe to call a function that initializes
    all the devices:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始NCCL通信之前，我们需要初始化GPU设备，以便它们知道它们在GPU组中的排名。由于我们将用单个进程测试带宽，我们可以安全地调用一个初始化所有设备的函数：
- en: '[PRE34]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: If we are testing the bandwidth with multiple processes, we need to call `ncclCommInitRank()`.
    We will need to provide GPU IDs for counting the process IDs and GPU ranks.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们要用多个进程测试带宽，我们需要调用`ncclCommInitRank()`。我们需要为计算进程ID和GPU排名提供GPU ID。
- en: 'Now, we can complete the all-reduce operations with NCCL. The following code
    is an example implementation of `ncclAllReduce`:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用NCCL完成all-reduce操作。以下代码是`ncclAllReduce`的示例实现：
- en: '[PRE35]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: For each device, we need to trigger the traffic. For this, we need to start
    and close NCCL group communication. Now, we have implemented some test code that
    uses `ncclAllReduce()`. Let's cover how NCCL works by micro-benchmarking our system.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个设备，我们需要触发流量。为此，我们需要启动和关闭NCCL组通信。现在，我们已经实现了一些使用`ncclAllReduce()`的测试代码。让我们通过微基准测试来了解NCCL的工作原理。
- en: 'Let''s test this code on a multi-GPU system, run the following command:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在多GPU系统上测试此代码，运行以下命令：
- en: '[PRE36]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following diagram shows the performance that was measured using four V100
    32G GPUs in DGX Station. The blue line denotes the NVLink-based bandwidth, while
    the orange line denotes PCIe-based bandwidth, which it does by setting `NCCL_P2P_DISABLE=1
    ./ncd` and turning off peer-to-peer GPUs:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了在DGX Station中使用四个V100 32G GPU测得的性能。蓝线表示基于NVLink的带宽，而橙线表示基于PCIe的带宽，通过设置`NCCL_P2P_DISABLE=1
    ./ncd`并关闭对等GPU来实现：
- en: '![](img/7576c92f-ae93-41d2-bd31-8943edd22b8c.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7576c92f-ae93-41d2-bd31-8943edd22b8c.png)'
- en: This NCCL test can be impacted by the system's configuration. This means that
    the result can vary, depending on your system's GPU topology.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这个NCCL测试可能会受到系统配置的影响。这意味着结果可能会有所不同，取决于您系统的GPU拓扑结构。
- en: 'This shows the difference between PCI express-based and NVLINK-based all-reduce performance.
    We can see its communication using `nvprof`. The following screenshot shows NCCL''s
    all-reduce communication in DGX Station via NCCL 2.3.7:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了基于PCI Express和基于NVLINK的all-reduce性能差异。我们可以使用`nvprof`来查看通信。以下屏幕截图显示了通过NCCL
    2.3.7在DGX Station上的all-reduce通信：
- en: '![](img/8acf126f-830f-4d1a-bb3b-58ff6ab62223.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8acf126f-830f-4d1a-bb3b-58ff6ab62223.png)'
- en: NCCL is getting faster and faster. By introducing new GPU interconnects with
    NVLink and NVSwitch, our experience with NCCL is increasing, so much so that we
    can achieve scalable performance.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: NCCL越来越快。通过引入新的GPU互连技术NVLink和NVSwitch，我们对NCCL的经验正在增加，以至于我们可以实现可扩展的性能。
- en: The following link provides a discussion about NCCL: [https://developer.nvidia.com/gtc/2019/video/S9656/video](https://developer.nvidia.com/gtc/2019/video/S9656/video).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了关于NCCL的讨论：[https://developer.nvidia.com/gtc/2019/video/S9656/video](https://developer.nvidia.com/gtc/2019/video/S9656/video)。
- en: Summary
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered different approaches to multi-GPU programming. With
    the help of an example Gaussian elimination, we saw how a single GPU application
    workload can be split across multiple GPUs, first into a single node and then
    into multiple nodes. We saw how system topology plays an important role in making
    use of features such as P2P transfer and GPUDirect RDMA. We also saw how multiple
    CUDA streams can be used to overlap communication and data transfer among multiple
    GPUs. We also briefly covered some additional topics that can help CUDA programmers
    optimize code such as MPS and the use of `nvprof` to profile multi-GPU applications.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了多GPU编程的不同方法。通过示例高斯消元，我们看到了如何将单个GPU应用程序工作负载分割到多个GPU中，首先是单个节点，然后是多个节点。我们看到了系统拓扑在利用P2P传输和GPUDirect
    RDMA等功能方面起着重要作用。我们还看到了如何使用多个CUDA流来重叠多个GPU之间的通信和数据传输。我们还简要介绍了一些其他主题，可以帮助CUDA程序员优化代码，如MPS和使用`nvprof`来分析多GPU应用程序。
- en: In the next chapter, we will look at common patterns that appear in most HPC
    applications and how to implement them in GPUs.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到大多数HPC应用程序中出现的常见模式以及如何在GPU中实现它们。
