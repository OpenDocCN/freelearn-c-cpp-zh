["```cpp\ncudaStream_t stream;\ncudaStreamCreate(&stream);\nfoo_kernel<<< grid_size, block_size, 0, stream >>>();\ncudaStreamDestroy(stream);\n```", "```cpp\n__global__ void foo_kernel(int step)\n{\n    printf(\"loop: %d\\n\", step);\n}\n\nint main()\n{\n    for (int i = 0; i < 5; i++)\n // CUDA kernel call with the default stream\n foo_kernel<<< 1, 1, 0, 0 >>>(i);\n    cudaDeviceSynchronize();\n    return 0;\n}\n```", "```cpp\n$ nvcc -m64 -run -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o cuda_default_stream ./1_cuda_default_stream.cu\n```", "```cpp\n__global__ void foo_kernel(int step)\n{\n    printf(\"loop: %d\\n\", step);\n}\n\nint main()\n{\n    int n_stream = 5;\n    cudaStream_t *ls_stream;\n    ls_stream = (cudaStream_t*) new cudaStream_t[n_stream];\n\n    // create multiple streams\n    for (int i = 0; i < n_stream; i++)\n        cudaStreamCreate(&ls_stream[i]);\n\n    // execute kernels with the CUDA stream each\n    for (int i = 0; i < n_stream; i++)\n        foo_kernel<<< 1, 1, 0, ls_stream[i] >>>(i);\n\n    // synchronize the host and GPU\n    cudaDeviceSynchronize();\n\n    // terminates all the created CUDA streams\n    for (int i = 0; i < n_stream; i++)\n        cudaStreamDestroy(ls_stream[i]);\n    delete [] ls_stream;\n\n    return 0;\n}\n```", "```cpp\n$ nvcc -m64 -run -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o cuda_mutli_stream ./2_cuda_multi_stream.cu\n```", "```cpp\n// execute kernels with the CUDA stream each\nfor (int i = 0; i < n_stream; i++) {\n   foo_kernel<<< 1, 1, 0, ls_stream[i] >>>(i);\n   cudaStreamSynchronize(ls_stream[i]);\n}\n```", "```cpp\n$ nvcc -m64 -run -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o cuda_mutli_stream_with_sync ./3_cuda_multi_stream_with_sync.cu\n```", "```cpp\nfor (int i = 0; i < n_stream; i++)\n    if (i == 3)\n        foo_kernel<<< 1, 1, 0, 0 >>>(i);\n    else\n        foo_kernel<<< 1, 1, 0, ls_stream[i] >>>(i);\n```", "```cpp\n$ nvcc -m64 -run -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o cuda_multi_stream_with_default ./4_cuda_multi_stream_with_default.cu\n```", "```cpp\ncudaStream_t stream;\nfloat *h_ptr, *d_ptr;    size_t byte_size = sizeof(float) * BUF_SIZE;\n\ncudaStreamCreate(&stream);               // create CUDA stream\ncudaMallocHost(h_ptr, byte_size);        // allocates pinned memory\ncudaMalloc((void**)&d_ptr, byte_size);   // allocates a global memory\n\n// transfer the data from host to the device asynchronously\ncudaMemcpyAsync(d_ptr, h_ptr, byte_size, cudaMemcpyHostToDevice, stream);\n\n... { kernel execution } ...\n\n// transfer the data from the device to host asynchronously\ncudaMemcpyAsync(h_ptr, d_ptr, byte_size, cudaMemcpyDeviceToHost, stream);\ncudaStreamSynchronize(stream);\n\n// terminates allocated resources\ncudaStreamDestroy(stream);\ncudaFree(d_ptr);\ncudaFreeHost(h_ptr);\n```", "```cpp\n__global__ void\nvecAdd_kernel(float *c, const float* a, const float* b)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    for (int i = 0; i < 500; i++)\n        c[idx] = a[idx] + b[idx];\n}\n```", "```cpp\nclass Operator\n{\nprivate:\n    int index;\n\npublic:\n    Operator() {\n        cudaStreamCreate(&stream);    // create a CUDA stream\n    }\n\n    ~Operator() {\n        cudaStreamDestroy(stream);    // terminate the CUDA stream\n    }\n\n    cudaStream_t stream;\n    void set_index(int idx) { index = idx; }\n    void async_operation(float *h_c, const float *h_a, \n                         const float *h_b,\n                         float *d_c, float *d_a, float *d_b,\n                         const int size, const int bufsize);\n\n}; // Operator\n```", "```cpp\nvoid Operator::async_operation(float *h_c, const float *h_a, \n                          const float *h_b,\n                          float *d_c, float *d_a, float *d_b,\n                          const int size, const int bufsize)\n{\n    // start timer\n    sdkStartTimer(&_p_timer);\n\n    // copy host -> device\n    cudaMemcpyAsync(d_a, h_a, bufsize, \n                    cudaMemcpyHostToDevice, stream);\n    cudaMemcpyAsync(d_b, h_b, bufsize, \n                    cudaMemcpyHostToDevice, stream);\n\n    // launch cuda kernel\n    dim3 dimBlock(256);\n    dim3 dimGrid(size / dimBlock.x);\n    vecAdd_kernel<<< dimGrid, dimBlock, 0, \n                     stream >>>(d_c, d_a, d_b);\n\n    // copy device -> host\n    cudaMemcpyAsync(h_c, d_c, bufsize, \n                    cudaMemcpyDeviceToHost, stream);\n\n    printf(\"Launched GPU task %d\\n\", index);\n}\n```", "```cpp\nint main(int argc, char* argv[])\n{\n    float *h_a, *h_b, *h_c;\n    float *d_a, *d_b, *d_c;\n    int size = 1 << 24;\n    int bufsize = size * sizeof(float);\n    int num_operator = 4;\n\n    if (argc != 1)\n        num_operator = atoi(argv[1]);\n```", "```cpp\n    cudaMallocHost((void**)&h_a, bufsize);\n    cudaMallocHost((void**)&h_b, bufsize);\n    cudaMallocHost((void**)&h_c, bufsize);\n\n    srand(2019);\n    init_buffer(h_a, size);\n    init_buffer(h_b, size);\n    init_buffer(h_c, size);\n```", "```cpp\n    cudaMalloc((void**)&d_a, bufsize);\n    cudaMalloc((void**)&d_b, bufsize);\n    cudaMalloc((void**)&d_c, bufsize);\n```", "```cpp\n    Operator *ls_operator = new Operator[num_operator];\n```", "```cpp\n    StopWatchInterface *timer;\n    sdkCreateTimer(&timer);\n    sdkStartTimer(&timer);\n```", "```cpp\n    for (int i = 0; i < num_operator; i++) {\n        int offset = i * size / num_operator;\n        ls_operator[i].set_index(i);\n        ls_operator[i].async_operation(&h_c[offset], \n                                       &h_a[offset], &h_b[offset],\n                                       &d_c[offset], \n                                       &d_a[offset], &d_b[offset],\n                                       size / num_operator, \n                                       bufsize / num_operator);\n    }\n\n    cudaDeviceSynchronize();\n    sdkStopTimer(&timer);\n```", "```cpp\n    // prints out the result\n    int print_idx = 256;\n    printf(\"compared a sample result...\\n\");\n    printf(\"host: %.6f, device: %.6f\\n\", h_a[print_idx] + \n           h_b[print_idx], h_c[print_idx]);\n\n    // prints out the performance\n    float elapsed_time_msed = sdkGetTimerValue(&timer);\n    float bandwidth = 3 * bufsize * sizeof(float) / \n                      elapsed_time_msed / 1e6;\n    printf(\"Time= %.3f msec, bandwidth= %f GB/s\\n\", \n           elapsed_time_msed, bandwidth);\n```", "```cpp\n    sdkDeleteTimer(&timer);\n    delete [] ls_operator;\n    cudaFree(d_a);    cudaFree(d_b);    cudaFree(d_c);\n    cudaFreeHost(h_a);cudaFreeHost(h_b);cudaFreeHost(h_c);\n```", "```cpp\n$ nvcc -m64 -run -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o cuda_pipelining ./cuda_pipelining.cu\n```", "```cpp\nLaunched GPU task 0\nLaunched GPU task 1\nLaunched GPU task 2\nLaunched GPU task 3\ncompared a sample result...\nhost: 1.523750, device: 1.523750\nTime= 29.508 msec, bandwidth= 27.291121 GB/s\n```", "```cpp\n$ nvprof -o overlapping_exec.nvvp ./overlapping_exec\n```", "```cpp\nfor (int i = 0; i < num_operator; i++) {\n    cudaStreamSynchronize(ls_operator[i]._stream);\n}\n```", "```cpp\nStopWatchInterface *_p_timer;\nstatic void CUDART_CB Callback(cudaStream_t stream, cudaError_t status, void* userData);\nvoid print_time();\n```", "```cpp\nvoid Operator::CUDART_CB Callback(cudaStream_t stream, cudaError_t status, void* userData) {\n    Operator* this_ = (Operator*) userData;\n    this_->print_time();\n}\n\nvoid Operator::print_time() {\n    sdkStopTimer(&p_timer);    // end timer\n    float elapsed_time_msed = sdkGetTimerValue(&p_timer);\n    printf(\"stream %2d - elapsed %.3f ms \\n\", index, \n           elapsed_time_msed);\n}\n```", "```cpp\n// register callback function\ncudaStreamAddCallback(stream, Operator::Callback, this, 0);\n```", "```cpp\n$ nvcc -m64 -run -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o cuda_callback ./cuda_callback.cu\n```", "```cpp\nstream 0 - elapsed 11.136 ms\nstream 1 - elapsed 16.998 ms\nstream 2 - elapsed 23.283 ms\nstream 3 - elapsed 29.487 ms\ncompared a sample result...\nhost: 1.523750, device: 1.523750\nTime= 29.771 msec, bandwidth= 27.050028 GB/s\n```", "```cpp\ncudaError_t cudaStreamCreateWithPriority(cudaStream_t* pStream, unsigned int flags, int priority)\n```", "```cpp\ncudaDeviceProp prop;\ncudaGetDeviceProperties(&prop, 0);\nif (prop.streamPrioritiesSupported == 0) { ... }\n```", "```cpp\n... { middle of the class Operator } ...\nprotected:\n    cudaStream_t stream = nullptr;\n\npublic:\n    Operator(bool create_stream = true) {\n        if (create_stream)\n            cudaStreamCreate(&stream);\n        sdkCreateTimer(&p_timer);\n    }\n... { middle of the class Operator } ...\n```", "```cpp\nclass Operator_with_priority: public Operator {\npublic:\n    Operator_with_priority() : Operator(false) {}\n\n    void set_priority(int priority) {\n        cudaStreamCreateWithPriority(&stream, \n            cudaStreamNonBlocking, priority);\n    }\n};\n```", "```cpp\nOperator_with_priority *ls_operator = new Operator_with_priority[num_operator];\n```", "```cpp\n// Get priority range\nint priority_low, priority_high;\ncudaDeviceGetStreamPriorityRange(&priority_low, &priority_high);\nprintf(\"Priority Range: low(%d), high(%d)\\n\", priority_low, priority_high);\n```", "```cpp\nfor (int i = 0; i < num_operator; i++) {\n    ls_operator[i].set_index(i);\n\n    // let the latest CUDA stream to have the high priority\n    if (i + 1 == num_operator)\n        ls_operator[i].set_priority(priority_high);\n    else\n        ls_operator[i].set_priority(priority_low);\n}\n```", "```cpp\nfor (int i = 0 ; i < num_operator; i++) { \n    int offset = i * size / num_operator;\n    ls_operator[i].async_operation(&h_c[offset], \n                                   &h_a[offset], &h_b[offset],\n                                   &d_c[offset], \n                                   &d_a[offset], &d_b[offset],\n                                   size / num_operator, \n                                   bufsize / num_operator);\n}\n```", "```cpp\n$ nvcc -m64 -run -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o prioritized_cuda_stream ./prioritized_cuda_stream.cu\n```", "```cpp\nPriority Range: low(0), high(-1)\nstream 0 - elapsed 11.119 ms\nstream 3 - elapsed 19.126 ms\nstream 1 - elapsed 23.327 ms\nstream 2 - elapsed 29.422 ms\ncompared a sample result...\nhost: 1.523750, device: 1.523750\nTime= 29.730 msec, bandwidth= 27.087332 GB/s\n```", "```cpp\n__global__ void\nvecAdd_kernel(float *c, const float* a, const float* b) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    for (int i = 0; i < 500; i++)\n        c[idx] = a[idx] + b[idx];\n}\n```", "```cpp\n... { memory initializations } ...\n\n// initialize the host timer\nStopWatchInterface *timer;\nsdkCreateTimer(&timer);\n\ncudaEvent_t start, stop;\n// create CUDA events\ncudaEventCreate(&start);\ncudaEventCreate(&stop);\n\n// start to measure the execution time\nsdkStartTimer(&timer);\ncudaEventRecord(start);\n\n// launch cuda kernel\ndim3 dimBlock(256);\ndim3 dimGrid(size / dimBlock.x);\nvecAdd_kernel<<< dimGrid, dimBlock >>>(d_c, d_a, d_b);\n\n// record the event right after the kernel execution finished\ncudaEventRecord(stop);\n\n// Synchronize the device to measure the execution time from the host side\ncudaEventSynchronize(stop); // we also can make synchronization based on CUDA event\nsdkStopTimer(&timer);\n```", "```cpp\n// print out the result\nint print_idx = 256;\nprintf(\"compared a sample result...\\n\");\nprintf(\"host: %.6f, device: %.6f\\n\", h_a[print_idx] + h_b[print_idx], h_c[print_idx]);\n\n// print estimated kernel execution time\nfloat elapsed_time_msed = 0.f;\ncudaEventElapsedTime(&elapsed_time_msed, start, stop);\nprintf(\"CUDA event estimated - elapsed %.3f ms \\n\", elapsed_time_msed);\n```", "```cpp\n// delete timer\nsdkDeleteTimer(&timer);\n\n// terminate CUDA events\ncudaEventDestroy(start);\ncudaEventDestroy(stop);\n```", "```cpp\n$ nvcc -m64 -run -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o cuda_event ./cuda_event.cu\ncompared a sample result...\nhost: 1.523750, device: 1.523750\nCUDA event estimated - elapsed 23.408 ms \nHost measured time= 35.063 msec/s\n```", "```cpp\nclass Operator\n{\nprivate:\n    int _index;\n    cudaStream_t stream;\n    StopWatchInterface *p_timer;\n    cudaEvent_t start, stop;\n\npublic:\n    Operator() {\n        cudaStreamCreate(&stream);\n\n // create cuda event\n cudaEventCreate(&start);\n cudaEventCreate(&stop);\n    }\n\n    ~Operator() {\n        cudaStreamDestroy(stream);\n\n // destroy cuda event\n cudaEventDestroy(start);\n cudaEventDestroy(stop);\n    }\n\n    void set_index(int idx) { index = idx; }\n    void async_operation(float *h_c, const float *h_a, \n                          const float *h_b,\n                          float *d_c, float *d_a, float *d_b,\n                          const int size, const int bufsize);\n void print_kernel_time();\n\n}; // Operator\n```", "```cpp\nvoid Operator::print_time() {\n    float milliseconds = 0;\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    printf(\"Stream %d time: %.4f ms\\n\", index, milliseconds);\n}\n```", "```cpp\nvoid Operator::async_operation( ... )\n{\n    // start timer\n    sdkStartTimer(&p_timer);\n\n    // copy host -> device\n    cudaMemcpyAsync(d_a, h_a, bufsize, \n                    cudaMemcpyHostToDevice, stream);\n    cudaMemcpyAsync(d_b, h_b, bufsize, \n                    cudaMemcpyHostToDevice, stream);\n\n    // record the event before the kernel execution\n cudaEventRecord(start, stream);\n\n    // launch cuda kernel\n    dim3 dimBlock(256);\n    dim3 dimGrid(size / dimBlock.x);\n    vecAdd_kernel<<< dimGrid, dimBlock, 0, \n                     stream >>>(d_c, d_a, d_b);\n\n    // record the event right after the kernel execution finished\n cudaEventRecord(stop, stream);\n\n    // copy device -> host\n    cudaMemcpyAsync(h_c, d_c, bufsize, \n                    cudaMemcpyDeviceToHost, stream);\n\n    // what happen if we include CUDA event synchronize?\n    // QUIZ: cudaEventSynchronize(stop);\n\n    // register callback function\n    cudaStreamAddCallback(stream, Operator::Callback, this, 0);\n}\n```", "```cpp\n$ nvcc -m64 -run -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o cuda_event_with_streams ./cuda_event_with_streams.cu\nPriority Range: low(0), high(-1)\nstream 0 - elapsed 11.348 ms \nstream 3 - elapsed 19.435 ms \nstream 1 - elapsed 22.707 ms \nstream 2 - elapsed 35.768 ms \nkernel in stream 0 - elapsed 6.052 ms \nkernel in stream 1 - elapsed 14.820 ms \nkernel in stream 2 - elapsed 17.461 ms \nkernel in stream 3 - elapsed 6.190 ms \ncompared a sample result...\nhost: 1.523750, device: 1.523750\nTime= 35.993 msec, bandwidth= 22.373972 GB/s\n```", "```cpp\n__global__ void child_kernel(int *data) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    atomicAdd(&data[idx], seed);\n}\n\n__global__ void parent_kernel(int *data)\n{\n if (threadIdx.x == 0) {\n        int child_size = BUF_SIZE/gridDim.x;\n        child_kernel<<< child_size/BLOCKDIM, BLOCKDIM >>>\n                        (&data[child_size*blockIdx.x], blockIdx.x+1);\n    }\n    // synchronization for other parent's kernel output\n    cudaDeviceSynchronize();\n}\n```", "```cpp\n#define BUF_SIZE (1 << 10)\n#define BLOCKDIM 256\n\n__global__ void child_kernel(int *data)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    atomicAdd(&data[idx], 1);\n}\n\n__global__ void parent_kernel(int *data)\n{\n    if (blockIdx.x * blockDim.x + threadIdx.x == 0)\n    {\n        int child_size = BUF_SIZE/gridDim.x;\n        child_kernel<<< child_size/BLOCKDIM, BLOCKDIM >>> \\\n                        (&data[child_size*blockIdx.x], \n                         blockIdx.x+1);\n    }\n    // synchronization for other parent's kernel output\n    cudaDeviceSynchronize();\n}\n```", "```cpp\n#define BUF_SIZE (1 << 10)\n#define BLOCKDIM 256\nint main()\n{\n    int *data;\n    int num_child = 4;\n\n    cudaMallocManaged((void**)&data, BUF_SIZE * sizeof(int));\n    cudaMemset(data, 0, BUF_SIZE * sizeof(int));\n\n    parent_kernel<<<num_child, 1>>>(data);\n    cudaDeviceSynchronize();\n\n    // Count elements value\n    int counter = 0;\n    for (int i = 0; i < BUF_SIZE; i++)\n        counter += data[i];\n\n    // getting answer\n    int counter_h = 0;\n    for (int i = 0; i < num_child; i++)\n        counter_h += (i+1);\n    counter_h *= BUF_SIZE / num_child;\n\n    if (counter_h == counter)\n        printf(\"Correct!!\\n\");\n    else\n        printf(\"Error!! Obtained %d. It should be %d\\n\", \n               counter, counter_h);\n\n    cudaFree(data);\n    return 0;\n}\n```", "```cpp\n$ nvcc -run -rdc=true -lcudadevrt -gencode arch=compute_70,code=sm_70 -o host_callback host_callback.cu -I/usr/local/cuda/samples/common/inc \n```", "```cpp\n__global__ void recursive_kernel(int *data, int size, int depth) {\n  int x_0 = blockIdx.x * size;\n\n  if (depth > 0) {\n    __syncthreads();\n if (threadIdx.x == 0) {\n        int dimGrid = size / dimBlock;\n        recursive_kernel<<<dimGrid, \n              dimBlock>>>(&data[x_0], size/dimGrid, depth-1);\n        cudaDeviceSynchronize();\n      }\n      __syncthreads();\n   }\n}\n```", "```cpp\n__host__ cudaError_t cudaLaunchCooperativeKernel\n    ( const T* func, dim3 gridDim, dim3 blockDim, \n      void** args, size_t sharedMem = 0, cudaStream_t stream = 0 )\n```", "```cpp\n__device__ void\nblock_reduction(float *out, float *in, float *s_data, int active_size, int size, \n          const cg::grid_group &grid, const cg::thread_block &block)\n{\n  int tid = block.thread_rank();\n\n  // Stride over grid and add the values to a shared memory buffer\n  s_data[tid] = 0.f;\n  for (int i = grid.thread_rank(); i < size; i += active_size)\n    s_data[tid] += in[i];\n\n  block.sync();\n\n  for (unsigned int stride = blockDim.x / 2; \n       stride > 0; stride >>= 1) {\n    if (tid < stride)\n      s_data[tid] += s_data[tid + stride];\n    block.sync();\n  }\n\n  if (block.thread_rank() == 0)\n    out[block.group_index().x] = s_data[0];\n}\n```", "```cpp\n__global__ void\nreduction_kernel(float *g_out, float *g_in, unsigned int size)\n{\n  cg::thread_block block = cg::this_thread_block();\n  cg::grid_group grid = cg::this_grid();\n  extern __shared__ float s_data[];\n\n  // do reduction for multiple blocks\n  block_reduction(g_out, g_in, s_data, grid.size(), \n                  size, grid, block);\n\n  grid.sync();\n\n  // do reduction with single block\n  if (block.group_index().x == 0)\n    block_reduction(g_out, g_out, s_data, block.size(), gridDim.x, grid, block);\n}\n```", "```cpp\nint reduction_grid_sync(float *g_outPtr, float *g_inPtr, int size, int n_threads)\n{ \n  int num_blocks_per_sm;\n  cudaDeviceProp deviceProp;\n\n  // Calculate the device occupancy to know \n  // how many blocks can be run concurrently\n  cudaGetDeviceProperties(&deviceProp, 0);\n  cudaOccupancyMaxActiveBlocksPerMultiprocessor(&num_blocks_per_sm, \n      reduction_kernel, n_threads, n_threads*sizeof(float));\n  int num_sms = deviceProp.multiProcessorCount;\n  int n_blocks = min(num_blocks_per_sm * num_sms, \n                     (size + n_threads - 1) / n_threads);\n\n  void *params[3];\n  params[0] = (void*)&g_outPtr;\n  params[1] = (void*)&g_inPtr;\n  params[2] = (void*)&size;\n  cudaLaunchCooperativeKernel((void*)reduction_kernel, \n                              n_blocks, n_threads, params, \n                              n_threads * sizeof(float), NULL);\n\n  return n_blocks;\n}\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -rdc=true -o reduction ./reduction.cpp ./reduction_kernel.cu\nTime= 0.474 msec, bandwidth= 141.541077 GB/s\nhost: 0.996007, device 0.996007\n```", "```cpp\n// execute each operator collesponding data\nomp_set_num_threads(num_operator);\n#pragma omp parallel\n{\n    int i = omp_get_thread_num();\n    printf(\"Launched GPU task %d\\n\", i);\n\n    int offset = i * size / num_operator;\n    ls_operator[i].set_index(i);\n    ls_operator[i].async_operation(&h_c[offset], &h_a[offset],   \n                                   &h_b[offset],&d_c[offset], \n                                   &d_a[offset], &d_b[offset],\n                                   size / num_operator, bufsize \n                                   / num_operator);\n}\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -Xcompiler -fopenmp -lgomp -o openmp ./openmp.cu\nstream 0 - elapsed 10.734 ms \nstream 2 - elapsed 16.153 ms \nstream 3 - elapsed 21.968 ms \nstream 1 - elapsed 27.668 ms \ncompared a sample result...\nhost: 1.523750, device: 1.523750\nTime= 27.836 msec, bandwidth= 28.930389 GB/s\n```", "```cpp\n$nvidia-smi -c EXCLUSIVE_PROCESS \n$nvidia-cuda-mps-control \u2013d\n```", "```cpp\n$ wget -O /tmp/openmpi-3.0.4.tar.gz https://www.open-mpi.org/software/ompi/v3.0/downloads/openmpi-3.0.4.tar.gz\n$ tar xzf /tmp/openmpi-3.0.4.tar.gz -C /tmp\n$ cd /tmp/openmpi-3.0.4\n$ ./configure --enable-orterun-prefix-by-default --with-cuda=/usr/local/cuda\n$ make -j $(nproc) all && sudo make install\n$ sudo ldconfig\n$ mpirun --version\nmpirun (Open MPI) 3.0.4\n\nReport bugs to http://www.open-mpi.org/community/help/\n```", "```cpp\n#include <mpi.h>\n```", "```cpp\n// set num_operator as the number of requested process\nint np, rank;\nMPI_Init(&argc, &argv);\nMPI_Comm_size(MPI_COMM_WORLD, &np);\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n```", "```cpp\nbufsize /= np;\nsize /= np;\n```", "```cpp\n// execute each operator collesponding data\nomp_set_num_threads(num_operator);\n#pragma omp parallel\n{\n    int i = omp_get_thread_num();\n    int offset = i * size / num_operator;\n    printf(\"Launched GPU task (%d, %d)\\n\", rank, i);\n\n    ls_operator[i].set_index(i);\n    ls_operator[i].async_operation(&h_c[offset], \n                                   &h_a[offset], &h_b[offset],\n                                   &d_c[offset], &d_a[offset], \n                                   &d_b[offset],\n                                   size / num_operator, \n                                   bufsize / num_operator);\n}\n```", "```cpp\n$ nvcc -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -I/usr/local/include/ -Xcompiler -fopenmp -lgomp -lmpi -o simpleMPI ./simpleMPI.cu\n```", "```cpp\n$ ./simpleMPI 2\n```", "```cpp\n$ mpirun -np 2 ./simpleMPI 2\nNumber of process: 2\nNumber of operations: 2\nLaunched GPU task (1, 0)\nLaunched GPU task (1, 1)\nNumber of operations: 2\nLaunched GPU task (0, 0)\nLaunched GPU task (0, 1)\nstream 0 - elapsed 13.390 ms \nstream 1 - elapsed 25.532 ms \ncompared a sample result...\nhost: 1.306925, device: 1.306925\nTime= 25.749 msec, bandwidth= 15.637624 GB/s\nstream 0 - elapsed 21.334 ms \nstream 1 - elapsed 26.010 ms \ncompared a sample result...\nhost: 1.306925, device: 1.306925\nTime= 26.111 msec, bandwidth= 15.420826 GB/s\n```", "```cpp\n$ export CUDA_VISIBLE_DEVICES=0\n$ sudo nvidia-smi -i 0 -c 3\n$ sudo nvidia-cuda-mps-control -d\n```", "```cpp\n$ mpirun -np 2 ./simpleMPI 2\nNumber of process: 2\nNumber of operations: 2\nLaunched GPU task (1, 0)\nLaunched GPU task (1, 1)\nstream 0 - elapsed 10.203 ms \nstream 1 - elapsed 15.903 ms \ncompared a sample result...\nhost: 1.306925, device: 1.306925\nTime= 16.129 msec, bandwidth= 24.964548 GB/s\nNumber of operations: 2\nLaunched GPU task (0, 0)\nLaunched GPU task (0, 1)\nstream 0 - elapsed 10.203 ms \nstream 1 - elapsed 15.877 ms \ncompared a sample result...\nhost: 1.306925, device: 1.306925\nTime= 15.997 msec, bandwidth= 25.170544 GB/s\n```", "```cpp\n$ echo \"quit\" | sudo nvidia-cuda-mps-control\n$ sudo nvidia-smi -i 0 -c 0\n```", "```cpp\n$ mpirun -np 2 nvprof -f -o simpleMPI.%q{OMPPI_COMM_WORLD_RANK}_2.nvvp ./simpleMPI 2\n```", "```cpp\n$ PROCS=2 STREAMS=2 make nvprof\n```", "```cpp\nmpirun -np 2 nvprof --annotate-mpi openmpi -o myMPIApp.%q{OMPI_COMM_WORLD_RANK}.nvprof ./myMPIApplciation\n```", "```cpp\n__global__ void\nsimple_saxpy_kernel(float *y, const float* x, const float alpha, const float beta)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    y[idx] = alpha * x[idx] + beta;\n}\n```", "```cpp\n__global__ void\niterative_saxpy_kernel(float *y, const float* x, \n                       const float alpha, const float beta, \n                       int n_loop)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    for (int i = 0; i < n_loop; i++)\n        y[idx] = alpha * x[idx] + beta;\n}\n\n```", "```cpp\n__global__ void\nrecursive_saxpy_kernel(float *y, const float* x, \n                       const float alpha, const float beta, \n                       int depth)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (depth == 0)\n        return;\n    else\n        y[idx] = alpha * x[idx] + beta;\n\n    if (idx == 0)\n        vecAdd_kernel_C<<< gridDim.x, blockDim.x \n                           >>>(y, x, alpha, beta, depth - 1);\n}\n```", "```cpp\nfor (int i = 0; i < n_loop; i++) {\n    simple_saxpy_kernel<<< dimGrid, dimBlock >>>(\n                           d_y, d_x, alpha, beta);\n}\n```", "```cpp\niterative_saxpy_kernel<<< dimGrid, dimBlock >>>(\n                          d_y, d_x, alpha, beta, n_loop);\n```", "```cpp\nrecursive_saxpy_kernel<<< dimGrid, dimBlock >>>(\n                          d_y, d_x, alpha, beta, n_loop);\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -rdc=true -o cuda_kernel ./cuda_kernel.cu\n```", "```cpp\nElapsed Time...\nsimple loop: 0.094 ms\ninner loop : 0.012 ms\nrecursion : 0.730 ms\n```"]