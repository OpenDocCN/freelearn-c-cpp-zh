- en: Kernel Execution Model and Optimization Strategies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心执行模型和优化策略
- en: CUDA programming has a procedure for host operations. For example, we need to
    allocate the global memory, transfer data to the GPU, execute kernel functions,
    transfer data back to the host, and clean the global memory. That's because the
    GPU is an extra-processing unit in the system, so we need to care about its execution
    and data transfer. This is another aspect of GPU programming that's different
    compared to CPU programming.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA编程有一个主机操作的过程。例如，我们需要分配全局内存，将数据传输到GPU，执行核心函数，将数据传输回主机，清理全局内存。这是因为GPU是系统中的一个额外处理单元，所以我们需要关心它的执行和数据传输。这是与CPU编程相比另一个不同的GPU编程方面。
- en: In this chapter, we will cover CUDA kernel execution models and CUDA streams,
    which control CUDA operations. Then, we will discuss optimization strategies at
    the system level. Then, we will cover CUDA events to measure GPU event time, and
    how to use CUDA events to measure kernel execution time. After that, we will cover
    various CUDA kernel executing models and discuss what those features bring to
    GPU operations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖CUDA核心执行模型和CUDA流，它们控制CUDA操作。然后，我们将讨论系统级别的优化策略。接下来，我们将涵盖CUDA事件来测量GPU事件时间，以及如何使用CUDA事件来测量核心执行时间。之后，我们将涵盖各种CUDA核心执行模型，并讨论这些特性对GPU操作的影响。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Kernel execution with a CUDA streams
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CUDA流的核心执行
- en: Pipelining GPU execution
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流水线化GPU执行
- en: The CUDA callback function
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA回调函数
- en: CUDA streams with priority
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有优先级的CUDA流
- en: Kernel execution time estimation using CUDA events
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CUDA事件估计核心执行时间
- en: CUDA dynamic parallelism
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA动态并行性
- en: Grid-level cooperative groups
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格级协作组
- en: CUDA kernel calls with OpenMP
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OpenMP的CUDA核心调用
- en: Multi-Process Services
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多进程服务
- en: Kernel execution overhead comparison
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心执行开销比较
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires us to use a CUDA version later than 9.x, and the GPU architecture
    should be Volta or Turing. If you use a GPU with Pascal architecture, skip the
    *Grid-level cooperative groups* section because this feature is introduced for
    Volta architecture.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求我们使用的CUDA版本应该晚于9.x，并且GPU架构应该是Volta或Turing。如果你使用的是Pascal架构的GPU，那么跳过*Grid-level
    cooperative groups*部分，因为这个特性是为Volta架构引入的。
- en: Kernel execution with CUDA streams
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA流的核心执行
- en: A stream is a sequence of commands that relate to the GPU in CUDA programming.
    In other words, all the kernel calls and data transfers are handled by the CUDA
    stream. By default, CUDA provides a default stream, and all the commands use the
    stream implicitly. Therefore, we do not have to handle this ourselves.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA编程中，流是与GPU相关的一系列命令。换句话说，所有的核心调用和数据传输都由CUDA流处理。默认情况下，CUDA提供了一个默认流，所有的命令都隐式地使用这个流。因此，我们不需要自己处理这个。
- en: CUDA supports additional streams, created explicitly. While the operations in
    a stream are sequential, CUDA can execute multiple operations concurrently by
    using the multiple streams. Let's learn how to handle streams, and what features
    they have.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA支持显式创建额外的流。虽然流中的操作是顺序的，但CUDA可以通过使用多个流同时执行多个操作。让我们学习如何处理流，以及它们具有哪些特性。
- en: The usage of CUDA streams
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA流的使用
- en: 'The following code shows an example of how CUDA streams can be created, used,
    and terminated:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何创建、使用和终止CUDA流的示例：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see, we can handle a CUDA stream using `cudaStream_t`. And, we can
    create this using `cudaStreamCreate()` and terminate it using `cudaStreamDestroy()`.
    Note that we should provide a pointer to `cudaStreamCreate()`. The created stream
    is passed to the kernel's fourth argument.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们可以使用`cudaStream_t`来处理CUDA流。而且，我们可以使用`cudaStreamCreate()`来创建它，使用`cudaStreamDestroy()`来终止它。注意我们应该提供一个指向`cudaStreamCreate()`的指针。创建的流会传递给核心函数的第四个参数。
- en: However, we did not provide such a stream previously. That's because CUDA provides
    a default stream so that all the CUDA operations can operate. Now, let's write
    an application that uses the default stream and multiple streams. Then, we will
    see how our application can be changed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们之前并没有提供这样的流。这是因为CUDA提供了一个默认流，以便所有的CUDA操作都可以进行。现在，让我们编写一个使用默认流和多个流的应用程序。然后，我们将看到我们的应用程序如何改变。
- en: 'First, let''s write an application that uses the default CUDA stream, as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们编写一个使用默认CUDA流的应用程序，如下所示：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see in the code, we call the kernel function with the stream ID
    as `0`, because the identification value of the default stream is `0`. Compile
    the code and see the execution output:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在代码中看到的，我们以流ID为`0`调用了核心函数，因为默认流的标识值为`0`。编译代码并查看执行输出：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'How is the output? We can expect that the output will be the order of the loop
    index. The following timeline view shows this code''s operation:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是什么？我们可以期待输出将是循环索引的顺序。以下时间轴视图显示了这段代码的操作：
- en: '![](img/870c3559-e2cf-4a27-a481-53697923141d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/870c3559-e2cf-4a27-a481-53697923141d.png)'
- en: 'It is to be expected that having loop operations in the same stream shows the
    order of kernel execution. Then, what can be changed if we use multiple CUDA streams,
    and each loop step uses different ones? The following code shows an example of
    printing the loop index from the CUDA kernel function with the different streams:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可以预期，在同一个流中进行循环操作将显示核心执行的顺序。那么，如果我们使用多个CUDA流，并且每个循环步骤使用不同的流，会有什么改变？以下代码展示了使用不同流从CUDA核心函数打印循环索引的示例：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this code, we have five calls, the same as in the previous code, but here
    we will use five different streams. To do this, we built an array of `cudaStream_t`
    and created streams for each. What can you expect with this change? The printed
    output would be the same as the previous version. Run the following command to
    compile this code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们有五个调用，与之前的代码相同，但这里我们将使用五个不同的流。为此，我们建立了一个`cudaStream_t`数组，并为每个流创建了流。你对这个改变有什么期待？打印输出将与之前的版本相同。运行以下命令来编译这段代码：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'However, that does not guarantee that they have the same operation. As we discussed
    at the beginning, this code shows the concurrency of the multiple streams, as
    in the following screenshot:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不保证它们具有相同的操作。正如我们在开始时讨论的，这段代码展示了多个流的并发性，如下面的截图所示：
- en: '![](img/c34f9def-ab64-4d9e-a159-f9ecb23c807d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c34f9def-ab64-4d9e-a159-f9ecb23c807d.png)'
- en: 'As you can see at the bottom of the screenshot, five individual streams execute
    the same kernel function concurrently and their operations are overlapped with
    each other. From this, we can discern two features of the streams, as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在截图底部所看到的，五个独立的流同时执行相同的内核函数，并且它们的操作相互重叠。由此，我们可以得出流的两个特点，如下所示：
- en: Kernel executions are asynchronous with the host.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内核执行与主机是异步的。
- en: CUDA operations in different streams are independent of each other.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同流中的CUDA操作是彼此独立的。
- en: Using the concurrency of the stream, we can make extra optimization opportunities
    by overlapping independent operations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 利用流的并发性，我们可以通过重叠独立操作来获得额外的优化机会。
- en: Stream-level synchronization
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流级别的同步
- en: CUDA streams provide stream-level synchronization with the `cudaStreamSynchronize()`
    function. Using this function forces the host to wait until the end of a certain
    stream's operation. This provides important optimization for the `cudaDeviceSynchronize()`
    function that we have used so far.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA流提供了流级别的同步，使用`cudaStreamSynchronize()`函数。使用这个函数会强制主机等待直到某个流的操作结束。这为我们迄今为止使用的`cudaDeviceSynchronize()`函数提供了重要的优化。
- en: 'We will discuss how to utilize this feature in the following sections, but
    let''s discuss its basic operations here. The previous example shows concurrent
    operations without synchronization within the loop. However, we can halt the host
    to execute the next kernel execution by using the `cudaStreamSynchronize()` function.
    The following code shows an example of using stream synchronization at the end
    of the kernel execution:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的部分讨论如何利用这一特性，但让我们在这里讨论它的基本操作。前面的例子展示了在循环中没有同步的并发操作。然而，我们可以通过使用`cudaStreamSynchronize()`函数来阻止主机执行下一个内核执行。下面的代码展示了在内核执行结束时使用流同步的示例：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can easily predict that the kernel operation''s concurrency will vanish
    due to synchronization. To confirm this, let''s profile this and see how this
    impacts the kernel execution:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地预测，由于同步，内核操作的并发性将消失。为了确认这一点，让我们对此进行分析，看看这对内核执行的影响：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following screenshot shows the result:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了结果：
- en: '![](img/f9a4b008-4225-4766-ac33-c26bec6a0fd6.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9a4b008-4225-4766-ac33-c26bec6a0fd6.png)'
- en: As you can see, all the kernel executions have no overlapping points, although
    they are executed with the different streams. Using this feature, we can let the
    host wait for the specific stream operation to start with the result.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，所有的内核执行没有重叠点，尽管它们是用不同的流执行的。利用这一特性，我们可以让主机等待特定流操作的开始和结果。
- en: Working with the default stream
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用默认流
- en: 'To have multiple streams operating concurrently, we should use streams we created
    explicitly, because all stream operations are synchronous with the default stream.
    The following screenshot shows the default stream''s synchronous operation effect:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让多个流同时运行，我们应该使用我们显式创建的流，因为所有流操作都与默认流同步。下面的截图显示了默认流的同步操作效果：
- en: '![](img/e31e5516-2fb9-434c-a0c7-9d729fbdcde0.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e31e5516-2fb9-434c-a0c7-9d729fbdcde0.png)'
- en: 'We can achieve this by modifying our multi-stream kernel call operation, like
    this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过修改我们的多流内核调用操作来实现这一点，就像这样：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run the following command to compile the code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令编译代码：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So, we can see that the last operation cannot be overlapped with the previous
    kernel executions, but that we have to wait until the fourth kernel execution
    has finished.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到最后一个操作无法与前面的内核执行重叠，而是必须等到第四个内核执行完成后才能进行。
- en: Pipelining the GPU execution
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU执行的流水线
- en: One of the major benefits of multiple streams is overlapping the data transfer
    with the kernel execution. By overlapping the kernel operation and data transfer,
    we can conceal the data transfer overhead and increase overall performance.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 多个流的主要好处之一是将数据传输与内核执行重叠。通过重叠内核操作和数据传输，我们可以隐藏数据传输开销并提高整体性能。
- en: Concept of GPU pipelining
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU流水线的概念
- en: 'When we execute the kernel function, we need to transfer data from the host
    to the GPU. Then, we transfer the result back from the GPU to the host. The following
    diagram shows an example of iterative operations that transfer data between the
    host and kernel executions:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行内核函数时，我们需要将数据从主机传输到GPU，然后将结果从GPU传输回主机。下面的图表显示了在主机和内核执行之间传输数据的迭代操作的示例：
- en: '![](img/d8db7a9f-522f-4e8f-aa07-19efc6233fc8.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8db7a9f-522f-4e8f-aa07-19efc6233fc8.png)'
- en: 'However, the kernel execution is basically asynchronous in that the host and GPU
    can operate concurrently with each other. If the data transfer between the host
    and GPU has the same feature, we would be able to overlap their execution, as
    we could see in the previous section. The following diagram shows the operation
    when the data transfer can be executed like a normal kernel operation, and handled
    along with the stream:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，内核执行基本上是异步的，主机和GPU可以同时运行。如果主机和GPU之间的数据传输具有相同的特性，我们就能够重叠它们的执行，就像我们在前面的部分中看到的那样。下面的图表显示了当数据传输可以像正常的内核操作一样执行，并与流一起处理时的操作：
- en: '![](img/3b531d26-ec42-4576-9b9c-511bd175b612.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b531d26-ec42-4576-9b9c-511bd175b612.png)'
- en: In this diagram, we can see that the data transfer between the host and the
    device can be overlapped with the kernel execution. Then, the benefit of this
    overlapped operation is to reduce the application execution time. By comparing
    the length of the two pictures, you will be able to confirm which operation would
    have higher operation throughput.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图表中，我们可以看到主机和设备之间的数据传输可以与内核执行重叠。然后，这种重叠操作的好处是减少应用程序的执行时间。通过比较两张图片的长度，您将能够确认哪个操作的吞吐量更高。
- en: 'Regarding CUDA streams, all CUDA operations—data transfers and kernel executions—are
    sequential in the same stream. However, those can operate simultaneously along
    with the different streams. The following diagram shows overlapped data transfer
    with the kernel operation for multiple streams:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 关于CUDA流，所有CUDA操作——数据传输和内核执行——在同一个流中是顺序的。然而，它们可以与不同的流同时操作。以下图表显示了多个流的重叠数据传输和内核操作：
- en: '![](img/c3141f46-c05d-4e22-85b4-bd4a2c193f9a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3141f46-c05d-4e22-85b4-bd4a2c193f9a.png)'
- en: 'To enable such a pipelining operation, CUDA has three prerequisites:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这样的流水线操作，CUDA有三个先决条件：
- en: The host memory should be allocated as pinned memory—CUDA provides the `cudaMallocHost()` and `cudaFreeHost()` functions
    for this purpose.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主机内存应该分配为固定内存——CUDA提供了`cudaMallocHost()`和`cudaFreeHost()`函数来实现这一目的。
- en: Transfer data between the host and GPUs without blocking the host—CUDA provides
    the `cudaMemcpyAsync()` function for this purpose.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主机和GPU之间传输数据而不阻塞主机——CUDA提供了`cudaMemcpyAsync()`函数来实现这一目的。
- en: Manage each operation along with the different CUDA streams to have concurrent
    operations.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管理每个操作以及不同的CUDA流，以实现并发操作。
- en: Now, let's write a simple application that pipelines the workloads.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一个简单的应用程序来对工作负载进行流水线处理。
- en: Building a pipelining execution
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建流水线执行
- en: 'The following code shows a snippet of asynchronous data transfer and the synchronization
    of a CUDA stream at the end of the execution:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了异步数据传输的片段以及在执行结束时CUDA流的同步：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This code shows how to allocate pinned memory, and transfer data with the user-created
    stream. By merging this example and the multiple CUDA stream operations, we can
    have the pipelining CUDA operation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码展示了如何分配固定内存，并使用用户创建的流传输数据。通过合并这个例子和多个CUDA流操作，我们可以实现流水线CUDA操作。
- en: 'Now, let''s build an application that has the pipelining operation with the
    data transfer and the kernel execution. In this application, we will use a kernel
    function that adds two vectors, by slicing the number of streams, and outputs
    its result. However, the kernel implementation does not require any changes with
    this since we will do this at the host code level. But, we will iterate the addition
    operation 500 times to extend the kernel execution time. As a result, the implemented
    kernel code is as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建一个应用程序，其中包含数据传输和内核执行的流水线操作。在这个应用程序中，我们将使用一个将两个向量相加的内核函数，通过切片流的数量，并输出其结果。然而，内核的实现在主机代码级别不需要任何更改。但是，我们将迭代加法操作500次以延长内核执行时间。因此，实现的内核代码如下：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To handle each stream''s operation, we will create a class that manages a CUDA
    stream and the CUDA operations. This class will allow us to manage the CUDA stream
    along with the index. The following code shows the basic architecture of the class:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理每个流的操作，我们将创建一个管理CUDA流和CUDA操作的类。这个类将允许我们管理CUDA流以及索引。以下代码显示了该类的基本架构：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let''s write some sequential GPU execution code that we have used in the
    previous section, but as a member function of the `Operator` class, as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一些顺序GPU执行代码，这些代码在前一节中已经使用过，但作为`Operator`类的成员函数，如下所示：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This function''s operation is no different to the basic CUDA host programming
    pattern we have used previously, except we applied `cudaMemcpyAsync()` with the
    given `_stream`. Then, we write `main()` to work with multiple operator instances
    and page-locked memory:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的操作与我们之前使用的基本CUDA主机编程模式没有什么不同，只是我们使用了给定的`_stream`应用了`cudaMemcpyAsync()`。然后，我们编写`main()`来处理多个操作符实例和页锁定内存：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we will allocate host memories using `cudaMallocHost()` to have pinned
    memories, and initialize them:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用`cudaMallocHost()`来分配主机内存，以获得固定内存，并对其进行初始化：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And, we will have device memories with the same size:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，我们将拥有相同大小的设备内存：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we will create a list of CUDA operators using the class we used:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用我们使用的类创建一个CUDA操作符列表：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We are ready to execute the pipelining operations. Before we start the execution,
    let''s place a stopwatch to see the overall execution time and see the overlapped
    data transfer benefit, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备执行流水线操作。在开始执行之前，让我们放一个秒表来查看整体执行时间，并查看重叠数据传输的好处，如下所示：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s execute each operator using a loop, and each operator will access the
    host and the device memory according to their order. We will, also, measure the
    execution time of the loop:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用循环执行每个操作符，并且每个操作符将根据其顺序访问主机和设备内存。我们还将测量循环的执行时间：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we will compare a sample''s result and print out the overall measured
    performance:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将比较一个样本的结果，并打印出整体测量性能：
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Terminate handles and memories, as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 终止句柄和内存，如下所示：
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To execute the code, let''s reuse the host initialization function and GPU
    kernel function from the previous recipes. We don''t have to modify these functions
    at this moment. Compile the code using the following command:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行代码，让我们重用前面的主机初始化函数和GPU内核函数。我们暂时不需要修改这些函数。使用以下命令编译代码：
- en: '[PRE21]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You must use your GPU''s compute capability version number for the `gencode`
    option. The output of the compilation is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须使用GPU的计算能力版本号作为`gencode`选项。编译的输出如下：
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As we can see, GPU tasks are executed following the order of the kernel execution
    along with the stream.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，GPU任务是按照内核执行的顺序以及流的顺序执行的。
- en: 'Now, let''s review how the application operates on the inside. By default,
    the sample code slices the host data into four and executes four CUDA streams
    concurrently. We can see each kernel''s outputs along with the streams'' execution.
    To see the overlapping operation, you need to profile the execution with the following
    command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来回顾一下应用程序在内部是如何运行的。默认情况下，示例代码将主机数据切片为四个部分，并同时执行四个CUDA流。我们可以看到每个核函数的输出以及流的执行情况。要查看重叠操作，您需要使用以下命令对执行进行分析：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following screenshot shows four CUDA streams'' operations by overlapping
    data transfer with kernel execution:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了通过重叠数据传输和核函数执行来操作四个CUDA流：
- en: '![](img/1dc75db9-3e38-4f1f-8309-e4942488014e.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1dc75db9-3e38-4f1f-8309-e4942488014e.png)'
- en: Overlaps between the kernel executions and data transfers
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数执行和数据传输之间的重叠
- en: As a result, the GPU can be busy until the last kernel execution is finished,
    and we can conceal most of the data transfers. This not only enhances the GPU
    utilization, but also reduces total application execution time.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，GPU可以忙碌直到最后一个核函数执行完成，并且我们可以隐藏大部分数据传输。这不仅增强了GPU的利用率，还减少了总应用程序执行时间。
- en: Between the kernel execution, we can find that none of them have no contention
    although they belong to different CUDA streams. That's because of the GPU scheduler
    being aware of the execution requests, and serving the first. However, when the
    current task is finished, the streaming multiprocessor can serve the next kernel
    in the other CUDA stream, since they have remained occupancies.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在核函数执行之间，我们可以发现它们虽然属于不同的CUDA流，但没有争用。这是因为GPU调度器知道执行请求，并优先服务第一个。然而，当当前任务完成时，流多处理器可以为另一个CUDA流中的下一个核函数提供服务，因为它们仍然保持占用。
- en: At the end of all the multiple CUDA stream operations, we need to synchronize
    the host and GPU to confirm that all the CUDA operations on the GPU have finished.
    To do this, we used `cudaDeviceSynchronize()` right after the loop. This function
    can synchronize all the selected GPU operations at the calling point.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个CUDA流操作结束时，我们需要同步主机和GPU，以确认GPU上的所有CUDA操作都已完成。为此，我们在循环结束后立即使用了`cudaDeviceSynchronize()`。此函数可以在调用点同步所选的所有GPU操作。
- en: 'For the synchronization task, we can replace the `cudaDeviceSynchronize()`
    function with the following code. To do this, we also have to change the private
    member `_stream` to be public:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于同步任务，我们可以用以下代码替换`cudaDeviceSynchronize()`函数。为此，我们还必须将私有成员`_stream`更改为公共成员：
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This can be used when we need to provide specific operations from a single host
    thread along with the stream after each stream finishes. But, this is not a good
    operational design since the following operation cannot avoid syncing with the
    other streams.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要在每个流完成后从单个主机线程提供特定操作时，可以使用这个。但是，这不是一个好的操作设计，因为后续操作无法避免与其他流同步。
- en: 'What about using `cudaStreamSynchronize()` in the loop? In this case, we cannot
    perform the overlapping operation that we did before. The following screenshot
    shows the situation:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环中使用`cudaStreamSynchronize()`怎么样？在这种情况下，我们无法执行之前的重叠操作。以下截图显示了这种情况：
- en: '![](img/7c3c1e57-e8fb-4393-9809-7e27ea4e2afc.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c3c1e57-e8fb-4393-9809-7e27ea4e2afc.png)'
- en: This is because `cudaStreamSynchronize()` will synchronize every iteration and
    the application will serialize all the CUDA executions, accordingly. In this situation,
    the execution time was measured as 41.521 ms, which is about 40% slower than the
    overlapped execution time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为`cudaStreamSynchronize()`将同步每次迭代，应用程序将按顺序执行所有CUDA执行。在这种情况下，执行时间为41.521毫秒，比重叠执行时间慢了约40%。
- en: The CUDA callback function
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA回调函数
- en: The **CUDA callback function** is a callable host function to be executed by
    the GPU execution context. Using this, the programmer can specify the host-desired
    host operation following the GPU operations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**CUDA回调函数**是可调用的主机函数，由GPU执行上下文执行。使用此函数，程序员可以指定在GPU操作之后执行主机所需的主机操作。'
- en: The CUDA callback function has a special datatype named `CUDART_CB`, so it should
    be defined using this type. With this type, the programmers can specify which
    CUDA stream launches this function, pass the GPU error status, and provide user
    data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA回调函数具有一个名为`CUDART_CB`的特殊数据类型，因此应该使用这种类型进行定义。使用此类型，程序员可以指定哪个CUDA流启动此函数，传递GPU错误状态，并提供用户数据。
- en: To register the callback function, CUDA provides `cudaStreamAddCallback()`.
    This function accepts CUDA streams, the CUDA callback function, and its parameters,
    so that the specified CUDA callback function can be called from the specified
    CUDA stream and obtain user data. This function has four input parameters, but
    the last one is reserved. So, we don't use that parameter and it remains as `0`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要注册回调函数，CUDA提供了`cudaStreamAddCallback()`。该函数接受CUDA流、CUDA回调函数及其参数，以便从指定的CUDA流中调用指定的CUDA回调函数并获取用户数据。该函数有四个输入参数，但最后一个是保留的。因此，我们不使用该参数，它保持为`0`。
- en: Now, let's enhance our code to use the callback function and output an individual
    stream's performance. Duplicate the source code if you want to separate the previous
    work and this.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们改进我们的代码，使用回调函数并输出单个流的性能。如果要分开之前的工作和这个工作，可以复制源代码。
- en: 'First, place these function declarations into the `private` area of the `Operator`
    class:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将这些函数声明放入`Operator`类的`private`区域：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `Callback()` function will be called after each stream''s operation has
    finished, and the `print_time()` function will report the estimated performance
    using the host side timer, `_p_timer`. The functions'' implementations are as
    follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`Callback()`函数将在每个流的操作完成后被调用，`print_time()`函数将使用主机端计时器`_p_timer`报告估计的性能。函数的实现如下：'
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To have the right timer operations, we need a timer initializer at the `Operator` class''s
    constructor and a timer destroyer at the class''s terminator. Also, we have to
    start the timer at the beginning of the `Operator::async_operation()` function.
    Then, insert the following code block at the end of the function. This allows
    the CUDA stream to call the host-side function when it finishes the previous CUDA
    operations:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行正确的计时操作，我们需要在`Operator`类的构造函数中进行计时器初始化，并在类的终结器中进行计时器销毁。此外，我们必须在`Operator::async_operation()`函数的开头启动计时器。然后，在函数的末尾插入以下代码块。这允许CUDA流在完成先前的CUDA操作时调用主机端函数：
- en: '[PRE27]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, let''s compile and see the execution result. You must use your GPU''s
    compute capability version number for the `gencode` option:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编译并查看执行结果。您必须使用您的GPU的计算能力版本号作为`gencode`选项：
- en: '[PRE28]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This is the execution result of our update:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们更新的执行结果：
- en: '[PRE29]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here, we can see the estimated execution time along with the CUDA stream. The
    callback function estimates its sequence''s execution time. Since there is overlapping
    with other streams and delays for later CUDA streams, we can see the prolonged
    execution time for the late CUDA streams'' execution time. We can confirm those
    elapsed times by matching with the profiled result, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到估计的执行时间以及CUDA流。回调函数估计其序列的执行时间。由于与其他流重叠并延迟后续CUDA流，我们可以看到后续CUDA流的执行时间延长。我们可以通过与分析结果匹配来确认这些经过的时间，如下所示：
- en: '![](img/9cf56a50-a62a-4ccc-839f-a86dbf6526c1.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9cf56a50-a62a-4ccc-839f-a86dbf6526c1.png)'
- en: Although their measured elapsed time is extended along with the stream execution,
    the delta between the streams is regular and we can see these operations from
    the profiled output.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们的测量经过时间随着流的执行而延长，但流之间的差值是固定的，我们可以从分析输出中看到这些操作。
- en: Therefore, we can conclude that we can write host code that can operate right
    after each individual CUDA stream's operation has finished. And, this is advanced
    one against to synchronize each stream from the main threads.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以得出结论，我们可以编写主机代码，以便在每个单独的CUDA流操作完成后立即执行。这比从主线程同步每个流更加先进。
- en: CUDA streams with priority
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有优先级的CUDA流
- en: By default, all CUDA streams have equal priority so they can execute their operations
    in the right order. On top of that, CUDA streams also can have priorities and
    can be superseded by a higher prioritized stream. With this feature, we can have
    GPU operations that meet time-critical requirements.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有CUDA流具有相同的优先级，因此它们可以按正确的顺序执行其操作。此外，CUDA流还可以具有优先级，并且可以被优先级更高的流取代。有了这个特性，我们可以有满足时间关键要求的GPU操作。
- en: Priorities in CUDA
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA中的优先级
- en: 'To use streams with priorities, we need to obtain the available priorities
    from the GPU first. We can obtain these using the `cudaDeviceGetStreamPriorityRange()`
    function. Its output is two numeric values, which are the lowest and highest priority
    values. Then, we can create a priority stream using the `cudaStreamCreaetWithPriority()`
    function, as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用具有优先级的流，我们首先需要从GPU获取可用的优先级。我们可以使用`cudaDeviceGetStreamPriorityRange()`函数来获取这些值。它的输出是两个数值，即最低和最高的优先级值。然后，我们可以使用`cudaStreamCreaetWithPriority()`函数创建一个优先级流，如下所示：
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: There are two additional parameters we should provide. The first one determines
    the created streams' behavior with the default stream. We can make the new stream
    be synchronous with the default stream, like normal streams, using `cudaStreamDefault`.
    On the other hand, we can make it operate concurrently with the default stream
    using `cudaStreamNonBlocking`. Lastly, we can set the stream's priority within
    the priority range. In CUDA programming, the lowest value has the highest priority.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该提供两个额外的参数。第一个确定了创建的流与默认流的行为。我们可以使用`cudaStreamDefault`使新流与默认流同步，就像普通流一样。另一方面，我们可以使用`cudaStreamNonBlocking`使其与默认流并行操作。最后，我们可以在优先级范围内设置流的优先级。在CUDA编程中，最低值具有最高优先级。
- en: 'Also, we can confirm whether the GPU supports this using the following code.
    But, we don''t have to worry too much about this because the priority stream has
    been available since CUDA compute capability 3.5:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用以下代码确认GPU是否支持这一点。但是，我们不必太担心这一点，因为自CUDA计算能力3.5以来，优先级流一直可用：
- en: '[PRE31]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: If the device properties value is `0`, we should stop the application since
    the GPU does not support the stream priorities.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设备属性值为`0`，我们应该停止应用程序，因为GPU不支持流优先级。
- en: Stream execution with priorities
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有优先级的流执行
- en: 'Now, we will reuse the previous multi-stream application with the callback.
    In this code, we can see that the streams can operate in order, and we will see
    how this order can be changed with priorities. We will make a derived class from
    the `Operator` class, and it will handle the priority of the stream. So, we change
    the member variable stream''s protection level from the private member to the
    protected member. And, the constructor can create the stream optionally since
    that can be done by the derived class. The change is shown with the following
    code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将重用之前带有回调的多流应用程序。在这段代码中，我们可以看到流可以按顺序操作，我们将看到如何使用优先级更改这个顺序。我们将从`Operator`类派生一个类，并且它将处理流的优先级。因此，我们将把成员变量流的保护级别从私有成员更改为受保护的成员。构造函数可以选择性地创建流，因为这可以由派生类完成。更改如下代码所示：
- en: '[PRE32]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The derived class, `Operator_with_priority`, will have a function that creates
    a CUDA stream manually with the given priority. That class configuration is as
    follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 派生类`Operator_with_priority`将具有一个函数，可以根据给定的优先级手动创建一个CUDA流。该类的配置如下：
- en: '[PRE33]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As we handle each stream''s operation with the class, we will update the `ls_operator` creation
    code to use the `Operator_with_priority` class in `main()`, to use the class we
    wrote before, as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用类处理每个流的操作时，我们将更新`main()`中的`ls_operator`创建代码，以使用我们之前编写的`Operator_with_priority`类，如下所示：
- en: '[PRE34]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'As we update the class, this class does not create streams before we request
    it to do so. As we discussed before, we need to obtain the available range of
    priority of the GPU using the following code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们更新类时，这个类在我们请求它之前不会创建流。正如我们之前讨论的，我们需要使用以下代码获取GPU可用优先级范围：
- en: '[PRE35]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, let''s create each operation to have different prioritized streams. To
    ease this task, we will let the last operation have the highest stream, and see
    how preemption in CUDA streams works. This can be done with the following code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们创建每个操作以拥有不同的优先级流。为了简化这个任务，我们将让最后一个操作拥有最高的流，并看看CUDA流中的抢占是如何工作的。可以使用以下代码来实现这一点：
- en: '[PRE36]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After that, we will execute each operation, as we did previously:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将执行每个操作，就像之前一样：
- en: '[PRE37]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: To have the proper output, let's synchronize the host and GPU using the `cudaDeviceSynchronize()`
    function. And, finally, we can terminate the CUDA streams. The streams with priorities
    can be terminated with the `cudaStreamDestroy()` function, so we have nothing
    to do in this application as we already did what was needed.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得正确的输出，让我们使用`cudaDeviceSynchronize()`函数同步主机和GPU。最后，我们可以终止CUDA流。具有优先级的流可以使用`cudaStreamDestroy()`函数终止，因此在这个应用程序中我们已经做了必要的事情。
- en: 'Now, let''s compile the code and see the effect. As always, you need to provide
    the right GPU compute capability version to the compiler:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编译代码并查看效果。和往常一样，您需要向编译器提供正确的GPU计算能力版本：
- en: '[PRE38]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'And, the following shows the output of the application:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是应用程序的输出：
- en: '[PRE39]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'From the output, you can see that the operation order has been changed. Stream
    3 precedes stream 1 and stream 2\. The following screenshot shows the profile
    result of how it changed:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，您可以看到操作顺序已经改变。Stream 3在Stream 1和Stream 2之前。下面的屏幕截图显示了它是如何改变的：
- en: '![](img/84f4080c-7d72-4b4c-ad7c-b0192fe69d97.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84f4080c-7d72-4b4c-ad7c-b0192fe69d97.png)'
- en: In this screenshot, there was preemption with the second CUDA stream (Stream
    19 in this case) by the prioritized-last CUDA stream (Stream 21), so that Stream
    19 could finish its work after Stream 21 finished execution. Note that the order
    of data transfer does not change according to this prioritization.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个屏幕截图中，第二个CUDA流（在这种情况下是Stream 19）被优先级最低的CUDA流（Stream 21）抢占，以便在Stream 21执行完毕后Stream
    19完成其工作。请注意，数据传输的顺序不会根据这种优先级而改变。
- en: Kernel execution time estimation using CUDA events
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA事件估计内核执行时间
- en: The previous GPU operation time estimation has one limitation in that it cannot
    measure the kernel execution time. That's because we used timing APIs on the host
    side. So, we need to have synchronized with the host and GPU to measure the kernel
    execution time, and this is impractical considering the overhead and impact on
    the application's performance.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的GPU操作时间估计有一个限制，即它无法测量内核执行时间。这是因为我们在主机端使用了计时API。因此，我们需要与主机和GPU同步以测量内核执行时间，考虑到对应用程序性能的开销和影响，这是不切实际的。
- en: This can be resolved using CUDA events. The CUDA event records GPU-side events
    along with the CUDA stream. CUDA events can be events based on the GPU states
    and record the scheduled timing. Using this, we can trigger the following operations
    or estimate the kernel execution time. In this section, we will cover how we can
    measure the kernel execution time using CUDA events.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过使用CUDA事件来解决。CUDA事件记录GPU端的事件以及CUDA流。CUDA事件可以是基于GPU状态的事件，并记录调度时间。使用这个，我们可以触发以下操作或估计内核执行时间。在本节中，我们将讨论如何使用CUDA事件测量内核执行时间。
- en: The CUDA event is managed with the `cudaEvent_t` handle. We can create a CUDA
    event handle using `cudaEventCreate()` and terminate it with `cudaEventDestroy()`. To
    record event time, you can use `cudaEventRecord()`. Then, the CUDA event handle
    records the event time for the GPU. This function also accepts CUDA streams, so
    that we can enumerate the event time to the specific CUDA stream. After obtaining
    the start and end events of kernel execution, you can obtain the elapsed time
    using `cudaEventElapsedTime()` with millisecond units.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA事件由`cudaEvent_t`句柄管理。我们可以使用`cudaEventCreate()`创建CUDA事件句柄，并使用`cudaEventDestroy()`终止它。要记录事件时间，可以使用`cudaEventRecord()`。然后，CUDA事件句柄记录GPU的事件时间。这个函数还接受CUDA流，这样我们就可以将事件时间枚举到特定的CUDA流。在获取内核执行的开始和结束事件之后，可以使用`cudaEventElapsedTime()`获取经过的时间，单位为毫秒。
- en: Now, let's cover how we can use CUDA events using those APIs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论如何使用CUDA事件来使用这些API。
- en: Using CUDA events
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA事件
- en: 'In this section, we will reuse the previous multi-stream application from the
    second section. Then, we enumerate each GPU kernel''s execution time using CUDA
    events:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重用第二节中的多流应用程序。然后，我们使用CUDA事件枚举每个GPU内核的执行时间：
- en: 'We will use a simple vector addition kernel function, as follows:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用一个简单的向量加法内核函数，如下所示：
- en: '[PRE40]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This code has an iteration that extends the kernel execution time.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码有一个迭代，它延长了内核执行时间。
- en: 'Then, we will use the following snippet to measure the kernel execution time.
    To compare the result, we will use the host side''s timer and CUDA event:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用以下片段来测量内核执行时间。为了比较结果，我们将使用主机端的计时器和CUDA事件：
- en: '[PRE41]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see in this code, we can record the CUDA event right after the kernel
    calls. However, the timer requires synchronization between the GPU and the host.
    For the synchronization, we use the `cudaEventSynchronize(stop)` function because
    we can also make the host thread synchronize with the event. Meanwhile, this code
    only covers handling the timing resources and the kernel execution. But, you also
    have to initialize the required memory to make it work.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在这段代码中所看到的，我们可以在内核调用之后立即记录CUDA事件。然而，计时器需要在GPU和主机之间进行同步。为了同步，我们使用`cudaEventSynchronize(stop)`函数，因为我们也可以使主机线程与事件同步。与此同时，这段代码只涵盖了处理计时资源和内核执行。但是，您还需要初始化所需的内存才能使其工作。
- en: 'After the kernel execution, let''s write code that reports the execution time
    from each timing resource:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在内核执行之后，让我们编写代码报告每个计时资源的执行时间：
- en: '[PRE42]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we will finalize our application by terminating the timing resources,
    using the following code:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将通过终止计时资源来完成我们的应用程序，使用以下代码：
- en: '[PRE43]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s compile and see the output using the following command:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编译并使用以下命令查看输出：
- en: '[PRE44]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'As you can see, we can measure kernel execution time using CUDA events. However,
    the measured times have gaps between the CUDA event and the timer. We can use
    NVIDIA Profiler to verify which provides more accurate information. When we use
    the `# nvprof ./cuda_event` command, the output is as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们可以使用CUDA事件来测量内核执行时间。但是，测量的时间在CUDA事件和计时器之间存在间隙。我们可以使用NVIDIA分析器来验证哪个提供更准确的信息。当我们使用`#
    nvprof ./cuda_event`命令时，输出如下：
- en: '![](img/533f3b7e-846e-4f23-a97d-b277876a05c4.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/533f3b7e-846e-4f23-a97d-b277876a05c4.png)'
- en: As you can see, CUDA events provide accurate results compared to measuring from
    the host.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，与从主机测量相比，CUDA事件提供了准确的结果。
- en: Another benefit of using CUDA events is that we can measure multiple kernel
    execution times simultaneously with multiple CUDA streams. Let's implement an
    example application and see its operation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CUDA事件的另一个好处是，我们可以使用多个CUDA流同时测量多个内核执行时间。让我们实现一个示例应用程序并查看其操作。
- en: Multiple stream estimation
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多流估计
- en: The `cudaEventRecord()` function is asynchronous to the host. In other words,
    there is no synchronization to measure the kernel execution time to the example
    code. To have synchronization with the event and the host, we need to use `cudaEventSynchronize()`.
    For example, kernel function prints can be placed ahead of asynchronous data transfers
    from the device to the host by the synchronization effect, when we place this
    function right after `cudaEventRecord(stop)`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaEventRecord()`函数对主机是异步的。换句话说，没有同步来测量内核执行时间到示例代码。为了使事件和主机同步，我们需要使用`cudaEventSynchronize()`。例如，当我们在`cudaEventRecord(stop)`之后立即放置这个函数时，可以在设备到主机的异步数据传输之前放置内核函数打印，通过同步效果来实现。'
- en: 'It is also useful to measure kernel execution time in multiple CUDA stream
    applications:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个CUDA流应用程序中测量内核执行时间也是有用的：
- en: 'Let''s apply this to the multiple CUDA streams overlapping recipe code in the `04_stream_priority` example
    code. Update the code with the following code:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将这应用到`04_stream_priority`示例代码中的多个CUDA流重叠的代码中。使用以下代码更新代码：
- en: '[PRE45]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, we will define the `print_time()` function we included at this time,
    as follows:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将定义此时包含的`print_time()`函数，如下所示：
- en: '[PRE46]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, insert the `cudaEventRecord()` function calls at the beginning and end
    of `Operator::async_operation()`, as in the following code:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在`Operator::async_operation()`的开头和结尾插入`cudaEventRecord()`函数调用，如下所示：
- en: '[PRE47]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: For this function, there is a challenge to place synchronization at the end
    of the function. Try this after finishing this section. This will impact the application's
    behavior. It is recommended to try to explain the output to yourself, and then
    confirm that using the profiler.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个函数，在函数的末尾放置同步是一个挑战。在完成本节后尝试这样做。这将影响应用程序的行为。建议尝试自己解释输出，然后使用分析器进行确认。
- en: 'Now, let''s compile and see the execution time report, as follows; it shows
    similar performance to the previous executions:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编译并查看执行时间报告，如下；它显示与先前执行类似的性能：
- en: '[PRE48]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In this output, we also can see each kernel's execution time thanks to the CUDA
    event. From this result, we could see that the kernel execution time is prolonged,
    as we saw in the previous section.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个输出中，我们还可以看到每个内核的执行时间，这要归功于CUDA事件。从这个结果中，我们可以看到内核执行时间延长了，就像我们在上一节中看到的那样。
- en: 'If you want to learn more about the features of CUDA events, check NVIDIA''s
    CUDA event documention: [https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于CUDA事件特性的信息，请查看NVIDIA的CUDA事件文档：[https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html)。
- en: Now, we will cover some other aspects of managing CUDA grids. The first item
    is dynamic parallelism, which enables kernel calls from the GPU kernel function.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将介绍管理CUDA网格的其他一些方面。第一项是动态并行性，它使GPU内核函数能够进行内核调用。
- en: CUDA dynamic parallelism
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA动态并行性
- en: '**CUDA dynamic parallelism** (**CDP**) is a device runtime feature that enables
    nested calls from device functions. These nested calls allow different parallelism
    for the child grid. This feature is useful when you need a different block size
    depending on the problem.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**CUDA动态并行性**（**CDP**）是一种设备运行时功能，它允许从设备函数进行嵌套调用。这些嵌套调用允许子网格具有不同的并行性。当问题需要不同的块大小时，此功能非常有用。'
- en: Understanding dynamic parallelism
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解动态并行性
- en: 'Like normal kernel calls from the host, the GPU kernel call can make a kernel
    call as well. The following sample code shows how it works:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 与主机的普通内核调用一样，GPU内核调用也可以进行内核调用。以下示例代码显示了它的工作原理：
- en: '[PRE49]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: As you can see in these functions, we need to make sure which CUDA thread makes
    kernel calls to control the amount that the grid creates. To learn more about
    this, let's implement the first application using this.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在这些函数中所见，我们需要确保哪个CUDA线程进行内核调用以控制网格创建的数量。要了解更多信息，让我们使用这个实现第一个应用程序。
- en: Usage of dynamic parallelism
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态并行性的使用
- en: 'Our dynamic parallelism code will create a parent grid, and that parent will
    create a couple of child grids:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的动态并行性代码将创建一个父网格，该父网格将创建一些子网格：
- en: 'First, we will write the `parent_kernel()` function and the `child_kernel()`
    function using the following code:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用以下代码编写`parent_kernel()`函数和`child_kernel()`函数：
- en: '[PRE50]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: As you can see in this code, the parent kernel function creates child kernel
    grids as the number of blocks. And, the child grids increment the designated memory
    by `1` to mark their operation. After the kernel execution, the parent kernel
    waits until all the child grids finish their jobs using the `cudaDeviceSynchronize()` function.
    When we make synchronization, we should determine the range of the synchronization.
    If we need to synchronize at the block level, we should choose `__synchthread()`
    instead.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在这段代码中所见，父内核函数创建子内核网格作为块的数量。然后，子网格递增指定的内存`1`来标记它们的操作。内核执行后，父内核使用`cudaDeviceSynchronize()`函数等待所有子网格完成其工作。在进行同步时，我们应确定同步的范围。如果我们需要在块级别进行同步，我们应选择`__synchthread()`。
- en: 'Write the `main()` function using the following code:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码编写`main()`函数：
- en: '[PRE51]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: As discussed earlier, we will create child grids along with the number of blocks.
    So, we will execute the parent kernel function with a grid size of `4`, whereas
    the block size is `1`.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面讨论的，我们将创建子网格以及块的数量。因此，我们将使用网格大小为`4`来执行父内核函数，而块大小为`1`。
- en: 'To compile a CDP application, we should provide the `-rdc=true` option to the
    `nvcc` compiler. Hence, the command to compile the source is as follows:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要编译CDP应用程序，我们应该为`nvcc`编译器提供`-rdc=true`选项。因此，编译源代码的命令如下：
- en: '[PRE52]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let''s profile this application to understand its operation. The following screenshot
    shows how this nested call works:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们对这个应用程序进行分析，以了解其操作。以下截图显示了这个嵌套调用的工作原理：
- en: '![](img/765aef65-f755-4eb1-9697-ff07d3efb6d7.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/765aef65-f755-4eb1-9697-ff07d3efb6d7.png)'
- en: As we can see in this screenshot, the parent kernel creates a child grid, and
    we can see their relationship with the right angle mark at the left panel. Then,
    the parent grid (parent_kernel) waits for its execution until the child to finish
    its job. CUDA does not support CDT profiling for SM70 (Volta architecture) at
    this time, so I have used Tesla P40 to obtain this output.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在这个屏幕截图中所见，父内核创建了一个子网格，我们可以在左侧面板的右角标中看到它们的关系。然后，父网格（parent_kernel）等待其执行，直到子网格完成其工作。CUDA目前不支持SM70（Volta架构）的CDT分析，因此我使用Tesla
    P40来获得这个输出。
- en: Recursion
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归
- en: 'One of the benefits of dynamic parallelism is that we can create a recursion.
    The following code shows an example of a recursive kernel function:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 动态并行性的一个好处是我们可以创建递归。以下代码显示了一个递归内核函数的示例：
- en: '[PRE53]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As you can see, there is not much difference from the previous dynamic parallelism
    kernel function. However, we should use this with caution considering the resource
    usage and limitations. In general, dynamic parallel kernels can conservatively
    reserve up to 150 MB of device memory to track pending grid launches and the parent
    grid status by synchronizing on a child grid launch. In addition, the synchronization
    must be carefully done across multiple levels, while the depth of nested kernel
    launches is limited to 24 levels. Finally, the runtime that controls nested kernel
    launches can affect overall performance.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，与以前的动态并行内核函数相比，没有太大的区别。但是，我们应该谨慎使用这个功能，考虑到资源使用和限制。一般来说，动态并行内核可以保守地保留高达150MB的设备内存来跟踪待处理的网格启动和通过在子网格启动上进行同步来同步父网格的状态。此外，同步必须在多个级别上小心进行，而嵌套内核启动的深度限制为24级。最后，控制嵌套内核启动的运行时可能会影响整体性能。
- en: If you need to learn about the restrictions and limitations of dynamic parallelism,
    see the following programming guide: [https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#implementation-restrictions-and-limitations](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#implementation-restrictions-and-limitations).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要了解动态并行性的限制和限制，请参阅以下编程指南：[https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#implementation-restrictions-and-limitations](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#implementation-restrictions-and-limitations)。
- en: 'We will cover its application for quick sorting implementation in [Chapter
    7](71d77c43-0064-491e-9b43-307a05bd6555.xhtml), *Parallel Programming Patterns
    in CUDA*. To learn more about dynamic parallelism, see the following documentation:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第7章](71d77c43-0064-491e-9b43-307a05bd6555.xhtml)中介绍其在CUDA中快速排序实现中的应用，即*CUDA中的并行编程模式*。要了解更多关于动态并行性的信息，请参阅以下文档：
- en: '[https://devblogs.nvidia.com/cuda-dynamic-parallelism-api-principles/](https://devblogs.nvidia.com/cuda-dynamic-parallelism-api-principles/)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: https://devblogs.nvidia.com/cuda-dynamic-parallelism-api-principles/
- en: '[http://on-demand.gputechconf.com/gtc/2012/presentations/S0338-New-Features-in-the-CUDA-Programming-Model.pdf](http://on-demand.gputechconf.com/gtc/2012/presentations/S0338-New-Features-in-the-CUDA-Programming-Model.pdf)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://on-demand.gputechconf.com/gtc/2012/presentations/S0338-New-Features-in-the-CUDA-Programming-Model.pdf](http://on-demand.gputechconf.com/gtc/2012/presentations/S0338-New-Features-in-the-CUDA-Programming-Model.pdf)'
- en: Grid-level cooperative groups
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网格级别的协作组
- en: 'As discussed in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA
    Thread Programming,* CUDA provides cooperative groups. Cooperative groups can
    be categorized by their grouping targets: warp-level, block-level, and grid-level
    groups. This recipe covers grid-level cooperative groups, and looks at how cooperative
    groups handle the CUDA grid.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第3章](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml) 中所讨论的，CUDA提供了协作组。协作组可以根据其分组目标进行分类：warp级别、块级别和网格级别的组。本文介绍了网格级别的协作组，并探讨了协作组如何处理CUDA网格。
- en: The most prominent benefit of the cooperative group is the explicit synchronization
    of the target parallel object. Using the cooperative group, the programmer can
    design their application to synchronize CUDA parallel objects, thread blocks,
    or grids explicitly. Using the block-level cooperative group covered in [Chapter
    3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA Thread Programming*, we
    can write more readable code by specifying which CUDA threads or blocks need to
    synchronize.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 协作组最显著的好处是对目标并行对象的显式同步。使用协作组，程序员可以设计他们的应用程序来显式同步CUDA并行对象、线程块或网格。使用[第3章](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml)中介绍的块级协作组，*CUDA线程编程*，我们可以通过指定需要同步的CUDA线程或块来编写更易读的代码。
- en: Understanding grid-level cooperative groups
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解网格级协作组
- en: 'Since version 9.0, CUDA provides another level of cooperative groups, working
    with grids. To be specific, there are two grid-level cooperative groups: `grid_group`
    and `multi_grid_group`. Using these groups, the programmer can describe the grid''s
    operation to sync on a single GPU or multiple GPUs.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 自9.0版本以来，CUDA提供了另一级协作组，与网格一起工作。具体来说，有两个网格级协作组：`grid_group`和`multi_grid_group`。使用这些组，程序员可以描述网格在单个GPU或多个GPU上的操作同步。
- en: 'In this recipe, we will explore the functionality of `grid_group`, which can
    synchronize grid with reduction problems, as mentioned in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml),
    *CUDA Thread Programming*, regarding the previous reduction design based on block-level
    reduction. Each thread block produces its own reduction results and stores them
    into the global memory. Then, another block-wise reduction kernel launches until
    we obtain a single reduced value. That''s because finishing kernel operation can
    guarantee that the next **reduction** kernel to read a reduced value from the
    multiple thread blocks. Its design is described by the diagram on the left:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将探索`grid_group`的功能，它可以同步网格与减少问题，就像[第3章](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml)中所提到的，*CUDA线程编程*，关于基于块级减少的先前减少设计。每个线程块产生自己的减少结果，并将它们存储到全局内存中。然后，另一个块级减少内核启动，直到我们获得单个减少值。这是因为完成内核操作可以保证下一个**减少**内核从多个线程块中读取减少值。其设计由左侧的图表描述：
- en: '![](img/e70bce0c-8537-4ad1-a5c6-82cc22c93cb0.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/e70bce0c-8537-4ad1-a5c6-82cc22c93cb0.png)
- en: On the other hand, grid-level synchronization enables another kernel design,
    which synchronizes block-wise **reduction** results internally, so that the host
    can have only a single kernel call to obtain the reduction **result**. In cooperative
    groups, `grid_group.sync()` provides such functionality, so we can write the reduction
    kernel without kernel-level iteration.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，网格级同步使另一种内部同步块式**减少**结果的内核设计成为可能，以便主机只需调用一次内核即可获得减少**结果**。在协作组中，`grid_group.sync()`提供了这样的功能，因此我们可以编写减少内核而无需内核级迭代。
- en: 'To use the `grid_group.sync()` function, we need to call the kernel function
    using the `cudaLaunchCooperativeKernel()` function. Its interface design is as
    follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`grid_group.sync()`函数，我们需要使用`cudaLaunchCooperativeKernel()`函数调用内核函数。其接口设计如下：
- en: '[PRE54]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: So, its usage is the same as the `cudaLaunchKernel()` function, which launches
    a kernel function.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它的使用方式与`cudaLaunchKernel()`函数相同，该函数启动内核函数。
- en: To make all the thread blocks in `grid_group` synchronize, the total number
    of active thread blocks in the grid should not exceed the number of maximum active
    blocks for the kernel function and the device. The maximum active block size on
    a GPU is a multiplication of the maximum amount of active blocks per SM and the
    number of streaming multiprocessors. The violation of this rule can result in
    deadlock or undefined behavior. We can obtain the maximum amount of active thread
    blocks of a kernel function per SM using the `cudaOccupancyMaxActiveBlocksPerMultiprocessor()`
    function, by passing the kernel function and block size information.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使`grid_group`中的所有线程块同步，网格中活动线程块的总数不应超过内核函数和设备的最大活动块数。GPU上的最大活动块大小是每个SM的最大活动块数和流处理器的数量的乘积。违反此规则可能导致死锁或未定义行为。我们可以使用`cudaOccupancyMaxActiveBlocksPerMultiprocessor()`函数来获取每个SM内核函数的最大活动线程块数，通过传递内核函数和块大小信息。
- en: Usage of grid_group
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`grid_group`的用法
- en: 'Now, let''s apply `grid_group` to the parallel reduction problem and see how
    the GPU programming can be changed:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将`grid_group`应用于并行减少问题，并看看GPU编程如何改变：
- en: We will reuse the host code from the previous parallel reduction code in `03_cuda_thread_programming/07_cooperative_groups`.
    In other words, we will change the GPU's operation with small changes in the host
    code. You also can use the code in the `07_grid_level_cg` directory.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重用之前并行减少代码中的主机代码，即`03_cuda_thread_programming/07_cooperative_groups`。换句话说，我们将通过对主机代码进行小的更改来改变GPU的操作。您还可以使用`07_grid_level_cg`目录中的代码。
- en: Now, let's write some block-level reduction code. When we have grid-level cooperative
    groups, all the thread blocks must be active. In other words, we cannot execute
    multiple thread blocks than the GPU-capable active blocks. So, this reduction
    will accumulate the input data first to cover all the data with a limited number
    of thread blocks. Then, it will conduct the parallel reduction at the block level
    as we covered in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA
    Thread Programming*.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们编写一些块级减少代码。当我们有网格级协作组时，所有线程块必须是活动的。换句话说，我们不能执行多个线程块，而GPU能够执行的活动块。因此，这个减少将首先累积输入数据，以覆盖所有数据，使用有限数量的线程块。然后，它将在块级进行并行减少，就像我们在[第3章](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml)中所介绍的那样，*CUDA线程编程*。
- en: 'The following code shows its implementation:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了它的实现：
- en: '[PRE55]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Then, let''s write a kernel function that executes the block-wise reduction
    considering the number of active blocks and `grid_group`. In this function, we
    will call the block-level reduction code and synchronize them at the grid level.
    Then, we will perform parallel reduction from the outputs as we covered in [Chapter
    3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA Thread Programming*. The
    following code shows its implementation:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们编写一个内核函数，考虑活动块数和`grid_group`执行块级减少。在这个函数中，我们将调用块级减少代码，并在网格级别进行同步。然后，我们将从输出中执行并行减少，就像我们在[第3章](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml)
    *CUDA线程编程*中所介绍的那样。以下代码显示了其实现：
- en: '[PRE56]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Finally, we will implement the host code that calls the kernel function with
    the available active thread block dimension. To do this, this function uses the
    `cudaoccupancyMaxActiveBlocksPerMultiprocessor()` function. Also, the grid-level
    cooperative group requires us to call the kernel function via the `cudaLaunchCooperativeKernel()`
    function. You can see the implementation here:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将实现调用具有可用活动线程块维度的内核函数的主机代码。为此，此函数使用`cudaoccupancyMaxActiveBlocksPerMultiprocessor()`函数。此外，网格级合作组要求我们通过`cudaLaunchCooperativeKernel()`函数调用内核函数。您可以在这里看到实现：
- en: '[PRE57]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Now, make sure that the host function can be called from the `reduction.cpp`
    file.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，请确保可以从`reduction.cpp`文件中调用主机函数。
- en: 'Then, let''s compile the code and see its operation. The following shell command
    compiles the code and executes the application. The compute capability should
    be equal to or greater than `70`:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们编译代码并查看其操作。以下shell命令编译代码并执行应用程序。计算能力应该等于或大于`70`：
- en: '[PRE58]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output performance is far behind what we saw in the final result of [Chapter
    3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA Thread Programming*. As
    the `block_reduction()` function uses high memory throughput at the beginning,
    it is highly memory bounded:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 输出性能远远落后于我们在[第3章](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml) *CUDA线程编程*的最终结果。由于`block_reduction()`函数在开始时使用了高内存吞吐量，因此它是高度内存绑定的：
- en: '![](img/fbd18a08-d2a1-4bc1-a9b3-647c67a72240.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbd18a08-d2a1-4bc1-a9b3-647c67a72240.png)'
- en: The major impact factor is that we can only use the active thread blocks. So,
    we cannot hide the memory access time. Actually, the usage of `grid_group` has
    other purposes, such as graph search, genetic algorithms, and particle simulation,
    which requires us to keep the states active for long times for performance.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 主要影响因素是我们只能使用活动线程块。因此，我们无法隐藏内存访问时间。实际上，使用`grid_group`还有其他目的，例如图搜索、遗传算法和粒子模拟，这要求我们保持状态长时间处于活动状态以获得性能。
- en: This grid-level synchronization can provide more benefits to performance and
    programmability. As this enables the kernel to synchronize itself, we can make
    the kernel iterate itself. So, it is useful to solve the graph search, genetic
    algorithms, and practical simulations. To learn more about cooperative groups
    in `grid_groups`, please refer to the documentation provided at [http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf](http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这种网格级同步可以为性能和可编程性提供更多好处。由于这使得内核可以自行同步，我们可以使内核自行迭代。因此，它对解决图搜索、遗传算法和实际模拟非常有用。要了解有关`grid_groups`中合作组的更多信息，请参阅提供的文档[http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf](http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf)。
- en: CUDA kernel calls with OpenMP
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenMP的CUDA内核调用
- en: To enlarge the concurrency of the application, we can make kernel calls from
    the host's parallel tasks. OpenMP, for instance, provides easy parallelism of
    the multi-core architecture. This recipe covers how CUDA can operate OpenMP.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加应用程序的并发性，我们可以从主机的并行任务中进行内核调用。例如，OpenMP提供了多核架构的简单并行性。本教程介绍了CUDA如何操作OpenMP。
- en: OpenMP and CUDA calls
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenMP和CUDA调用
- en: OpenMP uses a fork-and-join model of parallelism to target multi-core CPUs.
    The master thread initiates the parallel operations and creates worker threads.
    The host threads operate their own jobs in parallel and join after finishing their
    work.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP使用分叉-合并模型的并行性来针对多核CPU。主线程启动并行操作并创建工作线程。主机线程并行运行自己的工作，并在完成工作后加入。
- en: Using OpenMP, CUDA kernel calls can be executed in parallel with multiple threads.
    This helps the programmer to not have to maintain individual kernel calls, instead
    allowing them to have kernel executions depend on the host thread's index.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenMP，CUDA内核调用可以与多个线程并行执行。这有助于程序员不必维护单独的内核调用，而是允许它们的内核执行依赖于主机线程的索引。
- en: 'We will use the following OpenMP APIs in this section:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用以下OpenMP API：
- en: '`omp_set_num_threads()` sets a number of worker threads that will work in parallel.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`omp_set_num_threads()`设置将并行工作的工作线程数。'
- en: '`omp_get_thread_num()` returns an index of worker threads so that each thread
    can identify their task.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`omp_get_thread_num()`返回工作线程的索引，以便每个线程可以识别其任务。'
- en: '`#pragma omp parallel {}` specifies a parallel region that will be covered
    by the worker threads.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#pragma omp parallel {}` 指定了一个并行区域，将由工作线程覆盖。'
- en: Now, let's write some code in which OpenMP calls a CUDA kernel function.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一些代码，其中OpenMP调用CUDA内核函数。
- en: CUDA kernel calls with OpenMP
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA与OpenMP的内核调用
- en: 'In this section, we will implement a multi-stream vector add application that
    uses OpenMP. To do this, we will modify the previous version and see the difference:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个使用OpenMP的多流矢量加法应用程序。为此，我们将修改先前的版本并查看差异：
- en: To test OpenMP with CUDA, we will modify the code from the `03_cuda_callback` directory.
    We will modify the body of the `main()` function, or you can use the provided
    sample code placed in the `08_openmp_cuda` directory.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要测试CUDA中的OpenMP，我们将修改`03_cuda_callback`目录中的代码。我们将修改`main()`函数的主体，或者您可以使用放置在`08_openmp_cuda`目录中的提供的示例代码。
- en: 'Now, let''s include the OpenMP header file and modify the code. To use OpenMP
    in the code, we should use `#include <omp.h>`. And, we will update the code that
    iterates `for` each stream to use OpenMP:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们包括OpenMP头文件并修改代码。要在代码中使用OpenMP，我们应该使用`#include <omp.h>`。而且，我们将更新代码，使其使用OpenMP来迭代每个流：
- en: '[PRE59]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Compile the code with the following command:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令编译代码：
- en: '[PRE60]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Whenever you execute this application, you will see that each stream finishes
    their job out of order. Also, each stream shows a different time. That's because
    OpenMP can create multiple threads, and the operation is determined at runtime.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 每当您执行此应用程序时，您将看到每个流以无序方式完成其工作。此外，每个流显示不同的时间。这是因为OpenMP可以创建多个线程，并且操作是在运行时确定的。
- en: 'To understand its operation, let''s profile the application. The following
    screenshot shows the profiled timeline of the application. This can be different
    from yours due to the scheduling:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解其运行情况，让我们对应用程序进行分析。以下截图显示了应用程序的分析时间表。由于调度的原因，这可能与您的情况不同：
- en: '![](img/50012166-e450-4d38-8ebf-a44a55ad9844.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50012166-e450-4d38-8ebf-a44a55ad9844.png)'
- en: As you can see in this screenshot, you will be able to see that the data transfer
    has reversed compared to Stream 17\. For this reason, we can see that the second
    stream could finish its job at last.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在此截图中所见，您将能够看到数据传输与Stream 17相比已经反转。因此，我们可以看到第二个流最终完成了它的工作。
- en: Multi-Process Service
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多进程服务
- en: 'The GPU is capable of executing kernels from concurrent CPU processes. However,
    by default, they are only executed in a time-sliced manner even though each kernel
    doesn''t fully utilize GPU compute resources. To address this unnecessary serialization,
    the GPU provides **Multi-Process Service** (**MPS**) mode. This enables different
    processes to execute their kernels simultaneously on a GPU to fully utilize GPU
    resources. When it is enabled, the `nvidia-cuda-mps-control` daemon monitors the
    target GPU and manages process kernel operations using that GPU. This feature
    is only available on Linux. Here, we can see the MPS in which multiple processes
    share the same GPU:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: GPU能够从并发的CPU进程中执行内核。但是，默认情况下，它们只以分时方式执行，即使每个内核没有充分利用GPU计算资源。为了解决这种不必要的串行化，GPU提供了**多进程服务**（**MPS**）模式。这使得不同的进程能够同时在GPU上执行它们的内核，以充分利用GPU资源。启用时，`nvidia-cuda-mps-control`守护进程监视目标GPU，并使用该GPU管理进程内核操作。此功能仅在Linux上可用。在这里，我们可以看到多个进程共享同一个GPU的MPS：
- en: '![](img/2f97d170-6740-41d4-aa8b-7552745e9838.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f97d170-6740-41d4-aa8b-7552745e9838.jpg)'
- en: As we can see, each process has a part that runs in parallel in the GPU (green
    bars), while some part runs on the CPU (blue bars). Ideally, you would need both
    the blue bars and green bars to get the best performance. This can be made possible
    by making use of the MPS feature, which is supported by all the latest GPUs.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，每个进程在GPU上并行运行一部分（绿色条），而一部分在CPU上运行（蓝色条）。理想情况下，您需要蓝色条和绿色条都能获得最佳性能。这可以通过利用所有最新GPU支持的MPS功能来实现。
- en: Please note that multiple MPI processes running on the same GPU are beneficial
    when one MPI process is unable to saturate the whole GPU and a significant part
    of the code is also running on the CPU. If one MPI process utilizes the whole
    GPU, even though the CPU part (blue bar) will reduce, the green bar time will
    not as the GPU is completely utilized by one MPI process. The other MPI processes
    will access the GPU one after another in a time-sliced manner based on the GPU
    architecture. This is similar to the launching-concurrent-kernels scenario. If
    one kernel utilizes the whole GPU, then the other kernel will either wait for
    the first kernel to finish or be time-sliced.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当一个MPI进程无法饱和整个GPU并且代码的重要部分也在CPU上运行时，多个MPI进程在同一个GPU上运行是有益的。如果一个MPI进程利用整个GPU，即使CPU部分（蓝色条）会减少，绿色条的时间也不会减少，因为GPU完全被一个MPI进程利用。其他MPI进程将根据GPU架构以分时方式依次访问GPU。这类似于启动并发内核的情况。如果一个内核利用整个GPU，那么另一个内核要么等待第一个内核完成，要么进行分时。
- en: 'The good thing about this is that no changes need to be made to the application
    to make use of MPS. The MPS process runs as a daemon, as shown in the following
    commands:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的好处是不需要对应用程序进行任何更改即可使用MPS。MPS进程作为守护进程运行，如下命令所示：
- en: '[PRE61]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: After running this command, all the processes submit their commands to the MPS
    daemon, which takes care of submitting the CUDA commands to GPU. For the GPU,
    there is only one process accessing the GPU (MPS Daemon) and hence multiple kernels
    can run concurrently from multiple processes. This can help overlap memory copies
    from one process with kernel executions from other MPI processes.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令后，所有进程都将其命令提交给MPS守护进程，该守护进程负责将CUDA命令提交给GPU。对于GPU，只有一个进程访问GPU（MPS守护进程），因此多个进程可以同时运行来自多个进程的多个内核。这可以帮助将一个进程的内存复制与其他MPI进程的内核执行重叠。
- en: Introduction to Message Passing Interface
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消息传递接口简介
- en: '**Message Passing Interface** (**MPI**) is a parallel computing interface which
    enables to trigger multiple processes across the computing units - CPU cores,
    GPU, and nodes. The typical dense multi-GPU system contains 4-16 GPUs, while the
    number of CPU cores ranges between 20-40 CPUs. In MPI-enabled code, some parts
    of the application run in parallel as different MPI processes on multiple cores.
    Each MPI process will call CUDA. It is very important to understand mapping an
    MPI process to the respective GPU. The easiest mapping is 1:1, that is, each MPI
    process gets exclusive access to the respective GPU. Also, we can ideally map
    multiple MPI processes to a single GPU.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**消息传递接口**（**MPI**）是一种并行计算接口，它能够触发多个进程跨计算单元 - CPU核心、GPU和节点。典型的密集多GPU系统包含4-16个GPU，而CPU核心的数量在20-40个之间。在启用MPI的代码中，应用程序的某些部分作为不同的MPI进程在多个核心上并行运行。每个MPI进程都将调用CUDA。了解将MPI进程映射到相应的GPU非常重要。最简单的映射是1:1，即每个MPI进程都独占相应的GPU。此外，我们还可以将多个MPI进程理想地映射到单个GPU上。'
- en: 'To have the multi-processes application scenario to a single GPU we will use
    MPI. To use MPI, you need to install OpenMPI for your system. Follow these steps
    to install OpenMPI for Linux. This operation has been tested on Ubuntu 18.04,
    so this can vary if you use another distribution:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将多进程应用场景应用到单个GPU上，我们将使用MPI。要使用MPI，您需要为您的系统安装OpenMPI。按照以下步骤在Linux上安装OpenMPI。此操作已在Ubuntu
    18.04上进行了测试，因此如果您使用其他发行版，可能会有所不同：
- en: '[PRE62]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now, let's implement an application that can work with MPI and CUDA.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个可以与MPI和CUDA一起工作的应用程序。
- en: Implementing an MPI-enabled application
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个启用MPI的应用程序
- en: 'To make an application work with MPI, we need to put some code that can understand
    MPI commands in the application:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 要使应用程序与MPI一起工作，我们需要在应用程序中放入一些可以理解MPI命令的代码：
- en: We will reuse the OpenMP sample code, so copy the `openmp.cu` file in the `08_openmp_cuda`
    directory.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重用OpenMP示例代码，因此将`openmp.cu`文件复制到`08_openmp_cuda`目录中。
- en: 'Insert the `mpi` header `include` statement at the beginning of the code:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代码开头插入`mpi`头文件`include`语句：
- en: '[PRE63]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Insert the following code right after the stopwatch has been created in the
    `main()` function:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`main()`函数中创建秒表后立即插入以下代码：
- en: '[PRE64]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Slice the required memory size by the number of processes, after the code mentioned
    in step 3, like this:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照第3步中提到的代码后，将所需的内存大小切割为进程数，如下所示：
- en: '[PRE65]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We need to make each thread to report their process which they belong. Let''s
    update the `printf()` function in the parallel execution code block, as follows:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要让每个线程报告它们所属的进程。让我们更新并行执行代码块中的`printf()`函数，如下所示：
- en: '[PRE66]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: At the end of `main()`, place the `MPI_Finalize()` function to close the MPI
    instances.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`main()`的末尾放置`MPI_Finalize()`函数以关闭MPI实例。
- en: 'Compile the code with the following command:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令编译代码：
- en: '[PRE67]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: You must use your GPU's compute capability version number for the `gencode`
    option.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须使用GPU的计算能力版本号来选择`gencode`选项。
- en: 'Test the compiled application using the following command:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令测试编译后的应用程序：
- en: '[PRE68]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now, test MPI execution using the following command:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令测试MPI执行：
- en: '[PRE69]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Enabling MPS
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启用MPS
- en: Enabling MPS in GPUs requires some modification of the GPU operation mode. But,
    you need to have a GPU architecture later than the Kepler architecture.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上启用MPS需要对GPU操作模式进行一些修改。但是，您需要具有比Kepler架构更晚的GPU架构。
- en: 'Let''s follow the steps required to enable MPS as bellow:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤启用MPS：
- en: 'Enable MPS mode using the following commands:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令启用MPS模式：
- en: '[PRE70]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Or, you can use the `make enable_mps` command for this recipe sample code,
    which is pre-defined in `Makefile`. Then, we can see the updated compute mode
    from the `nivida-smi` output:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用`make enable_mps`命令来使用此预定义在`Makefile`中的配方示例代码。然后，我们可以从`nivida-smi`输出中看到更新后的计算模式：
- en: '![](img/329cace9-b5f9-43e7-8716-c2aa40c0547f.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/329cace9-b5f9-43e7-8716-c2aa40c0547f.png)'
- en: 'Now, test MPI execution with MPS mode using the following command:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令测试MPS模式下的MPI执行：
- en: '[PRE71]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: As you can see, each process' elapsed time has reduced compared to the previous
    executions.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，与之前的执行相比，每个进程的经过时间都有所减少。
- en: 'Now, let''s recover the original mode. To disable MPS mode, use the following
    commands:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们恢复原始模式。要禁用MPS模式，请使用以下命令：
- en: '[PRE72]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Or, you can use the `make disable_mps` command for this recipe sample code,
    which is pre-defined in `Makefile`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用`make disable_mps`命令来使用此预定义在`Makefile`中的配方示例代码。
- en: 'To learn more about MPS, please use the following links:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于MPS的信息，请使用以下链接：
- en: '[http://on-demand.gputechconf.com/gtc/2015/presentation/S5584-Priyanka-Sah.pdf](http://on-demand.gputechconf.com/gtc/2015/presentation/S5584-Priyanka-Sah.pdf)'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://on-demand.gputechconf.com/gtc/2015/presentation/S5584-Priyanka-Sah.pdf](http://on-demand.gputechconf.com/gtc/2015/presentation/S5584-Priyanka-Sah.pdf)'
- en: '[https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)'
- en: Profiling an MPI application and understanding MPS operation
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对MPI应用程序进行分析并了解MPS操作
- en: Using MPI, the kernel from multiple processes can share GPU resources at the
    same time, which enhances overall GPU utilization. Without MPS, the GPU resources
    are shared inefficiently due to time-sliced sharing and the context-switching
    overhead.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MPI，多个进程的内核可以同时共享GPU资源，从而增强整体GPU利用率。没有MPS，由于时间切片共享和上下文切换开销，GPU资源被低效地共享。
- en: 'The following screenshot shows the timeline profile result of multiple processes
    without MPS:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了没有MPS的多个进程的时间轴配置文件结果：
- en: '![](img/13ccf0f1-20b5-40be-8738-7b460f2f5cac.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13ccf0f1-20b5-40be-8738-7b460f2f5cac.png)'
- en: In this profile, we can see that two CUDA contexts share a GPU, and kernel execution
    times are prolonged due to the time-sharing between the contexts.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配置文件中，我们可以看到两个CUDA上下文共享一个GPU，并且由于上下文之间的时间共享，内核执行时间延长。
- en: 'On the other hand, MPS mode manages the kernel execution request, so all the
    kernel executions are launched as though using a single process. The following
    screenshot shows kernel execution with MPS mode:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，MPS模式管理内核执行请求，因此所有内核执行都会像使用单个进程一样启动。以下屏幕截图显示了MPS模式下的内核执行：
- en: '![](img/37c34d58-26dd-41ce-8289-65c68a535761.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37c34d58-26dd-41ce-8289-65c68a535761.png)'
- en: As you can see, only one CUDA stream resides on a GPU and controls all the CUDA
    streams. Also, all the kernel execution times are stabilized and the total elapsed
    time is reduced using MPS. In conclusion, using MPS mode benefits overall performance
    for multiple GPU processes and shares GPU resources.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，只有一个CUDA流驻留在GPU上并控制所有CUDA流。此外，所有内核执行时间都得到了稳定，并且使用MPS可以减少总的经过时间。总之，使用MPS模式有利于多个GPU进程的整体性能，并共享GPU资源。
- en: '`nvprof` supports dumping profiler information for multiple MPI processes in
    a different file. For example, for an Open MPI-based application, the following
    command will dump profiling information in multiple files, each with a unique
    name based on the MPI process rank:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '`nvprof`支持将多个MPI进程的分析器信息转储到不同的文件中。例如，对于基于Open MPI的应用程序，以下命令将在多个文件中转储分析信息，每个文件的名称都基于MPI进程的排名：'
- en: '[PRE73]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Or, you can use the following command for the sample recipe code:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用以下命令来执行示例代码：
- en: '[PRE74]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Then, you will get two `nvvp` files for each process.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将为每个进程获得两个`nvvp`文件。
- en: 'Now, we will review these `nvvp` files using NVIDIA Visual Profiler with the
    following steps:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用以下步骤使用NVIDIA Visual Profiler来查看这些`nvvp`文件：
- en: 'Open the File | Import menu to create a profiling session by importing the
    `nvvp` files:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开文件|导入菜单，通过导入`nvvp`文件创建一个分析会话：
- en: '![](img/3f802cd1-25da-4e02-b172-9e41b730799f.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f802cd1-25da-4e02-b172-9e41b730799f.png)'
- en: In Windows or Linux, the shortcut key is *Ctrl* + *I*, and OSX uses *command*
    + *I*.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows或Linux中，快捷键是*Ctrl* + *I*，OSX使用*command* + *I*。
- en: 'Then click the Next button after selecting Nvprof from the list:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后从列表中选择Nvprof后，点击下一步按钮：
- en: '![](img/2f84e5d6-9933-4c8d-81d1-e6a6bd03545e.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f84e5d6-9933-4c8d-81d1-e6a6bd03545e.png)'
- en: 'From the Nvprof option, select Multiple processes and click Next >:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Nvprof选项中，选择多个进程，然后单击下一步>：
- en: '![](img/e70799cb-c6f8-4e70-abb6-f8704a433c83.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e70799cb-c6f8-4e70-abb6-f8704a433c83.png)'
- en: 'From Import Nvprof Data, click the Browse... button, and select `nvvp` files,
    which are generated by `nvprof`. To profile an application with multi-process,
    you need to import `nvvp` files as there are processes:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从导入Nvprof数据中，单击浏览...按钮，并选择由`nvprof`生成的`nvvp`文件。要对具有多个进程的应用程序进行分析，您需要导入`nvvp`文件，因为存在多个进程：
- en: '![](img/1582fbfc-ddd9-42b4-882e-23772b33e767.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1582fbfc-ddd9-42b4-882e-23772b33e767.png)'
- en: 'Click Finish, then NVIDIA Visual Profiler shows profiled results in the timeline
    view, as follows:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击完成，然后NVIDIA Visual Profiler将以时间线视图显示分析结果，如下所示：
- en: '![](img/75dd51b7-edb6-4d14-80d3-a73b6725191d.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75dd51b7-edb6-4d14-80d3-a73b6725191d.png)'
- en: 'Please note that only the synchronous MPI calls will be annotated by `nvprof`.
    In the case of an asynchronous MPI API being used, other MPI-specialized profiling
    tools need to be used. Some of the most famous tools include the following:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只有同步MPI调用将由`nvprof`进行注释。如果使用异步MPI API，则需要使用其他MPI专用的分析工具。其中一些最著名的工具包括以下内容：
- en: '**TAU**: TAU is a performance profiling toolkit and is currently maintained
    by the University of Oregon.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TAU**：TAU是一种性能分析工具包，目前由俄勒冈大学维护。'
- en: '**Vampir**: This is a commercially available tool and provides good scalability
    to hundreds of MPI processes.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Vampir**：这是一种商业可用的工具，对数百个MPI进程具有良好的可伸缩性。'
- en: '**Intel VTune Amplifier**: Another option when it comes to commercial tools
    is Intel VTune Amplifier. It is one of the best tools available and can be used
    for MPI application analysis.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Intel VTune Amplifier**：商业工具的另一个选择是Intel VTune Amplifier。它是目前可用的最好的工具之一，可用于MPI应用程序分析。'
- en: 'The latest CUDA Toolkits also allow the MPI API to be annotated. For this,
    the `--annotate-mpi` flag needs to be passed to `nvprof`, as shown in the following
    command:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的CUDA工具包还允许对MPI API进行注释。为此，需要将`--annotate-mpi`标志传递给`nvprof`，如以下命令所示：
- en: '[PRE75]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Kernel execution overhead comparison
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核执行开销比较
- en: 'For iterative parallel GPU tasks, we have three kinds of kernel execution methods:
    iterative kernel calls, having an inner loop, and having recursions using dynamic
    parallelism. The best operation is determined by the algorithm and the application.
    But, you also may consider kernel execution options among them. This recipe helps
    you to compare those kernel execution overheads and review their programmability.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 对于迭代并行GPU任务，我们有三种内核执行方法：迭代内核调用，具有内部循环，以及使用动态并行性进行递归。最佳操作由算法和应用程序确定。但是，您也可以考虑它们之间的内核执行选项。本示例帮助您比较这些内核执行开销并审查它们的可编程性。
- en: To begin with, let's determine which operation we will test. This recipe will
    use a simple SAXPY operation. This helps us to focus and make iterative execution
    code. In addition, the operation control overhead will become heavier as the operation
    gets simpler. But, you can try with any other operation you want, of course.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们确定我们将测试哪种操作。本示例将使用一个简单的SAXPY操作。这有助于我们专注并制作迭代执行代码。此外，随着操作变得更简单，操作控制开销将变得更重。但是，您当然可以尝试任何其他操作。
- en: Implementing three types of kernel executions
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现三种内核执行方式
- en: 'The following steps cover the performance comparison of three different iterative
    operations:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤涵盖了三种不同迭代操作的性能比较：
- en: Create and navigate the `10_kernel_execution_overhead` directory.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建并导航到`10_kernel_execution_overhead`目录。
- en: 'Write the `simple_saxpy_kernel()` function with the following code:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写`simple_saxpy_kernel()`函数，代码如下：
- en: '[PRE76]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Write the `iterative_saxpy_kernel()` function with the following code:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写`iterative_saxpy_kernel()`函数，代码如下：
- en: '[PRE77]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Write the `recursive_saxpy_kernel()` function with the following code:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写`recursive_saxpy_kernel()`函数，代码如下：
- en: '[PRE78]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Write the host code that launches those CUDA kernel functions. At first, we
    will have an iterative function call of the `simple_saxpy_kernel()` function:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写启动这些CUDA内核函数的主机代码。首先，我们将对`simple_saxpy_kernel()`函数进行迭代调用：
- en: '[PRE79]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Secondly, we will call the `iterative_saxpy_kernel()` kernel function, which
    has an iterative loop inside:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们将调用`iterative_saxpy_kernel()`内核函数，该函数内部有一个迭代循环：
- en: '[PRE80]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Lastly, we will call the `recursive_saxpy_kernel()` kernel function, which
    calls itself in a recursive manner:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将调用`recursive_saxpy_kernel()`内核函数，该函数以递归方式调用自身：
- en: '[PRE81]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The number of loops is smaller than or equal to 24, since the maximum recursion
    depth is 24\. Other than simple loop operations, you do not have to place a loop
    operation at the host since it is already defined in the kernel code.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 循环次数小于或等于24，因为最大递归深度为24。除了简单的循环操作外，您不必在主机上放置循环操作，因为它已在内核代码中定义。
- en: 'Compile the code using the following command:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令编译代码：
- en: '[PRE82]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: You must use your GPU's compute capability version number for the `gencode`
    option.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须使用GPU的计算能力版本号来选择`gencode`选项。
- en: 'Test the compiled application. This result was measured using Tesla P40 because
    CUDA 9.x does not support **CUDA Dynamic Parallelism** (**CDP**) profile for Volta
    GPUs:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试编译后的应用程序。这个结果是使用Tesla P40测量的，因为CUDA 9.x不支持Volta GPU的CUDA动态并行性（CDP）配置文件：
- en: '[PRE83]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Comparison of three executions
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 三种执行的比较
- en: 'From the result, we can confirm that the inner loop is the fastest method for
    the iterative operation. The following screenshot shows a profiled result of this
    sample application:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，我们可以确认内部循环是迭代操作中最快的方法。以下截图显示了这个示例应用程序的分析结果：
- en: '![](img/0040c495-f138-4acc-bf8a-c2e2b02ebbda.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0040c495-f138-4acc-bf8a-c2e2b02ebbda.png)'
- en: The iterative kernel call shows the kernel launch overhead for each kernel call.
    The GPU needs to fetch all the required data from the device memory, and needs
    to schedule the GPU resources, and so on. On the other hand, the inner loop kernel
    shows one packed operation because all the required resources are pre-located
    and there's no need to reschedule its execution. The recursive kernel operation
    shows the most prolonged execution time due to the dynamic parallelism limitations
    we discussed previously.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代内核调用显示了每个内核调用的内核启动开销。GPU需要从设备内存中获取所有所需的数据，并需要调度GPU资源等。另一方面，内部循环内核显示了一个打包操作，因为所有所需的资源都是预先定位的，不需要重新调度其执行。由于我们之前讨论的动态并行性限制，递归内核操作显示了最长的执行时间。
- en: In general, using the approach with the least overhead is recommended. However,
    it is hard to say which kernel call design is superior to the others, since there's
    more to the algorithm and its problems than what we've covered here. For instance,
    CDP used to enhance parallelism in certain cases, such as for GPU trees and searches.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，建议使用开销最小的方法。然而，很难说哪种内核调用设计优于其他，因为算法和问题比我们在这里涵盖的要多。例如，CDP用于增强某些情况下的并行性，比如用于GPU树和搜索。
- en: Summary
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have covered several kernel execution mechanisms. We covered
    what CUDA streams are, and how to use them to execute multiple kernel functions
    concurrently. By utilizing the asynchronous operation between the host and the
    GPU, we have learned that we can hide the kernel execution time by making the
    pipelining architecture with data transfer and kernel executions. Also, we can
    make a CUDA stream call the host function using the callback function. We can
    create a prioritized stream, and confirm its prioritized execution, too. To measure
    the exact execution time of a kernel function, we have used CUDA events, and we
    also learned that CUDA events can be used to synchronize with the host. In the
    last section, we also discussed the performance of each kernel execution method.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了几种内核执行机制。我们讨论了CUDA流是什么，以及如何使用它们同时执行多个内核函数。通过利用主机和GPU之间的异步操作，我们学到可以通过数据传输和内核执行来隐藏内核执行时间。此外，我们可以使用回调函数使CUDA流调用主机函数。我们可以创建一个有优先级的流，并确认其有优先级的执行。为了测量内核函数的确切执行时间，我们使用了CUDA事件，并且我们也学到CUDA事件可以用于与主机同步。在最后一节中，我们还讨论了每种内核执行方法的性能。
- en: 'We also covered other kernel operation models: dynamic parallelism and grid-level
    cooperative groups. Dynamic parallelism enables kernel calls inside the kernel
    function so we can make recursive operations with that. The grid-level cooperative
    group enables versatile grid-level synchronization, and we discussed how this
    feature can be useful in a specific area: graph search, genetic algorithms, and
    particle simulations.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还涵盖了其他内核操作模型：动态并行性和网格级协作组。动态并行性使得内核函数内部可以进行内核调用，因此我们可以使用递归操作。网格级协作组实现了多功能的网格级同步，我们讨论了这个特性在特定领域的用途：图搜索、遗传算法和粒子模拟。
- en: Then, we expanded our coverage to the host. CUDA kernels can be called from
    multiple threads or multiple processes. To execute multiple threads, we used OpenMP
    with CUDA and discussed its usefulness. We used MPI to simulate multiple process
    operations, and could see how MPS benefits overall application performance.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们扩展了我们对主机的覆盖范围。CUDA内核可以从多个线程或多个进程中调用。为了执行多个线程，我们使用了带有CUDA的OpenMP，并讨论了它的用处。我们使用MPI来模拟多进程操作，并且可以看到MPS如何提高整体应用程序性能。
- en: As we saw in this chapter, choosing the right kernel execution model is an important
    topic, as is thread programming. This can optimize application execution time.
    Now, we will expand our discussion to multi-GPU programming to solve big problems.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中看到的，选择正确的内核执行模型是一个重要的话题，线程编程也是如此。这可以优化应用程序的执行时间。现在，我们将扩展我们的讨论到多GPU编程来解决大问题。
