- en: CUDA Thread Programming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA线程编程
- en: CUDA has a hierarchical thread architecture so that we can control CUDA threads
    in groups. Understanding how they work in parallel on a GPU helps you to write
    parallel programming code and achieve better performance. In this chapter, we
    will cover CUDA thread operations and their relationship with GPU resources. As
    a practical experience, we will investigate the parallel reduction algorithm and
    see how we can optimize CUDA code by using optimization strategies.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA具有分层线程架构，因此我们可以控制CUDA线程的分组。了解它们在GPU上并行工作的方式有助于您编写并行编程代码并实现更好的性能。在本章中，我们将介绍CUDA线程操作及其与GPU资源的关系。作为实际经验，我们将研究并行减少算法，并看看如何通过使用优化策略来优化CUDA代码。
- en: 'In this chapter, you will learn how CUDA threads operate in a GPU: parallel
    and concurrent thread execution, warp execution, memory bandwidth issues, control
    overheads, SIMD operation, and so on.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习CUDA线程在GPU中的操作：并行和并发线程执行，warp执行，内存带宽问题，控制开销，SIMD操作等等。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Hierarchical CUDA thread operations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次化的CUDA线程操作
- en: Understanding CUDA occupancy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解CUDA占用率
- en: Data sharing across multiple CUDA threads
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨多个CUDA线程共享数据
- en: Identifying an application's performance limiter
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别应用程序的性能限制
- en: Minimizing the CUDA warp divergence effect
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化CUDA warp分歧效应
- en: Increasing memory utilization and grid-stride loops
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加内存利用率和网格跨距循环
- en: Cooperative Groups for flexible thread handling
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于灵活线程处理的协作组
- en: Warp synchronous programming
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: warp同步编程
- en: Low-/mixed-precision operations
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低/混合精度操作
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter recommends using an NVIDIA GPU card later than Pascal architecture.
    In other words, your GPU's compute capability should be equal to or greater than
    60\. If you are unsure of your GPU's architecture, please visit NVIDIA's GPU site
    at [https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus),
    and confirm your GPU's compute capability.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章建议使用比Pascal架构更晚的NVIDIA GPU卡。换句话说，您的GPU的计算能力应等于或大于60。如果您不确定您的GPU架构，请访问NVIDIA的GPU网站[https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)，并确认您的GPU的计算能力。
- en: Sample code was developed and tested with 10.1 when we wrote this book. In general,
    it is recommended to use the latest CUDA version if applicable.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们写这本书的时候，示例代码是使用10.1版本开发和测试的。一般来说，如果适用的话，建议使用最新的CUDA版本。
- en: In this chapter, we'll perform CUDA programming by profiling the code. If your
    GPU architecture is Turing, it is recommended to install Nsight Compute to profile
    the code. It is free, and you can download it from [https://developer.nvidia.com/nsight-compute](https://developer.nvidia.com/nsight-compute).
    When we wrote this book, it was a transition moment of the profiler. You can learn
    about its basic usage in the *Profiling Kernel with Nsight Compute* section in
    [Chapter 5](ea24897f-252a-4e76-81e3-b5d5ff645bb6.xhtml), *CUDA Application Profiling
    and Debugging*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过对代码进行性能分析来进行CUDA编程。如果您的GPU架构是图灵架构，建议安装Nsight Compute来对代码进行性能分析。它是免费的，您可以从[https://developer.nvidia.com/nsight-compute](https://developer.nvidia.com/nsight-compute)下载。在我们写这本书的时候，这是性能分析工具的过渡时刻。您可以在[第5章](ea24897f-252a-4e76-81e3-b5d5ff645bb6.xhtml)的*使用Nsight
    Compute对内核进行性能分析*部分了解其基本用法，*CUDA应用性能分析和调试*。
- en: CUDA threads, blocks, and the GPU
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA线程、块和GPU
- en: The basic working unit in CUDA programming is the CUDA thread. The basic CUDA
    thread execution model is **Single Instruction and Multiple Thread** (**SIMT**).
    In other words, the body of the kernel function is working descriptions of a single
    CUDA thread. But, CUDA architecture executes multiple CUDA threads having the
    same actions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA编程中的基本工作单元是CUDA线程。基本的CUDA线程执行模型是**单指令多线程**（**SIMT**）。换句话说，内核函数的主体是单个CUDA线程的工作描述。但是，CUDA架构执行具有相同操作的多个CUDA线程。
- en: 'Conceptually, multiple CUDA threads work in parallel in a group. CUDA thread
    blocks are collections of multiple CUDA threads. Multiple thread blocks operate
    concurrently with each other. We call a group of thread blocks a grid. The following
    diagram shows their relationships:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念上，多个CUDA线程以组的形式并行工作。CUDA线程块是多个CUDA线程的集合。多个线程块同时运行。我们称线程块的组为网格。以下图表显示了它们之间的关系：
- en: '![](img/60928263-1c45-4083-8d5a-0b549796024d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60928263-1c45-4083-8d5a-0b549796024d.png)'
- en: 'These hierarchical CUDA thread operations match the hierarchical CUDA architecture.
    When we launch a CUDA kernel, one or multiple CUDA thread blocks execute on each
    streaming multiprocessor in the GPU. Also, a streaming multiprocessor can run
    multiple thread blocks depending on resource availability. The number of threads
    in a thread block varies, and the number of blocks in a grid does too:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分层的CUDA线程操作与分层的CUDA架构相匹配。当我们启动CUDA内核时，每个流多处理器上会执行一个或多个CUDA线程块。此外，根据资源的可用性，一个流多处理器可以运行多个线程块。线程块中的线程数量和网格中的块数量也会有所不同。
- en: '![](img/54d3c5aa-a4c2-4418-83e2-1ae060c8df0f.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54d3c5aa-a4c2-4418-83e2-1ae060c8df0f.png)'
- en: The streaming multiprocessors executes thread blocks arbitrarily and concurrently,
    executing as many as the GPU resources can afford. Therefore, the number of thread
    blocks executable in parallel varies depending on how much of the GPU's resources
    the block requires and the amount of GPU resources available. We will cover this
    in the following section. The number of streaming multiprocessors varies depending
    on the GPU specification. For instance, it is 80 for a Tesla V100, and it is 48
    for an RTX 2080 (Ti).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 流多处理器以任意和并发的方式执行线程块，执行尽可能多的GPU资源。因此，可并行执行的线程块数量取决于块需要的GPU资源量以及GPU资源的可用量。我们将在接下来的部分中介绍这一点。流多处理器的数量取决于GPU规格。例如，Tesla
    V100为80，RTX 2080（Ti）为48。
- en: 'The CUDA streaming multiprocessor controls CUDA threads in groups of 32\. A
    group is called a **warp**. In this manner, one or multiple warps configures a
    CUDA thread block. The following figure shows the relationship:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA流多处理器以32个线程的组形式控制CUDA线程。一个组被称为**warp**。这样，一个或多个warp配置一个CUDA线程块。以下图显示了它们的关系：
- en: '![](img/4d49ee70-c8f0-4ce4-b663-d6f2b49cd288.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d49ee70-c8f0-4ce4-b663-d6f2b49cd288.png)'
- en: The small green boxes are CUDA threads and they are grouped by a warp. The warp
    is a basic control unit of GPU architecture. Therefore, its size impacts CUDA
    programming implicitly or explicitly. For instance, the optimal thread block size
    is determined among multiple warp sizes that can fully utilize the block's warp
    scheduling and operations. We call this as occupancy, which will be covered in
    detail in the next section. Also, CUDA threads in a warp work in parallel and
    have synchronous operations, inherently. We will talk about this in the *Warp-level
    primitives programming* section of this chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 小绿色框是CUDA线程，它们被warp分组。warp是GPU架构的基本控制单元。因此，它的大小对CUDA编程具有隐式或显式的影响。例如，最佳线程块大小是在可以充分利用块的warp调度和操作的多个warp大小中确定的。我们称之为占用率，这将在下一节中详细介绍。此外，warp中的CUDA线程并行工作，并具有同步操作。我们将在本章的*Warp级别基元编程*部分讨论这一点。
- en: Exploiting a CUDA block and warp
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用CUDA块和warp
- en: Now, we will look at CUDA thread scheduling and their implicit synchronization
    using CUDA's `printf`. The execution of parallel CUDA threads and the blocks operation
    is concurrent. On the other hand, printing out from the device is a sequential
    task. So, we can see their execution order easily, since the output will be arbitrary
    for the concurrent tasks and consistent for the parallel tasks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将研究CUDA线程调度及其使用CUDA的`printf`进行隐式同步。并行CUDA线程的执行和块操作是并发的。另一方面，从设备打印输出是一个顺序任务。因此，我们可以轻松地看到它们的执行顺序，因为对于并发任务来说输出是任意的，而对于并行任务来说是一致的。
- en: 'We will begin to write kernel code that prints a global thread index, thread
    block index, warp index, and lane index. For that purpose, the code can be written as
    follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始编写打印全局线程索引、线程块索引、warp索引和lane索引的内核代码。为此，代码可以编写如下：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code will help us to understand the concurrency of the warp and CUDA thread
    scheduling. Let's make our code get arguments from the shell to test various grid
    and thread block configurations easily.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将帮助我们理解warp和CUDA线程调度的并发性。让我们让我们的代码从shell获取参数，以便轻松测试各种网格和线程块配置。
- en: 'Then, we will write the host code that calls the kernel function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将编写调用内核函数的主机代码：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, let''s compile the code, execute it, and see the result:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们编译代码，执行它，并查看结果：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following result is an example of the output result. The actual output
    might be different:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果是输出结果的一个示例。实际输出可能会有所不同：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'From the result, you will see that CUDA threads are launched in warp size and
    the order is not determined. On the other hand, the lane outputs are in order.
    From the given result, we can confirm the following facts:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，您将看到CUDA线程以warp大小启动，并且顺序是不确定的。另一方面，lane输出是有序的。从给定的结果中，我们可以确认以下事实：
- en: '**Out-of-order block execution:** The second column shows indexes of thread
    blocks. The result shows that it does not promise in-order execution following
    the block index.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无序块执行：**第二列显示线程块的索引。结果表明，它不保证按照块索引的顺序执行。'
- en: '**Out-of-order warp index with a thread block:** The third column shows the
    index of a warp in a block. The warp''s order varies across blocks. So, we can
    infer that there is no guarantee of warp execution order.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无序warp索引与线程块：**第三列显示块中warp的索引。warp的顺序在块之间变化。因此，我们可以推断warp执行顺序没有保证。'
- en: '**Grouped threads executed in a warp:** The fourth column shows the lane in
    a warp. To reduce the number of outputs, the application limits it to printing
    only two indices. From the in-order output within each warp, we can make an analogy
    that the `printf` function''s output order is fixed so that there is no inversion.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在warp中执行的分组线程：**第四列显示warp中的lane。为了减少输出数量，应用程序限制只打印两个索引。从每个warp内的有序输出中，我们可以类比`printf`函数的输出顺序是固定的，因此没有倒置。'
- en: To summarize, CUDA threads are grouped into 32 threads, and their output and
    the warp's execution have no order. Therefore, programmers have to keep this in
    mind for CUDA kernel development.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，CUDA线程被分组为32个线程，它们的输出和warp的执行没有顺序。因此，程序员必须牢记这一点，以便进行CUDA内核开发。
- en: Understanding CUDA occupancy
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解CUDA占用率
- en: CUDA occupancy is the ratio of active CUDA warps to the maximum warps that each
    streaming multiprocessor can execute concurrently. In general, higher occupancy
    leads to more effective GPU utilization because more warps are available to hide
    the latency of stalled warps. However, it might also degrade performance due to
    the increased resource contention between the CUDA threads. Thus, it is crucial
    for developers to understand this trade-off.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA占用率是活动CUDA warps与每个流多处理器可以同时执行的最大warps的比率。一般来说，更高的占用率会导致更有效的GPU利用率，因为有更多的warp可用来隐藏停滞warp的延迟。然而，它也可能由于CUDA线程之间资源争用的增加而降低性能。因此，开发人员理解这种权衡是至关重要的。
- en: The purpose of finding optimal CUDA occupancy is to make the GPU application
    issue warps instructions efficiently with the GPU resources. The GPU schedules
    multiple warps using multiple warp schedulers on a streaming multiprocessor. When
    multiple warps are scheduled effectively, the GPU can hide latencies between the
    GPU instructions or memory latencies. Then, the CUDA cores can execute instructions
    continuously issued from the multiple warps, while the unscheduled warps have
    to wait until they can issue the next instruction.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最佳的CUDA占用率的目的是使GPU应用程序能够有效地使用GPU资源发出warp指令。GPU在流多处理器上使用多个warp调度器调度多个warp。当多个warp有效地调度时，GPU可以隐藏GPU指令或内存延迟之间的延迟。然后，CUDA核心可以执行连续从多个warp发出的指令，而未调度的warp必须等待，直到它们可以发出下一条指令。
- en: 'Developers can determine CUDA occupancy using two methods:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员可以使用两种方法确定CUDA占用率：
- en: '**Theoretical occupancy** determined by the CUDA Occupancy Calculator: This
    calculator is an Excel sheet provided with the CUDA Toolkit. We can determine
    each kernel''s occupancy theoretically from the kernel resource usages and the
    GPU''s streaming multiprocessor.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由CUDA占用率计算器确定的**理论占用率**：这个计算器是CUDA工具包提供的一个Excel表。我们可以从内核资源使用和GPU流多处理器理论上确定每个内核的占用率。
- en: '**Achieved occupancy** determined by the GPU: The achieved occupancy reflects
    the true number of concurrent executed warps on a streaming multiprocessor and the
    maximum available warps. This occupancy can be measured by the NVIDIA profiler
    with metric analysis.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由GPU确定的**实现占用率**：实现占用率反映了在流多处理器上并发执行的warp的真实数量和最大可用warp。这种占用率可以通过NVIDIA分析器进行度量分析来测量。
- en: Theoretical occupancy can be regarded as the maximum upper-bound occupancy because
    the occupancy number does not consider instructional dependencies or memory bandwidth
    limitations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 理论占用率可以被视为最大的上限占用率，因为占用率数字不考虑指令依赖性或内存带宽限制。
- en: Now, let's see how this occupancy and CUDA C/C++ are related.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这个占用率和CUDA C/C++之间的关系。
- en: Setting NVCC to report GPU resource usages
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置NVCC报告GPU资源使用
- en: 'To begin with, we will use **simple matrix multiplication** (**SGEMM**) kernel
    code, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用**简单矩阵乘法**（**SGEMM**）内核代码，如下所示：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And, we will call the kernel function using the following kernel code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用以下内核代码调用内核函数：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You may want to provide appropriate GPU memory and its size information. We
    will use 2048 for `N`, `M`, and `K`. The memory size is the square of that number.
    We will set `BLOCK_DIM` as `16`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望提供适当的GPU内存及其大小信息。我们将使用2048作为`N`，`M`和`K`。内存大小是该数字的平方。我们将把`BLOCK_DIM`设置为`16`。
- en: Now, let's see how to make the `nvcc` compiler report the GPU resource usage
    of the kernel functions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使`nvcc`编译器报告内核函数的GPU资源使用情况。
- en: The settings for Linux
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Linux设置
- en: 'In a Linux environment, we should provide two compiler options, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux环境中，我们应该提供两个编译器选项，如下所示：
- en: '`--resource-usage` (`--res-usage`): Setting a verbose option for GPU resource
    usage'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--resource-usage`（`--res-usage`）：为GPU资源使用设置详细选项'
- en: '`-gencode`: Specifying the target architecture to compile and generate opcodes
    as follows:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-gencode`：指定要编译和生成操作码的目标架构如下：'
- en: 'Turing: `compute_75,sm_75`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turing：`compute_75,sm_75`
- en: 'Volta: `compute_70,sm_70`'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Volta：`compute_70,sm_70`
- en: 'Pascal: `compute_60,sm_60`, `compute_61,sm_61`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pascal：`compute_60,sm_60`，`compute_61,sm_61`
- en: 'If you are not sure which architecture you are using, you can find out from
    the CUDA GPU website ([https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)). For
    example, the `nvcc` compile command can have the compile option as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不确定您正在使用哪种架构，您可以从CUDA GPU网站上找到（[https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)）。例如，`nvcc`编译命令可以有以下编译选项：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can also compile the code to target multiple GPU architectures as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以编译代码以针对多个GPU架构，如下所示：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If you want to enable your code to be compatible with the new GPU architecture
    (Turing), you need to provide an additional option as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想使您的代码与新的GPU架构（Turing）兼容，您需要提供以下附加选项：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you want to learn more about these option, you can find the related information
    in this document: [https://docs.nvidia.com/cuda/turing-compatibility-guide/index.html#building-turing-compatible-apps-using-cuda-10-0](https://docs.nvidia.com/cuda/turing-compatibility-guide/index.html#building-turing-compatible-apps-using-cuda-10-0).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于这些选项的信息，您可以在这个文档中找到相关信息：[https://docs.nvidia.com/cuda/turing-compatibility-guide/index.html#building-turing-compatible-apps-using-cuda-10-0](https://docs.nvidia.com/cuda/turing-compatibility-guide/index.html#building-turing-compatible-apps-using-cuda-10-0)。
- en: 'Now, let''s compile the source. We can find a resource usage report from NVCC''s
    output. The following result is generated using the preceding commands:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编译源代码。我们可以从NVCC的输出中找到一个资源使用报告。以下结果是使用前面的命令生成的：
- en: '![](img/48ba5f5b-a5b2-4fae-a022-e2eaeba65b62.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48ba5f5b-a5b2-4fae-a022-e2eaeba65b62.png)'
- en: NVCC reports CUDA kernels resource usage information for each compute capability.
    In the preceding output screenshot, we can see the number of registers per thread
    and constant memory usage.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: NVCC为每个计算能力报告CUDA内核资源使用信息。在前面的输出截图中，我们可以看到每个线程的寄存器数量和常量内存使用情况。
- en: Settings for Windows
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Windows设置
- en: 'When we are developing a Windows application, we can set these settings on
    the project''s properties dialog of Visual Studio. The following the screenshot
    shows that dialog:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开发Windows应用程序时，我们可以在Visual Studio项目属性对话框中设置这些设置。以下是该对话框的截图：
- en: '![](img/056e4da9-6936-49c0-957b-ef911ecb8bd9.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/056e4da9-6936-49c0-957b-ef911ecb8bd9.png)'
- en: 'To open this dialog, we should open debug_vs Property Pages, then traverse
    to the CUDA C/C++ | Device tab on the left-hand panel. Then, we should set the
    following options as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要打开此对话框，我们应该打开debug_vs属性页，然后在左侧面板上转到CUDA C/C++ | 设备选项卡。然后，我们应该设置以下选项如下：
- en: 'Verbose PTXAS Output: No | Yes'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Verbose PTXAS Output: No | Yes'
- en: 'Code Generation: Update the option to specify your target architecture as follows:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码生成：更新选项以指定您的目标架构如下：
- en: 'Turing: `compute_75,sm_75`'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图灵：`compute_75,sm_75`
- en: 'Volta: `compute_70,sm_70`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伏尔塔：`compute_70,sm_70`
- en: 'Pascal: `compute_60,sm_60;compute_61,sm_61`'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帕斯卡：`compute_60,sm_60;compute_61,sm_61`
- en: We can specify multiple target architectures using a semi-colon (`;`) for each
    target.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用分号(`;`)指定多个目标架构。
- en: 'Now, let''s build the source code and we will see NVCC''s report on the output
    panel of Visual Studio. Then, you will see output similar to the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建源代码，我们将在Visual Studio的输出面板上看到NVCC的报告。然后，你会看到类似以下的输出：
- en: '![](img/9f073fc9-3503-4296-a33e-60416c95573e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f073fc9-3503-4296-a33e-60416c95573e.png)'
- en: It is the same as the NVCC output in Linux.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这与Linux中NVCC的输出相同。
- en: Now, let's use the resource usage report to analyze a kernel's occupancy.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用资源使用报告来分析内核的占用情况。
- en: Analyzing the optimal occupancy using the Occupancy Calculator
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用占用率计算器分析最佳占用率
- en: 'In practice, we can use the CUDA Occupancy Calculator, which is provided with
    the CUDA Toolkit. Using this, we can obtain theoretical occupancy by providing
    some kernel information. The calculator is an Excel file, and you can find it
    in the following, based on the OS you use:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以使用CUDA占用率计算器，它是CUDA工具包提供的。使用这个，我们可以通过提供一些内核信息来获得理论上的占用率。计算器是一个Excel文件，你可以在以下位置找到它，根据你使用的操作系统：
- en: '**Windows:** `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\<cuda-version>\tools`'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Windows:** `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\<cuda-version>\tools`'
- en: '**Linux:** `/usr/local/cuda/tools`'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Linux:** `/usr/local/cuda/tools`'
- en: '**macOS:** `/Developer/NVIDIA/<cuda-version>/tools`'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**macOS:** `/Developer/NVIDIA/<cuda-version>/tools`'
- en: 'The following is a screenshot of the calculator:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是计算器的屏幕截图：
- en: '![](img/7845ff21-4805-4c93-beb1-9e788919b60e.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7845ff21-4805-4c93-beb1-9e788919b60e.png)'
- en: CUDA Occupancy Calculator
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA占用率计算器
- en: 'This calculator has two parts: kernel information inputs and occupancy information
    outputs. As input, it requires two kinds of information, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算器有两部分：内核信息输入和占用信息输出。作为输入，它需要两种信息，如下所示：
- en: The GPU's compute capability (green)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU的计算能力（绿色）
- en: 'Thread block resource information (yellow):'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '线程块资源信息（黄色）:'
- en: Threads per CUDA thread block
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个CUDA线程块的线程
- en: Registers per CUDA thread
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个CUDA线程的寄存器
- en: Shared memory per block
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个块的共享内存
- en: 'The calculator shows the GPU''s occupancy information here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 计算器在这里显示了GPU的占用信息：
- en: GPU occupancy data (blue)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU占用数据（蓝色）
- en: The GPU's physical limitation for GPU compute capability (gray)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU的GPU计算能力的物理限制（灰色）
- en: Allocated resources per block (yellow)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个块分配的资源（黄色）
- en: Maximum thread blocks per stream multiprocessor (yellow, orange, and red)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个流多处理器的最大线程块（黄色、橙色和红色）
- en: Occupancy limit graph following three key occupancy resources, which are threads,
    registers, and shared memory per block
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据三个关键的占用资源（线程、寄存器和每个块的共享内存），绘制占用限制图
- en: Red triangles on graphs, which show the current occupancy data
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图表上的红色三角形，显示当前的占用数据
- en: 'Now, let''s put the obtained information into the calculator. We can edit the
    green-and orange-colored areas in the Excel sheet:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们把获得的信息放入计算器中。我们可以编辑Excel表格中的绿色和橙色区域：
- en: '![](img/d8220238-38e8-4301-a0af-6e667621ff39.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8220238-38e8-4301-a0af-6e667621ff39.png)'
- en: Enter your acquired kernel resource information, and see how the sheet changes.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 输入你获得的内核资源信息，看看表格如何变化。
- en: 'Depending on compute capability and input data, the occupancy changes, as shown
    in the following screenshot:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 根据计算能力和输入数据，占用情况会发生变化，如下面的屏幕截图所示：
- en: '![](img/d2ddc32f-0596-4d41-8b1b-0f8c4fe7fde8.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2ddc32f-0596-4d41-8b1b-0f8c4fe7fde8.png)'
- en: Changes in occupancy depending on compute capability and input data
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 根据计算能力和输入数据的变化
- en: 'The blue-colored area shows the kernel function''s achieved occupancy. In this
    screenshot, it shows 100% occupancy achievements. The right-hand side of the sheet
    presents the occupancy utilization graphs for GPU resources: CUDA threads, shared
    memory, and registers.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色区域显示了内核函数实现的占用率。在这个屏幕截图中，它显示了100%的占用率。表格的右侧显示了GPU资源的占用率利用图：CUDA线程、共享内存和寄存器。
- en: In general, kernel code cannot have 100% theoretical occupancy due to many reasons.
    However, setting the pick occupancy is the start of utilizing GPU resources efficiently.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，由于许多原因，内核代码不能达到100%的理论占用率。然而，设置峰值占用率是有效利用GPU资源的开始。
- en: Occupancy tuning – bounding register usage
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 占用率调整 - 限制寄存器使用
- en: CUDA register usage can increase when the kernel's algorithm is complicated,
    or the handling datatype is double precision. In that case, the occupancy drops
    due to the limited active warp size. In that situation, we can increase the theoretical
    occupancy by limiting the register usage and see whether the performance is enhanced.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当内核算法复杂或处理数据类型为双精度时，CUDA寄存器使用可能会增加。在这种情况下，由于活动warp大小有限，占用率会下降。在这种情况下，我们可以通过限制寄存器使用来增加理论上的占用率，并查看性能是否提高。
- en: 'One way of resource tuning GPU resource usage is to use the `__launch_bound__`
    qualifier with the kernel function. This informs NVCC to guarantee the minimum
    thread blocks per stream multiprocessed with the maximum block size. Then, NVCC
    finds the optimal register size to achieve the given condition. You can use this
    if you have an idea of the size that makes your algorithm run efficiently at compile
    time. The identifier can be used as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 调整GPU资源使用的一种方法是在内核函数中使用`__launch_bound__`限定符。这告诉NVCC保证每个流多处理器的最大块大小的最小线程块。然后，NVCC找到实现给定条件的最佳寄存器大小。如果你在编译时知道使你的算法有效运行的大小，你可以使用这个。标识符可以如下使用：
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then, the compiler checks the upper-bound resources and reduces the limiting
    resource usage per block. If its resource usage does not exceed the upper limit,
    the compiler adjusts the register usage if CUDA can schedule an extra thread block
    per multiprocessor, if the second parameter is not given. Alternatively, the compiler
    increases the register usage to hide single-thread instruction latency.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，编译器检查上限资源并减少每个块的限制资源使用。如果其资源使用没有超过上限，编译器会调整寄存器使用，如果CUDA可以调度额外的多处理器线程块，如果没有给出第二个参数。或者，编译器会增加寄存器使用以隐藏单线程指令延迟。
- en: 'Also, we can simply limit the number of occupied register usages at the application
    level. The `--maxrregcount` flag to `NVCC` will specify the number, and the compiler
    will reorder the register usages. The following compile command shows how to use
    that flag in the Linux Terminal:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以简单地在应用程序级别限制占用寄存器的数量。`--maxrregcount`标志到`NVCC`将指定数量，编译器将重新排列寄存器使用。以下编译命令显示了如何在Linux终端中使用该标志：
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: But, keep in mind that limiting register usage in this way can introduce thread
    performance drawn by register throttling. Even the compiler can split the registers
    into local memory if it cannot set them under the limit, and the local variables
    are placed in the global memory.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，请记住，以这种方式限制寄存器使用可能会引入由寄存器限制引起的线程性能下降。即使编译器无法将其设置在限制之下，也可以将寄存器分割为本地内存，并且本地变量放置在全局内存中。
- en: Getting the achieved occupancy from the profiler
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从分析器获取实现的占用
- en: 'Now, we can obtain the achieved occupancy from the profiled metric data using
    the Visual Profiler. Click the target kernel timeline bar. Then, we can see the
    theoretical and achieved occupancy in the Properties panel. We can also obtain
    more details from the Kernel Latency menu. The following screenshot shows the
    achieved performance of the example code we used:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用Visual Profiler从分析的度量数据中获取实现的占用。单击目标内核时间轴条。然后，我们可以在属性面板中看到理论和实现的占用。我们还可以从内核延迟菜单中获取更多详细信息。以下屏幕截图显示了我们使用的示例代码的实现性能：
- en: '![](img/aea2d647-f88b-4f6f-9a00-922010fb039f.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aea2d647-f88b-4f6f-9a00-922010fb039f.png)'
- en: Performance showing achieved and theoretical occupancy
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 显示实现和理论占用的性能
- en: With this occupancy tuning, we can design the CUDA block size to fully utilize
    warp scheduling in the streaming multiprocessor. However, this does not resolve
    the 54.75% memory throttling issue, which we found in the previous section. This
    implies that multiprocessors can stall and cannot conceal memory access latency
    due to hampered memory requests. We will discuss how to optimize this in this
    chapter, and in [Chapter 7](71d77c43-0064-491e-9b43-307a05bd6555.xhtml), *Parallel
    Programming Patterns in CUDA*, we'll discuss matrix-matrix multiplication optimization.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种占用调整，我们可以设计CUDA块大小，充分利用流多处理器中的warp调度。然而，这并没有解决我们在上一节中发现的54.75%的内存限制问题。这意味着多处理器可能会停顿，无法掩盖由于受阻内存请求而产生的内存访问延迟。我们将在本章讨论如何优化这一点，并且在[第7章](71d77c43-0064-491e-9b43-307a05bd6555.xhtml)《CUDA中的并行编程模式》中，我们将讨论矩阵乘法优化。
- en: Understanding parallel reduction
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解并行归约
- en: Reduction is a simple but useful algorithm to obtain a common parameter across
    many parameters. This task can be done in sequence or in parallel. When it comes
    to parallel processing to a parallel architecture, parallel reduction is the fastest
    way of getting a histogram, mean, or any other statistical values.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 归约是一种简单但有用的算法，可以获得许多参数的公共参数。这个任务可以按顺序或并行完成。当涉及到并行处理到并行架构时，并行归约是获得直方图、均值或任何其他统计值的最快方式。
- en: 'The following diagram shows the difference between sequential reduction and
    parallel reduction:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了顺序归约和并行归约之间的差异：
- en: '![](img/cff3f592-ee81-4d90-b567-de34f12202ea.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cff3f592-ee81-4d90-b567-de34f12202ea.png)'
- en: By having the reduction tasks in parallel, the parallel reduction algorithm
    can reduce the total steps at a log scale. Now, let's begin to implement this
    parallel reduction algorithm on the GPU. Firstly, we will implement this with
    a simple design using global memory. Then, we will implement another reduction
    version using the shared memory. By comparing the two implementations, we will
    discuss what brings a performance difference.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过并行进行归约任务，可以将并行归约算法的总步骤减少到对数级别。现在，让我们开始在GPU上实现这个并行归约算法。首先，我们将使用全局内存实现一个简单的设计。然后，我们将使用共享内存实现另一个归约版本。通过比较这两种实现，我们将讨论是什么带来了性能差异。
- en: Naive parallel reduction using global memory
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用全局内存的天真并行归约
- en: 'The first basic approach for reduction is to use parallel CUDA threads and
    share the reduction output using global memory. For every iteration, the CUDA
    kernel obtains cumulated values from global memory by reducing its size by two. The
    reduction works as shown in the following diagram, which displays naive parallel
    reduction with global memory data sharing:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 归约的第一种基本方法是使用并行的CUDA线程，并使用全局内存共享归约输出。对于每次迭代，CUDA内核通过将其大小减少两倍来从全局内存获取累积值。归约的工作如下图所示，显示了使用全局内存数据共享的天真并行归约：
- en: '![](img/873af97a-c719-4f57-94b1-2732193a2873.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/873af97a-c719-4f57-94b1-2732193a2873.png)'
- en: This approach is slow in CUDA because it wastes the global memory's bandwidth
    and does not utilize any faster on-chip memory. For better performance, it is
    recommended to use shared memory to save global memory bandwidth and reduce memory-fetch
    latency. We will discuss how this approach wastes bandwidth later.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在CUDA中很慢，因为它浪费了全局内存的带宽，并且没有利用任何更快的片上内存。为了获得更好的性能，建议使用共享内存来节省全局内存带宽并减少内存获取延迟。我们将讨论这种方法如何浪费带宽。
- en: 'Now, let''s implement this reduction. Firstly, we will write the reduction
    kernel function, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现这个归约。首先，我们将编写归约内核函数，如下所示：
- en: '[PRE11]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will call the kernel function while reducing the stride size by half iteratively,
    until the `stride` size is one, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在迭代过程中减半减小步长大小，直到`stride`大小为1时调用内核函数。
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this implementation, the kernel code fetches the device memory with stridden
    addressing and outputs one reduction result. The host code triggers reduction
    kernels for each step, and the parameter size reduces by half. We cannot have
    an internal kernel loop since CUDA does not guarantee synchronized operations
    across thread blocks and streaming multiprocessors.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，内核代码使用跨距寻址获取设备内存并输出一个减少结果。主机代码触发每个步骤的减少内核，并且参数大小减半。我们不能有内部内核循环，因为CUDA不能保证线程块和流多处理器之间的同步操作。
- en: Reducing kernels using shared memory
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用共享内存减少内核
- en: 'In this reduction, each CUDA thread block reduces input values, and the CUDA
    threads share data using shared memory. For a proper data update, they use the
    block-level intrinsic synchronization function, `__syncthreads()`. Then, the next
    iteration operates on the previous reduction result. Its design is shown in the
    following diagram, which displays parallel reduction using shared memory:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种减少中，每个CUDA线程块减少输入值，并且CUDA线程使用共享内存共享数据。为了进行适当的数据更新，它们使用块级内在同步函数`__syncthreads()`。然后，下一个迭代操作上一个减少结果。其设计如下图所示，显示了使用共享内存的并行减少：
- en: '![](img/606f158b-fc9c-4723-8a8c-a9c8b42bdacf.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/606f158b-fc9c-4723-8a8c-a9c8b42bdacf.png)'
- en: The yellow-dotted boxes represent a CUDA thread block's operation coverage.
    In this design, each CUDA thread block outputs one reduction result.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 黄点框表示CUDA线程块的操作范围。在这个设计中，每个CUDA线程块输出一个减少结果。
- en: Block-level reduction lets each CUDA thread block conduct reduction and outputs
    a single reduction output. Since it does not require us to save the intermediate
    result in the global memory, the CUDA kernel can store the transitional value
    in the shared memory. This design helps to save global memory bandwidth and reduce
    memory latency.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 块级减少允许每个CUDA线程块进行减少，并输出单个减少输出。由于它不需要我们将中间结果保存在全局内存中，CUDA内核可以将过渡值存储在共享内存中。这种设计有助于节省全局内存带宽并减少内存延迟。
- en: 'As we did for global reduction, we will implement the operation. Firstly, we
    will write the kernel function, as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 与全局减少一样，我们将实现这个操作。首先，我们将编写内核函数，如下所示：
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we will call the kernel function, as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将调用内核函数，如下所示：
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this code, we provide `n_threads * sizeof (float)` bytes, because each CUDA
    thread will share a single variable for each byte.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们提供了`n_threads * sizeof (float)`字节，因为每个CUDA线程将共享每个字节的单个变量。
- en: Writing performance measurement code
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写性能测量代码
- en: 'To measure each version''s performance, we will use the CUDA sample `timer`
    helper function:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量每个版本的性能，我们将使用CUDA示例`timer`辅助函数：
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This function set helps to measure execution time at the microsecond level.
    Also, it is recommended to call the kernel function ahead of performance measurement
    to eliminate the device initialization overhead. For a more detailed implementation,
    visit the implemented code in the `global_reduction.cu` and `reduction.cu` files.
    These code sets are used across this chapter to evaluate the optimization effect
    along with the profiler.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数集有助于在微秒级别测量执行时间。此外，建议在性能测量之前调用内核函数，以消除设备初始化开销。有关更详细的实现，请访问`global_reduction.cu`和`reduction.cu`文件中的实现代码。这些代码集在本章中用于评估优化效果以及分析器。
- en: Performance comparison for the two reductions – global and shared memory
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两种减少-全局和共享内存的性能比较
- en: 'Now, we can compare the two parallel reduction operations'' execution time.
    Performance can vary depending on GPUs and the implementation environments.Run
    the following commands for global reduction and reduction using shared memory
    respectively:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以比较两个并行减少操作的执行时间。性能可能会因GPU和实现环境而异。分别运行以下命令进行全局减少和使用共享内存进行减少：
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Using my Tesla V100 PCIe card, the estimated performance of both reductions
    is as follows. The number of elements was *2^(24)* items:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我的Tesla V100 PCIe卡，两种减少的估计性能如下。元素数量为*2^(24)*个：
- en: '| **Operation** | **Estimated time (ms)** | **Speed-up** |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| **操作** | **估计时间（毫秒）** | **加速** |'
- en: '| Original approach (reduction with global memory) | 4.609 | 1.0x |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 原始方法（使用全局内存进行减少） | 4.609 | 1.0x |'
- en: '| Reduction using shared memory | 0.624 | 7.4x |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 使用共享内存的减少 | 0.624 | 7.4x |'
- en: From this result, we can see how sharing data using shared memory in reduction
    returns the output quickly. The first implemented version is in `global_reduction.cu`,
    and the second version is in `shared_reduction.cu`, so you can compare the implementations
    for yourself.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个结果中，我们可以看到在减少中使用共享内存共享数据如何快速返回输出。第一个实现版本在`global_reduction.cu`中，第二个版本在`shared_reduction.cu`中，所以您可以自行比较实现。
- en: By dividing the reduction along with the shared memory, we could enhance the
    performance significantly. However, we cannot determine that it is the maximum
    performance we could get and do not know what bottleneck our application has.
    To analyze this, we will cover the performance limiter in the next section.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将减少与共享内存结合，我们可以显著提高性能。然而，我们无法确定这是否是我们可以获得的最大性能，并且不知道我们的应用程序有什么瓶颈。为了分析这一点，我们将在下一节中涵盖性能限制器。
- en: Identifying the application's performance limiter
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别应用程序的性能限制器
- en: Previously, we saw how saving global memory benefits the CUDA kernel's performance.
    In general, using an on-chip cache is better than using off-chip memory. But,
    we cannot determine whether much optimization room remains with this simple analogy.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们看到了如何通过保存全局内存来使CUDA内核的性能受益。一般来说，使用片上缓存比使用片外内存更好。但是，我们无法确定这种简单类比是否还有很多优化空间。
- en: 'The performance limiter shows the bounding factor, which limits the performance
    of an application most significantly. Based on its profiling information, it analyzes
    performance-limiting factors among computing and memory bandwidth. Based on these
    resources'' utilization, an application can be categorized into four types: **Compute
    Bound**, **Bandwidth Bound**, **Latency Bound**, and **Compute and Latency Bound**.
    The following graph shows these categories related to compute and memory utilization:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 性能限制因素显示了限制应用程序性能的因素，它最显著地限制了应用程序的性能。根据其分析信息，它分析了计算和内存带宽之间的性能限制因素。根据这些资源的利用率，应用程序可以被分类为四种类型：**计算受限**，**带宽受限**，**延迟受限**和**计算和延迟受限**。以下图表显示了这些类别与计算和内存利用率的关系：
- en: '![](img/2689638e-eff4-4f99-bb64-c3664f8d08c2.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2689638e-eff4-4f99-bb64-c3664f8d08c2.png)'
- en: After we identify the limiter, we can use the next optimization strategy. If
    either resource's utilization is high, we can focus on the optimization of the
    resource. If both are under-utilized, we can apply latency optimization from I/O
    aspects of the system. If both are high, we can investigate whether there is a
    memory operation stalling issue and computing-related issue.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了限制因素之后，我们可以使用下一个优化策略。如果任一资源的利用率很高，我们可以专注于优化该资源。如果两者都未充分利用，我们可以从系统的I/O方面应用延迟优化。如果两者都很高，我们可以调查是否存在内存操作停顿问题和与计算相关的问题。
- en: Now let's see how we can obtain that utilization information.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何获得利用率信息。
- en: Finding the performance limiter and optimization
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 找到性能限制因素并进行优化
- en: 'Now, let''s apply this analysis to both reduction implementations. We will
    compare them and discuss how shared memory contributes to the performance limiter
    analysis with improved performance. First, let''s profile the global memory-based
    reduction application with the metric analysis using the following command:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将此分析应用于两个减少实现。我们将对它们进行比较，并讨论共享内存如何有助于性能限制因素分析以改善性能。首先，让我们使用以下命令对基于全局内存的减少应用程序进行度量分析：
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we will obtain the following chart from NVIDIA profiler, which shows
    the first global memory-based reduction''s performance limiter:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将从NVIDIA分析器获得以下图表，显示了基于全局内存的第一个减少性能的限制因素：
- en: '![](img/2238b4cf-3f68-4583-a824-bf5c47467f6b.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2238b4cf-3f68-4583-a824-bf5c47467f6b.png)'
- en: 'On this chart, we need to review performance execution ratio to see if it is
    balanced by checking the kernel latency analysis. Because, as you can see in the
    preceding chart, the utilization gap between **Compute** and **Memory** is large
    and this could mean there will be a lot of latency in compute due to memory bottleneck.
    The following graph shows the result of the sampling-based analysis, and we can
    determine that CUDA cores are starved due to the memory dependency:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图表上，我们需要查看性能执行比来查看是否通过检查内核延迟分析来平衡。因为，如前图表所示，**计算**和**内存**之间的利用率差距很大，这可能意味着由于内存瓶颈，计算中会有很多延迟。以下图表显示了基于采样的分析结果，我们可以确定CUDA核心由于内存依赖而饥饿：
- en: '![](img/cca26c03-d198-426d-ad68-fee55109fcec.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cca26c03-d198-426d-ad68-fee55109fcec.png)'
- en: 'As you can see, the kernel execution is delayed due to memory waiting. Now,
    let''s profile the reduction based on shared memory. We can do this with the following
    command:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，由于内存等待，内核执行被延迟。现在，让我们基于共享内存对减少进行分析。我们可以使用以下命令来做到这一点：
- en: '[PRE18]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we will obtain the following chart, which shows the second shared memory-based
    reduction''s performance limiter:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将获得以下图表，显示了基于共享内存的第二个减少性能的限制因素：
- en: '![](img/ed221ef5-237f-4cb7-aa13-38e164a8d15f.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed221ef5-237f-4cb7-aa13-38e164a8d15f.png)'
- en: We can determine that it is compute-bounded and memory does not starve the CUDA
    cores.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以确定它是计算受限的，内存不会使CUDA核心饥饿。
- en: 'Now let''s review our kernel operation to optimize computing operations. The
    following code shows the parallel reduction part in the kernel function:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回顾我们的核心操作，以优化计算操作。以下代码显示了内核函数中的并行减少部分：
- en: '[PRE19]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As an arithmetic operation, modular is heavy operation. Since the `stride`
    variable is an exponential number of `2`, it can be replaced with a bitwise operation,
    as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 作为算术操作，模运算是一种重型操作。由于`stride`变量是`2`的指数倍数，因此可以用位操作替换，如下所示：
- en: '[PRE20]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Run the following command to see the optimized output:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令以查看优化后的输出：
- en: '[PRE21]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, the new estimated time is **0.399 ms**, and we could achieve a more optimized
    performance, as shown in the following table:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，新的估计时间为**0.399毫秒**，我们可以实现更优化的性能，如下表所示：
- en: '| **Operation** | **Estimated time (ms)** | **Speed-up** |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| **操作** | **估计时间（毫秒）** | **加速比** |'
- en: '| Original approach (reduction with global memory) | 4.609 | 1.0x |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 原始方法（使用全局内存进行减少） | 4.609 | 1.0倍 |'
- en: '| Reduction using shared memory | 0.624 | 7.4x |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 使用共享内存进行减少 | 0.624 | 7.4倍 |'
- en: '| Changing conditional operation from `%` to `&` | 0.399 | 11.55x |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 将条件操作从`%`更改为`&` | 0.399 | 11.55倍 |'
- en: 'The following graph shows the updated performance limiter:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了更新后的性能限制因素：
- en: '![](img/15e9f92a-af6d-4b94-ab56-1cd041292870.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15e9f92a-af6d-4b94-ab56-1cd041292870.png)'
- en: We can identify that its operation is **compute and latency bounded**. So, we
    can determine that we could increase memory utilization by optimizing computing
    efficiency.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以确定其操作是**计算和延迟受限**。因此，我们可以确定我们可以通过优化计算效率来增加内存利用率。
- en: Minimizing the CUDA warp divergence effect
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小化CUDA warp分歧效应
- en: In a **single instruction, multiple thread** (**SIMT**) execution model, threads
    are grouped into sets of 32 threads and each group is called a **warp**. If a
    warp encounters a conditional statement or branch, its threads can be diverged
    and serialized to execute each condition. This is called **branch divergence**,
    which impacts performance significantly.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在**单指令，多线程**（**SIMT**）执行模型中，线程被分组成32个线程的集合，每个集合称为**warp**。如果一个warp遇到条件语句或分支，其线程可以分歧并串行执行每个条件。这称为**分支分歧**，它会显著影响性能。
- en: CUDA warp divergence refers to such CUDA threads' divergent operation in a warp.
    If the conditional branch has an `if`-`else` structure and a warp has this warp
    divergence, all CUDA threads have an active and inactive operation part for the
    branched code block.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA warp分歧是指在warp中CUDA线程的分歧操作。如果条件分支具有`if`-`else`结构，并且warp具有此warp分歧，所有CUDA线程对于分支代码块都有活动和非活动操作部分。
- en: 'The following figure shows a warp divergence effect in a CUDA warp. CUDA threads
    that are not in the idle condition and reduce the efficient use of GPU threads:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了CUDA warp中的warp分歧效应。不处于空闲状态的CUDA线程会降低GPU线程的有效使用：
- en: '![](img/e0513741-a1a7-41e0-9598-534517e18798.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0513741-a1a7-41e0-9598-534517e18798.png)'
- en: 'As more of the branched part becomes significant, the GPU scheduling throughput
    becomes inefficient. Therefore, we need to avoid or minimize this warp divergence
    effect. There are several options you can choose:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 随着分支部分的增加，GPU调度吞吐量变得低效。因此，我们需要避免或最小化这种warp分歧效应。您可以选择几种选项：
- en: Divergence avoidance by handling different warps to execute the branched part
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过处理不同的warp来避免分歧效应
- en: Coalescing the branched part to reduce branches in a warp
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过合并分支部分来减少warp中的分支
- en: Shortening the branched part; only critical parts to be branched
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩短分支部分；只有关键部分进行分支
- en: Rearranging the data (that is, transposing, coalescing, and so on)
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新排列数据（即转置，合并等）
- en: Partitioning the group using `tiled_partition` in Cooperative Group
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用协作组中的`tiled_partition`来对组进行分区
- en: Determining divergence as a performance bottleneck
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定分歧作为性能瓶颈
- en: 'From the previous reduction optimization, you might find a warning about an
    inefficient kernel due to divergent branches in the computing analysis, as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 从先前的减少优化中，您可能会发现由于计算分析中的分歧分支而导致内核效率低下的警告，如下所示：
- en: '![](img/1e6569d1-6b97-4720-a49a-2b3883487a3a.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e6569d1-6b97-4720-a49a-2b3883487a3a.png)'
- en: '73.4 % divergence means that we have an inefficient operation path. We can
    determine that the reduction addressing is the issue, highlighted next:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 73.4％的分歧意味着我们有一个低效的操作路径。我们可以确定减少寻址是问题所在，如下所示：
- en: '[PRE22]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'When it comes to reduction addressing, we can select one of these CUDA thread
    indexing strategies:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在减少寻址方面，我们可以选择以下CUDA线程索引策略之一：
- en: Interleaved addressing
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交错寻址
- en: Sequential addressing
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序寻址
- en: Let's review what they are and compare their performance by implementing these
    strategies. Since we will just modify the reduction kernel, we can reuse the host
    code for the next two implementations.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下它们，并通过实施这些策略来比较它们的性能。由于我们只会修改减少内核，因此我们可以重用主机代码进行下两个实现。
- en: Interleaved addressing
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交错寻址
- en: 'In this strategy, the consecutive CUDA threads to fetch input data using the
    interleaved addressing strategy. Compared to the previous version, CUDA threads
    access input data by increasing the stride value. The following diagram shows
    how CUDA threads are interleaved with reduction items:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种策略中，连续的CUDA线程使用交错寻址策略获取输入数据。与之前的版本相比，CUDA线程通过增加步幅值来访问输入数据。以下图表显示了CUDA线程如何与减少项交错：
- en: '![](img/f4d697d2-a75f-4b7c-8bd0-712132017416.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4d697d2-a75f-4b7c-8bd0-712132017416.png)'
- en: 'This interleaving addressing can be implemented as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 可以实现以下交错寻址：
- en: '[PRE23]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Run the following command to compile the preceding code:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令来编译上述代码：
- en: '[PRE24]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The measured kernel execution time is **0.446 ms** on the Tesla V100\. It is
    slower than the previous version because each thread block is not fully utilized
    in this approach. We would be able to get more detail by profiling its metrics.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在Tesla V100上，测得的内核执行时间为0.446毫秒。这比之前的版本慢，因为在这种方法中每个线程块都没有完全利用。通过对其指标进行分析，我们可以得到更多细节。
- en: Now we will try another addressing approach, which is designed so that each
    thread block computes more data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将尝试另一种寻址方法，该方法旨在使每个线程块计算更多数据。
- en: Sequential addressing
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序寻址
- en: 'Compared to previous versions, this has highly coalesced indexing and addressing.
    This design is more efficient because there is no divergence when the stride size
    is greater than the warp size. The following diagram shows a coalesced thread
    operation:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的版本相比，这具有高度合并的索引和寻址。这种设计更有效，因为当步幅大小大于warp大小时就没有分歧。以下图表显示了合并的线程操作：
- en: '![](img/316964ae-b4ed-460f-9285-5043c3782407.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/316964ae-b4ed-460f-9285-5043c3782407.png)'
- en: 'Now, let''s write a kernel function to use sequential addressing on the reduction
    items:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一个内核函数，以在减少项上使用顺序寻址。
- en: '[PRE25]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Run the following command to compile the preceding code:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令来编译上述代码：
- en: '[PRE26]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Its measured execution time is **0.378 ms** on a Tesla V100 GPU, which is slightly
    faster than the previous strategy (0.399 ms).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在Tesla V100 GPU上，测得的执行时间为0.378毫秒，略快于之前的策略（0.399毫秒）。
- en: 'Thanks to the warp divergence avoiding, we could obtain a **12.2x** performance
    gain on the original compute. The following graph shows the updated performance
    limiter analysis:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 由于避免warp分歧，我们可以在原始计算上获得12.2倍的性能提升。以下图表显示了更新后的性能限制器分析：
- en: '![](img/1598561f-8e61-41ea-a891-d6744ba53a19.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1598561f-8e61-41ea-a891-d6744ba53a19.png)'
- en: Compared to the previous performance limiter, we can see the reduced control-flow
    operation and increased memory utilization.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的性能限制器相比，我们可以看到减少了控制流操作并增加了内存利用率。
- en: Performance modeling and balancing the limiter
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能建模和平衡限制器
- en: Following the performance limiter analysis, our current reduction performance
    is bounded by the compute latency due to the memory bandwidth, although the limiter
    analysis shows the full utilization of each resource. Let's cover why this is
    an issue and how we can resolve this by following the Roofline performance model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 根据性能限制器分析，我们当前的减少性能受到计算延迟的限制，这是由于内存带宽，尽管限制器分析显示每个资源的充分利用。让我们讨论为什么这是一个问题，以及如何通过遵循Roofline性能模型来解决这个问题。
- en: The Roofline model
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Roofline模型
- en: The **Roofline model** is an intuitive visual performance analysis model used
    to provide estimated performance for a given computing kernel on a parallel processing
    unit. Based on this model, developers in parallel programming can identify what
    the algorithm should be bounded to and determine which should be optimized.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Roofline模型是一种直观的视觉性能分析模型，用于为并行处理单元上的给定计算内核提供估计性能。根据这个模型，并行编程中的开发人员可以确定算法应该受到什么限制，并确定哪些应该进行优化。
- en: 'The following graph shows an example of the Roofline model:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Roofline模型的一个示例：
- en: '![](img/8d377d5f-92e2-43bd-84f5-bf446cea1edb.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d377d5f-92e2-43bd-84f5-bf446cea1edb.png)'
- en: The slanted part means memory-bound, and the flat part means arithmetic-bound.
    Each parallel algorithm and implementation has its own Roofline model since they
    have different computing power and memory bandwidth. With this model, algorithms
    can be placed depending on their operational intensity (flops/bytes). If an implementation
    does not meet the expected performance of this model, we can determine that this
    version is bounded by latency.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 倾斜部分表示内存受限，平坦部分表示算术受限。每个并行算法和实现都有自己的Roofline模型，因为它们具有不同的计算能力和内存带宽。有了这个模型，算法可以根据它们的操作密度（flops/bytes）进行放置。如果一个实现不符合这个模型的预期性能，我们可以确定这个版本受到延迟的限制。
- en: Considering our parallel reduction's complexity, it must be memory-bound. In
    other words, it has low operational intensity, so our strategy should maximize
    memory bandwidth as much as possible.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们并行减少的复杂性，它必须是内存受限的。换句话说，它的操作密度低，因此我们的策略应尽可能最大化内存带宽。
- en: 'Therefore, we need to confirm how our reduction kernel function consumes memory
    bandwidth using the performance analysis in the profiler. The following diagram
    shows the global memory''s bandwidth usages:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要确认我们的减少内核函数如何使用性能分析器中的内存带宽。以下图表显示了全局内存的带宽使用情况：
- en: '![](img/9b17a6cf-98a6-49ac-9f48-4392a4459f5a.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b17a6cf-98a6-49ac-9f48-4392a4459f5a.png)'
- en: As we can see in this diagram, we didn't achieve full utilization of the memory
    bandwidth. The total bandwidth is 343.376 GB/s on a Tesla V100 GPU, which utilizes
    about one-third of the bandwidth since this GPU has 900 GB/s bandwidth HBM2 memory.
    Therefore, the next step is to increase bandwidth usage by letting each CUDA thread
    digest more data. This will resolve the latency-bound situation and make our application
    be bounded to memory bandwidth.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，我们没有充分利用内存带宽。Tesla V100 GPU的总带宽为343.376 GB/s，利用了大约三分之一的带宽，因为这款GPU具有900
    GB/s带宽的HBM2内存。因此，下一步是通过让每个CUDA线程处理更多数据来增加带宽使用率。这将解决延迟限制的情况，并使我们的应用程序受限于内存带宽。
- en: Now, let's cover how to increase memory bandwidth.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论如何增加内存带宽。
- en: Maximizing memory bandwidth with grid-strided loops
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过网格跨步循环最大化内存带宽
- en: 'We can achieve this with a simple idea. The reduction problem allows us to
    accumulate input data with CUDA threads and start a reduction operation. Previously,
    our reduction implementation started with the input data size. But now, we will
    iterate to the input data with a group of CUDA threads, and that size will be
    the grid size of our kernel function. This style of iteration is called grid-strided
    loops. This technique has many benefits to control multiple CUDA cores, and they
    are introduced in this document: [https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops](https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个简单的想法实现这一点。减少问题允许我们使用CUDA线程累积输入数据并开始减少操作。以前，我们的减少实现是从输入数据大小开始的。但现在，我们将迭代到一组CUDA线程的输入数据，并且该大小将是我们内核函数的网格大小。这种迭代风格称为网格跨步循环。这种技术有许多好处，可以控制多个CUDA核心，并在本文中介绍：[https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops](https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops)。
- en: 'The following code shows the updated reduction kernel function:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了更新后的减少内核函数：
- en: '[PRE27]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You will find that this kernel function focuses on accumulating input data first,
    and then it reduces the loaded data.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 您会发现这个内核函数首先专注于累积输入数据，然后减少加载的数据。
- en: 'Now, we need to determine the grid size. To make our GPU code run on various
    GPU targets, we have to determine their size at runtime. Also, we need to utilize
    all the multiprocessors in the GPU. CUDA C provides related functions. We can
    obtain the occupancy-aware maximum active blocks per multiprocessor using the `cudaOccpancyMaxActiveBlocksPerMultiprocessor()`
    function. Also, we can obtain the multiprocessor numbers on the target GPU using
    the `cudaDeviceGetAttribte()` function. The following code shows how we can use
    those functions and call the kernel function:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要确定网格大小。为了使我们的GPU代码在各种GPU目标上运行，我们必须在运行时确定它们的大小。此外，我们需要利用GPU中的所有多处理器。CUDA
    C提供了相关函数。我们可以使用`cudaOccpancyMaxActiveBlocksPerMultiprocessor()`函数获得占用率感知的每个多处理器的最大活动块数。此外，我们可以使用`cudaDeviceGetAttribte()`函数获得目标GPU上的多处理器数量。以下代码显示了如何使用这些函数并调用内核函数：
- en: '[PRE28]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'There is one additional modification in this function. To save the occupancy
    calculation overhead, it launches the `reduction_kernel()` function once more
    with a single block. Run the following command:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数还有一个额外的修改。为了节省占用率计算开销，它再次启动`reduction_kernel()`函数，这次只使用一个块。运行以下命令：
- en: '[PRE29]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The updated reduction performance is **0.278** **ms** on a Tesla V100, which
    is about 100 ms faster than the previous method.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的减少性能为**0.278** **ms**，在Tesla V100上比以前的方法快了大约100 ms。
- en: 'Now, let''s review how we could utilize the memory bandwidth. The following
    diagram shows the memory utilization analysis in the Visual Profiler, and shows
    how we increased the memory bandwidth twice:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下我们如何利用内存带宽。以下图表显示了在Visual Profiler中的内存利用分析，并显示了我们如何将内存带宽增加了两倍：
- en: '![](img/44e2f316-82e4-494e-a957-3e859fdcba39.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44e2f316-82e4-494e-a957-3e859fdcba39.png)'
- en: Although it shows an increased bandwidth, we still have room to increase it
    further. Let's cover how we can achieve more bandwidth.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它显示出了增加的带宽，但我们仍然有进一步增加的空间。让我们来看看如何实现更多的带宽。
- en: Balancing the I/O throughput
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡I/O吞吐量
- en: 'From the result we got from the profiler, the local variable input has a substantial
    amount of load/store requests. Such massive I/O impacts the thread block''s scheduling
    due to the operational dependency. The worst thing in the current data accumulation
    is that it has a dependency on device memory. So, we will use extra registers
    to issue more load instructions to ease the dependency. The following code shows
    how we can do this:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 从分析器得到的结果来看，局部变量input有大量的加载/存储请求。这样大量的I/O会影响线程块的调度，因为存在操作依赖。当前数据累积中最糟糕的是它对设备内存有依赖。因此，我们将使用额外的寄存器来发出更多的加载指令以减轻依赖。以下代码显示了我们如何做到这一点：
- en: '[PRE30]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This code uses three more registers to collect global memory data. The value
    of `NUM_LOAD` can vary depending on the GPU because it is affected by the GPU''s
    memory bandwidth and the number of CUDA cores in a GPU:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用了三个额外的寄存器来收集全局内存数据。`NUM_LOAD`的值可能会因GPU的不同而有所不同，因为它受GPU的内存带宽和GPU中CUDA核心数量的影响：
- en: '![](img/3e7d7521-757d-4041-8999-c04e3cdfc52c.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e7d7521-757d-4041-8999-c04e3cdfc52c.png)'
- en: 'On running the following command, the achieved performance using Tesla V100
    card is **0.264** ms:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令时，使用Tesla V100卡的性能达到了**0.264**毫秒：
- en: '[PRE31]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Warp-level primitive programming
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: warp级原语编程
- en: CUDA 9.0 introduces new warp synchronous programming. This major change aims
    to avoid CUDA programming relying on implicit warp synchronize operations and
    handling synchronous targets explicitly. This helps to prevent inattentive race
    conditions and deadlocks in warp-wise synchronous operations.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 9.0引入了新的warp同步编程。这一重大变化旨在避免CUDA编程依赖隐式warp同步操作，并明确处理同步目标。这有助于防止warp级同步操作中的疏忽竞争条件和死锁。
- en: 'Historically, CUDA provided only one explicit synchronization API, `__syncthreads()`
    for the CUDA threads in a thread block and it relied on the implicit synchronization
    of a warp. The following figure shows two levels of synchronization of a CUDA
    thread block''s operation:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，CUDA只提供了一个显式同步API，即`__syncthreads()`用于线程块中的CUDA线程，并依赖于warp的隐式同步。下图显示了CUDA线程块操作的两个级别的同步：
- en: '![](img/e9d730cb-7dbe-4aa1-a5ba-f727ac229f21.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9d730cb-7dbe-4aa1-a5ba-f727ac229f21.png)'
- en: 'However, the latest GPU architectures (Volta and Turing) have an enhanced thread
    control model, where each thread can execute a different instruction, while they
    keep its SIMT programming model. The following diagram shows how it has changed:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最新的GPU架构（Volta和Turing）具有增强的线程控制模型，其中每个线程可以执行不同的指令，同时保持其SIMT编程模型。下图显示了它是如何改变的：
- en: '![](img/19f1bde5-e031-4aac-bb90-b3162e1d8069.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19f1bde5-e031-4aac-bb90-b3162e1d8069.png)'
- en: Until the Pascal architecture (left), threads were scheduled at warp level,
    and they were synchronized implicitly within a warp. Therefore, CUDA threads in
    a warp synchronized implicitly. However, this had unintended deadlock potential.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 直到Pascal架构（左图），线程是在warp级别进行调度的，并且它们在warp内部隐式同步。因此，CUDA线程在warp中隐式同步。然而，这可能会导致意外的死锁。
- en: The Volta architecture renovated this and introduced **independent thread scheduling**.
    This control model enables each CUDA thread to have its program counter and allows
    sets of participating threads in a warp. In this model, we have to use an explicit
    synchronous API to specify each CUDA thread's operations.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Volta架构对此进行了改进，并引入了**独立线程调度**。这种控制模型使每个CUDA线程都有自己的程序计数器，并允许warp中的一组参与线程。在这个模型中，我们必须使用显式的同步API来指定每个CUDA线程的操作。
- en: 'As a result, CUDA 9 introduced explicit warp-level primitive functions:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，CUDA 9引入了显式的warp级原语函数：
- en: '|  | **Warp-level primitive functions** |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | **warp级原语函数** |'
- en: '| **Identifying active threads** | `__activemask()` |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| **识别活动线程** | `__activemask()` |'
- en: '| **Masking active threads** | `__all_sync()`, `__any_sync()`, `__uni_sync()`, `__ballot_sync()``__match_any_sync()`, `__match_all_sync()`
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| **屏蔽活动线程** | `__all_sync()`, `__any_sync()`, `__uni_sync()`, `__ballot_sync()``__match_any_sync()`, `__match_all_sync()`
    |'
- en: '| **Synchronized data exchange** | `__shfl_sync()`, `__shfl_up_sync()`, `__shfl_down_sync()`, `__shfl_xor_sync()`
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| **同步数据交换** | `__shfl_sync()`, `__shfl_up_sync()`, `__shfl_down_sync()`, `__shfl_xor_sync()`
    |'
- en: '| **Threads synchronization** | `__syncwarp()` |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| **线程同步** | `__syncwarp()` |'
- en: There are three categories of warp-wise primitive functions, which are warp
    identification, warp operations, and synchronization. All these functions implicitly
    specify synchronization targets to avoid unintended race conditions.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 有三类warp级原语函数，分别是warp识别、warp操作和同步。所有这些函数都隐式地指定了同步目标，以避免意外的竞争条件。
- en: Parallel reduction with warp primitives
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用warp原语进行并行归约
- en: 'Let''s see how this can benefit our parallel reduction implementation. This
    recipe will use the `shfl_down()` function in Cooperative Groups, and `shfl_down_sync()`
    in warp primitive functions. The following figure shows how shift down operation
    works with `shfl_down_sync()`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这如何有益于我们的并行归约实现。这个示例将使用Cooperative Groups中的`shfl_down()`函数和warp原语函数中的`shfl_down_sync()`。下图显示了`shfl_down_sync()`如何与shift
    down操作一起工作：
- en: '![](img/f62b8889-8529-494e-8b4f-61ec7ec2ca8e.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f62b8889-8529-494e-8b4f-61ec7ec2ca8e.png)'
- en: 'In this collective operation, CUDA threads in a warp can shift a specified
    register value to another thread in the same warp and synchronize with it. To
    be specific, the collective operation has two steps (the third one is optional):'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个集体操作中，warp中的CUDA线程可以将指定的寄存器值移动到同一个warp中的另一个线程，并与其同步。具体来说，集体操作有两个步骤（第三个是可选的）：
- en: Identifying, masking, or ballot sourcing CUDA threads in a warp that will have
    an operation.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别、屏蔽或投票源CUDA线程在一个warp中将进行操作。
- en: Letting CUDA thread shift data.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让CUDA线程移动数据。
- en: All the CUDA threads in a warp are synchronization (optional).
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: warp中的所有CUDA线程都在同步（可选）。
- en: 'For the parallel reduction problem, we can use warp-level reduction using `__shfl_down_sync()`.
    Now, we can enhance our thread block-level reduction with the following figure:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对于并行归约问题，我们可以使用`__shfl_down_sync()`进行warp级别的归约。现在，我们可以通过以下图来增强我们的线程块级别的归约：
- en: '![](img/10831534-803f-4d34-ba5f-b944331803d3.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10831534-803f-4d34-ba5f-b944331803d3.png)'
- en: Each warp's reduction result is stored to shared memory to share with other
    warp. Then, the final block-wise reduction can be obtained by doing warp-wise
    collection again.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 每个warp的归约结果都存储在共享内存中，以与其他warp共享。然后，通过再次进行warp-wise收集，可以获得最终的块级归约。
- en: We use `__shfl_down_sync()` since we need only one thread to have warp-level
    reduction. If you need to make all the CUDA threads have warp-level reduction,
    you can use `__shfl_xor_sync()` instead.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`__shfl_down_sync()`，因为我们只需要一个线程进行warp级别的归约。如果您需要让所有CUDA线程都进行warp级别的归约，可以使用`__shfl_xor_sync()`。
- en: The number of the first block-level reductions is the dimension of the grid,
    and the outputs are stored in global memory. By calling once again, we can build
    a parallel reduction kernel using a warp-level synchronous function.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个块级别的归约的数量是网格的维度，输出存储在全局内存中。通过再次调用，我们可以使用warp级别的同步函数构建一个并行归约核。
- en: 'Now, let''s implement warp-level reduction using warp-level primitive functions.
    Firstly, we will write a function that uses warp-shifting functions to make warp-level
    reductions. The following code shows how this can be implemented:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用warp级别的原始函数来实现warp级别的归约。首先，我们将编写一个使用warp-shifting函数进行warp级别归约的函数。以下代码显示了如何实现这一点：
- en: '[PRE32]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: For the warp-shifting, we need to let the CUDA scheduler identify the active
    threads and let the warp-shifting function do the reduction.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于warp-shifting，我们需要让CUDA调度程序识别活动线程，并让warp-shifting函数进行归约。
- en: 'The second step is to write a block-level reduction function using the previous
    warp-level reduction. We will collect the previous result in the shared memory
    and make the second reduction from the result. The following code shows how this
    can be implemented:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是使用先前的warp级别归约编写一个块级别的归约函数。我们将在共享内存中收集先前的结果，并从结果中进行第二次归约。以下代码显示了如何实现这一点：
- en: '[PRE33]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, we will implement the reduction kernel function that cumulates input data,
    and do the reduction from the block-level reduction that we have implemented.
    Since we just focused on optimizing warp-level optimization, the overall design
    is the same as the previous version. The following code shows the kernel function:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现归约核函数，累积输入数据，并从我们实现的块级归约中进行归约。由于我们只关注优化warp级别的优化，因此整体设计与之前的版本相同。以下代码显示了核函数：
- en: '[PRE34]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then, let''s compile the code using the following command:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们使用以下命令编译代码：
- en: '[PRE35]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following screenshot shows the reduction in execution time:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了执行时间的减少：
- en: '![](img/71029119-ad15-4383-9eb8-ccc6b3850aa7.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71029119-ad15-4383-9eb8-ccc6b3850aa7.png)'
- en: No host code modification was available to switch from the warp-primitive to
    Cooperative Groups. So, we could use the same host code for the two reduction
    implementations.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在主机代码修改上，没有从warp原语切换到协作组。因此，我们可以对两种归约实现使用相同的主机代码。
- en: 'We have covered warp synchronous programming in CUDA. Its application is not
    only limited to reduction but can be used for other parallel algorithms: scan,
    bitonic sort, and transpose. If you need to learn more, you can checkout the following
    articles:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了CUDA中的warp同步编程。它的应用不仅限于归约，还可以用于其他并行算法：扫描、双调排序和转置。如果您需要了解更多信息，可以查看以下文章：
- en: '[http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf](http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf)'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf](http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf)'
- en: '[https://devblogs.nvidia.com/using-cuda-warp-level-primitives/](https://devblogs.nvidia.com/using-cuda-warp-level-primitives/)'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://devblogs.nvidia.com/using-cuda-warp-level-primitives/](https://devblogs.nvidia.com/using-cuda-warp-level-primitives/)'
- en: '[https://devblogs.nvidia.com/faster-parallel-reductions-kepler/](https://devblogs.nvidia.com/faster-parallel-reductions-kepler/)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://devblogs.nvidia.com/faster-parallel-reductions-kepler/](https://devblogs.nvidia.com/faster-parallel-reductions-kepler/)'
- en: '[http://on-demand.gputechconf.com/gtc/2013/presentations/S3174-Kepler-Shuffle-Tips-Tricks.pdf](http://on-demand.gputechconf.com/gtc/2013/presentations/S3174-Kepler-Shuffle-Tips-Tricks.pdf)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://on-demand.gputechconf.com/gtc/2013/presentations/S3174-Kepler-Shuffle-Tips-Tricks.pdf](http://on-demand.gputechconf.com/gtc/2013/presentations/S3174-Kepler-Shuffle-Tips-Tricks.pdf)'
- en: Cooperative Groups for flexible thread handling
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 灵活处理线程的协作组
- en: CUDA 9.0 introduces a new CUDA programming feature named **Cooperative Groups**.
    This introduces a new CUDA programming design pattern for CUDA collective operations
    by specifying group-wise operations. Using this, programmers can write CUDA code
    that controls CUDA threads explicitly.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 9.0引入了一个名为**协作组**的新CUDA编程特性。这通过指定组操作来引入了一种新的CUDA编程设计模式。使用这个特性，程序员可以编写显式控制CUDA线程的CUDA代码。
- en: To begin with, let's see what Cooperative Groups is and its programming advantages.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看协作组是什么，以及它的编程优势。
- en: Cooperative Groups in a CUDA thread block
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA线程块中的协作组
- en: Cooperative Groups provides explicit CUDA thread-grouping objects, which help
    programmers to write collective operations more clearly and conveniently. For
    instance, we need to obtain a mask to control the active CUDA threads in a warp
    to have warp-shifting operations. Cooperative Group objects, on the other hand,
    bind the available threads as a tile, and we control them as an object. This brings
    C++ language benefits to the CUDA C programming.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 协作组提供了显式的CUDA线程分组对象，帮助程序员更清晰、更方便地编写集体操作。例如，我们需要获取一个掩码来控制warp中活动的CUDA线程以进行warp-shifting操作。另一方面，协作组对象将可用的线程绑定为一个瓦片，并将它们作为一个对象进行控制。这为CUDA
    C编程带来了C++语言的好处。
- en: 'The fundamental type of Cooperative Group is `thread_group`. This enables a
    C++ class-style type, `thread_group`, which can provide its configuration information
    with the `is_valid()`, `size()`, and `thread_rank()` functions. Also, this provides
    collective functions that can apply to all the CUDA threads in a group. These
    functions are as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 协作组的基本类型是`thread_group`。这使得C++类样式的类型`thread_group`能够提供其配置信息，使用`is_valid()`、`size()`和`thread_rank()`函数。此外，这提供了可以应用于组中所有CUDA线程的集体函数。这些函数如下：
- en: '|  | thread_group collective functions |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  | thread_group集体函数 |'
- en: '| **Identifying active threads** | `tiled_partition()`, `coalesced_threads()`
    |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| **识别活动线程** | `tiled_partition()`, `coalesced_threads()` |'
- en: '| **Masking active threads** | `any()`, `all()`, `ballot()``match_any()`, `match_all()`
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| **屏蔽活动线程** | `any()`, `all()`, `ballot()``match_any()`, `match_all()` |'
- en: '| **Synchronized data exchange** | `shfl()`, `shfl_up()`, `shfl_down()`, `shfl_xor()`
    |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| **同步数据交换** | `shfl()`, `shfl_up()`, `shfl_down()`, `shfl_xor()` |'
- en: '| **Thread synchronization** | `sync()` |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| **线程同步** | `sync()` |'
- en: These function lists are similar to warp-level primitive functions. So, warp-level
    primitive operations can be replaced with Cooperative Groups. `thread_group` can
    be split by a smaller `thread_group`, `thread_block_tile`, or `coalesced_group`.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数列表类似于warp级别的原始函数。因此，warp级别的原始操作可以用协作组替换。`thread_group`可以被较小的`thread_group`、`thread_block_tile`或`coalesced_group`分割。
- en: 'Cooperative Groups also provides flexibility in thread block programming. Using
    the following line of code, we can handle a thread block:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 协作组还提供了线程块编程的灵活性。使用以下代码行，我们可以处理一个线程块：
- en: '[PRE36]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`thread_block` provides CUDA built-in keyword wrapping functions, which we
    use to obtain a block index and thread index:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '`thread_block`提供了CUDA内置关键字包装函数，我们使用它来获取块索引和线程索引：'
- en: '[PRE37]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can obtain a thread block object using `this_thread_block()`, as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`this_thread_block()`来获取一个线程块对象，如下所示：
- en: '[PRE38]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now, let's see what the benefits of Cooperative Groups are compared to traditional
    CUDA built-in variables.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看协作组的好处与传统的CUDA内置变量相比有什么好处。
- en: Benefits of Cooperative Groups
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协作组的好处
- en: Using Cooperative Groups provides more C++ programmability, rather than using
    traditional CUDA built-in variables. Using a `thread_block` group, you can switch
    your kernel code from using built-in variables to Cooperative Group's indexing.
    But, the real power of Cooperative Groups is more than that. Let's cover its benefits
    in the following sections.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 使用协作组提供了更多的C++可编程性，而不是使用传统的CUDA内置变量。使用`thread_block`组，您可以将您的内核代码从使用内置变量切换到协作组的索引。但是，协作组的真正力量不仅仅如此。让我们在以下部分介绍其优势。
- en: Modularity
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块化
- en: 'With Cooperative Groups, programmers can modularize their collective operation
    kernel codes corresponding to the barrier target. This helps to avoid oversights,
    causing deadlock and race conditions by assuming all threads are running concurrently.
    The following is an example of a deadlock and normal operations by CUDA thread
    synchronization:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 使用协作组，程序员可以将集体操作的内核代码模块化，对应于屏障目标。这有助于避免假设所有线程都在同时运行而导致的疏忽，从而引发死锁和竞争条件。以下是CUDA线程同步的死锁和正常操作的示例：
- en: '![](img/50c46410-4362-4702-8911-d95b431f6817.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50c46410-4362-4702-8911-d95b431f6817.png)'
- en: For the left-hand side example, the kernel code intends to synchronize a part
    of the thread in a CUDA thread block. This code minimizes synchronization overhead
    by specifying barrier targets. However, it introduces a deadlock situation because `__syncthreads()` invokes
    a barrier, which waits for all CUDA threads to reach the barrier. However, `__synchthroead()`
    cannot meet the others and waits. The right-handed side example shows sound operation
    since it does not have any deadlock point because all the threads in the thread
    block can meet `__syncthreads()`.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 对于左侧的示例，内核代码意图同步CUDA线程块中的一部分线程。通过指定屏障目标，此代码最小化了同步开销。然而，它引入了死锁情况，因为`__syncthreads()`调用了一个屏障，等待所有CUDA线程到达屏障。然而，`__synchthroead()`无法满足其他线程的要求并等待。右侧的示例显示了良好的操作，因为它没有任何死锁点，因为线程块中的所有线程都可以满足`__syncthreads()`。
- en: In the Cooperative Groups API, on the other hand, the CUDA programmers specify
    thread groups to synchronize. The Cooperative Groups enable explicit synchronization
    targets so that the programmers can let CUDA threads synchronize explicitly. This
    item can also be treated as an instance so that we can pass the instance to the
    device functions.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在协作组API中，CUDA程序员指定线程组进行同步。协作组使得显式同步目标成为可能，因此程序员可以让CUDA线程显式同步。这个项目也可以被视为一个实例，因此我们可以将实例传递给设备函数。
- en: 'The following code shows how Cooperative Groups provide explicit synchronization
    objects and let them be handled as an instance:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了协作组如何提供显式同步对象并将它们作为实例处理：
- en: '[PRE39]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As you can see in the preceding example code, the kernel code can specify synchronization
    groups and pass them as a parameter as a `thread_group`. This helps us to specify
    the synchronize targets in the subroutines. Therefore, programmers can prevent
    inadvertent deadlock by using Cooperative Groups. Also, we can set different types
    of groups as a `thread_group` type and reuse synchronization code.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在前面的示例代码中所示，内核代码可以指定同步组并将它们作为`thread_group`参数传递。这有助于我们在子例程中指定同步目标。因此，程序员可以通过使用协作组来防止意外死锁。此外，我们可以将不同类型的组设置为`thread_group`类型并重用同步代码。
- en: Explicit grouped threads' operation and race condition avoidance
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显式分组线程的操作和避免竞争条件
- en: Cooperative Groups support warp-level cooperative operations by tiling threads
    in a warp. If the tile size matches the warp size, CUDA can omit warps' implicit
    synchronization, ensuring correct memory operation to avoid race conditions. By
    eliminating implicit synchronizations, the GPU's performance can be enhanced.
    Historically, experienced CUDA programmers used separated warps for warp-level
    synchronizations. This meant the cooperative operations in a warp did not have
    to sync with other warp operations. This unleashed GPU performance. However, it
    was risky because it introduces race conditions between cooperative operations.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 协作组通过在warp中平铺线程来支持warp级协作操作。如果tile大小与warp大小匹配，CUDA可以省略warps的隐式同步，确保正确的内存操作以避免竞争条件。通过消除隐式同步，可以增强GPU的性能。从历史上看，有经验的CUDA程序员使用分离的warps进行warp级同步。这意味着warp中的协作操作不必与其他warp操作同步。这释放了GPU的性能。但是，这是有风险的，因为它引入了协作操作之间的竞争条件。
- en: Dynamic active thread selection
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态活动线程选择
- en: Another benefit of CUDA Cooperative Groups is that the programmers can pick
    active threads in a warp to avoid a branch divergence effect. Since CUDA is a
    SIMT architecture, an instruction unit issues a group of threads, and there is
    no way to disable divergence if they meet a branch. But, from CUDA 9.0 onward,
    programmer can select active threads that will be active in the branched block
    using `coalesced_threads()`. This returns coalesced threads by disabling threads
    that do not take branches. Then, SM's instruction unit issues the next threads
    that are active in the active thread group.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA Cooperative Groups的另一个好处是程序员可以选择warp中的活动线程，以避免分支分歧效应。由于CUDA是SIMT架构，一个指令单元发出一组线程，并且如果它们遇到分支，就无法禁用分歧。但是，从CUDA
    9.0开始，程序员可以使用`coalesced_threads()`选择在分支块中活动的线程。这通过禁用不参与分支的线程返回聚合的线程。然后，SM的指令单元发出下一个活动线程组中的活动线程。
- en: Applying to the parallel reduction
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用于并行减少
- en: 'We will update the previous reduction kernel code to use the Cooperative Group. From
    the previous kernel code, you can easily apply Cooperative Groups'' `thread_block`,
    as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更新以前的减少内核代码以使用协作组。从以前的内核代码中，您可以轻松应用协作组的`thread_block`，如下所示：
- en: '[PRE40]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We don''t have to update the data input accumulation part, so let''s update
    the reduction parts for each thread block. The following code shows an example
    of block-sized reduction:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必更新数据输入累积部分，因此让我们为每个线程块更新减少部分。以下代码显示了一个块大小的减少的示例：
- en: '[PRE41]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The estimated operation performance is 0.264 ms using the following command:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令的估计操作性能为0.264毫秒：
- en: '[PRE42]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The preceding command shows the same performance as in the previous version.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令显示了与以前版本相同的性能。
- en: Cooperative Groups to avoid deadlock
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协作组以避免死锁
- en: 'Cooperative Groups can support independent CUDA threads scheduling. So, we
    can control CUDA threads individually with a group, and synchronize them explicitly.
    The target group can be a predefined tile, but it can also be determined following
    the conditional branch, as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 协作组可以支持独立的CUDA线程调度。因此，我们可以使用一个组单独控制CUDA线程，并显式地对它们进行同步。目标组可以是预定义的tile，但也可以根据条件分支确定，如下所示：
- en: '[PRE43]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This code has four-thread block synchronization options. Options `(1)` and `(2)`
    are equivalent operations with different APIs. On the other hand, options `(3)`
    and `(4)` are not. Option `(3)` introduces a deadlock of CUDA threads, and the
    host cannot have the return of the CUDA kernel, because active CUDA threads cannot
    synchronize with the non-activated CUDA threads. On the other hand, option `(4)`
    works thanks to Cooperative Groups' automatic active thread identification. This
    helps us to avoid unintended errors and develop sophisticated algorithms easily.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码有四种线程块同步选项。选项`(1)`和`(2)`是具有不同API的等效操作。另一方面，选项`(3)`和`(4)`则不是。选项`(3)`引入了CUDA线程的死锁，主机无法返回CUDA内核，因为活动的CUDA线程无法与未激活的CUDA线程同步。另一方面，选项`(4)`由于协作组的自动活动线程识别而起作用。这有助于我们避免意外错误并轻松开发复杂的算法。
- en: 'NVIDIA provides detailed descriptions of Cooperative Groups in the following
    documents:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA提供了有关协作组的详细描述，可以在以下文档中找到：
- en: '[https://devblogs.nvidia.com/cuda-9-features-revealed](https://devblogs.nvidia.com/cuda-9-features-revealed)'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://devblogs.nvidia.com/cuda-9-features-revealed](https://devblogs.nvidia.com/cuda-9-features-revealed)'
- en: '[http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf](http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf)'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf](http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf)'
- en: You can also learn about its architecture and full API lists from `cooperative_groups.h`
    itself.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以从`cooperative_groups.h`本身了解其架构和完整的API列表。
- en: Loop unrolling in the CUDA kernel
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA内核中的循环展开
- en: CUDA can also reap the benefits of loop unrolling like other programming languages.
    With this technique, CUDA threads can reduce or remove loop control overheads
    such as *the end of loop* tests on each iteration, branch penalties, and so on.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA也可以像其他编程语言一样受益于循环展开。通过这种技术，CUDA线程可以减少或消除循环控制开销，例如*循环结束*测试每次迭代，分支惩罚等。
- en: CUDA Compiler unrolls small loops automatically if it can identify the number
    of iterations for the loops. Programmers can also place the `#pragma unroll` directive
    to give a hint to the compiler, or just rewrite the loop code as a group of independent
    statements. Applying loop unrolling is simple, so you can easily apply this to
    your current working code.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 如果CUDA可以识别循环的迭代次数，它会自动展开小循环。程序员还可以使用`#pragma unroll`指令向编译器提供提示，或者将循环代码重写为一组独立的语句。应用循环展开很简单，因此您可以轻松应用到当前的工作代码中。
- en: 'Let''s apply this to our parallel reduction implementation. Like the normal
    loop unrolling directive in C/C++, we can place the `#pragma` loop unrolling directive on
    top of `for` loops. The NVCC compiler can unroll the loop since the compiler can
    obtain the exact size of `group.size()` by itself:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这应用到我们的并行减少实现中。就像C/C++中的普通循环展开指令一样，我们可以在`for`循环的顶部放置`#pragma`循环展开指令。NVCC编译器可以展开循环，因为编译器可以自行获得`group.size()`的确切大小：
- en: '[PRE44]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The estimated operation performance is 0.263 ms using the following command:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令，估计的操作性能为0.263毫秒：
- en: '[PRE45]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'If you prefer to use the warp primitive function, you can write `warp_reduce_sum`
    like the following. Loop code can be reused by replacing `group.size()` with `warpSize`,
    but this was slightly faster in this case:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您更喜欢使用warp原始函数，可以像下面这样编写`warp_reduce_sum`。循环代码可以通过用`warpSize`替换`group.size()`来重用，但在这种情况下稍微更快：
- en: '[PRE46]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Run the following command to compile the preceding code:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令来编译上述代码：
- en: '[PRE47]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Its result is 0.263 ms, the same as the previous result.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 其结果是0.263毫秒，与先前的结果相同。
- en: There is a pitfall of using loop unrolling. The unrolled code execution may
    result in lower occupancy by the increased register usage. Also, there can be
    a higher instruction cache miss penalty by the increased code execution size.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 使用循环展开存在一个陷阱。展开的代码执行可能导致寄存器使用增加而降低占用率。此外，由于代码执行大小增加，可能会出现更高的指令缓存未命中惩罚。
- en: Atomic operations
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原子操作
- en: 'In CUDA programming, programmers can use atomic APIs to update shared resources
    from multiple CUDA threads. These atomic API guarantee to eliminate race conditions
    to the shared resource, so we can expect consistent outputs from the parallel
    execution. This operation is especially useful for getting statistical parameters
    such as a histogram, mean, sum, and so on. We can also simplify the code implementation.
    For example, the reduction operation can be written using the `atomicAdd()` function
    in the following code:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA编程中，程序员可以使用原子API从多个CUDA线程更新共享资源。这些原子API保证消除对共享资源的竞争条件，因此我们可以期望并行执行产生一致的输出。这个操作对于获取统计参数（如直方图、均值、总和等）特别有用。我们还可以简化代码实现。例如，可以使用以下代码中的`atomicAdd()`函数编写减少操作：
- en: '[PRE48]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As you can see, the atomic function simplifies the required operation. However,
    its performance is slow because the atomic operation serializes all the requests
    to the shared resource. Run the following command to see the execution time:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，原子函数简化了所需的操作。但是，由于原子操作将所有请求串行化到共享资源，因此其性能较慢。运行以下命令查看执行时间：
- en: '[PRE49]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This kernel function shown took 39 ms on my Tesla V100, which is far slower
    than the original version (4.609 ms). Therefore, the recommended atomic operation
    usage is to limit the request only if it is necessary. For the parallel reduction
    problem, for instance, we can reduce items in parallel at a certain level and
    use the atomic operation to output the final result.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 这个显示的内核函数在我的Tesla V100上花费了39毫秒，比原始版本（4.609毫秒）慢得多。因此，建议的原子操作使用是只在必要时限制请求。例如，对于并行减少问题，我们可以在某个级别并行减少项目，并使用原子操作输出最终结果。
- en: 'The following diagram shows another possible approach. This replaces block-wise
    reduction as `atomicAdd`:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了另一种可能的方法。这将块内减少替换为`atomicAdd`：
- en: '![](img/654607c4-860e-4c99-a464-97de231fd946.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![](img/654607c4-860e-4c99-a464-97de231fd946.png)'
- en: 'In the preceding diagram, we can see that there are two reduction points: a
    **warp** and a **thread block**, and, the block-wise reduction result is accumulated
    by the single global memory variable atomically. As a result, we can eliminate
    the second reduction iteration. The following screenshot shows the Kernel Optimization
    Priorities (on the left) and the Performance Limiter Analysis (on the right) of
    the second reduction iteration:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以看到有两个减少点：一个是**warp**，一个是**线程块**，并且，块内减少结果通过单个全局内存变量原子地累积。因此，我们可以消除第二次减少迭代。以下截图显示了第二次减少迭代的内核优化优先级（左侧）和性能限制分析（右侧）：
- en: '![](img/3c35e7bf-577c-4903-be51-7f79d9a57267.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c35e7bf-577c-4903-be51-7f79d9a57267.png)'
- en: Kernel Optimization Priorities with Performance Limiter Analysis (2nd iteration)
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 内核优化优先级与性能限制分析（第二次迭代）
- en: In other words, the second iteration's performance is bounded by the latency
    due to its small grid size. So, we would be able to reduce the execution time
    by removing this.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，第二次迭代的性能受其较小的网格大小的延迟限制。因此，通过删除这一点，我们将能够减少执行时间。
- en: 'Now let''s implement that design and see how the performance can be changed.
    We just need to update the last part of the reduction kernel function:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现该设计并看看性能如何改变。我们只需要更新减少内核函数的最后部分：
- en: '[PRE50]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then, we will remove the second iterative function call. As a result, we can
    remove kernel call latency and achieve better performance if the atomic operation''s
    latency is shorter than that. Run the following command:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将删除第二次迭代函数调用。因此，如果原子操作的延迟短于那个，我们可以消除内核调用延迟并实现更好的性能。运行以下命令：
- en: '[PRE51]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Fortunately, the estimated execution time is 0.259 ms on a Tesla V100, so we
    could achieve a slightly enhanced result.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在Tesla V100上估计的执行时间为0.259毫秒，因此我们可以获得稍微增强的结果。
- en: 'If you want to learn more about atomic operations in CUDA C, please checkout
    the programming guide at this link: [https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions).'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解CUDA C中原子操作的更多信息，请查看此链接的编程指南：[https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions)。
- en: Low/mixed precision operations
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低/混合精度操作
- en: 'Mixed precision is a technique for exploring low-precision, and obtains a high
    accuracy result. This technique computes core operations with low precision and
    generates output with high-precision operations. Low precision operation computation
    has the benefits of reduced memory bandwidth and higher computing throughput compared
    with high-precision computing. If low precision suffices to get target accuracy
    from an application with high precision, this technique can benefit performance
    with this trade-off. NVIDIA Developer Blog introduces this programmability: [https://devblogs.nvidia.com/mixed-precision-programming-cuda-8](https://devblogs.nvidia.com/mixed-precision-programming-cuda-8).'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度是一种探索低精度并获得高精度结果的技术。这种技术使用低精度计算核心操作，并使用高精度操作生成输出。与高精度计算相比，低精度操作计算具有减少内存带宽和更高的计算吞吐量的优势。如果低精度足以从具有高精度的应用程序中获得目标精度，这种技术可以通过这种权衡来提高性能。NVIDIA开发者博客介绍了这种可编程性：[https://devblogs.nvidia.com/mixed-precision-programming-cuda-8](https://devblogs.nvidia.com/mixed-precision-programming-cuda-8)。
- en: In these circumstances, CUDA extends its supports to low-precision tools lower
    than 32-bit data types, such as 8/16-bit integers (INT8/INT16) and 16-bit floating
    points (FP16). For those low-precision data types, a GPU can use **single instruction, multiple data** (**SIMD**)
    operations with some specific APIs. In this section, we will look at these two
    kinds of instructions for low-precision operations for a mixed-precision purpose.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，CUDA将其支持扩展到低于32位数据类型的低精度工具，例如8/16位整数（INT8/INT16）和16位浮点数（FP16）。对于这些低精度数据类型，GPU可以使用一些特定的API进行**单指令，多数据**（**SIMD**）操作。在本节中，我们将研究这两种用于混合精度目的的低精度操作的指令。
- en: To get benefits from this, you need to confirm that your GPU can support low
    mixed-precision operations and supporting data types. Supporting low-precision
    computing is possible in specific GPUs, and the precision varies depending on
    the GPU chipsets. To be specific, GP102 (Tesla P40 and Titan X), GP104 (Tesla
    P4), and GP106 support INT8; and GP100 (Tesla P100) and GV100 (Tesla V100) support
    FP16 (half-precision) operations. The Tesla GV100 is compatible with INT8 operation
    and has no performance degradation.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 要从中受益，您需要确认您的GPU是否支持低混合精度操作和支持的数据类型。特定GPU支持低精度计算是可能的，精度取决于GPU芯片组。具体来说，GP102（Tesla
    P40和Titan X），GP104（Tesla P4）和GP106支持INT8；GP100（Tesla P100）和GV100（Tesla V100）支持FP16（半精度）操作。Tesla
    GV100兼容INT8操作，没有性能下降。
- en: CUDA has some special intrinsic functions that enable SIMD operations for low-precision
    data types.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA具有一些特殊的内置函数，可以为低精度数据类型启用SIMD操作。
- en: Half-precision operation
  id: totrans-401
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半精度操作
- en: CUDA provides intrinsic functions for the half-sized float data type (FP16)
    and developers can choose whether CUDA computes one or two values for each instruction.
    CUDA also provides type conversion functions between single-precision and half-precision.
    Due to the accuracy limitation of FP16, you must use the conversion intrinsic
    to work with single-precision values.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA为半精度浮点数据类型（FP16）提供了内置函数，并且开发人员可以选择CUDA是否为每条指令计算一个或两个值。CUDA还提供了单精度和半精度之间的类型转换函数。由于FP16的精度限制，您必须使用转换内置函数来处理单精度值。
- en: Now, let's implement and test the GPU's FP16 operation. GPUs can support the
    native computing with this type higher than computing capability 5.3\. But some
    GPUs do not support this, so please double-check whether your GPU supports this
    half-precision operation.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现和测试GPU的FP16操作。GPU可以支持高于计算能力5.3的这种类型的本机计算。但是有些GPU不支持这一点，因此请仔细检查您的GPU是否支持这种半精度操作。
- en: 'The half-precision datatype in CUDA C is `half`, but you can use the `__half` type
    too. For the API, CUDA provides relevant intrinsic functions with this datatype such
    as `__hfma()`, `__hmul()`, and `__hadd()`. These intrinsic functions also provide
    native operations with two data at one time using `__hfma2()`, `__hmul2()`, and
    `__hadd2()`. With these functions, we can write mixed-precision operation kernel
    code:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA C中的半精度数据类型是`half`，但您也可以使用`__half`类型。对于API，CUDA提供了与此数据类型相关的内置函数，例如`__hfma()`、`__hmul()`和`__hadd()`。这些内置函数还提供了使用`__hfma2()`、`__hmul2()`和`__hadd2()`一次处理两个数据的本机操作。使用这些函数，我们可以编写混合精度操作的核心代码：
- en: '[PRE52]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: For those GPUs that do not support native half-precision operations, our code
    checks CUDA's compute capability at compile time and determines which operation
    it should take.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些不支持本机半精度操作的GPU，我们的代码在编译时检查CUDA的计算能力，并确定应采取哪种操作。
- en: 'The following code calls the kernel function with the half-sized grid size
    since each CUDA thread will operate two data:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码调用了半精度网格大小的核函数，因为每个CUDA线程将操作两个数据：
- en: '[PRE53]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Other initialization code and benchmark code is implemented in the sample recipe
    code, so please review it.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 其他初始化代码和基准代码在示例配方代码中实现，因此请查看它。
- en: We have covered FMA operations in FP16 precision operations. CUDA C provides
    various half-precision operations ([https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__HALF.html](https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__HALF.html)).
    Please check that for the other operations.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了FP16精度操作中的FMA操作。CUDA C提供了各种半精度操作（[https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__HALF.html](https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__HALF.html)）。请查看其他操作。
- en: Dot product operations and accumulation for 8-bit integers and 16-bit data (DP4A
    and DP2A)
  id: totrans-411
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8位整数和16位数据的点积运算和累加（DP4A和DP2A）
- en: 'For 8-bit/16-bit integers, CUDA provides vectorized dot product operations.
    These are DP4A (a four element dot product with accumulation) and DP2A (a two
    element dot product with accumulation). Using these functions, CUDA developers
    can make faster operations. CUDA 8.0 Development Blog introduces these functions
    with intuitive figures  ([https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/](https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/)). The
    following shows how the GPU''s dot product and accumulation operations work:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 对于8位/16位整数，CUDA提供了矢量化的点积操作。这些是DP4A（四元素点积累加）和DP2A（两元素点积累加）。使用这些函数，CUDA开发人员可以进行更快的操作。CUDA
    8.0开发博客通过直观的图示介绍了这些函数 ([https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/](https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/))。以下显示了GPU的点积和累加操作的工作原理：
- en: '![](img/4c84a489-8c06-421c-baf2-0930fb1b51c4.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c84a489-8c06-421c-baf2-0930fb1b51c4.png)'
- en: Using this, you can write 8-bit only or 8-bit/16-bit mixed operations with 32-bit
    integer accumulation. Other operations such as sum, add, and compare are also
    available with SIMD intrinsic functions.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个，你可以编写只有8位或8位/16位混合操作的32位整数累加。其他操作，如求和、加法和比较，也可以使用SIMD内在函数。
- en: As discussed previously, there are specific GPUs that can support INT8/INT16
    operations with special functions (`dp4a` and `dp2a`). The supporting GPUs' compute
    capability must be higher than 6.1.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，有特定的GPU可以支持INT8/INT16操作，具有特殊功能（`dp4a`和`dp2a`）。支持的GPU的计算能力必须高于6.1。
- en: 'Now, let''s implement a kernel function that uses the `dp4a` API, as follows:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个使用`dp4a`API的内核函数，如下所示：
- en: '[PRE54]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: In this function, `__dp4a` fetches two arrays of characters coalescing four
    items and outputs its dot product outputs. This API is supported since Pascal
    with CUDA compute capability (version 6.1). But old GPU architectures, lower than
    version 6.1, need to use the original operations.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，`__dp4a`获取两个字符数组，合并四个项目，并输出其点积输出。自帕斯卡以来，这个API就得到了支持，具有CUDA计算能力（版本6.1）。但是旧的GPU架构，低于版本6.1，需要使用原始操作。
- en: 'The following code shows how we will call the implemented kernel function.
    Its grid size is reduced by four since each CUDA thread will operate on the four
    items:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了我们将如何调用实现的内核函数。由于每个CUDA线程将操作四个项目，其网格大小减小了四倍：
- en: '[PRE55]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Other initialization code and benchmark code is implemented in samples code such
    as the previous example code.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 其他初始化代码和基准代码都在示例代码中实现，就像前面的示例代码一样。
- en: We have covered the dot operation of INT8, but CUDA C also provides other INT8-type
    SIMD intrinsic functions ([https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__SIMD.html](https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__SIMD.html)).
    Please check this document for the other operations.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了INT8的点操作，但CUDA C还提供了其他INT8类型的SIMD内在函数（[https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__SIMD.html](https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__SIMD.html)）。请查阅此文档以了解其他操作。
- en: Measuring the performance
  id: totrans-423
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能测量
- en: 'The sample code has three versions of mixed precision operations: single-precision,
    half-precision, and INT8\. As the precision drops, we can add more operations
    for each CUDA thread.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 示例代码有三个混合精度操作的版本：单精度、半精度和INT8。随着精度的降低，我们可以为每个CUDA线程添加更多的操作。
- en: 'Run the following commands for single-precision, half-precision, and INT8 operations:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令进行单精度、半精度和INT8操作：
- en: '[PRE56]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The following table shows the estimated performance for each precision operation:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了每种精度操作的估计性能：
- en: '| Precision | Measured performance |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 精度 | 测得的性能 |'
- en: '| FP32 | 59.441 GFlops |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| FP32 | 59.441 GFlops |'
- en: '| FP16 | 86.037 GFlops |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 86.037 GFlops |'
- en: '| INT8 | 196.225 Gops |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| INT8 | 196.225 Gops |'
- en: Since our implementations are not optimized, the measured performance is quite
    lower than the theoretical performance of the Tesla V100\. When you profile them,
    they will report that they are highly memory-bounded. In other words, we need
    to optimize them to be arithmetically bounded to achieve close to the theoretical
    performance.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的实现没有经过优化，所以测得的性能比Tesla V100的理论性能要低得多。当你对它们进行分析时，它们会报告它们的内存绑定性很高。换句话说，我们需要优化它们，使它们在算术上受限，以接近理论性能。
- en: Summary
  id: totrans-433
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered how to configure CUDA parallel operations and optimize
    them. To do this, we have to understand the relationship between CUDA's hierarchical
    architecture thread block and streaming multiprocessors. With some performance
    models—occupancy, performance limiter analysis, and the Roofline model—we could
    optimize more performance. Then, we covered some new CUDA thread programmability,
    Cooperative Groups, and learned how this simplifies parallel programming. We optimized
    parallel reduction problems and achieved 0.259 ms with ![](img/204bf10b-1a7d-4b62-ad48-17dbbb31f177.png) elements,
    which is a 17.8 increase in speed with the same GPU. Finally, we learned about
    CUDA's SIMD operations with half-precision (FP16) and INT8 precision.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何配置CUDA并行操作并对其进行优化。为了做到这一点，我们必须了解CUDA的分层体系结构线程块和流多处理器之间的关系。通过一些性能模型——占用率、性能限制分析和Roofline模型——我们可以优化更多性能。然后，我们介绍了一些新的CUDA线程可编程性，合作组，并学习了如何简化并行编程。我们优化了并行减少问题，并在 ![](img/204bf10b-1a7d-4b62-ad48-17dbbb31f177.png) 元素中实现了0.259毫秒，这是与相同GPU相比速度提高了17.8。最后，我们了解了CUDA的半精度（FP16）和INT8精度的SIMD操作。
- en: Our experience from this chapter focuses on the GPU's parallel processing level
    programming. However, CUDA programming includes system-level programming. Basically,
    the GPU is an extra computing resource and works independently from the host.
    This introduces extra computing power, but can also introduce latency on the other
    hand. CUDA provides API functions that can utilize this and conceal the latency
    and enables the full performance of the GPU. We will cover this in the next chapter.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们的经验集中在GPU的并行处理级别编程上。然而，CUDA编程包括系统级编程。基本上，GPU是额外的计算资源，独立于主机工作。这增加了额外的计算能力，但另一方面也可能引入延迟。CUDA提供了可以利用这一点并隐藏延迟并实现GPU的全面性能的API函数。我们将在下一章中介绍这一点。
