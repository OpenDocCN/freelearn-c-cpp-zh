["```cpp\n#include <cuda_profiler_api.h>\n```", "```cpp\ncudaProfilerStart();\n... {target of profile} ...\ncudaProfilerStop();\n```", "```cpp\nint n_iter = 5;\nfor (int i = 0; i < n_iter; i++)\n    sgemm_gpu_A(d_A, d_B, d_C, N, M, K, alpha, beta);\nfor (int i = 0; i < n_iter; i++)\n    sgemm_gpu_B(d_A, d_B, d_C, N, M, K, alpha, beta);\n```", "```cpp\n$ nvcc -m64 -gencode arch=compute_70,code=sm_70 -o sgemm sgemm.cu\n$ nvprof -f -o profile-original.nvvp ./sgemm\n```", "```cpp\ncudaProfilerStart();\nfor (int i = 0; i < n_iter; i++)\n    sgemm_gpu_B(d_A, d_B, d_C, N, M, K, alpha, beta);\ncudaProfilerStop();\n```", "```cpp\n$ nvcc -m64 -gencode arch=compute_70,code=sm_70 -o sgemm sgemm.cu\n$ nvprof -f -o profile-start-stop.nvvp --profile-from-start off ./sgemm\n```", "```cpp\n$ nvprof -f -o profile_kernels_metric.nvvp --kernels sgemm_kernel_B --metrics all ./sgemm\n```", "```cpp\nnvtxRangePushA(\"Annotation\");\n.. { Range of GPU operations } ..\ncudaDeviceSynchronization();     // in case if the target code block is pure kernel calls\nnvtxRangePop();\n```", "```cpp\n#include \"nvToolsExt.h\"\n```", "```cpp\n    cudaProfileStart();\n    // copy initial value for gpu memory\n    nvtxRangePushA(\"Data Transfer\");\n    cudaMemcpy(d_A, A, N * K * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, A, K * M * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_C, A, N * M * sizeof(float), cudaMemcpyHostToDevice);\n    nvtxRangePop();\n\n    nvtxRangePushA(\"Kernel Execution\");\n    // do operation\n    nvtxRangePushA(\"Kernel A\");\n    for (int i = 0; i < n_iter; i++)\n        sgemm_gpu_A(d_A, d_B, d_C, N, M, K, alpha, beta);\n    cudaDeviceSynchronize();\n    nvtxRangePop();    // Kernel A\n\n    nvtxRangePushA(\"Kernel B\");\n    for (int i = 0; i < n_iter; i++)\n        sgemm_gpu_B(d_A, d_B, d_C, N, M, K, alpha, beta);\n    cudaDeviceSynchronize();\n\n    nvtxRangePop();    // Kernel B\n    nvtxRangePop();    // Kernel Execution\n    cudaProfileStop();\n```", "```cpp\n$ nvcc -m64 -gencode arch=compute_70,code=sm_70 -lnvToolsExt -o sgemm sgemm.cu\n```", "```cpp\n$ nvprof -f --profile-from-start off -o sgemm.nvvp ./sgemm.nvvp\n```", "```cpp\ncudaMalloc((void**)&ptr, byte_size);\n```", "```cpp\nEnum cudaErorr_t {\n    cudaSuccess = 0,\n    cudaErrorMemoryAllocation = 2, \n    cudaErrorUnknown = 30,\n    cudaErrorNoDevice = 38,\n    cudaErrorAssert = 59,\n    cudaErrorTooManyPeers = 60,\n    cudaErrorNotSupported = 71,\n    ....\n};\n```", "```cpp\n#define checkCudaErrors(err) { \\\n    if (err != cudaSuccess) {  \\\n\n        fprintf(stderr, \"checkCudaErrors() API error = %04d \\\"%s\\\" from file <%s>, line %i.\\n\", \\\n                err, cudaGetErrorString(err), __FILE__, __LINE__); \\\n        exit(-1); \\\n    } \\\n}\n#endif\n```", "```cpp\ncheckCudaErrors(cudaMalloc((void **)&d_A, N * K * sizeof(float)));\ncheckCudaErrors(cudaMalloc((void **)&d_B, K * M * sizeof(float)));\ncheckCudaErrors(cudaMalloc((void **)&d_C, N * M * sizeof(float)));\n```", "```cpp\nsgemm_kernel_A<<<dimGrid, dimBlock>>>(A, B, C, N, M, K, alpha, beta);\ncheckCudaErrors(cudaGetLastError());\n```", "```cpp\nsgemm_kernel_A<<<dimGrid, dimBlock>>>(A, B, C, N, M, K, alpha, beta);\ncheckCudaErrors(cudaDeviceSynchronize());\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -lnvToolsExt -o sgemm ./sgemm.cu\nCUDA error at sgemm.cu:93 code=11(cudaErrorInvalidValue) \"cudaMemcpy(d_A, A, N * K * sizeof(float), cudaMemcpyHostToDevice)\"\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -lnvToolsExt -o sgemm ./sgemm.cu\nCUDA error at sgemm.cu:104 code=77(cudaErrorIllegalAddress) \"cudaDeviceSynchronize()\"\n```", "```cpp\n$ ./sgemm\nCUDA error at sgemm.cu:104 code=77(cudaErrorIllegalAddress) \"cudaDeviceSynchronize()\" \n$ CUDA_LAUNCH_BLOCKING=1 ./sgemm\nCUDA error at sgemm.cu:36 code=77(cudaErrorIllegalAddress) \"cudaGetLastError()\"\n```", "```cpp\nvoid assert(int expression);\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -lnvToolsExt -o sgemm ./sgemm.cu\nsgemm.cu:29: void sgemm_kernel_A(const float *, const float *, float *, int, int, int, float, float): block: [16,64,0], thread: [0,0,0] Assertion `sum == 0.f` failed.     \n```", "```cpp\n__global__ void sgemm_kernel_A(const float *A, const float *B, float *C, int N, int M, int K, float alpha, float beta)\n  {\n      int col = blockIdx.x * blockDim.x + threadIdx.x;\n      int row = blockIdx.y * blockDim.y + threadIdx.y;\n      float sum = 0.f;\n      for (int i = 0; i < K; ++i) \n          sum += A[row * K + i] * B[i * K + col];\n\n      if (row == 0 && col == 0)\n assert(sum == 0.f);\n\n      C[row * M + col] = alpha * sum + beta * C[row * M + col];\n  }\n```", "```cpp\nsgemm.cu:29: void sgemm_kernel_A(const float *, const float *, float *, int, int, int, float, float): block: [0,0,0], thread: [0,0,0] Assertion `sum == 0.f` failed.\n```", "```cpp\n$ nvcc -run -m64 -g -G -Xcompiler -rdynamic -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o simple_sgemm ./simple_sgemm.cu\n```", "```cpp\n$ cuda-gdb simple_sgemm\n```", "```cpp\n(cuda-gdb) break simple_gemm.cu:21\n```", "```cpp\n(cuda-gdb) break sgemm_kernel\n```", "```cpp\n(cuda-gdb) break sgemm_kernel if blockIdx.y == 2\n```", "```cpp\n(cuda-gdb) cond 3 // break 3 is defined previously\n```", "```cpp\n(cuda-gdb) run\n[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (5,0,0), device 0, sm 0, warp 0, lane 5]\nThread 1 \"simple_sgemm\" hit Breakpoint 1, sgemm_kernel<<<(128,128,1),(16,16,1)>>> (A=0x7fffb6000000, B=0x7fffb7000000, C=0x7fffb4000000, N=2048, M=2048, K=2048, alpha=2, beta=1) at simple_sgemm.cu:21\n21 int col = blockIdx.x * blockDim.x + threadIdx.x;\n```", "```cpp\n(cuda-gdb) info cuda kernels\nKernel Parent Dev Grid Status   SMs Mask     GridDim  BlockDim Invocation\n*      0      -   0    1 Active 0xffffffff (128,128,1) (16,16,1) sgemm_kernel(A=0x7ffff5a79010, B=0x7ffff4a78010, C=0x7ffff3a77010, N=2048, M=2048, K=2048, alpha=2, beta=1)\n```", "```cpp\n(cuda-gdb) print col\n$1 = <optimized out>\n(cuda-gdb) cuda kernel 0 block 1,2,0 thread 3,4,0\n21 int col = blockIdx.x * blockDim.x + threadIdx.x;\n(cuda-gdb) s\n22 int row = blockIdx.y * blockDim.y + threadIdx.y;\n(cuda-gdb) p col\n$2 = 19\n```", "```cpp\n(cuda-gdb) cuda device kernel block thread\nkernel 3, block (1,2,0), thread (3,4,0), device 0\n```", "```cpp\n$ cuda-memcheck [options] <application>\n```", "```cpp\n$ nvcc -m64 -g -G -Xcompiler -rdynamic -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc -o simple_sgemm ./simple_sgemm.cu\n$ cuda-memcheck simple_sgemm\n========= CUDA-MEMCHECK\nApplication finished successfully.========= ERROR SUMMARY: 0 errors\n```", "```cpp\nFor instance, you may add one to the row value.\n__global__ void sgemm_kernel(const float *A, const float *B, float *C, int N, int M, int K, float alpha, float beta)\n{\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    row += 1;\n\n    float sum = 0.f;\n    for (int i = 0; i < K; ++i)\n        sum += A[row * K + i] * B[i * K + col];\n    C[row * M + col] = alpha * sum + beta * C[row * M + col];\n}\n```", "```cpp\nCUDA error at simple_sgemm_oob.cu:78 code=77(cudaErrorIllegalAddress) \"cudaDeviceSynchronize()\"\n```", "```cpp\n$ cuda-memcheck simple_sgemm_oob\n```", "```cpp\n$ cuda-gdb simple_sgemm_oob\n(cuda-gdb) set cuda memcheck on\n(cuda-gdb) run\n```", "```cpp\n(cuda-gdb) print A[row * K + i]\nError: Failed to read generic memory at address 0x7fffc7600000 on device 0 sm 41 warp 20 lane 16, error=CUDBG_ERROR_INVALID_MEMORY_SEGMENT(0x7).\n(cuda-gdb) print row * K + i\n$1 = 4194304\n```", "```cpp\nexport PATH=$PATH:/usr/local/cuda/bin:/usr/local/cuda-10.1/NsightSystems-2019.3/Target-x86_64/x86_64\n```", "```cpp\n$ nsys profile -t osrt,cuda,nvtx,cublas,cudnn -o baseline -w true <command>\n```", "```cpp\n$ nsys profile -t osrt,cuda,nvtx -o sgemm -w true ./sgemm\n```", "```cpp\n$ nv-nsight-cu-cli -o <output filename> <application command>\n```", "```cpp\n$ nv-nsight-cu-cli -o reduction reduction\n```"]