["```cpp\n$ pgc++ -c -acc -ta=tesla:pinned scrImagePgmPpmPackage.cpp\n$ pgc++ -c -acc -ta=tesla:pinned -Minfo=accel image_merging.cpp\n$ pgc++ -o merging.out -acc -ta=tesla:pinned -Minfo=accel scrImagePgmPpmPackage.o image_merging.o\n$ ./merging.out\n```", "```cpp\n.... < More compiler output above>\nmerge_parallel_pragma(unsigned char *, unsigned char *, unsigned char *, long, long):\n    30, Generating copyin(in1[:w*h])\n    Generating copyout(out[:w*h])\n    Generating copyin(in2[:w*h])\n    Accelerator kernel generated\n    Generating Tesla code\n    30, #pragma acc loop gang /* blockIdx.x */\n    32, #pragma acc loop vector(128) /* threadIdx.x */\n    32, Loop is parallelizable\n... < More compile output below >\n```", "```cpp\n$ ./merging.out\nReading image width height and width [1536][2048]\nTime taken for serial merge: 0.0028 seconds\nTime taken for OpenACC merge(data+kernel): 0.0010 seconds\nTime taken for OpenACC merge(kernel only) with Blocking: 0.0002 seconds\n Time taken for OpenACC merge(data _kernel) with blocking: 0.0014 seconds\nTime taken for OpenACC merge (data+kernel)with Pipeline Async: 0.0008 seconds\n```", "```cpp\nvoid merge_serial(unsigned char *in1, unsigned char*in2, unsigned char *out, long w, long h)\n{\n    long x, y;\n     for(y = 0; y < h; y++) {\n         for(x = 0; x < w; x++) {\n             out[y * w + x] = (in1[y * w + x]+in2[y * w + x])/2;\n         }\n     }\n}\n```", "```cpp\n#pragma acc <directive> <clauses> \n!$acc parallel [clause [[,] clause]\u2026] \n```", "```cpp\n#pragma acc parallel loop \nfor (int i = 0; i < N; i++ ) {  \n    //loop code \n}\n```", "```cpp\n#pragma acc parallel\n{\n    #pragma acc loop\n    for (int i = 0; i < N; i++ )\n    {\n        < loop code >\n    }\n}\n```", "```cpp\n#pragma acc parallel loop\nfor (int i = 0; i < N; i++ )\n{\n    //loop code \n}\n```", "```cpp\n#pragma acc parallel loop\nfor (int i = 0; i < N; i++ )\n{\n    #pragma acc loop\n    for( int j = 0; j < M; j++ )\n    {\n        //loop code\n    }\n}\n```", "```cpp\nint *A = (int*) malloc(N * sizeof(int));\n\n#pragma acc parallel loop\nfor( int i = 0; i < N; i++ )\n{\n    A[i] = 0;\n}\n```", "```cpp\nint *a = (int*) malloc(N * sizeof(int));\n#pragma acc parallel loop copy(a[0:N])\nfor( int i = 0; i < N; i++ )\n{\n     a[i] = 0;\n}\n```", "```cpp\n#pragma acc parallel loop copy(A[1:N-2])\n```", "```cpp\nvoid merge_parallel_pragma(unsigned char *in1, unsigned char*in2,unsigned char *out, long w, long h)\n{\n    long x, y;\n    #pragma acc parallel loop gang copyin(in1[:h*w],\n                                          in2[:h*w]) \n                                          copyout(out[:h*w])\n     for(y = 0; y < h; y++) {\n        #pragma acc loop vector\n        for(x = 0; x < w; x++) {\n            out[y * w + x] = (in1[y * w + x]+in2[y * w + x])/2;\n        }\n    }\n}\n```", "```cpp\nmerge_parallel_pragma(unsigned char *, unsigned char *, unsigned char *, long, long):\n    30, Generating copyin(in1[:w*h])\n        Generating copyout(out[:w*h])\n        Generating copyin(in2[:w*h])\n        Accelerator kernel generated\n        Generating Tesla code\n        30, #pragma acc loop gang /* blockIdx.x */\n        32, #pragma acc loop vector(128) /* threadIdx.x */\n32, Loop is parallelizable\n```", "```cpp\n$ nvprof ./merging.out\n==26601== DoneProfiling application: ./merging.out\n==26601== Profiling result:\nType Time(%) Time Calls Avg Min Max Name\nGPU activities: 67.36% 609.41us 2 304.71us 286.34us 323.08us [CUDA memcpy HtoD]\n27.63% 250.02us 1 250.02us 250.02us 250.02us [CUDA memcpy DtoH]\n5.01% 45.344us 1 45.344us 45.344us 45.344us merge_parallel_pragma_30_gpu(unsigned char*, unsigned char*, unsigned char*, long, long)\n...\n```", "```cpp\n< Initialize data on host (CPU) >\n#pragma acc data < data clauses >\n{\n    //< Code >\n}\n```", "```cpp\n#pragma acc data copyin(A[0:N]) create(C[0:N])\n{\n    #pragma acc parallel loop\n    for( int i = 0; i < N; i++ )\n    {\n        C[i] = A[i] + 10;\n    }\n    #pragma acc parallel loop\n    for( int i = 0; i < N; i++ )\n    {\n        C[i] = C[i] / 10;\n    }\n}\n```", "```cpp\n#define N 1024\nint* allocate(int size)\n{\n    int *ptr = (int*) malloc(size * sizeof(int));\n    #pragma acc enter data create(ptr[0:size])\n    return ptr;\n} \nvoid deallocate(int *ptr)\n{\n    #pragma acc exit data delete(ptr)\n    free(ptr);\n}\nint main()\n{\n    int *ptr = allocate(N);\n    #pragma acc parallel loop\n    for( int i = 0; i < N; i++ )\n    {\n        ptr[i] = 0;\n    }\n    deallocate(ptr);\n}\n```", "```cpp\n#pragma acc data copyin(a[:N]) async \n// performing copyin asynchronously \n#pragma acc parallel loop async \n//performing parallel loop asynchronously. \n```", "```cpp\nvoid merge_async_pipelined(unsigned char *in1, unsigned char*in2,unsigned char *out, long w, long h)\n{\n    long x, y;\n    #pragma acc enter data create(in1[:w*h], in2[:h*w], out[:w*h])\n    const long numBlocks = 8;\n    const long rowsPerBlock = (h+(numBlocks-1))/numBlocks;\n    for(long block = 0; block < numBlocks; block++) {\n        long lower = block*rowsPerBlock; // Compute Lower\n        long upper = MIN(h, lower+rowsPerBlock); // Compute Upper\n        #pragma acc update device(in1[lower*w:(upper-lower)*w],\n                                  in2[lower*w:(upper-lower)*w]) \n                                  async(block%2)\n        #pragma acc parallel loop present(in1,in2, out) async(block%2)\n        for(y = lower; y < upper; y++) {\n            #pragma acc loop\n            for(x = 0; x < w; x++) {\n                out[y * w + x] = (in1[y * w + x]+in2[y * w + x])/2;\n            }\n        }\n        #pragma acc update self(out[lower*w:(upper-lower)*w]) \n                                async(block%2)\n    }\n#pragma acc wait\n#pragma acc exit data delete(in1, in2, out)\n}\n```", "```cpp\nmerge_async_pipelined(unsigned char *, unsigned char *, \n                      unsigned char *, long, long):\n     67, Generating enter data create(out[:h*w],in2[:h*w],in1[:h*w])\n     74, Generating update device(in1[w*lower:w*(upper-lower)],\n                                  in2[w*lower:w*(upper-lower)])\n         Generating present(in1[:],out[:],in2[:])\n         Accelerator kernel generated\n         Generating Tesla code\n         74, #pragma acc loop gang /* blockIdx.x */\n         76, #pragma acc loop vector(128) /* threadIdx.x */\n     76, Loop is parallelizable\n     81, Generating update self(out[w*lower:w*(upper-lower)])\n     84, Generating exit data delete(out[:1],in2[:1],in1[:1])\n```", "```cpp\n#pragma acc parallel loop gang\nfor( i = 0; i < size; i++ )\n    #pragma acc loop worker\n    for( j = 0; j < size; j++ )\n        #pragma acc loop vector\n        for( k = 0; k < size; k++ )\n          c[i][j] += a[i][k] * b[k][j];\n```", "```cpp\n#pragma acc parallel num_gangs(2) \\\n  num_workers(2) vector_length(32)\n{\n  #pragma acc loop gang worker\n  for(int x = 0; x < 4; x++){\n    #pragma acc loop vector\n    for(int y = 0; y < 32; y++){\n      array[x][y]++;\n    }\n  }\n}\n```", "```cpp\n$ pgc++ -c -acc -ta=tesla:managed scrImagePgmPpmPackage.cpp\n$ pgc++ -c -acc -ta=tesla:managed -Minfo=accel image_merging.cpp\n$ pgc++ -o merging.out -acc -ta=tesla:managed -Minfo=accel scrImagePgmPpmPackage.o image_merging.o\n$ ./blurring.out\n```", "```cpp\n#pragma acc kernels \nfor (int i = 0; i < N; i++ ) \n{ \n    //< loop code > \n}\n```", "```cpp\n#pragma acc kernels loop independent\nfor (int i = 0; i < N; i++ )\n{\n    //< loop code >\n}\n```", "```cpp\n#pragma acc kernels\n{\n    for (int i = 0; i < N; i++ )\n    {\n        //< loop code >\n    } \n... some other sequential code\n    for (int j = 0; j < M; j++ )\n    {\n        //< loop code >\n    }\n}\n```", "```cpp\n#pragma acc parallel loop collapse( 3 )\nfor(int i = 0; i < N; i++)\n{\n    for(int j = 0; j < M; j++)\n    {\n        for(int k = 0; k < Q; k++)\n        {\n            < loop code >\n        }\n    }\n}\n```", "```cpp\n#pragma acc parallel loop tile( 32, 32 )\nfor(int i = 0; i < N; i++)\n{\n    for(int j = 0; j < M; j++)\n    {\n        < loop code >\n    }\n}\n```", "```cpp\ndouble *cuda_allocate(int size) {\n    double *ptr;\n    cudaMalloc((void**) &ptr, size * sizeof(double));\n    return ptr;\n}\nint main() {\n    double *cuda_ptr = cuda_allocate(100); \n    // Allocated on the device, but not the host!\n\n    #pragma acc parallel loop deviceptr(cuda_ptr)\n    for(int i = 0; i < 100; i++) {\n        cuda_ptr[i] = 0.0;\n    }\n}\n```", "```cpp\n//In CUDA code\nextern \"C\" __device__\nint cuda_func(int x) {\n        return x*x;\n}\n\n//In OpenACC Code\n#pragma acc routine seq\nextern int cuda_func(int);\n\n...\n\nint main() {\n    A = (int*) malloc(100 * sizeof(int));\n    #pragma acc parallel loop copyout(A[:100])\n    for(int i = 0; i < 100; i++) {\n        A[i] = cuda_func(i);\n    }\n}\n```"]