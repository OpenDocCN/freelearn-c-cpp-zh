- en: Appendix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: CUDA is a parallel programming platform. Learning CUDA means not only learning
    the language, but also having some engineering prowess with the GPU. That engineering
    area can be monitoring, environment settings, performance understanding, containerization,
    and so on. This chapter provides some tips to help engineers use GPUs. We could
    cover even more topics, but the following topics will be helpful for those of
    you who want to learn about CUDA and its GPU operations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA是一个并行编程平台。学习CUDA不仅意味着学习语言，还意味着具有一些与GPU相关的工程技能。这个工程领域可以是监控、环境设置、性能理解、容器化等等。本章提供了一些提示，以帮助工程师使用GPU。我们可以涵盖更多的主题，但以下主题对于那些想要学习CUDA及其GPU操作的人来说将是有帮助的。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Useful `nvidia-smi` commands
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有用的`nvidia-smi`命令
- en: WDDM/TCC mode in Windows
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Windows中的WDDM/TCC模式
- en: Performance modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能建模
- en: Exploring container-based development
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索基于容器的开发
- en: Useful nvidia-smi commands
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有用的nvidia-smi命令
- en: In this section, we will cover the monitoring features and management operations
    of `nvidia-smi`. `nvidia-smi` is the **command-line interface** (**CLI**) of the
    **NVIDIA Management Library** (**NVML**). This library enables the management
    and monitoring of NVIDIA devices. `nvidia-smi` also provides direct queries and
    commands to the device through the library. The data is presented in either plain
    text or XML format via `stdout` or a file. It provides several management tools
    for changing device statistics.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖`nvidia-smi`的监控功能和管理操作。`nvidia-smi`是**NVIDIA管理库**（**NVML**）的**命令行接口**（**CLI**）。该库使得对NVIDIA设备进行管理和监控成为可能。`nvidia-smi`还通过该库提供了对设备的直接查询和命令。数据以纯文本或XML格式通过`stdout`或文件呈现。它提供了几个管理工具，用于更改设备统计信息。
- en: '`nvidia-smi` is a CLI application that wraps NVML C/C++ APIs. It obtains requested
    information from the `NVIDIA` driver via NVML. NVML also provides APIs to work
    with other languages, such as Python and Perl.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`nvidia-smi`是一个包装NVML C/C++ API的CLI应用程序。它通过NVML从NVIDIA驱动程序获取请求的信息。NVML还提供了用于其他语言（如Python和Perl）的API。'
- en: 'Basically, `nvidia-smi` reports the following installed GPU stats for the user:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，`nvidia-smi`为用户报告了以下已安装的GPU统计信息：
- en: The first row reports the driver version, and the CUDA version supported
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一行报告了驱动程序版本和支持的CUDA版本
- en: The second row shows the GPU stats format
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二行显示了GPU的统计格式
- en: 'Each consecutive row contains each GPU''s stats, including the following:'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个连续的行包含每个GPU的统计信息，包括以下内容：
- en: GPU ID
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU ID
- en: 'Operation mode:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作模式：
- en: Persistence mode (ON/OFF)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久模式（开/关）
- en: '**Tesla Compute Cluster** (**TCC**)/**Windows Display Driver Model** (**WDDM**)
    mode'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tesla计算集群**（**TCC**）/**Windows显示驱动模型**（**WDDM**）模式'
- en: Fan speed
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风扇速度
- en: GPU temperature
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU温度
- en: Performance mode
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能模式
- en: Power usage and capacity
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功率使用和容量
- en: Bus-ID
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总线ID
- en: Memory usage and installed memory
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存使用和已安装内存
- en: Counted **error-correcting code** (**ECC**)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计数的**纠错码**（**ECC**）
- en: GPU utilization
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU利用率
- en: Compute mode
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算模式
- en: Basically, `nvidia-smi` can handle all NVIDIA GPU cards, including Tesla, Quadro,
    and GeForce. Enabled features can vary in terms of the model and type. For example,
    the ECC error count is available in Tesla and Quadro cards, while it isn't in
    GeForce because it doesn't provide the ECC feature in its device memory.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，`nvidia-smi`可以处理包括Tesla、Quadro和GeForce在内的所有NVIDIA GPU卡。启用的功能可能因型号和类型而异。例如，ECC错误计数在Tesla和Quadro卡中可用，而在GeForce中不可用，因为它在设备内存中不提供ECC功能。
- en: 'The format of `nvidia-smi` reports is the same across operating systems. The
    following screenshot shows the output of Windows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`nvidia-smi`报告的格式在各个操作系统上都是相同的。以下截图显示了Windows的输出：'
- en: '![](img/5c002293-324c-4ee6-b99d-346e3295257c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c002293-324c-4ee6-b99d-346e3295257c.png)'
- en: 'The following screenshot shows the output of Linux:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了Linux的输出：
- en: '![](img/fbf9fd23-5ea3-458b-9e02-82b2b9882431.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbf9fd23-5ea3-458b-9e02-82b2b9882431.png)'
- en: 'Therefore, we can read the reports and set GPU operations in the same format.
    Now, let''s move on and look at the commands that are frequently used. The default
    `nvidia-smi` CLI''s usage is as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以阅读报告并以相同的格式设置GPU操作。现在，让我们继续看一下经常使用的命令。默认的`nvidia-smi` CLI的用法如下：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To begin with, the following options are frequently used depending on the monitoring
    purpose:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，根据监视目的，经常使用以下选项：
- en: '`-i`, `--id=`: For selecting the targeting GPU'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-i`，`--id=`：用于选择目标GPU'
- en: '`-l`, `--loop=`: Reports the GPU''s status at a specified second interval'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-l`，`--loop=`：以指定的秒间隔报告GPU的状态'
- en: '`-f`, `--filename=`: For logging in to a specified file'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-f`，`--filename=`：用于记录到指定文件中'
- en: This list covers `nvidia-smi` options that can help us to obtain detailed information
    from the GPUs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表涵盖了可以帮助我们从GPU获取详细信息的`nvidia-smi`选项。
- en: Getting the GPU's information
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取GPU的信息
- en: '`nvidia-smi` reports structured output when we use the `--query` (`-q`) option.
    Therefore, we can learn about which information is collected. We can obtain GPU
    information such as utilization, power, memory, and clock speed stats. On the
    other hand, this format is not helpful if we wish to monitor the GPU''s status
    continuously.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用`--query`（`-q`）选项时，`nvidia-smi`报告结构化输出。因此，我们可以了解收集了哪些信息。我们可以获取GPU的利用率、功率、内存和时钟速度统计信息。另一方面，如果我们希望连续监视GPU的状态，这种格式就不太有用了。
- en: Getting formatted information
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取格式化信息
- en: 'The basic GPU stats that we need to monitor are power, temperature, core utilization,
    and memory usage. This can easily be done with the `--query-gpu` command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要监视的基本GPU统计信息是功率、温度、核心利用率和内存使用情况。这可以很容易地通过`--query-gpu`命令完成：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following command shows some options that we can use to detect performance
    draw reasons for clock throttling:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令显示了我们可以使用的一些选项，以检测时钟调节的性能降低原因：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The reasons for GPU clock throttling can be power brake, overheating, and sync
    boost. Power brake means that the GPU's power consumption is limited by the user's
    setting or the power supplier's performance in the system. Overheating is also
    a frequent throttling reason due to a poor cooling environment.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: GPU时钟调节的原因可能是功率限制、过热和同步提升。功率限制意味着GPU的功耗受用户设置或系统中电源供应商的性能限制。过热也是由于散热环境不佳而频繁出现的调节原因。
- en: Power management mode settings
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 功耗管理模式设置
- en: 'You can find out the maximum power consumption per GPU using the following
    command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令找出每个GPU的最大功耗：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Setting the GPU's clock speed
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置GPU的时钟速度
- en: By default, the GPU's clock speed changes based on demand and saves power consumption
    to maximize power efficiency. To maximize your GPU's performance and reduce latency,
    especially in a benchmark situation, we can ensure that the GPU has a maximum
    clock speed and disable the GPU driver.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，GPU的时钟速度会根据需求变化，以节省功耗，最大化功耗效率。为了最大化GPU的性能并减少延迟，特别是在基准测试情况下，我们可以确保GPU具有最大时钟速度并禁用GPU驱动程序。
- en: 'First, we need to set the GPU in persistence mode. Doing this means that the
    GPU driver module is always loaded to the kernel and reduces the initial response
    time. This option is only available on Linux since Windows does not unload the
    GPU driver. The persistent mode setting command is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将GPU设置为持久模式。这样做意味着GPU驱动程序模块始终加载到内核中，并减少了初始响应时间。这个选项只在Linux上可用，因为Windows不会卸载GPU驱动程序。持久模式设置命令如下：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we can set the maximum supported clocks. This value will vary based on
    the GPU you are using:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以设置最大支持的时钟。这个值会根据您使用的GPU而有所不同：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For example, the Tesla V100 card can be set with the following commands:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Tesla V100卡可以使用以下命令进行设置：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: GPU device monitoring
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU设备监控
- en: 'This command probes the selected GPU''s device status every second:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令每秒探测一次所选GPU的设备状态：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following screenshot shows the result of the previous command. The device
    we are monitoring states that it has a GPU device status of `0`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了前面命令的结果。我们正在监控的设备状态显示为`0`：
- en: '![](img/fb906ac4-8892-4d60-a69a-13552a6b92ac.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb906ac4-8892-4d60-a69a-13552a6b92ac.png)'
- en: 'The collected information can be specified with the `-s` option, as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 收集到的信息可以使用`-s`选项指定，如下所示：
- en: '`p`: Power usage and temperature'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p`: 功耗和温度'
- en: '`u`: Utilization'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`u`: 利用率'
- en: '`c`: Proc and mem clocks'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`c`: 处理器和内存时钟'
- en: '`v`: Power and thermal violations'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`v`: 功耗和温度违规'
- en: '`m`: FB and Bar1 memory'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`m`: FB和Bar1内存'
- en: '`e`: ECC errors and PCIe replay errors'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`e`: ECC错误和PCIe重播错误'
- en: '`t`: PCIe Rx and Tx throughput'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t`: PCIe接收和发送吞吐量'
- en: Monitoring GPU utilization along with multiple processes
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控GPU利用率以及多个进程
- en: 'If you''re using multiple process operations on a single GPU, you may consider
    using this command. This command collects GPU stats, along with the process they
    are being used on. This means you can determine which process has been throttled
    by GPU sharing, the room for memory timings, and so on:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在单个GPU上使用多个进程操作，可以考虑使用这个命令。这个命令收集GPU统计信息，以及它们正在使用的进程。这意味着您可以确定哪个进程被GPU共享限制，内存时序的空间等等：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following screenshot shows the output of `nvidia-smi` with **Process ID** (**PID**),
    which helps in determining which process is using what GPU resources:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了带有**进程ID**（**PID**）的`nvidia-smi`输出，这有助于确定哪个进程正在使用哪个GPU资源：
- en: '![](img/b31adbb3-a103-4fe1-9f65-b36c37da194e.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b31adbb3-a103-4fe1-9f65-b36c37da194e.png)'
- en: 'Each column in the preceding screenshot shows each GPU''s computing unit utilization
    or memory usage:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 前面截图中的每一列显示了每个GPU的计算单元利用率或内存使用情况：
- en: '`sm%`: CUDA core utilization'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sm%`: CUDA核心利用率'
- en: '`mem%`: Sampled time ratio for memory operations'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mem%`: 内存操作的采样时间比率'
- en: '`enc%`/`dec%`: HW encoder''s utilization'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enc%`/`dec%`: 硬件编码器利用率'
- en: '`fb`: FB memory usage'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fb`: FB内存使用'
- en: Getting GPU topology information
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取GPU拓扑信息
- en: 'In a multi-GPU system, it is useful to use `nvidia-smi` to obtain GPU topology
    information. The following command is an `nvidia-smi` command that shows the GPU
    topology of a multi-GPU system:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在多GPU系统中，使用`nvidia-smi`获取GPU拓扑信息非常有用。以下命令是一个显示多GPU系统GPU拓扑的`nvidia-smi`命令：
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following screenshot shows the output of `nvidia-smi` showing the system''s
    topology. The result of DGX Station is that we have four NVLink-enabled V100 GPUs:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了`nvidia-smi`的输出，显示了系统的拓扑结构。DGX Station的结果是我们有四个支持NVLink的V100 GPU：
- en: '![](img/c16a6ff8-8869-4136-8fc4-3b8ddd1b30db.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c16a6ff8-8869-4136-8fc4-3b8ddd1b30db.png)'
- en: 'Following this result, we can confirm that the system''s GPU topology is as
    follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个结果，我们可以确认系统的GPU拓扑如下：
- en: '![](img/b2c9864c-3133-47d2-bf3e-12687a9e7fa7.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2c9864c-3133-47d2-bf3e-12687a9e7fa7.png)'
- en: 'The following command identifies the peer-to-peer accessibility between GPUs.
    We used this command in [Chapter 6](ba3092b0-9a57-4137-8ec9-229253c98552.xhtml),
    *Scalable Multi-GPU Programming*:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令识别了GPU之间的点对点可访问性。我们在[第6章](ba3092b0-9a57-4137-8ec9-229253c98552.xhtml) *可伸缩多GPU编程*中使用了这个命令：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following is the output of the `nvidia-smi` topology, which has four GPUs
    in a system:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个拥有四个GPU的系统的`nvidia-smi`拓扑输出：
- en: '![](img/332cf62f-5f61-45cf-a65a-66b005de6d18.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/332cf62f-5f61-45cf-a65a-66b005de6d18.png)'
- en: Peer-to-peer access is an important factor for scalability or operations. This
    command helps you confirm that the GPUs and your system can support peer-to-peer
    access between GPUs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 点对点访问是可伸缩性或操作的重要因素。这个命令可以帮助您确认GPU和您的系统是否支持GPU之间的点对点访问。
- en: WDDM/TCC mode in Windows
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Windows中的WDDM/TCC模式
- en: 'On the Windows platform, the NVIDIA GPU has two modes: WDDM and TCC. WDDM is
    the graphics driver for video cards so that it can render desktops and applications.
    If the installed GPU is only used for computing, display rendering is useless
    overhead. In this situation, the NVIDIA GPU can switch to a mode that only focuses
    on computing. This mode is known as TCC mode.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows平台上，NVIDIA GPU有两种模式：WDDM和TCC。WDDM是视频卡的图形驱动程序，因此它可以渲染桌面和应用程序。如果安装的GPU仅用于计算，则显示渲染是无用的开销。在这种情况下，NVIDIA
    GPU可以切换到仅专注于计算的模式。这种模式称为TCC模式。
- en: WDDM allows the NVIDIA GPU to cooperate with Windows' WDDM driver, which serves
    displays. Supporting WDDM mode is a requirement for Windows graphics. On the other
    hand, TCC mode only works toward computing. Depending on your GPU products and
    configuration, the GPU's mode can be changed.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: WDDM允许NVIDIA GPU与Windows的WDDM驱动程序合作，用于显示。支持WDDM模式是Windows图形的要求。另一方面，TCC模式只用于计算。根据您的GPU产品和配置，GPU的模式可以更改。
- en: 'Operation mode follows four NVIDIA product classes, and its default mode can
    vary, as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 操作模式遵循四个NVIDIA产品类别，并且其默认模式可以变化，如下所示：
- en: '**GeForce**:WDDM mode only.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GeForce**：仅支持WDDM模式。'
- en: '**Quadro/Titan**: WDDM mode by default, but can be used in TCC mode too.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quadro/Titan：默认情况下为WDDM模式，但也可以在TCC模式下使用。
- en: '**Tesla**: Typically defaults to TCC mode.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tesla**：通常默认为TCC模式。'
- en: '**Tegra**: Supports Linux only. No WDDM/TCC issues.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tegra**：仅支持Linux。没有WDDM/TCC问题。'
- en: WDDM mode supports CUDA operations and debugging CUDA applications with Nsight,
    while also supporting the display. As a single host machine, you can do everything
    that the GPU can. However, TCC mode disables graphics on the graphics driver and
    enables GPU as a computing accelerator. In other words, this should be used when
    the graphics card does not have to serve displays.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: WDDM模式支持CUDA操作和使用Nsight调试CUDA应用程序，同时也支持显示。作为单个主机机器，您可以做GPU能做的一切。但是，TCC模式禁用了图形驱动程序上的图形，并将GPU作为计算加速器启用。换句话说，当显卡不必提供显示时应使用此模式。
- en: 'TCC mode has some benefits over WDDM mode in CUDA processing, as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: TCC模式在CUDA处理中比WDDM模式具有一些优势，如下所示：
- en: Serves large-scale computing
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于大规模计算
- en: Ignores Windows' display timeout interval (typically two seconds) to enable
    kernel operations that are longer than two seconds
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忽略Windows的显示超时间隔（通常为两秒），以启用长于两秒的内核操作
- en: Reduces CUDA's kernel launch overhead on Windows
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少Windows上CUDA的核心启动开销
- en: Supports CUDA processing with Windows remote desktop service
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持在Windows远程桌面服务中进行CUDA处理
- en: Enables the use of NVIDIA GPUs with non-NVIDIA integrated graphics so that you
    can save global memory
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使非NVIDIA集成图形的NVIDIA GPU可用，以便您可以保存全局内存
- en: Therefore, TCC mode brings optimal configuration for GPUs as accelerators if
    they do not serve displays.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果GPU不提供显示服务，TCC模式为GPU作为加速器带来了最佳配置。
- en: Setting TCC/WDDM mode
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置TCC/WDDM模式
- en: 'To change TCC or WDDM mode, use the `nvidia-smi` utility, as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改TCC或WDDM模式，请使用`nvidia-smi`实用程序，如下所示：
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`0` means WDDM mode, and `1` means TCC mode.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`0`表示WDDM模式，`1`表示TCC模式。'
- en: 'If you want to set TCC mode for the selected GPUs, use the `-g` option to specify
    target GPUs:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要为所选的GPU设置TCC模式，请使用`-g`选项指定目标GPU：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This option is useful when you want to separate the purpose of GPU use for display
    and compute. After you've applied these settings, you may want to *reboot* your
    machine to apply these changes.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想要将GPU用途分开为显示和计算时，此选项非常有用。应用这些设置后，您可能需要*重新启动*您的机器以应用这些更改。
- en: 'We can identify that TCC mode is enabled by using `nvidia-smi`. The following
    screenshot shows GPU operation mode in TCC:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`nvidia-smi`来确定TCC模式是否已启用。以下截图显示了TCC中的GPU操作模式：
- en: '![](img/596401fd-59e4-4156-a3af-22ed06088a29.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/596401fd-59e4-4156-a3af-22ed06088a29.png)'
- en: By looking at the right-hand side of the GPU name in the first column, we can
    confirm that TCC mode is enabled.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看第一列中GPU名称的右侧，我们可以确认TCC模式已启用。
- en: Performance modeling
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能建模
- en: It is important to understand the characteristics of the application/algorithm
    and the GPU hardware to set realistic speedup targets. This can be achieved by
    adding parallelism. We also need to determine whether there's room to optimize
    the GPU when we optimize an application.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 了解应用程序/算法和GPU硬件的特性以设定实际加速目标非常重要。这可以通过增加并行性来实现。我们还需要确定在优化应用程序时是否有优化GPU的空间。
- en: One simple approach is to apply Amdahl's law. We can predict that achievable
    performance gain in an application is limited by the sequential portion of the
    code. For example, only 50% of the code can be made parallel, while the rest is
    sequential in nature (such as reading from a file). If this is the case, the maximum
    speedup that can be achieved is 2x; that is, the program can only run twice as
    fast. However, this performance modeling only shows the maximum speedup. We can't
    help but assume that we can completely parallelize and eliminate the execution
    time in the parallel portion of the code.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的方法是应用阿姆达尔定律。我们可以预测应用程序中可实现的性能增益受到代码顺序部分的限制。例如，只有50%的代码可以并行执行，而其余部分是顺序的（例如从文件中读取）。如果是这种情况，那么可以实现的最大加速比为2倍；也就是说，程序只能运行两倍快。然而，这种性能建模只显示了最大加速比。我们不得不假设我们可以完全并行化并消除代码并行部分的执行时间。
- en: Another performance modeling practice is to perform analysis based on the target
    architecture's performance bounding factors. In practice, we have hardware specifications,
    and its operations introduce inevitable performance limitations. By analyzing
    these limitations, we can establish whether there is room to perform optimization
    and look at the next set of optimization strategies.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种性能建模实践是基于目标架构的性能限制因素进行分析。实际上，我们有硬件规格，其操作引入了不可避免的性能限制。通过分析这些限制，我们可以确定是否有优化的空间，并查看下一组优化策略。
- en: The Roofline model
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Roofline模型
- en: 'Every kernel function can be classified into one of the following categories:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 每个核函数可以归类为以下类别之一：
- en: '**Compute bound**: The kernel function does more arithmetic operations for
    every byte of data that is read or written. These applications demand more compute
    FLOPS from the hardware.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算受限**：核心函数对每个读取或写入的数据字节进行更多的算术运算。这些应用程序需要硬件提供更多的计算FLOPS。'
- en: '**Memory bound**: The application spends most of its time reading and writing
    from memory and less computation. The application gets affected most by the memory
    bandwidth of the system rather than the FLOP rating of the hardware.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存受限**：应用程序大部分时间都在读写内存，而不是计算。应用程序受到系统内存带宽的影响最大，而不是硬件的FLOP评级。'
- en: '**Latency bound**: The kernel function''s CUDA threads spend most of their
    time in waiting rather than executing. There are many reasons for this scenario
    to occur. The primary reason is the sub-optimal level of parallelism or non-optimal
    usage of memory and computes resources.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟受限**：核心函数的CUDA线程大部分时间都在等待而不是执行。出现这种情况有很多原因。主要原因是并行性水平不佳或内存和计算资源的使用不佳。'
- en: 'Since all of these bindings are introduced by the hardware, we can graph the
    target hardware''s peak performance and memory bandwidth along with their arithmetic
    intensity. The performance curve is bounded by the hardware''s peak performance. We
    briefly touched on this in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml),
    *CUDA Thread Programming*, to determine the next optimization strategy. The following
    illustration was used in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA
    Thread Programming*, and shows an example of the Roofline model:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有这些限制都是由硬件引入的，我们可以绘制目标硬件的峰值性能和内存带宽以及它们的算术强度。性能曲线受硬件的峰值性能限制。我们在[第3章](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml)中简要提到了这一点，*CUDA线程编程*，以确定下一个优化策略。以下插图在[第3章](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml)中使用，*CUDA线程编程*，展示了屋顶线模型的一个示例：
- en: '![](img/47a31ebb-4282-499a-bcd0-c43c007e024f.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47a31ebb-4282-499a-bcd0-c43c007e024f.png)'
- en: For any computation to happen, the data needs to be transported from memory
    to the arithmetic units. It traverses a different level of the memory hierarchy,
    and the peak memory bandwidth varies depending on the memory type. The algorithm's
    peak performance can be categorized following its arithmetic intensity. This intensity
    is determined by the amount of computing data versus loaded data. Also, there
    is a computational ceiling introduced by these latency-bound factors. By measuring
    the performance and analysis against the hardware specification, we can confirm
    that the target algorithm achieved peak performance or was bound by memory or
    latency. In either case, we can determine the next step. In [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml),
    *CUDA Thread Programming*, we explored this in depth. In this section, we will
    focus on the Roofline model by looking at an example and seeing how useful it
    is.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行任何计算，数据需要从内存传输到算术单元。它遍历不同级别的内存层次结构，峰值内存带宽取决于内存类型。算法的峰值性能可以根据其算术强度进行分类。这种强度由计算数据与加载数据的量确定。此外，这些延迟限制因素引入了计算上限。通过针对硬件规格的性能和分析，我们可以确认目标算法是否达到了峰值性能或受到了内存或延迟的限制。在任何情况下，我们都可以确定下一步。在[第3章](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml)中，*CUDA线程编程*，我们深入探讨了这一点。在本节中，我们将通过一个示例来关注屋顶线模型，并看看它有多有用。
- en: The Roofline model takes into consideration the operational intensity of the
    application. In simple terms, this means that operations are done per byte from
    the main memory (DRAM). While there are more complicated models that also consider
    the cache to processor transfers, the Roofline model focuses more on the data
    transfer from DRAM to cache and, hence, focuses on the DRAM bandwidth that's needed
    by the CUDA kernel on a particular GPU architecture.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 屋顶线模型考虑了应用程序的操作强度。简单来说，这意味着每个操作都是从主存储器（DRAM）中的字节进行的。虽然还有更复杂的模型考虑了缓存到处理器的传输，但屋顶线模型更专注于从DRAM到缓存的数据传输，因此更专注于CUDA核心在特定GPU架构上所需的DRAM带宽。
- en: 'The Roofline model states the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 屋顶线模型陈述如下：
- en: '"Attainable performance ( GFLOP/s) = min (Peak Floating-Point Performance,
    Peak Memory Bandwidth * Operational Intensity)"'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '"可达性能（GFLOP/s）= min（峰值浮点性能，峰值内存带宽*操作强度）"'
- en: Analyzing the Jacobi method
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析雅可比方法
- en: 'Let''s try to understand this formula and get the Roofline model for the V100
    GPU card. The V100 GPU has the following specifications:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解这个公式，并得到V100 GPU卡的屋顶线模型。V100 GPU的规格如下：
- en: 80 SMs, each with 32 FP64 cores
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 80个SM，每个SM有32个FP64核心
- en: 900 GB/s aggregate bandwidth
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 900GB/s的总带宽
- en: 'L2 cache: 6 MB'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2缓存：6MB
- en: 'L1 Cache: 10 MB'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1缓存：10MB
- en: 'Register: 62 KB/SM'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寄存器：每个SM 62KB
- en: 'Let''s try to analyze a simple Jacobi method:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试分析一个简单的雅可比方法：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s analyze the data transfer for the preceding code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析上述代码的数据传输：
- en: 'Memory load of the vector (`Anew`, `rhs`, `Aref`): *I[Load]* *= NoRow * NoCol
    * 3 * 8 Bytes (double precision)*'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量（`Anew`，`rhs`，`Aref`）的内存加载：*I[Load]* *= NoRow * NoCol * 3 * 8字节（双精度）*
- en: Store for the vector (`Anew`): *I[store] = NoRow * NoCol * 8 Bytes*
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量（`Anew`）的存储：*I[store] = NoRow * NoCol * 8字节*
- en: 'Floating-point operations: *I[FP] = NoRow * NoCol * 6 FLOP*'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浮点运算：*I[FP] = NoRow * NoCol * 6 FLOP*
- en: 'The following graph shows the Roofline analysis of the Tesla V100 card and
    the Jacobi method''s arithmetic intensity:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Tesla V100卡的屋顶线分析和雅可比方法的算术强度：
- en: '![](img/b1660d18-bfa6-463b-8ed3-569519b1db7a.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1660d18-bfa6-463b-8ed3-569519b1db7a.jpg)'
- en: The arithmetic intensity of Jacobi on the V100 will be *I[FP]/(I[Load]+I[Strore])
    = 0.18 FLOP/byte.*
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: V100上雅可比的算术强度将是*I[FP]/(I[Load]+I[Strore]) = 0.18 FLOP/字节*
- en: The Roofline model clearly shows that the algorithm is memory bound and that
    the maximum attainable performance is 0.18 FLOP/byte only, and so will not be
    able to reach the peak FLOP rating of the V100, which is 7.8 TFLOPS. However,
    we can also predict the attainable performance after optimization by reusing the
    fetched data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Roofline模型清楚地显示了算法是内存绑定的，最大可达性能仅为0.18 FLOP/字节，因此将无法达到V100的峰值FLOP评级，即7.8 TFLOPS。然而，我们也可以通过重用获取的数据来预测优化后的可达性能。
- en: The Roofline model helps in defining the upper-performance limit for algorithms
    based on their hardware characteristics.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Roofline模型有助于根据硬件特性定义算法的上限性能。
- en: '**Jacobi method**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**雅各比方法**'
- en: This is an iterative algorithm for finding solutions for a system of linear
    equations. Its basic operations and GPU optimization are explained at [https://www.olcf.ornl.gov/wp-content/uploads/2016/01/Introduction-to-Accelerated-Computing-with-OpenACC-Jeff-Larkin.pdf](https://www.olcf.ornl.gov/wp-content/uploads/2016/01/Introduction-to-Accelerated-Computing-with-OpenACC-Jeff-Larkin.pdf).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种用于解决线性方程组的迭代算法。其基本操作和GPU优化在[https://www.olcf.ornl.gov/wp-content/uploads/2016/01/Introduction-to-Accelerated-Computing-with-OpenACC-Jeff-Larkin.pdf](https://www.olcf.ornl.gov/wp-content/uploads/2016/01/Introduction-to-Accelerated-Computing-with-OpenACC-Jeff-Larkin.pdf)中有解释。
- en: Exploring container-based development
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索基于容器的开发
- en: One of the key challenges that developers and IT administrators who are maintaining
    the cluster face is the complexity of the software stack. Each and every application/framework
    has many dependencies. More complexity gets added when these dependencies are
    of different versions. For example, in DL, Caffe has different requirements of
    versions of cuDNN and Python than TensorFlow. In a particular organization/institute,
    there are many users, and each and every user may use different versions of the
    same framework. Installing all the right dependencies and setting up the right
    environment results in the loss of productivity. More time is spent on installation
    rather than doing the work. Another challenge that's faced is that it is almost
    impossible to reproduce the result/performance numbers by different individuals,
    even though they might run on the same system, due to dependency mismatch. For
    example, the GROMACS Molecular Dynamics framework has many settings, such as compile
    with multithreading or **Message Passing Interface** (**MPI**) support, the version
    on MPI, and the MPI type. Another challenge, especially in AI, is that every software
    framework that you can possibly think of is moving very fast, and new patches
    are added frequently.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员和维护集群的IT管理员面临的一个关键挑战是软件堆栈的复杂性。每个应用程序/框架都有许多依赖关系。当这些依赖关系是不同版本时，复杂性会增加。例如，在DL中，Caffe对cuDNN和Python的版本有不同的要求，与TensorFlow不同。在特定的组织/学院中，有许多用户，每个用户可能使用相同框架的不同版本。安装所有正确的依赖关系并设置正确的环境会导致生产力的损失。花在安装上的时间比实际工作的时间更多。另一个面临的挑战是，即使在相同的系统上运行，由于依赖关系不匹配，不同的个体几乎不可能复制结果/性能数字。例如，GROMACS分子动力学框架有许多设置，比如编译多线程或**消息传递接口**（**MPI**）支持，MPI的版本和MPI类型。特别是在人工智能领域，另一个挑战是，你能想到的每个软件框架都在快速发展，并且经常添加新的补丁。
- en: 'Containers provide a solution to these problems. The key advantages of using
    containers are as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 容器为这些问题提供了解决方案。使用容器的主要优势如下：
- en: '**Isolation**: Containers provide isolation of the environment for applications'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隔离**：容器为应用程序提供环境隔离'
- en: '**Run anywhere**: Containers provide an easy way to share and test applications
    in different environments'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随处运行**：容器提供了在不同环境中共享和测试应用程序的简单方法'
- en: '**Lightweight**: Containers are lightweight compared to using virtual machine-based
    solutions and provide almost negligible latency and overhead'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轻量级**：与基于虚拟机的解决方案相比，容器轻量级，并且几乎没有延迟和开销'
- en: Two of the most famous container environments are Docker and Singularity. Both
    have their advantages and disadvantages. Note, however, that this section is not
    an extensive guide to Docker or Singularity.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 两个最著名的容器环境是Docker和Singularity。它们都有各自的优缺点。但请注意，本节不是Docker或Singularity的详尽指南。
- en: Developers usually create containers and publish them online for others to use.
    We will be explaining one such repository that's maintained by NVIDIA called **Nvidia
    GPU Cloud** (**NGC**) in detail. NGC is like a repository that hosts containers
    for popular **Deep Learning** (**DL**), **High-Performance Computing** (**HPC**),
    and **Virtual Reality** (**VR**) frameworks. NVIDIA tests these applications for
    different environments of GPU and goes through an extensive QA process before
    being made available to the public. This means that performance is guaranteed.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员通常会创建容器并将其发布到网上供他人使用。我们将详细解释由NVIDIA维护的名为**Nvidia GPU Cloud**（**NGC**）的一个这样的存储库。NGC就像是一个为流行的**深度学习**（**DL**）、**高性能计算**（**HPC**）和**虚拟现实**（**VR**）框架提供容器的存储库。NVIDIA会在不同的GPU环境中测试这些应用程序，并在向公众提供之前经过广泛的质量保证过程。这意味着性能是有保证的。
- en: An analogy of NGC is the Android App Store, which provides a repository for
    different applications that can run on different mobiles running the Android OS.
    These applications get verified and go through a QA process. The NGC name sometimes
    confuses people and developers think that it is a cloud. It should be made clear
    that it is a repository of containers that can be pulled into a system with a
    GPU and run locally. The container can run on different systems with GPUs, just
    like it can run on the desktop with NVIDIA Titan cards, servers with Tesla V100
    cards, or the NVIDIA AI supercomputer DGX. NGC containers can also be run on cloud
    platforms such as AWS and Azure.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: NGC的类比是Android应用商店，为可以在运行Android操作系统的不同手机上运行的不同应用程序提供了存储库。这些应用程序经过验证并经过质量保证流程。NGC的名称有时会让人们感到困惑，开发人员认为它是一个云。应明确指出，它是一个容器的存储库，可以被拉取到具有GPU的系统中并在本地运行。该容器可以在具有GPU的不同系统上运行，就像它可以在具有NVIDIA
    Titan卡的台式机、具有Tesla V100卡的服务器或NVIDIA AI超级计算机DGX上运行一样。NGC容器也可以在AWS和Azure等云平台上运行。
- en: NGC configuration for a host machine
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主机机器的NGC配置
- en: 'The following steps cover how to configure an NGC working environment and find available
    images in NGC:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤涵盖了如何配置NGC工作环境以及在NGC中查找可用的映像：
- en: '**Basic installation**: To make use of containers on a GPU system, you need
    to have installed the following:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基本安装**：要在GPU系统上使用容器，您需要安装以下内容：'
- en: Nvidia drivers
  id: totrans-166
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nvidia驱动程序
- en: Docker
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker
- en: '`nvidia-docker`'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nvidia-docker`'
- en: '`nvidia-docker` is an open source project that loads the NVIDIA components
    and modules into a container. It is basically a wrapper around Docker. You can
    download and see the installation instructions at [https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`nvidia-docker`是一个开源项目，它将NVIDIA组件和模块加载到容器中。它基本上是Docker的包装器。您可以在[https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0))上下载并查看安装说明。'
- en: '**Visit the NGC website**: Now, you can go to the NGC website to choose a container
    ([nvidia.com/ngc](https://www.nvidia.com/en-us/gpu-cloud/)), as shown in the following
    screenshot:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**访问NGC网站**：现在，您可以转到NGC网站选择一个容器（[nvidia.com/ngc](https://www.nvidia.com/en-us/gpu-cloud/)），如下面的屏幕截图所示：'
- en: '![](img/6605ca00-405e-43a5-b447-9a99c2a2f8b3.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6605ca00-405e-43a5-b447-9a99c2a2f8b3.png)'
- en: As you can see, there are six categories for containers. Choose the one that's
    relevant to you. The earlier version of NGC required users to register, but this
    requirement was removed recently.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，容器有六个类别。选择与您相关的类别。NGC的早期版本要求用户注册，但最近取消了此要求。
- en: Basic usage of the NGC container
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NGC容器的基本用法
- en: In this section, we will cover how to pull containers from the NGC registry
    and how to customize our own. It's no different from using Docker, except we can
    access the NGC registry, `nvcr.io`. If you are already familiar with Docker commands,
    you can skip this section.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何从NGC注册表中拉取容器以及如何自定义我们自己的容器。这与使用Docker没有区别，只是我们可以访问NGC注册表`nvcr.io`。如果您已经熟悉Docker命令，可以跳过本节。
- en: 'The following steps explain how to obtain and launch an NGC container on your
    local Linux machine in a Terminal session:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤解释了如何在本地Linux机器上的终端会话中获取和启动NGC容器：
- en: Find the software you want to use and copy the command from the NGC site.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到您想要使用的软件并从NGC网站复制命令。
- en: 'Then, pull the container image by pasting the command into the Terminal. The
    following screenshot shows the `pull` commands and their Docker operation:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，通过将命令粘贴到终端中来拉取容器映像。下面的屏幕截图显示了`pull`命令及其Docker操作：
- en: '![](img/9f5c6c17-d493-45bb-b8fc-7972af831865.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f5c6c17-d493-45bb-b8fc-7972af831865.png)'
- en: As you can see, Docker uses a layer-based approach. The CUDA container is built
    over the basic layer of Ubuntu. Also, the Docker images command showed us the
    locally pulled container on our machine.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，Docker使用基于层的方法。CUDA容器是在Ubuntu的基本层上构建的。此外，Docker images命令向我们展示了我们机器上本地拉取的容器。
- en: 'Use the following command to launch the pulled container:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令启动拉取的容器：
- en: '[PRE14]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The GPUs are shown in the following screenshot:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: GPU显示在下面的屏幕截图中：
- en: '![](img/85548425-3f21-4f14-afbc-a011124d86e3.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85548425-3f21-4f14-afbc-a011124d86e3.png)'
- en: As soon as we run Docker, the shell login changes and we are logged into the
    container that's running as root. Due to this, we were able to run the `nvidia-smi` command
    inside the container.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行Docker，shell登录会更改，并且我们登录到作为root运行的容器中。由于这个原因，我们能够在容器内运行`nvidia-smi`命令。
- en: 'We can also use the container to access the host resources by using its additional
    options. The most frequently used options are as follows:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用容器通过其附加选项访问主机资源。最常用的选项如下：
- en: '`-v`: For mounting the volume'
  id: totrans-186
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: -v：用于挂载卷
- en: '`-p`: For port forwarding'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -p：用于端口转发
- en: '`-u`: For user forwarding'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -u：用于用户转发
- en: The basic usage of `nvidia-docker` is similar to normal Docker usage, except
    we can use the GPUs. This means you can also get the added benefits of Docker.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`nvidia-docker`的基本用法与普通Docker用法类似，只是我们可以使用GPU。这意味着您还可以获得Docker的附加好处。'
- en: Creating and saving a new container from the NGC container
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从NGC容器创建和保存新容器
- en: 'You can also add layers to the existing container and save them for future
    use. Let''s learn how to do this:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以向现有容器添加层并保存它们以供将来使用。让我们学习如何做到这一点：
- en: 'Create a `Dockerfile` and create some layers over the base image. For example,
    we can update APEX ([https://github.com/nvidia/apex](https://github.com/nvidia/apex))
    in the NGC PyTorch container so that we can use its latest version:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`Dockerfile`并在基础镜像上创建一些层。例如，我们可以在NGC PyTorch容器中更新APEX（[https://github.com/nvidia/apex](https://github.com/nvidia/apex)）以便我们可以使用其最新版本：
- en: '[PRE15]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can add your desired Ubuntu packages or Python package installation code
    to that file too.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将所需的Ubuntu软件包或Python软件包安装代码添加到该文件中。
- en: 'Then, we can build a customized container with the `docker build` command.
    The following command shows the basic format of the Docker image `build` command:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`docker build`命令构建一个定制的容器。以下命令显示了Docker镜像`build`命令的基本格式：
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This command will find the `Dockerfile` we created and launch each command line
    by line. Each line of the `Dockerfile` will create a Docker layer, so it is recommended
    to write a `RUN` command to cover a single objective.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将找到我们创建的`Dockerfile`并逐行启动每个命令。`Dockerfile`的每一行都将创建一个Docker层，因此建议编写一个`RUN`命令来覆盖一个单一目标。
- en: 'Now, you need to back up your Docker images into your private registry or create
    a file. After you''ve finalized the container, you may want to propagate or reuse
    the container in other systems. In that case, you can push the Docker image into
    your registry. For instance, Docker provides a free registry if you have an account
    on `DockerHub`. You can push your container into the registry with the following
    command:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您需要将Docker镜像备份到您的私有注册表或创建一个文件。在完成容器后，您可能希望在其他系统中传播或重用该容器。在这种情况下，您可以将Docker镜像推送到您的注册表中。例如，如果您在DockerHub上有帐户，Docker提供了一个免费的注册表。您可以使用以下命令将容器推送到注册表中：
- en: '[PRE17]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can also create backup files and copy them over your local filesystem.
    The following command shows you how to create a container backup with compression:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以创建备份文件并将其复制到本地文件系统上。以下命令向您展示了如何使用压缩创建容器备份：
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, you can load that image using the following command:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用以下命令加载该镜像：
- en: '[PRE19]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You can create a local backup image without compression, but the output file
    is too large to deliver to the other systems in general.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以创建一个本地备份镜像而不进行压缩，但通常输出文件太大，无法传送到其他系统。
- en: In this section, we have covered some basic operations of Docker. However, Docker
    provides other plentiful functions and benefits too. Although Linux is only available
    for the use of CUDA in the Docker container, Docker will save you time when it
    comes to building the working environment and help you focus on your code development.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经介绍了Docker的一些基本操作。然而，Docker还提供了其他丰富的功能和好处。尽管Linux只能在Docker容器中使用CUDA，但在构建工作环境和帮助您专注于代码开发方面，Docker会为您节省时间。
- en: Setting the default runtime as NVIDIA Docker
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将默认运行时设置为NVIDIA Docker
- en: 'With some modifications to the `nvidia-docker` configuration, we can launch
    GPU containers without notifying the GPU about this use. Because we can set the
    GPU runtime option to `nvidia-docker`, we can adopt Docker''s runtime design.
    To do that, you need to insert `default-runtime": "nvidia",` as an option into
    `/etc/docker/daemon.json`. Then, the `daemon.json` file can be configured as follows
    if there is no other Docker configuration:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '通过对`nvidia-docker`配置进行一些修改，我们可以在不通知GPU的情况下启动GPU容器。因为我们可以将GPU运行时选项设置为`nvidia-docker`，所以我们可以采用Docker的运行时设计。为此，您需要将`default-runtime":
    "nvidia",`作为选项插入到`/etc/docker/daemon.json`中。然后，如果没有其他Docker配置，可以将`daemon.json`文件配置如下：'
- en: '[PRE20]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After doing this, reboot the system or restart the Docker daemon with the following
    command:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，使用以下命令重新启动系统或重新启动Docker守护程序：
- en: '[PRE21]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, we can enjoy GPU containers without the GPU command option in Docker commands.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在Docker命令中享受GPU容器而无需使用GPU命令选项。
- en: An introduction to `nvidia-docker` is provided in the NVIDIA development blog,
    which can be found at [https://devblogs.nvidia.com/gpu-containers-runtime](https://devblogs.nvidia.com/gpu-containers-runtime/).
    Here, you will learn not only about its configuration, but also how to integrate
    it with Docker compose or **Linux Containers** (**LXC**). It even allows GPU containers
    to work with Kubernetes via its GPU device plugin.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA开发博客提供了关于`nvidia-docker`的介绍，可以在[https://devblogs.nvidia.com/gpu-containers-runtime](https://devblogs.nvidia.com/gpu-containers-runtime/)找到。在这里，您不仅将了解其配置，还将了解如何将其与Docker
    compose或Linux Containers（LXC）集成。它甚至允许GPU容器通过其GPU设备插件与Kubernetes一起工作。
