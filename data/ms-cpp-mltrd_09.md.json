["```cpp\n/******************************************************************************\n * FILE: omp_hello.c\n * DESCRIPTION:\n *   OpenMP Example - Hello World - C/C++ Version\n *   In this simple example, the master thread forks a parallel region.\n *   All threads in the team obtain their unique thread number and print it.\n *   The master thread only prints the total number of threads.  Two OpenMP\n *   library routines are used to obtain the number of threads and each\n *   thread's number.\n * AUTHOR: Blaise Barney  5/99\n * LAST REVISED: 04/06/05\n ******************************************************************************/\n #include <omp.h>\n #include <stdio.h>\n #include <stdlib.h>\n\n int main (int argc, char *argv[])  {\n    int nthreads, tid;\n\n    /* Fork a team of threads giving them their own copies of variables */\n #pragma omp parallel private(nthreads, tid) {\n          /* Obtain thread number */\n          tid = omp_get_thread_num();\n          printf(\"Hello World from thread = %d\\n\", tid);\n\n          /* Only master thread does this */\n          if (tid == 0) {\n                nthreads = omp_get_num_threads();\n                printf(\"Number of threads = %d\\n\", nthreads);\n                }\n\n    }  /* All threads join master thread and disband */ \n} \n\n```", "```cpp\n#include <mpi.h> \n#include <stdio.h> \n\nint main(int argc, char** argv) { \n         // Initialize the MPI environment \n         MPI_Init(NULL, NULL); \n\n         // Get the number of processes \n         int world_size; \n         MPI_Comm_size(MPI_COMM_WORLD, &world_size); \n\n         // Get the rank of the process \n         int world_rank; \n         MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); \n\n         // Get the name of the processor \n         char processor_name[MPI_MAX_PROCESSOR_NAME]; \n         int name_len; \n         MPI_Get_processor_name(processor_name, &name_len); \n\n         // Print off a hello world message \n         printf(\"Hello world from processor %s, rank %d\" \n                     \" out of %d processors\\n\", \n                     processor_name, world_rank, world_size); \n\n         // Finalize the MPI environment. \n         MPI_Finalize(); \n} \n\n```", "```cpp\n    $ mpicc -o mpi_hello_world mpi_hello_world.c\n\n```", "```cpp\n    $ gcc mpi_hello_world.c -lmsmpi -o mpi_hello_world\n\n```", "```cpp\n    $ mpiexec.exe -n 4 mpi_hello_world.exe\n    Hello world from processor Generic_PC, rank 0 out of 4 processors\n    Hello world from processor Generic_PC, rank 2 out of 4 processors\n    Hello world from processor Generic_PC, rank 1 out of 4 processors\n    Hello world from processor Generic_PC, rank 3 out of 4 processors\n\n```", "```cpp\n    $ sudo apt-get install openmpi-bin openmpi-doc libopenmpi-dev\n\n```", "```cpp\n    $ pacman -S base-devel mingw-w64-x86_64-toolchain\n\n```", "```cpp\n    $ printenv | grep \"WIN\\|MSMPI\"\n    MSMPI_INC=D:\\Dev\\MicrosoftSDKs\\MPI\\Include\\\n    MSMPI_LIB32=D:\\Dev\\MicrosoftSDKs\\MPI\\Lib\\x86\\\n    MSMPI_LIB64=D:\\Dev\\MicrosoftSDKs\\MPI\\Lib\\x64\\\n    WINDIR=C:\\Windows\n\n```", "```cpp\n    $ mkdir ~/msmpi\n    $ cd ~/msmpi\n    $ cp \"$MSMPI_LIB64/msmpi.lib\" .\n    $ cp \"$WINDIR/system32/msmpi.dll\" .\n    $ gendef msmpi.dll\n    $ dlltool -d msmpi.def -D msmpi.dll -l libmsmpi.a\n    $ cp libmsmpi.a /mingw64/lib/.\n\n```", "```cpp\n    $ cp \"$MSMPI_INC/mpi.h\" .\n\n```", "```cpp\ntypedef __int64 MPI_Aint \n\n```", "```cpp\n    #include <stdint.h>\n\n```", "```cpp\n    $ cp mpi.h /mingw64/include\n\n```", "```cpp\n    $ ssh-copy-id mpiuser@node1\n\n```", "```cpp\n    192.168.0.1 master\n    192.168.0.2 node0\n    192.168.0.3 node1\n\n```", "```cpp\n    master\n    node0\n    node1\n\n```", "```cpp\n    node0 slots=2\n    node1 slots=4\n\n```", "```cpp\n    $ mpirun --hostfile my_hostfile hello_mpi_world\n\n```", "```cpp\nint MPI_Send( \n         void* data, \n         int count, \n         MPI_Datatype datatype, \n         int destination, \n         int tag, \n         MPI_Comm communicator) \n\nint MPI_Recv( \n         void* data, \n         int count, \n         MPI_Datatype datatype, \n         int source, \n         int tag, \n         MPI_Comm communicator, \n         MPI_Status* status) \n\n```", "```cpp\nint MPI_Type_create_struct( \n   int count,  \n   int array_of_blocklengths[], \n         const MPI_Aint array_of_displacements[],  \n   const MPI_Datatype array_of_types[], \n         MPI_Datatype *newtype) \n\n```", "```cpp\n#include <cstdio> \n#include <cstdlib> \n#include <mpi.h> \n#include <cstddef> \n\nstruct car { \n        int shifts; \n        int topSpeed; \n}; \n\nint main(int argc, char **argv) { \n         const int tag = 13; \n         int size, rank; \n\n         MPI_Init(&argc, &argv); \n         MPI_Comm_size(MPI_COMM_WORLD, &size); \n\n         if (size < 2) { \n               fprintf(stderr,\"Requires at least two processes.\\n\"); \n               MPI_Abort(MPI_COMM_WORLD, 1); \n         } \n\n         const int nitems = 2; \n         int blocklengths[2] = {1,1}; \n   MPI_Datatype types[2] = {MPI_INT, MPI_INT}; \n         MPI_Datatype mpi_car_type; \n         MPI_Aint offsets[2]; \n\n         offsets[0] = offsetof(car, shifts); \n         offsets[1] = offsetof(car, topSpeed); \n\n         MPI_Type_create_struct(nitems, blocklengths, offsets, types, &mpi_car_type); \n         MPI_Type_commit(&mpi_car_type); \n\n         MPI_Comm_rank(MPI_COMM_WORLD, &rank); \n         if (rank == 0) { \n               car send; \n               send.shifts = 4; \n               send.topSpeed = 100; \n\n               const int dest = 1; \n\n         MPI_Send(&send, 1, mpi_car_type, dest, tag, MPI_COMM_WORLD); \n\n               printf(\"Rank %d: sent structure car\\n\", rank); \n         } \n\n   if (rank == 1) { \n               MPI_Status status; \n               const int src = 0; \n\n         car recv; \n\n         MPI_Recv(&recv, 1, mpi_car_type, src, tag, MPI_COMM_WORLD, &status); \n         printf(\"Rank %d: Received: shifts = %d topSpeed = %d\\n\", rank, recv.shifts, recv.topSpeed); \n    } \n\n    MPI_Type_free(&mpi_car_type); \n    MPI_Finalize(); \n\n         return 0; \n} \n\n```", "```cpp\n#include <mpi.h> \n#include <stdio.h> \n#include <stdlib.h> \n\nint main(int argc, char** argv) { \n   // Initialize the MPI environment. \n   MPI_Init(NULL, NULL); \n\n   // Find out rank, size. \n   int world_rank; \n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); \n   int world_size; \n   MPI_Comm_size(MPI_COMM_WORLD, &world_size); \n\n   // We are assuming at least 2 processes for this task. \n   if (world_size < 2) { \n               fprintf(stderr, \"World size must be greater than 1 for %s.\\n\", argv[0]); \n               MPI_Abort(MPI_COMM_WORLD, 1); \n   } \n\n   int number; \n   if (world_rank == 0) { \n         // If we are rank 0, set the number to -1 and send it to process 1\\. \n               number = -1; \n               MPI_Send(&number, 1, MPI_INT, 1, 0, MPI_COMM_WORLD); \n   }  \n   else if (world_rank == 1) { \n               MPI_Recv(&number, 1, MPI_INT, 0, 0,  \n                           MPI_COMM_WORLD,  \n                           MPI_STATUS_IGNORE); \n               printf(\"Process 1 received number %d from process 0.\\n\", number); \n   } \n\n   MPI_Finalize(); \n} \n\n```", "```cpp\n    $ mpirun -n 2 ./send_recv_demo\n    Process 1 received number -1 from process 0\n\n```", "```cpp\nint MPI_Bcast( \n   void *buffer,  \n   int count,  \n   MPI_Datatype datatype, \n         int root,    \n   MPI_Comm comm) \n\n```", "```cpp\nint MPI_Scatter( \n         void* send_data, \n         int send_count, \n         MPI_Datatype send_datatype, \n         void* recv_data, \n         int recv_count, \n         MPI_Datatype recv_datatype, \n         int root, \n         MPI_Comm communicator) \n\n```", "```cpp\nint MPI_Gather( \n         void* send_data, \n         int send_count, \n         MPI_Datatype send_datatype, \n         void* recv_data, \n         int recv_count, \n         MPI_Datatype recv_datatype, \n         int root, \n         MPI_Comm communicator) \n\n```", "```cpp\n#include <stdio.h>\n#include <mpi.h>\n#include <omp.h>\n\nint main(int argc, char *argv[]) {\n  int numprocs, rank, len;\n  char procname[MPI_MAX_PROCESSOR_NAME];\n  int tnum = 0, tc = 1;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Get_processor_name(procname, &len);\n\n  #pragma omp parallel default(shared) private(tnum, tc) {\n      np = omp_get_num_threads();\n      tnum = omp_get_thread_num();\n      printf(\"Thread %d out of %d from process %d out of %d on %s\\n\", \n      tnum, tc, rank, numprocs, procname);\n  }\n\n  MPI_Finalize();\n}\n\n```", "```cpp\n$ mpicc -openmp hellohybrid.c -o hellohybrid\n\n```", "```cpp\n$ export OMP_NUM_THREADS=8\n$ mpirun -np 2 --hostfile my_hostfile -x OMP_NUM_THREADS ./hellohybrid\n\n```"]