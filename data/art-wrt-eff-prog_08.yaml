- en: '*Chapter 6*: Concurrency and Performance'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：并发和性能'
- en: In the last chapter, we learned about the fundamental factors that affect the
    performance of concurrent programs. Now it is time to put this knowledge to practical
    use and learn about developing high-performance concurrent algorithms and data
    structures for thread-safe programs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了影响并发程序性能的基本因素。现在是时候将这些知识付诸实践，学习开发高性能并发算法和数据结构，以实现线程安全的程序。
- en: 'On the one hand, to take full advantage of concurrency, one must take a high-level
    view of the problem and the solution strategy: data organization, work partitioning,
    sometimes even the definition of what constitutes a solution are the choices that
    critically affect the performance of the program. On the other hand, as we have
    seen in the last chapter, the performance is greatly impacted by low-level factors
    such as the arrangement of the data in the cache, and even the best design can
    be ruined by poor implementation. These low-level details are often difficult
    to analyze, hard to express in code, and require very careful coding. This is
    not the kind of code you want to be scattered around in your program, so the encapsulation
    of the tricky code is a necessity. We will have to give some thought to the best
    way to encapsulate this complexity.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，要充分利用并发，必须对问题和解决方案策略进行高层次的考虑：数据组织、工作分区，有时甚至是解决方案的定义，这些选择对程序的性能产生重要影响。另一方面，正如我们在上一章中所看到的，性能受低级因素的影响很大，比如缓存中数据的排列，甚至最佳设计也可能被糟糕的实现破坏。这些低级细节通常很难分析，在代码中很难表达，并且需要非常小心的编码。这不是您希望散布在程序中的代码类型，因此封装棘手的代码是必要的。我们将不得不考虑最佳的封装这种复杂性的方法。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Efficient concurrency
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效的并发
- en: Use of locks, pitfalls of locking, and an introduction to lock-free programming
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 锁的使用、锁定的陷阱和无锁编程的介绍
- en: Thread-safe counters and accumulators
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程安全的计数器和累加器
- en: Thread-safe smart pointers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程安全的智能指针
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Again, you will need a C++ compiler and a micro-benchmarking tool, such as the
    Google Benchmark library we used in the previous chapter (found at [https://github.com/google/benchmark](https://github.com/google/benchmark)).
    The code accompanying this chapter can be found at [https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter06](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter06).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，您将需要一个C++编译器和一个微基准测试工具，比如我们在上一章中使用的Google Benchmark库（可在[https://github.com/google/benchmark](https://github.com/google/benchmark)找到）。本章附带的代码可以在[https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter06](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter06)找到。
- en: What is needed to use concurrency effectively?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有效使用并发需要什么？
- en: 'Fundamentally, using concurrency to improve performance is very simple: you
    really need to do just two things. The first one is to have enough work for the
    concurrent threads and processes to do so they are busy at all times. The second
    one is to reduce the use of the shared data since, as we have seen in the previous
    chapter, accessing a shared variable concurrently is very expensive. The rest
    is just a matter of the implementation.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，利用并发来提高性能非常简单：您只需要做两件事。第一件事是为并发线程和进程提供足够的工作，以便它们始终保持忙碌状态。第二件事是减少对共享数据的使用，因为正如我们在上一章中所看到的，同时访问共享变量非常昂贵。其余的只是实现的问题。
- en: Unfortunately, the implementation tends to be quite difficult, and the difficulty
    increases when the desired performance gains are larger and when the hardware
    becomes more powerful. This is due to Amdahl's Law, which is something every programmer
    working with concurrency has heard about, but not everyone has understood the
    full extent of its implications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，实现往往相当困难，而且当期望的性能增益更大，硬件变得更强大时，困难程度会增加。这是由于阿姆达尔定律，这是每个处理并发的程序员都听说过的东西，但并非每个人都完全理解其影响的全部范围。
- en: 'The law itself is simple enough. It states that, for a program that has a parallel
    (scalable) part and a single-threaded part, the maximum possible speedup *s* is
    as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 法律本身非常简单。它规定，对于具有并行（可扩展）部分和单线程部分的程序，最大可能的加速度*s*如下：
- en: '![](img/Formula_1.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_1.jpg)'
- en: 'Here, ![](img/Formula_2.png) is the speedup of the parallel part of the program,
    and ![](img/Formula_3.png) is the fraction of the program that is parallel. Now
    consider the consequences for a program that is running on a large multi-processor
    system: if we have 256 processors and are able to fully utilize them except for
    a measly 1/256th of the run time, the total speedup of the program is limited
    to 128, that is, it is cut in half. In other words, if only 1/256th of the program
    is single-threaded or executed under a lock, that 256-processor system will never
    be used at more than 50% of its total capacity, no matter how much we optimize
    the rest of the program.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_2.png)是程序并行部分的加速度，![](img/Formula_3.png)是程序的并行部分。现在考虑一下对于在大型多处理器系统上运行的程序的后果：如果我们有256个处理器，并且能够除了微不足道的1/256的运行时间之外充分利用它们，那么程序的总加速度将受到限制，最多为128，也就是说，它被减半了。换句话说，如果程序只有1/256是单线程或在锁定状态下执行，那么即使我们对程序的其余部分进行了多少优化，该256处理器系统的总容量也永远不会超过50%。
- en: This is why, when it comes to developing concurrent programs, the focus of the
    design, implementation, and optimization should be on making the remaining single-threaded
    computations concurrent and on reducing the amount of time the program spends
    accessing the shared data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么在开发并发程序时，设计、实现和优化的重点应该放在使剩余的单线程计算并发化上，并减少程序访问共享数据的时间上。
- en: 'The first objective, making the computations concurrent, starts with the choice
    of the algorithms, but many design decisions influence the outcome, so we should
    learn more about it. The second one, reducing the cost of the data sharing, is
    the continuation of the theme from the last chapter: when all threads are waiting
    to access some shared variable or a lock (which is also a shared variable in itself),
    the program is effectively single-threaded only the thread that has access at
    the moment is running. This is why global locks and globally shared data are particularly
    bad for performance. But even the data shared between several threads limit the
    performance of these threads if it is accessed concurrently.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个目标，使计算并发，从选择算法开始，但许多设计决策都会影响结果，因此我们应该更多地了解它。第二个目标，减少数据共享的成本，是上一章的主题的延续：当所有线程都在等待访问某个共享变量或锁（锁本身也是一个共享变量）时，程序实际上是单线程的，只有当前具有访问权限的线程在运行。这就是为什么全局锁和全局共享数据对性能特别不利。但即使是在几个线程之间共享的数据，如果同时访问，也会限制这些线程的性能。
- en: As we have mentioned several times earlier, the need for data sharing is driven,
    fundamentally, by the nature of the problem itself. The amount of data sharing
    for any particular problem can be greatly influenced by the algorithm, the choice
    of data structures, and other design decisions, as well as by the implementation.
    Some data sharing is the artifact of the implementation or the consequence of
    the choice of the data structures, but other shared data is inherent in the problem.
    If we need to count data elements that satisfy a certain property, at the end
    of the day, there is only one count, and all threads must update it as a shared
    variable. How much sharing actually happens and what the impact is on the total
    program speedup is, however, can depend greatly on the implementation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前多次提到的，数据共享的需求基本上是由问题本身的性质驱动的。任何特定问题的数据共享量都可能受算法、数据结构的选择以及其他设计决策的极大影响，同时也受实现的影响。一些数据共享是实现的产物或者是数据结构选择的结果，但其他共享数据则是问题本身固有的。如果我们需要计算满足某种属性的数据元素的数量，最终只有一个计数，所有线程都必须将其更新为共享变量。然而，实际发生了多少共享以及对总程序加速的影响如何，这取决于实现。
- en: 'There are two tracks we will pursue in this chapter: first, given that some
    amount of data sharing is inevitable, we will look at making this process more
    efficient. Then we will consider the design and implementation techniques that
    can be used to reduce the need for data sharing or the time spent waiting for
    access to this data. We start with the first problem, efficient data sharing.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将追求两个方向：首先，鉴于某种程度的数据共享是不可避免的，我们将探讨如何使这个过程更加高效。然后，我们将考虑可以用来减少数据共享需求或减少等待访问这些数据的时间的设计和实现技术。我们首先解决第一个问题，即高效的数据共享。
- en: Locks, alternatives, and their performance
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 锁、替代方案及其性能
- en: Once we have accepted that some data sharing is going to happen, we have to
    also accept the need for the synchronization of concurrent accesses to the shared
    data. Remember that any concurrent access to the same data without such synchronization
    leads to data races and undefined behavior.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们接受了一些数据共享是不可避免的，我们也必须接受对共享数据的并发访问进行同步的需求。请记住，任何对相同数据的并发访问，如果没有这样的同步，都会导致数据竞争和未定义的行为。
- en: 'The most common way to guard shared data is with a mutex:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 保护共享数据最常见的方法是使用互斥锁：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we take advantage of the C++17 template type deduction for `std::lock_guard`;
    in C++14, we would have to specify the template type argument.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们利用了C++17模板类型推导的`std::lock_guard`；在C++14中，我们需要指定模板类型参数。
- en: 'Using mutexes is usually fairly straightforward: any code that accesses the
    shared data should be inside a critical section, that is, sandwiched between the
    calls to lock and unlock the mutex. The mutex implementation comes with the correct
    memory barriers to ensure that the code in the critical section cannot be moved
    outside of it by the hardware or the compiler (the compilers usually don''t move
    the code across the locking operations at all, but, in theory, they could do such
    optimizations as long as they respect the memory barrier semantics).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用互斥锁通常是相当简单的：任何访问共享数据的代码都应该在临界区内，也就是在锁定和解锁互斥锁的调用之间。互斥锁的实现带有正确的内存屏障，以确保临界区中的代码不能被硬件或编译器移出它（编译器通常根本不会在锁定操作之间移动代码，但理论上，它们可以进行这样的优化，只要它们遵守内存屏障的语义）。
- en: 'The question that is usually asked at this point is, "How expensive is that
    mutex?" However, the question is not well defined: we can certainly give the absolute
    answer, in nanoseconds, for a particular piece of hardware and a given mutex implementation,
    but what does this value mean? It is certainly more expensive than not having
    a mutex, but without one, the program would be incorrect (and there are easier
    ways to make incorrect programs very fast). So, "expensive" can be defined only
    in comparison with the alternatives, which naturally leads to the question, what
    are the alternatives?'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在这一点上提出的问题是：“互斥锁的成本有多高？”然而，这个问题并没有很好地定义：我们当然可以给出绝对的答案，以纳秒为单位，针对特定的硬件和给定的互斥锁实现，但这个值意味着什么？它肯定比没有互斥锁更昂贵，但没有互斥锁，程序将不正确（而且有更简单的方法使不正确的程序运行得非常快）。因此，“昂贵”只能与替代方案进行比较来定义，这自然地引出了另一个问题，那就是替代方案是什么？
- en: 'The most obvious alternative is to make the count atomic:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的替代方案是将计数设为原子的：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We also have to consider what memory order do we really need to be associated
    with operations on the count. If the count is later used to, say, index into an
    array, we probably need the release-acquire order. But if it''s just a count,
    we just want to count some events and report the number, we have no need for any
    memory order restrictions:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须考虑的是，我们真正需要与计数操作相关联的内存顺序是什么。如果计数稍后用于，比如，索引到一个数组中，我们可能需要释放-获取顺序。但如果它只是一个计数，我们只是想计算一些事件并报告数量，我们就不需要任何内存顺序限制：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Whether we actually get any barriers or not depends on the hardware: on X86,
    the atomic increment instruction had the bidirectional memory barrier "built-in,"
    and requesting the relaxed memory order is not going to make it any faster. Still,
    it is important to specify the requirement your code truly needs, both for portability
    and for clarity: remember that your real audience is not so much the compilers
    that have to parse your code but other programmers that need to read it later.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否真的得到任何屏障取决于硬件：在X86上，原子增量指令具有双向内存屏障“内置”，并且请求松散的内存顺序不会使其更快。然而，明确指定代码真正需要的要求非常重要，无论是为了可移植性还是为了清晰度：请记住，你真正的受众不是必须解析你的代码的编译器，而是需要以后阅读它的其他程序员。
- en: 'The program with the atomic increment has no locks and does not need any. However,
    it relies on a particular hardware capability: the processor has an atomic increment
    instruction. The set of such instructions is fairly small. What would we do if
    we needed an operation for which there are no atomic instructions? We don''t have
    to go very far for an example: in C++, there is no atomic multiplication (and
    I don''t know of any hardware that has such capability; certainly, it''s not found
    on X86 or ARM or any other common CPU architecture).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 具有原子增量的程序没有锁，也不需要任何锁。然而，它依赖于特定的硬件能力：处理器具有原子增量指令。这类指令的集合相当小。如果我们需要一个没有原子指令的操作，我们会怎么做？我们不必为一个例子走得太远：在C++中，没有原子乘法（我不知道有哪个硬件具有这样的能力；当然，在X86或ARM或任何其他常见的CPU架构上都找不到）。
- en: 'Fortunately, there is a kind of "universal" atomic operation that can be used
    to build, with varying degrees of difficulty, any read-modify-write operation.
    This operation is known as `compare_exchange`. It takes two parameters: the first
    one is the expected current value of the atomic variable, and the second one is
    the desired new value. If the actual current value does not match the expected
    one, nothing happens, there is no change to the atomic variable. However, if the
    current value does match the expected one, the desired value is written into the
    atomic variable. The C++ `compare_exchange` operation returns true or false to
    indicate whether the write did happen (true if it did). If the variable did not
    match the expected value, the actual value is returned in the first parameter.
    With compare-and-swap, we can implement our atomic increment operation in the
    following way:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一种“通用”的原子操作可以用来构建各种不同难度的读-修改-写操作。这个操作被称为`compare_exchange`。它有两个参数：第一个是原子变量的预期当前值，第二个是期望的新值。如果实际当前值与预期值不匹配，什么也不会发生，原子变量不会发生变化。然而，如果当前值与预期值匹配，期望的值将被写入原子变量。C++的`compare_exchange`操作返回true或false，表示写入是否发生（如果发生则为true）。如果变量与预期值不匹配，则实际值将在第一个参数中返回。通过比较和交换，我们可以以以下方式实现我们的原子增量操作：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Several notes are in order: first, the actual name of the operation in C++
    is `compare_exchange_strong.` There is also `compare_exchange_weak`; the difference
    is that the weak version can sometimes return false even when the current and
    the expected values match (on X86, it makes no difference, but on some platforms,
    the weak version can result in a faster overall operation). Second, the operation
    takes not one but two memory order arguments: the second one applies when the
    compare fails (so it is the memory order for just the comparison part of the operation).
    The first one applies when the compare succeeds and the write happens.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要注意：首先，在C++中，该操作的实际名称是`compare_exchange_strong`。还有`compare_exchange_weak`；它们的区别在于弱版本有时即使当前值和预期值匹配也可能返回false（在X86上没有区别，但在某些平台上，弱版本可能导致更快的整体操作）。其次，该操作不是一个而是两个内存顺序参数：第二个适用于比较失败时（因此它是操作的比较部分的内存顺序）。第一个适用于比较成功和写入发生时。
- en: 'Let us analyze how this implementation works. First, we atomically read the
    current value of the count, `c`. The incremented value is, of course, `c + 1`,
    but we cannot just assign it to the count because another thread could have incremented
    the count after we read it but before we update it. So we have to do a conditional
    write: if the current value of the count is still `c`, replace it with the desired
    value `c + 1`. Otherwise, update `c` with the new current value (`compare_exchange_strong`
    does that for us) and try again. The loop exits only when we finally catch a moment
    when the atomic variable did not change between the time we last read it and the
    time we''re trying to update it. Of course, there is no reason to do any of this
    to increment the count when we have the atomic increment operation. But this approach
    can be generalized to any computation: instead of `c + 1`, we could use any other
    expression, and the program would work the same way.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下这个实现是如何工作的。首先，我们原子地读取计数的当前值`c`。递增的值当然是`c + 1`，但我们不能简单地将其分配给计数，因为另一个线程在我们读取它之后但在我们更新它之前可能已经递增了计数。因此，我们必须进行条件写入：如果计数的当前值仍然是`c`，则用期望的值`c
    + 1`替换它。否则，用新的当前值更新`c`（`compare_exchange_strong`为我们做到了这一点），然后重试。只有当我们最终捕捉到一个时刻，即原子变量在我们最后一次读取它和我们尝试更新它之间没有发生变化时，循环才会退出。当然，当我们有原子增量操作时，没有理由做任何这些来增加计数。但这种方法可以推广到任何计算：我们可以使用任何其他表达式而不是`c
    + 1`，程序仍然可以正常工作。
- en: While all three versions of the code do the same operation, increment the count,
    there are fundamental differences between them that we must explore in more detail.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代码的三个版本都执行相同的操作，即增加计数，但它们之间存在根本的区别，我们必须更详细地探讨这些区别。
- en: Lock-based, lock-free, and wait-free programs
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于锁的、无锁的和无等待的程序
- en: 'The first version, with the mutex, is the simplest to understand: one thread
    can hold the lock at any time, so that thread can increment the count without
    any further precautions. Once the lock is released, another thread can acquire
    it and increment the count, and so on. At any time, at most one thread can hold
    the lock and make any progress; all remaining threads that need the access are
    waiting on the lock. But even the thread that has the lock is not guaranteed to
    proceed forward, in general: if it needs access to another shared variable before
    it can complete its job, it may be waiting on that lock, which is held by some
    other thread. This is the common lock-based program, often not the fastest, but
    the easiest to understand and reason about.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个版本，使用互斥锁，是最容易理解的：任何时候只有一个线程可以持有锁，因此该线程可以增加计数而无需进一步预防措施。一旦锁被释放，另一个线程可以获取它并增加计数，依此类推。在任何时候，最多只有一个线程可以持有锁并取得任何进展；所有需要访问的剩余线程都在等待锁。但即使持有锁的线程通常也不能保证向前进行：如果它在完成工作之前需要访问另一个共享变量，它可能在等待由其他线程持有的锁。这是常见的基于锁的程序，通常不是最快的，但是最容易理解和推理。
- en: 'The second program presents a very different scenario: any thread that arrives
    at the atomic increment operation executes it without delay. Of course, the hardware
    itself must lock access to the shared data to ensure the atomicity of the operations
    (as we have seen in the last chapter, this is done by granting exclusive access
    to the entire cache line to one processor at a time). From the programmer''s point
    of view, this exclusive access manifests itself as an increase in the time it
    takes to execute the atomic operation. However, in the code itself, there is no
    waiting for anything, no trying and retrying. This kind of program is called **wait-free**.
    In a wait-free program, all threads are making progress, that is, executing operations,
    at all times (although some operations may take longer if there is severe contention
    between threads for access to the same shared variable). A wait-free implementation
    is usually possible only for very simple operations (such as incrementing a count),
    but whenever it is available, it is often even simpler than the lock-based implementation.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个程序呈现了一个非常不同的情景：到达原子增量操作的任何线程都会立即执行它。当然，硬件本身必须锁定对共享数据的访问，以确保操作的原子性（正如我们在上一章中所看到的，这是通过一次只向一个处理器授予对整个缓存行的独占访问来实现的）。从程序员的角度来看，这种独占访问表现为执行原子操作所需的时间增加。然而，在代码本身中，没有等待任何东西，也没有尝试和重试。这种程序被称为**无等待**。在无等待程序中，所有线程始终在取得进展，也就是执行操作（尽管如果线程之间为了访问相同的共享变量而存在严重争用，一些操作可能需要更长时间）。无等待实现通常只适用于非常简单的操作（例如增加计数），但每当它可用时，通常甚至比基于锁的实现更简单。
- en: 'It takes a bit more effort to understand the behavior of the last program.
    There are no locks; however, there is a loop that is repeated an unknown number
    of times. In this regard, the implementation is similar to the lock: any thread
    waiting on a lock is also stuck in a similar loop, trying and failing to acquire
    the lock. However, there is one key difference: in a lock-based program, when
    a thread has failed to acquire the lock and must try again, we can deduce that
    some other thread has the lock. We cannot be sure whether that thread is going
    to release the lock any time soon or that it, in fact, is making any progress
    toward completing its work and releasing the lock it holds (it may, for example,
    be waiting for a user to input something). In the program based on compare-and-swap,
    the only way our thread can fail to update the shared count is because some other
    thread updated it first. Therefore, we know that, of all threads trying to increment
    the count at the same time, at least one will always succeed. This kind of program
    is known as **lock-free**.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 理解最后一个程序的行为需要更多的努力。没有锁；然而，有一个重复未知次数的循环。在这方面，实现类似于锁：任何等待锁的线程也被困在类似的循环中，试图并失败地获取锁。然而，有一个关键的区别：在基于锁的程序中，当一个线程未能获取锁并且必须重试时，我们可以推断其他线程持有锁。我们无法确定该线程是否会很快释放锁，或者实际上是否在完成工作并释放它持有的锁（例如，它可能正在等待用户输入）。在基于比较和交换的程序中，我们的线程失败更新共享计数的唯一方式是因为其他线程首先更新了它。因此，我们知道，在同时尝试增加计数的所有线程中，至少有一个始终会成功。这种程序被称为**无锁**。
- en: 'We have just seen examples of the three main types of concurrent programs:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了三种主要类型的并发程序的示例：
- en: In a wait-free program, each thread is executing the operations it needs and
    is always making progress toward the final goal; there is no waiting for access,
    and no work needs to be redone.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在无等待程序中，每个线程都在执行它需要的操作，并始终朝着最终目标取得进展；没有等待访问，也不需要重新做任何工作。
- en: In a lock-free program, multiple threads may be trying to update the same shared
    value, but only one of them will succeed. The rest will have to discard the work
    they have already done based on the original value, read the updated value, and
    do the computation again. But at least one thread is always guaranteed to commit
    its work and not have to redo it; thus, the entire program is always making progress,
    although not necessarily at full speed.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在无锁程序中，多个线程可能尝试更新相同的共享值，但只有一个线程会成功。其余的将不得不丢弃他们已经基于原始值完成的工作，读取更新后的值，并重新计算。但至少有一个线程始终保证提交其工作并且不必重新做；因此，整个程序始终在取得进展，尽管不一定以最快的速度。
- en: Finally, in a lock-based program, one thread is holding the lock that gives
    it access to the shared data. Just because it's holding the lock does not mean
    it's doing anything with this data, though. So, when the concurrent access happens,
    at most one thread is making progress, but even that is not guaranteed.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，在基于锁的程序中，一个线程持有可以访问共享数据的锁。但仅仅因为它持有锁并不意味着它正在处理这些数据。因此，当并发访问发生时，最多只有一个线程在取得进展，但这也不是保证的。
- en: 'The difference between the three programs is clear, in theory. But, I bet every
    reader wants to know the answer to the same question: which one is faster? We
    can run each version of the code inside a Google benchmark. For example, here
    is the lock-based version:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，这三个程序之间的差异是明显的。但我打赌每个读者都想知道同一个问题的答案：哪一个更快？我们可以在Google基准测试中运行代码的每个版本。例如，这是基于锁的版本：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The variables that must be shared between threads are declared at the global
    scope. The initial setup, if any, can be restricted to just one thread. Other
    benchmarks are similar; only the measured code changes. Here is the result:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 必须在全局范围内声明在线程之间必须共享的变量。初始设置（如果有的话）可以限制在一个线程中。其他基准测试类似；只有被测量的代码会发生变化。以下是结果：
- en: '![Figure 6.1 – Performance of a shared count increment: mutex-based, lock-free
    (compare-and-swap, or CAS), and wait-free (atomic)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1 - 共享计数增量的性能：基于互斥锁，无锁（比较和交换，或CAS），无等待（原子）'
- en: '](img/Figure_6.1_B16229.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_6.1_B16229.jpg)'
- en: 'Figure 6.1 – Performance of a shared count increment: mutex-based, lock-free
    (compare-and-swap, or CAS), and wait-free (atomic)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 - 共享计数增量的性能：基于互斥锁，无锁（比较和交换，或CAS），无等待（原子）
- en: The only result that may be unexpected here is just how badly the lock-based
    version is performing. However, this is a data point, not the whole story. In
    particular, while all mutexes are locks, not all locks are mutexes. We can attempt
    to come up with a more efficient lock implementation (at least, more efficient
    for our needs).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里唯一可能出乎意料的结果是基于锁的版本表现得有多糟糕。然而，这只是一个数据点，而不是整个故事。特别是，虽然所有互斥锁都是锁，但并非所有锁都是互斥锁。我们可以尝试提出更有效的锁实现（至少对我们的需求来说更有效）。
- en: Different locks for different problems
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同的问题需要不同的锁
- en: We have just seen that a standard C++ mutex performs very poorly when it is
    used to guard access to a shared variable, especially when there are many threads
    trying to modify this variable at the same time (if all threads were reading the
    variable, we would not need to guard it at all; concurrent read-only access does
    not lead to any data races). But is the lock inefficient because of its implementation,
    or is the problem inherent in the nature of the lock? From what we learned in
    the previous chapter, we can expect any lock to be somewhat less efficient than
    the atomically incremented counter simply because a lock-based scheme uses two
    shared variables, the lock and the count, versus just one, shared variable for
    an atomic counter. However, the mutexes provided by the operating system are usually
    not particularly efficient for locking very short operations such as our count
    increment.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到，当使用标准的C++互斥锁来保护对共享变量的访问时，其性能非常差，特别是当有许多线程同时尝试修改此变量时（如果所有线程都在读取变量，则根本不需要保护它；并发只读访问不会导致任何数据竞争）。但是，锁的效率低是因为其实现，还是因为锁的性质固有的问题？根据我们在上一章中学到的知识，我们可以预期任何锁都会比原子递增计数器要低效一些，因为基于锁的方案使用了两个共享变量，即锁和计数器，而原子计数器只使用了一个共享变量。然而，操作系统提供的互斥锁通常对于锁定非常短的操作（比如我们的计数增量）并不特别高效。
- en: 'The simplest and one of the most efficient locks for this situation is a basic
    spinlock. The idea of the spinlock is this: the lock itself is just a flag that
    can have two values, let''s say 0 and 1\. If the value of the flag is 0, the lock
    is not locked. Any thread that sees this value can set the flag to 1 and proceed;
    of course, the entire operation to read the flag and set it to 1 has to be a single
    atomic operation. Any thread that sees the value of 1 must wait until the value
    changes back to 0 to indicate that the lock is available. Finally, when a thread
    that changed the flag from 0 to 1 is ready to release the lock, it changes the
    value back to 0.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，最简单且最有效的锁之一是基本自旋锁。自旋锁的想法是：锁本身只是一个标志，可以有两个值，比如0和1。如果标志的值为0，则锁未被锁定。任何看到这个值的线程都可以将标志设置为1并继续；当然，读取标志并将其设置为1的整个操作必须是一个单一的原子操作。任何看到值为1的线程都必须等待，直到值再次变为0，表示锁可用。最后，当将标志从0更改为1的线程准备释放锁时，将值再次更改为0。
- en: 'The code to implement this lock looks like this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 实现此锁的代码如下：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We show only the locking and unlocking functions in the code snippet; the class
    also needs the default constructor (an atomic integer is initialized to 0 in its
    own default constructor), as well as the declarations that make it non-copyable.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只在代码片段中显示了锁定和解锁函数；该类还需要默认构造函数（原子整数在其默认构造函数中初始化为0），以及使其不可复制的声明。
- en: 'Note that locking the flag does not use a conditional exchange: we always write
    1 into the flag. The reason it works is that, if the original value of the flag
    was 0, the exchange operation sets it to 1 and returns 0 (and the loop ends),
    which is what we want. But if the original value was 1, it is replaced by 1, that
    is, does not change at all.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，锁定标志不使用条件交换：我们总是将1写入标志。它能够工作的原因是，如果标志的原始值为0，则交换操作将其设置为1并返回0（循环结束），这正是我们想要的。但是，如果原始值为1，则它被替换为1，也就是根本没有变化。
- en: 'Also, note the two memory barriers: locking is accompanied by the acquire barrier,
    while unlocking is done with the release barrier. Together, these barriers delimit
    the critical section and ensure that any code written between the calls to `lock()`
    and `unlock()` stays there.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意两个内存屏障：锁定伴随着获取屏障，而解锁则使用释放屏障。这些屏障一起限定了临界区，并确保在调用`lock()`和`unlock()`之间编写的任何代码都留在那里。
- en: 'You may be expecting to see the comparison benchmark of this lock versus the
    standard mutex, but we are not going to show it: the performance of this spinlock
    is terrible. To make it useful, it needs several optimizations.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能期望看到此锁与标准互斥锁的比较基准，但我们不打算展示它：这个自旋锁的性能很糟糕。为了使其有用，需要进行几项优化。
- en: 'First of all, note that if the value of the flag is 1, we don''t actually need
    to replace it with 1, we can just leave it alone. Why does it matter? The exchange
    is a read-modify-write operation. Even if it changes the old value to the same
    value, it needs exclusive access to the cache line containing the flag. We don''t
    need exclusive access to just read the flag. This matters in the following scenario:
    a lock is locked, the thread that has the lock is not changing it (it is busy
    doing its work), but all other threads are checking the lock and waiting for the
    value to change to 0\. If they do not try to write into the flag, the cache line
    does not need to bounce between different CPUs: they all have the same copy of
    the memory in their caches, and this copy is current, no need to send any data
    anywhere. Only when one of the threads actually changes the value does the hardware
    need to send the new content of the memory to all CPUs. Here is the optimization
    we just described, done in code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，如果标志的值为1，我们实际上不需要将其替换为1，我们可以让它保持不变。为什么这很重要？交换是一个读-修改-写操作。即使它将旧值更改为相同的值，它也需要独占访问包含标志的缓存行。我们不需要独占访问只是为了读取标志。这在以下情况下很重要：锁定了一个锁，拥有锁的线程没有改变它（它正忙于工作），但所有其他线程都在检查锁，并等待值更改为0。如果它们不尝试写入标志，缓存行就不需要在不同的CPU之间反弹：它们都有内存的相同副本在它们的缓存中，并且这个副本是当前的，不需要将任何数据发送到任何地方。只有当其中一个线程实际更改值时，硬件才需要将内存的新内容发送到所有CPU。这是我们刚刚描述的优化，在代码中完成：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The optimization here is that we first read the flag until we see 0, then we
    swap it with 1\. The value could have changed to 1 between the time we did the
    check and the time we did the exchange if another thread got the lock first. Also,
    note that, when pre-checking the flag, we don't care about the memory barrier
    at all since the final definitive check is always done using the exchange and
    its memory barrier.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的优化是，我们首先读取标志，直到看到0，然后将其与1交换。如果另一个线程首先获得了锁，那么在我们进行检查和交换之间，值可能已经更改为1。另外，请注意，在预先检查标志时，我们根本不关心内存屏障，因为最终的确定性检查总是使用交换及其内存屏障完成。
- en: Even with this optimization, the lock performs pretty poorly. The reason has
    to do with the way the operating systems tend to prioritize threads. In general,
    a thread that is doing heavy computing will get more CPU time on the assumption
    that it's doing something useful. Unfortunately, in our case, the most heavily
    computing thread is the one hammering on the flag while waiting for it to change.
    This can lead to an undesirable situation where one thread is trying to get the
    lock and has the CPU allocated to it, while another thread would like to release
    the lock but doesn't get scheduled for execution for some time. The solution is
    for the waiting thread to give up the CPU after several attempts, so some other
    thread can run and, hopefully, finish its work and release the lock.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 即使进行了这种优化，锁的性能仍然相当差。原因在于操作系统倾向于优先考虑线程的方式。一般来说，进行大量计算的线程将获得更多的CPU时间，因为假设它正在做一些有用的事情。不幸的是，在我们的情况下，最大量计算的线程是在等待标志改变时不断尝试获取锁。这可能导致一种不良情况，即一个线程试图获取锁并且已经分配了CPU给它，而另一个线程想要释放锁，但却没有被调度执行一段时间。解决方法是让等待的线程在多次尝试后放弃CPU，以便其他线程可以运行，并且希望完成它的工作并释放锁。
- en: 'There are several ways for a thread to release the control on the CPU; most
    are done by a system function call. There isn''t a universal best way to do so.
    Experimentally, on Linux, a call to sleep for a very short time (1 nanosecond)
    by calling `nanosleep()` seems to yield the best results, usually better than
    a call to `sched_yield()`, which is another system function to yield CPU access.
    All system calls are expensive compared to hardware instructions, so you don''t
    want to call them too often. The best balance is achieved when we try to get the
    lock several times, then yield the CPU to another thread, then try again:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 线程释放CPU的方式有几种，大多数是通过系统函数调用完成的。没有一种通用的最佳方法。在Linux上，通过调用`nanosleep()`似乎能够产生最佳结果，通常比调用`sched_yield()`更好，后者是另一个系统函数，用于让出CPU访问权。所有系统调用与硬件指令相比都很昂贵，因此不要经常调用它们。最佳平衡是当我们尝试多次获取锁，然后将CPU让给另一个线程，然后再次尝试：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The optimal number of attempts to acquire the lock before releasing the CPU
    will depend on the hardware and the number of threads, but generally, values between
    8 and 16 work well.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在释放CPU之前获取锁的最佳尝试次数取决于硬件和线程数量，但通常，8到16之间的值效果很好。
- en: 'Now we are ready for the second round of benchmarks, and here is the result:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备进行第二轮基准测试，以下是结果：
- en: '![Figure 6.2 – Performance of a shared count increment: spinlock-based, lock-free
    (compare-and-swap, or CAS), and wait-free (atomic)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2 - 共享计数增量的性能：基于自旋锁、无锁（比较和交换，或CAS）和无等待（原子）的性能比较'
- en: '](img/Figure_6.2_B16229.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_6.2_B16229.jpg)'
- en: 'Figure 6.2 – Performance of a shared count increment: spinlock-based, lock-free
    (compare-and-swap, or CAS), and wait-free (atomic)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 - 共享计数增量的性能：基于自旋锁、无锁（比较和交换，或CAS）和无等待（原子）的性能比较
- en: 'The spinlock has done very well: it is soundly outperforming the compare-and-swap
    implementation and gives the wait-free operation tough competition.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 自旋锁表现非常出色：它明显优于比较和交换实现，并给无等待操作带来了激烈的竞争。
- en: 'These results leave us with two questions: first, why don''t all locks use
    spinlocks if they are so much faster? Second, why do we even need the atomic operations
    if the spinlock is so good (other than for implementing the lock, of course)?'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果给我们留下了两个问题：首先，如果自旋锁如此快，为什么不所有的锁都使用自旋锁？其次，如果自旋锁如此出色，为什么我们甚至需要原子操作（除了用于实现锁之外）？
- en: 'The answer to the first question boils down to the title of this section: different
    locks for different problems. The downside of the spinlock is that the waiting
    thread continuously uses the CPU or is "busy waiting." On the other hand, the
    thread waiting on a system mutex is mostly idle (sleeping). Busy waiting is great
    if you need to wait for a few cycles, the duration of an increment operation:
    it''s much faster than putting the thread to sleep. On the other hand, if the
    locked computation consists of more than a handful of instructions, the threads
    waiting on the spinlock waste a lot of CPU time and deprive the other working
    threads of access to the hardware resources they need. Overall, the C++ mutex
    (`std::mutex`) or the OS mutex is usually chosen for its balance: it''s somewhat
    inefficient for locking a single instruction, it''s OK for locking a computation
    that takes dozens of nanoseconds, and it beats the alternative if we need to hold
    the lock for a long time (long is relative here, processors are fast, so 1 millisecond
    is very long). Now, we are writing about extreme performance (and the extreme
    efforts to achieve it) here, so most HPC programmers either implement their own
    fast locks for guarding short computations or use a library that provides them.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对第一个问题的答案归结为本节的标题：不同的问题需要不同的锁。自旋锁的缺点是等待线程不断使用CPU或“忙等待”。另一方面，等待系统互斥锁的线程大部分时间处于空闲状态（睡眠）。如果需要等待几个周期，例如增量操作的持续时间，忙等待是很好的选择：它比让线程进入睡眠状态要快得多。另一方面，如果锁定的计算包含超过几条指令，那么等待自旋锁的线程将浪费大量的CPU时间，并且剥夺其他工作线程访问它们所需的硬件资源。总的来说，C++互斥锁（`std::mutex`）或操作系统互斥锁通常被选择是因为它的平衡性：对于锁定单个指令来说效率不高，对于需要几十纳秒的计算来说还可以，如果需要长时间持有锁，它比替代方案更好（长时间在这里是相对的，处理器速度很快，所以1毫秒就是很长的时间）。现在，我们在这里写的是极端性能（以及为实现它所做的极端努力），所以大多数高性能计算程序员要么实现自己的快速锁来保护短计算，要么使用提供这些锁的库。
- en: The second question, "Is there any other downside to the locks?" takes us to
    the next section.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题，“锁还有其他缺点吗？”将我们带到下一节。
- en: Lock-based versus lock-free, what is the real difference?
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于锁定与无锁定，真正的区别是什么？
- en: 'When the conversation turns to the advantages of lock-free programming, the
    first argument is usually "it is faster." As we have just seen, this is not necessarily
    true: lock implementations can be very efficient if optimized for a particular
    task. However, there are other disadvantages that are inherent in the lock-based
    approach and do not depend on the implementation.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈论无锁编程的优势时，第一个论点通常是“它更快”。正如我们刚才看到的，这并不一定是真的：如果针对特定任务进行了优化，锁的实现可以非常高效。然而，锁定方法的另一个固有的缺点并不取决于实现。
- en: The first and the most infamous is the possibility of the dreaded deadlock.
    The deadlock occurs when the program uses several locks, let's say lock1 and lock2\.
    Thread A has lock1 and needs to acquire lock2\. Thread B already has lock2 and
    needs to acquire lock1\. Neither thread can proceed, and both will wait forever
    because the only thread that can release the lock they need is itself blocked
    on a lock.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个也是最臭名昭著的是可怕的死锁的可能性。当程序使用多个锁时，比如lock1和lock2时，死锁发生。线程A持有lock1并需要获取lock2。线程B已经持有lock2并需要获取lock1。两个线程都无法继续进行，并且都将永远等待，因为唯一能释放它们需要的锁的线程本身也被锁定。
- en: 'If both locks are acquired at the same time, the deadlock can be avoided if
    the locks are always acquired in the same order; C++ has a utility function for
    this purpose, `std::lock()`. However, often locks cannot be acquired at the same
    time: when thread A acquired lock1, there was no way to know that we will need
    lock2 as well since that information itself was hidden in the data that is guarded
    by lock1\. We will see examples later in the next chapter when we talk about concurrent
    data structures.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个锁同时被获取，死锁可以通过始终以相同的顺序获取锁来避免；C++有一个用于此目的的实用函数`std::lock()`。然而，通常无法同时获取锁：当线程A获取lock1时，无法知道我们将需要lock2，因为这个信息本身是隐藏在由lock1保护的数据中。我们将在下一章中讨论并发数据结构时，在后面的例子中看到。
- en: 'If we cannot reliably acquire multiple locks, perhaps the solution is to try
    to acquire them, then, if we fail to get them all, release the locks we already
    hold so the other thread can get them? In our example, thread A holds lock1, it
    would try to get lock2 as well but without blocking: most locks have a `try_lock()`
    call that either acquires the lock or returns false. In the latter case, thread
    A releases lock1 and tries to lock them both again. This might work, especially
    in a simple test. But it has a danger of its own: the livelock, when two threads
    constantly pass locks to each other: thread A has lock1 but not lock2, thread
    B has lock2, gives it up, gets lock1, now it can''t get lock2 back because thread
    A has it. There are algorithms for acquiring multiple locks that guarantee success,
    eventually. Unfortunately, in practice, a long time may pass between now and eventually.
    These algorithms are also quite complex.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们无法可靠地获取多个锁，也许解决方案是尝试获取它们，然后，如果我们未能全部获取它们，释放我们已经持有的锁，以便其他线程可以获取它们？在我们的示例中，线程A持有lock1，它将尝试获取lock2，但不会阻塞：大多数锁都有一个`try_lock()`调用，它要么获取锁，要么返回false。在后一种情况下，线程A释放lock1，然后再次尝试同时锁定它们。这可能有效，特别是在简单的测试中。但它也有自己的危险：活锁，当两个线程不断地相互传递锁：线程A持有lock1但没有lock2，线程B持有lock2，放弃它，获取lock1，现在它无法再获取lock2，因为线程A已经持有它。有一些算法可以保证最终成功获取多个锁。不幸的是，在实践中，现在和最终之间可能会经过很长时间。这些算法也非常复杂。
- en: 'The fundamental problem of dealing with multiple locks is that the mutexes
    are not composable: there is no good way to combine two or more locks into one.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 处理多个锁的基本问题是互斥锁不可组合：没有好的方法将两个或多个锁合并为一个。
- en: 'Even without the dangers of the livelock and the deadlock, lock-based programs
    suffer from other problems. One of the more frequent ones and one that is hard
    to diagnose is called **convoying**. It can happen with multiple locks or just
    one lock. Convoying looks like this: say we have a computation that is protected
    by a lock. Thread A currently has the lock and is doing its work on the shared
    data; other threads are waiting to do their part of the work. However, the work
    is not a one-shot deal: each thread has many tasks to do, and a part of each task
    requires exclusive access to the shared data. Thread A finishes one task, releases
    the lock, then zips through the next task until it gets to the point where it
    needs the lock again. The lock was released, any other thread can get it, but
    they are still waking up, whereas thread A is "hot" on the CPU. So, thread A gets
    the lock again simply because the competition is not ready for it. The tasks of
    thread A rush through the execution like trucks in a convoy, while nothing gets
    done on other threads.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有活锁和死锁的危险，基于锁的程序仍然存在其他问题。其中一个更频繁且难以诊断的问题称为**护航**。它可能发生在多个锁或只有一个锁的情况下。护航的情况是这样的：假设我们有一个由锁保护的计算。线程A当前持有锁并在共享数据上进行工作；其他线程正在等待进行他们的工作。然而，工作不是一次性的：每个线程有许多任务要做，每个任务的一部分需要对共享数据进行独占访问。线程A完成一个任务，释放锁，然后快速进行下一个任务，直到再次需要锁。锁已经被释放，任何其他线程都可以获取它，但它们仍在唤醒，而线程A正在CPU上“热”。因此，线程A再次获取锁只是因为竞争者还没有准备好。线程A的任务像车队一样快速执行，而其他线程上什么也没做。
- en: 'Yet another problem with locks is that they do not respect any notion of priority:
    a low-priority thread that is currently holding the lock will preempt any high-priority
    thread that needs the same lock. The high-priority thread thus has to wait for
    as long as the low-priority thread determines, the situation that seems entirely
    inconsistent with the notion of high priority. For this reason, this scenario
    is sometimes called **priority inversion**.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 锁的另一个问题是它们不尊重任何优先级的概念：当前持有锁的低优先级线程将抢占任何需要相同锁的高优先级线程。因此，高优先级线程必须等待低优先级线程确定的时间，这种情况似乎与高优先级的概念完全不一致。因此，这种情况有时被称为**优先级反转**。
- en: 'Now that we understand that the problems with locks are not limited to performance,
    let''s see how a lock-free program would fare with regard to the same complications.
    First of all, in a lock-free program, at least one thread is guaranteed to not
    be blocked: in the worst-case scenario, when all threads arrive at a **compare-and-swap**
    (**CAS**) operation simultaneously and with the same expected current value of
    the atomic variable, one of them is guaranteed to see the expected value (since
    the only way it can change is via a successful CAS operation). All the remaining
    threads will have to discard their computation results, reload the atomic variable,
    and repeat the computation, but the one thread that succeeded on the CAS can move
    to the next task. This prevents the possibility of a deadlock. Without the deadlock
    and the attempts to avoid it, we do not need to worry about the livelock either.
    Since all threads are busy computing their way toward the atomic operation (such
    as CAS), the high-priority thread is more likely to get there first and commit
    its results, while the low-priority thread is more likely to fail the CAS and
    have to redo its work. Similarly, a single success in committing the results does
    not position the "winning" thread for any advantage over all the other threads:
    whichever thread is ready to attempt to execute CAS first is the one that succeeds.
    This naturally eliminates the convoying.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们明白了锁的问题不仅限于性能，让我们看看无锁程序在同样的复杂情况下会表现如何。首先，在无锁程序中，至少有一个线程保证不会被阻塞：在最坏的情况下，当所有线程同时到达一个**比较和交换**（**CAS**）操作，并且期望的当前原子变量值相同时，其中一个线程保证会看到期望的值（因为它可以改变的唯一方式是通过成功的CAS操作）。所有剩下的线程将不得不丢弃他们的计算结果，重新加载原子变量，并重复计算，但成功进行CAS的一个线程可以继续下一个任务。这可以防止死锁的可能性。没有死锁和避免死锁的尝试，我们也不需要担心活锁。由于所有线程都在忙于计算通向原子操作（如CAS）的方式，高优先级线程更有可能首先到达并提交其结果，而低优先级线程更有可能失败CAS并不得不重新做工作。同样，单个成功提交结果并不会使“获胜”的线程对其他所有线程有任何优势：准备尝试执行CAS的线程是成功的。这自然地消除了护航。
- en: 'So, what''s not to like about lock-free programming, then? Just two drawbacks,
    but they are major ones. The first is the flip side of its advantages: as we said,
    even the threads that fail their CAS attempts stay busy. This solves the priority
    problem, but at a very high cost: in the case of high contention, a lot of CPU
    time is wasted doing the work only to have it redone. Worse, these threads competing
    for access to the single atomic variable are taking away the CPU resources from
    other threads that are doing some unrelated computations at the same time.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，无锁编程有什么不好呢？只有两个缺点，但它们都是主要的。第一个是它的优点的反面：正如我们所说，即使失败了CAS尝试的线程也保持忙碌。这解决了优先级问题，但代价非常高：在高争用情况下，大量的CPU时间被浪费在做工作，只是为了重新做。更糟糕的是，竞争访问单个原子变量的这些线程正在从其他同时进行一些不相关计算的线程中夺走CPU资源。
- en: 'The second drawback is of an entirely different nature. While most concurrent
    programs are not easy to write or understand, lock-free programs are incredibly
    difficult to design and implement correctly. A lock-based program just has to
    guarantee that any set of operations that constitutes a single logical transaction
    is executed under a lock. It gets harder when there are multiple logical transactions
    such that some, but not all, shared data is common to several different transactions.
    That is how we arrive at the problem of multiple locks. Still, reasoning about
    the correctness of a lock-based program is not that difficult: if I see a piece
    of shared data in your code, you must show me which lock guards this data and
    prove that no thread can access this data without acquiring this lock first. If
    this is not so, you have a data race, even if you haven''t found it yet. If these
    requirements are met, you do not have data races (although you may have deadlocks
    and other problems).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个缺点完全不同。虽然大多数并发程序不容易编写或理解，但无锁程序设计和实现起来非常困难。基于锁的程序只需保证构成单个逻辑事务的任何操作集在锁下执行。当存在多个逻辑事务时，某些但不是所有共享数据是几个不同事务共有的时，情况就会变得更加困难。这就是我们遇到多个锁的问题。尽管如此，推理基于锁的程序的正确性并不那么困难：如果我在你的代码中看到一块共享数据，你必须向我展示哪个锁保护了这些数据，并证明没有线程可以在未先获取此锁的情况下访问这些数据。如果不是这样，你就会出现数据竞争，即使你还没有发现它。如果满足这些要求，就不会出现数据竞争（尽管可能会出现死锁和其他问题）。
- en: Lock-free programs, on the other hand, have an almost infinite variety of data
    synchronization schemes. Since no thread is ever paused, we have to convince ourselves
    that, no matter the order in which the threads execute the atomic operations,
    the result is correct. Moreover, without the benefit of a clearly defined critical
    section, we have to worry about the memory order and the visibility of all the
    data in the program, not just the atomic variables. We have to ask ourselves,
    is there any way one thread can change the data, and the other thread can see
    the old version of it because the memory order requirements are not strict enough?
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，无锁程序有几乎无限种类的数据同步方案。由于没有线程会被暂停，我们必须确信，无论线程以何种顺序执行原子操作，结果都是正确的。此外，没有明确定义的临界区，我们必须担心程序中所有数据的内存顺序和可见性，而不仅仅是原子变量。我们必须问自己，有没有一种方法可以使一个线程更改数据，而另一个线程可以看到旧版本，因为内存顺序要求不够严格？
- en: 'The usual solution to the problem of complexity is modularization and encapsulation.
    We collect the difficult code into modules where each one has a well-defined interface
    and a clear set of requirements and guarantees. A lot of attention is paid to
    the modules that implement various concurrent algorithms. This book takes you
    in a different direction: the rest of the chapter is dedicated instead to concurrent
    data structures.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 解决复杂性问题的常规方法是模块化和封装。我们将困难的代码收集到模块中，每个模块都有明确定义的接口和一组清晰的要求和保证。对实现各种并发算法的模块进行了大量关注。本书将带您走向不同的方向：本章的其余部分专门讨论并发数据结构。
- en: Building blocks for concurrent programming
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发编程的构建模块
- en: 'The development of concurrent programs is, generally, quite difficult. Several
    factors can make it even more difficult: for example, it is much harder to write
    concurrent programs that also need to be correct and efficient (in other words,
    all of them). Complex programs with many mutexes, or lock-free programs, are harder
    still.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 并发程序的开发通常非常困难。有几个因素可能使其变得更加困难：例如，编写需要正确和高效的并发程序要困难得多（换句话说，所有这些都是）。具有许多互斥锁或无锁程序的复杂程序更加困难。
- en: As was said at the conclusion of the last section, the only hope of managing
    this complexity is to corral it into small, well-defined sections of the code,
    or modules. As long as the interfaces and requirements are clear, the clients
    of these modules don't need to know whether the implementation is lock-free or
    lock-based. It does affect the performance, so the module may be too slow for
    a particular need until it's optimized, but we do these optimizations as needed,
    and they are confined to the particular module.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上一节的结论所说，管理这种复杂性的唯一希望是将其限制在代码或模块的小而明确定义的部分中。只要接口和要求清晰，这些模块的客户端就不需要知道实现是无锁还是基于锁的。这会影响性能，因此模块可能对特定需求太慢，直到优化为止，但我们会根据需要进行这些优化，并且这些优化限于特定模块。
- en: 'In this chapter, we focus on the modules that implement data structures for
    concurrent programming. Why data structures and not algorithms? First of all,
    there is much more literature on concurrent algorithms out there. Second, most
    programmers have a much easier time dealing with the algorithms: the code gets
    profiled, there is a function that takes an excessively long time, we find a different
    way to implement the algorithm and move on to the next high pole on the performance
    chart. Then you end up with a program where no single computation takes a large
    portion of time, but you still have this feeling that it''s nowhere as fast as
    it should be. We have said it before, but it bears repeating: when you have no
    hot code, you probably have hot data.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于实现并发编程数据结构的模块。为什么是数据结构而不是算法？首先，关于并发算法的文献要多得多。其次，大多数程序员更容易处理算法：代码进行了分析，有一个花费过长时间的函数，我们找到了另一种实现算法的方法，然后转向性能图表上的下一个高点。然后，您最终得到一个程序，其中没有任何单个计算占用大部分时间，但您仍然感觉它的速度远不及应有的水平。我们之前已经说过，但需要重复一遍：当您没有热点代码时，您可能有热点数据。
- en: The data structures play an even more important role in concurrent programs
    because they determine what guarantees the algorithms can rely on and what the
    restrictions are. Which concurrent operations can be done safely on the same data?
    How consistent is the view of the data as seen by different threads? We cannot
    write much code if we don't have answers to these questions, and the answers are
    determined by our choice of the data structures.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 数据结构在并发程序中扮演着更加重要的角色，因为它们决定了算法可以依赖的保证和限制。哪些并发操作可以安全地在相同的数据上进行？不同线程看到的数据视图有多一致？如果我们没有这些问题的答案，我们就不能写太多的代码，而这些答案是由我们选择的数据结构决定的。
- en: At the same time, the design decisions, such as the choice of interfaces and
    module boundaries, can critically impact the choices we can make when writing
    concurrent programs. Concurrency cannot be added to a design as an afterthought;
    the design has to be drawn up with the concurrency in mind from the very beginning,
    especially the organization of the data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，设计决策，比如接口和模块边界的选择，可以在编写并发程序时对我们的选择产生关键影响。并发不能作为事后的想法添加到设计中；设计必须从一开始就考虑并发，特别是数据的组织。
- en: We begin the exploration of the concurrent data structures by defining a few
    basic terms and concepts.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过定义一些基本术语和概念来开始探索并发数据结构。
- en: The basics of concurrent data structures
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并发数据结构的基础
- en: 'Concurrent programs that use multiple threads need thread-safe data structures.
    This seems obvious enough. But what is thread safety, and what makes a data structure
    thread-safe? At first glance, it seems simple: if a data structure can be used
    by multiple threads at the same time without any data races (shared between threads),
    then it is thread-safe.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个线程的并发程序需要线程安全的数据结构。这似乎是显而易见的。但什么是线程安全，什么使一个数据结构是线程安全的？乍一看，这似乎很简单：如果一个数据结构可以被多个线程同时使用而不会发生任何数据竞争（在线程之间共享），那么它就是线程安全的。
- en: 'However, this definition turns out to be too simplistic:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个定义结果太过简单：
- en: It raises the plank very high – for example, none of the STL containers would
    be considered thread-safe.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它把标准提得很高——例如，STL容器中的任何一个都不会被认为是线程安全的。
- en: It carries a very high performance cost.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它带来了非常高的性能成本。
- en: It is often unnecessary, and so is the cost.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这通常是不必要的，成本也是如此。
- en: On top of everything else, it would be completely useless in many cases.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除此之外，在许多情况下它将是完全无用的。
- en: 'Let''s tackle these considerations one at a time. Why could a thread-safe data
    structure be unnecessary even in a multi-threaded program? One trivial possibility
    is that it is used in a single-threaded portion of the program. We strive to minimize
    such portions due to their deleterious impact on the overall runtime (remember
    Amdahl''s Law?), but most programs have some, and one of the ways we make such
    code faster is by not paying unnecessary overhead. The more common scenario for
    not needing thread safety is when an object is used exclusively by one thread,
    even in a multi-threaded program. This is very common and very desirable: as we
    have said several times, shared data is the main source of inefficiency in concurrent
    programs, so we try to do as much work as possible on each thread independently,
    using only local objects and data.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一解决这些考虑。即使在多线程程序中，为什么线程安全的数据结构可能是不必要的呢？一个微不足道的可能性是它被用在程序的单线程部分。我们努力尽量减少这样的部分，因为它们对整体运行时间有害（还记得阿姆达尔定律吗？），但大多数程序都有一些，我们使这样的代码更快的一种方式是不支付不必要的开销。不需要线程安全的更常见的情况是当一个对象在多线程程序中只被一个线程使用。这是非常常见和非常理想的：正如我们已经说过好几次，共享数据是并发程序中效率低下的主要原因，所以我们尽量让每个线程独立地完成尽可能多的工作，只使用本地对象和数据。
- en: 'But can we be certain that a class or a data structure is safe to use in a
    multi-threaded program, even if each object is never shared between threads? Not
    necessarily: just because we do not see any sharing at the interface level does
    not mean that none is going on at the implementation level. Multiple objects could
    be sharing the same data internally: static members and memory allocators are
    just some of the possibilities (we tend to think that all objects that need memory
    get it by calling `malloc()` and that `malloc()` is thread-safe, but a class could
    implement its own allocator as well).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们能确定一个类或数据结构在多线程程序中是安全的吗，即使每个对象从未在线程之间共享？不一定：仅仅因为我们在接口层面上看不到任何共享，并不意味着在实现层面上没有共享。多个对象可能在内部共享相同的数据：静态成员和内存分配器只是一些可能性（我们倾向于认为所有需要内存的对象都通过调用`malloc()`来获得内存，并且`malloc()`是线程安全的，但一个类也可以实现自己的分配器）。
- en: 'On the other hand, many data structures are perfectly safe to use in a multi-threaded
    code as long as none of the threads modify the object. While this may seem obvious,
    again, we have to consider the implementation: the interface may be read-only,
    but the implementation may still modify the object. If you think that it is an
    exotic possibility, consider the standard C++ shared pointer, `std::shared_ptr`:
    when you make a copy of a shared pointer, the copied object is not modified, at
    least not visibly (it is passed to the constructor of the new pointer by `const`
    reference). At the same time, you know that the reference count in the object
    has to be incremented, which means the copied-from object has changed (shared
    pointers are thread-safe in this scenario, but this did not happen by accident,
    and neither is it free, there is a performance cost).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，许多数据结构在多线程代码中使用起来是完全安全的，只要没有线程修改对象。虽然这似乎是显而易见的，但我们必须再次考虑实现：接口可能是只读的，但实现可能仍然修改对象。如果你认为这是一个奇特的可能性，考虑一下标准的C++共享指针`std::shared_ptr`：当你复制一个共享指针时，复制的对象没有被修改，至少不是显而易见的（它通过`const`引用传递给新指针的构造函数）。与此同时，你知道对象中的引用计数必须被增加，这意味着被复制的对象已经改变了（在这种情况下，共享指针是线程安全的，但这并不是偶然发生的，也不是免费的，这是有性能成本的）。
- en: 'The bottom line is, we need a more nuanced definition of thread safety. Unfortunately,
    there is no common vocabulary for this very common concept, but there are several
    popular versions. The highest level of thread safety is often called a `const`
    member functions of the class), and, second, any thread that has exclusive access
    to an object can perform any otherwise valid operations on it, no matter what
    other threads are doing at the same time. An object that does not provide any
    such guarantee cannot be used in a multi-threaded program at all: even if the
    object itself is not shared, something inside its implementation is vulnerable
    to modifications by other threads.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，我们需要一个更细致的线程安全定义。不幸的是，对于这个非常常见的概念，没有共同的词汇，但有几个流行的版本。线程安全的最高级别通常被称为`const`类的成员函数，其次，任何具有对对象的独占访问权的线程都可以执行任何其他有效的操作，无论其他线程同时做什么。不提供任何此类保证的对象根本不能在多线程程序中使用：即使对象本身没有被共享，其实现中的某些部分也容易受到其他线程的修改。
- en: In this book, we will use the language of strong and weak thread-safety guarantees.
    A class that provides a strong guarantee is sometimes called simply `const` member
    functions. Finally, the classes that do not offer any guarantees at all are called
    **thread-hostile** and, generally, cannot be used in a multi-threaded program
    at all.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用强和弱线程安全保证的语言。提供强保证的类有时被简单地称为`const`成员函数。最后，根本不提供任何保证的类被称为**线程敌意**，通常根本不能在多线程程序中使用。
- en: 'In practice, we often encounter a mix of strong and weak guarantees: a subset
    of the interface offers a strong guarantee, but the rest of it provides only the
    weak guarantee.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们经常遇到强和弱保证的混合：接口的一个子集提供了强保证，但其余部分只提供了弱保证。
- en: 'So, why do we not try to design every object with a strong thread safety guarantee?
    The first reason we already mentioned: there is usually performance overhead,
    the guarantee is often unnecessary because the objects are not shared between
    threads, and the key to writing an efficient program is not doing any work that
    can be avoided. The more interesting objection is the one we mentioned earlier,
    in passing: even in the case where the object is shared in a way that would require
    thread safety, the strong thread safety guarantee may be useless. Consider this
    problem: you need to develop a game where the players recruit an army and do battles.
    The names of all the units in the army are stored in a container, let''s say a
    list of strings. Another container stores the current strength of each unit. During
    the campaign, the units get killed or recruited all the time, and the gaming engine
    is multi-threaded and needs to be efficient to manage a large army. While the
    STL containers provide only the weak thread safety guarantee, let''s assume that
    we have a library of strongly thread-safe containers. It is easy to see that this
    is not enough: adding a unit requires inserting its name into one container and
    its initial strength into the other. Both operations are thread-safe by themselves.
    One thread creates a new unit and inserts it into the first container. Before
    this thread can also add its strength value, another thread sees the new unit
    and needs to look up its strength, but there is nothing in the second container
    yet. The problem is that the thread safety guarantee is offered at the wrong level:
    from the application point of view, creating a new unit is a transaction, and
    all gaming engine threads should be able to see the database either before the
    unit is added or after, but not in the intermediate state. We can accomplish that,
    for example, by using a mutex: it will be locked before the unit is added and
    unlocked only after both containers have been updated. However, in this scenario,
    we don''t care about the thread safety guarantees provided by the individual containers,
    as long as all accesses to these objects are guarded by a mutex anyway. Obviously,
    what we need instead is a unit database that itself provides the desired thread
    safety guarantees, for example, by using mutexes. This database may internally
    use several container objects, and the implementation of the database may or may
    not need any thread safety guarantees from these containers, but this should be
    invisible to the clients of the database (having thread-safe containers may make
    the implementation easier, or not).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们不尝试为每个对象设计强线程安全保证呢？我们已经提到的第一个原因是：通常会有性能开销，保证通常是不必要的，因为对象不在线程之间共享，编写高效程序的关键是不做任何可以避免的工作。更有趣的反对意见是我们之前提到的，即使在需要线程安全的情况下，强线程安全保证可能是无用的。考虑这个问题：你需要开发一个玩家招募军队并进行战斗的游戏。军队中所有单位的名称都存储在一个容器中，比如一个字符串列表。另一个容器存储每个单位的当前力量。在战役中，单位一直在被杀死或招募，游戏引擎是多线程的，需要高效地管理大军。虽然STL容器只提供了弱线程安全保证，假设我们有一个强线程安全容器的库。很容易看出这是不够的：添加一个单位需要将其名称插入一个容器，将其初始力量插入另一个容器。这两个操作本身是线程安全的。一个线程创建一个新单位并将其插入第一个容器。在这个线程也能添加其力量值之前，另一个线程看到了新单位并需要查找其力量，但第二个容器中还没有任何内容。问题在于线程安全保证提供在错误的级别：从应用程序的角度来看，创建一个新单位是一个事务，所有游戏引擎线程都应该能够在单位被添加之前或之后看到数据库，而不是在中间状态。我们可以通过使用互斥锁来实现这一点，例如：在单位被添加之前将其锁定，只有在两个容器都被更新后才解锁。然而，在这种情况下，我们并不关心单个容器提供的线程安全保证，只要对这些对象的所有访问都受到互斥锁的保护。显然，我们需要的是一个自身提供所需线程安全保证的单位数据库，例如，通过使用互斥锁。这个数据库可能在内部使用几个容器对象，并且数据库的实现可能需要或不需要来自这些容器的任何线程安全保证，但这对数据库的客户端来说应该是不可见的（使用线程安全的容器可能会使实现更容易，也可能不会）。
- en: 'This leads us to a very important conclusion: thread safety begins at the design
    stage. The data structures and the interfaces used by the program must be chosen
    wisely, so they represent the appropriate level of abstraction and the correct
    transactions at the level where thread interaction is taking place.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个非常重要的结论：线程安全从设计阶段开始。程序使用的数据结构和接口必须明智选择，以便它们在线程交互发生的层次上代表适当的抽象级别和正确的事务。
- en: 'With this in mind, the rest of this chapter should be seen from two sides:
    on the one hand, we show how to design and implement some basic thread-safe data
    structures that can be used as building blocks for the more complex (and infinitely
    more varied) ones you will need in your programs. On the other hand, we also show
    the basic techniques for building thread-safe classes that can be used to design
    these more complex data structures.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个想法，本章的其余部分应该从两个方面来看：一方面，我们展示如何设计和实现一些基本的线程安全数据结构，这些数据结构可以作为更复杂（并且无限多样）的数据结构的构建模块。另一方面，我们还展示了构建线程安全类的基本技术，这些类可以用于设计这些更复杂的数据结构。
- en: Counters and accumulators
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计数器和累加器
- en: One of the simplest thread-safe objects is a humble counter or its more general
    form, an accumulator. The counter simply counts some events that can occur on
    any of the threads. All threads may need to increment the counter or access the
    current value, so there is potential for a race condition.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的线程安全对象之一是一个普通的计数器或者更一般的形式，一个累加器。计数器简单地计算一些可以在任何线程上发生的事件。所有线程可能需要增加计数器或者访问当前值，因此存在竞争条件的可能性。
- en: 'To be of value, we need the strong thread safety guarantee here: the weak guarantee
    is trivial; reading a value that nobody is changing is always thread-safe. We
    have already seen the available options for the implementation: a lock of some
    kind, an atomic operation (when one is available), or a lock-free CAS loop.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有价值，我们需要在这里提供强线程安全保证：弱保证是微不足道的；读取一个没有人在改变的值总是线程安全的。我们已经看到了实现的可用选项：某种类型的锁，原子操作（如果有的话），或者无锁CAS循环。
- en: The performance of a lock varies with the implementation, but a spinlock is,
    in general, preferred. The wait time for a thread that did not get immediate access
    to the counter is going to be very short. So, it does not make sense to incur
    the cost of putting the thread to sleep and waking it up later. On the other hand,
    the amount of CPU time wasted because of the busy waiting (polling the spinlock)
    is going to be negligible, most likely just a few instructions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 锁的性能因实现而异，但一般来说，自旋锁是首选。对于没有立即访问计数器的线程的等待时间将会非常短。因此，付出将线程置于休眠状态并稍后唤醒它的成本是没有意义的。另一方面，因为忙等待（轮询自旋锁）而浪费的CPU时间将是微不足道的，很可能只是几条指令。
- en: 'The atomic instruction delivers good performance, but the choice of operations
    is rather limited: in C++, you can atomically add to an integer but not, for example,
    multiply it. This is enough for a basic counter but may be insufficient for a
    more general accumulator (the accumulating operation does not have to be limited
    to a sum). However, if one is available, you just cannot beat the simplicity of
    an atomic operation.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 原子指令提供了良好的性能，但操作的选择相当有限：在C++中，你可以原子地向整数添加，但不能，例如，将其乘以。这对于基本计数器已经足够了，但对于更一般的累加器可能不够（累加操作不必局限于求和）。然而，如果有一个可用，你就无法击败原子操作的简单性。
- en: The CAS loop can be used to implement any accumulator, regardless of the operation
    we need to use. However, on most modern hardware, it is not the fastest option
    and is outperformed by a spinlock (see *Figure 6.2*).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: CAS循环可以用于实现任何累加器，无论我们需要使用的操作是什么。然而，在大多数现代硬件上，它并不是最快的选择，并且被自旋锁（见*图6.2*）所超越。
- en: 'The spinlock can be further optimized for the case when it is used to access
    a single variable or a single object. Instead of a generic flag, we can make the
    lock itself be the only reference to the object it is guarding. The atomic variable
    is going to be a pointer, not an integer, but otherwise, the locking mechanism
    remains unchanged. The `lock()` function is non-standard because it returns the
    pointer to the counter:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 自旋锁可以进一步优化，用于访问单个变量或单个对象的情况。我们可以使锁本身成为守护的对象的唯一引用，而不是通用标志。原子变量将是一个指针，而不是整数，但锁定机制保持不变。`lock()`函数是非标准的，因为它返回指向计数器的指针。
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Compared to the earlier implementation of the spinlock, the meaning of the atomic
    variable is "inverted:" the lock is available if the atomic variable `p_` is not
    null, otherwise it is taken. All the optimizations we have done for the spinlock
    are applicable here as well and look exactly the same, so we are not going to
    repeat them. Also, to be complete, the class needs a set of deleted copy operations
    (locks are non-copyable). It may be movable if the ability to transfer the lock
    and the responsibility to release it to another object is desirable. If the lock
    also owns the object it is pointing to, the destructor should delete it (this
    combines the functionality of a spinlock and a unique pointer in a single class).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期自旋锁的实现相比，原子变量的含义是“反转的”：如果原子变量`p_`不为空，则锁可用，否则被占用。我们为自旋锁所做的所有优化在这里同样适用，并且看起来完全一样，因此我们不会重复它们。此外，为了完整，该类需要一组删除的复制操作（锁是不可复制的）。如果希望能够转移锁并将释放锁的责任转移到另一个对象，则它可能是可移动的。如果锁还拥有它指向的对象，析构函数应该删除它（这将自旋锁和唯一指针的功能结合在一个类中）。
- en: 'One obvious advantage of the pointer spinlock is that, as long as it provides
    the only way to access the guarded object, it is not possible to accidentally
    create a race condition and access the shared data without a lock. The second
    advantage is that this lock slightly outperforms the regular spinlock more often
    than not. Whether or not the spinlock also outperforms the atomic operation depends
    on the hardware as well. The same benchmark yields very different results on different
    processors:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 指针自旋锁的一个明显优势是，只要它提供了访问受保护对象的唯一方式，就不可能意外地创建竞争条件并在没有锁的情况下访问共享数据。第二个优势是，这个锁往往比常规自旋锁稍微更快。自旋锁是否也优于原子操作取决于硬件。同样的基准测试在不同处理器上产生非常不同的结果：
- en: '![Figure 6.3 – Performance of a shared count increment: regular spinlock, pointer
    spinlock, lock-free (compare-and-swap, or CAS), and wait-free (atomic) for different
    hardware systems (a) and (b)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3 - 共享计数增量的性能：常规自旋锁、指针自旋锁、无锁（比较和交换，或CAS）、无等待（原子）对不同硬件系统（a）和（b）的影响'
- en: '](img/Figure_6.3_B16229.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_6.3_B16229.jpg)'
- en: 'Figure 6.3 – Performance of a shared count increment: regular spinlock, pointer
    spinlock, lock-free (compare-and-swap, or CAS), and wait-free (atomic) for different
    hardware systems (a) and (b)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 - 共享计数增量的性能：常规自旋锁、指针自旋锁、无锁（比较和交换，或CAS）、无等待（原子）对不同硬件系统（a）和（b）的影响
- en: As a rule, the more recent processors handle locks and busy waiting better,
    and it is more likely that the spinlock delivers better performance on the latest
    hardware (in *Figure 6.3*, system *b* uses Intel X86 CPUs that are one generation
    behind those in system *a*).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，较新的处理器更好地处理锁和忙等待，而且旋转锁更有可能在最新的硬件上提供更好的性能（在*图6.3*中，系统*b*使用的是Intel X86处理器，比系统*a*的处理器晚一代）。
- en: 'The average time it takes to execute an operation (or its inverse, the throughput)
    is the metric that we are mainly concerned with in most HPC systems. However,
    this is not the only possible metric used to gauge the performance of concurrent
    programs. For example, if the program runs on a mobile device, the power consumption
    may be of greater importance. The total CPU time used by all threads is a reasonable
    proxy for the average power consumption. The same benchmark we used to measure
    the average real time of the counter increment can be used to measure the CPU
    time as well:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 执行操作所需的平均时间（或其倒数，吞吐量）是我们在大多数HPC系统中主要关注的度量标准。然而，这并不是衡量并发程序性能的唯一可能度量标准。例如，如果程序在移动设备上运行，功耗可能更为重要。所有线程使用的总CPU时间是平均功耗的一个合理代理。我们用来测量计数器增量的平均实际时间的相同基准测试也可以用来测量CPU时间：
- en: '![Figure 6.4 – Average CPU time used by different implementations of the thread-safe
    counter'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.4 - 不同线程安全计数器实现的平均CPU使用时间'
- en: '](img/Figure_6.4_B16229.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_6.4_B16229.jpg)'
- en: Figure 6.4 – Average CPU time used by different implementations of the thread-safe
    counter
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 - 不同线程安全计数器实现的平均CPU使用时间
- en: The bad news here is that no matter the implementation, the cost of accessing
    the shared data by multiple threads at once increases exponentially with the number
    of threads, at least when we have many threads (note that the *y* axis scale in
    *Figure 6.4* is logarithmic). However, the efficiency varies greatly between the
    implementations, and, at least for the most efficient implementations, the exponential
    rise does not really kick in until at least eight threads. Note that the results
    will, again, vary from one hardware system to another, so the choice must be made
    with your target platform in mind and only after the measurements have been done.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 坏消息是，无论实现方式如何，多个线程同时访问共享数据的成本都会随着线程数量的增加呈指数级增长，至少当我们有很多线程时是这样（请注意*图6.4*中的*y*轴刻度是对数刻度）。然而，效率在不同实现之间差异很大，至少对于最有效的实现来说，指数增长实际上直到至少八个线程才会真正开始。请注意，结果将再次因硬件系统而异，因此选择必须考虑目标平台，并且只能在测量完成后进行。
- en: Whatever the chosen implementation, a thread-safe accumulator or a counter should
    not expose it but encapsulate it in a class. One reason is to provide the clients
    of the class with a stable interface while retaining the freedom to optimize the
    implementation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择哪种实现方式，线程安全的累加器或计数器都不应该暴露出来，而是应该封装在一个类中。一个原因是为了为类的客户提供稳定的接口，同时保留优化实现的自由。
- en: 'The second reason is more subtle, and it has to do with the exact guarantees
    the counter provides. So far, we have focused on the counter value itself, making
    sure that it is modified and accessed by all threads without any races. Whether
    or not this is enough depends on how we use the counter. If all we want is to
    count some events, and nothing else depends on the value of the counter, then
    we only care that the value itself is correct. On the other hand, if what we are
    counting is, say, the number of elements in an array, then we are dealing with
    a data dependency. Let''s say that we have a large pre-allocated array (or a container
    that can grow without disturbing the elements already in it), and all threads
    are computing new elements to be inserted into this array. The counter counts
    the number of elements that are computed and inserted into the array and can be
    used by other threads. In other words, if a thread reads the value `N` from the
    counter, it must be assured that the first `N` elements of the array are safe
    to read (which implies that no other thread is modifying them anymore). But the
    array itself is neither atomic nor protected by a lock. To be sure, we could have
    protected the access to the entire array by a lock, but this is probably going
    to kill the performance of the program: if there are many elements already in
    the array but only one thread can read them at any time, the program might as
    well be single-threaded. On the other hand, we know that any constant, immutable
    data is safe to read from multiple threads without any locks. We just need to
    know where the boundary between the immutable and the changing data is, and that
    is exactly what the counter is supposed to provide. The key issue here is the
    memory visibility: we need a guarantee that any changes to the first `N` elements
    of the array become visible to all threads before the value of the counter changes
    from `N-1` to `N`.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因更微妙，它与计数器提供的确切保证有关。到目前为止，我们已经专注于计数器的值本身，确保它被所有线程修改和访问而没有任何竞争。是否足够取决于我们如何使用计数器。如果我们只是想计算一些事件，而且没有其他东西依赖于计数器的值，那么我们只关心值本身是否正确。另一方面，如果我们要计算的是，比如说，数组中元素的数量，那么我们就涉及到数据依赖性。假设我们有一个大的预分配数组（或者一个可以在不干扰已有元素的情况下增长的容器），所有线程都在计算要插入到这个数组中的新元素。计数器计算已计算并插入数组中的元素的数量，并且可以被其他线程使用。换句话说，如果一个线程从计数器中读取值`N`，它必须确保数组的前`N`个元素是安全可读的（这意味着没有其他线程再修改它们）。但是数组本身既不是原子的，也没有受到锁的保护。当然，我们可以通过锁来保护对整个数组的访问，但这可能会降低程序的性能：如果数组中已经有很多元素，但只有一个线程可以读取它们，那么程序可能就像单线程一样。另一方面，我们知道任何常量、不可变的数据都可以在多个线程中安全地读取，而不需要任何锁。我们只需要知道不可变数据和可变数据之间的边界在哪里，这正是计数器应该提供的。这里的关键问题是内存可见性：我们需要保证数组的前`N`个元素的任何更改在计数器的值从`N-1`变为`N`之前对所有线程都是可见的。
- en: 'We studied memory visibility in the previous chapter when we discussed the
    memory model. At the time, it might have appeared to be a largely theoretical
    matter, but not anymore. From the last chapter, we know that the way we control
    the visibility is by restricting the memory order or by using memory barriers
    (two different ways to talk about the same thing). The key difference between
    a count and an index in a multi-threaded program is that the index provides an
    additional guarantee: if the thread that increments the index from `N-1` to `N`
    had completed the initialization of the array element `N` before it incremented
    the index, then any other thread that reads the index and gets the value of `N`
    (or greater) is guaranteed to see at least `N` fully initialized and safe to read
    elements in the array (assuming no other thread writes into these elements, of
    course). This is a non-trivial guarantee, do not easily dismiss it: multiple threads
    are accessing the same location in memory (the array element `N`) *without any
    locking*, and one of these threads is *writing* into this location, and yet, the
    access is safe, there is no data race. If we could not arrange for this guarantee
    using the shared index, we would have to lock all accesses to the array, and only
    one thread would be able to read it at any time. Instead, we can use this atomic
    index class:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章中研究了内存可见性，当时它可能看起来是一个主要是理论性的问题，但现在不是了。从上一章我们知道，我们控制可见性的方式是通过限制内存顺序或使用内存屏障（谈论同一件事的两种不同方式）。多线程程序中计数和索引之间的关键区别在于索引提供了额外的保证：如果将索引从`N-1`增加到`N`的线程在增加索引之前已经完成了数组元素`N`的初始化，那么读取索引并得到值`N`（或更大）的任何其他线程都保证能够在数组中看到至少`N`个完全初始化和安全可读的元素（当然假设没有其他线程写入这些元素）。这是一个非平凡的保证，不要轻易忽视它：多个线程在访问内存中的同一位置（数组元素`N`）*而没有任何锁*，并且其中一个线程*写入*这个位置，然而，访问是安全的，没有数据竞争。如果我们不能使用共享索引来安排这个保证，我们将不得不锁定对数组的所有访问，只有一个线程能够每次读取它。相反，我们可以使用这个原子索引类：
- en: '[PRE9]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The only difference between the index at the count is in the memory visibility
    guarantees; the count offers none:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 计数和索引之间唯一的区别在于内存可见性的保证；计数没有提供：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The thread safety and memory visibility guarantees should be documented for
    each of the classes, of course. Whether or not there is a performance difference
    between the two depends on the hardware. On an X86 CPU, there is none because
    the hardware instructions for atomic increment and atomic read have the "index-like"
    memory barriers whether we request them or not. On ARM CPUs, relaxed (or no-barrier)
    memory operations are noticeably faster. But, regardless of the performance, clarity
    and intent matter and should not be forgotten: if a programmer uses an index class
    that explicitly offers the memory order guarantees but does not index anything
    with it, every reader will wonder what is going on and where is that subtle and
    hidden place in the code that uses these guarantees. By using the interfaces with
    the correct set of documented guarantees, you signal to your readers what your
    intent was when writing this code.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，每个类的线程安全性和内存可见性保证都应该有文档记录。两者之间是否存在性能差异取决于硬件。在X86 CPU上，没有差异，因为原子递增和原子读取的硬件指令具有“类似索引”的内存屏障，无论我们是否请求。在ARM
    CPU上，放松（或无屏障）内存操作明显更快。但是，无论性能如何，清晰和意图都很重要，不应被忘记：如果程序员使用明确提供内存顺序保证的索引类，但没有使用它进行任何索引，每个读者都会想知道发生了什么，代码中的这些保证被使用在了哪个微妙而隐藏的地方。通过使用具有正确一组文档保证的接口，您向读者表明编写此代码时的意图。
- en: 'Let us now return to what may be the main "hidden" accomplishment in this section.
    We learned about thread-safe counters, but along the way, we came up with an algorithm
    to seemingly violate the first rule of writing multi-threaded code: any time two
    or more threads access the same memory location and at least one of these threads
    is writing, all accesses must be locked (or atomic). We did not lock the shared
    array, we allow arbitrary data in its elements (so it''s probably not atomic),
    and we got away with it! The approach we used to avoid data races turns out to
    be the cornerstone of almost every data structure designed specifically for concurrency,
    and we will now take time to better understand and generalize it.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到本节可能是主要的“隐藏”成就。我们学习了关于线程安全计数器，但在这个过程中，我们提出了一个似乎违反了编写多线程代码的第一规则的算法：任何时候两个或更多线程访问同一内存位置，并且至少有一个线程在写入，所有访问都必须被锁定（或原子化）。我们没有锁定共享数组，我们允许其元素中的任意数据（所以它可能不是原子的），但我们却得以逃脱！我们用来避免数据竞争的方法，事实证明是几乎每个专为并发设计的数据结构的基石，我们现在将花时间更好地理解和概括它。
- en: Publishing protocol
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发布协议
- en: 'The general problem we are trying to solve is a very common one in data structure
    design and, by extension, the development of concurrent programs: one thread is
    creating new data, and the rest of the program must be able to see this data when
    it is ready, but not before. The former thread is often called the writer thread
    or the producer thread. All the other threads are reader or consumer threads.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图解决的一般问题在数据结构设计中非常常见，通过扩展，也是并发程序的开发：一个线程正在创建新数据，而程序的其余部分必须在数据准备好时能够看到这些数据，但在此之前不能看到。前一个线程通常被称为写入线程或生产者线程。所有其他线程都是读取或消费者线程。
- en: 'The most obvious solution is to use a lock and follow the rule of avoiding
    the data races to the letter. If multiple threads (check) must access the same
    memory location (check) and at least one thread is writing at this location (exactly
    one thread in our case – check), then all threads must acquire a lock before accessing
    this memory location for either reading or writing. The downside of this solution
    is the performance: long after the producer is done and no more writing happens,
    all the consumer threads keep locking each other out of reading the data concurrently.
    Now, read-only access does not require any locking at all, but the problem is,
    we need to have a guaranteed point in the program such that all the writing happens
    before this point and all the reading happens after this point. Then we can say
    that all consumer threads operate in a read-only environment and do not need any
    locking. The challenge is to guarantee that boundary between reading and writing:
    remember that, unless we do some sort of synchronization, memory visibility is
    not guaranteed: just because the writer has finished modifying the memory doesn''t
    mean the reader sees the final state of that memory. The locks include the appropriate
    memory barriers, as we have seen earlier; they border the critical section and
    ensure that any operation executed after the critical section will see all the
    changes to the memory that happened before or during the critical section. But
    now we want to get the same guarantee without the locks.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的解决方案是使用锁，并严格遵循避免数据竞争的规则。如果多个线程（检查）必须访问同一内存位置（检查），并且至少有一个线程在该位置写入（在我们的情况下确切地是一个线程
    - 检查），那么所有线程在访问该内存位置之前都必须获取锁，无论是读取还是写入。这种解决方案的缺点是性能：在生产者完成并且不再有写入发生之后，所有消费者线程仍然互相阻止并发地读取数据。现在，只读访问根本不需要任何锁定，但问题是，我们需要在程序中有一个保证的点，使得所有写入在此点之前发生，所有读取在此点之后发生。然后我们可以说所有消费者线程在只读环境中操作，不需要任何锁定。挑战在于保证读取和写入之间的边界：请记住，除非我们进行某种同步，否则内存可见性是不被保证的：仅仅因为写入者已经完成了对内存的修改，并不意味着读取者看到了该内存的最终状态。锁包括适当的内存屏障，正如我们之前所见；它们界定了临界区，并确保在临界区之后执行的任何操作都会看到在临界区之前或期间发生的所有对内存的更改。但现在我们希望在没有锁定的情况下获得相同的保证。
- en: 'The lock-free solution to this problem relies on a very specific protocol for
    passing information between the producer and the consumer threads:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个无锁解决方案依赖于生产者和消费者线程之间传递信息的一个非常具体的协议：
- en: The producer thread prepares the data in a memory that is not accessible to
    other threads. It could be the memory allocated by the producer threads, or it
    could be pre-allocated memory, but the important point is that the producer is
    the only thread with a valid reference to this memory, and that valid reference
    is not shared with other threads (there may be a way for other threads to access
    this memory, but that would be a bug in the program, similar to indexing an array
    out of bounds). Since there is only one thread accessing the new data, no synchronization
    is required. As far as the other threads are concerned, the data simply does not
    exist.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产者线程在其他线程无法访问的内存中准备数据。这可能是生产者线程分配的内存，也可能是预先分配的内存，但重要的是生产者是唯一拥有对这个内存的有效引用的线程，并且这个有效引用不与其他线程共享（其他线程可能有访问这个内存的方法，但这将是程序中的一个错误，类似于超出数组边界索引）。由于只有一个线程访问新数据，因此不需要同步。就其他线程而言，这些数据根本不存在。
- en: 'All consumer threads must use a single shared pointer for any access to the
    data, which we call the root pointer, and this pointer is initially null. It remains
    null while the producer thread is constructing the data. Again, from the point
    of view of the consumer threads, there is no data at this time. More generally,
    the "pointer" does not need to be an actual pointer: any kind of handle or reference
    can be used as long as it gives access to the memory location and can be set to
    a predetermined invalid value. For example, if all new objects are created in
    a pre-allocated array, the "pointer" could, in fact, be an index into the array,
    and the invalid value could be any value greater or equal to the array size.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有消费者线程必须使用一个共享指针来访问数据，我们称之为根指针，这个指针最初为空。在生产者线程构造数据时，它保持为空。同样，从消费者线程的角度来看，这个时候没有数据。更一般地说，这个“指针”不需要是实际的指针：只要它能够访问内存位置并且可以设置为预定的无效值，任何类型的句柄或引用都可以使用。例如，如果所有新对象都是在预先分配的数组中创建的，那么“指针”实际上可以是数组的索引，无效值可以是大于或等于数组大小的任何值。
- en: 'The key to the protocol is that the only way for the consumer to access the
    data is through the root pointer, and this pointer remains null until the producer
    is ready to reveal, or publish, the data. The act of publishing the data is very
    simple: the producer must atomically store the correct memory location of the
    data in the root pointer, and this change must be accompanied by the release memory
    barrier.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协议的关键在于消费者访问数据的唯一方式是通过根指针，而且在生产者准备揭示或发布数据之前，这个指针始终为空。发布数据的行为非常简单：生产者必须原子地将数据的正确内存位置存储在根指针中，并且这个变化必须伴随着释放内存屏障。
- en: The consumer thread can, at any time, query the root pointer, again atomically.
    If the query returns null, then there is no data (as far as the consumer is concerned),
    and the consumer thread should wait or, ideally, do some other work. If the query
    returns a non-null value, then the data is ready, and the producer will not change
    it anymore. The query must be done with the acquire memory barrier, which, in
    combination with the release barrier on the producer side, guarantees that the
    new data is visible when the change of the pointer value is observed.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者线程可以随时原子地查询根指针。如果查询返回空值，那么就没有数据（就消费者而言），消费者线程应该等待，或者最好做一些其他工作。如果查询返回非空值，那么数据已准备好，生产者将不再更改它。查询必须使用获取内存屏障进行，这与生产者端的释放屏障结合使用，可以保证当观察到指针值的变化时，新数据是可见的。
- en: This process is sometimes called the **publishing protocol** because it allows
    the producer thread to publish information for other threads to consume in a way
    that guarantees no data races. As we said, the publishing protocol can be implemented
    using any handle that gives access to the memory as long as this handle can be
    changed atomically. Pointers are the most common handle, of course, followed by
    array indices.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程有时被称为**发布协议**，因为它允许生产者线程发布信息供其他线程消费，以一种保证没有数据竞争的方式。正如我们所说，发布协议可以使用任何允许访问内存的句柄来实现，只要这个句柄可以被原子地改变。指针是最常见的句柄，当然，其次是数组索引。
- en: 'The data that is being published can be simple or complex; it doesn''t matter.
    It does not even have to be a single object or a single memory location: the object
    that the root pointer points to can itself contain pointers to more data. The
    key elements of the publishing protocol are as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 被发布的数据可以是简单的或复杂的；这并不重要。它甚至不必是单个对象或单个内存位置：根指针指向的对象本身可以包含指向更多数据的指针。发布协议的关键要素如下：
- en: All consumers access a particular set of data through one root pointer. The
    only way to gain access to the data is to read a non-null value of the root pointer.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有消费者通过一个根指针访问特定的数据集。获得访问数据的唯一方法是读取根指针的非空值。
- en: 'The producer can prepare the data any way it wants, but the root pointer remains
    null: the producer has its own reference to the data that is local to this thread.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产者可以以任何方式准备数据，但根指针始终为空：生产者有自己的对数据的引用，这是本地线程的。
- en: When the producer wants to publish the data, it sets the root pointer to the
    correct address atomically and with a release barrier. After the data is published,
    the producer cannot change it (neither can anyone else).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当生产者想要发布数据时，它会原子地使用释放屏障将根指针设置为正确的地址。数据发布后，生产者不能再更改它（其他人也不能）。
- en: The consumer threads must read the root pointer atomically and with an acquire
    barrier. If they read a non-null value, they can read the data accessible through
    the root pointer.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者线程必须原子地并且使用获取屏障读取根指针。如果他们读取到非空值，他们可以读取通过根指针访问的数据。
- en: The atomic reads and writes used to implement the publishing protocol should
    not be, of course, scattered throughout the code. We should implement a publishing
    pointer class to encapsulate this functionality. In the next section, we will
    see a simple version of such a class.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 用于实现发布协议的原子读写当然不应该散布在整个代码中。我们应该实现一个发布指针类来封装这个功能。在下一节中，我们将看到这样一个类的简单版本。
- en: Smart pointers for concurrent programming
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于并发编程的智能指针
- en: The challenge of concurrent (thread-safe) data structures is how to add, remove,
    and change the data in a way that maintains certain thread safety guarantees.
    The publishing protocol, which gives us a way to release new data to all threads,
    is usually the first step in adding new data to any such data structure. Thus,
    it should come as no surprise that the first class we will learn about is a pointer
    that encapsulates this protocol.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 并发（线程安全）数据结构的挑战在于如何以一种保持特定线程安全保证的方式添加、删除和更改数据。发布协议为我们提供了一种向所有线程发布新数据的方法，通常是向任何此类数据结构添加新数据的第一步。因此，毫无疑问，我们将学习的第一个类是封装了这个协议的指针。
- en: Publishing pointer
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发布指针
- en: 'Here is a basic publishing pointer that also includes the functionality of
    a unique, or owning, pointer (so we can call it a thread-safe unique pointer):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基本的发布指针，还包括唯一或拥有指针的功能（所以我们可以称之为线程安全的唯一指针）：
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Of course, this is a very bare-bones design; a complete implementation should
    support a custom deleter, a move constructor and assignment operator, and maybe
    a few more features, similar to `std::unique_ptr`. By the way, the standard does
    not guarantee that accessing the pointer value stored in a `std::unique_ptr` object
    is atomic or that the necessary memory barriers are used, so the standard unique
    pointer cannot be used to implement the publishing protocol.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是一个非常简单的设计；一个完整的实现应该支持自定义删除器、移动构造函数和赋值运算符，以及可能还有一些类似于`std::unique_ptr`的其他功能。顺便说一句，标准并不保证访问存储在`std::unique_ptr`对象中的指针值是原子的，或者使用了必要的内存屏障，因此标准唯一指针不能用于实现发布协议。
- en: 'By now, it should be clear to the reader what our thread-safe unique pointer
    offers: the key functions are `publish()` and `get()`, and they implement the
    publishing protocol. Note that the `publish()` method does not delete the old
    data; it is assumed that the producer thread calls `publish()` only once and only
    on a null pointer. We could add an assert for that, and it may be a good idea
    to do so in a debug build, but we are also concerned with the performance. Speaking
    of performance, a benchmark shows that the single-threaded dereferencing of our
    publishing pointer takes the same time as that of a raw pointer or of `std::unique_ptr`.
    The benchmark is not complicated:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，读者应该清楚我们的线程安全唯一指针提供了什么：关键函数是`publish()`和`get()`，它们实现了发布协议。请注意，`publish()`方法不会删除旧数据；假定生产者线程只调用一次`publish()`，而且只在空指针上调用。我们可以为此添加一个断言，在调试构建中这样做可能是个好主意，但我们也关心性能。说到性能，基准测试显示，我们的发布指针的单线程解引用所花费的时间与原始指针或`std::unique_ptr`的时间相同。基准测试并不复杂：
- en: '[PRE12]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Running this benchmark gives us an idea of how fast the dereferencing of our
    lock-free publishing pointer is:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个基准测试可以让我们了解我们的无锁发布指针的解引用速度有多快：
- en: '![Figure 6.5 – The performance of the publishing pointer (consumer threads)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.5 - 发布指针的性能（消费者线程）'
- en: '](img/Figure_6.5_B16229.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_6.5_B16229.jpg)'
- en: Figure 6.5 – The performance of the publishing pointer (consumer threads)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 - 发布指针的性能（消费者线程）
- en: 'The result should be compared with dereferencing a raw pointer, which we can
    also do on multiple threads:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 应该将结果与解引用原始指针进行比较，我们也可以在多个线程上执行此操作：
- en: '![Figure 6.6 – The performance of the raw pointer, for comparison with Figure
    6.5'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 - 原始指针的性能，用于与图6.5进行比较'
- en: '](img/Figure_6.6_B16229.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_6.6_B16229.jpg)'
- en: Figure 6.6 – The performance of the raw pointer, for comparison with Figure
    6.5
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 - 原始指针的性能，用于与图6.5进行比较
- en: 'The performance numbers are very close. We can also compare the speed of publishing,
    but, usually, the consumer side is more important: each object is published only
    once but then accessed many times.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 性能数字非常接近。我们也可以比较发布的速度，但通常来说，消费者端更重要：每个对象只发布一次，但会被访问多次。
- en: It is equally important to understand what the publishing pointer does not do.
    First of all, there is no thread safety in the construction of the pointer. We
    have assumed that both the producer and the consumer threads share access to the
    already constructed pointer, which is initialized to null. Who constructed and
    initialized the pointer? Usually, in any data structure, there is a root pointer
    through which the entire data structure can be accessed; it was initialized by
    whatever thread constructed the initial data structure. Then there are pointers
    that serve as a root for some data element and are themselves contained in another
    data element. For now, imagine a simple singly linked list where the "next" pointer
    of every list element is the root for the next element, and the head of the list
    is the root for the entire list. The thread that produces an element of the list
    must, among other things, initialize the "next" pointer to null. Then, another
    producer can add a new element and publish it. Note that this deviates from the
    general rule that the data, once published, is immutable. This is OK, however,
    because all changes to the thread-safe unique pointer are atomic. One way or another,
    it is critical that no thread can access the pointer while it is being constructed
    (this is a very common restriction, most constructions are not thread-safe, even
    the question of their thread safety is ill-posed since the object does not exist
    until it is constructed, so no guarantees can be given).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是要理解发布指针不做的事情。首先，在指针的构造中没有线程安全性。我们假设生产者和消费者线程共享对已构造的指针的访问权，该指针初始化为null。谁构造并初始化了指针？通常，在任何数据结构中，都有一个根指针，通过它可以访问整个数据结构；它是由构造初始数据结构的任何线程初始化的。然后有一些指针，它们作为某个数据元素的根，并且它们本身包含在另一个数据元素中。现在，想象一个简单的单链表，其中每个列表元素的“下一个”指针是下一个元素的根，列表的头是整个列表的根。生产列表元素的线程必须在其他事情之间将“下一个”指针初始化为null。然后，另一个生产者可以添加一个新元素并发布它。请注意，这与一般规则不同，即一旦发布的数据就是不可变的。然而，这是可以的，因为对线程安全的唯一指针的所有更改都是原子的。无论如何，关键是在构造指针时没有线程可以访问它（这是一个非常常见的限制，大多数构造都不是线程安全的，甚至它们的线程安全性的问题都是不合适的，因为对象直到构造出来才存在，所以不能给出任何保证）。
- en: 'The next thing our pointer does not do is this: it does not offer any synchronization
    for multiple producer threads. If two threads attempt to publish their new data
    elements through the same pointer, the results are undefined, and there is a data
    race (some consumer threads will see one set of data, and others will see different
    data). If there is more than one producer thread that operates on a particular
    data structure, they must use another mechanism for synchronization.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的指针接下来没有做的事情是：它不为多个生产者线程提供任何同步。如果两个线程尝试通过相同的指针发布它们的新数据元素，结果是未定义的，并且存在数据竞争（一些消费者线程将看到一组数据，而其他线程将看到不同的数据）。如果有多个生产者线程在特定数据结构上操作，它们必须使用另一种同步机制。
- en: Finally, while our pointer implements a thread-safe publishing protocol, it
    does nothing to safely "un-publish" and delete the data. It is an owning pointer,
    so when it is deleted, so is the data it points to. However, any consumer thread
    can access the data using the value it had acquired earlier, even after the pointer
    is deleted. The issues of data ownership and lifetime must be handled in some
    other way. Ideally, we would have a point in the program where the entire data
    structure or some subset of it is known to be no longer needed; no consumer thread
    should try to access this data or even retain any pointers to it. At that point,
    the root pointer and anything accessible through it can be safely deleted. Arranging
    for such a point in the execution is a different matter entirely; it is often
    controlled by the overall algorithm.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，虽然我们的指针实现了线程安全的发布协议，但它并没有安全地“取消发布”和删除数据。它是一个拥有指针，所以当它被删除时，它指向的数据也会被删除。然而，任何消费者线程都可以使用它之前获取的值来访问数据，即使指针已被删除。数据所有权和生命周期的问题必须以其他方式处理。理想情况下，我们的程序中会有一个点，整个数据结构或其中的一部分被认为不再需要；没有消费者线程应该尝试访问这些数据，甚至保留任何指向它的指针。在那时，根指针和通过它可访问的任何内容都可以安全地删除。安排执行中的这种点是完全不同的事情；通常由整体算法控制。
- en: Sometimes we want a pointer that manages both the creation and the deletion
    of the data in a thread-safe way. In this case, we need a thread-safe shared pointer.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要一个指针以线程安全的方式管理数据的创建和删除。在这种情况下，我们需要一个线程安全的共享指针。
- en: Atomic shared pointer
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原子共享指针
- en: 'If we cannot guarantee that there is a known point in the program where the
    data can be safely deleted, we have to keep track of how many consumer threads
    hold valid pointers to the data. If we want to delete this data, we have to wait
    until there is only one pointer to it in the entire program; then, it is safe
    to delete the data and the pointer itself (or at least reset it to null). This
    is a typical job for a shared pointer that does reference counting: it counts
    how many pointers to the same object are still out there in the program; the data
    is deleted by the last such pointer.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不能保证程序中有一个已知的点可以安全地删除数据，我们必须跟踪有多少消费者线程持有数据的有效指针。如果我们想删除这些数据，我们必须等到整个程序中只有一个指向它的指针；然后，才能安全地删除数据和指针本身（或者至少将其重置为null）。这是共享指针的典型工作：它对同一对象的指针在程序中还有多少进行引用计数；数据由最后一个这样的指针删除。
- en: 'When talking about thread-safe shared pointers, it is vitally important to
    understand precisely what guarantees are required from the pointer. The C++ standard
    shared pointer, `std::shared_ptr`, is often referred to as thread-safe. Specifically,
    it offers the following guarantee: if multiple threads operate on different shared
    pointers that all point to the same object, then the operations on the reference
    counter are thread safe even if two threads cause the counter to change at the
    same time. For example, if one thread is making a copy of its shared pointer while
    another thread is deleting its shared pointer and the reference count was `N`
    before these operations started, the counter will go up to `N+1`, then back to
    `N` (or down first, then up, depending on the actual order of execution) and in
    the end will have the same value `N`. The intermediate value could be either `N+1`
    or `N-1`, but there is no data race, and the behavior is well defined, including
    the final state. This guarantee implies that the operations on the reference counter
    are atomic; indeed, the reference counter is an atomic integer and the implementation
    used `fetch_add()` to atomically increment or decrement it.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 谈论线程安全的共享指针时，准确理解指针需要什么保证是非常重要的。C++标准共享指针`std::shared_ptr`经常被称为线程安全。具体来说，它提供了以下保证：如果多个线程操作指向同一对象的不同共享指针，那么即使两个线程同时导致计数器发生变化，对引用计数器的操作也是线程安全的。例如，如果一个线程正在复制其共享指针，而另一个线程正在删除其共享指针，并且在这些操作开始之前引用计数为`N`，那么计数器将增加到`N+1`，然后返回到`N`（或者先减少，然后增加，取决于实际的执行顺序），最终将具有相同的值`N`。中间值可以是`N+1`或`N-1`，但没有数据竞争，行为是明确定义的，包括最终状态。这一保证意味着对引用计数器的操作是原子的；实际上，引用计数器是一个原子整数，并且实现使用`fetch_add()`来原子地增加或减少它。
- en: 'This guarantee applies as long as no two threads share access to the same shared
    pointer. How to get each thread its own shared pointer is a separate issue: since
    all shared pointers pointing to the same object must be created from the very
    first such pointer, these pointers had to have been passed from one thread to
    another at some point in time. For simplicity, let us assume, for a moment, that
    the code that made copies of the shared pointer is protected by a mutex. If two
    threads access the same shared pointer, then all bets are off. For example, if
    one thread is trying to copy the shared pointer while another thread is resetting
    it at the same time, the results are undefined. In particular, the standard shared
    pointer cannot be used to implement the publishing protocol. However, once the
    copies of the shared pointer have been distributed to all threads (possibly under
    lock), the shared ownership is maintained, and the deletion of the object is handled
    in a thread-safe manner. The object will be deleted once the last shared pointer
    that points to it is deleted. Note that, since we agreed that each particular
    shared pointer is never handled by more than one thread, this is completely safe.
    If, during the execution of the program, the time comes when there is only one
    shared pointer that owns our object, then there is also only one thread that can
    access this object. Other threads cannot make copies of this pointer (we don''t
    let two threads share the same pointer object) and don''t have any other way to
    get a pointer to the same object, so the deletion will proceed effectively single-threaded.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 只要没有两个线程共享对同一个共享指针的访问，此保证就适用。如何为每个线程获取其自己的共享指针是一个单独的问题：因为指向同一对象的所有共享指针必须从第一个这样的指针开始创建，这些指针必须曾经在某个时间点从一个线程传递到另一个线程。为简单起见，让我们假设一下，做共享指针的复制的代码受到互斥锁的保护。如果两个线程访问同一个共享指针，那么一切都不确定。例如，如果一个线程正在尝试复制共享指针，而另一个线程同时正在重置它，结果是未定义的。特别是，标准共享指针不能用于实现发布协议。然而，一旦共享指针的副本已经分发给所有线程（可能在锁定状态下），共享所有权就得到了维护，并且对象的删除是以线程安全的方式处理的。一旦指向对象的最后一个共享指针被删除，对象就会被删除。请注意，由于我们同意每个特定的共享指针永远不会被多个线程处理，这是完全安全的。如果在程序执行过程中，当只有一个共享指针拥有我们的对象时，那么也只有一个线程可以访问这个对象。其他线程无法复制这个指针（我们不让两个线程共享同一个指针对象），也没有其他方法获得指向同一对象的指针，因此删除将有效地以单线程方式进行。
- en: 'This is all well and good, but what if we cannot guarantee that two threads
    won''t try to access the same shared pointer? The first example of such access
    is our publishing protocol: the consumer threads are reading the value of the
    pointer while the producer thread may be changing it. We need the operations on
    the shared pointer itself to be atomic. In C++20, we can do just that: it lets
    us write `std::atomic<std::shared_ptr<T>>`. Note that the early proposals featured
    a new class, `std::atomic_shared_ptr<T>`, instead. In the end, this is not the
    path that was chosen.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都很好，但如果我们不能保证两个线程不会尝试访问同一个共享指针怎么办？这种访问的第一个例子是我们的发布协议：消费者线程正在读取指针的值，而生产者线程可能正在更改它。我们需要共享指针本身的操作是原子的。在C++20中，我们可以做到这一点：它让我们编写`std::atomic<std::shared_ptr<T>>`。请注意，早期的提案中提到了一个新类`std::atomic_shared_ptr<T>`。最终，这不是选择的路径。
- en: 'If you do not have a C++20-compliant compiler and the corresponding standard
    library or cannot use C++20 in your code, you can still do atomic operations on
    `std::shared_ptr`, but you must do so explicitly. In order to publish the object
    using the pointer `p_` that is shared between all threads, the producer thread
    must do this:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有符合C++20标准的编译器和相应的标准库，或者无法在您的代码中使用C++20，您仍然可以在`std::shared_ptr`上执行原子操作，但必须明确这样做。为了使用在所有线程之间共享的指针`p_`发布对象，生产者线程必须这样做：
- en: '[PRE13]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'On the other hand, to acquire the pointer, the consumer thread must do this:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，为了获取指针，消费者线程必须这样做：
- en: '[PRE14]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The major downside of this approach, compared to the C++20 atomic shared pointer,
    is that there is no protection against accidental non-atomic access. It is up
    to the programmer to remember to always use atomic functions to operate on the
    shared pointer.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与C++20原子共享指针相比，这种方法的主要缺点是没有保护意外的非原子访问。程序员必须记住始终使用原子函数来操作共享指针。
- en: 'It should be noted that, while convenient, `std::shared_ptr` is not a particularly
    efficient pointer, and the atomic accesses make it even slower. We can compare
    the speed of publishing an object using the thread-safe publishing pointer from
    the last section versus the shared pointer with explicit atomic accesses:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，虽然方便，`std::shared_ptr`并不是特别高效的指针，而原子访问使其变得更慢。我们可以比较使用上一节中的线程安全发布指针与显式原子访问的共享指针发布对象的速度：
- en: '![Figure 6.7 – The performance of the atomic shared publishing pointer (consumer
    threads)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.7 - 原子共享发布指针的性能（消费者线程）'
- en: '](img/Figure_6.7_B16229.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_6.7_B16229.jpg)'
- en: Figure 6.7 – The performance of the atomic shared publishing pointer (consumer
    threads)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 - 原子共享发布指针的性能（消费者线程）
- en: 'Again, the numbers should be compared with those from *Figure 6.5*: the publishing
    pointer is 60 times faster on one thread, and the advantage increases with the
    number of threads. Of course, the whole point of the shared pointer is that it
    provides shared resource ownership, so naturally, it takes more time to do more
    work. The point of the comparison is to show the cost of this shared ownership:
    if you can avoid it, your program will be much more efficient.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这些数字应该与*图6.5*中的数字进行比较：在一个线程上，发布指针比共享指针快60倍，随着线程数量的增加，优势也会增加。当然，共享指针的整个目的是提供共享资源所有权，因此自然需要更多时间来完成更多的工作。比较的重点是显示这种共享所有权的成本：如果可以避免，程序将更加高效。
- en: 'Even if you need shared ownership (and there are some concurrent data structures
    that are really hard to design without it), usually, you can do much better if
    you design your own reference-counted pointer with limited functionality and optimal
    implementation. One very common approach is to use intrusive reference counting.
    An **intrusive shared pointer** stores its reference count in the object it points
    to. When designed for a specific object, such as a list node in our particular
    data structure, the object is designed with the shared ownership in mind and contains
    a reference counter. Otherwise, we can use a wrapper class for almost any type
    and augment it with a reference counter:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 即使需要共享所有权（有一些并发数据结构确实很难在没有共享所有权的情况下设计），通常情况下，如果设计自己的具有有限功能和最佳实现的引用计数指针，通常可以做得更好。一种非常常见的方法是使用侵入式引用计数。**侵入式共享指针**将其引用计数存储在其指向的对象中。当为特定对象设计时，例如我们特定数据结构中的列表节点，对象是以共享所有权为目的设计的，并包含一个引用计数器。否则，我们可以为几乎任何类型使用包装类，并用引用计数器增强它：
- en: '[PRE15]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When decrementing the reference count, it is important to know when it reaches
    0 (or was 1 before decrementing): the shared pointer must then delete the object.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在减少引用计数时，重要的是要知道何时达到0（或在减少之前为1）：共享指针必须删除对象。
- en: The implementation of even the simplest atomic shared pointer is quite lengthy;
    a very rudimentary example can be found in the sample code for this chapter. Again,
    this example contains only the bare minimum necessary for the pointer to correctly
    perform several tasks such as publishing an object and accessing the same pointer
    concurrently by multiple threads. The aim of the example is to make it easier
    to understand the essential elements of implementing such pointer (and even then,
    the code is several pages long).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是最简单的原子共享指针的实现也相当冗长；本章的示例代码中可以找到一个非常基本的示例。再次强调，该示例仅包含使指针能够正确执行发布对象和多个线程同时访问同一指针等多项任务所必需的最低限度。该示例的目的是使实现这种指针的基本要素更容易理解（即使如此，代码也有几页长）。
- en: 'In addition to using an intrusive reference counter, an application-specific
    shared pointer can forgo other features of `std::shared_ptr`. For example, many
    applications do not require a weak pointer, but there is an overhead for supporting
    it even if it''s never used. A minimalistic reference-counted pointer can be several
    times more efficient than the standard one:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用侵入式引用计数外，特定于应用程序的共享指针可以放弃`std::shared_ptr`的其他功能。例如，许多应用程序不需要弱指针，但即使从未使用过，支持它也会带来开销。一个最简化的引用计数指针可以比标准指针高出几倍效率：
- en: '![Figure 6.8 – The performance of a custom atomic shared publishing pointer
    (consumer threads)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.8 - 自定义原子共享发布指针的性能（消费者线程）'
- en: '](img/Figure_6.8_B16229.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_6.8_B16229.jpg)'
- en: Figure 6.8 – The performance of a custom atomic shared publishing pointer (consumer
    threads)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 - 自定义原子共享发布指针的性能（消费者线程）
- en: It is similarly more efficient for assignment and reassignment of the pointer,
    atomic exchange of two pointers, and other atomic operations on the pointer. Even
    this shared pointer is still much less efficient than a unique pointer, so again,
    if you can manage the data ownership explicitly, without reference-counting, do
    so.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于指针的赋值和重新赋值、两个指针的原子交换以及指针的其他原子操作，这样做同样更有效。即使这种共享指针仍然比唯一指针效率低得多，所以如果可以明确管理数据所有权而不使用引用计数，那么请这样做。
- en: 'We now have the two key building blocks of almost any data structure: we can
    add new data and publish it (reveal it to other threads), and we can track the
    ownership, even across threads (although it comes at a price).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们几乎可以构建任何数据结构的两个关键构件：我们可以添加新数据并发布它（向其他线程公开），甚至可以跨线程跟踪所有权（尽管这是有代价的）。
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we have learned about the performance of the basic building
    blocks of any concurrent program. All accesses to the shared data must be protected
    or synchronized, but there is a wide range of options when it comes to implementing
    such synchronization. While mutex is the most commonly used and the simplest alternative,
    we have learned several other, better-performing options: spinlocks and their
    variants, as well as lock-free synchronization.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经了解了任何并发程序的基本构建块的性能。所有对共享数据的访问都必须受到保护或同步，但在实现这种同步时有很多选择。虽然互斥锁是最常用和最简单的选择，但我们还学习了其他几种性能更好的选择：自旋锁及其变体，以及无锁同步。
- en: The key to an efficient concurrent program is to make as much data as possible
    local to one thread and minimize the operations on the shared data. The requirements
    specific to each problem usually dictate that such operations cannot be eliminated
    completely, so this chapter is all about making the concurrent data accesses more
    efficient.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 高效并发程序的关键是尽可能将数据局部化到一个线程，并最小化对共享数据的操作。每个问题特定的要求通常决定了这些操作不能完全被消除，因此本章重点是使并发数据访问更加高效。
- en: We studied how to count or accumulate results across multiple threads, again
    with and without locks. Understanding the data dependency issues led us to the
    discovery of the publishing protocol and its implementation in several thread-safe
    smart pointers, suitable for different applications.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何在多个线程之间计数或累积结果，有锁和无锁的情况下。了解数据依赖性问题使我们发现了发布协议及其在几种线程安全的智能指针中的实现，适用于不同的应用程序。
- en: We are now well prepared to take our study to the next level and put several
    of these building blocks together in the form of more complex thread-safe data
    structures. In the next chapter, you will learn how to use these techniques to
    design practical data structures for concurrent programs.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经准备好将我们的研究提升到下一个水平，并将其中几个构建块组合成更复杂的线程安全数据结构。在下一章中，您将学习如何使用这些技术来设计并发程序的实用数据结构。
- en: Questions
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are the defining properties of lock-based, lock-free, and wait-free programs?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 锁定型、无锁型和无等待型程序的定义特性是什么？
- en: If an algorithm is wait-free, does it mean that it will scale perfectly?
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果算法是无等待的，这是否意味着它将完美扩展？
- en: What are the drawbacks of the locks that prompt us to look for alternatives?
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 锁定的缺点是什么，促使我们寻找替代方案？
- en: What is the difference between a shared counter and a shared index into an array
    or another container?
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 共享计数器和数组或其他容器中的共享索引之间有什么区别？
- en: What is the key advantage of the publishing protocol?
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发布协议的主要优势是什么？
