["```cpp\ncublasHandle_t handle;\ncublasCreate(&handle);\n\n.. { data operation } ..\n\ncublasSgemm(...);\n\n.. { data operation } ..\n\ncublasDestroy(handle);\n```", "```cpp\ncublasStatus_t cublasSgemm(cublasHandle_t handle,\n                          cublasOperation_t transa, \n                           cublasOperation_t transb,\n                           int m, int n, int k,\n                           const float *alpha,\n                           const float *A, int lda,\n                           const float *B, int ldb,\n                           const float *beta,\n                           float *C, int ldc);\n```", "```cpp\ncublasHandle_t handle;\n\n    // Prepare input matrices\n    float *A, *B, *C;\n    int M, N, K;\n    float alpha, beta;\n\n    M = 3;    N = 4;    K = 7;\n    alpha = 1.f;    beta = 0.f;\n\n    // create cuBLAS handle\n    cublasCreate(&handle);\n\n    srand(2019);\n    A = getMatrix(K, M);\n    B = getMatrix(N, K);\n    C = getMatrix(M, N);\n\n    std::cout << \"A:\" << std::endl;\n    printMatrix(A, K, M);\n    std::cout << \"B:\" << std::endl;\n    printMatrix(B, N, K);\n    std::cout << \"C:\" << std::endl;\n    printMatrix(C, M, N);\n\n    // Gemm\n    cublasSgemm(handle, CUBLAS_OP_T, CUBLAS_OP_T, \n        M, N, K, &alpha, A, K, B, N, &beta, C, M);\n\n    cudaDeviceSynchronize();\n    std::cout << \"C out:\" << std::endl;\n    printMatrix(C, M, N);\n\n    cublasDestroy(handle);\n    cudaFree(A);    cudaFree(B);    cudaFree(C);\n    return 0;\n```", "```cpp\n$ nvcc -run -gencode arch=compute_70,code=sm_70 -lcublas -o cublasSgemm ./cublasSgemm.cpp\n```", "```cpp\nA:\n 0.0492 0.4790 0.0226\n 0.7750 0.2794 0.8169\n 0.3732 0.6248 0.2636\n 0.9241 0.5841 0.8532\n 0.7188 0.5052 0.5398\n 0.9869 0.6572 0.0520\n 0.6815 0.7814 0.5988\nB:\n 0.8957 0.0481 0.7958 0.7825 0.3264 0.5189 0.5018\n 0.4559 0.6342 0.0759 0.5051 0.1132 0.0985 0.2802\n 0.3926 0.9153 0.6534 0.0174 0.1790 0.5775 0.6015\n 0.0322 0.2963 0.1068 0.5720 0.2832 0.7640 0.6240\nC:\n 0.9647 0.5454 0.2229 0.8604\n 0.5935 0.0186 0.6430 0.9198\n 0.5375 0.1448 0.3757 0.1718\nC out:\n 1.1785 2.5682 2.4854 0.6066\n 0.5817 0.8091 1.1724 2.0773\n 2.0882 1.4503 2.1331 1.8450\n```", "```cpp\ncublasXtHandle_t handle;\ncublasXtCreate(&handle);\n\ncudaGetDeviceCount(&num_of_total_devices);\ndevices = (int *)calloc(num_of_devices, sizeof(int));\nfor (int i = 0; i < num_of_devices; i++)\n    devices[i] = i;\ncublasXtDeviceSelect(handle, num_of_devices, devices);\n\ncublasXtSgemm( ... );\n\ncublasXtDestroy(handle);\n```", "```cpp\n#include <cublas_v2.h>\n#include \"helper.cuh\"    // for CBuffer and printMatrix()\n```", "```cpp\nint M = 4, N = 5, K = 6;\nCBuffer<half> A, B;\nCBuffer<float> C;\n\nA.init(K * M, true);\nB.init(N * K, true);\nC.init(N * M, true);\n```", "```cpp\ncudaDataType TYPE_A, TYPE_B, TYPE_C;\nif (typeid(*A.h_ptr_) == typeid(float)) {\n    TYPE_A = TYPE_B = CUDA_R_32F;\n}\nelse if (typeid(*A.h_ptr_) == typeid(half)) {\n    TYPE_A = TYPE_B = CUDA_R_16F;\n}\nelse if (typeid(*A.h_ptr_) == typeid(int8_t)) {\n    TYPE_A = TYPE_B = CUDA_R_8I;\n}\nelse {\n    printf(\"Not supported precision\\n\");\n    return -1;\n}\n\nif (typeid(*C.h_ptr_) == typeid(float)) {\n    TYPE_C = CUDA_R_32F;\n}\nelse if (typeid(*C.h_ptr_) == typeid(int)) {\n    TYPE_C = CUDA_R_32I;\n}\nelse {\n    printf(\"Not supported precision\\n\");\n    return -1;\n}\n```", "```cpp\nfloat alpha = 1.f, beta = 0.f;\ncublasHandle_t cublas_handle;\ncublasCreate(&cublas_handle);\n```", "```cpp\nA.cuda(true);\nB.cuda(true);\nC.cuda(true);\n```", "```cpp\ncublasGemmEx(cublas_handle,\n                CUBLAS_OP_N, CUBLAS_OP_N,\n                M, N, K,\n                &alpha, A.d_ptr_, TYPE_A, M, B.d_ptr_, TYPE_B, K,\n                &beta,  C.d_ptr_, TYPE_C, M, TYPE_C,\n                CUBLAS_GEMM_DEFAULT);\n```", "```cpp\nstd::cout << \"A:\" << std::endl;\nprintMatrix(A.h_ptr_, K, M);\nstd::cout << \"B:\" << std::endl;\nprintMatrix(B.h_ptr_, N, K);\n```", "```cpp\ntemplate <typename T>\nvoid printMatrix(const T *matrix, const int ldm, const int n) {\n    std::cout << \"[\" << __FUNCTION__ << \"]:: \n                Not supported type request\" << std::endl;\n}\nvoid printMatrix(const float *matrix, const int ldm, const int n) {\n    for (int j = 0; j < n; j++) {\n        for (int i = 0; i < ldm; i++)\n            std::cout << std::fixed << std::setw(8) << \n                         std::setprecision(4) << \n                         matrix[IDX2C(i, j, ldm)];\n        std::cout << std::endl;\n    }\n}\nvoid printMatrix(const half *matrix, const int ldm, const int n) {\n    for (int j = 0; j < n; j++) {\n        for (int i = 0; i < ldm; i++)\n            std::cout << std::fixed << std::setw(8) <<\n                         std::setprecision(4) << __half2float(matrix[IDX2C(i, j, ldm)]);\n        std::cout << std::endl;\n    }\n}\n... ( functions for other data types ) ...\n```", "```cpp\n$ nvcc -run -m64 -std=c++11 -I/usr/local/cuda/samples/common/inc -gencode arch=compute_70,code=sm_70 -lcublas -o cublasGemmEx ./cublasGemmEx.cu\nA:\n 0.0049 0.0479 0.0023 0.0775 0.0279 0.0817\n 0.0373 0.0625 0.0264 0.0924 0.0584 0.0853\n 0.0719 0.0505 0.0540 0.0987 0.0657 0.0052\n 0.0682 0.0781 0.0599 0.0896 0.0048 0.0796\nB:\n 0.0624 0.0965 0.0545 0.0223 0.0861\n 0.0594 0.0019 0.0643 0.0920 0.0537\n 0.0145 0.0376 0.0172 0.0221 0.0881\n 0.0285 0.0319 0.0161 0.0677 0.0235\n 0.0814 0.0695 0.0414 0.0392 0.0296\n 0.0446 0.0688 0.0403 0.0018 0.0971\nC:\n 0.0509 0.0117 0.0877 0.0445 0.0830\n 0.0742 0.0242 0.0136 0.0625 0.0681\n 0.0362 0.0046 0.0265 0.0963 0.0638\n 0.0070 0.0446 0.0516 0.0194 0.0089\nC out:\n 0.0153 0.0228 0.0143 0.0292 0.0113\n 0.0200 0.0118 0.0214 0.0081 0.0138\n 0.0098 0.0168 0.0132 0.0199 0.0125\n 0.0269 0.0120 0.0222 0.0085 0.0228\n```", "```cpp\ncurandGenerator_t curand_gen;\ncurandCreateGenerator(&curand_gen, CURAND_RNG_PSEUDO_DEFAULT);\ncurandSetPseudoRandomGeneratorSeed(curand_gen, 2019UL);\n```", "```cpp\ncurandGenerateUniform(curand_gen, random_numbers_on_device_memory, length);\n```", "```cpp\ncurandDestroyGenerator(curand_gen);\n```", "```cpp\n#include <iostream>\n#include <iomanip>\n#include <curand.h>\n```", "```cpp\n#define IDX2C(i, j, ld) (((j) * (ld)) + (i))\n\ntemplate <typename T>\nvoid printMatrix(const T *matrix, const int ldm, const int n)\n{\n    for (int j = 0; j < ldm; j++) {\n        for (int i = 0; i < n; i++)\n            std::cout << std::fixed << std::setw(12) \n                    << std::setprecision(4) << matrix[IDX2C(i, j, ldm)];\n        std::cout << std::endl;\n    }\n}\n```", "```cpp\n// create curand generator & set random seed\ncurandGenerator_t curand_gen;\ncurandCreateGenerator(&curand_gen, CURAND_RNG_PSEUDO_DEFAULT);\ncurandSetPseudoRandomGeneratorSeed(curand_gen, 2019UL);\n```", "```cpp\nsize_t size = M * N;\nunsigned int *np_random;\nfloat *fp_random;\ncudaMallocManaged((void**)&np_random, sizeof(*np_random) * size);\ncudaMallocManaged((void**)&fp_random, sizeof(*fp_random) * size);\n```", "```cpp\n// random number generation\nstd::cout << \"Generated random numbers\" << std::endl;\ncurandGenerate(curand_gen, np_random, size);\ncudaDeviceSynchronize();\nprintMatrix(np_random, M, N);\n\n// uniform distributed random number generation\nstd::cout << \"Generated uniform random numbers\" << std::endl;\ncurandGenerateUniform(curand_gen, fp_random, size);\ncudaDeviceSynchronize();\nprintMatrix(fp_random, M, N);\n```", "```cpp\n// terminates used resources\ncurandDestroyGenerator(curand_gen);\ncudaFree(np_random);\ncudaFree(fp_random);\n```", "```cpp\n$ nvcc -run -gencode arch=compute_70,code=sm_70 -lcurand -o curand_host curand_host.cpp\nGenerated random numbers\n 3395652512 793372546 2133571103 595847267 2461872808\n 595847267 2461872808 500895635 498154070 2385617847\n 498154070 2385617847 196336856 388563169 745758309\nGenerated uniform random numbers\n 0.7134 0.0830 0.1458 0.2099 0.6066\n 0.2099 0.6066 0.3078 0.5122 0.8856\n 0.5122 0.8856 0.3530 0.8477 0.8370\n```", "```cpp\ncudaMalloc((void **)&devStates, length * sizeof(curandState_t));\n```", "```cpp\nint idx = blockIdx.x * blockDim.x + threadIdx.x;\ncurand_init(2019UL, idx, 0, &state[idx]);\ngenerated_out[idx] = curand_uniform(&state[idx]);\n```", "```cpp\ncudaFree(devStates);\n```", "```cpp\n#include <iostream>\n#include <iomanip>\n#include <curand_kernel.h>\n```", "```cpp\n  __global__ void setup_kernel(curandState_t *state)\n  {\n      int idx = blockIdx.x * blockDim.x + threadIdx.x;\n      // Each thread gets same seed, \n      // a different sequence number, no offset */\n      curand_init(2019UL, idx, 0, &state[idx]);\n  }\n```", "```cpp\n__global__ void generate_kernel(unsigned int *generated_out, \n                                curandState_t *state)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    generated_out[idx] = curand(&state[idx]) & 0xFF;\n}\n\n__global__ void generate_uniform_kernel(float *generated_out, \n                                        curandState_t *state)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    generated_out[idx] = curand_uniform(&state[idx]);\n}\n```", "```cpp\ncudaMalloc((void **)&devStates, sizeof(curandState) * size);\nsetup_kernel<<<(size + BLOCK_DIM - 1) / BLOCK_DIM, BLOCK_DIM>>>(devStates);\n```", "```cpp\n// random number generation\nstd::cout << \"Generated random numbers\" << std::endl;\ncudaMallocManaged((void**)&np_random, sizeof(*np_random) * size);\ngenerate_kernel<<<(size + BLOCK_DIM - 1) / BLOCK_DIM, BLOCK_DIM>>>\n                (np_random, const_cast<curandState_t *>(devStates));\ncudaDeviceSynchronize();\nprintMatrix(np_random, M, N);\n```", "```cpp\n// uniform distributed random number generation\nstd::cout << \"Generated uniform random numbers\" << std::endl;\ncudaMallocManaged((void**)&fp_random, sizeof(*fp_random) * size);\ngenerate_uniform_kernel<<<(size + BLOCK_DIM - 1) / BLOCK_DIM, BLOCK_DIM>>>\n                (fp_random, const_cast<curandState_t *>(devStates));\ncudaDeviceSynchronize();\nprintMatrix(fp_random, M, N);\n```", "```cpp\n// terminates used resources\ncurandDestroyGenerator(curand_gen);\ncudaFree(np_random);\ncudaFree(fp_random);\n```", "```cpp\n$ nvcc -run -gencode arch=compute_70,code=sm_70 -lcurand -o curand_device curand_device.cpp\nGenerated random numbers\n 3395652512 793372546 2133571103 595847267 2461872808\n 595847267 2461872808 500895635 498154070 2385617847\n 498154070 2385617847 196336856 388563169 745758309\nGenerated uniform random numbers\n 0.8064 0.2783 0.2971 0.2386 0.7491\n 0.2386 0.7491 0.4782 0.1060 0.2922\n 0.1060 0.2922 0.1823 0.6199 0.9137\n```", "```cpp\nnamespace fp16{\n__global__ void float2half_kernel(half *out, float *in)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    out[idx] = __float2half(in[idx]);\n}\n\nvoid float2half(half *out, float *in, size_t length)\n{\n    float2half_kernel<<< (length + BLOCK_DIM - 1) / BLOCK_DIM, BLOCK_DIM >>>(out, in);\n}\n```", "```cpp\ntemplate <typename T>\ntypename std::enable_if<(std::is_same<T, float>::value), float>::type\n*curand(curandGenerator_t generator, size_t length)\n{\n    T *buffer = nullptr;\n    cudaMalloc((void **)&buffer, length * sizeof(float));\n    curandGenerateUniform(generator, buffer, length);\n    return buffer;\n}\ntemplate <typename T>\ntypename std::enable_if<std::is_same<T, half>::value, half>::type\n*curand(curandGenerator_t generator, size_t length)\n{\n    T *buffer = nullptr;\n    float *buffer_fp32;\n\n    cudaMalloc((void **)&buffer_fp32, length * sizeof(float));\n    curandGenerateUniform(generator, buffer_fp32, length);\n\n    cudaMalloc((void **)&buffer, length * sizeof(T));\n    fp16::float2half(buffer, buffer_fp32, length);\n    cudaFree(buffer_fp32);\n\n    return buffer;\n}\n```", "```cpp\nvoid *d_A, *d_B, *d_C;\ncudaDataType AType, BType, CType, computeType;\nint M = 8192, N = 8192, K = 8192;\nfloat alpha = 1.f, beta = 1.f;\nstd::string precision = \"fp32\";\nbool tensor_core = true;\n```", "```cpp\nif (precision == \"fp32\") {\n    auto *a = curand<float>(curand_gen, M * K);\n    auto *b = curand<float>(curand_gen, K * N);\n    auto *c = curand<float>(curand_gen, M * N);\n    AType = BType = CType = CUDA_R_32F;\n    computeType = CUDA_R_32F;\n    d_A = a, d_B = b, d_C = c;\n}\nelse if (precision == \"fp16\") {\n    auto *a = curand<half>(curand_gen, M * K);\n    auto *b = curand<half>(curand_gen, K * N);\n    auto *c = curand<float>(curand_gen, M * N);\n    AType = BType = CUDA_R_16F, CType = CUDA_R_32F;\n    computeType = CUDA_R_32F;\n    d_A = a, d_B = b, d_C = c;\n}\nelse {\n    exit(EXIT_FAILURE);\n}\n```", "```cpp\ncublasCreate(&cublas_handle);\ncurandCreateGenerator(&curand_gen, CURAND_RNG_PSEUDO_DEFAULT);\ncurandSetPseudoRandomGeneratorSeed(curand_gen, 2019UL);\n```", "```cpp\ncublasGemmAlgo_t gemm_algo = (tensor_core) ? \n                             CUBLAS_GEMM_DEFAULT_TENSOR_OP : CUBLAS_GEMM_DEFAULT;\n```", "```cpp\ncublasGemmEx(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N,\n             M, N, K,\n             &alpha, d_A, AType, M, d_B, BType, K,\n             &beta,  d_C, CType, M,\n             computeType, gemm_algo);\n```", "```cpp\n#include <iostream>\n#include <iomanip>\n#include <cuda_runtime.h>\n#include <cufft.h>\n#include <curand.h>\n```", "```cpp\ntypedef cufftReal Real;\ntypedef cufftComplex Complex;\n```", "```cpp\ncudaMallocManaged((void**)&p_sample, sizeof(Real) * sample_size * batch_size);\ncudaMalloc((void**)&d_freq, sizeof(Complex) * sample_size * batch_size);\n```", "```cpp\ncurandGenerator_t curand_gen;\ncurandCreateGenerator(&curand_gen, CURAND_RNG_PSEUDO_DEFAULT);\ncurandSetPseudoRandomGeneratorSeed(curand_gen, 2019UL);\ncurandGenerateUniform(curand_gen, p_sample, sample_size * batch_size);\n```", "```cpp\nint rank = 1;\nint stride_sample = 1, stride_freq = 1;\nint dist_sample = sample_size, dist_freq = sample_size / 2 + 1;\nint embed_sample[] = {0};\nint embed_freq[] = {0};\ncufftPlanMany(&plan_forward, rank, &sample_size,\n                             embed_sample, stride_sample, \n                             dist_sample, \n                             embed_freq, stride_freq, dist_freq,\n                             CUFFT_R2C, batch_size);\ncufftPlanMany(&plan_inverse, rank, &sample_size,\n                             embed_freq, stride_freq, dist_freq, \n                             embed_sample, stride_sample, \n                             dist_sample,\n                             CUFFT_C2R, batch_size);\n```", "```cpp\ncufftExecR2C(plan_forward, p_sample, d_freq);\ncufftExecC2R(plan_inverse, d_freq, p_sample);\n\n```", "```cpp\n$ nvcc -run -gencode arch=compute_70,code=sm_70 -lcufft -lcurand -o cufft.1d cufft.1d.cpp\n```", "```cpp\nFFT operation time for 1048576 elements with 512 batch..\nForward (ms): 21.5322\nInverse (ms): 21.4\n```", "```cpp\nint rank = 1;\nint stride_sample = 1, stride_freq = 1;\nlong long int dist_sample = sample_size, dist_freq = sample_size / 2 + 1;\nlong long embed_sample[] = {0};\nlong long embed_freq[] = {0};\nsize_t workSize = 0;\ncufftCreate(&plan_forward);\ncufftXtMakePlanMany(plan_forward, \n        rank, &sample_size, \n        embed_sample, stride_sample, dist_sample, CUDA_R_16F, \n        embed_freq, stride_freq, dist_freq, CUDA_C_16F, \n        batch_size, &workSize, CUDA_C_16F);\ncufftCreate(&plan_inverse);\ncufftXtMakePlanMany(plan_inverse,\n        rank, &sample_size,\n        embed_freq, stride_freq, dist_freq, CUDA_C_16F,\n        embed_sample, stride_sample, dist_sample, CUDA_R_16F,\n        batch_size, &workSize, CUDA_R_16F);\n```", "```cpp\ntemplate <typename T>\ntypename std::enable_if<std::is_same<T, half>::value>::type\ncurand(curandGenerator_t generator, T *buffer, size_t length) {\n    float *buffer_fp32;\n\n    cudaMalloc((void **)&buffer_fp32, length * sizeof(float));\n    curandGenerateUniform(generator, buffer_fp32, length);\n\n    // convert generated single floating to half floating\n    fp16::float2half(buffer, buffer_fp32, length);\n    cudaFree(buffer_fp32);\n}\n```", "```cpp\n FFT operation time for 1048576 elements with 512 batch..\nForward (ms): 15.3236\nInverse (ms): 15.4881\n```", "```cpp\ncufftHandle cufft_plan;\nint n_gpu = 2, devices[2] = {0,1};\ncufftCreaet(&cufft_plan); // create an empty plan\ncufftXtSetGPUs(cufft_plan, n_gpu, devices); // set multi-gpu information\n```", "```cpp\nsize_t *work_size = (size_t*) new size_t[num_gpus];\ncufftXtMakePlanMany(cufft_plan, 1 &sample_size, \n                    nullptr, 1, 1, CUDA_C_32F, \n                    nullptr, 1, 1,  CUDA_C_32F, \n                    batch_size, work_size, CUDA_C_32F);\n```", "```cpp\ncudaLibXtDesc *d_sample;\ncufftXtMalloc(cufft_plan, &d_sample, CUFFT_XT_FORMAT_INPLACE);\ncufftXtMemcpy(cufft_plan, d_sample, h_sample, CUFFT_COPY_HOST_TO_DEVICE);\n```", "```cpp\ncufftXtExecDesciptor(cufft_plan, d_sample, d_sample, CUFFT_FORWARD);\n```", "```cpp\n$ sudo apt-get install libfreeimage-dev\n```", "```cpp\n$ wget http://downloads.sourceforge.net/freeimage/FreeImage3180.zip\n$ unzip FreeImage3180.zip\n$ cd FreeImage && make -j && sudo make install\n```", "```cpp\n#include <iostream>\n#include <iomanip>\n#include <cassert>\n#include <cstring>\n#include <cuda_runtime.h>\n#include <npp.h>\n#include <FreeImage.h>\n#include <helper_timer.h>\n```", "```cpp\nstruct ImageInfo_t\n{\n    /* image information */\n    FIBITMAP* dib; // FreeImage bitmap\n    int nHeight;   // image height size\n    int nWidth;    // image width size\n    int nPitch;    // image pitch size\n    int nBPP;      // Bit Per Pixel (i.e. 24 for BGR color)\n    int nChannel;  // number of channels \n    BYTE* pData;   // bytes from freeimage library\n\n    /* CUDA */\n    Npp8u *pDataCUDA; // CUDA global memory for nppi processing\n    int nPitchCUDA;   // image pitch size on CUDA device\n};\n```", "```cpp\nvoid LoadImage(const char *szInputFile, ImageInfo_t &srcImage) {\n    FIBITMAP *pSrcImageBitmap = FreeImage_Load(FIF_JPEG, szInputFile, JPEG_DEFAULT);\n    if (!pSrcImageBitmap) {\n        std::cout << \"Couldn't load \" << szInputFile << std::endl;\n        FreeImage_DeInitialise();\n        exit(1);\n    }\n\n    srcImage.dib = pSrcImageBitmap;\n    srcImage.nWidth = FreeImage_GetWidth(pSrcImageBitmap);\n    srcImage.nHeight = FreeImage_GetHeight(pSrcImageBitmap);\n    srcImage.nPitch = FreeImage_GetPitch(pSrcImageBitmap);\n    srcImage.nBPP = FreeImage_GetBPP(pSrcImageBitmap);\n    srcImage.pData = FreeImage_GetBits(pSrcImageBitmap);\n    assert(srcImage.nBPP == (unsigned int)24); // BGR color image\n    srcImage.nChannel = 3;\n}\n```", "```cpp\nNppiSize GetImageSize(ImageInfo_t imageInfo)\n{\n    NppiSize imageSize;\n\n    imageSize.width = imageInfo.nWidth;\n    imageSize.height = imageInfo.nHeight;\n\n    return imageSize;\n}\n\nNppiRect GetROI(ImageInfo_t imageInfo)\n{\n    NppiRect imageROI;\n\n    imageROI.x = 0;    imageROI.y = 0;\n    imageROI.width = imageInfo.nWidth;\n    imageROI.height = imageInfo.nHeight;\n\n    return imageROI;\n}\n```", "```cpp\nint ResizeGPU(ImageInfo_t &dstImage, ImageInfo_t &srcImage, \n                 NppiSize &dstSize, NppiRect &dstROI, \n                 NppiSize &srcSize, NppiRect &srcROI, scale)\n{\n    // update output image size\n    dstSize.width = dstROI.width = dstImage.nWidth;\n    dstSize.height = dstROI.height = dstImage.nHeight;\n\n    nppiResize_8u_C3R(srcImage.pDataCUDA, srcImage.nPitchCUDA, \n                      srcSize, srcROI, \n                      dstImage.pDataCUDA, dstImage.nPitchCUDA, \n                      dstSize, dstROI,\n                      NPPI_INTER_LANCZOS);\n    return 0;\n}\n```", "```cpp\nvoid ResizeCPU(const char* szInputFile, ImageInfo_t &dstImage) {\n    FreeImage_Rescale(dib, dstImage.nWidth, dstImage.nHeight, FILTER_LANCZOS3);\n}\n```", "```cpp\nFreeImage_Initialise();\nImageInfo_t srcImage, dstImage;\nLoadImage(szInputFile, srcImage);\n```", "```cpp\n// copy loaded image to the device memory\nsrcImage.pDataCUDA = \n             nppiMalloc_8u_C3(srcImage.nWidth, srcImage.nHeight, \n                              &srcImage.nPitchCUDA);\ncudaMemcpy2D(srcImage.pDataCUDA, srcImage.nPitchCUDA, \n             srcImage.pData, srcImage.nPitch, \n             srcImage.nWidth * srcImage.nChannel * sizeof(Npp8u), \n             srcImage.nHeight,\n             cudaMemcpyHostToDevice);\n```", "```cpp\nstd::memcpy(&dstImage, &srcImage, sizeof(ImageInfo_t));\ndstImage.nWidth *= scaleRatio;\nsrcImage.nHeight *= scaleRatio;\ndstImage.pDataCUDA = \n                nppiMalloc_8u_C3(dstImage.nWidth, dstImage.nHeight, \n                                 &dstImage.nPitchCUDA);\n```", "```cpp\nRunNppResize(dstImage, srcImage, dstImageSize, dstROI, srcImageSize, srcROI, scaleRatio);\nRunCpuResize(szInputFile, dstImage);\n```", "```cpp\n// Save resized image as file from the device\nFIBITMAP *pDstImageBitmap = \n                FreeImage_Allocate(dstImage.nWidth, dstImage.nHeight, \n                                   dstImage.nBPP);\n\ndstImage.nPitch = FreeImage_GetPitch(pDstImageBitmap);\ndstImage.pData = FreeImage_GetBits(pDstImageBitmap);\n\ncudaMemcpy2D(dstImage.pData, dstImage.nPitch, \n             dstImage.pDataCUDA, dstImage.nPitchCUDA, \n             dstImage.nWidth * dstImage.nChannel * sizeof(Npp8u),\n             dstImage.nHeight, cudaMemcpyDeviceToHost);\n\nFreeImage_Save(FIF_JPEG, pDstImageBitmap, szOutputFile, JPEG_DEFAULT);\n```", "```cpp\nnppiFree(srcImage.pDataCUDA);\nnppiFree(dstImage.pDataCUDA);\n\nFreeImage_DeInitialise();\n```", "```cpp\n$ nvcc -run -m64 -std=c++11 -I/usr/local/cuda/samples/common/inc -gencode arch=compute_70,code=sm_70 -lnppc -lnppif -lnppisu -lnppig -lnppicom -lnpps -lfreeimage -o imageFilter ./imageFilter.cpp\n```", "```cpp\n$ ls -alh *.jpg\n-rw-rw-r-- 1 ubuntu ubuntu 91K Nov 13 22:31 flower.jpg\n-rw-rw-r-- 1 ubuntu ubuntu 23K Nov 17 02:46 output.jpg\n```", "```cpp\nRescale flower.jpg in 0.5 ratio.\nCPU: 23.857 ms\nGPU: 0.04576 ms\nDone (generated output.jpg)\n```", "```cpp\n#include <iostream>\n#include <cuda_runtime.h>\n#include <npp.h>\n```", "```cpp\nvoid GetData(float** buffer, size_t size)\n{\n    (*buffer) = (float*) new float[size];\n\n    for (int i = 0; i < size; i++) {\n        (*buffer)[i] = float(rand() % 0xFFFF) / RAND_MAX;\n    }\n}\n```", "```cpp\nint GetWorkspaceSize(int signalSize)\n{\n    int bufferSize, tempBufferSize;\n\n    nppsSumGetBufferSize_32f(signalSize, &tempBufferSize);\n    bufferSize = std::max(bufferSize, tempBufferSize);\n    nppsMinGetBufferSize_32f(signalSize, &tempBufferSize);\n    bufferSize = std::max(bufferSize, tempBufferSize);\n    nppsMaxGetBufferSize_32f(signalSize, &tempBufferSize);\n    bufferSize = std::max(bufferSize, tempBufferSize);\n    nppsMeanGetBufferSize_32f(signalSize, &tempBufferSize);\n    bufferSize = std::max(bufferSize, tempBufferSize);\n    nppsNormDiffL2GetBufferSize_32f(signalSize, &tempBufferSize);\n    bufferSize = std::max(bufferSize, tempBufferSize);\n\n    return bufferSize;\n}\n```", "```cpp\nGetData(&h_input1, buf_size);\nGetData(&h_input2, buf_size);\nworkspace_size = GetWorkspaceSize(buf_size);\n```", "```cpp\ncudaMalloc((void **)&d_input1, buf_size * sizeof(float));\ncudaMalloc((void **)&d_input2, buf_size * sizeof(float));\ncudaMalloc((void **)&d_output, sizeof(float));\ncudaMalloc((void **)&d_workspace, workspace_size * sizeof(Npp8u));\n```", "```cpp\nnppsSum_32f(d_input1, buf_size, d_output, d_workspace);\ncudaMemcpy(&h_output, d_output, sizeof(float), cudaMemcpyDeviceToHost);\nstd::cout << \"Sum: \" << h_output << std::endl;\n\nnppsMin_32f(d_input1, buf_size, d_output, d_workspace);\ncudaMemcpy(&h_output, d_output, sizeof(float), cudaMemcpyDeviceToHost);\nstd::cout << \"Min: \" << h_output << std::endl;\n\nnppsMax_32f(d_input1, buf_size, d_output, d_workspace);\ncudaMemcpy(&h_output, d_output, sizeof(float), cudaMemcpyDeviceToHost);\nstd::cout << \"Max: \" << h_output << std::endl;\n\nnppsMean_32f(d_input1, buf_size, d_output, d_workspace);\ncudaMemcpy(&h_output, d_output, sizeof(float), cudaMemcpyDeviceToHost);\nstd::cout << \"Mean: \" << h_output << std::endl;\n```", "```cpp\nnppsNormDiff_L2_32f(d_input1, d_input2, buf_size, d_output, d_workspace); \ncudaMemcpy(&h_output, d_output, sizeof(float), cudaMemcpyDeviceToHost);\nstd::cout << \"NormDiffL2: \" << h_output << std::endl;\n```", "```cpp\n$ nvcc -run -m64 -std=c++11 -I/usr/local/cuda/samples/common/inc -gencode arch=compute_70,code=sm_70 -lnppc -lnppif -lnppisu -lnppig -lnppicom -lnpps -o statisticsNPP ./statisticsNPP.cpp\n```", "```cpp\nSum: 0.00100016\nMin: 1.30432e-06\nMax: 3.04836e-05\nMean: 1.56275e-05\nNormDiffL2: 9.46941e-05\n```", "```cpp\n$ sudo apt-get install -y --no-install-recommends \\\n cmake git libgtk2.0-dev pkg-config libavcodec-dev \\\n    libavformat-dev libswscale-dev \\\n libatlas-base-dev gfortran libeigen3-dev \\\n libgtkglext1 libgtkglext1-dev\n```", "```cpp\n$ sudo apt-get install -y \u2014no-install-recommends \\\n Libgtkglext1 libgtkglext1-dev\n```", "```cpp\n# We are install OpenCV 4.1.1\nOPENCV_VERSION=4.1.1\nOPENCV_DIR=opencv\n\n# Download OpenCV and contrib source codes\nmkdir -p ${OPENCV_DIR}\nwget -O ${OPENCV_DIR}/opencv-${OPENCV_VERSION}.tar.gz https://github.com/opencv/opencv/archive/${OPENCV_VERSION}.tar.gz\nwget -O ${OPENCV_DIR}/opencv_contrib-${OPENCV_VERSION}.tar.gz https://github.com/opencv/opencv_contrib/archive/${OPENCV_VERSION}.tar.gz\n\n# Untar the files\ntar -C ${OPENCV_DIR} -xzf ${OPENCV_DIR}/opencv-${OPENCV_VERSION}.tar.gz\ntar -C ${OPENCV_DIR} -xzf ${OPENCV_DIR}/opencv_contrib-${OPENCV_VERSION}.tar.gz\n```", "```cpp\n# Build the codes and install\ncd ${OPENCV_DIR}/opencv-${OPENCV_VERSION}\nmkdir build\ncd build\ncmake -D CMAKE_BUILD_TYPE=RELEASE \\\n -D CMAKE_INSTALL_PREFIX=/usr/local \\\n -D ENABLE_PRECOMPILED_HEADERS=OFF \\\n -D OPENCV_GENERATE_PKGCONFIG=ON \\\n -D WITH_CUDA=ON -D WITH_CUVID=OFF -D BUILD_opencv_cudacodec=OFF \\\n -D ENABLE_FAST_MATH=1 \\\n -D CUDA_FAST_MATH=1 \\\n -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-${OPENCV_VERSION}/modules \\\n -D WITH_CUBLAS=1 \\\n -D PYTHON_DEFAULT_EXECUTABLE=`which python3` \\\n -D INSTALL_PYTHON_EXAMPLES=ON \\\n -D BUILD_EXAMPLES=ON ..\nmake -j$(nproc)\nsudo make install -j$(nproc) \n```", "```cpp\n$ pkg-config \u2014cflags opencv4 -I/usr/local/include/opencv4/opencv -I/usr/local/include/opencv4\n```", "```cpp\ncv::cuda::GpuMat cuda_mem = cv::cuda::GpuMat(src.rows, src.cols, CV_8UC1);\n```", "```cpp\n#include <iostream>\n#include <string>\n#include \"opencv2/opencv.hpp\"\n```", "```cpp\nvoid BlurHost(std::string filename)\n{\n    cv::Mat src = cv::imread(filename, 1);\n    cv::Mat dst; \n    cv::TickMeter tm;\n\n    tm.reset();\n    tm.start();\n    cv::bilateralFilter(src, dst, 10, 50, 50);\n    tm.stop();\n\n    std::cout << \"CPU Time: \" << tm.getTimeMilli() << \" ms.\" << std::endl;\n    cv::imwrite(\"result_host.jpg\", dst);\n}\n```", "```cpp\nvoid BlurCuda(std::string filename)\n{\n    cv::Mat src = cv::imread(filename, 1);\n    cv::Mat dst;\n    cv::cuda::GpuMat src_cuda = cv::cuda::GpuMat(src.rows, \n                                                 src.cols, CV_8UC1);\n    cv::cuda::GpuMat dst_cuda = cv::cuda::GpuMat(src.rows, \n                                                 src.cols, CV_8UC1);\n    cv::TickMeter tm;\n\n    // warm-up\n    cv::cuda::bilateralFilter(src_cuda, dst_cuda, 10, 50, 50);\n\n    tm.reset();\n    tm.start();\n    src_cuda.upload(src);\n    cv::cuda::bilateralFilter(src_cuda, dst_cuda, 10, 50, 50);\n    dst_cuda.download(dst);\n    tm.stop();\n\n    std::cout << \"GPU Time: \" << tm.getTimeMilli() \n                  << \" ms.\" << std::endl;\n    cv::imwrite(\"result_cuda.jpg\", dst);\n}\n```", "```cpp\n  int main(int argc, char *argv[])\n  {\n      std::string filename(\"flower.JPG\");\n\n      BlurHost(filename);\n      BlurCuda(filename);\n\n      return 0;\n  }\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc `pkg-config opencv4 --cflags --libs` -o blur ./blur.cpp\n```", "```cpp\nCPU Time: 57.6544 ms.\nGPU Time: 2.97784 ms.\n```", "```cpp\nMat::setDefaultAllocator(cuda::HostMem::getAllocator(cuda::HostMem::PAGE_LOCKED));\n```", "```cpp\nconst int num_stream = 4;\ncuda::Stream stream[num_stream];\n```", "```cpp\nMat src = imread(filename, 1);\nMat dst;\ncuda::GpuMat src_cuda[num_stream], dst_cuda[num_stream];\nfor (int i = 0; i < num_stream; i++)\n    src_cuda[i] = cuda::GpuMat(src);\n```", "```cpp\nfor (int i = 0; i < num_stream; i++) {\n    src_cuda[i].upload(src, stream[i]);\n    cuda::bilateralFilter(src_cuda[i], dst_cuda[i], 21, 150.f, \n                          150.f, BORDER_DEFAULT, stream[i]);\n    dst_cuda[i].download(dst, stream[i]);\n}\n```", "```cpp\nfor (int i = 0; i < num_stream; i++)\n    stream[i].waitForCompletion();\n```", "```cpp\nbilateralFilter(src, dst, 21, 150, 150);\n```", "```cpp\n$ nvcc -run -m64 -gencode arch=compute_70,code=sm_70 -I/usr/local/cuda/samples/common/inc `pkg-config opencv4 --cflags --libs` -o blur ./blur.cpp\nCPU Time: 84.8649 ms.\nGPU Time: 1.60979 ms.\n```", "```cpp\nfrom numba import vectorize\n@vectorize([\"float32(float32, float32, float32)\"], target='cuda')\ndef saxpy(scala, a, b):\nreturn scala * a + b\n```", "```cpp\nfrom numba import cuda\n\n@cuda.jit\ndef matmul(d_c, d_a, d_b):\n    x, y = cuda.grid(2)\n    if (x < d_c.shape[0] and y < d_c.shape[1]):\n        sum = 0\n        for k in range(d_a.shape[1]):\n            sum += d_a[x, k] * d_b[k, y]\n        d_c[x, y] = sum\n```", "```cpp\nmatmul[dimGrid, dimBlock](d_c, d_a, d_b)\n```", "```cpp\n$ pip3 install numba\n$ export NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so\n$ export NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice/\n```", "```cpp\nnumba.cuda.cudadrv.error.NvvmSupportError: libNVVM cannot be found. Do `conda install cudatoolkit`:\nlibrary nvvm not found\n```", "```cpp\nimport numpy as np\nfrom numba import vectorize\nfrom timeit import default_timer as timer\n```", "```cpp\n@vectorize([\"float32(float32, float32, float32)\"], target='cuda')\ndef saxpy_cuda(scala, a, b):\n    return scala * a + b\n```", "```cpp\n@vectorize([\"float32(float32, float32, float32)\"], target='parallel')\ndef saxpy_host(scala, a, b):\n    return scala * a + b\n```", "```cpp\nscala = 2.0\nnp.random.seed(2019)\nprint(\"size \\t\\t CUDA \\t\\t CPU\")\nfor i in range(16,20):\n    N = 1 << i\n    a = np.random.rand(N).astype(np.float32)\n    b = np.random.rand(N).astype(np.float32)\n    c = np.zeros(N, dtype=np.float32)\n\n    # warm-up\n    c = saxpy_cuda(scala, a, b)\n\n    # measuring execution time\n    start = timer()\n    c = saxpy_host(scala, a, b)\n    elapsed_time_host= (timer() - start) * 1e3\n    start = timer()\n    c = saxpy_cuda(scala, a, b)\n    elapsed_time_cuda = (timer() - start) * 1e3\n    print(\"[%d]: \\t%.3f ms\\t %.3f ms\" % (N, elapsed_time_cuda, elapsed_time_host))\n```", "```cpp\nsize         CUDA        CPU\n[65536]:   1.174 ms    0.199 ms\n[131072]:  1.362 ms    0.201 ms\n[262144]:  2.240 ms    0.284 ms\n[524288]:  2.384 ms    0.337 ms\n```", "```cpp\nimport numpy as np\nfrom numba import cuda\nfrom timeit import default_timer as timer\n```", "```cpp\n@cuda.jit\ndef matmul(d_c, d_a, d_b):\n    x, y = cuda.grid(2)\n    if (x < d_c.shape[0] and y < d_c.shape[1]):\n        sum = 0\n        for k in range(d_a.shape[1]):\n            sum += d_a[x, k] * d_b[k, y]\n        d_c[x, y] = sum\n```", "```cpp\nN = 8192\na = np.random.rand(N, N).astype(np.float32)\nb = np.random.rand(N, N).astype(np.float32)\n```", "```cpp\nd_a = cuda.to_device(a)\nd_b = cuda.to_device(b)\n```", "```cpp\nd_c = cuda.device_array((N, N))\n```", "```cpp\nstart = timer()\nmatmul[dimGrid, dimBlock](d_c, d_a, d_b)\nelapsed_time_gpu = (timer() - start) * 1e3\n```", "```cpp\nc = d_c.copy_to_host()\n```", "```cpp\n# matrix multiplication (cpu)\nstart = timer()\nc_host = np.matmul(a, b)\nelapsed_time_cpu = (timer() - start) * 1e3\n\n# print elapse times\nprint(\"Elapsed Time\")\nprint(\"GPU: %.3f ms\" % elapsed_time_gpu)\nprint(\"CPU: %.3f ms\" % elapsed_time_cpu)\n\nif (np.allclose(c_host, c)):\nprint(\"Done.\")\nelse:\nprint(\"GPU and host results are mismatching.\")\n```", "```cpp\nElapsed Time\nGPU: 104.694 ms\nCPU: 1539.005 ms\nDone.\n```", "```cpp\n$ pip3 install cupy-cuda101    # CUDA 10.1\n$ pip3 install cupy-cuda101    # CUDA 10.0\n$ pip3 install cupy-cuda902    # CUDA 9.2\n```", "```cpp\n>>> x = cp.arange(5).astype('f') \n>>> x \narray([0., 1., 2., 3., 4.], dtype=float32) \n>>> y = cp.arange(5).astype('f') \n>>> 0.5 * x + y \narray([0\\. , 1.5, 3\\. , 4.5, 6\\. ], dtype=float32)\n```", "```cpp\n>>> x = cp.random.uniform(0, 1, (2, 4)).astype('float32') \n>>> y = cp.random.uniform(0, 1, (4, 2)).astype('float32') \n>>> cp.matmul(x, y)\narray([[0.6514087, 0.826463 ], \n [0.7826104, 0.2878886]], dtype=float32)\n```", "```cpp\n>>> x = cp.random.uniform(0, 1, (2, 4)).astype('float32') \n>>> type(x) \n<class 'cupy.core.core.ndarray'>\n```", "```cpp\ntype(cp.asnumpy(x))\n<class 'numpy.ndarray'>\n```", "```cpp\n>>> gpu = cp.random.uniform(0, 1, (2, 4)).astype('float32') \n>>> cpu = np.random.uniform(0, 1, (2, 4)).astype('float32')\n>>> gpu + cp.asarray(cpu) \narray([[0.8649391 , 1.1412742 , 1.1280626 , 0.38262686],\n [0.44767308, 0.738155 , 0.8397665 , 1.5165564 ]], dtype=float32)\n>>> cpu + cp.asnumpy(gpu) \narray([[0.8649391 , 1.1412742 , 1.1280626 , 0.38262686], \n [0.44767308, 0.738155 , 0.8397665 , 1.5165564 ]], dtype=float32)\n```", "```cpp\n>>> squared_diff = cp.ElementwiseKernel( \n...     'float32 x, float32 y', \n...     'float32 z', \n...     'z = (x - y) * (x - y)', \n...     'squared_diff')\n```", "```cpp\n>>> x = cp.random.uniform(0, 1, (2, 4)).astype('float32') \n>>> y = cp.random.uniform(0, 1, (2, 4)).astype('float32') \n>>> squared_diff(x, y) \narray([[0.54103416, 0.01342529, 0.01425287, 0.67101586], \n [0.04841561, 0.09939388, 0.46790633, 0.00203693]], dtype=float32)\n>>> squared_diff(x, 0.5) \narray([[0.23652133, 0.22603741, 0.08065639, 0.00647551], \n [0.00029328, 0.07454127, 0.00666 , 0.18399356]], dtype=float32)\n```", "```cpp\nimport pycuda.autoinit     # initialize CUDA devices\nfrom pycuda import driver, compiler, gpuarray\nfrom string import Template\n\nkernel_code_template = Template(\"\"\"\n__global__ void matmul_kernel(float *d_C, float *d_A, float *d_B)\n{\n    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n    ...\n}\n\"\"\")\n\nmod = compiler.SourceModule(kernel_code_template.substitute(MATRIX_SIZE=N))\nmatmul_kernel = mod.get_function(\"matmul_kernel\")\nmatmul_kernel(driver.Out(C), driver.In(A), driver.In(B), block=(dimBlock, dimBlock, 1), grid=(dimGrid, dimGrid))\n```", "```cpp\nd_A = driver.to_device(A) # cudaMemcpyHostToDevice\nA = driver.from_device_like(d_A) # cudaMemcpyDeviceToHost\n```", "```cpp\n$ sudo apt-get install build-essential python-dev python-setuptools libboost-python-dev libboost-thread-dev\n\n$ tar -xzf pycuda-2019.1.2.tar.gz\n$ cd pycuda-2019.1.2\n$ python3 ./configure.py --cuda-root=/usr/local/cuda --cudadrv-lib-dir=/usr/lib \\\n --boost-inc-dir=/usr/include --boost-lib-dir=/usr/lib \\\n --boost-python-libname=boost_python-py36 --boost-thread-libname=boost_thread\n$ python3 setup.py build\n$ sudo python3 setup.py install\n```", "```cpp\nimport pycuda.autoinit\nfrom pycuda import driver, compiler, gpuarray\nimport numpy as np\nfrom string import Template\nimport timeit\n```", "```cpp\nkernel_code_template = Template(\"\"\"\n__global__ void matmul_kernel(float *d_C, float *d_A, float *d_B)\n{\n    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.f;\n    for (int e = 0; e < ${MATRIX_SIZE}; e++)\n        sum += d_A[idx_y * ${MATRIX_SIZE} + e] * d_B[e * ${MATRIX_SIZE} + idx_x];\n    d_C[idx_y * ${MATRIX_SIZE} + idx_x] = sum;\n}\n\"\"\")\n```", "```cpp\nN = 8192\nnp.random.seed(2019)\nA = np.random.rand(N, N).astype(np.float32)\nB = np.random.rand(N, N).astype(np.float32)\nC = np.zeros((N, N), dtype=np.float32)\n```", "```cpp\nmod = compiler.SourceModule( \\\n        kernel_code_template.substitute(MATRIX_SIZE=N))\n```", "```cpp\nmatmul_kernel = mod.get_function(\"matmul_kernel\")\n```", "```cpp\nd_A = gpuarray.to_gpu(A)\nd_B = gpuarray.to_gpu(B)\nd_C = gpuarray.zeros((N, N), dtype=np.float32)\n```", "```cpp\ndimBlock = 16\ndimGrid = int((N + dimBlock - 1) / dimBlock)\n```", "```cpp\nstart = driver.Event()\nstop = driver.Event()\n```", "```cpp\nprint(\"Started GPU operation...\")\nstart.record()\n\nmatmul_kernel(d_C, d_A, d_B, \n    block=(dimBlock, dimBlock, 1), \n    grid=(dimGrid, dimGrid))\n\nstop.record()\nstop.synchronize()\ngpu_time = stop.time_since(start)\nprint(\"GPU Execution Time: %.3f ms\" % (gpu_time))\n```", "```cpp\nprint(\"Started Host operation...\")\nstart = timeit.default_timer()\nc_host = np.matmul(A, B)\nhost_time = timeit.default_timer() - start\n\nprint(\"CPU Execution Time: %.3f ms\" % (host_time * 1e3))\n\nif (np.allclose(c_host, d_C.get())):\n    print(\"Done.\")\nelse:\n    print(\"GPU and host results are mismatching.\")\n```", "```cpp\nStarted GPU operation...\nGPU Execution Time: 657.547 ms\nStarted Host operation...\nCPU Execution Time: 1531.133 ms\nDone.\n```", "```cpp\nNVBLAS_CPU_BLAS_LIB libopenblas.so\nNVBLAS_LOGFILE nvblas.log\nNVBLAS_GPU_LIST 0\nNVBLAS_AUTOPIN_MEM_ENABLED\n```", "```cpp\n$ sudo apt-get install libopenblas-base libopenblas-dev\n```", "```cpp\n$ LD_PRELOAD=libnvblas.so octave sgemm.m\n```", "```cpp\n$ LD_PRELOAD=libnvblas.so Rscript sgemm.R\n```", "```cpp\n$ sudo apt-get install octave # for octave installation\n$ sudo apt-get install r-base # for R installation\n```", "```cpp\nfor i = 1:5 \n    N = 512*(2^i);\n    A = single(rand(N,N));\n    B = single(rand(N,N));\n\n    start = clock();\n    C = A * B;\n    elapsedTime = etime(clock(), start);\n\n    gFlops = 2*N*N*N/(elapsedTime * 1e+9);\n    printf(\"Elapsed Time [%d]: %.3f ms, %.3f GFlops\\n\", N, elapsedTime, gFlops);\nend\n```", "```cpp\n$ LD_PRELOAD=libnvblas.so octave sgemm.m\n```", "```cpp\nset.seed(2019)\nfor(i in seq(1:5)) {\n    N = 512*(2^i)\n    A = matrix(rnorm(N^2, mean=0, sd=1), nrow=N) \n    B = matrix(rnorm(N^2, mean=0, sd=1), nrow=N) \n    elapsedTime = system.time({C = A %*% B})[3]\n    gFlops = 2*N*N*N/(elapsedTime * 1e+9);\n    print(sprintf(\"Elapsed Time [%d]: %3.3f ms, %.3f GFlops\", N, elapsedTime, gFlops))\n}\n```", "```cpp\n$ LD_PRELOAD=libnvblas.so Rscript sgemm.R\n```", "```cpp\nd_A = gpuArray(A);\n```", "```cpp\nN = 8192;\nA = single(rand(N,N));\nB = single(rand(N,N));\n\nstart = clock();\nC = A * B; \nelapsedTime = etime(clock(), start);\ngFlops = 2*N*N*N/(elapsedTime * 1e+9);\nfprintf(\"Elapsed Time: %.3f ms, %.3f GFlops\\n\", elapsedTime, gFlops);\n```", "```cpp\n$ matlab -r \"run('host.m'); exit;\" -nodisplay\nElapsed Time: 6.421 ms, 171.243 Gflops\n```", "```cpp\nN = 8192;\nA = single(rand(N,N));\nB = single(rand(N,N));\n\nd_A = gpuArray(A);    'GPU memory allocation\nd_B = gpuArray(B);    'GPU memory allocation\n\nstart = clock();\nd_C = d_A * d_B;\nelapsedTime = etime(clock(), start);\ngFlops = 2*N*N*N/(elapsedTime * 1e+9);\nfprintf(\"Elapsed Time: %.3f ms, %.3f GFlops\\n\", elapsedTime, gFlops);\n```", "```cpp\n$ matlab -r \"run('cuda.m'); exit;\" -nodisplay\nElapsed Time: 0.179 ms, 6140.739 Gflops.\n```"]